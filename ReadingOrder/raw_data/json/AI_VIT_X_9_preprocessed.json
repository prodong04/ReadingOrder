{
    "id": "329909b4-0f94-11ef-8828-426932df3dcf",
    "pdf_path": "./pdf/AI_VIT_X/1602.01783v2.pdf",
    "elements": [
        {
            "bounding_box": [
                {
                    "x": 498,
                    "y": 368
                },
                {
                    "x": 1991,
                    "y": 368
                },
                {
                    "x": 1991,
                    "y": 445
                },
                {
                    "x": 498,
                    "y": 445
                }
            ],
            "category": "paragraph",
            "html": "<p id='0' style='font-size:22px'>Asynchronous Methods for Deep Reinforcement Learning</p>",
            "id": 0,
            "page": 1,
            "text": "Asynchronous Methods for Deep Reinforcement Learning"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 587
                },
                {
                    "x": 735,
                    "y": 587
                },
                {
                    "x": 735,
                    "y": 1001
                },
                {
                    "x": 224,
                    "y": 1001
                }
            ],
            "category": "paragraph",
            "html": "<p id='1' style='font-size:18px'>Volodymyr Mnih1<br>Adria Puigdomenech Badia1<br>Mehdi Mirza 1,2<br>Alex Graves1<br>Tim Harley1<br>Timothy P. Lillicrap1<br>David Silver1<br>Koray Kavukcuoglu 1</p>",
            "id": 1,
            "page": 1,
            "text": "Volodymyr Mnih1 Adria Puigdomenech Badia1 Mehdi Mirza 1,2 Alex Graves1 Tim Harley1 Timothy P. Lillicrap1 David Silver1 Koray Kavukcuoglu 1"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1000
                },
                {
                    "x": 1527,
                    "y": 1000
                },
                {
                    "x": 1527,
                    "y": 1098
                },
                {
                    "x": 224,
                    "y": 1098
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='2' style='font-size:18px'>1 Google DeepMind<br>2 Montreal Institute for Learning Algorithms (MILA), University of Montreal</p>",
            "id": 2,
            "page": 1,
            "text": "1 Google DeepMind 2 Montreal Institute for Learning Algorithms (MILA), University of Montreal"
        },
        {
            "bounding_box": [
                {
                    "x": 619,
                    "y": 1162
                },
                {
                    "x": 818,
                    "y": 1162
                },
                {
                    "x": 818,
                    "y": 1217
                },
                {
                    "x": 619,
                    "y": 1217
                }
            ],
            "category": "paragraph",
            "html": "<p id='3' style='font-size:20px'>Abstract</p>",
            "id": 3,
            "page": 1,
            "text": "Abstract"
        },
        {
            "bounding_box": [
                {
                    "x": 303,
                    "y": 1233
                },
                {
                    "x": 1135,
                    "y": 1233
                },
                {
                    "x": 1135,
                    "y": 2135
                },
                {
                    "x": 303,
                    "y": 2135
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='4' style='font-size:16px'>We propose a conceptually simple and<br>lightweight framework for deep reinforce-<br>ment learning that uses asynchronous gradient<br>descent for optimization of deep neural network<br>controllers. We present asynchronous variants of<br>four standard reinforcement learning algorithms<br>and show that parallel actor-learners have a<br>stabilizing effect on training allowing all four<br>methods to successfully train neural network<br>controllers. The best performing method, an<br>asynchronous variant of actor-critic, surpasses<br>the current state-of-the-art on the Atari domain<br>while training for half the time on a single<br>multi-core CPU instead of a GPU. Furthermore,<br>we show that asynchronous actor-critic succeeds<br>on a wide variety of continuous motor control<br>problems as well as on a new task of navigating<br>random 3D mazes using a visual input.</p>",
            "id": 4,
            "page": 1,
            "text": "We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input."
        },
        {
            "bounding_box": [
                {
                    "x": 226,
                    "y": 2224
                },
                {
                    "x": 555,
                    "y": 2224
                },
                {
                    "x": 555,
                    "y": 2279
                },
                {
                    "x": 226,
                    "y": 2279
                }
            ],
            "category": "paragraph",
            "html": "<p id='5' style='font-size:20px'>1. Introduction</p>",
            "id": 5,
            "page": 1,
            "text": "1. Introduction"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2310
                },
                {
                    "x": 1214,
                    "y": 2310
                },
                {
                    "x": 1214,
                    "y": 2811
                },
                {
                    "x": 223,
                    "y": 2811
                }
            ],
            "category": "paragraph",
            "html": "<p id='6' style='font-size:16px'>Deep neural networks provide rich representations that can<br>enable reinforcement learning (RL) algorithms to perform<br>effectively. However, it was previously thought that the<br>combination of simple online RL algorithms with deep<br>neural networks was fundamentally unstable. Instead, a va-<br>riety of solutions have been proposed to stabilize the algo-<br>rithm (Riedmiller, 2005; Mnih et al., 2013; 2015; Van Has-<br>selt et al., 2015; Schulman et al., 2015a). These approaches<br>share a common idea: the sequence of observed data en-<br>countered by an online RL agent is non-stationary, and on-</p>",
            "id": 6,
            "page": 1,
            "text": "Deep neural networks provide rich representations that can enable reinforcement learning (RL) algorithms to perform effectively. However, it was previously thought that the combination of simple online RL algorithms with deep neural networks was fundamentally unstable. Instead, a variety of solutions have been proposed to stabilize the algorithm (Riedmiller, 2005; Mnih , 2013; 2015; Van Hasselt , 2015; Schulman , 2015a). These approaches share a common idea: the sequence of observed data encountered by an online RL agent is non-stationary, and on-"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2842
                },
                {
                    "x": 1219,
                    "y": 2842
                },
                {
                    "x": 1219,
                    "y": 2974
                },
                {
                    "x": 224,
                    "y": 2974
                }
            ],
            "category": "paragraph",
            "html": "<p id='7' style='font-size:14px'>Proceedings of the 33rd International Conference on Machine<br>Learning, New York, NY, USA, 2016. JMLR: W&CP volume<br>48. Copyright 2016 by the author(s).</p>",
            "id": 7,
            "page": 1,
            "text": "Proceedings of the 33rd International Conference on Machine Learning, New York, NY, USA, 2016. JMLR: W&CP volume 48. Copyright 2016 by the author(s)."
        },
        {
            "bounding_box": [
                {
                    "x": 1603,
                    "y": 589
                },
                {
                    "x": 2234,
                    "y": 589
                },
                {
                    "x": 2234,
                    "y": 989
                },
                {
                    "x": 1603,
                    "y": 989
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='8' style='font-size:16px'>VMNIH@GOOGLE.COM<br>ADRIAP@GOOGLE.COM<br>MIRZAMOM@IRO.UMONTREAL.CA<br>GRAVESA@GOOGLE.COM<br>THARLEY@GOOGLE.COM<br>COUNTZERO@GOOGLE.COM<br>DAVIDSILVER@GOOGLE.COM<br>KORAYK@GOOGLE.COM</p>",
            "id": 8,
            "page": 1,
            "text": "VMNIH@GOOGLE.COM ADRIAP@GOOGLE.COM MIRZAMOM@IRO.UMONTREAL.CA GRAVESA@GOOGLE.COM THARLEY@GOOGLE.COM COUNTZERO@GOOGLE.COM DAVIDSILVER@GOOGLE.COM KORAYK@GOOGLE.COM"
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1167
                },
                {
                    "x": 2263,
                    "y": 1167
                },
                {
                    "x": 2263,
                    "y": 1569
                },
                {
                    "x": 1273,
                    "y": 1569
                }
            ],
            "category": "paragraph",
            "html": "<p id='9' style='font-size:16px'>line RL updates are strongly correlated. By storing the<br>agent's data in an experience replay memory, the data can<br>be batched (Riedmiller, 2005; Schulman et al., 2015a) or<br>randomly sampled (Mnih et al., 2013; 2015; Van Hasselt<br>et al., 2015) from different time-steps. Aggregating over<br>memory in this way reduces non-stationarity and decorre-<br>lates updates, but at the same time limits the methods to<br>off-policy reinforcement learning algorithms.</p>",
            "id": 9,
            "page": 1,
            "text": "line RL updates are strongly correlated. By storing the agent's data in an experience replay memory, the data can be batched (Riedmiller, 2005; Schulman , 2015a) or randomly sampled (Mnih , 2013; 2015; Van Hasselt , 2015) from different time-steps. Aggregating over memory in this way reduces non-stationarity and decorrelates updates, but at the same time limits the methods to off-policy reinforcement learning algorithms."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1591
                },
                {
                    "x": 2264,
                    "y": 1591
                },
                {
                    "x": 2264,
                    "y": 1893
                },
                {
                    "x": 1273,
                    "y": 1893
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='10' style='font-size:16px'>Deep RL algorithms based on experience replay have<br>achieved unprecedented success in challenging domains<br>such as Atari 2600. However, experience replay has several<br>drawbacks: it uses more memory and computation per real<br>interaction; and it requires off-policy learning algorithms<br>that can update from data generated by an older policy.</p>",
            "id": 10,
            "page": 1,
            "text": "Deep RL algorithms based on experience replay have achieved unprecedented success in challenging domains such as Atari 2600. However, experience replay has several drawbacks: it uses more memory and computation per real interaction; and it requires off-policy learning algorithms that can update from data generated by an older policy."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 1916
                },
                {
                    "x": 2264,
                    "y": 1916
                },
                {
                    "x": 2264,
                    "y": 2514
                },
                {
                    "x": 1271,
                    "y": 2514
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='11' style='font-size:16px'>In this paper we provide a very different paradigm for deep<br>reinforcement learning. Instead of experience replay, we<br>asynchronously execute multiple agents in parallel, on mul-<br>tiple instances of the environment. This parallelism also<br>decorrelates the agents' data into a more stationary process,<br>since at any given time-step the parallel agents will be ex-<br>periencing a variety of different states. This simple idea<br>enables a much larger spectrum of fundamental on-policy<br>RL algorithms, such as Sarsa, n-step methods, and actor-<br>critic methods, as well as off-policy RL algorithms such<br>as Q-learning, to be applied robustly and effectively using<br>deep neural networks.</p>",
            "id": 11,
            "page": 1,
            "text": "In this paper we provide a very different paradigm for deep reinforcement learning. Instead of experience replay, we asynchronously execute multiple agents in parallel, on multiple instances of the environment. This parallelism also decorrelates the agents' data into a more stationary process, since at any given time-step the parallel agents will be experiencing a variety of different states. This simple idea enables a much larger spectrum of fundamental on-policy RL algorithms, such as Sarsa, n-step methods, and actorcritic methods, as well as off-policy RL algorithms such as Q-learning, to be applied robustly and effectively using deep neural networks."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2537
                },
                {
                    "x": 2264,
                    "y": 2537
                },
                {
                    "x": 2264,
                    "y": 2989
                },
                {
                    "x": 1273,
                    "y": 2989
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='12' style='font-size:16px'>Our parallel reinforcement learning paradigm also offers<br>practical benefits. Whereas previous approaches to deep re-<br>inforcement learning rely heavily on specialized hardware<br>such as GPUs (Mnih et al., 2015; Van Hasselt et al., 2015;<br>Schaul et al., 2015) or massively distributed architectures<br>(Nair et al., 2015), our experiments run on a single machine<br>with a standard multi-core CPU. When applied to a vari-<br>ety of Atari 2600 domains, on many games asynchronous<br>reinforcement learning achieves better results, in far less</p>",
            "id": 12,
            "page": 1,
            "text": "Our parallel reinforcement learning paradigm also offers practical benefits. Whereas previous approaches to deep reinforcement learning rely heavily on specialized hardware such as GPUs (Mnih , 2015; Van Hasselt , 2015; Schaul , 2015) or massively distributed architectures (Nair , 2015), our experiments run on a single machine with a standard multi-core CPU. When applied to a variety of Atari 2600 domains, on many games asynchronous reinforcement learning achieves better results, in far less"
        },
        {
            "bounding_box": [
                {
                    "x": 58,
                    "y": 894
                },
                {
                    "x": 148,
                    "y": 894
                },
                {
                    "x": 148,
                    "y": 2332
                },
                {
                    "x": 58,
                    "y": 2332
                }
            ],
            "category": "footer",
            "html": "<br><footer id='13' style='font-size:14px'>2016<br>Jun<br>16<br>[cs.LG]<br>arXiv:1602.01783v2</footer>",
            "id": 13,
            "page": 1,
            "text": "2016 Jun 16 [cs.LG] arXiv:1602.01783v2"
        },
        {
            "bounding_box": [
                {
                    "x": 778,
                    "y": 192
                },
                {
                    "x": 1710,
                    "y": 192
                },
                {
                    "x": 1710,
                    "y": 236
                },
                {
                    "x": 778,
                    "y": 236
                }
            ],
            "category": "header",
            "html": "<header id='14' style='font-size:18px'>Asynchronous Methods for Deep Reinforcement Learning</header>",
            "id": 14,
            "page": 2,
            "text": "Asynchronous Methods for Deep Reinforcement Learning"
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 284
                },
                {
                    "x": 1213,
                    "y": 284
                },
                {
                    "x": 1213,
                    "y": 784
                },
                {
                    "x": 222,
                    "y": 784
                }
            ],
            "category": "paragraph",
            "html": "<p id='15' style='font-size:16px'>time than previous GPU-based algorithms, using far less<br>resource than massively distributed approaches. The best<br>of the proposed methods, asynchronous advantage actor-<br>critic (A3C), also mastered a variety of continuous motor<br>control tasks as well as learned general strategies for ex-<br>ploring 3D mazes purely from visual inputs. We believe<br>that the success of A3C on both 2D and 3D games, discrete<br>and continuous action spaces, as well as its ability to train<br>feedforward and recurrent agents makes it the most general<br>and successful reinforcement learning agent to date.</p>",
            "id": 15,
            "page": 2,
            "text": "time than previous GPU-based algorithms, using far less resource than massively distributed approaches. The best of the proposed methods, asynchronous advantage actorcritic (A3C), also mastered a variety of continuous motor control tasks as well as learned general strategies for exploring 3D mazes purely from visual inputs. We believe that the success of A3C on both 2D and 3D games, discrete and continuous action spaces, as well as its ability to train feedforward and recurrent agents makes it the most general and successful reinforcement learning agent to date."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 833
                },
                {
                    "x": 581,
                    "y": 833
                },
                {
                    "x": 581,
                    "y": 888
                },
                {
                    "x": 223,
                    "y": 888
                }
            ],
            "category": "paragraph",
            "html": "<p id='16' style='font-size:20px'>2. Related Work</p>",
            "id": 16,
            "page": 2,
            "text": "2. Related Work"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 920
                },
                {
                    "x": 1215,
                    "y": 920
                },
                {
                    "x": 1215,
                    "y": 1770
                },
                {
                    "x": 223,
                    "y": 1770
                }
            ],
            "category": "paragraph",
            "html": "<p id='17' style='font-size:16px'>The General Reinforcement Learning Architecture (Gorila)<br>of (Nair et al., 2015) performs asynchronous training of re-<br>inforcement learning agents in a distributed setting. In Go-<br>rila, each process contains an actor that acts in its own copy<br>of the environment, a separate replay memory, and a learner<br>that samples data from the replay memory and computes<br>gradients of the DQN loss (Mnih et al., 2015) with respect<br>to the policy parameters. The gradients are asynchronously<br>sent to a central parameter server which updates a central<br>copy of the model. The updated policy parameters are sent<br>to the actor-learners at fixed intervals. By using 100 sep-<br>arate actor-learner processes and 30 parameter server in-<br>stances, a total of 130 machines, Gorila was able to signif-<br>icantly outperform DQN over 49 Atari games. On many<br>games Gorila reached the score achieved by DQN over 20<br>times faster than DQN. We also note that a similar way of<br>parallelizing DQN was proposed by (Chavez et al., 2015).</p>",
            "id": 17,
            "page": 2,
            "text": "The General Reinforcement Learning Architecture (Gorila) of (Nair , 2015) performs asynchronous training of reinforcement learning agents in a distributed setting. In Gorila, each process contains an actor that acts in its own copy of the environment, a separate replay memory, and a learner that samples data from the replay memory and computes gradients of the DQN loss (Mnih , 2015) with respect to the policy parameters. The gradients are asynchronously sent to a central parameter server which updates a central copy of the model. The updated policy parameters are sent to the actor-learners at fixed intervals. By using 100 separate actor-learner processes and 30 parameter server instances, a total of 130 machines, Gorila was able to significantly outperform DQN over 49 Atari games. On many games Gorila reached the score achieved by DQN over 20 times faster than DQN. We also note that a similar way of parallelizing DQN was proposed by (Chavez , 2015)."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1791
                },
                {
                    "x": 1214,
                    "y": 1791
                },
                {
                    "x": 1214,
                    "y": 2341
                },
                {
                    "x": 223,
                    "y": 2341
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='18' style='font-size:18px'>In earlier work, (Li & Schuurmans, 2011) applied the<br>Map Reduce framework to parallelizing batch reinforce-<br>ment learning methods with linear function approximation.<br>Parallelism was used to speed up large matrix operations<br>but not to parallelize the collection of experience or sta-<br>bilize learning. (Grounds & Kudenko, 2008) proposed a<br>parallel version of the Sarsa algorithm that uses multiple<br>separate actor-learners to accelerate training. Each actor-<br>learner learns separately and periodically sends updates to<br>weights that have changed significantly to the other learn-<br>ers using peer-to-peer communication.</p>",
            "id": 18,
            "page": 2,
            "text": "In earlier work, (Li & Schuurmans, 2011) applied the Map Reduce framework to parallelizing batch reinforcement learning methods with linear function approximation. Parallelism was used to speed up large matrix operations but not to parallelize the collection of experience or stabilize learning. (Grounds & Kudenko, 2008) proposed a parallel version of the Sarsa algorithm that uses multiple separate actor-learners to accelerate training. Each actorlearner learns separately and periodically sends updates to weights that have changed significantly to the other learners using peer-to-peer communication."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2364
                },
                {
                    "x": 1213,
                    "y": 2364
                },
                {
                    "x": 1213,
                    "y": 2765
                },
                {
                    "x": 223,
                    "y": 2765
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='19' style='font-size:18px'>(Tsitsiklis, 1994) studied convergence properties of Q-<br>learning in the asynchronous optimization setting. These<br>results show that Q-learning is still guaranteed to converge<br>when some of the information is outdated as long as out-<br>dated information is always eventually discarded and sev-<br>eral other technical assumptions are satisfied. Even earlier,<br>(Bertsekas, 1982) studied the related problem of distributed<br>dynamic programming.</p>",
            "id": 19,
            "page": 2,
            "text": "(Tsitsiklis, 1994) studied convergence properties of Qlearning in the asynchronous optimization setting. These results show that Q-learning is still guaranteed to converge when some of the information is outdated as long as outdated information is always eventually discarded and several other technical assumptions are satisfied. Even earlier, (Bertsekas, 1982) studied the related problem of distributed dynamic programming."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2789
                },
                {
                    "x": 1212,
                    "y": 2789
                },
                {
                    "x": 1212,
                    "y": 2991
                },
                {
                    "x": 223,
                    "y": 2991
                }
            ],
            "category": "paragraph",
            "html": "<p id='20' style='font-size:18px'>Another related area of work is in evolutionary meth-<br>ods, which are often straightforward to parallelize by dis-<br>tributing fitness evaluations over multiple machines or<br>threads (Tomassini, 1999). Such parallel evolutionary ap-</p>",
            "id": 20,
            "page": 2,
            "text": "Another related area of work is in evolutionary methods, which are often straightforward to parallelize by distributing fitness evaluations over multiple machines or threads (Tomassini, 1999). Such parallel evolutionary ap-"
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 285
                },
                {
                    "x": 2262,
                    "y": 285
                },
                {
                    "x": 2262,
                    "y": 534
                },
                {
                    "x": 1274,
                    "y": 534
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='21' style='font-size:16px'>proaches have recently been applied to some visual rein-<br>forcement learning tasks. In one example, (Koutnik et al.,<br>2014) evolved convolutional neural network controllers for<br>the TORCS driving simulator by performing fitness evalu-<br>ations on 8 CPU cores in parallel.</p>",
            "id": 21,
            "page": 2,
            "text": "proaches have recently been applied to some visual reinforcement learning tasks. In one example, (Koutnik , 2014) evolved convolutional neural network controllers for the TORCS driving simulator by performing fitness evaluations on 8 CPU cores in parallel."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 584
                },
                {
                    "x": 2130,
                    "y": 584
                },
                {
                    "x": 2130,
                    "y": 641
                },
                {
                    "x": 1272,
                    "y": 641
                }
            ],
            "category": "paragraph",
            "html": "<p id='22' style='font-size:22px'>3. Reinforcement Learning Background</p>",
            "id": 22,
            "page": 2,
            "text": "3. Reinforcement Learning Background"
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 670
                },
                {
                    "x": 2264,
                    "y": 670
                },
                {
                    "x": 2264,
                    "y": 1270
                },
                {
                    "x": 1270,
                    "y": 1270
                }
            ],
            "category": "paragraph",
            "html": "<p id='23' style='font-size:14px'>We consider the standard reinforcement learning setting<br>where an agent interacts with an environment 3 over a<br>number of discrete time steps. At each time step t, the<br>agent receives a state St and selects an action at from some<br>set of possible actions A according to its policy �, where<br>� is a mapping from states St to actions at. In return, the<br>agent receives the next state St+1 and receives a scalar re-<br>ward rt. The process continues until the agent reaches a<br>terminal state after which the process restarts. The return<br>Rt = �k=0 ykrt+k is the total accumulated return from<br>time step t with discount factor 7 E (0,1]. The goal of the<br>agentis to maximize the expected return from each state St.</p>",
            "id": 23,
            "page": 2,
            "text": "We consider the standard reinforcement learning setting where an agent interacts with an environment 3 over a number of discrete time steps. At each time step t, the agent receives a state St and selects an action at from some set of possible actions A according to its policy �, where � is a mapping from states St to actions at. In return, the agent receives the next state St+1 and receives a scalar reward rt. The process continues until the agent reaches a terminal state after which the process restarts. The return Rt = �k=0 ykrt+k is the total accumulated return from time step t with discount factor 7 E (0,1]. The goal of the agentis to maximize the expected return from each state St."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1293
                },
                {
                    "x": 2265,
                    "y": 1293
                },
                {
                    "x": 2265,
                    "y": 1694
                },
                {
                    "x": 1273,
                    "y": 1694
                }
            ],
            "category": "paragraph",
            "html": "<p id='24' style='font-size:16px'>The action value Q�(s, a) = E [Rt|st = s, a] is the ex-<br>pected return for selecting action a in state s and follow-<br>ing policy �. The optimal value function Q*(s, a) =<br>max� Q�(s, a) gives the maximum action value for state<br>s and action a achievable by any policy. Similarly, the<br>value of state s under policy � is defined as V� (s) =<br>E [Rt|st = s] and is simply the expected return for follow-<br>ing policy � from state s.</p>",
            "id": 24,
            "page": 2,
            "text": "The action value Q�(s, a) = E [Rt|st = s, a] is the expected return for selecting action a in state s and following policy �. The optimal value function Q*(s, a) = max� Q�(s, a) gives the maximum action value for state s and action a achievable by any policy. Similarly, the value of state s under policy � is defined as V� (s) = E [Rt|st = s] and is simply the expected return for following policy � from state s."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 1716
                },
                {
                    "x": 2262,
                    "y": 1716
                },
                {
                    "x": 2262,
                    "y": 2266
                },
                {
                    "x": 1271,
                    "y": 2266
                }
            ],
            "category": "paragraph",
            "html": "<p id='25' style='font-size:18px'>In value-based model-free reinforcement learning methods,<br>the action value function is represented using a function ap-<br>proximator, such as a neural network. Let Q(s, a; 0) be an<br>approximate action-value function with parameters 0. The<br>updates to 0 can be derived from a variety of reinforcement<br>learning algorithms. One example of such an algorithm is<br>Q-learning, which aims to directly approximate the optimal<br>action value function: Q*(s, a) 21 Q(s, a; 0). In one-step<br>Q-learning, the parameters 0 of the action value function<br>Q(s, a;0) are learned by iteratively minimizing a sequence<br>of loss functions, where the ith loss function defined as</p>",
            "id": 25,
            "page": 2,
            "text": "In value-based model-free reinforcement learning methods, the action value function is represented using a function approximator, such as a neural network. Let Q(s, a; 0) be an approximate action-value function with parameters 0. The updates to 0 can be derived from a variety of reinforcement learning algorithms. One example of such an algorithm is Q-learning, which aims to directly approximate the optimal action value function: Q*(s, a) 21 Q(s, a; 0). In one-step Q-learning, the parameters 0 of the action value function Q(s, a;0) are learned by iteratively minimizing a sequence of loss functions, where the ith loss function defined as"
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 2420
                },
                {
                    "x": 2037,
                    "y": 2420
                },
                {
                    "x": 2037,
                    "y": 2468
                },
                {
                    "x": 1275,
                    "y": 2468
                }
            ],
            "category": "paragraph",
            "html": "<p id='26' style='font-size:14px'>where s' is the state encountered after state s.</p>",
            "id": 26,
            "page": 2,
            "text": "where s' is the state encountered after state s."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2494
                },
                {
                    "x": 2263,
                    "y": 2494
                },
                {
                    "x": 2263,
                    "y": 2994
                },
                {
                    "x": 1273,
                    "y": 2994
                }
            ],
            "category": "paragraph",
            "html": "<p id='27' style='font-size:16px'>We refer to the above method as one-step Q-learning be-<br>cause it updates the action value Q(s, a) toward the one-<br>step return r + 2 max�' Q(s', a'; 0). One drawback of us-<br>ing one-step methods is that obtaining a reward r only di-<br>rectly affects the value of the state action pair s, a that led<br>to the reward. The values of other state action pairs are<br>affected only indirectly through the updated value Q(s, a).<br>This can make the learning process slow since many up-<br>dates are required the propagate a reward to the relevant<br>preceding states and actions.</p>",
            "id": 27,
            "page": 2,
            "text": "We refer to the above method as one-step Q-learning because it updates the action value Q(s, a) toward the onestep return r + 2 max�' Q(s', a'; 0). One drawback of using one-step methods is that obtaining a reward r only directly affects the value of the state action pair s, a that led to the reward. The values of other state action pairs are affected only indirectly through the updated value Q(s, a). This can make the learning process slow since many updates are required the propagate a reward to the relevant preceding states and actions."
        },
        {
            "bounding_box": [
                {
                    "x": 778,
                    "y": 191
                },
                {
                    "x": 1711,
                    "y": 191
                },
                {
                    "x": 1711,
                    "y": 236
                },
                {
                    "x": 778,
                    "y": 236
                }
            ],
            "category": "header",
            "html": "<header id='28' style='font-size:20px'>Asynchronous Methods for Deep Reinforcement Learning</header>",
            "id": 28,
            "page": 3,
            "text": "Asynchronous Methods for Deep Reinforcement Learning"
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 285
                },
                {
                    "x": 1212,
                    "y": 285
                },
                {
                    "x": 1212,
                    "y": 683
                },
                {
                    "x": 222,
                    "y": 683
                }
            ],
            "category": "paragraph",
            "html": "<p id='29' style='font-size:18px'>One way of propagating rewards faster is by using n-<br>step returns (Watkins, 1989; Peng & Williams, 1996).<br>In n-step Q-learning, Q(s,a) is updated toward the n-<br>step return defined as rt + Yrt+1 + · · · + yn- 1 rt+n-1 +<br>maxa ynQ(st+n, a). This results in a single reward r di-<br>rectly affecting the values of n preceding state action pairs.<br>This makes the process of propagating rewards to relevant<br>state-action pairs potentially much more efficient.</p>",
            "id": 29,
            "page": 3,
            "text": "One way of propagating rewards faster is by using nstep returns (Watkins, 1989; Peng & Williams, 1996). In n-step Q-learning, Q(s,a) is updated toward the nstep return defined as rt + Yrt+1 + · · · + yn- 1 rt+n-1 + maxa ynQ(st+n, a). This results in a single reward r directly affecting the values of n preceding state action pairs. This makes the process of propagating rewards to relevant state-action pairs potentially much more efficient."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 706
                },
                {
                    "x": 1215,
                    "y": 706
                },
                {
                    "x": 1215,
                    "y": 1308
                },
                {
                    "x": 224,
                    "y": 1308
                }
            ],
            "category": "paragraph",
            "html": "<p id='30' style='font-size:20px'>In contrast to value-based methods, policy-based model-<br>free methods directly parameterize the policy �(a|s; 0) and<br>update the parameters 0 by performing, typically approx-<br>imate, gradient ascent on E[Rt]. One example of such<br>a method is the REINFORCE family of algorithms due<br>to Williams (1992). Standard REINFORCE updates the<br>policy parameters 0 in the direction ▽� log �(at|St; 0)Rt,<br>which is an unbiased estimate of V�E[Rt]. Itis possible to<br>reduce the variance of this estimate while keeping it unbi-<br>ased by subtracting a learned function of the state bt(st),<br>known as a baseline (Williams, 1992), from the return. The<br>resulting gradient is ▽� log �(at|St; 0) (Rt - bt(st)).</p>",
            "id": 30,
            "page": 3,
            "text": "In contrast to value-based methods, policy-based modelfree methods directly parameterize the policy �(a|s; 0) and update the parameters 0 by performing, typically approximate, gradient ascent on E[Rt]. One example of such a method is the REINFORCE family of algorithms due to Williams (1992). Standard REINFORCE updates the policy parameters 0 in the direction ▽� log �(at|St; 0)Rt, which is an unbiased estimate of V�E[Rt]. Itis possible to reduce the variance of this estimate while keeping it unbiased by subtracting a learned function of the state bt(st), known as a baseline (Williams, 1992), from the return. The resulting gradient is ▽� log �(at|St; 0) (Rt - bt(st))."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1329
                },
                {
                    "x": 1216,
                    "y": 1329
                },
                {
                    "x": 1216,
                    "y": 1880
                },
                {
                    "x": 224,
                    "y": 1880
                }
            ],
            "category": "paragraph",
            "html": "<p id='31' style='font-size:18px'>A learned estimate of the value function is commonly used<br>as the baseline bt (st) 21 V� (st) leading to a much lower<br>variance estimate of the policy gradient. When an approx-<br>imate value function is used as the baseline, the quantity<br>Rt - bt used to scale the policy gradient can be seen as<br>an estimate of the advantage of action at in state St, or<br>A(at, St) = Q(at, st) - V(st), because Rt is an estimate of<br>Q� (at, st) and bt is an estimate of V� (st). This approach<br>can be viewed as an actor-critic architecture where the pol-<br>icy � is the actor and the baseline bt is the critic (Sutton &<br>Barto, 1998; Degris et al., 2012).</p>",
            "id": 31,
            "page": 3,
            "text": "A learned estimate of the value function is commonly used as the baseline bt (st) 21 V� (st) leading to a much lower variance estimate of the policy gradient. When an approximate value function is used as the baseline, the quantity Rt - bt used to scale the policy gradient can be seen as an estimate of the advantage of action at in state St, or A(at, St) = Q(at, st) - V(st), because Rt is an estimate of Q� (at, st) and bt is an estimate of V� (st). This approach can be viewed as an actor-critic architecture where the policy � is the actor and the baseline bt is the critic (Sutton & Barto, 1998; Degris , 2012)."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1929
                },
                {
                    "x": 935,
                    "y": 1929
                },
                {
                    "x": 935,
                    "y": 1985
                },
                {
                    "x": 223,
                    "y": 1985
                }
            ],
            "category": "paragraph",
            "html": "<p id='32' style='font-size:22px'>4. Asynchronous RL Framework</p>",
            "id": 32,
            "page": 3,
            "text": "4. Asynchronous RL Framework"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2015
                },
                {
                    "x": 1213,
                    "y": 2015
                },
                {
                    "x": 1213,
                    "y": 2515
                },
                {
                    "x": 223,
                    "y": 2515
                }
            ],
            "category": "paragraph",
            "html": "<p id='33' style='font-size:20px'>We now present multi-threaded asynchronous variants of<br>one-step Sarsa, one-step Q-learning, n-step Q-learning, and<br>advantage actor-critic. The aim in designing these methods<br>was to find RL algorithms that can train deep neural net-<br>work policies reliably and without large resource require-<br>ments. While the underlying RL methods are quite dif-<br>ferent, with actor-critic being an on-policy policy search<br>method and Q-learning being an off-policy value-based<br>method, we use two main ideas to make all four algorithms<br>practical given our design goal.</p>",
            "id": 33,
            "page": 3,
            "text": "We now present multi-threaded asynchronous variants of one-step Sarsa, one-step Q-learning, n-step Q-learning, and advantage actor-critic. The aim in designing these methods was to find RL algorithms that can train deep neural network policies reliably and without large resource requirements. While the underlying RL methods are quite different, with actor-critic being an on-policy policy search method and Q-learning being an off-policy value-based method, we use two main ideas to make all four algorithms practical given our design goal."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2539
                },
                {
                    "x": 1212,
                    "y": 2539
                },
                {
                    "x": 1212,
                    "y": 2890
                },
                {
                    "x": 223,
                    "y": 2890
                }
            ],
            "category": "paragraph",
            "html": "<p id='34' style='font-size:18px'>First, we use asynchronous actor-learners, similarly to the<br>Gorila framework (Nair et al., 2015), but instead of using<br>separate machines and a parameter server, we use multi-<br>ple CPU threads on a single machine. Keeping the learn-<br>ers on a single machine removes the communication costs<br>of sending gradients and parameters and enables us to use<br>Hogwild! (Recht et al., 2011) style updates for training.</p>",
            "id": 34,
            "page": 3,
            "text": "First, we use asynchronous actor-learners, similarly to the Gorila framework (Nair , 2015), but instead of using separate machines and a parameter server, we use multiple CPU threads on a single machine. Keeping the learners on a single machine removes the communication costs of sending gradients and parameters and enables us to use Hogwild! (Recht , 2011) style updates for training."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2914
                },
                {
                    "x": 1211,
                    "y": 2914
                },
                {
                    "x": 1211,
                    "y": 2963
                },
                {
                    "x": 224,
                    "y": 2963
                }
            ],
            "category": "paragraph",
            "html": "<p id='35' style='font-size:18px'>Second, we make the observation that multiple actors-</p>",
            "id": 35,
            "page": 3,
            "text": "Second, we make the observation that multiple actors-"
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 285
                },
                {
                    "x": 2260,
                    "y": 285
                },
                {
                    "x": 2260,
                    "y": 377
                },
                {
                    "x": 1276,
                    "y": 377
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='36' style='font-size:18px'>Algorithm 1 Asynchronous one-step Q-learning - pseu-<br>docode for each actor-learner thread.</p>",
            "id": 36,
            "page": 3,
            "text": "Algorithm 1 Asynchronous one-step Q-learning - pseudocode for each actor-learner thread."
        },
        {
            "bounding_box": [
                {
                    "x": 1312,
                    "y": 388
                },
                {
                    "x": 2095,
                    "y": 388
                },
                {
                    "x": 2095,
                    "y": 578
                },
                {
                    "x": 1312,
                    "y": 578
                }
            ],
            "category": "paragraph",
            "html": "<p id='37' style='font-size:14px'>// Assume global shared 0, 0- , and counter T = 0.<br>Initialize thread step counter t ← 0<br>Initialize target network weights 0 ← 0<br>Initialize network gradients d0 ← 0<br>Get initial state s</p>",
            "id": 37,
            "page": 3,
            "text": "// Assume global shared 0, 0- , and counter T = 0. Initialize thread step counter t ← 0 Initialize target network weights 0 ← 0 Initialize network gradients d0 ← 0 Get initial state s"
        },
        {
            "bounding_box": [
                {
                    "x": 1318,
                    "y": 601
                },
                {
                    "x": 1427,
                    "y": 601
                },
                {
                    "x": 1427,
                    "y": 635
                },
                {
                    "x": 1318,
                    "y": 635
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='38' style='font-size:16px'>repeat</p>",
            "id": 38,
            "page": 3,
            "text": "repeat"
        },
        {
            "bounding_box": [
                {
                    "x": 1371,
                    "y": 637
                },
                {
                    "x": 2202,
                    "y": 637
                },
                {
                    "x": 2202,
                    "y": 718
                },
                {
                    "x": 1371,
                    "y": 718
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='39' style='font-size:16px'>Take action a with e-greedy policy based on Q(s, a; 0)<br>Receive new state s' and reward r</p>",
            "id": 39,
            "page": 3,
            "text": "Take action a with e-greedy policy based on Q(s, a; 0) Receive new state s' and reward r"
        },
        {
            "bounding_box": [
                {
                    "x": 1372,
                    "y": 814
                },
                {
                    "x": 2195,
                    "y": 814
                },
                {
                    "x": 2195,
                    "y": 1028
                },
                {
                    "x": 1372,
                    "y": 1028
                }
            ],
            "category": "paragraph",
            "html": "<p id='40' style='font-size:14px'>Accumulate gradients wrt 0: d0 ← d0 + d(y-Q(s,a;0))2<br>au<br>s = s'<br>T ← T + 1 and t ← t + 1<br>ifT mod Itarget == 0 then<br>Update the target network 0- ← 0</p>",
            "id": 40,
            "page": 3,
            "text": "Accumulate gradients wrt 0: d0 ← d0 + d(y-Q(s,a;0))2 au s = s' T ← T + 1 and t ← t + 1 ifT mod Itarget == 0 then Update the target network 0- ← 0"
        },
        {
            "bounding_box": [
                {
                    "x": 1373,
                    "y": 1036
                },
                {
                    "x": 1473,
                    "y": 1036
                },
                {
                    "x": 1473,
                    "y": 1069
                },
                {
                    "x": 1373,
                    "y": 1069
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='41' style='font-size:16px'>end if</p>",
            "id": 41,
            "page": 3,
            "text": "end if"
        },
        {
            "bounding_box": [
                {
                    "x": 1379,
                    "y": 1063
                },
                {
                    "x": 2164,
                    "y": 1063
                },
                {
                    "x": 2164,
                    "y": 1200
                },
                {
                    "x": 1379,
                    "y": 1200
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='42' style='font-size:16px'>ift mod IAsyncUpdate == 0 or s is terminal then<br>Perform asynchronous update of 0 using d0.<br>Clear gradients d0 ← 0.</p>",
            "id": 42,
            "page": 3,
            "text": "ift mod IAsyncUpdate == 0 or s is terminal then Perform asynchronous update of 0 using d0. Clear gradients d0 ← 0."
        },
        {
            "bounding_box": [
                {
                    "x": 1372,
                    "y": 1200
                },
                {
                    "x": 1473,
                    "y": 1200
                },
                {
                    "x": 1473,
                    "y": 1236
                },
                {
                    "x": 1372,
                    "y": 1236
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='43' style='font-size:16px'>end if</p>",
            "id": 43,
            "page": 3,
            "text": "end if"
        },
        {
            "bounding_box": [
                {
                    "x": 1316,
                    "y": 1239
                },
                {
                    "x": 1571,
                    "y": 1239
                },
                {
                    "x": 1571,
                    "y": 1281
                },
                {
                    "x": 1316,
                    "y": 1281
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='44' style='font-size:16px'>until T > Tmax</p>",
            "id": 44,
            "page": 3,
            "text": "until T > Tmax"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1396
                },
                {
                    "x": 2263,
                    "y": 1396
                },
                {
                    "x": 2263,
                    "y": 1997
                },
                {
                    "x": 1272,
                    "y": 1997
                }
            ],
            "category": "paragraph",
            "html": "<p id='45' style='font-size:20px'>learners running in parallel are likely to be exploring dif-<br>ferent parts of the environment. Moreover, one can explic-<br>itly use different exploration policies in each actor-learner<br>to maximize this diversity. By running different explo-<br>ration policies in different threads, the overall changes be-<br>ing made to the parameters by multiple actor-learners ap-<br>plying online updates in parallel are likely to be less corre-<br>lated in time than a single agent applying online updates.<br>Hence, we do not use a replay memory and rely on parallel<br>actors employing different exploration policies to perform<br>the stabilizing role undertaken by experience replay in the<br>DQN training algorithm.</p>",
            "id": 45,
            "page": 3,
            "text": "learners running in parallel are likely to be exploring different parts of the environment. Moreover, one can explicitly use different exploration policies in each actor-learner to maximize this diversity. By running different exploration policies in different threads, the overall changes being made to the parameters by multiple actor-learners applying online updates in parallel are likely to be less correlated in time than a single agent applying online updates. Hence, we do not use a replay memory and rely on parallel actors employing different exploration policies to perform the stabilizing role undertaken by experience replay in the DQN training algorithm."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2019
                },
                {
                    "x": 2263,
                    "y": 2019
                },
                {
                    "x": 2263,
                    "y": 2517
                },
                {
                    "x": 1273,
                    "y": 2517
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='46' style='font-size:20px'>In addition to stabilizing learning, using multiple parallel<br>actor-learners has multiple practical benefits. First, we ob-<br>tain a reduction in training time that is roughly linear in<br>the number of parallel actor-learners. Second, since we no<br>longer rely on experience replay for stabilizing learning we<br>are able to use on-policy reinforcement learning methods<br>such as Sarsa and actor-critic to train neural networks in a<br>stable way. We now describe our variants of one-step Q-<br>learning, one-step Sarsa, n-step Q-learning and advantage<br>actor-critic.</p>",
            "id": 46,
            "page": 3,
            "text": "In addition to stabilizing learning, using multiple parallel actor-learners has multiple practical benefits. First, we obtain a reduction in training time that is roughly linear in the number of parallel actor-learners. Second, since we no longer rely on experience replay for stabilizing learning we are able to use on-policy reinforcement learning methods such as Sarsa and actor-critic to train neural networks in a stable way. We now describe our variants of one-step Qlearning, one-step Sarsa, n-step Q-learning and advantage actor-critic."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2544
                },
                {
                    "x": 2263,
                    "y": 2544
                },
                {
                    "x": 2263,
                    "y": 2994
                },
                {
                    "x": 1273,
                    "y": 2994
                }
            ],
            "category": "paragraph",
            "html": "<p id='47' style='font-size:20px'>Asynchronous one-step Q-learning: Pseudocode for our<br>variant of Q-learning, which we call Asynchronous one-<br>step Q-learning, is shown in Algorithm 1. Each thread in-<br>teracts with its own copy of the environment and at each<br>step computes a gradient of the Q-learning loss. We use<br>a shared and slowly changing target network in comput-<br>ing the Q-learning loss, as was proposed in the DQN train-<br>ing method. We also accumulate gradients over multiple<br>timesteps before they are applied, which is similar to us-</p>",
            "id": 47,
            "page": 3,
            "text": "Asynchronous one-step Q-learning: Pseudocode for our variant of Q-learning, which we call Asynchronous onestep Q-learning, is shown in Algorithm 1. Each thread interacts with its own copy of the environment and at each step computes a gradient of the Q-learning loss. We use a shared and slowly changing target network in computing the Q-learning loss, as was proposed in the DQN training method. We also accumulate gradients over multiple timesteps before they are applied, which is similar to us-"
        },
        {
            "bounding_box": [
                {
                    "x": 777,
                    "y": 191
                },
                {
                    "x": 1711,
                    "y": 191
                },
                {
                    "x": 1711,
                    "y": 236
                },
                {
                    "x": 777,
                    "y": 236
                }
            ],
            "category": "header",
            "html": "<header id='48' style='font-size:18px'>Asynchronous Methods for Deep Reinforcement Learning</header>",
            "id": 48,
            "page": 4,
            "text": "Asynchronous Methods for Deep Reinforcement Learning"
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 285
                },
                {
                    "x": 1212,
                    "y": 285
                },
                {
                    "x": 1212,
                    "y": 484
                },
                {
                    "x": 222,
                    "y": 484
                }
            ],
            "category": "paragraph",
            "html": "<p id='49' style='font-size:20px'>ing minibatches. This reduces the chances of multiple ac-<br>tor learners overwriting each other's updates. Accumulat-<br>ing updates over several steps also provides some ability to<br>trade off computational efficiency for data efficiency.</p>",
            "id": 49,
            "page": 4,
            "text": "ing minibatches. This reduces the chances of multiple actor learners overwriting each other's updates. Accumulating updates over several steps also provides some ability to trade off computational efficiency for data efficiency."
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 507
                },
                {
                    "x": 1212,
                    "y": 507
                },
                {
                    "x": 1212,
                    "y": 857
                },
                {
                    "x": 222,
                    "y": 857
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='50' style='font-size:20px'>Finally, we found that giving each thread a different explo-<br>ration policy helps improve robustness. Adding diversity<br>to exploration in this manner also generally improves per-<br>formance through better exploration. While there are many<br>possible ways of making the exploration policies differ we<br>experiment with using e-greedy exploration with E periodi-<br>cally sampled from some distribution by each thread.</p>",
            "id": 50,
            "page": 4,
            "text": "Finally, we found that giving each thread a different exploration policy helps improve robustness. Adding diversity to exploration in this manner also generally improves performance through better exploration. While there are many possible ways of making the exploration policies differ we experiment with using e-greedy exploration with E periodically sampled from some distribution by each thread."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 881
                },
                {
                    "x": 1213,
                    "y": 881
                },
                {
                    "x": 1213,
                    "y": 1282
                },
                {
                    "x": 223,
                    "y": 1282
                }
            ],
            "category": "paragraph",
            "html": "<p id='51' style='font-size:18px'>Asynchronous one-step Sarsa: The asynchronous one-<br>step Sarsa algorithm is the same as asynchronous one-step<br>Q-learning as given in Algorithm 1 except that it uses a dif-<br>ferent target value for Q(s, a). The target value used by<br>one-step Sarsa is r + YQ(s' , a'; 0-) where a' is the action<br>taken in state s' (Rummery & Niranjan, 1994; Sutton &<br>Barto, 1998). We again use a target network and updates<br>accumulated over multiple timesteps to stabilize learning.</p>",
            "id": 51,
            "page": 4,
            "text": "Asynchronous one-step Sarsa: The asynchronous onestep Sarsa algorithm is the same as asynchronous one-step Q-learning as given in Algorithm 1 except that it uses a different target value for Q(s, a). The target value used by one-step Sarsa is r + YQ(s' , a'; 0-) where a' is the action taken in state s' (Rummery & Niranjan, 1994; Sutton & Barto, 1998). We again use a target network and updates accumulated over multiple timesteps to stabilize learning."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1303
                },
                {
                    "x": 1215,
                    "y": 1303
                },
                {
                    "x": 1215,
                    "y": 2304
                },
                {
                    "x": 223,
                    "y": 2304
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='52' style='font-size:18px'>Asynchronous n-step Q-learning: Pseudocode for our<br>variant of multi-step Q-learning is shown in Supplementary<br>Algorithm S2. The algorithm is somewhat unusual because<br>it operates in the forward view by explicitly computing n-<br>step returns, as opposed to the more common backward<br>view used by techniques like eligibility traces (Sutton &<br>Barto, 1998). We found that using the forward view is eas-<br>ier when training neural networks with momentum-based<br>methods and backpropagation through time. In order to<br>compute a single update, the algorithm first selects actions<br>using its exploration policy for up to tmax steps or until a<br>terminal state is reached. This process results in the agent<br>receiving up to tmax rewards from the environment since<br>its last update. The algorithm then computes gradients for<br>n-step Q-learning updates for each of the state-action pairs<br>encountered since the last update. Each n-step update uses<br>the longest possible n-step return resulting in a one-step<br>update for the last state, a two-step update for the second<br>last state, and so on for a total of up to tmax updates. The<br>accumulated updates are applied in a single gradient step.</p>",
            "id": 52,
            "page": 4,
            "text": "Asynchronous n-step Q-learning: Pseudocode for our variant of multi-step Q-learning is shown in Supplementary Algorithm S2. The algorithm is somewhat unusual because it operates in the forward view by explicitly computing nstep returns, as opposed to the more common backward view used by techniques like eligibility traces (Sutton & Barto, 1998). We found that using the forward view is easier when training neural networks with momentum-based methods and backpropagation through time. In order to compute a single update, the algorithm first selects actions using its exploration policy for up to tmax steps or until a terminal state is reached. This process results in the agent receiving up to tmax rewards from the environment since its last update. The algorithm then computes gradients for n-step Q-learning updates for each of the state-action pairs encountered since the last update. Each n-step update uses the longest possible n-step return resulting in a one-step update for the last state, a two-step update for the second last state, and so on for a total of up to tmax updates. The accumulated updates are applied in a single gradient step."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2324
                },
                {
                    "x": 1215,
                    "y": 2324
                },
                {
                    "x": 1215,
                    "y": 2982
                },
                {
                    "x": 223,
                    "y": 2982
                }
            ],
            "category": "paragraph",
            "html": "<p id='53' style='font-size:18px'>Asynchronous advantage actor-critic: The algorithm,<br>which we call asynchronous advantage actor-critic (A3C),<br>maintains a policy �(at|St; 0) and an estimate of the value<br>function V (St; 0v). Like our variant of n-step Q-learning,<br>our variant of actor-critic also operates in the forward view<br>and uses the same mix of n-step returns to update both the<br>policy and the value-function. The policy and the value<br>function are updated after every tmax actions or when a<br>terminal state is reached. The update performed by the al-<br>gorithm can be seen as ▽0' log �(at |st; 0')A(st, at; 0, 0v)<br>where A(st, at; 0, 0v) is an estimate of the advantage func-<br>tion given by �k=0 yirt+i + ykV (St+k; 0v) - V (st; 0v),<br>where k can vary from state to state and is upper-bounded</p>",
            "id": 53,
            "page": 4,
            "text": "Asynchronous advantage actor-critic: The algorithm, which we call asynchronous advantage actor-critic (A3C), maintains a policy �(at|St; 0) and an estimate of the value function V (St; 0v). Like our variant of n-step Q-learning, our variant of actor-critic also operates in the forward view and uses the same mix of n-step returns to update both the policy and the value-function. The policy and the value function are updated after every tmax actions or when a terminal state is reached. The update performed by the algorithm can be seen as ▽0' log �(at |st; 0')A(st, at; 0, 0v) where A(st, at; 0, 0v) is an estimate of the advantage function given by �k=0 yirt+i + ykV (St+k; 0v) - V (st; 0v), where k can vary from state to state and is upper-bounded"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 285
                },
                {
                    "x": 2264,
                    "y": 285
                },
                {
                    "x": 2264,
                    "y": 383
                },
                {
                    "x": 1272,
                    "y": 383
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='54' style='font-size:20px'>by tmax. The pseudocode for the algorithm is presented in<br>Supplementary Algorithm S3.</p>",
            "id": 54,
            "page": 4,
            "text": "by tmax. The pseudocode for the algorithm is presented in Supplementary Algorithm S3."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 410
                },
                {
                    "x": 2263,
                    "y": 410
                },
                {
                    "x": 2263,
                    "y": 858
                },
                {
                    "x": 1272,
                    "y": 858
                }
            ],
            "category": "paragraph",
            "html": "<p id='55' style='font-size:18px'>As with the value-based methods we rely on parallel actor-<br>learners and accumulated updates for improving training<br>stability. Note that while the parameters 0 of the policy<br>and 0v of the value function are shown as being separate<br>for generality, we always share some of the parameters in<br>practice. We typically use a convolutional neural network<br>that has one softmax output for the policy �(at |st; 0) and<br>one linear output for the value function V(st; 0v), with all<br>non-output layers shared.</p>",
            "id": 55,
            "page": 4,
            "text": "As with the value-based methods we rely on parallel actorlearners and accumulated updates for improving training stability. Note that while the parameters 0 of the policy and 0v of the value function are shown as being separate for generality, we always share some of the parameters in practice. We typically use a convolutional neural network that has one softmax output for the policy �(at |st; 0) and one linear output for the value function V(st; 0v), with all non-output layers shared."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 880
                },
                {
                    "x": 2264,
                    "y": 880
                },
                {
                    "x": 2264,
                    "y": 1480
                },
                {
                    "x": 1270,
                    "y": 1480
                }
            ],
            "category": "paragraph",
            "html": "<p id='56' style='font-size:20px'>We also found that adding the entropy of the policy � to the<br>objective function improved exploration by discouraging<br>premature convergence to suboptimal deterministic poli-<br>cies. This technique was originally proposed by (Williams<br>& Peng, 1991), who found that it was particularly help-<br>ful on tasks requiring hierarchical behavior. The gradi-<br>ent of the full objective function including the entropy<br>regularization term with respect to the policy parame-<br>ters takes the form ▽ 0' log �(at|St; 0')(Rt - V (st; 0v)) +<br>B▽ 01 H(�(St; 0')), where H is the entropy. The hyperpa-<br>rameter B controls the strength of the entropy regulariza-<br>tion term.</p>",
            "id": 56,
            "page": 4,
            "text": "We also found that adding the entropy of the policy � to the objective function improved exploration by discouraging premature convergence to suboptimal deterministic policies. This technique was originally proposed by (Williams & Peng, 1991), who found that it was particularly helpful on tasks requiring hierarchical behavior. The gradient of the full objective function including the entropy regularization term with respect to the policy parameters takes the form ▽ 0' log �(at|St; 0')(Rt - V (st; 0v)) + B▽ 01 H(�(St; 0')), where H is the entropy. The hyperparameter B controls the strength of the entropy regularization term."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1502
                },
                {
                    "x": 2264,
                    "y": 1502
                },
                {
                    "x": 2264,
                    "y": 1805
                },
                {
                    "x": 1273,
                    "y": 1805
                }
            ],
            "category": "paragraph",
            "html": "<p id='57' style='font-size:20px'>Optimization: We investigated three different optimiza-<br>tion algorithms in our asynchronous framework - SGD<br>with momentum, RMSProp (Tieleman & Hinton, 2012)<br>without shared statistics, and RMSProp with shared statis-<br>tics. We used the standard non-centered RMSProp update<br>given by</p>",
            "id": 57,
            "page": 4,
            "text": "Optimization: We investigated three different optimization algorithms in our asynchronous framework - SGD with momentum, RMSProp (Tieleman & Hinton, 2012) without shared statistics, and RMSProp with shared statistics. We used the standard non-centered RMSProp update given by"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1958
                },
                {
                    "x": 2263,
                    "y": 1958
                },
                {
                    "x": 2263,
                    "y": 2258
                },
                {
                    "x": 1272,
                    "y": 2258
                }
            ],
            "category": "paragraph",
            "html": "<p id='58' style='font-size:16px'>where all operations are performed elementwise. A com-<br>parison on a subset of Atari 2600 games showed that a vari-<br>ant of RMSProp where statistics g are shared across threads<br>is considerably more robust than the other two methods.<br>Full details of the methods and comparisons are included<br>in Supplementary Section 7.</p>",
            "id": 58,
            "page": 4,
            "text": "where all operations are performed elementwise. A comparison on a subset of Atari 2600 games showed that a variant of RMSProp where statistics g are shared across threads is considerably more robust than the other two methods. Full details of the methods and comparisons are included in Supplementary Section 7."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 2308
                },
                {
                    "x": 1606,
                    "y": 2308
                },
                {
                    "x": 1606,
                    "y": 2363
                },
                {
                    "x": 1274,
                    "y": 2363
                }
            ],
            "category": "paragraph",
            "html": "<p id='59' style='font-size:22px'>5. Experiments</p>",
            "id": 59,
            "page": 4,
            "text": "5. Experiments"
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 2393
                },
                {
                    "x": 2264,
                    "y": 2393
                },
                {
                    "x": 2264,
                    "y": 2995
                },
                {
                    "x": 1270,
                    "y": 2995
                }
            ],
            "category": "paragraph",
            "html": "<p id='60' style='font-size:18px'>We use four different platforms for assessing the properties<br>of the proposed framework. We perform most of our exper-<br>iments using the Arcade Learning Environment (Bellemare<br>et al., 2012), which provides a simulator for Atari 2600<br>games. This is one of the most commonly used benchmark<br>environments for RL algorithms. We use the Atari domain<br>to compare against state of the art results (Van Hasselt et al.,<br>2015; Wang et al., 2015; Schaul et al., 2015; Nair et al.,<br>2015; Mnih et al., 2015), as well as to carry out a detailed<br>stability and scalability analysis of the proposed methods.<br>We performed further comparisons using the TORCS 3D<br>car racing simulator (Wymann et al., 2013). We also use</p>",
            "id": 60,
            "page": 4,
            "text": "We use four different platforms for assessing the properties of the proposed framework. We perform most of our experiments using the Arcade Learning Environment (Bellemare , 2012), which provides a simulator for Atari 2600 games. This is one of the most commonly used benchmark environments for RL algorithms. We use the Atari domain to compare against state of the art results (Van Hasselt , 2015; Wang , 2015; Schaul , 2015; Nair , 2015; Mnih , 2015), as well as to carry out a detailed stability and scalability analysis of the proposed methods. We performed further comparisons using the TORCS 3D car racing simulator (Wymann , 2013). We also use"
        },
        {
            "bounding_box": [
                {
                    "x": 777,
                    "y": 191
                },
                {
                    "x": 1711,
                    "y": 191
                },
                {
                    "x": 1711,
                    "y": 236
                },
                {
                    "x": 777,
                    "y": 236
                }
            ],
            "category": "header",
            "html": "<header id='61' style='font-size:20px'>Asynchronous Methods for Deep Reinforcement Learning</header>",
            "id": 61,
            "page": 5,
            "text": "Asynchronous Methods for Deep Reinforcement Learning"
        },
        {
            "bounding_box": [
                {
                    "x": 233,
                    "y": 272
                },
                {
                    "x": 2251,
                    "y": 272
                },
                {
                    "x": 2251,
                    "y": 735
                },
                {
                    "x": 233,
                    "y": 735
                }
            ],
            "category": "figure",
            "html": "<figure><img id='62' style='font-size:14px' alt=\"Beamrider Breakout Pong Q*bert Space Invaders\n16000 600 30 12000 1600\nDQN DQN DQN DQN\n14000 1-step Q 1-step Q 1-step Q 1400 1-step Q\n500 20 10000\n1-step SARSA 1-step SARSA 1-step SARSA 1-step SARSA\n12000 1200\nn-step Q n-step Q n-step Q n-step Q\n400 10 8000\n10000 A3C A3C A3C A3C\n1000\nScore\nScore\nScore 8000 Score 300 0 6000 Score 800\n6000 DQN 600\n200 -10 4000\n1-step Q\n4000 400\n1-step SARSA\n100 -20 2000\n2000 n-step Q 200\nA3C\n0 0 -30 0 0\n0 2 4 6 8 10 12 14 0 2 4 6 8 10 12 14 0 2 4 6 8 10 12 14 0 2 4 6 8 10 12 14 0 2 4 6 8 10 12 14\nTraining time (hours) Training time (hours) Training time (hours) Training time (hours) Training time (hours)\" data-coord=\"top-left:(233,272); bottom-right:(2251,735)\" /></figure>",
            "id": 62,
            "page": 5,
            "text": "Beamrider Breakout Pong Q*bert Space Invaders 16000 600 30 12000 1600 DQN DQN DQN DQN 14000 1-step Q 1-step Q 1-step Q 1400 1-step Q 500 20 10000 1-step SARSA 1-step SARSA 1-step SARSA 1-step SARSA 12000 1200 n-step Q n-step Q n-step Q n-step Q 400 10 8000 10000 A3C A3C A3C A3C 1000 Score Score Score 8000 Score 300 0 6000 Score 800 6000 DQN 600 200 -10 4000 1-step Q 4000 400 1-step SARSA 100 -20 2000 2000 n-step Q 200 A3C 0 0 -30 0 0 0 2 4 6 8 10 12 14 0 2 4 6 8 10 12 14 0 2 4 6 8 10 12 14 0 2 4 6 8 10 12 14 0 2 4 6 8 10 12 14 Training time (hours) Training time (hours) Training time (hours) Training time (hours) Training time (hours)"
        },
        {
            "bounding_box": [
                {
                    "x": 221,
                    "y": 783
                },
                {
                    "x": 2266,
                    "y": 783
                },
                {
                    "x": 2266,
                    "y": 978
                },
                {
                    "x": 221,
                    "y": 978
                }
            ],
            "category": "caption",
            "html": "<caption id='63' style='font-size:16px'>Figure 1. Learning speed comparison for DQN and the new asynchronous algorithms on five Atari 2600 games. DQN was trained on<br>a single Nvidia K40 GPU while the asynchronous methods were trained using 16 CPU cores. The plots are averaged over 5 runs. In<br>the case of DQN the runs were for different seeds with fixed hyperparameters. For asynchronous methods we average over the best 5<br>models from 50 experiments with learning rates sampled from LogU nif orm (10-4 10-2) and all other hyperparameters fixed.</caption>",
            "id": 63,
            "page": 5,
            "text": "Figure 1. Learning speed comparison for DQN and the new asynchronous algorithms on five Atari 2600 games. DQN was trained on a single Nvidia K40 GPU while the asynchronous methods were trained using 16 CPU cores. The plots are averaged over 5 runs. In the case of DQN the runs were for different seeds with fixed hyperparameters. For asynchronous methods we average over the best 5 models from 50 experiments with learning rates sampled from LogU nif orm (10-4 10-2) and all other hyperparameters fixed."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1024
                },
                {
                    "x": 1213,
                    "y": 1024
                },
                {
                    "x": 1213,
                    "y": 1423
                },
                {
                    "x": 223,
                    "y": 1423
                }
            ],
            "category": "paragraph",
            "html": "<p id='64' style='font-size:18px'>two additional domains to evaluate only the A3C algorithm<br>- Mujoco and Labyrinth. MuJoCo (Todorov, 2015) is a<br>physics simulator for evaluating agents on continuous mo-<br>tor control tasks with contact dynamics. Labyrinth is a new<br>3D environment where the agent must learn to find rewards<br>in randomly generated mazes from a visual input. The pre-<br>cise details of our experimental setup can be found in Sup-<br>plementary Section 8.</p>",
            "id": 64,
            "page": 5,
            "text": "two additional domains to evaluate only the A3C algorithm - Mujoco and Labyrinth. MuJoCo (Todorov, 2015) is a physics simulator for evaluating agents on continuous motor control tasks with contact dynamics. Labyrinth is a new 3D environment where the agent must learn to find rewards in randomly generated mazes from a visual input. The precise details of our experimental setup can be found in Supplementary Section 8."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1477
                },
                {
                    "x": 629,
                    "y": 1477
                },
                {
                    "x": 629,
                    "y": 1526
                },
                {
                    "x": 223,
                    "y": 1526
                }
            ],
            "category": "paragraph",
            "html": "<p id='65' style='font-size:20px'>5.1. Atari 2600 Games</p>",
            "id": 65,
            "page": 5,
            "text": "5.1. Atari 2600 Games"
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 1552
                },
                {
                    "x": 1214,
                    "y": 1552
                },
                {
                    "x": 1214,
                    "y": 2256
                },
                {
                    "x": 225,
                    "y": 2256
                }
            ],
            "category": "paragraph",
            "html": "<p id='66' style='font-size:20px'>We first present results on a subset of Atari 2600 games to<br>demonstrate the training speed of the new methods. Fig-<br>ure 1 compares the learning speed of the DQN algorithm<br>trained on an Nvidia K40 GPU with the asynchronous<br>methods trained using 16 CPU cores on five Atari 2600<br>games. The results show that all four asynchronous meth-<br>ods we presented can successfully train neural network<br>controllers on the Atari domain. The asynchronous meth-<br>ods tend to learn faster than DQN, with significantly faster<br>learning on some games, while training on only 16 CPU<br>cores. Additionally, the results suggest that n-step methods<br>learn faster than one-step methods on some games. Over-<br>all, the policy-based advantage actor-critic method signifi-<br>cantly outperforms all three value-based methods.</p>",
            "id": 66,
            "page": 5,
            "text": "We first present results on a subset of Atari 2600 games to demonstrate the training speed of the new methods. Figure 1 compares the learning speed of the DQN algorithm trained on an Nvidia K40 GPU with the asynchronous methods trained using 16 CPU cores on five Atari 2600 games. The results show that all four asynchronous methods we presented can successfully train neural network controllers on the Atari domain. The asynchronous methods tend to learn faster than DQN, with significantly faster learning on some games, while training on only 16 CPU cores. Additionally, the results suggest that n-step methods learn faster than one-step methods on some games. Overall, the policy-based advantage actor-critic method significantly outperforms all three value-based methods."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2276
                },
                {
                    "x": 1215,
                    "y": 2276
                },
                {
                    "x": 1215,
                    "y": 2979
                },
                {
                    "x": 224,
                    "y": 2979
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='67' style='font-size:20px'>We then evaluated asynchronous advantage actor-critic on<br>57 Atari games. In order to compare with the state of the<br>art in Atari game playing, we largely followed the train-<br>ing and evaluation protocol of (Van Hasselt et al., 2015).<br>Specifically, we tuned hyperparameters (learning rate and<br>amount of gradient norm clipping) using a search on six<br>Atari games (Beamrider, Breakout, Pong, Q*bert, Seaquest<br>and Space Invaders) and then fixed all hyperparameters for<br>all 57 games. We trained both a feedforward agent with the<br>same architecture as (Mnih et al., 2015; Nair et al., 2015;<br>Van Hasselt et al., 2015) as well as a recurrent agent with an<br>additional 256 LSTM cells after the final hidden layer. We<br>additionally used the final network weights for evaluation<br>to make the results more comparable to the original results</p>",
            "id": 67,
            "page": 5,
            "text": "We then evaluated asynchronous advantage actor-critic on 57 Atari games. In order to compare with the state of the art in Atari game playing, we largely followed the training and evaluation protocol of (Van Hasselt , 2015). Specifically, we tuned hyperparameters (learning rate and amount of gradient norm clipping) using a search on six Atari games (Beamrider, Breakout, Pong, Q*bert, Seaquest and Space Invaders) and then fixed all hyperparameters for all 57 games. We trained both a feedforward agent with the same architecture as (Mnih , 2015; Nair , 2015; Van Hasselt , 2015) as well as a recurrent agent with an additional 256 LSTM cells after the final hidden layer. We additionally used the final network weights for evaluation to make the results more comparable to the original results"
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 1016
                },
                {
                    "x": 2286,
                    "y": 1016
                },
                {
                    "x": 2286,
                    "y": 1417
                },
                {
                    "x": 1269,
                    "y": 1417
                }
            ],
            "category": "table",
            "html": "<br><table id='68' style='font-size:16px'><tr><td>Method</td><td>Training Time</td><td>Mean</td><td>Median</td></tr><tr><td>DQN</td><td>8 days on GPU</td><td>121.9%</td><td>47.5%</td></tr><tr><td>Gorila</td><td>4 days, 100 machines</td><td>215.2%</td><td>71.3%</td></tr><tr><td>D-DQN</td><td>8 days on GPU</td><td>332.9%</td><td>110.9%</td></tr><tr><td>Dueling D-DQN</td><td>8 days on GPU</td><td>343.8%</td><td>117.1%</td></tr><tr><td>Prioritized DQN</td><td>8 days on GPU</td><td>463.6%</td><td>127.6%</td></tr><tr><td>A3C, FF</td><td>1 day on CPU</td><td>344.1%</td><td>68.2%</td></tr><tr><td>A3C, FF</td><td>4 days on CPU</td><td>496.8%</td><td>116.6%</td></tr><tr><td>A3C, LSTM</td><td>4 days on CPU</td><td>623.0%</td><td>112.6%</td></tr></table>",
            "id": 68,
            "page": 5,
            "text": "Method Training Time Mean Median  DQN 8 days on GPU 121.9% 47.5%  Gorila 4 days, 100 machines 215.2% 71.3%  D-DQN 8 days on GPU 332.9% 110.9%  Dueling D-DQN 8 days on GPU 343.8% 117.1%  Prioritized DQN 8 days on GPU 463.6% 127.6%  A3C, FF 1 day on CPU 344.1% 68.2%  A3C, FF 4 days on CPU 496.8% 116.6%  A3C, LSTM 4 days on CPU 623.0%"
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 1449
                },
                {
                    "x": 2262,
                    "y": 1449
                },
                {
                    "x": 2262,
                    "y": 1586
                },
                {
                    "x": 1275,
                    "y": 1586
                }
            ],
            "category": "caption",
            "html": "<caption id='69' style='font-size:14px'>Table 1. Mean and median human-normalized scores on 57 Atari<br>games using the human starts evaluation metric. Supplementary<br>Table SS3 shows the raw scores for all games.</caption>",
            "id": 69,
            "page": 5,
            "text": "Table 1. Mean and median human-normalized scores on 57 Atari games using the human starts evaluation metric. Supplementary Table SS3 shows the raw scores for all games."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 1662
                },
                {
                    "x": 2265,
                    "y": 1662
                },
                {
                    "x": 2265,
                    "y": 2564
                },
                {
                    "x": 1271,
                    "y": 2564
                }
            ],
            "category": "paragraph",
            "html": "<p id='70' style='font-size:20px'>from (Bellemare et al., 2012). We trained our agents for<br>four days using 16 CPU cores, while the other agents were<br>trained for 8 to 10 days on Nvidia K40 GPUs. Table 1<br>shows the average and median human-normalized scores<br>obtained by our agents trained by asynchronous advantage<br>actor-critic (A3C) as well as the current state-of-the art.<br>Supplementary Table S3 shows the scores on all games.<br>A3C significantly improves on state-of-the-art the average<br>score over 57 games in half the training time of the other<br>methods while using only 16 CPU cores and no GPU. Fur-<br>thermore, after just one day of training, A3C matches the<br>average human normalized score of Dueling Double DQN<br>and almost reaches the median human normalized score of<br>Gorila. We note that many of the improvements that are<br>presented in Double DQN (Van Hasselt et al., 2015) and<br>Dueling Double DQN (Wang et al., 2015) can be incorpo-<br>rated to 1-step Q and n-step Q methods presented in this<br>work with similar potential improvements.</p>",
            "id": 70,
            "page": 5,
            "text": "from (Bellemare , 2012). We trained our agents for four days using 16 CPU cores, while the other agents were trained for 8 to 10 days on Nvidia K40 GPUs. Table 1 shows the average and median human-normalized scores obtained by our agents trained by asynchronous advantage actor-critic (A3C) as well as the current state-of-the art. Supplementary Table S3 shows the scores on all games. A3C significantly improves on state-of-the-art the average score over 57 games in half the training time of the other methods while using only 16 CPU cores and no GPU. Furthermore, after just one day of training, A3C matches the average human normalized score of Dueling Double DQN and almost reaches the median human normalized score of Gorila. We note that many of the improvements that are presented in Double DQN (Van Hasselt , 2015) and Dueling Double DQN (Wang , 2015) can be incorporated to 1-step Q and n-step Q methods presented in this work with similar potential improvements."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 2615
                },
                {
                    "x": 1904,
                    "y": 2615
                },
                {
                    "x": 1904,
                    "y": 2665
                },
                {
                    "x": 1275,
                    "y": 2665
                }
            ],
            "category": "paragraph",
            "html": "<p id='71' style='font-size:22px'>5.2. TORCS Car Racing Simulator</p>",
            "id": 71,
            "page": 5,
            "text": "5.2. TORCS Car Racing Simulator"
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2692
                },
                {
                    "x": 2264,
                    "y": 2692
                },
                {
                    "x": 2264,
                    "y": 2994
                },
                {
                    "x": 1273,
                    "y": 2994
                }
            ],
            "category": "paragraph",
            "html": "<p id='72' style='font-size:20px'>We also compared the four asynchronous methods on<br>the TORCS 3D car racing game (Wymann et al., 2013).<br>TORCS not only has more realistic graphics than Atari<br>2600 games, but also requires the agent to learn the dy-<br>namics of the car it is controlling. At each step, an agent<br>received only a visual input in the form of an RGB image</p>",
            "id": 72,
            "page": 5,
            "text": "We also compared the four asynchronous methods on the TORCS 3D car racing game (Wymann , 2013). TORCS not only has more realistic graphics than Atari 2600 games, but also requires the agent to learn the dynamics of the car it is controlling. At each step, an agent received only a visual input in the form of an RGB image"
        },
        {
            "bounding_box": [
                {
                    "x": 778,
                    "y": 191
                },
                {
                    "x": 1711,
                    "y": 191
                },
                {
                    "x": 1711,
                    "y": 236
                },
                {
                    "x": 778,
                    "y": 236
                }
            ],
            "category": "header",
            "html": "<header id='73' style='font-size:18px'>Asynchronous Methods for Deep Reinforcement Learning</header>",
            "id": 73,
            "page": 6,
            "text": "Asynchronous Methods for Deep Reinforcement Learning"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 283
                },
                {
                    "x": 1216,
                    "y": 283
                },
                {
                    "x": 1216,
                    "y": 985
                },
                {
                    "x": 223,
                    "y": 985
                }
            ],
            "category": "paragraph",
            "html": "<p id='74' style='font-size:16px'>of the current frame as well as a reward proportional to the<br>agent's velocity along the center of the track at the agent's<br>current position. We used the same neural network archi-<br>tecture as the one used in the Atari experiments specified in<br>Supplementary Section 8. We performed experiments us-<br>ing four different settings - the agent controlling a slow car<br>with and without opponent bots, and the agent controlling a<br>fast car with and without opponent bots. Full results can be<br>found in Supplementary Figure S6. A3C was the best per-<br>forming agent, reaching between roughly 75% and 90% of<br>the score obtained by a human tester on all four game con-<br>figurations in about 12 hours of training. A video showing<br>the learned driving behavior of the A3C agent can be found<br>at https : / / youtu · be / 0xo1Ldx3L5Q.</p>",
            "id": 74,
            "page": 6,
            "text": "of the current frame as well as a reward proportional to the agent's velocity along the center of the track at the agent's current position. We used the same neural network architecture as the one used in the Atari experiments specified in Supplementary Section 8. We performed experiments using four different settings - the agent controlling a slow car with and without opponent bots, and the agent controlling a fast car with and without opponent bots. Full results can be found in Supplementary Figure S6. A3C was the best performing agent, reaching between roughly 75% and 90% of the score obtained by a human tester on all four game configurations in about 12 hours of training. A video showing the learned driving behavior of the A3C agent can be found at https : / / youtu · be / 0xo1Ldx3L5Q."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1035
                },
                {
                    "x": 1136,
                    "y": 1035
                },
                {
                    "x": 1136,
                    "y": 1134
                },
                {
                    "x": 224,
                    "y": 1134
                }
            ],
            "category": "paragraph",
            "html": "<p id='75' style='font-size:20px'>5.3. Continuous Action Control Using the MuJoCo<br>Physics Simulator</p>",
            "id": 75,
            "page": 6,
            "text": "5.3. Continuous Action Control Using the MuJoCo Physics Simulator"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1159
                },
                {
                    "x": 1215,
                    "y": 1159
                },
                {
                    "x": 1215,
                    "y": 1914
                },
                {
                    "x": 223,
                    "y": 1914
                }
            ],
            "category": "paragraph",
            "html": "<p id='76' style='font-size:16px'>We also examined a set of tasks where the action space<br>is continuous. In particular, we looked at a set of rigid<br>body physics domains with contact dynamics where the<br>tasks include many examples of manipulation and loco-<br>motion. These tasks were simulated using the Mujoco<br>physics engine. We evaluated only the asynchronous ad-<br>vantage actor-critic algorithm since, unlike the value-based<br>methods, it is easily extended to continuous actions. In all<br>problems, using either the physical state or pixels as in-<br>put, Asynchronous Advantage-Critic found good solutions<br>in less than 24 hours of training and typically in under a few<br>hours. Some successful policies learned by our agent can<br>be seen in the following video https : / / youtu · be /<br>Ajjc08- iPx8. Further details about this experiment can<br>be found in Supplementary Section 9.</p>",
            "id": 76,
            "page": 6,
            "text": "We also examined a set of tasks where the action space is continuous. In particular, we looked at a set of rigid body physics domains with contact dynamics where the tasks include many examples of manipulation and locomotion. These tasks were simulated using the Mujoco physics engine. We evaluated only the asynchronous advantage actor-critic algorithm since, unlike the value-based methods, it is easily extended to continuous actions. In all problems, using either the physical state or pixels as input, Asynchronous Advantage-Critic found good solutions in less than 24 hours of training and typically in under a few hours. Some successful policies learned by our agent can be seen in the following video https : / / youtu · be / Ajjc08- iPx8. Further details about this experiment can be found in Supplementary Section 9."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1964
                },
                {
                    "x": 490,
                    "y": 1964
                },
                {
                    "x": 490,
                    "y": 2015
                },
                {
                    "x": 224,
                    "y": 2015
                }
            ],
            "category": "paragraph",
            "html": "<p id='77' style='font-size:22px'>5.4. Labyrinth</p>",
            "id": 77,
            "page": 6,
            "text": "5.4. Labyrinth"
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 2042
                },
                {
                    "x": 1215,
                    "y": 2042
                },
                {
                    "x": 1215,
                    "y": 2995
                },
                {
                    "x": 222,
                    "y": 2995
                }
            ],
            "category": "paragraph",
            "html": "<p id='78' style='font-size:14px'>We performed an additional set of experiments with A3C<br>on a new 3D environment called Labyrinth. The specific<br>task we considered involved the agent learning to find re-<br>wards in randomly generated mazes. At the beginning of<br>each episode the agent was placed in a new randomly gen-<br>erated maze consisting of rooms and corridors. Each maze<br>contained two types of objects that the agent was rewarded<br>for finding - apples and portals. Picking up an apple led to<br>a reward of 1. Entering a portal led to a reward of 10 after<br>which the agent was respawned in a new random location in<br>the maze and all previously collected apples were regener-<br>ated. An episode terminated after 60 seconds after which a<br>new episode would begin. The aim of the agent is to collect<br>as many points as possible in the time limit and the optimal<br>strategy involves first finding the portal and then repeatedly<br>going back to it after each respawn. This task is much more<br>challenging than the TORCS driving domain because the<br>agent is faced with a new maze in each episode and must<br>learn a general strategy for exploring random mazes.</p>",
            "id": 78,
            "page": 6,
            "text": "We performed an additional set of experiments with A3C on a new 3D environment called Labyrinth. The specific task we considered involved the agent learning to find rewards in randomly generated mazes. At the beginning of each episode the agent was placed in a new randomly generated maze consisting of rooms and corridors. Each maze contained two types of objects that the agent was rewarded for finding - apples and portals. Picking up an apple led to a reward of 1. Entering a portal led to a reward of 10 after which the agent was respawned in a new random location in the maze and all previously collected apples were regenerated. An episode terminated after 60 seconds after which a new episode would begin. The aim of the agent is to collect as many points as possible in the time limit and the optimal strategy involves first finding the portal and then repeatedly going back to it after each respawn. This task is much more challenging than the TORCS driving domain because the agent is faced with a new maze in each episode and must learn a general strategy for exploring random mazes."
        },
        {
            "bounding_box": [
                {
                    "x": 1367,
                    "y": 269
                },
                {
                    "x": 2170,
                    "y": 269
                },
                {
                    "x": 2170,
                    "y": 545
                },
                {
                    "x": 1367,
                    "y": 545
                }
            ],
            "category": "table",
            "html": "<br><table id='79' style='font-size:14px'><tr><td></td><td colspan=\"5\">Number of threads</td></tr><tr><td>Method</td><td>1</td><td>2</td><td>4</td><td>8</td><td>16</td></tr><tr><td>1-step Q</td><td>1.0</td><td>3.0</td><td>6.3</td><td>13.3</td><td>24.1</td></tr><tr><td>1-step SARSA</td><td>1.0</td><td>2.8</td><td>5.9</td><td>13.1</td><td>22.1</td></tr><tr><td>n-step Q</td><td>1.0</td><td>2.7</td><td>5.9</td><td>10.7</td><td>17.2</td></tr><tr><td>A3C</td><td>1.0</td><td>2.1</td><td>3.7</td><td>6.9</td><td>12.5</td></tr></table>",
            "id": 79,
            "page": 6,
            "text": "Number of threads  Method 1 2 4 8 16  1-step Q 1.0 3.0 6.3 13.3 24.1  1-step SARSA 1.0 2.8 5.9 13.1 22.1  n-step Q 1.0 2.7 5.9 10.7 17.2  A3C 1.0 2.1 3.7 6.9"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 573
                },
                {
                    "x": 2264,
                    "y": 573
                },
                {
                    "x": 2264,
                    "y": 1035
                },
                {
                    "x": 1272,
                    "y": 1035
                }
            ],
            "category": "caption",
            "html": "<caption id='80' style='font-size:14px'>Table 2. The average training speedup for each method and num-<br>ber of threads averaged over seven Atari games. To compute the<br>training speed-up on a single game we measured the time to re-<br>quired reach a fixed reference score using each method and num-<br>ber of threads. The speedup from using n threads on a game was<br>defined as the time required to reach a fixed reference score using<br>one thread divided the time required to reach the reference score<br>using n threads. The table shows the speedups averaged over<br>seven Atari games (Beamrider, Breakout, Enduro, Pong, Q*bert,<br>Seaquest, and Space Invaders).</caption>",
            "id": 80,
            "page": 6,
            "text": "Table 2. The average training speedup for each method and number of threads averaged over seven Atari games. To compute the training speed-up on a single game we measured the time to required reach a fixed reference score using each method and number of threads. The speedup from using n threads on a game was defined as the time required to reach a fixed reference score using one thread divided the time required to reach the reference score using n threads. The table shows the speedups averaged over seven Atari games (Beamrider, Breakout, Enduro, Pong, Q*bert, Seaquest, and Space Invaders)."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 1090
                },
                {
                    "x": 2262,
                    "y": 1090
                },
                {
                    "x": 2262,
                    "y": 1442
                },
                {
                    "x": 1271,
                    "y": 1442
                }
            ],
            "category": "paragraph",
            "html": "<p id='81' style='font-size:16px'>We trained an A3C LSTM agent on this task using only<br>84 x 84 RGB images as input. The final average score<br>of around 50 indicates that the agent learned a reason-<br>able strategy for exploring random 3D maxes using only<br>a visual input. A video showing one of the agents ex-<br>ploring previously unseen mazes is included at https :<br>/ / youtu · be / nMR5m jCFZCw.</p>",
            "id": 81,
            "page": 6,
            "text": "We trained an A3C LSTM agent on this task using only 84 x 84 RGB images as input. The final average score of around 50 indicates that the agent learned a reasonable strategy for exploring random 3D maxes using only a visual input. A video showing one of the agents exploring previously unseen mazes is included at https : / / youtu · be / nMR5m jCFZCw."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1494
                },
                {
                    "x": 1902,
                    "y": 1494
                },
                {
                    "x": 1902,
                    "y": 1546
                },
                {
                    "x": 1272,
                    "y": 1546
                }
            ],
            "category": "paragraph",
            "html": "<p id='82' style='font-size:20px'>5.5. Scalability and Data Efficiency</p>",
            "id": 82,
            "page": 6,
            "text": "5.5. Scalability and Data Efficiency"
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 1571
                },
                {
                    "x": 2264,
                    "y": 1571
                },
                {
                    "x": 2264,
                    "y": 2470
                },
                {
                    "x": 1271,
                    "y": 2470
                }
            ],
            "category": "paragraph",
            "html": "<p id='83' style='font-size:16px'>We analyzed the effectiveness of our proposed framework<br>by looking at how the training time and data efficiency<br>changes with the number of parallel actor-learners. When<br>using multiple workers in parallel and updating a shared<br>model, one would expect that in an ideal case, for a given<br>task and algorithm, the number of training steps to achieve<br>a certain score would remain the same with varying num-<br>bers of workers. Therefore, the advantage would be solely<br>due to the ability of the system to consume more data in<br>the same amount of wall clock time and possibly improved<br>exploration. Table 2 shows the training speed-up achieved<br>by using increasing numbers of parallel actor-learners av-<br>eraged over seven Atari games. These results show that all<br>four methods achieve substantial speedups from using mul-<br>tiple worker threads, with 16 threads leading to at least an<br>order of magnitude speedup. This confirms that our pro-<br>posed framework scales well with the number of parallel<br>workers, making efficient use of resources.</p>",
            "id": 83,
            "page": 6,
            "text": "We analyzed the effectiveness of our proposed framework by looking at how the training time and data efficiency changes with the number of parallel actor-learners. When using multiple workers in parallel and updating a shared model, one would expect that in an ideal case, for a given task and algorithm, the number of training steps to achieve a certain score would remain the same with varying numbers of workers. Therefore, the advantage would be solely due to the ability of the system to consume more data in the same amount of wall clock time and possibly improved exploration. Table 2 shows the training speed-up achieved by using increasing numbers of parallel actor-learners averaged over seven Atari games. These results show that all four methods achieve substantial speedups from using multiple worker threads, with 16 threads leading to at least an order of magnitude speedup. This confirms that our proposed framework scales well with the number of parallel workers, making efficient use of resources."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2494
                },
                {
                    "x": 2262,
                    "y": 2494
                },
                {
                    "x": 2262,
                    "y": 2993
                },
                {
                    "x": 1273,
                    "y": 2993
                }
            ],
            "category": "paragraph",
            "html": "<p id='84' style='font-size:16px'>Somewhat surprisingly, asynchronous one-step Q-learning<br>and Sarsa algorithms exhibit superlinear speedups that<br>cannot be explained by purely computational gains. We<br>observe that one-step methods (one-step Q and one-step<br>Sarsa) often require less data to achieve a particular score<br>when using more parallel actor-learners. We believe this<br>is due to positive effect of multiple threads to reduce the<br>bias in one-step methods. These effects are shown more<br>clearly in Figure 3, which shows plots of the average score<br>against the total number of training frames for different</p>",
            "id": 84,
            "page": 6,
            "text": "Somewhat surprisingly, asynchronous one-step Q-learning and Sarsa algorithms exhibit superlinear speedups that cannot be explained by purely computational gains. We observe that one-step methods (one-step Q and one-step Sarsa) often require less data to achieve a particular score when using more parallel actor-learners. We believe this is due to positive effect of multiple threads to reduce the bias in one-step methods. These effects are shown more clearly in Figure 3, which shows plots of the average score against the total number of training frames for different"
        },
        {
            "bounding_box": [
                {
                    "x": 777,
                    "y": 191
                },
                {
                    "x": 1711,
                    "y": 191
                },
                {
                    "x": 1711,
                    "y": 236
                },
                {
                    "x": 777,
                    "y": 236
                }
            ],
            "category": "header",
            "html": "<header id='85' style='font-size:16px'>Asynchronous Methods for Deep Reinforcement Learning</header>",
            "id": 85,
            "page": 7,
            "text": "Asynchronous Methods for Deep Reinforcement Learning"
        },
        {
            "bounding_box": [
                {
                    "x": 277,
                    "y": 275
                },
                {
                    "x": 2199,
                    "y": 275
                },
                {
                    "x": 2199,
                    "y": 633
                },
                {
                    "x": 277,
                    "y": 633
                }
            ],
            "category": "figure",
            "html": "<figure><img id='86' style='font-size:14px' alt=\"A3C, Beamrider A3C, Breakout A3C, Pong A3C, Q*bert A3C, Space Invaders\n16000 1000 30 12000 1400\n14000\n10000 1200\n800 20\n12000\n8000 1000\n10000 600 10\n6000 800\nScore 8000 Score 400 Score 0 Score Score\n6000\n4000 600\n4000 200 10\n2000 400\n2000\n0 -20\n0 200\n0\n-2000 -200 -30 -2000 0\n10-4 10-3 10-2 10-4 10-3 10-2 10-4 10 3 10-2 10-4 10-3 10-2 10~4 10-3 10-2\nLearning rate Learning rate Learning rate Learning rate Learning rate\" data-coord=\"top-left:(277,275); bottom-right:(2199,633)\" /></figure>",
            "id": 86,
            "page": 7,
            "text": "A3C, Beamrider A3C, Breakout A3C, Pong A3C, Q*bert A3C, Space Invaders 16000 1000 30 12000 1400 14000 10000 1200 800 20 12000 8000 1000 10000 600 10 6000 800 Score 8000 Score 400 Score 0 Score Score 6000 4000 600 4000 200 10 2000 400 2000 0 -20 0 200 0 -2000 -200 -30 -2000 0 10-4 10-3 10-2 10-4 10-3 10-2 10-4 10 3 10-2 10-4 10-3 10-2 10~4 10-3 10-2 Learning rate Learning rate Learning rate Learning rate Learning rate"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 656
                },
                {
                    "x": 2264,
                    "y": 656
                },
                {
                    "x": 2264,
                    "y": 797
                },
                {
                    "x": 224,
                    "y": 797
                }
            ],
            "category": "caption",
            "html": "<caption id='87' style='font-size:14px'>Figure 2. Scatter plots of scores obtained by asynchronous advantage actor-critic on five games (Beamrider, Breakout, Pong, Q*bert,<br>Space Invaders) for 50 different learning rates and random initializations. On each game, there is a wide range of learning rates for<br>which all random initializations acheive good scores. This shows that A3C is quite robust to learning rates and initial random weights.</caption>",
            "id": 87,
            "page": 7,
            "text": "Figure 2. Scatter plots of scores obtained by asynchronous advantage actor-critic on five games (Beamrider, Breakout, Pong, Q*bert, Space Invaders) for 50 different learning rates and random initializations. On each game, there is a wide range of learning rates for which all random initializations acheive good scores. This shows that A3C is quite robust to learning rates and initial random weights."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 826
                },
                {
                    "x": 1215,
                    "y": 826
                },
                {
                    "x": 1215,
                    "y": 976
                },
                {
                    "x": 224,
                    "y": 976
                }
            ],
            "category": "paragraph",
            "html": "<p id='88' style='font-size:16px'>numbers of actor-learners and training methods on five<br>Atari games, and Figure 4, which shows plots of the av-<br>erage score against wall-clock time.</p>",
            "id": 88,
            "page": 7,
            "text": "numbers of actor-learners and training methods on five Atari games, and Figure 4, which shows plots of the average score against wall-clock time."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1027
                },
                {
                    "x": 742,
                    "y": 1027
                },
                {
                    "x": 742,
                    "y": 1080
                },
                {
                    "x": 223,
                    "y": 1080
                }
            ],
            "category": "paragraph",
            "html": "<p id='89' style='font-size:20px'>5.6. Robustness and Stability</p>",
            "id": 89,
            "page": 7,
            "text": "5.6. Robustness and Stability"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1104
                },
                {
                    "x": 1216,
                    "y": 1104
                },
                {
                    "x": 1216,
                    "y": 1809
                },
                {
                    "x": 224,
                    "y": 1809
                }
            ],
            "category": "paragraph",
            "html": "<p id='90' style='font-size:18px'>Finally, we analyzed the stability and robustness of the<br>four proposed asynchronous algorithms. For each of the<br>four algorithms we trained models on five games (Break-<br>out, Beamrider, Pong, Q*bert, Space Invaders) using 50<br>different learning rates and random initializations. Figure 2<br>shows scatter plots of the resulting scores for A3C, while<br>Supplementary Figure S11 shows plots for the other three<br>methods. There is usually a range of learning rates for each<br>method and game combination that leads to good scores,<br>indicating that all methods are quite robust to the choice of<br>learning rate and random initialization. The fact that there<br>are virtually no points with scores of 0 in regions with good<br>learning rates indicates that the methods are stable and do<br>not collapse or diverge once they are learning.</p>",
            "id": 90,
            "page": 7,
            "text": "Finally, we analyzed the stability and robustness of the four proposed asynchronous algorithms. For each of the four algorithms we trained models on five games (Breakout, Beamrider, Pong, Q*bert, Space Invaders) using 50 different learning rates and random initializations. Figure 2 shows scatter plots of the resulting scores for A3C, while Supplementary Figure S11 shows plots for the other three methods. There is usually a range of learning rates for each method and game combination that leads to good scores, indicating that all methods are quite robust to the choice of learning rate and random initialization. The fact that there are virtually no points with scores of 0 in regions with good learning rates indicates that the methods are stable and do not collapse or diverge once they are learning."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1856
                },
                {
                    "x": 870,
                    "y": 1856
                },
                {
                    "x": 870,
                    "y": 1912
                },
                {
                    "x": 224,
                    "y": 1912
                }
            ],
            "category": "paragraph",
            "html": "<p id='91' style='font-size:22px'>6. Conclusions and Discussion</p>",
            "id": 91,
            "page": 7,
            "text": "6. Conclusions and Discussion"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1941
                },
                {
                    "x": 1213,
                    "y": 1941
                },
                {
                    "x": 1213,
                    "y": 2543
                },
                {
                    "x": 223,
                    "y": 2543
                }
            ],
            "category": "paragraph",
            "html": "<p id='92' style='font-size:16px'>We have presented asynchronous versions of four standard<br>reinforcement learning algorithms and showed that they<br>are able to train neural network controllers on a variety<br>of domains in a stable manner. Our results show that in<br>our proposed framework stable training of neural networks<br>through reinforcement learning is possible with both value-<br>based and policy-based methods, off-policy as well as on-<br>policy methods, and in discrete as well as continuous do-<br>mains. When trained on the Atari domain using 16 CPU<br>cores, the proposed asynchronous algorithms train faster<br>than DQN trained on an Nvidia K40 GPU, with A3C sur-<br>passing the current state-of-the-art in half the training time.</p>",
            "id": 92,
            "page": 7,
            "text": "We have presented asynchronous versions of four standard reinforcement learning algorithms and showed that they are able to train neural network controllers on a variety of domains in a stable manner. Our results show that in our proposed framework stable training of neural networks through reinforcement learning is possible with both valuebased and policy-based methods, off-policy as well as onpolicy methods, and in discrete as well as continuous domains. When trained on the Atari domain using 16 CPU cores, the proposed asynchronous algorithms train faster than DQN trained on an Nvidia K40 GPU, with A3C surpassing the current state-of-the-art in half the training time."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2565
                },
                {
                    "x": 1213,
                    "y": 2565
                },
                {
                    "x": 1213,
                    "y": 2967
                },
                {
                    "x": 224,
                    "y": 2967
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='93' style='font-size:16px'>One of our main findings is that using parallel actor-<br>learners to update a shared model had a stabilizing effect on<br>the learning process of the three value-based methods we<br>considered. While this shows that stable online Q-learning<br>is possible without experience replay, which was used for<br>this purpose in DQN, it does not mean that experience re-<br>play is not useful. Incorporating experience replay into<br>the asynchronous reinforcement learning framework could</p>",
            "id": 93,
            "page": 7,
            "text": "One of our main findings is that using parallel actorlearners to update a shared model had a stabilizing effect on the learning process of the three value-based methods we considered. While this shows that stable online Q-learning is possible without experience replay, which was used for this purpose in DQN, it does not mean that experience replay is not useful. Incorporating experience replay into the asynchronous reinforcement learning framework could"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 826
                },
                {
                    "x": 2263,
                    "y": 826
                },
                {
                    "x": 2263,
                    "y": 1075
                },
                {
                    "x": 1272,
                    "y": 1075
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='94' style='font-size:16px'>substantially improve the data efficiency of these methods<br>by reusing old data. This could in turn lead to much faster<br>training times in domains like TORCS where interacting<br>with the environment is more expensive than updating the<br>model for the architecture we used.</p>",
            "id": 94,
            "page": 7,
            "text": "substantially improve the data efficiency of these methods by reusing old data. This could in turn lead to much faster training times in domains like TORCS where interacting with the environment is more expensive than updating the model for the architecture we used."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 1099
                },
                {
                    "x": 2264,
                    "y": 1099
                },
                {
                    "x": 2264,
                    "y": 2099
                },
                {
                    "x": 1270,
                    "y": 2099
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='95' style='font-size:18px'>Combining other existing reinforcement learning meth-<br>ods or recent advances in deep reinforcement learning<br>with our asynchronous framework presents many possibil-<br>ities for immediate improvements to the methods we pre-<br>sented. While our n-step methods operate in the forward<br>view (Sutton & Barto, 1998) by using corrected n-step re-<br>turns directly as targets, it has been more common to use<br>the backward view to implicitly combine different returns<br>through eligibility traces (Watkins, 1989; Sutton & Barto,<br>1998; Peng & Williams, 1996). The asynchronous ad-<br>vantage actor-critic method could be potentially improved<br>by using other ways of estimating the advantage function,<br>such as generalized advantage estimation of (Schulman<br>et al., 2015b). All of the value-based methods we inves-<br>tigated could benefit from different ways of reducing over-<br>estimation bias of Q-values (Van Hasselt et al., 2015; Belle-<br>mare et al., 2016). Yet another, more speculative, direction<br>is to try and combine the recent work on true online tempo-<br>ral difference methods (van Seijen et al., 2015) with non-<br>linear function approximation.</p>",
            "id": 95,
            "page": 7,
            "text": "Combining other existing reinforcement learning methods or recent advances in deep reinforcement learning with our asynchronous framework presents many possibilities for immediate improvements to the methods we presented. While our n-step methods operate in the forward view (Sutton & Barto, 1998) by using corrected n-step returns directly as targets, it has been more common to use the backward view to implicitly combine different returns through eligibility traces (Watkins, 1989; Sutton & Barto, 1998; Peng & Williams, 1996). The asynchronous advantage actor-critic method could be potentially improved by using other ways of estimating the advantage function, such as generalized advantage estimation of (Schulman , 2015b). All of the value-based methods we investigated could benefit from different ways of reducing overestimation bias of Q-values (Van Hasselt , 2015; Bellemare , 2016). Yet another, more speculative, direction is to try and combine the recent work on true online temporal difference methods (van Seijen , 2015) with nonlinear function approximation."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2120
                },
                {
                    "x": 2263,
                    "y": 2120
                },
                {
                    "x": 2263,
                    "y": 2571
                },
                {
                    "x": 1272,
                    "y": 2571
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='96' style='font-size:16px'>In addition to these algorithmic improvements, a number<br>of complementary improvements to the neural network ar-<br>chitecture are possible. The dueling architecture of (Wang<br>et al., 2015) has been shown to produce more accurate es-<br>timates of Q-values by including separate streams for the<br>state value and advantage in the network. The spatial soft-<br>max proposed by (Levine et al., 2015) could improve both<br>value-based and policy-based methods by making it easier<br>for the network to represent feature coordinates.</p>",
            "id": 96,
            "page": 7,
            "text": "In addition to these algorithmic improvements, a number of complementary improvements to the neural network architecture are possible. The dueling architecture of (Wang , 2015) has been shown to produce more accurate estimates of Q-values by including separate streams for the state value and advantage in the network. The spatial softmax proposed by (Levine , 2015) could improve both value-based and policy-based methods by making it easier for the network to represent feature coordinates."
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 2620
                },
                {
                    "x": 1667,
                    "y": 2620
                },
                {
                    "x": 1667,
                    "y": 2664
                },
                {
                    "x": 1276,
                    "y": 2664
                }
            ],
            "category": "paragraph",
            "html": "<p id='97' style='font-size:16px'>ACKNOWLEDGMENTS</p>",
            "id": 97,
            "page": 7,
            "text": "ACKNOWLEDGMENTS"
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2694
                },
                {
                    "x": 2262,
                    "y": 2694
                },
                {
                    "x": 2262,
                    "y": 2946
                },
                {
                    "x": 1273,
                    "y": 2946
                }
            ],
            "category": "paragraph",
            "html": "<p id='98' style='font-size:18px'>We thank Thomas Degris, Remi Munos, Marc Lanctot,<br>Sasha Vezhnevets and Joseph Modayil for many helpful<br>discussions, suggestions and comments on the paper. We<br>also thank the DeepMind evaluation team for setting up the<br>environments used to evaluate the agents in the paper.</p>",
            "id": 98,
            "page": 7,
            "text": "We thank Thomas Degris, Remi Munos, Marc Lanctot, Sasha Vezhnevets and Joseph Modayil for many helpful discussions, suggestions and comments on the paper. We also thank the DeepMind evaluation team for setting up the environments used to evaluate the agents in the paper."
        },
        {
            "bounding_box": [
                {
                    "x": 776,
                    "y": 191
                },
                {
                    "x": 1712,
                    "y": 191
                },
                {
                    "x": 1712,
                    "y": 237
                },
                {
                    "x": 776,
                    "y": 237
                }
            ],
            "category": "header",
            "html": "<header id='99' style='font-size:22px'>Asynchronous Methods for Deep Reinforcement Learning</header>",
            "id": 99,
            "page": 8,
            "text": "Asynchronous Methods for Deep Reinforcement Learning"
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 326
                },
                {
                    "x": 2194,
                    "y": 326
                },
                {
                    "x": 2194,
                    "y": 1395
                },
                {
                    "x": 289,
                    "y": 1395
                }
            ],
            "category": "figure",
            "html": "<figure><img id='100' style='font-size:16px' alt=\"Beamrider Breakout Pong Q*bert Space Invaders\n10000 350 20 4500 800\n1-step Q. 1 threads 1-step Q. 1 threads 1-step Q. 1 threads\n1-step Q, 2 threads 1-step Q. 2 threads 1-step Q. 2 threads\n15 4000\n1-step Q. 4 threads 300 1-step Q. 4 threads 1-step Q. 4 threads 700\n8000 1-step Q. 8threads 1-step Q. 8 threads 1-step Q, 8 threads\n10 3500\n1-step Q. 16 threads 1-step Q. 16 threads 1-step Q. 16 threads\n250 600\n5 3000\n6000\n200 2500 500\n0 Score\nScore\nScore\n-5 2000 Score\n150 Score\n400\n4000\n-10 1500\n100 300\n-15 1-step Q. 1 threads 1-step Q, 1 threads\n1000\n2000 1-step Q, 2 threads 1-step Q. 2 threads\n50 1-step Q. 4 threads 200 1-step Q, 4 threads\n-20 500 1-step Q. 8 threads\n1-step Q. 8 threads\n1-step Q. 16 threads 1-step Q. 16 threads\n0 0 -25 0 100\n0 10 20 30 40 0 10 20 30 40 0 10 20 30 40 0 10 20 30 40 0 10 20 30 40\nTraining epochs Training epochs Training epochs Training epochs Training epochs\nBeamrider Breakout Pong Q*bert Space Invaders\n12000 350 20 6000 800\nn-step Q. 1 threads n-step Q. 1 threads n-step Q. 1 threads\nn-step Q. 2 threads n-step Q. 2 threads n-step Q. 2 threads\n15\nn-step Q, 4 threads 300 n-step Q. 4 threads n-step Q. 4 threads 700\n10000\n5000\nn-step Q, 8 threads n-step Q. 8 threads n-step Q, 8 threads\n10\nn-step Q. 16 threads n-step Q. 16 threads n-step Q. 16 threads\n250 600\n8000 5 4000\n200 500\n0 Score\nScore 6000 Score 3000\nScore\n-5\n150 Score\n400\n4000 -10 2000\n100 300\nn-step Q. 1 threads n-step Q. 1 threads\n-15\nn-step Q. 2 threads n-step Q, 2 threads\n2000 1000\n50 n-step Q, 4 threads 200 n-step Q, 4 threads\n-20 8 threads\nn-step Q. 8 threads n-step Q.\nn-step Q. 16 threads n-step Q, 16 threads\n0 0 -25 0 100\n0 10 20 30 40 0 10 20 30 40 0 10 20 30 40 0 10 20 30 40 0 10 20 30 40\nTraining epochs Training epochs Training epochs Training epochs Training epochs\nBeamrider Breakout Pong Q*bert Space Invaders\n16000 800 30 12000 1400\nA3C, 1 threads A3C, 1 threads A3C, 1 threads A3C, 1 threads\nA3C, 2 threads A3C, 2 threads A3C, 2 threads A3C, 2 threads\n14000 4 threads A3C, 4 threads A3C, 4 threads 1200 A3C, 4 threads\n700\nA3C,\n20 10000\nA3C, 8 threads A3C, 8 threads A3C, 8 threads A3C, 8 threads\n12000 A3C, 16 threads 600 A3C, 16 threads A3C, 16 threads A3C, 16 threads\n1000\n10 8000\n10000 500\nScore 800\nScore\nScore\n8000 400 0 Score 6000 Score\n600\n6000 300\n-10 4000\n400\n4000 200 A3C, 1 threads\nA3C, 2 threads\n-20 2000\n2000 100 A3C, 4 threads 200\nA3C, 8 threads\nA3C, 16 threads\n0 0 -30 0 0\n0 10 20 30 40 0 10 20 30 40 0 10 20 30 40 0 10 20 30 40 0 10 20 30 40\nTraining epochs Training epochs Training epochs Training epochs Training epochs\" data-coord=\"top-left:(289,326); bottom-right:(2194,1395)\" /></figure>",
            "id": 100,
            "page": 8,
            "text": "Beamrider Breakout Pong Q*bert Space Invaders 10000 350 20 4500 800 1-step Q. 1 threads 1-step Q. 1 threads 1-step Q. 1 threads 1-step Q, 2 threads 1-step Q. 2 threads 1-step Q. 2 threads 15 4000 1-step Q. 4 threads 300 1-step Q. 4 threads 1-step Q. 4 threads 700 8000 1-step Q. 8threads 1-step Q. 8 threads 1-step Q, 8 threads 10 3500 1-step Q. 16 threads 1-step Q. 16 threads 1-step Q. 16 threads 250 600 5 3000 6000 200 2500 500 0 Score Score Score -5 2000 Score 150 Score 400 4000 -10 1500 100 300 -15 1-step Q. 1 threads 1-step Q, 1 threads 1000 2000 1-step Q, 2 threads 1-step Q. 2 threads 50 1-step Q. 4 threads 200 1-step Q, 4 threads -20 500 1-step Q. 8 threads 1-step Q. 8 threads 1-step Q. 16 threads 1-step Q. 16 threads 0 0 -25 0 100 0 10 20 30 40 0 10 20 30 40 0 10 20 30 40 0 10 20 30 40 0 10 20 30 40 Training epochs Training epochs Training epochs Training epochs Training epochs Beamrider Breakout Pong Q*bert Space Invaders 12000 350 20 6000 800 n-step Q. 1 threads n-step Q. 1 threads n-step Q. 1 threads n-step Q. 2 threads n-step Q. 2 threads n-step Q. 2 threads 15 n-step Q, 4 threads 300 n-step Q. 4 threads n-step Q. 4 threads 700 10000 5000 n-step Q, 8 threads n-step Q. 8 threads n-step Q, 8 threads 10 n-step Q. 16 threads n-step Q. 16 threads n-step Q. 16 threads 250 600 8000 5 4000 200 500 0 Score Score 6000 Score 3000 Score -5 150 Score 400 4000 -10 2000 100 300 n-step Q. 1 threads n-step Q. 1 threads -15 n-step Q. 2 threads n-step Q, 2 threads 2000 1000 50 n-step Q, 4 threads 200 n-step Q, 4 threads -20 8 threads n-step Q. 8 threads n-step Q. n-step Q. 16 threads n-step Q, 16 threads 0 0 -25 0 100 0 10 20 30 40 0 10 20 30 40 0 10 20 30 40 0 10 20 30 40 0 10 20 30 40 Training epochs Training epochs Training epochs Training epochs Training epochs Beamrider Breakout Pong Q*bert Space Invaders 16000 800 30 12000 1400 A3C, 1 threads A3C, 1 threads A3C, 1 threads A3C, 1 threads A3C, 2 threads A3C, 2 threads A3C, 2 threads A3C, 2 threads 14000 4 threads A3C, 4 threads A3C, 4 threads 1200 A3C, 4 threads 700 A3C, 20 10000 A3C, 8 threads A3C, 8 threads A3C, 8 threads A3C, 8 threads 12000 A3C, 16 threads 600 A3C, 16 threads A3C, 16 threads A3C, 16 threads 1000 10 8000 10000 500 Score 800 Score Score 8000 400 0 Score 6000 Score 600 6000 300 -10 4000 400 4000 200 A3C, 1 threads A3C, 2 threads -20 2000 2000 100 A3C, 4 threads 200 A3C, 8 threads A3C, 16 threads 0 0 -30 0 0 0 10 20 30 40 0 10 20 30 40 0 10 20 30 40 0 10 20 30 40 0 10 20 30 40 Training epochs Training epochs Training epochs Training epochs Training epochs"
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 1443
                },
                {
                    "x": 2265,
                    "y": 1443
                },
                {
                    "x": 2265,
                    "y": 1631
                },
                {
                    "x": 222,
                    "y": 1631
                }
            ],
            "category": "caption",
            "html": "<caption id='101' style='font-size:18px'>Figure 3. Data efficiency comparison of different numbers of actor-learners for three asynchronous methods on five Atari games. The<br>x-axis shows the total number of training epochs where an epoch corresponds to four million frames (across all threads). The y-axis<br>shows the average score. Each curve shows the average over the three best learning rates. Single step methods show increased data<br>efficiency from more parallel workers. Results for Sarsa are shown in Supplementary Figure S9.</caption>",
            "id": 101,
            "page": 8,
            "text": "Figure 3. Data efficiency comparison of different numbers of actor-learners for three asynchronous methods on five Atari games. The x-axis shows the total number of training epochs where an epoch corresponds to four million frames (across all threads). The y-axis shows the average score. Each curve shows the average over the three best learning rates. Single step methods show increased data efficiency from more parallel workers. Results for Sarsa are shown in Supplementary Figure S9."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 1698
                },
                {
                    "x": 2195,
                    "y": 1698
                },
                {
                    "x": 2195,
                    "y": 2759
                },
                {
                    "x": 293,
                    "y": 2759
                }
            ],
            "category": "figure",
            "html": "<figure><img id='102' style='font-size:14px' alt=\"Beamrider Breakout Pong Q*bert Space Invaders\n9000 300 20 4000 800\n1-step Q, 1 threads 1-step Q. 1 threads 1-step Q, 1 threads 1-step Q. 1 threads 1-step Q. 1 threads\n1-step Q. 2 threads 1-step Q. 2 threads 1-step Q. 2 threads 1-step Q, 2 threads 1-step Q. 2 threads\n8000\n15\n3500 4 threads\n1-step Q, 4 threads 1-step Q. 4 threads 1-step Q, 4 threads 1-step Q. 4 threads 700 1-step Q,\n250\n1-step Q. 8 threads 1-step Q. 8 threads 1-step Q, 8 threads 1-step Q. 8threads 1-step Q. 8threads\n7000 10\n1-step Q. 16 threads 1-step Q. 16 threads 1-step Q. 16 threads 3000 1-step Q, 16 threads 1-step Q. 16 threads\n600\n6000 200 5\n2500\n500\n5000 0 Score\nScore\nScore\n150 2000\n4000 Score\n-5 Score\n400\n1500\n3000 100 -10\n300\n1000\n2000 -15\n50\n200\n1000 -20 500\n0 0 -25 0 100\n0 2 4 6 8 10 12 14 0 2 4 6 8 10 12 14 0 2 4 6 8 10 12 14 0 2 4 6 8 10 12 14 0 2 4 6 8 10 12 14\nTraining time (hours) Training time (hours) Training time (hours) Training time (hours) Training time (hours)\nBeamrider Breakout Pong Q*bert Space Invaders\n12000 350 20 4500 800\nn-step Q, 1 threads n-step Q. 1 threads n-step Q. 1 threads n-step Q. 1 threads\nn-step Q. 2 threads n-step Q. 2 threads 15 4000 n-step Q. 2 threads n-step Q, 2 threads\n10000 n-step Q. 4 threads 300 n-step Q. 4 threads n-step Q. 4 threads 700 n-step Q. 4 threads\nn-step Q. 8 threads n-step Q. 8 threads n-step Q, 8 threads n-step Q. 8 threads\n10 3500\nn-step Q. 16 threads n-step Q. 16 threads n-step Q. 16 threads n-step Q: 16 threads\n250 600\n8000 5 3000\n200 0 2500 500\nScore\nScore\n6000\n2000 Score\n-5 Score\n150 Score\n400\n4000 -10 1500\n100 300\nn-step Q. 1 threads\n-15 1000\nn-step Q. 2 threads\n2000\n50 n-step Q. 4 threads 200\n-20 500\nn-step Q. 8 threads\nn-step Q. 16 threads\n0 0 25 0 100\n0 2 4 6 8 10 12 14 0 2 4 6 8 10 12 14 0 2 4 6 8 10 12 14 0 2 4 6 8 10 12 14 0 2 4 6 8 10 12 14\nTraining time (hours) Training time (hours) Training time (hours) Training time (hours) Training time (hours)\nBeamrider Breakout Pong Q*bert Space Invaders\n16000 600 30 12000 1600\nA3C, 1 threads A3C, 1 threads A3C, 1 threads A3C, 1 threads A3C, 1 threads\nA3C, 2 threads A3C, 2 threads A3C, 2 threads A3C, 2 threads A3C, 2 threads\n14000 1400\nA3C, 4 threads A3C, 4 threads A3C, 4 threads A3C, 4 threads A3C, 4 threads\n500 20 10000\nA3C, 8 threads A3C, 8 threads A3C, 8 threads A3C, 8 threads A3C, 8 threads\n12000 A3C, 16 threads A3C, 16 threads A3C, 16 threads A3C, 16 threads 1200 A3C, 16threads\n400 10 8000\n10000 1000\nScore\nScore\nScore\nScore\nScore\n8000 300 0 6000 800\n6000 600\n200 -10 4000\n4000 400\n100 -20 2000\n2000 200\n0 0 -30 0 0\n0 2 4 6 8 10 12 14 0 2 4 6 8 10 12 14 0 2 4 6 8 10 12 14 0 2 4 6 8 10 12 14 0 2 4 6 8 10 12 14\nTraining time (hours) Training time (hours) Training time (hours) Training time (hours) Training time (hours)\" data-coord=\"top-left:(293,1698); bottom-right:(2195,2759)\" /></figure>",
            "id": 102,
            "page": 8,
            "text": "Beamrider Breakout Pong Q*bert Space Invaders 9000 300 20 4000 800 1-step Q, 1 threads 1-step Q. 1 threads 1-step Q, 1 threads 1-step Q. 1 threads 1-step Q. 1 threads 1-step Q. 2 threads 1-step Q. 2 threads 1-step Q. 2 threads 1-step Q, 2 threads 1-step Q. 2 threads 8000 15 3500 4 threads 1-step Q, 4 threads 1-step Q. 4 threads 1-step Q, 4 threads 1-step Q. 4 threads 700 1-step Q, 250 1-step Q. 8 threads 1-step Q. 8 threads 1-step Q, 8 threads 1-step Q. 8threads 1-step Q. 8threads 7000 10 1-step Q. 16 threads 1-step Q. 16 threads 1-step Q. 16 threads 3000 1-step Q, 16 threads 1-step Q. 16 threads 600 6000 200 5 2500 500 5000 0 Score Score Score 150 2000 4000 Score -5 Score 400 1500 3000 100 -10 300 1000 2000 -15 50 200 1000 -20 500 0 0 -25 0 100 0 2 4 6 8 10 12 14 0 2 4 6 8 10 12 14 0 2 4 6 8 10 12 14 0 2 4 6 8 10 12 14 0 2 4 6 8 10 12 14 Training time (hours) Training time (hours) Training time (hours) Training time (hours) Training time (hours) Beamrider Breakout Pong Q*bert Space Invaders 12000 350 20 4500 800 n-step Q, 1 threads n-step Q. 1 threads n-step Q. 1 threads n-step Q. 1 threads n-step Q. 2 threads n-step Q. 2 threads 15 4000 n-step Q. 2 threads n-step Q, 2 threads 10000 n-step Q. 4 threads 300 n-step Q. 4 threads n-step Q. 4 threads 700 n-step Q. 4 threads n-step Q. 8 threads n-step Q. 8 threads n-step Q, 8 threads n-step Q. 8 threads 10 3500 n-step Q. 16 threads n-step Q. 16 threads n-step Q. 16 threads n-step Q: 16 threads 250 600 8000 5 3000 200 0 2500 500 Score Score 6000 2000 Score -5 Score 150 Score 400 4000 -10 1500 100 300 n-step Q. 1 threads -15 1000 n-step Q. 2 threads 2000 50 n-step Q. 4 threads 200 -20 500 n-step Q. 8 threads n-step Q. 16 threads 0 0 25 0 100 0 2 4 6 8 10 12 14 0 2 4 6 8 10 12 14 0 2 4 6 8 10 12 14 0 2 4 6 8 10 12 14 0 2 4 6 8 10 12 14 Training time (hours) Training time (hours) Training time (hours) Training time (hours) Training time (hours) Beamrider Breakout Pong Q*bert Space Invaders 16000 600 30 12000 1600 A3C, 1 threads A3C, 1 threads A3C, 1 threads A3C, 1 threads A3C, 1 threads A3C, 2 threads A3C, 2 threads A3C, 2 threads A3C, 2 threads A3C, 2 threads 14000 1400 A3C, 4 threads A3C, 4 threads A3C, 4 threads A3C, 4 threads A3C, 4 threads 500 20 10000 A3C, 8 threads A3C, 8 threads A3C, 8 threads A3C, 8 threads A3C, 8 threads 12000 A3C, 16 threads A3C, 16 threads A3C, 16 threads A3C, 16 threads 1200 A3C, 16threads 400 10 8000 10000 1000 Score Score Score Score Score 8000 300 0 6000 800 6000 600 200 -10 4000 4000 400 100 -20 2000 2000 200 0 0 -30 0 0 0 2 4 6 8 10 12 14 0 2 4 6 8 10 12 14 0 2 4 6 8 10 12 14 0 2 4 6 8 10 12 14 0 2 4 6 8 10 12 14 Training time (hours) Training time (hours) Training time (hours) Training time (hours) Training time (hours)"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2809
                },
                {
                    "x": 2264,
                    "y": 2809
                },
                {
                    "x": 2264,
                    "y": 2999
                },
                {
                    "x": 224,
                    "y": 2999
                }
            ],
            "category": "caption",
            "html": "<caption id='103' style='font-size:20px'>Figure 4. Training speed comparison of different numbers of actor-learners on five Atari games. The x-axis shows training time in<br>hours while the y-axis shows the average score. Each curve shows the average over the three best learning rates. All asynchronous<br>methods show significant speedups from using greater numbers of parallel actor-learners. Results for Sarsa are shown in Supplementary<br>Figure S10.</caption>",
            "id": 103,
            "page": 8,
            "text": "Figure 4. Training speed comparison of different numbers of actor-learners on five Atari games. The x-axis shows training time in hours while the y-axis shows the average score. Each curve shows the average over the three best learning rates. All asynchronous methods show significant speedups from using greater numbers of parallel actor-learners. Results for Sarsa are shown in Supplementary Figure S10."
        },
        {
            "bounding_box": [
                {
                    "x": 777,
                    "y": 191
                },
                {
                    "x": 1710,
                    "y": 191
                },
                {
                    "x": 1710,
                    "y": 236
                },
                {
                    "x": 777,
                    "y": 236
                }
            ],
            "category": "header",
            "html": "<header id='104' style='font-size:14px'>Asynchronous Methods for Deep Reinforcement Learning</header>",
            "id": 104,
            "page": 9,
            "text": "Asynchronous Methods for Deep Reinforcement Learning"
        },
        {
            "bounding_box": [
                {
                    "x": 226,
                    "y": 278
                },
                {
                    "x": 468,
                    "y": 278
                },
                {
                    "x": 468,
                    "y": 332
                },
                {
                    "x": 226,
                    "y": 332
                }
            ],
            "category": "paragraph",
            "html": "<p id='105' style='font-size:22px'>References</p>",
            "id": 105,
            "page": 9,
            "text": "References"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 357
                },
                {
                    "x": 1213,
                    "y": 357
                },
                {
                    "x": 1213,
                    "y": 557
                },
                {
                    "x": 224,
                    "y": 557
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='106' style='font-size:16px'>Bellemare, Marc G, Naddaf, Yavar, Veness, Joel, and<br>Bowling, Michael. The arcade learning environment:<br>An evaluation platform for general agents. Journal of<br>Artificial Intelligence Research, 2012.</p>",
            "id": 106,
            "page": 9,
            "text": "Bellemare, Marc G, Naddaf, Yavar, Veness, Joel, and Bowling, Michael. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 2012."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 586
                },
                {
                    "x": 1213,
                    "y": 586
                },
                {
                    "x": 1213,
                    "y": 834
                },
                {
                    "x": 225,
                    "y": 834
                }
            ],
            "category": "paragraph",
            "html": "<p id='107' style='font-size:16px'>Bellemare, Marc G., Ostrovski, Georg, Guez, Arthur,<br>Thomas, Philip S., and Munos, Remi. Increasing the ac-<br>tion gap: New operators for reinforcement learning. In<br>Proceedings of the AAAI Conference on Artificial Intel-<br>ligence, 2016.</p>",
            "id": 107,
            "page": 9,
            "text": "Bellemare, Marc G., Ostrovski, Georg, Guez, Arthur, Thomas, Philip S., and Munos, Remi. Increasing the action gap: New operators for reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, 2016."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 865
                },
                {
                    "x": 1211,
                    "y": 865
                },
                {
                    "x": 1211,
                    "y": 1012
                },
                {
                    "x": 223,
                    "y": 1012
                }
            ],
            "category": "paragraph",
            "html": "<p id='108' style='font-size:14px'>Bertsekas, Dimitri P. Distributed dynamic programming.<br>Automatic Control, IEEE Transactions on, 27(3):610-<br>616, 1982.</p>",
            "id": 108,
            "page": 9,
            "text": "Bertsekas, Dimitri P. Distributed dynamic programming. Automatic Control, IEEE Transactions on, 27(3):610616, 1982."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1044
                },
                {
                    "x": 1211,
                    "y": 1044
                },
                {
                    "x": 1211,
                    "y": 1193
                },
                {
                    "x": 223,
                    "y": 1193
                }
            ],
            "category": "paragraph",
            "html": "<p id='109' style='font-size:18px'>Chavez, Kevin, Ong, Hao Yi, and Hong, Augustus. Dis-<br>tributed deep q-learning. Technical report, Stanford Uni-<br>versity, June 2015.</p>",
            "id": 109,
            "page": 9,
            "text": "Chavez, Kevin, Ong, Hao Yi, and Hong, Augustus. Distributed deep q-learning. Technical report, Stanford University, June 2015."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 1223
                },
                {
                    "x": 1210,
                    "y": 1223
                },
                {
                    "x": 1210,
                    "y": 1420
                },
                {
                    "x": 225,
                    "y": 1420
                }
            ],
            "category": "paragraph",
            "html": "<p id='110' style='font-size:14px'>Degris, Thomas, Pilarski, Patrick M, and Sutton, Richard S.<br>Model-free reinforcement learning with continuous ac-<br>tion in practice. In American Control Conference (ACC),<br>2012, pp. 2177-2182. IEEE, 2012.</p>",
            "id": 110,
            "page": 9,
            "text": "Degris, Thomas, Pilarski, Patrick M, and Sutton, Richard S. Model-free reinforcement learning with continuous action in practice. In American Control Conference (ACC), 2012, pp. 2177-2182. IEEE, 2012."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 1452
                },
                {
                    "x": 1212,
                    "y": 1452
                },
                {
                    "x": 1212,
                    "y": 1750
                },
                {
                    "x": 225,
                    "y": 1750
                }
            ],
            "category": "paragraph",
            "html": "<p id='111' style='font-size:16px'>Grounds, Matthew and Kudenko, Daniel. Parallel rein-<br>forcement learning with linear function approximation.<br>In Proceedings of the 5th, 6th and 7th European Confer-<br>ence on Adaptive and Learning Agents and Multi-agent<br>Systems: Adaptation and Multi-agent Learning, pp. 60-<br>74. Springer-Verlag, 2008.</p>",
            "id": 111,
            "page": 9,
            "text": "Grounds, Matthew and Kudenko, Daniel. Parallel reinforcement learning with linear function approximation. In Proceedings of the 5th, 6th and 7th European Conference on Adaptive and Learning Agents and Multi-agent Systems: Adaptation and Multi-agent Learning, pp. 6074. Springer-Verlag, 2008."
        },
        {
            "bounding_box": [
                {
                    "x": 227,
                    "y": 1780
                },
                {
                    "x": 1211,
                    "y": 1780
                },
                {
                    "x": 1211,
                    "y": 2029
                },
                {
                    "x": 227,
                    "y": 2029
                }
            ],
            "category": "paragraph",
            "html": "<p id='112' style='font-size:16px'>Koutnik, Jan, Schmidhuber, Jurgen, and Gomez, Faustino.<br>Evolving deep unsupervised convolutional networks for<br>vision-based reinforcement learning. In Proceedings of<br>the 2014 conference on Genetic and evolutionary com-<br>putation, pp. 541-548. ACM, 2014.</p>",
            "id": 112,
            "page": 9,
            "text": "Koutnik, Jan, Schmidhuber, Jurgen, and Gomez, Faustino. Evolving deep unsupervised convolutional networks for vision-based reinforcement learning. In Proceedings of the 2014 conference on Genetic and evolutionary computation, pp. 541-548. ACM, 2014."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2059
                },
                {
                    "x": 1211,
                    "y": 2059
                },
                {
                    "x": 1211,
                    "y": 2205
                },
                {
                    "x": 223,
                    "y": 2205
                }
            ],
            "category": "paragraph",
            "html": "<p id='113' style='font-size:18px'>Levine, Sergey, Finn, Chelsea, Darrell, Trevor, and Abbeel,<br>Pieter. End-to-end training of deep visuomotor policies.<br>arXiv preprint arXiv:1504.00702, 2015.</p>",
            "id": 113,
            "page": 9,
            "text": "Levine, Sergey, Finn, Chelsea, Darrell, Trevor, and Abbeel, Pieter. End-to-end training of deep visuomotor policies. arXiv preprint arXiv:1504.00702, 2015."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 2237
                },
                {
                    "x": 1211,
                    "y": 2237
                },
                {
                    "x": 1211,
                    "y": 2484
                },
                {
                    "x": 225,
                    "y": 2484
                }
            ],
            "category": "paragraph",
            "html": "<p id='114' style='font-size:14px'>Li, Yuxi and Schuurmans, Dale. Mapreduce for parallel re-<br>inforcement learning. In Recent Advances in Reinforce-<br>ment Learning - 9th European Workshop, EWRL 2011,<br>Athens, Greece, September 9-11, 2011, Revised Selected<br>Papers, pp. 309-320, 2011.</p>",
            "id": 114,
            "page": 9,
            "text": "Li, Yuxi and Schuurmans, Dale. Mapreduce for parallel reinforcement learning. In Recent Advances in Reinforcement Learning - 9th European Workshop, EWRL 2011, Athens, Greece, September 9-11, 2011, Revised Selected Papers, pp. 309-320, 2011."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2516
                },
                {
                    "x": 1211,
                    "y": 2516
                },
                {
                    "x": 1211,
                    "y": 2761
                },
                {
                    "x": 224,
                    "y": 2761
                }
            ],
            "category": "paragraph",
            "html": "<p id='115' style='font-size:16px'>Lillicrap, Timothy P, Hunt, Jonathan J, Pritzel, Alexander,<br>Heess, Nicolas, Erez, Tom, Tassa, Yuval, Silver, David,<br>and Wierstra, Daan. Continuous control with deep re-<br>inforcement learning. arXiv preprint arXiv:1509.02971,<br>2015.</p>",
            "id": 115,
            "page": 9,
            "text": "Lillicrap, Timothy P, Hunt, Jonathan J, Pritzel, Alexander, Heess, Nicolas, Erez, Tom, Tassa, Yuval, Silver, David, and Wierstra, Daan. Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971, 2015."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 2792
                },
                {
                    "x": 1213,
                    "y": 2792
                },
                {
                    "x": 1213,
                    "y": 2994
                },
                {
                    "x": 225,
                    "y": 2994
                }
            ],
            "category": "paragraph",
            "html": "<p id='116' style='font-size:20px'>Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David,<br>Graves, Alex, Antonoglou, Ioannis, Wierstra, Daan, and<br>Riedmiller, Martin. Playing atari with deep reinforce-<br>ment learning. In NIPS Deep Learning Workshop. 2013.</p>",
            "id": 116,
            "page": 9,
            "text": "Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David, Graves, Alex, Antonoglou, Ioannis, Wierstra, Daan, and Riedmiller, Martin. Playing atari with deep reinforcement learning. In NIPS Deep Learning Workshop. 2013."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 282
                },
                {
                    "x": 2265,
                    "y": 282
                },
                {
                    "x": 2265,
                    "y": 732
                },
                {
                    "x": 1271,
                    "y": 732
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='117' style='font-size:16px'>Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David,<br>Rusu, Andrei A., Veness, Joel, Bellemare, Marc G.,<br>Graves, Alex, Riedmiller, Martin, Fidjeland, Andreas K.,<br>Ostrovski, Georg, Petersen, Stig, Beattie, Charles, Sadik,<br>Amir, Antonoglou, Ioannis, King, Helen, Kumaran,<br>Dharshan, Wierstra, Daan, Legg, Shane, and Hassabis,<br>Demis. Human-level control through deep reinforcement<br>learning. Nature, 518(7540):529-533, 02 2015. URL<br>http : / / dx · doi · org/ 1 0 · 1038 /naturel 4236.</p>",
            "id": 117,
            "page": 9,
            "text": "Mnih, Volodymyr, Kavukcuoglu, Koray, Silver, David, Rusu, Andrei A., Veness, Joel, Bellemare, Marc G., Graves, Alex, Riedmiller, Martin, Fidjeland, Andreas K., Ostrovski, Georg, Petersen, Stig, Beattie, Charles, Sadik, Amir, Antonoglou, Ioannis, King, Helen, Kumaran, Dharshan, Wierstra, Daan, Legg, Shane, and Hassabis, Demis. Human-level control through deep reinforcement learning. Nature, 518(7540):529-533, 02 2015. URL http : / / dx · doi · org/ 1 0 · 1038 /naturel 4236."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 760
                },
                {
                    "x": 2264,
                    "y": 760
                },
                {
                    "x": 2264,
                    "y": 1110
                },
                {
                    "x": 1273,
                    "y": 1110
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='118' style='font-size:20px'>Nair, Arun, Srinivasan, Praveen, Blackwell, Sam, Alci-<br>cek, Cagdas, Fearon, Rory, Maria, Alessandro De, Pan-<br>neershelvam, Vedavyas, Suleyman, Mustafa, Beattie,<br>Charles, Petersen, Stig, Legg, Shane, Mnih, Volodymyr,<br>Kavukcuoglu, Koray, and Silver, David. Massively par-<br>allel methods for deep reinforcement learning. In ICML<br>Deep Learning Workshop. 2015.</p>",
            "id": 118,
            "page": 9,
            "text": "Nair, Arun, Srinivasan, Praveen, Blackwell, Sam, Alcicek, Cagdas, Fearon, Rory, Maria, Alessandro De, Panneershelvam, Vedavyas, Suleyman, Mustafa, Beattie, Charles, Petersen, Stig, Legg, Shane, Mnih, Volodymyr, Kavukcuoglu, Koray, and Silver, David. Massively parallel methods for deep reinforcement learning. In ICML Deep Learning Workshop. 2015."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 1138
                },
                {
                    "x": 2262,
                    "y": 1138
                },
                {
                    "x": 2262,
                    "y": 1240
                },
                {
                    "x": 1275,
                    "y": 1240
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='119' style='font-size:18px'>Peng, Jing and Williams, Ronald J. Incremental multi-step<br>q-learning. Machine Learning, 22(1-3):283-290, 1996.</p>",
            "id": 119,
            "page": 9,
            "text": "Peng, Jing and Williams, Ronald J. Incremental multi-step q-learning. Machine Learning, 22(1-3):283-290, 1996."
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 1267
                },
                {
                    "x": 2263,
                    "y": 1267
                },
                {
                    "x": 2263,
                    "y": 1466
                },
                {
                    "x": 1276,
                    "y": 1466
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='120' style='font-size:18px'>Recht, Benjamin, Re, Christopher, Wright, Stephen, and<br>Niu, Feng. Hogwild: A lock-free approach to paralleliz-<br>ing stochastic gradient descent. In Advances in Neural<br>Information Processing Systems, pp. 693-701, 2011.</p>",
            "id": 120,
            "page": 9,
            "text": "Recht, Benjamin, Re, Christopher, Wright, Stephen, and Niu, Feng. Hogwild: A lock-free approach to parallelizing stochastic gradient descent. In Advances in Neural Information Processing Systems, pp. 693-701, 2011."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 1495
                },
                {
                    "x": 2263,
                    "y": 1495
                },
                {
                    "x": 2263,
                    "y": 1694
                },
                {
                    "x": 1274,
                    "y": 1694
                }
            ],
            "category": "paragraph",
            "html": "<p id='121' style='font-size:14px'>Riedmiller, Martin. Neural fitted 9 iteration-first experi-<br>ences with a data efficient neural reinforcement learning<br>method. In Machine Learning: ECML 2005, pp. 317-<br>328. Springer Berlin Heidelberg, 2005.</p>",
            "id": 121,
            "page": 9,
            "text": "Riedmiller, Martin. Neural fitted 9 iteration-first experiences with a data efficient neural reinforcement learning method. In Machine Learning: ECML 2005, pp. 317328. Springer Berlin Heidelberg, 2005."
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 1724
                },
                {
                    "x": 2259,
                    "y": 1724
                },
                {
                    "x": 2259,
                    "y": 1822
                },
                {
                    "x": 1276,
                    "y": 1822
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='122' style='font-size:16px'>Rummery, Gavin A and Niranjan, Mahesan. On-line 9-<br>learning using connectionist systems. 1994.</p>",
            "id": 122,
            "page": 9,
            "text": "Rummery, Gavin A and Niranjan, Mahesan. On-line 9learning using connectionist systems. 1994."
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 1853
                },
                {
                    "x": 2265,
                    "y": 1853
                },
                {
                    "x": 2265,
                    "y": 1999
                },
                {
                    "x": 1276,
                    "y": 1999
                }
            ],
            "category": "paragraph",
            "html": "<p id='123' style='font-size:16px'>Schaul, Tom, Quan, John, Antonoglou, Ioannis, and Sil-<br>ver, David. Prioritized experience replay. arXiv preprint<br>arXiv:1511.05952, 2015.</p>",
            "id": 123,
            "page": 9,
            "text": "Schaul, Tom, Quan, John, Antonoglou, Ioannis, and Silver, David. Prioritized experience replay. arXiv preprint arXiv:1511.05952, 2015."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 2031
                },
                {
                    "x": 2262,
                    "y": 2031
                },
                {
                    "x": 2262,
                    "y": 2226
                },
                {
                    "x": 1275,
                    "y": 2226
                }
            ],
            "category": "paragraph",
            "html": "<p id='124' style='font-size:18px'>Schulman, John, Levine, Sergey, Moritz, Philipp, Jordan,<br>Michael I, and Abbeel, Pieter. Trust region policy op-<br>timization. In International Conference on Machine<br>Learning (ICML), 2015a.</p>",
            "id": 124,
            "page": 9,
            "text": "Schulman, John, Levine, Sergey, Moritz, Philipp, Jordan, Michael I, and Abbeel, Pieter. Trust region policy optimization. In International Conference on Machine Learning (ICML), 2015a."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 2259
                },
                {
                    "x": 2262,
                    "y": 2259
                },
                {
                    "x": 2262,
                    "y": 2457
                },
                {
                    "x": 1275,
                    "y": 2457
                }
            ],
            "category": "paragraph",
            "html": "<p id='125' style='font-size:18px'>Schulman, John, Moritz, Philipp, Levine, Sergey, Jordan,<br>Michael, and Abbeel, Pieter. High-dimensional con-<br>tinuous control using generalized advantage estimation.<br>arXiv preprint arXiv:1506.02438, 2015b.</p>",
            "id": 125,
            "page": 9,
            "text": "Schulman, John, Moritz, Philipp, Levine, Sergey, Jordan, Michael, and Abbeel, Pieter. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015b."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 2487
                },
                {
                    "x": 2260,
                    "y": 2487
                },
                {
                    "x": 2260,
                    "y": 2583
                },
                {
                    "x": 1275,
                    "y": 2583
                }
            ],
            "category": "paragraph",
            "html": "<p id='126' style='font-size:14px'>Sutton, R. and Barto, A. Reinforcement Learning: an In-<br>troduction. MIT Press, 1998.</p>",
            "id": 126,
            "page": 9,
            "text": "Sutton, R. and Barto, A. Reinforcement Learning: an Introduction. MIT Press, 1998."
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 2615
                },
                {
                    "x": 2264,
                    "y": 2615
                },
                {
                    "x": 2264,
                    "y": 2814
                },
                {
                    "x": 1276,
                    "y": 2814
                }
            ],
            "category": "paragraph",
            "html": "<p id='127' style='font-size:14px'>Tieleman, Tijmen and Hinton, Geoffrey. Lecture 6.5-<br>rmsprop: Divide the gradient by a running average of<br>its recent magnitude. COURSERA: Neural Networks for<br>Machine Learning, 4, 2012.</p>",
            "id": 127,
            "page": 9,
            "text": "Tieleman, Tijmen and Hinton, Geoffrey. Lecture 6.5rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 4, 2012."
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 2843
                },
                {
                    "x": 2263,
                    "y": 2843
                },
                {
                    "x": 2263,
                    "y": 2993
                },
                {
                    "x": 1276,
                    "y": 2993
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='128' style='font-size:16px'>Todorov, E. MuJoCo: Modeling, Simulation and Visual-<br>ization of Multi-Joint Dynamics with Contact (ed 1.0).<br>Roboti Publishing, 2015.</p>",
            "id": 128,
            "page": 9,
            "text": "Todorov, E. MuJoCo: Modeling, Simulation and Visualization of Multi-Joint Dynamics with Contact (ed 1.0). Roboti Publishing, 2015."
        },
        {
            "bounding_box": [
                {
                    "x": 778,
                    "y": 191
                },
                {
                    "x": 1710,
                    "y": 191
                },
                {
                    "x": 1710,
                    "y": 236
                },
                {
                    "x": 778,
                    "y": 236
                }
            ],
            "category": "header",
            "html": "<header id='129' style='font-size:16px'>Asynchronous Methods for Deep Reinforcement Learning</header>",
            "id": 129,
            "page": 10,
            "text": "Asynchronous Methods for Deep Reinforcement Learning"
        },
        {
            "bounding_box": [
                {
                    "x": 226,
                    "y": 284
                },
                {
                    "x": 1211,
                    "y": 284
                },
                {
                    "x": 1211,
                    "y": 383
                },
                {
                    "x": 226,
                    "y": 383
                }
            ],
            "category": "paragraph",
            "html": "<p id='130' style='font-size:14px'>Tomassini, Marco. Parallel and distributed evolutionary al-<br>gorithms: A review. Technical report, 1999.</p>",
            "id": 130,
            "page": 10,
            "text": "Tomassini, Marco. Parallel and distributed evolutionary algorithms: A review. Technical report, 1999."
        },
        {
            "bounding_box": [
                {
                    "x": 226,
                    "y": 416
                },
                {
                    "x": 1213,
                    "y": 416
                },
                {
                    "x": 1213,
                    "y": 564
                },
                {
                    "x": 226,
                    "y": 564
                }
            ],
            "category": "paragraph",
            "html": "<p id='131' style='font-size:20px'>Tsitsiklis, John N. Asynchronous stochastic approxima-<br>tion and q-learning. Machine Learning, 16(3):185-202,<br>1994.</p>",
            "id": 131,
            "page": 10,
            "text": "Tsitsiklis, John N. Asynchronous stochastic approximation and q-learning. Machine Learning, 16(3):185-202, 1994."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 598
                },
                {
                    "x": 1214,
                    "y": 598
                },
                {
                    "x": 1214,
                    "y": 749
                },
                {
                    "x": 225,
                    "y": 749
                }
            ],
            "category": "paragraph",
            "html": "<p id='132' style='font-size:20px'>Van Hasselt, Hado, Guez, Arthur, and Silver, David. Deep<br>reinforcement learning with double q-learning. arXiv<br>preprint arXiv:1509.06461, 2015.</p>",
            "id": 132,
            "page": 10,
            "text": "Van Hasselt, Hado, Guez, Arthur, and Silver, David. Deep reinforcement learning with double q-learning. arXiv preprint arXiv:1509.06461, 2015."
        },
        {
            "bounding_box": [
                {
                    "x": 227,
                    "y": 783
                },
                {
                    "x": 1212,
                    "y": 783
                },
                {
                    "x": 1212,
                    "y": 979
                },
                {
                    "x": 227,
                    "y": 979
                }
            ],
            "category": "paragraph",
            "html": "<p id='133' style='font-size:18px'>van Seijen, H., Rupam Mahmood, A., Pilarski, P. M.,<br>Machado, M. C., and Sutton, R. S. True Online<br>Temporal-Difference Learning. ArXiv e-prints, Decem-<br>ber 2015.</p>",
            "id": 133,
            "page": 10,
            "text": "van Seijen, H., Rupam Mahmood, A., Pilarski, P. M., Machado, M. C., and Sutton, R. S. True Online Temporal-Difference Learning. ArXiv e-prints, December 2015."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 1015
                },
                {
                    "x": 1213,
                    "y": 1015
                },
                {
                    "x": 1213,
                    "y": 1163
                },
                {
                    "x": 225,
                    "y": 1163
                }
            ],
            "category": "paragraph",
            "html": "<p id='134' style='font-size:20px'>Wang, Z., de Freitas, N., and Lanctot, M. Dueling Network<br>Architectures for Deep Reinforcement Learning. ArXiv<br>e-prints, November 2015.</p>",
            "id": 134,
            "page": 10,
            "text": "Wang, Z., de Freitas, N., and Lanctot, M. Dueling Network Architectures for Deep Reinforcement Learning. ArXiv e-prints, November 2015."
        },
        {
            "bounding_box": [
                {
                    "x": 226,
                    "y": 1196
                },
                {
                    "x": 1214,
                    "y": 1196
                },
                {
                    "x": 1214,
                    "y": 1347
                },
                {
                    "x": 226,
                    "y": 1347
                }
            ],
            "category": "paragraph",
            "html": "<p id='135' style='font-size:22px'>Watkins, Christopher John Cornish Hellaby. Learning from<br>delayed rewards. PhD thesis, University of Cambridge<br>England, 1989.</p>",
            "id": 135,
            "page": 10,
            "text": "Watkins, Christopher John Cornish Hellaby. Learning from delayed rewards. PhD thesis, University of Cambridge England, 1989."
        },
        {
            "bounding_box": [
                {
                    "x": 226,
                    "y": 1379
                },
                {
                    "x": 1211,
                    "y": 1379
                },
                {
                    "x": 1211,
                    "y": 1529
                },
                {
                    "x": 226,
                    "y": 1529
                }
            ],
            "category": "paragraph",
            "html": "<p id='136' style='font-size:20px'>Williams, R.J. Simple statistical gradient-following algo-<br>rithms for connectionist reinforcement learning. Ma-<br>chine Learning, 8(3):229-256, 1992.</p>",
            "id": 136,
            "page": 10,
            "text": "Williams, R.J. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8(3):229-256, 1992."
        },
        {
            "bounding_box": [
                {
                    "x": 226,
                    "y": 1560
                },
                {
                    "x": 1213,
                    "y": 1560
                },
                {
                    "x": 1213,
                    "y": 1712
                },
                {
                    "x": 226,
                    "y": 1712
                }
            ],
            "category": "paragraph",
            "html": "<p id='137' style='font-size:20px'>Williams, Ronald J and Peng, Jing. Function optimization<br>using connectionist reinforcement learning algorithms.<br>Connection Science, 3(3):241-268, 1991.</p>",
            "id": 137,
            "page": 10,
            "text": "Williams, Ronald J and Peng, Jing. Function optimization using connectionist reinforcement learning algorithms. Connection Science, 3(3):241-268, 1991."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 1744
                },
                {
                    "x": 1214,
                    "y": 1744
                },
                {
                    "x": 1214,
                    "y": 1895
                },
                {
                    "x": 225,
                    "y": 1895
                }
            ],
            "category": "paragraph",
            "html": "<p id='138' style='font-size:20px'>Wymann, B., EspiAl', E., Guionneau, C., Dimitrakakis, C.,<br>Coulom, R., and Sumner, A. Torcs: The open racing car<br>simulator, v1.3.5, 2013.</p>",
            "id": 138,
            "page": 10,
            "text": "Wymann, B., EspiAl', E., Guionneau, C., Dimitrakakis, C., Coulom, R., and Sumner, A. Torcs: The open racing car simulator, v1.3.5, 2013."
        },
        {
            "bounding_box": [
                {
                    "x": 368,
                    "y": 456
                },
                {
                    "x": 2182,
                    "y": 456
                },
                {
                    "x": 2182,
                    "y": 614
                },
                {
                    "x": 368,
                    "y": 614
                }
            ],
            "category": "paragraph",
            "html": "<p id='139' style='font-size:22px'>Supplementary Material for \" Asynchronous Methods for Deep<br>Reinforcement Learning\"</p>",
            "id": 139,
            "page": 11,
            "text": "Supplementary Material for \" Asynchronous Methods for Deep Reinforcement Learning\""
        },
        {
            "bounding_box": [
                {
                    "x": 1130,
                    "y": 824
                },
                {
                    "x": 1420,
                    "y": 824
                },
                {
                    "x": 1420,
                    "y": 876
                },
                {
                    "x": 1130,
                    "y": 876
                }
            ],
            "category": "paragraph",
            "html": "<p id='140' style='font-size:20px'>June 17, 2016</p>",
            "id": 140,
            "page": 11,
            "text": "June 17, 2016"
        },
        {
            "bounding_box": [
                {
                    "x": 367,
                    "y": 1023
                },
                {
                    "x": 872,
                    "y": 1023
                },
                {
                    "x": 872,
                    "y": 1081
                },
                {
                    "x": 367,
                    "y": 1081
                }
            ],
            "category": "paragraph",
            "html": "<p id='141' style='font-size:20px'>7. Optimization Details</p>",
            "id": 141,
            "page": 11,
            "text": "7. Optimization Details"
        },
        {
            "bounding_box": [
                {
                    "x": 366,
                    "y": 1110
                },
                {
                    "x": 2184,
                    "y": 1110
                },
                {
                    "x": 2184,
                    "y": 1261
                },
                {
                    "x": 366,
                    "y": 1261
                }
            ],
            "category": "paragraph",
            "html": "<p id='142' style='font-size:16px'>We investigated two different optimization algorithms with our asynchronous framework - stochastic gradient<br>descent and RMSProp. Our implementations of these algorithms do not use any locking in order to maximize<br>throughput when using a large number of threads.</p>",
            "id": 142,
            "page": 11,
            "text": "We investigated two different optimization algorithms with our asynchronous framework - stochastic gradient descent and RMSProp. Our implementations of these algorithms do not use any locking in order to maximize throughput when using a large number of threads."
        },
        {
            "bounding_box": [
                {
                    "x": 366,
                    "y": 1284
                },
                {
                    "x": 2182,
                    "y": 1284
                },
                {
                    "x": 2182,
                    "y": 1585
                },
                {
                    "x": 366,
                    "y": 1585
                }
            ],
            "category": "paragraph",
            "html": "<p id='143' style='font-size:16px'>Momentum SGD: The implementation of SGD in an asynchronous setting is relatively straightforward and<br>well studied (Recht et al., 2011). Let 0 be the parameter vector that is shared across all threads and let △0i<br>be the accumulated gradients of the loss with respect to parameters 0 computed by thread number i. Each<br>thread i independently applies the standard momentum SGD update mi = ami + (1 - �)△0i followed by<br>0 ← 0 - nmi with learning rate 7, momentum a and without any locks. Note that in this setting, each thread<br>maintains its own separate gradient and momentum vector.</p>",
            "id": 143,
            "page": 11,
            "text": "Momentum SGD: The implementation of SGD in an asynchronous setting is relatively straightforward and well studied (Recht , 2011). Let 0 be the parameter vector that is shared across all threads and let △0i be the accumulated gradients of the loss with respect to parameters 0 computed by thread number i. Each thread i independently applies the standard momentum SGD update mi = ami + (1 - �)△0i followed by 0 ← 0 - nmi with learning rate 7, momentum a and without any locks. Note that in this setting, each thread maintains its own separate gradient and momentum vector."
        },
        {
            "bounding_box": [
                {
                    "x": 367,
                    "y": 1608
                },
                {
                    "x": 2182,
                    "y": 1608
                },
                {
                    "x": 2182,
                    "y": 1760
                },
                {
                    "x": 367,
                    "y": 1760
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='144' style='font-size:18px'>RMSProp: While RMSProp (Tieleman & Hinton, 2012) has been widely used in the deep learning literature,<br>it has not been extensively studied in the asynchronous optimization setting. The standard non-centered<br>RMSProp update is given by</p>",
            "id": 144,
            "page": 11,
            "text": "RMSProp: While RMSProp (Tieleman & Hinton, 2012) has been widely used in the deep learning literature, it has not been extensively studied in the asynchronous optimization setting. The standard non-centered RMSProp update is given by"
        },
        {
            "bounding_box": [
                {
                    "x": 365,
                    "y": 1999
                },
                {
                    "x": 2182,
                    "y": 1999
                },
                {
                    "x": 2182,
                    "y": 2348
                },
                {
                    "x": 365,
                    "y": 2348
                }
            ],
            "category": "paragraph",
            "html": "<p id='145' style='font-size:16px'>where all operations are performed elementwise. In order to apply RMSProp in the asynchronous optimiza-<br>tion setting one must decide whether the moving average of elementwise squared gradients g is shared or<br>per-thread. We experimented with two versions of the algorithm. In one version, which we refer to as RM-<br>SProp, each thread maintains its own g shown in Equation S2. In the other version, which we call Shared<br>RMSProp, the vector g is shared among threads and is updated asynchronously and without locking. Sharing<br>statistics among threads also reduces memory requirements by using one fewer copy of the parameter vector<br>per thread.</p>",
            "id": 145,
            "page": 11,
            "text": "where all operations are performed elementwise. In order to apply RMSProp in the asynchronous optimization setting one must decide whether the moving average of elementwise squared gradients g is shared or per-thread. We experimented with two versions of the algorithm. In one version, which we refer to as RMSProp, each thread maintains its own g shown in Equation S2. In the other version, which we call Shared RMSProp, the vector g is shared among threads and is updated asynchronously and without locking. Sharing statistics among threads also reduces memory requirements by using one fewer copy of the parameter vector per thread."
        },
        {
            "bounding_box": [
                {
                    "x": 365,
                    "y": 2370
                },
                {
                    "x": 2184,
                    "y": 2370
                },
                {
                    "x": 2184,
                    "y": 2873
                },
                {
                    "x": 365,
                    "y": 2873
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='146' style='font-size:16px'>We compared these three asynchronous optimization algorithms in terms of their sensitivity to different learn-<br>ing rates and random network initializations. Figure S5 shows a comparison of the methods for two different<br>reinforcement learning methods (Async n-step Q and Async Advantage Actor-Critic) on four different games<br>(Breakout, Beamrider, Seaquest and Space Invaders). Each curve shows the scores for 50 experiments that<br>correspond to 50 different random learning rates and initializations. The x-axis shows the rank of the model<br>after sorting in descending order by final average score and the y-axis shows the final average score achieved<br>by the corresponding model. In this representation, the algorithm that performs better would achieve higher<br>maximum rewards on the y-axis and the algorithm that is most robust would have its slope closest to horizon-<br>tal, thus maximizing the area under the curve. RMSProp with shared statistics tends to be more robust than<br>RMSProp with per-thread statistics, which is in turn more robust than Momentum SGD.</p>",
            "id": 146,
            "page": 11,
            "text": "We compared these three asynchronous optimization algorithms in terms of their sensitivity to different learning rates and random network initializations. Figure S5 shows a comparison of the methods for two different reinforcement learning methods (Async n-step Q and Async Advantage Actor-Critic) on four different games (Breakout, Beamrider, Seaquest and Space Invaders). Each curve shows the scores for 50 experiments that correspond to 50 different random learning rates and initializations. The x-axis shows the rank of the model after sorting in descending order by final average score and the y-axis shows the final average score achieved by the corresponding model. In this representation, the algorithm that performs better would achieve higher maximum rewards on the y-axis and the algorithm that is most robust would have its slope closest to horizontal, thus maximizing the area under the curve. RMSProp with shared statistics tends to be more robust than RMSProp with per-thread statistics, which is in turn more robust than Momentum SGD."
        },
        {
            "bounding_box": [
                {
                    "x": 920,
                    "y": 146
                },
                {
                    "x": 1855,
                    "y": 146
                },
                {
                    "x": 1855,
                    "y": 189
                },
                {
                    "x": 920,
                    "y": 189
                }
            ],
            "category": "header",
            "html": "<header id='147' style='font-size:18px'>Asynchronous Methods for Deep Reinforcement Learning</header>",
            "id": 147,
            "page": 12,
            "text": "Asynchronous Methods for Deep Reinforcement Learning"
        },
        {
            "bounding_box": [
                {
                    "x": 367,
                    "y": 306
                },
                {
                    "x": 856,
                    "y": 306
                },
                {
                    "x": 856,
                    "y": 367
                },
                {
                    "x": 367,
                    "y": 367
                }
            ],
            "category": "paragraph",
            "html": "<p id='148' style='font-size:22px'>8. Experimental Setup</p>",
            "id": 148,
            "page": 12,
            "text": "8. Experimental Setup"
        },
        {
            "bounding_box": [
                {
                    "x": 365,
                    "y": 389
                },
                {
                    "x": 2186,
                    "y": 389
                },
                {
                    "x": 2186,
                    "y": 1042
                },
                {
                    "x": 365,
                    "y": 1042
                }
            ],
            "category": "paragraph",
            "html": "<p id='149' style='font-size:16px'>The experiments performed on a subset of Atari games (Figures 1, 3, 4 and Table 2) as well as the TORCS<br>experiments (Figure S6) used the following setup. Each experiment used 16 actor-learner threads running<br>on a single machine and no GPUs. All methods performed updates after every 5 actions (tmax = 5 and<br>IU pdate = 5) and shared RMSProp was used for optimization. The three asynchronous value-based methods<br>used a shared target network that was updated every 40000 frames. The Atari experiments used the same<br>input preprocessing as (Mnih et al., 2015) and an action repeat of 4. The agents used the network architecture<br>from (Mnih et al., 2013). The network used a convolutional layer with 16 filters of size 8 x 8 with stride<br>4, followed by a convolutional layer with with 32 filters of size 4 x 4 with stride 2, followed by a fully<br>connected layer with 256 hidden units. All three hidden layers were followed by a rectifier nonlinearity. The<br>value-based methods had a single linear output unit for each action representing the action-value. The model<br>used by actor-critic agents had two set of outputs - a softmax output with one entry per action representing the<br>probability of selecting the action, and a single linear output representing the value function. All experiments<br>used a discount of 2 = 0.99 and an RMSProp decay factor of a = 0.99.</p>",
            "id": 149,
            "page": 12,
            "text": "The experiments performed on a subset of Atari games (Figures 1, 3, 4 and Table 2) as well as the TORCS experiments (Figure S6) used the following setup. Each experiment used 16 actor-learner threads running on a single machine and no GPUs. All methods performed updates after every 5 actions (tmax = 5 and IU pdate = 5) and shared RMSProp was used for optimization. The three asynchronous value-based methods used a shared target network that was updated every 40000 frames. The Atari experiments used the same input preprocessing as (Mnih , 2015) and an action repeat of 4. The agents used the network architecture from (Mnih , 2013). The network used a convolutional layer with 16 filters of size 8 x 8 with stride 4, followed by a convolutional layer with with 32 filters of size 4 x 4 with stride 2, followed by a fully connected layer with 256 hidden units. All three hidden layers were followed by a rectifier nonlinearity. The value-based methods had a single linear output unit for each action representing the action-value. The model used by actor-critic agents had two set of outputs - a softmax output with one entry per action representing the probability of selecting the action, and a single linear output representing the value function. All experiments used a discount of 2 = 0.99 and an RMSProp decay factor of a = 0.99."
        },
        {
            "bounding_box": [
                {
                    "x": 365,
                    "y": 1064
                },
                {
                    "x": 2183,
                    "y": 1064
                },
                {
                    "x": 2183,
                    "y": 1466
                },
                {
                    "x": 365,
                    "y": 1466
                }
            ],
            "category": "paragraph",
            "html": "<p id='150' style='font-size:16px'>The value based methods sampled the exploration rate E from a distribution taking three values €1, E2, E3 with<br>probabilities 0.4, 0.3, 0.3. The values of €1, E2, E3 were annealed from 1 to 0.1, 0.01, 0.5 respectively over<br>the first four million frames. Advantage actor-critic used entropy regularization with a weight B = 0.01 for<br>all Atari and TORCS experiments. We performed a set of 50 experiments for five Atari games and every<br>TORCS level, each using a different random initialization and initial learning rate. The initial learning rate<br>was sampled from a LogU nif orm(10-4 , 10-2) distribution and annealed to 0 over the course of training.<br>Note that in comparisons to prior work (Tables 1 and S3) we followed standard evaluation protocol and used<br>fixed hyperparameters.</p>",
            "id": 150,
            "page": 12,
            "text": "The value based methods sampled the exploration rate E from a distribution taking three values €1, E2, E3 with probabilities 0.4, 0.3, 0.3. The values of €1, E2, E3 were annealed from 1 to 0.1, 0.01, 0.5 respectively over the first four million frames. Advantage actor-critic used entropy regularization with a weight B = 0.01 for all Atari and TORCS experiments. We performed a set of 50 experiments for five Atari games and every TORCS level, each using a different random initialization and initial learning rate. The initial learning rate was sampled from a LogU nif orm(10-4 , 10-2) distribution and annealed to 0 over the course of training. Note that in comparisons to prior work (Tables 1 and S3) we followed standard evaluation protocol and used fixed hyperparameters."
        },
        {
            "bounding_box": [
                {
                    "x": 366,
                    "y": 1527
                },
                {
                    "x": 1813,
                    "y": 1527
                },
                {
                    "x": 1813,
                    "y": 1586
                },
                {
                    "x": 366,
                    "y": 1586
                }
            ],
            "category": "paragraph",
            "html": "<p id='151' style='font-size:20px'>9. Continuous Action Control Using the MuJoCo Physics Simulator</p>",
            "id": 151,
            "page": 12,
            "text": "9. Continuous Action Control Using the MuJoCo Physics Simulator"
        },
        {
            "bounding_box": [
                {
                    "x": 366,
                    "y": 1610
                },
                {
                    "x": 2183,
                    "y": 1610
                },
                {
                    "x": 2183,
                    "y": 1914
                },
                {
                    "x": 366,
                    "y": 1914
                }
            ],
            "category": "paragraph",
            "html": "<p id='152' style='font-size:16px'>To apply the asynchronous advantage actor-critic algorithm to the Mujoco tasks the necessary setup is nearly<br>identical to that used in the discrete action domains, so here we enumerate only the differences required for<br>the continuous action domains. The essential elements for many of the tasks (i.e. the physics models and<br>task objectives) are near identical to the tasks examined in (Lillicrap et al., 2015). However, the rewards and<br>thus performance are not comparable for most of the tasks due to changes made by the developers of Mujoco<br>which altered the contact model.</p>",
            "id": 152,
            "page": 12,
            "text": "To apply the asynchronous advantage actor-critic algorithm to the Mujoco tasks the necessary setup is nearly identical to that used in the discrete action domains, so here we enumerate only the differences required for the continuous action domains. The essential elements for many of the tasks (i.e. the physics models and task objectives) are near identical to the tasks examined in (Lillicrap , 2015). However, the rewards and thus performance are not comparable for most of the tasks due to changes made by the developers of Mujoco which altered the contact model."
        },
        {
            "bounding_box": [
                {
                    "x": 365,
                    "y": 1936
                },
                {
                    "x": 2184,
                    "y": 1936
                },
                {
                    "x": 2184,
                    "y": 2788
                },
                {
                    "x": 365,
                    "y": 2788
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='153' style='font-size:16px'>For all the domains we attempted to learn the task using the physical state as input. The physical state<br>consisted of the joint positions and velocities as well as the target position if the task required a target. In<br>addition, for three of the tasks (pendulum, pointmass2D, and gripper) we also examined training directly from<br>RGB pixel inputs. In the low dimensional physical state case, the inputs are mapped to a hidden state using<br>one hidden layer with 200 ReLU units. In the cases where we used pixels, the input was passed through two<br>layers of spatial convolutions without any non-linearity or pooling. In either case, the output of the encoder<br>layers were fed to a single layer of 128 LSTM cells. The most important difference in the architecture is in the<br>the output layer of the policy network. Unlike the discrete action domain where the action output is a Softmax,<br>here the two outputs of the policy network are two real number vectors which we treat as the mean vector m<br>and scalar variance �2 of a multidimensional normal distribution with a spherical covariance. To act, the input<br>is passed through the model to the output layer where we sample from the normal distribution determined by<br>m and 02. In practice, H is modeled by a linear layer and �2 by a SoftPlus operation, log(1 + exp(x)), as the<br>activation computed as a function of the output of a linear layer. In our experiments with continuous control<br>problems the networks for policy network and value network do not share any parameters, though this detail<br>is unlikely to be crucial. Finally, since the episodes were typically at most several hundred time steps long,<br>we did not use any bootstrapping in the policy or value function updates and batched each episode into a<br>single update.</p>",
            "id": 153,
            "page": 12,
            "text": "For all the domains we attempted to learn the task using the physical state as input. The physical state consisted of the joint positions and velocities as well as the target position if the task required a target. In addition, for three of the tasks (pendulum, pointmass2D, and gripper) we also examined training directly from RGB pixel inputs. In the low dimensional physical state case, the inputs are mapped to a hidden state using one hidden layer with 200 ReLU units. In the cases where we used pixels, the input was passed through two layers of spatial convolutions without any non-linearity or pooling. In either case, the output of the encoder layers were fed to a single layer of 128 LSTM cells. The most important difference in the architecture is in the the output layer of the policy network. Unlike the discrete action domain where the action output is a Softmax, here the two outputs of the policy network are two real number vectors which we treat as the mean vector m and scalar variance �2 of a multidimensional normal distribution with a spherical covariance. To act, the input is passed through the model to the output layer where we sample from the normal distribution determined by m and 02. In practice, H is modeled by a linear layer and �2 by a SoftPlus operation, log(1 + exp(x)), as the activation computed as a function of the output of a linear layer. In our experiments with continuous control problems the networks for policy network and value network do not share any parameters, though this detail is unlikely to be crucial. Finally, since the episodes were typically at most several hundred time steps long, we did not use any bootstrapping in the policy or value function updates and batched each episode into a single update."
        },
        {
            "bounding_box": [
                {
                    "x": 366,
                    "y": 2809
                },
                {
                    "x": 2182,
                    "y": 2809
                },
                {
                    "x": 2182,
                    "y": 2862
                },
                {
                    "x": 366,
                    "y": 2862
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='154' style='font-size:14px'>As in the discrete action case, we included an entropy cost which encouraged exploration. In the continuous</p>",
            "id": 154,
            "page": 12,
            "text": "As in the discrete action case, we included an entropy cost which encouraged exploration. In the continuous"
        },
        {
            "bounding_box": [
                {
                    "x": 920,
                    "y": 146
                },
                {
                    "x": 1854,
                    "y": 146
                },
                {
                    "x": 1854,
                    "y": 189
                },
                {
                    "x": 920,
                    "y": 189
                }
            ],
            "category": "header",
            "html": "<header id='155' style='font-size:22px'>Asynchronous Methods for Deep Reinforcement Learning</header>",
            "id": 155,
            "page": 13,
            "text": "Asynchronous Methods for Deep Reinforcement Learning"
        },
        {
            "bounding_box": [
                {
                    "x": 365,
                    "y": 313
                },
                {
                    "x": 2184,
                    "y": 313
                },
                {
                    "x": 2184,
                    "y": 763
                },
                {
                    "x": 365,
                    "y": 763
                }
            ],
            "category": "paragraph",
            "html": "<p id='156' style='font-size:20px'>case the we used a cost on the differential entropy of the normal distribution defined by the output of the<br>actor network, -1 (10g(2��2) + 1), we used a constant multiplier of 10-4 for this cost across all of the tasks<br>examined. The asynchronous advantage actor-critic algorithm finds solutions for all the domains. Figure S8<br>shows learning curves against wall-clock time, and demonstrates that most of the domains from states can be<br>solved within a few hours. All of the experiments, including those done from pixel based observations, were<br>run on CPU. Even in the case of solving the domains directly from pixel inputs we found that it was possible<br>to reliably discover solutions within 24 hours. Figure S7 shows scatter plots of the top scores against the<br>sampled learning rates. In most of the domains there is large range of learning rates that consistently achieve<br>good performance on the task.</p>",
            "id": 156,
            "page": 13,
            "text": "case the we used a cost on the differential entropy of the normal distribution defined by the output of the actor network, -1 (10g(2��2) + 1), we used a constant multiplier of 10-4 for this cost across all of the tasks examined. The asynchronous advantage actor-critic algorithm finds solutions for all the domains. Figure S8 shows learning curves against wall-clock time, and demonstrates that most of the domains from states can be solved within a few hours. All of the experiments, including those done from pixel based observations, were run on CPU. Even in the case of solving the domains directly from pixel inputs we found that it was possible to reliably discover solutions within 24 hours. Figure S7 shows scatter plots of the top scores against the sampled learning rates. In most of the domains there is large range of learning rates that consistently achieve good performance on the task."
        },
        {
            "bounding_box": [
                {
                    "x": 368,
                    "y": 809
                },
                {
                    "x": 1891,
                    "y": 809
                },
                {
                    "x": 1891,
                    "y": 860
                },
                {
                    "x": 368,
                    "y": 860
                }
            ],
            "category": "paragraph",
            "html": "<p id='157' style='font-size:22px'>Algorithm S2 Asynchronous n-step Q-learning - pseudocode for each actor-learner thread.</p>",
            "id": 157,
            "page": 13,
            "text": "Algorithm S2 Asynchronous n-step Q-learning - pseudocode for each actor-learner thread."
        },
        {
            "bounding_box": [
                {
                    "x": 410,
                    "y": 868
                },
                {
                    "x": 1206,
                    "y": 868
                },
                {
                    "x": 1206,
                    "y": 1164
                },
                {
                    "x": 410,
                    "y": 1164
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='158' style='font-size:16px'>// Assume global shared parameter vector 0.<br>// Assume global shared target parameter vector 0-<br>// Assume global shared counter T = 0.<br>Initialize thread step counter t ← 1<br>Initialize target network parameters 0- 0<br>Initialize thread-specific parameters 0' 0<br>Initialize network gradients d0 ← 0</p>",
            "id": 158,
            "page": 13,
            "text": "// Assume global shared parameter vector 0. // Assume global shared target parameter vector 0// Assume global shared counter T = 0. Initialize thread step counter t ← 1 Initialize target network parameters 0- 0 Initialize thread-specific parameters 0' 0 Initialize network gradients d0 ← 0"
        },
        {
            "bounding_box": [
                {
                    "x": 412,
                    "y": 1164
                },
                {
                    "x": 522,
                    "y": 1164
                },
                {
                    "x": 522,
                    "y": 1200
                },
                {
                    "x": 412,
                    "y": 1200
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='159' style='font-size:18px'>repeat</p>",
            "id": 159,
            "page": 13,
            "text": "repeat"
        },
        {
            "bounding_box": [
                {
                    "x": 465,
                    "y": 1201
                },
                {
                    "x": 1176,
                    "y": 1201
                },
                {
                    "x": 1176,
                    "y": 1366
                },
                {
                    "x": 465,
                    "y": 1366
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='160' style='font-size:16px'>Clear gradients d0 ← 0<br>Synchronize thread-specific parameters 0' = 0<br>tstart = t<br>Get state St</p>",
            "id": 160,
            "page": 13,
            "text": "Clear gradients d0 ← 0 Synchronize thread-specific parameters 0' = 0 tstart = t Get state St"
        },
        {
            "bounding_box": [
                {
                    "x": 467,
                    "y": 1371
                },
                {
                    "x": 576,
                    "y": 1371
                },
                {
                    "x": 576,
                    "y": 1406
                },
                {
                    "x": 467,
                    "y": 1406
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='161' style='font-size:20px'>repeat</p>",
            "id": 161,
            "page": 13,
            "text": "repeat"
        },
        {
            "bounding_box": [
                {
                    "x": 519,
                    "y": 1409
                },
                {
                    "x": 1564,
                    "y": 1409
                },
                {
                    "x": 1564,
                    "y": 1564
                },
                {
                    "x": 519,
                    "y": 1564
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='162' style='font-size:16px'>Take action at according to the e-greedy policy based on Q(St, a; 0')<br>Receive reward rt and new state St+1<br>t ← t + 1<br>T ← T + 1</p>",
            "id": 162,
            "page": 13,
            "text": "Take action at according to the e-greedy policy based on Q(St, a; 0') Receive reward rt and new state St+1 t ← t + 1 T ← T + 1"
        },
        {
            "bounding_box": [
                {
                    "x": 466,
                    "y": 1576
                },
                {
                    "x": 1082,
                    "y": 1576
                },
                {
                    "x": 1082,
                    "y": 1616
                },
                {
                    "x": 466,
                    "y": 1616
                }
            ],
            "category": "paragraph",
            "html": "<p id='163' style='font-size:14px'>until terminal St or t - tstart = tmax</p>",
            "id": 163,
            "page": 13,
            "text": "until terminal St or t - tstart = tmax"
        },
        {
            "bounding_box": [
                {
                    "x": 469,
                    "y": 1622
                },
                {
                    "x": 1253,
                    "y": 1622
                },
                {
                    "x": 1253,
                    "y": 1701
                },
                {
                    "x": 469,
                    "y": 1701
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='164' style='font-size:20px'>0 for terminal St<br>R = { maxa Q(St, a;0- ) for non-terminal St</p>",
            "id": 164,
            "page": 13,
            "text": "0 for terminal St R = { maxa Q(St, a;0- ) for non-terminal St"
        },
        {
            "bounding_box": [
                {
                    "x": 468,
                    "y": 1704
                },
                {
                    "x": 933,
                    "y": 1704
                },
                {
                    "x": 933,
                    "y": 1746
                },
                {
                    "x": 468,
                    "y": 1746
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='165' style='font-size:14px'>for i E {t - 1, · . · , tstart} do</p>",
            "id": 165,
            "page": 13,
            "text": "for i E {t - 1, · . · , tstart} do"
        },
        {
            "bounding_box": [
                {
                    "x": 530,
                    "y": 1798
                },
                {
                    "x": 1427,
                    "y": 1798
                },
                {
                    "x": 1427,
                    "y": 1851
                },
                {
                    "x": 530,
                    "y": 1851
                }
            ],
            "category": "paragraph",
            "html": "<p id='166' style='font-size:16px'>a(R-Q(si,ai;0'))2<br>Accumulate gradients wrt 0': d0 ← d0 +<br>au'</p>",
            "id": 166,
            "page": 13,
            "text": "a(R-Q(si,ai;0'))2 Accumulate gradients wrt 0': d0 ← d0 + au'"
        },
        {
            "bounding_box": [
                {
                    "x": 466,
                    "y": 1854
                },
                {
                    "x": 1136,
                    "y": 1854
                },
                {
                    "x": 1136,
                    "y": 1980
                },
                {
                    "x": 466,
                    "y": 1980
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='167' style='font-size:18px'>end for<br>Perform asynchronous update of 0 using d0.<br>ifT mod Itarget == 0 then</p>",
            "id": 167,
            "page": 13,
            "text": "end for Perform asynchronous update of 0 using d0. ifT mod Itarget == 0 then"
        },
        {
            "bounding_box": [
                {
                    "x": 524,
                    "y": 1975
                },
                {
                    "x": 574,
                    "y": 1975
                },
                {
                    "x": 574,
                    "y": 2011
                },
                {
                    "x": 524,
                    "y": 2011
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='168' style='font-size:16px'>0-</p>",
            "id": 168,
            "page": 13,
            "text": "0-"
        },
        {
            "bounding_box": [
                {
                    "x": 577,
                    "y": 1977
                },
                {
                    "x": 656,
                    "y": 1977
                },
                {
                    "x": 656,
                    "y": 2014
                },
                {
                    "x": 577,
                    "y": 2014
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='169' style='font-size:14px'>← 0</p>",
            "id": 169,
            "page": 13,
            "text": "← 0"
        },
        {
            "bounding_box": [
                {
                    "x": 465,
                    "y": 2018
                },
                {
                    "x": 571,
                    "y": 2018
                },
                {
                    "x": 571,
                    "y": 2055
                },
                {
                    "x": 465,
                    "y": 2055
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='170' style='font-size:16px'>end if</p>",
            "id": 170,
            "page": 13,
            "text": "end if"
        },
        {
            "bounding_box": [
                {
                    "x": 411,
                    "y": 2059
                },
                {
                    "x": 665,
                    "y": 2059
                },
                {
                    "x": 665,
                    "y": 2101
                },
                {
                    "x": 411,
                    "y": 2101
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='171' style='font-size:20px'>until T > Tmax</p>",
            "id": 171,
            "page": 13,
            "text": "until T > Tmax"
        },
        {
            "bounding_box": [
                {
                    "x": 920,
                    "y": 146
                },
                {
                    "x": 1856,
                    "y": 146
                },
                {
                    "x": 1856,
                    "y": 189
                },
                {
                    "x": 920,
                    "y": 189
                }
            ],
            "category": "header",
            "html": "<header id='172' style='font-size:20px'>Asynchronous Methods for Deep Reinforcement Learning</header>",
            "id": 172,
            "page": 14,
            "text": "Asynchronous Methods for Deep Reinforcement Learning"
        },
        {
            "bounding_box": [
                {
                    "x": 366,
                    "y": 1065
                },
                {
                    "x": 1960,
                    "y": 1065
                },
                {
                    "x": 1960,
                    "y": 1114
                },
                {
                    "x": 366,
                    "y": 1114
                }
            ],
            "category": "paragraph",
            "html": "<p id='173' style='font-size:20px'>Algorithm S3 Asynchronous advantage actor-critic - pseudocode for each actor-learner thread.</p>",
            "id": 173,
            "page": 14,
            "text": "Algorithm S3 Asynchronous advantage actor-critic - pseudocode for each actor-learner thread."
        },
        {
            "bounding_box": [
                {
                    "x": 406,
                    "y": 1123
                },
                {
                    "x": 1711,
                    "y": 1123
                },
                {
                    "x": 1711,
                    "y": 1250
                },
                {
                    "x": 406,
                    "y": 1250
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='174' style='font-size:18px'>// Assume global shared parameter vectors 0 and 0v and global shared counter T = 0<br>// Assume thread-specific parameter vectors 0' and 0'v<br>Initialize thread step counter t ← 1</p>",
            "id": 174,
            "page": 14,
            "text": "// Assume global shared parameter vectors 0 and 0v and global shared counter T = 0 // Assume thread-specific parameter vectors 0' and 0'v Initialize thread step counter t ← 1"
        },
        {
            "bounding_box": [
                {
                    "x": 412,
                    "y": 1252
                },
                {
                    "x": 523,
                    "y": 1252
                },
                {
                    "x": 523,
                    "y": 1289
                },
                {
                    "x": 412,
                    "y": 1289
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='175' style='font-size:20px'>repeat</p>",
            "id": 175,
            "page": 14,
            "text": "repeat"
        },
        {
            "bounding_box": [
                {
                    "x": 466,
                    "y": 1290
                },
                {
                    "x": 1369,
                    "y": 1290
                },
                {
                    "x": 1369,
                    "y": 1459
                },
                {
                    "x": 466,
                    "y": 1459
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='176' style='font-size:16px'>Reset gradients: d0 ← 0 and dov ← 0.<br>Synchronize thread-specific parameters 0' = 0 and 0'v = 0v<br>tstart = t<br>Get state St</p>",
            "id": 176,
            "page": 14,
            "text": "Reset gradients: d0 ← 0 and dov ← 0. Synchronize thread-specific parameters 0' = 0 and 0'v = 0v tstart = t Get state St"
        },
        {
            "bounding_box": [
                {
                    "x": 467,
                    "y": 1459
                },
                {
                    "x": 579,
                    "y": 1459
                },
                {
                    "x": 579,
                    "y": 1496
                },
                {
                    "x": 467,
                    "y": 1496
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='177' style='font-size:18px'>repeat</p>",
            "id": 177,
            "page": 14,
            "text": "repeat"
        },
        {
            "bounding_box": [
                {
                    "x": 519,
                    "y": 1498
                },
                {
                    "x": 1181,
                    "y": 1498
                },
                {
                    "x": 1181,
                    "y": 1660
                },
                {
                    "x": 519,
                    "y": 1660
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='178' style='font-size:16px'>Perform at according to policy �(at|st; 0')<br>Receive reward rt and new state St+1<br>t ← t + 1<br>T ← T + 1</p>",
            "id": 178,
            "page": 14,
            "text": "Perform at according to policy �(at|st; 0') Receive reward rt and new state St+1 t ← t + 1 T ← T + 1"
        },
        {
            "bounding_box": [
                {
                    "x": 466,
                    "y": 1664
                },
                {
                    "x": 1087,
                    "y": 1664
                },
                {
                    "x": 1087,
                    "y": 1706
                },
                {
                    "x": 466,
                    "y": 1706
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='179' style='font-size:14px'>until terminal St or t - tstart == tmax</p>",
            "id": 179,
            "page": 14,
            "text": "until terminal St or t - tstart == tmax"
        },
        {
            "bounding_box": [
                {
                    "x": 598,
                    "y": 1705
                },
                {
                    "x": 625,
                    "y": 1705
                },
                {
                    "x": 625,
                    "y": 1740
                },
                {
                    "x": 598,
                    "y": 1740
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='180' style='font-size:18px'>0</p>",
            "id": 180,
            "page": 14,
            "text": "0"
        },
        {
            "bounding_box": [
                {
                    "x": 463,
                    "y": 1696
                },
                {
                    "x": 759,
                    "y": 1696
                },
                {
                    "x": 759,
                    "y": 1798
                },
                {
                    "x": 463,
                    "y": 1798
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='181' style='font-size:22px'>R = { V(St, 0%)</p>",
            "id": 181,
            "page": 14,
            "text": "R = { V(St, 0%)"
        },
        {
            "bounding_box": [
                {
                    "x": 765,
                    "y": 1706
                },
                {
                    "x": 1512,
                    "y": 1706
                },
                {
                    "x": 1512,
                    "y": 1788
                },
                {
                    "x": 765,
                    "y": 1788
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='182' style='font-size:18px'>for terminal St<br>for non-terminal stll Bootstrap from last state</p>",
            "id": 182,
            "page": 14,
            "text": "for terminal St for non-terminal stll Bootstrap from last state"
        },
        {
            "bounding_box": [
                {
                    "x": 467,
                    "y": 1793
                },
                {
                    "x": 930,
                    "y": 1793
                },
                {
                    "x": 930,
                    "y": 1835
                },
                {
                    "x": 467,
                    "y": 1835
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='183' style='font-size:14px'>for i E {t - 1 · · · , tstart} do</p>",
            "id": 183,
            "page": 14,
            "text": "for i E {t - 1 · · · , tstart} do"
        },
        {
            "bounding_box": [
                {
                    "x": 525,
                    "y": 1835
                },
                {
                    "x": 752,
                    "y": 1835
                },
                {
                    "x": 752,
                    "y": 1875
                },
                {
                    "x": 525,
                    "y": 1875
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='184' style='font-size:16px'>R ← ri + yR</p>",
            "id": 184,
            "page": 14,
            "text": "R ← ri + yR"
        },
        {
            "bounding_box": [
                {
                    "x": 527,
                    "y": 1871
                },
                {
                    "x": 1713,
                    "y": 1871
                },
                {
                    "x": 1713,
                    "y": 1967
                },
                {
                    "x": 527,
                    "y": 1967
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='185' style='font-size:20px'>Accumulate gradients wrt 0': d0 ← d0 + ▽�' log �(ai|Si; 0')(R - V(si; 0%))<br>Accumulate gradients wrt 0'v: dov ← dov + a (R - V(Si; 0%))2 /00'</p>",
            "id": 185,
            "page": 14,
            "text": "Accumulate gradients wrt 0': d0 ← d0 + ▽�' log �(ai|Si; 0')(R - V(si; 0%)) Accumulate gradients wrt 0'v: dov ← dov + a (R - V(Si; 0%))2 /00'"
        },
        {
            "bounding_box": [
                {
                    "x": 466,
                    "y": 1969
                },
                {
                    "x": 589,
                    "y": 1969
                },
                {
                    "x": 589,
                    "y": 2006
                },
                {
                    "x": 466,
                    "y": 2006
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='186' style='font-size:18px'>end for</p>",
            "id": 186,
            "page": 14,
            "text": "end for"
        },
        {
            "bounding_box": [
                {
                    "x": 474,
                    "y": 2008
                },
                {
                    "x": 1442,
                    "y": 2008
                },
                {
                    "x": 1442,
                    "y": 2052
                },
                {
                    "x": 474,
                    "y": 2052
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='187' style='font-size:20px'>Perform asynchronous update of 0 using d0 and of 0v using dov.</p>",
            "id": 187,
            "page": 14,
            "text": "Perform asynchronous update of 0 using d0 and of 0v using dov."
        },
        {
            "bounding_box": [
                {
                    "x": 410,
                    "y": 2051
                },
                {
                    "x": 667,
                    "y": 2051
                },
                {
                    "x": 667,
                    "y": 2093
                },
                {
                    "x": 410,
                    "y": 2093
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='188' style='font-size:18px'>until T > Tmax</p>",
            "id": 188,
            "page": 14,
            "text": "until T > Tmax"
        },
        {
            "bounding_box": [
                {
                    "x": 920,
                    "y": 146
                },
                {
                    "x": 1856,
                    "y": 146
                },
                {
                    "x": 1856,
                    "y": 190
                },
                {
                    "x": 920,
                    "y": 190
                }
            ],
            "category": "header",
            "html": "<header id='189' style='font-size:20px'>Asynchronous Methods for Deep Reinforcement Learning</header>",
            "id": 189,
            "page": 15,
            "text": "Asynchronous Methods for Deep Reinforcement Learning"
        },
        {
            "bounding_box": [
                {
                    "x": 563,
                    "y": 348
                },
                {
                    "x": 2007,
                    "y": 348
                },
                {
                    "x": 2007,
                    "y": 1069
                },
                {
                    "x": 563,
                    "y": 1069
                }
            ],
            "category": "figure",
            "html": "<figure><img id='190' style='font-size:14px' alt=\"Breakout Beamrider Seaquest Space Invaders\n400 25000 6000 1800\nn-step Q, SGD ····· n-step Q, SGD ..... n-step Q, SGD ····· n-step Q, SGD\nn-step Q, RMSProp n-step Q, RMSProp n-step Q, RMSProp 1600 n-step Q, RMSProp\n350\nn-step Q, Shared RMSProp n-step Q, Shared RMSProp 5000 n-step Q, Shared RMSProp n-step Q, Shared RMSProp\n20000\n1400\n300\n4000 1200\n250\n15000\nScore\n200 Score Score 3000 Score 1000\n800\n10000\n150\n2000 600\n100\n400\n5000\n1000\n50 200\n0 0 0 0\n10 20 30 40 50 10 20 30 40 50 10 20 30 40 50 10 20 30 40 50\nModel Rank Model Rank Model Rank Model Rank\nBreakout Beamrider Seaquest Space Invaders\n900 25000 1800 4000\nA3C, SGD ····· A3C, SGD A3C, SGD A3C, SGD\n800 A3C, RMSProp A3C, RMSProp A3C, RMSProp A3C, RMSProp\n1600 3500\nA3C, Shared RMSProp A3C, Shared RMSProp A3C, Shared RMSProp A3C, Shared RMSProp\n20000\n700\n1400 3000\n600\n1200 2500\n15000\nScore 500 Score Score 1000 Score 2000\n400\n10000\n800 1500\n300\n600 1000\n200\n5000\n100 400 500\n0 0 200 0\n10 20 30 40 50 10 20 30 40 50 10 20 30 40 50 10 20 30 40 50\nModel Rank Model Rank Model Rank Model Rank\" data-coord=\"top-left:(563,348); bottom-right:(2007,1069)\" /></figure>",
            "id": 190,
            "page": 15,
            "text": "Breakout Beamrider Seaquest Space Invaders 400 25000 6000 1800 n-step Q, SGD ····· n-step Q, SGD ..... n-step Q, SGD ····· n-step Q, SGD n-step Q, RMSProp n-step Q, RMSProp n-step Q, RMSProp 1600 n-step Q, RMSProp 350 n-step Q, Shared RMSProp n-step Q, Shared RMSProp 5000 n-step Q, Shared RMSProp n-step Q, Shared RMSProp 20000 1400 300 4000 1200 250 15000 Score 200 Score Score 3000 Score 1000 800 10000 150 2000 600 100 400 5000 1000 50 200 0 0 0 0 10 20 30 40 50 10 20 30 40 50 10 20 30 40 50 10 20 30 40 50 Model Rank Model Rank Model Rank Model Rank Breakout Beamrider Seaquest Space Invaders 900 25000 1800 4000 A3C, SGD ····· A3C, SGD A3C, SGD A3C, SGD 800 A3C, RMSProp A3C, RMSProp A3C, RMSProp A3C, RMSProp 1600 3500 A3C, Shared RMSProp A3C, Shared RMSProp A3C, Shared RMSProp A3C, Shared RMSProp 20000 700 1400 3000 600 1200 2500 15000 Score 500 Score Score 1000 Score 2000 400 10000 800 1500 300 600 1000 200 5000 100 400 500 0 0 200 0 10 20 30 40 50 10 20 30 40 50 10 20 30 40 50 10 20 30 40 50 Model Rank Model Rank Model Rank Model Rank"
        },
        {
            "bounding_box": [
                {
                    "x": 367,
                    "y": 1121
                },
                {
                    "x": 2183,
                    "y": 1121
                },
                {
                    "x": 2183,
                    "y": 1449
                },
                {
                    "x": 367,
                    "y": 1449
                }
            ],
            "category": "caption",
            "html": "<caption id='191' style='font-size:18px'>Figure S5. Comparison of three different optimization methods (Momentum SGD, RMSProp, Shared RMSProp) tested<br>using two different algorithms (Async n-step Q and Async Advantage Actor-Critic) on four different Atari games (Break-<br>out, Beamrider, Seaquest and Space Invaders). Each curve shows the final scores for 50 experiments sorted in descending<br>order that covers a search over 50 random initializations and learning rates. The top row shows results using Async n-step<br>Q algorithm and bottom row shows results with Async Advantage Actor-Critic. Each individual graph shows results for<br>one of the four games and three different optimization methods. Shared RMSProp tends to be more robust to different<br>learning rates and random initializations than Momentum SGD and RMSProp without sharing.</caption>",
            "id": 191,
            "page": 15,
            "text": "Figure S5. Comparison of three different optimization methods (Momentum SGD, RMSProp, Shared RMSProp) tested using two different algorithms (Async n-step Q and Async Advantage Actor-Critic) on four different Atari games (Breakout, Beamrider, Seaquest and Space Invaders). Each curve shows the final scores for 50 experiments sorted in descending order that covers a search over 50 random initializations and learning rates. The top row shows results using Async n-step Q algorithm and bottom row shows results with Async Advantage Actor-Critic. Each individual graph shows results for one of the four games and three different optimization methods. Shared RMSProp tends to be more robust to different learning rates and random initializations than Momentum SGD and RMSProp without sharing."
        },
        {
            "bounding_box": [
                {
                    "x": 541,
                    "y": 1590
                },
                {
                    "x": 1940,
                    "y": 1590
                },
                {
                    "x": 1940,
                    "y": 2568
                },
                {
                    "x": 541,
                    "y": 2568
                }
            ],
            "category": "figure",
            "html": "<figure><img id='192' style='font-size:16px' alt=\"Slow car, no bots Slow car, bots\n5000 5000\n4000 4000\n3000 3000\nScore\nScore\n2000 2000\n1000 Async 1-step Q 1000 Async 1-step Q\nAsync SARSA Async SARSA\nAsync n-step Q Async n-stepQ\n0 0\nAsync actor-critic Async actor-critic\nHuman tester Human tester\n-1000 -1000\n0 10 20 30 40 0 10 20 30 40\nTraining time (hours) Training time (hours)\nFast car, no bots Fast car, bots\n6000 6000\n5000 5000\n4000 4000\n3000 3000\nScore\n2000 Score\n2000\nAsync 1-step Q Async 1-step Q\n1000 1000\nAsync SARSA Async SARSA\nAsync n-step Q Async n-step Q\n0 actor-critic 0 actor-critic\nAsync\nAsync\nHuman tester Human tester\n-1000 -1000\n0 10 20 30 40 0 10 20 30 40\nTraining time (hours) Training time (hours)\" data-coord=\"top-left:(541,1590); bottom-right:(1940,2568)\" /></figure>",
            "id": 192,
            "page": 15,
            "text": "Slow car, no bots Slow car, bots 5000 5000 4000 4000 3000 3000 Score Score 2000 2000 1000 Async 1-step Q 1000 Async 1-step Q Async SARSA Async SARSA Async n-step Q Async n-stepQ 0 0 Async actor-critic Async actor-critic Human tester Human tester -1000 -1000 0 10 20 30 40 0 10 20 30 40 Training time (hours) Training time (hours) Fast car, no bots Fast car, bots 6000 6000 5000 5000 4000 4000 3000 3000 Score 2000 Score 2000 Async 1-step Q Async 1-step Q 1000 1000 Async SARSA Async SARSA Async n-step Q Async n-step Q 0 actor-critic 0 actor-critic Async Async Human tester Human tester -1000 -1000 0 10 20 30 40 0 10 20 30 40 Training time (hours) Training time (hours)"
        },
        {
            "bounding_box": [
                {
                    "x": 364,
                    "y": 2617
                },
                {
                    "x": 2186,
                    "y": 2617
                },
                {
                    "x": 2186,
                    "y": 2857
                },
                {
                    "x": 364,
                    "y": 2857
                }
            ],
            "category": "caption",
            "html": "<caption id='193' style='font-size:18px'>Figure S6. Comparison of algorithms on the TORCS car racing simulator. Four different configurations of car speed and<br>opponent presence or absence are shown. In each plot, all four algorithms (one-step Q, one-step Sarsa, n-step Q and<br>Advantage Actor-Critic) are compared on score VS training time in wall clock hours. Multi-step algorithms achieve better<br>policies much faster than one-step algorithms on all four levels. The curves show averages over the 5 best runs from 50<br>experiments with learning rates sampled from LogU nif orm (10 -4 2) and all other hyperparameters fixed.<br>, 10</caption>",
            "id": 193,
            "page": 15,
            "text": "Figure S6. Comparison of algorithms on the TORCS car racing simulator. Four different configurations of car speed and opponent presence or absence are shown. In each plot, all four algorithms (one-step Q, one-step Sarsa, n-step Q and Advantage Actor-Critic) are compared on score VS training time in wall clock hours. Multi-step algorithms achieve better policies much faster than one-step algorithms on all four levels. The curves show averages over the 5 best runs from 50 experiments with learning rates sampled from LogU nif orm (10 -4 2) and all other hyperparameters fixed. , 10"
        },
        {
            "bounding_box": [
                {
                    "x": 921,
                    "y": 147
                },
                {
                    "x": 1854,
                    "y": 147
                },
                {
                    "x": 1854,
                    "y": 189
                },
                {
                    "x": 921,
                    "y": 189
                }
            ],
            "category": "header",
            "html": "<header id='194' style='font-size:20px'>Asynchronous Methods for Deep Reinforcement Learning</header>",
            "id": 194,
            "page": 16,
            "text": "Asynchronous Methods for Deep Reinforcement Learning"
        },
        {
            "bounding_box": [
                {
                    "x": 365,
                    "y": 602
                },
                {
                    "x": 2142,
                    "y": 602
                },
                {
                    "x": 2142,
                    "y": 2357
                },
                {
                    "x": 365,
                    "y": 2357
                }
            ],
            "category": "figure",
            "html": "<figure><img id='195' style='font-size:14px' alt=\"Cheetah\nCanada2d Cart\n-0.4 0.0 Cartpole\n-0.10\n-0.6 -0.12\n-0.5\n-0.14\n-0.8\n-0.16\n-1.0\nScore\nScore\nScore\n2\n-1.0 Score -0.18\n-1.2\n-0.20\n1\n-1.4\n-1.5 -0.22\n0\n-1.6\n-0.24\n-1.8 -2.0 -0.26\n10-5 10 10-3 10-2 10-1 10-5 10-4 10-3 10-2 10-1 10th 10-4 103 10~2 10-1 105 10 10⌀ 10:2 10-1\nLearning rate Learning rate Learning rate Learning rate\nGripper Pendulum Reacher Pendulum\n0.5 0.0 0.0 0.0\n-0.5 -0.5\n0.0\n-1.0 -1.0\n-0.5\n-0.5\n-1.5 -1.5\n-1.0\n-2.0 -2.0\nScore\nScore\n-1.0 Score\n-2.5 Score\n-2.5\n-1.5\n-3.0 -3.0\n-2.0\n-1.5\n-3.5 -3.5\n-2.5\n-4.0 -4.0\n-3.0 -4.5 -2.0 -4.5\n10~5 10-4 10-3 10-2 10-1 10:5 10° 4 10-3 10-2 10-1 10:5 10-4 10-3 10-2 10-2 10-5 10-4 10~3 10-2 10-1\nLearning rate Learning rate Learning rate Learning rate\nHopper Walker 2D Jaco Pendulum RGB\n0.2 -0.2 0.0 -1.0\n-0.4\n0.0 -0.2 -1.5\n-0.6\n-0.2 -0.8 -0.4 -2.0\n-1.0\nScore -0.4 Score Score -0.6 Score -2 5\n-1.2\n-0.6 -1.4 -0.8 -3.0\n-1.6\n-0.8 -1.0 -3.5\n-1.8\n-1.0 2.0 1.2 -4.0\n10-5 10 4 10-3 10-2 10 10 10 10-3 10-2 10-1 10 10-4 10-3 10-2 10-1 10-5 104 10~3 10~2 10-1\nLearning rate Learning rate Learning rate Learning rate\nPointmass2D RGB Gripper RGB\n-0.2 -2.0\n-2.5\n-0.3\n-3.0\n-0.4\n-3.5\n-0.5\nScore Score -4.0\n-0.6\n-4.5\n·\n-0.7\n-5.0\n-0.8\n-5.5\n-0.9 6.0\n10-5 10-4 10-3 10-2 10 10-5 10 10-3 10-2 10-1\nLearning rate Learning rate\" data-coord=\"top-left:(365,602); bottom-right:(2142,2357)\" /></figure>",
            "id": 195,
            "page": 16,
            "text": "Cheetah Canada2d Cart -0.4 0.0 Cartpole -0.10 -0.6 -0.12 -0.5 -0.14 -0.8 -0.16 -1.0 Score Score Score 2 -1.0 Score -0.18 -1.2 -0.20 1 -1.4 -1.5 -0.22 0 -1.6 -0.24 -1.8 -2.0 -0.26 10-5 10 10-3 10-2 10-1 10-5 10-4 10-3 10-2 10-1 10th 10-4 103 10~2 10-1 105 10 10⌀ 10:2 10-1 Learning rate Learning rate Learning rate Learning rate Gripper Pendulum Reacher Pendulum 0.5 0.0 0.0 0.0 -0.5 -0.5 0.0 -1.0 -1.0 -0.5 -0.5 -1.5 -1.5 -1.0 -2.0 -2.0 Score Score -1.0 Score -2.5 Score -2.5 -1.5 -3.0 -3.0 -2.0 -1.5 -3.5 -3.5 -2.5 -4.0 -4.0 -3.0 -4.5 -2.0 -4.5 10~5 10-4 10-3 10-2 10-1 10:5 10° 4 10-3 10-2 10-1 10:5 10-4 10-3 10-2 10-2 10-5 10-4 10~3 10-2 10-1 Learning rate Learning rate Learning rate Learning rate Hopper Walker 2D Jaco Pendulum RGB 0.2 -0.2 0.0 -1.0 -0.4 0.0 -0.2 -1.5 -0.6 -0.2 -0.8 -0.4 -2.0 -1.0 Score -0.4 Score Score -0.6 Score -2 5 -1.2 -0.6 -1.4 -0.8 -3.0 -1.6 -0.8 -1.0 -3.5 -1.8 -1.0 2.0 1.2 -4.0 10-5 10 4 10-3 10-2 10 10 10 10-3 10-2 10-1 10 10-4 10-3 10-2 10-1 10-5 104 10~3 10~2 10-1 Learning rate Learning rate Learning rate Learning rate Pointmass2D RGB Gripper RGB -0.2 -2.0 -2.5 -0.3 -3.0 -0.4 -3.5 -0.5 Score Score -4.0 -0.6 -4.5 · -0.7 -5.0 -0.8 -5.5 -0.9 6.0 10-5 10-4 10-3 10-2 10 10-5 10 10-3 10-2 10-1 Learning rate Learning rate"
        },
        {
            "bounding_box": [
                {
                    "x": 366,
                    "y": 2402
                },
                {
                    "x": 2184,
                    "y": 2402
                },
                {
                    "x": 2184,
                    "y": 2544
                },
                {
                    "x": 366,
                    "y": 2544
                }
            ],
            "category": "caption",
            "html": "<caption id='196' style='font-size:16px'>Figure S7. Performance for the Mujoco continuous action domains. Scatter plot of the best score obtained against<br>learning rates sampled from LogUnif orm(10-5 , 10-1). For nearly all of the tasks there is a wide range of learning<br>rates that lead to good performance on the task.</caption>",
            "id": 196,
            "page": 16,
            "text": "Figure S7. Performance for the Mujoco continuous action domains. Scatter plot of the best score obtained against learning rates sampled from LogUnif orm(10-5 , 10-1). For nearly all of the tasks there is a wide range of learning rates that lead to good performance on the task."
        },
        {
            "bounding_box": [
                {
                    "x": 920,
                    "y": 146
                },
                {
                    "x": 1855,
                    "y": 146
                },
                {
                    "x": 1855,
                    "y": 189
                },
                {
                    "x": 920,
                    "y": 189
                }
            ],
            "category": "header",
            "html": "<header id='197' style='font-size:22px'>Asynchronous Methods for Deep Reinforcement Learning</header>",
            "id": 197,
            "page": 17,
            "text": "Asynchronous Methods for Deep Reinforcement Learning"
        },
        {
            "bounding_box": [
                {
                    "x": 368,
                    "y": 310
                },
                {
                    "x": 2140,
                    "y": 310
                },
                {
                    "x": 2140,
                    "y": 2076
                },
                {
                    "x": 368,
                    "y": 2076
                }
            ],
            "category": "figure",
            "html": "<figure><img id='198' style='font-size:14px' alt=\"Cheetah\nCart 5\nCanada2d -0.2\nCartpole\n-0.12\n-0.6\n-0.4\n-0.14\n-0.8\n-0.6\n-1.0 -0.16\n-0.8\n-1.2\nScore\nScore\nScore\nScore -0.18\n2\n-1.0\n-1.4\n1\n-0.20\n-1.2\n-1.6\n-0.22 0\n-1.4\n-1.8\n-2.00.0 0.5 1.0 1.5 2.0 2.5 0.0 0.5 1.0 1.5 0.0 0.2 0.4 0.6 0.8 1.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5\n-1.6 -0.24 0.0\nTraning time (hours) Traning time (hours) Traning time (hours) Traning time (hours)\nGripper Pendulum Reacher\n0.0\n0.0 Pointmass 2D\n0.00\n-0.4\n-0.5 -0 .05\n-0.5\n-0.6\n-0.10\n-1.0\n-0.8\n-1.0 -0.15\n-1.5\nScore\nScore\nScore\n-1.0 Score -0.20\n-2.0\n-1.5\n-1.2\n-0.25\n-2.5\n-1.4\n-0.30\n-2.0\n-3.0\n-1.6 -0.35\n-2.5 -3.5 -1.8 -0.400.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5\n0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0 2 4 6 8 10\nTraning time (hours) Traning time (hours) Traning time (hours) Traning time (hours)\nWalker 2D -0.2 -1.5\nJaco Pendulum RGB\n0 Hopper -0.4\n-0.3\n-50\n-0.6\n-2.0\n-0.4\n-100\n-0.8\n-150 -0.5\n-2.5\n-1.0\nScore\nScore\nScore\nScore\n-200 0.6\n-1.2\n-3.0\n-250 -0.7\n-1.4\n-300 -0.8\n-3.5\n-1.6\n-350 -0.9\n-400 -1.8 -1.0 4.0\n0 1 2 3 4 5 6 7 0 1 2 3 4 5 0 2 4 6 8 10 12 14 16 0 1 2 3 7\nTraning time (hours) Traning time (hours) Traning time (hours) Traning time (hours)\nPointmass2D RGB Gripper RGB\n-0.2 0.0\n-0.5\n-0.3\n-1.0\n-0.4\n-1.5\n-0.5\nScore\nScore\n-2.0\n-0.6\n-2.5\n-0.7\n-3.0\n-0.8\n-3.5\n-0.9 4.0\n0 2 4 6 8 10 12 14 0 5 10 15 20\nTraning time (hours) Traning time (hours)\" data-coord=\"top-left:(368,310); bottom-right:(2140,2076)\" /></figure>",
            "id": 198,
            "page": 17,
            "text": "Cheetah Cart 5 Canada2d -0.2 Cartpole -0.12 -0.6 -0.4 -0.14 -0.8 -0.6 -1.0 -0.16 -0.8 -1.2 Score Score Score Score -0.18 2 -1.0 -1.4 1 -0.20 -1.2 -1.6 -0.22 0 -1.4 -1.8 -2.00.0 0.5 1.0 1.5 2.0 2.5 0.0 0.5 1.0 1.5 0.0 0.2 0.4 0.6 0.8 1.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 -1.6 -0.24 0.0 Traning time (hours) Traning time (hours) Traning time (hours) Traning time (hours) Gripper Pendulum Reacher 0.0 0.0 Pointmass 2D 0.00 -0.4 -0.5 -0 .05 -0.5 -0.6 -0.10 -1.0 -0.8 -1.0 -0.15 -1.5 Score Score Score -1.0 Score -0.20 -2.0 -1.5 -1.2 -0.25 -2.5 -1.4 -0.30 -2.0 -3.0 -1.6 -0.35 -2.5 -3.5 -1.8 -0.400.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 0.0 0.5 1.0 1.5 2.0 2.5 3.0 0 2 4 6 8 10 Traning time (hours) Traning time (hours) Traning time (hours) Traning time (hours) Walker 2D -0.2 -1.5 Jaco Pendulum RGB 0 Hopper -0.4 -0.3 -50 -0.6 -2.0 -0.4 -100 -0.8 -150 -0.5 -2.5 -1.0 Score Score Score Score -200 0.6 -1.2 -3.0 -250 -0.7 -1.4 -300 -0.8 -3.5 -1.6 -350 -0.9 -400 -1.8 -1.0 4.0 0 1 2 3 4 5 6 7 0 1 2 3 4 5 0 2 4 6 8 10 12 14 16 0 1 2 3 7 Traning time (hours) Traning time (hours) Traning time (hours) Traning time (hours) Pointmass2D RGB Gripper RGB -0.2 0.0 -0.5 -0.3 -1.0 -0.4 -1.5 -0.5 Score Score -2.0 -0.6 -2.5 -0.7 -3.0 -0.8 -3.5 -0.9 4.0 0 2 4 6 8 10 12 14 0 5 10 15 20 Traning time (hours) Traning time (hours)"
        },
        {
            "bounding_box": [
                {
                    "x": 366,
                    "y": 2117
                },
                {
                    "x": 2183,
                    "y": 2117
                },
                {
                    "x": 2183,
                    "y": 2211
                },
                {
                    "x": 366,
                    "y": 2211
                }
            ],
            "category": "caption",
            "html": "<caption id='199' style='font-size:20px'>Figure S8. Score per episode VS wall-clock time plots for the Mujoco domains. Each plot shows error bars for the top 5<br>experiments.</caption>",
            "id": 199,
            "page": 17,
            "text": "Figure S8. Score per episode VS wall-clock time plots for the Mujoco domains. Each plot shows error bars for the top 5 experiments."
        },
        {
            "bounding_box": [
                {
                    "x": 379,
                    "y": 2271
                },
                {
                    "x": 2179,
                    "y": 2271
                },
                {
                    "x": 2179,
                    "y": 2604
                },
                {
                    "x": 379,
                    "y": 2604
                }
            ],
            "category": "figure",
            "html": "<figure><img id='200' style='font-size:16px' alt=\"Beamrider Breakout Pong Q*bert Space Invaders\n12000 350 20 4500 900\n1-step SARSA, 1 threads 1-step SARSA, 1 threads 1-step SARSA, 1 threads 1-step SARSA, 1 threads\n1-step SARSA, 2 threads 1-step SARSA, 2 threads 1-step SARSA, 2 threads 1-step SARSA, 2 threads\n15 4000\n800\n10000 1-step SARSA, 4 threads 300 1-step SARSA, 4 threads 1-step SARSA, 4 threads 1-step SARSA, 4 threads\n1-step SARSA, 8 threads 1-step SARSA, 8 threads 1-step SARSA, 8 threads 1-step SARSA, 8 threads\n10 3500\n1-step SARSA, 16 threads 1-step SARSA, 16 threads 1-step SARSA, 16 threads 700 1-step SARSA, 16 threads\n250\n8000 5 3000\n600\n200 0 Score 2500 Score 500\nScore\nScore\n6000\n-5 2000\n150 Score\n400\n4000 -10 1500\n100\n1-step SARSA, 1 threads 300\n-15 1000\n1-step SARSA, 2 threads\n2000\n50 1-step SARSA, 4 threads\n-20 500 200\n1-step SARSA, 8 threads\n1-step SARSA, 16 threads\n0 0 -25 0 100\n0 10 20 30 40 0 10 20 30 40 0 10 20 30 40 0 10 20 30 40 0 10 20 30 40\nTraining epochs Training epochs Training epochs Training epochs Training epochs\" data-coord=\"top-left:(379,2271); bottom-right:(2179,2604)\" /></figure>",
            "id": 200,
            "page": 17,
            "text": "Beamrider Breakout Pong Q*bert Space Invaders 12000 350 20 4500 900 1-step SARSA, 1 threads 1-step SARSA, 1 threads 1-step SARSA, 1 threads 1-step SARSA, 1 threads 1-step SARSA, 2 threads 1-step SARSA, 2 threads 1-step SARSA, 2 threads 1-step SARSA, 2 threads 15 4000 800 10000 1-step SARSA, 4 threads 300 1-step SARSA, 4 threads 1-step SARSA, 4 threads 1-step SARSA, 4 threads 1-step SARSA, 8 threads 1-step SARSA, 8 threads 1-step SARSA, 8 threads 1-step SARSA, 8 threads 10 3500 1-step SARSA, 16 threads 1-step SARSA, 16 threads 1-step SARSA, 16 threads 700 1-step SARSA, 16 threads 250 8000 5 3000 600 200 0 Score 2500 Score 500 Score Score 6000 -5 2000 150 Score 400 4000 -10 1500 100 1-step SARSA, 1 threads 300 -15 1000 1-step SARSA, 2 threads 2000 50 1-step SARSA, 4 threads -20 500 200 1-step SARSA, 8 threads 1-step SARSA, 16 threads 0 0 -25 0 100 0 10 20 30 40 0 10 20 30 40 0 10 20 30 40 0 10 20 30 40 0 10 20 30 40 Training epochs Training epochs Training epochs Training epochs Training epochs"
        },
        {
            "bounding_box": [
                {
                    "x": 366,
                    "y": 2659
                },
                {
                    "x": 2185,
                    "y": 2659
                },
                {
                    "x": 2185,
                    "y": 2848
                },
                {
                    "x": 366,
                    "y": 2848
                }
            ],
            "category": "caption",
            "html": "<caption id='201' style='font-size:18px'>Figure S9. Data efficiency comparison of different numbers of actor-learners one-step Sarsa on five Atari games. The<br>x-axis shows the total number of training epochs where an epoch corresponds to four million frames (across all threads).<br>The y-axis shows the average score. Each curve shows the average of the three best performing agents from a search over<br>50 random learning rates. Sarsa shows increased data efficiency with increased numbers of parallel workers.</caption>",
            "id": 201,
            "page": 17,
            "text": "Figure S9. Data efficiency comparison of different numbers of actor-learners one-step Sarsa on five Atari games. The x-axis shows the total number of training epochs where an epoch corresponds to four million frames (across all threads). The y-axis shows the average score. Each curve shows the average of the three best performing agents from a search over 50 random learning rates. Sarsa shows increased data efficiency with increased numbers of parallel workers."
        },
        {
            "bounding_box": [
                {
                    "x": 920,
                    "y": 146
                },
                {
                    "x": 1854,
                    "y": 146
                },
                {
                    "x": 1854,
                    "y": 190
                },
                {
                    "x": 920,
                    "y": 190
                }
            ],
            "category": "header",
            "html": "<header id='202' style='font-size:22px'>Asynchronous Methods for Deep Reinforcement Learning</header>",
            "id": 202,
            "page": 18,
            "text": "Asynchronous Methods for Deep Reinforcement Learning"
        },
        {
            "bounding_box": [
                {
                    "x": 368,
                    "y": 477
                },
                {
                    "x": 2178,
                    "y": 477
                },
                {
                    "x": 2178,
                    "y": 835
                },
                {
                    "x": 368,
                    "y": 835
                }
            ],
            "category": "figure",
            "html": "<figure><img id='203' style='font-size:14px' alt=\"Beamrider Breakout Pong Q*bert Space Invaders\n12000 350 20 3500 800\n1-step SARSA, 1 threads 1-step SARSA, 1 threads 1-step SARSA, 1 threads 1-step SARSA, 1 threads 1-step SARSA, 1 threads\n1-step SARSA, 2 threads 1-step SARSA, 2 threads 1-step SARSA, 2 threads\n1-step SARSA, 2 threads 1-step SARSA, 2 threads 15\n1-step SARSA, 4 threads 300 1-step SARSA, 4 threads 1-step SARSA, 4 threads 3000 1-step SARSA, 4 threads 700 1-step SARSA, 4 threads\n10000\n1-step SARSA, 8 threads 1-step SARSA, 8 threads 1-step SARSA, 8 threads 1-step SARSA, 8threads 1-step SARSA, 8 threads\n10\n1-step SARSA, 16 threads 1-step SARSA, 16 threads 1-step SARSA, 16 threads 1-step SARSA, 16 threads 1-step SARSA, 16 threads\n250 2500 600\n8000 5\n200 2000 500\nScore\nScore\n6000\n-5 Score\n150 Score 0\n1500 Score\n400\n4000 -10\n100 1000 300\n-15\n2000\n50 500 200\n-20\n0 0 -25 0 100\n0 2 4 6 8 10 12 14 0 2 4 6 8 10 12 14 0 2 4 6 8 10 12 14 0 2 4 6 8 10 12 14 0 2 4 6 8 10 12 14\nTraining time (hours) Training time (hours) Training time (hours) Training time (hours) Training time (hours)\" data-coord=\"top-left:(368,477); bottom-right:(2178,835)\" /></figure>",
            "id": 203,
            "page": 18,
            "text": "Beamrider Breakout Pong Q*bert Space Invaders 12000 350 20 3500 800 1-step SARSA, 1 threads 1-step SARSA, 1 threads 1-step SARSA, 1 threads 1-step SARSA, 1 threads 1-step SARSA, 1 threads 1-step SARSA, 2 threads 1-step SARSA, 2 threads 1-step SARSA, 2 threads 1-step SARSA, 2 threads 1-step SARSA, 2 threads 15 1-step SARSA, 4 threads 300 1-step SARSA, 4 threads 1-step SARSA, 4 threads 3000 1-step SARSA, 4 threads 700 1-step SARSA, 4 threads 10000 1-step SARSA, 8 threads 1-step SARSA, 8 threads 1-step SARSA, 8 threads 1-step SARSA, 8threads 1-step SARSA, 8 threads 10 1-step SARSA, 16 threads 1-step SARSA, 16 threads 1-step SARSA, 16 threads 1-step SARSA, 16 threads 1-step SARSA, 16 threads 250 2500 600 8000 5 200 2000 500 Score Score 6000 -5 Score 150 Score 0 1500 Score 400 4000 -10 100 1000 300 -15 2000 50 500 200 -20 0 0 -25 0 100 0 2 4 6 8 10 12 14 0 2 4 6 8 10 12 14 0 2 4 6 8 10 12 14 0 2 4 6 8 10 12 14 0 2 4 6 8 10 12 14 Training time (hours) Training time (hours) Training time (hours) Training time (hours) Training time (hours)"
        },
        {
            "bounding_box": [
                {
                    "x": 365,
                    "y": 878
                },
                {
                    "x": 2186,
                    "y": 878
                },
                {
                    "x": 2186,
                    "y": 1069
                },
                {
                    "x": 365,
                    "y": 1069
                }
            ],
            "category": "caption",
            "html": "<caption id='204' style='font-size:18px'>Figure S10. Training speed comparison of different numbers of actor-learners for all one-step Sarsa on five Atari games.<br>The x-axis shows training time in hours while the y-axis shows the average score. Each curve shows the average of the<br>three best performing agents from a search over 50 random learning rates. Sarsa shows significant speedups from using<br>greater numbers of parallel actor-learners.</caption>",
            "id": 204,
            "page": 18,
            "text": "Figure S10. Training speed comparison of different numbers of actor-learners for all one-step Sarsa on five Atari games. The x-axis shows training time in hours while the y-axis shows the average score. Each curve shows the average of the three best performing agents from a search over 50 random learning rates. Sarsa shows significant speedups from using greater numbers of parallel actor-learners."
        },
        {
            "bounding_box": [
                {
                    "x": 369,
                    "y": 1460
                },
                {
                    "x": 2177,
                    "y": 1460
                },
                {
                    "x": 2177,
                    "y": 2487
                },
                {
                    "x": 369,
                    "y": 2487
                }
            ],
            "category": "figure",
            "html": "<figure><img id='205' style='font-size:16px' alt=\"1-step Q, Beamrider 1-step Q, Breakout 1-step Q, Pong 1-step Q, Q*bert 1-step Q, Space Invaders\n12000 400 30 5000 800\n350\n700\n10000 20 4000\n300\n600\n8000 250 10 3000\n·\n500\nScore\n0 Score 2000 Score\n6000 Score 200\n150 Score\n400\n4000 100 -10 1000\n300\n50\n·\n2000 -20 0\noo. 200\n0\n0 50 -30 -1000 100\n10-4 10 3 10 10-4 10:3 10 -2 10 -4 10:3 10 2 10 4 10-3 102\n2\n10 4 10 3 10 2\nLearning rate Learning rate Learning rate Learning rate Learning rate\n1-step SARSA, Beamrider 1-step SARSA, Breakout 1-step SARSA, Pong 1-step SARSA, Q*bert 1-step SARSA, Space Invaders\n14000 400 20 5000 900\n12000 350 15 800\n4000\n300 10\n10000 700\n250 5 3000\n8000 600\nScore\nScore 2000 Score 500\n6000 Score 200 0\n150 Score\n-5\n4000 400\n100 -10 1000 ·\n2000 300\n50 -15\n.. 0\n0 200\n0 -20\n-2000 -50 -25 -1000 100\n10:4 10:3 10 -2 10-4 10-3 10:2 10 4 10-3 10:2 10-4 10:3 10:2 10-4 10-3 10:2\nLearning rate Learning rate Learning rate Learning rate Learning rate\nn-step Q, Beamrider n-step Q, Breakout n-step Q, Pong n-step Q, Q*bert n-step Q, Space Invaders\n16000 400 30 5000 1000\n14000 350\n900\n20 4000\n12000 300\n800\n10000 250 10 3000\n700\n8000 Score 200 Score 2000\nScore\nScore\n0\n6000 150 Score\n600\n4000 100 -10 1000\n500\n2000 50\n-20 0\n400\n0 0 ·\n-2000 -50 -30 -1000 300\n10:2 10-4 10-3 10~2 10-4 10~3 10-2 10-4 10:3 10-4 10-3 10:2\n10-4 10-3\n102\nLearning rate Learning rate Learning rate Learning rate Learning rate\" data-coord=\"top-left:(369,1460); bottom-right:(2177,2487)\" /></figure>",
            "id": 205,
            "page": 18,
            "text": "1-step Q, Beamrider 1-step Q, Breakout 1-step Q, Pong 1-step Q, Q*bert 1-step Q, Space Invaders 12000 400 30 5000 800 350 700 10000 20 4000 300 600 8000 250 10 3000 · 500 Score 0 Score 2000 Score 6000 Score 200 150 Score 400 4000 100 -10 1000 300 50 · 2000 -20 0 oo. 200 0 0 50 -30 -1000 100 10-4 10 3 10 10-4 10:3 10 -2 10 -4 10:3 10 2 10 4 10-3 102 2 10 4 10 3 10 2 Learning rate Learning rate Learning rate Learning rate Learning rate 1-step SARSA, Beamrider 1-step SARSA, Breakout 1-step SARSA, Pong 1-step SARSA, Q*bert 1-step SARSA, Space Invaders 14000 400 20 5000 900 12000 350 15 800 4000 300 10 10000 700 250 5 3000 8000 600 Score Score 2000 Score 500 6000 Score 200 0 150 Score -5 4000 400 100 -10 1000 · 2000 300 50 -15 .. 0 0 200 0 -20 -2000 -50 -25 -1000 100 10:4 10:3 10 -2 10-4 10-3 10:2 10 4 10-3 10:2 10-4 10:3 10:2 10-4 10-3 10:2 Learning rate Learning rate Learning rate Learning rate Learning rate n-step Q, Beamrider n-step Q, Breakout n-step Q, Pong n-step Q, Q*bert n-step Q, Space Invaders 16000 400 30 5000 1000 14000 350 900 20 4000 12000 300 800 10000 250 10 3000 700 8000 Score 200 Score 2000 Score Score 0 6000 150 Score 600 4000 100 -10 1000 500 2000 50 -20 0 400 0 0 · -2000 -50 -30 -1000 300 10:2 10-4 10-3 10~2 10-4 10~3 10-2 10-4 10:3 10-4 10-3 10:2 10-4 10-3 102 Learning rate Learning rate Learning rate Learning rate Learning rate"
        },
        {
            "bounding_box": [
                {
                    "x": 366,
                    "y": 2530
                },
                {
                    "x": 2183,
                    "y": 2530
                },
                {
                    "x": 2183,
                    "y": 2675
                },
                {
                    "x": 366,
                    "y": 2675
                }
            ],
            "category": "caption",
            "html": "<caption id='206' style='font-size:20px'>Figure S11. Scatter plots of scores obtained by one-step Q, one-step Sarsa, and n-step Q on five games (Beamrider,<br>Breakout, Pong, Q*bert, Space Invaders) for 50 different learning rates and random initializations. All algorithms exhibit<br>some level of robustness to the choice of learning rate.</caption>",
            "id": 206,
            "page": 18,
            "text": "Figure S11. Scatter plots of scores obtained by one-step Q, one-step Sarsa, and n-step Q on five games (Beamrider, Breakout, Pong, Q*bert, Space Invaders) for 50 different learning rates and random initializations. All algorithms exhibit some level of robustness to the choice of learning rate."
        },
        {
            "bounding_box": [
                {
                    "x": 919,
                    "y": 145
                },
                {
                    "x": 1856,
                    "y": 145
                },
                {
                    "x": 1856,
                    "y": 191
                },
                {
                    "x": 919,
                    "y": 191
                }
            ],
            "category": "header",
            "html": "<header id='207' style='font-size:20px'>Asynchronous Methods for Deep Reinforcement Learning</header>",
            "id": 207,
            "page": 19,
            "text": "Asynchronous Methods for Deep Reinforcement Learning"
        },
        {
            "bounding_box": [
                {
                    "x": 428,
                    "y": 515
                },
                {
                    "x": 2115,
                    "y": 515
                },
                {
                    "x": 2115,
                    "y": 2470
                },
                {
                    "x": 428,
                    "y": 2470
                }
            ],
            "category": "paragraph",
            "html": "<p id='208' style='font-size:14px'>Game DQN Gorila Double Dueling Prioritized A3C FF, 1 day A3C FF A3C LSTM<br>Alien 570.2 813.5 1033.4 1486.5 900.5 182.1 518.4 945.3<br>Amidar 133.4 189.2 169.1 172.7 218.4 283.9 263.9 173.0<br>Assault 3332.3 1195.8 6060.8 3994.8 7748.5 3746.1 5474.9 14497.9<br>Asterix 124.5 3324.7 16837.0 15840.0 31907.5 6723.0 22140.5 17244.5<br>Asteroids 697.1 933.6 1193.2 2035.4 1654.0 3009.4 4474.5 5093.1<br>Atlantis 76108.0 629166.5 319688.0 445360.0 593642.0 772392.0 911091.0 875822.0<br>Bank Heist 176.3 399.4 886.0 1129.3 816.8 946.0 970.1 932.8<br>Battle Zone 17560.0 19938.0 24740.0 31320.0 29100.0 11340.0 12950.0 20760.0<br>Beam Rider 8672.4 3822.1 17417.2 14591.3 26172.7 13235.9 22707.9 24622.2<br>Berzerk 1011.1 910.6 1165.6 1433.4 817.9 862.2<br>Bowling 41.2 54.0 69.6 65.7 65.8 36.2 35.1 41.8<br>Boxing 25.8 74.2 73.5 77.3 68.6 33.7 59.8 37.3<br>Breakout 303.9 313.0 368.9 411.6 371.6 551.6 681.9 766.8<br>Centipede 3773.1 6296.9 3853.5 4881.0 3421.9 3306.5 3755.8 1997.0<br>Chopper Comman 3046.0 3191.8 3495.0 3784.0 6604.0 4669.0 7021.0 10150.0<br>Crazy Climber 50992.0 65451.0 113782.0 124566.0 131086.0 101624.0 112646.0 138518.0<br>Defender 27510.0 33996.0 21093.5 36242.5 56533.0 233021.5<br>Demon Attack 12835.2 14880.1 69803.4 56322.8 73185.8 84997.5 113308.4 115201.9<br>Double Dunk -21.6 -11.3 -0.3 -0.8 2.7 0.1 -0.1 0.1<br>Enduro 475.6 71.0 1216.6 2077.4 1884.4 -82.2 -82.5 -82.5<br>Fishing Derby -2.3 4.6 3.2 -4.1 9.2 13.6 18.8 22.6<br>Freeway 25.8 10.2 28.8 0.2 27.9 0.1 0.1 0.1<br>Frostbite 157.4 426.6 1448.1 2332.4 2930.2 180.1 190.5 197.6<br>Gopher 2731.8 4373.0 15253.0 20051.4 57783.8 8442.8 10022.8 17106.8<br>Gravitar 216.5 538.4 200.5 297.0 218.0 269.5 303.5 320.0<br>H.E.R.O. 12952.5 8963.4 14892.5 15207.9 20506.4 28765.8 32464.1 28889.5<br>Ice Hockey -3.8 -1.7 -2.5 -1.3 -1.0 -4.7 -2.8 -1.7<br>James Bond 348.5 444.0 573.0 835.5 3511.5 351.5 541.0 613.0<br>Kangaroo 2696.0 1431.0 11204.0 10334.0 10241.0 106.0 94.0 125.0<br>Krull 3864.0 6363.1 6796.1 8051.6 7406.5 8066.6 5560.0 5911.4<br>Kung-Fu Master 11875.0 20620.0 30207.0 24288.0 31244.0 3046.0 28819.0 40835.0<br>Montezuma's Revenge 50.0 84.0 42.0 22.0 13.0 53.0 67.0 41.0<br>Ms. Pacman 763.5 1263.0 1241.3 2250.6 1824.6 594.4 653.7 850.7<br>Name This Game 5439.9 9238.5 8960.3 11185.1 11836.1 5614.0 10476.1 12093.7<br>Phoenix 12366.5 20410.5 27430.1 28181.8 52894.1 74786.7<br>Pit Fall -186.7 -46.9 -14.8 -123.0 -78.5 -135.7<br>Pong 16.2 16.7 19.1 18.8 18.9 11.4 5.6 10.7<br>Private Eye 298.2 2598.6 -575.5 292.6 179.0 194.4 206.9 421.1<br>Q*Bert 4589.8 7089.8 11020.8 14175.8 11277.0 13752.3 15148.8 21307.5<br>River Raid 4065.3 5310.3 10838.4 16569.4 18184.4 10001.2 12201.8 6591.9<br>Road Runner 9264.0 43079.8 43156.0 58549.0 56990.0 31769.0 34216.0 73949.0<br>Robotank 58.5 61.8 59.1 62.0 55.4 2.3 32.8 2.6<br>Seaquest 2793.9 10145.9 14498.0 37361.6 39096.7 2300.2 2355.4 1326.1<br>Skiing -11490.4 -11928.0 -10852.8 -13700.0 -10911.1 -14863.8<br>Solaris 810.0 1768.4 2238.2 1884.8 1956.0 1936.4<br>Space Invaders 1449.7 1183.3 2628.7 5993.1 9063.0 2214.7 15730.5 23846.0<br>Star Gunner 34081.0 14919.2 58365.0 90804.0 51959.0 64393.0 138218.0 164766.0<br>Surround 1.9 4.0 -0.9 -9.6 -9.7 -8.3<br>Tennis -2.3 -0.7 -7.8 4.4 -2.0 -10.2 -6.3 -6.4<br>Time Pilot 5640.0 8267.8 6608.0 6601.0 7448.0 5825.0 12679.0 27202.0<br>Tutankham 32.4 118.5 92.2 48.0 33.6 26.1 156.3 144.2<br>Up and Down 3311.3 8747.7 19086.9 24759.2 29443.7 54525.4 74705.7 105728.7<br>Venture 54.0 523.4 21.0 200.0 244.0 19.0 23.0 25.0<br>Video Pinball 20228.1 112093.4 367823.7 110976.2 374886.9 185852.6 331628.1 470310.5<br>Wizard of Wor 246.0 10431.0 6201.0 7054.0 7451.0 5278.0 17244.0 18082.0<br>Yars Revenge 6270.6 25976.5 5965.1 7270.8 7157.5 5615.5<br>Zaxxon 831.0 6159.4 8593.0 10164.0 9501.0 2659.0 24622.0 23519.0</p>",
            "id": 208,
            "page": 19,
            "text": "Game DQN Gorila Double Dueling Prioritized A3C FF, 1 day A3C FF A3C LSTM Alien 570.2 813.5 1033.4 1486.5 900.5 182.1 518.4 945.3 Amidar 133.4 189.2 169.1 172.7 218.4 283.9 263.9 173.0 Assault 3332.3 1195.8 6060.8 3994.8 7748.5 3746.1 5474.9 14497.9 Asterix 124.5 3324.7 16837.0 15840.0 31907.5 6723.0 22140.5 17244.5 Asteroids 697.1 933.6 1193.2 2035.4 1654.0 3009.4 4474.5 5093.1 Atlantis 76108.0 629166.5 319688.0 445360.0 593642.0 772392.0 911091.0 875822.0 Bank Heist 176.3 399.4 886.0 1129.3 816.8 946.0 970.1 932.8 Battle Zone 17560.0 19938.0 24740.0 31320.0 29100.0 11340.0 12950.0 20760.0 Beam Rider 8672.4 3822.1 17417.2 14591.3 26172.7 13235.9 22707.9 24622.2 Berzerk 1011.1 910.6 1165.6 1433.4 817.9 862.2 Bowling 41.2 54.0 69.6 65.7 65.8 36.2 35.1 41.8 Boxing 25.8 74.2 73.5 77.3 68.6 33.7 59.8 37.3 Breakout 303.9 313.0 368.9 411.6 371.6 551.6 681.9 766.8 Centipede 3773.1 6296.9 3853.5 4881.0 3421.9 3306.5 3755.8 1997.0 Chopper Comman 3046.0 3191.8 3495.0 3784.0 6604.0 4669.0 7021.0 10150.0 Crazy Climber 50992.0 65451.0 113782.0 124566.0 131086.0 101624.0 112646.0 138518.0 Defender 27510.0 33996.0 21093.5 36242.5 56533.0 233021.5 Demon Attack 12835.2 14880.1 69803.4 56322.8 73185.8 84997.5 113308.4 115201.9 Double Dunk -21.6 -11.3 -0.3 -0.8 2.7 0.1 -0.1 0.1 Enduro 475.6 71.0 1216.6 2077.4 1884.4 -82.2 -82.5 -82.5 Fishing Derby -2.3 4.6 3.2 -4.1 9.2 13.6 18.8 22.6 Freeway 25.8 10.2 28.8 0.2 27.9 0.1 0.1 0.1 Frostbite 157.4 426.6 1448.1 2332.4 2930.2 180.1 190.5 197.6 Gopher 2731.8 4373.0 15253.0 20051.4 57783.8 8442.8 10022.8 17106.8 Gravitar 216.5 538.4 200.5 297.0 218.0 269.5 303.5 320.0 H.E.R.O. 12952.5 8963.4 14892.5 15207.9 20506.4 28765.8 32464.1 28889.5 Ice Hockey -3.8 -1.7 -2.5 -1.3 -1.0 -4.7 -2.8 -1.7 James Bond 348.5 444.0 573.0 835.5 3511.5 351.5 541.0 613.0 Kangaroo 2696.0 1431.0 11204.0 10334.0 10241.0 106.0 94.0 125.0 Krull 3864.0 6363.1 6796.1 8051.6 7406.5 8066.6 5560.0 5911.4 Kung-Fu Master 11875.0 20620.0 30207.0 24288.0 31244.0 3046.0 28819.0 40835.0 Montezuma's Revenge 50.0 84.0 42.0 22.0 13.0 53.0 67.0 41.0 Ms. Pacman 763.5 1263.0 1241.3 2250.6 1824.6 594.4 653.7 850.7 Name This Game 5439.9 9238.5 8960.3 11185.1 11836.1 5614.0 10476.1 12093.7 Phoenix 12366.5 20410.5 27430.1 28181.8 52894.1 74786.7 Pit Fall -186.7 -46.9 -14.8 -123.0 -78.5 -135.7 Pong 16.2 16.7 19.1 18.8 18.9 11.4 5.6 10.7 Private Eye 298.2 2598.6 -575.5 292.6 179.0 194.4 206.9 421.1 Q*Bert 4589.8 7089.8 11020.8 14175.8 11277.0 13752.3 15148.8 21307.5 River Raid 4065.3 5310.3 10838.4 16569.4 18184.4 10001.2 12201.8 6591.9 Road Runner 9264.0 43079.8 43156.0 58549.0 56990.0 31769.0 34216.0 73949.0 Robotank 58.5 61.8 59.1 62.0 55.4 2.3 32.8 2.6 Seaquest 2793.9 10145.9 14498.0 37361.6 39096.7 2300.2 2355.4 1326.1 Skiing -11490.4 -11928.0 -10852.8 -13700.0 -10911.1 -14863.8 Solaris 810.0 1768.4 2238.2 1884.8 1956.0 1936.4 Space Invaders 1449.7 1183.3 2628.7 5993.1 9063.0 2214.7 15730.5 23846.0 Star Gunner 34081.0 14919.2 58365.0 90804.0 51959.0 64393.0 138218.0 164766.0 Surround 1.9 4.0 -0.9 -9.6 -9.7 -8.3 Tennis -2.3 -0.7 -7.8 4.4 -2.0 -10.2 -6.3 -6.4 Time Pilot 5640.0 8267.8 6608.0 6601.0 7448.0 5825.0 12679.0 27202.0 Tutankham 32.4 118.5 92.2 48.0 33.6 26.1 156.3 144.2 Up and Down 3311.3 8747.7 19086.9 24759.2 29443.7 54525.4 74705.7 105728.7 Venture 54.0 523.4 21.0 200.0 244.0 19.0 23.0 25.0 Video Pinball 20228.1 112093.4 367823.7 110976.2 374886.9 185852.6 331628.1 470310.5 Wizard of Wor 246.0 10431.0 6201.0 7054.0 7451.0 5278.0 17244.0 18082.0 Yars Revenge 6270.6 25976.5 5965.1 7270.8 7157.5 5615.5 Zaxxon 831.0 6159.4 8593.0 10164.0 9501.0 2659.0 24622.0 23519.0"
        },
        {
            "bounding_box": [
                {
                    "x": 365,
                    "y": 2494
                },
                {
                    "x": 2188,
                    "y": 2494
                },
                {
                    "x": 2188,
                    "y": 2640
                },
                {
                    "x": 365,
                    "y": 2640
                }
            ],
            "category": "caption",
            "html": "<caption id='209' style='font-size:16px'>Table S3. Raw scores for the human start condition (30 minutes emulator time). DQN scores taken from (Nair et al.,<br>2015). Double DQN scores taken from (Van Hasselt et al., 2015), Dueling scores from (Wang et al., 2015) and Prioritized<br>scores taken from (Schaul et al., 2015)</caption>",
            "id": 209,
            "page": 19,
            "text": "Table S3. Raw scores for the human start condition (30 minutes emulator time). DQN scores taken from (Nair , 2015). Double DQN scores taken from (Van Hasselt , 2015), Dueling scores from (Wang , 2015) and Prioritized scores taken from (Schaul , 2015)"
        }
    ]
}