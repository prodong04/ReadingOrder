{
    "id": "32b64916-0f94-11ef-8828-426932df3dcf",
    "pdf_path": "./pdf/AI_VIT_X/2301.12661v1.pdf",
    "elements": [
        {
            "bounding_box": [
                {
                    "x": 241,
                    "y": 367
                },
                {
                    "x": 2247,
                    "y": 367
                },
                {
                    "x": 2247,
                    "y": 508
                },
                {
                    "x": 241,
                    "y": 508
                }
            ],
            "category": "paragraph",
            "html": "<p id='0' style='font-size:22px'>Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion<br>Models</p>",
            "id": 0,
            "page": 1,
            "text": "Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models"
        },
        {
            "bounding_box": [
                {
                    "x": 303,
                    "y": 649
                },
                {
                    "x": 2180,
                    "y": 649
                },
                {
                    "x": 2180,
                    "y": 757
                },
                {
                    "x": 303,
                    "y": 757
                }
            ],
            "category": "paragraph",
            "html": "<p id='1' style='font-size:16px'>Rongjie Huang * 1 Jiawei Huang * 1 Dongchao Yang *2 Yi Ren 3 Luping liu 1 Mingze Li 1 Zhenhui Ye 1<br>Jinglin Liu 1 Xiang Yin 3 Zhou Zhao 1</p>",
            "id": 1,
            "page": 1,
            "text": "Rongjie Huang * 1 Jiawei Huang * 1 Dongchao Yang *2 Yi Ren 3 Luping liu 1 Mingze Li 1 Zhenhui Ye 1 Jinglin Liu 1 Xiang Yin 3 Zhou Zhao 1"
        },
        {
            "bounding_box": [
                {
                    "x": 618,
                    "y": 842
                },
                {
                    "x": 817,
                    "y": 842
                },
                {
                    "x": 817,
                    "y": 898
                },
                {
                    "x": 618,
                    "y": 898
                }
            ],
            "category": "paragraph",
            "html": "<p id='2' style='font-size:22px'>Abstract</p>",
            "id": 2,
            "page": 1,
            "text": "Abstract"
        },
        {
            "bounding_box": [
                {
                    "x": 303,
                    "y": 920
                },
                {
                    "x": 1142,
                    "y": 920
                },
                {
                    "x": 1142,
                    "y": 2204
                },
                {
                    "x": 303,
                    "y": 2204
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='3' style='font-size:18px'>Large-scale multimodal generative modeling has<br>created milestones in text-to-image and text-to-<br>video generation. Its application to audio still<br>lags behind for two main reasons: the lack of<br>large-scale datasets with high-quality text-audio<br>pairs, and the complexity of modeling long con-<br>tinuous audio data. In this work, we propose<br>Make-An-Audio with a prompt-enhanced diffu-<br>sion model that addresses these gaps by 1) in-<br>troducing pseudo prompt enhancement with a<br>distill-then-reprogram approach, it alleviates data<br>scarcity with orders of magnitude concept compo-<br>sitions by using language-free audios; 2) lever-<br>aging spectrogram autoencoder to predict the<br>self-supervised audio representation instead of<br>waveforms. Together with robust contrastive<br>language-audio pretraining (CLAP) representa-<br>tions, Make-An-Audio achieves state-of-the-art<br>results in both objective and subjective bench-<br>mark evaluation. Moreover, we present its control-<br>lability and generalization for X-to-Audio with<br>\"No Modality Left Behind\", for the first time<br>unlocking the ability to generate high-definition,<br>high-fidelity audios given a user-defined modality<br>input. Audio samples are available at https :<br>/ / Text-to-Audio github io</p>",
            "id": 3,
            "page": 1,
            "text": "Large-scale multimodal generative modeling has created milestones in text-to-image and text-tovideo generation. Its application to audio still lags behind for two main reasons: the lack of large-scale datasets with high-quality text-audio pairs, and the complexity of modeling long continuous audio data. In this work, we propose Make-An-Audio with a prompt-enhanced diffusion model that addresses these gaps by 1) introducing pseudo prompt enhancement with a distill-then-reprogram approach, it alleviates data scarcity with orders of magnitude concept compositions by using language-free audios; 2) leveraging spectrogram autoencoder to predict the self-supervised audio representation instead of waveforms. Together with robust contrastive language-audio pretraining (CLAP) representations, Make-An-Audio achieves state-of-the-art results in both objective and subjective benchmark evaluation. Moreover, we present its controllability and generalization for X-to-Audio with \"No Modality Left Behind\", for the first time unlocking the ability to generate high-definition, high-fidelity audios given a user-defined modality input. Audio samples are available at https : / / Text-to-Audio github io"
        },
        {
            "bounding_box": [
                {
                    "x": 308,
                    "y": 2177
                },
                {
                    "x": 944,
                    "y": 2177
                },
                {
                    "x": 944,
                    "y": 2221
                },
                {
                    "x": 308,
                    "y": 2221
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='4' style='font-size:14px'>·</p>",
            "id": 4,
            "page": 1,
            "text": "·"
        },
        {
            "bounding_box": [
                {
                    "x": 227,
                    "y": 2360
                },
                {
                    "x": 554,
                    "y": 2360
                },
                {
                    "x": 554,
                    "y": 2416
                },
                {
                    "x": 227,
                    "y": 2416
                }
            ],
            "category": "paragraph",
            "html": "<p id='5' style='font-size:20px'>1. Introduction</p>",
            "id": 5,
            "page": 1,
            "text": "1. Introduction"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2446
                },
                {
                    "x": 1216,
                    "y": 2446
                },
                {
                    "x": 1216,
                    "y": 2748
                },
                {
                    "x": 223,
                    "y": 2748
                }
            ],
            "category": "paragraph",
            "html": "<p id='6' style='font-size:20px'>Deep generative models (Goodfellow et al., 2020; Kingma<br>& Dhariwal, 2018; Ho et al., 2020) have recently exhibited<br>high-quality samples in various data modalities. With large-<br>scale training data and powerful models, kinds of text-to-<br>image (Saharia et al., 2022; Ramesh et al., 2021; Nichol<br>et al., 2021) and text-to-video (Singer et al., 2022; Hong</p>",
            "id": 6,
            "page": 1,
            "text": "Deep generative models (Goodfellow , 2020; Kingma & Dhariwal, 2018; Ho , 2020) have recently exhibited high-quality samples in various data modalities. With largescale training data and powerful models, kinds of text-toimage (Saharia , 2022; Ramesh , 2021; Nichol , 2021) and text-to-video (Singer , 2022; Hong"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2778
                },
                {
                    "x": 1213,
                    "y": 2778
                },
                {
                    "x": 1213,
                    "y": 2909
                },
                {
                    "x": 224,
                    "y": 2909
                }
            ],
            "category": "paragraph",
            "html": "<p id='7' style='font-size:20px'>* Equal contribution 1Zhejiang University 2Peking University<br>3Speech & Audio Team, ByteDance AI Lab. Correspondence to:<br>Zhou Zhao <ZhaoZhou@zju.edu.cn>.</p>",
            "id": 7,
            "page": 1,
            "text": "* Equal contribution 1Zhejiang University 2Peking University 3Speech & Audio Team, ByteDance AI Lab. Correspondence to: Zhou Zhao <ZhaoZhou@zju.edu.cn>."
        },
        {
            "bounding_box": [
                {
                    "x": 1332,
                    "y": 847
                },
                {
                    "x": 2197,
                    "y": 847
                },
                {
                    "x": 2197,
                    "y": 1610
                },
                {
                    "x": 1332,
                    "y": 1610
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='8' style='font-size:16px' alt=\"Text-to-audio Rain falls softly in the distance\nAudio-to-audio (Inpainting) Image-to-audio\nVideo-to-audio\" data-coord=\"top-left:(1332,847); bottom-right:(2197,1610)\" /></figure>",
            "id": 8,
            "page": 1,
            "text": "Text-to-audio Rain falls softly in the distance Audio-to-audio (Inpainting) Image-to-audio Video-to-audio"
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1623
                },
                {
                    "x": 2267,
                    "y": 1623
                },
                {
                    "x": 2267,
                    "y": 1858
                },
                {
                    "x": 1273,
                    "y": 1858
                }
            ],
            "category": "caption",
            "html": "<br><caption id='9' style='font-size:16px'>Figure 1. No Modality Left Behind: Make-An-Audio general-<br>izes well to X-to-Audio with multiple user-defined inputs (text,<br>audio, image and video), it empowers humans to create rich and<br>diverse audio content, opening up to a various applications with<br>personalized transfer and fine-grained control.</caption>",
            "id": 9,
            "page": 1,
            "text": "Figure 1. No Modality Left Behind: Make-An-Audio generalizes well to X-to-Audio with multiple user-defined inputs (text, audio, image and video), it empowers humans to create rich and diverse audio content, opening up to a various applications with personalized transfer and fine-grained control."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1921
                },
                {
                    "x": 2266,
                    "y": 1921
                },
                {
                    "x": 2266,
                    "y": 2271
                },
                {
                    "x": 1273,
                    "y": 2271
                }
            ],
            "category": "paragraph",
            "html": "<p id='10' style='font-size:18px'>et al., 2022) models are now able to vividly depict the visual<br>scene described by a text prompt, and empower humans to<br>create rich and diverse visual content with unprecedented<br>ease. However, replicating this success for audios is limited<br>for the lack of large-scale datasets with high-quality text-<br>audio pairs, and the extreme complexity of modeling long<br>continuous signal data.</p>",
            "id": 10,
            "page": 1,
            "text": ", 2022) models are now able to vividly depict the visual scene described by a text prompt, and empower humans to create rich and diverse visual content with unprecedented ease. However, replicating this success for audios is limited for the lack of large-scale datasets with high-quality textaudio pairs, and the extreme complexity of modeling long continuous signal data."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 2295
                },
                {
                    "x": 2268,
                    "y": 2295
                },
                {
                    "x": 2268,
                    "y": 2992
                },
                {
                    "x": 1270,
                    "y": 2992
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='11' style='font-size:18px'>In this work, we propose Make-An-Audio, with a prompt-<br>enhanced diffusion model for text-to-audio (T2A) genera-<br>tion. To alleviate the issue of data scarcity, we introduce a<br>pseudo prompt enhancement approach to construct natural<br>languages that align well with audio, opening up the usage<br>of orders of magnitude unsupervised language-free data. To<br>tackle the challenge of modeling complex audio signals in<br>T2A generation, we introduce a spectrogram autoencoder to<br>predict the self-supervised representations instead of wave-<br>forms, which guarantees efficient compression and high-<br>level semantic understanding. Together with the power of<br>contrastive language-audio pretraining (CLAP) (Radford<br>et al., 2021; Elizalde et al., 2022) and high-fidelity diffusion<br>models (Ho et al., 2020; Song et al., 2020; Rombach et al.,</p>",
            "id": 11,
            "page": 1,
            "text": "In this work, we propose Make-An-Audio, with a promptenhanced diffusion model for text-to-audio (T2A) generation. To alleviate the issue of data scarcity, we introduce a pseudo prompt enhancement approach to construct natural languages that align well with audio, opening up the usage of orders of magnitude unsupervised language-free data. To tackle the challenge of modeling complex audio signals in T2A generation, we introduce a spectrogram autoencoder to predict the self-supervised representations instead of waveforms, which guarantees efficient compression and highlevel semantic understanding. Together with the power of contrastive language-audio pretraining (CLAP) (Radford , 2021; Elizalde , 2022) and high-fidelity diffusion models (Ho , 2020; Song , 2020; Rombach ,"
        },
        {
            "bounding_box": [
                {
                    "x": 58,
                    "y": 899
                },
                {
                    "x": 149,
                    "y": 899
                },
                {
                    "x": 149,
                    "y": 2331
                },
                {
                    "x": 58,
                    "y": 2331
                }
            ],
            "category": "footer",
            "html": "<br><footer id='12' style='font-size:14px'>2023<br>Jan<br>30<br>[cs.SD]<br>arXiv:2301.12661v1</footer>",
            "id": 12,
            "page": 1,
            "text": "2023 Jan 30 [cs.SD] arXiv:2301.12661v1"
        },
        {
            "bounding_box": [
                {
                    "x": 554,
                    "y": 191
                },
                {
                    "x": 1934,
                    "y": 191
                },
                {
                    "x": 1934,
                    "y": 236
                },
                {
                    "x": 554,
                    "y": 236
                }
            ],
            "category": "header",
            "html": "<header id='13' style='font-size:14px'>Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models</header>",
            "id": 13,
            "page": 2,
            "text": "Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 284
                },
                {
                    "x": 1213,
                    "y": 284
                },
                {
                    "x": 1213,
                    "y": 383
                },
                {
                    "x": 223,
                    "y": 383
                }
            ],
            "category": "paragraph",
            "html": "<p id='14' style='font-size:18px'>2022), it achieves a deep level of language understanding<br>with high-fidelity generation.</p>",
            "id": 14,
            "page": 2,
            "text": "2022), it achieves a deep level of language understanding with high-fidelity generation."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 408
                },
                {
                    "x": 1215,
                    "y": 408
                },
                {
                    "x": 1215,
                    "y": 857
                },
                {
                    "x": 223,
                    "y": 857
                }
            ],
            "category": "paragraph",
            "html": "<p id='15' style='font-size:14px'>While conceptually simple and easy to train, Make-An-<br>Audio yields surprisingly strong results. Both subjective<br>and objective evaluations demonstrate that Make-An-Audio<br>achieves new state-of-the-art in text-to-audio with natural<br>and controllable synthesis. Make-An-Audio exhibits su-<br>perior audio quality and text-audio alignment faithfulness<br>on the benchmark AudioCaption dataset and even general-<br>izes well to the unsupervised Clotho dataset in a zero-shot<br>fashion.</p>",
            "id": 15,
            "page": 2,
            "text": "While conceptually simple and easy to train, Make-AnAudio yields surprisingly strong results. Both subjective and objective evaluations demonstrate that Make-An-Audio achieves new state-of-the-art in text-to-audio with natural and controllable synthesis. Make-An-Audio exhibits superior audio quality and text-audio alignment faithfulness on the benchmark AudioCaption dataset and even generalizes well to the unsupervised Clotho dataset in a zero-shot fashion."
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 881
                },
                {
                    "x": 1216,
                    "y": 881
                },
                {
                    "x": 1216,
                    "y": 1232
                },
                {
                    "x": 222,
                    "y": 1232
                }
            ],
            "category": "paragraph",
            "html": "<p id='16' style='font-size:14px'>For the first time, we contextualize the need for audio gen-<br>eration with different input modalities. Besides natural<br>language, Make-An-Audio generalizes well to multiple user-<br>defined input modalities (audio, image, and video), which<br>empowers humans to create rich and diverse audio content<br>and opens up a host of applications for personalized transfer<br>and fine-grained control.</p>",
            "id": 16,
            "page": 2,
            "text": "For the first time, we contextualize the need for audio generation with different input modalities. Besides natural language, Make-An-Audio generalizes well to multiple userdefined input modalities (audio, image, and video), which empowers humans to create rich and diverse audio content and opens up a host of applications for personalized transfer and fine-grained control."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 1255
                },
                {
                    "x": 886,
                    "y": 1255
                },
                {
                    "x": 886,
                    "y": 1307
                },
                {
                    "x": 225,
                    "y": 1307
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='17' style='font-size:16px'>Key contributions of the paper include:</p>",
            "id": 17,
            "page": 2,
            "text": "Key contributions of the paper include:"
        },
        {
            "bounding_box": [
                {
                    "x": 269,
                    "y": 1360
                },
                {
                    "x": 1218,
                    "y": 1360
                },
                {
                    "x": 1218,
                    "y": 2421
                },
                {
                    "x": 269,
                    "y": 2421
                }
            ],
            "category": "paragraph",
            "html": "<p id='18' style='font-size:14px'>● We present Make-An-Audio - an effective method that<br>leverages latent diffusion with a spectrogram autoen-<br>coder to model the long continuous waveforms.<br>● We introduce a pseudo prompt enhancement with the<br>distill-then-reprogram approach, it includes a large<br>number of concept compositions by opening up the<br>usage of language-free audios to alleviate data scarcity.<br>· We investigate textual representation and emphasize<br>the advantages of contrastive language-audio pretrain-<br>ing for a deep understanding of natural languages with<br>computational efficiency.<br>· We evaluate Make-An-Audio and present state-of-the-<br>art quantitative results and thorough evaluation with<br>qualitative findings.<br>· We generalize the powerful model to X-to-Audio gen-<br>eration, for the first time unlocking the ability to gen-<br>erate high-definition, high-fidelity audios given a user-<br>defined modality input.</p>",
            "id": 18,
            "page": 2,
            "text": "● We present Make-An-Audio - an effective method that leverages latent diffusion with a spectrogram autoencoder to model the long continuous waveforms. ● We introduce a pseudo prompt enhancement with the distill-then-reprogram approach, it includes a large number of concept compositions by opening up the usage of language-free audios to alleviate data scarcity. · We investigate textual representation and emphasize the advantages of contrastive language-audio pretraining for a deep understanding of natural languages with computational efficiency. · We evaluate Make-An-Audio and present state-of-theart quantitative results and thorough evaluation with qualitative findings. · We generalize the powerful model to X-to-Audio generation, for the first time unlocking the ability to generate high-definition, high-fidelity audios given a userdefined modality input."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2478
                },
                {
                    "x": 601,
                    "y": 2478
                },
                {
                    "x": 601,
                    "y": 2532
                },
                {
                    "x": 223,
                    "y": 2532
                }
            ],
            "category": "paragraph",
            "html": "<p id='19' style='font-size:22px'>2. Related Works</p>",
            "id": 19,
            "page": 2,
            "text": "2. Related Works"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2564
                },
                {
                    "x": 942,
                    "y": 2564
                },
                {
                    "x": 942,
                    "y": 2616
                },
                {
                    "x": 224,
                    "y": 2616
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='20' style='font-size:20px'>2.1. Text-Guided Image/Video Synthesis</p>",
            "id": 20,
            "page": 2,
            "text": "2.1. Text-Guided Image/Video Synthesis"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2642
                },
                {
                    "x": 1216,
                    "y": 2642
                },
                {
                    "x": 1216,
                    "y": 2995
                },
                {
                    "x": 223,
                    "y": 2995
                }
            ],
            "category": "paragraph",
            "html": "<p id='21' style='font-size:14px'>With the rapid development of deep generative models, text-<br>guided synthesis has been widely studied in images and<br>videos. The pioneering work of DALL-E (Ramesh et al.,<br>2021) encodes images into discrete latent tokens using VQ-<br>VAE (Van Den Oord et al., 2017) and considers T2I genera-<br>tion as a sequence-to-sequence translation problem. More<br>recently, impressive visual results have been achieved by</p>",
            "id": 21,
            "page": 2,
            "text": "With the rapid development of deep generative models, textguided synthesis has been widely studied in images and videos. The pioneering work of DALL-E (Ramesh , 2021) encodes images into discrete latent tokens using VQVAE (Van Den Oord , 2017) and considers T2I generation as a sequence-to-sequence translation problem. More recently, impressive visual results have been achieved by"
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 283
                },
                {
                    "x": 2266,
                    "y": 283
                },
                {
                    "x": 2266,
                    "y": 933
                },
                {
                    "x": 1270,
                    "y": 933
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='22' style='font-size:14px'>leveraging large-scale diffusion models. GLIDE (Nichol<br>et al., 2021) trains a T2I upsampling model for a cascaded<br>generation. Imagen (Saharia et al., 2022) presents T2I with<br>an unprecedented degree of photorealism and a deep level of<br>language understanding. Stable diffusion (Rombach et al.,<br>2022) utilizes latent space diffusion instead of pixel space<br>to improve computational efficiency. A large body of work<br>also explores the usage of T2I models for video genera-<br>tion. CogVideo (Hong et al., 2022) is built on top of a<br>CogView2 (Ding et al., 2022) T2I model with a multi-frame-<br>rate hierarchical training strategy. Make-A-Video (Singer<br>et al., 2022) extends a diffusion-based T2I model to T2V<br>through a spatiotemporally factorized diffusion model.</p>",
            "id": 22,
            "page": 2,
            "text": "leveraging large-scale diffusion models. GLIDE (Nichol , 2021) trains a T2I upsampling model for a cascaded generation. Imagen (Saharia , 2022) presents T2I with an unprecedented degree of photorealism and a deep level of language understanding. Stable diffusion (Rombach , 2022) utilizes latent space diffusion instead of pixel space to improve computational efficiency. A large body of work also explores the usage of T2I models for video generation. CogVideo (Hong , 2022) is built on top of a CogView2 (Ding , 2022) T2I model with a multi-framerate hierarchical training strategy. Make-A-Video (Singer , 2022) extends a diffusion-based T2I model to T2V through a spatiotemporally factorized diffusion model."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 955
                },
                {
                    "x": 2266,
                    "y": 955
                },
                {
                    "x": 2266,
                    "y": 1106
                },
                {
                    "x": 1273,
                    "y": 1106
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='23' style='font-size:14px'>Moving beyond visual generation, our approach aims to<br>generate high-fidelity audio from arbitrary natural language,<br>which has been relatively overlooked.</p>",
            "id": 23,
            "page": 2,
            "text": "Moving beyond visual generation, our approach aims to generate high-fidelity audio from arbitrary natural language, which has been relatively overlooked."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1159
                },
                {
                    "x": 1874,
                    "y": 1159
                },
                {
                    "x": 1874,
                    "y": 1211
                },
                {
                    "x": 1272,
                    "y": 1211
                }
            ],
            "category": "paragraph",
            "html": "<p id='24' style='font-size:18px'>2.2. Text-Guided Audio Synthesis</p>",
            "id": 24,
            "page": 2,
            "text": "2.2. Text-Guided Audio Synthesis"
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 1236
                },
                {
                    "x": 2267,
                    "y": 1236
                },
                {
                    "x": 2267,
                    "y": 1839
                },
                {
                    "x": 1270,
                    "y": 1839
                }
            ],
            "category": "paragraph",
            "html": "<p id='25' style='font-size:14px'>While there is remarkable progress in text-guided visual gen-<br>eration, the progress of text-to-audio (T2A) generation lags<br>behind mainly due to two main reasons: the lack of large-<br>scale datasets with high-quality text-audio pairs, and the<br>complexity of modeling long continuous waveforms data.<br>DiffSound (Yang et al., 2022) is the first to explore text-<br>to-audio generation with a discrete diffusion process that<br>operates on audio codes obtained from a VQ-VAE, lever-<br>aging masked text generation with CLIP representations.<br>AudioLM (Borsos et al., 2022) introduces the discretized ac-<br>tivations of a masked language model pre-trained on audio<br>and generates syntactically plausible speech or music.</p>",
            "id": 25,
            "page": 2,
            "text": "While there is remarkable progress in text-guided visual generation, the progress of text-to-audio (T2A) generation lags behind mainly due to two main reasons: the lack of largescale datasets with high-quality text-audio pairs, and the complexity of modeling long continuous waveforms data. DiffSound (Yang , 2022) is the first to explore textto-audio generation with a discrete diffusion process that operates on audio codes obtained from a VQ-VAE, leveraging masked text generation with CLIP representations. AudioLM (Borsos , 2022) introduces the discretized activations of a masked language model pre-trained on audio and generates syntactically plausible speech or music."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1860
                },
                {
                    "x": 2267,
                    "y": 1860
                },
                {
                    "x": 2267,
                    "y": 2313
                },
                {
                    "x": 1272,
                    "y": 2313
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='26' style='font-size:16px'>Very recently, the concurrent work AudioGen (Kreuk et al.,<br>2022) propose to generate audio samples autoregressively<br>conditioned on text inputs, while our proposed method dif-<br>ferentiates from it in the following: 1) we introduce pseudo<br>prompt enhancement and leverage the power of contrastive<br>language-audio pre-training and diffusion models for high-<br>fidelity generation. 2) We predict the continuous spectro-<br>gram representations, significantly improving computational<br>efficiency and reducing training costs.</p>",
            "id": 26,
            "page": 2,
            "text": "Very recently, the concurrent work AudioGen (Kreuk , 2022) propose to generate audio samples autoregressively conditioned on text inputs, while our proposed method differentiates from it in the following: 1) we introduce pseudo prompt enhancement and leverage the power of contrastive language-audio pre-training and diffusion models for highfidelity generation. 2) We predict the continuous spectrogram representations, significantly improving computational efficiency and reducing training costs."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2363
                },
                {
                    "x": 1917,
                    "y": 2363
                },
                {
                    "x": 1917,
                    "y": 2415
                },
                {
                    "x": 1273,
                    "y": 2415
                }
            ],
            "category": "paragraph",
            "html": "<p id='27' style='font-size:18px'>2.3. Audio Representation Learning</p>",
            "id": 27,
            "page": 2,
            "text": "2.3. Audio Representation Learning"
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2442
                },
                {
                    "x": 2266,
                    "y": 2442
                },
                {
                    "x": 2266,
                    "y": 2942
                },
                {
                    "x": 1273,
                    "y": 2942
                }
            ],
            "category": "paragraph",
            "html": "<p id='28' style='font-size:14px'>Different from modeling fine-grain details of the signal, the<br>usage of high-level self-supervised learning (SSL) (Baevski<br>et al., 2020; Hsu et al., 2021; He et al., 2022) has been shown<br>to effectively reduce the sampling space of generative algo-<br>rithms. Inspired by vector quantization (VQ) techniques,<br>SoundStream (Zeghidour et al., 2021) presents a hierar-<br>chical architecture for high-level representations that carry<br>semantic information. Data2vec (Baevski et al., 2022) uses<br>a fast convolutional decoder and explores the contextualized<br>target representations in a self-supervised manner.</p>",
            "id": 28,
            "page": 2,
            "text": "Different from modeling fine-grain details of the signal, the usage of high-level self-supervised learning (SSL) (Baevski , 2020; Hsu , 2021; He , 2022) has been shown to effectively reduce the sampling space of generative algorithms. Inspired by vector quantization (VQ) techniques, SoundStream (Zeghidour , 2021) presents a hierarchical architecture for high-level representations that carry semantic information. Data2vec (Baevski , 2022) uses a fast convolutional decoder and explores the contextualized target representations in a self-supervised manner."
        },
        {
            "bounding_box": [
                {
                    "x": 551,
                    "y": 189
                },
                {
                    "x": 1934,
                    "y": 189
                },
                {
                    "x": 1934,
                    "y": 237
                },
                {
                    "x": 551,
                    "y": 237
                }
            ],
            "category": "header",
            "html": "<header id='29' style='font-size:18px'>Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models</header>",
            "id": 29,
            "page": 3,
            "text": "Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models"
        },
        {
            "bounding_box": [
                {
                    "x": 324,
                    "y": 268
                },
                {
                    "x": 2169,
                    "y": 268
                },
                {
                    "x": 2169,
                    "y": 606
                },
                {
                    "x": 324,
                    "y": 606
                }
            ],
            "category": "figure",
            "html": "<figure><img id='30' style='font-size:14px' alt=\"Zt U-Net 0 zt-1 zo G\nDiffusion\nDenoising 0\nAudio q(xt|xt-1) Audio\n→ po(xt|xt-1)\nEncoder Decoder\nx\nx\nxN\n'Rain falls softly Text\nTransformer\nVocoder\nin the distance' Encoder Cross-attention\nGenerated Audio\" data-coord=\"top-left:(324,268); bottom-right:(2169,606)\" /></figure>",
            "id": 30,
            "page": 3,
            "text": "Zt U-Net 0 zt-1 zo G Diffusion Denoising 0 Audio q(xt|xt-1) Audio → po(xt|xt-1) Encoder Decoder x x xN \"Rain falls softly Text Transformer Vocoder in the distance\" Encoder Cross-attention Generated Audio"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 635
                },
                {
                    "x": 2260,
                    "y": 635
                },
                {
                    "x": 2260,
                    "y": 684
                },
                {
                    "x": 223,
                    "y": 684
                }
            ],
            "category": "caption",
            "html": "<caption id='31' style='font-size:14px'>Figure 2. A high-level overview of Make-An-Audio. Note that some modules (printed with a lock) are frozen for training the T2A model.</caption>",
            "id": 31,
            "page": 3,
            "text": "Figure 2. A high-level overview of Make-An-Audio. Note that some modules (printed with a lock) are frozen for training the T2A model."
        },
        {
            "bounding_box": [
                {
                    "x": 359,
                    "y": 702
                },
                {
                    "x": 2136,
                    "y": 702
                },
                {
                    "x": 2136,
                    "y": 1006
                },
                {
                    "x": 359,
                    "y": 1006
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='32' style='font-size:14px' alt=\"Dynamic Reprogramming\nExpert Distillation Data Sample Birds\nBase\nFootsteps\nAudio Captioning\nCLAPS 'Rain falls softly in the\n'Rain falls softly\nLanguage-Free distance before hearing\nAudio-Text Retrieval in the distance'\nAudio sounds of birds and footsteps'\nCandidates\" data-coord=\"top-left:(359,702); bottom-right:(2136,1006)\" /></figure>",
            "id": 32,
            "page": 3,
            "text": "Dynamic Reprogramming Expert Distillation Data Sample Birds Base Footsteps Audio Captioning CLAPS \"Rain falls softly in the \"Rain falls softly Language-Free distance before hearing Audio-Text Retrieval in the distance\" Audio sounds of birds and footsteps\" Candidates"
        },
        {
            "bounding_box": [
                {
                    "x": 221,
                    "y": 1017
                },
                {
                    "x": 2265,
                    "y": 1017
                },
                {
                    "x": 2265,
                    "y": 1157
                },
                {
                    "x": 221,
                    "y": 1157
                }
            ],
            "category": "caption",
            "html": "<br><caption id='33' style='font-size:14px'>Figure 3. The process of pseudo prompt enhancement. Our semi-parametric diffusion model consists of a fixed expert distillation and a<br>dynamic reprogramming stage. The database D contains audio examples with a sampling strategy 5 to create unseen object compositions.<br>We use CLAPS to denote the CLAP selection.</caption>",
            "id": 33,
            "page": 3,
            "text": "Figure 3. The process of pseudo prompt enhancement. Our semi-parametric diffusion model consists of a fixed expert distillation and a dynamic reprogramming stage. The database D contains audio examples with a sampling strategy 5 to create unseen object compositions. We use CLAPS to denote the CLAP selection."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1205
                },
                {
                    "x": 1218,
                    "y": 1205
                },
                {
                    "x": 1218,
                    "y": 1909
                },
                {
                    "x": 223,
                    "y": 1909
                }
            ],
            "category": "paragraph",
            "html": "<p id='34' style='font-size:18px'>Recently, spectrograms (akin to 1-channel 2D images) au-<br>toencoder (Gong et al., 2022; He et al., 2022) with recon-<br>struction objective as self-supervision have demonstrated<br>the effectiveness of heterogeneous image-to-audio transfer,<br>advancing the field of speech and audio processing on a vari-<br>ety of downstream tasks. Among these approaches, Xu et al.<br>(2022) study the Masked Autoencoders (MAE) (He et al.,<br>2022) to self-supervised representation learning from audio<br>spectrograms. Gong et al. (2022) adopt audio spectrogram<br>transformer with joint discriminative and generative masked<br>spectrogram modeling. Inspired by these, we inherit the<br>recent success of spectrogram SSL in the frequency domain,<br>which guarantees efficient compression and high-level se-<br>mantic understanding.</p>",
            "id": 34,
            "page": 3,
            "text": "Recently, spectrograms (akin to 1-channel 2D images) autoencoder (Gong , 2022; He , 2022) with reconstruction objective as self-supervision have demonstrated the effectiveness of heterogeneous image-to-audio transfer, advancing the field of speech and audio processing on a variety of downstream tasks. Among these approaches, Xu  (2022) study the Masked Autoencoders (MAE) (He , 2022) to self-supervised representation learning from audio spectrograms. Gong  (2022) adopt audio spectrogram transformer with joint discriminative and generative masked spectrogram modeling. Inspired by these, we inherit the recent success of spectrogram SSL in the frequency domain, which guarantees efficient compression and high-level semantic understanding."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1967
                },
                {
                    "x": 632,
                    "y": 1967
                },
                {
                    "x": 632,
                    "y": 2024
                },
                {
                    "x": 224,
                    "y": 2024
                }
            ],
            "category": "paragraph",
            "html": "<p id='35' style='font-size:22px'>3. Make-An-Audio</p>",
            "id": 35,
            "page": 3,
            "text": "3. Make-An-Audio"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2055
                },
                {
                    "x": 1216,
                    "y": 2055
                },
                {
                    "x": 1216,
                    "y": 2406
                },
                {
                    "x": 223,
                    "y": 2406
                }
            ],
            "category": "paragraph",
            "html": "<p id='36' style='font-size:18px'>In this section, we first overview the Make-An-Audio frame-<br>work and illustrate pseudo prompt enhancement to better<br>align text and audio semantics, following which we intro-<br>duce textual and audio representations for multimodal learn-<br>ing. Together with the power of diffusion models with<br>classifier-free guidance, Make-An-Audio explicits high-<br>fidelity synthesis with superior generalization.</p>",
            "id": 36,
            "page": 3,
            "text": "In this section, we first overview the Make-An-Audio framework and illustrate pseudo prompt enhancement to better align text and audio semantics, following which we introduce textual and audio representations for multimodal learning. Together with the power of diffusion models with classifier-free guidance, Make-An-Audio explicits highfidelity synthesis with superior generalization."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2457
                },
                {
                    "x": 477,
                    "y": 2457
                },
                {
                    "x": 477,
                    "y": 2506
                },
                {
                    "x": 223,
                    "y": 2506
                }
            ],
            "category": "paragraph",
            "html": "<p id='37' style='font-size:18px'>3.1. Overview</p>",
            "id": 37,
            "page": 3,
            "text": "3.1. Overview"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2535
                },
                {
                    "x": 1215,
                    "y": 2535
                },
                {
                    "x": 1215,
                    "y": 2988
                },
                {
                    "x": 223,
                    "y": 2988
                }
            ],
            "category": "paragraph",
            "html": "<p id='38' style='font-size:18px'>Deep generative models have achieved leading perfor-<br>mances in text-guided visual synthesis. However, the current<br>development of text-to-audio (T2A) generation is hampered<br>by two major challenges: 1) Model training is faced with<br>data scarcity, as human-labeled audios are expensive to<br>create, and few audio resources provide natural language<br>descriptions. 2) Modeling long continuous waveforms (e.g.,<br>typically 16,000 data points for 1s 16 kHz waveforms) poses<br>a challenge for all high-quality neural synthesizers.</p>",
            "id": 38,
            "page": 3,
            "text": "Deep generative models have achieved leading performances in text-guided visual synthesis. However, the current development of text-to-audio (T2A) generation is hampered by two major challenges: 1) Model training is faced with data scarcity, as human-labeled audios are expensive to create, and few audio resources provide natural language descriptions. 2) Modeling long continuous waveforms (e.g., typically 16,000 data points for 1s 16 kHz waveforms) poses a challenge for all high-quality neural synthesizers."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 1207
                },
                {
                    "x": 2265,
                    "y": 1207
                },
                {
                    "x": 2265,
                    "y": 1758
                },
                {
                    "x": 1269,
                    "y": 1758
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='39' style='font-size:18px'>As illustrated in Figure 2, Make-An-Audio consists of the<br>following main components: 1) the pseudo prompt enhance-<br>ment to alleviate the issue of data scarcity, opening up the us-<br>age of orders of magnitude language-free audios; 2) a spec-<br>trogram autoencoder for predicting self-supervised represen-<br>tation instead of long continuous waveforms; 3) a diffusion<br>model that maps natural language to latent representations<br>with the power of contrastive language-audio pretraining<br>(CLAP) and 4) a separately-trained neural vocoder to con-<br>vert mel-spectrograms to raw waveforms. In the following<br>sections, we describe these components in detail.</p>",
            "id": 39,
            "page": 3,
            "text": "As illustrated in Figure 2, Make-An-Audio consists of the following main components: 1) the pseudo prompt enhancement to alleviate the issue of data scarcity, opening up the usage of orders of magnitude language-free audios; 2) a spectrogram autoencoder for predicting self-supervised representation instead of long continuous waveforms; 3) a diffusion model that maps natural language to latent representations with the power of contrastive language-audio pretraining (CLAP) and 4) a separately-trained neural vocoder to convert mel-spectrograms to raw waveforms. In the following sections, we describe these components in detail."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 1809
                },
                {
                    "x": 1900,
                    "y": 1809
                },
                {
                    "x": 1900,
                    "y": 1911
                },
                {
                    "x": 1275,
                    "y": 1911
                }
            ],
            "category": "paragraph",
            "html": "<p id='40' style='font-size:20px'>3.2. Pseudo Prompt Enhancement:<br>Distill-then-Reprogram</p>",
            "id": 40,
            "page": 3,
            "text": "3.2. Pseudo Prompt Enhancement: Distill-then-Reprogram"
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1936
                },
                {
                    "x": 2267,
                    "y": 1936
                },
                {
                    "x": 2267,
                    "y": 2290
                },
                {
                    "x": 1273,
                    "y": 2290
                }
            ],
            "category": "paragraph",
            "html": "<p id='41' style='font-size:18px'>To mitigate the data scarcity, we propose to construct<br>prompts aligned well with audios, enabling a better under-<br>standing of the text-audio dynamics from orders of magni-<br>tude unsupervised data. As illustrated in Figure 3, it consists<br>of two stages: an expert distillation approach to produce<br>prompts aligned with audio, and a dynamic reprogramming<br>procedure to construct a variety of concept compositions.</p>",
            "id": 41,
            "page": 3,
            "text": "To mitigate the data scarcity, we propose to construct prompts aligned well with audios, enabling a better understanding of the text-audio dynamics from orders of magnitude unsupervised data. As illustrated in Figure 3, it consists of two stages: an expert distillation approach to produce prompts aligned with audio, and a dynamic reprogramming procedure to construct a variety of concept compositions."
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 2336
                },
                {
                    "x": 1807,
                    "y": 2336
                },
                {
                    "x": 1807,
                    "y": 2384
                },
                {
                    "x": 1276,
                    "y": 2384
                }
            ],
            "category": "paragraph",
            "html": "<p id='42' style='font-size:16px'>3.2.1. EXPERT DISTILLATION</p>",
            "id": 42,
            "page": 3,
            "text": "3.2.1. EXPERT DISTILLATION"
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 2412
                },
                {
                    "x": 2266,
                    "y": 2412
                },
                {
                    "x": 2266,
                    "y": 2965
                },
                {
                    "x": 1270,
                    "y": 2965
                }
            ],
            "category": "paragraph",
            "html": "<p id='43' style='font-size:16px'>We consider the pre-trained automatic audio captioning (Xu<br>et al., 2020) and audio-text retrieval (Deshmukh et al., 2022;<br>Koepke et al., 2022) systems as our experts for prompt gen-<br>eration. Captioning models aim to generate diverse natural<br>language sentences to describe the content of audio clips.<br>Audio-text retrieval takes a natural language as a query to<br>retrieve relevant audio files in a database. To this end, ex-<br>perts jointly distill knowledge to construct a caption aligned<br>with audio, following which we select from these candidates<br>that endow high CLAP (Elizalde et al., 2022) score as the<br>final caption (we include a threshold to selectly consider</p>",
            "id": 43,
            "page": 3,
            "text": "We consider the pre-trained automatic audio captioning (Xu , 2020) and audio-text retrieval (Deshmukh , 2022; Koepke , 2022) systems as our experts for prompt generation. Captioning models aim to generate diverse natural language sentences to describe the content of audio clips. Audio-text retrieval takes a natural language as a query to retrieve relevant audio files in a database. To this end, experts jointly distill knowledge to construct a caption aligned with audio, following which we select from these candidates that endow high CLAP (Elizalde , 2022) score as the final caption (we include a threshold to selectly consider"
        },
        {
            "bounding_box": [
                {
                    "x": 552,
                    "y": 191
                },
                {
                    "x": 1933,
                    "y": 191
                },
                {
                    "x": 1933,
                    "y": 236
                },
                {
                    "x": 552,
                    "y": 236
                }
            ],
            "category": "header",
            "html": "<header id='44' style='font-size:16px'>Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models</header>",
            "id": 44,
            "page": 4,
            "text": "Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 285
                },
                {
                    "x": 1214,
                    "y": 285
                },
                {
                    "x": 1214,
                    "y": 532
                },
                {
                    "x": 223,
                    "y": 532
                }
            ],
            "category": "paragraph",
            "html": "<p id='45' style='font-size:18px'>faithful results). This simple yet effective procedure largely<br>alleviates data scarcity issues and explicit generalization<br>to different audio domains, and we refer the reader to Sec-<br>tion 6.3.2 for a summary of our findings. Details have been<br>attached in Appendix E.2.</p>",
            "id": 45,
            "page": 4,
            "text": "faithful results). This simple yet effective procedure largely alleviates data scarcity issues and explicit generalization to different audio domains, and we refer the reader to Section 6.3.2 for a summary of our findings. Details have been attached in Appendix E.2."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 582
                },
                {
                    "x": 865,
                    "y": 582
                },
                {
                    "x": 865,
                    "y": 629
                },
                {
                    "x": 225,
                    "y": 629
                }
            ],
            "category": "paragraph",
            "html": "<p id='46' style='font-size:14px'>3.2.2. DYNAMIC REPROGR AMMING</p>",
            "id": 46,
            "page": 4,
            "text": "3.2.2. DYNAMIC REPROGR AMMING"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 658
                },
                {
                    "x": 1217,
                    "y": 658
                },
                {
                    "x": 1217,
                    "y": 1360
                },
                {
                    "x": 223,
                    "y": 1360
                }
            ],
            "category": "paragraph",
            "html": "<p id='47' style='font-size:18px'>To prevent overfitting and enable a better understanding<br>of concept compositions, we introduce a dynamic repro-<br>gramming technique that constructs a variety of concept<br>compositions. It proceeds in three steps as illustrated in<br>Figure 3, where we elaborate the process as follows: 1) We<br>first prepare our sound event database D annotated with a<br>single label. 2) Each time N concepts are sampled from<br>the database D, where N E {0, 1, 2}. 3) The original text-<br>audio pair data has been randomly concatenated with the<br>sampled events according to the template, constructing a<br>new training example with varied concept compositions. It<br>can be conducted online, significantly reducing the time con-<br>sumed for data preparation. The reprogramming templates<br>are attached in Appendix F.</p>",
            "id": 47,
            "page": 4,
            "text": "To prevent overfitting and enable a better understanding of concept compositions, we introduce a dynamic reprogramming technique that constructs a variety of concept compositions. It proceeds in three steps as illustrated in Figure 3, where we elaborate the process as follows: 1) We first prepare our sound event database D annotated with a single label. 2) Each time N concepts are sampled from the database D, where N E {0, 1, 2}. 3) The original textaudio pair data has been randomly concatenated with the sampled events according to the template, constructing a new training example with varied concept compositions. It can be conducted online, significantly reducing the time consumed for data preparation. The reprogramming templates are attached in Appendix F."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1411
                },
                {
                    "x": 719,
                    "y": 1411
                },
                {
                    "x": 719,
                    "y": 1461
                },
                {
                    "x": 223,
                    "y": 1461
                }
            ],
            "category": "paragraph",
            "html": "<p id='48' style='font-size:22px'>3.3. Textual Representation</p>",
            "id": 48,
            "page": 4,
            "text": "3.3. Textual Representation"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1489
                },
                {
                    "x": 1217,
                    "y": 1489
                },
                {
                    "x": 1217,
                    "y": 2236
                },
                {
                    "x": 223,
                    "y": 2236
                }
            ],
            "category": "paragraph",
            "html": "<p id='49' style='font-size:18px'>Text-guided synthesis models need powerful semantic text<br>encoders to capture the meaning of arbitrary natural lan-<br>guage inputs, which could be grouped into two major cate-<br>gories: 1) Contrastive pretraining. Similar to CLIP (Radford<br>et al., 2021) pre-trained on image-text data, recent progress<br>on contrastive language-audio pretraining (CLAP) (Elizalde<br>et al., 2022) brings audio and text descriptions into a joint<br>space and demonstrates the outperformed zero-shot gener-<br>alization to multiple downstream domains. 2) Large-scale<br>language modeling (LLM). Saharia et al. (2022) and Kreuk<br>et al. (2022) utilize language models (e.g., BERT (Devlin<br>et al., 2018), T5 (Raffel et al., 2020)) for text-guided gen-<br>eration. Language models are trained on text-only corpus<br>significantly larger than paired multimodal data, thus being<br>exposed to a rich distribution of text.</p>",
            "id": 49,
            "page": 4,
            "text": "Text-guided synthesis models need powerful semantic text encoders to capture the meaning of arbitrary natural language inputs, which could be grouped into two major categories: 1) Contrastive pretraining. Similar to CLIP (Radford , 2021) pre-trained on image-text data, recent progress on contrastive language-audio pretraining (CLAP) (Elizalde , 2022) brings audio and text descriptions into a joint space and demonstrates the outperformed zero-shot generalization to multiple downstream domains. 2) Large-scale language modeling (LLM). Saharia  (2022) and Kreuk  (2022) utilize language models (e.g., BERT (Devlin , 2018), T5 (Raffel , 2020)) for text-guided generation. Language models are trained on text-only corpus significantly larger than paired multimodal data, thus being exposed to a rich distribution of text."
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 2262
                },
                {
                    "x": 1214,
                    "y": 2262
                },
                {
                    "x": 1214,
                    "y": 2613
                },
                {
                    "x": 222,
                    "y": 2613
                }
            ],
            "category": "paragraph",
            "html": "<p id='50' style='font-size:16px'>Following the common practice (Saharia et al., 2022;<br>Ramesh et al., 2022), we freeze the weights of these text<br>encoders. We find that both CLAP and T5-Large achieve<br>similar results on benchmark evaluation, while CLAP could<br>be more efficient without offline computation of embed-<br>dings required by LLM. We refer the reader to Section 6.3.1<br>for a summary of our findings.</p>",
            "id": 50,
            "page": 4,
            "text": "Following the common practice (Saharia , 2022; Ramesh , 2022), we freeze the weights of these text encoders. We find that both CLAP and T5-Large achieve similar results on benchmark evaluation, while CLAP could be more efficient without offline computation of embeddings required by LLM. We refer the reader to Section 6.3.1 for a summary of our findings."
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 2664
                },
                {
                    "x": 694,
                    "y": 2664
                },
                {
                    "x": 694,
                    "y": 2715
                },
                {
                    "x": 222,
                    "y": 2715
                }
            ],
            "category": "paragraph",
            "html": "<p id='51' style='font-size:22px'>3.4. Audio Representation</p>",
            "id": 51,
            "page": 4,
            "text": "3.4. Audio Representation"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2743
                },
                {
                    "x": 1215,
                    "y": 2743
                },
                {
                    "x": 1215,
                    "y": 2995
                },
                {
                    "x": 224,
                    "y": 2995
                }
            ],
            "category": "paragraph",
            "html": "<p id='52' style='font-size:18px'>Recently, spectrograms (akin to 1-channel 2D images) au-<br>toencoder (Gong et al., 2022; He et al., 2022) with recon-<br>struction objective as self-supervision have demonstrated<br>the effectiveness of heterogeneous image-to-audio trans-<br>fer, advancing the field of speech and audio processing on</p>",
            "id": 52,
            "page": 4,
            "text": "Recently, spectrograms (akin to 1-channel 2D images) autoencoder (Gong , 2022; He , 2022) with reconstruction objective as self-supervision have demonstrated the effectiveness of heterogeneous image-to-audio transfer, advancing the field of speech and audio processing on"
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 284
                },
                {
                    "x": 2266,
                    "y": 284
                },
                {
                    "x": 2266,
                    "y": 831
                },
                {
                    "x": 1270,
                    "y": 831
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='53' style='font-size:18px'>a variety of downstream tasks. The audio signal is a se-<br>quence of mel-spectrogram sample x E [0, 1]CaxT, , where<br>Ca, T respectively denote the mel channels and the number<br>of frames. Our spectrogram autoencoder is composed of<br>1) an encoder network E which takes samples x as input<br>and outputs latent representations z; 2) a decoder network<br>G reconstructs the mel-spectrogram signals x' from the<br>compressed representation z; and 3) a multi-window dis-<br>criminator Dis learns to distinguish the generated samples<br>G(z) from real ones in different multi-receptive fields of<br>mel-spectrograms.</p>",
            "id": 53,
            "page": 4,
            "text": "a variety of downstream tasks. The audio signal is a sequence of mel-spectrogram sample x E CaxT, , where Ca, T respectively denote the mel channels and the number of frames. Our spectrogram autoencoder is composed of 1) an encoder network E which takes samples x as input and outputs latent representations z; 2) a decoder network G reconstructs the mel-spectrogram signals x' from the compressed representation z; and 3) a multi-window discriminator Dis learns to distinguish the generated samples G(z) from real ones in different multi-receptive fields of mel-spectrograms."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 857
                },
                {
                    "x": 2266,
                    "y": 857
                },
                {
                    "x": 2266,
                    "y": 1206
                },
                {
                    "x": 1273,
                    "y": 1206
                }
            ],
            "category": "paragraph",
            "html": "<p id='54' style='font-size:18px'>The whole system is trained end-to-end to minimize 1)<br>Reconstruction loss Lre, which improves the training ef-<br>ficiency and the fidelity of the generated spectrograms; 2)<br>GAN losses LGAN, where the discriminator and genera-<br>tor play an adversarial game; and 3) KL-penalty loss LKL,<br>which restricts spectrogram encoders to learn standard 2 and<br>avoid arbitrarily high-variance latent spaces.</p>",
            "id": 54,
            "page": 4,
            "text": "The whole system is trained end-to-end to minimize 1) Reconstruction loss Lre, which improves the training efficiency and the fidelity of the generated spectrograms; 2) GAN losses LGAN, where the discriminator and generator play an adversarial game; and 3) KL-penalty loss LKL, which restricts spectrogram encoders to learn standard 2 and avoid arbitrarily high-variance latent spaces."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1228
                },
                {
                    "x": 2267,
                    "y": 1228
                },
                {
                    "x": 2267,
                    "y": 1481
                },
                {
                    "x": 1272,
                    "y": 1481
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='55' style='font-size:20px'>To this end, Make-An-Audio takes advantage of the spec-<br>trogram autoencoder to predict the self-supervised repre-<br>sentations instead of waveforms. It largely alleviates the<br>challenges of modeling long continuous data and guarantees<br>high-level semantic understanding.</p>",
            "id": 55,
            "page": 4,
            "text": "To this end, Make-An-Audio takes advantage of the spectrogram autoencoder to predict the self-supervised representations instead of waveforms. It largely alleviates the challenges of modeling long continuous data and guarantees high-level semantic understanding."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1533
                },
                {
                    "x": 1857,
                    "y": 1533
                },
                {
                    "x": 1857,
                    "y": 1584
                },
                {
                    "x": 1273,
                    "y": 1584
                }
            ],
            "category": "paragraph",
            "html": "<p id='56' style='font-size:18px'>3.5. Generative Latent Diffusion</p>",
            "id": 56,
            "page": 4,
            "text": "3.5. Generative Latent Diffusion"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1612
                },
                {
                    "x": 2265,
                    "y": 1612
                },
                {
                    "x": 2265,
                    "y": 2060
                },
                {
                    "x": 1272,
                    "y": 2060
                }
            ],
            "category": "paragraph",
            "html": "<p id='57' style='font-size:16px'>We implement our method over Latent Diffusion Models<br>(LDMs) (Rombach et al., 2022), a recently introduced class<br>of Denoising Diffusion Probabilistic Models (DDPMs) (Ho<br>et al., 2020) that operate in the latent space. Itis conditioned<br>on textual representation, breaking the generation process<br>into several conditional diffusion steps. The training loss is<br>defined as the mean squared error in the noise E ~ N(0,I)<br>space, and efficient training is optimizing a random term of<br>t with stochastic gradient descent:</p>",
            "id": 57,
            "page": 4,
            "text": "We implement our method over Latent Diffusion Models (LDMs) (Rombach , 2022), a recently introduced class of Denoising Diffusion Probabilistic Models (DDPMs) (Ho , 2020) that operate in the latent space. Itis conditioned on textual representation, breaking the generation process into several conditional diffusion steps. The training loss is defined as the mean squared error in the noise E ~ N(0,I) space, and efficient training is optimizing a random term of t with stochastic gradient descent:"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2164
                },
                {
                    "x": 2265,
                    "y": 2164
                },
                {
                    "x": 2265,
                    "y": 2463
                },
                {
                    "x": 1272,
                    "y": 2463
                }
            ],
            "category": "paragraph",
            "html": "<p id='58' style='font-size:18px'>where a denotes the small positive constant, and EA denotes<br>the denoising network. To conclude, the diffusion model can<br>be efficiently trained by optimizing ELBO without adver-<br>sarial feedback, ensuring extremely faithful reconstructions<br>that match the ground-truth distribution. Detailed formula-<br>tion of DDPM has been attached in Appendix D.</p>",
            "id": 58,
            "page": 4,
            "text": "where a denotes the small positive constant, and EA denotes the denoising network. To conclude, the diffusion model can be efficiently trained by optimizing ELBO without adversarial feedback, ensuring extremely faithful reconstructions that match the ground-truth distribution. Detailed formulation of DDPM has been attached in Appendix D."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2515
                },
                {
                    "x": 1798,
                    "y": 2515
                },
                {
                    "x": 1798,
                    "y": 2565
                },
                {
                    "x": 1272,
                    "y": 2565
                }
            ],
            "category": "paragraph",
            "html": "<p id='59' style='font-size:18px'>3.6. Classifier-Free Guidance</p>",
            "id": 59,
            "page": 4,
            "text": "3.6. Classifier-Free Guidance"
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 2593
                },
                {
                    "x": 2267,
                    "y": 2593
                },
                {
                    "x": 2267,
                    "y": 2996
                },
                {
                    "x": 1274,
                    "y": 2996
                }
            ],
            "category": "paragraph",
            "html": "<p id='60' style='font-size:18px'>For classifier-free guidance shown in (Dhariwal & Nichol,<br>2021; Ho & Salimans, 2022), by jointly training a condi-<br>tional and an unconditional diffusion model, it could be pos-<br>sible to combine the conditional and unconditional scores<br>to attain a trade-off between sample quality and diversity.<br>The textual condition in a latent diffusion model EA(Zt, t, c)<br>is replaced by an empty prompt c⌀ with a fixed probability<br>during training. During sampling, the output of the model is</p>",
            "id": 60,
            "page": 4,
            "text": "For classifier-free guidance shown in (Dhariwal & Nichol, 2021; Ho & Salimans, 2022), by jointly training a conditional and an unconditional diffusion model, it could be possible to combine the conditional and unconditional scores to attain a trade-off between sample quality and diversity. The textual condition in a latent diffusion model EA(Zt, t, c) is replaced by an empty prompt c⌀ with a fixed probability during training. During sampling, the output of the model is"
        },
        {
            "bounding_box": [
                {
                    "x": 550,
                    "y": 190
                },
                {
                    "x": 1932,
                    "y": 190
                },
                {
                    "x": 1932,
                    "y": 236
                },
                {
                    "x": 550,
                    "y": 236
                }
            ],
            "category": "header",
            "html": "<header id='61' style='font-size:18px'>Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models</header>",
            "id": 61,
            "page": 5,
            "text": "Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models"
        },
        {
            "bounding_box": [
                {
                    "x": 535,
                    "y": 266
                },
                {
                    "x": 1941,
                    "y": 266
                },
                {
                    "x": 1941,
                    "y": 914
                },
                {
                    "x": 535,
                    "y": 914
                }
            ],
            "category": "figure",
            "html": "<figure><img id='62' style='font-size:14px' alt=\"Make-An-Audio\nText Encoder\nImage-to-Audio ○ Transformer Vocoder\nGenerated Audio\nPooling\nPretrained Model\nNot used Module\nCLIP Image Encoder\nVideo-to-Audio\n○··· User-optional Flow\nFrame #1 Frame #2 Frame #4\" data-coord=\"top-left:(535,266); bottom-right:(1941,914)\" /></figure>",
            "id": 62,
            "page": 5,
            "text": "Make-An-Audio Text Encoder Image-to-Audio ○ Transformer Vocoder Generated Audio Pooling Pretrained Model Not used Module CLIP Image Encoder Video-to-Audio ○··· User-optional Flow Frame #1 Frame #2 Frame #4"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 939
                },
                {
                    "x": 2042,
                    "y": 939
                },
                {
                    "x": 2042,
                    "y": 987
                },
                {
                    "x": 445,
                    "y": 987
                }
            ],
            "category": "caption",
            "html": "<br><caption id='63' style='font-size:16px'>Figure 4. A high-level overview of visual-to-audio generation (I2A/V2A) pipeline using Make-An-Audio.</caption>",
            "id": 63,
            "page": 5,
            "text": "Figure 4. A high-level overview of visual-to-audio generation (I2A/V2A) pipeline using Make-An-Audio."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1048
                },
                {
                    "x": 1216,
                    "y": 1048
                },
                {
                    "x": 1216,
                    "y": 1170
                },
                {
                    "x": 223,
                    "y": 1170
                }
            ],
            "category": "paragraph",
            "html": "<p id='64' style='font-size:18px'>extrapolated further in the direction of EA(Zt,t,c) and away<br>from EA(Zt, t, c⌀ ) with the guidance scale s ≥ 1:<br>��(zt,t,c) = EA(Zt, c⌀ ) + s · (EA(Zt,t,c) - Ed(Zt,t,c⌀ ) (2)</p>",
            "id": 64,
            "page": 5,
            "text": "extrapolated further in the direction of EA(Zt,t,c) and away from EA(Zt, t, c⌀ ) with the guidance scale s ≥ 1: ��(zt,t,c) = EA(Zt, c⌀ ) + s · (EA(Zt,t,c) - Ed(Zt,t,c⌀ ) (2)"
        },
        {
            "bounding_box": [
                {
                    "x": 233,
                    "y": 1145
                },
                {
                    "x": 1214,
                    "y": 1145
                },
                {
                    "x": 1214,
                    "y": 1193
                },
                {
                    "x": 233,
                    "y": 1193
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='65' style='font-size:18px'>t,</p>",
            "id": 65,
            "page": 5,
            "text": "t,"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1253
                },
                {
                    "x": 1101,
                    "y": 1253
                },
                {
                    "x": 1101,
                    "y": 1309
                },
                {
                    "x": 223,
                    "y": 1309
                }
            ],
            "category": "paragraph",
            "html": "<p id='66' style='font-size:22px'>4. X-To-Audio: No Modality Left Behind</p>",
            "id": 66,
            "page": 5,
            "text": "4. X-To-Audio: No Modality Left Behind"
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 1338
                },
                {
                    "x": 1216,
                    "y": 1338
                },
                {
                    "x": 1216,
                    "y": 1743
                },
                {
                    "x": 222,
                    "y": 1743
                }
            ],
            "category": "paragraph",
            "html": "<p id='67' style='font-size:18px'>In this section, we generalize our powerful conditional dif-<br>fusion model for X-To-Audio generation. For the first time,<br>we contextualize the need for audio generation with different<br>conditional modalities, including: 1) text, 2) audio (inpaint-<br>ing), and 3) visual. Make-An-Audio empowers humans to<br>create rich and diverse audio content with unprecedented<br>ease, unlocking the ability to generate high-definition, high-<br>fidelity audio given a user-defined modality input.</p>",
            "id": 67,
            "page": 5,
            "text": "In this section, we generalize our powerful conditional diffusion model for X-To-Audio generation. For the first time, we contextualize the need for audio generation with different conditional modalities, including: 1) text, 2) audio (inpainting), and 3) visual. Make-An-Audio empowers humans to create rich and diverse audio content with unprecedented ease, unlocking the ability to generate high-definition, highfidelity audio given a user-defined modality input."
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 1792
                },
                {
                    "x": 1011,
                    "y": 1792
                },
                {
                    "x": 1011,
                    "y": 1844
                },
                {
                    "x": 222,
                    "y": 1844
                }
            ],
            "category": "paragraph",
            "html": "<p id='68' style='font-size:16px'>4.1. Personalized Text- To-Audio Generation</p>",
            "id": 68,
            "page": 5,
            "text": "4.1. Personalized Text- To-Audio Generation"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1870
                },
                {
                    "x": 1217,
                    "y": 1870
                },
                {
                    "x": 1217,
                    "y": 2522
                },
                {
                    "x": 224,
                    "y": 2522
                }
            ],
            "category": "paragraph",
            "html": "<p id='69' style='font-size:18px'>Adapting models (Chen et al., 2020b; Huang et al., 2022)<br>to a specific individual or object is a long-standing goal<br>in machine learning research. More recently, personaliza-<br>tion (Gal et al., 2022; Benhamdi et al., 2017) efforts can be<br>found in vision and graphics, which allows to inject unique<br>objects into new scenes, transform them across different<br>styles, and even produce new products. For instance, when<br>asked to generate \"baby crying\" given the initial sound of<br>\"thunder\", our model produces realistic and faithful audio<br>describing \"a baby cries in the thunder day\". Distinctly,<br>it has a wide range of uses for audio mixing and tuning,<br>e.g., adding background sound for an existing clip or editing<br>audio by inserting a speaking object.</p>",
            "id": 69,
            "page": 5,
            "text": "Adapting models (Chen , 2020b; Huang , 2022) to a specific individual or object is a long-standing goal in machine learning research. More recently, personalization (Gal , 2022; Benhamdi , 2017) efforts can be found in vision and graphics, which allows to inject unique objects into new scenes, transform them across different styles, and even produce new products. For instance, when asked to generate \"baby crying\" given the initial sound of \"thunder\", our model produces realistic and faithful audio describing \"a baby cries in the thunder day\". Distinctly, it has a wide range of uses for audio mixing and tuning, e.g., adding background sound for an existing clip or editing audio by inserting a speaking object."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2542
                },
                {
                    "x": 1215,
                    "y": 2542
                },
                {
                    "x": 1215,
                    "y": 2995
                },
                {
                    "x": 223,
                    "y": 2995
                }
            ],
            "category": "paragraph",
            "html": "<p id='70' style='font-size:18px'>We investigate the personalized text-to-audio generation by<br>stochastic differential editing (Meng et al., 2021), which has<br>been demonstrated to produce realistic samples with high-<br>fidelity manipulation. Given input audio with a user guide<br>(prompt), we select a particular time to with total denoising<br>steps N, and add noise to the raw data ZO for ZT (T =<br>to x N) according to Equation 4. It is then subsequently<br>denoised through a reverse process parameterized by shared<br>0 to increase its realism according to Equation 6.</p>",
            "id": 70,
            "page": 5,
            "text": "We investigate the personalized text-to-audio generation by stochastic differential editing (Meng , 2021), which has been demonstrated to produce realistic samples with highfidelity manipulation. Given input audio with a user guide (prompt), we select a particular time to with total denoising steps N, and add noise to the raw data ZO for ZT (T = to x N) according to Equation 4. It is then subsequently denoised through a reverse process parameterized by shared 0 to increase its realism according to Equation 6."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1044
                },
                {
                    "x": 2265,
                    "y": 1044
                },
                {
                    "x": 2265,
                    "y": 1347
                },
                {
                    "x": 1272,
                    "y": 1347
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='71' style='font-size:16px'>A trade-off between faithfulness (text-caption alignment)<br>and realism (audio quality) could be witnessed: As T in-<br>creases, a large amount of noise would be added to the<br>initial audio, and the generated samples become more real-<br>istic while less faithful. We refer the reader to Figure 5 for<br>a summary of our findings.</p>",
            "id": 71,
            "page": 5,
            "text": "A trade-off between faithfulness (text-caption alignment) and realism (audio quality) could be witnessed: As T increases, a large amount of noise would be added to the initial audio, and the generated samples become more realistic while less faithful. We refer the reader to Figure 5 for a summary of our findings."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 1399
                },
                {
                    "x": 1663,
                    "y": 1399
                },
                {
                    "x": 1663,
                    "y": 1449
                },
                {
                    "x": 1274,
                    "y": 1449
                }
            ],
            "category": "paragraph",
            "html": "<p id='72' style='font-size:20px'>4.2. Audio Inpainting</p>",
            "id": 72,
            "page": 5,
            "text": "4.2. Audio Inpainting"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1476
                },
                {
                    "x": 2265,
                    "y": 1476
                },
                {
                    "x": 2265,
                    "y": 1976
                },
                {
                    "x": 1272,
                    "y": 1976
                }
            ],
            "category": "paragraph",
            "html": "<p id='73' style='font-size:18px'>Inpainting (Liu et al., 2020; Nazeri et al., 2019) is the task<br>of filling masked regions of an audio with new content<br>since parts of the audio are corrupted or undesired. Though<br>diffusion model inpainting can be performed by adding<br>noise to initial audio and sampling with SDEdit, it may<br>result in undesired edge artifacts since there could be an<br>information loss during the sampling process (the model<br>can only see a noised version of the context). To achieve<br>better results, we explicitly fine-tune Make-An-Audio for<br>audio inpainting.</p>",
            "id": 73,
            "page": 5,
            "text": "Inpainting (Liu , 2020; Nazeri , 2019) is the task of filling masked regions of an audio with new content since parts of the audio are corrupted or undesired. Though diffusion model inpainting can be performed by adding noise to initial audio and sampling with SDEdit, it may result in undesired edge artifacts since there could be an information loss during the sampling process (the model can only see a noised version of the context). To achieve better results, we explicitly fine-tune Make-An-Audio for audio inpainting."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 1998
                },
                {
                    "x": 2266,
                    "y": 1998
                },
                {
                    "x": 2266,
                    "y": 2552
                },
                {
                    "x": 1270,
                    "y": 2552
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='74' style='font-size:18px'>During training, the way masks are generated greatly in-<br>fluences the final performance of the system. As such, we<br>adopt irregular masks (thick, medium, and thin masks) sug-<br>gested by LaMa (Suvorov et al., 2022), which uniformly<br>uses polygonal chains dilated by a high random width (wide<br>masks) and rectangles of arbitrary aspect ratios (box masks).<br>In addition, we investigate the frame-based masking strategy<br>commonly adopted in speech liteature (Baevski et al., 2020;<br>Hsu et al., 2021). It is implemented using the algorithm<br>from wav2vec 2.0 (Baevski et al., 2020), where spans of<br>length are masked with a p probability.</p>",
            "id": 74,
            "page": 5,
            "text": "During training, the way masks are generated greatly influences the final performance of the system. As such, we adopt irregular masks (thick, medium, and thin masks) suggested by LaMa (Suvorov , 2022), which uniformly uses polygonal chains dilated by a high random width (wide masks) and rectangles of arbitrary aspect ratios (box masks). In addition, we investigate the frame-based masking strategy commonly adopted in speech liteature (Baevski , 2020; Hsu , 2021). It is implemented using the algorithm from wav2vec 2.0 (Baevski , 2020), where spans of length are masked with a p probability."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 2601
                },
                {
                    "x": 1859,
                    "y": 2601
                },
                {
                    "x": 1859,
                    "y": 2651
                },
                {
                    "x": 1274,
                    "y": 2651
                }
            ],
            "category": "paragraph",
            "html": "<p id='75' style='font-size:18px'>4.3. Visual- To-Audio Generation</p>",
            "id": 75,
            "page": 5,
            "text": "4.3. Visual- To-Audio Generation"
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2680
                },
                {
                    "x": 2264,
                    "y": 2680
                },
                {
                    "x": 2264,
                    "y": 2984
                },
                {
                    "x": 1273,
                    "y": 2984
                }
            ],
            "category": "paragraph",
            "html": "<p id='76' style='font-size:18px'>Recent advances in deep generative models have shown im-<br>pressive results in the visually-induced audio generation (Su<br>et al., 2020; Gan et al., 2020), towards generating realistic<br>audio that describes the content of images or videos: Hsu<br>et al. (2020) show that spoken language could be learned<br>by a visually-grounded generative model of speech. Iashin</p>",
            "id": 76,
            "page": 5,
            "text": "Recent advances in deep generative models have shown impressive results in the visually-induced audio generation (Su , 2020; Gan , 2020), towards generating realistic audio that describes the content of images or videos: Hsu  (2020) show that spoken language could be learned by a visually-grounded generative model of speech. Iashin"
        },
        {
            "bounding_box": [
                {
                    "x": 554,
                    "y": 190
                },
                {
                    "x": 1933,
                    "y": 190
                },
                {
                    "x": 1933,
                    "y": 236
                },
                {
                    "x": 554,
                    "y": 236
                }
            ],
            "category": "header",
            "html": "<header id='77' style='font-size:14px'>Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models</header>",
            "id": 77,
            "page": 6,
            "text": "Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 285
                },
                {
                    "x": 1218,
                    "y": 285
                },
                {
                    "x": 1218,
                    "y": 384
                },
                {
                    "x": 224,
                    "y": 384
                }
            ],
            "category": "paragraph",
            "html": "<p id='78' style='font-size:14px'>& Rahtu (2021) propose a multi-class visual guided sound<br>synthesis that relies on a codebook prior-based transformer.</p>",
            "id": 78,
            "page": 6,
            "text": "& Rahtu (2021) propose a multi-class visual guided sound synthesis that relies on a codebook prior-based transformer."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 408
                },
                {
                    "x": 1218,
                    "y": 408
                },
                {
                    "x": 1218,
                    "y": 1157
                },
                {
                    "x": 223,
                    "y": 1157
                }
            ],
            "category": "paragraph",
            "html": "<p id='79' style='font-size:16px'>To pursue this research further, we extend Make-An-Audio<br>for visual-to-audio generation. For the lack of large-scale<br>visual-audio datasets in image-to-audio (I2A) research, our<br>main idea is to utilize contrastive language-image pretrain-<br>ing (CLIP) with CLIP-guided T2A model and leverage tex-<br>tual representations to bridge the modality gap between<br>visual and audio world. As CLIP encoders embed images<br>and text to the joint latent space, our T2A model provides<br>a unique opportunity to visualize what the CLIP image<br>encoder is seeing. Considering the complexity of V2A gen-<br>eration, it is natural to leverage image priors for videos to<br>simplify the learning process. On this account, we uniformly<br>pick up 4 frames from the video and pool these CLIP image<br>features to formulate the \"averaged\" video representation,<br>which is then deteriorated to I2A generation.</p>",
            "id": 79,
            "page": 6,
            "text": "To pursue this research further, we extend Make-An-Audio for visual-to-audio generation. For the lack of large-scale visual-audio datasets in image-to-audio (I2A) research, our main idea is to utilize contrastive language-image pretraining (CLIP) with CLIP-guided T2A model and leverage textual representations to bridge the modality gap between visual and audio world. As CLIP encoders embed images and text to the joint latent space, our T2A model provides a unique opportunity to visualize what the CLIP image encoder is seeing. Considering the complexity of V2A generation, it is natural to leverage image priors for videos to simplify the learning process. On this account, we uniformly pick up 4 frames from the video and pool these CLIP image features to formulate the \"averaged\" video representation, which is then deteriorated to I2A generation."
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 1180
                },
                {
                    "x": 1216,
                    "y": 1180
                },
                {
                    "x": 1216,
                    "y": 1432
                },
                {
                    "x": 222,
                    "y": 1432
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='80' style='font-size:14px'>To conclude, the visual-to-audio inference scheme can be<br>formulated in Figure 4. It significantly reduces the require-<br>ment for pair visual datasets, and the plug-and-play module<br>with pre-trained Make-An-Audio empowers humans to cre-<br>ate rich and diverse audio content from the visual world.</p>",
            "id": 80,
            "page": 6,
            "text": "To conclude, the visual-to-audio inference scheme can be formulated in Figure 4. It significantly reduces the requirement for pair visual datasets, and the plug-and-play module with pre-trained Make-An-Audio empowers humans to create rich and diverse audio content from the visual world."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1493
                },
                {
                    "x": 806,
                    "y": 1493
                },
                {
                    "x": 806,
                    "y": 1548
                },
                {
                    "x": 224,
                    "y": 1548
                }
            ],
            "category": "paragraph",
            "html": "<p id='81' style='font-size:22px'>5. Training and Evaluation</p>",
            "id": 81,
            "page": 6,
            "text": "5. Training and Evaluation"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1580
                },
                {
                    "x": 441,
                    "y": 1580
                },
                {
                    "x": 441,
                    "y": 1628
                },
                {
                    "x": 223,
                    "y": 1628
                }
            ],
            "category": "paragraph",
            "html": "<p id='82' style='font-size:14px'>5.1. Dataset</p>",
            "id": 82,
            "page": 6,
            "text": "5.1. Dataset"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1654
                },
                {
                    "x": 1217,
                    "y": 1654
                },
                {
                    "x": 1217,
                    "y": 2506
                },
                {
                    "x": 223,
                    "y": 2506
                }
            ],
            "category": "paragraph",
            "html": "<p id='83' style='font-size:16px'>We train on a combination of several datasets: AudioSet,<br>BBC sound effects, Audiostock, AudioCaps-train, ESC-50,<br>FSD50K, Free To Use Sounds, Sonniss Game Effects, We-<br>SoundEffects, MACS, Epidemic Sound, UrbanSound8K,<br>WavText5Ks, LibriSpeech, and Medley-solos-DB. For au-<br>dios without natural language annotation, we apply the<br>pseudo prompt enhancement to construct captions aligned<br>well with the audio. Overall we have ~3k hours with<br>1M audio-text pairs for training data. For evaluating text-<br>to-audio models (Yang et al., 2022; Kreuk et al., 2022),<br>the AudioCaption validation set is adopted as the stan-<br>dard benchmark, which contains 494 samples with five<br>human-annotated captions in each audio clip. For a more<br>challenging zero-shot scenario, we also provide results in<br>Clotho (Drossos et al., 2020) validation set which contain<br>multiple audio events. A more detailed data setup has been<br>attached in Appendix A.</p>",
            "id": 83,
            "page": 6,
            "text": "We train on a combination of several datasets: AudioSet, BBC sound effects, Audiostock, AudioCaps-train, ESC-50, FSD50K, Free To Use Sounds, Sonniss Game Effects, WeSoundEffects, MACS, Epidemic Sound, UrbanSound8K, WavText5Ks, LibriSpeech, and Medley-solos-DB. For audios without natural language annotation, we apply the pseudo prompt enhancement to construct captions aligned well with the audio. Overall we have ~3k hours with 1M audio-text pairs for training data. For evaluating textto-audio models (Yang , 2022; Kreuk , 2022), the AudioCaption validation set is adopted as the standard benchmark, which contains 494 samples with five human-annotated captions in each audio clip. For a more challenging zero-shot scenario, we also provide results in Clotho (Drossos , 2020) validation set which contain multiple audio events. A more detailed data setup has been attached in Appendix A."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2529
                },
                {
                    "x": 1214,
                    "y": 2529
                },
                {
                    "x": 1214,
                    "y": 2980
                },
                {
                    "x": 223,
                    "y": 2980
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='84' style='font-size:14px'>We conduct preprocessing on the text and audio data: 1)<br>convert the sampling rate of audios to 16kHz and pad short<br>clips to 10-second long; 2) extract the spectrogram with<br>the FFT size of 1024, hop size of 256 and crop it to a<br>mel-spectrogram of size 80 x 624; 3) non-standard words<br>(e.g., abbreviations, numbers, and currency expressions) and<br>semiotic classes (Taylor, 2009) (text tokens that represent<br>particular entities that are semantically constrained, such as<br>measure phrases, addresses, and dates) are normalized.</p>",
            "id": 84,
            "page": 6,
            "text": "We conduct preprocessing on the text and audio data: 1) convert the sampling rate of audios to 16kHz and pad short clips to 10-second long; 2) extract the spectrogram with the FFT size of 1024, hop size of 256 and crop it to a mel-spectrogram of size 80 x 624; 3) non-standard words (e.g., abbreviations, numbers, and currency expressions) and semiotic classes (Taylor, 2009) (text tokens that represent particular entities that are semantically constrained, such as measure phrases, addresses, and dates) are normalized."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 284
                },
                {
                    "x": 1748,
                    "y": 284
                },
                {
                    "x": 1748,
                    "y": 332
                },
                {
                    "x": 1273,
                    "y": 332
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='85' style='font-size:18px'>5.2. Model Configurations</p>",
            "id": 85,
            "page": 6,
            "text": "5.2. Model Configurations"
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 360
                },
                {
                    "x": 2267,
                    "y": 360
                },
                {
                    "x": 2267,
                    "y": 1012
                },
                {
                    "x": 1270,
                    "y": 1012
                }
            ],
            "category": "paragraph",
            "html": "<p id='86' style='font-size:14px'>We train a continuous autoencoder to compress the percep-<br>tual space with downsampling to a 4-channel latent represen-<br>tation, which balances efficiency and perceptually faithful<br>results. For our main experiments, we train a U-Net (Ron-<br>neberger et al., 2015) based text-conditional diffusion model,<br>which is optimized using 18 NVIDIA V100 GPU until 2M<br>optimization steps. The base learning rate is set to 0.005,<br>and we scale it by the number of GPUs and the batch size<br>following LDM. We utilize HiFi-GAN (Kong et al., 2020)<br>(V1) trained on VGGSound dataset (Chen et al., 2020a)<br>as the vocoder to synthesize waveform from the generated<br>mel-spectrogram in all our experiments. Hyperparameters<br>are included in Appendix B.</p>",
            "id": 86,
            "page": 6,
            "text": "We train a continuous autoencoder to compress the perceptual space with downsampling to a 4-channel latent representation, which balances efficiency and perceptually faithful results. For our main experiments, we train a U-Net (Ronneberger , 2015) based text-conditional diffusion model, which is optimized using 18 NVIDIA V100 GPU until 2M optimization steps. The base learning rate is set to 0.005, and we scale it by the number of GPUs and the batch size following LDM. We utilize HiFi-GAN (Kong , 2020) (V1) trained on VGGSound dataset (Chen , 2020a) as the vocoder to synthesize waveform from the generated mel-spectrogram in all our experiments. Hyperparameters are included in Appendix B."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1063
                },
                {
                    "x": 1699,
                    "y": 1063
                },
                {
                    "x": 1699,
                    "y": 1112
                },
                {
                    "x": 1273,
                    "y": 1112
                }
            ],
            "category": "paragraph",
            "html": "<p id='87' style='font-size:14px'>5.3. Evaluation Metrics</p>",
            "id": 87,
            "page": 6,
            "text": "5.3. Evaluation Metrics"
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 1139
                },
                {
                    "x": 2266,
                    "y": 1139
                },
                {
                    "x": 2266,
                    "y": 1692
                },
                {
                    "x": 1270,
                    "y": 1692
                }
            ],
            "category": "paragraph",
            "html": "<p id='88' style='font-size:16px'>We evaluate models using objective and subjective metrics<br>over audio quality and text-audio alignment faithfulness.<br>Following common practice (Yang et al., 2022; Iashin &<br>Rahtu, 2021), the key automated performance metrics used<br>are melception-based (Koutini et al., 2021) FID (Heusel<br>et al., 2017) and KL divergence to measure audio fidelity.<br>Additionally, we introduce the CLAP score to measure<br>audio-text alignment for this work. CLAP score is adapted<br>from the CLIP score (Hessel et al., 2021; Radford et al.,<br>2021) to the audio domain and is a reference-free evaluation<br>metric that closely correlates with human perception.</p>",
            "id": 88,
            "page": 6,
            "text": "We evaluate models using objective and subjective metrics over audio quality and text-audio alignment faithfulness. Following common practice (Yang , 2022; Iashin & Rahtu, 2021), the key automated performance metrics used are melception-based (Koutini , 2021) FID (Heusel , 2017) and KL divergence to measure audio fidelity. Additionally, we introduce the CLAP score to measure audio-text alignment for this work. CLAP score is adapted from the CLIP score (Hessel , 2021; Radford , 2021) to the audio domain and is a reference-free evaluation metric that closely correlates with human perception."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1714
                },
                {
                    "x": 2267,
                    "y": 1714
                },
                {
                    "x": 2267,
                    "y": 2066
                },
                {
                    "x": 1272,
                    "y": 2066
                }
            ],
            "category": "paragraph",
            "html": "<p id='89' style='font-size:14px'>For subjective metrics, we use crowd-sourced human evalu-<br>ation via Amazon Mechanical Turk, where raters are asked<br>to rate MOS (mean opinion score) on a 20-100 Likert scale.<br>We assess the audio quality and text-audio alignment faith-<br>fulness by respectively scoring MOS-Q and MOS-F, which<br>is reported with 95% confidence intervals (CI). More infor-<br>mation on evaluation has been attached in Appendix C.</p>",
            "id": 89,
            "page": 6,
            "text": "For subjective metrics, we use crowd-sourced human evaluation via Amazon Mechanical Turk, where raters are asked to rate MOS (mean opinion score) on a 20-100 Likert scale. We assess the audio quality and text-audio alignment faithfulness by respectively scoring MOS-Q and MOS-F, which is reported with 95% confidence intervals (CI). More information on evaluation has been attached in Appendix C."
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 2128
                },
                {
                    "x": 1492,
                    "y": 2128
                },
                {
                    "x": 1492,
                    "y": 2180
                },
                {
                    "x": 1276,
                    "y": 2180
                }
            ],
            "category": "paragraph",
            "html": "<p id='90' style='font-size:20px'>6. Results</p>",
            "id": 90,
            "page": 6,
            "text": "6. Results"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2214
                },
                {
                    "x": 1720,
                    "y": 2214
                },
                {
                    "x": 1720,
                    "y": 2261
                },
                {
                    "x": 1272,
                    "y": 2261
                }
            ],
            "category": "paragraph",
            "html": "<p id='91' style='font-size:18px'>6.1. Quantitative Results</p>",
            "id": 91,
            "page": 6,
            "text": "6.1. Quantitative Results"
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 2289
                },
                {
                    "x": 2266,
                    "y": 2289
                },
                {
                    "x": 2266,
                    "y": 2891
                },
                {
                    "x": 1271,
                    "y": 2891
                }
            ],
            "category": "paragraph",
            "html": "<p id='92' style='font-size:16px'>Automatic Objective Evaluation The objective evaluation<br>comparison with baseline Diffsound (the only publicly-<br>available T2A generation model) are presented in Table 1,<br>and we have the following observations: 1) In terms of au-<br>dio qualty, Make-An-Audio achieves the highest perceptual<br>quality in AudioCaption with FID of 4.61 and KL of 2.79.<br>For zero-shot generation, it also demonstrates the outper-<br>formed results superior to the baseline model; 2) On text-<br>audio similarity, Make-An-Audio scores the highest CLAP<br>with a gap of 0.037 compared to the ground truth audio, sug-<br>gesting Make-An-Audio's ability to generate faithful audio<br>that aligns well with descriptions.</p>",
            "id": 92,
            "page": 6,
            "text": "Automatic Objective Evaluation The objective evaluation comparison with baseline Diffsound (the only publiclyavailable T2A generation model) are presented in Table 1, and we have the following observations: 1) In terms of audio qualty, Make-An-Audio achieves the highest perceptual quality in AudioCaption with FID of 4.61 and KL of 2.79. For zero-shot generation, it also demonstrates the outperformed results superior to the baseline model; 2) On textaudio similarity, Make-An-Audio scores the highest CLAP with a gap of 0.037 compared to the ground truth audio, suggesting Make-An-Audio's ability to generate faithful audio that aligns well with descriptions."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2913
                },
                {
                    "x": 2260,
                    "y": 2913
                },
                {
                    "x": 2260,
                    "y": 2964
                },
                {
                    "x": 1272,
                    "y": 2964
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='93' style='font-size:16px'>Subjective Human Evaluation The evaluation of the T2A</p>",
            "id": 93,
            "page": 6,
            "text": "Subjective Human Evaluation The evaluation of the T2A"
        },
        {
            "bounding_box": [
                {
                    "x": 551,
                    "y": 191
                },
                {
                    "x": 1934,
                    "y": 191
                },
                {
                    "x": 1934,
                    "y": 236
                },
                {
                    "x": 551,
                    "y": 236
                }
            ],
            "category": "caption",
            "html": "<caption id='94' style='font-size:18px'>Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models</caption>",
            "id": 94,
            "page": 7,
            "text": "Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models"
        },
        {
            "bounding_box": [
                {
                    "x": 369,
                    "y": 267
                },
                {
                    "x": 2113,
                    "y": 267
                },
                {
                    "x": 2113,
                    "y": 673
                },
                {
                    "x": 369,
                    "y": 673
                }
            ],
            "category": "table",
            "html": "<table id='95' style='font-size:14px'><tr><td>Model</td><td>Text-cond</td><td>Params</td><td>FID</td><td>KL</td><td>CLAP</td><td>MOS-Q</td><td>MOS-F</td><td>FID-Z</td><td>KL-Z</td></tr><tr><td>Reference</td><td>/</td><td>/</td><td>/</td><td>/</td><td>0.526</td><td>74.7±0.94</td><td>80.5±1.84</td><td>/</td><td>/</td></tr><tr><td>Diffsound</td><td>CLIP</td><td>520M</td><td>7.17</td><td>3.57</td><td>0.420</td><td>67.1±1.03</td><td>70.9±1.05</td><td>24.97</td><td>6.53</td></tr><tr><td rowspan=\"4\">Make-An-Audio</td><td>CLAP</td><td>332M</td><td>4.61</td><td>2.79</td><td>0.482</td><td>72.5±0.90</td><td>78.6±1.01</td><td>17.38</td><td>6.98</td></tr><tr><td>BERT</td><td>809M</td><td>5.15</td><td>2.89</td><td>0.480</td><td>70.5±0.87</td><td>77.2±0.98</td><td>18.75</td><td>7.01</td></tr><tr><td>T5-Large</td><td>563M</td><td>4.83</td><td>2.81</td><td>0.486</td><td>71.8±0.91</td><td>77.2±0.93</td><td>17.23</td><td>7.02</td></tr><tr><td>CLIP</td><td>576M</td><td>6.45</td><td>2.91</td><td>0.444</td><td>72.1±0.92</td><td>75.4±0.96</td><td>17.55</td><td>7.09</td></tr></table>",
            "id": 95,
            "page": 7,
            "text": "Model Text-cond Params FID KL CLAP MOS-Q MOS-F FID-Z KL-Z  Reference / / / / 0.526 74.7±0.94 80.5±1.84 / /  Diffsound CLIP 520M 7.17 3.57 0.420 67.1±1.03 70.9±1.05 24.97 6.53  Make-An-Audio CLAP 332M 4.61 2.79 0.482 72.5±0.90 78.6±1.01 17.38 6.98  BERT 809M 5.15 2.89 0.480 70.5±0.87 77.2±0.98 18.75 7.01  T5-Large 563M 4.83 2.81 0.486 71.8±0.91 77.2±0.93 17.23 7.02  CLIP 576M 6.45 2.91 0.444 72.1±0.92 75.4±0.96 17.55"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 684
                },
                {
                    "x": 2260,
                    "y": 684
                },
                {
                    "x": 2260,
                    "y": 776
                },
                {
                    "x": 223,
                    "y": 776
                }
            ],
            "category": "caption",
            "html": "<br><caption id='96' style='font-size:16px'>Table 1. Text-to-audio evaluation. We report the evaluation metrics including MOS(↑), FID(↓), KL(↓), and CLAP(↑). FID-Z and KL-Z<br>denote the zero-shot results in the Clotho dataset.</caption>",
            "id": 96,
            "page": 7,
            "text": "Table 1. Text-to-audio evaluation. We report the evaluation metrics including MOS(↑), FID(↓), KL(↓), and CLAP(↑). FID-Z and KL-Z denote the zero-shot results in the Clotho dataset."
        },
        {
            "bounding_box": [
                {
                    "x": 309,
                    "y": 805
                },
                {
                    "x": 1409,
                    "y": 805
                },
                {
                    "x": 1409,
                    "y": 1181
                },
                {
                    "x": 309,
                    "y": 1181
                }
            ],
            "category": "table",
            "html": "<table id='97' style='font-size:14px'><tr><td rowspan=\"2\">Training Masks</td><td colspan=\"3\">Narrow Masks</td><td colspan=\"3\">Wide Masks</td></tr><tr><td>FID</td><td>KL</td><td>MOS-Q</td><td>FID</td><td>KL</td><td>MOS-Q</td></tr><tr><td>Irregular (Thin)</td><td>1.83</td><td>0.46</td><td>68.3±1.38</td><td>4.01</td><td>0.86</td><td>66.2±1.20</td></tr><tr><td>Irregular (Medium)</td><td>1.76</td><td>0.31</td><td>67.8±1.41</td><td>3.93</td><td>0.65</td><td>66.9±1.22</td></tr><tr><td>Irregular (Thick)</td><td>1.73</td><td>0.32</td><td>69.6±1.36</td><td>3.83</td><td>0.67</td><td>69.3±1.05</td></tr><tr><td>Frame (p=30%)</td><td>1.64</td><td>0.29</td><td>66.9±1.60</td><td>3.68</td><td>0.62</td><td>66.1±1.29</td></tr><tr><td>Frame (p=50%)</td><td>1.77</td><td>0.32</td><td>68.6±1.42</td><td>3.66</td><td>0.63</td><td>67.4±1.27</td></tr><tr><td>Frame (p=70%)</td><td>1.59</td><td>0.32</td><td>71.0±1.12</td><td>3.49</td><td>0.65</td><td>70.8±1.50</td></tr></table>",
            "id": 97,
            "page": 7,
            "text": "Training Masks Narrow Masks Wide Masks  FID KL MOS-Q FID KL MOS-Q  Irregular (Thin) 1.83 0.46 68.3±1.38 4.01 0.86 66.2±1.20  Irregular (Medium) 1.76 0.31 67.8±1.41 3.93 0.65 66.9±1.22  Irregular (Thick) 1.73 0.32 69.6±1.36 3.83 0.67 69.3±1.05  Frame (p=30%) 1.64 0.29 66.9±1.60 3.68 0.62 66.1±1.29  Frame (p=50%) 1.77 0.32 68.6±1.42 3.66 0.63 67.4±1.27  Frame (p=70%) 1.59 0.32 71.0±1.12 3.49 0.65"
        },
        {
            "bounding_box": [
                {
                    "x": 388,
                    "y": 1189
                },
                {
                    "x": 2106,
                    "y": 1189
                },
                {
                    "x": 2106,
                    "y": 1234
                },
                {
                    "x": 388,
                    "y": 1234
                }
            ],
            "category": "caption",
            "html": "<br><caption id='98' style='font-size:18px'>Table 2. Audio inpainting evaluation with variety masking strategies. Table 3. Image/Video-to-audio evaluation.</caption>",
            "id": 98,
            "page": 7,
            "text": "Table 2. Audio inpainting evaluation with variety masking strategies. Table 3. Image/Video-to-audio evaluation."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1308
                },
                {
                    "x": 1214,
                    "y": 1308
                },
                {
                    "x": 1214,
                    "y": 1605
                },
                {
                    "x": 224,
                    "y": 1605
                }
            ],
            "category": "paragraph",
            "html": "<p id='99' style='font-size:18px'>models is very challenging due to its subjective nature in<br>perceptual quality, and thus we include a human evaluation<br>in Table 1: Make-An-Audio (CLAP) achieves the highest<br>perceptual quality with MOS-Q of 72.5 and MOS-F of 78.6.<br>It indicates that raters prefer our model synthesis against<br>baselines in terms of audio naturalness and faithfulness.</p>",
            "id": 99,
            "page": 7,
            "text": "models is very challenging due to its subjective nature in perceptual quality, and thus we include a human evaluation in Table 1: Make-An-Audio (CLAP) achieves the highest perceptual quality with MOS-Q of 72.5 and MOS-F of 78.6. It indicates that raters prefer our model synthesis against baselines in terms of audio naturalness and faithfulness."
        },
        {
            "bounding_box": [
                {
                    "x": 1480,
                    "y": 800
                },
                {
                    "x": 2134,
                    "y": 800
                },
                {
                    "x": 2134,
                    "y": 1174
                },
                {
                    "x": 1480,
                    "y": 1174
                }
            ],
            "category": "table",
            "html": "<br><table id='100' style='font-size:14px'><tr><td>Method</td><td>MOS-Q</td><td>MOS-F</td></tr><tr><td colspan=\"3\">Image-to-Audio Generation</td></tr><tr><td>Reference</td><td>72.0±1.54</td><td>76.4±1.83</td></tr><tr><td>Make-An-Audio</td><td>68.4±1.09</td><td>78.0±1.20</td></tr><tr><td colspan=\"3\">Video-to-Audio Generation</td></tr><tr><td>Reference</td><td>69.5±1.22</td><td>81.0±1.43</td></tr><tr><td>Make-An-Audio</td><td>60.0±1.31</td><td>69.0±1.08</td></tr></table>",
            "id": 100,
            "page": 7,
            "text": "Method MOS-Q MOS-F  Image-to-Audio Generation  Reference 72.0±1.54 76.4±1.83  Make-An-Audio 68.4±1.09 78.0±1.20  Video-to-Audio Generation  Reference 69.5±1.22 81.0±1.43  Make-An-Audio 60.0±1.31"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1631
                },
                {
                    "x": 1216,
                    "y": 1631
                },
                {
                    "x": 1216,
                    "y": 2430
                },
                {
                    "x": 224,
                    "y": 2430
                }
            ],
            "category": "paragraph",
            "html": "<p id='101' style='font-size:20px'>For audio-inpainting, we compare different masking designs,<br>including the irregular (thick, medium, and thin) strategy<br>from visual world (Suvorov et al., 2022), as well as the<br>frame-based (with varying p) strategy commonly used in<br>speech (Baevski et al., 2020; Hsu et al., 2021). During<br>evaluation, we randomly mask the wide or narrow regions<br>and utilize FID and KL metrics to measure performance.<br>The results have been presented in Table 2, and we have the<br>following observations: 1) In both frame-based or irregular<br>strategies, larger masked regions in training have witnessed<br>the improved perceptual quality, which force the network to<br>exploit the high receptive field of continuous spectrograms<br>fully. 2) With the similar size of the masked region, the<br>frame-based strategy consistently outperforms the irregular<br>one, suggesting that it could be better to mask the audio<br>spectrograms which align in time series.</p>",
            "id": 101,
            "page": 7,
            "text": "For audio-inpainting, we compare different masking designs, including the irregular (thick, medium, and thin) strategy from visual world (Suvorov , 2022), as well as the frame-based (with varying p) strategy commonly used in speech (Baevski , 2020; Hsu , 2021). During evaluation, we randomly mask the wide or narrow regions and utilize FID and KL metrics to measure performance. The results have been presented in Table 2, and we have the following observations: 1) In both frame-based or irregular strategies, larger masked regions in training have witnessed the improved perceptual quality, which force the network to exploit the high receptive field of continuous spectrograms fully. 2) With the similar size of the masked region, the frame-based strategy consistently outperforms the irregular one, suggesting that it could be better to mask the audio spectrograms which align in time series."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2451
                },
                {
                    "x": 1214,
                    "y": 2451
                },
                {
                    "x": 1214,
                    "y": 2753
                },
                {
                    "x": 224,
                    "y": 2753
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='102' style='font-size:18px'>We also present our visual-to-audio generation results in<br>Table 3. As can be seen, Make-An-Audio can generalize to<br>a wide variety of images and videos. Leveraging contrastive<br>pre-training, the model provides a high-level understanding<br>of visual input, which generates high-fidelity audio spectro-<br>grams well-aligned with their semantic meanings.</p>",
            "id": 102,
            "page": 7,
            "text": "We also present our visual-to-audio generation results in Table 3. As can be seen, Make-An-Audio can generalize to a wide variety of images and videos. Leveraging contrastive pre-training, the model provides a high-level understanding of visual input, which generates high-fidelity audio spectrograms well-aligned with their semantic meanings."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2806
                },
                {
                    "x": 672,
                    "y": 2806
                },
                {
                    "x": 672,
                    "y": 2856
                },
                {
                    "x": 224,
                    "y": 2856
                }
            ],
            "category": "paragraph",
            "html": "<p id='103' style='font-size:22px'>6.2. Qualitative Findings</p>",
            "id": 103,
            "page": 7,
            "text": "6.2. Qualitative Findings"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2884
                },
                {
                    "x": 1212,
                    "y": 2884
                },
                {
                    "x": 1212,
                    "y": 2985
                },
                {
                    "x": 224,
                    "y": 2985
                }
            ],
            "category": "paragraph",
            "html": "<p id='104' style='font-size:18px'>Firstly, we explore the classifier-free guidance in text-to-<br>audio synthesis. We sweep over guidance values and present</p>",
            "id": 104,
            "page": 7,
            "text": "Firstly, we explore the classifier-free guidance in text-toaudio synthesis. We sweep over guidance values and present"
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1308
                },
                {
                    "x": 2267,
                    "y": 1308
                },
                {
                    "x": 2267,
                    "y": 1604
                },
                {
                    "x": 1273,
                    "y": 1604
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='105' style='font-size:18px'>trade-off curves between CLAP and FID scores in Figure 7.<br>Consistent with the observations in Ho & Salimans (2022),<br>the choice of the classifier guidance weight could scale<br>conditional and unconditional synthesis, offering a trade-off<br>between sample faithfulness and realism with respect to the<br>conditioning text.</p>",
            "id": 105,
            "page": 7,
            "text": "trade-off curves between CLAP and FID scores in Figure 7. Consistent with the observations in Ho & Salimans (2022), the choice of the classifier guidance weight could scale conditional and unconditional synthesis, offering a trade-off between sample faithfulness and realism with respect to the conditioning text."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1631
                },
                {
                    "x": 2267,
                    "y": 1631
                },
                {
                    "x": 2267,
                    "y": 1931
                },
                {
                    "x": 1273,
                    "y": 1931
                }
            ],
            "category": "paragraph",
            "html": "<p id='106' style='font-size:18px'>For better comparison in audio inpainting, we visualize<br>different masking strategies and synthesis results in Fig-<br>ure 6. As can be seen, given the initial audio with undesired<br>content, our model correctly fills and reconstruct the audio<br>robust to different shapes of masked regions, suggesting that<br>it is capable of a high-level understanding of audio content.</p>",
            "id": 106,
            "page": 7,
            "text": "For better comparison in audio inpainting, we visualize different masking strategies and synthesis results in Figure 6. As can be seen, given the initial audio with undesired content, our model correctly fills and reconstruct the audio robust to different shapes of masked regions, suggesting that it is capable of a high-level understanding of audio content."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1955
                },
                {
                    "x": 2267,
                    "y": 1955
                },
                {
                    "x": 2267,
                    "y": 2357
                },
                {
                    "x": 1273,
                    "y": 2357
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='107' style='font-size:20px'>On the personalized text-to-audio generation, we explore<br>different to E (0, 1) to add Gaussian noise and conduct<br>reverse sampling. As shown in Figure 5, a trade-off between<br>faithfulness (measured by CLAP score) and realism (mea-<br>sured by 1-MSE distance) could be witnessed. We find that<br>to E [0.2, 0.5] works well for faithful guidance with realistic<br>generation, suggesting that audio variants (e.g., speed, tim-<br>bre, and energy) could be easily destroyed as to increases.</p>",
            "id": 107,
            "page": 7,
            "text": "On the personalized text-to-audio generation, we explore different to E (0, 1) to add Gaussian noise and conduct reverse sampling. As shown in Figure 5, a trade-off between faithfulness (measured by CLAP score) and realism (measured by 1-MSE distance) could be witnessed. We find that to E [0.2, 0.5] works well for faithful guidance with realistic generation, suggesting that audio variants (e.g., speed, timbre, and energy) could be easily destroyed as to increases."
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 2409
                },
                {
                    "x": 1888,
                    "y": 2409
                },
                {
                    "x": 1888,
                    "y": 2457
                },
                {
                    "x": 1276,
                    "y": 2457
                }
            ],
            "category": "paragraph",
            "html": "<p id='108' style='font-size:20px'>6.3. Analysis and Ablation Studies</p>",
            "id": 108,
            "page": 7,
            "text": "6.3. Analysis and Ablation Studies"
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2485
                },
                {
                    "x": 2267,
                    "y": 2485
                },
                {
                    "x": 2267,
                    "y": 2737
                },
                {
                    "x": 1273,
                    "y": 2737
                }
            ],
            "category": "paragraph",
            "html": "<p id='109' style='font-size:18px'>To verify the effectiveness of several designs in Make-An-<br>Audio, including pseudo prompt enhancement, textual and<br>audio representation, we conduct ablation studies and dis-<br>cuss the key findings as follows. More analysis on audio<br>representation has been attached in Appendix E.1.</p>",
            "id": 109,
            "page": 7,
            "text": "To verify the effectiveness of several designs in Make-AnAudio, including pseudo prompt enhancement, textual and audio representation, we conduct ablation studies and discuss the key findings as follows. More analysis on audio representation has been attached in Appendix E.1."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 2785
                },
                {
                    "x": 1900,
                    "y": 2785
                },
                {
                    "x": 1900,
                    "y": 2830
                },
                {
                    "x": 1275,
                    "y": 2830
                }
            ],
            "category": "paragraph",
            "html": "<p id='110' style='font-size:18px'>6.3.1. TEXTUAL REPRESENTATION</p>",
            "id": 110,
            "page": 7,
            "text": "6.3.1. TEXTUAL REPRESENTATION"
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 2860
                },
                {
                    "x": 2268,
                    "y": 2860
                },
                {
                    "x": 2268,
                    "y": 2962
                },
                {
                    "x": 1275,
                    "y": 2962
                }
            ],
            "category": "paragraph",
            "html": "<p id='111' style='font-size:18px'>We explore several pretrained text encoders, including lan-<br>guage models BERT (Devlin et al., 2018), T5-Large (Raf-</p>",
            "id": 111,
            "page": 7,
            "text": "We explore several pretrained text encoders, including language models BERT (Devlin , 2018), T5-Large (Raf-"
        },
        {
            "bounding_box": [
                {
                    "x": 550,
                    "y": 189
                },
                {
                    "x": 1934,
                    "y": 189
                },
                {
                    "x": 1934,
                    "y": 237
                },
                {
                    "x": 550,
                    "y": 237
                }
            ],
            "category": "header",
            "html": "<header id='112' style='font-size:20px'>Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models</header>",
            "id": 112,
            "page": 8,
            "text": "Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models"
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 284
                },
                {
                    "x": 2255,
                    "y": 284
                },
                {
                    "x": 2255,
                    "y": 730
                },
                {
                    "x": 222,
                    "y": 730
                }
            ],
            "category": "figure",
            "html": "<figure><img id='113' style='font-size:14px' alt=\"0.990 Better align with initial audio (More realistic) Better align with prompt (More faithful)\n0.50\n0.988\n0.45\n0.986\n0.40\nScore\n0.984 MSE\n0.35\nCLAP\n,\n1\n0.982\n0.30\n0.980\n0.25\nFaithfulness: CLAP Score\nRealism: 1 - MSE Distance 0.978\n0.20\n0.0 0.2 0.4 0.6 0.8 1.0 to = 0.0 to = 0.2 to = 0.4 to = 0.6\" data-coord=\"top-left:(222,284); bottom-right:(2255,730)\" /></figure>",
            "id": 113,
            "page": 8,
            "text": "0.990 Better align with initial audio (More realistic) Better align with prompt (More faithful) 0.50 0.988 0.45 0.986 0.40 Score 0.984 MSE 0.35 CLAP , 1 0.982 0.30 0.980 0.25 Faithfulness: CLAP Score Realism: 1 - MSE Distance 0.978 0.20 0.0 0.2 0.4 0.6 0.8 1.0 to = 0.0 to = 0.2 to = 0.4 to = 0.6"
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 756
                },
                {
                    "x": 2265,
                    "y": 756
                },
                {
                    "x": 2265,
                    "y": 897
                },
                {
                    "x": 222,
                    "y": 897
                }
            ],
            "category": "caption",
            "html": "<caption id='114' style='font-size:16px'>Figure 5. We illustrate personalized text-to-audio results with various to initializations. to = 0 indicates the initial audio itself, whereas<br>to = 1 indicates a text-to-audio synthesis from scratch. For comparison, realism is measured by the 1-MSE distance between generated<br>and initial audio, and faithfulness is measured by the CLAP score between the generated sample. Prompt: A clock ticktocks.</caption>",
            "id": 114,
            "page": 8,
            "text": "Figure 5. We illustrate personalized text-to-audio results with various to initializations. to = 0 indicates the initial audio itself, whereas to = 1 indicates a text-to-audio synthesis from scratch. For comparison, realism is measured by the 1-MSE distance between generated and initial audio, and faithfulness is measured by the CLAP score between the generated sample. Prompt: A clock ticktocks."
        },
        {
            "bounding_box": [
                {
                    "x": 240,
                    "y": 972
                },
                {
                    "x": 1253,
                    "y": 972
                },
                {
                    "x": 1253,
                    "y": 1570
                },
                {
                    "x": 240,
                    "y": 1570
                }
            ],
            "category": "figure",
            "html": "<figure><img id='115' style='font-size:16px' alt=\"Frame\n\nInput\nIrregular (Thin)\nResult Irregular (Medium)\nIrregular (Thick)\nGT\n(a) Sample 1 (b) Sample 2 (c) Masking Strategy\" data-coord=\"top-left:(240,972); bottom-right:(1253,1570)\" /></figure>",
            "id": 115,
            "page": 8,
            "text": "Frame  Input Irregular (Thin) Result Irregular (Medium) Irregular (Thick) GT (a) Sample 1 (b) Sample 2 (c) Masking Strategy"
        },
        {
            "bounding_box": [
                {
                    "x": 299,
                    "y": 1605
                },
                {
                    "x": 1135,
                    "y": 1605
                },
                {
                    "x": 1135,
                    "y": 1654
                },
                {
                    "x": 299,
                    "y": 1654
                }
            ],
            "category": "caption",
            "html": "<caption id='116' style='font-size:18px'>Figure 6. Qualitative results with our inpainting model.</caption>",
            "id": 116,
            "page": 8,
            "text": "Figure 6. Qualitative results with our inpainting model."
        },
        {
            "bounding_box": [
                {
                    "x": 353,
                    "y": 1711
                },
                {
                    "x": 1067,
                    "y": 1711
                },
                {
                    "x": 1067,
                    "y": 2247
                },
                {
                    "x": 353,
                    "y": 2247
                }
            ],
            "category": "figure",
            "html": "<figure><img id='117' style='font-size:14px' alt=\"5.8\n5.6\n5.4\nFID\n5.2\n5.0\n4.8\n0.44 0.46 0.48 0.50 0.52 0.54\nCLAP Score\" data-coord=\"top-left:(353,1711); bottom-right:(1067,2247)\" /></figure>",
            "id": 117,
            "page": 8,
            "text": "5.8 5.6 5.4 FID 5.2 5.0 4.8 0.44 0.46 0.48 0.50 0.52 0.54 CLAP Score"
        },
        {
            "bounding_box": [
                {
                    "x": 333,
                    "y": 2285
                },
                {
                    "x": 1103,
                    "y": 2285
                },
                {
                    "x": 1103,
                    "y": 2331
                },
                {
                    "x": 333,
                    "y": 2331
                }
            ],
            "category": "caption",
            "html": "<caption id='118' style='font-size:18px'>Figure 7. Classifier-free guidance trade-off curves.</caption>",
            "id": 118,
            "page": 8,
            "text": "Figure 7. Classifier-free guidance trade-off curves."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2390
                },
                {
                    "x": 1218,
                    "y": 2390
                },
                {
                    "x": 1218,
                    "y": 2995
                },
                {
                    "x": 224,
                    "y": 2995
                }
            ],
            "category": "paragraph",
            "html": "<p id='119' style='font-size:18px'>fel et al., 2020), as well as the multimodal contrastive<br>pre-trained encoder CLIP (Radford et al., 2021) and<br>CLAP (Elizalde et al., 2022). We freeze the weights of<br>text encoders for T2A generation. For easy comparison, we<br>present the results in Table 1 and have the following obser-<br>vations: 1) Since CLIP is introduced as a scalable approach<br>for learning joint representations between text and images,<br>it could be less useful in deriving semantic representation<br>for T2A in contrast to Yang et al. (2022). 2) CLAP and T5-<br>Large achieve similar performances on benchmarks dataset,<br>while CLAP could be more computationally efficient (with<br>only %59 params), without the need for offline computation</p>",
            "id": 119,
            "page": 8,
            "text": "fel , 2020), as well as the multimodal contrastive pre-trained encoder CLIP (Radford , 2021) and CLAP (Elizalde , 2022). We freeze the weights of text encoders for T2A generation. For easy comparison, we present the results in Table 1 and have the following observations: 1) Since CLIP is introduced as a scalable approach for learning joint representations between text and images, it could be less useful in deriving semantic representation for T2A in contrast to Yang  (2022). 2) CLAP and T5Large achieve similar performances on benchmarks dataset, while CLAP could be more computationally efficient (with only %59 params), without the need for offline computation"
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 955
                },
                {
                    "x": 2064,
                    "y": 955
                },
                {
                    "x": 2064,
                    "y": 1005
                },
                {
                    "x": 1271,
                    "y": 1005
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='120' style='font-size:20px'>of embeddings in large-scale language models.</p>",
            "id": 120,
            "page": 8,
            "text": "of embeddings in large-scale language models."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 1053
                },
                {
                    "x": 1999,
                    "y": 1053
                },
                {
                    "x": 1999,
                    "y": 1103
                },
                {
                    "x": 1274,
                    "y": 1103
                }
            ],
            "category": "paragraph",
            "html": "<p id='121' style='font-size:18px'>6.3.2. PSEUDO PROMPT ENHANCEMENT</p>",
            "id": 121,
            "page": 8,
            "text": "6.3.2. PSEUDO PROMPT ENHANCEMENT"
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 1131
                },
                {
                    "x": 2267,
                    "y": 1131
                },
                {
                    "x": 2267,
                    "y": 1482
                },
                {
                    "x": 1270,
                    "y": 1482
                }
            ],
            "category": "paragraph",
            "html": "<p id='122' style='font-size:20px'>Our prompt enhancement approach alleviates the issue of<br>data scarcity, which consists of two stages with a distill-then-<br>reprogram approach. As shown in Table 5 in Appendix A,<br>we calculate and compare the prompt-audio faithfulness av-<br>eraged across datasets: The joint expert distillation produces<br>high-quality captions aligned well with audio, and suggests<br>strong generalization to diverse audio domains.</p>",
            "id": 122,
            "page": 8,
            "text": "Our prompt enhancement approach alleviates the issue of data scarcity, which consists of two stages with a distill-thenreprogram approach. As shown in Table 5 in Appendix A, we calculate and compare the prompt-audio faithfulness averaged across datasets: The joint expert distillation produces high-quality captions aligned well with audio, and suggests strong generalization to diverse audio domains."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1504
                },
                {
                    "x": 2267,
                    "y": 1504
                },
                {
                    "x": 2267,
                    "y": 2004
                },
                {
                    "x": 1273,
                    "y": 2004
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='123' style='font-size:20px'>To highlight the effectiveness of the proposed dynamic re-<br>programming strategy to create unseen object compositions,<br>we additionally train our Make-An-Audio in the static train-<br>ing dataset, and attach the results in Table 7 in Appendix E:<br>1) Removing the dynamic reprogramming approach results<br>in a slight drop in evaluation; 2) When migrating to a more<br>challenging scenario to Clotho in a zero-shot fashion, a sig-<br>nificant degradation could be witnessed, demonstrating its<br>effectiveness in constructing diverse object compositions<br>for better generalization.</p>",
            "id": 123,
            "page": 8,
            "text": "To highlight the effectiveness of the proposed dynamic reprogramming strategy to create unseen object compositions, we additionally train our Make-An-Audio in the static training dataset, and attach the results in Table 7 in Appendix E: 1) Removing the dynamic reprogramming approach results in a slight drop in evaluation; 2) When migrating to a more challenging scenario to Clotho in a zero-shot fashion, a significant degradation could be witnessed, demonstrating its effectiveness in constructing diverse object compositions for better generalization."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 2065
                },
                {
                    "x": 1572,
                    "y": 2065
                },
                {
                    "x": 1572,
                    "y": 2121
                },
                {
                    "x": 1275,
                    "y": 2121
                }
            ],
            "category": "paragraph",
            "html": "<p id='124' style='font-size:22px'>7. Conclusion</p>",
            "id": 124,
            "page": 8,
            "text": "7. Conclusion"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 2153
                },
                {
                    "x": 2268,
                    "y": 2153
                },
                {
                    "x": 2268,
                    "y": 2950
                },
                {
                    "x": 1268,
                    "y": 2950
                }
            ],
            "category": "paragraph",
            "html": "<p id='125' style='font-size:20px'>In this work, we presented Make-An-Audio with a prompt-<br>enhanced diffusion model for text-to-audio generation.<br>Leveraging the prompt enhancement with the distill-then-<br>reprogram approach, Make-An-Audio was endowed with<br>various concept compositions with orders of magnitude un-<br>supervised data. We investigated textual representation and<br>emphasized the advantages of contrastive pre-training for<br>a deep understanding of natural languages with computa-<br>tional efficiency. Both objective and subjective evaluation<br>demonstrated that Make-An-Audio achieved new state-of-<br>the-art results in text-to-audio with realistic and faithful<br>synthesis. Make-An-Audio was the first attempt to gener-<br>ate high-definition, high-fidelity audio given a user-defined<br>modality input, opening up a host of applications for person-<br>alized transfer and fine-grained control. We envisage that<br>our work serve as a basis for future audio synthesis studies.</p>",
            "id": 125,
            "page": 8,
            "text": "In this work, we presented Make-An-Audio with a promptenhanced diffusion model for text-to-audio generation. Leveraging the prompt enhancement with the distill-thenreprogram approach, Make-An-Audio was endowed with various concept compositions with orders of magnitude unsupervised data. We investigated textual representation and emphasized the advantages of contrastive pre-training for a deep understanding of natural languages with computational efficiency. Both objective and subjective evaluation demonstrated that Make-An-Audio achieved new state-ofthe-art results in text-to-audio with realistic and faithful synthesis. Make-An-Audio was the first attempt to generate high-definition, high-fidelity audio given a user-defined modality input, opening up a host of applications for personalized transfer and fine-grained control. We envisage that our work serve as a basis for future audio synthesis studies."
        },
        {
            "bounding_box": [
                {
                    "x": 553,
                    "y": 190
                },
                {
                    "x": 1933,
                    "y": 190
                },
                {
                    "x": 1933,
                    "y": 236
                },
                {
                    "x": 553,
                    "y": 236
                }
            ],
            "category": "header",
            "html": "<header id='126' style='font-size:14px'>Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models</header>",
            "id": 126,
            "page": 9,
            "text": "Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models"
        },
        {
            "bounding_box": [
                {
                    "x": 226,
                    "y": 277
                },
                {
                    "x": 468,
                    "y": 277
                },
                {
                    "x": 468,
                    "y": 332
                },
                {
                    "x": 226,
                    "y": 332
                }
            ],
            "category": "paragraph",
            "html": "<p id='127' style='font-size:22px'>References</p>",
            "id": 127,
            "page": 9,
            "text": "References"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 358
                },
                {
                    "x": 1216,
                    "y": 358
                },
                {
                    "x": 1216,
                    "y": 556
                },
                {
                    "x": 224,
                    "y": 556
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='128' style='font-size:18px'>Baevski, A., Zhou, Y., Mohamed, A., and Auli, M. wav2vec<br>2.0: A framework for self-supervised learning of speech<br>representations. Advances in Neural Information Process-<br>ing Systems, 33, 2020.</p>",
            "id": 128,
            "page": 9,
            "text": "Baevski, A., Zhou, Y., Mohamed, A., and Auli, M. wav2vec 2.0: A framework for self-supervised learning of speech representations. Advances in Neural Information Processing Systems, 33, 2020."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 593
                },
                {
                    "x": 1216,
                    "y": 593
                },
                {
                    "x": 1216,
                    "y": 792
                },
                {
                    "x": 224,
                    "y": 792
                }
            ],
            "category": "paragraph",
            "html": "<p id='129' style='font-size:18px'>Baevski, A., Hsu, W.-N., Xu, Q., Babu, A., Gu, J., and Auli,<br>M. Data2vec: A general framework for self-supervised<br>learning in speech, vision and language. arXiv preprint<br>arXiv:2202.03555, 2022.</p>",
            "id": 129,
            "page": 9,
            "text": "Baevski, A., Hsu, W.-N., Xu, Q., Babu, A., Gu, J., and Auli, M. Data2vec: A general framework for self-supervised learning in speech, vision and language. arXiv preprint arXiv:2202.03555, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 828
                },
                {
                    "x": 1216,
                    "y": 828
                },
                {
                    "x": 1216,
                    "y": 978
                },
                {
                    "x": 224,
                    "y": 978
                }
            ],
            "category": "paragraph",
            "html": "<p id='130' style='font-size:18px'>Benhamdi, S., Babouri, A., and Chiky, R. Personalized rec-<br>ommender system for e-learning environment. Education<br>and Information Technologies, 22(4):1455-1477, 2017.</p>",
            "id": 130,
            "page": 9,
            "text": "Benhamdi, S., Babouri, A., and Chiky, R. Personalized recommender system for e-learning environment. Education and Information Technologies, 22(4):1455-1477, 2017."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1013
                },
                {
                    "x": 1216,
                    "y": 1013
                },
                {
                    "x": 1216,
                    "y": 1212
                },
                {
                    "x": 224,
                    "y": 1212
                }
            ],
            "category": "paragraph",
            "html": "<p id='131' style='font-size:16px'>Bittner, R. M., Salamon, J., Tierney, M., Mauch, M., Can-<br>nam, C., and Bello, J. P. Medleydb: A multitrack dataset<br>for annotation-intensive mir research. In ISMIR, vol-<br>ume 14, pp. 155-160, 2014.</p>",
            "id": 131,
            "page": 9,
            "text": "Bittner, R. M., Salamon, J., Tierney, M., Mauch, M., Cannam, C., and Bello, J. P. Medleydb: A multitrack dataset for annotation-intensive mir research. In ISMIR, volume 14, pp. 155-160, 2014."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 1248
                },
                {
                    "x": 1217,
                    "y": 1248
                },
                {
                    "x": 1217,
                    "y": 1497
                },
                {
                    "x": 225,
                    "y": 1497
                }
            ],
            "category": "paragraph",
            "html": "<p id='132' style='font-size:20px'>Borsos, Z., Marinier, R., Vincent, D., Kharitonov, E.,<br>Pietquin, 0., Sharifi, M., Teboul, 0., Grangier, D.,<br>Tagliasacchi, M., and Zeghidour, N. Audiolm: a lan-<br>guage modeling approach to audio generation. arXiv<br>preprint arXiv:2209.03143, 2022.</p>",
            "id": 132,
            "page": 9,
            "text": "Borsos, Z., Marinier, R., Vincent, D., Kharitonov, E., Pietquin, 0., Sharifi, M., Teboul, 0., Grangier, D., Tagliasacchi, M., and Zeghidour, N. Audiolm: a language modeling approach to audio generation. arXiv preprint arXiv:2209.03143, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 226,
                    "y": 1533
                },
                {
                    "x": 1217,
                    "y": 1533
                },
                {
                    "x": 1217,
                    "y": 1779
                },
                {
                    "x": 226,
                    "y": 1779
                }
            ],
            "category": "paragraph",
            "html": "<p id='133' style='font-size:18px'>Chen, H., Xie, W., Vedaldi, A., and Zisserman, A. Vg-<br>gsound: A large-scale audio-visual dataset. In ICASSP<br>2020-2020 IEEE International Conference on Acoustics,<br>Speech and Signal Processing (ICASSP), pp. 721-725.<br>IEEE, 2020a.</p>",
            "id": 133,
            "page": 9,
            "text": "Chen, H., Xie, W., Vedaldi, A., and Zisserman, A. Vggsound: A large-scale audio-visual dataset. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 721-725. IEEE, 2020a."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1818
                },
                {
                    "x": 1216,
                    "y": 1818
                },
                {
                    "x": 1216,
                    "y": 2014
                },
                {
                    "x": 224,
                    "y": 2014
                }
            ],
            "category": "paragraph",
            "html": "<p id='134' style='font-size:18px'>Chen, M., Tan, X., Li, B., Liu, Y., Qin, T., Liu, T.-Y., et al.<br>Adaspeech: Adaptive text to speech for custom voice. In<br>International Conference on Learning Representations,<br>2020b.</p>",
            "id": 134,
            "page": 9,
            "text": "Chen, M., Tan, X., Li, B., Liu, Y., Qin, T., Liu, T.-Y.,  Adaspeech: Adaptive text to speech for custom voice. In International Conference on Learning Representations, 2020b."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2053
                },
                {
                    "x": 1216,
                    "y": 2053
                },
                {
                    "x": 1216,
                    "y": 2199
                },
                {
                    "x": 224,
                    "y": 2199
                }
            ],
            "category": "paragraph",
            "html": "<p id='135' style='font-size:16px'>Deshmukh, S., Elizalde, B., and Wang, H. Audio re-<br>trieval with wavtext5k and clap training. arXiv preprint<br>arXiv:2209.14275, 2022.</p>",
            "id": 135,
            "page": 9,
            "text": "Deshmukh, S., Elizalde, B., and Wang, H. Audio retrieval with wavtext5k and clap training. arXiv preprint arXiv:2209.14275, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2238
                },
                {
                    "x": 1217,
                    "y": 2238
                },
                {
                    "x": 1217,
                    "y": 2434
                },
                {
                    "x": 224,
                    "y": 2434
                }
            ],
            "category": "paragraph",
            "html": "<p id='136' style='font-size:18px'>Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:<br>Pre-training of deep bidirectional transformers for lan-<br>guage understanding. arXiv preprint arXiv:1810.04805,<br>2018.</p>",
            "id": 136,
            "page": 9,
            "text": "Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2472
                },
                {
                    "x": 1214,
                    "y": 2472
                },
                {
                    "x": 1214,
                    "y": 2572
                },
                {
                    "x": 224,
                    "y": 2572
                }
            ],
            "category": "paragraph",
            "html": "<p id='137' style='font-size:14px'>Dhariwal, P. and Nichol, A. Diffusion models beat gans on<br>image synthesis. In Proc. ofNeurIPS, volume 34, 2021.</p>",
            "id": 137,
            "page": 9,
            "text": "Dhariwal, P. and Nichol, A. Diffusion models beat gans on image synthesis. In Proc. ofNeurIPS, volume 34, 2021."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2607
                },
                {
                    "x": 1213,
                    "y": 2607
                },
                {
                    "x": 1213,
                    "y": 2758
                },
                {
                    "x": 224,
                    "y": 2758
                }
            ],
            "category": "paragraph",
            "html": "<p id='138' style='font-size:18px'>Ding, M., Zheng, W., Hong, W., and Tang, J. Cogview2:<br>Faster and better text-to-image generation via hierarchical<br>transformers. arXiv preprint arXiv:2204.14217, 2022.</p>",
            "id": 138,
            "page": 9,
            "text": "Ding, M., Zheng, W., Hong, W., and Tang, J. Cogview2: Faster and better text-to-image generation via hierarchical transformers. arXiv preprint arXiv:2204.14217, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2792
                },
                {
                    "x": 1215,
                    "y": 2792
                },
                {
                    "x": 1215,
                    "y": 2993
                },
                {
                    "x": 224,
                    "y": 2993
                }
            ],
            "category": "paragraph",
            "html": "<p id='139' style='font-size:16px'>Drossos, K., Lipping, S., and Virtanen, T. Clotho: An audio<br>captioning dataset. In ICASSP 2020-2020 IEEE Inter-<br>national Conference on Acoustics, Speech and Signal<br>Processing (ICASSP), pp. 736-740. IEEE, 2020.</p>",
            "id": 139,
            "page": 9,
            "text": "Drossos, K., Lipping, S., and Virtanen, T. Clotho: An audio captioning dataset. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 736-740. IEEE, 2020."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 282
                },
                {
                    "x": 2266,
                    "y": 282
                },
                {
                    "x": 2266,
                    "y": 433
                },
                {
                    "x": 1271,
                    "y": 433
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='140' style='font-size:20px'>Elizalde, B., Deshmukh, S., Ismail, M. A., and Wang, H.<br>Clap: Learning audio concepts from natural language<br>supervision. arXiv preprint arXiv:2206.04769, 2022.</p>",
            "id": 140,
            "page": 9,
            "text": "Elizalde, B., Deshmukh, S., Ismail, M. A., and Wang, H. Clap: Learning audio concepts from natural language supervision. arXiv preprint arXiv:2206.04769, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 464
                },
                {
                    "x": 2267,
                    "y": 464
                },
                {
                    "x": 2267,
                    "y": 664
                },
                {
                    "x": 1271,
                    "y": 664
                }
            ],
            "category": "paragraph",
            "html": "<p id='141' style='font-size:18px'>Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano,<br>A. H., Chechik, G., and Cohen-Or, D. An image is worth<br>one word: Personalizing text-to-image generation using<br>textual inversion. arXiv preprint arXiv:2208.01618, 2022.</p>",
            "id": 141,
            "page": 9,
            "text": "Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A. H., Chechik, G., and Cohen-Or, D. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 695
                },
                {
                    "x": 2267,
                    "y": 695
                },
                {
                    "x": 2267,
                    "y": 893
                },
                {
                    "x": 1272,
                    "y": 893
                }
            ],
            "category": "paragraph",
            "html": "<p id='142' style='font-size:18px'>Gan, C., Huang, D., Chen, P., Tenenbaum, J. B., and Tor-<br>ralba, A. Foley music: Learning to generate music from<br>videos. In European Conference on Computer Vision, pp.<br>758-775. Springer, 2020.</p>",
            "id": 142,
            "page": 9,
            "text": "Gan, C., Huang, D., Chen, P., Tenenbaum, J. B., and Torralba, A. Foley music: Learning to generate music from videos. In European Conference on Computer Vision, pp. 758-775. Springer, 2020."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 925
                },
                {
                    "x": 2267,
                    "y": 925
                },
                {
                    "x": 2267,
                    "y": 1220
                },
                {
                    "x": 1272,
                    "y": 1220
                }
            ],
            "category": "paragraph",
            "html": "<p id='143' style='font-size:14px'>Gemmeke, J. F., Ellis, D. P., Freedman, D., Jansen, A.,<br>Lawrence, W., Moore, R. C., Plakal, M., and Ritter, M.<br>Audio set: An ontology and human-labeled dataset for<br>audio events. In 2017 IEEE international conference on<br>acoustics, speech and signal processing (ICASSP), pp.<br>776-780. IEEE, 2017.</p>",
            "id": 143,
            "page": 9,
            "text": "Gemmeke, J. F., Ellis, D. P., Freedman, D., Jansen, A., Lawrence, W., Moore, R. C., Plakal, M., and Ritter, M. Audio set: An ontology and human-labeled dataset for audio events. In 2017 IEEE international conference on acoustics, speech and signal processing (ICASSP), pp. 776-780. IEEE, 2017."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1252
                },
                {
                    "x": 2267,
                    "y": 1252
                },
                {
                    "x": 2267,
                    "y": 1452
                },
                {
                    "x": 1272,
                    "y": 1452
                }
            ],
            "category": "paragraph",
            "html": "<p id='144' style='font-size:20px'>Gong, Y., Lai, C.-I., Chung, Y.-A., and Glass, J. Ssast: Self-<br>supervised audio spectrogram transformer. In Proceed-<br>ings of the AAAI Conference on Artificial Intelligence,<br>volume 36, pp. 10699-10709, 2022.</p>",
            "id": 144,
            "page": 9,
            "text": "Gong, Y., Lai, C.-I., Chung, Y.-A., and Glass, J. Ssast: Selfsupervised audio spectrogram transformer. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pp. 10699-10709, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1484
                },
                {
                    "x": 2267,
                    "y": 1484
                },
                {
                    "x": 2267,
                    "y": 1680
                },
                {
                    "x": 1272,
                    "y": 1680
                }
            ],
            "category": "paragraph",
            "html": "<p id='145' style='font-size:20px'>Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,<br>Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y.<br>Generative adversarial networks. Communications of the<br>ACM, 63(11):139-144, 2020.</p>",
            "id": 145,
            "page": 9,
            "text": "Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative adversarial networks. Communications of the ACM, 63(11):139-144, 2020."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1713
                },
                {
                    "x": 2267,
                    "y": 1713
                },
                {
                    "x": 2267,
                    "y": 1913
                },
                {
                    "x": 1273,
                    "y": 1913
                }
            ],
            "category": "paragraph",
            "html": "<p id='146' style='font-size:18px'>He, K., Chen, X., Xie, S., Li, Y., Dollar, P., and Girshick,<br>R. Masked autoencoders are scalable vision learners. In<br>Proceedings of the IEEE/CVF Conference on Computer<br>Vision and Pattern Recognition, pp. 16000-16009, 2022.</p>",
            "id": 146,
            "page": 9,
            "text": "He, K., Chen, X., Xie, S., Li, Y., Dollar, P., and Girshick, R. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 16000-16009, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 1943
                },
                {
                    "x": 2267,
                    "y": 1943
                },
                {
                    "x": 2267,
                    "y": 2092
                },
                {
                    "x": 1274,
                    "y": 2092
                }
            ],
            "category": "paragraph",
            "html": "<p id='147' style='font-size:16px'>Hessel, J., Holtzman, A., Forbes, M., Bras, R. L., and Choi,<br>Y. Clipscore: A reference-free evaluation metric for im-<br>age captioning. arXiv preprint arXiv:2104.08718, 2021.</p>",
            "id": 147,
            "page": 9,
            "text": "Hessel, J., Holtzman, A., Forbes, M., Bras, R. L., and Choi, Y. Clipscore: A reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 2123
                },
                {
                    "x": 2265,
                    "y": 2123
                },
                {
                    "x": 2265,
                    "y": 2322
                },
                {
                    "x": 1274,
                    "y": 2322
                }
            ],
            "category": "paragraph",
            "html": "<p id='148' style='font-size:16px'>Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and<br>Hochreiter, S. Gans trained by a two time-scale update<br>rule converge to a local nash equilibrium. Advances in<br>neural information processing systems, 30, 2017.</p>",
            "id": 148,
            "page": 9,
            "text": "Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 2353
                },
                {
                    "x": 2268,
                    "y": 2353
                },
                {
                    "x": 2268,
                    "y": 2451
                },
                {
                    "x": 1275,
                    "y": 2451
                }
            ],
            "category": "paragraph",
            "html": "<p id='149' style='font-size:18px'>Ho, J. and Salimans, T. Classifier-free diffusion guidance.<br>arXiv preprint arXiv:2207.12598, 2022.</p>",
            "id": 149,
            "page": 9,
            "text": "Ho, J. and Salimans, T. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 2482
                },
                {
                    "x": 2268,
                    "y": 2482
                },
                {
                    "x": 2268,
                    "y": 2581
                },
                {
                    "x": 1276,
                    "y": 2581
                }
            ],
            "category": "paragraph",
            "html": "<p id='150' style='font-size:18px'>Ho, J., Jain, A., and Abbeel, P. Denoising diffusion proba-<br>bilistic models. In Proc. ofNeurIPS, 2020.</p>",
            "id": 150,
            "page": 9,
            "text": "Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. In Proc. ofNeurIPS, 2020."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 2613
                },
                {
                    "x": 2268,
                    "y": 2613
                },
                {
                    "x": 2268,
                    "y": 2808
                },
                {
                    "x": 1275,
                    "y": 2808
                }
            ],
            "category": "paragraph",
            "html": "<p id='151' style='font-size:20px'>Hong, W., Ding, M., Zheng, W., Liu, X., and Tang, J.<br>Cogvideo: Large-scale pretraining for text-to-video gener-<br>ation via transformers. arXiv preprint arXiv:2205.15868,<br>2022.</p>",
            "id": 151,
            "page": 9,
            "text": "Hong, W., Ding, M., Zheng, W., Liu, X., and Tang, J. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 2842
                },
                {
                    "x": 2268,
                    "y": 2842
                },
                {
                    "x": 2268,
                    "y": 2993
                },
                {
                    "x": 1275,
                    "y": 2993
                }
            ],
            "category": "paragraph",
            "html": "<p id='152' style='font-size:20px'>Hsu, W.-N., Harwath, D., Song, C., and Glass, J. Text-free<br>image-to-speech synthesis using learned segmental units.<br>arXiv preprint arXiv:2012.15454, 2020.</p>",
            "id": 152,
            "page": 9,
            "text": "Hsu, W.-N., Harwath, D., Song, C., and Glass, J. Text-free image-to-speech synthesis using learned segmental units. arXiv preprint arXiv:2012.15454, 2020."
        },
        {
            "bounding_box": [
                {
                    "x": 552,
                    "y": 190
                },
                {
                    "x": 1932,
                    "y": 190
                },
                {
                    "x": 1932,
                    "y": 236
                },
                {
                    "x": 552,
                    "y": 236
                }
            ],
            "category": "header",
            "html": "<header id='153' style='font-size:14px'>Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models</header>",
            "id": 153,
            "page": 10,
            "text": "Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 282
                },
                {
                    "x": 1215,
                    "y": 282
                },
                {
                    "x": 1215,
                    "y": 580
                },
                {
                    "x": 223,
                    "y": 580
                }
            ],
            "category": "paragraph",
            "html": "<p id='154' style='font-size:18px'>Hsu, W.-N., Bolte, B., Tsai, Y.-H. H., Lakhotia, K.,<br>Salakhutdinov, R., and Mohamed, A. Hubert: Self-<br>supervised speech representation learning by masked<br>prediction of hidden units. IEEE/ACM Transactions on<br>Audio, Speech, and Language Processing, 29:3451-3460,<br>2021.</p>",
            "id": 154,
            "page": 10,
            "text": "Hsu, W.-N., Bolte, B., Tsai, Y.-H. H., Lakhotia, K., Salakhutdinov, R., and Mohamed, A. Hubert: Selfsupervised speech representation learning by masked prediction of hidden units. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29:3451-3460, 2021."
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 614
                },
                {
                    "x": 1216,
                    "y": 614
                },
                {
                    "x": 1216,
                    "y": 814
                },
                {
                    "x": 222,
                    "y": 814
                }
            ],
            "category": "paragraph",
            "html": "<p id='155' style='font-size:18px'>Huang, R., Ren, Y., Liu, J., Cui, C., and Zhao, Z. Gen-<br>erspeech: Towards style transfer for generalizable out-<br>of-domain text-to-speech synthesis. arXiv preprint<br>arXiv:2205.07211, 2022.</p>",
            "id": 155,
            "page": 10,
            "text": "Huang, R., Ren, Y., Liu, J., Cui, C., and Zhao, Z. Generspeech: Towards style transfer for generalizable outof-domain text-to-speech synthesis. arXiv preprint arXiv:2205.07211, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 849
                },
                {
                    "x": 1213,
                    "y": 849
                },
                {
                    "x": 1213,
                    "y": 949
                },
                {
                    "x": 224,
                    "y": 949
                }
            ],
            "category": "paragraph",
            "html": "<p id='156' style='font-size:16px'>Iashin, V. and Rahtu, E. Taming visually guided sound<br>generation. arXiv preprint arXiv:2110.08791, 2021.</p>",
            "id": 156,
            "page": 10,
            "text": "Iashin, V. and Rahtu, E. Taming visually guided sound generation. arXiv preprint arXiv:2110.08791, 2021."
        },
        {
            "bounding_box": [
                {
                    "x": 226,
                    "y": 981
                },
                {
                    "x": 1215,
                    "y": 981
                },
                {
                    "x": 1215,
                    "y": 1281
                },
                {
                    "x": 226,
                    "y": 1281
                }
            ],
            "category": "paragraph",
            "html": "<p id='157' style='font-size:18px'>Kim, C. D., Kim, B., Lee, H., and Kim, G. Audiocaps: Gen-<br>erating captions for audios in the wild. In Proceedings<br>of the 2019 Conference of the North American Chapter<br>of the Association for Computational Linguistics: Hu-<br>man Language Technologies, Volume 1 (Long and Short<br>Papers), pp. 119-132, 2019.</p>",
            "id": 157,
            "page": 10,
            "text": "Kim, C. D., Kim, B., Lee, H., and Kim, G. Audiocaps: Generating captions for audios in the wild. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 119-132, 2019."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 1314
                },
                {
                    "x": 1215,
                    "y": 1314
                },
                {
                    "x": 1215,
                    "y": 1463
                },
                {
                    "x": 225,
                    "y": 1463
                }
            ],
            "category": "paragraph",
            "html": "<p id='158' style='font-size:16px'>Kingma, D. P. and Dhariwal, P. Glow: Generative flow<br>with invertible 1x1 convolutions. Advances in Neural<br>Information Processing Systems, 31:10215-10224, 2018.</p>",
            "id": 158,
            "page": 10,
            "text": "Kingma, D. P. and Dhariwal, P. Glow: Generative flow with invertible 1x1 convolutions. Advances in Neural Information Processing Systems, 31:10215-10224, 2018."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 1497
                },
                {
                    "x": 1214,
                    "y": 1497
                },
                {
                    "x": 1214,
                    "y": 1692
                },
                {
                    "x": 225,
                    "y": 1692
                }
            ],
            "category": "paragraph",
            "html": "<p id='159' style='font-size:16px'>Koepke, A. S., Oncescu, A.-M., Henriques, J., Akata, Z.,<br>and Albanie, S. Audio retrieval with natural language<br>queries: A benchmark study. IEEE Transactions on Mul-<br>timedia, 2022.</p>",
            "id": 159,
            "page": 10,
            "text": "Koepke, A. S., Oncescu, A.-M., Henriques, J., Akata, Z., and Albanie, S. Audio retrieval with natural language queries: A benchmark study. IEEE Transactions on Multimedia, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1729
                },
                {
                    "x": 1215,
                    "y": 1729
                },
                {
                    "x": 1215,
                    "y": 1878
                },
                {
                    "x": 224,
                    "y": 1878
                }
            ],
            "category": "paragraph",
            "html": "<p id='160' style='font-size:18px'>Kong, J., Kim, J., and Bae, J. Hifi-gan: Generative ad-<br>versarial networks for efficient and high fidelity speech<br>synthesis. arXiv preprint arXiv:2010.05646, 2020.</p>",
            "id": 160,
            "page": 10,
            "text": "Kong, J., Kim, J., and Bae, J. Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis. arXiv preprint arXiv:2010.05646, 2020."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 1912
                },
                {
                    "x": 1215,
                    "y": 1912
                },
                {
                    "x": 1215,
                    "y": 2061
                },
                {
                    "x": 225,
                    "y": 2061
                }
            ],
            "category": "paragraph",
            "html": "<p id='161' style='font-size:18px'>Koutini, K., Schluter, J., Eghbal-zadeh, H., and Widmer,<br>G. Efficient training of audio transformers with patchout.<br>arXiv preprint arXiv:2110.05069, 2021.</p>",
            "id": 161,
            "page": 10,
            "text": "Koutini, K., Schluter, J., Eghbal-zadeh, H., and Widmer, G. Efficient training of audio transformers with patchout. arXiv preprint arXiv:2110.05069, 2021."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 2094
                },
                {
                    "x": 1216,
                    "y": 2094
                },
                {
                    "x": 1216,
                    "y": 2293
                },
                {
                    "x": 225,
                    "y": 2293
                }
            ],
            "category": "paragraph",
            "html": "<p id='162' style='font-size:22px'>Kreuk, F., Synnaeve, G., Polyak, A., Singer, U., Defossez,<br>A., Copet, J., Parikh, D., Taigman, Y., and Adi, Y. Audio-<br>gen: Textually guided audio generation. arXiv preprint<br>arXiv:2209.15352, 2022.</p>",
            "id": 162,
            "page": 10,
            "text": "Kreuk, F., Synnaeve, G., Polyak, A., Singer, U., Defossez, A., Copet, J., Parikh, D., Taigman, Y., and Adi, Y. Audiogen: Textually guided audio generation. arXiv preprint arXiv:2209.15352, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2328
                },
                {
                    "x": 1215,
                    "y": 2328
                },
                {
                    "x": 1215,
                    "y": 2526
                },
                {
                    "x": 223,
                    "y": 2526
                }
            ],
            "category": "paragraph",
            "html": "<p id='163' style='font-size:20px'>Liu, H., Jiang, B., Song, Y., Huang, W., and Yang, C. Re-<br>thinking image inpainting via a mutual encoder-decoder<br>with feature equalizations. In European Conference on<br>Computer Vision, pp. 725-741. Springer, 2020.</p>",
            "id": 163,
            "page": 10,
            "text": "Liu, H., Jiang, B., Song, Y., Huang, W., and Yang, C. Rethinking image inpainting via a mutual encoder-decoder with feature equalizations. In European Conference on Computer Vision, pp. 725-741. Springer, 2020."
        },
        {
            "bounding_box": [
                {
                    "x": 226,
                    "y": 2561
                },
                {
                    "x": 1215,
                    "y": 2561
                },
                {
                    "x": 1215,
                    "y": 2759
                },
                {
                    "x": 226,
                    "y": 2759
                }
            ],
            "category": "paragraph",
            "html": "<p id='164' style='font-size:18px'>Mart�n-Morato, I. and Mesaros, A. What is the ground<br>truth? reliability of multi-annotator data for audio tagging.<br>In 2021 29th European Signal Processing Conference<br>(EUSIPCO), pp. 76-80. IEEE, 2021.</p>",
            "id": 164,
            "page": 10,
            "text": "Mart�n-Morato, I. and Mesaros, A. What is the ground truth? reliability of multi-annotator data for audio tagging. In 2021 29th European Signal Processing Conference (EUSIPCO), pp. 76-80. IEEE, 2021."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 2792
                },
                {
                    "x": 1214,
                    "y": 2792
                },
                {
                    "x": 1214,
                    "y": 2993
                },
                {
                    "x": 225,
                    "y": 2993
                }
            ],
            "category": "paragraph",
            "html": "<p id='165' style='font-size:20px'>Meng, C., He, Y., Song, Y., Song, J., Wu, J., Zhu, J.-Y., and<br>Ermon, S. Sdedit: Guided image synthesis and editing<br>with stochastic differential equations. In International<br>Conference on Learning Representations, 2021.</p>",
            "id": 165,
            "page": 10,
            "text": "Meng, C., He, Y., Song, Y., Song, J., Wu, J., Zhu, J.-Y., and Ermon, S. Sdedit: Guided image synthesis and editing with stochastic differential equations. In International Conference on Learning Representations, 2021."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 283
                },
                {
                    "x": 2267,
                    "y": 283
                },
                {
                    "x": 2267,
                    "y": 480
                },
                {
                    "x": 1273,
                    "y": 480
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='166' style='font-size:20px'>Nazeri, K., Ng, E., Joseph, T., Qureshi, F. Z., and Ebrahimi,<br>M. Edgeconnect: Generative image inpainting with ad-<br>versarial edge learning. arXiv preprint arXiv:1901.00212,<br>2019.</p>",
            "id": 166,
            "page": 10,
            "text": "Nazeri, K., Ng, E., Joseph, T., Qureshi, F. Z., and Ebrahimi, M. Edgeconnect: Generative image inpainting with adversarial edge learning. arXiv preprint arXiv:1901.00212, 2019."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 519
                },
                {
                    "x": 2267,
                    "y": 519
                },
                {
                    "x": 2267,
                    "y": 768
                },
                {
                    "x": 1272,
                    "y": 768
                }
            ],
            "category": "paragraph",
            "html": "<p id='167' style='font-size:18px'>Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin,<br>P., McGrew, B., Sutskever, I., and Chen, M. Glide:<br>Towards photorealistic image generation and editing<br>with text-guided diffusion models. arXiv preprint<br>arXiv:2112.10741, 2021.</p>",
            "id": 167,
            "page": 10,
            "text": "Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., Sutskever, I., and Chen, M. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 805
                },
                {
                    "x": 2266,
                    "y": 805
                },
                {
                    "x": 2266,
                    "y": 955
                },
                {
                    "x": 1273,
                    "y": 955
                }
            ],
            "category": "paragraph",
            "html": "<p id='168' style='font-size:14px'>Piczak, K. J. Esc: Dataset for environmental sound classi-<br>fication. In Proceedings of the 23rd ACM international<br>conference on Multimedia, pp. 1015-1018, 2015.</p>",
            "id": 168,
            "page": 10,
            "text": "Piczak, K. J. Esc: Dataset for environmental sound classification. In Proceedings of the 23rd ACM international conference on Multimedia, pp. 1015-1018, 2015."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 990
                },
                {
                    "x": 2267,
                    "y": 990
                },
                {
                    "x": 2267,
                    "y": 1242
                },
                {
                    "x": 1273,
                    "y": 1242
                }
            ],
            "category": "paragraph",
            "html": "<p id='169' style='font-size:20px'>Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,<br>Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J.,<br>et al. Learning transferable visual models from natural<br>language supervision. In International Conference on<br>Machine Learning, pp. 8748-8763. PMLR, 2021.</p>",
            "id": 169,
            "page": 10,
            "text": "Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J.,  Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pp. 8748-8763. PMLR, 2021."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 1277
                },
                {
                    "x": 2267,
                    "y": 1277
                },
                {
                    "x": 2267,
                    "y": 1477
                },
                {
                    "x": 1274,
                    "y": 1477
                }
            ],
            "category": "paragraph",
            "html": "<p id='170' style='font-size:16px'>Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,<br>Matena, M., Zhou, Y., Li, W., Liu, P. J., et al. Exploring<br>the limits of transfer learning with a unified text-to-text<br>transformer. J. Mach. Learn. Res., 21(140):1-67, 2020.</p>",
            "id": 170,
            "page": 10,
            "text": "Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., Liu, P. J.,  Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1-67, 2020."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 1514
                },
                {
                    "x": 2268,
                    "y": 1514
                },
                {
                    "x": 2268,
                    "y": 1712
                },
                {
                    "x": 1275,
                    "y": 1712
                }
            ],
            "category": "paragraph",
            "html": "<p id='171' style='font-size:20px'>Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Rad-<br>ford, A., Chen, M., and Sutskever, I. Zero-shot text-<br>to-image generation. In International Conference on<br>Machine Learning, pp. 8821-8831. PMLR, 2021.</p>",
            "id": 171,
            "page": 10,
            "text": "Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and Sutskever, I. Zero-shot textto-image generation. In International Conference on Machine Learning, pp. 8821-8831. PMLR, 2021."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 1751
                },
                {
                    "x": 2267,
                    "y": 1751
                },
                {
                    "x": 2267,
                    "y": 1900
                },
                {
                    "x": 1275,
                    "y": 1900
                }
            ],
            "category": "paragraph",
            "html": "<p id='172' style='font-size:18px'>Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen,<br>M. Hierarchical text-conditional image generation with<br>clip latents. arXiv preprint arXiv:2204.06125, 2022.</p>",
            "id": 172,
            "page": 10,
            "text": "Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 1936
                },
                {
                    "x": 2267,
                    "y": 1936
                },
                {
                    "x": 2267,
                    "y": 2183
                },
                {
                    "x": 1276,
                    "y": 2183
                }
            ],
            "category": "paragraph",
            "html": "<p id='173' style='font-size:18px'>Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and<br>Ommer, B. High-resolution image synthesis with latent<br>diffusion models. In Proceedings of the IEEE/CVF Con-<br>ference on Computer Vision and Pattern Recognition, pp.<br>10684-10695, 2022.</p>",
            "id": 173,
            "page": 10,
            "text": "Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10684-10695, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 2220
                },
                {
                    "x": 2266,
                    "y": 2220
                },
                {
                    "x": 2266,
                    "y": 2468
                },
                {
                    "x": 1274,
                    "y": 2468
                }
            ],
            "category": "paragraph",
            "html": "<p id='174' style='font-size:16px'>Ronneberger, 0., Fischer, P., and Brox, T. U-net: Convolu-<br>tional networks for biomedical image segmentation. In In-<br>ternational Conference on Medical image computing and<br>computer-assisted intervention, pp. 234-241. Springer,<br>2015.</p>",
            "id": 174,
            "page": 10,
            "text": "Ronneberger, 0., Fischer, P., and Brox, T. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pp. 234-241. Springer, 2015."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 2508
                },
                {
                    "x": 2267,
                    "y": 2508
                },
                {
                    "x": 2267,
                    "y": 2755
                },
                {
                    "x": 1274,
                    "y": 2755
                }
            ],
            "category": "paragraph",
            "html": "<p id='175' style='font-size:20px'>Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton,<br>E., Ghasemipour, S. K. S., Ayan, B. K., Mahdavi, S. S.,<br>Lopes, R. G., et al. Photorealistic text-to-image diffusion<br>models with deep language understanding. arXiv preprint<br>arXiv:2205.11487, 2022.</p>",
            "id": 175,
            "page": 10,
            "text": "Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., Ghasemipour, S. K. S., Ayan, B. K., Mahdavi, S. S., Lopes, R. G.,  Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 2793
                },
                {
                    "x": 2268,
                    "y": 2793
                },
                {
                    "x": 2268,
                    "y": 2992
                },
                {
                    "x": 1275,
                    "y": 2992
                }
            ],
            "category": "paragraph",
            "html": "<p id='176' style='font-size:14px'>Salamon, J., Jacoby, C., and Bello, J. P. A dataset and<br>taxonomy for urban sound research. In 22nd ACM Inter-<br>national Conference on Multimedia (ACM-MM'14), pp.<br>1041-1044, Orlando, FL, USA, Nov. 2014.</p>",
            "id": 176,
            "page": 10,
            "text": "Salamon, J., Jacoby, C., and Bello, J. P. A dataset and taxonomy for urban sound research. In 22nd ACM International Conference on Multimedia (ACM-MM'14), pp. 1041-1044, Orlando, FL, USA, Nov. 2014."
        },
        {
            "bounding_box": [
                {
                    "x": 551,
                    "y": 190
                },
                {
                    "x": 1932,
                    "y": 190
                },
                {
                    "x": 1932,
                    "y": 236
                },
                {
                    "x": 551,
                    "y": 236
                }
            ],
            "category": "header",
            "html": "<header id='177' style='font-size:14px'>Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models</header>",
            "id": 177,
            "page": 11,
            "text": "Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 282
                },
                {
                    "x": 1217,
                    "y": 282
                },
                {
                    "x": 1217,
                    "y": 483
                },
                {
                    "x": 223,
                    "y": 483
                }
            ],
            "category": "paragraph",
            "html": "<p id='178' style='font-size:20px'>Singer, U., Polyak, A., Hayes, T., Yin, X., An, J., Zhang, S.,<br>Hu, Q., Yang, H., Ashual, 0., Gafni, 0., et al. Make-a-<br>video: Text-to-video generation without text-video data.<br>arXiv preprint arXiv:2209.14792, 2022.</p>",
            "id": 178,
            "page": 11,
            "text": "Singer, U., Polyak, A., Hayes, T., Yin, X., An, J., Zhang, S., Hu, Q., Yang, H., Ashual, 0., Gafni, 0.,  Make-avideo: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 515
                },
                {
                    "x": 1215,
                    "y": 515
                },
                {
                    "x": 1215,
                    "y": 617
                },
                {
                    "x": 222,
                    "y": 617
                }
            ],
            "category": "paragraph",
            "html": "<p id='179' style='font-size:20px'>Song, J., Meng, C., and Ermon, S. Denoising diffusion<br>implicit models. In Proc. of ICLR, 2020.</p>",
            "id": 179,
            "page": 11,
            "text": "Song, J., Meng, C., and Ermon, S. Denoising diffusion implicit models. In Proc. of ICLR, 2020."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 648
                },
                {
                    "x": 1216,
                    "y": 648
                },
                {
                    "x": 1216,
                    "y": 799
                },
                {
                    "x": 223,
                    "y": 799
                }
            ],
            "category": "paragraph",
            "html": "<p id='180' style='font-size:16px'>Su, K., Liu, X., and Shlizerman, E. Audeo: Audio genera-<br>tion for a silent performance video. Advances in Neural<br>Information Processing Systems, 33:3325-3337, 2020.</p>",
            "id": 180,
            "page": 11,
            "text": "Su, K., Liu, X., and Shlizerman, E. Audeo: Audio generation for a silent performance video. Advances in Neural Information Processing Systems, 33:3325-3337, 2020."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 832
                },
                {
                    "x": 1216,
                    "y": 832
                },
                {
                    "x": 1216,
                    "y": 1130
                },
                {
                    "x": 224,
                    "y": 1130
                }
            ],
            "category": "paragraph",
            "html": "<p id='181' style='font-size:20px'>Suvorov, R., Logacheva, E., Mashikhin, A., Remizova, A.,<br>Ashukha, A., Silvestrov, A., Kong, N., Goka, H., Park,<br>K., and Lempitsky, V. Resolution-robust large mask in-<br>painting with fourier convolutions. In Proceedings of the<br>IEEE/CVF Winter Conference on Applications of Com-<br>puter Vision, pp. 2149-2159, 2022.</p>",
            "id": 181,
            "page": 11,
            "text": "Suvorov, R., Logacheva, E., Mashikhin, A., Remizova, A., Ashukha, A., Silvestrov, A., Kong, N., Goka, H., Park, K., and Lempitsky, V. Resolution-robust large mask inpainting with fourier convolutions. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 2149-2159, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1163
                },
                {
                    "x": 1211,
                    "y": 1163
                },
                {
                    "x": 1211,
                    "y": 1264
                },
                {
                    "x": 223,
                    "y": 1264
                }
            ],
            "category": "paragraph",
            "html": "<p id='182' style='font-size:22px'>Taylor, P. Text-to-speech synthesis. Cambridge university<br>press, 2009.</p>",
            "id": 182,
            "page": 11,
            "text": "Taylor, P. Text-to-speech synthesis. Cambridge university press, 2009."
        },
        {
            "bounding_box": [
                {
                    "x": 226,
                    "y": 1297
                },
                {
                    "x": 1216,
                    "y": 1297
                },
                {
                    "x": 1216,
                    "y": 1445
                },
                {
                    "x": 226,
                    "y": 1445
                }
            ],
            "category": "paragraph",
            "html": "<p id='183' style='font-size:18px'>Van Den Oord, A., Vinyals, 0., et al. Neural discrete rep-<br>resentation learning. Advances in neural information<br>processing systems, 30, 2017.</p>",
            "id": 183,
            "page": 11,
            "text": "Van Den Oord, A., Vinyals, 0.,  Neural discrete representation learning. Advances in neural information processing systems, 30, 2017."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1479
                },
                {
                    "x": 1216,
                    "y": 1479
                },
                {
                    "x": 1216,
                    "y": 1628
                },
                {
                    "x": 224,
                    "y": 1628
                }
            ],
            "category": "paragraph",
            "html": "<p id='184' style='font-size:20px'>Xu, H., Li, J., Baevski, A., Auli, M., Galuba, W., Metze, F.,<br>Feichtenhofer, C., et al. Masked autoencoders that listen.<br>arXiv preprint arXiv:2207.06405, 2022.</p>",
            "id": 184,
            "page": 11,
            "text": "Xu, H., Li, J., Baevski, A., Auli, M., Galuba, W., Metze, F., Feichtenhofer, C.,  Masked autoencoders that listen. arXiv preprint arXiv:2207.06405, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1662
                },
                {
                    "x": 1215,
                    "y": 1662
                },
                {
                    "x": 1215,
                    "y": 1811
                },
                {
                    "x": 223,
                    "y": 1811
                }
            ],
            "category": "paragraph",
            "html": "<p id='185' style='font-size:18px'>Xu, X., Dinkel, H., Wu, M., and Yu, K. A crnn-gru based<br>reinforcement learning approach to audio captioning. In<br>DCASE, pp. 225-229, 2020.</p>",
            "id": 185,
            "page": 11,
            "text": "Xu, X., Dinkel, H., Wu, M., and Yu, K. A crnn-gru based reinforcement learning approach to audio captioning. In DCASE, pp. 225-229, 2020."
        },
        {
            "bounding_box": [
                {
                    "x": 226,
                    "y": 1845
                },
                {
                    "x": 1214,
                    "y": 1845
                },
                {
                    "x": 1214,
                    "y": 2041
                },
                {
                    "x": 226,
                    "y": 2041
                }
            ],
            "category": "paragraph",
            "html": "<p id='186' style='font-size:20px'>Yang, D., Yu, J., Wang, H., Wang, W., Weng, C., Zou, Y.,<br>and Yu, D. Diffsound: Discrete diffusion model for text-<br>to-sound generation. arXiv preprint arXiv:2207.09983,<br>2022.</p>",
            "id": 186,
            "page": 11,
            "text": "Yang, D., Yu, J., Wang, H., Wang, W., Weng, C., Zou, Y., and Yu, D. Diffsound: Discrete diffusion model for textto-sound generation. arXiv preprint arXiv:2207.09983, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2076
                },
                {
                    "x": 1216,
                    "y": 2076
                },
                {
                    "x": 1216,
                    "y": 2277
                },
                {
                    "x": 223,
                    "y": 2277
                }
            ],
            "category": "paragraph",
            "html": "<p id='187' style='font-size:18px'>Zeghidour, N., Luebs, A., Omran, A., Skoglund, J., and<br>Tagliasacchi, M. Soundstream: An end-to-end neural<br>audio codec. IEEE/ACM Transactions on Audio, Speech,<br>and Language Processing, 30:495-507, 2021.</p>",
            "id": 187,
            "page": 11,
            "text": "Zeghidour, N., Luebs, A., Omran, A., Skoglund, J., and Tagliasacchi, M. Soundstream: An end-to-end neural audio codec. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 30:495-507, 2021."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2309
                },
                {
                    "x": 1217,
                    "y": 2309
                },
                {
                    "x": 1217,
                    "y": 2509
                },
                {
                    "x": 223,
                    "y": 2509
                }
            ],
            "category": "paragraph",
            "html": "<p id='188' style='font-size:20px'>Zen, H., Dang, V., Clark, R., Zhang, Y., Weiss, R. J.,<br>Jia, Y., Chen, Z., and Wu, Y. Libritts: A corpus de-<br>rived from librispeech for text-to-speech. arXiv preprint<br>arXiv:1904.02882, 2019.</p>",
            "id": 188,
            "page": 11,
            "text": "Zen, H., Dang, V., Clark, R., Zhang, Y., Weiss, R. J., Jia, Y., Chen, Z., and Wu, Y. Libritts: A corpus derived from librispeech for text-to-speech. arXiv preprint arXiv:1904.02882, 2019."
        },
        {
            "bounding_box": [
                {
                    "x": 553,
                    "y": 190
                },
                {
                    "x": 1931,
                    "y": 190
                },
                {
                    "x": 1931,
                    "y": 236
                },
                {
                    "x": 553,
                    "y": 236
                }
            ],
            "category": "header",
            "html": "<header id='189' style='font-size:18px'>Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models</header>",
            "id": 189,
            "page": 12,
            "text": "Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models"
        },
        {
            "bounding_box": [
                {
                    "x": 1060,
                    "y": 272
                },
                {
                    "x": 1426,
                    "y": 272
                },
                {
                    "x": 1426,
                    "y": 345
                },
                {
                    "x": 1060,
                    "y": 345
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='190' style='font-size:22px'>Appendices</p>",
            "id": 190,
            "page": 12,
            "text": "Appendices"
        },
        {
            "bounding_box": [
                {
                    "x": 240,
                    "y": 415
                },
                {
                    "x": 2245,
                    "y": 415
                },
                {
                    "x": 2245,
                    "y": 534
                },
                {
                    "x": 240,
                    "y": 534
                }
            ],
            "category": "paragraph",
            "html": "<p id='191' style='font-size:20px'>Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion<br>Models</p>",
            "id": 191,
            "page": 12,
            "text": "Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models"
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 647
                },
                {
                    "x": 912,
                    "y": 647
                },
                {
                    "x": 912,
                    "y": 703
                },
                {
                    "x": 225,
                    "y": 703
                }
            ],
            "category": "paragraph",
            "html": "<p id='192' style='font-size:20px'>A. Detailed Experimental Setup</p>",
            "id": 192,
            "page": 12,
            "text": "A. Detailed Experimental Setup"
        },
        {
            "bounding_box": [
                {
                    "x": 380,
                    "y": 792
                },
                {
                    "x": 2086,
                    "y": 792
                },
                {
                    "x": 2086,
                    "y": 1552
                },
                {
                    "x": 380,
                    "y": 1552
                }
            ],
            "category": "table",
            "html": "<table id='193' style='font-size:14px'><tr><td>Dataset</td><td>Hours</td><td>Type</td><td>Source</td></tr><tr><td>Clotho</td><td>152</td><td>Caption</td><td>Drossos et al. (2020)</td></tr><tr><td>AudioCaps</td><td>109</td><td>Caption</td><td>Kim et al. (2019)</td></tr><tr><td>MACS</td><td>100</td><td>Caption</td><td>Mart�n-Morato & Mesaros (2021)</td></tr><tr><td>WavText5Ks</td><td>25</td><td>Caption</td><td>Deshmukh et al. (2022)</td></tr><tr><td>BBC sound effects</td><td>481</td><td>Caption</td><td>https : / / sound-effects . bbcrewind. CO . uk/</td></tr><tr><td>Audiostock</td><td>43</td><td>Caption</td><td>https : / /audiostock. net/ se</td></tr><tr><td>Filter AudioSet</td><td>2084</td><td>Label</td><td>Gemmeke et al. (2017)</td></tr><tr><td>ESC-50</td><td>3</td><td>Label</td><td>Piczak (2015)</td></tr><tr><td>FSD50K</td><td>108</td><td>Label</td><td>https : / / annotator · freesound. org/ fsd/</td></tr><tr><td>Sonniss Game Effects</td><td>20</td><td>Label</td><td>https : / / sonniss · com/ gameaudiogdc/</td></tr><tr><td>WeSoundEffects</td><td>11</td><td>Label</td><td>https : / /wesoundeffects · com/</td></tr><tr><td>Epidemic Sound</td><td>220</td><td>Label</td><td>https : / /www . epidemicsound. com/</td></tr><tr><td>UrbanSound8K</td><td>8</td><td>Label</td><td>Salamon et al. (2014)</td></tr><tr><td>LibriTTS</td><td>300</td><td>Language-free</td><td>Zen et al. (2019)</td></tr><tr><td>Medley-solos-DB</td><td>7</td><td>Language-free</td><td>Bittner et al. (2014)</td></tr></table>",
            "id": 193,
            "page": 12,
            "text": "Dataset Hours Type Source  Clotho 152 Caption Drossos  (2020)  AudioCaps 109 Caption Kim  (2019)  MACS 100 Caption Mart�n-Morato & Mesaros (2021)  WavText5Ks 25 Caption Deshmukh  (2022)  BBC sound effects 481 Caption https : / / sound-effects . bbcrewind. CO . uk/  Audiostock 43 Caption https : / /audiostock. net/ se  Filter AudioSet 2084 Label Gemmeke  (2017)  ESC-50 3 Label Piczak (2015)  FSD50K 108 Label https : / / annotator · freesound. org/ fsd/  Sonniss Game Effects 20 Label https : / / sonniss · com/ gameaudiogdc/  WeSoundEffects 11 Label https : / /wesoundeffects · com/  Epidemic Sound 220 Label https : / /www . epidemicsound. com/  UrbanSound8K 8 Label Salamon  (2014)  LibriTTS 300 Language-free Zen  (2019)  Medley-solos-DB 7 Language-free"
        },
        {
            "bounding_box": [
                {
                    "x": 810,
                    "y": 1588
                },
                {
                    "x": 1677,
                    "y": 1588
                },
                {
                    "x": 1677,
                    "y": 1632
                },
                {
                    "x": 810,
                    "y": 1632
                }
            ],
            "category": "caption",
            "html": "<caption id='194' style='font-size:14px'>Table 4. Statistics for the combination of several datasets.</caption>",
            "id": 194,
            "page": 12,
            "text": "Table 4. Statistics for the combination of several datasets."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 1712
                },
                {
                    "x": 2265,
                    "y": 1712
                },
                {
                    "x": 2265,
                    "y": 1967
                },
                {
                    "x": 225,
                    "y": 1967
                }
            ],
            "category": "paragraph",
            "html": "<p id='195' style='font-size:18px'>As shown in Table 5, we collect a large-scale audio-text dataset consisting of 1M audio samples with a total duration of<br>~3k hours. It contains audio of human activities, natural sounds, and audio effects, consisting of several data sources from<br>publicly available websites. For audio with text descriptions, we download the parallel audio-text data. For audios without<br>natural language annotation (or with labels), we discard the corresponding class label (if any) and apply the pseudo prompt<br>enhancement to construct natural language descriptions aligned well with the audio.</p>",
            "id": 195,
            "page": 12,
            "text": "As shown in Table 5, we collect a large-scale audio-text dataset consisting of 1M audio samples with a total duration of ~3k hours. It contains audio of human activities, natural sounds, and audio effects, consisting of several data sources from publicly available websites. For audio with text descriptions, we download the parallel audio-text data. For audios without natural language annotation (or with labels), we discard the corresponding class label (if any) and apply the pseudo prompt enhancement to construct natural language descriptions aligned well with the audio."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1987
                },
                {
                    "x": 2265,
                    "y": 1987
                },
                {
                    "x": 2265,
                    "y": 2240
                },
                {
                    "x": 224,
                    "y": 2240
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='196' style='font-size:16px'>As speech and music are the dominant classes in Audioset, we filter these samples to construct a more balanced dataset.<br>Overall we are left with 3k hours with 1M audio-text pairs for training data. For evaluating text-to-audio models (Yang<br>et al., 2022; Kreuk et al., 2022), the AudioCaption validation set is the standard benchmark, which contains 494 samples<br>with five human-annotated captions in each audio clip. In both training and inference, we pad short clips to 10-second long<br>and randomly crop a 624 x 80 mel-spectrogram from 10-second 16 kHz audio.</p>",
            "id": 196,
            "page": 12,
            "text": "As speech and music are the dominant classes in Audioset, we filter these samples to construct a more balanced dataset. Overall we are left with 3k hours with 1M audio-text pairs for training data. For evaluating text-to-audio models (Yang , 2022; Kreuk , 2022), the AudioCaption validation set is the standard benchmark, which contains 494 samples with five human-annotated captions in each audio clip. In both training and inference, we pad short clips to 10-second long and randomly crop a 624 x 80 mel-spectrogram from 10-second 16 kHz audio."
        },
        {
            "bounding_box": [
                {
                    "x": 739,
                    "y": 2344
                },
                {
                    "x": 1732,
                    "y": 2344
                },
                {
                    "x": 1732,
                    "y": 2652
                },
                {
                    "x": 739,
                    "y": 2652
                }
            ],
            "category": "table",
            "html": "<table id='197' style='font-size:14px'><tr><td>Method</td><td>FSD50K</td><td>ESC-50</td><td>Urbansound8k</td></tr><tr><td>Original</td><td>0.40</td><td>0.43</td><td>0.33</td></tr><tr><td>Captioning</td><td>0.35</td><td>0.46</td><td>0.37</td></tr><tr><td>Retrieval</td><td>0.31</td><td>0.44</td><td>0.38</td></tr><tr><td>Both + CLAP Select</td><td>0.54</td><td>0.62</td><td>0.55</td></tr></table>",
            "id": 197,
            "page": 12,
            "text": "Method FSD50K ESC-50 Urbansound8k  Original 0.40 0.43 0.33  Captioning 0.35 0.46 0.37  Retrieval 0.31 0.44 0.38  Both + CLAP Select 0.54 0.62"
        },
        {
            "bounding_box": [
                {
                    "x": 615,
                    "y": 2681
                },
                {
                    "x": 1871,
                    "y": 2681
                },
                {
                    "x": 1871,
                    "y": 2727
                },
                {
                    "x": 615,
                    "y": 2727
                }
            ],
            "category": "caption",
            "html": "<caption id='198' style='font-size:16px'>Table 5. Text-audio alignment CLAP score averaged across the single-label dataset.</caption>",
            "id": 198,
            "page": 12,
            "text": "Table 5. Text-audio alignment CLAP score averaged across the single-label dataset."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 2855
                },
                {
                    "x": 760,
                    "y": 2855
                },
                {
                    "x": 760,
                    "y": 2912
                },
                {
                    "x": 225,
                    "y": 2912
                }
            ],
            "category": "paragraph",
            "html": "<p id='199' style='font-size:20px'>B. Model Configurations</p>",
            "id": 199,
            "page": 12,
            "text": "B. Model Configurations"
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 2941
                },
                {
                    "x": 1342,
                    "y": 2941
                },
                {
                    "x": 1342,
                    "y": 2991
                },
                {
                    "x": 225,
                    "y": 2991
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='200' style='font-size:16px'>We list the model hyper-parameters of Make-An-Audio in Table 6.</p>",
            "id": 200,
            "page": 12,
            "text": "We list the model hyper-parameters of Make-An-Audio in Table 6."
        },
        {
            "bounding_box": [
                {
                    "x": 553,
                    "y": 188
                },
                {
                    "x": 1935,
                    "y": 188
                },
                {
                    "x": 1935,
                    "y": 240
                },
                {
                    "x": 553,
                    "y": 240
                }
            ],
            "category": "paragraph",
            "html": "<p id='201' style='font-size:18px'>Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models</p>",
            "id": 201,
            "page": 13,
            "text": "Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models"
        },
        {
            "bounding_box": [
                {
                    "x": 603,
                    "y": 273
                },
                {
                    "x": 1879,
                    "y": 273
                },
                {
                    "x": 1879,
                    "y": 1057
                },
                {
                    "x": 603,
                    "y": 1057
                }
            ],
            "category": "table",
            "html": "<table id='202' style='font-size:16px'><tr><td colspan=\"2\">Hyperparameter</td><td>Make-An-Audio</td></tr><tr><td>Spectrogram Autoencoders</td><td>Input/Output Channels Hidden Channels Residual Blocks Spectrogram Size Channel Mult</td><td>1 4 2 80 x 624 [1, 2, 2,4]</td></tr><tr><td>Denoising Unet</td><td>Input/Output Channels Model Channels Attention Heads Condition Channels Latent Size Channel Mult</td><td>4 320 8 1024 10 x 78 [1, 2]</td></tr><tr><td>CLAP Text Encoder</td><td>Transformer Embed Channels Output Project Channels Token Length</td><td>768 1024 77</td></tr><tr><td colspan=\"2\">Total Number of Parameters</td><td>332M</td></tr></table>",
            "id": 202,
            "page": 13,
            "text": "Hyperparameter Make-An-Audio  Spectrogram Autoencoders Input/Output Channels Hidden Channels Residual Blocks Spectrogram Size Channel Mult 1 4 2 80 x 624   Denoising Unet Input/Output Channels Model Channels Attention Heads Condition Channels Latent Size Channel Mult 4 320 8 1024 10 x 78   CLAP Text Encoder Transformer Embed Channels Output Project Channels Token Length 768 1024 77  Total Number of Parameters"
        },
        {
            "bounding_box": [
                {
                    "x": 831,
                    "y": 1090
                },
                {
                    "x": 1654,
                    "y": 1090
                },
                {
                    "x": 1654,
                    "y": 1136
                },
                {
                    "x": 831,
                    "y": 1136
                }
            ],
            "category": "caption",
            "html": "<caption id='203' style='font-size:16px'>Table 6. Hyperparameters of Make-An-Audio models.</caption>",
            "id": 203,
            "page": 13,
            "text": "Table 6. Hyperparameters of Make-An-Audio models."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1202
                },
                {
                    "x": 530,
                    "y": 1202
                },
                {
                    "x": 530,
                    "y": 1259
                },
                {
                    "x": 224,
                    "y": 1259
                }
            ],
            "category": "paragraph",
            "html": "<p id='204' style='font-size:22px'>C. Evaluation</p>",
            "id": 204,
            "page": 13,
            "text": "C. Evaluation"
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 1290
                },
                {
                    "x": 2264,
                    "y": 1290
                },
                {
                    "x": 2264,
                    "y": 1439
                },
                {
                    "x": 222,
                    "y": 1439
                }
            ],
            "category": "paragraph",
            "html": "<p id='205' style='font-size:18px'>To probe audio quality, we conduct the MOS (mean opinion score) tests and explicitly instruct the raters to \"focus on<br>examining the audio quality and naturalness. The testers present and rate the samples, and each tester is asked to evaluate<br>the subjective naturalness on a 20-100 Likert scale.</p>",
            "id": 205,
            "page": 13,
            "text": "To probe audio quality, we conduct the MOS (mean opinion score) tests and explicitly instruct the raters to \"focus on examining the audio quality and naturalness. The testers present and rate the samples, and each tester is asked to evaluate the subjective naturalness on a 20-100 Likert scale."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1465
                },
                {
                    "x": 2263,
                    "y": 1465
                },
                {
                    "x": 2263,
                    "y": 1613
                },
                {
                    "x": 223,
                    "y": 1613
                }
            ],
            "category": "paragraph",
            "html": "<p id='206' style='font-size:18px'>To probe text-audio alignment, human raters are shown an audio and a prompt and asked \"Does the natural language<br>description align with audio faithfully?\". They must respond with \"completely\", \"mostly\", or \"somewhat\" on a 20-100<br>Likert scale.</p>",
            "id": 206,
            "page": 13,
            "text": "To probe text-audio alignment, human raters are shown an audio and a prompt and asked \"Does the natural language description align with audio faithfully?\". They must respond with \"completely\", \"mostly\", or \"somewhat\" on a 20-100 Likert scale."
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 1639
                },
                {
                    "x": 2265,
                    "y": 1639
                },
                {
                    "x": 2265,
                    "y": 1841
                },
                {
                    "x": 222,
                    "y": 1841
                }
            ],
            "category": "paragraph",
            "html": "<p id='207' style='font-size:18px'>Our subjective evaluation tests are crowd-sourced and conducted via Amazon Mechanical Turk. These ratings are obtained<br>independently for model samples and reference audio, and both are reported. The screenshots of instructions for testers have<br>been shown in Figure 8. We paid $8 to participants hourly and totally spent about $750 on participant compensation. A<br>small subset of speech samples used in the test is available at https : / / Text-to-Audio · github · io/.</p>",
            "id": 207,
            "page": 13,
            "text": "Our subjective evaluation tests are crowd-sourced and conducted via Amazon Mechanical Turk. These ratings are obtained independently for model samples and reference audio, and both are reported. The screenshots of instructions for testers have been shown in Figure 8. We paid $8 to participants hourly and totally spent about $750 on participant compensation. A small subset of speech samples used in the test is available at https : / / Text-to-Audio · github · io/."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1900
                },
                {
                    "x": 966,
                    "y": 1900
                },
                {
                    "x": 966,
                    "y": 1958
                },
                {
                    "x": 224,
                    "y": 1958
                }
            ],
            "category": "paragraph",
            "html": "<p id='208' style='font-size:22px'>D. Detailed Formulation of DDPM</p>",
            "id": 208,
            "page": 13,
            "text": "D. Detailed Formulation of DDPM"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1989
                },
                {
                    "x": 2263,
                    "y": 1989
                },
                {
                    "x": 2263,
                    "y": 2087
                },
                {
                    "x": 223,
                    "y": 2087
                }
            ],
            "category": "paragraph",
            "html": "<p id='209' style='font-size:16px'>We define the data distribution as q(xo). The diffusion process is defined by a fixed Markov chain from data X0 to the latent<br>variable XT:</p>",
            "id": 209,
            "page": 13,
            "text": "We define the data distribution as q(xo). The diffusion process is defined by a fixed Markov chain from data X0 to the latent variable XT:"
        },
        {
            "bounding_box": [
                {
                    "x": 221,
                    "y": 2228
                },
                {
                    "x": 2260,
                    "y": 2228
                },
                {
                    "x": 2260,
                    "y": 2330
                },
                {
                    "x": 221,
                    "y": 2330
                }
            ],
            "category": "paragraph",
            "html": "<p id='210' style='font-size:16px'>For a small positive constant Bt, a small Gaussian noise is added from Xt-1 to the distribution of Xt under the function of<br>q(xt|xt-1).</p>",
            "id": 210,
            "page": 13,
            "text": "For a small positive constant Bt, a small Gaussian noise is added from Xt-1 to the distribution of Xt under the function of q(xt|xt-1)."
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 2353
                },
                {
                    "x": 2266,
                    "y": 2353
                },
                {
                    "x": 2266,
                    "y": 2452
                },
                {
                    "x": 222,
                    "y": 2452
                }
            ],
            "category": "paragraph",
            "html": "<p id='211' style='font-size:14px'>The whole process gradually converts data X0 to whitened latents XT according to the fixed noise schedule B1, · . · , BT,<br>where E ~ N(0,I):</p>",
            "id": 211,
            "page": 13,
            "text": "The whole process gradually converts data X0 to whitened latents XT according to the fixed noise schedule B1, · . · , BT, where E ~ N(0,I):"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2527
                },
                {
                    "x": 1620,
                    "y": 2527
                },
                {
                    "x": 1620,
                    "y": 2576
                },
                {
                    "x": 224,
                    "y": 2576
                }
            ],
            "category": "paragraph",
            "html": "<p id='212' style='font-size:18px'>Efficient training is optimizing a random term of t with stochastic gradient descent:</p>",
            "id": 212,
            "page": 13,
            "text": "Efficient training is optimizing a random term of t with stochastic gradient descent:"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2742
                },
                {
                    "x": 2262,
                    "y": 2742
                },
                {
                    "x": 2262,
                    "y": 2841
                },
                {
                    "x": 223,
                    "y": 2841
                }
            ],
            "category": "paragraph",
            "html": "<p id='213' style='font-size:14px'>Unlike the diffusion process, the reverse process is to recover samples from Gaussian noises. The reverse process is a<br>Markov chain from XT to xo parameterized by shared 0:</p>",
            "id": 213,
            "page": 13,
            "text": "Unlike the diffusion process, the reverse process is to recover samples from Gaussian noises. The reverse process is a Markov chain from XT to xo parameterized by shared 0:"
        },
        {
            "bounding_box": [
                {
                    "x": 550,
                    "y": 189
                },
                {
                    "x": 1933,
                    "y": 189
                },
                {
                    "x": 1933,
                    "y": 237
                },
                {
                    "x": 550,
                    "y": 237
                }
            ],
            "category": "caption",
            "html": "<caption id='214' style='font-size:18px'>Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models</caption>",
            "id": 214,
            "page": 14,
            "text": "Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models"
        },
        {
            "bounding_box": [
                {
                    "x": 277,
                    "y": 256
                },
                {
                    "x": 2220,
                    "y": 256
                },
                {
                    "x": 2220,
                    "y": 1267
                },
                {
                    "x": 277,
                    "y": 1267
                }
            ],
            "category": "figure",
            "html": "<figure><img id='215' style='font-size:14px' alt=\"Select an option\nNatural language discriptions: a cat meowing and young female speaking Excellent - Completely faithful - 100 1\nGood - Mostly faithful - 80 2\nGernerated audio:\nFair - Equally faithful and inconsistent - 60 3\n0:00 / 0:09 ◀) : Poor - Mostly inconsistent - 40 4\nBad - Completely inconsistent - 20 5\n(a) Screenshot of MOS-F testing.\nInstructions Shortcuts How natural is this audio recording? Please focus on examining the audio quality and naturalness (noise, timbre, sound clarity and high-frequency details).\nSelect an option\nTesting audio: Excellent - Completely natural audio - 100 1\nGood - Mostly natural audio - 80 2\n0:00 / 0:09 ◐ :\nFair - Equally natural and unnatural audio - 60 3\nPoor - Mostly unnatural audio - 40 4\nBad - Completely unnatural audio - 20 5\" data-coord=\"top-left:(277,256); bottom-right:(2220,1267)\" /></figure>",
            "id": 215,
            "page": 14,
            "text": "Select an option Natural language discriptions: a cat meowing and young female speaking Excellent - Completely faithful - 100 1 Good - Mostly faithful - 80 2 Gernerated audio: Fair - Equally faithful and inconsistent - 60 3 0:00 / 0:09 ◀) : Poor - Mostly inconsistent - 40 4 Bad - Completely inconsistent - 20 5 (a) Screenshot of MOS-F testing. Instructions Shortcuts How natural is this audio recording? Please focus on examining the audio quality and naturalness (noise, timbre, sound clarity and high-frequency details). Select an option Testing audio: Excellent - Completely natural audio - 100 1 Good - Mostly natural audio - 80 2 0:00 / 0:09 ◐ : Fair - Equally natural and unnatural audio - 60 3 Poor - Mostly unnatural audio - 40 4 Bad - Completely unnatural audio - 20 5"
        },
        {
            "bounding_box": [
                {
                    "x": 973,
                    "y": 1270
                },
                {
                    "x": 1499,
                    "y": 1270
                },
                {
                    "x": 1499,
                    "y": 1318
                },
                {
                    "x": 973,
                    "y": 1318
                }
            ],
            "category": "caption",
            "html": "<br><caption id='216' style='font-size:20px'>(b) Screenshot of MOS-Q testing.</caption>",
            "id": 216,
            "page": 14,
            "text": "(b) Screenshot of MOS-Q testing."
        },
        {
            "bounding_box": [
                {
                    "x": 881,
                    "y": 1371
                },
                {
                    "x": 1604,
                    "y": 1371
                },
                {
                    "x": 1604,
                    "y": 1419
                },
                {
                    "x": 881,
                    "y": 1419
                }
            ],
            "category": "caption",
            "html": "<caption id='217' style='font-size:16px'>Figure 8. Screenshots of subjective evaluations.</caption>",
            "id": 217,
            "page": 14,
            "text": "Figure 8. Screenshots of subjective evaluations."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1503
                },
                {
                    "x": 1591,
                    "y": 1503
                },
                {
                    "x": 1591,
                    "y": 1551
                },
                {
                    "x": 224,
                    "y": 1551
                }
            ],
            "category": "paragraph",
            "html": "<p id='218' style='font-size:16px'>where each iteration eliminates the Gaussian noise added in the diffusion process:</p>",
            "id": 218,
            "page": 14,
            "text": "where each iteration eliminates the Gaussian noise added in the diffusion process:"
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 1711
                },
                {
                    "x": 794,
                    "y": 1711
                },
                {
                    "x": 794,
                    "y": 1770
                },
                {
                    "x": 225,
                    "y": 1770
                }
            ],
            "category": "paragraph",
            "html": "<p id='219' style='font-size:22px'>E. Implementation Details</p>",
            "id": 219,
            "page": 14,
            "text": "E. Implementation Details"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1799
                },
                {
                    "x": 798,
                    "y": 1799
                },
                {
                    "x": 798,
                    "y": 1851
                },
                {
                    "x": 224,
                    "y": 1851
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='220' style='font-size:20px'>E.1. Spectrogram Autoencoders</p>",
            "id": 220,
            "page": 14,
            "text": "E.1. Spectrogram Autoencoders"
        },
        {
            "bounding_box": [
                {
                    "x": 221,
                    "y": 1876
                },
                {
                    "x": 2264,
                    "y": 1876
                },
                {
                    "x": 2264,
                    "y": 2028
                },
                {
                    "x": 221,
                    "y": 2028
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='221' style='font-size:18px'>We also investigate the effectiveness of several audio autoencoder variants in Table 7, and find that deeper representation<br>(i.e., 32 or 128) relatively brings more compression, while the information deterioration could burden the Unet model in<br>generative modeling.</p>",
            "id": 221,
            "page": 14,
            "text": "We also investigate the effectiveness of several audio autoencoder variants in Table 7, and find that deeper representation (i.e., 32 or 128) relatively brings more compression, while the information deterioration could burden the Unet model in generative modeling."
        },
        {
            "bounding_box": [
                {
                    "x": 848,
                    "y": 2092
                },
                {
                    "x": 1625,
                    "y": 2092
                },
                {
                    "x": 1625,
                    "y": 2610
                },
                {
                    "x": 848,
                    "y": 2610
                }
            ],
            "category": "table",
            "html": "<table id='222' style='font-size:16px'><tr><td>Method</td><td>Channel</td><td>FID</td><td>KL</td></tr><tr><td colspan=\"4\">Supervised Evaluation in AudioCaps dataset</td></tr><tr><td rowspan=\"3\">Base</td><td>4</td><td>5.15</td><td>2.89</td></tr><tr><td>32</td><td>9.22</td><td>3.54</td></tr><tr><td>128</td><td>10.92</td><td>3.68</td></tr><tr><td>w/o PPE</td><td>4</td><td>5.37</td><td>3.05</td></tr><tr><td colspan=\"4\">Zero-Shot Evaluation in Clotho dataset</td></tr><tr><td rowspan=\"2\">Base w/o PPE</td><td>4</td><td>18.75</td><td>7.01</td></tr><tr><td>4</td><td>22.31</td><td>7.19</td></tr></table>",
            "id": 222,
            "page": 14,
            "text": "Method Channel FID KL  Supervised Evaluation in AudioCaps dataset  Base 4 5.15 2.89  32 9.22 3.54  128 10.92 3.68  w/o PPE 4 5.37 3.05  Zero-Shot Evaluation in Clotho dataset  Base w/o PPE 4 18.75 7.01  4 22.31"
        },
        {
            "bounding_box": [
                {
                    "x": 227,
                    "y": 2637
                },
                {
                    "x": 2263,
                    "y": 2637
                },
                {
                    "x": 2263,
                    "y": 2686
                },
                {
                    "x": 227,
                    "y": 2686
                }
            ],
            "category": "caption",
            "html": "<caption id='223' style='font-size:14px'>Table 7. Audio quality comparisons for ablation study with Make-An-Audio BERT. We use PPE to denote pseudo prompt enhancement.</caption>",
            "id": 223,
            "page": 14,
            "text": "Table 7. Audio quality comparisons for ablation study with Make-An-Audio BERT. We use PPE to denote pseudo prompt enhancement."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2763
                },
                {
                    "x": 554,
                    "y": 2763
                },
                {
                    "x": 554,
                    "y": 2813
                },
                {
                    "x": 224,
                    "y": 2813
                }
            ],
            "category": "paragraph",
            "html": "<p id='224' style='font-size:18px'>E.2. Text-to-audio</p>",
            "id": 224,
            "page": 14,
            "text": "E.2. Text-to-audio"
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 2841
                },
                {
                    "x": 2265,
                    "y": 2841
                },
                {
                    "x": 2265,
                    "y": 2994
                },
                {
                    "x": 222,
                    "y": 2994
                }
            ],
            "category": "paragraph",
            "html": "<p id='225' style='font-size:18px'>We first encode the text into a sequence of K tokens, and utilize the cross-attention mechanism to learn a language and<br>mel-spectrograms representation mapping in a powerful model. After the initial training run, we fine-tuned our base model<br>to support unconditional generation, with 20% of text token sequences being replaced with the empty sequence. This</p>",
            "id": 225,
            "page": 14,
            "text": "We first encode the text into a sequence of K tokens, and utilize the cross-attention mechanism to learn a language and mel-spectrograms representation mapping in a powerful model. After the initial training run, we fine-tuned our base model to support unconditional generation, with 20% of text token sequences being replaced with the empty sequence. This"
        },
        {
            "bounding_box": [
                {
                    "x": 553,
                    "y": 190
                },
                {
                    "x": 1933,
                    "y": 190
                },
                {
                    "x": 1933,
                    "y": 237
                },
                {
                    "x": 553,
                    "y": 237
                }
            ],
            "category": "header",
            "html": "<header id='226' style='font-size:18px'>Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models</header>",
            "id": 226,
            "page": 15,
            "text": "Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models"
        },
        {
            "bounding_box": [
                {
                    "x": 221,
                    "y": 285
                },
                {
                    "x": 2262,
                    "y": 285
                },
                {
                    "x": 2262,
                    "y": 384
                },
                {
                    "x": 221,
                    "y": 384
                }
            ],
            "category": "paragraph",
            "html": "<p id='227' style='font-size:18px'>way, the model retains its ability to generate text-conditional outputs, but can also generate spectrogram representation<br>unconditionally.</p>",
            "id": 227,
            "page": 15,
            "text": "way, the model retains its ability to generate text-conditional outputs, but can also generate spectrogram representation unconditionally."
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 406
                },
                {
                    "x": 2267,
                    "y": 406
                },
                {
                    "x": 2267,
                    "y": 663
                },
                {
                    "x": 222,
                    "y": 663
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='228' style='font-size:18px'>We consider the pre-trained automatic audio captioning (Xu et al., 2020) and audio-text retrieval (Deshmukh et al., 2022;<br>Koepke et al., 2022) systems as our experts for prompt generation. Regarding automatic audio captioning, the model consists<br>of a 10-layer convolution neural network (CNN) encoder and a temporal attentional single-layer gated recurrent unit (GRU)<br>decoder. The CNN encoder is pre-trained on a large-scale Audioset dataset. As for audio-text retrieval, the model leverages<br>BERT with a multi-modal transformer encoder for representation learning. It is trained on AudioCaps and Clotho datasets.</p>",
            "id": 228,
            "page": 15,
            "text": "We consider the pre-trained automatic audio captioning (Xu , 2020) and audio-text retrieval (Deshmukh , 2022; Koepke , 2022) systems as our experts for prompt generation. Regarding automatic audio captioning, the model consists of a 10-layer convolution neural network (CNN) encoder and a temporal attentional single-layer gated recurrent unit (GRU) decoder. The CNN encoder is pre-trained on a large-scale Audioset dataset. As for audio-text retrieval, the model leverages BERT with a multi-modal transformer encoder for representation learning. It is trained on AudioCaps and Clotho datasets."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 710
                },
                {
                    "x": 589,
                    "y": 710
                },
                {
                    "x": 589,
                    "y": 761
                },
                {
                    "x": 224,
                    "y": 761
                }
            ],
            "category": "paragraph",
            "html": "<p id='229' style='font-size:18px'>E.3. Visual-to-audio</p>",
            "id": 229,
            "page": 15,
            "text": "E.3. Visual-to-audio"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 786
                },
                {
                    "x": 2265,
                    "y": 786
                },
                {
                    "x": 2265,
                    "y": 1041
                },
                {
                    "x": 224,
                    "y": 1041
                }
            ],
            "category": "paragraph",
            "html": "<p id='230' style='font-size:18px'>For visual-to-audio (image/video) synthesis, we utilize the CLIP-guided T2A model and leverage global textual represen-<br>tations to bridge the modality gap between the visual and audio worlds. However, we empirically find that global CLIP<br>conditions have a limited ability to control faithful synthesis with high text-audio similarity. On that account, we use the<br>110h FSD50K audios annotated with a class label for training, and this simplification avoids multimodal prediction (a<br>conditional vector may refer to different concepts) with complex distribution.</p>",
            "id": 230,
            "page": 15,
            "text": "For visual-to-audio (image/video) synthesis, we utilize the CLIP-guided T2A model and leverage global textual representations to bridge the modality gap between the visual and audio worlds. However, we empirically find that global CLIP conditions have a limited ability to control faithful synthesis with high text-audio similarity. On that account, we use the 110h FSD50K audios annotated with a class label for training, and this simplification avoids multimodal prediction (a conditional vector may refer to different concepts) with complex distribution."
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 1060
                },
                {
                    "x": 2265,
                    "y": 1060
                },
                {
                    "x": 2265,
                    "y": 1365
                },
                {
                    "x": 222,
                    "y": 1365
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='231' style='font-size:18px'>We conduct ablation studies to compare various training settings, including datasets and global conditions. The results have<br>been presented in Table 8, and we have the following observations: 1) Replacing the FSD50K dataset with AudioCaps (Kim<br>et al., 2019) have witnessed a significant decrease in faithfulness. The dynamic concepts compositions confuse the<br>global-condition models, and the multimodal distribution hinders its capacity for controllable synthesis; 2) Removing the<br>normalization in the condition vector has witnessed the realism degradation measured by FID, demonstrating its efficiency<br>in reducing variance in latent space.</p>",
            "id": 231,
            "page": 15,
            "text": "We conduct ablation studies to compare various training settings, including datasets and global conditions. The results have been presented in Table 8, and we have the following observations: 1) Replacing the FSD50K dataset with AudioCaps (Kim , 2019) have witnessed a significant decrease in faithfulness. The dynamic concepts compositions confuse the global-condition models, and the multimodal distribution hinders its capacity for controllable synthesis; 2) Removing the normalization in the condition vector has witnessed the realism degradation measured by FID, demonstrating its efficiency in reducing variance in latent space."
        },
        {
            "bounding_box": [
                {
                    "x": 722,
                    "y": 1412
                },
                {
                    "x": 1754,
                    "y": 1412
                },
                {
                    "x": 1754,
                    "y": 1628
                },
                {
                    "x": 722,
                    "y": 1628
                }
            ],
            "category": "table",
            "html": "<table id='232' style='font-size:14px'><tr><td>Training/Testing Dataset</td><td>Condition</td><td>FID</td><td>KL</td><td>CLAP</td></tr><tr><td>AudioCaption</td><td>Global</td><td>/</td><td>/</td><td>0.12</td></tr><tr><td>FSD50k</td><td>Global</td><td>40.7</td><td>8.2</td><td>0.40</td></tr><tr><td>FSD50k</td><td>NormGlobal</td><td>31.1</td><td>8.0</td><td>0.42</td></tr></table>",
            "id": 232,
            "page": 15,
            "text": "Training/Testing Dataset Condition FID KL CLAP  AudioCaption Global / / 0.12  FSD50k Global 40.7 8.2 0.40  FSD50k NormGlobal 31.1 8.0"
        },
        {
            "bounding_box": [
                {
                    "x": 658,
                    "y": 1664
                },
                {
                    "x": 1828,
                    "y": 1664
                },
                {
                    "x": 1828,
                    "y": 1710
                },
                {
                    "x": 658,
                    "y": 1710
                }
            ],
            "category": "caption",
            "html": "<caption id='233' style='font-size:16px'>Table 8. Ablation studies for training Make-An-Audio with global conditions.</caption>",
            "id": 233,
            "page": 15,
            "text": "Table 8. Ablation studies for training Make-An-Audio with global conditions."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1795
                },
                {
                    "x": 1070,
                    "y": 1795
                },
                {
                    "x": 1070,
                    "y": 1856
                },
                {
                    "x": 224,
                    "y": 1856
                }
            ],
            "category": "paragraph",
            "html": "<p id='234' style='font-size:22px'>F. Dynamic Reprogramming Templates</p>",
            "id": 234,
            "page": 15,
            "text": "F. Dynamic Reprogramming Templates"
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 1883
                },
                {
                    "x": 1735,
                    "y": 1883
                },
                {
                    "x": 1735,
                    "y": 1938
                },
                {
                    "x": 225,
                    "y": 1938
                }
            ],
            "category": "paragraph",
            "html": "<p id='235' style='font-size:20px'>Below we provide the list of text templates used when providing dynamic reprogramming:</p>",
            "id": 235,
            "page": 15,
            "text": "Below we provide the list of text templates used when providing dynamic reprogramming:"
        },
        {
            "bounding_box": [
                {
                    "x": 268,
                    "y": 1995
                },
                {
                    "x": 811,
                    "y": 1995
                },
                {
                    "x": 811,
                    "y": 2995
                },
                {
                    "x": 268,
                    "y": 2995
                }
            ],
            "category": "paragraph",
            "html": "<p id='236' style='font-size:14px'>· before v q a n of &, X<br>· X before v q a n of &,<br>● in front of v q a n of &, X<br>· first is X second is q a n of &<br>· after X, v q a n of &<br>· after v q a n of &, X<br>· behind v q a n of &, X<br>· v q a n of &, then X<br>· v q a n of &, following X<br>· v q a n of &, later X<br>· X after v q a n of &<br>· before X, v q a n of &</p>",
            "id": 236,
            "page": 15,
            "text": "· before v q a n of &, X · X before v q a n of &, ● in front of v q a n of &, X · first is X second is q a n of & · after X, v q a n of & · after v q a n of &, X · behind v q a n of &, X · v q a n of &, then X · v q a n of &, following X · v q a n of &, later X · X after v q a n of & · before X, v q a n of &"
        },
        {
            "bounding_box": [
                {
                    "x": 552,
                    "y": 190
                },
                {
                    "x": 1932,
                    "y": 190
                },
                {
                    "x": 1932,
                    "y": 236
                },
                {
                    "x": 552,
                    "y": 236
                }
            ],
            "category": "header",
            "html": "<header id='237' style='font-size:14px'>Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models</header>",
            "id": 237,
            "page": 16,
            "text": "Make-An-Audio: Text-To-Audio Generation with Prompt-Enhanced Diffusion Models"
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 285
                },
                {
                    "x": 2263,
                    "y": 285
                },
                {
                    "x": 2263,
                    "y": 382
                },
                {
                    "x": 222,
                    "y": 382
                }
            ],
            "category": "paragraph",
            "html": "<p id='238' style='font-size:14px'>Specifically, we replace X and &, respectively, with the natural language of sampled data and the class label of sampled<br>events from the database.</p>",
            "id": 238,
            "page": 16,
            "text": "Specifically, we replace X and &, respectively, with the natural language of sampled data and the class label of sampled events from the database."
        },
        {
            "bounding_box": [
                {
                    "x": 221,
                    "y": 407
                },
                {
                    "x": 2265,
                    "y": 407
                },
                {
                    "x": 2265,
                    "y": 560
                },
                {
                    "x": 221,
                    "y": 560
                }
            ],
            "category": "paragraph",
            "html": "<p id='239' style='font-size:18px'>For verb (denoted as v), we have {'hearing', 'noticing', 'listening to', 'appearing'}; for adjective (denoted as a), we<br>have {'clear', 'noisy', 'close-up', 'weird', 'clean'}; for noun (denoted as n), we have {'audio', 'sound', 'voice'}; for<br>numeral/quantifier (denoted as q), we have {'a', 'the', 'some'};</p>",
            "id": 239,
            "page": 16,
            "text": "For verb (denoted as v), we have {'hearing', 'noticing', 'listening to', 'appearing'}; for adjective (denoted as a), we have {'clear', 'noisy', 'close-up', 'weird', 'clean'}; for noun (denoted as n), we have {'audio', 'sound', 'voice'}; for numeral/quantifier (denoted as q), we have {'a', 'the', 'some'};"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 620
                },
                {
                    "x": 1050,
                    "y": 620
                },
                {
                    "x": 1050,
                    "y": 679
                },
                {
                    "x": 224,
                    "y": 679
                }
            ],
            "category": "paragraph",
            "html": "<p id='240' style='font-size:22px'>G. Potential Negative Societal Impacts</p>",
            "id": 240,
            "page": 16,
            "text": "G. Potential Negative Societal Impacts"
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 707
                },
                {
                    "x": 2266,
                    "y": 707
                },
                {
                    "x": 2266,
                    "y": 908
                },
                {
                    "x": 222,
                    "y": 908
                }
            ],
            "category": "paragraph",
            "html": "<p id='241' style='font-size:14px'>This paper aims to advance open-domain text-to-audio generation, which will ease the effort of short video and digital art<br>creation. The efficient training method also transfers knowledge from text-to-audio models to X-to-audio generation, which<br>helps avoid training from scratch, and thus reduces the issue of data scarcity. A negative impact is the risk of misinformation.<br>To alleviate it, we can train an additional classifier to discriminate the fakes. We believe the benefits outweigh the downsides.</p>",
            "id": 241,
            "page": 16,
            "text": "This paper aims to advance open-domain text-to-audio generation, which will ease the effort of short video and digital art creation. The efficient training method also transfers knowledge from text-to-audio models to X-to-audio generation, which helps avoid training from scratch, and thus reduces the issue of data scarcity. A negative impact is the risk of misinformation. To alleviate it, we can train an additional classifier to discriminate the fakes. We believe the benefits outweigh the downsides."
        },
        {
            "bounding_box": [
                {
                    "x": 221,
                    "y": 930
                },
                {
                    "x": 2266,
                    "y": 930
                },
                {
                    "x": 2266,
                    "y": 1134
                },
                {
                    "x": 221,
                    "y": 1134
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='242' style='font-size:14px'>Make-An-Audio lowers the requirements for high-quality text-to-audio synthesis, which may cause unemployment for<br>people with related occupations, such as sound engineers and radio hosts. In addition, there is the potential for harm from<br>non-consensual voice cloning or the generation of fake media, and the voices in the recordings might be overused than they<br>expect.</p>",
            "id": 242,
            "page": 16,
            "text": "Make-An-Audio lowers the requirements for high-quality text-to-audio synthesis, which may cause unemployment for people with related occupations, such as sound engineers and radio hosts. In addition, there is the potential for harm from non-consensual voice cloning or the generation of fake media, and the voices in the recordings might be overused than they expect."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 1194
                },
                {
                    "x": 546,
                    "y": 1194
                },
                {
                    "x": 546,
                    "y": 1248
                },
                {
                    "x": 225,
                    "y": 1248
                }
            ],
            "category": "paragraph",
            "html": "<p id='243' style='font-size:20px'>H. Limitations</p>",
            "id": 243,
            "page": 16,
            "text": "H. Limitations"
        },
        {
            "bounding_box": [
                {
                    "x": 221,
                    "y": 1278
                },
                {
                    "x": 2266,
                    "y": 1278
                },
                {
                    "x": 2266,
                    "y": 1483
                },
                {
                    "x": 221,
                    "y": 1483
                }
            ],
            "category": "paragraph",
            "html": "<p id='244' style='font-size:16px'>Make-An-Audio adopts generative diffusion models for high-quality synthesis, and thus it inherently requires multiple<br>iterative refinements for better results. Besides, latent diffusion models require typically require more computational<br>resources, and degradation could be witnessed with decreased training data. One of our future directions is to develop<br>lightweight and fast diffusion models for accelerating sampling.</p>",
            "id": 244,
            "page": 16,
            "text": "Make-An-Audio adopts generative diffusion models for high-quality synthesis, and thus it inherently requires multiple iterative refinements for better results. Besides, latent diffusion models require typically require more computational resources, and degradation could be witnessed with decreased training data. One of our future directions is to develop lightweight and fast diffusion models for accelerating sampling."
        }
    ]
}