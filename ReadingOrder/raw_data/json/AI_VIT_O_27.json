{
  "id": "62a59b06-0f92-11ef-8230-426932df3dcf",
  "pdf_path": "./pdf/AI_VIT_O/2103.00112v3.pdf",
  "elements": [
    {
      "bounding_box": [
        {
          "x": 829,
          "y": 410
        },
        {
          "x": 1717,
          "y": 410
        },
        {
          "x": 1717,
          "y": 489
        },
        {
          "x": 829,
          "y": 489
        }
      ],
      "category": "paragraph",
      "html": "<p id='0' style='font-size:22px'>Transformer in Transformer</p>",
      "id": 0,
      "page": 1,
      "text": "Transformer in Transformer"
    },
    {
      "bounding_box": [
        {
          "x": 517,
          "y": 647
        },
        {
          "x": 2026,
          "y": 647
        },
        {
          "x": 2026,
          "y": 704
        },
        {
          "x": 517,
          "y": 704
        }
      ],
      "category": "paragraph",
      "html": "<p id='1' style='font-size:18px'>Kai Han 1,2 An Xiao2 Enhua Wu1,3* Jianyuan Guo2 Chunjing Xu2 Yunhe Wang 2*</p>",
      "id": 1,
      "page": 1,
      "text": "Kai Han 1,2 An Xiao2 Enhua Wu1,3* Jianyuan Guo2 Chunjing Xu2 Yunhe Wang 2*"
    },
    {
      "bounding_box": [
        {
          "x": 818,
          "y": 695
        },
        {
          "x": 1737,
          "y": 695
        },
        {
          "x": 1737,
          "y": 892
        },
        {
          "x": 818,
          "y": 892
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='2' style='font-size:16px'>1State Key Lab of Computer Science, ISCAS & UCAS<br>2Noah's Ark Lab, Huawei Technologies<br>3University of Macau<br>{hankai , weh}@ios ac · cn, yunhe · wang@huawei · com</p>",
      "id": 2,
      "page": 1,
      "text": "1State Key Lab of Computer Science, ISCAS & UCAS\n2Noah's Ark Lab, Huawei Technologies\n3University of Macau\n{hankai , weh}@ios ac · cn, yunhe · wang@huawei · com"
    },
    {
      "bounding_box": [
        {
          "x": 1176,
          "y": 1004
        },
        {
          "x": 1373,
          "y": 1004
        },
        {
          "x": 1373,
          "y": 1061
        },
        {
          "x": 1176,
          "y": 1061
        }
      ],
      "category": "paragraph",
      "html": "<p id='3' style='font-size:20px'>Abstract</p>",
      "id": 3,
      "page": 1,
      "text": "Abstract"
    },
    {
      "bounding_box": [
        {
          "x": 590,
          "y": 1115
        },
        {
          "x": 1960,
          "y": 1115
        },
        {
          "x": 1960,
          "y": 2035
        },
        {
          "x": 590,
          "y": 2035
        }
      ],
      "category": "paragraph",
      "html": "<p id='4' style='font-size:16px'>Transformer is a new kind of neural architecture which encodes the input data as<br>powerful features via the attention mechanism. Basically, the visual transformers<br>first divide the input images into several local patches and then calculate both<br>representations and their relationship. Since natural images are of high complexity<br>with abundant detail and color information, the granularity of the patch dividing is<br>not fine enough for excavating features of objects in different scales and locations.<br>In this paper, we point out that the attention inside these local patches are also<br>essential for building visual transformers with high performance and we explore<br>a new architecture, namely, Transformer iN Transformer (TNT). Specifically, we<br>regard the local patches (e.g., 16x16) as \"visual sentences\" and present to further<br>divide them into smaller patches (e.g., 4x4) as \"visual words\". The attention of<br>each word will be calculated with other words in the given visual sentence with<br>negligible computational costs. Features of both words and sentences will be ag-<br>gregated to enhance the representation ability. Experiments on several benchmarks<br>demonstrate the effectiveness of the proposed TNT architecture, e.g., we achieve an<br>81.5% top-1 accuracy on the ImageNet, which is about 1.7% higher than that of the<br>state-of-the-art visual transformer with similar computational cost. The PyTorch<br>code is available at https : // github · com/huawei -noah/ CV-Backbones, and<br>the MindSpore code is available at https : / /gitee · com/mindspore/models/<br>tree/master /research/ cv /TNT.</p>",
      "id": 4,
      "page": 1,
      "text": "Transformer is a new kind of neural architecture which encodes the input data as\npowerful features via the attention mechanism. Basically, the visual transformers\nfirst divide the input images into several local patches and then calculate both\nrepresentations and their relationship. Since natural images are of high complexity\nwith abundant detail and color information, the granularity of the patch dividing is\nnot fine enough for excavating features of objects in different scales and locations.\nIn this paper, we point out that the attention inside these local patches are also\nessential for building visual transformers with high performance and we explore\na new architecture, namely, Transformer iN Transformer (TNT). Specifically, we\nregard the local patches (e.g., 16x16) as \"visual sentences\" and present to further\ndivide them into smaller patches (e.g., 4x4) as \"visual words\". The attention of\neach word will be calculated with other words in the given visual sentence with\nnegligible computational costs. Features of both words and sentences will be ag-\ngregated to enhance the representation ability. Experiments on several benchmarks\ndemonstrate the effectiveness of the proposed TNT architecture, e.g., we achieve an\n81.5% top-1 accuracy on the ImageNet, which is about 1.7% higher than that of the\nstate-of-the-art visual transformer with similar computational cost. The PyTorch\ncode is available at https : // github · com/huawei -noah/ CV-Backbones, and\nthe MindSpore code is available at https : / /gitee · com/mindspore/models/\ntree/master /research/ cv /TNT."
    },
    {
      "bounding_box": [
        {
          "x": 446,
          "y": 2137
        },
        {
          "x": 798,
          "y": 2137
        },
        {
          "x": 798,
          "y": 2196
        },
        {
          "x": 446,
          "y": 2196
        }
      ],
      "category": "paragraph",
      "html": "<p id='5' style='font-size:20px'>1 Introduction</p>",
      "id": 5,
      "page": 1,
      "text": "1 Introduction"
    },
    {
      "bounding_box": [
        {
          "x": 441,
          "y": 2247
        },
        {
          "x": 2111,
          "y": 2247
        },
        {
          "x": 2111,
          "y": 2525
        },
        {
          "x": 441,
          "y": 2525
        }
      ],
      "category": "paragraph",
      "html": "<p id='6' style='font-size:18px'>In the past decade, the mainstream deep neural architectures used in the computer vision (CV) are<br>mainly established on convolutional neural networks (CNNs) [19, 13, 12]. Differently, transformer<br>is a type of neural network mainly based on self-attention mechanism [39], which can provide the<br>relationships between different features. Transformer is widely used in the field of natural language<br>processing (NLP), e.g., the famous BERT [9] and GPT-3 [2] models. The power of these transformer<br>models inspires the whole community to investigate the use of transformer for visual tasks.</p>",
      "id": 6,
      "page": 1,
      "text": "In the past decade, the mainstream deep neural architectures used in the computer vision (CV) are\nmainly established on convolutional neural networks (CNNs) [19, 13, 12]. Differently, transformer\nis a type of neural network mainly based on self-attention mechanism [39], which can provide the\nrelationships between different features. Transformer is widely used in the field of natural language\nprocessing (NLP), e.g., the famous BERT [9] and GPT-3 [2] models. The power of these transformer\nmodels inspires the whole community to investigate the use of transformer for visual tasks."
    },
    {
      "bounding_box": [
        {
          "x": 440,
          "y": 2542
        },
        {
          "x": 2113,
          "y": 2542
        },
        {
          "x": 2113,
          "y": 2869
        },
        {
          "x": 440,
          "y": 2869
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='7' style='font-size:18px'>To utilize the transformer architectures for conducting visual tasks, a number of researchers have<br>explored for representing the sequence information from different data. For example, Wang et al. ex-<br>plore self-attention mechanism in non-local networks [41] for capturing long-range dependencies<br>in video and image recognition. Carion et al. present DETR [3], which treats object detection<br>as a direct set prediction problem and solve it using a transformer encoder-decoder architecture.<br>Chen et al. propose the iGPT [6], which is the pioneering work applying pure transformer model (i.e.,<br>without convolution) on image recognition by self-supervised pre-training.</p>",
      "id": 7,
      "page": 1,
      "text": "To utilize the transformer architectures for conducting visual tasks, a number of researchers have\nexplored for representing the sequence information from different data. For example, Wang et al. ex-\nplore self-attention mechanism in non-local networks [41] for capturing long-range dependencies\nin video and image recognition. Carion et al. present DETR [3], which treats object detection\nas a direct set prediction problem and solve it using a transformer encoder-decoder architecture.\nChen et al. propose the iGPT [6], which is the pioneering work applying pure transformer model (i.e.,\nwithout convolution) on image recognition by self-supervised pre-training."
    },
    {
      "bounding_box": [
        {
          "x": 498,
          "y": 2916
        },
        {
          "x": 854,
          "y": 2916
        },
        {
          "x": 854,
          "y": 2961
        },
        {
          "x": 498,
          "y": 2961
        }
      ],
      "category": "paragraph",
      "html": "<p id='8' style='font-size:14px'>* Corresponding author.</p>",
      "id": 8,
      "page": 1,
      "text": "* Corresponding author."
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 3048
        },
        {
          "x": 1609,
          "y": 3048
        },
        {
          "x": 1609,
          "y": 3095
        },
        {
          "x": 443,
          "y": 3095
        }
      ],
      "category": "paragraph",
      "html": "<p id='9' style='font-size:16px'>35th Conference on Neural Information Processing Systems (NeurIPS 2021).</p>",
      "id": 9,
      "page": 1,
      "text": "35th Conference on Neural Information Processing Systems (NeurIPS 2021)."
    },
    {
      "bounding_box": [
        {
          "x": 64,
          "y": 889
        },
        {
          "x": 149,
          "y": 889
        },
        {
          "x": 149,
          "y": 2339
        },
        {
          "x": 64,
          "y": 2339
        }
      ],
      "category": "footer",
      "html": "<br><footer id='10' style='font-size:14px'>2021<br>Oct<br>26<br>[cs.CV]<br>arXiv:2103.00112v3</footer>",
      "id": 10,
      "page": 1,
      "text": "2021\nOct\n26\n[cs.CV]\narXiv:2103.00112v3"
    },
    {
      "bounding_box": [
        {
          "x": 454,
          "y": 297
        },
        {
          "x": 2097,
          "y": 297
        },
        {
          "x": 2097,
          "y": 1036
        },
        {
          "x": 454,
          "y": 1036
        }
      ],
      "category": "figure",
      "html": "<figure><img id='11' style='font-size:14px' alt=\"L x\nOuter Transformer Block Sentence position\n1\nencoding\nOutput\nInner Inner Inner\n+ Transformer + Transformer ··· + Transformer\nBlock Block Block Sentence\nembedding\n0 * 1 6 2 9 6 Class token\n*\nWord position\nLinear Projection of Visual Sentences and Words 1\nencoding\nWord embedding\" data-coord=\"top-left:(454,297); bottom-right:(2097,1036)\" /></figure>",
      "id": 11,
      "page": 2,
      "text": "L x\nOuter Transformer Block Sentence position\n1\nencoding\nOutput\nInner Inner Inner\n+ Transformer + Transformer ··· + Transformer\nBlock Block Block Sentence\nembedding\n0 * 1 6 2 9 6 Class token\n*\nWord position\nLinear Projection of Visual Sentences and Words 1\nencoding\nWord embedding"
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 1098
        },
        {
          "x": 2106,
          "y": 1098
        },
        {
          "x": 2106,
          "y": 1235
        },
        {
          "x": 442,
          "y": 1235
        }
      ],
      "category": "caption",
      "html": "<caption id='12' style='font-size:18px'>Figure 1: Illustration of the proposed Transformer-iN-Transformer (TNT) framework. The inner<br>transformer block is shared in the same layer. The word position encodings are shared across visual<br>sentences.</caption>",
      "id": 12,
      "page": 2,
      "text": "Figure 1: Illustration of the proposed Transformer-iN-Transformer (TNT) framework. The inner\ntransformer block is shared in the same layer. The word position encodings are shared across visual\nsentences."
    },
    {
      "bounding_box": [
        {
          "x": 441,
          "y": 1283
        },
        {
          "x": 2107,
          "y": 1283
        },
        {
          "x": 2107,
          "y": 1831
        },
        {
          "x": 441,
          "y": 1831
        }
      ],
      "category": "paragraph",
      "html": "<p id='13' style='font-size:20px'>Different from the data in NLP tasks, there exists a semantic gap between input images and the<br>ground-truth labels in CV tasks. To this end, Dosovitskiy et al. develop the ViT [10], which paves the<br>way for transferring the success of transformer based NLP models. Concretely, ViT divides the given<br>image into several local patches as a visual sequence. Then, the attention can be naturally calculated<br>between any two image patches for generating effective feature representations for the recognition<br>task. Subsequently, Touvron et al. explore the data-efficient training and distillation to enhance the<br>performance of ViT on the ImageNet benchmark and obtain an about 81.8% ImageNet top-1 accuracy,<br>which is comparable to that of the state-of-the-art convolutional networks. Chen et al. further treat the<br>image processing tasks (e.g., denosing and super-resolution) as a series of translations and develop<br>the IPT model for handling multiple low-level computer vision problems [4]. Nowadays, transformer<br>architectures have been used in a growing number of computer vision tasks [11] such as image<br>recognition [7, 44, 33], object detection [50], and segmentation [47, 42].</p>",
      "id": 13,
      "page": 2,
      "text": "Different from the data in NLP tasks, there exists a semantic gap between input images and the\nground-truth labels in CV tasks. To this end, Dosovitskiy et al. develop the ViT [10], which paves the\nway for transferring the success of transformer based NLP models. Concretely, ViT divides the given\nimage into several local patches as a visual sequence. Then, the attention can be naturally calculated\nbetween any two image patches for generating effective feature representations for the recognition\ntask. Subsequently, Touvron et al. explore the data-efficient training and distillation to enhance the\nperformance of ViT on the ImageNet benchmark and obtain an about 81.8% ImageNet top-1 accuracy,\nwhich is comparable to that of the state-of-the-art convolutional networks. Chen et al. further treat the\nimage processing tasks (e.g., denosing and super-resolution) as a series of translations and develop\nthe IPT model for handling multiple low-level computer vision problems [4]. Nowadays, transformer\narchitectures have been used in a growing number of computer vision tasks [11] such as image\nrecognition [7, 44, 33], object detection [50], and segmentation [47, 42]."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 1851
        },
        {
          "x": 2109,
          "y": 1851
        },
        {
          "x": 2109,
          "y": 2306
        },
        {
          "x": 442,
          "y": 2306
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='14' style='font-size:20px'>Although the aforementioned visual transformers have made great efforts to boost the models'<br>performances, most of existing works follow the conventional representation scheme used in ViT, i.e.,<br>dividing the input images into patches. Such a exquisite paradigm can effectively capture the visual<br>sequential information and estimate the attention between different image patches. However, the<br>diversity of natural images in modern benchmarks is very high, e.g., there are over 120 M images with<br>1000 different categories in the ImageNet dataset [30]. As shown in Figure 1, representing the given<br>image into local patches can help us to find the relationship and similarity between them. However,<br>there are also some sub-patches inside them with high similarity. Therefore, we are motivated to<br>explore a more exquisite visual image dividing method for generating visual sequences and improve<br>the performance.</p>",
      "id": 14,
      "page": 2,
      "text": "Although the aforementioned visual transformers have made great efforts to boost the models'\nperformances, most of existing works follow the conventional representation scheme used in ViT, i.e.,\ndividing the input images into patches. Such a exquisite paradigm can effectively capture the visual\nsequential information and estimate the attention between different image patches. However, the\ndiversity of natural images in modern benchmarks is very high, e.g., there are over 120 M images with\n1000 different categories in the ImageNet dataset [30]. As shown in Figure 1, representing the given\nimage into local patches can help us to find the relationship and similarity between them. However,\nthere are also some sub-patches inside them with high similarity. Therefore, we are motivated to\nexplore a more exquisite visual image dividing method for generating visual sequences and improve\nthe performance."
    },
    {
      "bounding_box": [
        {
          "x": 441,
          "y": 2328
        },
        {
          "x": 2108,
          "y": 2328
        },
        {
          "x": 2108,
          "y": 3014
        },
        {
          "x": 441,
          "y": 3014
        }
      ],
      "category": "paragraph",
      "html": "<p id='15' style='font-size:18px'>In this paper, we propose a novel Transformer-iN-Transformer (TNT) architecture for visual recog-<br>nition as shown in Figure 1. To enhance the feature representation ability of visual transformers,<br>we first divide the input images into several patches as \"visual sentences\" and then further divide<br>them into sub-patches as \"visual words\" Besides the conventional transformer blocks for extracting<br>features and attentions of visual sentences, we further embed a sub-transformer into the architecture<br>for excavating the features and details of smaller visual words. Specifically, features and attentions<br>between visual words in each visual sentence are calculated independently using a shared network SO<br>that the increased amount of parameters and FLOPs (floating-point operations) is negligible. Then,<br>features of words will be aggregated into the corresponding visual sentence. The class token is also<br>used for the subsequent visual recognition task via a fully-connected head. Through the proposed<br>TNT model, we can extract visual information with fine granularity and provide features with more<br>details. We then conduct a series of experiments on the ImageNet benchmark and downstream<br>tasks to demonstrate its superiority and thoroughly analyze the impact of the size for dividing visual<br>words. The results show that our TNT can achieve better accuracy and FLOPs trade-off over the<br>state-of-the-art transformer networks.</p>",
      "id": 15,
      "page": 2,
      "text": "In this paper, we propose a novel Transformer-iN-Transformer (TNT) architecture for visual recog-\nnition as shown in Figure 1. To enhance the feature representation ability of visual transformers,\nwe first divide the input images into several patches as \"visual sentences\" and then further divide\nthem into sub-patches as \"visual words\" Besides the conventional transformer blocks for extracting\nfeatures and attentions of visual sentences, we further embed a sub-transformer into the architecture\nfor excavating the features and details of smaller visual words. Specifically, features and attentions\nbetween visual words in each visual sentence are calculated independently using a shared network SO\nthat the increased amount of parameters and FLOPs (floating-point operations) is negligible. Then,\nfeatures of words will be aggregated into the corresponding visual sentence. The class token is also\nused for the subsequent visual recognition task via a fully-connected head. Through the proposed\nTNT model, we can extract visual information with fine granularity and provide features with more\ndetails. We then conduct a series of experiments on the ImageNet benchmark and downstream\ntasks to demonstrate its superiority and thoroughly analyze the impact of the size for dividing visual\nwords. The results show that our TNT can achieve better accuracy and FLOPs trade-off over the\nstate-of-the-art transformer networks."
    },
    {
      "bounding_box": [
        {
          "x": 1261,
          "y": 3093
        },
        {
          "x": 1289,
          "y": 3093
        },
        {
          "x": 1289,
          "y": 3129
        },
        {
          "x": 1261,
          "y": 3129
        }
      ],
      "category": "footer",
      "html": "<footer id='16' style='font-size:16px'>2</footer>",
      "id": 16,
      "page": 2,
      "text": "2"
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 302
        },
        {
          "x": 739,
          "y": 302
        },
        {
          "x": 739,
          "y": 356
        },
        {
          "x": 443,
          "y": 356
        }
      ],
      "category": "paragraph",
      "html": "<p id='17' style='font-size:22px'>2 Approach</p>",
      "id": 17,
      "page": 3,
      "text": "2 Approach"
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 402
        },
        {
          "x": 2105,
          "y": 402
        },
        {
          "x": 2105,
          "y": 494
        },
        {
          "x": 443,
          "y": 494
        }
      ],
      "category": "paragraph",
      "html": "<p id='18' style='font-size:18px'>In this section, we describe the proposed transformer-in-transformer architecture and analyze the<br>computation and parameter complexity in details.</p>",
      "id": 18,
      "page": 3,
      "text": "In this section, we describe the proposed transformer-in-transformer architecture and analyze the\ncomputation and parameter complexity in details."
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 547
        },
        {
          "x": 788,
          "y": 547
        },
        {
          "x": 788,
          "y": 594
        },
        {
          "x": 443,
          "y": 594
        }
      ],
      "category": "paragraph",
      "html": "<p id='19' style='font-size:18px'>2.1 Preliminaries</p>",
      "id": 19,
      "page": 3,
      "text": "2.1 Preliminaries"
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 631
        },
        {
          "x": 2105,
          "y": 631
        },
        {
          "x": 2105,
          "y": 723
        },
        {
          "x": 443,
          "y": 723
        }
      ],
      "category": "paragraph",
      "html": "<p id='20' style='font-size:18px'>We first briefly describe the basic components in transformer [39], including MSA (Multi-head<br>Self-Attention), MLP (Multi-Layer Perceptron) and LN (Layer Normalization).</p>",
      "id": 20,
      "page": 3,
      "text": "We first briefly describe the basic components in transformer [39], including MSA (Multi-head\nSelf-Attention), MLP (Multi-Layer Perceptron) and LN (Layer Normalization)."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 768
        },
        {
          "x": 2108,
          "y": 768
        },
        {
          "x": 2108,
          "y": 956
        },
        {
          "x": 442,
          "y": 956
        }
      ],
      "category": "paragraph",
      "html": "<p id='21' style='font-size:18px'>MSA. In the self-attention module, the inputs X E Rnxd are linearly transformed to three parts,<br>i.e., queries Q E Rnxdk, keys K E Rnxdk and values V E Rnxdv where n is the sequence length, d,<br>dk, dv are the dimensions of inputs, queries (keys) and values, respectively. The scaled dot-product<br>attention is applied on Q, K, V:</p>",
      "id": 21,
      "page": 3,
      "text": "MSA. In the self-attention module, the inputs X E Rnxd are linearly transformed to three parts,\ni.e., queries Q E Rnxdk, keys K E Rnxdk and values V E Rnxdv where n is the sequence length, d,\ndk, dv are the dimensions of inputs, queries (keys) and values, respectively. The scaled dot-product\nattention is applied on Q, K, V:"
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 1098
        },
        {
          "x": 2106,
          "y": 1098
        },
        {
          "x": 2106,
          "y": 1238
        },
        {
          "x": 442,
          "y": 1238
        }
      ],
      "category": "paragraph",
      "html": "<p id='22' style='font-size:18px'>Finally, a linear layer is used to produce the output. Multi-head self-attention splits the queries, keys<br>and values to h parts and perform the attention function in parallel, and then the output values of each<br>head are concatenated and linearly projected to form the final output.</p>",
      "id": 22,
      "page": 3,
      "text": "Finally, a linear layer is used to produce the output. Multi-head self-attention splits the queries, keys\nand values to h parts and perform the attention function in parallel, and then the output values of each\nhead are concatenated and linearly projected to form the final output."
    },
    {
      "bounding_box": [
        {
          "x": 444,
          "y": 1284
        },
        {
          "x": 2106,
          "y": 1284
        },
        {
          "x": 2106,
          "y": 1331
        },
        {
          "x": 444,
          "y": 1331
        }
      ],
      "category": "paragraph",
      "html": "<p id='23' style='font-size:18px'>MLP. The MLP is applied between self-attention layers for feature transformation and non-linearity:</p>",
      "id": 23,
      "page": 3,
      "text": "MLP. The MLP is applied between self-attention layers for feature transformation and non-linearity:"
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 1423
        },
        {
          "x": 2102,
          "y": 1423
        },
        {
          "x": 2102,
          "y": 1514
        },
        {
          "x": 442,
          "y": 1514
        }
      ],
      "category": "paragraph",
      "html": "<p id='24' style='font-size:18px'>where W and 6 are the weight and bias term of fully-connected layer respectively, and �(·) is the<br>activation function such as GELU [14].</p>",
      "id": 24,
      "page": 3,
      "text": "where W and 6 are the weight and bias term of fully-connected layer respectively, and �(·) is the\nactivation function such as GELU [14]."
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 1561
        },
        {
          "x": 2107,
          "y": 1561
        },
        {
          "x": 2107,
          "y": 1652
        },
        {
          "x": 443,
          "y": 1652
        }
      ],
      "category": "paragraph",
      "html": "<p id='25' style='font-size:16px'>LN. Layer normalization [1] is a key part in transformer for stable training and faster convergence.<br>LN is applied over each sample x E Rd follows:<br>as</p>",
      "id": 25,
      "page": 3,
      "text": "LN. Layer normalization [1] is a key part in transformer for stable training and faster convergence.\nLN is applied over each sample x E Rd follows:\nas"
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 1779
        },
        {
          "x": 2104,
          "y": 1779
        },
        {
          "x": 2104,
          "y": 1872
        },
        {
          "x": 442,
          "y": 1872
        }
      ],
      "category": "paragraph",
      "html": "<p id='26' style='font-size:16px'>where H E R, 8 E R are the mean and standard deviation of the feature respectively, ○ is the<br>element-wise dot, and 2 E Rd, B E Rd are learnable affine transform parameters.</p>",
      "id": 26,
      "page": 3,
      "text": "where H E R, 8 E R are the mean and standard deviation of the feature respectively, ○ is the\nelement-wise dot, and 2 E Rd, B E Rd are learnable affine transform parameters."
    },
    {
      "bounding_box": [
        {
          "x": 444,
          "y": 1923
        },
        {
          "x": 1054,
          "y": 1923
        },
        {
          "x": 1054,
          "y": 1971
        },
        {
          "x": 444,
          "y": 1971
        }
      ],
      "category": "paragraph",
      "html": "<p id='27' style='font-size:16px'>2.2 Transformer in Transformer</p>",
      "id": 27,
      "page": 3,
      "text": "2.2 Transformer in Transformer"
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 2006
        },
        {
          "x": 2107,
          "y": 2006
        },
        {
          "x": 2107,
          "y": 2282
        },
        {
          "x": 443,
          "y": 2282
        }
      ],
      "category": "paragraph",
      "html": "<p id='28' style='font-size:16px'>Given a 2D image, we uniformly split it into n patches X = [X1 , X2 , , Xn] E Rnxpxpx3 where<br>,<br>· · ·<br>(p,p) is the resolution of each image patch. ViT [10] just utilizes a standard transformer to process the<br>sequence of patches which corrupts the local structure of a patch, as shown in Fig. 1(a). Instead, we<br>propose Transformer-iN-Transformer (TNT) architecture to learn both global and local information<br>in an image. In TNT, we view the patches as visual sentences that represent the image. Each patch is<br>further divided into m sub-patches, i.e., a visual sentence is composed of a sequence of visual words:</p>",
      "id": 28,
      "page": 3,
      "text": "Given a 2D image, we uniformly split it into n patches X = [X1 , X2 , , Xn] E Rnxpxpx3 where\n,\n· · ·\n(p,p) is the resolution of each image patch. ViT [10] just utilizes a standard transformer to process the\nsequence of patches which corrupts the local structure of a patch, as shown in Fig. 1(a). Instead, we\npropose Transformer-iN-Transformer (TNT) architecture to learn both global and local information\nin an image. In TNT, we view the patches as visual sentences that represent the image. Each patch is\nfurther divided into m sub-patches, i.e., a visual sentence is composed of a sequence of visual words:"
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 2369
        },
        {
          "x": 2105,
          "y": 2369
        },
        {
          "x": 2105,
          "y": 2506
        },
        {
          "x": 443,
          "y": 2506
        }
      ],
      "category": "paragraph",
      "html": "<p id='29' style='font-size:16px'>where xi,j E Rsxsx3 is the j-th visual word of the i-th visual sentence, (s, s) is the spatial size of<br>sub-patches, and j = 1, 2, · · · m. With a linear projection, we transform the visual words into a<br>,<br>sequence of word embeddings:</p>",
      "id": 29,
      "page": 3,
      "text": "where xi,j E Rsxsx3 is the j-th visual word of the i-th visual sentence, (s, s) is the spatial size of\nsub-patches, and j = 1, 2, · · · m. With a linear projection, we transform the visual words into a\n,\nsequence of word embeddings:"
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 2599
        },
        {
          "x": 2104,
          "y": 2599
        },
        {
          "x": 2104,
          "y": 2688
        },
        {
          "x": 442,
          "y": 2688
        }
      ],
      "category": "paragraph",
      "html": "<p id='30' style='font-size:18px'>where yi,j E Rc is the j-th word embedding, cis the dimension of word embedding, and Vec(·) is<br>the vectorization operation.</p>",
      "id": 30,
      "page": 3,
      "text": "where yi,j E Rc is the j-th word embedding, cis the dimension of word embedding, and Vec(·) is\nthe vectorization operation."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 2711
        },
        {
          "x": 2106,
          "y": 2711
        },
        {
          "x": 2106,
          "y": 2846
        },
        {
          "x": 442,
          "y": 2846
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='31' style='font-size:14px'>In TNT, we have two data flows in which one flow operates across the visual sentences and the other<br>processes the visual words inside each sentence. For the word embeddings, we utilize a transformer<br>block to explore the relation between visual words:</p>",
      "id": 31,
      "page": 3,
      "text": "In TNT, we have two data flows in which one flow operates across the visual sentences and the other\nprocesses the visual words inside each sentence. For the word embeddings, we utilize a transformer\nblock to explore the relation between visual words:"
    },
    {
      "bounding_box": [
        {
          "x": 1260,
          "y": 3094
        },
        {
          "x": 1289,
          "y": 3094
        },
        {
          "x": 1289,
          "y": 3129
        },
        {
          "x": 1260,
          "y": 3129
        }
      ],
      "category": "footer",
      "html": "<footer id='32' style='font-size:14px'>3</footer>",
      "id": 32,
      "page": 3,
      "text": "3"
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 306
        },
        {
          "x": 2107,
          "y": 306
        },
        {
          "x": 2107,
          "y": 580
        },
        {
          "x": 442,
          "y": 580
        }
      ],
      "category": "paragraph",
      "html": "<p id='33' style='font-size:14px'>where l = 1, 2, · · · L is the index of the l-th block, and L is the total number of stacked blocks. The<br>,<br>input of the first block Yo is just Yo in Eq. 5. All word embeddings in the image after transformation<br>are Vi = [Yi1 , Y12 , , Yin]. This can be viewed as an inner transformer block, denoted as Tin. This<br>· · ·<br>process builds the relationships among visual words by computing interactions between any two<br>visual words. For example, in a patch of human face, a word corresponding to the eye is more related<br>to other words of eyes while interacts less with forehead part.</p>",
      "id": 33,
      "page": 4,
      "text": "where l = 1, 2, · · · L is the index of the l-th block, and L is the total number of stacked blocks. The\n,\ninput of the first block Yo is just Yo in Eq. 5. All word embeddings in the image after transformation\nare Vi = [Yi1 , Y12 , , Yin]. This can be viewed as an inner transformer block, denoted as Tin. This\n· · ·\nprocess builds the relationships among visual words by computing interactions between any two\nvisual words. For example, in a patch of human face, a word corresponding to the eye is more related\nto other words of eyes while interacts less with forehead part."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 603
        },
        {
          "x": 2108,
          "y": 603
        },
        {
          "x": 2108,
          "y": 835
        },
        {
          "x": 442,
          "y": 835
        }
      ],
      "category": "paragraph",
      "html": "<p id='34' style='font-size:14px'>For the sentence level, we create the sentence embedding memories to store the sequence of sentence-<br>level representations: Zo = [Zclass, Zo, Zo , · · · Zn] E R(n+1)xd where Zclass is the class token<br>,<br>similar to ViT [10], and all of them are initialized as zero. In each layer, the sequence of word<br>embeddings are transformed into the domain of sentence embedding by linear projection and added<br>into the sentence embedding:</p>",
      "id": 34,
      "page": 4,
      "text": "For the sentence level, we create the sentence embedding memories to store the sequence of sentence-\nlevel representations: Zo = [Zclass, Zo, Zo , · · · Zn] E R(n+1)xd where Zclass is the class token\n,\nsimilar to ViT [10], and all of them are initialized as zero. In each layer, the sequence of word\nembeddings are transformed into the domain of sentence embedding by linear projection and added\ninto the sentence embedding:"
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 944
        },
        {
          "x": 2105,
          "y": 944
        },
        {
          "x": 2105,
          "y": 1082
        },
        {
          "x": 443,
          "y": 1082
        }
      ],
      "category": "paragraph",
      "html": "<p id='35' style='font-size:16px'>where Zi-1 E Rd and the fully-connected layer FC makes the dimension match for addition. With the<br>above addition operation, the representation of sentence embedding is augmented by the word-level<br>features. We use the standard transformer block for transforming the sentence embeddings:</p>",
      "id": 35,
      "page": 4,
      "text": "where Zi-1 E Rd and the fully-connected layer FC makes the dimension match for addition. With the\nabove addition operation, the representation of sentence embedding is augmented by the word-level\nfeatures. We use the standard transformer block for transforming the sentence embeddings:"
    },
    {
      "bounding_box": [
        {
          "x": 444,
          "y": 1249
        },
        {
          "x": 2086,
          "y": 1249
        },
        {
          "x": 2086,
          "y": 1295
        },
        {
          "x": 444,
          "y": 1295
        }
      ],
      "category": "paragraph",
      "html": "<p id='36' style='font-size:18px'>This outer transformer block Tout is used for modeling relationships among sentence embeddings.</p>",
      "id": 36,
      "page": 4,
      "text": "This outer transformer block Tout is used for modeling relationships among sentence embeddings."
    },
    {
      "bounding_box": [
        {
          "x": 444,
          "y": 1320
        },
        {
          "x": 2104,
          "y": 1320
        },
        {
          "x": 2104,
          "y": 1408
        },
        {
          "x": 444,
          "y": 1408
        }
      ],
      "category": "paragraph",
      "html": "<p id='37' style='font-size:16px'>In summary, the inputs and outputs of the TNT block include the visual word embeddings and<br>sentence embeddings as shown in Fig. 1(b), SO the TNT can be formulated as</p>",
      "id": 37,
      "page": 4,
      "text": "In summary, the inputs and outputs of the TNT block include the visual word embeddings and\nsentence embeddings as shown in Fig. 1(b), SO the TNT can be formulated as"
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 1512
        },
        {
          "x": 2106,
          "y": 1512
        },
        {
          "x": 2106,
          "y": 1741
        },
        {
          "x": 442,
          "y": 1741
        }
      ],
      "category": "paragraph",
      "html": "<p id='38' style='font-size:16px'>In our TNT block, the inner transformer block is used to model the relationship between visual<br>words for local feature extraction, and the outer transformer block captures the intrinsic information<br>from the sequence of sentences. By stacking the TNT blocks for L times, we build the transformer-<br>in-transformer network. Finally, the classification token serves as the image representation and a<br>fully-connected layer is applied for classification.</p>",
      "id": 38,
      "page": 4,
      "text": "In our TNT block, the inner transformer block is used to model the relationship between visual\nwords for local feature extraction, and the outer transformer block captures the intrinsic information\nfrom the sequence of sentences. By stacking the TNT blocks for L times, we build the transformer-\nin-transformer network. Finally, the classification token serves as the image representation and a\nfully-connected layer is applied for classification."
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 1787
        },
        {
          "x": 2105,
          "y": 1787
        },
        {
          "x": 2105,
          "y": 1973
        },
        {
          "x": 443,
          "y": 1973
        }
      ],
      "category": "paragraph",
      "html": "<p id='39' style='font-size:16px'>Position encoding. Spatial information is an important factor in image recognition. For sentence<br>embeddings and word embeddings, we both add the corresponding position encodings to retain<br>spatial information as shown in Fig. 1. The standard learnable 1D position encodings are utilized<br>here. Specifically, each sentence is assigned with a position encodings:</p>",
      "id": 39,
      "page": 4,
      "text": "Position encoding. Spatial information is an important factor in image recognition. For sentence\nembeddings and word embeddings, we both add the corresponding position encodings to retain\nspatial information as shown in Fig. 1. The standard learnable 1D position encodings are utilized\nhere. Specifically, each sentence is assigned with a position encodings:"
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 2082
        },
        {
          "x": 2105,
          "y": 2082
        },
        {
          "x": 2105,
          "y": 2175
        },
        {
          "x": 443,
          "y": 2175
        }
      ],
      "category": "paragraph",
      "html": "<p id='40' style='font-size:16px'>where Esentence E R(n+1)xd<br>are the sentence position encodings. As for the visual words in a<br>sentence, a word position encoding is added to each word embedding:</p>",
      "id": 40,
      "page": 4,
      "text": "where Esentence E R(n+1)xd\nare the sentence position encodings. As for the visual words in a\nsentence, a word position encoding is added to each word embedding:"
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 2285
        },
        {
          "x": 2104,
          "y": 2285
        },
        {
          "x": 2104,
          "y": 2423
        },
        {
          "x": 442,
          "y": 2423
        }
      ],
      "category": "paragraph",
      "html": "<p id='41' style='font-size:16px'>where Eword E Rmxc the word position encodings which are shared across sentences. In this<br>are<br>way, sentence position encoding can maintain the global spatial information, while word position<br>encoding is used for preserving the local relative position.</p>",
      "id": 41,
      "page": 4,
      "text": "where Eword E Rmxc the word position encodings which are shared across sentences. In this\nare\nway, sentence position encoding can maintain the global spatial information, while word position\nencoding is used for preserving the local relative position."
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 2475
        },
        {
          "x": 912,
          "y": 2475
        },
        {
          "x": 912,
          "y": 2523
        },
        {
          "x": 443,
          "y": 2523
        }
      ],
      "category": "paragraph",
      "html": "<p id='42' style='font-size:20px'>2.3 Complexity Analysis</p>",
      "id": 42,
      "page": 4,
      "text": "2.3 Complexity Analysis"
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 2561
        },
        {
          "x": 2105,
          "y": 2561
        },
        {
          "x": 2105,
          "y": 2741
        },
        {
          "x": 443,
          "y": 2741
        }
      ],
      "category": "paragraph",
      "html": "<p id='43' style='font-size:16px'>A standard transformer block includes two parts, i.e., the multi-head self-attention and multi-layer<br>perceptron. The FLOPs of MSA are 2nd(dk + dv) + n2(dk + dv), and the FLOPs of MLP are<br>2ndvrdv where r is the dimension expansion ratio of hidden layer in MLP. Overall, the FLOPs of a<br>standard transformer block are</p>",
      "id": 43,
      "page": 4,
      "text": "A standard transformer block includes two parts, i.e., the multi-head self-attention and multi-layer\nperceptron. The FLOPs of MSA are 2nd(dk + dv) + n2(dk + dv), and the FLOPs of MLP are\n2ndvrdv where r is the dimension expansion ratio of hidden layer in MLP. Overall, the FLOPs of a\nstandard transformer block are"
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 2847
        },
        {
          "x": 2102,
          "y": 2847
        },
        {
          "x": 2102,
          "y": 2934
        },
        {
          "x": 443,
          "y": 2934
        }
      ],
      "category": "paragraph",
      "html": "<p id='44' style='font-size:14px'>Since r is usually set as 4, and the dimensions of input, key (query) and value are usually set as the<br>same, the FLOPs calculation can be simplified as</p>",
      "id": 44,
      "page": 4,
      "text": "Since r is usually set as 4, and the dimensions of input, key (query) and value are usually set as the\nsame, the FLOPs calculation can be simplified as"
    },
    {
      "bounding_box": [
        {
          "x": 1260,
          "y": 3096
        },
        {
          "x": 1287,
          "y": 3096
        },
        {
          "x": 1287,
          "y": 3125
        },
        {
          "x": 1260,
          "y": 3125
        }
      ],
      "category": "footer",
      "html": "<footer id='45' style='font-size:14px'>4</footer>",
      "id": 45,
      "page": 4,
      "text": "4"
    },
    {
      "bounding_box": [
        {
          "x": 445,
          "y": 308
        },
        {
          "x": 1206,
          "y": 308
        },
        {
          "x": 1206,
          "y": 351
        },
        {
          "x": 445,
          "y": 351
        }
      ],
      "category": "paragraph",
      "html": "<p id='46' style='font-size:16px'>The number of parameters can be obtained as</p>",
      "id": 46,
      "page": 5,
      "text": "The number of parameters can be obtained as"
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 489
        },
        {
          "x": 2108,
          "y": 489
        },
        {
          "x": 2108,
          "y": 668
        },
        {
          "x": 442,
          "y": 668
        }
      ],
      "category": "paragraph",
      "html": "<p id='47' style='font-size:18px'>Our TNT block consists of three parts: an inner transformer block Tin, an outer transformer block<br>Tout and a linear layer. The computation complexity of Tin and Tout are 2nmc(6c + m) and<br>2nd(6d + n) respectively. The linear layer has FLOPs of nmcd. In total, the FLOPs of TNT block<br>are</p>",
      "id": 47,
      "page": 5,
      "text": "Our TNT block consists of three parts: an inner transformer block Tin, an outer transformer block\nTout and a linear layer. The computation complexity of Tin and Tout are 2nmc(6c + m) and\n2nd(6d + n) respectively. The linear layer has FLOPs of nmcd. In total, the FLOPs of TNT block\nare"
    },
    {
      "bounding_box": [
        {
          "x": 445,
          "y": 749
        },
        {
          "x": 1551,
          "y": 749
        },
        {
          "x": 1551,
          "y": 794
        },
        {
          "x": 445,
          "y": 794
        }
      ],
      "category": "paragraph",
      "html": "<p id='48' style='font-size:18px'>Similarly, the parameter complexity of TNT block is calculated as</p>",
      "id": 48,
      "page": 5,
      "text": "Similarly, the parameter complexity of TNT block is calculated as"
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 931
        },
        {
          "x": 2105,
          "y": 931
        },
        {
          "x": 2105,
          "y": 1296
        },
        {
          "x": 443,
          "y": 1296
        }
      ],
      "category": "paragraph",
      "html": "<p id='49' style='font-size:16px'>Although we add two more components in our TNT block, the increase of FLOPs is small since<br>c 《 d and 0(m) 2 O(n) in practice. For example, in the DeiT-S configuration, we have d = 384<br>and n = 196. We set c = 24 and m = 16 in our structure of TNT-S correspondingly. From Eq. 15<br>and Eq. 17, we can obtain that FLOPsT = 376M and FLOPSTNT = 429M. The FLOPs ratio of<br>TNT block over standard transformer block is about 1.14x. Similarly, the parameters ratio is about<br>1.08x. With a small increase of computation and memory cost, our TNT block can efficiently model<br>the local structure information and achieve a much better trade-off between accuracy and complexity<br>as demonstrated in the experiments.</p>",
      "id": 49,
      "page": 5,
      "text": "Although we add two more components in our TNT block, the increase of FLOPs is small since\nc 《 d and 0(m) 2 O(n) in practice. For example, in the DeiT-S configuration, we have d = 384\nand n = 196. We set c = 24 and m = 16 in our structure of TNT-S correspondingly. From Eq. 15\nand Eq. 17, we can obtain that FLOPsT = 376M and FLOPSTNT = 429M. The FLOPs ratio of\nTNT block over standard transformer block is about 1.14x. Similarly, the parameters ratio is about\n1.08x. With a small increase of computation and memory cost, our TNT block can efficiently model\nthe local structure information and achieve a much better trade-off between accuracy and complexity\nas demonstrated in the experiments."
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 1355
        },
        {
          "x": 938,
          "y": 1355
        },
        {
          "x": 938,
          "y": 1404
        },
        {
          "x": 443,
          "y": 1404
        }
      ],
      "category": "paragraph",
      "html": "<p id='50' style='font-size:18px'>2.4 Network Architecture</p>",
      "id": 50,
      "page": 5,
      "text": "2.4 Network Architecture"
    },
    {
      "bounding_box": [
        {
          "x": 441,
          "y": 1443
        },
        {
          "x": 2108,
          "y": 1443
        },
        {
          "x": 2108,
          "y": 1717
        },
        {
          "x": 441,
          "y": 1717
        }
      ],
      "category": "paragraph",
      "html": "<p id='51' style='font-size:18px'>We build our TNT architectures by following the basic configuration of ViT [10] and DeiT [35]. The<br>patch size is set as 16x 16. The number of sub-patches is set as m = 4 · 4 = 16 by default. Other<br>size values are evaluated in the ablation studies. As shown in Table 1, there are three variants of<br>TNT networks with different model sizes, namely, TNT-Ti, TNT-S and TNT-B. They consist of 6.1M,<br>23.8M and 65.6M parameters respectively. The corresponding FLOPs for processing a 224x224<br>image are 1.4B, 5.2B and 14.1B respectively.</p>",
      "id": 51,
      "page": 5,
      "text": "We build our TNT architectures by following the basic configuration of ViT [10] and DeiT [35]. The\npatch size is set as 16x 16. The number of sub-patches is set as m = 4 · 4 = 16 by default. Other\nsize values are evaluated in the ablation studies. As shown in Table 1, there are three variants of\nTNT networks with different model sizes, namely, TNT-Ti, TNT-S and TNT-B. They consist of 6.1M,\n23.8M and 65.6M parameters respectively. The corresponding FLOPs for processing a 224x224\nimage are 1.4B, 5.2B and 14.1B respectively."
    },
    {
      "bounding_box": [
        {
          "x": 444,
          "y": 1742
        },
        {
          "x": 2109,
          "y": 1742
        },
        {
          "x": 2109,
          "y": 1832
        },
        {
          "x": 444,
          "y": 1832
        }
      ],
      "category": "caption",
      "html": "<caption id='52' style='font-size:16px'>Table 1: Variants of our TNT architecture. 'Ti' means tiny, 'S' means small, and 'B' means base.<br>The FLOPs are calculated for images at resolution 224x224.</caption>",
      "id": 52,
      "page": 5,
      "text": "Table 1: Variants of our TNT architecture. 'Ti' means tiny, 'S' means small, and 'B' means base.\nThe FLOPs are calculated for images at resolution 224x224."
    },
    {
      "bounding_box": [
        {
          "x": 478,
          "y": 1862
        },
        {
          "x": 2056,
          "y": 1862
        },
        {
          "x": 2056,
          "y": 2137
        },
        {
          "x": 478,
          "y": 2137
        }
      ],
      "category": "table",
      "html": "<table id='53' style='font-size:14px'><tr><td rowspan=\"2\">Model</td><td rowspan=\"2\">Depth</td><td colspan=\"3\">Inner transformer</td><td colspan=\"3\">Outer transformer</td><td rowspan=\"2\">Params (M)</td><td rowspan=\"2\">FLOPs (B)</td></tr><tr><td>dim c</td><td>#heads</td><td>MLP r</td><td>dim d</td><td>#heads</td><td>MLP r</td></tr><tr><td>TNT-Ti</td><td>12</td><td>12</td><td>2</td><td>4</td><td>192</td><td>3</td><td>4</td><td>6.1</td><td>1.4</td></tr><tr><td>TNT-S</td><td>12</td><td>24</td><td>4</td><td>4</td><td>384</td><td>6</td><td>4</td><td>23.8</td><td>5.2</td></tr><tr><td>TNT-B</td><td>12</td><td>40</td><td>4</td><td>4</td><td>640</td><td>10</td><td>4</td><td>65.6</td><td>14.1</td></tr></table>",
      "id": 53,
      "page": 5,
      "text": "Model Depth Inner transformer Outer transformer Params (M) FLOPs (B)\n dim c #heads MLP r dim d #heads MLP r\n TNT-Ti 12 12 2 4 192 3 4 6.1 1.4\n TNT-S 12 24 4 4 384 6 4 23.8 5.2\n TNT-B 12 40 4 4 640 10 4 65.6"
    },
    {
      "bounding_box": [
        {
          "x": 444,
          "y": 2202
        },
        {
          "x": 801,
          "y": 2202
        },
        {
          "x": 801,
          "y": 2258
        },
        {
          "x": 444,
          "y": 2258
        }
      ],
      "category": "paragraph",
      "html": "<p id='54' style='font-size:22px'>3 Experiments</p>",
      "id": 54,
      "page": 5,
      "text": "3 Experiments"
    },
    {
      "bounding_box": [
        {
          "x": 441,
          "y": 2309
        },
        {
          "x": 2104,
          "y": 2309
        },
        {
          "x": 2104,
          "y": 2401
        },
        {
          "x": 441,
          "y": 2401
        }
      ],
      "category": "paragraph",
      "html": "<p id='55' style='font-size:16px'>In this section, we conduct extensive experiments on visual benchmarks to evaluate the effectiveness<br>of the proposed TNT architecture.</p>",
      "id": 55,
      "page": 5,
      "text": "In this section, we conduct extensive experiments on visual benchmarks to evaluate the effectiveness\nof the proposed TNT architecture."
    },
    {
      "bounding_box": [
        {
          "x": 945,
          "y": 2449
        },
        {
          "x": 1600,
          "y": 2449
        },
        {
          "x": 1600,
          "y": 2488
        },
        {
          "x": 945,
          "y": 2488
        }
      ],
      "category": "caption",
      "html": "<caption id='56' style='font-size:18px'>Table 2: Details of used visual datasets.</caption>",
      "id": 56,
      "page": 5,
      "text": "Table 2: Details of used visual datasets."
    },
    {
      "bounding_box": [
        {
          "x": 638,
          "y": 2492
        },
        {
          "x": 1894,
          "y": 2492
        },
        {
          "x": 1894,
          "y": 2989
        },
        {
          "x": 638,
          "y": 2989
        }
      ],
      "category": "table",
      "html": "<br><table id='57' style='font-size:18px'><tr><td>Dataset</td><td>Type</td><td>Train size</td><td>Val size</td><td>#Classes</td></tr><tr><td>ImageNet [30]</td><td>Pretrain</td><td>1,281,167</td><td>50,000</td><td>1,000</td></tr><tr><td>Oxford 102 Flowers [25]</td><td rowspan=\"5\">Classification</td><td>2,040</td><td>6,149</td><td>102</td></tr><tr><td>Oxford-IIIT Pets [26]</td><td>3,680</td><td>3,669</td><td>37</td></tr><tr><td>iNaturalist 2019 [38]</td><td>265,240</td><td>3,003</td><td>1,010</td></tr><tr><td>CIFAR-10 [18]</td><td>50,000</td><td>10,000</td><td>10</td></tr><tr><td>CIFAR-100 [18]</td><td>50,000</td><td>10,000</td><td>100</td></tr><tr><td>COCO2017 [22]</td><td>Detection</td><td>118,287</td><td>I 5,000</td><td>80</td></tr><tr><td>ADE20K [49]</td><td>Segmentation</td><td>20,210</td><td>2,000</td><td>150</td></tr></table>",
      "id": 57,
      "page": 5,
      "text": "Dataset Type Train size Val size #Classes\n ImageNet [30] Pretrain 1,281,167 50,000 1,000\n Oxford 102 Flowers [25] Classification 2,040 6,149 102\n Oxford-IIIT Pets [26] 3,680 3,669 37\n iNaturalist 2019 [38] 265,240 3,003 1,010\n CIFAR-10 [18] 50,000 10,000 10\n CIFAR-100 [18] 50,000 10,000 100\n COCO2017 [22] Detection 118,287 I 5,000 80\n ADE20K [49] Segmentation 20,210 2,000"
    },
    {
      "bounding_box": [
        {
          "x": 1260,
          "y": 3093
        },
        {
          "x": 1289,
          "y": 3093
        },
        {
          "x": 1289,
          "y": 3129
        },
        {
          "x": 1260,
          "y": 3129
        }
      ],
      "category": "footer",
      "html": "<footer id='58' style='font-size:14px'>5</footer>",
      "id": 58,
      "page": 5,
      "text": "5"
    },
    {
      "bounding_box": [
        {
          "x": 445,
          "y": 306
        },
        {
          "x": 1178,
          "y": 306
        },
        {
          "x": 1178,
          "y": 353
        },
        {
          "x": 445,
          "y": 353
        }
      ],
      "category": "paragraph",
      "html": "<p id='59' style='font-size:22px'>3.1 Datasets and Experimental Settings</p>",
      "id": 59,
      "page": 6,
      "text": "3.1 Datasets and Experimental Settings"
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 390
        },
        {
          "x": 2110,
          "y": 390
        },
        {
          "x": 2110,
          "y": 620
        },
        {
          "x": 442,
          "y": 620
        }
      ],
      "category": "paragraph",
      "html": "<p id='60' style='font-size:20px'>Datasets. ImageNet ILSVRC 2012 [30] is an image classification benchmark consisting of 1.2M<br>training images belonging to 1000 classes, and 50K validation images with 50 images per class. We<br>adopt the same data augmentation strategy as that in DeiT [35] including random crop, random clip,<br>Rand-Augment [8], Random Erasing [48], Mixup [46] and CutMix [45]. For the license of ImageNet<br>dataset, please refer to http : / / www · image-net · org/ downl oad.</p>",
      "id": 60,
      "page": 6,
      "text": "Datasets. ImageNet ILSVRC 2012 [30] is an image classification benchmark consisting of 1.2M\ntraining images belonging to 1000 classes, and 50K validation images with 50 images per class. We\nadopt the same data augmentation strategy as that in DeiT [35] including random crop, random clip,\nRand-Augment [8], Random Erasing [48], Mixup [46] and CutMix [45]. For the license of ImageNet\ndataset, please refer to http : / / www · image-net · org/ downl oad."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 641
        },
        {
          "x": 2109,
          "y": 641
        },
        {
          "x": 2109,
          "y": 870
        },
        {
          "x": 442,
          "y": 870
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='61' style='font-size:16px'>In addition to ImageNet, we also test on the downstream tasks with transfer learning to evaluate<br>the generalization ability of TNT. The details of used visual datasets are listed in Table 2. The data<br>augmentation strategy of image classification datasets are the same as that of ImageNet. For COCO<br>and ADE20K, the data augmentation strategy follows that in PVT [40]. For the licenses of these<br>datasets, please refer to the original papers.</p>",
      "id": 61,
      "page": 6,
      "text": "In addition to ImageNet, we also test on the downstream tasks with transfer learning to evaluate\nthe generalization ability of TNT. The details of used visual datasets are listed in Table 2. The data\naugmentation strategy of image classification datasets are the same as that of ImageNet. For COCO\nand ADE20K, the data augmentation strategy follows that in PVT [40]. For the licenses of these\ndatasets, please refer to the original papers."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 921
        },
        {
          "x": 2108,
          "y": 921
        },
        {
          "x": 2108,
          "y": 1195
        },
        {
          "x": 442,
          "y": 1195
        }
      ],
      "category": "paragraph",
      "html": "<p id='62' style='font-size:20px'>Implementation Details. We utilize the training strategy provided in DeiT [35]. The main ad-<br>vanced technologies apart from common settings [13] include Adam W [23], label smoothing [31],<br>DropPath [20], and repeated augmentation [15]. We list the hyper-parameters in Table 3 for better<br>understanding. All the models are implemented with PyTorch [27] and MindSpore [17] and trained on<br>NVIDIA Tesla V100 GPUs. The potential negative societal impacts may include energy consumption<br>and carbon dioxide emissions of GPU computation.</p>",
      "id": 62,
      "page": 6,
      "text": "Implementation Details. We utilize the training strategy provided in DeiT [35]. The main ad-\nvanced technologies apart from common settings [13] include Adam W [23], label smoothing [31],\nDropPath [20], and repeated augmentation [15]. We list the hyper-parameters in Table 3 for better\nunderstanding. All the models are implemented with PyTorch [27] and MindSpore [17] and trained on\nNVIDIA Tesla V100 GPUs. The potential negative societal impacts may include energy consumption\nand carbon dioxide emissions of GPU computation."
    },
    {
      "bounding_box": [
        {
          "x": 550,
          "y": 1209
        },
        {
          "x": 1995,
          "y": 1209
        },
        {
          "x": 1995,
          "y": 1253
        },
        {
          "x": 550,
          "y": 1253
        }
      ],
      "category": "caption",
      "html": "<br><caption id='63' style='font-size:18px'>Table 3: Default training hyper-parameters used in our method, unless stated otherwise.</caption>",
      "id": 63,
      "page": 6,
      "text": "Table 3: Default training hyper-parameters used in our method, unless stated otherwise."
    },
    {
      "bounding_box": [
        {
          "x": 455,
          "y": 1257
        },
        {
          "x": 2097,
          "y": 1257
        },
        {
          "x": 2097,
          "y": 1438
        },
        {
          "x": 455,
          "y": 1438
        }
      ],
      "category": "table",
      "html": "<br><table id='64' style='font-size:16px'><tr><td>Epochs</td><td>Optimizer</td><td>Batch size</td><td>Learning rate</td><td>LR decay</td><td>Weight decay</td><td>Warmup epochs</td><td>Label smooth</td><td>Drop path</td><td>Repeated Aug</td></tr><tr><td>300</td><td>Adam W</td><td>1024</td><td>1e-3</td><td>cosine</td><td>0.05</td><td>5</td><td>0.1</td><td>0.1</td><td>V</td></tr></table>",
      "id": 64,
      "page": 6,
      "text": "Epochs Optimizer Batch size Learning rate LR decay Weight decay Warmup epochs Label smooth Drop path Repeated Aug\n 300 Adam W 1024 1e-3 cosine 0.05 5 0.1 0.1"
    },
    {
      "bounding_box": [
        {
          "x": 444,
          "y": 1490
        },
        {
          "x": 870,
          "y": 1490
        },
        {
          "x": 870,
          "y": 1537
        },
        {
          "x": 444,
          "y": 1537
        }
      ],
      "category": "paragraph",
      "html": "<p id='65' style='font-size:18px'>3.2 TNT on ImageNet</p>",
      "id": 65,
      "page": 6,
      "text": "3.2 TNT on ImageNet"
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 1573
        },
        {
          "x": 2109,
          "y": 1573
        },
        {
          "x": 2109,
          "y": 2032
        },
        {
          "x": 442,
          "y": 2032
        }
      ],
      "category": "paragraph",
      "html": "<p id='66' style='font-size:16px'>We train our TNT models with the same training settings as that of DeiT [35]. The recent transformer-<br>based models like ViT [10] and DeiT [35] are compared. To have a better understanding of current<br>progress of visual transformers, we also include the representative CNN-based models such as<br>ResNet [13], RegNet [28] and EfficientNet [32]. The results are shown in Table 4. We can see<br>that our transformer-based model, i.e., TNT outperforms all other visual transformer models. In<br>particular, TNT-S achieves 81.5% top-1 accuracy which is 1.7% higher than the baseline model DeiT-<br>S, indicating the benefit of the introduced TNT framework to preserve local structure information<br>inside the patch. Compared to CNNs, TNT can outperform the widely-used ResNet and RegNet.<br>Note that all the transformer-based models are still inferior to EfficientNet which utilizes special<br>depth-wise convolutions, so it is yet a challenge of how to beat EfficientNet using pure transformer.</p>",
      "id": 66,
      "page": 6,
      "text": "We train our TNT models with the same training settings as that of DeiT [35]. The recent transformer-\nbased models like ViT [10] and DeiT [35] are compared. To have a better understanding of current\nprogress of visual transformers, we also include the representative CNN-based models such as\nResNet [13], RegNet [28] and EfficientNet [32]. The results are shown in Table 4. We can see\nthat our transformer-based model, i.e., TNT outperforms all other visual transformer models. In\nparticular, TNT-S achieves 81.5% top-1 accuracy which is 1.7% higher than the baseline model DeiT-\nS, indicating the benefit of the introduced TNT framework to preserve local structure information\ninside the patch. Compared to CNNs, TNT can outperform the widely-used ResNet and RegNet.\nNote that all the transformer-based models are still inferior to EfficientNet which utilizes special\ndepth-wise convolutions, so it is yet a challenge of how to beat EfficientNet using pure transformer."
    },
    {
      "bounding_box": [
        {
          "x": 787,
          "y": 2045
        },
        {
          "x": 1758,
          "y": 2045
        },
        {
          "x": 1758,
          "y": 2086
        },
        {
          "x": 787,
          "y": 2086
        }
      ],
      "category": "caption",
      "html": "<br><caption id='67' style='font-size:16px'>Table 4: Results of TNT and other networks on ImageNet.</caption>",
      "id": 67,
      "page": 6,
      "text": "Table 4: Results of TNT and other networks on ImageNet."
    },
    {
      "bounding_box": [
        {
          "x": 512,
          "y": 2086
        },
        {
          "x": 2025,
          "y": 2086
        },
        {
          "x": 2025,
          "y": 3005
        },
        {
          "x": 512,
          "y": 3005
        }
      ],
      "category": "table",
      "html": "<br><table id='68' style='font-size:14px'><tr><td>Model</td><td>Resolution</td><td>Params (M)</td><td>FLOPs (B)</td><td>Top-1</td><td>Top-5</td></tr><tr><td>CNN-based</td><td></td><td></td><td></td><td></td><td>92.9</td></tr><tr><td>ResNet-50 [13] ResNet-152 [13] RegNetY-8GF [28] RegNetY-16GF [28] EfficientNet-B3 [32] EfficientNet-B4 [32]</td><td>224x224 224x224 224x224 224x224 300x300 380x380</td><td>25.6 60.2 39.2 83.6 12.0 19.0</td><td>4.1 11.5 8.0 15.9 1.8 4.2</td><td>76.2 78.3 79.9 80.4 81.6 82.9</td><td>94.1 - - 94.9 96.4</td></tr><tr><td>DeiT-Ti [35] TNT-Ti</td><td>224x224 224x224</td><td>5.7</td><td>1.3 1.4</td><td>72.2</td><td></td></tr><tr><td>Transformer-based</td><td>224x224</td><td>6.1 22.1</td><td></td><td>73.9</td><td>91.9</td></tr><tr><td></td><td>224x224 224x224</td><td>24.5 21.5</td><td></td><td>79.8 79.8 80.7 81.5</td><td></td></tr><tr><td></td><td></td><td>86.4</td><td>4.6 3.8 5.2 5.2</td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>DeiT-S [35] PVT-Small [40] T2T-ViT_t-14 [44] TNT-S ViT-B/16 [10] DeiT-B [35] T2T-ViT_t-24 [44]</td><td>224x224 384x 384 224x224</td><td>23.8 86.4 63.9</td><td>55.5 17.6 13.2</td><td>77.9 81.8 82.2 82.9</td><td>- - 95.7 - 96.3</td></tr><tr><td>TNT-B</td><td>224x224 224x224</td><td>65.6</td><td>14.1</td><td></td><td>-</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>-</td></tr></table>",
      "id": 68,
      "page": 6,
      "text": "Model Resolution Params (M) FLOPs (B) Top-1 Top-5\n CNN-based     92.9\n ResNet-50 [13] ResNet-152 [13] RegNetY-8GF [28] RegNetY-16GF [28] EfficientNet-B3 [32] EfficientNet-B4 [32] 224x224 224x224 224x224 224x224 300x300 380x380 25.6 60.2 39.2 83.6 12.0 19.0 4.1 11.5 8.0 15.9 1.8 4.2 76.2 78.3 79.9 80.4 81.6 82.9 94.1 - - 94.9 96.4\n DeiT-Ti [35] TNT-Ti 224x224 224x224 5.7 1.3 1.4 72.2 \n Transformer-based 224x224 6.1 22.1  73.9 91.9\n  224x224 224x224 24.5 21.5  79.8 79.8 80.7 81.5 \n   86.4 4.6 3.8 5.2 5.2  \n      \n      \n DeiT-S [35] PVT-Small [40] T2T-ViT_t-14 [44] TNT-S ViT-B/16 [10] DeiT-B [35] T2T-ViT_t-24 [44] 224x224 384x 384 224x224 23.8 86.4 63.9 55.5 17.6 13.2 77.9 81.8 82.2 82.9 - - 95.7 - 96.3\n TNT-B 224x224 224x224 65.6 14.1  -"
    },
    {
      "bounding_box": [
        {
          "x": 1259,
          "y": 3097
        },
        {
          "x": 1288,
          "y": 3097
        },
        {
          "x": 1288,
          "y": 3131
        },
        {
          "x": 1259,
          "y": 3131
        }
      ],
      "category": "footer",
      "html": "<footer id='69' style='font-size:14px'>6</footer>",
      "id": 69,
      "page": 6,
      "text": "6"
    },
    {
      "bounding_box": [
        {
          "x": 508,
          "y": 315
        },
        {
          "x": 2029,
          "y": 315
        },
        {
          "x": 2029,
          "y": 910
        },
        {
          "x": 508,
          "y": 910
        }
      ],
      "category": "figure",
      "html": "<figure><img id='70' style='font-size:14px' alt=\"83 83\n82 82\n81 81\n(%) 80 (%) 80\nAccuracy\nAccuracy\n79 79\n78 78\n△ △\n77 77\nTNT ResNet TNT ResNet\n76 A· ViT ResNeXt 76 A· ViT ResNeXt\nDeiT RegNet DeiT RegNet\n75 75\n20 30 40 50 60 70 80 90 4 6 8 10 12 14 16 18 20\nParams (M) FLOPs (B)\" data-coord=\"top-left:(508,315); bottom-right:(2029,910)\" /></figure>",
      "id": 70,
      "page": 7,
      "text": "83 83\n82 82\n81 81\n(%) 80 (%) 80\nAccuracy\nAccuracy\n79 79\n78 78\n△ △\n77 77\nTNT ResNet TNT ResNet\n76 A· ViT ResNeXt 76 A· ViT ResNeXt\nDeiT RegNet DeiT RegNet\n75 75\n20 30 40 50 60 70 80 90 4 6 8 10 12 14 16 18 20\nParams (M) FLOPs (B)"
    },
    {
      "bounding_box": [
        {
          "x": 460,
          "y": 896
        },
        {
          "x": 2082,
          "y": 896
        },
        {
          "x": 2082,
          "y": 992
        },
        {
          "x": 460,
          "y": 992
        }
      ],
      "category": "caption",
      "html": "<br><caption id='71' style='font-size:16px'>(a) Acc V.S. Params (b) Acc V.S. FLOPs<br>Figure 2: Performance comparison of the representative visual backbone networks on ImageNet.</caption>",
      "id": 71,
      "page": 7,
      "text": "(a) Acc V.S. Params (b) Acc V.S. FLOPs\nFigure 2: Performance comparison of the representative visual backbone networks on ImageNet."
    },
    {
      "bounding_box": [
        {
          "x": 441,
          "y": 1034
        },
        {
          "x": 2107,
          "y": 1034
        },
        {
          "x": 2107,
          "y": 1174
        },
        {
          "x": 441,
          "y": 1174
        }
      ],
      "category": "paragraph",
      "html": "<p id='72' style='font-size:18px'>We also plot the accuracy-parameters and accuracy-FLOPs line charts in Fig. 2 to have an intuitive<br>comparison of these models. Our TNT models consistently outperform other transformer-based<br>models by a significant margin.</p>",
      "id": 72,
      "page": 7,
      "text": "We also plot the accuracy-parameters and accuracy-FLOPs line charts in Fig. 2 to have an intuitive\ncomparison of these models. Our TNT models consistently outperform other transformer-based\nmodels by a significant margin."
    },
    {
      "bounding_box": [
        {
          "x": 441,
          "y": 1220
        },
        {
          "x": 2108,
          "y": 1220
        },
        {
          "x": 2108,
          "y": 1543
        },
        {
          "x": 441,
          "y": 1543
        }
      ],
      "category": "paragraph",
      "html": "<p id='73' style='font-size:18px'>Inference speed. Deployment of transformer models on devices is important for practical applica-<br>tions, SO we test the inference speed of our TNT model. Following [35], the throughput is measured<br>on an NVIDIA V100 GPU and PyTorch, with 224x224 input size. Since the resolution and content<br>inside the patch is smaller than that of the whole image, we may need fewer blocks to learn its<br>representation. Thus, we can reduce the used TNT blocks and replace some with vanilla transformer<br>blocks. From the results in Table 5, we can see that our TNT is more efficient than DeiT and PVT by<br>achieving higher accuracy with similar inference speed.</p>",
      "id": 73,
      "page": 7,
      "text": "Inference speed. Deployment of transformer models on devices is important for practical applica-\ntions, SO we test the inference speed of our TNT model. Following [35], the throughput is measured\non an NVIDIA V100 GPU and PyTorch, with 224x224 input size. Since the resolution and content\ninside the patch is smaller than that of the whole image, we may need fewer blocks to learn its\nrepresentation. Thus, we can reduce the used TNT blocks and replace some with vanilla transformer\nblocks. From the results in Table 5, we can see that our TNT is more efficient than DeiT and PVT by\nachieving higher accuracy with similar inference speed."
    },
    {
      "bounding_box": [
        {
          "x": 707,
          "y": 1580
        },
        {
          "x": 1840,
          "y": 1580
        },
        {
          "x": 1840,
          "y": 1627
        },
        {
          "x": 707,
          "y": 1627
        }
      ],
      "category": "caption",
      "html": "<caption id='74' style='font-size:18px'>Table 5: GPU throughput comparison of vision transformer models.</caption>",
      "id": 74,
      "page": 7,
      "text": "Table 5: GPU throughput comparison of vision transformer models."
    },
    {
      "bounding_box": [
        {
          "x": 449,
          "y": 1654
        },
        {
          "x": 2099,
          "y": 1654
        },
        {
          "x": 2099,
          "y": 2151
        },
        {
          "x": 449,
          "y": 2151
        }
      ],
      "category": "table",
      "html": "<table id='75' style='font-size:14px'><tr><td>Model</td><td>Indices of TNT blocks</td><td>FLOPs (B)</td><td>Throughput (images/s)</td><td>Top-1</td></tr><tr><td>DeiT-S [35]</td><td>-</td><td>4.6</td><td>907</td><td>79.8</td></tr><tr><td>DeiT-B [35]</td><td>-</td><td>17.6</td><td>292</td><td>81.8</td></tr><tr><td>PVT-Small [40]</td><td>-</td><td>3.8</td><td>820</td><td>79.8</td></tr><tr><td>PVT-Medium [40]</td><td></td><td>6.7</td><td>526</td><td>81.2</td></tr><tr><td>TNT-S</td><td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]</td><td>5.2</td><td>428</td><td>81.5</td></tr><tr><td>TNT-S-1</td><td>[1, 4, 8, 12]</td><td>4.8</td><td>668</td><td>81.4</td></tr><tr><td>TNT-S-2</td><td>[1, 6, 12]</td><td>4.7</td><td>704</td><td>81.3</td></tr><tr><td>TNT-S-3</td><td>[1, 6]</td><td>4.7</td><td>757</td><td>81.1</td></tr><tr><td>TNT-S-4</td><td>[1]</td><td>4.6</td><td>822</td><td>80.8</td></tr></table>",
      "id": 75,
      "page": 7,
      "text": "Model Indices of TNT blocks FLOPs (B) Throughput (images/s) Top-1\n DeiT-S [35] - 4.6 907 79.8\n DeiT-B [35] - 17.6 292 81.8\n PVT-Small [40] - 3.8 820 79.8\n PVT-Medium [40]  6.7 526 81.2\n TNT-S [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12] 5.2 428 81.5\n TNT-S-1 [1, 4, 8, 12] 4.8 668 81.4\n TNT-S-2 [1, 6, 12] 4.7 704 81.3\n TNT-S-3 [1, 6] 4.7 757 81.1\n TNT-S-4 [1] 4.6 822"
    },
    {
      "bounding_box": [
        {
          "x": 445,
          "y": 2194
        },
        {
          "x": 843,
          "y": 2194
        },
        {
          "x": 843,
          "y": 2240
        },
        {
          "x": 445,
          "y": 2240
        }
      ],
      "category": "paragraph",
      "html": "<p id='76' style='font-size:18px'>3.3 Ablation Studies</p>",
      "id": 76,
      "page": 7,
      "text": "3.3 Ablation Studies"
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 2277
        },
        {
          "x": 1242,
          "y": 2277
        },
        {
          "x": 1242,
          "y": 2716
        },
        {
          "x": 443,
          "y": 2716
        }
      ],
      "category": "paragraph",
      "html": "<p id='77' style='font-size:20px'>Effect of position encodings. Position infor-<br>mation is important for image recognition. In<br>TNT structure, sentence position encoding is<br>for maintaining global spatial information, and<br>word position encoding is used to preserve lo-<br>cally relative position. We verify their effect by<br>removing them separately. As shown in Table 6,<br>we can see that TNT-S with both patch position<br>encoding and word position encoding performs<br>the best by achieving 81.5% top-1 accuracy.</p>",
      "id": 77,
      "page": 7,
      "text": "Effect of position encodings. Position infor-\nmation is important for image recognition. In\nTNT structure, sentence position encoding is\nfor maintaining global spatial information, and\nword position encoding is used to preserve lo-\ncally relative position. We verify their effect by\nremoving them separately. As shown in Table 6,\nwe can see that TNT-S with both patch position\nencoding and word position encoding performs\nthe best by achieving 81.5% top-1 accuracy."
    },
    {
      "bounding_box": [
        {
          "x": 1377,
          "y": 2277
        },
        {
          "x": 1988,
          "y": 2277
        },
        {
          "x": 1988,
          "y": 2319
        },
        {
          "x": 1377,
          "y": 2319
        }
      ],
      "category": "caption",
      "html": "<br><caption id='78' style='font-size:20px'>Table 6: Effect of position encoding.</caption>",
      "id": 78,
      "page": 7,
      "text": "Table 6: Effect of position encoding."
    },
    {
      "bounding_box": [
        {
          "x": 1300,
          "y": 2326
        },
        {
          "x": 2053,
          "y": 2326
        },
        {
          "x": 2053,
          "y": 2662
        },
        {
          "x": 1300,
          "y": 2662
        }
      ],
      "category": "table",
      "html": "<br><table id='79' style='font-size:14px'><tr><td></td><td colspan=\"2\">Position encoding</td><td></td></tr><tr><td>Model</td><td>Sentence-level</td><td>Word-level</td><td>Top-1</td></tr><tr><td rowspan=\"4\">TNT-S</td><td>X</td><td>X</td><td>80.5</td></tr><tr><td></td><td></td><td>80.8</td></tr><tr><td></td><td></td><td>80.7</td></tr><tr><td></td><td>V</td><td>81.5</td></tr></table>",
      "id": 79,
      "page": 7,
      "text": "Position encoding \n Model Sentence-level Word-level Top-1\n TNT-S X X 80.5\n   80.8\n   80.7\n  V"
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 2693
        },
        {
          "x": 2103,
          "y": 2693
        },
        {
          "x": 2103,
          "y": 2825
        },
        {
          "x": 442,
          "y": 2825
        }
      ],
      "category": "paragraph",
      "html": "<p id='80' style='font-size:22px'>Removing sentence/word position encoding results in<br>a 0.8%/0.7% accuracy drop respectively, and removing all position encodings heavily decrease the<br>accuracy by 1.0%.</p>",
      "id": 80,
      "page": 7,
      "text": "Removing sentence/word position encoding results in\na 0.8%/0.7% accuracy drop respectively, and removing all position encodings heavily decrease the\naccuracy by 1.0%."
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 2873
        },
        {
          "x": 2107,
          "y": 2873
        },
        {
          "x": 2107,
          "y": 3013
        },
        {
          "x": 443,
          "y": 3013
        }
      ],
      "category": "paragraph",
      "html": "<p id='81' style='font-size:18px'>Number of heads. The effect of #heads in standard transformer has been investigated in multiple<br>works [24, 39] and a head width of 64 is recommended for visual tasks [10, 35]. We adopt the head<br>width of 64 in outer transformer block in our model. The number of heads in inner transformer block</p>",
      "id": 81,
      "page": 7,
      "text": "Number of heads. The effect of #heads in standard transformer has been investigated in multiple\nworks [24, 39] and a head width of 64 is recommended for visual tasks [10, 35]. We adopt the head\nwidth of 64 in outer transformer block in our model. The number of heads in inner transformer block"
    },
    {
      "bounding_box": [
        {
          "x": 1261,
          "y": 3092
        },
        {
          "x": 1289,
          "y": 3092
        },
        {
          "x": 1289,
          "y": 3128
        },
        {
          "x": 1261,
          "y": 3128
        }
      ],
      "category": "footer",
      "html": "<footer id='82' style='font-size:14px'>7</footer>",
      "id": 82,
      "page": 7,
      "text": "7"
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 309
        },
        {
          "x": 2110,
          "y": 309
        },
        {
          "x": 2110,
          "y": 398
        },
        {
          "x": 442,
          "y": 398
        }
      ],
      "category": "paragraph",
      "html": "<p id='83' style='font-size:20px'>is another hyper-parameter for investigation. We evaluate the effect of #heads in inner transformer<br>block (Table 7). We can see that a proper number of heads (e.g., 2 or 4) achieve the best performance.</p>",
      "id": 83,
      "page": 8,
      "text": "is another hyper-parameter for investigation. We evaluate the effect of #heads in inner transformer\nblock (Table 7). We can see that a proper number of heads (e.g., 2 or 4) achieve the best performance."
    },
    {
      "bounding_box": [
        {
          "x": 748,
          "y": 433
        },
        {
          "x": 1795,
          "y": 433
        },
        {
          "x": 1795,
          "y": 476
        },
        {
          "x": 748,
          "y": 476
        }
      ],
      "category": "caption",
      "html": "<caption id='84' style='font-size:18px'>Table 7: Effect of #heads in inner transformer block in TNT-S.</caption>",
      "id": 84,
      "page": 8,
      "text": "Table 7: Effect of #heads in inner transformer block in TNT-S."
    },
    {
      "bounding_box": [
        {
          "x": 843,
          "y": 502
        },
        {
          "x": 1687,
          "y": 502
        },
        {
          "x": 1687,
          "y": 633
        },
        {
          "x": 843,
          "y": 633
        }
      ],
      "category": "table",
      "html": "<table id='85' style='font-size:14px'><tr><td>#heads</td><td>I 1</td><td>I 2</td><td>I 4</td><td></td><td>6</td><td>8</td></tr><tr><td>Top-1</td><td>I 81.0</td><td>I 81.4</td><td></td><td>I 81.5</td><td>I 81.3</td><td>I 81.1</td></tr></table>",
      "id": 85,
      "page": 8,
      "text": "#heads I 1 I 2 I 4  6 8\n Top-1 I 81.0 I 81.4  I 81.5 I 81.3"
    },
    {
      "bounding_box": [
        {
          "x": 446,
          "y": 658
        },
        {
          "x": 2098,
          "y": 658
        },
        {
          "x": 2098,
          "y": 1073
        },
        {
          "x": 446,
          "y": 1073
        }
      ],
      "category": "paragraph",
      "html": "<p id='86' style='font-size:18px'>Number of visual words. In TNT, the input image is Table 8: Effect of #words m.<br>split into a number of 16x 16 patches and each patch is m c Params FLOPs Top-1<br>further split into m sub-patches (visual words) of size<br>64 6 23.8M 5.1B 81.0<br>(s,s) for computational efficiency. Here we test the ef-<br>16 24 23.8M 5.2B 81.5<br>fect of hyper-parameter m on TNT-S architecture. When<br>4 96 25.1M 6.0B 81.1<br>we change m, the embedding dimension c also changes<br>correspondingly to control the FLOPs. As shown in Table 8, we can see that the value of m has<br>slight influence on the performance, and we use m = 16 by default for its efficiency, unless stated<br>otherwise.</p>",
      "id": 86,
      "page": 8,
      "text": "Number of visual words. In TNT, the input image is Table 8: Effect of #words m.\nsplit into a number of 16x 16 patches and each patch is m c Params FLOPs Top-1\nfurther split into m sub-patches (visual words) of size\n64 6 23.8M 5.1B 81.0\n(s,s) for computational efficiency. Here we test the ef-\n16 24 23.8M 5.2B 81.5\nfect of hyper-parameter m on TNT-S architecture. When\n4 96 25.1M 6.0B 81.1\nwe change m, the embedding dimension c also changes\ncorrespondingly to control the FLOPs. As shown in Table 8, we can see that the value of m has\nslight influence on the performance, and we use m = 16 by default for its efficiency, unless stated\notherwise."
    },
    {
      "bounding_box": [
        {
          "x": 448,
          "y": 1105
        },
        {
          "x": 2090,
          "y": 1105
        },
        {
          "x": 2090,
          "y": 1726
        },
        {
          "x": 448,
          "y": 1726
        }
      ],
      "category": "figure",
      "html": "<figure><img id='87' style='font-size:14px' alt=\"TNT\nDeiT\nDeiT\nBlock-1 Block-6 Block-12\nTNT\nOR\n(a) Feature in Block-1/6/12. (b) T-SNE of Block-12.\" data-coord=\"top-left:(448,1105); bottom-right:(2090,1726)\" /></figure>",
      "id": 87,
      "page": 8,
      "text": "TNT\nDeiT\nDeiT\nBlock-1 Block-6 Block-12\nTNT\nOR\n(a) Feature in Block-1/6/12. (b) T-SNE of Block-12."
    },
    {
      "bounding_box": [
        {
          "x": 943,
          "y": 1707
        },
        {
          "x": 1030,
          "y": 1707
        },
        {
          "x": 1030,
          "y": 1746
        },
        {
          "x": 943,
          "y": 1746
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='88' style='font-size:20px'>maps</p>",
      "id": 88,
      "page": 8,
      "text": "maps"
    },
    {
      "bounding_box": [
        {
          "x": 767,
          "y": 1771
        },
        {
          "x": 1783,
          "y": 1771
        },
        {
          "x": 1783,
          "y": 1817
        },
        {
          "x": 767,
          "y": 1817
        }
      ],
      "category": "caption",
      "html": "<br><caption id='89' style='font-size:20px'>Figure 3: Visualization of the features of DeiT-S and TNT-S.</caption>",
      "id": 89,
      "page": 8,
      "text": "Figure 3: Visualization of the features of DeiT-S and TNT-S."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 1856
        },
        {
          "x": 780,
          "y": 1856
        },
        {
          "x": 780,
          "y": 1902
        },
        {
          "x": 442,
          "y": 1902
        }
      ],
      "category": "paragraph",
      "html": "<p id='90' style='font-size:22px'>3.4 Visualization</p>",
      "id": 90,
      "page": 8,
      "text": "3.4 Visualization"
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 1938
        },
        {
          "x": 2107,
          "y": 1938
        },
        {
          "x": 2107,
          "y": 2351
        },
        {
          "x": 442,
          "y": 2351
        }
      ],
      "category": "paragraph",
      "html": "<p id='91' style='font-size:20px'>Visualization of Feature Maps. We visualize the learned features of DeiT and TNT to further<br>understand the effect of the proposed method. For better visualization, the input image is resized<br>to 1024x 1024. The feature maps are formed by reshaping the patch embeddings according to their<br>spatial positions. The feature maps in the 1-st, 6-th and 12-th blocks are shown in Fig. 3(a) where 12<br>feature maps are randomly sampled for these blocks each. In TNT, the local information are better<br>preserved compared to DeiT. We also visualize all the 384 feature maps in the 12-th block using<br>t-SNE [37] (Fig. 3(b)). We can see that the features of TNT are more diverse and contain richer<br>information than those of DeiT. These benefits owe to the introduction of inner transformer block for<br>modeling local features.</p>",
      "id": 91,
      "page": 8,
      "text": "Visualization of Feature Maps. We visualize the learned features of DeiT and TNT to further\nunderstand the effect of the proposed method. For better visualization, the input image is resized\nto 1024x 1024. The feature maps are formed by reshaping the patch embeddings according to their\nspatial positions. The feature maps in the 1-st, 6-th and 12-th blocks are shown in Fig. 3(a) where 12\nfeature maps are randomly sampled for these blocks each. In TNT, the local information are better\npreserved compared to DeiT. We also visualize all the 384 feature maps in the 12-th block using\nt-SNE [37] (Fig. 3(b)). We can see that the features of TNT are more diverse and contain richer\ninformation than those of DeiT. These benefits owe to the introduction of inner transformer block for\nmodeling local features."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 2372
        },
        {
          "x": 2109,
          "y": 2372
        },
        {
          "x": 2109,
          "y": 2646
        },
        {
          "x": 442,
          "y": 2646
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='92' style='font-size:20px'>In addition to the patch-level features, we also visualize the pixel-level embeddings of TNT in Fig. 4.<br>For each patch, we reshape the word embeddings according to their spatial positions to form the<br>feature maps and then average these feature maps by the channel dimension. The averaged feature<br>maps corresponding to the 14x 14 patches are shown in Fig. 4. We can see that the local information<br>is well preserved in the shallow layers, and the representations become more abstract gradually as the<br>network goes deeper.</p>",
      "id": 92,
      "page": 8,
      "text": "In addition to the patch-level features, we also visualize the pixel-level embeddings of TNT in Fig. 4.\nFor each patch, we reshape the word embeddings according to their spatial positions to form the\nfeature maps and then average these feature maps by the channel dimension. The averaged feature\nmaps corresponding to the 14x 14 patches are shown in Fig. 4. We can see that the local information\nis well preserved in the shallow layers, and the representations become more abstract gradually as the\nnetwork goes deeper."
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 2690
        },
        {
          "x": 2108,
          "y": 2690
        },
        {
          "x": 2108,
          "y": 3013
        },
        {
          "x": 443,
          "y": 3013
        }
      ],
      "category": "paragraph",
      "html": "<p id='93' style='font-size:20px'>Visualization of Attention Maps. There are two self-attention layers in our TNT block, i.e., an<br>inner self-attention and an outer self-attention for modeling relationship among visual words and<br>sentences respectively. We show the attention maps of different queries in the inner transformer<br>in Figure 5. For a given query visual word, the attention values of visual words with similar<br>appearance are higher, indicating their features will be interacted more relevantly with the query.<br>These interactions are missed in ViT and DeiT, etc. The attention maps in the outer transformer can<br>be found in the supplemental material.</p>",
      "id": 93,
      "page": 8,
      "text": "Visualization of Attention Maps. There are two self-attention layers in our TNT block, i.e., an\ninner self-attention and an outer self-attention for modeling relationship among visual words and\nsentences respectively. We show the attention maps of different queries in the inner transformer\nin Figure 5. For a given query visual word, the attention values of visual words with similar\nappearance are higher, indicating their features will be interacted more relevantly with the query.\nThese interactions are missed in ViT and DeiT, etc. The attention maps in the outer transformer can\nbe found in the supplemental material."
    },
    {
      "bounding_box": [
        {
          "x": 1261,
          "y": 3095
        },
        {
          "x": 1287,
          "y": 3095
        },
        {
          "x": 1287,
          "y": 3127
        },
        {
          "x": 1261,
          "y": 3127
        }
      ],
      "category": "footer",
      "html": "<footer id='94' style='font-size:16px'>8</footer>",
      "id": 94,
      "page": 8,
      "text": "8"
    },
    {
      "bounding_box": [
        {
          "x": 550,
          "y": 278
        },
        {
          "x": 1977,
          "y": 278
        },
        {
          "x": 1977,
          "y": 705
        },
        {
          "x": 550,
          "y": 705
        }
      ],
      "category": "figure",
      "html": "<figure><img id='95' style='font-size:14px' alt=\"\n품 MG  \nV\n\nBlock-1 Block-6 Block-12\" data-coord=\"top-left:(550,278); bottom-right:(1977,705)\" /></figure>",
      "id": 95,
      "page": 9,
      "text": "\n품 MG  \nV\n\nBlock-1 Block-6 Block-12"
    },
    {
      "bounding_box": [
        {
          "x": 700,
          "y": 743
        },
        {
          "x": 1832,
          "y": 743
        },
        {
          "x": 1832,
          "y": 785
        },
        {
          "x": 700,
          "y": 785
        }
      ],
      "category": "caption",
      "html": "<caption id='96' style='font-size:22px'>Figure 4: Visualization of the averaged word embeddings of TNT-S.</caption>",
      "id": 96,
      "page": 9,
      "text": "Figure 4: Visualization of the averaged word embeddings of TNT-S."
    },
    {
      "bounding_box": [
        {
          "x": 640,
          "y": 797
        },
        {
          "x": 1893,
          "y": 797
        },
        {
          "x": 1893,
          "y": 965
        },
        {
          "x": 640,
          "y": 965
        }
      ],
      "category": "figure",
      "html": "<figure><img id='97' alt=\"\" data-coord=\"top-left:(640,797); bottom-right:(1893,965)\" /></figure>",
      "id": 97,
      "page": 9,
      "text": ""
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 989
        },
        {
          "x": 2102,
          "y": 989
        },
        {
          "x": 2102,
          "y": 1083
        },
        {
          "x": 442,
          "y": 1083
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='98' style='font-size:18px'>Figure 5: Attention maps of different queries in the inner transformer. Red cross symbol denotes the<br>query location.</p>",
      "id": 98,
      "page": 9,
      "text": "Figure 5: Attention maps of different queries in the inner transformer. Red cross symbol denotes the\nquery location."
    },
    {
      "bounding_box": [
        {
          "x": 444,
          "y": 1113
        },
        {
          "x": 875,
          "y": 1113
        },
        {
          "x": 875,
          "y": 1162
        },
        {
          "x": 444,
          "y": 1162
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='99' style='font-size:22px'>3.5 Transfer Learning</p>",
      "id": 99,
      "page": 9,
      "text": "3.5 Transfer Learning"
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 1198
        },
        {
          "x": 2104,
          "y": 1198
        },
        {
          "x": 2104,
          "y": 1291
        },
        {
          "x": 442,
          "y": 1291
        }
      ],
      "category": "paragraph",
      "html": "<p id='100' style='font-size:18px'>To demonstrate the strong generalization ability of TNT, we transfer TNT-S, TNT-B models trained<br>on ImageNet to the downstream tasks.</p>",
      "id": 100,
      "page": 9,
      "text": "To demonstrate the strong generalization ability of TNT, we transfer TNT-S, TNT-B models trained\non ImageNet to the downstream tasks."
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 1342
        },
        {
          "x": 2107,
          "y": 1342
        },
        {
          "x": 2107,
          "y": 1894
        },
        {
          "x": 443,
          "y": 1894
        }
      ],
      "category": "paragraph",
      "html": "<p id='101' style='font-size:18px'>Pure Transformer Image Classification. Following DeiT [35], we evaluate our models on 4<br>image classification datasets with training set size ranging from 2,040 to 50,000 images. These<br>datasets include superordinate-level object classification (CIFAR-10 [18], CIFAR-100 [18]) and<br>fine-grained object classification (Oxford-IIIT Pets [26], Oxford 102 Flowers [25] and iNaturalist<br>2019 [38]), shown in Table 2. All models are fine-tuned with an image resolution of 384x 384. We<br>adopt the same training settings as those at the pre-training stage by preserving all data augmentation<br>strategies. In order to fine-tune in a different resolution, we also interpolate the position embeddings<br>of new patches. For CIFAR-10 and CIFAR-100, we fine-tune the models for 64 epochs, and for<br>fine-grained datasets, we fine-tune the models for 300 epochs. Table 9 compares the transfer learning<br>results of TNT to those of ViT, DeiT and other convolutional networks. We find that TNT outperforms<br>DeiT in most datasets with less parameters, which shows the superiority of modeling pixel-level<br>relations to get better feature representation.</p>",
      "id": 101,
      "page": 9,
      "text": "Pure Transformer Image Classification. Following DeiT [35], we evaluate our models on 4\nimage classification datasets with training set size ranging from 2,040 to 50,000 images. These\ndatasets include superordinate-level object classification (CIFAR-10 [18], CIFAR-100 [18]) and\nfine-grained object classification (Oxford-IIIT Pets [26], Oxford 102 Flowers [25] and iNaturalist\n2019 [38]), shown in Table 2. All models are fine-tuned with an image resolution of 384x 384. We\nadopt the same training settings as those at the pre-training stage by preserving all data augmentation\nstrategies. In order to fine-tune in a different resolution, we also interpolate the position embeddings\nof new patches. For CIFAR-10 and CIFAR-100, we fine-tune the models for 64 epochs, and for\nfine-grained datasets, we fine-tune the models for 300 epochs. Table 9 compares the transfer learning\nresults of TNT to those of ViT, DeiT and other convolutional networks. We find that TNT outperforms\nDeiT in most datasets with less parameters, which shows the superiority of modeling pixel-level\nrelations to get better feature representation."
    },
    {
      "bounding_box": [
        {
          "x": 444,
          "y": 1936
        },
        {
          "x": 2105,
          "y": 1936
        },
        {
          "x": 2105,
          "y": 2027
        },
        {
          "x": 444,
          "y": 2027
        }
      ],
      "category": "caption",
      "html": "<caption id='102' style='font-size:20px'>Table 9: Results on downstream image classification tasks with ImageNet pre-training. ↑ 384 denotes<br>fine-tuning with 384x 384 resolution.</caption>",
      "id": 102,
      "page": 9,
      "text": "Table 9: Results on downstream image classification tasks with ImageNet pre-training. ↑ 384 denotes\nfine-tuning with 384x 384 resolution."
    },
    {
      "bounding_box": [
        {
          "x": 448,
          "y": 2037
        },
        {
          "x": 2085,
          "y": 2037
        },
        {
          "x": 2085,
          "y": 2531
        },
        {
          "x": 448,
          "y": 2531
        }
      ],
      "category": "table",
      "html": "<table id='103' style='font-size:14px'><tr><td>Model</td><td>Params (M)</td><td>ImageNet</td><td>CIFAR10</td><td>I CIFAR100</td><td>Flowers</td><td>Pets</td><td>iNat-19</td></tr><tr><td colspan=\"8\">CNN-based</td></tr><tr><td>Grafit ResNet-50 [36]</td><td>25.6</td><td>79.6</td><td>-</td><td>-</td><td>98.2</td><td>-</td><td>75.9</td></tr><tr><td>Grafit RegNetY-8GF [36]</td><td>39.2</td><td></td><td>-</td><td>-</td><td>99.1</td><td>-</td><td>80.0</td></tr><tr><td>EfficientNet-B5 [32]</td><td>30</td><td>83.6</td><td>98.7</td><td>91.1</td><td>98.5</td><td>-</td><td>-</td></tr><tr><td colspan=\"8\">Transformer-based</td></tr><tr><td>ViT-B/16↑384 [10]</td><td>86.4</td><td>77.9</td><td>98.1</td><td>87.1</td><td>89.5</td><td>93.8</td><td>-</td></tr><tr><td>DeiT-B ↑384 [35]</td><td>86.4</td><td>83.1</td><td>99.1</td><td>90.8</td><td>98.4</td><td>-</td><td>-</td></tr><tr><td>TNT-S↑384</td><td>23.8</td><td>83.1</td><td>98.7</td><td>90.1</td><td>98.8</td><td>94.7</td><td>81.4</td></tr><tr><td>TNT-B ↑384</td><td>65.6</td><td>83.9</td><td>99.1</td><td>91.1</td><td>99.0</td><td>95.0</td><td>83.2</td></tr></table>",
      "id": 103,
      "page": 9,
      "text": "Model Params (M) ImageNet CIFAR10 I CIFAR100 Flowers Pets iNat-19\n CNN-based\n Grafit ResNet-50 [36] 25.6 79.6 - - 98.2 - 75.9\n Grafit RegNetY-8GF [36] 39.2  - - 99.1 - 80.0\n EfficientNet-B5 [32] 30 83.6 98.7 91.1 98.5 - -\n Transformer-based\n ViT-B/16↑384 [10] 86.4 77.9 98.1 87.1 89.5 93.8 -\n DeiT-B ↑384 [35] 86.4 83.1 99.1 90.8 98.4 - -\n TNT-S↑384 23.8 83.1 98.7 90.1 98.8 94.7 81.4\n TNT-B ↑384 65.6 83.9 99.1 91.1 99.0 95.0"
    },
    {
      "bounding_box": [
        {
          "x": 441,
          "y": 2558
        },
        {
          "x": 2107,
          "y": 2558
        },
        {
          "x": 2107,
          "y": 2972
        },
        {
          "x": 441,
          "y": 2972
        }
      ],
      "category": "paragraph",
      "html": "<p id='104' style='font-size:18px'>Pure Transformer Object Detection. We construct a pure transformer object detection pipeline<br>by combining our TNT and DETR [3]. For fair comparison, we adopt the training and testing settings<br>in PVT [40] and add a 2x2 average pooling to make the output size of TNT backbone the same as<br>that of PVT and ResNet. All the compared models are trained using Adam W [23] with batch size<br>of 16 for 50 epochs. The training images are randomly resized to have a shorter side in the range<br>of [640,800] and a longer side within 1333 pixels. For testing, the shorter side is set as 800 pixels.<br>The results on COCO val2017 are shown in Table 10. Under the same setting, DETR with TNT-S<br>backbone outperforms the representative pure transformer detector DETR+PVT-Small by 3.5 AP<br>with similar parameters.</p>",
      "id": 104,
      "page": 9,
      "text": "Pure Transformer Object Detection. We construct a pure transformer object detection pipeline\nby combining our TNT and DETR [3]. For fair comparison, we adopt the training and testing settings\nin PVT [40] and add a 2x2 average pooling to make the output size of TNT backbone the same as\nthat of PVT and ResNet. All the compared models are trained using Adam W [23] with batch size\nof 16 for 50 epochs. The training images are randomly resized to have a shorter side in the range\nof [640,800] and a longer side within 1333 pixels. For testing, the shorter side is set as 800 pixels.\nThe results on COCO val2017 are shown in Table 10. Under the same setting, DETR with TNT-S\nbackbone outperforms the representative pure transformer detector DETR+PVT-Small by 3.5 AP\nwith similar parameters."
    },
    {
      "bounding_box": [
        {
          "x": 1260,
          "y": 3093
        },
        {
          "x": 1289,
          "y": 3093
        },
        {
          "x": 1289,
          "y": 3128
        },
        {
          "x": 1260,
          "y": 3128
        }
      ],
      "category": "footer",
      "html": "<footer id='105' style='font-size:16px'>9</footer>",
      "id": 105,
      "page": 9,
      "text": "9"
    },
    {
      "bounding_box": [
        {
          "x": 444,
          "y": 299
        },
        {
          "x": 2104,
          "y": 299
        },
        {
          "x": 2104,
          "y": 392
        },
        {
          "x": 444,
          "y": 392
        }
      ],
      "category": "caption",
      "html": "<caption id='106' style='font-size:16px'>Table 10: Results of object detection on COCO2017 val set with ImageNet pre-training. †Results<br>from our implementation.</caption>",
      "id": 106,
      "page": 10,
      "text": "Table 10: Results of object detection on COCO2017 val set with ImageNet pre-training. †Results\nfrom our implementation."
    },
    {
      "bounding_box": [
        {
          "x": 558,
          "y": 401
        },
        {
          "x": 1974,
          "y": 401
        },
        {
          "x": 1974,
          "y": 712
        },
        {
          "x": 558,
          "y": 712
        }
      ],
      "category": "table",
      "html": "<br><table id='107' style='font-size:14px'><tr><td>Backbone</td><td>Params</td><td>Epochs</td><td>AP</td><td>AP50</td><td>AP75</td><td>APs</td><td>AP M</td><td>APL</td></tr><tr><td>ResNet-50 [40]</td><td>41M</td><td>50</td><td>32.3</td><td>53.9</td><td>32.3</td><td>10.7</td><td>33.8</td><td>53.0</td></tr><tr><td>DeiT-S† [35]</td><td>38M</td><td>50</td><td>33.9</td><td>54.7</td><td>34.3</td><td>11.0</td><td>35.4</td><td>56.6</td></tr><tr><td>PVT-Small [40]</td><td>40M</td><td>50</td><td>34.7</td><td>55.7</td><td>35.4</td><td>12.0</td><td>36.4</td><td>56.7</td></tr><tr><td>PVT-Medium [40]</td><td>57M</td><td>50</td><td>36.4</td><td>57.9</td><td>37.2</td><td>13.0</td><td>38.7</td><td>59.1</td></tr><tr><td>TNT-S</td><td>39M</td><td>50</td><td>38.2</td><td>58.9</td><td>39.4</td><td>15.5</td><td>41.1</td><td>58.8</td></tr></table>",
      "id": 107,
      "page": 10,
      "text": "Backbone Params Epochs AP AP50 AP75 APs AP M APL\n ResNet-50 [40] 41M 50 32.3 53.9 32.3 10.7 33.8 53.0\n DeiT-S† [35] 38M 50 33.9 54.7 34.3 11.0 35.4 56.6\n PVT-Small [40] 40M 50 34.7 55.7 35.4 12.0 36.4 56.7\n PVT-Medium [40] 57M 50 36.4 57.9 37.2 13.0 38.7 59.1\n TNT-S 39M 50 38.2 58.9 39.4 15.5 41.1"
    },
    {
      "bounding_box": [
        {
          "x": 446,
          "y": 787
        },
        {
          "x": 1238,
          "y": 787
        },
        {
          "x": 1238,
          "y": 830
        },
        {
          "x": 446,
          "y": 830
        }
      ],
      "category": "paragraph",
      "html": "<p id='108' style='font-size:18px'>Pure Transformer Semantic Segmentation.</p>",
      "id": 108,
      "page": 10,
      "text": "Pure Transformer Semantic Segmentation."
    },
    {
      "bounding_box": [
        {
          "x": 444,
          "y": 831
        },
        {
          "x": 1240,
          "y": 831
        },
        {
          "x": 1240,
          "y": 1243
        },
        {
          "x": 444,
          "y": 1243
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='109' style='font-size:16px'>We adopt the segmentation framework of<br>Trans2Seg [42] to build the pure transformer<br>semantic segmentation based on TNT backbone.<br>We follow the training and testing configuration<br>in PVT [40] for fair comparison. All the com-<br>pared models are trained by AdamW optimizer<br>with initial learning rate of 1e-4 and polynomial<br>decay schedule. We apply random resize and<br>crop of 512x512 during training. The ADE20K</p>",
      "id": 109,
      "page": 10,
      "text": "We adopt the segmentation framework of\nTrans2Seg [42] to build the pure transformer\nsemantic segmentation based on TNT backbone.\nWe follow the training and testing configuration\nin PVT [40] for fair comparison. All the com-\npared models are trained by AdamW optimizer\nwith initial learning rate of 1e-4 and polynomial\ndecay schedule. We apply random resize and\ncrop of 512x512 during training. The ADE20K"
    },
    {
      "bounding_box": [
        {
          "x": 1275,
          "y": 797
        },
        {
          "x": 2105,
          "y": 797
        },
        {
          "x": 2105,
          "y": 931
        },
        {
          "x": 1275,
          "y": 931
        }
      ],
      "category": "caption",
      "html": "<br><caption id='110' style='font-size:16px'>Table 11: Results of semantic segmentation on<br>ADE20K val set with ImageNet pre-training.<br>†Results from our implementation.</caption>",
      "id": 110,
      "page": 10,
      "text": "Table 11: Results of semantic segmentation on\nADE20K val set with ImageNet pre-training.\n†Results from our implementation."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 932
        },
        {
          "x": 2099,
          "y": 932
        },
        {
          "x": 2099,
          "y": 1196
        },
        {
          "x": 1280,
          "y": 1196
        }
      ],
      "category": "table",
      "html": "<br><table id='111' style='font-size:14px'><tr><td>Backbone</td><td>Params</td><td>FLOPs</td><td>Steps</td><td>mIoU</td></tr><tr><td>ResNet-50 [42]</td><td>56. 1M</td><td>79.3G</td><td>40k</td><td>39.7</td></tr><tr><td>DeiT-St [35]</td><td>30.3M</td><td>27.2G</td><td>40k</td><td>40.5</td></tr><tr><td>PVT-Small [47]</td><td>32.1M</td><td>31.6G</td><td>40k</td><td>42.6</td></tr><tr><td>TNT-S</td><td>32.1M</td><td>30.4G</td><td>40k</td><td>43.6</td></tr></table>",
      "id": 111,
      "page": 10,
      "text": "Backbone Params FLOPs Steps mIoU\n ResNet-50 [42] 56. 1M 79.3G 40k 39.7\n DeiT-St [35] 30.3M 27.2G 40k 40.5\n PVT-Small [47] 32.1M 31.6G 40k 42.6\n TNT-S 32.1M 30.4G 40k"
    },
    {
      "bounding_box": [
        {
          "x": 444,
          "y": 1240
        },
        {
          "x": 2105,
          "y": 1240
        },
        {
          "x": 2105,
          "y": 1378
        },
        {
          "x": 444,
          "y": 1378
        }
      ],
      "category": "paragraph",
      "html": "<p id='112' style='font-size:16px'>results with single scale testing are shown in Table 11. With similar parameters, Trans2Seg with<br>TNT-S backbone achieves 43.6% mIoU, which is 1.0% higher than that of PVT-small backbone and<br>2.8% higher than that of DeiT-S backbone.</p>",
      "id": 112,
      "page": 10,
      "text": "results with single scale testing are shown in Table 11. With similar parameters, Trans2Seg with\nTNT-S backbone achieves 43.6% mIoU, which is 1.0% higher than that of PVT-small backbone and\n2.8% higher than that of DeiT-S backbone."
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 1449
        },
        {
          "x": 766,
          "y": 1449
        },
        {
          "x": 766,
          "y": 1501
        },
        {
          "x": 443,
          "y": 1501
        }
      ],
      "category": "paragraph",
      "html": "<p id='113' style='font-size:20px'>4 Conclusion</p>",
      "id": 113,
      "page": 10,
      "text": "4 Conclusion"
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 1554
        },
        {
          "x": 2107,
          "y": 1554
        },
        {
          "x": 2107,
          "y": 2013
        },
        {
          "x": 442,
          "y": 2013
        }
      ],
      "category": "paragraph",
      "html": "<p id='114' style='font-size:14px'>In this paper, we propose a novel Transformer-iN-Transformer (TNT) network architecture for visual<br>recognition. In particular, we uniformly split the image into a sequence of patches (visual sentences)<br>and view each patch as a sequence of sub-patches (visual words). We introduce a TNT block in<br>which an outer transformer block is utilized for processing the sentence embeddings and an inner<br>transformer block is used to model the relation among word embeddings. The information of visual<br>word embeddings is added to the visual sentence embedding after the projection of a linear layer.<br>We build our TNT architecture by stacking the TNT blocks. Compared to the conventional vision<br>transformers (ViT) which corrupts the local structure of the patch, our TNT can better preserve<br>and model the local information for visual recognition. Extensive experiments on ImageNet and<br>downstream tasks have demonstrate the effectiveness of the proposed TNT architecture.</p>",
      "id": 114,
      "page": 10,
      "text": "In this paper, we propose a novel Transformer-iN-Transformer (TNT) network architecture for visual\nrecognition. In particular, we uniformly split the image into a sequence of patches (visual sentences)\nand view each patch as a sequence of sub-patches (visual words). We introduce a TNT block in\nwhich an outer transformer block is utilized for processing the sentence embeddings and an inner\ntransformer block is used to model the relation among word embeddings. The information of visual\nword embeddings is added to the visual sentence embedding after the projection of a linear layer.\nWe build our TNT architecture by stacking the TNT blocks. Compared to the conventional vision\ntransformers (ViT) which corrupts the local structure of the patch, our TNT can better preserve\nand model the local information for visual recognition. Extensive experiments on ImageNet and\ndownstream tasks have demonstrate the effectiveness of the proposed TNT architecture."
    },
    {
      "bounding_box": [
        {
          "x": 445,
          "y": 2085
        },
        {
          "x": 845,
          "y": 2085
        },
        {
          "x": 845,
          "y": 2135
        },
        {
          "x": 445,
          "y": 2135
        }
      ],
      "category": "paragraph",
      "html": "<p id='115' style='font-size:22px'>Acknowledgement</p>",
      "id": 115,
      "page": 10,
      "text": "Acknowledgement"
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 2188
        },
        {
          "x": 2104,
          "y": 2188
        },
        {
          "x": 2104,
          "y": 2281
        },
        {
          "x": 443,
          "y": 2281
        }
      ],
      "category": "paragraph",
      "html": "<p id='116' style='font-size:18px'>This work was supported by NSFC (62072449, 61632003), Guangdong-Hongkong-Macao Joint<br>Research Grant (2020B1515130004) and Macao FDCT (0018/2019/AKP, 0015/2019/AKP).</p>",
      "id": 116,
      "page": 10,
      "text": "This work was supported by NSFC (62072449, 61632003), Guangdong-Hongkong-Macao Joint\nResearch Grant (2020B1515130004) and Macao FDCT (0018/2019/AKP, 0015/2019/AKP)."
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 2354
        },
        {
          "x": 747,
          "y": 2354
        },
        {
          "x": 747,
          "y": 2408
        },
        {
          "x": 443,
          "y": 2408
        }
      ],
      "category": "paragraph",
      "html": "<p id='117' style='font-size:20px'>A Appendix</p>",
      "id": 117,
      "page": 10,
      "text": "A Appendix"
    },
    {
      "bounding_box": [
        {
          "x": 445,
          "y": 2459
        },
        {
          "x": 1122,
          "y": 2459
        },
        {
          "x": 1122,
          "y": 2505
        },
        {
          "x": 445,
          "y": 2505
        }
      ],
      "category": "paragraph",
      "html": "<p id='118' style='font-size:16px'>A.1 Visualization of Attention Maps</p>",
      "id": 118,
      "page": 10,
      "text": "A.1 Visualization of Attention Maps"
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 2544
        },
        {
          "x": 2107,
          "y": 2544
        },
        {
          "x": 2107,
          "y": 2817
        },
        {
          "x": 443,
          "y": 2817
        }
      ],
      "category": "paragraph",
      "html": "<p id='119' style='font-size:14px'>Attention between Patches. In Figure 6, we plot the attention maps from each patch to all the<br>patches. We can see that for both DeiT-S and TNT-S, more patches are related as layer goes deeper.<br>This is because the information between patches has been fully communicated with each other in<br>deeper layers. As for the difference between DeiT and TNT, the attention of TNT can focus on the<br>meaningful patches in Block-12, while DeiT still pays attention to the tree which is not related to the<br>pandas.</p>",
      "id": 119,
      "page": 10,
      "text": "Attention between Patches. In Figure 6, we plot the attention maps from each patch to all the\npatches. We can see that for both DeiT-S and TNT-S, more patches are related as layer goes deeper.\nThis is because the information between patches has been fully communicated with each other in\ndeeper layers. As for the difference between DeiT and TNT, the attention of TNT can focus on the\nmeaningful patches in Block-12, while DeiT still pays attention to the tree which is not related to the\npandas."
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 2875
        },
        {
          "x": 2106,
          "y": 2875
        },
        {
          "x": 2106,
          "y": 3012
        },
        {
          "x": 443,
          "y": 3012
        }
      ],
      "category": "paragraph",
      "html": "<p id='120' style='font-size:16px'>Attention between Class Token and Patches. In Figure 7, we plot the attention maps between<br>class token to all the patches for some randomly sampled images. We can see that the output feature<br>mainly focus on the patches related to the object to be recognized.</p>",
      "id": 120,
      "page": 10,
      "text": "Attention between Class Token and Patches. In Figure 7, we plot the attention maps between\nclass token to all the patches for some randomly sampled images. We can see that the output feature\nmainly focus on the patches related to the object to be recognized."
    },
    {
      "bounding_box": [
        {
          "x": 1253,
          "y": 3093
        },
        {
          "x": 1299,
          "y": 3093
        },
        {
          "x": 1299,
          "y": 3130
        },
        {
          "x": 1253,
          "y": 3130
        }
      ],
      "category": "footer",
      "html": "<footer id='121' style='font-size:14px'>10</footer>",
      "id": 121,
      "page": 10,
      "text": "10"
    },
    {
      "bounding_box": [
        {
          "x": 531,
          "y": 293
        },
        {
          "x": 2026,
          "y": 293
        },
        {
          "x": 2026,
          "y": 1061
        },
        {
          "x": 531,
          "y": 1061
        }
      ],
      "category": "figure",
      "html": "<figure><img id='122' style='font-size:14px' alt=\"DeiT\nBlock-1 Block-6 Block-12\nTNT\" data-coord=\"top-left:(531,293); bottom-right:(2026,1061)\" /></figure>",
      "id": 122,
      "page": 11,
      "text": "DeiT\nBlock-1 Block-6 Block-12\nTNT"
    },
    {
      "bounding_box": [
        {
          "x": 503,
          "y": 1084
        },
        {
          "x": 2042,
          "y": 1084
        },
        {
          "x": 2042,
          "y": 1134
        },
        {
          "x": 503,
          "y": 1134
        }
      ],
      "category": "caption",
      "html": "<br><caption id='123' style='font-size:20px'>Figure 6: Visualization of the attention maps between all patches in outer transformer block.</caption>",
      "id": 123,
      "page": 11,
      "text": "Figure 6: Visualization of the attention maps between all patches in outer transformer block."
    },
    {
      "bounding_box": [
        {
          "x": 483,
          "y": 1195
        },
        {
          "x": 2082,
          "y": 1195
        },
        {
          "x": 2082,
          "y": 2052
        },
        {
          "x": 483,
          "y": 2052
        }
      ],
      "category": "figure",
      "html": "<figure><img id='124' alt=\"\" data-coord=\"top-left:(483,1195); bottom-right:(2082,2052)\" /></figure>",
      "id": 124,
      "page": 11,
      "text": ""
    },
    {
      "bounding_box": [
        {
          "x": 647,
          "y": 2096
        },
        {
          "x": 1904,
          "y": 2096
        },
        {
          "x": 1904,
          "y": 2147
        },
        {
          "x": 647,
          "y": 2147
        }
      ],
      "category": "caption",
      "html": "<caption id='125' style='font-size:22px'>Figure 7: Example attention maps from the output token to the input space.</caption>",
      "id": 125,
      "page": 11,
      "text": "Figure 7: Example attention maps from the output token to the input space."
    },
    {
      "bounding_box": [
        {
          "x": 444,
          "y": 2270
        },
        {
          "x": 1079,
          "y": 2270
        },
        {
          "x": 1079,
          "y": 2320
        },
        {
          "x": 444,
          "y": 2320
        }
      ],
      "category": "paragraph",
      "html": "<p id='126' style='font-size:22px'>A.2 Exploring SE module in TNT</p>",
      "id": 126,
      "page": 11,
      "text": "A.2 Exploring SE module in TNT"
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 2363
        },
        {
          "x": 2109,
          "y": 2363
        },
        {
          "x": 2109,
          "y": 2642
        },
        {
          "x": 442,
          "y": 2642
        }
      ],
      "category": "paragraph",
      "html": "<p id='127' style='font-size:20px'>Inspired by squeeze-and-excitation (SE) network for CNNs [16], we propose to explore channel-wise<br>attention for transformers. We first average all the sentence (word) embeddings and use a two-layer<br>MLP to calculate the attention values. The attention is multiplied to all the embeddings. The SE<br>module only brings in a few extra parameters but is able to perform dimension-wise attention for<br>feature enhancement. From the results in Table 12, adding SE module into TNT can further improve<br>the accuracy slightly.</p>",
      "id": 127,
      "page": 11,
      "text": "Inspired by squeeze-and-excitation (SE) network for CNNs [16], we propose to explore channel-wise\nattention for transformers. We first average all the sentence (word) embeddings and use a two-layer\nMLP to calculate the attention values. The attention is multiplied to all the embeddings. The SE\nmodule only brings in a few extra parameters but is able to perform dimension-wise attention for\nfeature enhancement. From the results in Table 12, adding SE module into TNT can further improve\nthe accuracy slightly."
    },
    {
      "bounding_box": [
        {
          "x": 937,
          "y": 2700
        },
        {
          "x": 1610,
          "y": 2700
        },
        {
          "x": 1610,
          "y": 2744
        },
        {
          "x": 937,
          "y": 2744
        }
      ],
      "category": "caption",
      "html": "<caption id='128' style='font-size:20px'>Table 12: Exploring SE module in TNT.</caption>",
      "id": 128,
      "page": 11,
      "text": "Table 12: Exploring SE module in TNT."
    },
    {
      "bounding_box": [
        {
          "x": 563,
          "y": 2771
        },
        {
          "x": 1979,
          "y": 2771
        },
        {
          "x": 1979,
          "y": 2959
        },
        {
          "x": 563,
          "y": 2959
        }
      ],
      "category": "table",
      "html": "<table id='129' style='font-size:18px'><tr><td>Model</td><td>Resolution</td><td>Params (M)</td><td>FLOPs (B)</td><td>Top-1 (%)</td><td>Top-5 (%)</td></tr><tr><td>TNT-S</td><td>224x224</td><td>23.8</td><td>5.2</td><td>81.5</td><td>95.7</td></tr><tr><td>TNT-S + SE</td><td>224x224</td><td>24.7</td><td>5.2</td><td>81.7</td><td>95.7</td></tr></table>",
      "id": 129,
      "page": 11,
      "text": "Model Resolution Params (M) FLOPs (B) Top-1 (%) Top-5 (%)\n TNT-S 224x224 23.8 5.2 81.5 95.7\n TNT-S + SE 224x224 24.7 5.2 81.7"
    },
    {
      "bounding_box": [
        {
          "x": 1252,
          "y": 3093
        },
        {
          "x": 1295,
          "y": 3093
        },
        {
          "x": 1295,
          "y": 3131
        },
        {
          "x": 1252,
          "y": 3131
        }
      ],
      "category": "footer",
      "html": "<footer id='130' style='font-size:16px'>11</footer>",
      "id": 130,
      "page": 11,
      "text": "11"
    },
    {
      "bounding_box": [
        {
          "x": 446,
          "y": 307
        },
        {
          "x": 1196,
          "y": 307
        },
        {
          "x": 1196,
          "y": 351
        },
        {
          "x": 446,
          "y": 351
        }
      ],
      "category": "paragraph",
      "html": "<p id='131' style='font-size:20px'>A.3 Object Detection with Faster RCNN</p>",
      "id": 131,
      "page": 12,
      "text": "A.3 Object Detection with Faster RCNN"
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 388
        },
        {
          "x": 2109,
          "y": 388
        },
        {
          "x": 2109,
          "y": 806
        },
        {
          "x": 443,
          "y": 806
        }
      ],
      "category": "paragraph",
      "html": "<p id='132' style='font-size:16px'>As a general backbone network, TNT can also be applied with multi-scale vision models like Faster<br>RCNN [29]. We extract the features from different layers of TNT to construct multi-scale features.<br>1 1 1) as input, while the resolution of feature<br>In particular, FPN takes 4 levels of features (1, 8 , 16'<br>of every TNT block is 16. We select the 4 layers from shallow to deep (3rd, 6th, 9th, 12th) to form<br>multi-level representation. To match the feature shape, we insert deconvolution/convolution layers<br>with proper stride. We evaluate TNT-S and DeiT-S on Faster RCNN with FPN [21]. The DeiT<br>model is used in the same way. The COCO2017 val results are shown in Table 13. TNT achieves<br>much better performance than ResNet and DeiT backbones, indicating its generalization for FPN-like<br>framework.</p>",
      "id": 132,
      "page": 12,
      "text": "As a general backbone network, TNT can also be applied with multi-scale vision models like Faster\nRCNN [29]. We extract the features from different layers of TNT to construct multi-scale features.\n1 1 1) as input, while the resolution of feature\nIn particular, FPN takes 4 levels of features (1, 8 , 16'\nof every TNT block is 16. We select the 4 layers from shallow to deep (3rd, 6th, 9th, 12th) to form\nmulti-level representation. To match the feature shape, we insert deconvolution/convolution layers\nwith proper stride. We evaluate TNT-S and DeiT-S on Faster RCNN with FPN [21]. The DeiT\nmodel is used in the same way. The COCO2017 val results are shown in Table 13. TNT achieves\nmuch better performance than ResNet and DeiT backbones, indicating its generalization for FPN-like\nframework."
    },
    {
      "bounding_box": [
        {
          "x": 447,
          "y": 817
        },
        {
          "x": 2109,
          "y": 817
        },
        {
          "x": 2109,
          "y": 909
        },
        {
          "x": 447,
          "y": 909
        }
      ],
      "category": "caption",
      "html": "<br><caption id='133' style='font-size:18px'>Table 13: Results of Faster RCNN object detection on COCO minival set with ImageNet pre-training.<br>†Results from our implementation.</caption>",
      "id": 133,
      "page": 12,
      "text": "Table 13: Results of Faster RCNN object detection on COCO minival set with ImageNet pre-training.\n†Results from our implementation."
    },
    {
      "bounding_box": [
        {
          "x": 526,
          "y": 937
        },
        {
          "x": 2005,
          "y": 937
        },
        {
          "x": 2005,
          "y": 1163
        },
        {
          "x": 526,
          "y": 1163
        }
      ],
      "category": "table",
      "html": "<table id='134' style='font-size:14px'><tr><td>Backbone</td><td>Params (M)</td><td>Epochs</td><td>AP</td><td>AP50</td><td>AP75</td><td>AP s</td><td>AP M</td><td>APL</td></tr><tr><td>ResNet-50 [21, 5]</td><td>41.5</td><td>12</td><td>37.4</td><td>58.1</td><td>40.4</td><td>21.2</td><td>41.0</td><td>48.1</td></tr><tr><td>DeiT-S† [35]</td><td>46.4</td><td>12</td><td>39.9</td><td>62.8</td><td>42.6</td><td>23.4</td><td>42.5</td><td>54.0</td></tr><tr><td>TNT-S</td><td>48.1</td><td>12</td><td>41.5</td><td>64.1</td><td>44.5</td><td>25.7</td><td>44.6</td><td>55.4</td></tr></table>",
      "id": 134,
      "page": 12,
      "text": "Backbone Params (M) Epochs AP AP50 AP75 AP s AP M APL\n ResNet-50 [21, 5] 41.5 12 37.4 58.1 40.4 21.2 41.0 48.1\n DeiT-S† [35] 46.4 12 39.9 62.8 42.6 23.4 42.5 54.0\n TNT-S 48.1 12 41.5 64.1 44.5 25.7 44.6"
    },
    {
      "bounding_box": [
        {
          "x": 446,
          "y": 1223
        },
        {
          "x": 686,
          "y": 1223
        },
        {
          "x": 686,
          "y": 1273
        },
        {
          "x": 446,
          "y": 1273
        }
      ],
      "category": "paragraph",
      "html": "<p id='135' style='font-size:22px'>References</p>",
      "id": 135,
      "page": 12,
      "text": "References"
    },
    {
      "bounding_box": [
        {
          "x": 451,
          "y": 1293
        },
        {
          "x": 2117,
          "y": 1293
        },
        {
          "x": 2117,
          "y": 3025
        },
        {
          "x": 451,
          "y": 3025
        }
      ],
      "category": "paragraph",
      "html": "<p id='136' style='font-size:14px'>[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint<br>arXiv:1607.06450, 2016.<br>[2] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind<br>Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.<br>In NeurIPS, 2020.<br>[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey<br>Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020.<br>[4] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu,<br>Chao Xu, and Wen Gao. Pre-trained image processing transformer. In CVPR, 2021.<br>[5] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng,<br>Ziwei Liu, Jiarui Xu, et al. Mmdetection: Open mmlab detection toolbox and benchmark. arXiv preprint<br>arXiv:1906.07155, 2019.<br>[6] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.<br>Generative pretraining from pixels. In ICML, 2020.<br>[7] Xiangxiang Chu, Bo Zhang, Zhi Tian, Xiaolin Wei, and Huaxia Xia. Do we really need explicit position<br>encodings for vision transformers? arXiv preprint arXiv:2102.10882, 2021.<br>[8] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data<br>augmentation with a reduced search space. In CVPR Workshops, 2020.<br>[9] Jacob Devlin, Ming- Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec-<br>tional transformers for language understanding. In NAACL-HLT (1), 2019.<br>[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas<br>Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth<br>16x16 words: Transformers for image recognition at scale. In ICLR, 2021.<br>[11] Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao,<br>Chunjing Xu, Yixing Xu, et al. A survey on vision transformer. arXiv preprint arXiv:2012.12556, 2020.<br>[12] Kai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, Chunjing Xu, and Chang Xu. Ghostnet: More features<br>from cheap operations. In CVPR, 2020.<br>[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.<br>In CVPR, 2016.<br>[14] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415,<br>2016.<br>[15] Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoefler, and Daniel Soudry. Augment your<br>batch: Improving generalization through instance repetition. In CVPR, 2020.<br>[16] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In CVPR, 2018.<br>[17] Huawei. Mindspore. https : / / www · mindspore · cn/, 2020.</p>",
      "id": 136,
      "page": 12,
      "text": "[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\n[2] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nIn NeurIPS, 2020.\n[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. End-to-end object detection with transformers. In ECCV, 2020.\n[4] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu,\nChao Xu, and Wen Gao. Pre-trained image processing transformer. In CVPR, 2021.\n[5] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng,\nZiwei Liu, Jiarui Xu, et al. Mmdetection: Open mmlab detection toolbox and benchmark. arXiv preprint\narXiv:1906.07155, 2019.\n[6] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.\nGenerative pretraining from pixels. In ICML, 2020.\n[7] Xiangxiang Chu, Bo Zhang, Zhi Tian, Xiaolin Wei, and Huaxia Xia. Do we really need explicit position\nencodings for vision transformers? arXiv preprint arXiv:2102.10882, 2021.\n[8] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data\naugmentation with a reduced search space. In CVPR Workshops, 2020.\n[9] Jacob Devlin, Ming- Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. In NAACL-HLT (1), 2019.\n[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. In ICLR, 2021.\n[11] Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang, An Xiao,\nChunjing Xu, Yixing Xu, et al. A survey on vision transformer. arXiv preprint arXiv:2012.12556, 2020.\n[12] Kai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, Chunjing Xu, and Chang Xu. Ghostnet: More features\nfrom cheap operations. In CVPR, 2020.\n[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\nIn CVPR, 2016.\n[14] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415,\n2016.\n[15] Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoefler, and Daniel Soudry. Augment your\nbatch: Improving generalization through instance repetition. In CVPR, 2020.\n[16] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In CVPR, 2018.\n[17] Huawei. Mindspore. https : / / www · mindspore · cn/, 2020."
    },
    {
      "bounding_box": [
        {
          "x": 1253,
          "y": 3093
        },
        {
          "x": 1300,
          "y": 3093
        },
        {
          "x": 1300,
          "y": 3129
        },
        {
          "x": 1253,
          "y": 3129
        }
      ],
      "category": "footer",
      "html": "<footer id='137' style='font-size:14px'>12</footer>",
      "id": 137,
      "page": 12,
      "text": "12"
    },
    {
      "bounding_box": [
        {
          "x": 441,
          "y": 291
        },
        {
          "x": 2117,
          "y": 291
        },
        {
          "x": 2117,
          "y": 3007
        },
        {
          "x": 441,
          "y": 3007
        }
      ],
      "category": "paragraph",
      "html": "<p id='138' style='font-size:14px'>[18] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.<br>[19] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional<br>neural networks. In NeurIPS, pages 1097-1105, 2012.<br>[20] Gustav Larsson, Michael Maire, and Gregory Shakhnarovich. Fractalnet: Ultra-deep neural networks<br>without residuals. arXiv preprint arXiv:1605.07648, 2016.<br>[21] Tsung- Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature<br>pyramid networks for object detection. In CVPR, 2017.<br>[22] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar,<br>and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, pages 740-755, 2014.<br>[23] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint<br>arXiv:1711.05101, 2017.<br>[24] Paul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? In NeurIPS, 2019.<br>[25] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of<br>classes. In 2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing, pages<br>722-729. IEEE, 2008.<br>[26] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In CVPR, pages<br>3498-3505. IEEE, 2012.<br>[27] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,<br>Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep<br>learning library. NeurIPS, 2019.<br>[28] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Dollar. Designing network<br>design spaces. In CVPR, 2020.<br>[29] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection<br>with region proposal networks. In Advances in neural information processing systems, pages 91-99, 2015.<br>[30] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,<br>Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge.<br>International Journal of Computer Vision, 115(3):211-252, 2015.<br>[31] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the<br>inception architecture for computer vision. In CVPR, 2016.<br>[32] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In<br>ICML, 2019.<br>[33] Yehui Tang, Kai Han, Chang Xu, An Xiao, Yiping Deng, Chao Xu, and Yunhe Wang. Augmented shortcuts<br>for vision transformers. arXiv preprint arXiv:2106.15941, 2021.<br>[34] Yehui Tang, Yunhe Wang, Yixing Xu, Yiping Deng, Chao Xu, Dacheng Tao, and Chang Xu. Manifold<br>regularized dynamic network pruning. In CVPR, pages 5018-5028, 2021.<br>[35] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve<br>Jegou. Training data-efficient image transformers & distillation through attention. In ICML, 2021.<br>[36] Hugo Touvron, Alexandre Sablayrolles, Matthijs Douze, Matthieu Cord, and Herve Jegou. Grafit: Learning<br>fine-grained image representations with coarse labels. arXiv preprint arXiv:2011.12982, 2020.<br>[37] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning<br>research, 9(11), 2008.<br>[38] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro<br>Perona, and Serge Belongie. The inaturalist species classification and detection dataset. In CVPR, pages<br>8769-8778, 2018.<br>[39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz<br>Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 2017.<br>[40] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and<br>Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions.<br>In ICCV, 2021.<br>[41] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In CVPR,<br>pages 7794-7803, 2018.<br>[42] Enze Xie, Wenjia Wang, Wenhai Wang, Peize Sun, Hang Xu, Ding Liang, and Ping Luo. Segmenting<br>transparent object in the wild with transformer. In IJCAI, 2021.<br>[43] Yixing Xu, Yunhe Wang, Kai Han, Yehui Tang, Shangling Jui, Chunjing Xu, and Chang Xu. Renas:<br>Relativistic evaluation of neural architecture search. In CVPR, pages 441 1-4420, 2021.</p>",
      "id": 138,
      "page": 13,
      "text": "[18] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\n[19] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional\nneural networks. In NeurIPS, pages 1097-1105, 2012.\n[20] Gustav Larsson, Michael Maire, and Gregory Shakhnarovich. Fractalnet: Ultra-deep neural networks\nwithout residuals. arXiv preprint arXiv:1605.07648, 2016.\n[21] Tsung- Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature\npyramid networks for object detection. In CVPR, 2017.\n[22] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar,\nand C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, pages 740-755, 2014.\n[23] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101, 2017.\n[24] Paul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? In NeurIPS, 2019.\n[25] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of\nclasses. In 2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing, pages\n722-729. IEEE, 2008.\n[26] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In CVPR, pages\n3498-3505. IEEE, 2012.\n[27] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,\nZeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep\nlearning library. NeurIPS, 2019.\n[28] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Dollar. Designing network\ndesign spaces. In CVPR, 2020.\n[29] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection\nwith region proposal networks. In Advances in neural information processing systems, pages 91-99, 2015.\n[30] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,\nAndrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge.\nInternational Journal of Computer Vision, 115(3):211-252, 2015.\n[31] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the\ninception architecture for computer vision. In CVPR, 2016.\n[32] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In\nICML, 2019.\n[33] Yehui Tang, Kai Han, Chang Xu, An Xiao, Yiping Deng, Chao Xu, and Yunhe Wang. Augmented shortcuts\nfor vision transformers. arXiv preprint arXiv:2106.15941, 2021.\n[34] Yehui Tang, Yunhe Wang, Yixing Xu, Yiping Deng, Chao Xu, Dacheng Tao, and Chang Xu. Manifold\nregularized dynamic network pruning. In CVPR, pages 5018-5028, 2021.\n[35] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve\nJegou. Training data-efficient image transformers & distillation through attention. In ICML, 2021.\n[36] Hugo Touvron, Alexandre Sablayrolles, Matthijs Douze, Matthieu Cord, and Herve Jegou. Grafit: Learning\nfine-grained image representations with coarse labels. arXiv preprint arXiv:2011.12982, 2020.\n[37] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning\nresearch, 9(11), 2008.\n[38] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro\nPerona, and Serge Belongie. The inaturalist species classification and detection dataset. In CVPR, pages\n8769-8778, 2018.\n[39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 2017.\n[40] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and\nLing Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions.\nIn ICCV, 2021.\n[41] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In CVPR,\npages 7794-7803, 2018.\n[42] Enze Xie, Wenjia Wang, Wenhai Wang, Peize Sun, Hang Xu, Ding Liang, and Ping Luo. Segmenting\ntransparent object in the wild with transformer. In IJCAI, 2021.\n[43] Yixing Xu, Yunhe Wang, Kai Han, Yehui Tang, Shangling Jui, Chunjing Xu, and Chang Xu. Renas:\nRelativistic evaluation of neural architecture search. In CVPR, pages 441 1-4420, 2021."
    },
    {
      "bounding_box": [
        {
          "x": 1252,
          "y": 3093
        },
        {
          "x": 1299,
          "y": 3093
        },
        {
          "x": 1299,
          "y": 3130
        },
        {
          "x": 1252,
          "y": 3130
        }
      ],
      "category": "footer",
      "html": "<footer id='139' style='font-size:18px'>13</footer>",
      "id": 139,
      "page": 13,
      "text": "13"
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 305
        },
        {
          "x": 2113,
          "y": 305
        },
        {
          "x": 2113,
          "y": 1080
        },
        {
          "x": 443,
          "y": 1080
        }
      ],
      "category": "paragraph",
      "html": "<p id='140' style='font-size:14px'>[44] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay, Jiashi Feng,<br>and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. arXiv<br>preprint arXiv:2101.11986, 2021.<br>[45] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo.<br>Cutmix: Regularization strategy to train strong classifiers with localizable features. In ICCV, 2019.<br>[46] HongyiZhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk<br>minimization. arXiv preprint arXiv:1710.09412, 2017.<br>[47] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng<br>Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a sequence-to-sequence<br>perspective with transformers. In CVPR, 2021.<br>[48] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation.<br>In AAAI, volume 34, pages 13001-13008, 2020.<br>[49] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing<br>through ade20k dataset. In CVPR, 2017.<br>[50] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable<br>transformers for end-to-end object detection. In ICLR, 2021.</p>",
      "id": 140,
      "page": 14,
      "text": "[44] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay, Jiashi Feng,\nand Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. arXiv\npreprint arXiv:2101.11986, 2021.\n[45] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo.\nCutmix: Regularization strategy to train strong classifiers with localizable features. In ICCV, 2019.\n[46] HongyiZhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk\nminimization. arXiv preprint arXiv:1710.09412, 2017.\n[47] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng\nFeng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a sequence-to-sequence\nperspective with transformers. In CVPR, 2021.\n[48] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation.\nIn AAAI, volume 34, pages 13001-13008, 2020.\n[49] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing\nthrough ade20k dataset. In CVPR, 2017.\n[50] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable\ntransformers for end-to-end object detection. In ICLR, 2021."
    },
    {
      "bounding_box": [
        {
          "x": 1252,
          "y": 3090
        },
        {
          "x": 1299,
          "y": 3090
        },
        {
          "x": 1299,
          "y": 3130
        },
        {
          "x": 1252,
          "y": 3130
        }
      ],
      "category": "footer",
      "html": "<footer id='141' style='font-size:14px'>14</footer>",
      "id": 141,
      "page": 14,
      "text": "14"
    }
  ]
}