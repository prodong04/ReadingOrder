{
  "id": "62a3a1de-0f92-11ef-8230-426932df3dcf",
  "pdf_path": "./pdf/AI_VIT_O/2201.03545v2.pdf",
  "elements": [
    {
      "bounding_box": [
        {
          "x": 923,
          "y": 436
        },
        {
          "x": 1558,
          "y": 436
        },
        {
          "x": 1558,
          "y": 500
        },
        {
          "x": 923,
          "y": 500
        }
      ],
      "category": "paragraph",
      "html": "<p id='0' style='font-size:22px'>A ConvNet for the 2020s</p>",
      "id": 0,
      "page": 1,
      "text": "A ConvNet for the 2020s"
    },
    {
      "bounding_box": [
        {
          "x": 196,
          "y": 595
        },
        {
          "x": 2350,
          "y": 595
        },
        {
          "x": 2350,
          "y": 740
        },
        {
          "x": 196,
          "y": 740
        }
      ],
      "category": "paragraph",
      "html": "<p id='1' style='font-size:20px'>Zhuang Liu1,2* Hanzi Mao1 Chao- Yuan Wu1 Christoph Feichtenhofer1 Trevor Darrell2 Saining Xie1t<br>1Facebook AI Research (FAIR) 2UC Berkeley</p>",
      "id": 1,
      "page": 1,
      "text": "Zhuang Liu1,2* Hanzi Mao1 Chao- Yuan Wu1 Christoph Feichtenhofer1 Trevor Darrell2 Saining Xie1t\n1Facebook AI Research (FAIR) 2UC Berkeley"
    },
    {
      "bounding_box": [
        {
          "x": 700,
          "y": 757
        },
        {
          "x": 1795,
          "y": 757
        },
        {
          "x": 1795,
          "y": 798
        },
        {
          "x": 700,
          "y": 798
        }
      ],
      "category": "paragraph",
      "html": "<p id='2' style='font-size:16px'>Code: https : / / github · com/ facebookresearch / ConvNeXt</p>",
      "id": 2,
      "page": 1,
      "text": "Code: https : / / github · com/ facebookresearch / ConvNeXt"
    },
    {
      "bounding_box": [
        {
          "x": 602,
          "y": 856
        },
        {
          "x": 799,
          "y": 856
        },
        {
          "x": 799,
          "y": 910
        },
        {
          "x": 602,
          "y": 910
        }
      ],
      "category": "paragraph",
      "html": "<p id='3' style='font-size:22px'>Abstract</p>",
      "id": 3,
      "page": 1,
      "text": "Abstract"
    },
    {
      "bounding_box": [
        {
          "x": 199,
          "y": 960
        },
        {
          "x": 1201,
          "y": 960
        },
        {
          "x": 1201,
          "y": 2212
        },
        {
          "x": 199,
          "y": 2212
        }
      ],
      "category": "paragraph",
      "html": "<p id='4' style='font-size:18px'>The \"Roaring 20s\" of visual recognition began with the<br>introduction of Vision Transformers (ViTs), which quickly<br>superseded ConvNets as the state-of-the-art image classifica-<br>tion model. A vanilla ViT, on the other hand, faces difficulties<br>when applied to general computer vision tasks such as object<br>detection and semantic segmentation. It is the hierarchical<br>Transformers (e.g., Swin Transformers) that reintroduced sev-<br>eral ConvNet priors, making Transformers practically viable<br>as a generic vision backbone and demonstrating remarkable<br>performance on a wide variety of vision tasks. However,<br>the effectiveness of such hybrid approaches is still largely<br>credited to the intrinsic superiority of Transformers, rather<br>than the inherent inductive biases of convolutions. In this<br>work, we reexamine the design spaces and test the limits of<br>what a pure ConvNet can achieve. We gradually \"modernize\"<br>a standard ResNet toward the design of a vision Transformer,<br>and discover several key components that contribute to the<br>performance difference along the way. The outcome of this<br>exploration is a family of pure ConvNet models dubbed Con-<br>vNeXt. Constructed entirely from standard ConvNet modules,<br>ConvNeXts compete favorably with Transformers in terms of<br>accuracy and scalability, achieving 87.8% ImageNet top-1<br>accuracy and outperforming Swin Transformers on COCO<br>detection and ADE20K segmentation, while maintaining the<br>simplicity and efficiency of standard ConvNets.</p>",
      "id": 4,
      "page": 1,
      "text": "The \"Roaring 20s\" of visual recognition began with the\nintroduction of Vision Transformers (ViTs), which quickly\nsuperseded ConvNets as the state-of-the-art image classifica-\ntion model. A vanilla ViT, on the other hand, faces difficulties\nwhen applied to general computer vision tasks such as object\ndetection and semantic segmentation. It is the hierarchical\nTransformers (e.g., Swin Transformers) that reintroduced sev-\neral ConvNet priors, making Transformers practically viable\nas a generic vision backbone and demonstrating remarkable\nperformance on a wide variety of vision tasks. However,\nthe effectiveness of such hybrid approaches is still largely\ncredited to the intrinsic superiority of Transformers, rather\nthan the inherent inductive biases of convolutions. In this\nwork, we reexamine the design spaces and test the limits of\nwhat a pure ConvNet can achieve. We gradually \"modernize\"\na standard ResNet toward the design of a vision Transformer,\nand discover several key components that contribute to the\nperformance difference along the way. The outcome of this\nexploration is a family of pure ConvNet models dubbed Con-\nvNeXt. Constructed entirely from standard ConvNet modules,\nConvNeXts compete favorably with Transformers in terms of\naccuracy and scalability, achieving 87.8% ImageNet top-1\naccuracy and outperforming Swin Transformers on COCO\ndetection and ADE20K segmentation, while maintaining the\nsimplicity and efficiency of standard ConvNets."
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 2312
        },
        {
          "x": 533,
          "y": 2312
        },
        {
          "x": 533,
          "y": 2366
        },
        {
          "x": 204,
          "y": 2366
        }
      ],
      "category": "paragraph",
      "html": "<p id='5' style='font-size:20px'>1. Introduction</p>",
      "id": 5,
      "page": 1,
      "text": "1. Introduction"
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 2399
        },
        {
          "x": 1202,
          "y": 2399
        },
        {
          "x": 1202,
          "y": 2851
        },
        {
          "x": 201,
          "y": 2851
        }
      ],
      "category": "paragraph",
      "html": "<p id='6' style='font-size:18px'>Looking back at the 2010s, the decade was marked by<br>the monumental progress and impact of deep learning. The<br>primary driver was the renaissance of neural networks, partic-<br>ularly convolutional neural networks (ConvNets). Through<br>the decade, the field of visual recognition successfully<br>shifted from engineering features to designing (ConvNet)<br>architectures. Although the invention of back-propagation-<br>trained ConvNets dates all the way back to the 1980s [42],<br>it was not until late 2012 that we saw its true potential for</p>",
      "id": 6,
      "page": 1,
      "text": "Looking back at the 2010s, the decade was marked by\nthe monumental progress and impact of deep learning. The\nprimary driver was the renaissance of neural networks, partic-\nularly convolutional neural networks (ConvNets). Through\nthe decade, the field of visual recognition successfully\nshifted from engineering features to designing (ConvNet)\narchitectures. Although the invention of back-propagation-\ntrained ConvNets dates all the way back to the 1980s [42],\nit was not until late 2012 that we saw its true potential for"
    },
    {
      "bounding_box": [
        {
          "x": 252,
          "y": 2893
        },
        {
          "x": 1044,
          "y": 2893
        },
        {
          "x": 1044,
          "y": 2973
        },
        {
          "x": 252,
          "y": 2973
        }
      ],
      "category": "paragraph",
      "html": "<p id='7' style='font-size:16px'>* Work done during an internship at Facebook AI Research.<br>1 Corresponding author.</p>",
      "id": 7,
      "page": 1,
      "text": "* Work done during an internship at Facebook AI Research.\n1 Corresponding author."
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 852
        },
        {
          "x": 2271,
          "y": 852
        },
        {
          "x": 2271,
          "y": 1501
        },
        {
          "x": 1281,
          "y": 1501
        }
      ],
      "category": "figure",
      "html": "<br><figure><img id='8' style='font-size:14px' alt=\"ImageNet-1K Acc.\n90\n88\nConvNeXt\n86\nSwin Transformer\n(2021) ConvNeXt\nSwin Transformer\n84\nDeiT ViT (2021)\nResNet\n(2020) (2020)\n(2015)\n82\nDiameter\n80\n4 8 16 256 GFLOPs\n78\nImageNet-1K Trained ImageNet-22K Pre-trained\" data-coord=\"top-left:(1281,852); bottom-right:(2271,1501)\" /></figure>",
      "id": 8,
      "page": 1,
      "text": "ImageNet-1K Acc.\n90\n88\nConvNeXt\n86\nSwin Transformer\n(2021) ConvNeXt\nSwin Transformer\n84\nDeiT ViT (2021)\nResNet\n(2020) (2020)\n(2015)\n82\nDiameter\n80\n4 8 16 256 GFLOPs\n78\nImageNet-1K Trained ImageNet-22K Pre-trained"
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 1530
        },
        {
          "x": 2279,
          "y": 1530
        },
        {
          "x": 2279,
          "y": 1900
        },
        {
          "x": 1279,
          "y": 1900
        }
      ],
      "category": "caption",
      "html": "<caption id='9' style='font-size:16px'>Figure 1. ImageNet-1K classification results for ● ConvNets and<br>○ vision Transformers. Each bubble's area is proportional to FLOPs<br>of a variant in a model family. ImageNet-1K/22K models here<br>take 2242/3842 images respectively. ResNet and ViT results were<br>obtained with improved training procedures over the original papers.<br>We demonstrate that a standard ConvNet model can achieve the<br>same level of scalability as hierarchical vision Transformers while<br>being much simpler in design.</caption>",
      "id": 9,
      "page": 1,
      "text": "Figure 1. ImageNet-1K classification results for ● ConvNets and\n○ vision Transformers. Each bubble's area is proportional to FLOPs\nof a variant in a model family. ImageNet-1K/22K models here\ntake 2242/3842 images respectively. ResNet and ViT results were\nobtained with improved training procedures over the original papers.\nWe demonstrate that a standard ConvNet model can achieve the\nsame level of scalability as hierarchical vision Transformers while\nbeing much simpler in design."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 1927
        },
        {
          "x": 2277,
          "y": 1927
        },
        {
          "x": 2277,
          "y": 2324
        },
        {
          "x": 1280,
          "y": 2324
        }
      ],
      "category": "paragraph",
      "html": "<p id='10' style='font-size:18px'>visual feature learning. The introduction of AlexNet [40]<br>precipitated the \"ImageNet moment\" [59], ushering in a new<br>era of computer vision. The field has since evolved at a<br>rapid speed. Representative ConvNets like VGGNet [64],<br>Inceptions [68], ResNe(X)t [28, 87], DenseNet [36], Mo-<br>bileNet [34], EfficientNet [71] and RegNet [54] focused on<br>different aspects of accuracy, efficiency and scalability, and<br>popularized many useful design principles.</p>",
      "id": 10,
      "page": 1,
      "text": "visual feature learning. The introduction of AlexNet [40]\nprecipitated the \"ImageNet moment\" [59], ushering in a new\nera of computer vision. The field has since evolved at a\nrapid speed. Representative ConvNets like VGGNet [64],\nInceptions [68], ResNe(X)t [28, 87], DenseNet [36], Mo-\nbileNet [34], EfficientNet [71] and RegNet [54] focused on\ndifferent aspects of accuracy, efficiency and scalability, and\npopularized many useful design principles."
    },
    {
      "bounding_box": [
        {
          "x": 1277,
          "y": 2330
        },
        {
          "x": 2278,
          "y": 2330
        },
        {
          "x": 2278,
          "y": 2977
        },
        {
          "x": 1277,
          "y": 2977
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='11' style='font-size:18px'>The full dominance of ConvNets in computer vision was<br>not a coincidence: in many application scenarios, a \"sliding<br>window\" strategy is intrinsic to visual processing, particu-<br>larly when working with high-resolution images. ConvNets<br>have several built-in inductive biases that make them well-<br>suited to a wide variety of computer vision applications. The<br>most important one is translation equivariance, which is a de-<br>sirable property for tasks like objection detection. ConvNets<br>are also inherently efficient due to the fact that when used in<br>a sliding-window manner, the computations are shared [62].<br>For many decades, this has been the default use of ConvNets,<br>generally on limited object categories such as digits [43],<br>faces [58, 76] and pedestrians [19, 63]. Entering the 2010s,</p>",
      "id": 11,
      "page": 1,
      "text": "The full dominance of ConvNets in computer vision was\nnot a coincidence: in many application scenarios, a \"sliding\nwindow\" strategy is intrinsic to visual processing, particu-\nlarly when working with high-resolution images. ConvNets\nhave several built-in inductive biases that make them well-\nsuited to a wide variety of computer vision applications. The\nmost important one is translation equivariance, which is a de-\nsirable property for tasks like objection detection. ConvNets\nare also inherently efficient due to the fact that when used in\na sliding-window manner, the computations are shared [62].\nFor many decades, this has been the default use of ConvNets,\ngenerally on limited object categories such as digits [43],\nfaces [58, 76] and pedestrians [19, 63]. Entering the 2010s,"
    },
    {
      "bounding_box": [
        {
          "x": 64,
          "y": 871
        },
        {
          "x": 150,
          "y": 871
        },
        {
          "x": 150,
          "y": 2322
        },
        {
          "x": 64,
          "y": 2322
        }
      ],
      "category": "footer",
      "html": "<br><footer id='12' style='font-size:14px'>2022<br>Mar<br>2<br>[cs.CV]<br>arXiv:2201.03545v2</footer>",
      "id": 12,
      "page": 1,
      "text": "2022\nMar\n2\n[cs.CV]\narXiv:2201.03545v2"
    },
    {
      "bounding_box": [
        {
          "x": 199,
          "y": 305
        },
        {
          "x": 1201,
          "y": 305
        },
        {
          "x": 1201,
          "y": 453
        },
        {
          "x": 199,
          "y": 453
        }
      ],
      "category": "paragraph",
      "html": "<p id='13' style='font-size:20px'>the region-based detectors [23, 24, 27, 57] further elevated<br>ConvNets to the position of being the fundamental building<br>block in a visual recognition system.</p>",
      "id": 13,
      "page": 2,
      "text": "the region-based detectors [23, 24, 27, 57] further elevated\nConvNets to the position of being the fundamental building\nblock in a visual recognition system."
    },
    {
      "bounding_box": [
        {
          "x": 199,
          "y": 463
        },
        {
          "x": 1202,
          "y": 463
        },
        {
          "x": 1202,
          "y": 1761
        },
        {
          "x": 199,
          "y": 1761
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='14' style='font-size:18px'>Around the same time, the odyssey of neural network<br>design for natural language processing (NLP) took a very<br>different path, as the Transformers replaced recurrent neural<br>networks to become the dominant backbone architecture.<br>Despite the disparity in the task of interest between language<br>and vision domains, the two streams surprisingly converged<br>in the year 2020, as the introduction of Vision Transformers<br>(ViT) completely altered the landscape of network architec-<br>ture design. Except for the initial \"patchify\" layer, which<br>splits an image into a sequence of patches, ViT introduces no<br>image-specific inductive bias and makes minimal changes<br>to the original NLP Transformers. One primary focus of<br>ViT is on the scaling behavior: with the help of larger model<br>and dataset sizes, Transformers can outperform standard<br>ResNets by a significant margin. Those results on image<br>classification tasks are inspiring, but computer vision is not<br>limited to image classification. As discussed previously,<br>solutions to numerous computer vision tasks in the past<br>decade depended significantly on a sliding-window, fully-<br>convolutional paradigm. Without the ConvNet inductive<br>biases, a vanilla ViT model faces many challenges in being<br>adopted as a generic vision backbone. The biggest chal-<br>lenge is ViT's global attention design, which has a quadratic<br>complexity with respect to the input size. This might be<br>acceptable for ImageNet classification, but quickly becomes<br>intractable with higher-resolution inputs.</p>",
      "id": 14,
      "page": 2,
      "text": "Around the same time, the odyssey of neural network\ndesign for natural language processing (NLP) took a very\ndifferent path, as the Transformers replaced recurrent neural\nnetworks to become the dominant backbone architecture.\nDespite the disparity in the task of interest between language\nand vision domains, the two streams surprisingly converged\nin the year 2020, as the introduction of Vision Transformers\n(ViT) completely altered the landscape of network architec-\nture design. Except for the initial \"patchify\" layer, which\nsplits an image into a sequence of patches, ViT introduces no\nimage-specific inductive bias and makes minimal changes\nto the original NLP Transformers. One primary focus of\nViT is on the scaling behavior: with the help of larger model\nand dataset sizes, Transformers can outperform standard\nResNets by a significant margin. Those results on image\nclassification tasks are inspiring, but computer vision is not\nlimited to image classification. As discussed previously,\nsolutions to numerous computer vision tasks in the past\ndecade depended significantly on a sliding-window, fully-\nconvolutional paradigm. Without the ConvNet inductive\nbiases, a vanilla ViT model faces many challenges in being\nadopted as a generic vision backbone. The biggest chal-\nlenge is ViT's global attention design, which has a quadratic\ncomplexity with respect to the input size. This might be\nacceptable for ImageNet classification, but quickly becomes\nintractable with higher-resolution inputs."
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 1770
        },
        {
          "x": 1201,
          "y": 1770
        },
        {
          "x": 1201,
          "y": 2368
        },
        {
          "x": 200,
          "y": 2368
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='15' style='font-size:18px'>Hierarchical Transformers employ a hybrid approach to<br>bridge this gap. For example, the \"sliding window\" strategy<br>(e.g. attention within local windows) was reintroduced to<br>Transformers, allowing them to behave more similarly to<br>ConvNets. Swin Transformer [45] is a milestone work in this<br>direction, demonstrating for the first time that Transformers<br>can be adopted as a generic vision backbone and achieve<br>state-of-the-art performance across a range of computer vi-<br>sion tasks beyond image classification. Swin Transformer's<br>success and rapid adoption also revealed one thing: the<br>essence of convolution is not becoming irrelevant; rather, it<br>remains much desired and has never faded.</p>",
      "id": 15,
      "page": 2,
      "text": "Hierarchical Transformers employ a hybrid approach to\nbridge this gap. For example, the \"sliding window\" strategy\n(e.g. attention within local windows) was reintroduced to\nTransformers, allowing them to behave more similarly to\nConvNets. Swin Transformer [45] is a milestone work in this\ndirection, demonstrating for the first time that Transformers\ncan be adopted as a generic vision backbone and achieve\nstate-of-the-art performance across a range of computer vi-\nsion tasks beyond image classification. Swin Transformer's\nsuccess and rapid adoption also revealed one thing: the\nessence of convolution is not becoming irrelevant; rather, it\nremains much desired and has never faded."
    },
    {
      "bounding_box": [
        {
          "x": 199,
          "y": 2378
        },
        {
          "x": 1201,
          "y": 2378
        },
        {
          "x": 1201,
          "y": 2978
        },
        {
          "x": 199,
          "y": 2978
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='16' style='font-size:18px'>Under this perspective, many of the advancements of<br>Transformers for computer vision have been aimed at bring-<br>ing back convolutions. These attempts, however, come<br>at a cost: a naive implementation of sliding window self-<br>attention can be expensive [55]; with advanced approaches<br>such as cyclic shifting [45], the speed can be optimized but<br>the system becomes more sophisticated in design. On the<br>other hand, itis almost ironic that a ConvNet already satisfies<br>many of those desired properties, albeit in a straightforward,<br>no-frills way. The only reason ConvNets appear to be losing<br>steam is that (hierarchical) Transformers surpass them in<br>many vision tasks, and the performance difference is usually</p>",
      "id": 16,
      "page": 2,
      "text": "Under this perspective, many of the advancements of\nTransformers for computer vision have been aimed at bring-\ning back convolutions. These attempts, however, come\nat a cost: a naive implementation of sliding window self-\nattention can be expensive [55]; with advanced approaches\nsuch as cyclic shifting [45], the speed can be optimized but\nthe system becomes more sophisticated in design. On the\nother hand, itis almost ironic that a ConvNet already satisfies\nmany of those desired properties, albeit in a straightforward,\nno-frills way. The only reason ConvNets appear to be losing\nsteam is that (hierarchical) Transformers surpass them in\nmany vision tasks, and the performance difference is usually"
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 307
        },
        {
          "x": 2281,
          "y": 307
        },
        {
          "x": 2281,
          "y": 402
        },
        {
          "x": 1279,
          "y": 402
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='17' style='font-size:20px'>attributed to the superior scaling behavior of Transformers,<br>with multi-head self-attention being the key component.</p>",
      "id": 17,
      "page": 2,
      "text": "attributed to the superior scaling behavior of Transformers,\nwith multi-head self-attention being the key component."
    },
    {
      "bounding_box": [
        {
          "x": 1276,
          "y": 406
        },
        {
          "x": 2280,
          "y": 406
        },
        {
          "x": 2280,
          "y": 1149
        },
        {
          "x": 1276,
          "y": 1149
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='18' style='font-size:16px'>Unlike ConvNets, which have progressively improved<br>over the last decade, the adoption of Vision Transformers<br>was a step change. In recent literature, system-level com-<br>parisons (e.g. a Swin Transformer vs. a ResNet) are usually<br>adopted when comparing the two. ConvNets and hierar-<br>chical vision Transformers become different and similar at<br>the same time: they are both equipped with similar induc-<br>tive biases, but differ significantly in the training procedure<br>and macro/micro-level architecture design. In this work,<br>we investigate the architectural distinctions between Con-<br>vNets and Transformers and try to identify the confounding<br>variables when comparing the network performance. Our<br>research is intended to bridge the gap between the pre- ViT<br>and post- ViT eras for ConvNets, as well as to test the limits<br>of what a pure ConvNet can achieve.</p>",
      "id": 18,
      "page": 2,
      "text": "Unlike ConvNets, which have progressively improved\nover the last decade, the adoption of Vision Transformers\nwas a step change. In recent literature, system-level com-\nparisons (e.g. a Swin Transformer vs. a ResNet) are usually\nadopted when comparing the two. ConvNets and hierar-\nchical vision Transformers become different and similar at\nthe same time: they are both equipped with similar induc-\ntive biases, but differ significantly in the training procedure\nand macro/micro-level architecture design. In this work,\nwe investigate the architectural distinctions between Con-\nvNets and Transformers and try to identify the confounding\nvariables when comparing the network performance. Our\nresearch is intended to bridge the gap between the pre- ViT\nand post- ViT eras for ConvNets, as well as to test the limits\nof what a pure ConvNet can achieve."
    },
    {
      "bounding_box": [
        {
          "x": 1277,
          "y": 1153
        },
        {
          "x": 2281,
          "y": 1153
        },
        {
          "x": 2281,
          "y": 2048
        },
        {
          "x": 1277,
          "y": 2048
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='19' style='font-size:18px'>To do this, we start with a standard ResNet (e.g. ResNet-<br>50) trained with an improved procedure. We gradually \"mod-<br>ernize\" the architecture to the construction of a hierarchical<br>vision Transformer (e.g. Swin-T). Our exploration is directed<br>by a key question: How do design decisions in Transformers<br>impact ConvNets' performance? We discover several key<br>components that contribute to the performance difference<br>along the way. As a result, we propose a family of pure<br>ConvNets dubbed ConvNeXt. We evaluate ConvNeXts on a<br>variety of vision tasks such as ImageNet classification [17],<br>object detection/segmentation on COCO [44], and semantic<br>segmentation on ADE20K [92]. Surprisingly, ConvNeXts,<br>constructed entirely from standard ConvNet modules, com-<br>pete favorably with Transformers in terms of accuracy, scal-<br>ability and robustness across all major benchmarks. Con-<br>vNeXt maintains the efficiency of standard ConvNets, and<br>the fully-convolutional nature for both training and testing<br>makes it extremely simple to implement.</p>",
      "id": 19,
      "page": 2,
      "text": "To do this, we start with a standard ResNet (e.g. ResNet-\n50) trained with an improved procedure. We gradually \"mod-\nernize\" the architecture to the construction of a hierarchical\nvision Transformer (e.g. Swin-T). Our exploration is directed\nby a key question: How do design decisions in Transformers\nimpact ConvNets' performance? We discover several key\ncomponents that contribute to the performance difference\nalong the way. As a result, we propose a family of pure\nConvNets dubbed ConvNeXt. We evaluate ConvNeXts on a\nvariety of vision tasks such as ImageNet classification [17],\nobject detection/segmentation on COCO [44], and semantic\nsegmentation on ADE20K [92]. Surprisingly, ConvNeXts,\nconstructed entirely from standard ConvNet modules, com-\npete favorably with Transformers in terms of accuracy, scal-\nability and robustness across all major benchmarks. Con-\nvNeXt maintains the efficiency of standard ConvNets, and\nthe fully-convolutional nature for both training and testing\nmakes it extremely simple to implement."
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 2050
        },
        {
          "x": 2281,
          "y": 2050
        },
        {
          "x": 2281,
          "y": 2199
        },
        {
          "x": 1279,
          "y": 2199
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='20' style='font-size:16px'>We hope the new observations and discussions can chal-<br>lenge some common beliefs and encourage people to rethink<br>the importance of convolutions in computer vision.</p>",
      "id": 20,
      "page": 2,
      "text": "We hope the new observations and discussions can chal-\nlenge some common beliefs and encourage people to rethink\nthe importance of convolutions in computer vision."
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 2243
        },
        {
          "x": 2127,
          "y": 2243
        },
        {
          "x": 2127,
          "y": 2299
        },
        {
          "x": 1281,
          "y": 2299
        }
      ],
      "category": "paragraph",
      "html": "<p id='21' style='font-size:22px'>2. Modernizing a ConvNet: a Roadmap</p>",
      "id": 21,
      "page": 2,
      "text": "2. Modernizing a ConvNet: a Roadmap"
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 2329
        },
        {
          "x": 2279,
          "y": 2329
        },
        {
          "x": 2279,
          "y": 2774
        },
        {
          "x": 1279,
          "y": 2774
        }
      ],
      "category": "paragraph",
      "html": "<p id='22' style='font-size:14px'>In this section, we provide a trajectory going from a<br>ResNet to a ConvNet that bears a resemblance to Transform-<br>ers. We consider two model sizes in terms of FLOPs, one is<br>the ResNet-50 / Swin-T regime with FLOPs around 4.5 x 109<br>and the other being ResNet-200 / Swin-B regime which has<br>FLOPs around 15.0 x 109. For simplicity, we will present<br>the results with the ResNet-50 / Swin-T complexity models.<br>The conclusions for higher capacity models are consistent<br>and results can be found in Appendix C.</p>",
      "id": 22,
      "page": 2,
      "text": "In this section, we provide a trajectory going from a\nResNet to a ConvNet that bears a resemblance to Transform-\ners. We consider two model sizes in terms of FLOPs, one is\nthe ResNet-50 / Swin-T regime with FLOPs around 4.5 x 109\nand the other being ResNet-200 / Swin-B regime which has\nFLOPs around 15.0 x 109. For simplicity, we will present\nthe results with the ResNet-50 / Swin-T complexity models.\nThe conclusions for higher capacity models are consistent\nand results can be found in Appendix C."
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 2779
        },
        {
          "x": 2279,
          "y": 2779
        },
        {
          "x": 2279,
          "y": 2977
        },
        {
          "x": 1279,
          "y": 2977
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='23' style='font-size:16px'>At a high level, our explorations are directed to inves-<br>tigate and follow different levels of designs from a Swin<br>Transformer while maintaining the network's simplicity as<br>a standard ConvNet. The roadmap of our exploration is as</p>",
      "id": 23,
      "page": 2,
      "text": "At a high level, our explorations are directed to inves-\ntigate and follow different levels of designs from a Swin\nTransformer while maintaining the network's simplicity as\na standard ConvNet. The roadmap of our exploration is as"
    },
    {
      "bounding_box": [
        {
          "x": 210,
          "y": 290
        },
        {
          "x": 1191,
          "y": 290
        },
        {
          "x": 1191,
          "y": 1552
        },
        {
          "x": 210,
          "y": 1552
        }
      ],
      "category": "figure",
      "html": "<figure><img id='24' style='font-size:14px' alt=\"GFLOPs\nResNet-50/200 78.8 4.1\nstage ratio 79.4\nMacro\nDesign\n'patchify' stem 79.5\ndepth conv 783\nResNeXt\nwidth ↑ 80.5 3\nInverted\ninverting dims 80.6\nBottleneck\nmove ↑ d. conv 79.9 1\nkernel sz. → 5 80.4 4.1\nLarge\nkernel sz. →7 80.6 4.2\nKernel\nkernel sz. → 9 80.6 2\nkernel sz. → 11 80.5 4.3\nReLU→GELU 80.6 4.2\nfewer activations 81.3 4.2\nMicro\nDesign fewer norms 81.4 4.2\nBN → LN 81.5 4.2\nsep. d.s. conv\n82.0 ★4.5\nConvNeXt-T/B\n■\nSwin-T/B 81.3 4.5\nImageNet\nTop1 Acc (%) 78 80 82\" data-coord=\"top-left:(210,290); bottom-right:(1191,1552)\" /></figure>",
      "id": 24,
      "page": 3,
      "text": "GFLOPs\nResNet-50/200 78.8 4.1\nstage ratio 79.4\nMacro\nDesign\n\"patchify\" stem 79.5\ndepth conv 783\nResNeXt\nwidth ↑ 80.5 3\nInverted\ninverting dims 80.6\nBottleneck\nmove ↑ d. conv 79.9 1\nkernel sz. → 5 80.4 4.1\nLarge\nkernel sz. →7 80.6 4.2\nKernel\nkernel sz. → 9 80.6 2\nkernel sz. → 11 80.5 4.3\nReLU→GELU 80.6 4.2\nfewer activations 81.3 4.2\nMicro\nDesign fewer norms 81.4 4.2\nBN → LN 81.5 4.2\nsep. d.s. conv\n82.0 ★4.5\nConvNeXt-T/B\n■\nSwin-T/B 81.3 4.5\nImageNet\nTop1 Acc (%) 78 80 82"
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 1555
        },
        {
          "x": 1201,
          "y": 1555
        },
        {
          "x": 1201,
          "y": 2011
        },
        {
          "x": 201,
          "y": 2011
        }
      ],
      "category": "caption",
      "html": "<br><caption id='25' style='font-size:14px'>Figure 2. We modernize a standard ConvNet (ResNet) towards<br>the design of a hierarchical vision Transformer (Swin), without<br>introducing any attention-based modules. The foreground bars are<br>model accuracies in the ResNet-50/Swin-T FLOP regime; results<br>for the ResNet-200/Swin-B regime are shown with the gray bars. A<br>hatched bar means the modification is not adopted. Detailed results<br>for both regimes are in the appendix. Many Transformer archi-<br>tectural choices can be incorporated in a ConvNet, and they lead<br>to increasingly better performance. In the end, our pure ConvNet<br>model, named ConvNeXt, can outperform the Swin Transformer.</caption>",
      "id": 25,
      "page": 3,
      "text": "Figure 2. We modernize a standard ConvNet (ResNet) towards\nthe design of a hierarchical vision Transformer (Swin), without\nintroducing any attention-based modules. The foreground bars are\nmodel accuracies in the ResNet-50/Swin-T FLOP regime; results\nfor the ResNet-200/Swin-B regime are shown with the gray bars. A\nhatched bar means the modification is not adopted. Detailed results\nfor both regimes are in the appendix. Many Transformer archi-\ntectural choices can be incorporated in a ConvNet, and they lead\nto increasingly better performance. In the end, our pure ConvNet\nmodel, named ConvNeXt, can outperform the Swin Transformer."
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 2048
        },
        {
          "x": 1202,
          "y": 2048
        },
        {
          "x": 1202,
          "y": 2748
        },
        {
          "x": 200,
          "y": 2748
        }
      ],
      "category": "paragraph",
      "html": "<p id='26' style='font-size:16px'>follows. Our starting point is a ResNet-50 model. We first<br>train it with similar training techniques used to train vision<br>Transformers and obtain much improved results compared to<br>the original ResNet-50. This will be our baseline. We then<br>study a series of design decisions which we summarized<br>as 1) macro design, 2) ResNeXt, 3) inverted bottleneck, 4)<br>large kernel size, and 5) various layer-wise micro designs. In<br>Figure 2, we show the procedure and the results we are able<br>to achieve with each step of the \"network modernization\".<br>Since network complexity is closely correlated with the fi-<br>nal performance, the FLOPs are roughly controlled over the<br>course of the exploration, though at intermediate steps the<br>FLOPs might be higher or lower than the reference models.<br>All models are trained and evaluated on ImageNet-1K.</p>",
      "id": 26,
      "page": 3,
      "text": "follows. Our starting point is a ResNet-50 model. We first\ntrain it with similar training techniques used to train vision\nTransformers and obtain much improved results compared to\nthe original ResNet-50. This will be our baseline. We then\nstudy a series of design decisions which we summarized\nas 1) macro design, 2) ResNeXt, 3) inverted bottleneck, 4)\nlarge kernel size, and 5) various layer-wise micro designs. In\nFigure 2, we show the procedure and the results we are able\nto achieve with each step of the \"network modernization\".\nSince network complexity is closely correlated with the fi-\nnal performance, the FLOPs are roughly controlled over the\ncourse of the exploration, though at intermediate steps the\nFLOPs might be higher or lower than the reference models.\nAll models are trained and evaluated on ImageNet-1K."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 2793
        },
        {
          "x": 692,
          "y": 2793
        },
        {
          "x": 692,
          "y": 2844
        },
        {
          "x": 202,
          "y": 2844
        }
      ],
      "category": "paragraph",
      "html": "<p id='27' style='font-size:22px'>2.1. Training Techniques</p>",
      "id": 27,
      "page": 3,
      "text": "2.1. Training Techniques"
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 2877
        },
        {
          "x": 1199,
          "y": 2877
        },
        {
          "x": 1199,
          "y": 2975
        },
        {
          "x": 201,
          "y": 2975
        }
      ],
      "category": "paragraph",
      "html": "<p id='28' style='font-size:18px'>Apart from the design of the network architecture, the<br>training procedure also affects the ultimate performance. Not</p>",
      "id": 28,
      "page": 3,
      "text": "Apart from the design of the network architecture, the\ntraining procedure also affects the ultimate performance. Not"
    },
    {
      "bounding_box": [
        {
          "x": 1276,
          "y": 308
        },
        {
          "x": 2280,
          "y": 308
        },
        {
          "x": 2280,
          "y": 1653
        },
        {
          "x": 1276,
          "y": 1653
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='29' style='font-size:18px'>only did vision Transformers bring a new set of modules<br>and architectural design decisions, but they also introduced<br>different training techniques (e.g. AdamW optimizer) to vi-<br>sion. This pertains mostly to the optimization strategy and<br>associated hyper-parameter settings. Thus, the first step<br>of our exploration is to train a baseline model with the vi-<br>sion Transformer training procedure, in this case, ResNet-<br>50/200. Recent studies [7, 81] demonstrate that a set of<br>modern training techniques can significantly enhance the<br>performance of a simple ResNet-50 model. In our study,<br>we use a training recipe that is close to DeiT`s [73] and<br>Swin Transformer's [45]. The training is extended to 300<br>epochs from the original 90 epochs for ResNets. We use the<br>AdamW optimizer [46], data augmentation techniques such<br>as Mixup [90], Cutmix [89], RandAugment [14], Random<br>Erasing [91], and regularization schemes including Stochas-<br>tic Depth [36] and Label Smoothing [69]. The complete set<br>of hyper-parameters we use can be found in Appendix A.1.<br>By itself, this enhanced training recipe increased the perfor-<br>mance of the ResNet-50 model from 76.1% [1] to 78.8%<br>(+2.7%), implying that a significant portion of the perfor-<br>mance difference between traditional ConvNets and vision<br>Transformers may be due to the training techniques. We will<br>use this fixed training recipe with the same hyperparameters<br>throughout the \"modernization\" process. Each reported ac-<br>curacy on the ResNet-50 regime is an average obtained from<br>training with three different random seeds.</p>",
      "id": 29,
      "page": 3,
      "text": "only did vision Transformers bring a new set of modules\nand architectural design decisions, but they also introduced\ndifferent training techniques (e.g. AdamW optimizer) to vi-\nsion. This pertains mostly to the optimization strategy and\nassociated hyper-parameter settings. Thus, the first step\nof our exploration is to train a baseline model with the vi-\nsion Transformer training procedure, in this case, ResNet-\n50/200. Recent studies [7, 81] demonstrate that a set of\nmodern training techniques can significantly enhance the\nperformance of a simple ResNet-50 model. In our study,\nwe use a training recipe that is close to DeiT`s [73] and\nSwin Transformer's [45]. The training is extended to 300\nepochs from the original 90 epochs for ResNets. We use the\nAdamW optimizer [46], data augmentation techniques such\nas Mixup [90], Cutmix [89], RandAugment [14], Random\nErasing [91], and regularization schemes including Stochas-\ntic Depth [36] and Label Smoothing [69]. The complete set\nof hyper-parameters we use can be found in Appendix A.1.\nBy itself, this enhanced training recipe increased the perfor-\nmance of the ResNet-50 model from 76.1% [1] to 78.8%\n(+2.7%), implying that a significant portion of the perfor-\nmance difference between traditional ConvNets and vision\nTransformers may be due to the training techniques. We will\nuse this fixed training recipe with the same hyperparameters\nthroughout the \"modernization\" process. Each reported ac-\ncuracy on the ResNet-50 regime is an average obtained from\ntraining with three different random seeds."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 1687
        },
        {
          "x": 1643,
          "y": 1687
        },
        {
          "x": 1643,
          "y": 1735
        },
        {
          "x": 1280,
          "y": 1735
        }
      ],
      "category": "paragraph",
      "html": "<p id='30' style='font-size:20px'>2.2. Macro Design</p>",
      "id": 30,
      "page": 3,
      "text": "2.2. Macro Design"
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 1767
        },
        {
          "x": 2279,
          "y": 1767
        },
        {
          "x": 2279,
          "y": 2013
        },
        {
          "x": 1280,
          "y": 2013
        }
      ],
      "category": "paragraph",
      "html": "<p id='31' style='font-size:16px'>We now analyze Swin Transformers' macro network de-<br>sign. Swin Transformers follow ConvNets [28, 65] to use a<br>multi-stage design, where each stage has a different feature<br>map resolution. There are two interesting design considera-<br>tions: the stage compute ratio, and the \"stem cell\" structure.</p>",
      "id": 31,
      "page": 3,
      "text": "We now analyze Swin Transformers' macro network de-\nsign. Swin Transformers follow ConvNets [28, 65] to use a\nmulti-stage design, where each stage has a different feature\nmap resolution. There are two interesting design considera-\ntions: the stage compute ratio, and the \"stem cell\" structure."
    },
    {
      "bounding_box": [
        {
          "x": 1276,
          "y": 2045
        },
        {
          "x": 2278,
          "y": 2045
        },
        {
          "x": 2278,
          "y": 2741
        },
        {
          "x": 1276,
          "y": 2741
        }
      ],
      "category": "paragraph",
      "html": "<p id='32' style='font-size:18px'>Changing stage compute ratio. The original design of the<br>computation distribution across stages in ResNet was largely<br>empirical. The heavy \"res4\" stage was meant to be compat-<br>ible with downstream tasks like object detection, where a<br>detector head operates on the 14x 14 feature plane. Swin-T,<br>on the other hand, followed the same principle but with a<br>slightly different stage compute ratio of 1:1:3:1. For larger<br>Swin Transformers, the ratio is 1:1:9:1. Following the de-<br>sign, we adjust the number of blocks in each stage from<br>(3, 4, 6, 3) in ResNet-50 to (3, 3, 9, 3), which also aligns<br>the FLOPs with Swin-T. This improves the model accuracy<br>from 78.8% to 79.4%. Notably, researchers have thoroughly<br>investigated the distribution of computation [53, 54], and a<br>more optimal design is likely to exist.</p>",
      "id": 32,
      "page": 3,
      "text": "Changing stage compute ratio. The original design of the\ncomputation distribution across stages in ResNet was largely\nempirical. The heavy \"res4\" stage was meant to be compat-\nible with downstream tasks like object detection, where a\ndetector head operates on the 14x 14 feature plane. Swin-T,\non the other hand, followed the same principle but with a\nslightly different stage compute ratio of 1:1:3:1. For larger\nSwin Transformers, the ratio is 1:1:9:1. Following the de-\nsign, we adjust the number of blocks in each stage from\n(3, 4, 6, 3) in ResNet-50 to (3, 3, 9, 3), which also aligns\nthe FLOPs with Swin-T. This improves the model accuracy\nfrom 78.8% to 79.4%. Notably, researchers have thoroughly\ninvestigated the distribution of computation [53, 54], and a\nmore optimal design is likely to exist."
    },
    {
      "bounding_box": [
        {
          "x": 1331,
          "y": 2752
        },
        {
          "x": 2179,
          "y": 2752
        },
        {
          "x": 2179,
          "y": 2793
        },
        {
          "x": 1331,
          "y": 2793
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='33' style='font-size:14px'>From now on, we will use this stage compute ratio.</p>",
      "id": 33,
      "page": 3,
      "text": "From now on, we will use this stage compute ratio."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 2827
        },
        {
          "x": 2278,
          "y": 2827
        },
        {
          "x": 2278,
          "y": 2976
        },
        {
          "x": 1280,
          "y": 2976
        }
      ],
      "category": "paragraph",
      "html": "<p id='34' style='font-size:18px'>Changing stem to \"Patchify\". Typically, the stem cell de-<br>sign is concerned with how the input images will be pro-<br>cessed at the network's beginning. Due to the redundancy</p>",
      "id": 34,
      "page": 3,
      "text": "Changing stem to \"Patchify\". Typically, the stem cell de-\nsign is concerned with how the input images will be pro-\ncessed at the network's beginning. Due to the redundancy"
    },
    {
      "bounding_box": [
        {
          "x": 199,
          "y": 308
        },
        {
          "x": 1203,
          "y": 308
        },
        {
          "x": 1203,
          "y": 1151
        },
        {
          "x": 199,
          "y": 1151
        }
      ],
      "category": "paragraph",
      "html": "<p id='35' style='font-size:16px'>inherent in natural images, a common stem cell will aggres-<br>sively downsample the input images to an appropriate feature<br>map size in both standard ConvNets and vision Transformers.<br>The stem cell in standard ResNet contains a 7 x7 convolution<br>layer with stride 2, followed by a max pool, which results<br>in a 4x downsampling of the input images. In vision Trans-<br>formers, a more aggressive \"patchify\" strategy is used as<br>the stem cell, which corresponds to a large kernel size (e.g.<br>kernel size = 14 or 16) and non-overlapping convolution.<br>Swin Transformer uses a similar \"patchify\" layer, but with<br>a smaller patch size of 4 to accommodate the architecture's<br>multi-stage design. We replace the ResNet-style stem cell<br>with a patchify layer implemented using a 4x4, stride 4 con-<br>volutional layer. The accuracy has changed from 79.4% to<br>79.5%. This suggests that the stem cell in a ResNet may be<br>substituted with a simpler \"patchify\" layer a la ViT which<br>will result in similar performance.</p>",
      "id": 35,
      "page": 4,
      "text": "inherent in natural images, a common stem cell will aggres-\nsively downsample the input images to an appropriate feature\nmap size in both standard ConvNets and vision Transformers.\nThe stem cell in standard ResNet contains a 7 x7 convolution\nlayer with stride 2, followed by a max pool, which results\nin a 4x downsampling of the input images. In vision Trans-\nformers, a more aggressive \"patchify\" strategy is used as\nthe stem cell, which corresponds to a large kernel size (e.g.\nkernel size = 14 or 16) and non-overlapping convolution.\nSwin Transformer uses a similar \"patchify\" layer, but with\na smaller patch size of 4 to accommodate the architecture's\nmulti-stage design. We replace the ResNet-style stem cell\nwith a patchify layer implemented using a 4x4, stride 4 con-\nvolutional layer. The accuracy has changed from 79.4% to\n79.5%. This suggests that the stem cell in a ResNet may be\nsubstituted with a simpler \"patchify\" layer a la ViT which\nwill result in similar performance."
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 1156
        },
        {
          "x": 1200,
          "y": 1156
        },
        {
          "x": 1200,
          "y": 1252
        },
        {
          "x": 200,
          "y": 1252
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='36' style='font-size:16px'>We will use the \"patchify stem (4x4 non-overlapping<br>convolution) in the network.</p>",
      "id": 36,
      "page": 4,
      "text": "We will use the \"patchify stem (4x4 non-overlapping\nconvolution) in the network."
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 1293
        },
        {
          "x": 528,
          "y": 1293
        },
        {
          "x": 528,
          "y": 1344
        },
        {
          "x": 203,
          "y": 1344
        }
      ],
      "category": "paragraph",
      "html": "<p id='37' style='font-size:22px'>2.3. ResNeXt-ify</p>",
      "id": 37,
      "page": 4,
      "text": "2.3. ResNeXt-ify"
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 1364
        },
        {
          "x": 1203,
          "y": 1364
        },
        {
          "x": 1203,
          "y": 1811
        },
        {
          "x": 201,
          "y": 1811
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='38' style='font-size:16px'>In this part, we attempt to adopt the idea of ResNeXt [87],<br>which has a better FLOPs/accuracy trade-off than a vanilla<br>ResNet. The core component is grouped convolution, where<br>the convolutional filters are separated into different groups.<br>At a high level, ResNeXt's guiding principle is to \"use more<br>groups, expand width\" More precisely, ResNeXt employs<br>grouped convolution for the 3x3 conv layer in a bottleneck<br>block. As this significantly reduces the FLOPs, the network<br>width is expanded to compensate for the capacity loss.</p>",
      "id": 38,
      "page": 4,
      "text": "In this part, we attempt to adopt the idea of ResNeXt [87],\nwhich has a better FLOPs/accuracy trade-off than a vanilla\nResNet. The core component is grouped convolution, where\nthe convolutional filters are separated into different groups.\nAt a high level, ResNeXt's guiding principle is to \"use more\ngroups, expand width\" More precisely, ResNeXt employs\ngrouped convolution for the 3x3 conv layer in a bottleneck\nblock. As this significantly reduces the FLOPs, the network\nwidth is expanded to compensate for the capacity loss."
    },
    {
      "bounding_box": [
        {
          "x": 199,
          "y": 1819
        },
        {
          "x": 1202,
          "y": 1819
        },
        {
          "x": 1202,
          "y": 2664
        },
        {
          "x": 199,
          "y": 2664
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='39' style='font-size:16px'>In our case we use depthwise convolution, a special case<br>of grouped convolution where the number of groups equals<br>the number of channels. Depthwise conv has been popular-<br>ized by MobileNet [34] and Xception [11]. We note that<br>depthwise convolution is similar to the weighted sum op-<br>eration in self-attention, which operates on a per-channel<br>basis, i.e., only mixing information in the spatial dimension.<br>The combination of depthwise conv and 1 x 1 convs leads<br>to a separation of spatial and channel mixing, a property<br>shared by vision Transformers, where each operation either<br>mixes information across spatial or channel dimension, but<br>not both. The use of depthwise convolution effectively re-<br>duces the network FLOPs and, as expected, the accuracy.<br>Following the strategy proposed in ResNeXt, we increase the<br>network width to the same number of channels as Swin-T's<br>(from 64 to 96). This brings the network performance to<br>80.5% with increased FLOPs (5.3G).</p>",
      "id": 39,
      "page": 4,
      "text": "In our case we use depthwise convolution, a special case\nof grouped convolution where the number of groups equals\nthe number of channels. Depthwise conv has been popular-\nized by MobileNet [34] and Xception [11]. We note that\ndepthwise convolution is similar to the weighted sum op-\neration in self-attention, which operates on a per-channel\nbasis, i.e., only mixing information in the spatial dimension.\nThe combination of depthwise conv and 1 x 1 convs leads\nto a separation of spatial and channel mixing, a property\nshared by vision Transformers, where each operation either\nmixes information across spatial or channel dimension, but\nnot both. The use of depthwise convolution effectively re-\nduces the network FLOPs and, as expected, the accuracy.\nFollowing the strategy proposed in ResNeXt, we increase the\nnetwork width to the same number of channels as Swin-T's\n(from 64 to 96). This brings the network performance to\n80.5% with increased FLOPs (5.3G)."
    },
    {
      "bounding_box": [
        {
          "x": 255,
          "y": 2666
        },
        {
          "x": 934,
          "y": 2666
        },
        {
          "x": 934,
          "y": 2713
        },
        {
          "x": 255,
          "y": 2713
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='40' style='font-size:16px'>We will now employ the ResNeXt design.</p>",
      "id": 40,
      "page": 4,
      "text": "We will now employ the ResNeXt design."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 2754
        },
        {
          "x": 676,
          "y": 2754
        },
        {
          "x": 676,
          "y": 2804
        },
        {
          "x": 201,
          "y": 2804
        }
      ],
      "category": "paragraph",
      "html": "<p id='41' style='font-size:20px'>2.4. Inverted Bottleneck</p>",
      "id": 41,
      "page": 4,
      "text": "2.4. Inverted Bottleneck"
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 2827
        },
        {
          "x": 1201,
          "y": 2827
        },
        {
          "x": 1201,
          "y": 2977
        },
        {
          "x": 201,
          "y": 2977
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='42' style='font-size:16px'>One important design in every Transformer block is that it<br>creates an inverted bottleneck, i.e., the hidden dimension of<br>the MLP block is four times wider than the input dimension</p>",
      "id": 42,
      "page": 4,
      "text": "One important design in every Transformer block is that it\ncreates an inverted bottleneck, i.e., the hidden dimension of\nthe MLP block is four times wider than the input dimension"
    },
    {
      "bounding_box": [
        {
          "x": 1290,
          "y": 303
        },
        {
          "x": 2275,
          "y": 303
        },
        {
          "x": 2275,
          "y": 635
        },
        {
          "x": 1290,
          "y": 635
        }
      ],
      "category": "figure",
      "html": "<br><figure><img id='43' style='font-size:14px' alt=\"1x1, 384→96 1x1, 96→384 d3x3, 96→96\nd3x3, 96→96 d3x3, 384→384 1x1, 96→384\n1x1, 96→384 1x1, 384→96 1x1, 384→96\n(a) (b) (c)\" data-coord=\"top-left:(1290,303); bottom-right:(2275,635)\" /></figure>",
      "id": 43,
      "page": 4,
      "text": "1x1, 384→96 1x1, 96→384 d3x3, 96→96\nd3x3, 96→96 d3x3, 384→384 1x1, 96→384\n1x1, 96→384 1x1, 384→96 1x1, 384→96\n(a) (b) (c)"
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 660
        },
        {
          "x": 2279,
          "y": 660
        },
        {
          "x": 2279,
          "y": 800
        },
        {
          "x": 1278,
          "y": 800
        }
      ],
      "category": "caption",
      "html": "<caption id='44' style='font-size:14px'>Figure 3. Block modifications and resulted specifications. (a) is<br>a ResNeXt block; in (b) we create an inverted bottleneck block and<br>in (c) the position of the spatial depthwise conv layer is moved up.</caption>",
      "id": 44,
      "page": 4,
      "text": "Figure 3. Block modifications and resulted specifications. (a) is\na ResNeXt block; in (b) we create an inverted bottleneck block and\nin (c) the position of the spatial depthwise conv layer is moved up."
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 844
        },
        {
          "x": 2278,
          "y": 844
        },
        {
          "x": 2278,
          "y": 1090
        },
        {
          "x": 1279,
          "y": 1090
        }
      ],
      "category": "paragraph",
      "html": "<p id='45' style='font-size:16px'>(see Figure 4). Interestingly, this Transformer design is con-<br>nected to the inverted bottleneck design with an expansion<br>ratio of 4 used in ConvNets. The idea was popularized by<br>MobileNetV2 [61], and has subsequently gained traction in<br>several advanced ConvNet architectures [70, 71].</p>",
      "id": 45,
      "page": 4,
      "text": "(see Figure 4). Interestingly, this Transformer design is con-\nnected to the inverted bottleneck design with an expansion\nratio of 4 used in ConvNets. The idea was popularized by\nMobileNetV2 [61], and has subsequently gained traction in\nseveral advanced ConvNet architectures [70, 71]."
    },
    {
      "bounding_box": [
        {
          "x": 1277,
          "y": 1094
        },
        {
          "x": 2280,
          "y": 1094
        },
        {
          "x": 2280,
          "y": 1542
        },
        {
          "x": 1277,
          "y": 1542
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='46' style='font-size:16px'>Here we explore the inverted bottleneck design. Figure 3<br>(a) to (b) illustrate the configurations. Despite the increased<br>FLOPs for the depthwise convolution layer, this change<br>reduces the whole network FLOPs to 4.6G, due to the signif-<br>icant FLOPs reduction in the downsampling residual blocks'<br>shortcut 1 x1 conv layer. Interestingly, this results in slightly<br>improved performance (80.5% to 80.6%). In the ResNet-200<br>/ Swin-B regime, this step brings even more gain (81.9% to<br>82.6%) also with reduced FLOPs.</p>",
      "id": 46,
      "page": 4,
      "text": "Here we explore the inverted bottleneck design. Figure 3\n(a) to (b) illustrate the configurations. Despite the increased\nFLOPs for the depthwise convolution layer, this change\nreduces the whole network FLOPs to 4.6G, due to the signif-\nicant FLOPs reduction in the downsampling residual blocks'\nshortcut 1 x1 conv layer. Interestingly, this results in slightly\nimproved performance (80.5% to 80.6%). In the ResNet-200\n/ Swin-B regime, this step brings even more gain (81.9% to\n82.6%) also with reduced FLOPs."
    },
    {
      "bounding_box": [
        {
          "x": 1332,
          "y": 1542
        },
        {
          "x": 1958,
          "y": 1542
        },
        {
          "x": 1958,
          "y": 1588
        },
        {
          "x": 1332,
          "y": 1588
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='47' style='font-size:14px'>We will now use inverted bottlenecks.</p>",
      "id": 47,
      "page": 4,
      "text": "We will now use inverted bottlenecks."
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 1621
        },
        {
          "x": 1738,
          "y": 1621
        },
        {
          "x": 1738,
          "y": 1672
        },
        {
          "x": 1281,
          "y": 1672
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='48' style='font-size:20px'>2.5. Large Kernel Sizes</p>",
      "id": 48,
      "page": 4,
      "text": "2.5. Large Kernel Sizes"
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 1689
        },
        {
          "x": 2281,
          "y": 1689
        },
        {
          "x": 2281,
          "y": 2338
        },
        {
          "x": 1278,
          "y": 2338
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='49' style='font-size:16px'>In this part of the exploration, we focus on the behav-<br>ior of large convolutional kernels. One of the most distin-<br>guishing aspects of vision Transformers is their non-local<br>self-attention, which enables each layer to have a global<br>receptive field. While large kernel sizes have been used in<br>the past with ConvNets [40, 68], the gold standard (popular-<br>ized by VGGNet [65]) is to stack small kernel-sized (3x3)<br>conv layers, which have efficient hardware implementations<br>on modern GPUs [41]. Although Swin Transformers rein-<br>troduced the local window to the self-attention block, the<br>window size is at least 7x7, significantly larger than the<br>ResNe(X)t kernel size of 3x3. Here we revisit the use of<br>large kernel-sized convolutions for ConvNets.</p>",
      "id": 49,
      "page": 4,
      "text": "In this part of the exploration, we focus on the behav-\nior of large convolutional kernels. One of the most distin-\nguishing aspects of vision Transformers is their non-local\nself-attention, which enables each layer to have a global\nreceptive field. While large kernel sizes have been used in\nthe past with ConvNets [40, 68], the gold standard (popular-\nized by VGGNet [65]) is to stack small kernel-sized (3x3)\nconv layers, which have efficient hardware implementations\non modern GPUs [41]. Although Swin Transformers rein-\ntroduced the local window to the self-attention block, the\nwindow size is at least 7x7, significantly larger than the\nResNe(X)t kernel size of 3x3. Here we revisit the use of\nlarge kernel-sized convolutions for ConvNets."
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 2356
        },
        {
          "x": 2281,
          "y": 2356
        },
        {
          "x": 2281,
          "y": 2856
        },
        {
          "x": 1278,
          "y": 2856
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='50' style='font-size:16px'>Moving up depthwise conv layer. To explore large kernels,<br>one prerequisite is to move up the position of the depthwise<br>conv layer (Figure 3 (b) to (c)). That is a design decision<br>also evident in Transformers: the MSA block is placed prior<br>to the MLP layers. As we have an inverted bottleneck block,<br>this is a natural design choice the complex/inefficient<br>modules (MSA, large-kernel conv) will have fewer channels,<br>while the efficient, dense 1 x 1 layers will do the heavy lifting.<br>This intermediate step reduces the FLOPs to 4.1G, resulting<br>in a temporary performance degradation to 79.9%.</p>",
      "id": 50,
      "page": 4,
      "text": "Moving up depthwise conv layer. To explore large kernels,\none prerequisite is to move up the position of the depthwise\nconv layer (Figure 3 (b) to (c)). That is a design decision\nalso evident in Transformers: the MSA block is placed prior\nto the MLP layers. As we have an inverted bottleneck block,\nthis is a natural design choice the complex/inefficient\nmodules (MSA, large-kernel conv) will have fewer channels,\nwhile the efficient, dense 1 x 1 layers will do the heavy lifting.\nThis intermediate step reduces the FLOPs to 4.1G, resulting\nin a temporary performance degradation to 79.9%."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 2876
        },
        {
          "x": 2280,
          "y": 2876
        },
        {
          "x": 2280,
          "y": 2979
        },
        {
          "x": 1280,
          "y": 2979
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='51' style='font-size:18px'>Increasing the kernel size. With all of these preparations,<br>the benefit of adopting larger kernel-sized convolutions is sig-</p>",
      "id": 51,
      "page": 4,
      "text": "Increasing the kernel size. With all of these preparations,\nthe benefit of adopting larger kernel-sized convolutions is sig-"
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 308
        },
        {
          "x": 1202,
          "y": 308
        },
        {
          "x": 1202,
          "y": 702
        },
        {
          "x": 201,
          "y": 702
        }
      ],
      "category": "paragraph",
      "html": "<p id='52' style='font-size:18px'>nificant. We experimented with several kernel sizes, includ-<br>ing 3, 5, 7, 9, and 11. The network's performance increases<br>from 79.9% (3x3) to 80.6% (7x7), while the network's<br>FLOPs stay roughly the same. Additionally, we observe that<br>the benefit of larger kernel sizes reaches a saturation point at<br>7x7. We verified this behavior in the large capacity model<br>too: a ResNet-200 regime model does not exhibit further<br>gain when we increase the kernel size beyond 7x7.</p>",
      "id": 52,
      "page": 5,
      "text": "nificant. We experimented with several kernel sizes, includ-\ning 3, 5, 7, 9, and 11. The network's performance increases\nfrom 79.9% (3x3) to 80.6% (7x7), while the network's\nFLOPs stay roughly the same. Additionally, we observe that\nthe benefit of larger kernel sizes reaches a saturation point at\n7x7. We verified this behavior in the large capacity model\ntoo: a ResNet-200 regime model does not exhibit further\ngain when we increase the kernel size beyond 7x7."
    },
    {
      "bounding_box": [
        {
          "x": 256,
          "y": 707
        },
        {
          "x": 1035,
          "y": 707
        },
        {
          "x": 1035,
          "y": 749
        },
        {
          "x": 256,
          "y": 749
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='53' style='font-size:16px'>We will use 7x7 depthwise conv in each block.</p>",
      "id": 53,
      "page": 5,
      "text": "We will use 7x7 depthwise conv in each block."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 757
        },
        {
          "x": 1203,
          "y": 757
        },
        {
          "x": 1203,
          "y": 953
        },
        {
          "x": 201,
          "y": 953
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='54' style='font-size:16px'>At this point, we have concluded our examination of<br>network architectures on a macro scale. Intriguingly, a sig-<br>nificant portion of the design choices taken in a vision Trans-<br>former may be mapped to ConvNet instantiations.</p>",
      "id": 54,
      "page": 5,
      "text": "At this point, we have concluded our examination of\nnetwork architectures on a macro scale. Intriguingly, a sig-\nnificant portion of the design choices taken in a vision Trans-\nformer may be mapped to ConvNet instantiations."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 986
        },
        {
          "x": 551,
          "y": 986
        },
        {
          "x": 551,
          "y": 1036
        },
        {
          "x": 202,
          "y": 1036
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='55' style='font-size:22px'>2.6. Micro Design</p>",
      "id": 55,
      "page": 5,
      "text": "2.6. Micro Design"
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 1064
        },
        {
          "x": 1202,
          "y": 1064
        },
        {
          "x": 1202,
          "y": 1261
        },
        {
          "x": 201,
          "y": 1261
        }
      ],
      "category": "paragraph",
      "html": "<p id='56' style='font-size:18px'>In this section, we investigate several other architectural<br>differences at a micro scale most of the explorations here<br>are done at the layer level, focusing on specific choices of<br>activation functions and normalization layers.</p>",
      "id": 56,
      "page": 5,
      "text": "In this section, we investigate several other architectural\ndifferences at a micro scale most of the explorations here\nare done at the layer level, focusing on specific choices of\nactivation functions and normalization layers."
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 1284
        },
        {
          "x": 1202,
          "y": 1284
        },
        {
          "x": 1202,
          "y": 1933
        },
        {
          "x": 200,
          "y": 1933
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='57' style='font-size:18px'>Replacing ReLU with GELU One discrepancy between<br>NLP and vision architectures is the specifics of which ac-<br>tivation functions to use. Numerous activation functions<br>have been developed over time, but the Rectified Linear Unit<br>(ReLU) [49] is still extensively used in ConvNets due to its<br>simplicity and efficiency. ReLU is also used as an activation<br>function in the original Transformer paper [77]. The Gaus-<br>sian Error Linear Unit, or GELU [32], which can be thought<br>of as a smoother variant of ReLU, is utilized in the most<br>advanced Transformers, including Google's BERT [18] and<br>OpenAI's GPT-2 [52], and, most recently, ViTs. We find<br>that ReLU can be substituted with GELU in our ConvNet<br>too, although the accuracy stays unchanged (80.6%).</p>",
      "id": 57,
      "page": 5,
      "text": "Replacing ReLU with GELU One discrepancy between\nNLP and vision architectures is the specifics of which ac-\ntivation functions to use. Numerous activation functions\nhave been developed over time, but the Rectified Linear Unit\n(ReLU) [49] is still extensively used in ConvNets due to its\nsimplicity and efficiency. ReLU is also used as an activation\nfunction in the original Transformer paper [77]. The Gaus-\nsian Error Linear Unit, or GELU [32], which can be thought\nof as a smoother variant of ReLU, is utilized in the most\nadvanced Transformers, including Google's BERT [18] and\nOpenAI's GPT-2 [52], and, most recently, ViTs. We find\nthat ReLU can be substituted with GELU in our ConvNet\ntoo, although the accuracy stays unchanged (80.6%)."
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 1956
        },
        {
          "x": 1201,
          "y": 1956
        },
        {
          "x": 1201,
          "y": 2651
        },
        {
          "x": 200,
          "y": 2651
        }
      ],
      "category": "paragraph",
      "html": "<p id='58' style='font-size:18px'>Fewer activation functions. One minor distinction be-<br>tween a Transformer and a ResNet block is that Transform-<br>ers have fewer activation functions. Consider a Transformer<br>block with key/query/value linear embedding layers, the pro-<br>jection layer, and two linear layers in an MLP block. There<br>is only one activation function present in the MLP block. In<br>comparison, it is common practice to append an activation<br>function to each convolutional layer, including the 1 x 1<br>convs. Here we examine how performance changes when<br>we stick to the same strategy. As depicted in Figure 4, we<br>eliminate all GELU layers from the residual block except<br>for one between two 1 x 1 layers, replicating the style of a<br>Transformer block. This process improves the result by 0.7%<br>to 81.3%, practically matching the performance of Swin-T.</p>",
      "id": 58,
      "page": 5,
      "text": "Fewer activation functions. One minor distinction be-\ntween a Transformer and a ResNet block is that Transform-\ners have fewer activation functions. Consider a Transformer\nblock with key/query/value linear embedding layers, the pro-\njection layer, and two linear layers in an MLP block. There\nis only one activation function present in the MLP block. In\ncomparison, it is common practice to append an activation\nfunction to each convolutional layer, including the 1 x 1\nconvs. Here we examine how performance changes when\nwe stick to the same strategy. As depicted in Figure 4, we\neliminate all GELU layers from the residual block except\nfor one between two 1 x 1 layers, replicating the style of a\nTransformer block. This process improves the result by 0.7%\nto 81.3%, practically matching the performance of Swin-T."
    },
    {
      "bounding_box": [
        {
          "x": 258,
          "y": 2656
        },
        {
          "x": 1200,
          "y": 2656
        },
        {
          "x": 1200,
          "y": 2700
        },
        {
          "x": 258,
          "y": 2700
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='59' style='font-size:16px'>We will now use a single GELU activation in each block.</p>",
      "id": 59,
      "page": 5,
      "text": "We will now use a single GELU activation in each block."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 2728
        },
        {
          "x": 1201,
          "y": 2728
        },
        {
          "x": 1201,
          "y": 2977
        },
        {
          "x": 201,
          "y": 2977
        }
      ],
      "category": "paragraph",
      "html": "<p id='60' style='font-size:18px'>Fewer normalization layers. Transformer blocks usually<br>have fewer normalization layers as well. Here we remove<br>two BatchNorm (BN) layers, leaving only one BN layer<br>before the conv 1 x 1 layers. This further boosts the perfor-<br>mance to 81.4%, already surpassing Swin-T's result. Note</p>",
      "id": 60,
      "page": 5,
      "text": "Fewer normalization layers. Transformer blocks usually\nhave fewer normalization layers as well. Here we remove\ntwo BatchNorm (BN) layers, leaving only one BN layer\nbefore the conv 1 x 1 layers. This further boosts the perfor-\nmance to 81.4%, already surpassing Swin-T's result. Note"
    },
    {
      "bounding_box": [
        {
          "x": 1293,
          "y": 310
        },
        {
          "x": 1739,
          "y": 310
        },
        {
          "x": 1739,
          "y": 353
        },
        {
          "x": 1293,
          "y": 353
        }
      ],
      "category": "caption",
      "html": "<br><caption id='61' style='font-size:18px'>Swin Transformer Block</caption>",
      "id": 61,
      "page": 5,
      "text": "Swin Transformer Block"
    },
    {
      "bounding_box": [
        {
          "x": 1293,
          "y": 345
        },
        {
          "x": 2288,
          "y": 345
        },
        {
          "x": 2288,
          "y": 1374
        },
        {
          "x": 1293,
          "y": 1374
        }
      ],
      "category": "figure",
      "html": "<br><figure><img id='62' style='font-size:14px' alt=\"96-d\nLN\n1x1, 96x3\n+ rel. pos. win. shift ResNet Block ConvNeXt Block\nMSA, w7x7, H=3\n256-d 96-d\n1x1, 64 d7x7, 96\n1x1, 96\nBN, ReLU LN\n3x3, 64 1x1, 384\n96-d\nBN, ReLU GELU\nLN\n1x1, 256 1x1, 96\n1x1, 384\nBN\nGELU\nReLU\n1x1, 96\" data-coord=\"top-left:(1293,345); bottom-right:(2288,1374)\" /></figure>",
      "id": 62,
      "page": 5,
      "text": "96-d\nLN\n1x1, 96x3\n+ rel. pos. win. shift ResNet Block ConvNeXt Block\nMSA, w7x7, H=3\n256-d 96-d\n1x1, 64 d7x7, 96\n1x1, 96\nBN, ReLU LN\n3x3, 64 1x1, 384\n96-d\nBN, ReLU GELU\nLN\n1x1, 256 1x1, 96\n1x1, 384\nBN\nGELU\nReLU\n1x1, 96"
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 1383
        },
        {
          "x": 2281,
          "y": 1383
        },
        {
          "x": 2281,
          "y": 1613
        },
        {
          "x": 1278,
          "y": 1613
        }
      ],
      "category": "caption",
      "html": "<br><caption id='63' style='font-size:14px'>Figure 4. Block designs for a ResNet, a Swin Transformer, and a<br>ConvNeXt. Swin Transformer's block is more sophisticated due to<br>the presence of multiple specialized modules and two residual con-<br>nections. For simplicity, we note the linear layers in Transformer<br>MLP blocks also as \"1 x1 convs\" since they are equivalent.</caption>",
      "id": 63,
      "page": 5,
      "text": "Figure 4. Block designs for a ResNet, a Swin Transformer, and a\nConvNeXt. Swin Transformer's block is more sophisticated due to\nthe presence of multiple specialized modules and two residual con-\nnections. For simplicity, we note the linear layers in Transformer\nMLP blocks also as \"1 x1 convs\" since they are equivalent."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 1665
        },
        {
          "x": 2278,
          "y": 1665
        },
        {
          "x": 2278,
          "y": 1862
        },
        {
          "x": 1280,
          "y": 1862
        }
      ],
      "category": "paragraph",
      "html": "<p id='64' style='font-size:18px'>that we have even fewer normalization layers per block than<br>Transformers, as empirically we find that adding one ad-<br>ditional BN layer at the beginning of the block does not<br>improve the performance.</p>",
      "id": 64,
      "page": 5,
      "text": "that we have even fewer normalization layers per block than\nTransformers, as empirically we find that adding one ad-\nditional BN layer at the beginning of the block does not\nimprove the performance."
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 1894
        },
        {
          "x": 2280,
          "y": 1894
        },
        {
          "x": 2280,
          "y": 2392
        },
        {
          "x": 1278,
          "y": 2392
        }
      ],
      "category": "paragraph",
      "html": "<p id='65' style='font-size:20px'>Substituting BN with LN. BatchNorm [38] is an essen-<br>tial component in ConvNets as it improves the convergence<br>and reduces overfitting. However, BN also has many in-<br>tricacies that can have a detrimental effect on the model's<br>performance [84]. There have been numerous attempts at<br>developing alternative normalization [60, 75, 83] techniques,<br>but BN has remained the preferred option in most vision<br>tasks. On the other hand, the simpler Layer Normaliza-<br>tion [5] (LN) has been used in Transformers, resulting in<br>good performance across different application scenarios.</p>",
      "id": 65,
      "page": 5,
      "text": "Substituting BN with LN. BatchNorm [38] is an essen-\ntial component in ConvNets as it improves the convergence\nand reduces overfitting. However, BN also has many in-\ntricacies that can have a detrimental effect on the model's\nperformance [84]. There have been numerous attempts at\ndeveloping alternative normalization [60, 75, 83] techniques,\nbut BN has remained the preferred option in most vision\ntasks. On the other hand, the simpler Layer Normaliza-\ntion [5] (LN) has been used in Transformers, resulting in\ngood performance across different application scenarios."
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 2396
        },
        {
          "x": 2280,
          "y": 2396
        },
        {
          "x": 2280,
          "y": 2740
        },
        {
          "x": 1279,
          "y": 2740
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='66' style='font-size:20px'>Directly substituting LN for BN in the original ResNet<br>will result in suboptimal performance [83]. With all the mod-<br>ifications in network architecture and training techniques,<br>here we revisit the impact of using LN in place of BN. We<br>observe that our ConvNet model does not have any difficul-<br>ties training with LN; in fact, the performance is slightly<br>better, obtaining an accuracy of 81.5%.</p>",
      "id": 66,
      "page": 5,
      "text": "Directly substituting LN for BN in the original ResNet\nwill result in suboptimal performance [83]. With all the mod-\nifications in network architecture and training techniques,\nhere we revisit the impact of using LN in place of BN. We\nobserve that our ConvNet model does not have any difficul-\nties training with LN; in fact, the performance is slightly\nbetter, obtaining an accuracy of 81.5%."
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 2748
        },
        {
          "x": 2277,
          "y": 2748
        },
        {
          "x": 2277,
          "y": 2841
        },
        {
          "x": 1281,
          "y": 2841
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='67' style='font-size:14px'>From now on, we will use one LayerNorm as our choice<br>of normalization in each residual block.</p>",
      "id": 67,
      "page": 5,
      "text": "From now on, we will use one LayerNorm as our choice\nof normalization in each residual block."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 2877
        },
        {
          "x": 2277,
          "y": 2877
        },
        {
          "x": 2277,
          "y": 2977
        },
        {
          "x": 1280,
          "y": 2977
        }
      ],
      "category": "paragraph",
      "html": "<p id='68' style='font-size:20px'>Separate downsampling layers. In ResNet, the spatial<br>downsampling is achieved by the residual block at the start of</p>",
      "id": 68,
      "page": 5,
      "text": "Separate downsampling layers. In ResNet, the spatial\ndownsampling is achieved by the residual block at the start of"
    },
    {
      "bounding_box": [
        {
          "x": 199,
          "y": 307
        },
        {
          "x": 1201,
          "y": 307
        },
        {
          "x": 1201,
          "y": 904
        },
        {
          "x": 199,
          "y": 904
        }
      ],
      "category": "paragraph",
      "html": "<p id='69' style='font-size:16px'>each stage, using 3x3 conv with stride 2 (and 1x1 conv with<br>stride 2 at the shortcut connection). In Swin Transformers, a<br>separate downsampling layer is added between stages. We<br>explore a similar strategy in which we use 2x2 conv layers<br>with stride 2 for spatial downsampling. This modification<br>surprisingly leads to diverged training. Further investigation<br>shows that, adding normalization layers wherever spatial<br>resolution is changed can help stablize training. These in-<br>clude several LN layers also used in Swin Transformers: one<br>before each downsampling layer, one after the stem, and one<br>after the final global average pooling. We can improve the<br>accuracy to 82.0%, significantly exceeding Swin-T's 81.3%.</p>",
      "id": 69,
      "page": 6,
      "text": "each stage, using 3x3 conv with stride 2 (and 1x1 conv with\nstride 2 at the shortcut connection). In Swin Transformers, a\nseparate downsampling layer is added between stages. We\nexplore a similar strategy in which we use 2x2 conv layers\nwith stride 2 for spatial downsampling. This modification\nsurprisingly leads to diverged training. Further investigation\nshows that, adding normalization layers wherever spatial\nresolution is changed can help stablize training. These in-\nclude several LN layers also used in Swin Transformers: one\nbefore each downsampling layer, one after the stem, and one\nafter the final global average pooling. We can improve the\naccuracy to 82.0%, significantly exceeding Swin-T's 81.3%."
    },
    {
      "bounding_box": [
        {
          "x": 198,
          "y": 909
        },
        {
          "x": 1199,
          "y": 909
        },
        {
          "x": 1199,
          "y": 1002
        },
        {
          "x": 198,
          "y": 1002
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='70' style='font-size:16px'>We will use separate downsampling layers. This brings<br>us to our final model, which we have dubbed ConvNeXt.</p>",
      "id": 70,
      "page": 6,
      "text": "We will use separate downsampling layers. This brings\nus to our final model, which we have dubbed ConvNeXt."
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 1010
        },
        {
          "x": 1204,
          "y": 1010
        },
        {
          "x": 1204,
          "y": 1205
        },
        {
          "x": 200,
          "y": 1205
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='71' style='font-size:16px'>A comparison of ResNet, Swin, and ConvNeXt block struc-<br>tures can be found in Figure 4. A comparison of ResNet-50,<br>Swin-T and ConvNeXt-T's detailed architecture specifica-<br>tions can be found in Table 9.</p>",
      "id": 71,
      "page": 6,
      "text": "A comparison of ResNet, Swin, and ConvNeXt block struc-\ntures can be found in Figure 4. A comparison of ResNet-50,\nSwin-T and ConvNeXt-T's detailed architecture specifica-\ntions can be found in Table 9."
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 1239
        },
        {
          "x": 1203,
          "y": 1239
        },
        {
          "x": 1203,
          "y": 1838
        },
        {
          "x": 200,
          "y": 1838
        }
      ],
      "category": "paragraph",
      "html": "<p id='72' style='font-size:14px'>Closing remarks. We have finished our first \"playthrough\"<br>and discovered ConvNeXt, a pure ConvNet, that can outper-<br>form the Swin Transformer for ImageNet-1K classification<br>in this compute regime. It is worth noting that all design<br>choices discussed so far are adapted from vision Transform-<br>ers. In addition, these designs are not novel even in the<br>ConvNet literature - they have all been researched sepa-<br>rately, but not collectively, over the last decade. Our Con-<br>vNeXt model has approximately the same FLOPs, #params.,<br>throughput, and memory use as the Swin Transformer, but<br>does not require specialized modules such as shifted window<br>attention or relative position biases.</p>",
      "id": 72,
      "page": 6,
      "text": "Closing remarks. We have finished our first \"playthrough\"\nand discovered ConvNeXt, a pure ConvNet, that can outper-\nform the Swin Transformer for ImageNet-1K classification\nin this compute regime. It is worth noting that all design\nchoices discussed so far are adapted from vision Transform-\ners. In addition, these designs are not novel even in the\nConvNet literature - they have all been researched sepa-\nrately, but not collectively, over the last decade. Our Con-\nvNeXt model has approximately the same FLOPs, #params.,\nthroughput, and memory use as the Swin Transformer, but\ndoes not require specialized modules such as shifted window\nattention or relative position biases."
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 1842
        },
        {
          "x": 1202,
          "y": 1842
        },
        {
          "x": 1202,
          "y": 2342
        },
        {
          "x": 200,
          "y": 2342
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='73' style='font-size:14px'>These findings are encouraging but not yet completely<br>convincing our exploration thus far has been limited to<br>a small scale, but vision Transformers' scaling behavior is<br>what truly distinguishes them. Additionally, the question of<br>whether a ConvNet can compete with Swin Transformers<br>on downstream tasks such as object detection and semantic<br>segmentation is a central concern for computer vision practi-<br>tioners. In the next section, we will scale up our ConvNeXt<br>models both in terms of data and model size, and evaluate<br>them on a diverse set of visual recognition tasks.</p>",
      "id": 73,
      "page": 6,
      "text": "These findings are encouraging but not yet completely\nconvincing our exploration thus far has been limited to\na small scale, but vision Transformers' scaling behavior is\nwhat truly distinguishes them. Additionally, the question of\nwhether a ConvNet can compete with Swin Transformers\non downstream tasks such as object detection and semantic\nsegmentation is a central concern for computer vision practi-\ntioners. In the next section, we will scale up our ConvNeXt\nmodels both in terms of data and model size, and evaluate\nthem on a diverse set of visual recognition tasks."
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 2390
        },
        {
          "x": 1024,
          "y": 2390
        },
        {
          "x": 1024,
          "y": 2447
        },
        {
          "x": 200,
          "y": 2447
        }
      ],
      "category": "paragraph",
      "html": "<p id='74' style='font-size:20px'>3. Empirical Evaluations on ImageNet</p>",
      "id": 74,
      "page": 6,
      "text": "3. Empirical Evaluations on ImageNet"
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 2477
        },
        {
          "x": 1203,
          "y": 2477
        },
        {
          "x": 1203,
          "y": 2975
        },
        {
          "x": 200,
          "y": 2975
        }
      ],
      "category": "paragraph",
      "html": "<p id='75' style='font-size:16px'>We construct different ConvNeXt variants, ConvNeXt-<br>T/S/B/L, to be of similar complexities to Swin-T/S/B/L [45].<br>ConvNeXt-T/B is the end product of the \"modernizing\" pro-<br>cedure on ResNet-50/200 regime, respectively. In addition,<br>we build a larger ConvNeXt-XL to further test the scalabil-<br>ity of ConvNeXt. The variants only differ in the number<br>of channels C, and the number of blocks B in each stage.<br>Following both ResNets and Swin Transformers, the number<br>of channels doubles at each new stage. We summarize the<br>configurations below:</p>",
      "id": 75,
      "page": 6,
      "text": "We construct different ConvNeXt variants, ConvNeXt-\nT/S/B/L, to be of similar complexities to Swin-T/S/B/L [45].\nConvNeXt-T/B is the end product of the \"modernizing\" pro-\ncedure on ResNet-50/200 regime, respectively. In addition,\nwe build a larger ConvNeXt-XL to further test the scalabil-\nity of ConvNeXt. The variants only differ in the number\nof channels C, and the number of blocks B in each stage.\nFollowing both ResNets and Swin Transformers, the number\nof channels doubles at each new stage. We summarize the\nconfigurations below:"
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 307
        },
        {
          "x": 2266,
          "y": 307
        },
        {
          "x": 2266,
          "y": 566
        },
        {
          "x": 1279,
          "y": 566
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='76' style='font-size:14px'>● ConvNeXt-T: C = (96, 192, 384, 768), B = (3,3,9,3)<br>● ConvNeXt-S: C = (96, 192, 384, 768), B = (3,3, 27,3)<br>● ConvNeXt-B: C = (128, 256, 512, 1024), B = (3,3, 27,3)<br>● ConvNeXt-L: C = (192, 384, 768, 1536), B = (3,3, 27,3)<br>ConvNeXt-XL: C = (256, 512, 1024, 2048), B = (3,3,27,3)</p>",
      "id": 76,
      "page": 6,
      "text": "● ConvNeXt-T: C = (96, 192, 384, 768), B = (3,3,9,3)\n● ConvNeXt-S: C = (96, 192, 384, 768), B = (3,3, 27,3)\n● ConvNeXt-B: C = (128, 256, 512, 1024), B = (3,3, 27,3)\n● ConvNeXt-L: C = (192, 384, 768, 1536), B = (3,3, 27,3)\nConvNeXt-XL: C = (256, 512, 1024, 2048), B = (3,3,27,3)"
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 593
        },
        {
          "x": 1525,
          "y": 593
        },
        {
          "x": 1525,
          "y": 641
        },
        {
          "x": 1280,
          "y": 641
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='77' style='font-size:22px'>3.1. Settings</p>",
      "id": 77,
      "page": 6,
      "text": "3.1. Settings"
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 669
        },
        {
          "x": 2280,
          "y": 669
        },
        {
          "x": 2280,
          "y": 1066
        },
        {
          "x": 1279,
          "y": 1066
        }
      ],
      "category": "paragraph",
      "html": "<p id='78' style='font-size:16px'>The ImageNet-1K dataset consists of 1000 object classes<br>with 1.2M training images. We report ImageNet-1K top-1<br>accuracy on the validation set. We also conduct pre-training<br>on ImageNet-22K, a larger dataset of 21841 classes (a super-<br>set of the 1000 ImageNet-1K classes) with ~14M images<br>for pre-training, and then fine-tune the pre-trained model on<br>ImageNet-1K for evaluation. We summarize our training<br>setups below. More details can be found in Appendix A.</p>",
      "id": 78,
      "page": 6,
      "text": "The ImageNet-1K dataset consists of 1000 object classes\nwith 1.2M training images. We report ImageNet-1K top-1\naccuracy on the validation set. We also conduct pre-training\non ImageNet-22K, a larger dataset of 21841 classes (a super-\nset of the 1000 ImageNet-1K classes) with ~14M images\nfor pre-training, and then fine-tune the pre-trained model on\nImageNet-1K for evaluation. We summarize our training\nsetups below. More details can be found in Appendix A."
    },
    {
      "bounding_box": [
        {
          "x": 1277,
          "y": 1085
        },
        {
          "x": 2280,
          "y": 1085
        },
        {
          "x": 2280,
          "y": 1635
        },
        {
          "x": 1277,
          "y": 1635
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='79' style='font-size:18px'>Training on ImageNet-1K. We train ConvNeXts for 300<br>epochs using AdamW [46] with a learning rate of 4e-3.<br>There is a 20-epoch linear warmup and a cosine decaying<br>schedule afterward. We use a batch size of 4096 and a<br>weight decay of 0.05. For data augmentations, we adopt<br>common schemes including Mixup [90], Cutmix [89], Ran-<br>dAugment [14], and Random Erasing [91]. We regularize<br>the networks with Stochastic Depth [37] and Label Smooth-<br>ing [69]. Layer Scale [74] of initial value 1e-6 is applied.<br>We use Exponential Moving Average (EMA) [51] as we find<br>it alleviates larger models' overfitting.</p>",
      "id": 79,
      "page": 6,
      "text": "Training on ImageNet-1K. We train ConvNeXts for 300\nepochs using AdamW [46] with a learning rate of 4e-3.\nThere is a 20-epoch linear warmup and a cosine decaying\nschedule afterward. We use a batch size of 4096 and a\nweight decay of 0.05. For data augmentations, we adopt\ncommon schemes including Mixup [90], Cutmix [89], Ran-\ndAugment [14], and Random Erasing [91]. We regularize\nthe networks with Stochastic Depth [37] and Label Smooth-\ning [69]. Layer Scale [74] of initial value 1e-6 is applied.\nWe use Exponential Moving Average (EMA) [51] as we find\nit alleviates larger models' overfitting."
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 1655
        },
        {
          "x": 2279,
          "y": 1655
        },
        {
          "x": 2279,
          "y": 1804
        },
        {
          "x": 1281,
          "y": 1804
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='80' style='font-size:14px'>Pre-training on ImageNet-22K. We pre-train ConvNeXts<br>on ImageNet-22K for 90 epochs with a warmup of 5 epochs.<br>We do not use EMA. Other settings follow ImageNet-1K.</p>",
      "id": 80,
      "page": 6,
      "text": "Pre-training on ImageNet-22K. We pre-train ConvNeXts\non ImageNet-22K for 90 epochs with a warmup of 5 epochs.\nWe do not use EMA. Other settings follow ImageNet-1K."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 1824
        },
        {
          "x": 2278,
          "y": 1824
        },
        {
          "x": 2278,
          "y": 2220
        },
        {
          "x": 1280,
          "y": 2220
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='81' style='font-size:18px'>Fine-tuning on ImageNet-1K. We fine-tune ImageNet-<br>22K pre-trained models on ImageNet-1K for 30 epochs. We<br>use AdamW, a learning rate of 5e-5, cosine learning rate<br>schedule, layer-wise learning rate decay [6, 12], no warmup,<br>a batch size of 512, and weight decay of 1e-8. The default<br>pre-training, fine-tuning, and testing resolution is 2242. Ad-<br>ditionally, we fine-tune at a larger resolution of 3842, for<br>both ImageNet-22K and ImageNet-1K pre-trained models.</p>",
      "id": 81,
      "page": 6,
      "text": "Fine-tuning on ImageNet-1K. We fine-tune ImageNet-\n22K pre-trained models on ImageNet-1K for 30 epochs. We\nuse AdamW, a learning rate of 5e-5, cosine learning rate\nschedule, layer-wise learning rate decay [6, 12], no warmup,\na batch size of 512, and weight decay of 1e-8. The default\npre-training, fine-tuning, and testing resolution is 2242. Ad-\nditionally, we fine-tune at a larger resolution of 3842, for\nboth ImageNet-22K and ImageNet-1K pre-trained models."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 2225
        },
        {
          "x": 2278,
          "y": 2225
        },
        {
          "x": 2278,
          "y": 2423
        },
        {
          "x": 1280,
          "y": 2423
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='82' style='font-size:14px'>Compared with ViTs/Swin Transformers, ConvNeXts are<br>simpler to fine-tune at different resolutions, as the network<br>is fully-convolutional and there is no need to adjust the input<br>patch size or interpolate absolute/relative position biases.</p>",
      "id": 82,
      "page": 6,
      "text": "Compared with ViTs/Swin Transformers, ConvNeXts are\nsimpler to fine-tune at different resolutions, as the network\nis fully-convolutional and there is no need to adjust the input\npatch size or interpolate absolute/relative position biases."
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 2450
        },
        {
          "x": 1513,
          "y": 2450
        },
        {
          "x": 1513,
          "y": 2498
        },
        {
          "x": 1281,
          "y": 2498
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='83' style='font-size:18px'>3.2. Results</p>",
      "id": 83,
      "page": 6,
      "text": "3.2. Results"
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 2528
        },
        {
          "x": 2280,
          "y": 2528
        },
        {
          "x": 2280,
          "y": 2977
        },
        {
          "x": 1280,
          "y": 2977
        }
      ],
      "category": "paragraph",
      "html": "<p id='84' style='font-size:16px'>ImageNet-1K. Table 1 (upper) shows the result compari-<br>son with two recent Transformer variants, DeiT [73] and<br>Swin Transformers [45], as well as two ConvNets from<br>architecture search - RegNets [54], EfficientNets [71] and<br>EfficientNetsV2 [72]. ConvNeXt competes favorably with<br>two strong ConvNet baselines (RegNet [54] and Efficient-<br>Net [71]) in terms of the accuracy-computation trade-off, as<br>well as the inference throughputs. ConvNeXt also outper-<br>forms Swin Transformer of similar complexities across the</p>",
      "id": 84,
      "page": 6,
      "text": "ImageNet-1K. Table 1 (upper) shows the result compari-\nson with two recent Transformer variants, DeiT [73] and\nSwin Transformers [45], as well as two ConvNets from\narchitecture search - RegNets [54], EfficientNets [71] and\nEfficientNetsV2 [72]. ConvNeXt competes favorably with\ntwo strong ConvNet baselines (RegNet [54] and Efficient-\nNet [71]) in terms of the accuracy-computation trade-off, as\nwell as the inference throughputs. ConvNeXt also outper-\nforms Swin Transformer of similar complexities across the"
    },
    {
      "bounding_box": [
        {
          "x": 206,
          "y": 326
        },
        {
          "x": 311,
          "y": 326
        },
        {
          "x": 311,
          "y": 363
        },
        {
          "x": 206,
          "y": 363
        }
      ],
      "category": "caption",
      "html": "<caption id='85' style='font-size:16px'>model</caption>",
      "id": 85,
      "page": 7,
      "text": "model"
    },
    {
      "bounding_box": [
        {
          "x": 524,
          "y": 302
        },
        {
          "x": 1179,
          "y": 302
        },
        {
          "x": 1179,
          "y": 355
        },
        {
          "x": 524,
          "y": 355
        }
      ],
      "category": "caption",
      "html": "<br><caption id='86' style='font-size:20px'>image throughput IN-1K<br>#param. FLOPs</caption>",
      "id": 86,
      "page": 7,
      "text": "image throughput IN-1K\n#param. FLOPs"
    },
    {
      "bounding_box": [
        {
          "x": 199,
          "y": 348
        },
        {
          "x": 1213,
          "y": 348
        },
        {
          "x": 1213,
          "y": 2107
        },
        {
          "x": 199,
          "y": 2107
        }
      ],
      "category": "table",
      "html": "<br><table id='87' style='font-size:14px'><tr><td></td><td colspan=\"3\">size</td><td colspan=\"2\">(image / s) top-1 acc.</td></tr><tr><td colspan=\"6\">ImageNet-1K trained models</td></tr><tr><td>● RegNetY-16G [54]</td><td>2242</td><td>84M</td><td>16.0G</td><td>334.7</td><td>82.9</td></tr><tr><td>· EffNet-B7 [71]</td><td>6002</td><td>66M</td><td>37.0G</td><td>55.1</td><td>84.3</td></tr><tr><td>● EffNetV2-L [72]</td><td>4802</td><td>120M</td><td>53.0G</td><td>83.7</td><td>85.7</td></tr><tr><td>○ DeiT-S [73]</td><td>2242</td><td>22M</td><td>4.6G</td><td>978.5</td><td>79.8</td></tr><tr><td>○ DeiT-B [73]</td><td>2242</td><td>87M</td><td>17.6G</td><td>302.1</td><td>81.8</td></tr><tr><td>○ Swin-T</td><td>2242</td><td>28M</td><td>4.5G</td><td>757.9</td><td>81.3</td></tr><tr><td>· ConvNeXt-T</td><td>2242</td><td>29M</td><td>4.5G</td><td>774.7</td><td>82.1</td></tr><tr><td>Swin-S</td><td>2242</td><td>50M</td><td>8.7G</td><td>436.7</td><td>83.0</td></tr><tr><td>● ConvNeXt-S</td><td>2242</td><td>50M</td><td>8.7G</td><td>447.1</td><td>83.1</td></tr><tr><td>Swin-B</td><td>2242</td><td>88M</td><td>15.4G</td><td>286.6</td><td>83.5</td></tr><tr><td>· ConvNeXt-B</td><td>2242</td><td>89M</td><td>15.4G</td><td>292.1</td><td>83.8</td></tr><tr><td>○ Swin-B</td><td>3842</td><td>88M</td><td>47.1G</td><td>85.1</td><td>84.5</td></tr><tr><td>● ConvNeXt-B</td><td>3842</td><td>89M</td><td>45.0G</td><td>95.7</td><td>85.1</td></tr><tr><td>● ConvNeXt-L</td><td>2242</td><td>198M</td><td>34.4G</td><td>146.8</td><td>84.3</td></tr><tr><td>· ConvNeXt-L</td><td>3842</td><td>198M</td><td>101.0G</td><td>50.4</td><td>85.5</td></tr><tr><td colspan=\"6\">ImageNet-22K pre-trained models</td></tr><tr><td>● R-101x3 [39]</td><td>3842</td><td>388M</td><td>204.6G</td><td>-</td><td>84.4</td></tr><tr><td>● R-152x4 [39]</td><td>4802</td><td>937M</td><td>840.5G</td><td>-</td><td>85.4</td></tr><tr><td>● EffNetV2-L [72]</td><td>4802</td><td>120M</td><td>53.0G</td><td>83.7</td><td>86.8</td></tr><tr><td>● EffNetV2-XL [72]</td><td>4802</td><td>208M</td><td>94.0G</td><td>56.5</td><td>87.3</td></tr><tr><td>○ ViT-B/16 (☎) [67]</td><td>3842</td><td>87M</td><td>55.5G</td><td>93.1</td><td>85.4</td></tr><tr><td>○ ViT-L/16 (☎) [67]</td><td>3842</td><td>305M</td><td>191.1G</td><td>28.5</td><td>86.8</td></tr><tr><td>· ConvNeXt-T</td><td>2242</td><td>29M</td><td>4.5G</td><td>774.7</td><td>82.9</td></tr><tr><td>● ConvNeXt-T</td><td>3842</td><td>29M</td><td>13.1G</td><td>282.8</td><td>84.1</td></tr><tr><td>● ConvNeXt-S</td><td>2242</td><td>50M</td><td>8.7G</td><td>447.1</td><td>84.6</td></tr><tr><td>· ConvNeXt-S</td><td>3842</td><td>50M</td><td>25.5G</td><td>163.5</td><td>85.8</td></tr><tr><td>○ Swin-B</td><td>2242</td><td>88M</td><td>15.4G</td><td>286.6</td><td>85.2</td></tr><tr><td>● ConvNeXt-B</td><td>2242</td><td>89M</td><td>15.4G</td><td>292.1</td><td>85.8</td></tr><tr><td>○ Swin-B</td><td>3842</td><td>88M</td><td>47.0G</td><td>85.1</td><td>86.4</td></tr><tr><td>· ConvNeXt-B</td><td>3842</td><td>89M</td><td>45.1G</td><td>95.7</td><td>86.8</td></tr><tr><td>Swin-L</td><td>2242</td><td>197M</td><td>34.5G</td><td>145.0</td><td>86.3</td></tr><tr><td>● ConvNeXt-L</td><td>2242</td><td>198M</td><td>34.4G</td><td>146.8</td><td>86.6</td></tr><tr><td>○ Swin-L</td><td>3842</td><td>197M</td><td>103.9G</td><td>46.0</td><td>87.3</td></tr><tr><td>● ConvNeXt-L</td><td>3842</td><td>198M</td><td>101.0G</td><td>50.4</td><td>87.5</td></tr><tr><td>● ConvNeXt-XL</td><td>2242</td><td>350M</td><td>60.9G</td><td>89.3</td><td>87.0</td></tr><tr><td>● ConvNeXt-XL</td><td>3842</td><td>350M</td><td>179.0G</td><td>30.2</td><td>87.8</td></tr></table>",
      "id": 87,
      "page": 7,
      "text": "size (image / s) top-1 acc.\n ImageNet-1K trained models\n ● RegNetY-16G [54] 2242 84M 16.0G 334.7 82.9\n · EffNet-B7 [71] 6002 66M 37.0G 55.1 84.3\n ● EffNetV2-L [72] 4802 120M 53.0G 83.7 85.7\n ○ DeiT-S [73] 2242 22M 4.6G 978.5 79.8\n ○ DeiT-B [73] 2242 87M 17.6G 302.1 81.8\n ○ Swin-T 2242 28M 4.5G 757.9 81.3\n · ConvNeXt-T 2242 29M 4.5G 774.7 82.1\n Swin-S 2242 50M 8.7G 436.7 83.0\n ● ConvNeXt-S 2242 50M 8.7G 447.1 83.1\n Swin-B 2242 88M 15.4G 286.6 83.5\n · ConvNeXt-B 2242 89M 15.4G 292.1 83.8\n ○ Swin-B 3842 88M 47.1G 85.1 84.5\n ● ConvNeXt-B 3842 89M 45.0G 95.7 85.1\n ● ConvNeXt-L 2242 198M 34.4G 146.8 84.3\n · ConvNeXt-L 3842 198M 101.0G 50.4 85.5\n ImageNet-22K pre-trained models\n ● R-101x3 [39] 3842 388M 204.6G - 84.4\n ● R-152x4 [39] 4802 937M 840.5G - 85.4\n ● EffNetV2-L [72] 4802 120M 53.0G 83.7 86.8\n ● EffNetV2-XL [72] 4802 208M 94.0G 56.5 87.3\n ○ ViT-B/16 (☎) [67] 3842 87M 55.5G 93.1 85.4\n ○ ViT-L/16 (☎) [67] 3842 305M 191.1G 28.5 86.8\n · ConvNeXt-T 2242 29M 4.5G 774.7 82.9\n ● ConvNeXt-T 3842 29M 13.1G 282.8 84.1\n ● ConvNeXt-S 2242 50M 8.7G 447.1 84.6\n · ConvNeXt-S 3842 50M 25.5G 163.5 85.8\n ○ Swin-B 2242 88M 15.4G 286.6 85.2\n ● ConvNeXt-B 2242 89M 15.4G 292.1 85.8\n ○ Swin-B 3842 88M 47.0G 85.1 86.4\n · ConvNeXt-B 3842 89M 45.1G 95.7 86.8\n Swin-L 2242 197M 34.5G 145.0 86.3\n ● ConvNeXt-L 2242 198M 34.4G 146.8 86.6\n ○ Swin-L 3842 197M 103.9G 46.0 87.3\n ● ConvNeXt-L 3842 198M 101.0G 50.4 87.5\n ● ConvNeXt-XL 2242 350M 60.9G 89.3 87.0\n ● ConvNeXt-XL 3842 350M 179.0G 30.2"
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 2134
        },
        {
          "x": 1202,
          "y": 2134
        },
        {
          "x": 1202,
          "y": 2500
        },
        {
          "x": 202,
          "y": 2500
        }
      ],
      "category": "paragraph",
      "html": "<p id='88' style='font-size:16px'>Table 1. Classification accuracy on ImageNet-1K. Similar to<br>Transformers, ConvNeXt also shows promising scaling behavior<br>with higher-capacity models and a larger (pre-training) dataset. In-<br>ference throughput is measured on a V100 GPU, following [45]. On<br>an A100 GPU, ConvNeXt can have a much higher throughput than<br>Swin Transformer. See Appendix E. (☎)ViT results with 90-epoch<br>AugReg [67] training, provided through personal communication<br>with the authors.</p>",
      "id": 88,
      "page": 7,
      "text": "Table 1. Classification accuracy on ImageNet-1K. Similar to\nTransformers, ConvNeXt also shows promising scaling behavior\nwith higher-capacity models and a larger (pre-training) dataset. In-\nference throughput is measured on a V100 GPU, following [45]. On\nan A100 GPU, ConvNeXt can have a much higher throughput than\nSwin Transformer. See Appendix E. (☎)ViT results with 90-epoch\nAugReg [67] training, provided through personal communication\nwith the authors."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 2562
        },
        {
          "x": 1201,
          "y": 2562
        },
        {
          "x": 1201,
          "y": 2759
        },
        {
          "x": 202,
          "y": 2759
        }
      ],
      "category": "paragraph",
      "html": "<p id='89' style='font-size:20px'>board, sometimes with a substantial margin (e.g. 0.8% for<br>ConvNeXt-T). Without specialized modules such as shifted<br>windows or relative position bias, ConvNeXts also enjoy<br>improved throughput compared to Swin Transformers.</p>",
      "id": 89,
      "page": 7,
      "text": "board, sometimes with a substantial margin (e.g. 0.8% for\nConvNeXt-T). Without specialized modules such as shifted\nwindows or relative position bias, ConvNeXts also enjoy\nimproved throughput compared to Swin Transformers."
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 2778
        },
        {
          "x": 1202,
          "y": 2778
        },
        {
          "x": 1202,
          "y": 2975
        },
        {
          "x": 203,
          "y": 2975
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='90' style='font-size:20px'>A highlight from the results is ConvNeXt-B at 3842: it<br>outperforms Swin-B by 0.6% (85.1% VS. 84.5%), but with<br>12.5% higher inference throughput (95.7 VS. 85.1 image/s).<br>We note that the FLOPs/throughput advantage of ConvNeXt-</p>",
      "id": 90,
      "page": 7,
      "text": "A highlight from the results is ConvNeXt-B at 3842: it\noutperforms Swin-B by 0.6% (85.1% VS. 84.5%), but with\n12.5% higher inference throughput (95.7 VS. 85.1 image/s).\nWe note that the FLOPs/throughput advantage of ConvNeXt-"
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 306
        },
        {
          "x": 2279,
          "y": 306
        },
        {
          "x": 2279,
          "y": 453
        },
        {
          "x": 1279,
          "y": 453
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='91' style='font-size:18px'>B over Swin-B becomes larger when the resolution increases<br>from 2242 to 3842. Additionally, we observe an improved<br>result of 85.5% when further scaling to ConvNeXt-L.</p>",
      "id": 91,
      "page": 7,
      "text": "B over Swin-B becomes larger when the resolution increases\nfrom 2242 to 3842. Additionally, we observe an improved\nresult of 85.5% when further scaling to ConvNeXt-L."
    },
    {
      "bounding_box": [
        {
          "x": 1277,
          "y": 477
        },
        {
          "x": 2281,
          "y": 477
        },
        {
          "x": 2281,
          "y": 1125
        },
        {
          "x": 1277,
          "y": 1125
        }
      ],
      "category": "paragraph",
      "html": "<p id='92' style='font-size:18px'>ImageNet-22K. We present results with models fine-tuned<br>from ImageNet-22K pre-training at Table 1 (lower). These<br>experiments are important since a widely held view is that<br>vision Transformers have fewer inductive biases thus can per-<br>form better than ConvNets when pre-trained on a larger scale.<br>Our results demonstrate that properly designed ConvNets<br>are not inferior to vision Transformers when pre-trained<br>with large dataset - ConvNeXts still perform on par or<br>better than similarly-sized Swin Transformers, with slightly<br>higher throughput. Additionally, our ConvNeXt-XL model<br>achieves an accuracy of 87.8% - a decent improvement<br>over ConvNeXt-L at 3842, demonstrating that ConvNeXts<br>are scalable architectures.</p>",
      "id": 92,
      "page": 7,
      "text": "ImageNet-22K. We present results with models fine-tuned\nfrom ImageNet-22K pre-training at Table 1 (lower). These\nexperiments are important since a widely held view is that\nvision Transformers have fewer inductive biases thus can per-\nform better than ConvNets when pre-trained on a larger scale.\nOur results demonstrate that properly designed ConvNets\nare not inferior to vision Transformers when pre-trained\nwith large dataset - ConvNeXts still perform on par or\nbetter than similarly-sized Swin Transformers, with slightly\nhigher throughput. Additionally, our ConvNeXt-XL model\nachieves an accuracy of 87.8% - a decent improvement\nover ConvNeXt-L at 3842, demonstrating that ConvNeXts\nare scalable architectures."
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 1127
        },
        {
          "x": 2280,
          "y": 1127
        },
        {
          "x": 2280,
          "y": 1423
        },
        {
          "x": 1278,
          "y": 1423
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='93' style='font-size:20px'>On ImageNet-1K, EfficientNetV2-L, a searched architec-<br>ture equipped with advanced modules (such as Squeeze-and-<br>Excitation [35]) and progressive training procedure achieves<br>top performance. However, with ImageNet-22K pre-training,<br>ConvNeXt is able to outperform EfficientNetV2, further<br>demonstrating the importance of large-scale training.</p>",
      "id": 93,
      "page": 7,
      "text": "On ImageNet-1K, EfficientNetV2-L, a searched architec-\nture equipped with advanced modules (such as Squeeze-and-\nExcitation [35]) and progressive training procedure achieves\ntop performance. However, with ImageNet-22K pre-training,\nConvNeXt is able to outperform EfficientNetV2, further\ndemonstrating the importance of large-scale training."
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 1427
        },
        {
          "x": 2277,
          "y": 1427
        },
        {
          "x": 2277,
          "y": 1522
        },
        {
          "x": 1281,
          "y": 1522
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='94' style='font-size:18px'>In Appendix B, we discuss robustness and out-of-domain<br>generalization results for ConvNeXt.</p>",
      "id": 94,
      "page": 7,
      "text": "In Appendix B, we discuss robustness and out-of-domain\ngeneralization results for ConvNeXt."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 1554
        },
        {
          "x": 1905,
          "y": 1554
        },
        {
          "x": 1905,
          "y": 1606
        },
        {
          "x": 1280,
          "y": 1606
        }
      ],
      "category": "paragraph",
      "html": "<p id='95' style='font-size:22px'>3.3. Isotropic ConvNeXt vs. ViT</p>",
      "id": 95,
      "page": 7,
      "text": "3.3. Isotropic ConvNeXt vs. ViT"
    },
    {
      "bounding_box": [
        {
          "x": 1277,
          "y": 1633
        },
        {
          "x": 2281,
          "y": 1633
        },
        {
          "x": 2281,
          "y": 2430
        },
        {
          "x": 1277,
          "y": 2430
        }
      ],
      "category": "paragraph",
      "html": "<p id='96' style='font-size:18px'>In this ablation, we examine if our ConvNeXt block de-<br>sign is generalizable to ViT-style [20] isotropic architec-<br>tures which have no downsampling layers and keep the<br>same feature resolutions (e.g. 14x 14) at all depths. We<br>construct isotropic ConvNeXt-S/B/L using the same feature<br>dimensions as ViT-S/B/L (384/768/1024). Depths are set<br>at 18/18/36 to match the number of parameters and FLOPs.<br>The block structure remains the same (Fig. 4). We use the<br>supervised training results from DeiT [73] for ViT-S/B and<br>MAE [26] for ViT-L, as they employ improved training<br>procedures over the original ViTs [20]. ConvNeXt models<br>are trained with the same settings as before, but with longer<br>warmup epochs. Results for ImageNet-1K at 2242 resolution<br>are in Table 2. We observe ConvNeXt can perform generally<br>on par with ViT, showing that our ConvNeXt block design<br>is competitive when used in non-hierarchical models.</p>",
      "id": 96,
      "page": 7,
      "text": "In this ablation, we examine if our ConvNeXt block de-\nsign is generalizable to ViT-style [20] isotropic architec-\ntures which have no downsampling layers and keep the\nsame feature resolutions (e.g. 14x 14) at all depths. We\nconstruct isotropic ConvNeXt-S/B/L using the same feature\ndimensions as ViT-S/B/L (384/768/1024). Depths are set\nat 18/18/36 to match the number of parameters and FLOPs.\nThe block structure remains the same (Fig. 4). We use the\nsupervised training results from DeiT [73] for ViT-S/B and\nMAE [26] for ViT-L, as they employ improved training\nprocedures over the original ViTs [20]. ConvNeXt models\nare trained with the same settings as before, but with longer\nwarmup epochs. Results for ImageNet-1K at 2242 resolution\nare in Table 2. We observe ConvNeXt can perform generally\non par with ViT, showing that our ConvNeXt block design\nis competitive when used in non-hierarchical models."
    },
    {
      "bounding_box": [
        {
          "x": 1283,
          "y": 2509
        },
        {
          "x": 2267,
          "y": 2509
        },
        {
          "x": 2267,
          "y": 2833
        },
        {
          "x": 1283,
          "y": 2833
        }
      ],
      "category": "table",
      "html": "<table id='97' style='font-size:14px'><tr><td>model</td><td>#param.</td><td>FLOPs</td><td>throughput (image / s)</td><td>training mem. (GB)</td><td>IN-1K acc.</td></tr><tr><td>○ ViT-S</td><td>22M</td><td>4.6G</td><td>978.5</td><td>4.9</td><td>79.8</td></tr><tr><td>● ConvNeXt-S (iso.)</td><td>22M</td><td>4.3G</td><td>1038.7</td><td>4.2</td><td>79.7</td></tr><tr><td>○ ViT-B</td><td>87M</td><td>17.6G</td><td>302.1</td><td>9.1</td><td>81.8</td></tr><tr><td>● ConvNeXt-B (iso.)</td><td>87M</td><td>16.9G</td><td>320.1</td><td>7.7</td><td>82.0</td></tr><tr><td>ViT-L</td><td>304M</td><td>61.6G</td><td>93.1</td><td>22.5</td><td>82.6</td></tr><tr><td>● ConvNeXt-L (iso.)</td><td>306M</td><td>59.7G</td><td>94.4</td><td>20.4</td><td>82.6</td></tr></table>",
      "id": 97,
      "page": 7,
      "text": "model #param. FLOPs throughput (image / s) training mem. (GB) IN-1K acc.\n ○ ViT-S 22M 4.6G 978.5 4.9 79.8\n ● ConvNeXt-S (iso.) 22M 4.3G 1038.7 4.2 79.7\n ○ ViT-B 87M 17.6G 302.1 9.1 81.8\n ● ConvNeXt-B (iso.) 87M 16.9G 320.1 7.7 82.0\n ViT-L 304M 61.6G 93.1 22.5 82.6\n ● ConvNeXt-L (iso.) 306M 59.7G 94.4 20.4"
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 2869
        },
        {
          "x": 2279,
          "y": 2869
        },
        {
          "x": 2279,
          "y": 2961
        },
        {
          "x": 1279,
          "y": 2961
        }
      ],
      "category": "caption",
      "html": "<caption id='98' style='font-size:16px'>Table 2. Comparing isotropic ConvNeXt and ViT. Training<br>memory is measured on V100 GPUs with 32 per-GPU batch size.</caption>",
      "id": 98,
      "page": 7,
      "text": "Table 2. Comparing isotropic ConvNeXt and ViT. Training\nmemory is measured on V100 GPUs with 32 per-GPU batch size."
    },
    {
      "bounding_box": [
        {
          "x": 205,
          "y": 328
        },
        {
          "x": 1197,
          "y": 328
        },
        {
          "x": 1197,
          "y": 1120
        },
        {
          "x": 205,
          "y": 1120
        }
      ],
      "category": "table",
      "html": "<table id='99' style='font-size:14px'><tr><td>backbone</td><td>FLOPs</td><td>FPS</td><td>Apbox</td><td>APbox</td><td>APbox</td><td>APmask</td><td>APmask</td><td>APmask</td></tr><tr><td colspan=\"9\">Mask-RCNN 3x schedule</td></tr><tr><td>○ Swin-T</td><td>267G</td><td>23.1</td><td>46.0</td><td>68.1</td><td>50.3</td><td>41.6</td><td>65.1</td><td>44.9</td></tr><tr><td>· ConvNeXt-T</td><td>262G</td><td>25.6</td><td>46.2</td><td>67.9</td><td>50.8</td><td>41.7</td><td>65.0</td><td>44.9</td></tr><tr><td colspan=\"9\">Cascade Mask-RCNN 3x schedule</td></tr><tr><td>● ResNet-50</td><td>739G</td><td>16.2</td><td>46.3</td><td>64.3</td><td>50.5</td><td>40.1</td><td>61.7</td><td>43.4</td></tr><tr><td>· X101-32</td><td>819G</td><td>13.8</td><td>48.1</td><td>66.5</td><td>52.4</td><td>41.6</td><td>63.9</td><td>45.2</td></tr><tr><td>● X101-64</td><td>972G</td><td>12.6</td><td>48.3</td><td>66.4</td><td>52.3</td><td>41.7</td><td>64.0</td><td>45.1</td></tr><tr><td>○ Swin-T</td><td>745G</td><td>12.2</td><td>50.4</td><td>69.2</td><td>54.7</td><td>43.7</td><td>66.6</td><td>47.3</td></tr><tr><td>● ConvNeXt-T</td><td>741G</td><td>13.5</td><td>50.4</td><td>69.1</td><td>54.8</td><td>43.7</td><td>66.5</td><td>47.3</td></tr><tr><td>○ Swin-S</td><td>838G</td><td>11.4</td><td>51.9</td><td>70.7</td><td>56.3</td><td>45.0</td><td>68.2</td><td>48.8</td></tr><tr><td>● ConvNeXt-S</td><td>827G</td><td>12.0</td><td>51.9</td><td>70.8</td><td>56.5</td><td>45.0</td><td>68.4</td><td>49.1</td></tr><tr><td>○ Swin-B</td><td>982G</td><td>10.7</td><td>51.9</td><td>70.5</td><td>56.4</td><td>45.0</td><td>68.1</td><td>48.9</td></tr><tr><td>● ConvNeXt-B</td><td>964G</td><td>11.4</td><td>52.7</td><td>71.3</td><td>57.2</td><td>45.6</td><td>68.9</td><td>49.5</td></tr><tr><td>○ Swin-B‡</td><td>982G</td><td>10.7</td><td>53.0</td><td>71.8</td><td>57.5</td><td>45.8</td><td>69.4</td><td>49.7</td></tr><tr><td>● ConvNeXt-B±</td><td>964G</td><td>11.5</td><td>54.0</td><td>73.1</td><td>58.8</td><td>46.9</td><td>70.6</td><td>51.3</td></tr><tr><td>○ Swin-L‡</td><td>1382G</td><td>9.2</td><td>53.9</td><td>72.4</td><td>58.8</td><td>46.7</td><td>70.1</td><td>50.8</td></tr><tr><td>● ConvNeXt-L‡</td><td>1354G</td><td>10.0</td><td>54.8</td><td>73.8</td><td>59.8</td><td>47.6</td><td>71.3</td><td>51.7</td></tr><tr><td>● ConvNeXt-XL±</td><td>1898G</td><td>8.6</td><td>55.2</td><td>74.2</td><td>59.9</td><td>47.7</td><td>71.6</td><td>52.2</td></tr></table>",
      "id": 99,
      "page": 8,
      "text": "backbone FLOPs FPS Apbox APbox APbox APmask APmask APmask\n Mask-RCNN 3x schedule\n ○ Swin-T 267G 23.1 46.0 68.1 50.3 41.6 65.1 44.9\n · ConvNeXt-T 262G 25.6 46.2 67.9 50.8 41.7 65.0 44.9\n Cascade Mask-RCNN 3x schedule\n ● ResNet-50 739G 16.2 46.3 64.3 50.5 40.1 61.7 43.4\n · X101-32 819G 13.8 48.1 66.5 52.4 41.6 63.9 45.2\n ● X101-64 972G 12.6 48.3 66.4 52.3 41.7 64.0 45.1\n ○ Swin-T 745G 12.2 50.4 69.2 54.7 43.7 66.6 47.3\n ● ConvNeXt-T 741G 13.5 50.4 69.1 54.8 43.7 66.5 47.3\n ○ Swin-S 838G 11.4 51.9 70.7 56.3 45.0 68.2 48.8\n ● ConvNeXt-S 827G 12.0 51.9 70.8 56.5 45.0 68.4 49.1\n ○ Swin-B 982G 10.7 51.9 70.5 56.4 45.0 68.1 48.9\n ● ConvNeXt-B 964G 11.4 52.7 71.3 57.2 45.6 68.9 49.5\n ○ Swin-B‡ 982G 10.7 53.0 71.8 57.5 45.8 69.4 49.7\n ● ConvNeXt-B± 964G 11.5 54.0 73.1 58.8 46.9 70.6 51.3\n ○ Swin-L‡ 1382G 9.2 53.9 72.4 58.8 46.7 70.1 50.8\n ● ConvNeXt-L‡ 1354G 10.0 54.8 73.8 59.8 47.6 71.3 51.7\n ● ConvNeXt-XL± 1898G 8.6 55.2 74.2 59.9 47.7 71.6"
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 1160
        },
        {
          "x": 1200,
          "y": 1160
        },
        {
          "x": 1200,
          "y": 1434
        },
        {
          "x": 203,
          "y": 1434
        }
      ],
      "category": "caption",
      "html": "<caption id='100' style='font-size:16px'>Table 3. COCO object detection and segmentation results using<br>Mask-RCNN and Cascade Mask-RCNN. I indicates that the model<br>is pre-trained on ImageNet-22K. ImageNet-1K pre-trained Swin<br>results are from their Github repository [3]. AP numbers of the<br>ResNet-50 and X101 models are from [45]. We measure FPS on<br>an A100 GPU. FLOPs are calculated with image size (1280, 800).</caption>",
      "id": 100,
      "page": 8,
      "text": "Table 3. COCO object detection and segmentation results using\nMask-RCNN and Cascade Mask-RCNN. I indicates that the model\nis pre-trained on ImageNet-22K. ImageNet-1K pre-trained Swin\nresults are from their Github repository [3]. AP numbers of the\nResNet-50 and X101 models are from [45]. We measure FPS on\nan A100 GPU. FLOPs are calculated with image size (1280, 800)."
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 1522
        },
        {
          "x": 1197,
          "y": 1522
        },
        {
          "x": 1197,
          "y": 1576
        },
        {
          "x": 203,
          "y": 1576
        }
      ],
      "category": "paragraph",
      "html": "<p id='101' style='font-size:22px'>4. Empirical Evaluation on Downstream Tasks</p>",
      "id": 101,
      "page": 8,
      "text": "4. Empirical Evaluation on Downstream Tasks"
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 1610
        },
        {
          "x": 1200,
          "y": 1610
        },
        {
          "x": 1200,
          "y": 1908
        },
        {
          "x": 203,
          "y": 1908
        }
      ],
      "category": "paragraph",
      "html": "<p id='102' style='font-size:20px'>Object detection and segmentation on COCO. We fine-<br>tune Mask R-CNN [27] and Cascade Mask R-CNN [9] on<br>the COCO dataset with ConvNeXt backbones. Following<br>Swin Transformer [45], we use multi-scale training, AdamW<br>optimizer, and a 3x schedule. Further details and hyper-<br>parameter settings can be found in Appendix A.3.</p>",
      "id": 102,
      "page": 8,
      "text": "Object detection and segmentation on COCO. We fine-\ntune Mask R-CNN [27] and Cascade Mask R-CNN [9] on\nthe COCO dataset with ConvNeXt backbones. Following\nSwin Transformer [45], we use multi-scale training, AdamW\noptimizer, and a 3x schedule. Further details and hyper-\nparameter settings can be found in Appendix A.3."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 1914
        },
        {
          "x": 1202,
          "y": 1914
        },
        {
          "x": 1202,
          "y": 2308
        },
        {
          "x": 202,
          "y": 2308
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='103' style='font-size:20px'>Table 3 shows object detection and instance segmentation<br>results comparing Swin Transformer, ConvNeXt, and tradi-<br>tional ConvNet such as ResNeXt. Across different model<br>complexities, ConvNeXt achieves on-par or better perfor-<br>mance than Swin Transformer. When scaled up to bigger<br>models (ConvNeXt-B/L/XL) pre-trained on ImageNet-22K,<br>in many cases ConvNeXt is significantly better (e.g. +1.0 AP)<br>than Swin Transformers in terms of box and mask AP.</p>",
      "id": 103,
      "page": 8,
      "text": "Table 3 shows object detection and instance segmentation\nresults comparing Swin Transformer, ConvNeXt, and tradi-\ntional ConvNet such as ResNeXt. Across different model\ncomplexities, ConvNeXt achieves on-par or better perfor-\nmance than Swin Transformer. When scaled up to bigger\nmodels (ConvNeXt-B/L/XL) pre-trained on ImageNet-22K,\nin many cases ConvNeXt is significantly better (e.g. +1.0 AP)\nthan Swin Transformers in terms of box and mask AP."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 2344
        },
        {
          "x": 1202,
          "y": 2344
        },
        {
          "x": 1202,
          "y": 2794
        },
        {
          "x": 202,
          "y": 2794
        }
      ],
      "category": "paragraph",
      "html": "<p id='104' style='font-size:20px'>Semantic segmentation on ADE20K. We also evaluate<br>ConvNeXt backbones on the ADE20K semantic segmen-<br>tation task with UperNet [85]. All model variants are trained<br>for 160K iterations with a batch size of 16. Other experimen-<br>tal settings follow [6] (see Appendix A.3 for more details).<br>In Table 4, we report validation mloU with multi-scale test-<br>ing. ConvNeXt models can achieve competitive performance<br>across different model capacities, further validating the ef-<br>fectiveness of our architecture design.</p>",
      "id": 104,
      "page": 8,
      "text": "Semantic segmentation on ADE20K. We also evaluate\nConvNeXt backbones on the ADE20K semantic segmen-\ntation task with UperNet [85]. All model variants are trained\nfor 160K iterations with a batch size of 16. Other experimen-\ntal settings follow [6] (see Appendix A.3 for more details).\nIn Table 4, we report validation mloU with multi-scale test-\ning. ConvNeXt models can achieve competitive performance\nacross different model capacities, further validating the ef-\nfectiveness of our architecture design."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 2827
        },
        {
          "x": 1201,
          "y": 2827
        },
        {
          "x": 1201,
          "y": 2978
        },
        {
          "x": 201,
          "y": 2978
        }
      ],
      "category": "paragraph",
      "html": "<p id='105' style='font-size:18px'>Remarks on model efficiency. Under similar FLOPs, mod-<br>els with depthwise convolutions are known to be slower<br>and consume more memory than ConvNets with only dense</p>",
      "id": 105,
      "page": 8,
      "text": "Remarks on model efficiency. Under similar FLOPs, mod-\nels with depthwise convolutions are known to be slower\nand consume more memory than ConvNets with only dense"
    },
    {
      "bounding_box": [
        {
          "x": 1315,
          "y": 333
        },
        {
          "x": 2233,
          "y": 333
        },
        {
          "x": 2233,
          "y": 993
        },
        {
          "x": 1315,
          "y": 993
        }
      ],
      "category": "table",
      "html": "<br><table id='106' style='font-size:14px'><tr><td>backbone</td><td>input crop.</td><td>mloU</td><td>#param.</td><td>FLOPs</td></tr><tr><td colspan=\"5\">ImageNet-1K pre-trained</td></tr><tr><td>Swin-T</td><td>5122</td><td>45.8</td><td>60M</td><td>945G</td></tr><tr><td>ConvNeXt-T</td><td>5122</td><td>46.7</td><td>60M</td><td>939G</td></tr><tr><td>Swin-S</td><td>5122</td><td>49.5</td><td>81M</td><td>1038G</td></tr><tr><td>ConvNeXt-S</td><td>5122</td><td>49.6</td><td>82M</td><td>1027G</td></tr><tr><td>○ Swin-B</td><td>5122</td><td>49.7</td><td>121M</td><td>1188G</td></tr><tr><td>● ConvNeXt-B</td><td>5122</td><td>49.9</td><td>122M</td><td>1170G</td></tr><tr><td colspan=\"5\">ImageNet-22K pre-trained</td></tr><tr><td>Swin-B‡</td><td>6402</td><td>51.7</td><td>121M</td><td>1841G</td></tr><tr><td>ConvNeXt-B :</td><td>6402</td><td>53.1</td><td>122M</td><td>1828G</td></tr><tr><td>○ Swin-L‡</td><td>6402</td><td>53.5</td><td>234M</td><td>2468G</td></tr><tr><td>· ConvNeXt-L ‡</td><td>6402</td><td>53.7</td><td>235M</td><td>2458G</td></tr><tr><td>ConvNeXt-XL :</td><td>6402</td><td>54.0</td><td>391M</td><td>3335G</td></tr></table>",
      "id": 106,
      "page": 8,
      "text": "backbone input crop. mloU #param. FLOPs\n ImageNet-1K pre-trained\n Swin-T 5122 45.8 60M 945G\n ConvNeXt-T 5122 46.7 60M 939G\n Swin-S 5122 49.5 81M 1038G\n ConvNeXt-S 5122 49.6 82M 1027G\n ○ Swin-B 5122 49.7 121M 1188G\n ● ConvNeXt-B 5122 49.9 122M 1170G\n ImageNet-22K pre-trained\n Swin-B‡ 6402 51.7 121M 1841G\n ConvNeXt-B : 6402 53.1 122M 1828G\n ○ Swin-L‡ 6402 53.5 234M 2468G\n · ConvNeXt-L ‡ 6402 53.7 235M 2458G\n ConvNeXt-XL : 6402 54.0 391M"
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 1042
        },
        {
          "x": 2280,
          "y": 1042
        },
        {
          "x": 2280,
          "y": 1279
        },
        {
          "x": 1278,
          "y": 1279
        }
      ],
      "category": "caption",
      "html": "<caption id='107' style='font-size:18px'>Table 4. ADE20K validation results using UperNet [85]. : in-<br>dicates IN-22K pre-training. Swins' results are from its GitHub<br>repository [2]. Following Swin, we report mloU results with multi-<br>scale testing. FLOPs are based on input sizes of (2048, 512) and<br>(2560, 640) for IN-1K and IN-22K pre-trained models, respectively.</caption>",
      "id": 107,
      "page": 8,
      "text": "Table 4. ADE20K validation results using UperNet [85]. : in-\ndicates IN-22K pre-training. Swins' results are from its GitHub\nrepository [2]. Following Swin, we report mloU results with multi-\nscale testing. FLOPs are based on input sizes of (2048, 512) and\n(2560, 640) for IN-1K and IN-22K pre-trained models, respectively."
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 1369
        },
        {
          "x": 2280,
          "y": 1369
        },
        {
          "x": 2280,
          "y": 2217
        },
        {
          "x": 1278,
          "y": 2217
        }
      ],
      "category": "paragraph",
      "html": "<p id='108' style='font-size:20px'>convolutions. It is natural to ask whether the design of<br>ConvNeXt will render it practically inefficient. As demon-<br>strated throughout the paper, the inference throughputs of<br>ConvNeXts are comparable to or exceed that of Swin Trans-<br>formers. This is true for both classification and other tasks<br>requiring higher-resolution inputs (see Table 1,3 for com-<br>parisons of throughput/FPS). Furthermore, we notice that<br>training ConvNeXts requires less memory than training Swin<br>Transformers. For example, training Cascade Mask-RCNN<br>using ConvNeXt-B backbone consumes 17.4GB of peak<br>memory with a per-GPU batch size of2, while the reference<br>number for Swin-B is 18.5GB. In comparison to vanilla ViT,<br>both ConvNeXt and Swin Transformer exhibit a more favor-<br>able accuracy-FLOPs trade-off due to the local computations.<br>It is worth noting that this improved efficiency is a result of<br>the ConvNet inductive bias, and is not directly related to the<br>self-attention mechanism in vision Transformers.</p>",
      "id": 108,
      "page": 8,
      "text": "convolutions. It is natural to ask whether the design of\nConvNeXt will render it practically inefficient. As demon-\nstrated throughout the paper, the inference throughputs of\nConvNeXts are comparable to or exceed that of Swin Trans-\nformers. This is true for both classification and other tasks\nrequiring higher-resolution inputs (see Table 1,3 for com-\nparisons of throughput/FPS). Furthermore, we notice that\ntraining ConvNeXts requires less memory than training Swin\nTransformers. For example, training Cascade Mask-RCNN\nusing ConvNeXt-B backbone consumes 17.4GB of peak\nmemory with a per-GPU batch size of2, while the reference\nnumber for Swin-B is 18.5GB. In comparison to vanilla ViT,\nboth ConvNeXt and Swin Transformer exhibit a more favor-\nable accuracy-FLOPs trade-off due to the local computations.\nIt is worth noting that this improved efficiency is a result of\nthe ConvNet inductive bias, and is not directly related to the\nself-attention mechanism in vision Transformers."
    },
    {
      "bounding_box": [
        {
          "x": 1282,
          "y": 2264
        },
        {
          "x": 1636,
          "y": 2264
        },
        {
          "x": 1636,
          "y": 2315
        },
        {
          "x": 1282,
          "y": 2315
        }
      ],
      "category": "paragraph",
      "html": "<p id='109' style='font-size:22px'>5. Related Work</p>",
      "id": 109,
      "page": 8,
      "text": "5. Related Work"
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 2349
        },
        {
          "x": 2279,
          "y": 2349
        },
        {
          "x": 2279,
          "y": 2800
        },
        {
          "x": 1278,
          "y": 2800
        }
      ],
      "category": "paragraph",
      "html": "<p id='110' style='font-size:20px'>Hybrid models. In both the pre- and post-ViT eras, the<br>hybrid model combining convolutions and self-attentions<br>has been actively studied. Prior to ViT, the focus was<br>on augmenting a ConvNet with self-attention/non-local<br>modules [8, 55, 66, 79] to capture long-range dependen-<br>cies. The original ViT [20] first studied a hybrid config-<br>uration, and a large body of follow-up works focused on<br>reintroducing convolutional priors to ViT, either in an ex-<br>plicit [15, 16,21, 82, 86, 88] or implicit [45] fashion.</p>",
      "id": 110,
      "page": 8,
      "text": "Hybrid models. In both the pre- and post-ViT eras, the\nhybrid model combining convolutions and self-attentions\nhas been actively studied. Prior to ViT, the focus was\non augmenting a ConvNet with self-attention/non-local\nmodules [8, 55, 66, 79] to capture long-range dependen-\ncies. The original ViT [20] first studied a hybrid config-\nuration, and a large body of follow-up works focused on\nreintroducing convolutional priors to ViT, either in an ex-\nplicit [15, 16,21, 82, 86, 88] or implicit [45] fashion."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 2826
        },
        {
          "x": 2279,
          "y": 2826
        },
        {
          "x": 2279,
          "y": 2977
        },
        {
          "x": 1280,
          "y": 2977
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='111' style='font-size:20px'>Recent convolution-based approaches. Han et al. [25]<br>show that local Transformer attention is equivalent to in-<br>homogeneous dynamic depthwise conv. The MSA block in</p>",
      "id": 111,
      "page": 8,
      "text": "Recent convolution-based approaches. Han et al. [25]\nshow that local Transformer attention is equivalent to in-\nhomogeneous dynamic depthwise conv. The MSA block in"
    },
    {
      "bounding_box": [
        {
          "x": 198,
          "y": 303
        },
        {
          "x": 1203,
          "y": 303
        },
        {
          "x": 1203,
          "y": 955
        },
        {
          "x": 198,
          "y": 955
        }
      ],
      "category": "paragraph",
      "html": "<p id='112' style='font-size:16px'>Swin is then replaced with a dynamic or regular depthwise<br>convolution, achieving comparable performance to Swin.<br>A concurrent work ConvMixer [4] demonstrates that, in<br>small-scale settings, depthwise convolution can be used as a<br>promising mixing strategy. ConvMixer uses a smaller patch<br>size to achieve the best results, making the throughput much<br>lower than other baselines. GFNet [56] adopts Fast Fourier<br>Transform (FFT) for token mixing. FFT is also a form of con-<br>volution, but with a global kernel size and circular padding.<br>Unlike many recent Transformer or ConvNet designs, one<br>primary goal of our study is to provide an in-depth look at<br>the process of modernizing a standard ResNet and achieving<br>state-of-the-art performance.</p>",
      "id": 112,
      "page": 9,
      "text": "Swin is then replaced with a dynamic or regular depthwise\nconvolution, achieving comparable performance to Swin.\nA concurrent work ConvMixer [4] demonstrates that, in\nsmall-scale settings, depthwise convolution can be used as a\npromising mixing strategy. ConvMixer uses a smaller patch\nsize to achieve the best results, making the throughput much\nlower than other baselines. GFNet [56] adopts Fast Fourier\nTransform (FFT) for token mixing. FFT is also a form of con-\nvolution, but with a global kernel size and circular padding.\nUnlike many recent Transformer or ConvNet designs, one\nprimary goal of our study is to provide an in-depth look at\nthe process of modernizing a standard ResNet and achieving\nstate-of-the-art performance."
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 1002
        },
        {
          "x": 522,
          "y": 1002
        },
        {
          "x": 522,
          "y": 1055
        },
        {
          "x": 203,
          "y": 1055
        }
      ],
      "category": "paragraph",
      "html": "<p id='113' style='font-size:16px'>6. Conclusions</p>",
      "id": 113,
      "page": 9,
      "text": "6. Conclusions"
    },
    {
      "bounding_box": [
        {
          "x": 199,
          "y": 1086
        },
        {
          "x": 1202,
          "y": 1086
        },
        {
          "x": 1202,
          "y": 1884
        },
        {
          "x": 199,
          "y": 1884
        }
      ],
      "category": "paragraph",
      "html": "<p id='114' style='font-size:14px'>In the 2020s, vision Transformers, particularly hierar-<br>chical ones such as Swin Transformers, began to overtake<br>ConvNets as the favored choice for generic vision backbones.<br>The widely held belief is that vision Transformers are more<br>accurate, efficient, and scalable than ConvNets. We propose<br>ConvNeXts, a pure ConvNet model that can compete favor-<br>ably with state-of-the-art hierarchical vision Transformers<br>across multiple computer vision benchmarks, while retaining<br>the simplicity and efficiency of standard ConvNets. In some<br>ways, our observations are surprising while our ConvNeXt<br>model itself is not completely new many design choices<br>have all been examined separately over the last decade, but<br>not collectively. We hope that the new results reported in this<br>study will challenge several widely held views and prompt<br>people to rethink the importance of convolution in computer<br>vision.</p>",
      "id": 114,
      "page": 9,
      "text": "In the 2020s, vision Transformers, particularly hierar-\nchical ones such as Swin Transformers, began to overtake\nConvNets as the favored choice for generic vision backbones.\nThe widely held belief is that vision Transformers are more\naccurate, efficient, and scalable than ConvNets. We propose\nConvNeXts, a pure ConvNet model that can compete favor-\nably with state-of-the-art hierarchical vision Transformers\nacross multiple computer vision benchmarks, while retaining\nthe simplicity and efficiency of standard ConvNets. In some\nways, our observations are surprising while our ConvNeXt\nmodel itself is not completely new many design choices\nhave all been examined separately over the last decade, but\nnot collectively. We hope that the new results reported in this\nstudy will challenge several widely held views and prompt\npeople to rethink the importance of convolution in computer\nvision."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 1913
        },
        {
          "x": 1203,
          "y": 1913
        },
        {
          "x": 1203,
          "y": 2063
        },
        {
          "x": 201,
          "y": 2063
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='115' style='font-size:16px'>Acknowledgments. We thank Kaiming He, Eric Mintun,<br>Xingyi Zhou, Ross Girshick, and Yann LeCun for valuable<br>discussions and feedback.</p>",
      "id": 115,
      "page": 9,
      "text": "Acknowledgments. We thank Kaiming He, Eric Mintun,\nXingyi Zhou, Ross Girshick, and Yann LeCun for valuable\ndiscussions and feedback."
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 2124
        },
        {
          "x": 460,
          "y": 2124
        },
        {
          "x": 460,
          "y": 2186
        },
        {
          "x": 204,
          "y": 2186
        }
      ],
      "category": "paragraph",
      "html": "<p id='116' style='font-size:22px'>Appendix</p>",
      "id": 116,
      "page": 9,
      "text": "Appendix"
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 2217
        },
        {
          "x": 1202,
          "y": 2217
        },
        {
          "x": 1202,
          "y": 2516
        },
        {
          "x": 200,
          "y": 2516
        }
      ],
      "category": "paragraph",
      "html": "<p id='117' style='font-size:16px'>In this Appendix, we provide further experimental details<br>(§A), robustness evaluation results (§B), more modernization<br>experiment results (§C), and a detailed network specification<br>(§D). We further benchmark model throughput on A100<br>GPUs (§E). Finally, we discuss the limitations (§F) and<br>societal impact (§G) of our work.</p>",
      "id": 117,
      "page": 9,
      "text": "In this Appendix, we provide further experimental details\n(§A), robustness evaluation results (§B), more modernization\nexperiment results (§C), and a detailed network specification\n(§D). We further benchmark model throughput on A100\nGPUs (§E). Finally, we discuss the limitations (§F) and\nsocietal impact (§G) of our work."
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 2562
        },
        {
          "x": 745,
          "y": 2562
        },
        {
          "x": 745,
          "y": 2621
        },
        {
          "x": 203,
          "y": 2621
        }
      ],
      "category": "paragraph",
      "html": "<p id='118' style='font-size:20px'>A. Experimental Settings</p>",
      "id": 118,
      "page": 9,
      "text": "A. Experimental Settings"
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 2648
        },
        {
          "x": 777,
          "y": 2648
        },
        {
          "x": 777,
          "y": 2699
        },
        {
          "x": 203,
          "y": 2699
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='119' style='font-size:18px'>A.1. ImageNet (Pre-)training</p>",
      "id": 119,
      "page": 9,
      "text": "A.1. ImageNet (Pre-)training"
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 2726
        },
        {
          "x": 1202,
          "y": 2726
        },
        {
          "x": 1202,
          "y": 2976
        },
        {
          "x": 200,
          "y": 2976
        }
      ],
      "category": "paragraph",
      "html": "<p id='120' style='font-size:14px'>We provide ConvNeXts' ImageNet-1K training and<br>ImageNet-22K pre-training settings in Table 5. The settings<br>are used for our main results in Table 1 (Section 3.2). All<br>ConvNeXt variants use the same setting, except the stochas-<br>tic depth rate is customized for model variants.</p>",
      "id": 120,
      "page": 9,
      "text": "We provide ConvNeXts' ImageNet-1K training and\nImageNet-22K pre-training settings in Table 5. The settings\nare used for our main results in Table 1 (Section 3.2). All\nConvNeXt variants use the same setting, except the stochas-\ntic depth rate is customized for model variants."
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 305
        },
        {
          "x": 2281,
          "y": 305
        },
        {
          "x": 2281,
          "y": 503
        },
        {
          "x": 1279,
          "y": 503
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='121' style='font-size:16px'>For experiments in \"modernizing a ConvNet\" (Section 2),<br>we also use Table 5's setting for ImageNet-1K, except EMA<br>is disabled, as we find using EMA severely hurts models<br>with BatchNorm layers.</p>",
      "id": 121,
      "page": 9,
      "text": "For experiments in \"modernizing a ConvNet\" (Section 2),\nwe also use Table 5's setting for ImageNet-1K, except EMA\nis disabled, as we find using EMA severely hurts models\nwith BatchNorm layers."
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 513
        },
        {
          "x": 2280,
          "y": 513
        },
        {
          "x": 2280,
          "y": 761
        },
        {
          "x": 1278,
          "y": 761
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='122' style='font-size:16px'>For isotropic ConvNeXts (Section 3.3), the setting for<br>ImageNet-1K in Table A is also adopted, but warmup is ex-<br>tended to 50 epochs, and layer scale is disabled for isotropic<br>ConvNeXt-S/B. The stochastic depth rates are 0.1/0.2/0.5<br>for isotropic ConvNeXt-S/B/L.</p>",
      "id": 122,
      "page": 9,
      "text": "For isotropic ConvNeXts (Section 3.3), the setting for\nImageNet-1K in Table A is also adopted, but warmup is ex-\ntended to 50 epochs, and layer scale is disabled for isotropic\nConvNeXt-S/B. The stochastic depth rates are 0.1/0.2/0.5\nfor isotropic ConvNeXt-S/B/L."
    },
    {
      "bounding_box": [
        {
          "x": 1273,
          "y": 812
        },
        {
          "x": 2319,
          "y": 812
        },
        {
          "x": 2319,
          "y": 1795
        },
        {
          "x": 1273,
          "y": 1795
        }
      ],
      "category": "table",
      "html": "<table id='123' style='font-size:14px'><tr><td rowspan=\"2\">(pre-)training config</td><td colspan=\"2\">ConvNeXt-T/S/B/L ConvNeXt-T/S/B/L/XL</td></tr><tr><td>ImageNet-1K 2242</td><td>ImageNet-22K 2242</td></tr><tr><td>weight init</td><td>trunc. normal (0.2)</td><td>trunc. normal (0.2)</td></tr><tr><td>optimizer</td><td>AdamW</td><td>AdamW</td></tr><tr><td>base learning rate</td><td>4e-3</td><td>4e-3</td></tr><tr><td>weight decay</td><td>0.05</td><td>0.05</td></tr><tr><td>optimizer momentum</td><td>B1, B2=0.9, 0.999</td><td>B1, B2=0.9, 0.999</td></tr><tr><td>batch size</td><td>4096</td><td>4096</td></tr><tr><td>training epochs</td><td>300</td><td>90</td></tr><tr><td>learning rate schedule</td><td>cosine decay</td><td>cosine decay</td></tr><tr><td>warmup epochs</td><td>20</td><td>5</td></tr><tr><td>warmup schedule</td><td>linear</td><td>linear</td></tr><tr><td>layer-wise lr decay [6,12]</td><td>None</td><td>None</td></tr><tr><td>randaugment [14]</td><td>(9, 0.5)</td><td>(9, 0.5)</td></tr><tr><td>mixup [90]</td><td>0.8</td><td>0.8</td></tr><tr><td>cutmix [89]</td><td>1.0</td><td>1.0</td></tr><tr><td>random erasing [91]</td><td>0.25</td><td>0.25</td></tr><tr><td>label smoothing [69]</td><td>0.1</td><td>0.1</td></tr><tr><td>stochastic depth [37]</td><td>0.1/0.4/0.5/0.5</td><td>0.0/0.0/0.1/0.1/0.2</td></tr><tr><td>layer scale [74]</td><td>1e-6</td><td>1e-6</td></tr><tr><td>head init scale [74]</td><td>None</td><td>None</td></tr><tr><td>gradient clip</td><td>None</td><td>None</td></tr><tr><td>exp. mov. avg. (EMA) [51]</td><td>0.9999</td><td>None</td></tr></table>",
      "id": 123,
      "page": 9,
      "text": "(pre-)training config ConvNeXt-T/S/B/L ConvNeXt-T/S/B/L/XL\n ImageNet-1K 2242 ImageNet-22K 2242\n weight init trunc. normal (0.2) trunc. normal (0.2)\n optimizer AdamW AdamW\n base learning rate 4e-3 4e-3\n weight decay 0.05 0.05\n optimizer momentum B1, B2=0.9, 0.999 B1, B2=0.9, 0.999\n batch size 4096 4096\n training epochs 300 90\n learning rate schedule cosine decay cosine decay\n warmup epochs 20 5\n warmup schedule linear linear\n layer-wise lr decay [6,12] None None\n randaugment [14] (9, 0.5) (9, 0.5)\n mixup [90] 0.8 0.8\n cutmix [89] 1.0 1.0\n random erasing [91] 0.25 0.25\n label smoothing [69] 0.1 0.1\n stochastic depth [37] 0.1/0.4/0.5/0.5 0.0/0.0/0.1/0.1/0.2\n layer scale [74] 1e-6 1e-6\n head init scale [74] None None\n gradient clip None None\n exp. mov. avg. (EMA) [51] 0.9999"
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 1831
        },
        {
          "x": 2281,
          "y": 1831
        },
        {
          "x": 2281,
          "y": 1971
        },
        {
          "x": 1279,
          "y": 1971
        }
      ],
      "category": "caption",
      "html": "<caption id='124' style='font-size:14px'>Table 5. ImageNet-1K/22K (pre-)training settings. Multiple<br>stochastic depth rates (e.g., 0.1/0.4/0.5/0.5) are for each model<br>(e.g., ConvNeXt-T/S/B/L) respectively.</caption>",
      "id": 124,
      "page": 9,
      "text": "Table 5. ImageNet-1K/22K (pre-)training settings. Multiple\nstochastic depth rates (e.g., 0.1/0.4/0.5/0.5) are for each model\n(e.g., ConvNeXt-T/S/B/L) respectively."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 2085
        },
        {
          "x": 1813,
          "y": 2085
        },
        {
          "x": 1813,
          "y": 2138
        },
        {
          "x": 1280,
          "y": 2138
        }
      ],
      "category": "paragraph",
      "html": "<p id='125' style='font-size:20px'>A.2. ImageNet Fine-tuning</p>",
      "id": 125,
      "page": 9,
      "text": "A.2. ImageNet Fine-tuning"
    },
    {
      "bounding_box": [
        {
          "x": 1277,
          "y": 2169
        },
        {
          "x": 2280,
          "y": 2169
        },
        {
          "x": 2280,
          "y": 2720
        },
        {
          "x": 1277,
          "y": 2720
        }
      ],
      "category": "paragraph",
      "html": "<p id='126' style='font-size:16px'>We list the settings for fine-tuning on ImageNet-1K in<br>Table 6. The fine-tuning starts from the final model weights<br>obtained in pre-training, without using the EMA weights,<br>even if in pre-training EMA is used and EMA accuracy is<br>reported. This is because we do not observe improvement if<br>we fine-tune with the EMA weights (consistent with observa-<br>tions in [73]). The only exception is ConvNeXt-L pre-trained<br>on ImageNet-1K, where the model accuracy is significantly<br>lower than the EMA accuracy due to overfitting, and we<br>select its best EMA model during pre-training as the starting<br>point for fine-tuning.</p>",
      "id": 126,
      "page": 9,
      "text": "We list the settings for fine-tuning on ImageNet-1K in\nTable 6. The fine-tuning starts from the final model weights\nobtained in pre-training, without using the EMA weights,\neven if in pre-training EMA is used and EMA accuracy is\nreported. This is because we do not observe improvement if\nwe fine-tune with the EMA weights (consistent with observa-\ntions in [73]). The only exception is ConvNeXt-L pre-trained\non ImageNet-1K, where the model accuracy is significantly\nlower than the EMA accuracy due to overfitting, and we\nselect its best EMA model during pre-training as the starting\npoint for fine-tuning."
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 2727
        },
        {
          "x": 2281,
          "y": 2727
        },
        {
          "x": 2281,
          "y": 2975
        },
        {
          "x": 1279,
          "y": 2975
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='127' style='font-size:16px'>In fine-tuning, we use layer-wise learning rate decay [6,<br>12] with every 3 consecutive blocks forming a group. When<br>the model is fine-tuned at 3842 resolution, we use a crop ratio<br>of 1.0 (i.e., no cropping) during testing following [2, 74, 80],<br>instead of 0.875 at 2242.</p>",
      "id": 127,
      "page": 9,
      "text": "In fine-tuning, we use layer-wise learning rate decay [6,\n12] with every 3 consecutive blocks forming a group. When\nthe model is fine-tuned at 3842 resolution, we use a crop ratio\nof 1.0 (i.e., no cropping) during testing following [2, 74, 80],\ninstead of 0.875 at 2242."
    },
    {
      "bounding_box": [
        {
          "x": 198,
          "y": 290
        },
        {
          "x": 1214,
          "y": 290
        },
        {
          "x": 1214,
          "y": 1314
        },
        {
          "x": 198,
          "y": 1314
        }
      ],
      "category": "table",
      "html": "<table id='128' style='font-size:14px'><tr><td>pre-training config</td><td>ConvNeXt-B/L ImageNet-1K 2242</td><td>ConvNeXt-T/S/B/L/XL ImageNet-22K 2242</td></tr><tr><td rowspan=\"2\">fine-tuning config</td><td>ImageNet-1K</td><td>ImageNet-1K</td></tr><tr><td>3842</td><td>2242 and 3842</td></tr><tr><td>optimizer</td><td>AdamW</td><td>AdamW</td></tr><tr><td>base learning rate</td><td>5e-5</td><td>5e-5</td></tr><tr><td>weight decay</td><td>1e-8</td><td>1e-8</td></tr><tr><td>optimizer momentum</td><td>B1, B2=0.9, 0.999</td><td>B1, B2=0.9, 0.999</td></tr><tr><td>batch size</td><td>512</td><td>512</td></tr><tr><td>training epochs</td><td>30</td><td>30</td></tr><tr><td>learning rate schedule</td><td>cosine decay</td><td>cosine decay</td></tr><tr><td>layer-wise lr decay</td><td>0.7</td><td>0.8</td></tr><tr><td>warmup epochs</td><td>None</td><td>None</td></tr><tr><td>warmup schedule</td><td>N/A</td><td>N/A</td></tr><tr><td>randaugment</td><td>(9, 0.5)</td><td>(9, 0.5)</td></tr><tr><td>mixup</td><td>None</td><td>None</td></tr><tr><td>cutmix</td><td>None</td><td>None</td></tr><tr><td>random erasing</td><td>0.25</td><td>0.25</td></tr><tr><td>label smoothing</td><td>0.1</td><td>0.1</td></tr><tr><td>stochastic depth</td><td>0.8/0.95</td><td>0.0/0.1/0.2/0.3/0.4</td></tr><tr><td>layer scale</td><td>pre-trained</td><td>pre-trained</td></tr><tr><td>head init scale</td><td>0.001</td><td>0.001</td></tr><tr><td>gradient clip</td><td>None</td><td>None</td></tr><tr><td>exp. mov. avg. (EMA)</td><td>None</td><td>None(T-L)/0.9999(XL)</td></tr></table>",
      "id": 128,
      "page": 10,
      "text": "pre-training config ConvNeXt-B/L ImageNet-1K 2242 ConvNeXt-T/S/B/L/XL ImageNet-22K 2242\n fine-tuning config ImageNet-1K ImageNet-1K\n 3842 2242 and 3842\n optimizer AdamW AdamW\n base learning rate 5e-5 5e-5\n weight decay 1e-8 1e-8\n optimizer momentum B1, B2=0.9, 0.999 B1, B2=0.9, 0.999\n batch size 512 512\n training epochs 30 30\n learning rate schedule cosine decay cosine decay\n layer-wise lr decay 0.7 0.8\n warmup epochs None None\n warmup schedule N/A N/A\n randaugment (9, 0.5) (9, 0.5)\n mixup None None\n cutmix None None\n random erasing 0.25 0.25\n label smoothing 0.1 0.1\n stochastic depth 0.8/0.95 0.0/0.1/0.2/0.3/0.4\n layer scale pre-trained pre-trained\n head init scale 0.001 0.001\n gradient clip None None\n exp. mov. avg. (EMA) None"
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 1352
        },
        {
          "x": 1201,
          "y": 1352
        },
        {
          "x": 1201,
          "y": 1445
        },
        {
          "x": 202,
          "y": 1445
        }
      ],
      "category": "caption",
      "html": "<caption id='129' style='font-size:18px'>Table 6. ImageNet-1K fine-tuning settings. Multiple values (e.g.,<br>0.8/0.95) are for each model (e.g., ConvNeXt-B/L) respectively.</caption>",
      "id": 129,
      "page": 10,
      "text": "Table 6. ImageNet-1K fine-tuning settings. Multiple values (e.g.,\n0.8/0.95) are for each model (e.g., ConvNeXt-B/L) respectively."
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 1530
        },
        {
          "x": 671,
          "y": 1530
        },
        {
          "x": 671,
          "y": 1579
        },
        {
          "x": 204,
          "y": 1579
        }
      ],
      "category": "paragraph",
      "html": "<p id='130' style='font-size:20px'>A.3. Downstream Tasks</p>",
      "id": 130,
      "page": 10,
      "text": "A.3. Downstream Tasks"
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 1607
        },
        {
          "x": 1202,
          "y": 1607
        },
        {
          "x": 1202,
          "y": 1854
        },
        {
          "x": 202,
          "y": 1854
        }
      ],
      "category": "paragraph",
      "html": "<p id='131' style='font-size:18px'>For ADE20K and COCO experiments, we follow the<br>training settings used in BEiT [6] and Swin [45]. We also<br>use MMDetection [10] and MMSegmentation [13] toolboxes.<br>We use the final model weights (instead of EMA weights)<br>from ImageNet pre-training as network initializations.</p>",
      "id": 131,
      "page": 10,
      "text": "For ADE20K and COCO experiments, we follow the\ntraining settings used in BEiT [6] and Swin [45]. We also\nuse MMDetection [10] and MMSegmentation [13] toolboxes.\nWe use the final model weights (instead of EMA weights)\nfrom ImageNet pre-training as network initializations."
    },
    {
      "bounding_box": [
        {
          "x": 199,
          "y": 1859
        },
        {
          "x": 1202,
          "y": 1859
        },
        {
          "x": 1202,
          "y": 2153
        },
        {
          "x": 199,
          "y": 2153
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='132' style='font-size:20px'>We conduct a lightweight sweep for COCO experiments<br>including learning rate {1e-4, 2e-4}, layer-wise learning rate<br>decay [6] {0.7, 0.8, 0.9, 0.95}, and stochastic depth rate<br>{0.3, 0.4, 0.5, 0.6, 0.7, 0.8}. We fine-tune the ImageNet-22K<br>pre-trained Swin-B/L on COCO using the same sweep. We<br>use the official code and pre-trained model weights [3].</p>",
      "id": 132,
      "page": 10,
      "text": "We conduct a lightweight sweep for COCO experiments\nincluding learning rate {1e-4, 2e-4}, layer-wise learning rate\ndecay [6] {0.7, 0.8, 0.9, 0.95}, and stochastic depth rate\n{0.3, 0.4, 0.5, 0.6, 0.7, 0.8}. We fine-tune the ImageNet-22K\npre-trained Swin-B/L on COCO using the same sweep. We\nuse the official code and pre-trained model weights [3]."
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 2157
        },
        {
          "x": 1202,
          "y": 2157
        },
        {
          "x": 1202,
          "y": 2406
        },
        {
          "x": 200,
          "y": 2406
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='133' style='font-size:20px'>The hyperparameters we sweep for ADE20K experiments<br>include learning rate {8e-5, 1e-4}, layer-wise learning rate<br>decay {0.8, 0.9}, and stochastic depth rate {0.3, 0.4, 0.5}.<br>We report validation mIoU results using multi-scale testing.<br>Additional single-scale testing results are in Table 7.</p>",
      "id": 133,
      "page": 10,
      "text": "The hyperparameters we sweep for ADE20K experiments\ninclude learning rate {8e-5, 1e-4}, layer-wise learning rate\ndecay {0.8, 0.9}, and stochastic depth rate {0.3, 0.4, 0.5}.\nWe report validation mIoU results using multi-scale testing.\nAdditional single-scale testing results are in Table 7."
    },
    {
      "bounding_box": [
        {
          "x": 390,
          "y": 2448
        },
        {
          "x": 1011,
          "y": 2448
        },
        {
          "x": 1011,
          "y": 2881
        },
        {
          "x": 390,
          "y": 2881
        }
      ],
      "category": "table",
      "html": "<table id='134' style='font-size:14px'><tr><td>backbone</td><td>input crop.</td><td>mIoU</td></tr><tr><td colspan=\"3\">ImageNet-1K pre-trained</td></tr><tr><td>● ConvNeXt-T</td><td>5122</td><td>46.0</td></tr><tr><td>● ConvNeXt-S</td><td>5122</td><td>48.7</td></tr><tr><td>● ConvNeXt-B</td><td>5122</td><td>49.1</td></tr><tr><td colspan=\"3\">ImageNet-22K pre-trained</td></tr><tr><td>● ConvNeXt-B±</td><td>6402</td><td>52.6</td></tr><tr><td>· ConvNeXt-L‡</td><td>6402</td><td>53.2</td></tr><tr><td>● ConvNeXt-XL‡</td><td>6402</td><td>53.6</td></tr></table>",
      "id": 134,
      "page": 10,
      "text": "backbone input crop. mIoU\n ImageNet-1K pre-trained\n ● ConvNeXt-T 5122 46.0\n ● ConvNeXt-S 5122 48.7\n ● ConvNeXt-B 5122 49.1\n ImageNet-22K pre-trained\n ● ConvNeXt-B± 6402 52.6\n · ConvNeXt-L‡ 6402 53.2\n ● ConvNeXt-XL‡ 6402"
    },
    {
      "bounding_box": [
        {
          "x": 224,
          "y": 2913
        },
        {
          "x": 1169,
          "y": 2913
        },
        {
          "x": 1169,
          "y": 2961
        },
        {
          "x": 224,
          "y": 2961
        }
      ],
      "category": "caption",
      "html": "<caption id='135' style='font-size:16px'>Table 7. ADE20K validation results with single-scale testing.</caption>",
      "id": 135,
      "page": 10,
      "text": "Table 7. ADE20K validation results with single-scale testing."
    },
    {
      "bounding_box": [
        {
          "x": 1283,
          "y": 299
        },
        {
          "x": 1833,
          "y": 299
        },
        {
          "x": 1833,
          "y": 353
        },
        {
          "x": 1283,
          "y": 353
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='136' style='font-size:22px'>B. Robustness Evaluation</p>",
      "id": 136,
      "page": 10,
      "text": "B. Robustness Evaluation"
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 388
        },
        {
          "x": 2280,
          "y": 388
        },
        {
          "x": 2280,
          "y": 782
        },
        {
          "x": 1278,
          "y": 782
        }
      ],
      "category": "paragraph",
      "html": "<p id='137' style='font-size:18px'>Additional robustness evaluation results for ConvNeXt<br>models are presented in Table 8. We directly test our<br>ImageNet-1K trained/fine-tuned classification models on sev-<br>eral robustness benchmark datasets such as ImageNet-A [33],<br>ImageNet-R [30], ImageNet-Sketch [78] and ImageNet-<br>C/C [31, 48] datasets. We report mean corruption error<br>(mCE) for ImageNet-C, corruption error for ImageNet-C,<br>and top-1 Accuracy for all other datasets.</p>",
      "id": 137,
      "page": 10,
      "text": "Additional robustness evaluation results for ConvNeXt\nmodels are presented in Table 8. We directly test our\nImageNet-1K trained/fine-tuned classification models on sev-\neral robustness benchmark datasets such as ImageNet-A [33],\nImageNet-R [30], ImageNet-Sketch [78] and ImageNet-\nC/C [31, 48] datasets. We report mean corruption error\n(mCE) for ImageNet-C, corruption error for ImageNet-C,\nand top-1 Accuracy for all other datasets."
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 788
        },
        {
          "x": 2280,
          "y": 788
        },
        {
          "x": 2280,
          "y": 1235
        },
        {
          "x": 1279,
          "y": 1235
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='138' style='font-size:18px'>ConvNeXt (in particular the large-scale model variants)<br>exhibits promising robustness behaviors, outperforming<br>state-of-the-art robust transformer models [47] on several<br>benchmarks. With extra ImageNet-22K data, ConvNeXt-<br>XL demonstrates strong domain generalization capabilities<br>(e.g. achieving 69.3%/68.2%/55.0% accuracy on ImageNet-<br>A/R/Sketch benchmarks, respectively). We note that these ro-<br>bustness evaluation results were acquired without using any<br>specialized modules or additional fine-tuning procedures.</p>",
      "id": 138,
      "page": 10,
      "text": "ConvNeXt (in particular the large-scale model variants)\nexhibits promising robustness behaviors, outperforming\nstate-of-the-art robust transformer models [47] on several\nbenchmarks. With extra ImageNet-22K data, ConvNeXt-\nXL demonstrates strong domain generalization capabilities\n(e.g. achieving 69.3%/68.2%/55.0% accuracy on ImageNet-\nA/R/Sketch benchmarks, respectively). We note that these ro-\nbustness evaluation results were acquired without using any\nspecialized modules or additional fine-tuning procedures."
    },
    {
      "bounding_box": [
        {
          "x": 1286,
          "y": 1278
        },
        {
          "x": 2284,
          "y": 1278
        },
        {
          "x": 2284,
          "y": 1752
        },
        {
          "x": 1286,
          "y": 1752
        }
      ],
      "category": "table",
      "html": "<table id='139' style='font-size:14px'><tr><td>Model</td><td>Data/Size</td><td>FLOPs / Params</td><td>Clean</td><td>C(↓)</td><td>c (↓)</td><td>A</td><td>R</td><td>SK</td></tr><tr><td>ResNet-50</td><td>1K/2242</td><td>4.1 /25.6</td><td>76.1</td><td>76.7</td><td>57.7</td><td>0.0</td><td>36.1</td><td>24.1</td></tr><tr><td>Swin-T [45]</td><td>1K/2242</td><td>4.5/28.3</td><td>81.2</td><td>62.0</td><td>-</td><td>21.6</td><td>41.3</td><td>29.1</td></tr><tr><td>RVT-S* [47]</td><td>1K/2242</td><td>4.7/23.3</td><td>81.9</td><td>49.4</td><td>37.5</td><td>25.7</td><td>47.7</td><td>34.7</td></tr><tr><td>ConvNeXt-T</td><td>1K/2242</td><td>4.5/28.6</td><td>82.1</td><td>53.2</td><td>40.0</td><td>24.2</td><td>47.2</td><td>33.8</td></tr><tr><td>Swin-B [45]</td><td>1K/2242</td><td>15.4/87.8</td><td>83.4</td><td>54.4</td><td>-</td><td>35.8</td><td>46.6</td><td>32.4</td></tr><tr><td>RVT-B* [47]</td><td>1K/2242</td><td>17.7/91.8</td><td>82.6</td><td>46.8</td><td>30.8</td><td>28.5</td><td>48.7</td><td>36.0</td></tr><tr><td>ConvNeXt-B</td><td>1K/2242</td><td>15.4 / 88.6</td><td>83.8</td><td>46.8</td><td>34.4</td><td>36.7</td><td>51.3</td><td>38.2</td></tr><tr><td>ConvNeXt-B</td><td>22K/3842</td><td>45.1 / 88.6</td><td>86.8</td><td>43.1</td><td>30.7</td><td>62.3</td><td>64.9</td><td>51.6</td></tr><tr><td>ConvNeXt-L</td><td>22K/3842</td><td>101.0/ 197.8</td><td>87.5</td><td>40.2</td><td>29.9</td><td>65.5</td><td>66.7</td><td>52.8</td></tr><tr><td>ConvNeXt-XL</td><td>22K/3842</td><td>179.0/350.2</td><td>87.8</td><td>38.8</td><td>27.1</td><td>69.3</td><td>68.2</td><td>55.0</td></tr></table>",
      "id": 139,
      "page": 10,
      "text": "Model Data/Size FLOPs / Params Clean C(↓) c (↓) A R SK\n ResNet-50 1K/2242 4.1 /25.6 76.1 76.7 57.7 0.0 36.1 24.1\n Swin-T [45] 1K/2242 4.5/28.3 81.2 62.0 - 21.6 41.3 29.1\n RVT-S* [47] 1K/2242 4.7/23.3 81.9 49.4 37.5 25.7 47.7 34.7\n ConvNeXt-T 1K/2242 4.5/28.6 82.1 53.2 40.0 24.2 47.2 33.8\n Swin-B [45] 1K/2242 15.4/87.8 83.4 54.4 - 35.8 46.6 32.4\n RVT-B* [47] 1K/2242 17.7/91.8 82.6 46.8 30.8 28.5 48.7 36.0\n ConvNeXt-B 1K/2242 15.4 / 88.6 83.8 46.8 34.4 36.7 51.3 38.2\n ConvNeXt-B 22K/3842 45.1 / 88.6 86.8 43.1 30.7 62.3 64.9 51.6\n ConvNeXt-L 22K/3842 101.0/ 197.8 87.5 40.2 29.9 65.5 66.7 52.8\n ConvNeXt-XL 22K/3842 179.0/350.2 87.8 38.8 27.1 69.3 68.2"
    },
    {
      "bounding_box": [
        {
          "x": 1282,
          "y": 1788
        },
        {
          "x": 2279,
          "y": 1788
        },
        {
          "x": 2279,
          "y": 1881
        },
        {
          "x": 1282,
          "y": 1881
        }
      ],
      "category": "caption",
      "html": "<caption id='140' style='font-size:16px'>Table 8. Robustness evaluation of ConvNeXt. We do not make<br>use of any specialized modules or additional fine-tuning procedures.</caption>",
      "id": 140,
      "page": 10,
      "text": "Table 8. Robustness evaluation of ConvNeXt. We do not make\nuse of any specialized modules or additional fine-tuning procedures."
    },
    {
      "bounding_box": [
        {
          "x": 1282,
          "y": 1988
        },
        {
          "x": 2163,
          "y": 1988
        },
        {
          "x": 2163,
          "y": 2044
        },
        {
          "x": 1282,
          "y": 2044
        }
      ],
      "category": "paragraph",
      "html": "<p id='141' style='font-size:22px'>C. Modernizing ResNets: detailed results</p>",
      "id": 141,
      "page": 10,
      "text": "C. Modernizing ResNets: detailed results"
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 2074
        },
        {
          "x": 2280,
          "y": 2074
        },
        {
          "x": 2280,
          "y": 2371
        },
        {
          "x": 1279,
          "y": 2371
        }
      ],
      "category": "paragraph",
      "html": "<p id='142' style='font-size:16px'>Here we provide detailed tabulated results for the mod-<br>ernization experiments, at both ResNet-50 / Swin-T and<br>ResNet-200 / Swin-B regimes. The ImageNet-1K top-1 ac-<br>curacies and FLOPs for each step are shown in Table 10<br>and 1 1. ResNet-50 regime experiments are run with 3 ran-<br>dom seeds.</p>",
      "id": 142,
      "page": 10,
      "text": "Here we provide detailed tabulated results for the mod-\nernization experiments, at both ResNet-50 / Swin-T and\nResNet-200 / Swin-B regimes. The ImageNet-1K top-1 ac-\ncuracies and FLOPs for each step are shown in Table 10\nand 1 1. ResNet-50 regime experiments are run with 3 ran-\ndom seeds."
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 2378
        },
        {
          "x": 2278,
          "y": 2378
        },
        {
          "x": 2278,
          "y": 2723
        },
        {
          "x": 1279,
          "y": 2723
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='143' style='font-size:18px'>For ResNet-200, the initial number of blocks at each stage<br>is (3, 24, 36, 3). We change it to Swin-B's (3, 3, 27, 3) at<br>the step of changing stage ratio. This drastically reduces the<br>FLOPs, SO at the same time, we also increase the width from<br>64 to 84 to keep the FLOPs at a similar level. After the step<br>of adopting depthwise convolutions, we further increase the<br>width to 128 (same as Swin-B's) as a separate step.</p>",
      "id": 143,
      "page": 10,
      "text": "For ResNet-200, the initial number of blocks at each stage\nis (3, 24, 36, 3). We change it to Swin-B's (3, 3, 27, 3) at\nthe step of changing stage ratio. This drastically reduces the\nFLOPs, SO at the same time, we also increase the width from\n64 to 84 to keep the FLOPs at a similar level. After the step\nof adopting depthwise convolutions, we further increase the\nwidth to 128 (same as Swin-B's) as a separate step."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 2729
        },
        {
          "x": 2279,
          "y": 2729
        },
        {
          "x": 2279,
          "y": 2977
        },
        {
          "x": 1280,
          "y": 2977
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='144' style='font-size:18px'>The observations on the ResNet-200 regime are mostly<br>consistent with those on ResNet-50 as described in the main<br>paper. One interesting difference is that inverting dimensions<br>brings a larger improvement at ResNet-200 regime than at<br>ResNet-50 regime (+0.79% vs. +0.14%). The performance</p>",
      "id": 144,
      "page": 10,
      "text": "The observations on the ResNet-200 regime are mostly\nconsistent with those on ResNet-50 as described in the main\npaper. One interesting difference is that inverting dimensions\nbrings a larger improvement at ResNet-200 regime than at\nResNet-50 regime (+0.79% vs. +0.14%). The performance"
    },
    {
      "bounding_box": [
        {
          "x": 490,
          "y": 360
        },
        {
          "x": 1982,
          "y": 360
        },
        {
          "x": 1982,
          "y": 1453
        },
        {
          "x": 490,
          "y": 1453
        }
      ],
      "category": "table",
      "html": "<table id='145' style='font-size:14px'><tr><td></td><td>output size</td><td colspan=\"2\">● ResNet-50</td><td colspan=\"2\">● ConvNeXt-T</td><td colspan=\"2\">○ Swin-T</td></tr><tr><td>stem</td><td>56x56</td><td colspan=\"2\">7x7, 64, stride 2 3x3 max pool, stride 2</td><td colspan=\"2\">4x4, 96, stride 4</td><td colspan=\"2\">4x4, 96, stride 4</td></tr><tr><td>res2</td><td>56x56</td><td>1x1, 64 3x3, 64 1x1, 256」</td><td>x 3</td><td>d7x7, 96 1x1, 384 x 3 1x1, 96</td><td></td><td>1x1, 96x3 MSA, w7x7, H=3, rel. pos. 1x1, 96 1x1, 384 1x1, 96</td><td>x 2</td></tr><tr><td>res3</td><td>28x28</td><td>1x1, 1287 3x3, 128 1x1, 512</td><td>x 4</td><td>「d7x7, 192 1x1, 768 1x1, 192</td><td>x 3</td><td>1x1, 192x3 MSA, w7x7, H=6, rel. pos. 1x1, 192 1x1, 768 1x1, 192</td><td>x 2</td></tr><tr><td>res4</td><td>14x14</td><td>1x1, 256 3x3, 256 1x1, 1024</td><td>x 6</td><td>d7x7, 384 1x1, 1536 1x1, 384</td><td>x 9</td><td>1x1, 384x3 MSA, w7x7, H=12, rel. pos. 1x1, 384 1x1, 1536 1x1, 384</td><td>x 6</td></tr><tr><td>res5</td><td>7x7</td><td>1x1, 512 3x3, 512 1x1, 2048</td><td>x 3</td><td>「d7x7, 768 1x1, 3072 1x1, 768</td><td>x 3</td><td>1x1, 768x3 MSA, w7x7, H=24, rel. pos. 1x1, 768 1x1, 3072 1x1, 768</td><td>x 2</td></tr><tr><td colspan=\"2\">FLOPs</td><td colspan=\"2\">4.1 x 109</td><td colspan=\"2\">4.5 x 109</td><td colspan=\"2\">4.5 x 109</td></tr><tr><td colspan=\"2\"># params.</td><td colspan=\"2\">25.6 x 106</td><td colspan=\"2\">28.6 x 106</td><td colspan=\"2\">28.3 x 106</td></tr></table>",
      "id": 145,
      "page": 11,
      "text": "output size ● ResNet-50 ● ConvNeXt-T ○ Swin-T\n stem 56x56 7x7, 64, stride 2 3x3 max pool, stride 2 4x4, 96, stride 4 4x4, 96, stride 4\n res2 56x56 1x1, 64 3x3, 64 1x1, 256」 x 3 d7x7, 96 1x1, 384 x 3 1x1, 96  1x1, 96x3 MSA, w7x7, H=3, rel. pos. 1x1, 96 1x1, 384 1x1, 96 x 2\n res3 28x28 1x1, 1287 3x3, 128 1x1, 512 x 4 「d7x7, 192 1x1, 768 1x1, 192 x 3 1x1, 192x3 MSA, w7x7, H=6, rel. pos. 1x1, 192 1x1, 768 1x1, 192 x 2\n res4 14x14 1x1, 256 3x3, 256 1x1, 1024 x 6 d7x7, 384 1x1, 1536 1x1, 384 x 9 1x1, 384x3 MSA, w7x7, H=12, rel. pos. 1x1, 384 1x1, 1536 1x1, 384 x 6\n res5 7x7 1x1, 512 3x3, 512 1x1, 2048 x 3 「d7x7, 768 1x1, 3072 1x1, 768 x 3 1x1, 768x3 MSA, w7x7, H=24, rel. pos. 1x1, 768 1x1, 3072 1x1, 768 x 2\n FLOPs 4.1 x 109 4.5 x 109 4.5 x 109\n # params. 25.6 x 106 28.6 x 106"
    },
    {
      "bounding_box": [
        {
          "x": 575,
          "y": 1483
        },
        {
          "x": 1903,
          "y": 1483
        },
        {
          "x": 1903,
          "y": 1532
        },
        {
          "x": 575,
          "y": 1532
        }
      ],
      "category": "caption",
      "html": "<caption id='146' style='font-size:18px'>Table 9. Detailed architecture specifications for ResNet-50, ConvNeXt-T and Swin-T.</caption>",
      "id": 146,
      "page": 11,
      "text": "Table 9. Detailed architecture specifications for ResNet-50, ConvNeXt-T and Swin-T."
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 1597
        },
        {
          "x": 2277,
          "y": 1597
        },
        {
          "x": 2277,
          "y": 2506
        },
        {
          "x": 200,
          "y": 2506
        }
      ],
      "category": "table",
      "html": "<table id='147' style='font-size:14px'><tr><td>model</td><td>IN-1K acc.</td><td>GFLOPs</td><td>model</td><td>IN-1K acc.</td><td>GFLOPs</td></tr><tr><td>ResNet-50 (PyTorch [1]</td><td>76.13</td><td>4.09</td><td>ResNet-200 [29]</td><td>78.20</td><td>15.01</td></tr><tr><td>ResNet-50 (enhanced recipe)</td><td>78.82 士 0.07</td><td>4.09</td><td>ResNet-200 (enhanced recipe)</td><td>81.14</td><td>15.01</td></tr><tr><td>stage ratio</td><td>79.36 土 0.07</td><td>4.53</td><td>stage ratio and increase width</td><td>81.33</td><td>14.52</td></tr><tr><td>\"patchify\" stem</td><td>79.51 士 0.18</td><td>4.42</td><td>\"patchify\" stem</td><td>81.59</td><td>14.38</td></tr><tr><td>depthwise conv</td><td>78.28 土 0.08</td><td>2.35</td><td>depthwise conv</td><td>80.54</td><td>7.23</td></tr><tr><td>increase width</td><td>80.50 土 0.02</td><td>5.27</td><td>increase width</td><td>81.85</td><td>16.76</td></tr><tr><td>inverting dimensions</td><td>80.64 土 0.03</td><td>4.64</td><td>inverting dimensions</td><td>82.64</td><td>15.68</td></tr><tr><td>move up depthwise conv</td><td>79.92 士 0.08</td><td>4.07</td><td>move up depthwise conv</td><td>82.04</td><td>14.63</td></tr><tr><td>kernel size → 5</td><td>80.35 土 0.08</td><td>4.10</td><td>kernel size → 5</td><td>82.32</td><td>14.70</td></tr><tr><td>kernel size → 7</td><td>80.57 土 0.14</td><td>4.15</td><td>kernel size → 7</td><td>82.30</td><td>14.81</td></tr><tr><td>kernel size → 9</td><td>80.57 士 0.06</td><td>4.21</td><td>kernel size → 9</td><td>82.27</td><td>14.95</td></tr><tr><td>kernel size → 11</td><td>80.47 土 0.11</td><td>4.29</td><td>kernel size → 11</td><td>82.18</td><td>15.13</td></tr><tr><td>ReLU → GELU</td><td>80.62 土 0.14</td><td>4.15</td><td>ReLU → GELU</td><td>82.19</td><td>14.81</td></tr><tr><td>fewer activations</td><td>81.27 士 0.06</td><td>4.15</td><td>fewer activations</td><td>82.71</td><td>14.81</td></tr><tr><td>fewer norms</td><td>81.41 士 0.09</td><td>4.15</td><td>fewer norms</td><td>83.17</td><td>14.81</td></tr><tr><td>BN → LN</td><td>81.47 土 0.09</td><td>4.46</td><td>BN → LN</td><td>83.35</td><td>14.81</td></tr><tr><td>separate d.s. conv (ConvNeXt-T)</td><td>81.97 土 0.06</td><td>4.49</td><td>separate d.s. conv (ConvNeXt-B)</td><td>83.60</td><td>15.35</td></tr><tr><td>Swin-T [45]</td><td>81.30</td><td>4.50</td><td>Swin-B [45]</td><td>83.50</td><td>15.43</td></tr></table>",
      "id": 147,
      "page": 11,
      "text": "model IN-1K acc. GFLOPs model IN-1K acc. GFLOPs\n ResNet-50 (PyTorch [1] 76.13 4.09 ResNet-200 [29] 78.20 15.01\n ResNet-50 (enhanced recipe) 78.82 士 0.07 4.09 ResNet-200 (enhanced recipe) 81.14 15.01\n stage ratio 79.36 土 0.07 4.53 stage ratio and increase width 81.33 14.52\n \"patchify\" stem 79.51 士 0.18 4.42 \"patchify\" stem 81.59 14.38\n depthwise conv 78.28 土 0.08 2.35 depthwise conv 80.54 7.23\n increase width 80.50 土 0.02 5.27 increase width 81.85 16.76\n inverting dimensions 80.64 土 0.03 4.64 inverting dimensions 82.64 15.68\n move up depthwise conv 79.92 士 0.08 4.07 move up depthwise conv 82.04 14.63\n kernel size → 5 80.35 土 0.08 4.10 kernel size → 5 82.32 14.70\n kernel size → 7 80.57 土 0.14 4.15 kernel size → 7 82.30 14.81\n kernel size → 9 80.57 士 0.06 4.21 kernel size → 9 82.27 14.95\n kernel size → 11 80.47 土 0.11 4.29 kernel size → 11 82.18 15.13\n ReLU → GELU 80.62 土 0.14 4.15 ReLU → GELU 82.19 14.81\n fewer activations 81.27 士 0.06 4.15 fewer activations 82.71 14.81\n fewer norms 81.41 士 0.09 4.15 fewer norms 83.17 14.81\n BN → LN 81.47 土 0.09 4.46 BN → LN 83.35 14.81\n separate d.s. conv (ConvNeXt-T) 81.97 土 0.06 4.49 separate d.s. conv (ConvNeXt-B) 83.60 15.35\n Swin-T [45] 81.30 4.50 Swin-B [45] 83.50"
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 2530
        },
        {
          "x": 1199,
          "y": 2530
        },
        {
          "x": 1199,
          "y": 2667
        },
        {
          "x": 202,
          "y": 2667
        }
      ],
      "category": "caption",
      "html": "<caption id='148' style='font-size:16px'>Table 10. Detailed results for modernizing a ResNet-50. Mean<br>and standard deviation are obtained by training the network with<br>three different random seeds.</caption>",
      "id": 148,
      "page": 11,
      "text": "Table 10. Detailed results for modernizing a ResNet-50. Mean\nand standard deviation are obtained by training the network with\nthree different random seeds."
    },
    {
      "bounding_box": [
        {
          "x": 1328,
          "y": 2531
        },
        {
          "x": 2225,
          "y": 2531
        },
        {
          "x": 2225,
          "y": 2575
        },
        {
          "x": 1328,
          "y": 2575
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='149' style='font-size:16px'>Table 11. Detailed results for modernizing a ResNet-200.</p>",
      "id": 149,
      "page": 11,
      "text": "Table 11. Detailed results for modernizing a ResNet-200."
    },
    {
      "bounding_box": [
        {
          "x": 199,
          "y": 2778
        },
        {
          "x": 1200,
          "y": 2778
        },
        {
          "x": 1200,
          "y": 2973
        },
        {
          "x": 199,
          "y": 2973
        }
      ],
      "category": "paragraph",
      "html": "<p id='150' style='font-size:20px'>gained by increasing kernel size also seems to saturate at<br>kernel size 5 instead of 7. Using fewer normalization layers<br>also has a bigger gain compared with the ResNet-50 regime<br>(+0.46% vs. +0.14%).</p>",
      "id": 150,
      "page": 11,
      "text": "gained by increasing kernel size also seems to saturate at\nkernel size 5 instead of 7. Using fewer normalization layers\nalso has a bigger gain compared with the ResNet-50 regime\n(+0.46% vs. +0.14%)."
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 2682
        },
        {
          "x": 1830,
          "y": 2682
        },
        {
          "x": 1830,
          "y": 2736
        },
        {
          "x": 1281,
          "y": 2736
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='151' style='font-size:22px'>D. Detailed Architectures</p>",
      "id": 151,
      "page": 11,
      "text": "D. Detailed Architectures"
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 2777
        },
        {
          "x": 2281,
          "y": 2777
        },
        {
          "x": 2281,
          "y": 2976
        },
        {
          "x": 1279,
          "y": 2976
        }
      ],
      "category": "paragraph",
      "html": "<p id='152' style='font-size:20px'>We present a detailed architecture comparison between<br>ResNet-50, ConvNeXt-T and Swin-T in Table 9. For differ-<br>ently sized ConvNeXts, only the number of blocks and the<br>number of channels at each stage differ from ConvNeXt-T</p>",
      "id": 152,
      "page": 11,
      "text": "We present a detailed architecture comparison between\nResNet-50, ConvNeXt-T and Swin-T in Table 9. For differ-\nently sized ConvNeXts, only the number of blocks and the\nnumber of channels at each stage differ from ConvNeXt-T"
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 308
        },
        {
          "x": 1203,
          "y": 308
        },
        {
          "x": 1203,
          "y": 455
        },
        {
          "x": 202,
          "y": 455
        }
      ],
      "category": "paragraph",
      "html": "<p id='153' style='font-size:20px'>(see Section 3 for details). ConvNeXts enjoy the simplic-<br>ity of standard ConvNets, but compete favorably with Swin<br>Transformers in visual recognition.</p>",
      "id": 153,
      "page": 12,
      "text": "(see Section 3 for details). ConvNeXts enjoy the simplic-\nity of standard ConvNets, but compete favorably with Swin\nTransformers in visual recognition."
    },
    {
      "bounding_box": [
        {
          "x": 205,
          "y": 497
        },
        {
          "x": 910,
          "y": 497
        },
        {
          "x": 910,
          "y": 549
        },
        {
          "x": 205,
          "y": 549
        }
      ],
      "category": "paragraph",
      "html": "<p id='154' style='font-size:20px'>E. Benchmarking on A100 GPUs</p>",
      "id": 154,
      "page": 12,
      "text": "E. Benchmarking on A100 GPUs"
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 582
        },
        {
          "x": 1202,
          "y": 582
        },
        {
          "x": 1202,
          "y": 978
        },
        {
          "x": 202,
          "y": 978
        }
      ],
      "category": "paragraph",
      "html": "<p id='155' style='font-size:18px'>Following Swin Transformer [45], the ImageNet models'<br>inference throughputs in Table 1 are benchmarked using a<br>V100 GPU, where ConvNeXt is slightly faster in inference<br>than Swin Transformer with a similar number of parameters.<br>We now benchmark them on the more advanced A100 GPUs,<br>which support the TensorFloat32 (TF32) tensor cores. We<br>employ PyTorch [50] version 1.10 to use the latest \"Channel<br>Last\" memory layout [22] for further speedup.</p>",
      "id": 155,
      "page": 12,
      "text": "Following Swin Transformer [45], the ImageNet models'\ninference throughputs in Table 1 are benchmarked using a\nV100 GPU, where ConvNeXt is slightly faster in inference\nthan Swin Transformer with a similar number of parameters.\nWe now benchmark them on the more advanced A100 GPUs,\nwhich support the TensorFloat32 (TF32) tensor cores. We\nemploy PyTorch [50] version 1.10 to use the latest \"Channel\nLast\" memory layout [22] for further speedup."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 982
        },
        {
          "x": 1202,
          "y": 982
        },
        {
          "x": 1202,
          "y": 1328
        },
        {
          "x": 202,
          "y": 1328
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='156' style='font-size:18px'>We present the results in Table 12. Swin Transformers and<br>ConvNeXts both achieve faster inference throughput than<br>V100 GPUs, but ConvNeXts' advantage is now significantly<br>greater, sometimes up to 49% faster. This preliminary study<br>shows promising signals that ConvNeXt, employed with<br>standard ConvNet modules and simple in design, could be<br>practically more efficient models on modern hardwares.</p>",
      "id": 156,
      "page": 12,
      "text": "We present the results in Table 12. Swin Transformers and\nConvNeXts both achieve faster inference throughput than\nV100 GPUs, but ConvNeXts' advantage is now significantly\ngreater, sometimes up to 49% faster. This preliminary study\nshows promising signals that ConvNeXt, employed with\nstandard ConvNet modules and simple in design, could be\npractically more efficient models on modern hardwares."
    },
    {
      "bounding_box": [
        {
          "x": 206,
          "y": 1387
        },
        {
          "x": 1224,
          "y": 1387
        },
        {
          "x": 1224,
          "y": 2127
        },
        {
          "x": 206,
          "y": 2127
        }
      ],
      "category": "table",
      "html": "<table id='157' style='font-size:14px'><tr><td>model</td><td>image size</td><td>FLOPs</td><td>throughput (image / s)</td><td>IN-1K / 22K trained, 1K acc.</td></tr><tr><td>○ Swin-T</td><td>2242</td><td>4.5G</td><td>1325.6</td><td>81.3 / -</td></tr><tr><td>ConvNeXt-T</td><td>2242</td><td>4.5G</td><td>1943.5 (+47%)</td><td>82.1 / -</td></tr><tr><td>Swin-S</td><td>2242</td><td>8.7G</td><td>857.3</td><td>83.0/ -</td></tr><tr><td>ConvNeXt-S</td><td>2242</td><td>8.7G</td><td>1275.3 (+49%)</td><td>83.1 / -</td></tr><tr><td>Swin-B</td><td>2242</td><td>15.4G</td><td>662.8</td><td>83.5 / 85.2</td></tr><tr><td>ConvNeXt-B</td><td>2242</td><td>15.4G</td><td>969.0 (+46%)</td><td>83.8 / 85.8</td></tr><tr><td>Swin-B</td><td>3842</td><td>47.1G</td><td>242.5</td><td>84.5 / 86.4</td></tr><tr><td>ConvNeXt-B</td><td>3842</td><td>45.0G</td><td>336.6 (+39%)</td><td>85.1 / 86.8</td></tr><tr><td>Swin-L</td><td>2242</td><td>34.5G</td><td>435.9</td><td>- / 86.3</td></tr><tr><td>ConvNeXt-L</td><td>2242</td><td>34.4G</td><td>611.5 (+40%)</td><td>84.3 / 86.6</td></tr><tr><td>○ Swin-L</td><td>3842</td><td>103.9G</td><td>157.9</td><td>- /87.3</td></tr><tr><td>· ConvNeXt-L</td><td>3842</td><td>101.0G</td><td>211.4 (+34%)</td><td>85.5 / 87.5</td></tr><tr><td>● ConvNeXt-XL</td><td>2242</td><td>60.9G</td><td>424.4</td><td>- /87.0</td></tr><tr><td>● ConvNeXt-XL</td><td>3842</td><td>179.0G</td><td>147.4</td><td>- / 87.8</td></tr></table>",
      "id": 157,
      "page": 12,
      "text": "model image size FLOPs throughput (image / s) IN-1K / 22K trained, 1K acc.\n ○ Swin-T 2242 4.5G 1325.6 81.3 / -\n ConvNeXt-T 2242 4.5G 1943.5 (+47%) 82.1 / -\n Swin-S 2242 8.7G 857.3 83.0/ -\n ConvNeXt-S 2242 8.7G 1275.3 (+49%) 83.1 / -\n Swin-B 2242 15.4G 662.8 83.5 / 85.2\n ConvNeXt-B 2242 15.4G 969.0 (+46%) 83.8 / 85.8\n Swin-B 3842 47.1G 242.5 84.5 / 86.4\n ConvNeXt-B 3842 45.0G 336.6 (+39%) 85.1 / 86.8\n Swin-L 2242 34.5G 435.9 - / 86.3\n ConvNeXt-L 2242 34.4G 611.5 (+40%) 84.3 / 86.6\n ○ Swin-L 3842 103.9G 157.9 - /87.3\n · ConvNeXt-L 3842 101.0G 211.4 (+34%) 85.5 / 87.5\n ● ConvNeXt-XL 2242 60.9G 424.4 - /87.0\n ● ConvNeXt-XL 3842 179.0G 147.4"
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 2164
        },
        {
          "x": 1203,
          "y": 2164
        },
        {
          "x": 1203,
          "y": 2347
        },
        {
          "x": 203,
          "y": 2347
        }
      ],
      "category": "caption",
      "html": "<caption id='158' style='font-size:14px'>Table 12. Inference throughput comparisons on an A100 GPU.<br>Using TF32 data format and \"channel last\" memory layout, Con-<br>vNeXt enjoys up to ~49% higher throughput compared with a<br>Swin Transformer with similar FLOPs.</caption>",
      "id": 158,
      "page": 12,
      "text": "Table 12. Inference throughput comparisons on an A100 GPU.\nUsing TF32 data format and \"channel last\" memory layout, Con-\nvNeXt enjoys up to ~49% higher throughput compared with a\nSwin Transformer with similar FLOPs."
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 2442
        },
        {
          "x": 515,
          "y": 2442
        },
        {
          "x": 515,
          "y": 2497
        },
        {
          "x": 204,
          "y": 2497
        }
      ],
      "category": "paragraph",
      "html": "<p id='159' style='font-size:22px'>F. Limitations</p>",
      "id": 159,
      "page": 12,
      "text": "F. Limitations"
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 2528
        },
        {
          "x": 1202,
          "y": 2528
        },
        {
          "x": 1202,
          "y": 2978
        },
        {
          "x": 202,
          "y": 2978
        }
      ],
      "category": "paragraph",
      "html": "<p id='160' style='font-size:16px'>We demonstrate ConvNeXt, a pure ConvNet model, can<br>perform as good as a hierarchical vision Transformer on<br>image classification, object detection, instance and semantic<br>segmentation tasks. While our goal is to offer a broad range<br>of evaluation tasks, we recognize computer vision applica-<br>tions are even more diverse. ConvNeXt may be more suited<br>for certain tasks, while Transformers may be more flexible<br>for others. A case in point is multi-modal learning, in which<br>a cross-attention module may be preferable for modeling</p>",
      "id": 160,
      "page": 12,
      "text": "We demonstrate ConvNeXt, a pure ConvNet model, can\nperform as good as a hierarchical vision Transformer on\nimage classification, object detection, instance and semantic\nsegmentation tasks. While our goal is to offer a broad range\nof evaluation tasks, we recognize computer vision applica-\ntions are even more diverse. ConvNeXt may be more suited\nfor certain tasks, while Transformers may be more flexible\nfor others. A case in point is multi-modal learning, in which\na cross-attention module may be preferable for modeling"
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 306
        },
        {
          "x": 2280,
          "y": 306
        },
        {
          "x": 2280,
          "y": 555
        },
        {
          "x": 1279,
          "y": 555
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='161' style='font-size:16px'>feature interactions across many modalities. Additionally,<br>Transformers may be more flexible when used for tasks re-<br>quiring discretized, sparse, or structured outputs. We believe<br>the architecture choice should meet the needs of the task at<br>hand while striving for simplicity.</p>",
      "id": 161,
      "page": 12,
      "text": "feature interactions across many modalities. Additionally,\nTransformers may be more flexible when used for tasks re-\nquiring discretized, sparse, or structured outputs. We believe\nthe architecture choice should meet the needs of the task at\nhand while striving for simplicity."
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 605
        },
        {
          "x": 1684,
          "y": 605
        },
        {
          "x": 1684,
          "y": 657
        },
        {
          "x": 1279,
          "y": 657
        }
      ],
      "category": "paragraph",
      "html": "<p id='162' style='font-size:22px'>G. Societal Impact</p>",
      "id": 162,
      "page": 12,
      "text": "G. Societal Impact"
    },
    {
      "bounding_box": [
        {
          "x": 1277,
          "y": 689
        },
        {
          "x": 2282,
          "y": 689
        },
        {
          "x": 2282,
          "y": 1788
        },
        {
          "x": 1277,
          "y": 1788
        }
      ],
      "category": "paragraph",
      "html": "<p id='163' style='font-size:18px'>In the 2020s, research on visual representation learn-<br>ing began to place enormous demands on computing re-<br>sources. While larger models and datasets improve per-<br>formance across the board, they also introduce a slew of<br>challenges. ViT, Swin, and ConvNeXt all perform best with<br>their huge model variants. Investigating those model designs<br>inevitably results in an increase in carbon emissions. One<br>important direction, and a motivation for our paper, is to<br>strive for simplicity - with more sophisticated modules,<br>the network's design space expands enormously, obscuring<br>critical components that contribute to the performance dif-<br>ference. Additionally, large models and datasets present<br>issues in terms of model robustness and fairness. Further<br>investigation on the robustness behavior of ConvNeXt VS.<br>Transformer will be an interesting research direction. In<br>terms of data, our findings indicate that ConvNeXt models<br>benefit from pre-training on large-scale datasets. While our<br>method makes use of the publicly available ImageNet-22K<br>dataset, individuals may wish to acquire their own data for<br>pre-training. A more circumspect and responsible approach<br>to data selection is required to avoid potential concerns with<br>data biases.</p>",
      "id": 163,
      "page": 12,
      "text": "In the 2020s, research on visual representation learn-\ning began to place enormous demands on computing re-\nsources. While larger models and datasets improve per-\nformance across the board, they also introduce a slew of\nchallenges. ViT, Swin, and ConvNeXt all perform best with\ntheir huge model variants. Investigating those model designs\ninevitably results in an increase in carbon emissions. One\nimportant direction, and a motivation for our paper, is to\nstrive for simplicity - with more sophisticated modules,\nthe network's design space expands enormously, obscuring\ncritical components that contribute to the performance dif-\nference. Additionally, large models and datasets present\nissues in terms of model robustness and fairness. Further\ninvestigation on the robustness behavior of ConvNeXt VS.\nTransformer will be an interesting research direction. In\nterms of data, our findings indicate that ConvNeXt models\nbenefit from pre-training on large-scale datasets. While our\nmethod makes use of the publicly available ImageNet-22K\ndataset, individuals may wish to acquire their own data for\npre-training. A more circumspect and responsible approach\nto data selection is required to avoid potential concerns with\ndata biases."
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 1837
        },
        {
          "x": 1523,
          "y": 1837
        },
        {
          "x": 1523,
          "y": 1888
        },
        {
          "x": 1281,
          "y": 1888
        }
      ],
      "category": "paragraph",
      "html": "<p id='164' style='font-size:20px'>References</p>",
      "id": 164,
      "page": 12,
      "text": "References"
    },
    {
      "bounding_box": [
        {
          "x": 1299,
          "y": 1921
        },
        {
          "x": 2286,
          "y": 1921
        },
        {
          "x": 2286,
          "y": 2976
        },
        {
          "x": 1299,
          "y": 2976
        }
      ],
      "category": "paragraph",
      "html": "<p id='165' style='font-size:16px'>[1] PyTorch Vision Models. https : / / pytorch · org /<br>vision / stable/ models html. Accessed: 2021-10-<br>01.<br>[2] GitHub repository: Swin transformer. https : / /github.<br>com/mi crosoft / Swin-Transformer, 2021.<br>[3] GitHub repository: Swin transformer for object detection.<br>https : / / github · com/ SwinTransformer / Swin-<br>Transformer-Object-Detection, 2021.<br>[4] Anonymous. Patches are all you need? Openreview, 2021.<br>[5] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.<br>Layer normalization. arXiv:1607.06450, 2016.<br>[6] Hangbo Bao, Li Dong, and Furu Wei. BEiT: BERT pre-<br>training of image transformers. arXiv:2106.08254, 2021.<br>[7] Irwan Bello, William Fedus, Xianzhi Du, Ekin Dogus Cubuk,<br>Aravind Srinivas, Tsung- Yi Lin, Jonathon Shlens, and Barret<br>Zoph. Revisiting resnets: Improved training and scaling<br>strategies. NeurIPS, 2021.<br>[8] Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens,<br>and Quoc V Le. Attention augmented convolutional networks.<br>In ICCV, 2019.<br>[9] Zhaowei Cai and Nuno Vasconcelos. Cascade R-CNN: Delv-<br>ing into high quality object detection. In CVPR, 2018.</p>",
      "id": 165,
      "page": 12,
      "text": "[1] PyTorch Vision Models. https : / / pytorch · org /\nvision / stable/ models html. Accessed: 2021-10-\n01.\n[2] GitHub repository: Swin transformer. https : / /github.\ncom/mi crosoft / Swin-Transformer, 2021.\n[3] GitHub repository: Swin transformer for object detection.\nhttps : / / github · com/ SwinTransformer / Swin-\nTransformer-Object-Detection, 2021.\n[4] Anonymous. Patches are all you need? Openreview, 2021.\n[5] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.\nLayer normalization. arXiv:1607.06450, 2016.\n[6] Hangbo Bao, Li Dong, and Furu Wei. BEiT: BERT pre-\ntraining of image transformers. arXiv:2106.08254, 2021.\n[7] Irwan Bello, William Fedus, Xianzhi Du, Ekin Dogus Cubuk,\nAravind Srinivas, Tsung- Yi Lin, Jonathon Shlens, and Barret\nZoph. Revisiting resnets: Improved training and scaling\nstrategies. NeurIPS, 2021.\n[8] Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens,\nand Quoc V Le. Attention augmented convolutional networks.\nIn ICCV, 2019.\n[9] Zhaowei Cai and Nuno Vasconcelos. Cascade R-CNN: Delv-\ning into high quality object detection. In CVPR, 2018."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 294
        },
        {
          "x": 1204,
          "y": 294
        },
        {
          "x": 1204,
          "y": 2974
        },
        {
          "x": 202,
          "y": 2974
        }
      ],
      "category": "paragraph",
      "html": "<p id='166' style='font-size:14px'>[10] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu<br>Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu,<br>Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tian-<br>heng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue<br>Wu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang,<br>Chen Change Loy, and Dahua Lin. MMDetection: Open<br>mmlab detection toolbox and benchmark. arXiv:1906.07155,<br>2019.<br>[11] Fran�ois Chollet. Xception: Deep learning with depthwise<br>separable convolutions. In CVPR, 2017.<br>[12] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christo-<br>pher D Manning. ELECTRA: Pre-training text encoders as<br>discriminators rather than generators. In ICLR, 2020.<br>[13] MMSegmentation contributors. MMSegmentation: Openmm-<br>lab semantic segmentation toolbox and benchmark. https:<br>/ / github · com / open - mmlab / mmsegmentation,<br>2020.<br>[14] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V<br>Le. Randaugment: Practical automated data augmentation<br>with a reduced search space. In CVPR Workshops, 2020.<br>[15] Zihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan.<br>Coatnet: Marrying convolution and attention for all data sizes.<br>NeurIPS, 2021.<br>[16] Stephane d' Ascoli, Hugo Touvron, Matthew Leavitt, Ari Mor-<br>cos, Giulio Biroli, and Levent Sagun. ConViT: Improving<br>vision transformers with soft convolutional inductive biases.<br>ICML, 2021.<br>[17] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li<br>Fei-Fei. ImageNet: A large-scale hierarchical image database.<br>In CVPR, 2009.<br>[18] Jacob Devlin, Ming- Wei Chang, Kenton Lee, and Kristina<br>Toutanova. BERT: Pre-training of deep bidirectional trans-<br>formers for language understanding. In NAACL, 2019.<br>[19] Piotr Dollar, Serge Belongie, and Pietro Perona. The fastest<br>pedestrian detector in the west. In BMVC, 2010.<br>[20] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,<br>Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,<br>Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-<br>vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is<br>worth 16x16 words: Transformers for image recognition at<br>scale. In ICLR, 2021.<br>[21] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li,<br>Zhicheng Yan, Jitendra Malik, and Christoph Feichtenhofer.<br>Multiscale vision transformers. ICCV, 2021.<br>[22] Vitaly Fedyunin. Tutorial: Channel last memory format<br>in PyTorch. https : / / pytorch · org / tutorials /<br>internediate/memory_format_tutorial html,<br>2021. Accessed: 2021-10-01.<br>[23] Ross Girshick. Fast R-CNN. In ICCV, 2015.<br>[24] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra<br>Malik. Rich feature hierarchies for accurate object detection<br>and semantic segmentation. In CVPR, 2014.<br>[25] Qi Han, Zejia Fan, Qi Dai, Lei Sun, Ming-Ming Cheng, Ji-<br>aying Liu, and Jingdong Wang. Demystifying local vision<br>transformer: Sparse connectivity, weight sharing, and dy-<br>namic weight. arXiv:2106.04263, 2021.</p>",
      "id": 166,
      "page": 13,
      "text": "[10] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu\nXiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu,\nJiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tian-\nheng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue\nWu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang,\nChen Change Loy, and Dahua Lin. MMDetection: Open\nmmlab detection toolbox and benchmark. arXiv:1906.07155,\n2019.\n[11] Fran�ois Chollet. Xception: Deep learning with depthwise\nseparable convolutions. In CVPR, 2017.\n[12] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christo-\npher D Manning. ELECTRA: Pre-training text encoders as\ndiscriminators rather than generators. In ICLR, 2020.\n[13] MMSegmentation contributors. MMSegmentation: Openmm-\nlab semantic segmentation toolbox and benchmark. https:\n/ / github · com / open - mmlab / mmsegmentation,\n2020.\n[14] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V\nLe. Randaugment: Practical automated data augmentation\nwith a reduced search space. In CVPR Workshops, 2020.\n[15] Zihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan.\nCoatnet: Marrying convolution and attention for all data sizes.\nNeurIPS, 2021.\n[16] Stephane d' Ascoli, Hugo Touvron, Matthew Leavitt, Ari Mor-\ncos, Giulio Biroli, and Levent Sagun. ConViT: Improving\nvision transformers with soft convolutional inductive biases.\nICML, 2021.\n[17] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li\nFei-Fei. ImageNet: A large-scale hierarchical image database.\nIn CVPR, 2009.\n[18] Jacob Devlin, Ming- Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: Pre-training of deep bidirectional trans-\nformers for language understanding. In NAACL, 2019.\n[19] Piotr Dollar, Serge Belongie, and Pietro Perona. The fastest\npedestrian detector in the west. In BMVC, 2010.\n[20] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. In ICLR, 2021.\n[21] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li,\nZhicheng Yan, Jitendra Malik, and Christoph Feichtenhofer.\nMultiscale vision transformers. ICCV, 2021.\n[22] Vitaly Fedyunin. Tutorial: Channel last memory format\nin PyTorch. https : / / pytorch · org / tutorials /\ninternediate/memory_format_tutorial html,\n2021. Accessed: 2021-10-01.\n[23] Ross Girshick. Fast R-CNN. In ICCV, 2015.\n[24] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra\nMalik. Rich feature hierarchies for accurate object detection\nand semantic segmentation. In CVPR, 2014.\n[25] Qi Han, Zejia Fan, Qi Dai, Lei Sun, Ming-Ming Cheng, Ji-\naying Liu, and Jingdong Wang. Demystifying local vision\ntransformer: Sparse connectivity, weight sharing, and dy-\nnamic weight. arXiv:2106.04263, 2021."
    },
    {
      "bounding_box": [
        {
          "x": 1276,
          "y": 300
        },
        {
          "x": 2292,
          "y": 300
        },
        {
          "x": 2292,
          "y": 2978
        },
        {
          "x": 1276,
          "y": 2978
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='167' style='font-size:14px'>[26] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr<br>Dollar, and Ross Girshick. Masked autoencoders are scalable<br>vision learners. arXiv:2111.06377, 2021.<br>[27] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Gir-<br>shick. Mask R-CNN. In ICCV, 2017.<br>[28] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.<br>Deep residual learning for image recognition. In CVPR, 2016.<br>[29] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.<br>Identity mappings in deep residual networks. In ECCV, 2016.<br>[30] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kada-<br>vath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu,<br>Samyak Parajuli, Mike Guo, et al. The many faces of robust-<br>ness: A critical analysis of out-of-distribution generalization.<br>In ICCV, 2021.<br>[31] Dan Hendrycks and Thomas Dietterich. Benchmarking neural<br>network robustness to common corruptions and perturbations.<br>In ICLR, 2018.<br>[32] Dan Hendrycks and Kevin Gimpel. Gaussian error linear<br>units (gelus). arXiv: 1606.08415, 2016.<br>[33] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt,<br>and Dawn Song. Natural adversarial examples. In CVPR,<br>2021.<br>[34] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry<br>Kalenichenko, Weijun Wang, Tobias Weyand, Marco An-<br>dreetto, and Hartwig Adam. MobileNets: Efficient con-<br>volutional neural networks for mobile vision applications.<br>arXiv:1704.04861, 2017.<br>[35] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation<br>networks. In CVPR, 2018.<br>[36] Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kil-<br>ian Q Weinberger. Densely connected convolutional networks.<br>In CVPR, 2017.<br>[37] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q<br>Weinberger. Deep networks with stochastic depth. In ECCV,<br>2016.<br>[38] Sergey Ioffe. Batch renormalization: Towards reducing mini-<br>batch dependence in batch-normalized models. In NeurIPS,<br>2017.<br>[39] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan<br>Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby.<br>Big Transfer (BiT): General visual representation learning. In<br>ECCV, 2020.<br>[40] Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton. Imagenet<br>classification with deep convolutional neural networks. In<br>NeurIPS, 2012.<br>[41] Andrew Lavin and Scott Gray. Fast algorithms for convolu-<br>tional neural networks. In CVPR, 2016.<br>[42] Yann LeCun, Bernhard Boser, John S Denker, Donnie Hen-<br>derson, Richard E Howard, Wayne Hubbard, and Lawrence D<br>Jackel. Backpropagation applied to handwritten zip code<br>recognition. Neural computation, 1989.<br>[43] Yann LeCun, Leon Bottou, Yoshua Bengio, Patrick Haffner,<br>et al. Gradient-based learning applied to document recogni-<br>tion. Proceedings of the IEEE, 1998.<br>[44] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,<br>Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence<br>Zitnick. Microsoft COCO: Common objects in context. In<br>ECCV. 2014.</p>",
      "id": 167,
      "page": 13,
      "text": "[26] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDollar, and Ross Girshick. Masked autoencoders are scalable\nvision learners. arXiv:2111.06377, 2021.\n[27] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Gir-\nshick. Mask R-CNN. In ICCV, 2017.\n[28] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR, 2016.\n[29] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nIdentity mappings in deep residual networks. In ECCV, 2016.\n[30] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kada-\nvath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu,\nSamyak Parajuli, Mike Guo, et al. The many faces of robust-\nness: A critical analysis of out-of-distribution generalization.\nIn ICCV, 2021.\n[31] Dan Hendrycks and Thomas Dietterich. Benchmarking neural\nnetwork robustness to common corruptions and perturbations.\nIn ICLR, 2018.\n[32] Dan Hendrycks and Kevin Gimpel. Gaussian error linear\nunits (gelus). arXiv: 1606.08415, 2016.\n[33] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt,\nand Dawn Song. Natural adversarial examples. In CVPR,\n2021.\n[34] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry\nKalenichenko, Weijun Wang, Tobias Weyand, Marco An-\ndreetto, and Hartwig Adam. MobileNets: Efficient con-\nvolutional neural networks for mobile vision applications.\narXiv:1704.04861, 2017.\n[35] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation\nnetworks. In CVPR, 2018.\n[36] Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kil-\nian Q Weinberger. Densely connected convolutional networks.\nIn CVPR, 2017.\n[37] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q\nWeinberger. Deep networks with stochastic depth. In ECCV,\n2016.\n[38] Sergey Ioffe. Batch renormalization: Towards reducing mini-\nbatch dependence in batch-normalized models. In NeurIPS,\n2017.\n[39] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan\nPuigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby.\nBig Transfer (BiT): General visual representation learning. In\nECCV, 2020.\n[40] Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton. Imagenet\nclassification with deep convolutional neural networks. In\nNeurIPS, 2012.\n[41] Andrew Lavin and Scott Gray. Fast algorithms for convolu-\ntional neural networks. In CVPR, 2016.\n[42] Yann LeCun, Bernhard Boser, John S Denker, Donnie Hen-\nderson, Richard E Howard, Wayne Hubbard, and Lawrence D\nJackel. Backpropagation applied to handwritten zip code\nrecognition. Neural computation, 1989.\n[43] Yann LeCun, Leon Bottou, Yoshua Bengio, Patrick Haffner,\net al. Gradient-based learning applied to document recogni-\ntion. Proceedings of the IEEE, 1998.\n[44] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence\nZitnick. Microsoft COCO: Common objects in context. In\nECCV. 2014."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 299
        },
        {
          "x": 1205,
          "y": 299
        },
        {
          "x": 1205,
          "y": 2981
        },
        {
          "x": 202,
          "y": 2981
        }
      ],
      "category": "paragraph",
      "html": "<p id='168' style='font-size:14px'>[45] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng<br>Zhang, Stephen Lin, and Baining Guo. Swin transformer:<br>Hierarchical vision transformer using shifted windows. 2021.<br>[46] Ilya Loshchilov and Frank Hutter. Decoupled weight decay<br>regularization. In ICLR, 2019.<br>[47] Xiaofeng Mao, Gege Qi, Yuefeng Chen, Xiaodan Li, Ranjie<br>Duan, Shaokai Ye, Yuan He, and Hui Xue. Towards robust<br>vision transformer. arXiv preprint arXiv:2105.07926, 2021.<br>[48] Eric Mintun, Alexander Kirillov, and Saining Xie. On in-<br>teraction between augmentations and corruptions in natural<br>corruption robustness. NeurIPS, 2021.<br>[49] Vinod Nair and Geoffrey E Hinton. Rectified linear units<br>improve restricted boltzmann machines. In ICML, 2010.<br>[50] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,<br>James Bradbury, Gregory Chanan, Trevor Killeen, Zeming<br>Lin, Natalia Gimelshein, Luca Antiga, et al. PyTorch: An<br>imperative style, high-performance deep learning library. In<br>NeurIPS, 2019.<br>[51] Boris T Polyak and Anatoli B Juditsky. Acceleration of<br>stochastic approximation by averaging. SIAM Journal on<br>Control and Optimization, 1992.<br>[52] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario<br>Amodei, and Ilya Sutskever. Language models are unsuper-<br>vised multitask learners. 2019.<br>[53] Ilija Radosavovic, Justin Johnson, Saining Xie, Wan- Yen<br>Lo, and Piotr Dollar. On network design spaces for visual<br>recognition. In ICCV, 2019.<br>[54] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaim-<br>ing He, and Piotr Dollar. Designing network design spaces.<br>In CVPR, 2020.<br>[55] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan<br>Bello, Anselm Levskaya, and Jonathon Shlens. Stand-alone<br>self-attention in vision models. NeurIPS, 2019.<br>[56] Yongming Rao, Wenliang Zhao, Zheng Zhu, Jiwen Lu, and<br>Jie Zhou. Global filter networks for image classification.<br>NeurIPS, 2021.<br>[57] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.<br>Faster R-CNN: Towards real-time object detection with region<br>proposal networks. In NeurIPS, 2015.<br>[58] Henry A Rowley, Shumeet Baluja, and Takeo Kanade. Neural<br>network-based face detection. TPAMI, 1998.<br>[59] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-<br>jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,<br>Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li<br>Fei-Fei. ImageNet Large Scale Visual Recognition Challenge.<br>IJCV, 2015.<br>[60] Tim Salimans and Diederik P Kingma. Weight normalization:<br>A simple reparameterization to accelerate training of deep<br>neural networks. In NeurIPS, 2016.<br>[61] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-<br>moginov, and Liang-Chieh Chen. Mobilenetv2: Inverted<br>residuals and linear bottlenecks. In CVPR, 2018.<br>[62] Pierre Sermanet, David Eigen, Xiang Zhang, Michael Math-<br>ieu, Rob Fergus, and Yann LeCun. Overfeat: Integrated<br>recognition, localization and detection using convolutional<br>networks. In ICLR, 2014.</p>",
      "id": 168,
      "page": 14,
      "text": "[45] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. 2021.\n[46] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. In ICLR, 2019.\n[47] Xiaofeng Mao, Gege Qi, Yuefeng Chen, Xiaodan Li, Ranjie\nDuan, Shaokai Ye, Yuan He, and Hui Xue. Towards robust\nvision transformer. arXiv preprint arXiv:2105.07926, 2021.\n[48] Eric Mintun, Alexander Kirillov, and Saining Xie. On in-\nteraction between augmentations and corruptions in natural\ncorruption robustness. NeurIPS, 2021.\n[49] Vinod Nair and Geoffrey E Hinton. Rectified linear units\nimprove restricted boltzmann machines. In ICML, 2010.\n[50] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, et al. PyTorch: An\nimperative style, high-performance deep learning library. In\nNeurIPS, 2019.\n[51] Boris T Polyak and Anatoli B Juditsky. Acceleration of\nstochastic approximation by averaging. SIAM Journal on\nControl and Optimization, 1992.\n[52] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\nAmodei, and Ilya Sutskever. Language models are unsuper-\nvised multitask learners. 2019.\n[53] Ilija Radosavovic, Justin Johnson, Saining Xie, Wan- Yen\nLo, and Piotr Dollar. On network design spaces for visual\nrecognition. In ICCV, 2019.\n[54] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaim-\ning He, and Piotr Dollar. Designing network design spaces.\nIn CVPR, 2020.\n[55] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan\nBello, Anselm Levskaya, and Jonathon Shlens. Stand-alone\nself-attention in vision models. NeurIPS, 2019.\n[56] Yongming Rao, Wenliang Zhao, Zheng Zhu, Jiwen Lu, and\nJie Zhou. Global filter networks for image classification.\nNeurIPS, 2021.\n[57] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster R-CNN: Towards real-time object detection with region\nproposal networks. In NeurIPS, 2015.\n[58] Henry A Rowley, Shumeet Baluja, and Takeo Kanade. Neural\nnetwork-based face detection. TPAMI, 1998.\n[59] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, Alexander C. Berg, and Li\nFei-Fei. ImageNet Large Scale Visual Recognition Challenge.\nIJCV, 2015.\n[60] Tim Salimans and Diederik P Kingma. Weight normalization:\nA simple reparameterization to accelerate training of deep\nneural networks. In NeurIPS, 2016.\n[61] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-\nmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted\nresiduals and linear bottlenecks. In CVPR, 2018.\n[62] Pierre Sermanet, David Eigen, Xiang Zhang, Michael Math-\nieu, Rob Fergus, and Yann LeCun. Overfeat: Integrated\nrecognition, localization and detection using convolutional\nnetworks. In ICLR, 2014."
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 303
        },
        {
          "x": 2292,
          "y": 303
        },
        {
          "x": 2292,
          "y": 2974
        },
        {
          "x": 1279,
          "y": 2974
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='169' style='font-size:14px'>[63] Pierre Sermanet, Koray Kavukcuoglu, Soumith Chintala, and<br>Yann LeCun. Pedestrian detection with unsupervised multi-<br>stage feature learning. In CVPR, 2013.<br>[64] Karen Simonyan and Andrew Zisserman. Two-stream convo-<br>lutional networks for action recognition in videos. In NeurIPS,<br>2014.<br>[65] Karen Simonyan and Andrew Zisserman. Very deep convolu-<br>tional networks for large-scale image recognition. In ICLR,<br>2015.<br>[66] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon<br>Shlens, Pieter Abbeel, and Ashish Vaswani. Bottleneck trans-<br>formers for visual recognition. In CVPR, 2021.<br>[67] Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross<br>Wightman, Jakob Uszkoreit, and Lucas Beyer. How to train<br>your vit? data, augmentation, and regularization in vision<br>transformers. arXiv preprint arXiv:2106. 10270, 2021.<br>[68] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,<br>Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent<br>Vanhoucke, and Andrew Rabinovich. Going deeper with<br>convolutions. In CVPR, 2015.<br>[69] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,<br>Jonathon Shlens, and Zbigniew Wojna. Rethinking the incep-<br>tion architecture for computer vision. In CVPR, 2016.<br>[70] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan,<br>Mark Sandler, Andrew Howard, and Quoc V Le. Mnasnet:<br>Platform-aware neural architecture search for mobile. In<br>CVPR, 2019.<br>[71] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model<br>scaling for convolutional neural networks. In ICML, 2019.<br>[72] Mingxing Tan and Quoc Le. Efficientnetv2: Smaller models<br>and faster training. In ICML, 2021.<br>[73] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco<br>Massa, Alexandre Sablayrolles, and Herve Jegou. Training<br>data-efficient image transformers & distillation through atten-<br>tion. arXiv:2012.12877, 2020.<br>[74] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles,<br>Gabriel Synnaeve, and Herve Jegou. Going deeper with<br>image transformers. ICCV, 2021.<br>[75] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. In-<br>stance normalization: The missing ingredient for fast styliza-<br>tion. arXiv:1607.08022, 2016.<br>[76] Regis Vaillant, Christophe Monrocq, and Yann Le Cun. Orig-<br>inal approach for the localisation of objects in images. Vision,<br>Image and Signal Processing, 1994.<br>[77] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-<br>reit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia<br>Polosukhin. Attention is all you need. In NeurIPS, 2017.<br>[78] Haohan Wang, Songwei Ge, Eric P Xing, and Zachary C<br>Lipton. Learning robust global representations by penalizing<br>local predictive power. NeurIPS, 2019.<br>[79] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming<br>He. Non-local neural networks. In CVPR, 2018.<br>[80] Ross Wightman. GitHub repository: Pytorch image mod-<br>els. https : / /github . com/ rwi ghtman / pytorch-<br>image-models, 2019.<br>[81] Ross Wightman, Hugo Touvron, and Herve Jegou. Resnet<br>strikes back: An improved training procedure in timm.<br>arXiv:2110.00476, 2021.</p>",
      "id": 169,
      "page": 14,
      "text": "[63] Pierre Sermanet, Koray Kavukcuoglu, Soumith Chintala, and\nYann LeCun. Pedestrian detection with unsupervised multi-\nstage feature learning. In CVPR, 2013.\n[64] Karen Simonyan and Andrew Zisserman. Two-stream convo-\nlutional networks for action recognition in videos. In NeurIPS,\n2014.\n[65] Karen Simonyan and Andrew Zisserman. Very deep convolu-\ntional networks for large-scale image recognition. In ICLR,\n2015.\n[66] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon\nShlens, Pieter Abbeel, and Ashish Vaswani. Bottleneck trans-\nformers for visual recognition. In CVPR, 2021.\n[67] Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross\nWightman, Jakob Uszkoreit, and Lucas Beyer. How to train\nyour vit? data, augmentation, and regularization in vision\ntransformers. arXiv preprint arXiv:2106. 10270, 2021.\n[68] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\nVanhoucke, and Andrew Rabinovich. Going deeper with\nconvolutions. In CVPR, 2015.\n[69] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,\nJonathon Shlens, and Zbigniew Wojna. Rethinking the incep-\ntion architecture for computer vision. In CVPR, 2016.\n[70] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan,\nMark Sandler, Andrew Howard, and Quoc V Le. Mnasnet:\nPlatform-aware neural architecture search for mobile. In\nCVPR, 2019.\n[71] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model\nscaling for convolutional neural networks. In ICML, 2019.\n[72] Mingxing Tan and Quoc Le. Efficientnetv2: Smaller models\nand faster training. In ICML, 2021.\n[73] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herve Jegou. Training\ndata-efficient image transformers & distillation through atten-\ntion. arXiv:2012.12877, 2020.\n[74] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles,\nGabriel Synnaeve, and Herve Jegou. Going deeper with\nimage transformers. ICCV, 2021.\n[75] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. In-\nstance normalization: The missing ingredient for fast styliza-\ntion. arXiv:1607.08022, 2016.\n[76] Regis Vaillant, Christophe Monrocq, and Yann Le Cun. Orig-\ninal approach for the localisation of objects in images. Vision,\nImage and Signal Processing, 1994.\n[77] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NeurIPS, 2017.\n[78] Haohan Wang, Songwei Ge, Eric P Xing, and Zachary C\nLipton. Learning robust global representations by penalizing\nlocal predictive power. NeurIPS, 2019.\n[79] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming\nHe. Non-local neural networks. In CVPR, 2018.\n[80] Ross Wightman. GitHub repository: Pytorch image mod-\nels. https : / /github . com/ rwi ghtman / pytorch-\nimage-models, 2019.\n[81] Ross Wightman, Hugo Touvron, and Herve Jegou. Resnet\nstrikes back: An improved training procedure in timm.\narXiv:2110.00476, 2021."
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 298
        },
        {
          "x": 1205,
          "y": 298
        },
        {
          "x": 1205,
          "y": 1725
        },
        {
          "x": 200,
          "y": 1725
        }
      ],
      "category": "paragraph",
      "html": "<p id='170' style='font-size:14px'>[82] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang<br>Dai, Lu Yuan, and Lei Zhang. Cvt: Introducing convolutions<br>to vision transformers. ICCV, 2021.<br>[83] Yuxin Wu and Kaiming He. Group normalization. In ECCV,<br>2018.<br>[84] Yuxin Wu and Justin Johnson. Rethinking \"batch\" in batch-<br>norm. arXiv:2105.07576, 2021.<br>[85] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and<br>Jian Sun. Unified perceptual parsing for scene understanding.<br>In ECCV, 2018.<br>[86] Tete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr<br>Dollar, and Ross Girshick. Early convolutions help transform-<br>ers see better. In NeurIPS, 2021.<br>[87] Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and<br>Kaiming He. Aggregated residual transformations for deep<br>neural networks. In CVPR, 2017.<br>[88] Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. Co-<br>scale conv-attentional image transformers. ICCV, 2021.<br>[89] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk<br>Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu-<br>larization strategy to train strong classifiers with localizable<br>features. In ICCV, 2019.<br>[90] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David<br>Lopez-Paz. mixup: Beyond empirical risk minimization. In<br>ICLR, 2018.<br>[91] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and<br>Yi Yang. Random erasing data augmentation. In AAAI, 2020.<br>[92] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler,<br>Adela Barriuso, and Antonio Torralba. Semantic understand-<br>ing of scenes through the ADE20K dataset. IJCV, 2019.</p>",
      "id": 170,
      "page": 15,
      "text": "[82] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang\nDai, Lu Yuan, and Lei Zhang. Cvt: Introducing convolutions\nto vision transformers. ICCV, 2021.\n[83] Yuxin Wu and Kaiming He. Group normalization. In ECCV,\n2018.\n[84] Yuxin Wu and Justin Johnson. Rethinking \"batch\" in batch-\nnorm. arXiv:2105.07576, 2021.\n[85] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and\nJian Sun. Unified perceptual parsing for scene understanding.\nIn ECCV, 2018.\n[86] Tete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr\nDollar, and Ross Girshick. Early convolutions help transform-\ners see better. In NeurIPS, 2021.\n[87] Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and\nKaiming He. Aggregated residual transformations for deep\nneural networks. In CVPR, 2017.\n[88] Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. Co-\nscale conv-attentional image transformers. ICCV, 2021.\n[89] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk\nChun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu-\nlarization strategy to train strong classifiers with localizable\nfeatures. In ICCV, 2019.\n[90] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David\nLopez-Paz. mixup: Beyond empirical risk minimization. In\nICLR, 2018.\n[91] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and\nYi Yang. Random erasing data augmentation. In AAAI, 2020.\n[92] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler,\nAdela Barriuso, and Antonio Torralba. Semantic understand-\ning of scenes through the ADE20K dataset. IJCV, 2019."
    }
  ]
}