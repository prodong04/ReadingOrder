{
    "id": "329d9d94-0f94-11ef-8828-426932df3dcf",
    "pdf_path": "./pdf/AI_VIT_X/1409.1556v6.pdf",
    "elements": [
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 110
                },
                {
                    "x": 1225,
                    "y": 110
                },
                {
                    "x": 1225,
                    "y": 159
                },
                {
                    "x": 444,
                    "y": 159
                }
            ],
            "category": "header",
            "html": "<header id='0' style='font-size:16px'>Published as a conference paper at ICLR 2015</header>",
            "id": 0,
            "page": 1,
            "text": "Published as a conference paper at ICLR 2015"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 329
                },
                {
                    "x": 1758,
                    "y": 329
                },
                {
                    "x": 1758,
                    "y": 485
                },
                {
                    "x": 442,
                    "y": 485
                }
            ],
            "category": "paragraph",
            "html": "<p id='1' style='font-size:22px'>VERY DEEP CONVOLUTIONAL NETWORKS<br>FOR LARGE-SCALE IMAGE RECOGNITION</p>",
            "id": 1,
            "page": 1,
            "text": "VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION"
        },
        {
            "bounding_box": [
                {
                    "x": 469,
                    "y": 565
                },
                {
                    "x": 1209,
                    "y": 565
                },
                {
                    "x": 1209,
                    "y": 611
                },
                {
                    "x": 469,
                    "y": 611
                }
            ],
            "category": "paragraph",
            "html": "<p id='2' style='font-size:18px'>Karen Simonyan* & Andrew Zisserman+</p>",
            "id": 2,
            "page": 1,
            "text": "Karen Simonyan* & Andrew Zisserman+"
        },
        {
            "bounding_box": [
                {
                    "x": 472,
                    "y": 613
                },
                {
                    "x": 1866,
                    "y": 613
                },
                {
                    "x": 1866,
                    "y": 705
                },
                {
                    "x": 472,
                    "y": 705
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='3' style='font-size:16px'>Visual Geometry Group, Department of Engineering Science, University of Oxford<br>{karen, az}@robots · ○X · ac · uk</p>",
            "id": 3,
            "page": 1,
            "text": "Visual Geometry Group, Department of Engineering Science, University of Oxford {karen, az}@robots · ○X · ac · uk"
        },
        {
            "bounding_box": [
                {
                    "x": 1154,
                    "y": 826
                },
                {
                    "x": 1394,
                    "y": 826
                },
                {
                    "x": 1394,
                    "y": 875
                },
                {
                    "x": 1154,
                    "y": 875
                }
            ],
            "category": "paragraph",
            "html": "<p id='4' style='font-size:20px'>ABSTRACT</p>",
            "id": 4,
            "page": 1,
            "text": "ABSTRACT"
        },
        {
            "bounding_box": [
                {
                    "x": 590,
                    "y": 930
                },
                {
                    "x": 1961,
                    "y": 930
                },
                {
                    "x": 1961,
                    "y": 1443
                },
                {
                    "x": 590,
                    "y": 1443
                }
            ],
            "category": "paragraph",
            "html": "<p id='5' style='font-size:18px'>In this work we investigate the effect of the convolutional network depth on its<br>accuracy in the large-scale image recognition setting. Our main contribution is<br>a thorough evaluation of networks of increasing depth using an architecture with<br>very small (3 x 3) convolution filters, which shows that a significant improvement<br>on the prior-art configurations can be achieved by pushing the depth to 16-19<br>weight layers. These findings were the basis of our ImageNet Challenge 2014<br>submission, where our team secured the first and the second places in the localisa-<br>tion and classification tracks respectively. We also show that our representations<br>generalise well to other datasets, where they achieve state-of-the-art results. We<br>have made our two best-performing ConvNet models publicly available to facili-<br>tate further research on the use of deep visual representations in computer vision.</p>",
            "id": 5,
            "page": 1,
            "text": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 x 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision."
        },
        {
            "bounding_box": [
                {
                    "x": 448,
                    "y": 1512
                },
                {
                    "x": 861,
                    "y": 1512
                },
                {
                    "x": 861,
                    "y": 1566
                },
                {
                    "x": 448,
                    "y": 1566
                }
            ],
            "category": "paragraph",
            "html": "<p id='6' style='font-size:20px'>1 INTRODUCTION</p>",
            "id": 6,
            "page": 1,
            "text": "1 INTRODUCTION"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1609
                },
                {
                    "x": 2109,
                    "y": 1609
                },
                {
                    "x": 2109,
                    "y": 2070
                },
                {
                    "x": 442,
                    "y": 2070
                }
            ],
            "category": "paragraph",
            "html": "<p id='7' style='font-size:18px'>Convolutional networks (ConvNets) have recently enjoyed a great success in large-scale im-<br>age and video recognition (Krizhevsky et al., 2012; Zeiler & Fergus, 2013; Sermanet et al., 2014;<br>Simonyan & Zisserman, 2014) which has become possible due to the large public image reposito-<br>ries, such as ImageNet (Deng et al., 2009), and high-performance computing systems, such as GPUs<br>or large-scale distributed clusters (Dean et al., 2012). In particular, an important role in the advance<br>of deep visual recognition architectures has been played by the ImageNet Large-Scale Visual Recog-<br>nition Challenge (ILSVRC) (Russakovsky et al., 2014), which has served as a testbed for a few<br>generations of large-scale image classification systems, from high-dimensional shallow feature en-<br>codings (Perronnin et al., 2010) (the winner of ILSVRC-2011) to deep ConvNets (Krizhevsky et al.,<br>2012) (the winner of ILSVRC-2012).</p>",
            "id": 7,
            "page": 1,
            "text": "Convolutional networks (ConvNets) have recently enjoyed a great success in large-scale image and video recognition (Krizhevsky , 2012; Zeiler & Fergus, 2013; Sermanet , 2014; Simonyan & Zisserman, 2014) which has become possible due to the large public image repositories, such as ImageNet (Deng , 2009), and high-performance computing systems, such as GPUs or large-scale distributed clusters (Dean , 2012). In particular, an important role in the advance of deep visual recognition architectures has been played by the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) (Russakovsky , 2014), which has served as a testbed for a few generations of large-scale image classification systems, from high-dimensional shallow feature encodings (Perronnin , 2010) (the winner of ILSVRC-2011) to deep ConvNets (Krizhevsky , 2012) (the winner of ILSVRC-2012)."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2091
                },
                {
                    "x": 2110,
                    "y": 2091
                },
                {
                    "x": 2110,
                    "y": 2552
                },
                {
                    "x": 441,
                    "y": 2552
                }
            ],
            "category": "paragraph",
            "html": "<p id='8' style='font-size:18px'>With ConvNets becoming more of a commodity in the computer vision field, a number of at-<br>tempts have been made to improve the original architecture of Krizhevsky et al. (2012) in a<br>bid to achieve better accuracy. For instance, the best-performing submissions to the ILSVRC-<br>2013 (Zeiler & Fergus, 2013; Sermanet et al., 2014) utilised smaller receptive window size and<br>smaller stride of the first convolutional layer. Another line of improvements dealt with training<br>and testing the networks densely over the whole image and over multiple scales (Sermanet et al.,<br>2014; Howard, 2014). In this paper, we address another important aspect of ConvNet architecture<br>design - its depth. To this end, we fix other parameters of the architecture, and steadily increase the<br>depth of the network by adding more convolutional layers, which is feasible due to the use of very<br>small (3 x 3) convolution filters in all layers.</p>",
            "id": 8,
            "page": 1,
            "text": "With ConvNets becoming more of a commodity in the computer vision field, a number of attempts have been made to improve the original architecture of Krizhevsky  (2012) in a bid to achieve better accuracy. For instance, the best-performing submissions to the ILSVRC2013 (Zeiler & Fergus, 2013; Sermanet , 2014) utilised smaller receptive window size and smaller stride of the first convolutional layer. Another line of improvements dealt with training and testing the networks densely over the whole image and over multiple scales (Sermanet , 2014; Howard, 2014). In this paper, we address another important aspect of ConvNet architecture design - its depth. To this end, we fix other parameters of the architecture, and steadily increase the depth of the network by adding more convolutional layers, which is feasible due to the use of very small (3 x 3) convolution filters in all layers."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2575
                },
                {
                    "x": 2110,
                    "y": 2575
                },
                {
                    "x": 2110,
                    "y": 2806
                },
                {
                    "x": 441,
                    "y": 2806
                }
            ],
            "category": "paragraph",
            "html": "<p id='9' style='font-size:18px'>As a result, we come up with significantly more accurate ConvNet architectures, which not only<br>achieve the state-of-the-art accuracy on ILSVRC classification and localisation tasks, but are also<br>applicable to other image recognition datasets, where they achieve excellent performance even when<br>used as a part of a relatively simple pipelines (e.g. deep features classified by a linear SVM without<br>fine-tuning). We have released our two best-performing models1 to facilitate further research.</p>",
            "id": 9,
            "page": 1,
            "text": "As a result, we come up with significantly more accurate ConvNet architectures, which not only achieve the state-of-the-art accuracy on ILSVRC classification and localisation tasks, but are also applicable to other image recognition datasets, where they achieve excellent performance even when used as a part of a relatively simple pipelines (e.g. deep features classified by a linear SVM without fine-tuning). We have released our two best-performing models1 to facilitate further research."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2828
                },
                {
                    "x": 2109,
                    "y": 2828
                },
                {
                    "x": 2109,
                    "y": 2922
                },
                {
                    "x": 442,
                    "y": 2922
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='10' style='font-size:16px'>The rest of the paper is organised as follows. In Sect. 2, we describe our ConvNet configurations.<br>The details of the image classification training and evaluation are then presented in Sect. 3, and the</p>",
            "id": 10,
            "page": 1,
            "text": "The rest of the paper is organised as follows. In Sect. 2, we describe our ConvNet configurations. The details of the image classification training and evaluation are then presented in Sect. 3, and the"
        },
        {
            "bounding_box": [
                {
                    "x": 495,
                    "y": 2964
                },
                {
                    "x": 2080,
                    "y": 2964
                },
                {
                    "x": 2080,
                    "y": 3052
                },
                {
                    "x": 495,
                    "y": 3052
                }
            ],
            "category": "paragraph",
            "html": "<p id='11' style='font-size:14px'>* affiliation: Google DeepMind +current affiliation: University of Oxford and Google DeepMind<br>current<br>1http : / / www · robots · ○X · ac · uk / ~vgg/ research/very_deep/</p>",
            "id": 11,
            "page": 1,
            "text": "* affiliation: Google DeepMind +current affiliation: University of Oxford and Google DeepMind current 1http : / / www · robots · ○X · ac · uk / ~vgg/ research/very_deep/"
        },
        {
            "bounding_box": [
                {
                    "x": 64,
                    "y": 806
                },
                {
                    "x": 150,
                    "y": 806
                },
                {
                    "x": 150,
                    "y": 2243
                },
                {
                    "x": 64,
                    "y": 2243
                }
            ],
            "category": "footer",
            "html": "<br><footer id='12' style='font-size:14px'>2015<br>Apr<br>10<br>[cs.CV]<br>arXiv:1409.1556v6</footer>",
            "id": 12,
            "page": 1,
            "text": "2015 Apr 10 [cs.CV] arXiv:1409.1556v6"
        },
        {
            "bounding_box": [
                {
                    "x": 1261,
                    "y": 3131
                },
                {
                    "x": 1289,
                    "y": 3131
                },
                {
                    "x": 1289,
                    "y": 3172
                },
                {
                    "x": 1261,
                    "y": 3172
                }
            ],
            "category": "footer",
            "html": "<footer id='13' style='font-size:16px'>1</footer>",
            "id": 13,
            "page": 1,
            "text": "1"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 111
                },
                {
                    "x": 1225,
                    "y": 111
                },
                {
                    "x": 1225,
                    "y": 158
                },
                {
                    "x": 444,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='14' style='font-size:16px'>Published as a conference paper at ICLR 2015</header>",
            "id": 14,
            "page": 2,
            "text": "Published as a conference paper at ICLR 2015"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 346
                },
                {
                    "x": 2110,
                    "y": 346
                },
                {
                    "x": 2110,
                    "y": 532
                },
                {
                    "x": 440,
                    "y": 532
                }
            ],
            "category": "paragraph",
            "html": "<p id='15' style='font-size:18px'>configurations are compared on the ILSVRC classification task in Sect. 4. Sect. 5 concludes the<br>paper. For completeness, we also describe and assess our ILSVRC-2014 object localisation system<br>in Appendix A, and discuss the generalisation of very deep features to other datasets in Appendix B.<br>Finally, Appendix C contains the list of major paper revisions.</p>",
            "id": 15,
            "page": 2,
            "text": "configurations are compared on the ILSVRC classification task in Sect. 4. Sect. 5 concludes the paper. For completeness, we also describe and assess our ILSVRC-2014 object localisation system in Appendix A, and discuss the generalisation of very deep features to other datasets in Appendix B. Finally, Appendix C contains the list of major paper revisions."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 585
                },
                {
                    "x": 1145,
                    "y": 585
                },
                {
                    "x": 1145,
                    "y": 637
                },
                {
                    "x": 443,
                    "y": 637
                }
            ],
            "category": "paragraph",
            "html": "<p id='16' style='font-size:22px'>2 CONVNET CONFIGURATIONS</p>",
            "id": 16,
            "page": 2,
            "text": "2 CONVNET CONFIGURATIONS"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 684
                },
                {
                    "x": 2109,
                    "y": 684
                },
                {
                    "x": 2109,
                    "y": 917
                },
                {
                    "x": 441,
                    "y": 917
                }
            ],
            "category": "paragraph",
            "html": "<p id='17' style='font-size:20px'>To measure the improvement brought by the increased ConvNet depth in a fair setting, all our<br>ConvNet layer configurations are designed using the same principles, inspired by Ciresan et al.<br>(2011); Krizhevsky et al. (2012). In this section, we first describe a generic layout of our ConvNet<br>configurations (Sect. 2.1) and then detail the specific configurations used in the evaluation (Sect. 2.2).<br>Our design choices are then discussed and compared to the prior art in Sect. 2.3.</p>",
            "id": 17,
            "page": 2,
            "text": "To measure the improvement brought by the increased ConvNet depth in a fair setting, all our ConvNet layer configurations are designed using the same principles, inspired by Ciresan  (2011); Krizhevsky  (2012). In this section, we first describe a generic layout of our ConvNet configurations (Sect. 2.1) and then detail the specific configurations used in the evaluation (Sect. 2.2). Our design choices are then discussed and compared to the prior art in Sect. 2.3."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 962
                },
                {
                    "x": 839,
                    "y": 962
                },
                {
                    "x": 839,
                    "y": 1009
                },
                {
                    "x": 445,
                    "y": 1009
                }
            ],
            "category": "paragraph",
            "html": "<p id='18' style='font-size:20px'>2.1 ARCHITECTURE</p>",
            "id": 18,
            "page": 2,
            "text": "2.1 ARCHITECTURE"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1050
                },
                {
                    "x": 2109,
                    "y": 1050
                },
                {
                    "x": 2109,
                    "y": 1512
                },
                {
                    "x": 442,
                    "y": 1512
                }
            ],
            "category": "paragraph",
            "html": "<p id='19' style='font-size:18px'>During training, the input to our ConvNets is a fixed-size 224 x 224 RGB image. The only pre-<br>processing we do is subtracting the mean RGB value, computed on the training set, from each pixel.<br>The image is passed through a stack of convolutional (conv.) layers, where we use filters with a very<br>small receptive field: 3 x 3 (which is the smallest size to capture the notion of left/right, up/down,<br>center). In one of the configurations we also utilise 1 x 1 convolution filters, which can be seen as<br>a linear transformation of the input channels (followed by non-linearity). The convolution stride is<br>fixed to 1 pixel; the spatial padding of conv. layer inputis such that the spatial resolution is preserved<br>after convolution, i.e. the padding is 1 pixel for 3 x 3 conv. layers. Spatial pooling is carried out by<br>five max-pooling layers, which follow some of the conv. layers (not all the conv. layers are followed<br>by max-pooling). Max-pooling is performed over a 2 x 2 pixel window, with stride 2.</p>",
            "id": 19,
            "page": 2,
            "text": "During training, the input to our ConvNets is a fixed-size 224 x 224 RGB image. The only preprocessing we do is subtracting the mean RGB value, computed on the training set, from each pixel. The image is passed through a stack of convolutional (conv.) layers, where we use filters with a very small receptive field: 3 x 3 (which is the smallest size to capture the notion of left/right, up/down, center). In one of the configurations we also utilise 1 x 1 convolution filters, which can be seen as a linear transformation of the input channels (followed by non-linearity). The convolution stride is fixed to 1 pixel; the spatial padding of conv. layer inputis such that the spatial resolution is preserved after convolution, i.e. the padding is 1 pixel for 3 x 3 conv. layers. Spatial pooling is carried out by five max-pooling layers, which follow some of the conv. layers (not all the conv. layers are followed by max-pooling). Max-pooling is performed over a 2 x 2 pixel window, with stride 2."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1536
                },
                {
                    "x": 2107,
                    "y": 1536
                },
                {
                    "x": 2107,
                    "y": 1719
                },
                {
                    "x": 441,
                    "y": 1719
                }
            ],
            "category": "paragraph",
            "html": "<p id='20' style='font-size:20px'>A stack of convolutional layers (which has a different depth in different architectures) is followed by<br>three Fully-Connected (FC) layers: the first two have 4096 channels each, the third performs 1000-<br>way ILSVRC classification and thus contains 1000 channels (one for each class). The final layer is<br>the soft-max layer. The configuration of the fully connected layers is the same in all networks.</p>",
            "id": 20,
            "page": 2,
            "text": "A stack of convolutional layers (which has a different depth in different architectures) is followed by three Fully-Connected (FC) layers: the first two have 4096 channels each, the third performs 1000way ILSVRC classification and thus contains 1000 channels (one for each class). The final layer is the soft-max layer. The configuration of the fully connected layers is the same in all networks."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1742
                },
                {
                    "x": 2109,
                    "y": 1742
                },
                {
                    "x": 2109,
                    "y": 2018
                },
                {
                    "x": 441,
                    "y": 2018
                }
            ],
            "category": "paragraph",
            "html": "<p id='21' style='font-size:18px'>All hidden layers are equipped with the rectification (ReLU (Krizhevsky et al., 2012)) non-linearity.<br>We note that none of our networks (except for one) contain Local Response Normalisation<br>(LRN) normalisation (Krizhevsky et al., 2012): as will be shown in Sect. 4, such normalisation<br>does not improve the performance on the ILSVRC dataset, but leads to increased memory con-<br>sumption and computation time. Where applicable, the parameters for the LRN layer are those<br>of (Krizhevsky et al., 2012).</p>",
            "id": 21,
            "page": 2,
            "text": "All hidden layers are equipped with the rectification (ReLU (Krizhevsky , 2012)) non-linearity. We note that none of our networks (except for one) contain Local Response Normalisation (LRN) normalisation (Krizhevsky , 2012): as will be shown in Sect. 4, such normalisation does not improve the performance on the ILSVRC dataset, but leads to increased memory consumption and computation time. Where applicable, the parameters for the LRN layer are those of (Krizhevsky , 2012)."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2063
                },
                {
                    "x": 873,
                    "y": 2063
                },
                {
                    "x": 873,
                    "y": 2112
                },
                {
                    "x": 444,
                    "y": 2112
                }
            ],
            "category": "paragraph",
            "html": "<p id='22' style='font-size:20px'>2.2 CONFIGURATIONS</p>",
            "id": 22,
            "page": 2,
            "text": "2.2 CONFIGURATIONS"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2154
                },
                {
                    "x": 2109,
                    "y": 2154
                },
                {
                    "x": 2109,
                    "y": 2431
                },
                {
                    "x": 441,
                    "y": 2431
                }
            ],
            "category": "paragraph",
            "html": "<p id='23' style='font-size:18px'>The ConvNet configurations, evaluated in this paper, are outlined in Table 1, one per column. In<br>the following we will refer to the nets by their names (A-E). All configurations follow the generic<br>design presented in Sect. 2.1, and differ only in the depth: from 11 weight layers in the network A<br>(8 conv. and 3 FC layers) to 19 weight layers in the network E (16 conv. and 3 FC layers). The width<br>of conv. layers (the number of channels) is rather small, starting from 64 in the first layer and then<br>increasing by a factor of 2 after each max-pooling layer, until it reaches 512.</p>",
            "id": 23,
            "page": 2,
            "text": "The ConvNet configurations, evaluated in this paper, are outlined in Table 1, one per column. In the following we will refer to the nets by their names (A-E). All configurations follow the generic design presented in Sect. 2.1, and differ only in the depth: from 11 weight layers in the network A (8 conv. and 3 FC layers) to 19 weight layers in the network E (16 conv. and 3 FC layers). The width of conv. layers (the number of channels) is rather small, starting from 64 in the first layer and then increasing by a factor of 2 after each max-pooling layer, until it reaches 512."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2455
                },
                {
                    "x": 2106,
                    "y": 2455
                },
                {
                    "x": 2106,
                    "y": 2595
                },
                {
                    "x": 442,
                    "y": 2595
                }
            ],
            "category": "paragraph",
            "html": "<p id='24' style='font-size:18px'>In Table 2 we report the number of parameters for each configuration. In spite of a large depth, the<br>number of weights in our nets is not greater than the number of weights in a more shallow net with<br>larger conv. layer widths and receptive fields (144M weights in (Sermanet et al., 2014)).</p>",
            "id": 24,
            "page": 2,
            "text": "In Table 2 we report the number of parameters for each configuration. In spite of a large depth, the number of weights in our nets is not greater than the number of weights in a more shallow net with larger conv. layer widths and receptive fields (144M weights in (Sermanet , 2014))."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2640
                },
                {
                    "x": 775,
                    "y": 2640
                },
                {
                    "x": 775,
                    "y": 2686
                },
                {
                    "x": 443,
                    "y": 2686
                }
            ],
            "category": "paragraph",
            "html": "<p id='25' style='font-size:18px'>2.3 DISCUSSION</p>",
            "id": 25,
            "page": 2,
            "text": "2.3 DISCUSSION"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2728
                },
                {
                    "x": 2109,
                    "y": 2728
                },
                {
                    "x": 2109,
                    "y": 3055
                },
                {
                    "x": 441,
                    "y": 3055
                }
            ],
            "category": "paragraph",
            "html": "<p id='26' style='font-size:18px'>Our ConvNet configurations are quite different from the ones used in the top-performing entries<br>of the ILSVRC-2012 (Krizhevsky et al., 2012) and ILSVRC-2013 competitions (Zeiler & Fergus,<br>2013; Sermanet et al., 2014). Rather than using relatively large receptive fields in the first conv. lay-<br>ers (e.g. 11 x 11 with stride 4 in (Krizhevsky et al., 2012), or 7 x 7 with stride 2 in (Zeiler & Fergus,<br>2013; Sermanet et al., 2014)), we use very small 3 x 3 receptive fields throughout the whole net,<br>which are convolved with the input at every pixel (with stride 1). It is easy to see that a stack of two<br>3 x 3 conv. layers (without spatial pooling in between) has an effective receptive field of 5 x 5; three</p>",
            "id": 26,
            "page": 2,
            "text": "Our ConvNet configurations are quite different from the ones used in the top-performing entries of the ILSVRC-2012 (Krizhevsky , 2012) and ILSVRC-2013 competitions (Zeiler & Fergus, 2013; Sermanet , 2014). Rather than using relatively large receptive fields in the first conv. layers (e.g. 11 x 11 with stride 4 in (Krizhevsky , 2012), or 7 x 7 with stride 2 in (Zeiler & Fergus, 2013; Sermanet , 2014)), we use very small 3 x 3 receptive fields throughout the whole net, which are convolved with the input at every pixel (with stride 1). It is easy to see that a stack of two 3 x 3 conv. layers (without spatial pooling in between) has an effective receptive field of 5 x 5; three"
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3133
                },
                {
                    "x": 1289,
                    "y": 3133
                },
                {
                    "x": 1289,
                    "y": 3170
                },
                {
                    "x": 1260,
                    "y": 3170
                }
            ],
            "category": "footer",
            "html": "<footer id='27' style='font-size:14px'>2</footer>",
            "id": 27,
            "page": 2,
            "text": "2"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 112
                },
                {
                    "x": 1225,
                    "y": 112
                },
                {
                    "x": 1225,
                    "y": 157
                },
                {
                    "x": 445,
                    "y": 157
                }
            ],
            "category": "header",
            "html": "<header id='28' style='font-size:18px'>Published as a conference paper at ICLR 2015</header>",
            "id": 28,
            "page": 3,
            "text": "Published as a conference paper at ICLR 2015"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 370
                },
                {
                    "x": 2109,
                    "y": 370
                },
                {
                    "x": 2109,
                    "y": 557
                },
                {
                    "x": 440,
                    "y": 557
                }
            ],
            "category": "caption",
            "html": "<caption id='29' style='font-size:22px'>Table 1: ConvNet configurations (shown in columns). The depth of the configurations increases<br>from the left (A) to the right (E), as more layers are added (the added layers are shown in bold). The<br>convolutional layer parameters are denoted as \"conv (receptive field size>-<number of channels)\".<br>The ReLU activation function is not shown for brevity.</caption>",
            "id": 29,
            "page": 3,
            "text": "Table 1: ConvNet configurations (shown in columns). The depth of the configurations increases from the left (A) to the right (E), as more layers are added (the added layers are shown in bold). The convolutional layer parameters are denoted as \"conv (receptive field size>-<number of channels)\". The ReLU activation function is not shown for brevity."
        },
        {
            "bounding_box": [
                {
                    "x": 629,
                    "y": 558
                },
                {
                    "x": 1907,
                    "y": 558
                },
                {
                    "x": 1907,
                    "y": 1854
                },
                {
                    "x": 629,
                    "y": 1854
                }
            ],
            "category": "table",
            "html": "<br><table id='30' style='font-size:14px'><tr><td colspan=\"6\">ConvNet Configuration</td></tr><tr><td>A</td><td>A-LRN</td><td>B</td><td>C</td><td>D</td><td>E</td></tr><tr><td>11 weight layers</td><td>11 weight layers</td><td>13 weight layers</td><td>16 weight layers</td><td>16 weight layers</td><td>19 weight layers</td></tr><tr><td colspan=\"6\">input (224 x 224 RGB image)</td></tr><tr><td>conv3-64</td><td>conv3-64 LRN</td><td>conv3-64 conv3-64</td><td>conv3-64 conv3-64</td><td>conv3-64 conv3-64</td><td>conv3-64 conv3-64</td></tr><tr><td colspan=\"6\">maxpool</td></tr><tr><td>conv3-128</td><td>conv3-128</td><td>conv3-128 conv3-128</td><td>conv3-128 conv3-128</td><td>conv3-128 conv3-128</td><td>conv3-128 conv3-128</td></tr><tr><td colspan=\"6\">maxpool</td></tr><tr><td>conv3-256 conv3-256</td><td>conv3-256 conv3-256</td><td>conv3-256 conv3-256</td><td>conv3-256 conv3-256 conv1-256</td><td>conv3-256 conv3-256 conv3-256</td><td>conv3-256 conv3-256 conv3-256 conv3-256</td></tr><tr><td colspan=\"6\">maxpool</td></tr><tr><td>conv3-512 conv3-512</td><td>conv3-512 conv3-512</td><td>conv3-512 conv3-512</td><td>conv3-512 conv3-512 conv1-512</td><td>conv3-512 conv3-512 conv3-512</td><td>conv3-512 conv3-512 conv3-512 conv3-512</td></tr><tr><td colspan=\"6\">maxpool</td></tr><tr><td>conv3-512 conv3-512</td><td>conv3-512 conv3-512</td><td>conv3-512 conv3-512</td><td>conv3-512 conv3-512 conv1-512</td><td>conv3-512 conv3-512 conv3-512</td><td>conv3-512 conv3-512 conv3-512 conv3-512</td></tr><tr><td colspan=\"6\">maxpool</td></tr><tr><td colspan=\"6\">FC-4096</td></tr><tr><td colspan=\"6\"></td></tr><tr><td colspan=\"6\">FC-4096 FC-1000 soft-max</td></tr></table>",
            "id": 30,
            "page": 3,
            "text": "ConvNet Configuration  A A-LRN B C D E  11 weight layers 11 weight layers 13 weight layers 16 weight layers 16 weight layers 19 weight layers  input (224 x 224 RGB image)  conv3-64 conv3-64 LRN conv3-64 conv3-64 conv3-64 conv3-64 conv3-64 conv3-64 conv3-64 conv3-64  maxpool  conv3-128 conv3-128 conv3-128 conv3-128 conv3-128 conv3-128 conv3-128 conv3-128 conv3-128 conv3-128  maxpool  conv3-256 conv3-256 conv3-256 conv3-256 conv3-256 conv3-256 conv3-256 conv3-256 conv1-256 conv3-256 conv3-256 conv3-256 conv3-256 conv3-256 conv3-256 conv3-256  maxpool  conv3-512 conv3-512 conv3-512 conv3-512 conv3-512 conv3-512 conv3-512 conv3-512 conv1-512 conv3-512 conv3-512 conv3-512 conv3-512 conv3-512 conv3-512 conv3-512  maxpool  conv3-512 conv3-512 conv3-512 conv3-512 conv3-512 conv3-512 conv3-512 conv3-512 conv1-512 conv3-512 conv3-512 conv3-512 conv3-512 conv3-512 conv3-512 conv3-512  maxpool  FC-4096"
        },
        {
            "bounding_box": [
                {
                    "x": 884,
                    "y": 1905
                },
                {
                    "x": 1662,
                    "y": 1905
                },
                {
                    "x": 1662,
                    "y": 1947
                },
                {
                    "x": 884,
                    "y": 1947
                }
            ],
            "category": "caption",
            "html": "<caption id='31' style='font-size:20px'>Table 2: Number of parameters (in millions).</caption>",
            "id": 31,
            "page": 3,
            "text": "Table 2: Number of parameters (in millions)."
        },
        {
            "bounding_box": [
                {
                    "x": 756,
                    "y": 1946
                },
                {
                    "x": 1788,
                    "y": 1946
                },
                {
                    "x": 1788,
                    "y": 2048
                },
                {
                    "x": 756,
                    "y": 2048
                }
            ],
            "category": "table",
            "html": "<br><table id='32' style='font-size:14px'><tr><td>Network</td><td>A,A-LRN</td><td>B</td><td>C</td><td>D</td><td>E</td></tr><tr><td>Number of parameters</td><td>133</td><td>133</td><td>134</td><td>138</td><td>144</td></tr></table>",
            "id": 32,
            "page": 3,
            "text": "Network A,A-LRN B C D E  Number of parameters 133 133 134 138"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2080
                },
                {
                    "x": 2107,
                    "y": 2080
                },
                {
                    "x": 2107,
                    "y": 2456
                },
                {
                    "x": 442,
                    "y": 2456
                }
            ],
            "category": "paragraph",
            "html": "<p id='33' style='font-size:20px'>such layers have a 7 x 7 effective receptive field. So what have we gained by using, for instance, a<br>stack of three 3 x 3 conv. layers instead of a single 7 x 7 layer? First, we incorporate three non-linear<br>rectification layers instead of a single one, which makes the decision function more discriminative.<br>Second, we decrease the number of parameters: assuming that both the input and the output of a<br>three-layer 3 x 3 convolution stack has C channels, the stack is parametrised by 3 (32C2) = 27C2<br>weights; at the same time, a single 7 x 7 conv. layer would require 72C2 = 49C2 parameters, i.e.<br>81% more. This can be seen as imposing a regularisation on the 7 x 7 conv. filters, forcing them to<br>have a decomposition through the 3 x 3 filters (with non-linearity injected in between).</p>",
            "id": 33,
            "page": 3,
            "text": "such layers have a 7 x 7 effective receptive field. So what have we gained by using, for instance, a stack of three 3 x 3 conv. layers instead of a single 7 x 7 layer? First, we incorporate three non-linear rectification layers instead of a single one, which makes the decision function more discriminative. Second, we decrease the number of parameters: assuming that both the input and the output of a three-layer 3 x 3 convolution stack has C channels, the stack is parametrised by 3 (32C2) = 27C2 weights; at the same time, a single 7 x 7 conv. layer would require 72C2 = 49C2 parameters, i.e. 81% more. This can be seen as imposing a regularisation on the 7 x 7 conv. filters, forcing them to have a decomposition through the 3 x 3 filters (with non-linearity injected in between)."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2477
                },
                {
                    "x": 2108,
                    "y": 2477
                },
                {
                    "x": 2108,
                    "y": 2754
                },
                {
                    "x": 441,
                    "y": 2754
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='34' style='font-size:20px'>The incorporation of 1 x 1 conv. layers (configuration C, Table 1) is a way to increase the non-<br>linearity of the decision function without affecting the receptive fields of the conv. layers. Even<br>though in our case the 1 x 1 convolution is essentially a linear projection onto the space of the same<br>dimensionality (the number of input and output channels is the same), an additional non-linearity is<br>introduced by the rectification function. It should be noted that 1 x 1 conv. layers have recently been<br>utilised in the \"Network in Network\" architecture of Lin et al. (2014).</p>",
            "id": 34,
            "page": 3,
            "text": "The incorporation of 1 x 1 conv. layers (configuration C, Table 1) is a way to increase the nonlinearity of the decision function without affecting the receptive fields of the conv. layers. Even though in our case the 1 x 1 convolution is essentially a linear projection onto the space of the same dimensionality (the number of input and output channels is the same), an additional non-linearity is introduced by the rectification function. It should be noted that 1 x 1 conv. layers have recently been utilised in the \"Network in Network\" architecture of Lin  (2014)."
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 2776
                },
                {
                    "x": 2109,
                    "y": 2776
                },
                {
                    "x": 2109,
                    "y": 3055
                },
                {
                    "x": 440,
                    "y": 3055
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='35' style='font-size:22px'>Small-size convolution filters have been previously used by Ciresan et al. (2011), but their nets<br>are significantly less deep than ours, and they did not evaluate on the large-scale ILSVRC<br>dataset. Goodfellow et al. (2014) applied deep ConvNets (11 weight layers) to the task of<br>street number recognition, and showed that the increased depth led to better performance.<br>GoogLeNet (Szegedy et al., 2014), a top-performing entry of the ILSVRC-2014 classification task,<br>was developed independently of our work, but is similar in that it is based on very deep ConvNets</p>",
            "id": 35,
            "page": 3,
            "text": "Small-size convolution filters have been previously used by Ciresan  (2011), but their nets are significantly less deep than ours, and they did not evaluate on the large-scale ILSVRC dataset. Goodfellow  (2014) applied deep ConvNets (11 weight layers) to the task of street number recognition, and showed that the increased depth led to better performance. GoogLeNet (Szegedy , 2014), a top-performing entry of the ILSVRC-2014 classification task, was developed independently of our work, but is similar in that it is based on very deep ConvNets"
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3132
                },
                {
                    "x": 1288,
                    "y": 3132
                },
                {
                    "x": 1288,
                    "y": 3170
                },
                {
                    "x": 1260,
                    "y": 3170
                }
            ],
            "category": "footer",
            "html": "<footer id='36' style='font-size:16px'>3</footer>",
            "id": 36,
            "page": 3,
            "text": "3"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 111
                },
                {
                    "x": 1224,
                    "y": 111
                },
                {
                    "x": 1224,
                    "y": 158
                },
                {
                    "x": 444,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='37' style='font-size:14px'>Published as a conference paper at ICLR 2015</header>",
            "id": 37,
            "page": 4,
            "text": "Published as a conference paper at ICLR 2015"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 343
                },
                {
                    "x": 2109,
                    "y": 343
                },
                {
                    "x": 2109,
                    "y": 578
                },
                {
                    "x": 441,
                    "y": 578
                }
            ],
            "category": "paragraph",
            "html": "<p id='38' style='font-size:16px'>(22 weight layers) and small convolution filters (apart from 3 x 3, they also use 1 x 1 and 5 x 5<br>convolutions). Their network topology is, however, more complex than ours, and the spatial reso-<br>lution of the feature maps is reduced more aggressively in the first layers to decrease the amount<br>of computation. As will be shown in Sect. 4.5, our model is outperforming that of Szegedy et al.<br>(2014) in terms of the single-network classification accuracy.</p>",
            "id": 38,
            "page": 4,
            "text": "(22 weight layers) and small convolution filters (apart from 3 x 3, they also use 1 x 1 and 5 x 5 convolutions). Their network topology is, however, more complex than ours, and the spatial resolution of the feature maps is reduced more aggressively in the first layers to decrease the amount of computation. As will be shown in Sect. 4.5, our model is outperforming that of Szegedy  (2014) in terms of the single-network classification accuracy."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 624
                },
                {
                    "x": 1184,
                    "y": 624
                },
                {
                    "x": 1184,
                    "y": 678
                },
                {
                    "x": 443,
                    "y": 678
                }
            ],
            "category": "paragraph",
            "html": "<p id='39' style='font-size:22px'>3 CLASSIFICATION FRAMEWORK</p>",
            "id": 39,
            "page": 4,
            "text": "3 CLASSIFICATION FRAMEWORK"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 721
                },
                {
                    "x": 2107,
                    "y": 721
                },
                {
                    "x": 2107,
                    "y": 816
                },
                {
                    "x": 441,
                    "y": 816
                }
            ],
            "category": "paragraph",
            "html": "<p id='40' style='font-size:14px'>In the previous section we presented the details of our network configurations. In this section, we<br>describe the details of classification ConvNet training and evaluation.</p>",
            "id": 40,
            "page": 4,
            "text": "In the previous section we presented the details of our network configurations. In this section, we describe the details of classification ConvNet training and evaluation."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 855
                },
                {
                    "x": 736,
                    "y": 855
                },
                {
                    "x": 736,
                    "y": 903
                },
                {
                    "x": 444,
                    "y": 903
                }
            ],
            "category": "paragraph",
            "html": "<p id='41' style='font-size:16px'>3.1 TRAINING</p>",
            "id": 41,
            "page": 4,
            "text": "3.1 TRAINING"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 943
                },
                {
                    "x": 2109,
                    "y": 943
                },
                {
                    "x": 2109,
                    "y": 1496
                },
                {
                    "x": 442,
                    "y": 1496
                }
            ],
            "category": "paragraph",
            "html": "<p id='42' style='font-size:18px'>The ConvNet training procedure generally follows Krizhevsky et al. (2012) (except for sampling<br>the input crops from multi-scale training images, as explained later). Namely, the training is carried<br>out by optimising the multinomial logistic regression objective using mini-batch gradient descent<br>(based on back-propagation (LeCun et al., 1989)) with momentum. The batch size was set to 256,<br>momentum to 0.9. The training was regularised by weight decay (the L2 penalty multiplier set to<br>5 · 10-4) and dropout regularisation for the first two fully-connected layers (dropout ratio set to 0.5).<br>The learning rate was initially set to 10-2 and then decreased by a factor of 10 when the validation<br>,<br>set accuracy stopped improving. In total, the learning rate was decreased 3 times, and the learning<br>was stopped after 370K iterations (74 epochs). We conjecture that in spite of the larger number of<br>parameters and the greater depth of our nets compared to (Krizhevsky et al., 2012), the nets required<br>less epochs to converge due to (a) implicit regularisation imposed by greater depth and smaller conv.<br>filter sizes; (b) pre-initialisation of certain layers.</p>",
            "id": 42,
            "page": 4,
            "text": "The ConvNet training procedure generally follows Krizhevsky  (2012) (except for sampling the input crops from multi-scale training images, as explained later). Namely, the training is carried out by optimising the multinomial logistic regression objective using mini-batch gradient descent (based on back-propagation (LeCun , 1989)) with momentum. The batch size was set to 256, momentum to 0.9. The training was regularised by weight decay (the L2 penalty multiplier set to 5 · 10-4) and dropout regularisation for the first two fully-connected layers (dropout ratio set to 0.5). The learning rate was initially set to 10-2 and then decreased by a factor of 10 when the validation , set accuracy stopped improving. In total, the learning rate was decreased 3 times, and the learning was stopped after 370K iterations (74 epochs). We conjecture that in spite of the larger number of parameters and the greater depth of our nets compared to (Krizhevsky , 2012), the nets required less epochs to converge due to (a) implicit regularisation imposed by greater depth and smaller conv. filter sizes; (b) pre-initialisation of certain layers."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1517
                },
                {
                    "x": 2108,
                    "y": 1517
                },
                {
                    "x": 2108,
                    "y": 1977
                },
                {
                    "x": 441,
                    "y": 1977
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='43' style='font-size:18px'>The initialisation of the network weights is important, since bad initialisation can stall learning due<br>to the instability of gradient in deep nets. To circumvent this problem, we began with training<br>the configuration A (Table 1), shallow enough to be trained with random initialisation. Then, when<br>training deeper architectures, we initialised the first four convolutional layers and the last three fully-<br>connected layers with the layers of net A (the intermediate layers were initialised randomly). We did<br>not decrease the learning rate for the pre-initialised layers, allowing them to change during learning.<br>For random initialisation (where applicable), we sampled the weights from a normal distribution<br>with the zero mean and 10-2 variance. The biases were initialised with zero. It is worth noting that<br>after the paper submission we found that it is possible to initialise the weights without pre-training<br>by using the random initialisation procedure of Glorot & Bengio (2010).</p>",
            "id": 43,
            "page": 4,
            "text": "The initialisation of the network weights is important, since bad initialisation can stall learning due to the instability of gradient in deep nets. To circumvent this problem, we began with training the configuration A (Table 1), shallow enough to be trained with random initialisation. Then, when training deeper architectures, we initialised the first four convolutional layers and the last three fullyconnected layers with the layers of net A (the intermediate layers were initialised randomly). We did not decrease the learning rate for the pre-initialised layers, allowing them to change during learning. For random initialisation (where applicable), we sampled the weights from a normal distribution with the zero mean and 10-2 variance. The biases were initialised with zero. It is worth noting that after the paper submission we found that it is possible to initialise the weights without pre-training by using the random initialisation procedure of Glorot & Bengio (2010)."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1998
                },
                {
                    "x": 2108,
                    "y": 1998
                },
                {
                    "x": 2108,
                    "y": 2184
                },
                {
                    "x": 441,
                    "y": 2184
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='44' style='font-size:20px'>To obtain the fixed-size 224 x 224 ConvNet input images, they were randomly cropped from rescaled<br>training images (one crop per image per SGD iteration). To further augment the training set, the<br>crops underwent random horizontal flipping and random RGB colour shift (Krizhevsky et al., 2012).<br>Training image rescaling is explained below.</p>",
            "id": 44,
            "page": 4,
            "text": "To obtain the fixed-size 224 x 224 ConvNet input images, they were randomly cropped from rescaled training images (one crop per image per SGD iteration). To further augment the training set, the crops underwent random horizontal flipping and random RGB colour shift (Krizhevsky , 2012). Training image rescaling is explained below."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2222
                },
                {
                    "x": 2108,
                    "y": 2222
                },
                {
                    "x": 2108,
                    "y": 2501
                },
                {
                    "x": 441,
                    "y": 2501
                }
            ],
            "category": "paragraph",
            "html": "<p id='45' style='font-size:16px'>Training image size. Let S be the smallest side of an isotropically-rescaled training image, from<br>which the ConvNet input is cropped (we also refer to S as the training scale). While the crop size<br>is fixed to 224 x 224, in principle S can take on any value not less than 224: for S = 224 the crop<br>will capture whole-image statistics, completely spanning the smallest side of a training image; for<br>S 》 224 the crop will correspond to a small part of the image, containing a small object or an object<br>part.</p>",
            "id": 45,
            "page": 4,
            "text": "Training image size. Let S be the smallest side of an isotropically-rescaled training image, from which the ConvNet input is cropped (we also refer to S as the training scale). While the crop size is fixed to 224 x 224, in principle S can take on any value not less than 224: for S = 224 the crop will capture whole-image statistics, completely spanning the smallest side of a training image; for S 》 224 the crop will correspond to a small part of the image, containing a small object or an object part."
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 2521
                },
                {
                    "x": 2109,
                    "y": 2521
                },
                {
                    "x": 2109,
                    "y": 2846
                },
                {
                    "x": 440,
                    "y": 2846
                }
            ],
            "category": "paragraph",
            "html": "<p id='46' style='font-size:16px'>We consider two approaches for setting the training scale S. The first is to fix S, which corresponds<br>to single-scale training (note that image content within the sampled crops can still represent multi-<br>scale image statistics). In our experiments, we evaluated models trained at two fixed scales: S =<br>256 (which has been widely used in the prior art (Krizhevsky et al., 2012; Zeiler & Fergus, 2013;<br>Sermanet et al., 2014)) and S = 384. Given a ConvNet configuration, we first trained the network<br>using S = 256. To speed-up training of the S = 384 network, it was initialised with the weights<br>pre-trained with S = 256, and we used a smaller initial learning rate of 10-3.</p>",
            "id": 46,
            "page": 4,
            "text": "We consider two approaches for setting the training scale S. The first is to fix S, which corresponds to single-scale training (note that image content within the sampled crops can still represent multiscale image statistics). In our experiments, we evaluated models trained at two fixed scales: S = 256 (which has been widely used in the prior art (Krizhevsky , 2012; Zeiler & Fergus, 2013; Sermanet , 2014)) and S = 384. Given a ConvNet configuration, we first trained the network using S = 256. To speed-up training of the S = 384 network, it was initialised with the weights pre-trained with S = 256, and we used a smaller initial learning rate of 10-3."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2868
                },
                {
                    "x": 2110,
                    "y": 2868
                },
                {
                    "x": 2110,
                    "y": 3055
                },
                {
                    "x": 441,
                    "y": 3055
                }
            ],
            "category": "paragraph",
            "html": "<p id='47' style='font-size:18px'>The second approach to setting S is multi-scale training, where each training image is individually<br>rescaled by randomly sampling S from a certain range [Smin, Smax] (we used Smin = 256 and<br>Smax = 512). Since objects in images can be of different size, itis beneficial to take this into account<br>during training. This can also be seen as training set augmentation by scale jittering, where a single</p>",
            "id": 47,
            "page": 4,
            "text": "The second approach to setting S is multi-scale training, where each training image is individually rescaled by randomly sampling S from a certain range [Smin, Smax] (we used Smin = 256 and Smax = 512). Since objects in images can be of different size, itis beneficial to take this into account during training. This can also be seen as training set augmentation by scale jittering, where a single"
        },
        {
            "bounding_box": [
                {
                    "x": 1258,
                    "y": 3133
                },
                {
                    "x": 1289,
                    "y": 3133
                },
                {
                    "x": 1289,
                    "y": 3169
                },
                {
                    "x": 1258,
                    "y": 3169
                }
            ],
            "category": "footer",
            "html": "<footer id='48' style='font-size:14px'>4</footer>",
            "id": 48,
            "page": 4,
            "text": "4"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 111
                },
                {
                    "x": 1225,
                    "y": 111
                },
                {
                    "x": 1225,
                    "y": 158
                },
                {
                    "x": 444,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='49' style='font-size:16px'>Published as a conference paper at ICLR 2015</header>",
            "id": 49,
            "page": 5,
            "text": "Published as a conference paper at ICLR 2015"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 346
                },
                {
                    "x": 2110,
                    "y": 346
                },
                {
                    "x": 2110,
                    "y": 486
                },
                {
                    "x": 441,
                    "y": 486
                }
            ],
            "category": "paragraph",
            "html": "<p id='50' style='font-size:18px'>model is trained to recognise objects over a wide range of scales. For speed reasons, we trained<br>multi-scale models by fine-tuning all layers of a single-scale model with the same configuration,<br>pre-trained with fixed S = 384.</p>",
            "id": 50,
            "page": 5,
            "text": "model is trained to recognise objects over a wide range of scales. For speed reasons, we trained multi-scale models by fine-tuning all layers of a single-scale model with the same configuration, pre-trained with fixed S = 384."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 537
                },
                {
                    "x": 711,
                    "y": 537
                },
                {
                    "x": 711,
                    "y": 586
                },
                {
                    "x": 444,
                    "y": 586
                }
            ],
            "category": "paragraph",
            "html": "<p id='51' style='font-size:20px'>3.2 TESTING</p>",
            "id": 51,
            "page": 5,
            "text": "3.2 TESTING"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 631
                },
                {
                    "x": 2109,
                    "y": 631
                },
                {
                    "x": 2109,
                    "y": 1229
                },
                {
                    "x": 441,
                    "y": 1229
                }
            ],
            "category": "paragraph",
            "html": "<p id='52' style='font-size:18px'>At test time, given a trained ConvNet and an input image, it is classified in the following way. First,<br>it is isotropically rescaled to a pre-defined smallest image side, denoted as Q (we also refer to it<br>as the test scale). We note that Q is not necessarily equal to the training scale S (as we will show<br>in Sect. 4, using several values of Q for each S leads to improved performance). Then, the network<br>is applied densely over the rescaled test image in a way similar to (Sermanet et al., 2014). Namely,<br>the fully-connected layers are first converted to convolutional layers (the first FC layer to a 7 x 7<br>conv. layer, the last two FC layers to 1 x 1 conv. layers). The resulting fully-convolutional net is<br>then applied to the whole (uncropped) image. The result is a class score map with the number of<br>channels equal to the number of classes, and a variable spatial resolution, dependent on the input<br>image size. Finally, to obtain a fixed-size vector of class scores for the image, the class score map is<br>spatially averaged (sum-pooled). We also augment the test set by horizontal flipping of the images;<br>the soft-max class posteriors of the original and flipped images are averaged to obtain the final scores<br>for the image.</p>",
            "id": 52,
            "page": 5,
            "text": "At test time, given a trained ConvNet and an input image, it is classified in the following way. First, it is isotropically rescaled to a pre-defined smallest image side, denoted as Q (we also refer to it as the test scale). We note that Q is not necessarily equal to the training scale S (as we will show in Sect. 4, using several values of Q for each S leads to improved performance). Then, the network is applied densely over the rescaled test image in a way similar to (Sermanet , 2014). Namely, the fully-connected layers are first converted to convolutional layers (the first FC layer to a 7 x 7 conv. layer, the last two FC layers to 1 x 1 conv. layers). The resulting fully-convolutional net is then applied to the whole (uncropped) image. The result is a class score map with the number of channels equal to the number of classes, and a variable spatial resolution, dependent on the input image size. Finally, to obtain a fixed-size vector of class scores for the image, the class score map is spatially averaged (sum-pooled). We also augment the test set by horizontal flipping of the images; the soft-max class posteriors of the original and flipped images are averaged to obtain the final scores for the image."
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 1248
                },
                {
                    "x": 2109,
                    "y": 1248
                },
                {
                    "x": 2109,
                    "y": 1848
                },
                {
                    "x": 440,
                    "y": 1848
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='53' style='font-size:18px'>Since the fully-convolutional network is applied over the whole image, there is no need to sample<br>multiple crops at test time (Krizhevsky et al., 2012), which is less efficient as it requires network<br>re-computation for each crop. At the same time, using a large set of crops, as done by Szegedy et al.<br>(2014), can lead to improved accuracy, as it results in a finer sampling of the input image compared<br>to the fully-convolutional net. Also, multi-crop evaluation is complementary to dense evaluation due<br>to different convolution boundary conditions: when applying a ConvNet to a crop, the convolved<br>feature maps are padded with zeros, while in the case of dense evaluation the padding for the same<br>crop naturally comes from the neighbouring parts of an image (due to both the convolutions and<br>spatial pooling), which substantially increases the overall network receptive field, SO more context<br>is captured. While we believe that in practice the increased computation time of multiple crops does<br>not justify the potential gains in accuracy, for reference we also evaluate our networks using 50 crops<br>per scale (5 x 5 regular grid with 2 flips), for a total of 150 crops over 3 scales, which is comparable<br>to 144 crops over 4 scales used by Szegedy et al. (2014).</p>",
            "id": 53,
            "page": 5,
            "text": "Since the fully-convolutional network is applied over the whole image, there is no need to sample multiple crops at test time (Krizhevsky , 2012), which is less efficient as it requires network re-computation for each crop. At the same time, using a large set of crops, as done by Szegedy  (2014), can lead to improved accuracy, as it results in a finer sampling of the input image compared to the fully-convolutional net. Also, multi-crop evaluation is complementary to dense evaluation due to different convolution boundary conditions: when applying a ConvNet to a crop, the convolved feature maps are padded with zeros, while in the case of dense evaluation the padding for the same crop naturally comes from the neighbouring parts of an image (due to both the convolutions and spatial pooling), which substantially increases the overall network receptive field, SO more context is captured. While we believe that in practice the increased computation time of multiple crops does not justify the potential gains in accuracy, for reference we also evaluate our networks using 50 crops per scale (5 x 5 regular grid with 2 flips), for a total of 150 crops over 3 scales, which is comparable to 144 crops over 4 scales used by Szegedy  (2014)."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1899
                },
                {
                    "x": 1042,
                    "y": 1899
                },
                {
                    "x": 1042,
                    "y": 1947
                },
                {
                    "x": 443,
                    "y": 1947
                }
            ],
            "category": "paragraph",
            "html": "<p id='54' style='font-size:18px'>3.3 IMPLEMENTATION DETAILS</p>",
            "id": 54,
            "page": 5,
            "text": "3.3 IMPLEMENTATION DETAILS"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1991
                },
                {
                    "x": 2108,
                    "y": 1991
                },
                {
                    "x": 2108,
                    "y": 2361
                },
                {
                    "x": 442,
                    "y": 2361
                }
            ],
            "category": "paragraph",
            "html": "<p id='55' style='font-size:20px'>Our implementation is derived from the publicly available C++ Caffe toolbox (Jia, 2013) (branched<br>out in December 2013), but contains a number of significant modifications, allowing us to perform<br>training and evaluation on multiple GPUs installed in a single system, as well as train and evaluate on<br>full-size (uncropped) images at multiple scales (as described above). Multi-GPU training exploits<br>data parallelism, and is carried out by splitting each batch of training images into several GPU<br>batches, processed in parallel on each GPU. After the GPU batch gradients are computed, they are<br>averaged to obtain the gradient of the full batch. Gradient computation is synchronous across the<br>GPUs, SO the result is exactly the same as when training on a single GPU.</p>",
            "id": 55,
            "page": 5,
            "text": "Our implementation is derived from the publicly available C++ Caffe toolbox (Jia, 2013) (branched out in December 2013), but contains a number of significant modifications, allowing us to perform training and evaluation on multiple GPUs installed in a single system, as well as train and evaluate on full-size (uncropped) images at multiple scales (as described above). Multi-GPU training exploits data parallelism, and is carried out by splitting each batch of training images into several GPU batches, processed in parallel on each GPU. After the GPU batch gradients are computed, they are averaged to obtain the gradient of the full batch. Gradient computation is synchronous across the GPUs, SO the result is exactly the same as when training on a single GPU."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2382
                },
                {
                    "x": 2107,
                    "y": 2382
                },
                {
                    "x": 2107,
                    "y": 2615
                },
                {
                    "x": 441,
                    "y": 2615
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='56' style='font-size:20px'>While more sophisticated methods of speeding up ConvNet training have been recently pro-<br>posed (Krizhevsky, 2014), which employ model and data parallelism for different layers of the net,<br>we have found that our conceptually much simpler scheme already provides a speedup of 3.75 times<br>on an off-the-shelf 4-GPU system, as compared to using a single GPU. On a system equipped with<br>four NVIDIA Titan Black GPUs, training a single net took 2-3 weeks depending on the architecture.</p>",
            "id": 56,
            "page": 5,
            "text": "While more sophisticated methods of speeding up ConvNet training have been recently proposed (Krizhevsky, 2014), which employ model and data parallelism for different layers of the net, we have found that our conceptually much simpler scheme already provides a speedup of 3.75 times on an off-the-shelf 4-GPU system, as compared to using a single GPU. On a system equipped with four NVIDIA Titan Black GPUs, training a single net took 2-3 weeks depending on the architecture."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2674
                },
                {
                    "x": 1204,
                    "y": 2674
                },
                {
                    "x": 1204,
                    "y": 2727
                },
                {
                    "x": 444,
                    "y": 2727
                }
            ],
            "category": "paragraph",
            "html": "<p id='57' style='font-size:22px'>4 CLASSIFICATION EXPERIMENTS</p>",
            "id": 57,
            "page": 5,
            "text": "4 CLASSIFICATION EXPERIMENTS"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2774
                },
                {
                    "x": 2109,
                    "y": 2774
                },
                {
                    "x": 2109,
                    "y": 3055
                },
                {
                    "x": 441,
                    "y": 3055
                }
            ],
            "category": "paragraph",
            "html": "<p id='58' style='font-size:18px'>Dataset. In this section, we present the image classification results achieved by the described<br>ConvNet architectures on the ILSVRC-2012 dataset (which was used for ILSVRC 2012-2014 chal-<br>lenges). The dataset includes images of 1000 classes, and is split into three sets: training (1.3M<br>images), validation (50K images), and testing (100K images with held-out class labels). The clas-<br>sification performance is evaluated using two measures: the top-1 and top-5 error. The former is a<br>multi-class classification error, i.e. the proportion of incorrectly classified images; the latter is the</p>",
            "id": 58,
            "page": 5,
            "text": "Dataset. In this section, we present the image classification results achieved by the described ConvNet architectures on the ILSVRC-2012 dataset (which was used for ILSVRC 2012-2014 challenges). The dataset includes images of 1000 classes, and is split into three sets: training (1.3M images), validation (50K images), and testing (100K images with held-out class labels). The classification performance is evaluated using two measures: the top-1 and top-5 error. The former is a multi-class classification error, i.e. the proportion of incorrectly classified images; the latter is the"
        },
        {
            "bounding_box": [
                {
                    "x": 1261,
                    "y": 3132
                },
                {
                    "x": 1288,
                    "y": 3132
                },
                {
                    "x": 1288,
                    "y": 3170
                },
                {
                    "x": 1261,
                    "y": 3170
                }
            ],
            "category": "footer",
            "html": "<footer id='59' style='font-size:14px'>5</footer>",
            "id": 59,
            "page": 5,
            "text": "5"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 111
                },
                {
                    "x": 1225,
                    "y": 111
                },
                {
                    "x": 1225,
                    "y": 158
                },
                {
                    "x": 444,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='60' style='font-size:18px'>Published as a conference paper at ICLR 2015</header>",
            "id": 60,
            "page": 6,
            "text": "Published as a conference paper at ICLR 2015"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 346
                },
                {
                    "x": 2106,
                    "y": 346
                },
                {
                    "x": 2106,
                    "y": 439
                },
                {
                    "x": 441,
                    "y": 439
                }
            ],
            "category": "paragraph",
            "html": "<p id='61' style='font-size:22px'>main evaluation criterion used in ILSVRC, and is computed as the proportion of images such that<br>the ground-truth category is outside the top-5 predicted categories.</p>",
            "id": 61,
            "page": 6,
            "text": "main evaluation criterion used in ILSVRC, and is computed as the proportion of images such that the ground-truth category is outside the top-5 predicted categories."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 463
                },
                {
                    "x": 2108,
                    "y": 463
                },
                {
                    "x": 2108,
                    "y": 602
                },
                {
                    "x": 441,
                    "y": 602
                }
            ],
            "category": "paragraph",
            "html": "<p id='62' style='font-size:16px'>For the majority of experiments, we used the validation set as the test set. Certain experiments were<br>also carried out on the test set and submitted to the official ILSVRC server as a \"VGG\" team entry<br>to the ILSVRC-2014 competition (Russakovsky et al., 2014).</p>",
            "id": 62,
            "page": 6,
            "text": "For the majority of experiments, we used the validation set as the test set. Certain experiments were also carried out on the test set and submitted to the official ILSVRC server as a \"VGG\" team entry to the ILSVRC-2014 competition (Russakovsky , 2014)."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 656
                },
                {
                    "x": 1062,
                    "y": 656
                },
                {
                    "x": 1062,
                    "y": 702
                },
                {
                    "x": 444,
                    "y": 702
                }
            ],
            "category": "paragraph",
            "html": "<p id='63' style='font-size:20px'>4.1 SINGLE SCALE EVALUATION</p>",
            "id": 63,
            "page": 6,
            "text": "4.1 SINGLE SCALE EVALUATION"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 747
                },
                {
                    "x": 2106,
                    "y": 747
                },
                {
                    "x": 2106,
                    "y": 888
                },
                {
                    "x": 441,
                    "y": 888
                }
            ],
            "category": "paragraph",
            "html": "<p id='64' style='font-size:22px'>We begin with evaluating the performance of individual ConvNet models at a single scale with the<br>layer configurations described in Sect. 2.2. The test image size was set as follows: Q = S for fixed<br>S, and Q = 0.5(Smin + Smax ) for jittered S E [Smin, Smax]. The results of are shown in Table 3.</p>",
            "id": 64,
            "page": 6,
            "text": "We begin with evaluating the performance of individual ConvNet models at a single scale with the layer configurations described in Sect. 2.2. The test image size was set as follows: Q = S for fixed S, and Q = 0.5(Smin + Smax ) for jittered S E [Smin, Smax]. The results of are shown in Table 3."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 911
                },
                {
                    "x": 2109,
                    "y": 911
                },
                {
                    "x": 2109,
                    "y": 1046
                },
                {
                    "x": 441,
                    "y": 1046
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='65' style='font-size:20px'>First, we note that using local response normalisation (A-LRN network) does not improve on the<br>model A without any normalisation layers. We thus do not employ normalisation in the deeper<br>architectures (B-E).</p>",
            "id": 65,
            "page": 6,
            "text": "First, we note that using local response normalisation (A-LRN network) does not improve on the model A without any normalisation layers. We thus do not employ normalisation in the deeper architectures (B-E)."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1073
                },
                {
                    "x": 2107,
                    "y": 1073
                },
                {
                    "x": 2107,
                    "y": 1621
                },
                {
                    "x": 442,
                    "y": 1621
                }
            ],
            "category": "paragraph",
            "html": "<p id='66' style='font-size:20px'>Second, we observe that the classification error decreases with the increased ConvNet depth: from<br>11 layers in A to 19 layers in E. Notably, in spite of the same depth, the configuration C (which<br>contains three 1 x 1 conv. layers), performs worse than the configuration D, which uses 3 x 3 conv.<br>layers throughout the network. This indicates that while the additional non-linearity does help (C is<br>better than B), it is also important to capture spatial context by using conv. filters with non-trivial<br>receptive fields (D is better than C). The error rate of our architecture saturates when the depth<br>reaches 19 layers, but even deeper models might be beneficial for larger datasets. We also compared<br>the net B with a shallow net with five 5 x 5 conv. layers, which was derived from B by replacing<br>each pair of 3 x 3 conv. layers with a single 5 x 5 conv. layer (which has the same receptive field as<br>explained in Sect. 2.3). The top-1 error of the shallow net was measured to be 7% higher than that<br>of B (on a center crop), which confirms that a deep net with small filters outperforms a shallow net<br>with larger filters.</p>",
            "id": 66,
            "page": 6,
            "text": "Second, we observe that the classification error decreases with the increased ConvNet depth: from 11 layers in A to 19 layers in E. Notably, in spite of the same depth, the configuration C (which contains three 1 x 1 conv. layers), performs worse than the configuration D, which uses 3 x 3 conv. layers throughout the network. This indicates that while the additional non-linearity does help (C is better than B), it is also important to capture spatial context by using conv. filters with non-trivial receptive fields (D is better than C). The error rate of our architecture saturates when the depth reaches 19 layers, but even deeper models might be beneficial for larger datasets. We also compared the net B with a shallow net with five 5 x 5 conv. layers, which was derived from B by replacing each pair of 3 x 3 conv. layers with a single 5 x 5 conv. layer (which has the same receptive field as explained in Sect. 2.3). The top-1 error of the shallow net was measured to be 7% higher than that of B (on a center crop), which confirms that a deep net with small filters outperforms a shallow net with larger filters."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1644
                },
                {
                    "x": 2109,
                    "y": 1644
                },
                {
                    "x": 2109,
                    "y": 1830
                },
                {
                    "x": 441,
                    "y": 1830
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='67' style='font-size:22px'>Finally, scale jittering at training time (S E [256; 512]) leads to significantly better results than<br>training on images with fixed smallest side (S = 256 or S = 384), even though a single scale is<br>used at test time. This confirms that training set augmentation by scale jittering is indeed helpful for<br>capturing multi-scale image statistics.</p>",
            "id": 67,
            "page": 6,
            "text": "Finally, scale jittering at training time (S E [256; 512]) leads to significantly better results than training on images with fixed smallest side (S = 256 or S = 384), even though a single scale is used at test time. This confirms that training set augmentation by scale jittering is indeed helpful for capturing multi-scale image statistics."
        },
        {
            "bounding_box": [
                {
                    "x": 807,
                    "y": 1847
                },
                {
                    "x": 1726,
                    "y": 1847
                },
                {
                    "x": 1726,
                    "y": 1893
                },
                {
                    "x": 807,
                    "y": 1893
                }
            ],
            "category": "caption",
            "html": "<br><caption id='68' style='font-size:20px'>Table 3: ConvNet performance at a single test scale.</caption>",
            "id": 68,
            "page": 6,
            "text": "Table 3: ConvNet performance at a single test scale."
        },
        {
            "bounding_box": [
                {
                    "x": 519,
                    "y": 1887
                },
                {
                    "x": 2023,
                    "y": 1887
                },
                {
                    "x": 2023,
                    "y": 2498
                },
                {
                    "x": 519,
                    "y": 2498
                }
            ],
            "category": "table",
            "html": "<br><table id='69' style='font-size:14px'><tr><td rowspan=\"2\">ConvNet config. (Table 1)</td><td colspan=\"2\">smallest image side</td><td rowspan=\"2\">top-1 val. error (%)</td><td rowspan=\"2\">top-5 val. error (%)</td></tr><tr><td>train (S)</td><td>test (Q)</td></tr><tr><td>A</td><td>256</td><td>256</td><td>29.6</td><td>10.4</td></tr><tr><td>A-LRN</td><td>256</td><td>256</td><td>29.7</td><td>10.5</td></tr><tr><td>B</td><td>256</td><td>256</td><td>28.7</td><td>9.9</td></tr><tr><td rowspan=\"3\">C</td><td>256</td><td>256</td><td>28.1</td><td>9.4</td></tr><tr><td>384</td><td>384</td><td>28.1</td><td>9.3</td></tr><tr><td>[256;512]</td><td>384</td><td>27.3</td><td>8.8</td></tr><tr><td rowspan=\"3\">D</td><td>256</td><td>256</td><td>27.0</td><td>8.8</td></tr><tr><td>384</td><td>384</td><td>26.8</td><td>8.7</td></tr><tr><td>[256;512]</td><td>384</td><td>25.6</td><td>8.1</td></tr><tr><td rowspan=\"3\">E</td><td>256</td><td>256</td><td>27.3</td><td>9.0</td></tr><tr><td>384</td><td>384</td><td>26.9</td><td>8.7</td></tr><tr><td>[256;512]</td><td>384</td><td>25.5</td><td>8.0</td></tr></table>",
            "id": 69,
            "page": 6,
            "text": "ConvNet config. (Table 1) smallest image side top-1 val. error (%) top-5 val. error (%)  train (S) test (Q)  A 256 256 29.6 10.4  A-LRN 256 256 29.7 10.5  B 256 256 28.7 9.9  C 256 256 28.1 9.4  384 384 28.1 9.3  [256;512] 384 27.3 8.8  D 256 256 27.0 8.8  384 384 26.8 8.7  [256;512] 384 25.6 8.1  E 256 256 27.3 9.0  384 384 26.9 8.7  [256;512] 384 25.5"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2592
                },
                {
                    "x": 1050,
                    "y": 2592
                },
                {
                    "x": 1050,
                    "y": 2638
                },
                {
                    "x": 444,
                    "y": 2638
                }
            ],
            "category": "paragraph",
            "html": "<p id='70' style='font-size:20px'>4.2 MULTI-SCALE EVALUATION</p>",
            "id": 70,
            "page": 6,
            "text": "4.2 MULTI-SCALE EVALUATION"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2683
                },
                {
                    "x": 2109,
                    "y": 2683
                },
                {
                    "x": 2109,
                    "y": 3057
                },
                {
                    "x": 442,
                    "y": 3057
                }
            ],
            "category": "paragraph",
            "html": "<p id='71' style='font-size:20px'>Having evaluated the ConvNet models at a single scale, we now assess the effect of scale jittering at<br>test time. It consists of running a model over several rescaled versions of a test image (corresponding<br>to different values of Q), followed by averaging the resulting class posteriors. Considering that a<br>large discrepancy between training and testing scales leads to a drop in performance, the models<br>trained with fixed S were evaluated over three test image sizes, close to the training one: Q =<br>{S - 32, S, S + 32}. At the same time, scale jittering at training time allows the network to be<br>applied to a wider range of scales at test time, SO the model trained with variable S E [Smin; Smax]<br>was evaluated over a larger range of sizes Q = {Smin, 0.5(Smin + Smax), Smax}.</p>",
            "id": 71,
            "page": 6,
            "text": "Having evaluated the ConvNet models at a single scale, we now assess the effect of scale jittering at test time. It consists of running a model over several rescaled versions of a test image (corresponding to different values of Q), followed by averaging the resulting class posteriors. Considering that a large discrepancy between training and testing scales leads to a drop in performance, the models trained with fixed S were evaluated over three test image sizes, close to the training one: Q = {S - 32, S, S + 32}. At the same time, scale jittering at training time allows the network to be applied to a wider range of scales at test time, SO the model trained with variable S E [Smin; Smax] was evaluated over a larger range of sizes Q = {Smin, 0.5(Smin + Smax), Smax}."
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3135
                },
                {
                    "x": 1289,
                    "y": 3135
                },
                {
                    "x": 1289,
                    "y": 3170
                },
                {
                    "x": 1260,
                    "y": 3170
                }
            ],
            "category": "footer",
            "html": "<footer id='72' style='font-size:18px'>6</footer>",
            "id": 72,
            "page": 6,
            "text": "6"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 111
                },
                {
                    "x": 1225,
                    "y": 111
                },
                {
                    "x": 1225,
                    "y": 158
                },
                {
                    "x": 444,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='73' style='font-size:16px'>Published as a conference paper at ICLR 2015</header>",
            "id": 73,
            "page": 7,
            "text": "Published as a conference paper at ICLR 2015"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 343
                },
                {
                    "x": 2109,
                    "y": 343
                },
                {
                    "x": 2109,
                    "y": 623
                },
                {
                    "x": 441,
                    "y": 623
                }
            ],
            "category": "paragraph",
            "html": "<p id='74' style='font-size:20px'>The results, presented in Table 4, indicate that scale jittering at test time leads to better performance<br>(as compared to evaluating the same model at a single scale, shown in Table 3). As before, the<br>deepest configurations (D and E) perform the best, and scale jittering is better than training with a<br>fixed smallest side S. Our best single-network performance on the validation set is 24.8%/7.5%<br>top-1/top-5 error (highlighted in bold in Table 4). On the test set, the configuration E achieves 7.3%<br>top-5 error.</p>",
            "id": 74,
            "page": 7,
            "text": "The results, presented in Table 4, indicate that scale jittering at test time leads to better performance (as compared to evaluating the same model at a single scale, shown in Table 3). As before, the deepest configurations (D and E) perform the best, and scale jittering is better than training with a fixed smallest side S. Our best single-network performance on the validation set is 24.8%/7.5% top-1/top-5 error (highlighted in bold in Table 4). On the test set, the configuration E achieves 7.3% top-5 error."
        },
        {
            "bounding_box": [
                {
                    "x": 791,
                    "y": 656
                },
                {
                    "x": 1743,
                    "y": 656
                },
                {
                    "x": 1743,
                    "y": 701
                },
                {
                    "x": 791,
                    "y": 701
                }
            ],
            "category": "caption",
            "html": "<caption id='75' style='font-size:18px'>Table 4: ConvNet performance at multiple test scales.</caption>",
            "id": 75,
            "page": 7,
            "text": "Table 4: ConvNet performance at multiple test scales."
        },
        {
            "bounding_box": [
                {
                    "x": 478,
                    "y": 697
                },
                {
                    "x": 2058,
                    "y": 697
                },
                {
                    "x": 2058,
                    "y": 1217
                },
                {
                    "x": 478,
                    "y": 1217
                }
            ],
            "category": "table",
            "html": "<br><table id='76' style='font-size:14px'><tr><td rowspan=\"2\">ConvNet config. (Table 1)</td><td colspan=\"2\">smallest image side</td><td rowspan=\"2\">top-1 val. error (%)</td><td rowspan=\"2\">top-5 val. error (%)</td></tr><tr><td>train (S)</td><td>test (Q)</td></tr><tr><td>B</td><td>256</td><td>224,256,288</td><td>28.2</td><td>9.6</td></tr><tr><td rowspan=\"3\">C</td><td>256</td><td>224,256,288</td><td>27.7</td><td>9.2</td></tr><tr><td>384</td><td>352,384,416</td><td>27.8</td><td>9.2</td></tr><tr><td>256;512</td><td>256,384,512</td><td>26.3</td><td>8.2</td></tr><tr><td rowspan=\"3\">D</td><td>256</td><td>224,256,288</td><td>26.6</td><td>8.6</td></tr><tr><td>384</td><td>352,384,416</td><td>26.5</td><td>8.6</td></tr><tr><td>256;512</td><td>256,384,512</td><td>24.8</td><td>7.5</td></tr><tr><td rowspan=\"3\">E</td><td>256</td><td>224,256,288</td><td>26.9</td><td>8.7</td></tr><tr><td>384</td><td>352,384,416</td><td>26.7</td><td>8.6</td></tr><tr><td>256; 512</td><td>256,384,512</td><td>24.8</td><td>7.5</td></tr></table>",
            "id": 76,
            "page": 7,
            "text": "ConvNet config. (Table 1) smallest image side top-1 val. error (%) top-5 val. error (%)  train (S) test (Q)  B 256 224,256,288 28.2 9.6  C 256 224,256,288 27.7 9.2  384 352,384,416 27.8 9.2  256;512 256,384,512 26.3 8.2  D 256 224,256,288 26.6 8.6  384 352,384,416 26.5 8.6  256;512 256,384,512 24.8 7.5  E 256 224,256,288 26.9 8.7  384 352,384,416 26.7 8.6  256; 512 256,384,512 24.8"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 1274
                },
                {
                    "x": 1020,
                    "y": 1274
                },
                {
                    "x": 1020,
                    "y": 1320
                },
                {
                    "x": 444,
                    "y": 1320
                }
            ],
            "category": "paragraph",
            "html": "<p id='77' style='font-size:16px'>4.3 MULTI-CROP EVALUATION</p>",
            "id": 77,
            "page": 7,
            "text": "4.3 MULTI-CROP EVALUATION"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1361
                },
                {
                    "x": 2109,
                    "y": 1361
                },
                {
                    "x": 2109,
                    "y": 1637
                },
                {
                    "x": 441,
                    "y": 1637
                }
            ],
            "category": "paragraph",
            "html": "<p id='78' style='font-size:18px'>In Table 5 we compare dense ConvNet evaluation with mult-crop evaluation (see Sect. 3.2 for de-<br>tails). We also assess the complementarity of the two evaluation techniques by averaging their soft-<br>max outputs. As can be seen, using multiple crops performs slightly better than dense evaluation,<br>and the two approaches are indeed complementary, as their combination outperforms each of them.<br>As noted above, we hypothesize that this is due to a different treatment of convolution boundary<br>conditions.</p>",
            "id": 78,
            "page": 7,
            "text": "In Table 5 we compare dense ConvNet evaluation with mult-crop evaluation (see Sect. 3.2 for details). We also assess the complementarity of the two evaluation techniques by averaging their softmax outputs. As can be seen, using multiple crops performs slightly better than dense evaluation, and the two approaches are indeed complementary, as their combination outperforms each of them. As noted above, we hypothesize that this is due to a different treatment of convolution boundary conditions."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1704
                },
                {
                    "x": 2104,
                    "y": 1704
                },
                {
                    "x": 2104,
                    "y": 1795
                },
                {
                    "x": 442,
                    "y": 1795
                }
            ],
            "category": "caption",
            "html": "<caption id='79' style='font-size:22px'>Table 5: ConvNet evaluation techniques comparison. In all experiments the training scale S was<br>sampled from [256; 512], and three test scales Q were considered: {256, 384, 512}.</caption>",
            "id": 79,
            "page": 7,
            "text": "Table 5: ConvNet evaluation techniques comparison. In all experiments the training scale S was sampled from [256; 512], and three test scales Q were considered: {256, 384, 512}."
        },
        {
            "bounding_box": [
                {
                    "x": 528,
                    "y": 1797
                },
                {
                    "x": 2008,
                    "y": 1797
                },
                {
                    "x": 2008,
                    "y": 2103
                },
                {
                    "x": 528,
                    "y": 2103
                }
            ],
            "category": "table",
            "html": "<br><table id='80' style='font-size:14px'><tr><td>ConvNet config. (Table 1)</td><td>Evaluation method</td><td>top-1 val. error (%)</td><td>top-5 val. error (%)</td></tr><tr><td rowspan=\"3\">D</td><td>dense</td><td>24.8</td><td>7.5</td></tr><tr><td>multi-crop</td><td>24.6</td><td>7.5</td></tr><tr><td>multi-crop & dense</td><td>24.4</td><td>7.2</td></tr><tr><td rowspan=\"3\">E</td><td>dense</td><td>24.8</td><td>7.5</td></tr><tr><td>multi-crop</td><td>24.6</td><td>7.4</td></tr><tr><td>multi-crop & dense</td><td>24.4</td><td>7.1</td></tr></table>",
            "id": 80,
            "page": 7,
            "text": "ConvNet config. (Table 1) Evaluation method top-1 val. error (%) top-5 val. error (%)  D dense 24.8 7.5  multi-crop 24.6 7.5  multi-crop & dense 24.4 7.2  E dense 24.8 7.5  multi-crop 24.6 7.4  multi-crop & dense 24.4"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2167
                },
                {
                    "x": 885,
                    "y": 2167
                },
                {
                    "x": 885,
                    "y": 2213
                },
                {
                    "x": 443,
                    "y": 2213
                }
            ],
            "category": "paragraph",
            "html": "<p id='81' style='font-size:20px'>4.4 CONVNET FUSION</p>",
            "id": 81,
            "page": 7,
            "text": "4.4 CONVNET FUSION"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2256
                },
                {
                    "x": 2108,
                    "y": 2256
                },
                {
                    "x": 2108,
                    "y": 2483
                },
                {
                    "x": 442,
                    "y": 2483
                }
            ],
            "category": "paragraph",
            "html": "<p id='82' style='font-size:20px'>Up until now, we evaluated the performance of individual ConvNet models. In this part of the exper-<br>iments, we combine the outputs of several models by averaging their soft-max class posteriors. This<br>improves the performance due to complementarity of the models, and was used in the top ILSVRC<br>submissions in 2012 (Krizhevsky et al., 2012) and 2013 (Zeiler & Fergus, 2013; Sermanet et al.,<br>2014).</p>",
            "id": 82,
            "page": 7,
            "text": "Up until now, we evaluated the performance of individual ConvNet models. In this part of the experiments, we combine the outputs of several models by averaging their soft-max class posteriors. This improves the performance due to complementarity of the models, and was used in the top ILSVRC submissions in 2012 (Krizhevsky , 2012) and 2013 (Zeiler & Fergus, 2013; Sermanet , 2014)."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2507
                },
                {
                    "x": 2108,
                    "y": 2507
                },
                {
                    "x": 2108,
                    "y": 2830
                },
                {
                    "x": 441,
                    "y": 2830
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='83' style='font-size:20px'>The results are shown in Table 6. By the time of ILSVRC submission we had only trained the<br>single-scale networks, as well as a multi-scale model D (by fine-tuning only the fully-connected<br>layers rather than all layers). The resulting ensemble of 7 networks has 7.3% ILSVRC test error.<br>After the submission, we considered an ensemble of only two best-performing multi-scale models<br>(configurations D and E), which reduced the test error to 7.0% using dense evaluation and 6.8%<br>using combined dense and multi-crop evaluation. For reference, our best-performing single model<br>achieves 7.1% error (model E, Table 5).</p>",
            "id": 83,
            "page": 7,
            "text": "The results are shown in Table 6. By the time of ILSVRC submission we had only trained the single-scale networks, as well as a multi-scale model D (by fine-tuning only the fully-connected layers rather than all layers). The resulting ensemble of 7 networks has 7.3% ILSVRC test error. After the submission, we considered an ensemble of only two best-performing multi-scale models (configurations D and E), which reduced the test error to 7.0% using dense evaluation and 6.8% using combined dense and multi-crop evaluation. For reference, our best-performing single model achieves 7.1% error (model E, Table 5)."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2871
                },
                {
                    "x": 1325,
                    "y": 2871
                },
                {
                    "x": 1325,
                    "y": 2918
                },
                {
                    "x": 444,
                    "y": 2918
                }
            ],
            "category": "paragraph",
            "html": "<p id='84' style='font-size:14px'>4.5 COMPARISON WITH THE STATE OF THE ART</p>",
            "id": 84,
            "page": 7,
            "text": "4.5 COMPARISON WITH THE STATE OF THE ART"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2958
                },
                {
                    "x": 2109,
                    "y": 2958
                },
                {
                    "x": 2109,
                    "y": 3052
                },
                {
                    "x": 442,
                    "y": 3052
                }
            ],
            "category": "paragraph",
            "html": "<p id='85' style='font-size:18px'>Finally, we compare our results with the state of the art in Table 7. In the classification task of<br>ILSVRC-2014 challenge (Russakovsky et al., 2014), our \"VGG\" team secured the 2nd place with</p>",
            "id": 85,
            "page": 7,
            "text": "Finally, we compare our results with the state of the art in Table 7. In the classification task of ILSVRC-2014 challenge (Russakovsky , 2014), our \"VGG\" team secured the 2nd place with"
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3134
                },
                {
                    "x": 1289,
                    "y": 3134
                },
                {
                    "x": 1289,
                    "y": 3169
                },
                {
                    "x": 1260,
                    "y": 3169
                }
            ],
            "category": "footer",
            "html": "<footer id='86' style='font-size:18px'>7</footer>",
            "id": 86,
            "page": 7,
            "text": "7"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 112
                },
                {
                    "x": 1224,
                    "y": 112
                },
                {
                    "x": 1224,
                    "y": 157
                },
                {
                    "x": 445,
                    "y": 157
                }
            ],
            "category": "header",
            "html": "<header id='87' style='font-size:16px'>Published as a conference paper at ICLR 2015</header>",
            "id": 87,
            "page": 8,
            "text": "Published as a conference paper at ICLR 2015"
        },
        {
            "bounding_box": [
                {
                    "x": 903,
                    "y": 376
                },
                {
                    "x": 1628,
                    "y": 376
                },
                {
                    "x": 1628,
                    "y": 419
                },
                {
                    "x": 903,
                    "y": 419
                }
            ],
            "category": "caption",
            "html": "<caption id='88' style='font-size:20px'>Table 6: Multiple ConvNet fusion results.</caption>",
            "id": 88,
            "page": 8,
            "text": "Table 6: Multiple ConvNet fusion results."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 415
                },
                {
                    "x": 2151,
                    "y": 415
                },
                {
                    "x": 2151,
                    "y": 855
                },
                {
                    "x": 443,
                    "y": 855
                }
            ],
            "category": "table",
            "html": "<br><table id='89' style='font-size:16px'><tr><td rowspan=\"2\">Combined ConvNet models</td><td colspan=\"3\">Error</td></tr><tr><td colspan=\"3\">top-1 val top-5 val top-5 test</td></tr><tr><td colspan=\"4\">ILSVRC submission</td></tr><tr><td>(D/256/224,256,288), (D/384/352,384,416), (D/[256;512]/256,384,512) (C/256/224,256,288), (C/384/352,384,416) (E/256/224,256,288), (E/384/352,384,416)</td><td>24.7</td><td>7.5</td><td>7.3</td></tr><tr><td colspan=\"4\">post-submission</td></tr><tr><td>(D/[256:512]/256,384,512), (E/[256;512]/256,384,512), dense eval.</td><td>24.0</td><td>7.1</td><td>7.0</td></tr><tr><td>(D/[256:512]/256,384,512), (E/[256;512]/256,384,512), multi-crop</td><td>23.9</td><td>7.2</td><td>-</td></tr><tr><td>(D/[256;512]/256,384,512), (E/[256;512]/256,384,512), multi-crop & dense eval.</td><td>23.7</td><td>6.8</td><td>6.8</td></tr></table>",
            "id": 89,
            "page": 8,
            "text": "Combined ConvNet models Error  top-1 val top-5 val top-5 test  ILSVRC submission  (D/256/224,256,288), (D/384/352,384,416), (D/[256;512]/256,384,512) (C/256/224,256,288), (C/384/352,384,416) (E/256/224,256,288), (E/384/352,384,416) 24.7 7.5 7.3  post-submission  (D/[256:512]/256,384,512), (E/[256;512]/256,384,512), dense eval. 24.0 7.1 7.0  (D/[256:512]/256,384,512), (E/[256;512]/256,384,512), multi-crop 23.9 7.2  (D/[256;512]/256,384,512), (E/[256;512]/256,384,512), multi-crop & dense eval. 23.7 6.8"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 882
                },
                {
                    "x": 2106,
                    "y": 882
                },
                {
                    "x": 2106,
                    "y": 976
                },
                {
                    "x": 442,
                    "y": 976
                }
            ],
            "category": "paragraph",
            "html": "<p id='90' style='font-size:14px'>7.3% test error using an ensemble of 7 models. After the submission, we decreased the error rate to<br>6.8% using an ensemble of 2 models.</p>",
            "id": 90,
            "page": 8,
            "text": "7.3% test error using an ensemble of 7 models. After the submission, we decreased the error rate to 6.8% using an ensemble of 2 models."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1001
                },
                {
                    "x": 2106,
                    "y": 1001
                },
                {
                    "x": 2106,
                    "y": 1458
                },
                {
                    "x": 443,
                    "y": 1458
                }
            ],
            "category": "paragraph",
            "html": "<p id='91' style='font-size:18px'>As can be seen from Table 7, our very deep ConvNets significantly outperform the previous gener-<br>ation of models, which achieved the best results in the ILSVRC-2012 and ILSVRC-2013 competi-<br>tions. Our result is also competitive with respect to the classification task winner (GoogLeNet with<br>6.7% error) and substantially outperforms the ILSVRC-2013 winning submission Clarifai, which<br>achieved 11.2% with outside training data and 11.7% without it. This is remarkable, considering<br>that our best result is achieved by combining just two models - significantly less than used in most<br>ILSVRC submissions. In terms of the single-net performance, our architecture achieves the best<br>result (7.0% test error), outperforming a single GoogLeNet by 0.9%. Notably, we did not depart<br>from the classical ConvNet architecture of LeCun et al. (1989), but improved it by substantially<br>increasing the depth.</p>",
            "id": 91,
            "page": 8,
            "text": "As can be seen from Table 7, our very deep ConvNets significantly outperform the previous generation of models, which achieved the best results in the ILSVRC-2012 and ILSVRC-2013 competitions. Our result is also competitive with respect to the classification task winner (GoogLeNet with 6.7% error) and substantially outperforms the ILSVRC-2013 winning submission Clarifai, which achieved 11.2% with outside training data and 11.7% without it. This is remarkable, considering that our best result is achieved by combining just two models - significantly less than used in most ILSVRC submissions. In terms of the single-net performance, our architecture achieves the best result (7.0% test error), outperforming a single GoogLeNet by 0.9%. Notably, we did not depart from the classical ConvNet architecture of LeCun  (1989), but improved it by substantially increasing the depth."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1505
                },
                {
                    "x": 2105,
                    "y": 1505
                },
                {
                    "x": 2105,
                    "y": 1594
                },
                {
                    "x": 443,
                    "y": 1594
                }
            ],
            "category": "caption",
            "html": "<caption id='92' style='font-size:18px'>Table 7: Comparison with the state of the art in ILSVRC classification. Our method is denoted<br>as \"VGG\" Only the results obtained without outside training data are reported.</caption>",
            "id": 92,
            "page": 8,
            "text": "Table 7: Comparison with the state of the art in ILSVRC classification. Our method is denoted as \"VGG\" Only the results obtained without outside training data are reported."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 1594
                },
                {
                    "x": 2152,
                    "y": 1594
                },
                {
                    "x": 2152,
                    "y": 2316
                },
                {
                    "x": 445,
                    "y": 2316
                }
            ],
            "category": "table",
            "html": "<br><table id='93' style='font-size:14px'><tr><td>Method</td><td>top-1 val. error (%)</td><td>top-5 val. error (%)</td><td>top-5 test error (%)</td></tr><tr><td>VGG (2 nets, multi-crop & dense eval.)</td><td>23.7</td><td>6.8</td><td>6.8</td></tr><tr><td>VGG (1 net, multi-crop & dense eval.)</td><td>24.4</td><td>7.1</td><td>7.0</td></tr><tr><td>VGG (ILSVRC submission, 7 nets, dense eval.)</td><td>24.7</td><td>7.5</td><td>7.3</td></tr><tr><td>GoogLeNet (Szegedy et al., 2014) (1 net)</td><td>-</td><td colspan=\"2\">7.9</td></tr><tr><td>GoogLeNet (Szegedy et al., 2014) (7 nets)</td><td>-</td><td colspan=\"2\">6.7</td></tr><tr><td>MSRA (He et al., 2014) (11 nets)</td><td>-</td><td></td><td>8.1</td></tr><tr><td>MSRA (He et al., 2014) (1 net)</td><td>27.9</td><td>9.1</td><td>9.1</td></tr><tr><td>Clarifai (Russakovsky et al., 2014) (multiple nets)</td><td>-</td><td>-</td><td>11.7</td></tr><tr><td>Clarifai (Russakovsky et al., 2014) (1 net)</td><td>-</td><td>-</td><td>12.5</td></tr><tr><td>Zeiler & Fergus (Zeiler & Fergus, 2013) (6 nets)</td><td>36.0</td><td>14.7</td><td>14.8</td></tr><tr><td>Zeiler & Fergus (Zeiler & Fergus, 2013) (1 net)</td><td>37.5</td><td>16.0</td><td>16.1</td></tr><tr><td>OverFeat (Sermanet et al., 2014) (7 nets)</td><td>34.0</td><td>13.2</td><td>13.6</td></tr><tr><td>OverFeat (Sermanet et al., 2014) ( 1 net)</td><td>35.7</td><td>14.2</td><td>-</td></tr><tr><td>Krizhevsky et al. (Krizhevsky et al., 2012) (5 nets)</td><td>38.1</td><td>16.4</td><td>16.4</td></tr><tr><td>Krizhevsky et al. (Krizhevsky et al., 2012) (1 net)</td><td>40.7</td><td>18.2</td><td>-</td></tr></table>",
            "id": 93,
            "page": 8,
            "text": "Method top-1 val. error (%) top-5 val. error (%) top-5 test error (%)  VGG (2 nets, multi-crop & dense eval.) 23.7 6.8 6.8  VGG (1 net, multi-crop & dense eval.) 24.4 7.1 7.0  VGG (ILSVRC submission, 7 nets, dense eval.) 24.7 7.5 7.3  GoogLeNet (Szegedy , 2014) (1 net) - 7.9  GoogLeNet (Szegedy , 2014) (7 nets) - 6.7  MSRA (He , 2014) (11 nets) -  8.1  MSRA (He , 2014) (1 net) 27.9 9.1 9.1  Clarifai (Russakovsky , 2014) (multiple nets) - - 11.7  Clarifai (Russakovsky , 2014) (1 net) - - 12.5  Zeiler & Fergus (Zeiler & Fergus, 2013) (6 nets) 36.0 14.7 14.8  Zeiler & Fergus (Zeiler & Fergus, 2013) (1 net) 37.5 16.0 16.1  OverFeat (Sermanet , 2014) (7 nets) 34.0 13.2 13.6  OverFeat (Sermanet , 2014) ( 1 net) 35.7 14.2  Krizhevsky  (Krizhevsky , 2012) (5 nets) 38.1 16.4 16.4  Krizhevsky  (Krizhevsky , 2012) (1 net) 40.7 18.2"
        },
        {
            "bounding_box": [
                {
                    "x": 446,
                    "y": 2360
                },
                {
                    "x": 815,
                    "y": 2360
                },
                {
                    "x": 815,
                    "y": 2411
                },
                {
                    "x": 446,
                    "y": 2411
                }
            ],
            "category": "paragraph",
            "html": "<p id='94' style='font-size:22px'>5 CONCLUSION</p>",
            "id": 94,
            "page": 8,
            "text": "5 CONCLUSION"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2458
                },
                {
                    "x": 2107,
                    "y": 2458
                },
                {
                    "x": 2107,
                    "y": 2825
                },
                {
                    "x": 442,
                    "y": 2825
                }
            ],
            "category": "paragraph",
            "html": "<p id='95' style='font-size:18px'>In this work we evaluated very deep convolutional networks (up to 19 weight layers) for large-<br>scale image classification. It was demonstrated that the representation depth is beneficial for the<br>classification accuracy, and that state-of-the-art performance on the ImageNet challenge dataset can<br>be achieved using a conventional ConvNet architecture (LeCun et al., 1989; Krizhevsky et al., 2012)<br>with substantially increased depth. In the appendix, we also show that our models generalise well to<br>a wide range of tasks and datasets, matching or outperforming more complex recognition pipelines<br>built around less deep image representations. Our results yet again confirm the importance of depth<br>in visual representations.</p>",
            "id": 95,
            "page": 8,
            "text": "In this work we evaluated very deep convolutional networks (up to 19 weight layers) for largescale image classification. It was demonstrated that the representation depth is beneficial for the classification accuracy, and that state-of-the-art performance on the ImageNet challenge dataset can be achieved using a conventional ConvNet architecture (LeCun , 1989; Krizhevsky , 2012) with substantially increased depth. In the appendix, we also show that our models generalise well to a wide range of tasks and datasets, matching or outperforming more complex recognition pipelines built around less deep image representations. Our results yet again confirm the importance of depth in visual representations."
        },
        {
            "bounding_box": [
                {
                    "x": 446,
                    "y": 2874
                },
                {
                    "x": 860,
                    "y": 2874
                },
                {
                    "x": 860,
                    "y": 2915
                },
                {
                    "x": 446,
                    "y": 2915
                }
            ],
            "category": "paragraph",
            "html": "<p id='96' style='font-size:18px'>ACKNOWLEDGEMENTS</p>",
            "id": 96,
            "page": 8,
            "text": "ACKNOWLEDGEMENTS"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2959
                },
                {
                    "x": 2107,
                    "y": 2959
                },
                {
                    "x": 2107,
                    "y": 3052
                },
                {
                    "x": 442,
                    "y": 3052
                }
            ],
            "category": "paragraph",
            "html": "<p id='97' style='font-size:18px'>This work was supported by ERC grant VisRec no. 228180. We gratefully acknowledge the support<br>of NVIDIA Corporation with the donation of the GPUs used for this research.</p>",
            "id": 97,
            "page": 8,
            "text": "This work was supported by ERC grant VisRec no. 228180. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the GPUs used for this research."
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3133
                },
                {
                    "x": 1289,
                    "y": 3133
                },
                {
                    "x": 1289,
                    "y": 3169
                },
                {
                    "x": 1260,
                    "y": 3169
                }
            ],
            "category": "footer",
            "html": "<footer id='98' style='font-size:16px'>8</footer>",
            "id": 98,
            "page": 8,
            "text": "8"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 112
                },
                {
                    "x": 1224,
                    "y": 112
                },
                {
                    "x": 1224,
                    "y": 158
                },
                {
                    "x": 444,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='99' style='font-size:16px'>Published as a conference paper at ICLR 2015</header>",
            "id": 99,
            "page": 9,
            "text": "Published as a conference paper at ICLR 2015"
        },
        {
            "bounding_box": [
                {
                    "x": 446,
                    "y": 341
                },
                {
                    "x": 731,
                    "y": 341
                },
                {
                    "x": 731,
                    "y": 391
                },
                {
                    "x": 446,
                    "y": 391
                }
            ],
            "category": "paragraph",
            "html": "<p id='100' style='font-size:22px'>REFERENCES</p>",
            "id": 100,
            "page": 9,
            "text": "REFERENCES"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 472
                },
                {
                    "x": 2106,
                    "y": 472
                },
                {
                    "x": 2106,
                    "y": 556
                },
                {
                    "x": 443,
                    "y": 556
                }
            ],
            "category": "paragraph",
            "html": "<p id='101' style='font-size:14px'>Bell, S., Upchurch, P., Snavely, N., and Bala, K. Material recognition in the wild with the materials in context<br>database. CoRR, abs/1412.0623, 2014.</p>",
            "id": 101,
            "page": 9,
            "text": "Bell, S., Upchurch, P., Snavely, N., and Bala, K. Material recognition in the wild with the materials in context database. CoRR, abs/1412.0623, 2014."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 567
                },
                {
                    "x": 2107,
                    "y": 567
                },
                {
                    "x": 2107,
                    "y": 651
                },
                {
                    "x": 444,
                    "y": 651
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='102' style='font-size:14px'>Chatfield, K., Simonyan, K., Vedaldi, A., and Zisserman, A. Return of the devil in the details: Delving deep<br>into convolutional nets. In Proc. BMVC., 2014.</p>",
            "id": 102,
            "page": 9,
            "text": "Chatfield, K., Simonyan, K., Vedaldi, A., and Zisserman, A. Return of the devil in the details: Delving deep into convolutional nets. In Proc. BMVC., 2014."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 663
                },
                {
                    "x": 2103,
                    "y": 663
                },
                {
                    "x": 2103,
                    "y": 744
                },
                {
                    "x": 444,
                    "y": 744
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='103' style='font-size:16px'>Cimpoi, M., Maji, S., and Vedaldi, A. Deep convolutional filter banks for texture recognition and segmentation.<br>CoRR, abs/1411.6836, 2014.</p>",
            "id": 103,
            "page": 9,
            "text": "Cimpoi, M., Maji, S., and Vedaldi, A. Deep convolutional filter banks for texture recognition and segmentation. CoRR, abs/1411.6836, 2014."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 759
                },
                {
                    "x": 2107,
                    "y": 759
                },
                {
                    "x": 2107,
                    "y": 841
                },
                {
                    "x": 442,
                    "y": 841
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='104' style='font-size:16px'>Ciresan, D. C., Meier, U., Masci, J., Gambardella, L. M., and Schmidhuber, J. Flexible, high performance<br>convolutional neural networks for image classification. In IJCAI, pp. 1237-1242, 2011.</p>",
            "id": 104,
            "page": 9,
            "text": "Ciresan, D. C., Meier, U., Masci, J., Gambardella, L. M., and Schmidhuber, J. Flexible, high performance convolutional neural networks for image classification. In IJCAI, pp. 1237-1242, 2011."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 855
                },
                {
                    "x": 2106,
                    "y": 855
                },
                {
                    "x": 2106,
                    "y": 938
                },
                {
                    "x": 445,
                    "y": 938
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='105' style='font-size:16px'>Dean, J., Corrado, G., Monga, R., Chen, K., Devin, M., Mao, M., Ranzato, M., Senior, A., Tucker, P., Yang,<br>K., Le, Q. V., and Ng, A. Y. Large scale distributed deep networks. In NIPS, pp. 1232-1240, 2012.</p>",
            "id": 105,
            "page": 9,
            "text": "Dean, J., Corrado, G., Monga, R., Chen, K., Devin, M., Mao, M., Ranzato, M., Senior, A., Tucker, P., Yang, K., Le, Q. V., and Ng, A. Y. Large scale distributed deep networks. In NIPS, pp. 1232-1240, 2012."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 948
                },
                {
                    "x": 2107,
                    "y": 948
                },
                {
                    "x": 2107,
                    "y": 999
                },
                {
                    "x": 443,
                    "y": 999
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='106' style='font-size:20px'>Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image</p>",
            "id": 106,
            "page": 9,
            "text": "Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image"
        },
        {
            "bounding_box": [
                {
                    "x": 485,
                    "y": 991
                },
                {
                    "x": 972,
                    "y": 991
                },
                {
                    "x": 972,
                    "y": 1033
                },
                {
                    "x": 485,
                    "y": 1033
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='107' style='font-size:14px'>database. In Proc. CVPR, 2009.</p>",
            "id": 107,
            "page": 9,
            "text": "database. In Proc. CVPR, 2009."
        },
        {
            "bounding_box": [
                {
                    "x": 446,
                    "y": 1046
                },
                {
                    "x": 2104,
                    "y": 1046
                },
                {
                    "x": 2104,
                    "y": 1128
                },
                {
                    "x": 446,
                    "y": 1128
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='108' style='font-size:16px'>Donahue, J., Jia, Y., Vinyals, 0., Hoffman, J., Zhang, N., Tzeng, E., and Darrell, T. Decaf: A deep convolutional<br>activation feature for generic visual recognition. CoRR, abs/1310.1531, 2013.</p>",
            "id": 108,
            "page": 9,
            "text": "Donahue, J., Jia, Y., Vinyals, 0., Hoffman, J., Zhang, N., Tzeng, E., and Darrell, T. Decaf: A deep convolutional activation feature for generic visual recognition. CoRR, abs/1310.1531, 2013."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 1142
                },
                {
                    "x": 2103,
                    "y": 1142
                },
                {
                    "x": 2103,
                    "y": 1224
                },
                {
                    "x": 445,
                    "y": 1224
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='109' style='font-size:16px'>Everingham, M., Eslami, S. M. A., Van Gool, L., Williams, C., Winn, J., and Zisserman, A. The Pascal visual<br>object classes challenge: A retrospective. IJCV, 111(1):98-136, 2015.</p>",
            "id": 109,
            "page": 9,
            "text": "Everingham, M., Eslami, S. M. A., Van Gool, L., Williams, C., Winn, J., and Zisserman, A. The Pascal visual object classes challenge: A retrospective. IJCV, 111(1):98-136, 2015."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1235
                },
                {
                    "x": 2106,
                    "y": 1235
                },
                {
                    "x": 2106,
                    "y": 1359
                },
                {
                    "x": 442,
                    "y": 1359
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='110' style='font-size:16px'>Fei-Fei, L., Fergus, R., and Perona, P. Learning generative visual models from few training examples: An<br>incremental bayesian approach tested on 101 object categories. In IEEE CVPR Workshop of Generative<br>Model Based Vision, 2004.</p>",
            "id": 110,
            "page": 9,
            "text": "Fei-Fei, L., Fergus, R., and Perona, P. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In IEEE CVPR Workshop of Generative Model Based Vision, 2004."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 1373
                },
                {
                    "x": 2106,
                    "y": 1373
                },
                {
                    "x": 2106,
                    "y": 1455
                },
                {
                    "x": 444,
                    "y": 1455
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='111' style='font-size:14px'>Girshick, R. B., Donahue, J., Darrell, T., and Malik, J. Rich feature hierarchies for accurate object detection<br>and semantic segmentation. CoRR, abs/1311.2524v5, 2014. Published in Proc. CVPR, 2014.</p>",
            "id": 111,
            "page": 9,
            "text": "Girshick, R. B., Donahue, J., Darrell, T., and Malik, J. Rich feature hierarchies for accurate object detection and semantic segmentation. CoRR, abs/1311.2524v5, 2014. Published in Proc. CVPR, 2014."
        },
        {
            "bounding_box": [
                {
                    "x": 446,
                    "y": 1471
                },
                {
                    "x": 2102,
                    "y": 1471
                },
                {
                    "x": 2102,
                    "y": 1550
                },
                {
                    "x": 446,
                    "y": 1550
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='112' style='font-size:14px'>Gkioxari, G., Girshick, R., and Malik, J. Actions and attributes from wholes and parts. CoRR, abs/1412.2604,<br>2014.</p>",
            "id": 112,
            "page": 9,
            "text": "Gkioxari, G., Girshick, R., and Malik, J. Actions and attributes from wholes and parts. CoRR, abs/1412.2604, 2014."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 1565
                },
                {
                    "x": 2101,
                    "y": 1565
                },
                {
                    "x": 2101,
                    "y": 1647
                },
                {
                    "x": 444,
                    "y": 1647
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='113' style='font-size:16px'>Glorot, X. and Bengio, Y. Understanding the difficulty of training deep feedforward neural networks. In Proc.<br>AISTATS, volume 9, pp. 249-256, 2010.</p>",
            "id": 113,
            "page": 9,
            "text": "Glorot, X. and Bengio, Y. Understanding the difficulty of training deep feedforward neural networks. In Proc. AISTATS, volume 9, pp. 249-256, 2010."
        },
        {
            "bounding_box": [
                {
                    "x": 446,
                    "y": 1660
                },
                {
                    "x": 2104,
                    "y": 1660
                },
                {
                    "x": 2104,
                    "y": 1742
                },
                {
                    "x": 446,
                    "y": 1742
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='114' style='font-size:16px'>Goodfellow, I. J., Bulatov, Y., Ibarz, J., Arnoud, S., and Shet, V. Multi-digit number recognition from street<br>view imagery using deep convolutional neural networks. In Proc. ICLR, 2014.</p>",
            "id": 114,
            "page": 9,
            "text": "Goodfellow, I. J., Bulatov, Y., Ibarz, J., Arnoud, S., and Shet, V. Multi-digit number recognition from street view imagery using deep convolutional neural networks. In Proc. ICLR, 2014."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 1755
                },
                {
                    "x": 2106,
                    "y": 1755
                },
                {
                    "x": 2106,
                    "y": 1838
                },
                {
                    "x": 444,
                    "y": 1838
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='115' style='font-size:18px'>Griffin, G., Holub, A., and Perona, P. Caltech-256 object category dataset. Technical Report 7694, California<br>Institute of Technology, 2007.</p>",
            "id": 115,
            "page": 9,
            "text": "Griffin, G., Holub, A., and Perona, P. Caltech-256 object category dataset. Technical Report 7694, California Institute of Technology, 2007."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 1852
                },
                {
                    "x": 2104,
                    "y": 1852
                },
                {
                    "x": 2104,
                    "y": 1934
                },
                {
                    "x": 444,
                    "y": 1934
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='116' style='font-size:18px'>He, K., Zhang, X., Ren, S., and Sun, J. Spatial pyramid pooling in deep convolutional networks for visual<br>recognition. CoRR, abs/1406.4729v2, 2014.</p>",
            "id": 116,
            "page": 9,
            "text": "He, K., Zhang, X., Ren, S., and Sun, J. Spatial pyramid pooling in deep convolutional networks for visual recognition. CoRR, abs/1406.4729v2, 2014."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 1945
                },
                {
                    "x": 1719,
                    "y": 1945
                },
                {
                    "x": 1719,
                    "y": 1989
                },
                {
                    "x": 444,
                    "y": 1989
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='117' style='font-size:16px'>Hoai, M. Regularized max pooling for image categorization. In Proc. BMVC., 2014.</p>",
            "id": 117,
            "page": 9,
            "text": "Hoai, M. Regularized max pooling for image categorization. In Proc. BMVC., 2014."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2002
                },
                {
                    "x": 2104,
                    "y": 2002
                },
                {
                    "x": 2104,
                    "y": 2082
                },
                {
                    "x": 443,
                    "y": 2082
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='118' style='font-size:14px'>Howard, A. G. Some improvements on deep convolutional neural network based image classification. In Proc.<br>ICLR, 2014.</p>",
            "id": 118,
            "page": 9,
            "text": "Howard, A. G. Some improvements on deep convolutional neural network based image classification. In Proc. ICLR, 2014."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2095
                },
                {
                    "x": 2103,
                    "y": 2095
                },
                {
                    "x": 2103,
                    "y": 2179
                },
                {
                    "x": 444,
                    "y": 2179
                }
            ],
            "category": "paragraph",
            "html": "<p id='119' style='font-size:14px'>Jia, Y. Caffe: An open source convolutional architecture for fast feature embedding.<br>http : / /caffe · berkeleyvision · org/, 2013.</p>",
            "id": 119,
            "page": 9,
            "text": "Jia, Y. Caffe: An open source convolutional architecture for fast feature embedding. http : / /caffe · berkeleyvision · org/, 2013."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2192
                },
                {
                    "x": 2106,
                    "y": 2192
                },
                {
                    "x": 2106,
                    "y": 2272
                },
                {
                    "x": 444,
                    "y": 2272
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='120' style='font-size:20px'>Karpathy, A. and Fei-Fei, L. Deep visual-semantic alignments for generating image descriptions. CoRR,<br>abs/1412.2306, 2014.</p>",
            "id": 120,
            "page": 9,
            "text": "Karpathy, A. and Fei-Fei, L. Deep visual-semantic alignments for generating image descriptions. CoRR, abs/1412.2306, 2014."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2287
                },
                {
                    "x": 2104,
                    "y": 2287
                },
                {
                    "x": 2104,
                    "y": 2370
                },
                {
                    "x": 444,
                    "y": 2370
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='121' style='font-size:16px'>Kiros, R., Salakhutdinov, R., and Zemel, R. S. Unifying visual-semantic embeddings with multimodal neural<br>language models. CoRR, abs/1411.2539, 2014.</p>",
            "id": 121,
            "page": 9,
            "text": "Kiros, R., Salakhutdinov, R., and Zemel, R. S. Unifying visual-semantic embeddings with multimodal neural language models. CoRR, abs/1411.2539, 2014."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2385
                },
                {
                    "x": 2087,
                    "y": 2385
                },
                {
                    "x": 2087,
                    "y": 2424
                },
                {
                    "x": 444,
                    "y": 2424
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='122' style='font-size:16px'>Krizhevsky, A. One weird trick for parallelizing convolutional neural networks. CoRR, abs/1404.5997, 2014.</p>",
            "id": 122,
            "page": 9,
            "text": "Krizhevsky, A. One weird trick for parallelizing convolutional neural networks. CoRR, abs/1404.5997, 2014."
        },
        {
            "bounding_box": [
                {
                    "x": 446,
                    "y": 2438
                },
                {
                    "x": 2104,
                    "y": 2438
                },
                {
                    "x": 2104,
                    "y": 2520
                },
                {
                    "x": 446,
                    "y": 2520
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='123' style='font-size:16px'>Krizhevsky, A., Sutskever, I., and Hinton, G. E. ImageNet classification with deep convolutional neural net-<br>works. In NIPS, pp. 1106-1114, 2012.</p>",
            "id": 123,
            "page": 9,
            "text": "Krizhevsky, A., Sutskever, I., and Hinton, G. E. ImageNet classification with deep convolutional neural networks. In NIPS, pp. 1106-1114, 2012."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2531
                },
                {
                    "x": 2102,
                    "y": 2531
                },
                {
                    "x": 2102,
                    "y": 2616
                },
                {
                    "x": 443,
                    "y": 2616
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='124' style='font-size:16px'>LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, L. D. Backpropa-<br>gation applied to handwritten zip code recognition. Neural Computation, 1(4):541-551, 1989.</p>",
            "id": 124,
            "page": 9,
            "text": "LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, L. D. Backpropagation applied to handwritten zip code recognition. Neural Computation, 1(4):541-551, 1989."
        },
        {
            "bounding_box": [
                {
                    "x": 447,
                    "y": 2628
                },
                {
                    "x": 1556,
                    "y": 2628
                },
                {
                    "x": 1556,
                    "y": 2669
                },
                {
                    "x": 447,
                    "y": 2669
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='125' style='font-size:14px'>Lin, M., Chen, Q., and Yan, S. Network in network. In Proc. ICLR, 2014.</p>",
            "id": 125,
            "page": 9,
            "text": "Lin, M., Chen, Q., and Yan, S. Network in network. In Proc. ICLR, 2014."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2682
                },
                {
                    "x": 2104,
                    "y": 2682
                },
                {
                    "x": 2104,
                    "y": 2762
                },
                {
                    "x": 444,
                    "y": 2762
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='126' style='font-size:16px'>Long, J., Shelhamer, E., and Darrell, T. Fully convolutional networks for semantic segmentation. CoRR,<br>abs/1411.4038, 2014.</p>",
            "id": 126,
            "page": 9,
            "text": "Long, J., Shelhamer, E., and Darrell, T. Fully convolutional networks for semantic segmentation. CoRR, abs/1411.4038, 2014."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2777
                },
                {
                    "x": 2106,
                    "y": 2777
                },
                {
                    "x": 2106,
                    "y": 2857
                },
                {
                    "x": 443,
                    "y": 2857
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='127' style='font-size:16px'>Oquab, M., Bottou, L., Laptev, I., and Sivic, J. Learning and Transferring Mid-Level Image Representations<br>using Convolutional Neural Networks. In Proc. CVPR, 2014.</p>",
            "id": 127,
            "page": 9,
            "text": "Oquab, M., Bottou, L., Laptev, I., and Sivic, J. Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks. In Proc. CVPR, 2014."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2872
                },
                {
                    "x": 2107,
                    "y": 2872
                },
                {
                    "x": 2107,
                    "y": 2955
                },
                {
                    "x": 443,
                    "y": 2955
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='128' style='font-size:16px'>Perronnin, F., Sanchez, J., and Mensink, T. Improving the Fisher kernel for large-scale image classification. In<br>Proc. ECCV, 2010.</p>",
            "id": 128,
            "page": 9,
            "text": "Perronnin, F., Sanchez, J., and Mensink, T. Improving the Fisher kernel for large-scale image classification. In Proc. ECCV, 2010."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2969
                },
                {
                    "x": 2107,
                    "y": 2969
                },
                {
                    "x": 2107,
                    "y": 3052
                },
                {
                    "x": 443,
                    "y": 3052
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='129' style='font-size:16px'>Razavian, A., Azizpour, H., Sullivan, J., and Carlsson, S. CNN Features off-the-shelf: an Astounding Baseline<br>for Recognition. CoRR, abs/1403.6382, 2014.</p>",
            "id": 129,
            "page": 9,
            "text": "Razavian, A., Azizpour, H., Sullivan, J., and Carlsson, S. CNN Features off-the-shelf: an Astounding Baseline for Recognition. CoRR, abs/1403.6382, 2014."
        },
        {
            "bounding_box": [
                {
                    "x": 1259,
                    "y": 3133
                },
                {
                    "x": 1289,
                    "y": 3133
                },
                {
                    "x": 1289,
                    "y": 3169
                },
                {
                    "x": 1259,
                    "y": 3169
                }
            ],
            "category": "footer",
            "html": "<footer id='130' style='font-size:16px'>9</footer>",
            "id": 130,
            "page": 9,
            "text": "9"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 112
                },
                {
                    "x": 1224,
                    "y": 112
                },
                {
                    "x": 1224,
                    "y": 158
                },
                {
                    "x": 444,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='131' style='font-size:16px'>Published as a conference paper at ICLR 2015</header>",
            "id": 131,
            "page": 10,
            "text": "Published as a conference paper at ICLR 2015"
        },
        {
            "bounding_box": [
                {
                    "x": 437,
                    "y": 338
                },
                {
                    "x": 2108,
                    "y": 338
                },
                {
                    "x": 2108,
                    "y": 961
                },
                {
                    "x": 437,
                    "y": 961
                }
            ],
            "category": "paragraph",
            "html": "<p id='132' style='font-size:16px'>Russakovsky, 0., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A.,<br>Bernstein, M., Berg, A. C., and Fei-Fei, L. ImageNet large scale visual recognition challenge. CoRR,<br>abs/1409.0575, 2014.<br>Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., and LeCun, Y. OverFeat: Integrated Recognition,<br>Localization and Detection using Convolutional Networks. In Proc. ICLR, 2014.<br>Simonyan, K. and Zisserman, A. Two-stream convolutional networks for action recognition in videos. CoRR,<br>abs/1406.2199, 2014. Published in Proc. NIPS, 2014.<br>Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich,<br>A. Going deeper with convolutions. CoRR, abs/1409.4842, 2014.<br>Wei, Y., Xia, W., Huang, J., Ni, B., Dong, J., Zhao, Y., and Yan, S. CNN: Single-label to multi-label. CoRR,<br>abs/1406.5726, 2014.<br>Zeiler, M. D. and Fergus, R. Visualizing and understanding convolutional networks. CoRR, abs/1311.2901,<br>2013. Published in Proc. ECCV, 2014.</p>",
            "id": 132,
            "page": 10,
            "text": "Russakovsky, 0., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. ImageNet large scale visual recognition challenge. CoRR, abs/1409.0575, 2014. Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., and LeCun, Y. OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks. In Proc. ICLR, 2014. Simonyan, K. and Zisserman, A. Two-stream convolutional networks for action recognition in videos. CoRR, abs/1406.2199, 2014. Published in Proc. NIPS, 2014. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich, A. Going deeper with convolutions. CoRR, abs/1409.4842, 2014. Wei, Y., Xia, W., Huang, J., Ni, B., Dong, J., Zhao, Y., and Yan, S. CNN: Single-label to multi-label. CoRR, abs/1406.5726, 2014. Zeiler, M. D. and Fergus, R. Visualizing and understanding convolutional networks. CoRR, abs/1311.2901, 2013. Published in Proc. ECCV, 2014."
        },
        {
            "bounding_box": [
                {
                    "x": 446,
                    "y": 1041
                },
                {
                    "x": 861,
                    "y": 1041
                },
                {
                    "x": 861,
                    "y": 1091
                },
                {
                    "x": 446,
                    "y": 1091
                }
            ],
            "category": "paragraph",
            "html": "<p id='133' style='font-size:22px'>A LOCALISATION</p>",
            "id": 133,
            "page": 10,
            "text": "A LOCALISATION"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1138
                },
                {
                    "x": 2107,
                    "y": 1138
                },
                {
                    "x": 2107,
                    "y": 1464
                },
                {
                    "x": 441,
                    "y": 1464
                }
            ],
            "category": "paragraph",
            "html": "<p id='134' style='font-size:18px'>In the main body of the paper we have considered the classification task of the ILSVRC challenge,<br>and performed a thorough evaluation of ConvNet architectures of different depth. In this section,<br>we turn to the localisation task of the challenge, which we have won in 2014 with 25.3% error. It<br>can be seen as a special case of object detection, where a single object bounding box should be<br>predicted for each of the top-5 classes, irrespective of the actual number of objects of the class. For<br>this we adopt the approach of Sermanet et al. (2014), the winners of the ILSVRC-2013 localisation<br>challenge, with a few modifications. Our methodis described in Sect. A.1 and evaluated in Sect. A.2.</p>",
            "id": 134,
            "page": 10,
            "text": "In the main body of the paper we have considered the classification task of the ILSVRC challenge, and performed a thorough evaluation of ConvNet architectures of different depth. In this section, we turn to the localisation task of the challenge, which we have won in 2014 with 25.3% error. It can be seen as a special case of object detection, where a single object bounding box should be predicted for each of the top-5 classes, irrespective of the actual number of objects of the class. For this we adopt the approach of Sermanet  (2014), the winners of the ILSVRC-2013 localisation challenge, with a few modifications. Our methodis described in Sect. A.1 and evaluated in Sect. A.2."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 1510
                },
                {
                    "x": 1029,
                    "y": 1510
                },
                {
                    "x": 1029,
                    "y": 1556
                },
                {
                    "x": 445,
                    "y": 1556
                }
            ],
            "category": "paragraph",
            "html": "<p id='135' style='font-size:16px'>A.1 LOCALISATION CONVNET</p>",
            "id": 135,
            "page": 10,
            "text": "A.1 LOCALISATION CONVNET"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1600
                },
                {
                    "x": 2108,
                    "y": 1600
                },
                {
                    "x": 2108,
                    "y": 1970
                },
                {
                    "x": 442,
                    "y": 1970
                }
            ],
            "category": "paragraph",
            "html": "<p id='136' style='font-size:20px'>To perform object localisation, we use a very deep ConvNet, where the last fully connected layer<br>predicts the bounding box location instead of the class scores. A bounding box is represented by<br>a 4-D vector storing its center coordinates, width, and height. There is a choice of whether the<br>bounding box prediction is shared across all classes (single-class regression, SCR (Sermanet et al.,<br>2014)) or is class-specific (per-class regression, PCR). In the former case, the last layer is 4-D, while<br>in the latter it is 4000-D (since there are 1000 classes in the dataset). Apart from the last bounding<br>box prediction layer, we use the ConvNet architecture D (Table 1), which contains 16 weight layers<br>and was found to be the best-performing in the classification task (Sect. 4).</p>",
            "id": 136,
            "page": 10,
            "text": "To perform object localisation, we use a very deep ConvNet, where the last fully connected layer predicts the bounding box location instead of the class scores. A bounding box is represented by a 4-D vector storing its center coordinates, width, and height. There is a choice of whether the bounding box prediction is shared across all classes (single-class regression, SCR (Sermanet , 2014)) or is class-specific (per-class regression, PCR). In the former case, the last layer is 4-D, while in the latter it is 4000-D (since there are 1000 classes in the dataset). Apart from the last bounding box prediction layer, we use the ConvNet architecture D (Table 1), which contains 16 weight layers and was found to be the best-performing in the classification task (Sect. 4)."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2013
                },
                {
                    "x": 2108,
                    "y": 2013
                },
                {
                    "x": 2108,
                    "y": 2430
                },
                {
                    "x": 442,
                    "y": 2430
                }
            ],
            "category": "paragraph",
            "html": "<p id='137' style='font-size:18px'>Training. Training of localisation ConvNets is similar to that of the classification ConvNets<br>(Sect. 3.1). The main difference is that we replace the logistic regression objective with a Euclidean<br>loss, which penalises the deviation of the predicted bounding box parameters from the ground-truth.<br>We trained two localisation models, each on a single scale: S = 256 and S = 384 (due to the time<br>constraints, we did not use training scale jittering for our ILSVRC-2014 submission). Training was<br>initialised with the corresponding classification models (trained on the same scales), and the initial<br>learning rate was set to 10-3 We explored both fine-tuning all layers and fine-tuning only the first<br>·<br>two fully-connected layers, as done in (Sermanet et al., 2014). The last fully-connected layer was<br>initialised randomly and trained from scratch.</p>",
            "id": 137,
            "page": 10,
            "text": "Training. Training of localisation ConvNets is similar to that of the classification ConvNets (Sect. 3.1). The main difference is that we replace the logistic regression objective with a Euclidean loss, which penalises the deviation of the predicted bounding box parameters from the ground-truth. We trained two localisation models, each on a single scale: S = 256 and S = 384 (due to the time constraints, we did not use training scale jittering for our ILSVRC-2014 submission). Training was initialised with the corresponding classification models (trained on the same scales), and the initial learning rate was set to 10-3 We explored both fine-tuning all layers and fine-tuning only the first · two fully-connected layers, as done in (Sermanet , 2014). The last fully-connected layer was initialised randomly and trained from scratch."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2476
                },
                {
                    "x": 2108,
                    "y": 2476
                },
                {
                    "x": 2108,
                    "y": 2663
                },
                {
                    "x": 442,
                    "y": 2663
                }
            ],
            "category": "paragraph",
            "html": "<p id='138' style='font-size:18px'>Testing. We consider two testing protocols. The first is used for comparing different network<br>modifications on the validation set, and considers only the bounding box prediction for the ground<br>truth class (to factor out the classification errors). The bounding box is obtained by applying the<br>network only to the central crop of the image.</p>",
            "id": 138,
            "page": 10,
            "text": "Testing. We consider two testing protocols. The first is used for comparing different network modifications on the validation set, and considers only the bounding box prediction for the ground truth class (to factor out the classification errors). The bounding box is obtained by applying the network only to the central crop of the image."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2684
                },
                {
                    "x": 2107,
                    "y": 2684
                },
                {
                    "x": 2107,
                    "y": 3055
                },
                {
                    "x": 442,
                    "y": 3055
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='139' style='font-size:18px'>The second, fully-fledged, testing procedure is based on the dense application of the localisation<br>ConvNet to the whole image, similarly to the classification task (Sect. 3.2). The difference is that<br>instead of the class score map, the output of the last fully-connected layer is a set of bounding<br>box predictions. To come up with the final prediction, we utilise the greedy merging procedure<br>of Sermanet et al. (2014), which first merges spatially close predictions (by averaging their coor-<br>dinates), and then rates them based on the class scores, obtained from the classification ConvNet.<br>When several localisation ConvNets are used, we first take the union of their sets of bounding box<br>predictions, and then run the merging procedure on the union. We did not use the multiple pooling</p>",
            "id": 139,
            "page": 10,
            "text": "The second, fully-fledged, testing procedure is based on the dense application of the localisation ConvNet to the whole image, similarly to the classification task (Sect. 3.2). The difference is that instead of the class score map, the output of the last fully-connected layer is a set of bounding box predictions. To come up with the final prediction, we utilise the greedy merging procedure of Sermanet  (2014), which first merges spatially close predictions (by averaging their coordinates), and then rates them based on the class scores, obtained from the classification ConvNet. When several localisation ConvNets are used, we first take the union of their sets of bounding box predictions, and then run the merging procedure on the union. We did not use the multiple pooling"
        },
        {
            "bounding_box": [
                {
                    "x": 1254,
                    "y": 3131
                },
                {
                    "x": 1299,
                    "y": 3131
                },
                {
                    "x": 1299,
                    "y": 3170
                },
                {
                    "x": 1254,
                    "y": 3170
                }
            ],
            "category": "footer",
            "html": "<footer id='140' style='font-size:14px'>10</footer>",
            "id": 140,
            "page": 10,
            "text": "10"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 111
                },
                {
                    "x": 1224,
                    "y": 111
                },
                {
                    "x": 1224,
                    "y": 158
                },
                {
                    "x": 444,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='141' style='font-size:16px'>Published as a conference paper at ICLR 2015</header>",
            "id": 141,
            "page": 11,
            "text": "Published as a conference paper at ICLR 2015"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 346
                },
                {
                    "x": 2107,
                    "y": 346
                },
                {
                    "x": 2107,
                    "y": 440
                },
                {
                    "x": 440,
                    "y": 440
                }
            ],
            "category": "paragraph",
            "html": "<p id='142' style='font-size:20px'>offsets technique of Sermanet et al. (2014), which increases the spatial resolution of the bounding<br>box predictions and can further improve the results.</p>",
            "id": 142,
            "page": 11,
            "text": "offsets technique of Sermanet  (2014), which increases the spatial resolution of the bounding box predictions and can further improve the results."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 481
                },
                {
                    "x": 1100,
                    "y": 481
                },
                {
                    "x": 1100,
                    "y": 528
                },
                {
                    "x": 445,
                    "y": 528
                }
            ],
            "category": "paragraph",
            "html": "<p id='143' style='font-size:20px'>A.2 LOCALISATION EXPERIMENTS</p>",
            "id": 143,
            "page": 11,
            "text": "A.2 LOCALISATION EXPERIMENTS"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 568
                },
                {
                    "x": 2108,
                    "y": 568
                },
                {
                    "x": 2108,
                    "y": 797
                },
                {
                    "x": 441,
                    "y": 797
                }
            ],
            "category": "paragraph",
            "html": "<p id='144' style='font-size:18px'>In this section we first determine the best-performing localisation setting (using the first test proto-<br>col), and then evaluate it in a fully-fledged scenario (the second protocol). The localisation error<br>is measured according to the ILSVRC criterion (Russakovsky et al., 2014), i.e. the bounding box<br>prediction is deemed correct if its intersection over union ratio with the ground-truth bounding box<br>is above 0.5.</p>",
            "id": 144,
            "page": 11,
            "text": "In this section we first determine the best-performing localisation setting (using the first test protocol), and then evaluate it in a fully-fledged scenario (the second protocol). The localisation error is measured according to the ILSVRC criterion (Russakovsky , 2014), i.e. the bounding box prediction is deemed correct if its intersection over union ratio with the ground-truth bounding box is above 0.5."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 839
                },
                {
                    "x": 2109,
                    "y": 839
                },
                {
                    "x": 2109,
                    "y": 1117
                },
                {
                    "x": 441,
                    "y": 1117
                }
            ],
            "category": "paragraph",
            "html": "<p id='145' style='font-size:18px'>Settings comparison. As can be seen from Table 8, per-class regression (PCR) outperforms the<br>class-agnostic single-class regression (SCR), which differs from the findings of Sermanet et al.<br>(2014), where PCR was outperformed by SCR. We also note that fine-tuning all layers for the 1o-<br>calisation task leads to noticeably better results than fine-tuning only the fully-connected layers (as<br>done in (Sermanet et al., 2014)). In these experiments, the smallest images side was set to S = 384;<br>the results with S = 256 exhibit the same behaviour and are not shown for brevity.</p>",
            "id": 145,
            "page": 11,
            "text": "Settings comparison. As can be seen from Table 8, per-class regression (PCR) outperforms the class-agnostic single-class regression (SCR), which differs from the findings of Sermanet  (2014), where PCR was outperformed by SCR. We also note that fine-tuning all layers for the 1ocalisation task leads to noticeably better results than fine-tuning only the fully-connected layers (as done in (Sermanet , 2014)). In these experiments, the smallest images side was set to S = 384; the results with S = 256 exhibit the same behaviour and are not shown for brevity."
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 1193
                },
                {
                    "x": 2109,
                    "y": 1193
                },
                {
                    "x": 2109,
                    "y": 1378
                },
                {
                    "x": 440,
                    "y": 1378
                }
            ],
            "category": "paragraph",
            "html": "<p id='146' style='font-size:20px'>Table 8: Localisation error for different modifications with the simplified testing protocol: the<br>bounding box is predicted from a single central image crop, and the ground-truth class is used. All<br>ConvNet layers (except for the last one) have the configuration D (Table 1), while the last layer<br>performs either single-class regression (SCR) or per-class regression (PCR).</p>",
            "id": 146,
            "page": 11,
            "text": "Table 8: Localisation error for different modifications with the simplified testing protocol: the bounding box is predicted from a single central image crop, and the ground-truth class is used. All ConvNet layers (except for the last one) have the configuration D (Table 1), while the last layer performs either single-class regression (SCR) or per-class regression (PCR)."
        },
        {
            "bounding_box": [
                {
                    "x": 799,
                    "y": 1375
                },
                {
                    "x": 1744,
                    "y": 1375
                },
                {
                    "x": 1744,
                    "y": 1559
                },
                {
                    "x": 799,
                    "y": 1559
                }
            ],
            "category": "table",
            "html": "<br><table id='147' style='font-size:14px'><tr><td>Fine-tuned layers</td><td>regression type</td><td>GT class localisation error</td></tr><tr><td rowspan=\"2\">1st and 2nd FC</td><td>SCR</td><td>36.4</td></tr><tr><td>PCR</td><td>34.3</td></tr><tr><td>all</td><td>PCR</td><td>33.1</td></tr></table>",
            "id": 147,
            "page": 11,
            "text": "Fine-tuned layers regression type GT class localisation error  1st and 2nd FC SCR 36.4  PCR 34.3  all PCR"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1618
                },
                {
                    "x": 2109,
                    "y": 1618
                },
                {
                    "x": 2109,
                    "y": 1989
                },
                {
                    "x": 441,
                    "y": 1989
                }
            ],
            "category": "paragraph",
            "html": "<p id='148' style='font-size:20px'>Fully-fledged evaluation. Having determined the best localisation setting (PCR, fine-tuning of all<br>layers), we now apply it in the fully-fledged scenario, where the top-5 class labels are predicted us-<br>ing our best-performing classification system (Sect. 4.5), and multiple densely-computed bounding<br>box predictions are merged using the method of Sermanet et al. (2014). As can be seen from Ta-<br>ble 9, application of the localisation ConvNet to the whole image substantially improves the results<br>compared to using a center crop (Table 8), despite using the top-5 predicted class labels instead of<br>the ground truth. Similarly to the classification task (Sect. 4), testing at several scales and combining<br>the predictions of multiple networks further improves the performance.</p>",
            "id": 148,
            "page": 11,
            "text": "Fully-fledged evaluation. Having determined the best localisation setting (PCR, fine-tuning of all layers), we now apply it in the fully-fledged scenario, where the top-5 class labels are predicted using our best-performing classification system (Sect. 4.5), and multiple densely-computed bounding box predictions are merged using the method of Sermanet  (2014). As can be seen from Table 9, application of the localisation ConvNet to the whole image substantially improves the results compared to using a center crop (Table 8), despite using the top-5 predicted class labels instead of the ground truth. Similarly to the classification task (Sect. 4), testing at several scales and combining the predictions of multiple networks further improves the performance."
        },
        {
            "bounding_box": [
                {
                    "x": 1031,
                    "y": 2068
                },
                {
                    "x": 1507,
                    "y": 2068
                },
                {
                    "x": 1507,
                    "y": 2110
                },
                {
                    "x": 1031,
                    "y": 2110
                }
            ],
            "category": "caption",
            "html": "<caption id='149' style='font-size:16px'>Table 9: Localisation error</caption>",
            "id": 149,
            "page": 11,
            "text": "Table 9: Localisation error"
        },
        {
            "bounding_box": [
                {
                    "x": 762,
                    "y": 2104
                },
                {
                    "x": 1782,
                    "y": 2104
                },
                {
                    "x": 1782,
                    "y": 2385
                },
                {
                    "x": 762,
                    "y": 2385
                }
            ],
            "category": "table",
            "html": "<br><table id='150' style='font-size:14px'><tr><td colspan=\"2\">smallest image side</td><td colspan=\"2\">top-5 localisation error (%)</td></tr><tr><td>train (S)</td><td>test (Q)</td><td>val.</td><td>test.</td></tr><tr><td>256</td><td>256</td><td>29.5</td><td>-</td></tr><tr><td>384</td><td>384</td><td>28.2</td><td>26.7</td></tr><tr><td>384</td><td>352,384</td><td>27.5</td><td>-</td></tr><tr><td colspan=\"2\">fusion: 256/256 and 384/352,384</td><td>26.9</td><td>25.3</td></tr></table>",
            "id": 150,
            "page": 11,
            "text": "smallest image side top-5 localisation error (%)  train (S) test (Q) val. test.  256 256 29.5  384 384 28.2 26.7  384 352,384 27.5  fusion: 256/256 and 384/352,384 26.9"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2445
                },
                {
                    "x": 2109,
                    "y": 2445
                },
                {
                    "x": 2109,
                    "y": 2816
                },
                {
                    "x": 441,
                    "y": 2816
                }
            ],
            "category": "paragraph",
            "html": "<p id='151' style='font-size:18px'>Comparison with the state of the art. We compare our best localisation result with the state<br>of the art in Table 10. With 25.3% test error, our \"VGG\" team won the localisation challenge of<br>ILSVRC-2014 (Russakovsky et al., 2014). Notably, our results are considerably better than those<br>of the ILSVRC-2013 winner Overfeat (Sermanet et al., 2014), even though we used less scales and<br>did not employ their resolution enhancement technique. We envisage that better localisation per-<br>formance can be achieved if this technique is incorporated into our method. This indicates the<br>performance advancement brought by our very deep ConvNets - we got better results with a simpler<br>localisation method, but a more powerful representation.</p>",
            "id": 151,
            "page": 11,
            "text": "Comparison with the state of the art. We compare our best localisation result with the state of the art in Table 10. With 25.3% test error, our \"VGG\" team won the localisation challenge of ILSVRC-2014 (Russakovsky , 2014). Notably, our results are considerably better than those of the ILSVRC-2013 winner Overfeat (Sermanet , 2014), even though we used less scales and did not employ their resolution enhancement technique. We envisage that better localisation performance can be achieved if this technique is incorporated into our method. This indicates the performance advancement brought by our very deep ConvNets - we got better results with a simpler localisation method, but a more powerful representation."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 2861
                },
                {
                    "x": 1484,
                    "y": 2861
                },
                {
                    "x": 1484,
                    "y": 2915
                },
                {
                    "x": 445,
                    "y": 2915
                }
            ],
            "category": "paragraph",
            "html": "<p id='152' style='font-size:22px'>B GENERALIS ATION OF VERY DEEP FEATURES</p>",
            "id": 152,
            "page": 11,
            "text": "B GENERALIS ATION OF VERY DEEP FEATURES"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2959
                },
                {
                    "x": 2110,
                    "y": 2959
                },
                {
                    "x": 2110,
                    "y": 3053
                },
                {
                    "x": 441,
                    "y": 3053
                }
            ],
            "category": "paragraph",
            "html": "<p id='153' style='font-size:16px'>In the previous sections we have discussed training and evaluation of very deep ConvNets on the<br>ILSVRC dataset. In this section, we evaluate our ConvNets, pre-trained on ILSVRC, as feature</p>",
            "id": 153,
            "page": 11,
            "text": "In the previous sections we have discussed training and evaluation of very deep ConvNets on the ILSVRC dataset. In this section, we evaluate our ConvNets, pre-trained on ILSVRC, as feature"
        },
        {
            "bounding_box": [
                {
                    "x": 1254,
                    "y": 3132
                },
                {
                    "x": 1296,
                    "y": 3132
                },
                {
                    "x": 1296,
                    "y": 3170
                },
                {
                    "x": 1254,
                    "y": 3170
                }
            ],
            "category": "footer",
            "html": "<footer id='154' style='font-size:16px'>11</footer>",
            "id": 154,
            "page": 11,
            "text": "11"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 113
                },
                {
                    "x": 1224,
                    "y": 113
                },
                {
                    "x": 1224,
                    "y": 157
                },
                {
                    "x": 445,
                    "y": 157
                }
            ],
            "category": "header",
            "html": "<header id='155' style='font-size:18px'>Published as a conference paper at ICLR 2015</header>",
            "id": 155,
            "page": 12,
            "text": "Published as a conference paper at ICLR 2015"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 375
                },
                {
                    "x": 2104,
                    "y": 375
                },
                {
                    "x": 2104,
                    "y": 467
                },
                {
                    "x": 441,
                    "y": 467
                }
            ],
            "category": "caption",
            "html": "<caption id='156' style='font-size:18px'>Table 10: Comparison with the state of the art in ILSVRC localisation. Our method is denoted<br>as \"VGG\"</caption>",
            "id": 156,
            "page": 12,
            "text": "Table 10: Comparison with the state of the art in ILSVRC localisation. Our method is denoted as \"VGG\""
        },
        {
            "bounding_box": [
                {
                    "x": 629,
                    "y": 457
                },
                {
                    "x": 1908,
                    "y": 457
                },
                {
                    "x": 1908,
                    "y": 682
                },
                {
                    "x": 629,
                    "y": 682
                }
            ],
            "category": "table",
            "html": "<br><table id='157' style='font-size:14px'><tr><td>Method</td><td>top-5 val. error (%)</td><td>top-5 test error (%)</td></tr><tr><td>VGG</td><td>26.9</td><td>25.3</td></tr><tr><td>GoogLeNet (Szegedy et al., 2014)</td><td>-</td><td>26.7</td></tr><tr><td>OverFeat (Sermanet et al., 2014)</td><td>30.0</td><td>29.9</td></tr><tr><td>Krizhevsky et al. (Krizhevsky et al., 2012)</td><td>-</td><td>34.2</td></tr></table>",
            "id": 157,
            "page": 12,
            "text": "Method top-5 val. error (%) top-5 test error (%)  VGG 26.9 25.3  GoogLeNet (Szegedy , 2014) - 26.7  OverFeat (Sermanet , 2014) 30.0 29.9  Krizhevsky  (Krizhevsky , 2012) -"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 704
                },
                {
                    "x": 2106,
                    "y": 704
                },
                {
                    "x": 2106,
                    "y": 1070
                },
                {
                    "x": 442,
                    "y": 1070
                }
            ],
            "category": "paragraph",
            "html": "<p id='158' style='font-size:18px'>extractors on other, smaller, datasets, where training large models from scratch is not feasible due<br>to over-fitting. Recently, there has been a lot of interest in such a use case (Zeiler & Fergus, 2013;<br>Donahue et al., 2013; Razavian et al., 2014; Chatfield et al., 2014), as it turns out that deep image<br>representations, learnt on ILSVRC, generalise well to other datasets, where they have outperformed<br>hand-crafted representations by a large margin. Following that line of work, we investigate if our<br>models lead to better performance than more shallow models utilised in the state-of-the-art methods.<br>In this evaluation, we consider two models with the best classification performance on ILSVRC<br>(Sect. 4) - configurations \"Net-D\" and \"Net-E\" (which we made publicly available).</p>",
            "id": 158,
            "page": 12,
            "text": "extractors on other, smaller, datasets, where training large models from scratch is not feasible due to over-fitting. Recently, there has been a lot of interest in such a use case (Zeiler & Fergus, 2013; Donahue , 2013; Razavian , 2014; Chatfield , 2014), as it turns out that deep image representations, learnt on ILSVRC, generalise well to other datasets, where they have outperformed hand-crafted representations by a large margin. Following that line of work, we investigate if our models lead to better performance than more shallow models utilised in the state-of-the-art methods. In this evaluation, we consider two models with the best classification performance on ILSVRC (Sect. 4) - configurations \"Net-D\" and \"Net-E\" (which we made publicly available)."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1092
                },
                {
                    "x": 2107,
                    "y": 1092
                },
                {
                    "x": 2107,
                    "y": 1369
                },
                {
                    "x": 441,
                    "y": 1369
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='159' style='font-size:20px'>To utilise the ConvNets, pre-trained on ILSVRC, for image classification on other datasets, we<br>remove the last fully-connected layer (which performs 1000-way ILSVRC classification), and use<br>4096-D activations of the penultimate layer as image features, which are aggregated across multiple<br>locations and scales. The resulting image descriptor is L2-normalised and combined with a linear<br>SVM classifier, trained on the target dataset. For simplicity, pre-trained ConvNet weights are kept<br>fixed (no fine-tuning is performed).</p>",
            "id": 159,
            "page": 12,
            "text": "To utilise the ConvNets, pre-trained on ILSVRC, for image classification on other datasets, we remove the last fully-connected layer (which performs 1000-way ILSVRC classification), and use 4096-D activations of the penultimate layer as image features, which are aggregated across multiple locations and scales. The resulting image descriptor is L2-normalised and combined with a linear SVM classifier, trained on the target dataset. For simplicity, pre-trained ConvNet weights are kept fixed (no fine-tuning is performed)."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1393
                },
                {
                    "x": 2107,
                    "y": 1393
                },
                {
                    "x": 2107,
                    "y": 1942
                },
                {
                    "x": 442,
                    "y": 1942
                }
            ],
            "category": "paragraph",
            "html": "<p id='160' style='font-size:18px'>Aggregation of features is carried out in a similar manner to our ILSVRC evaluation procedure<br>(Sect. 3.2). Namely, an image is first rescaled SO that its smallest side equals Q, and then the net-<br>work is densely applied over the image plane (which is possible when all weight layers are treated<br>as convolutional). We then perform global average pooling on the resulting feature map, which<br>produces a 4096-D image descriptor. The descriptor is then averaged with the descriptor of a hori-<br>zontally flipped image. As was shown in Sect. 4.2, evaluation over multiple scales is beneficial, SO<br>we extract features over several scales Q. The resulting multi-scale features can be either stacked<br>or pooled across scales. Stacking allows a subsequent classifier to learn how to optimally combine<br>image statistics over a range of scales; this, however, comes at the cost of the increased descriptor<br>dimensionality. We return to the discussion of this design choice in the experiments below. We also<br>assess late fusion of features, computed using two networks, which is performed by stacking their<br>respective image descriptors.</p>",
            "id": 160,
            "page": 12,
            "text": "Aggregation of features is carried out in a similar manner to our ILSVRC evaluation procedure (Sect. 3.2). Namely, an image is first rescaled SO that its smallest side equals Q, and then the network is densely applied over the image plane (which is possible when all weight layers are treated as convolutional). We then perform global average pooling on the resulting feature map, which produces a 4096-D image descriptor. The descriptor is then averaged with the descriptor of a horizontally flipped image. As was shown in Sect. 4.2, evaluation over multiple scales is beneficial, SO we extract features over several scales Q. The resulting multi-scale features can be either stacked or pooled across scales. Stacking allows a subsequent classifier to learn how to optimally combine image statistics over a range of scales; this, however, comes at the cost of the increased descriptor dimensionality. We return to the discussion of this design choice in the experiments below. We also assess late fusion of features, computed using two networks, which is performed by stacking their respective image descriptors."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2017
                },
                {
                    "x": 2106,
                    "y": 2017
                },
                {
                    "x": 2106,
                    "y": 2154
                },
                {
                    "x": 441,
                    "y": 2154
                }
            ],
            "category": "caption",
            "html": "<caption id='161' style='font-size:18px'>Table 11: Comparison with the state of the art in image classification on VOC-2007, VOC-2012,<br>Caltech-101, and Caltech-256. Our models are denoted as \"VGG\". Results marked with * were<br>achieved using ConvNets pre-trained on the extended ILSVRC dataset (2000 classes).</caption>",
            "id": 161,
            "page": 12,
            "text": "Table 11: Comparison with the state of the art in image classification on VOC-2007, VOC-2012, Caltech-101, and Caltech-256. Our models are denoted as \"VGG\". Results marked with * were achieved using ConvNets pre-trained on the extended ILSVRC dataset (2000 classes)."
        },
        {
            "bounding_box": [
                {
                    "x": 450,
                    "y": 2152
                },
                {
                    "x": 2088,
                    "y": 2152
                },
                {
                    "x": 2088,
                    "y": 2556
                },
                {
                    "x": 450,
                    "y": 2556
                }
            ],
            "category": "table",
            "html": "<br><table id='162' style='font-size:14px'><tr><td>Method</td><td>VOC-2007 (mean AP)</td><td>VOC-2012 (mean AP)</td><td>Caltech-101 (mean class recall)</td><td>Caltech-256 (mean class recall)</td></tr><tr><td>Zeiler & Fergus (Zeiler & Fergus, 2013)</td><td>-</td><td>79.0</td><td>86.5 土 0.5</td><td>74.2 土 0.3</td></tr><tr><td>Chatfield et al. (Chatfield et al., 2014)</td><td>82.4</td><td>83.2</td><td>88.4 土 0.6</td><td>77.6 土 0.1</td></tr><tr><td>He et al. (He et al., 2014)</td><td>82.4</td><td>-</td><td>93.4 土 0.5</td><td>-</td></tr><tr><td>Wei et al. (Wei et al., 2014)</td><td>81.5 (85.2*)</td><td>81.7 (90.3* )</td><td>-</td><td>-</td></tr><tr><td>VGG Net-D (16 layers)</td><td>89.3</td><td>89.0</td><td>91.8 土 1.0</td><td>85.0 土 0.2</td></tr><tr><td>VGG Net-E (19 layers)</td><td>89.3</td><td>89.0</td><td>92.3 土 0.5</td><td>85.1 土 0.3</td></tr><tr><td>VGG Net-D & Net-E</td><td>89.7</td><td>89.3</td><td>92.7 土 0.5</td><td>86.2 土 0.3</td></tr></table>",
            "id": 162,
            "page": 12,
            "text": "Method VOC-2007 (mean AP) VOC-2012 (mean AP) Caltech-101 (mean class recall) Caltech-256 (mean class recall)  Zeiler & Fergus (Zeiler & Fergus, 2013) - 79.0 86.5 土 0.5 74.2 土 0.3  Chatfield  (Chatfield , 2014) 82.4 83.2 88.4 土 0.6 77.6 土 0.1  He  (He , 2014) 82.4 - 93.4 土 0.5  Wei  (Wei , 2014) 81.5 (85.2*) 81.7 (90.3* ) -  VGG Net-D (16 layers) 89.3 89.0 91.8 土 1.0 85.0 土 0.2  VGG Net-E (19 layers) 89.3 89.0 92.3 土 0.5 85.1 土 0.3  VGG Net-D & Net-E 89.7 89.3 92.7 土 0.5"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2613
                },
                {
                    "x": 2107,
                    "y": 2613
                },
                {
                    "x": 2107,
                    "y": 2938
                },
                {
                    "x": 443,
                    "y": 2938
                }
            ],
            "category": "paragraph",
            "html": "<p id='163' style='font-size:20px'>Image Classification on VOC-2007 and VOC-2012. We begin with the evaluation on the image<br>classification task of PASCAL VOC-2007 and VOC-2012 benchmarks (Everingham et al., 2015).<br>These datasets contain 10K and 22.5K images respectively, and each image is annotated with one<br>or several labels, corresponding to 20 object categories. The VOC organisers provide a pre-defined<br>split into training, validation, and test data (the test data for VOC-2012 is not publicly available;<br>instead, an official evaluation server is provided). Recognition performance is measured using mean<br>average precision (mAP) across classes.</p>",
            "id": 163,
            "page": 12,
            "text": "Image Classification on VOC-2007 and VOC-2012. We begin with the evaluation on the image classification task of PASCAL VOC-2007 and VOC-2012 benchmarks (Everingham , 2015). These datasets contain 10K and 22.5K images respectively, and each image is annotated with one or several labels, corresponding to 20 object categories. The VOC organisers provide a pre-defined split into training, validation, and test data (the test data for VOC-2012 is not publicly available; instead, an official evaluation server is provided). Recognition performance is measured using mean average precision (mAP) across classes."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2960
                },
                {
                    "x": 2106,
                    "y": 2960
                },
                {
                    "x": 2106,
                    "y": 3055
                },
                {
                    "x": 442,
                    "y": 3055
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='164' style='font-size:22px'>Notably, by examining the performance on the validation sets of VOC-2007 and VOC-2012, we<br>found that aggregating image descriptors, computed at multiple scales, by averaging performs sim-</p>",
            "id": 164,
            "page": 12,
            "text": "Notably, by examining the performance on the validation sets of VOC-2007 and VOC-2012, we found that aggregating image descriptors, computed at multiple scales, by averaging performs sim-"
        },
        {
            "bounding_box": [
                {
                    "x": 1253,
                    "y": 3133
                },
                {
                    "x": 1299,
                    "y": 3133
                },
                {
                    "x": 1299,
                    "y": 3169
                },
                {
                    "x": 1253,
                    "y": 3169
                }
            ],
            "category": "footer",
            "html": "<footer id='165' style='font-size:16px'>12</footer>",
            "id": 165,
            "page": 12,
            "text": "12"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 112
                },
                {
                    "x": 1224,
                    "y": 112
                },
                {
                    "x": 1224,
                    "y": 157
                },
                {
                    "x": 445,
                    "y": 157
                }
            ],
            "category": "header",
            "html": "<header id='166' style='font-size:14px'>Published as a conference paper at ICLR 2015</header>",
            "id": 166,
            "page": 13,
            "text": "Published as a conference paper at ICLR 2015"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 345
                },
                {
                    "x": 2107,
                    "y": 345
                },
                {
                    "x": 2107,
                    "y": 624
                },
                {
                    "x": 441,
                    "y": 624
                }
            ],
            "category": "paragraph",
            "html": "<p id='167' style='font-size:20px'>ilarly to the aggregation by stacking. We hypothesize that this is due to the fact that in the VOC<br>dataset the objects appear over a variety of scales, SO there is no particular scale-specific seman-<br>tics which a classifier could exploit. Since averaging has a benefit of not inflating the descrip-<br>tor dimensionality, we were able to aggregated image descriptors over a wide range of scales:<br>Q E {256, 384, 512, 640, 768}. It is worth noting though that the improvement over a smaller<br>range of {256, 384, 512} was rather marginal (0.3%).</p>",
            "id": 167,
            "page": 13,
            "text": "ilarly to the aggregation by stacking. We hypothesize that this is due to the fact that in the VOC dataset the objects appear over a variety of scales, SO there is no particular scale-specific semantics which a classifier could exploit. Since averaging has a benefit of not inflating the descriptor dimensionality, we were able to aggregated image descriptors over a wide range of scales: Q E {256, 384, 512, 640, 768}. It is worth noting though that the improvement over a smaller range of {256, 384, 512} was rather marginal (0.3%)."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 644
                },
                {
                    "x": 2107,
                    "y": 644
                },
                {
                    "x": 2107,
                    "y": 1015
                },
                {
                    "x": 442,
                    "y": 1015
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='168' style='font-size:18px'>The test set performance is reported and compared with other approaches in Table 11. Our networks<br>\"Net-D\" and \"Net-E\" exhibit identical performance on VOC datasets, and their combination slightly<br>improves the results. Our methods set the new state of the art across image representations, pre-<br>trained on the ILSVRC dataset, outperforming the previous best result of Chatfield et al. (2014) by<br>more than 6%. It should be noted that the method of Wei et al. (2014), which achieves 1% better<br>mAP on VOC-2012, is pre-trained on an extended 2000-class ILSVRC dataset, which includes<br>additional 1000 categories, semantically close to those in VOC datasets. It also benefits from the<br>fusion with an object detection-assisted classification pipeline.</p>",
            "id": 168,
            "page": 13,
            "text": "The test set performance is reported and compared with other approaches in Table 11. Our networks \"Net-D\" and \"Net-E\" exhibit identical performance on VOC datasets, and their combination slightly improves the results. Our methods set the new state of the art across image representations, pretrained on the ILSVRC dataset, outperforming the previous best result of Chatfield  (2014) by more than 6%. It should be noted that the method of Wei  (2014), which achieves 1% better mAP on VOC-2012, is pre-trained on an extended 2000-class ILSVRC dataset, which includes additional 1000 categories, semantically close to those in VOC datasets. It also benefits from the fusion with an object detection-assisted classification pipeline."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1055
                },
                {
                    "x": 2107,
                    "y": 1055
                },
                {
                    "x": 2107,
                    "y": 1607
                },
                {
                    "x": 442,
                    "y": 1607
                }
            ],
            "category": "paragraph",
            "html": "<p id='169' style='font-size:18px'>Image Classification on Caltech-101 and Caltech-256. In this section we evaluate very deep fea-<br>tures on Caltech-101 (Fei-Fei et al., 2004) and Caltech-256 (Griffin et al., 2007) image classification<br>benchmarks. Caltech-101 contains 9K images labelled into 102 classes (101 object categories and a<br>background class), while Caltech-256 is larger with 31K images and 257 classes. A standard eval-<br>uation protocol on these datasets is to generate several random splits into training and test data and<br>report the average recognition performance across the splits, which is measured by the mean class<br>recall (which compensates for a different number of test images per class). Following Chatfield et al.<br>(2014); Zeiler & Fergus (2013); He et al. (2014), on Caltech-101 we generated 3 random splits into<br>training and test data, SO that each split contains 30 training images per class, and up to 50 test<br>images per class. On Caltech-256 we also generated 3 splits, each of which contains 60 training<br>images per class (and the rest is used for testing). In each split, 20% of training images were used<br>as a validation set for hyper-parameter selection.</p>",
            "id": 169,
            "page": 13,
            "text": "Image Classification on Caltech-101 and Caltech-256. In this section we evaluate very deep features on Caltech-101 (Fei-Fei , 2004) and Caltech-256 (Griffin , 2007) image classification benchmarks. Caltech-101 contains 9K images labelled into 102 classes (101 object categories and a background class), while Caltech-256 is larger with 31K images and 257 classes. A standard evaluation protocol on these datasets is to generate several random splits into training and test data and report the average recognition performance across the splits, which is measured by the mean class recall (which compensates for a different number of test images per class). Following Chatfield  (2014); Zeiler & Fergus (2013); He  (2014), on Caltech-101 we generated 3 random splits into training and test data, SO that each split contains 30 training images per class, and up to 50 test images per class. On Caltech-256 we also generated 3 splits, each of which contains 60 training images per class (and the rest is used for testing). In each split, 20% of training images were used as a validation set for hyper-parameter selection."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1629
                },
                {
                    "x": 2108,
                    "y": 1629
                },
                {
                    "x": 2108,
                    "y": 1861
                },
                {
                    "x": 441,
                    "y": 1861
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='170' style='font-size:22px'>We found that unlike VOC, on Caltech datasets the stacking of descriptors, computed over multi-<br>ple scales, performs better than averaging or max-pooling. This can be explained by the fact that<br>in Caltech images objects typically occupy the whole image, SO multi-scale image features are se-<br>mantically different (capturing the whole object vs. object parts), and stacking allows a classifier to<br>exploit such scale-specific representations. We used three scales Q E {256, 384, 512}.</p>",
            "id": 170,
            "page": 13,
            "text": "We found that unlike VOC, on Caltech datasets the stacking of descriptors, computed over multiple scales, performs better than averaging or max-pooling. This can be explained by the fact that in Caltech images objects typically occupy the whole image, SO multi-scale image features are semantically different (capturing the whole object vs. object parts), and stacking allows a classifier to exploit such scale-specific representations. We used three scales Q E {256, 384, 512}."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1883
                },
                {
                    "x": 2107,
                    "y": 1883
                },
                {
                    "x": 2107,
                    "y": 2115
                },
                {
                    "x": 442,
                    "y": 2115
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='171' style='font-size:18px'>Our models are compared to each other and the state of the art in Table 11. As can be seen, the deeper<br>19-layer Net-E performs better than the 16-layer Net-D, and their combination further improves the<br>performance. On Caltech-101, our representations are competitive with the approach of He et al.<br>(2014), which, however, performs significantly worse than our nets on VOC-2007. On Caltech-256,<br>our features outperform the state of the art (Chatfield et al., 2014) by a large margin (8.6%).</p>",
            "id": 171,
            "page": 13,
            "text": "Our models are compared to each other and the state of the art in Table 11. As can be seen, the deeper 19-layer Net-E performs better than the 16-layer Net-D, and their combination further improves the performance. On Caltech-101, our representations are competitive with the approach of He  (2014), which, however, performs significantly worse than our nets on VOC-2007. On Caltech-256, our features outperform the state of the art (Chatfield , 2014) by a large margin (8.6%)."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2158
                },
                {
                    "x": 2107,
                    "y": 2158
                },
                {
                    "x": 2107,
                    "y": 2571
                },
                {
                    "x": 443,
                    "y": 2571
                }
            ],
            "category": "paragraph",
            "html": "<p id='172' style='font-size:20px'>Action Classification on VOC-2012. We also evaluated our best-performing image representa-<br>tion (the stacking of Net-D and Net-E features) on the PASCAL VOC-2012 action classification<br>task (Everingham et al., 2015), which consists in predicting an action class from a single image,<br>given a bounding box of the person performing the action. The dataset contains 4.6K training im-<br>ages, labelled into 11 classes. Similarly to the VOC-2012 object classification task, the performance<br>is measured using the mAP. We considered two training settings: (i) computing the ConvNet fea-<br>tures on the whole image and ignoring the provided bounding box; (ii) computing the features on the<br>whole image and on the provided bounding box, and stacking them to obtain the final representation.<br>The results are compared to other approaches in Table 12.</p>",
            "id": 172,
            "page": 13,
            "text": "Action Classification on VOC-2012. We also evaluated our best-performing image representation (the stacking of Net-D and Net-E features) on the PASCAL VOC-2012 action classification task (Everingham , 2015), which consists in predicting an action class from a single image, given a bounding box of the person performing the action. The dataset contains 4.6K training images, labelled into 11 classes. Similarly to the VOC-2012 object classification task, the performance is measured using the mAP. We considered two training settings: (i) computing the ConvNet features on the whole image and ignoring the provided bounding box; (ii) computing the features on the whole image and on the provided bounding box, and stacking them to obtain the final representation. The results are compared to other approaches in Table 12."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2594
                },
                {
                    "x": 2107,
                    "y": 2594
                },
                {
                    "x": 2107,
                    "y": 2779
                },
                {
                    "x": 442,
                    "y": 2779
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='173' style='font-size:18px'>Our representation achieves the state of art on the VOC action classification task even without using<br>the provided bounding boxes, and the results are further improved when using both images and<br>bounding boxes. Unlike other approaches, we did not incorporate any task-specific heuristics, but<br>relied on the representation power of very deep convolutional features.</p>",
            "id": 173,
            "page": 13,
            "text": "Our representation achieves the state of art on the VOC action classification task even without using the provided bounding boxes, and the results are further improved when using both images and bounding boxes. Unlike other approaches, we did not incorporate any task-specific heuristics, but relied on the representation power of very deep convolutional features."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2821
                },
                {
                    "x": 2108,
                    "y": 2821
                },
                {
                    "x": 2108,
                    "y": 3054
                },
                {
                    "x": 441,
                    "y": 3054
                }
            ],
            "category": "paragraph",
            "html": "<p id='174' style='font-size:20px'>Other Recognition Tasks. Since the public release of our models, they have been actively used<br>by the research community for a wide range of image recognition tasks, consistently outperform-<br>ing more shallow representations. For instance, Girshick et al. (2014) achieve the state of the<br>object detection results by replacing the ConvNet of Krizhevsky et al. (2012) with our 16-layer<br>model. Similar gains over a more shallow architecture of Krizhevsky et al. (2012) have been ob-</p>",
            "id": 174,
            "page": 13,
            "text": "Other Recognition Tasks. Since the public release of our models, they have been actively used by the research community for a wide range of image recognition tasks, consistently outperforming more shallow representations. For instance, Girshick  (2014) achieve the state of the object detection results by replacing the ConvNet of Krizhevsky  (2012) with our 16-layer model. Similar gains over a more shallow architecture of Krizhevsky  (2012) have been ob-"
        },
        {
            "bounding_box": [
                {
                    "x": 1253,
                    "y": 3132
                },
                {
                    "x": 1298,
                    "y": 3132
                },
                {
                    "x": 1298,
                    "y": 3169
                },
                {
                    "x": 1253,
                    "y": 3169
                }
            ],
            "category": "footer",
            "html": "<footer id='175' style='font-size:16px'>13</footer>",
            "id": 175,
            "page": 13,
            "text": "13"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 112
                },
                {
                    "x": 1225,
                    "y": 112
                },
                {
                    "x": 1225,
                    "y": 157
                },
                {
                    "x": 444,
                    "y": 157
                }
            ],
            "category": "header",
            "html": "<header id='176' style='font-size:14px'>Published as a conference paper at ICLR 2015</header>",
            "id": 176,
            "page": 14,
            "text": "Published as a conference paper at ICLR 2015"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 372
                },
                {
                    "x": 2109,
                    "y": 372
                },
                {
                    "x": 2109,
                    "y": 514
                },
                {
                    "x": 440,
                    "y": 514
                }
            ],
            "category": "caption",
            "html": "<caption id='177' style='font-size:14px'>Table 12: Comparison with the state of the art in single-image action classification on VOC-<br>2012. Our models are denoted as \"VGG\". Results marked with * achieved using ConvNets<br>were<br>pre-trained on the extended ILSVRC dataset (1512 classes).</caption>",
            "id": 177,
            "page": 14,
            "text": "Table 12: Comparison with the state of the art in single-image action classification on VOC2012. Our models are denoted as \"VGG\". Results marked with * achieved using ConvNets were pre-trained on the extended ILSVRC dataset (1512 classes)."
        },
        {
            "bounding_box": [
                {
                    "x": 719,
                    "y": 509
                },
                {
                    "x": 1819,
                    "y": 509
                },
                {
                    "x": 1819,
                    "y": 789
                },
                {
                    "x": 719,
                    "y": 789
                }
            ],
            "category": "table",
            "html": "<br><table id='178' style='font-size:14px'><tr><td>Method</td><td>VOC-2012 (mean AP)</td></tr><tr><td>(Oquab et al., 2014)</td><td>70.2*</td></tr><tr><td>(Gkioxari et al., 2014)</td><td>73.6</td></tr><tr><td>(Hoai, 2014)</td><td>76.3</td></tr><tr><td>VGG Net-D & Net-E, image-only</td><td>79.2</td></tr><tr><td>VGG Net-D & Net-E, image and bounding box</td><td>84.0</td></tr></table>",
            "id": 178,
            "page": 14,
            "text": "Method VOC-2012 (mean AP)  (Oquab , 2014) 70.2*  (Gkioxari , 2014) 73.6  (Hoai, 2014) 76.3  VGG Net-D & Net-E, image-only 79.2  VGG Net-D & Net-E, image and bounding box"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 814
                },
                {
                    "x": 2110,
                    "y": 814
                },
                {
                    "x": 2110,
                    "y": 907
                },
                {
                    "x": 441,
                    "y": 907
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='179' style='font-size:20px'>served in semantic segmentation (Long et al., 2014), image caption generation (Kiros et al., 2014;<br>Karpathy & Fei-Fei, 2014), texture and material recognition (Cimpoi et al., 2014; Bell et al., 2014).</p>",
            "id": 179,
            "page": 14,
            "text": "served in semantic segmentation (Long , 2014), image caption generation (Kiros , 2014; Karpathy & Fei-Fei, 2014), texture and material recognition (Cimpoi , 2014; Bell , 2014)."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 954
                },
                {
                    "x": 922,
                    "y": 954
                },
                {
                    "x": 922,
                    "y": 1006
                },
                {
                    "x": 445,
                    "y": 1006
                }
            ],
            "category": "paragraph",
            "html": "<p id='180' style='font-size:22px'>C PAPER REVISIONS</p>",
            "id": 180,
            "page": 14,
            "text": "C PAPER REVISIONS"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1051
                },
                {
                    "x": 2104,
                    "y": 1051
                },
                {
                    "x": 2104,
                    "y": 1143
                },
                {
                    "x": 441,
                    "y": 1143
                }
            ],
            "category": "paragraph",
            "html": "<p id='181' style='font-size:16px'>Here we present the list of major paper revisions, outlining the substantial changes for the conve-<br>nience of the reader.</p>",
            "id": 181,
            "page": 14,
            "text": "Here we present the list of major paper revisions, outlining the substantial changes for the convenience of the reader."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1169
                },
                {
                    "x": 1906,
                    "y": 1169
                },
                {
                    "x": 1906,
                    "y": 1216
                },
                {
                    "x": 443,
                    "y": 1216
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='182' style='font-size:14px'>v1 Initial version. Presents the experiments carried out before the ILSVRC submission.</p>",
            "id": 182,
            "page": 14,
            "text": "v1 Initial version. Presents the experiments carried out before the ILSVRC submission."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1239
                },
                {
                    "x": 2106,
                    "y": 1239
                },
                {
                    "x": 2106,
                    "y": 1332
                },
                {
                    "x": 443,
                    "y": 1332
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='183' style='font-size:20px'>v2 Adds post-submission ILSVRC experiments with training set augmentation using scale jittering,<br>which improves the performance.</p>",
            "id": 183,
            "page": 14,
            "text": "v2 Adds post-submission ILSVRC experiments with training set augmentation using scale jittering, which improves the performance."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1355
                },
                {
                    "x": 2104,
                    "y": 1355
                },
                {
                    "x": 2104,
                    "y": 1447
                },
                {
                    "x": 442,
                    "y": 1447
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='184' style='font-size:18px'>v3 Adds generalisation experiments (Appendix B) on PASCAL VOC and Caltech image classifica-<br>tion datasets. The models used for these experiments are publicly available.</p>",
            "id": 184,
            "page": 14,
            "text": "v3 Adds generalisation experiments (Appendix B) on PASCAL VOC and Caltech image classification datasets. The models used for these experiments are publicly available."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1472
                },
                {
                    "x": 2105,
                    "y": 1472
                },
                {
                    "x": 2105,
                    "y": 1562
                },
                {
                    "x": 442,
                    "y": 1562
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='185' style='font-size:18px'>v4 The paper is converted to ICLR-2015 submission format. Also adds experiments with multiple<br>crops for classification.</p>",
            "id": 185,
            "page": 14,
            "text": "v4 The paper is converted to ICLR-2015 submission format. Also adds experiments with multiple crops for classification."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1587
                },
                {
                    "x": 2107,
                    "y": 1587
                },
                {
                    "x": 2107,
                    "y": 1682
                },
                {
                    "x": 442,
                    "y": 1682
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='186' style='font-size:14px'>v6 Camera-ready ICLR-2015 conference paper. Adds a comparison of the net B with a shallow net<br>and the results on PASCAL VOC action classification benchmark.</p>",
            "id": 186,
            "page": 14,
            "text": "v6 Camera-ready ICLR-2015 conference paper. Adds a comparison of the net B with a shallow net and the results on PASCAL VOC action classification benchmark."
        },
        {
            "bounding_box": [
                {
                    "x": 1253,
                    "y": 3131
                },
                {
                    "x": 1300,
                    "y": 3131
                },
                {
                    "x": 1300,
                    "y": 3170
                },
                {
                    "x": 1253,
                    "y": 3170
                }
            ],
            "category": "footer",
            "html": "<footer id='187' style='font-size:14px'>14</footer>",
            "id": 187,
            "page": 14,
            "text": "14"
        }
    ]
}