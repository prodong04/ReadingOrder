{
    "id": "32ae95cc-0f94-11ef-8828-426932df3dcf",
    "pdf_path": "./pdf/AI_VIT_X/2211.12588v4.pdf",
    "elements": [
        {
            "bounding_box": [
                {
                    "x": 295,
                    "y": 109
                },
                {
                    "x": 1539,
                    "y": 109
                },
                {
                    "x": 1539,
                    "y": 158
                },
                {
                    "x": 295,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='0' style='font-size:18px'>Published in Transactions on Machine Learning Research (10/2023)</header>",
            "id": 0,
            "page": 1,
            "text": "Published in Transactions on Machine Learning Research (10/2023)"
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 328
                },
                {
                    "x": 2253,
                    "y": 328
                },
                {
                    "x": 2253,
                    "y": 502
                },
                {
                    "x": 292,
                    "y": 502
                }
            ],
            "category": "paragraph",
            "html": "<p id='1' style='font-size:22px'>Program of Thoughts Prompting: Disentangling Computa-<br>tion from Reasoning for Numerical Reasoning Tasks</p>",
            "id": 1,
            "page": 1,
            "text": "Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks"
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 601
                },
                {
                    "x": 1833,
                    "y": 601
                },
                {
                    "x": 1833,
                    "y": 914
                },
                {
                    "x": 293,
                    "y": 914
                }
            ],
            "category": "paragraph",
            "html": "<p id='2' style='font-size:18px'>§,T Wenhu Chen* § Xueguang Ma* †Xinyi Wang, °William W. Cohen<br>,<br>§University of Waterloo<br>Vector Institute, Toronto<br>+University of California, Santa Barabra<br>°Google Research<br>{wenhuchen, x93ma}@uwaterloo · ca, xinyi_wang@ucsb · edu, wcohen@google · com</p>",
            "id": 2,
            "page": 1,
            "text": "§,T Wenhu Chen* § Xueguang Ma* †Xinyi Wang, °William W. Cohen , §University of Waterloo Vector Institute, Toronto +University of California, Santa Barabra °Google Research {wenhuchen, x93ma}@uwaterloo · ca, xinyi_wang@ucsb · edu, wcohen@google · com"
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 1006
                },
                {
                    "x": 1805,
                    "y": 1006
                },
                {
                    "x": 1805,
                    "y": 1059
                },
                {
                    "x": 292,
                    "y": 1059
                }
            ],
            "category": "paragraph",
            "html": "<p id='3' style='font-size:16px'>Reviewed on OpenReview: https : / / openreview · net/forum?id=YfZ4ZPt8zd</p>",
            "id": 3,
            "page": 1,
            "text": "Reviewed on OpenReview: https : / / openreview · net/forum?id=YfZ4ZPt8zd"
        },
        {
            "bounding_box": [
                {
                    "x": 1170,
                    "y": 1175
                },
                {
                    "x": 1379,
                    "y": 1175
                },
                {
                    "x": 1379,
                    "y": 1228
                },
                {
                    "x": 1170,
                    "y": 1228
                }
            ],
            "category": "paragraph",
            "html": "<p id='4' style='font-size:20px'>Abstract</p>",
            "id": 4,
            "page": 1,
            "text": "Abstract"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1307
                },
                {
                    "x": 2112,
                    "y": 1307
                },
                {
                    "x": 2112,
                    "y": 1960
                },
                {
                    "x": 442,
                    "y": 1960
                }
            ],
            "category": "paragraph",
            "html": "<p id='5' style='font-size:16px'>Recently, there has been significant progress in teaching language models to perform step-by-<br>step reasoning to solve complex numerical reasoning tasks. Chain-of-thoughts prompting<br>(CoT) is the state-of-art method for many of these tasks. CoT uses language models to<br>produce text describing reasoning, and computation, and finally the answer to a question.<br>Here we propose 'Program of Thoughts' (PoT), which uses language models (mainly Codex)<br>to generate text and programming language statements, and finally an answer. In PoT,<br>the computation can be delegated to a program interpreter, which is used to execute the<br>generated program, thus decoupling complex computation from reasoning and language<br>understanding. We evaluate PoT on five math word problem datasets and three financial-<br>QA datasets in both few-shot and zero-shot settings. We find that PoT has an average<br>performance gain over CoT of around 12% across all datasets. By combining PoT with<br>self-consistency decoding, we can achieve extremely strong performance on all the math<br>datasets and financial datasets. All of our data and code will be released.</p>",
            "id": 5,
            "page": 1,
            "text": "Recently, there has been significant progress in teaching language models to perform step-bystep reasoning to solve complex numerical reasoning tasks. Chain-of-thoughts prompting (CoT) is the state-of-art method for many of these tasks. CoT uses language models to produce text describing reasoning, and computation, and finally the answer to a question. Here we propose 'Program of Thoughts' (PoT), which uses language models (mainly Codex) to generate text and programming language statements, and finally an answer. In PoT, the computation can be delegated to a program interpreter, which is used to execute the generated program, thus decoupling complex computation from reasoning and language understanding. We evaluate PoT on five math word problem datasets and three financialQA datasets in both few-shot and zero-shot settings. We find that PoT has an average performance gain over CoT of around 12% across all datasets. By combining PoT with self-consistency decoding, we can achieve extremely strong performance on all the math datasets and financial datasets. All of our data and code will be released."
        },
        {
            "bounding_box": [
                {
                    "x": 295,
                    "y": 2079
                },
                {
                    "x": 666,
                    "y": 2079
                },
                {
                    "x": 666,
                    "y": 2133
                },
                {
                    "x": 295,
                    "y": 2133
                }
            ],
            "category": "paragraph",
            "html": "<p id='6' style='font-size:20px'>1 Introduction</p>",
            "id": 6,
            "page": 1,
            "text": "1 Introduction"
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 2188
                },
                {
                    "x": 2259,
                    "y": 2188
                },
                {
                    "x": 2259,
                    "y": 2491
                },
                {
                    "x": 292,
                    "y": 2491
                }
            ],
            "category": "paragraph",
            "html": "<p id='7' style='font-size:18px'>Numerical reasoning is a long-standing task in artificial intelligence. A surge of datasets has been proposed<br>recently to benchmark deep-learning models' capabilities to perform numerical/ arithmetic reasoning. Some<br>widely used benchmarks are based on Math word problems (MWP) (Cobbe et al., 2021; Patel et al., 2021;<br>Lu et al., 2022; Ling et al., 2017), where systems are supposed to answer math questions expressed with<br>natural text. Besides MWP, some datasets also consider financial problems (Chen et al., 2021b; 2022; Zhu<br>et al., 2021), where systems need to answer math-driven financial questions.</p>",
            "id": 7,
            "page": 1,
            "text": "Numerical reasoning is a long-standing task in artificial intelligence. A surge of datasets has been proposed recently to benchmark deep-learning models' capabilities to perform numerical/ arithmetic reasoning. Some widely used benchmarks are based on Math word problems (MWP) (Cobbe , 2021; Patel , 2021; Lu , 2022; Ling , 2017), where systems are supposed to answer math questions expressed with natural text. Besides MWP, some datasets also consider financial problems (Chen , 2021b; 2022; Zhu , 2021), where systems need to answer math-driven financial questions."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 2515
                },
                {
                    "x": 2261,
                    "y": 2515
                },
                {
                    "x": 2261,
                    "y": 2965
                },
                {
                    "x": 290,
                    "y": 2965
                }
            ],
            "category": "paragraph",
            "html": "<p id='8' style='font-size:18px'>Prior work (Ling et al., 2017; Cobbe et al., 2021) has studied how to train models from scratch or fine-tune<br>models to generate intermediate steps to derive the final answer. Such methods are data-intensive, requiring<br>a significant number of training examples with expert-annotated steps. Recently, Nye et al. (2021) have<br>discovered that the large language models (LLMs) (Brown et al., 2020; Chen et al., 2021a; Chowdhery et al.,<br>2022) can be prompted with a few input-output exemplars to solve these tasks without any training or fine-<br>tuning. In particular, when prompted with a few examples containing inputs, natural language 'rationales',<br>and outputs, LLMs can imitate the demonstrations to both generate rationales and answer these questions.<br>Such a prompting method is latter extended as 'Chain of Thoughts (CoT)' (Wei et al., 2022), and it is able<br>to achieve state-of-the-art performance on a wide spectrum of textual and numerical reasoning datasets.</p>",
            "id": 8,
            "page": 1,
            "text": "Prior work (Ling , 2017; Cobbe , 2021) has studied how to train models from scratch or fine-tune models to generate intermediate steps to derive the final answer. Such methods are data-intensive, requiring a significant number of training examples with expert-annotated steps. Recently, Nye  (2021) have discovered that the large language models (LLMs) (Brown , 2020; Chen , 2021a; Chowdhery , 2022) can be prompted with a few input-output exemplars to solve these tasks without any training or finetuning. In particular, when prompted with a few examples containing inputs, natural language 'rationales', and outputs, LLMs can imitate the demonstrations to both generate rationales and answer these questions. Such a prompting method is latter extended as 'Chain of Thoughts (CoT)' (Wei , 2022), and it is able to achieve state-of-the-art performance on a wide spectrum of textual and numerical reasoning datasets."
        },
        {
            "bounding_box": [
                {
                    "x": 341,
                    "y": 3009
                },
                {
                    "x": 1647,
                    "y": 3009
                },
                {
                    "x": 1647,
                    "y": 3052
                },
                {
                    "x": 341,
                    "y": 3052
                }
            ],
            "category": "paragraph",
            "html": "<p id='9' style='font-size:14px'>* Work done at University of Waterloo. Wenhu Chen is the corresponding author.</p>",
            "id": 9,
            "page": 1,
            "text": "* Work done at University of Waterloo. Wenhu Chen is the corresponding author."
        },
        {
            "bounding_box": [
                {
                    "x": 1262,
                    "y": 3133
                },
                {
                    "x": 1288,
                    "y": 3133
                },
                {
                    "x": 1288,
                    "y": 3170
                },
                {
                    "x": 1262,
                    "y": 3170
                }
            ],
            "category": "footer",
            "html": "<footer id='10' style='font-size:16px'>1</footer>",
            "id": 10,
            "page": 1,
            "text": "1"
        },
        {
            "bounding_box": [
                {
                    "x": 58,
                    "y": 886
                },
                {
                    "x": 150,
                    "y": 886
                },
                {
                    "x": 150,
                    "y": 2330
                },
                {
                    "x": 58,
                    "y": 2330
                }
            ],
            "category": "footer",
            "html": "<br><footer id='11' style='font-size:14px'>2023<br>Oct<br>23<br>[cs.CL]<br>arXiv:2211.12588v4</footer>",
            "id": 11,
            "page": 1,
            "text": "2023 Oct 23 [cs.CL] arXiv:2211.12588v4"
        },
        {
            "bounding_box": [
                {
                    "x": 299,
                    "y": 112
                },
                {
                    "x": 1537,
                    "y": 112
                },
                {
                    "x": 1537,
                    "y": 157
                },
                {
                    "x": 299,
                    "y": 157
                }
            ],
            "category": "header",
            "html": "<header id='12' style='font-size:20px'>Published in Transactions on Machine Learning Research (10/2023)</header>",
            "id": 12,
            "page": 2,
            "text": "Published in Transactions on Machine Learning Research (10/2023)"
        },
        {
            "bounding_box": [
                {
                    "x": 304,
                    "y": 379
                },
                {
                    "x": 2152,
                    "y": 379
                },
                {
                    "x": 2152,
                    "y": 461
                },
                {
                    "x": 304,
                    "y": 461
                }
            ],
            "category": "caption",
            "html": "<caption id='13'></caption>",
            "id": 13,
            "page": 2,
            "text": ""
        },
        {
            "bounding_box": [
                {
                    "x": 304,
                    "y": 379
                },
                {
                    "x": 2152,
                    "y": 379
                },
                {
                    "x": 2152,
                    "y": 461
                },
                {
                    "x": 304,
                    "y": 461
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='14' style='font-size:16px'>Question: In Fibonacci sequence, it follows the rule that each number is equal to the sum of the preceding two numbers.<br>Assuming the first two numbers are 0 and 1, what is the 50th number in Fibonacci sequence?</p>",
            "id": 14,
            "page": 2,
            "text": "Question: In Fibonacci sequence, it follows the rule that each number is equal to the sum of the preceding two numbers. Assuming the first two numbers are 0 and 1, what is the 50th number in Fibonacci sequence?"
        },
        {
            "bounding_box": [
                {
                    "x": 296,
                    "y": 485
                },
                {
                    "x": 2251,
                    "y": 485
                },
                {
                    "x": 2251,
                    "y": 1003
                },
                {
                    "x": 296,
                    "y": 1003
                }
            ],
            "category": "figure",
            "html": "<figure><img id='15' style='font-size:16px' alt=\"length_of_fibonacci_sequence = 50\nThe first number is 0, the second number is 1, therefore, the fibonacci_sequence = np.zeros(length_of_)\nthird number is 0+1=1. The fourth number is 1+1=2. The fifth fibonacci_sequence[0] = 0\nnumber is 1+2=3. The sixth number is 2+3=5. The seventh fibonacci_sequence[1] = 1\nnumber is 3+5=8. The eighth number is 5+8=13. For i in range(3, length_of_fibonacci_sequence):\n···· (Skip 1000 tokens) fibonacci_sequence[i] = fibonacci_sequence[i-1] +\nThe 50th number is 32,432,268,459. fibonacci_sequence[i-2]\nCoT ans = fibonacci_sequence[-1] PoT\npython\nX\n32,432,268,459 12,586,269,025\" data-coord=\"top-left:(296,485); bottom-right:(2251,1003)\" /></figure>",
            "id": 15,
            "page": 2,
            "text": "length_of_fibonacci_sequence = 50 The first number is 0, the second number is 1, therefore, the fibonacci_sequence = np.zeros(length_of_) third number is 0+1=1. The fourth number is 1+1=2. The fifth fibonacci_sequence = 0 number is 1+2=3. The sixth number is 2+3=5. The seventh fibonacci_sequence = 1 number is 3+5=8. The eighth number is 5+8=13. For i in range(3, length_of_fibonacci_sequence): ···· (Skip 1000 tokens) fibonacci_sequence[i] = fibonacci_sequence[i-1] + The 50th number is 32,432,268,459. fibonacci_sequence[i-2] CoT ans = fibonacci_sequence[-1] PoT python X 32,432,268,459 12,586,269,025"
        },
        {
            "bounding_box": [
                {
                    "x": 301,
                    "y": 1019
                },
                {
                    "x": 2225,
                    "y": 1019
                },
                {
                    "x": 2225,
                    "y": 1101
                },
                {
                    "x": 301,
                    "y": 1101
                }
            ],
            "category": "caption",
            "html": "<br><caption id='16' style='font-size:14px'>Question: Ketty saves 20000 dollars to the bank. After three years, the sum with compound interest rate is 1000 dollars more<br>than the sum with simple interest rate. What is the interest rate of the bank?</caption>",
            "id": 16,
            "page": 2,
            "text": "Question: Ketty saves 20000 dollars to the bank. After three years, the sum with compound interest rate is 1000 dollars more than the sum with simple interest rate. What is the interest rate of the bank?"
        },
        {
            "bounding_box": [
                {
                    "x": 297,
                    "y": 1133
                },
                {
                    "x": 2249,
                    "y": 1133
                },
                {
                    "x": 2249,
                    "y": 1653
                },
                {
                    "x": 297,
                    "y": 1653
                }
            ],
            "category": "figure",
            "html": "<figure><img id='17' style='font-size:14px' alt=\"interest_rate = Symbol('x')\nAssuming the interest rate is X. The sum after two years with\nsum_in_bwo_years_with_simple_interest= 20000 +\nsimple interest rate is 20000 + X * 20000 * 3 = 20000 +\ninterest_rate * 20000 * 3\n60000x. The sum after two years with compoud interest rate\nsum_in_two_years_with_compound_interest = 20000 * (1 +\nis 20000 * (1 + x) A 3 = 200000 + 60000 * X + 60000x^2 +\ninterest_rate)**3\n20000x^3. The difference can be written as 60000x^2 +\n# Since compound interest is 1000 more than simple interest.\n20000x^3 = 1000. In order to solve X, we can use the\nans = so.weisum_antai_n_yeras_wit_comcound_interesi -\nquadratic formula. X = (-b +- sqrt(b^2 - 4ac)) / 2a, ···· , X =\nsum_affer_two_years_in_compound_interest - 1000,\n(-20000 +- 6160) / 120000, X = -0.051333.\nCoT interest_rate) PoT\npython\nX SymPy\n-0.051333 X = 0.24814\" data-coord=\"top-left:(297,1133); bottom-right:(2249,1653)\" /></figure>",
            "id": 17,
            "page": 2,
            "text": "interest_rate = Symbol('x') Assuming the interest rate is X. The sum after two years with sum_in_bwo_years_with_simple_interest= 20000 + simple interest rate is 20000 + X * 20000 * 3 = 20000 + interest_rate * 20000 * 3 60000x. The sum after two years with compoud interest rate sum_in_two_years_with_compound_interest = 20000 * (1 + is 20000 * (1 + x) A 3 = 200000 + 60000 * X + 60000x^2 + interest_rate)**3 20000x^3. The difference can be written as 60000x^2 + # Since compound interest is 1000 more than simple interest. 20000x^3 = 1000. In order to solve X, we can use the ans = so.weisum_antai_n_yeras_wit_comcound_interesi quadratic formula. X = (-b +- sqrt(b^2 - 4ac)) / 2a, ···· , X = sum_affer_two_years_in_compound_interest - 1000, (-20000 +- 6160) / 120000, X = -0.051333. CoT interest_rate) PoT python X SymPy -0.051333 X = 0.24814"
        },
        {
            "bounding_box": [
                {
                    "x": 567,
                    "y": 1704
                },
                {
                    "x": 1982,
                    "y": 1704
                },
                {
                    "x": 1982,
                    "y": 1753
                },
                {
                    "x": 567,
                    "y": 1753
                }
            ],
            "category": "caption",
            "html": "<caption id='18' style='font-size:22px'>Figure 1: Comparison between Chain of Thoughts and Program of Thoughts.</caption>",
            "id": 18,
            "page": 2,
            "text": "Figure 1: Comparison between Chain of Thoughts and Program of Thoughts."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 1812
                },
                {
                    "x": 2256,
                    "y": 1812
                },
                {
                    "x": 2256,
                    "y": 2116
                },
                {
                    "x": 293,
                    "y": 2116
                }
            ],
            "category": "paragraph",
            "html": "<p id='19' style='font-size:20px'>CoT uses LLMs for both reasoning and computation, i.e. the language model not only needs to generate<br>the mathematical expressions but also needs to perform the computation in each step. We argue that<br>language models are not ideal for actually solving these mathematical expressions, because: 1) LLMs are<br>very prone to arithmetic calculation errors, especially when dealing with large numbers; 2) LLMs cannot<br>solve complex mathematical expressions like polynomial equations or even differential equations; 3) LLMs<br>are highly inefficient at expressing iteration, especially when the number of iteration steps is large.</p>",
            "id": 19,
            "page": 2,
            "text": "CoT uses LLMs for both reasoning and computation, i.e. the language model not only needs to generate the mathematical expressions but also needs to perform the computation in each step. We argue that language models are not ideal for actually solving these mathematical expressions, because: 1) LLMs are very prone to arithmetic calculation errors, especially when dealing with large numbers; 2) LLMs cannot solve complex mathematical expressions like polynomial equations or even differential equations; 3) LLMs are highly inefficient at expressing iteration, especially when the number of iteration steps is large."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 2140
                },
                {
                    "x": 2257,
                    "y": 2140
                },
                {
                    "x": 2257,
                    "y": 2588
                },
                {
                    "x": 291,
                    "y": 2588
                }
            ],
            "category": "paragraph",
            "html": "<p id='20' style='font-size:18px'>In order to solve these issues, we propose program-of-thoughts (PoT) prompting, which will delegate com-<br>putation steps to an external language interpreter. In PoT, LMs can express reasoning steps as Python<br>programs, and the computation can be accomplished by a Python interpreter. We depict the difference<br>between CoT and PoT in Figure 1. In the upper example, for CoT the iteration runs for 50 times, which<br>leads to extremely low accuracy;1 in the lower example, CoT cannot solve the cubic equation with language<br>models and outputs a wrong answer. In contrast, in the upper example, PoT can express the iteration pro-<br>cess with a few lines of code, which can be executed on a Python interpreter to derive an accurate answer;<br>and in the lower example, PoT can convert the problem into a program that relies on 'SymPy' library in<br>Python to solve the complex equation.</p>",
            "id": 20,
            "page": 2,
            "text": "In order to solve these issues, we propose program-of-thoughts (PoT) prompting, which will delegate computation steps to an external language interpreter. In PoT, LMs can express reasoning steps as Python programs, and the computation can be accomplished by a Python interpreter. We depict the difference between CoT and PoT in Figure 1. In the upper example, for CoT the iteration runs for 50 times, which leads to extremely low accuracy;1 in the lower example, CoT cannot solve the cubic equation with language models and outputs a wrong answer. In contrast, in the upper example, PoT can express the iteration process with a few lines of code, which can be executed on a Python interpreter to derive an accurate answer; and in the lower example, PoT can convert the problem into a program that relies on 'SymPy' library in Python to solve the complex equation."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 2609
                },
                {
                    "x": 2258,
                    "y": 2609
                },
                {
                    "x": 2258,
                    "y": 2964
                },
                {
                    "x": 291,
                    "y": 2964
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='21' style='font-size:20px'>We evaluate PoT prompting across five MWP datasets, GSM8K, AQuA, SVAMP, TabMWP, MultiArith;<br>and three financial datasets, FinQA, ConvFinQA, and TATQA. These datasets cover various input formats<br>including text, tables, and conversation. We give an overview of the results in Figure 2. Under both few-<br>shot and zero-shot settings, PoT outperforms CoT significantly across all the evaluated datasets. Under the<br>few-shot setting, the average gain over CoT is around 8% for the MWP datasets and 15% for the financial<br>datasets. Under the zero-shot setting, the average gain over CoT is around 12% for the MWP datasets. PoT<br>combined with self-consistency (SC) also outperforms CoT+SC (Wang et al., 2022b) by an average of 10%</p>",
            "id": 21,
            "page": 2,
            "text": "We evaluate PoT prompting across five MWP datasets, GSM8K, AQuA, SVAMP, TabMWP, MultiArith; and three financial datasets, FinQA, ConvFinQA, and TATQA. These datasets cover various input formats including text, tables, and conversation. We give an overview of the results in Figure 2. Under both fewshot and zero-shot settings, PoT outperforms CoT significantly across all the evaluated datasets. Under the few-shot setting, the average gain over CoT is around 8% for the MWP datasets and 15% for the financial datasets. Under the zero-shot setting, the average gain over CoT is around 12% for the MWP datasets. PoT combined with self-consistency (SC) also outperforms CoT+SC (Wang , 2022b) by an average of 10%"
        },
        {
            "bounding_box": [
                {
                    "x": 349,
                    "y": 3010
                },
                {
                    "x": 2241,
                    "y": 3010
                },
                {
                    "x": 2241,
                    "y": 3051
                },
                {
                    "x": 349,
                    "y": 3051
                }
            ],
            "category": "paragraph",
            "html": "<p id='22' style='font-size:14px'>1 Assuming each addition is correct with 90% chance, after 50 additions, the likelihood of a correct output is less than 1%.</p>",
            "id": 22,
            "page": 2,
            "text": "1 Assuming each addition is correct with 90% chance, after 50 additions, the likelihood of a correct output is less than 1%."
        },
        {
            "bounding_box": [
                {
                    "x": 1261,
                    "y": 3134
                },
                {
                    "x": 1288,
                    "y": 3134
                },
                {
                    "x": 1288,
                    "y": 3171
                },
                {
                    "x": 1261,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='23' style='font-size:16px'>2</footer>",
            "id": 23,
            "page": 2,
            "text": "2"
        },
        {
            "bounding_box": [
                {
                    "x": 296,
                    "y": 109
                },
                {
                    "x": 1540,
                    "y": 109
                },
                {
                    "x": 1540,
                    "y": 158
                },
                {
                    "x": 296,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='24' style='font-size:20px'>Published in Transactions on Machine Learning Research (10/2023)</header>",
            "id": 24,
            "page": 3,
            "text": "Published in Transactions on Machine Learning Research (10/2023)"
        },
        {
            "bounding_box": [
                {
                    "x": 283,
                    "y": 321
                },
                {
                    "x": 2173,
                    "y": 321
                },
                {
                    "x": 2173,
                    "y": 1894
                },
                {
                    "x": 283,
                    "y": 1894
                }
            ],
            "category": "figure",
            "html": "<figure><img id='25' style='font-size:14px' alt=\"1. CoT IIPoT\n85.2\n76.4\n73.2\n71.6\n69\n65.2 64.5 64.6\n63.1\n61.4\n54.1\n45.3 45.5\n40.4\nGSM8K\nTATQA\nFinQA\nConvFin\nSVAMP\nTabMWP\nAQuA\nIs CoT-SC In PoT-SC\n89.1\n86.8\n80 81.8\n78\n75.4\n73.4\n68.1 67.3\n63.9\n58.6\n52\n47.9\n44.4\nFinQA\nTATQA\nConvFin\nGSM8K\nSVAMP\nTabMWP\nAQuA\nI. ZS-CoT ZS-PoT\n70.8\n65.2\n63.7\n60.4\n57\n53.5 52.5\n50.4\n43.9\n40.5 40.5\n31.9\n29.5\n28.2\nTATQA\nFinQA\nGSM8K\nConvFin\nSVAMP\nTabMWP\nAQuA\" data-coord=\"top-left:(283,321); bottom-right:(2173,1894)\" /></figure>",
            "id": 25,
            "page": 3,
            "text": "1. CoT IIPoT 85.2 76.4 73.2 71.6 69 65.2 64.5 64.6 63.1 61.4 54.1 45.3 45.5 40.4 GSM8K TATQA FinQA ConvFin SVAMP TabMWP AQuA Is CoT-SC In PoT-SC 89.1 86.8 80 81.8 78 75.4 73.4 68.1 67.3 63.9 58.6 52 47.9 44.4 FinQA TATQA ConvFin GSM8K SVAMP TabMWP AQuA I. ZS-CoT ZS-PoT 70.8 65.2 63.7 60.4 57 53.5 52.5 50.4 43.9 40.5 40.5 31.9 29.5 28.2 TATQA FinQA GSM8K ConvFin SVAMP TabMWP AQuA"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 1961
                },
                {
                    "x": 2257,
                    "y": 1961
                },
                {
                    "x": 2257,
                    "y": 2065
                },
                {
                    "x": 291,
                    "y": 2065
                }
            ],
            "category": "caption",
            "html": "<caption id='26' style='font-size:18px'>Figure 2: Few-shot (upper), Few-shot + SC (middle) and Zero-Shot (lower) Performance overview of Codex<br>PoT and Codex CoT across different datasets.</caption>",
            "id": 26,
            "page": 3,
            "text": "Figure 2: Few-shot (upper), Few-shot + SC (middle) and Zero-Shot (lower) Performance overview of Codex PoT and Codex CoT across different datasets."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 2169
                },
                {
                    "x": 2258,
                    "y": 2169
                },
                {
                    "x": 2258,
                    "y": 2321
                },
                {
                    "x": 289,
                    "y": 2321
                }
            ],
            "category": "paragraph",
            "html": "<p id='27' style='font-size:16px'>across all datasets. Our PoT+SC achieves the best-known results on all the evaluated MWP datasets and<br>near best-known results on the financial datasets (excluding GPT-4 (OpenAI, 2023)). Finally, we conduct<br>comprehensive ablation studies to understand the different components of PoT.</p>",
            "id": 27,
            "page": 3,
            "text": "across all datasets. Our PoT+SC achieves the best-known results on all the evaluated MWP datasets and near best-known results on the financial datasets (excluding GPT-4 (OpenAI, 2023)). Finally, we conduct comprehensive ablation studies to understand the different components of PoT."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 2418
                },
                {
                    "x": 881,
                    "y": 2418
                },
                {
                    "x": 881,
                    "y": 2482
                },
                {
                    "x": 293,
                    "y": 2482
                }
            ],
            "category": "paragraph",
            "html": "<p id='28' style='font-size:22px'>2 Program of Thoughts</p>",
            "id": 28,
            "page": 3,
            "text": "2 Program of Thoughts"
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 2549
                },
                {
                    "x": 654,
                    "y": 2549
                },
                {
                    "x": 654,
                    "y": 2600
                },
                {
                    "x": 293,
                    "y": 2600
                }
            ],
            "category": "paragraph",
            "html": "<p id='29' style='font-size:16px'>2.1 Preliminaries</p>",
            "id": 29,
            "page": 3,
            "text": "2.1 Preliminaries"
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 2654
                },
                {
                    "x": 2260,
                    "y": 2654
                },
                {
                    "x": 2260,
                    "y": 3055
                },
                {
                    "x": 290,
                    "y": 3055
                }
            ],
            "category": "paragraph",
            "html": "<p id='30' style='font-size:20px'>In-context learning has been described in Brown et al. (2020); Chen et al. (2021a); Chowdhery et al. (2022);<br>Rae et al. (2021). Compared with fine-tuning, in-context learning (1) only takes a few annotations/demon-<br>strations as a prompt, and (2) performs inference without training the model parameters. With in-context<br>learning, LLMs receive the input-output exemplars as the prefix, followed by an input problem, and generate<br>outputs imitating the exemplars. More recently, 'chain of thoughts prompting' (Wei et al., 2022) has been<br>proposed as a specific type of in-context learning where the exemplar's output contains the 'thought process'<br>or rationale instead of just an output. This approach has been shown to elicit LLMs' strong reasoning<br>capabilities on various kinds of tasks.</p>",
            "id": 30,
            "page": 3,
            "text": "In-context learning has been described in Brown  (2020); Chen  (2021a); Chowdhery  (2022); Rae  (2021). Compared with fine-tuning, in-context learning (1) only takes a few annotations/demonstrations as a prompt, and (2) performs inference without training the model parameters. With in-context learning, LLMs receive the input-output exemplars as the prefix, followed by an input problem, and generate outputs imitating the exemplars. More recently, 'chain of thoughts prompting' (Wei , 2022) has been proposed as a specific type of in-context learning where the exemplar's output contains the 'thought process' or rationale instead of just an output. This approach has been shown to elicit LLMs' strong reasoning capabilities on various kinds of tasks."
        },
        {
            "bounding_box": [
                {
                    "x": 1259,
                    "y": 3133
                },
                {
                    "x": 1288,
                    "y": 3133
                },
                {
                    "x": 1288,
                    "y": 3171
                },
                {
                    "x": 1259,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='31' style='font-size:14px'>3</footer>",
            "id": 31,
            "page": 3,
            "text": "3"
        },
        {
            "bounding_box": [
                {
                    "x": 296,
                    "y": 109
                },
                {
                    "x": 1540,
                    "y": 109
                },
                {
                    "x": 1540,
                    "y": 158
                },
                {
                    "x": 296,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='32' style='font-size:18px'>Published in Transactions on Machine Learning Research (10/2023)</header>",
            "id": 32,
            "page": 4,
            "text": "Published in Transactions on Machine Learning Research (10/2023)"
        },
        {
            "bounding_box": [
                {
                    "x": 338,
                    "y": 336
                },
                {
                    "x": 2206,
                    "y": 336
                },
                {
                    "x": 2206,
                    "y": 1189
                },
                {
                    "x": 338,
                    "y": 1189
                }
            ],
            "category": "figure",
            "html": "<figure><img id='33' style='font-size:14px' alt=\"Exemplars # Question: Toulouse has twice as many sheep as Charleston.\nCharleston has 4 times as many sheep as Seattle. How many\nsheep do Toulouse, Charleston, and Seattle have together if\nQuestior Questior Questior\nQuestior Question: ··· Seattle has 20 sheep?\nThoughts Thoughts Thoughts Thoughts Thoughts: ··· # Answer this question by implementing a solver() function.\ndef solver():\nTrain 1 departs at 6 AM from city A and arrive in city B in 11 # Let's write a Python program step by step, and then return\nAM. Train 2 departs at 7:30AM from city B and arrive in City A the answer\nat 11 AM. If both trains depart from their station Firstly, we need define the following variable:\n#\nsimultaneously, when will they meet?\n Prompt  Prompt\nLLM LLM\ntrainl_travel_ time = 11 - 6 Seattle = 20\n# 7:30 AM refers to 7.5 Charleston = Seattle * 4\ntrain2_travel_ time = 11 - 7.5 Toulouse = Charleston * 2\ndistance_of_city_a_and_b = Symbol('D', positive=True) # Then, we need to calculate the sum of the three\ntrainl_speed = distance_of_city / trainl_travel_ time sum = Seattle + Charleston + Toulouse\ntrain2_speed = distance_of_city/ train2_travel_time # Finally, we need to return the answer\nans = distance_of_city / (trainl_speed + train2_speed) return sum\" data-coord=\"top-left:(338,336); bottom-right:(2206,1189)\" /></figure>",
            "id": 33,
            "page": 4,
            "text": "Exemplars # Question: Toulouse has twice as many sheep as Charleston. Charleston has 4 times as many sheep as Seattle. How many sheep do Toulouse, Charleston, and Seattle have together if Questior Questior Questior Questior Question: ··· Seattle has 20 sheep? Thoughts Thoughts Thoughts Thoughts Thoughts: ··· # Answer this question by implementing a solver() function. def solver(): Train 1 departs at 6 AM from city A and arrive in city B in 11 # Let's write a Python program step by step, and then return AM. Train 2 departs at 7:30AM from city B and arrive in City A the answer at 11 AM. If both trains depart from their station Firstly, we need define the following variable: # simultaneously, when will they meet?  Prompt  Prompt LLM LLM trainl_travel_ time = 11 - 6 Seattle = 20 # 7:30 AM refers to 7.5 Charleston = Seattle * 4 train2_travel_ time = 11 - 7.5 Toulouse = Charleston * 2 distance_of_city_a_and_b = Symbol(\"D\", positive=True) # Then, we need to calculate the sum of the three trainl_speed = distance_of_city / trainl_travel_ time sum = Seattle + Charleston + Toulouse train2_speed = distance_of_city/ train2_travel_time # Finally, we need to return the answer ans = distance_of_city / (trainl_speed + train2_speed) return sum"
        },
        {
            "bounding_box": [
                {
                    "x": 581,
                    "y": 1232
                },
                {
                    "x": 1966,
                    "y": 1232
                },
                {
                    "x": 1966,
                    "y": 1287
                },
                {
                    "x": 581,
                    "y": 1287
                }
            ],
            "category": "caption",
            "html": "<caption id='34' style='font-size:20px'>Figure 3: Left: Few-shot PoT prompting, Right: Zero-shot PoT prompting.</caption>",
            "id": 34,
            "page": 4,
            "text": "Figure 3: Left: Few-shot PoT prompting, Right: Zero-shot PoT prompting."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 1497
                },
                {
                    "x": 821,
                    "y": 1497
                },
                {
                    "x": 821,
                    "y": 1553
                },
                {
                    "x": 291,
                    "y": 1553
                }
            ],
            "category": "paragraph",
            "html": "<p id='35' style='font-size:22px'>2.2 Program of Thoughts</p>",
            "id": 35,
            "page": 4,
            "text": "2.2 Program of Thoughts"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 1655
                },
                {
                    "x": 2259,
                    "y": 1655
                },
                {
                    "x": 2259,
                    "y": 2010
                },
                {
                    "x": 291,
                    "y": 2010
                }
            ],
            "category": "paragraph",
            "html": "<p id='36' style='font-size:16px'>Besides natural language, programs can also be used to express our thought processes. By using semantically<br>meaningful variable names, a program can also be a natural representation to convey human thoughts. For ex-<br>ample, in the lower example in Figure 1, we first create an unknown variable named interest_rate. Then we<br>bind 'summation in two years with ... interest rate' to the variable sum in_two_years_ with_XXX_ interest<br>and write down the equation expressing their mathematical relations with interest_rate. These equations<br>are packaged into the 'solve' function provided by 'SymPy'. The program is executed with Python to solve<br>the equations to derive the answer variable interest_rate.</p>",
            "id": 36,
            "page": 4,
            "text": "Besides natural language, programs can also be used to express our thought processes. By using semantically meaningful variable names, a program can also be a natural representation to convey human thoughts. For example, in the lower example in Figure 1, we first create an unknown variable named interest_rate. Then we bind 'summation in two years with ... interest rate' to the variable sum in_two_years_ with_XXX_ interest and write down the equation expressing their mathematical relations with interest_rate. These equations are packaged into the 'solve' function provided by 'SymPy'. The program is executed with Python to solve the equations to derive the answer variable interest_rate."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 2031
                },
                {
                    "x": 2259,
                    "y": 2031
                },
                {
                    "x": 2259,
                    "y": 2232
                },
                {
                    "x": 290,
                    "y": 2232
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='37' style='font-size:18px'>Unlike CoT, PoT relegates some computation to an external process (a Python interpreter). The LLMs<br>are only responsible for expressing the 'reasoning process' in the programming language. In contrast, CoT<br>aims to use LLMs to perform both reasoning and computation. We argue that such an approach is more<br>expressive and accurate in terms of numerical reasoning.</p>",
            "id": 37,
            "page": 4,
            "text": "Unlike CoT, PoT relegates some computation to an external process (a Python interpreter). The LLMs are only responsible for expressing the 'reasoning process' in the programming language. In contrast, CoT aims to use LLMs to perform both reasoning and computation. We argue that such an approach is more expressive and accurate in terms of numerical reasoning."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 2255
                },
                {
                    "x": 2258,
                    "y": 2255
                },
                {
                    "x": 2258,
                    "y": 2608
                },
                {
                    "x": 290,
                    "y": 2608
                }
            ],
            "category": "paragraph",
            "html": "<p id='38' style='font-size:18px'>The 'program of thoughts' is different from generating equations directly, where the generation target would<br>be solve(20000 * (1 + x)3 - 2000 - x * 20000 * 3 - 1000, x). As observed by Wei et al. (2022) for CoT, directly<br>generating such equations is challenging for LLMs. PoT differs from equation generation in two aspects: (1)<br>PoT breaks down the equation into a multi-step 'thought' process, and (2) PoT binds semantic meanings<br>to variables to help ground the model in language. We found that this sort of 'thoughtful' process can<br>elicit language models' reasoning capabilities and generate more accurate programs. We provide a detailed<br>comparison in the experimental section.</p>",
            "id": 38,
            "page": 4,
            "text": "The 'program of thoughts' is different from generating equations directly, where the generation target would be solve(20000 * (1 + x)3 - 2000 - x * 20000 * 3 - 1000, x). As observed by Wei  (2022) for CoT, directly generating such equations is challenging for LLMs. PoT differs from equation generation in two aspects: (1) PoT breaks down the equation into a multi-step 'thought' process, and (2) PoT binds semantic meanings to variables to help ground the model in language. We found that this sort of 'thoughtful' process can elicit language models' reasoning capabilities and generate more accurate programs. We provide a detailed comparison in the experimental section."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 2626
                },
                {
                    "x": 2258,
                    "y": 2626
                },
                {
                    "x": 2258,
                    "y": 2933
                },
                {
                    "x": 290,
                    "y": 2933
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='39' style='font-size:18px'>We show the proposed PoT prompting method in Figure 3 under the few-shot and zero-shot settings. Under<br>the few-shot setting, a few exemplars of (question, 'program of thoughts') pairs will be prefixed as demon-<br>strations to teach the LLM how to generate 'thoughtful' programs. Under the zero-shot setting, the prompt<br>only contains an instruction without any exemplar demonstration. Unlike zero-shot CoT (Kojima et al.,<br>2022), which requires an extra step to extract the answer from the 'chain of thoughts', zero-shot PoT can<br>return the answer straightforwardly without extra steps.</p>",
            "id": 39,
            "page": 4,
            "text": "We show the proposed PoT prompting method in Figure 3 under the few-shot and zero-shot settings. Under the few-shot setting, a few exemplars of (question, 'program of thoughts') pairs will be prefixed as demonstrations to teach the LLM how to generate 'thoughtful' programs. Under the zero-shot setting, the prompt only contains an instruction without any exemplar demonstration. Unlike zero-shot CoT (Kojima , 2022), which requires an extra step to extract the answer from the 'chain of thoughts', zero-shot PoT can return the answer straightforwardly without extra steps."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 2952
                },
                {
                    "x": 2258,
                    "y": 2952
                },
                {
                    "x": 2258,
                    "y": 3056
                },
                {
                    "x": 291,
                    "y": 3056
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='40' style='font-size:16px'>In zero-shot PoT, a caveat is that LLM can fall back to generating a reasoning chain in comments rather than<br>in the program. Therefore, we propose to suppress '#' token logits to encourage it to generate programs.</p>",
            "id": 40,
            "page": 4,
            "text": "In zero-shot PoT, a caveat is that LLM can fall back to generating a reasoning chain in comments rather than in the program. Therefore, we propose to suppress '#' token logits to encourage it to generate programs."
        },
        {
            "bounding_box": [
                {
                    "x": 1259,
                    "y": 3135
                },
                {
                    "x": 1288,
                    "y": 3135
                },
                {
                    "x": 1288,
                    "y": 3172
                },
                {
                    "x": 1259,
                    "y": 3172
                }
            ],
            "category": "footer",
            "html": "<footer id='41' style='font-size:14px'>4</footer>",
            "id": 41,
            "page": 4,
            "text": "4"
        },
        {
            "bounding_box": [
                {
                    "x": 295,
                    "y": 109
                },
                {
                    "x": 1540,
                    "y": 109
                },
                {
                    "x": 1540,
                    "y": 158
                },
                {
                    "x": 295,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='42' style='font-size:18px'>Published in Transactions on Machine Learning Research (10/2023)</header>",
            "id": 42,
            "page": 5,
            "text": "Published in Transactions on Machine Learning Research (10/2023)"
        },
        {
            "bounding_box": [
                {
                    "x": 342,
                    "y": 323
                },
                {
                    "x": 2197,
                    "y": 323
                },
                {
                    "x": 2197,
                    "y": 685
                },
                {
                    "x": 342,
                    "y": 685
                }
            ],
            "category": "figure",
            "html": "<figure><img id='43' style='font-size:14px' alt=\"LLM\nKeep prompting CoT Answer\nLLM\nQuestion PoT Progarm Result\nEOS Answer\" data-coord=\"top-left:(342,323); bottom-right:(2197,685)\" /></figure>",
            "id": 43,
            "page": 5,
            "text": "LLM Keep prompting CoT Answer LLM Question PoT Progarm Result EOS Answer"
        },
        {
            "bounding_box": [
                {
                    "x": 713,
                    "y": 722
                },
                {
                    "x": 1836,
                    "y": 722
                },
                {
                    "x": 1836,
                    "y": 777
                },
                {
                    "x": 713,
                    "y": 777
                }
            ],
            "category": "caption",
            "html": "<caption id='44' style='font-size:20px'>Figure 4: PoT combined with CoT for multi-stage reasoning.</caption>",
            "id": 44,
            "page": 5,
            "text": "Figure 4: PoT combined with CoT for multi-stage reasoning."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 844
                },
                {
                    "x": 965,
                    "y": 844
                },
                {
                    "x": 965,
                    "y": 898
                },
                {
                    "x": 291,
                    "y": 898
                }
            ],
            "category": "paragraph",
            "html": "<p id='45' style='font-size:14px'>2.3 PoT as an Intermediate Step</p>",
            "id": 45,
            "page": 5,
            "text": "2.3 PoT as an Intermediate Step"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 942
                },
                {
                    "x": 2257,
                    "y": 942
                },
                {
                    "x": 2257,
                    "y": 1094
                },
                {
                    "x": 291,
                    "y": 1094
                }
            ],
            "category": "paragraph",
            "html": "<p id='46' style='font-size:16px'>For certain problems requiring additional textual reasoning, we propose to utilize PoT to tackle the compu-<br>tation part. The program generated by PoT can be executed to provide intermediate result, which is further<br>combined with the question to derive the final answer with CoT. We depict the whole process in Figure 8.</p>",
            "id": 46,
            "page": 5,
            "text": "For certain problems requiring additional textual reasoning, we propose to utilize PoT to tackle the computation part. The program generated by PoT can be executed to provide intermediate result, which is further combined with the question to derive the final answer with CoT. We depict the whole process in Figure 8."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 1118
                },
                {
                    "x": 2261,
                    "y": 1118
                },
                {
                    "x": 2261,
                    "y": 1268
                },
                {
                    "x": 290,
                    "y": 1268
                }
            ],
            "category": "paragraph",
            "html": "<p id='47' style='font-size:16px'>During demonstration, we present LLMs with examples to teach it predict whether to an additional CoT<br>reasoning needs to be used. If LLM outputs 'keep prompting' in the end, we will adopt the execution results<br>from PoT as input to further prompt LLMs to derive the answer through CoT.</p>",
            "id": 47,
            "page": 5,
            "text": "During demonstration, we present LLMs with examples to teach it predict whether to an additional CoT reasoning needs to be used. If LLM outputs 'keep prompting' in the end, we will adopt the execution results from PoT as input to further prompt LLMs to derive the answer through CoT."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 1291
                },
                {
                    "x": 2258,
                    "y": 1291
                },
                {
                    "x": 2258,
                    "y": 1546
                },
                {
                    "x": 290,
                    "y": 1546
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='48' style='font-size:16px'>For instance, in the left example in Figure 3, the program will be executed to return a float number 'ans=2.05',<br>which means that after 2.05 hours the two trains will meet. However, directly adding 2.05 to 11 AM does not<br>make sense because 2.05 hour needs to be translated to minutes to obtain the standard HH:MM time format<br>to make it aligned with provided option in the multi-choice questions. Please note that this prompting<br>strategy is only needed for the AQuA because the other datasets can all be solved by PoT-only prompting.</p>",
            "id": 48,
            "page": 5,
            "text": "For instance, in the left example in Figure 3, the program will be executed to return a float number 'ans=2.05', which means that after 2.05 hours the two trains will meet. However, directly adding 2.05 to 11 AM does not make sense because 2.05 hour needs to be translated to minutes to obtain the standard HH:MM time format to make it aligned with provided option in the multi-choice questions. Please note that this prompting strategy is only needed for the AQuA because the other datasets can all be solved by PoT-only prompting."
        },
        {
            "bounding_box": [
                {
                    "x": 294,
                    "y": 1624
                },
                {
                    "x": 669,
                    "y": 1624
                },
                {
                    "x": 669,
                    "y": 1684
                },
                {
                    "x": 294,
                    "y": 1684
                }
            ],
            "category": "paragraph",
            "html": "<p id='49' style='font-size:22px'>3 Experiments</p>",
            "id": 49,
            "page": 5,
            "text": "3 Experiments"
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 1742
                },
                {
                    "x": 783,
                    "y": 1742
                },
                {
                    "x": 783,
                    "y": 1798
                },
                {
                    "x": 292,
                    "y": 1798
                }
            ],
            "category": "paragraph",
            "html": "<p id='50' style='font-size:20px'>3.1 Experimental Setup</p>",
            "id": 50,
            "page": 5,
            "text": "3.1 Experimental Setup"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 1841
                },
                {
                    "x": 2260,
                    "y": 1841
                },
                {
                    "x": 2260,
                    "y": 2045
                },
                {
                    "x": 291,
                    "y": 2045
                }
            ],
            "category": "paragraph",
            "html": "<p id='51' style='font-size:16px'>Datasets We summarize our evaluated datasets in Table 1. We use the test set for all the evaluated<br>datasets except TATQA. These datasets are highly heterogeneous in terms of their input formats. We<br>conduct comprehensive experiments on this broad spectrum of datasets to show the generalizability and<br>applicability of PoT prompting.</p>",
            "id": 51,
            "page": 5,
            "text": "Datasets We summarize our evaluated datasets in Table 1. We use the test set for all the evaluated datasets except TATQA. These datasets are highly heterogeneous in terms of their input formats. We conduct comprehensive experiments on this broad spectrum of datasets to show the generalizability and applicability of PoT prompting."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 2102
                },
                {
                    "x": 2303,
                    "y": 2102
                },
                {
                    "x": 2303,
                    "y": 2565
                },
                {
                    "x": 293,
                    "y": 2565
                }
            ],
            "category": "table",
            "html": "<table id='52' style='font-size:16px'><tr><td>Dataset</td><td>Split</td><td>Example</td><td>Domain</td><td>Input</td><td>Output</td></tr><tr><td>GSM8K (Cobbe et al., 2021)</td><td>Test</td><td>1318</td><td>MWP</td><td>Question</td><td>Number</td></tr><tr><td>AQuA (Ling et al., 2017)</td><td>Test</td><td>253</td><td>MWP</td><td>Question</td><td>Option</td></tr><tr><td>SVAMP (Patel et al., 2021)</td><td>Test</td><td>1000</td><td>MWP</td><td>Question</td><td>Number</td></tr><tr><td>MultiArith (Roy & Roth, 2015)</td><td>Test</td><td>600</td><td>MWP</td><td>Question</td><td>Number</td></tr><tr><td>TabMWP (Lu et al., 2022)</td><td>Test</td><td>7861</td><td>MWP</td><td>Table + Question</td><td>Number + Text</td></tr><tr><td>FinQA (Chen et al., 2021b)</td><td>Test</td><td>1147</td><td>Finance</td><td>Table + Text + Question</td><td>Number + Binary</td></tr><tr><td>ConvFinQA (Chen et al., 2022)</td><td>Test</td><td>421</td><td>Finance</td><td>Table + Text + Conversation</td><td>Number + Binary</td></tr><tr><td>TATQA (Zhu et al., 2021)</td><td>Dev</td><td>1668</td><td>Finance</td><td>Table + Text + Question</td><td>Number + Text</td></tr></table>",
            "id": 52,
            "page": 5,
            "text": "Dataset Split Example Domain Input Output  GSM8K (Cobbe , 2021) Test 1318 MWP Question Number  AQuA (Ling , 2017) Test 253 MWP Question Option  SVAMP (Patel , 2021) Test 1000 MWP Question Number  MultiArith (Roy & Roth, 2015) Test 600 MWP Question Number  TabMWP (Lu , 2022) Test 7861 MWP Table + Question Number + Text  FinQA (Chen , 2021b) Test 1147 Finance Table + Text + Question Number + Binary  ConvFinQA (Chen , 2022) Test 421 Finance Table + Text + Conversation Number + Binary  TATQA (Zhu , 2021) Dev 1668 Finance Table + Text + Question"
        },
        {
            "bounding_box": [
                {
                    "x": 728,
                    "y": 2603
                },
                {
                    "x": 1822,
                    "y": 2603
                },
                {
                    "x": 1822,
                    "y": 2654
                },
                {
                    "x": 728,
                    "y": 2654
                }
            ],
            "category": "caption",
            "html": "<caption id='53' style='font-size:16px'>Table 1: Summarization of all the datasets being evaluated.</caption>",
            "id": 53,
            "page": 5,
            "text": "Table 1: Summarization of all the datasets being evaluated."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 2751
                },
                {
                    "x": 2261,
                    "y": 2751
                },
                {
                    "x": 2261,
                    "y": 3057
                },
                {
                    "x": 290,
                    "y": 3057
                }
            ],
            "category": "paragraph",
            "html": "<p id='54' style='font-size:16px'>To incorporate the diverse inputs, we propose to linearize these inputs in the prompt. For table inputs, we<br>adopt the same strategy as Chen (2022) to linearize a table into a text string. The columns of the table are<br>separated by 'P and the rows are separated by '\\n'. If a table cell is empty, it is filled by '-'. For text+table<br>hybrid inputs, we separate tables and text with '\\n'. For conversational history, we also separate conversation<br>turns by '\\n'. The prompt is constructed by the concatenation of task instruction, text, linearized table, and<br>question. For conversational question answering, we simply concatenate all the dialog history in the prompt.</p>",
            "id": 54,
            "page": 5,
            "text": "To incorporate the diverse inputs, we propose to linearize these inputs in the prompt. For table inputs, we adopt the same strategy as Chen (2022) to linearize a table into a text string. The columns of the table are separated by 'P and the rows are separated by '\\n'. If a table cell is empty, it is filled by '-'. For text+table hybrid inputs, we separate tables and text with '\\n'. For conversational history, we also separate conversation turns by '\\n'. The prompt is constructed by the concatenation of task instruction, text, linearized table, and question. For conversational question answering, we simply concatenate all the dialog history in the prompt."
        },
        {
            "bounding_box": [
                {
                    "x": 1259,
                    "y": 3132
                },
                {
                    "x": 1289,
                    "y": 3132
                },
                {
                    "x": 1289,
                    "y": 3172
                },
                {
                    "x": 1259,
                    "y": 3172
                }
            ],
            "category": "footer",
            "html": "<footer id='55' style='font-size:16px'>5</footer>",
            "id": 55,
            "page": 5,
            "text": "5"
        },
        {
            "bounding_box": [
                {
                    "x": 295,
                    "y": 109
                },
                {
                    "x": 1540,
                    "y": 109
                },
                {
                    "x": 1540,
                    "y": 158
                },
                {
                    "x": 295,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='56' style='font-size:20px'>Published in Transactions on Machine Learning Research (10/2023)</header>",
            "id": 56,
            "page": 6,
            "text": "Published in Transactions on Machine Learning Research (10/2023)"
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 343
                },
                {
                    "x": 2260,
                    "y": 343
                },
                {
                    "x": 2260,
                    "y": 794
                },
                {
                    "x": 290,
                    "y": 794
                }
            ],
            "category": "paragraph",
            "html": "<p id='57' style='font-size:22px'>Implementation Details We mainly use the OpenAI Codex (code-davinci-002) API2 for our experi-<br>ments. We also tested GPT-3 (text-davinci-002), ChatGPT (gpt-turbo-3.5), CodeGen (Nijkamp et al.,<br>2022) (codegen-16B-multi and codegen-16B-mono), CodeT5+ (Wang et al., 2023b) and Xgen3 for ablation<br>experiments. We use Python 3.8 with the SymPy library4 to execute the generated program. For the<br>few-shot setting, we use 4-8 shots for all the datasets, based on their difficulty. For simple datasets like<br>FinQA (Chen et al., 2021b), we tend to use fewer shots, while for more challenging datasets like AQuA (Ling<br>et al., 2017) and TATQA (Zhu et al., 2021), we use 8 shots to cover more diverse problems. The examples<br>are taken from the training set. We generally write prompts for 10-20 examples and then tune the exemplar<br>selection on a small validation set to choose the best 4-8 shots for the full set evaluation.</p>",
            "id": 57,
            "page": 6,
            "text": "Implementation Details We mainly use the OpenAI Codex (code-davinci-002) API2 for our experiments. We also tested GPT-3 (text-davinci-002), ChatGPT (gpt-turbo-3.5), CodeGen (Nijkamp , 2022) (codegen-16B-multi and codegen-16B-mono), CodeT5+ (Wang , 2023b) and Xgen3 for ablation experiments. We use Python 3.8 with the SymPy library4 to execute the generated program. For the few-shot setting, we use 4-8 shots for all the datasets, based on their difficulty. For simple datasets like FinQA (Chen , 2021b), we tend to use fewer shots, while for more challenging datasets like AQuA (Ling , 2017) and TATQA (Zhu , 2021), we use 8 shots to cover more diverse problems. The examples are taken from the training set. We generally write prompts for 10-20 examples and then tune the exemplar selection on a small validation set to choose the best 4-8 shots for the full set evaluation."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 817
                },
                {
                    "x": 2259,
                    "y": 817
                },
                {
                    "x": 2259,
                    "y": 1120
                },
                {
                    "x": 290,
                    "y": 1120
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='58' style='font-size:18px'>To elicit the LLM's capability to perform multi-step reasoning, we found a prompt to encourage LLMs to<br>generate reasonable programs without demonstration. The detailed prompt is shown in Figure 3. However,<br>a caveat is that LLM can fall back to generating a reasoning chain in comments rather than in the program.<br>Therefore, we suppress the '#' token logits by a small bias to decrease its probability to avoid such cases. In<br>our preliminary study, we found that -2 as the bias can achieve the best result. We found that this simple<br>strategy can greatly improve our performance.</p>",
            "id": 58,
            "page": 6,
            "text": "To elicit the LLM's capability to perform multi-step reasoning, we found a prompt to encourage LLMs to generate reasonable programs without demonstration. The detailed prompt is shown in Figure 3. However, a caveat is that LLM can fall back to generating a reasoning chain in comments rather than in the program. Therefore, we suppress the '#' token logits by a small bias to decrease its probability to avoid such cases. In our preliminary study, we found that -2 as the bias can achieve the best result. We found that this simple strategy can greatly improve our performance."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 1172
                },
                {
                    "x": 2259,
                    "y": 1172
                },
                {
                    "x": 2259,
                    "y": 1526
                },
                {
                    "x": 290,
                    "y": 1526
                }
            ],
            "category": "paragraph",
            "html": "<p id='59' style='font-size:20px'>Metrics We adopt exact match scores as our evaluation metrics for GSM8K, SVAMP, and MultiArith<br>datasets. We will round the predicted number to a specific precision and then compare it with the reference<br>number. For the AQuA dataset, we use PoT to compute the intermediate answer and then prompt the<br>LLM again to output the closest option to measure the accuracy. For TabMWP, ConvFinQA, and TATQA<br>datasets, we use the official evaluation scripts provided on Github. For FinQA, we relax the evaluation for<br>CoT because LLMs cannot perform the computation precisely (especially with high-precision floats and large<br>numbers), SO we adopt 'math.isclose' with relative tolerance of 0.001 to compare answers.</p>",
            "id": 59,
            "page": 6,
            "text": "Metrics We adopt exact match scores as our evaluation metrics for GSM8K, SVAMP, and MultiArith datasets. We will round the predicted number to a specific precision and then compare it with the reference number. For the AQuA dataset, we use PoT to compute the intermediate answer and then prompt the LLM again to output the closest option to measure the accuracy. For TabMWP, ConvFinQA, and TATQA datasets, we use the official evaluation scripts provided on Github. For FinQA, we relax the evaluation for CoT because LLMs cannot perform the computation precisely (especially with high-precision floats and large numbers), SO we adopt 'math.isclose' with relative tolerance of 0.001 to compare answers."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 1578
                },
                {
                    "x": 2260,
                    "y": 1578
                },
                {
                    "x": 2260,
                    "y": 1935
                },
                {
                    "x": 291,
                    "y": 1935
                }
            ],
            "category": "paragraph",
            "html": "<p id='60' style='font-size:22px'>Baselines We report results for three different models including Codex (Chen et al., 2021a), GPT-3 (Brown<br>et al., 2020), PaLM (Chowdhery et al., 2022) and LaMDA (Thoppilan et al., 2022). We consider two types<br>of prediction strategies including direct answer output and chain of thought to derive the answer. Since<br>PaLM API is not public, we only list PaLM results reported from previous work (Wei et al., 2022; Wang<br>et al., 2022b). We also leverage an external calculator as suggested in Wei et al. (2022) for all the equations<br>generated by CoT, which is denoted as CoT + calc. Besides greedy decoding, we use self-consistency (Wang<br>et al., 2022b) with CoT, taking the majority vote over 40 different completions as the prediction.</p>",
            "id": 60,
            "page": 6,
            "text": "Baselines We report results for three different models including Codex (Chen , 2021a), GPT-3 (Brown , 2020), PaLM (Chowdhery , 2022) and LaMDA (Thoppilan , 2022). We consider two types of prediction strategies including direct answer output and chain of thought to derive the answer. Since PaLM API is not public, we only list PaLM results reported from previous work (Wei , 2022; Wang , 2022b). We also leverage an external calculator as suggested in Wei  (2022) for all the equations generated by CoT, which is denoted as CoT + calc. Besides greedy decoding, we use self-consistency (Wang , 2022b) with CoT, taking the majority vote over 40 different completions as the prediction."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 1991
                },
                {
                    "x": 658,
                    "y": 1991
                },
                {
                    "x": 658,
                    "y": 2043
                },
                {
                    "x": 293,
                    "y": 2043
                }
            ],
            "category": "paragraph",
            "html": "<p id='61' style='font-size:20px'>3.2 Main Results</p>",
            "id": 61,
            "page": 6,
            "text": "3.2 Main Results"
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 2082
                },
                {
                    "x": 2259,
                    "y": 2082
                },
                {
                    "x": 2259,
                    "y": 2635
                },
                {
                    "x": 290,
                    "y": 2635
                }
            ],
            "category": "paragraph",
            "html": "<p id='62' style='font-size:20px'>Few-shot Results We give our few-shot results in Table 2. On MWP datasets, PoT with greedy decoding<br>improves on GSM8K / AQuA/' TabMWP by more than 8%. On SVAMP, the improvement is 4% mainly due<br>to its simplicity. For financial QA datasets, PoT improves over CoT by roughly 20% on FinQA/ ConvFinQA<br>and 8% on TATQA. The larger improvements in FinQA and ConvFinQA are mainly due to miscalculations<br>on LLMs for large numbers (e.g. in the millions). CoT adopts LLMs to perform the computation, which<br>is highly prone to miscalculation errors, while PoT adopts a highly precise external computer to solve the<br>problem. As an ablation, we also compare with CoT+calc, which leverages an external calculator to correct<br>the calculation results in the generated 'chain of thoughts'. The experiments show that adding an external<br>calculator only shows mild improvement over CoT on MWP datasets, much behind PoT. The main reason<br>for poor performance of 'calculator' is due to its rigid post-processing step, which can lead to low recall in<br>terms of calibrating the calculation results.</p>",
            "id": 62,
            "page": 6,
            "text": "Few-shot Results We give our few-shot results in Table 2. On MWP datasets, PoT with greedy decoding improves on GSM8K / AQuA/' TabMWP by more than 8%. On SVAMP, the improvement is 4% mainly due to its simplicity. For financial QA datasets, PoT improves over CoT by roughly 20% on FinQA/ ConvFinQA and 8% on TATQA. The larger improvements in FinQA and ConvFinQA are mainly due to miscalculations on LLMs for large numbers (e.g. in the millions). CoT adopts LLMs to perform the computation, which is highly prone to miscalculation errors, while PoT adopts a highly precise external computer to solve the problem. As an ablation, we also compare with CoT+calc, which leverages an external calculator to correct the calculation results in the generated 'chain of thoughts'. The experiments show that adding an external calculator only shows mild improvement over CoT on MWP datasets, much behind PoT. The main reason for poor performance of 'calculator' is due to its rigid post-processing step, which can lead to low recall in terms of calibrating the calculation results."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 2688
                },
                {
                    "x": 2260,
                    "y": 2688
                },
                {
                    "x": 2260,
                    "y": 2893
                },
                {
                    "x": 292,
                    "y": 2893
                }
            ],
            "category": "paragraph",
            "html": "<p id='63' style='font-size:20px'>Few-shot + Self- Consistency Results We leverage self-consistency (SC) decoding to understand the<br>upper bound of our method. This sampling-based decoding algorithm can greatly reduce randomness in<br>the generation procedure and boosts performance. Specifically, we set a temperature of 0.4 and K=40<br>throughout our experiments. According to Table 2, we found that PoT + SC still outperforms CoT + SC</p>",
            "id": 63,
            "page": 6,
            "text": "Few-shot + Self- Consistency Results We leverage self-consistency (SC) decoding to understand the upper bound of our method. This sampling-based decoding algorithm can greatly reduce randomness in the generation procedure and boosts performance. Specifically, we set a temperature of 0.4 and K=40 throughout our experiments. According to Table 2, we found that PoT + SC still outperforms CoT + SC"
        },
        {
            "bounding_box": [
                {
                    "x": 336,
                    "y": 2930
                },
                {
                    "x": 1042,
                    "y": 2930
                },
                {
                    "x": 1042,
                    "y": 3053
                },
                {
                    "x": 336,
                    "y": 3053
                }
            ],
            "category": "paragraph",
            "html": "<p id='64' style='font-size:14px'>2https : / /openai · com/blog/openai-codex/<br>3https:/ /blog,salesforceairesearch.com/: xgen/<br>4https: / /www · sympy · org/en/index. html</p>",
            "id": 64,
            "page": 6,
            "text": "2https : / /openai · com/blog/openai-codex/ 3https:/ /blog,salesforceairesearch.com/: xgen/ 4https: / /www · sympy · org/en/index. html"
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3133
                },
                {
                    "x": 1288,
                    "y": 3133
                },
                {
                    "x": 1288,
                    "y": 3171
                },
                {
                    "x": 1260,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='65' style='font-size:16px'>6</footer>",
            "id": 65,
            "page": 6,
            "text": "6"
        },
        {
            "bounding_box": [
                {
                    "x": 296,
                    "y": 110
                },
                {
                    "x": 1540,
                    "y": 110
                },
                {
                    "x": 1540,
                    "y": 158
                },
                {
                    "x": 296,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='66' style='font-size:20px'>Published in Transactions on Machine Learning Research (10/2023)</header>",
            "id": 66,
            "page": 7,
            "text": "Published in Transactions on Machine Learning Research (10/2023)"
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 332
                },
                {
                    "x": 2268,
                    "y": 332
                },
                {
                    "x": 2268,
                    "y": 1601
                },
                {
                    "x": 290,
                    "y": 1601
                }
            ],
            "category": "table",
            "html": "<table id='67' style='font-size:14px'><tr><td>Model</td><td>#Params</td><td>GSM8K</td><td>AQuA</td><td>SVAMP</td><td>TabWMP</td><td>FinQA</td><td>ConvFin</td><td>TATQA</td><td>Avg</td></tr><tr><td colspan=\"10\">Fine-tuned or few-shot prompt</td></tr><tr><td>Published SoTA</td><td>-</td><td>78.0</td><td>52.0</td><td>86.8</td><td>68.2</td><td>68.0</td><td>68.9</td><td>73.6</td><td>70.7</td></tr><tr><td colspan=\"10\">Few-shot prompt (Greedy Decoding)</td></tr><tr><td>Codex Direct</td><td>175B</td><td>19.7</td><td>29.5</td><td>69.9</td><td>59.4</td><td>25.6</td><td>40.0</td><td>55.0</td><td>42.7</td></tr><tr><td>Codex CoT</td><td>175B</td><td>63.1</td><td>45.3</td><td>76.4</td><td>65.2</td><td>40.4</td><td>45.6</td><td>61.4</td><td>56.7</td></tr><tr><td>GPT-3 Direct</td><td>175B</td><td>15.6</td><td>24.8</td><td>65.7</td><td>57.1</td><td>14.4</td><td>29.1</td><td>37.9</td><td>34.9</td></tr><tr><td>GPT-3 CoT</td><td>175B</td><td>46.9</td><td>35.8</td><td>68.9</td><td>62.9</td><td>26.1</td><td>37.4</td><td>42.5</td><td>45.7</td></tr><tr><td>PaLM Direct</td><td>540B</td><td>17.9</td><td>25.2</td><td>69.4</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>PaLM CoT</td><td>540B</td><td>56.9</td><td>35.8</td><td>79.0</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Codex COTcalc</td><td>175B</td><td>65.4</td><td>45.3</td><td>77.0</td><td>65.8</td><td>、</td><td></td><td>-</td><td></td></tr><tr><td>GPT-3 CoT calc</td><td>175B</td><td>49.6</td><td>35.8</td><td>70.3</td><td>63.4</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>PaLM COTcalc</td><td>540B</td><td>58.6</td><td>35.8</td><td>79.8</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>PoT-Codex</td><td>175B</td><td>71.6</td><td>54.1</td><td>85.2</td><td>73.2</td><td>64.5</td><td>64.6</td><td>69.0</td><td>68.9</td></tr><tr><td colspan=\"10\">Few-shot prompt (Self-Consistency Decoding)</td></tr><tr><td>LaMDA CoT-SC</td><td>137B</td><td>27.7</td><td>26.8</td><td>53.5</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Codex CoT-SC</td><td>175B</td><td>78.0</td><td>52.0</td><td>86.8</td><td>75.4</td><td>44.4</td><td>47.9</td><td>63.2</td><td>63.9</td></tr><tr><td>PaLM CoT-SC</td><td>540B</td><td>74.4</td><td>48.3</td><td>86.6</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>PoT-SC-Codex</td><td>175B</td><td>80.0</td><td>58.6</td><td>89.1</td><td>81.8</td><td>68.1</td><td>67.3</td><td>70.2</td><td>73.6</td></tr><tr><td colspan=\"10\">Few-shot prompt (GPT-4)</td></tr><tr><td>CoT-GPT4</td><td>175B</td><td>92.0</td><td>72.4</td><td>97.0</td><td>-</td><td>58.2</td><td>-</td><td>-</td><td></td></tr><tr><td>PoT-GPT4</td><td>175B</td><td>97.2</td><td>84.4</td><td>97.4</td><td>-</td><td>74.0</td><td></td><td>-</td><td></td></tr></table>",
            "id": 67,
            "page": 7,
            "text": "Model #Params GSM8K AQuA SVAMP TabWMP FinQA ConvFin TATQA Avg  Fine-tuned or few-shot prompt  Published SoTA - 78.0 52.0 86.8 68.2 68.0 68.9 73.6 70.7  Few-shot prompt (Greedy Decoding)  Codex Direct 175B 19.7 29.5 69.9 59.4 25.6 40.0 55.0 42.7  Codex CoT 175B 63.1 45.3 76.4 65.2 40.4 45.6 61.4 56.7  GPT-3 Direct 175B 15.6 24.8 65.7 57.1 14.4 29.1 37.9 34.9  GPT-3 CoT 175B 46.9 35.8 68.9 62.9 26.1 37.4 42.5 45.7  PaLM Direct 540B 17.9 25.2 69.4 - - - -  PaLM CoT 540B 56.9 35.8 79.0 - - - -  Codex COTcalc 175B 65.4 45.3 77.0 65.8 、  -   GPT-3 CoT calc 175B 49.6 35.8 70.3 63.4 - - -  PaLM COTcalc 540B 58.6 35.8 79.8 - - - -  PoT-Codex 175B 71.6 54.1 85.2 73.2 64.5 64.6 69.0 68.9  Few-shot prompt (Self-Consistency Decoding)  LaMDA CoT-SC 137B 27.7 26.8 53.5 - - - -  Codex CoT-SC 175B 78.0 52.0 86.8 75.4 44.4 47.9 63.2 63.9  PaLM CoT-SC 540B 74.4 48.3 86.6 - - - -  PoT-SC-Codex 175B 80.0 58.6 89.1 81.8 68.1 67.3 70.2 73.6  Few-shot prompt (GPT-4)  CoT-GPT4 175B 92.0 72.4 97.0 - 58.2 - -   PoT-GPT4 175B 97.2 84.4 97.4 - 74.0  -"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 1637
                },
                {
                    "x": 2260,
                    "y": 1637
                },
                {
                    "x": 2260,
                    "y": 1939
                },
                {
                    "x": 291,
                    "y": 1939
                }
            ],
            "category": "paragraph",
            "html": "<p id='68' style='font-size:22px'>Table 2: The few-shot results for different datasets. Published SoTA includes the best-known results (ex-<br>cluding results obtained by GPT-4). On GSM8K, AQuA and SVAMP, the prior SoTA results are CoT +<br>self-consistency decoding (Wang et al., 2022b). On FinQA, the prior best result is from Wang et al. (2022a).<br>On ConvFinQA, the prior best result is achieved by FinQANet (Chen et al., 2022). On TabWMP (Lu et al.,<br>2022), the prior best result is achieved by Dynamic Prompt Learning (Lu et al., 2022). On TATQA, the<br>SoTA result is by RegHNT (Lei et al., 2022).</p>",
            "id": 68,
            "page": 7,
            "text": "Table 2: The few-shot results for different datasets. Published SoTA includes the best-known results (excluding results obtained by GPT-4). On GSM8K, AQuA and SVAMP, the prior SoTA results are CoT + self-consistency decoding (Wang , 2022b). On FinQA, the prior best result is from Wang  (2022a). On ConvFinQA, the prior best result is achieved by FinQANet (Chen , 2022). On TabWMP (Lu , 2022), the prior best result is achieved by Dynamic Prompt Learning (Lu , 2022). On TATQA, the SoTA result is by RegHNT (Lei , 2022)."
        },
        {
            "bounding_box": [
                {
                    "x": 374,
                    "y": 2014
                },
                {
                    "x": 2182,
                    "y": 2014
                },
                {
                    "x": 2182,
                    "y": 2317
                },
                {
                    "x": 374,
                    "y": 2317
                }
            ],
            "category": "table",
            "html": "<table id='69' style='font-size:14px'><tr><td>Model</td><td>#Params</td><td>GSM8K</td><td>AQuA</td><td>SVAMP</td><td>TabMWP</td><td>MultiArith</td><td>Avg</td></tr><tr><td>Zero-shot Direct (GPT-3)</td><td>175B</td><td>12.6</td><td>22.4</td><td>58.7</td><td>38.9</td><td>22.7</td><td>31.0</td></tr><tr><td>Zero-shot CoT (GPT-3)</td><td>175B</td><td>40.5</td><td>31.9</td><td>63.7</td><td>53.5</td><td>79.3</td><td>53.7</td></tr><tr><td>Zero-shot CoT (PaLM)</td><td>540B</td><td>43.0</td><td>-</td><td>-</td><td>-</td><td>66.1</td><td>-</td></tr><tr><td>Zero-shot PoT (Ours)</td><td>175B</td><td>57.0</td><td>43.9</td><td>70.8</td><td>66.5</td><td>92.2</td><td>66.1</td></tr></table>",
            "id": 69,
            "page": 7,
            "text": "Model #Params GSM8K AQuA SVAMP TabMWP MultiArith Avg  Zero-shot Direct (GPT-3) 175B 12.6 22.4 58.7 38.9 22.7 31.0  Zero-shot CoT (GPT-3) 175B 40.5 31.9 63.7 53.5 79.3 53.7  Zero-shot CoT (PaLM) 540B 43.0 - - - 66.1  Zero-shot PoT (Ours) 175B 57.0 43.9 70.8 66.5 92.2"
        },
        {
            "bounding_box": [
                {
                    "x": 294,
                    "y": 2351
                },
                {
                    "x": 2252,
                    "y": 2351
                },
                {
                    "x": 2252,
                    "y": 2406
                },
                {
                    "x": 294,
                    "y": 2406
                }
            ],
            "category": "paragraph",
            "html": "<p id='70' style='font-size:16px'>Table 3: The zero-shot results for different datasets. The baseline results are taken from Kojima et al. (2022).</p>",
            "id": 70,
            "page": 7,
            "text": "Table 3: The zero-shot results for different datasets. The baseline results are taken from Kojima  (2022)."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 2523
                },
                {
                    "x": 2258,
                    "y": 2523
                },
                {
                    "x": 2258,
                    "y": 2674
                },
                {
                    "x": 291,
                    "y": 2674
                }
            ],
            "category": "paragraph",
            "html": "<p id='71' style='font-size:18px'>on MWP datasets with notable margins. On financial datasets, we observe that self-consistency decoding<br>is less impactful for both PoT and CoT. Similarly, PoT + SC outperforms CoT + SC by roughly 20% on<br>FinQA/ConvFinQA and 7% on TATQA.</p>",
            "id": 71,
            "page": 7,
            "text": "on MWP datasets with notable margins. On financial datasets, we observe that self-consistency decoding is less impactful for both PoT and CoT. Similarly, PoT + SC outperforms CoT + SC by roughly 20% on FinQA/ConvFinQA and 7% on TATQA."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 2752
                },
                {
                    "x": 2259,
                    "y": 2752
                },
                {
                    "x": 2259,
                    "y": 3056
                },
                {
                    "x": 292,
                    "y": 3056
                }
            ],
            "category": "paragraph",
            "html": "<p id='72' style='font-size:18px'>Zero-shot Results We also evaluate the zero-shot performance of PoT and compare with Kojima et al.<br>(2022) in Table 3. As can be seen, zero-shot PoT significantly outperforms zero-shot CoT across all the MWP<br>datasets evaluated. Compared to few-shot prompting, zero-shot PoT outperforms zero-shot CoT (Kojima<br>et al., 2022) by an even larger margin. On the evaluated datasets, PoT's outperforms CoT by an average of<br>12%. On TabMWP, zero-shot PoT is even higher than few-shot CoT. These results show the great potential<br>to directly generalize to many unseen numerical tasks even without any dataset-specific exemplars.</p>",
            "id": 72,
            "page": 7,
            "text": "Zero-shot Results We also evaluate the zero-shot performance of PoT and compare with Kojima  (2022) in Table 3. As can be seen, zero-shot PoT significantly outperforms zero-shot CoT across all the MWP datasets evaluated. Compared to few-shot prompting, zero-shot PoT outperforms zero-shot CoT (Kojima , 2022) by an even larger margin. On the evaluated datasets, PoT's outperforms CoT by an average of 12%. On TabMWP, zero-shot PoT is even higher than few-shot CoT. These results show the great potential to directly generalize to many unseen numerical tasks even without any dataset-specific exemplars."
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3134
                },
                {
                    "x": 1290,
                    "y": 3134
                },
                {
                    "x": 1290,
                    "y": 3171
                },
                {
                    "x": 1260,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='73' style='font-size:14px'>7</footer>",
            "id": 73,
            "page": 7,
            "text": "7"
        },
        {
            "bounding_box": [
                {
                    "x": 296,
                    "y": 111
                },
                {
                    "x": 1539,
                    "y": 111
                },
                {
                    "x": 1539,
                    "y": 157
                },
                {
                    "x": 296,
                    "y": 157
                }
            ],
            "category": "header",
            "html": "<header id='74' style='font-size:22px'>Published in Transactions on Machine Learning Research (10/2023)</header>",
            "id": 74,
            "page": 8,
            "text": "Published in Transactions on Machine Learning Research (10/2023)"
        },
        {
            "bounding_box": [
                {
                    "x": 795,
                    "y": 337
                },
                {
                    "x": 1755,
                    "y": 337
                },
                {
                    "x": 1755,
                    "y": 781
                },
                {
                    "x": 795,
                    "y": 781
                }
            ],
            "category": "table",
            "html": "<table id='75' style='font-size:16px'><tr><td>Model</td><td>#Params</td><td>GSM8K</td><td>SVAMP</td></tr><tr><td>code-davinci-002</td><td>175B</td><td>71.6</td><td>85.2</td></tr><tr><td>text-davinci-002</td><td>175B</td><td>60.4</td><td>80.1</td></tr><tr><td>gpt-3.5-turbo</td><td>-</td><td>76.3</td><td>88.2</td></tr><tr><td>codegen-16B-multi</td><td>16B</td><td>8.2</td><td>29.2</td></tr><tr><td>codegen-16B-mono</td><td>16B</td><td>12. 7</td><td>41.1</td></tr><tr><td>codeT5+</td><td>16B</td><td>12.5</td><td>38.5</td></tr><tr><td>xgen</td><td>7B</td><td>11.0</td><td>40.6</td></tr></table>",
            "id": 75,
            "page": 8,
            "text": "Model #Params GSM8K SVAMP  code-davinci-002 175B 71.6 85.2  text-davinci-002 175B 60.4 80.1  gpt-3.5-turbo - 76.3 88.2  codegen-16B-multi 16B 8.2 29.2  codegen-16B-mono 16B 12. 7 41.1  codeT5+ 16B 12.5 38.5  xgen 7B 11.0"
        },
        {
            "bounding_box": [
                {
                    "x": 648,
                    "y": 812
                },
                {
                    "x": 1898,
                    "y": 812
                },
                {
                    "x": 1898,
                    "y": 863
                },
                {
                    "x": 648,
                    "y": 863
                }
            ],
            "category": "caption",
            "html": "<br><caption id='76' style='font-size:22px'>Table 4: PoT prompting performance with different backend model.</caption>",
            "id": 76,
            "page": 8,
            "text": "Table 4: PoT prompting performance with different backend model."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 882
                },
                {
                    "x": 2171,
                    "y": 882
                },
                {
                    "x": 2171,
                    "y": 1738
                },
                {
                    "x": 290,
                    "y": 1738
                }
            ],
            "category": "figure",
            "html": "<figure><img id='77' style='font-size:14px' alt=\"Is v1 1. v2 IIv2\n0.74\n0.73\n0.7\n0.69\n0.68\n0.67\n0.65 0.65\n0.62\n0.6\n0.58\n0.55\n2-shots 4-shots 6-shots 8-shots\n0.66 0.66\n0.64 0.64 0.64\n0.63 0.63\n0.62 0.62 0.62 0.62\n0.58\n2-shots 4-shots 6-shots 8-shots\" data-coord=\"top-left:(290,882); bottom-right:(2171,1738)\" /></figure>",
            "id": 77,
            "page": 8,
            "text": "Is v1 1. v2 IIv2 0.74 0.73 0.7 0.69 0.68 0.67 0.65 0.65 0.62 0.6 0.58 0.55 2-shots 4-shots 6-shots 8-shots 0.66 0.66 0.64 0.64 0.64 0.63 0.63 0.62 0.62 0.62 0.62 0.58 2-shots 4-shots 6-shots 8-shots"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 1767
                },
                {
                    "x": 2257,
                    "y": 1767
                },
                {
                    "x": 2257,
                    "y": 1866
                },
                {
                    "x": 291,
                    "y": 1866
                }
            ],
            "category": "caption",
            "html": "<caption id='78' style='font-size:22px'>Figure 5: Exemplar sensitivity analysis for GSM8K and FinQA, where v1, v2 and v3 are three versions of<br>k-shot demonstration sampled from the pool.</caption>",
            "id": 78,
            "page": 8,
            "text": "Figure 5: Exemplar sensitivity analysis for GSM8K and FinQA, where v1, v2 and v3 are three versions of k-shot demonstration sampled from the pool."
        },
        {
            "bounding_box": [
                {
                    "x": 294,
                    "y": 1931
                },
                {
                    "x": 723,
                    "y": 1931
                },
                {
                    "x": 723,
                    "y": 1981
                },
                {
                    "x": 294,
                    "y": 1981
                }
            ],
            "category": "paragraph",
            "html": "<p id='79' style='font-size:22px'>3.3 Ablation Studies</p>",
            "id": 79,
            "page": 8,
            "text": "3.3 Ablation Studies"
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 2026
                },
                {
                    "x": 2257,
                    "y": 2026
                },
                {
                    "x": 2257,
                    "y": 2128
                },
                {
                    "x": 292,
                    "y": 2128
                }
            ],
            "category": "paragraph",
            "html": "<p id='80' style='font-size:22px'>We performed multiple ablation studies under the few-shot setting to understand the importance of different<br>factors in PoT including the backbone models, prompt engineering, etc.</p>",
            "id": 80,
            "page": 8,
            "text": "We performed multiple ablation studies under the few-shot setting to understand the importance of different factors in PoT including the backbone models, prompt engineering, etc."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 2190
                },
                {
                    "x": 2260,
                    "y": 2190
                },
                {
                    "x": 2260,
                    "y": 2641
                },
                {
                    "x": 290,
                    "y": 2641
                }
            ],
            "category": "paragraph",
            "html": "<p id='81' style='font-size:20px'>Backend Ablation To understand PoT's performance on different backbone models, we compare the<br>performance of text-davinci-002, code-davinci-002, gpt-3.5-turbo, codegen-16B-mono, codegen-16B-multi,<br>CodeT5+ and XGen. We choose three representative datasets GSM8K, SVAMP, and FinQA to analyze<br>the results. We show our experimental results in Table 4. As can be seen, gpt-3.5-turbo can achieve the<br>highest score to outperform codex (code-davinci-002) by a remarkable margin. In contrast, text-davinci-<br>002 is weaker than code-davinci-002, which is mainly because the following text-based instruction tuning<br>undermines the models' capabilities to generate code. A concerning fact we found is that the open source<br>model like codegen Nijkamp et al. (2022) is significantly behind across different benchmarks. We conjecture<br>that such a huge gap could be attributed to non-sufficient pre-training and model size.</p>",
            "id": 81,
            "page": 8,
            "text": "Backend Ablation To understand PoT's performance on different backbone models, we compare the performance of text-davinci-002, code-davinci-002, gpt-3.5-turbo, codegen-16B-mono, codegen-16B-multi, CodeT5+ and XGen. We choose three representative datasets GSM8K, SVAMP, and FinQA to analyze the results. We show our experimental results in Table 4. As can be seen, gpt-3.5-turbo can achieve the highest score to outperform codex (code-davinci-002) by a remarkable margin. In contrast, text-davinci002 is weaker than code-davinci-002, which is mainly because the following text-based instruction tuning undermines the models' capabilities to generate code. A concerning fact we found is that the open source model like codegen Nijkamp  (2022) is significantly behind across different benchmarks. We conjecture that such a huge gap could be attributed to non-sufficient pre-training and model size."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 2704
                },
                {
                    "x": 2261,
                    "y": 2704
                },
                {
                    "x": 2261,
                    "y": 3055
                },
                {
                    "x": 290,
                    "y": 3055
                }
            ],
            "category": "paragraph",
            "html": "<p id='82' style='font-size:20px'>Sensitivity to Exemplars To better understand how sensitive PoT is w.r.t different exemplars, we con-<br>duct a sensitivity analysis. Specifically, we wrote 20 total exemplars. For k-shot learning, we randomly<br>sample k = (2, 4, 6, 8) out of the 20 exemplars three times as v1, v2, and v3. We will use these randomly<br>sampled exemplars as demonstrations for PoT. We summarize our sensitivity analysis in Figure 5. First of<br>all, we found that increasing the number of shots helps more for GSM8K than FinQA. This is mainly due to<br>the diversity of questions in GSM8K. By adding more exemplars, the language models can better generalize<br>to diverse questions. Another observation is that when given fewer exemplars, PoT's performance variance is</p>",
            "id": 82,
            "page": 8,
            "text": "Sensitivity to Exemplars To better understand how sensitive PoT is w.r.t different exemplars, we conduct a sensitivity analysis. Specifically, we wrote 20 total exemplars. For k-shot learning, we randomly sample k = (2, 4, 6, 8) out of the 20 exemplars three times as v1, v2, and v3. We will use these randomly sampled exemplars as demonstrations for PoT. We summarize our sensitivity analysis in Figure 5. First of all, we found that increasing the number of shots helps more for GSM8K than FinQA. This is mainly due to the diversity of questions in GSM8K. By adding more exemplars, the language models can better generalize to diverse questions. Another observation is that when given fewer exemplars, PoT's performance variance is"
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3136
                },
                {
                    "x": 1289,
                    "y": 3136
                },
                {
                    "x": 1289,
                    "y": 3170
                },
                {
                    "x": 1260,
                    "y": 3170
                }
            ],
            "category": "footer",
            "html": "<footer id='83' style='font-size:18px'>8</footer>",
            "id": 83,
            "page": 8,
            "text": "8"
        },
        {
            "bounding_box": [
                {
                    "x": 297,
                    "y": 112
                },
                {
                    "x": 1539,
                    "y": 112
                },
                {
                    "x": 1539,
                    "y": 157
                },
                {
                    "x": 297,
                    "y": 157
                }
            ],
            "category": "header",
            "html": "<header id='84' style='font-size:20px'>Published in Transactions on Machine Learning Research (10/2023)</header>",
            "id": 84,
            "page": 9,
            "text": "Published in Transactions on Machine Learning Research (10/2023)"
        },
        {
            "bounding_box": [
                {
                    "x": 519,
                    "y": 339
                },
                {
                    "x": 2027,
                    "y": 339
                },
                {
                    "x": 2027,
                    "y": 548
                },
                {
                    "x": 519,
                    "y": 548
                }
            ],
            "category": "table",
            "html": "<table id='85' style='font-size:16px'><tr><td>Model</td><td>GSM8K</td><td>GSM8K-Hard</td><td>SVAMP</td><td>ASDIV</td><td>ADDSUB</td><td>MULTIARITH</td></tr><tr><td>PaL</td><td>72.0</td><td>61.2</td><td>79.4</td><td>79.6</td><td>92.5</td><td>99.2</td></tr><tr><td>PoT</td><td>71.6</td><td>61.8</td><td>85.2</td><td>85.2</td><td>92.2</td><td>99.5</td></tr></table>",
            "id": 85,
            "page": 9,
            "text": "Model GSM8K GSM8K-Hard SVAMP ASDIV ADDSUB MULTIARITH  PaL 72.0 61.2 79.4 79.6 92.5 99.2  PoT 71.6 61.8 85.2 85.2 92.2"
        },
        {
            "bounding_box": [
                {
                    "x": 541,
                    "y": 596
                },
                {
                    "x": 2009,
                    "y": 596
                },
                {
                    "x": 2009,
                    "y": 643
                },
                {
                    "x": 541,
                    "y": 643
                }
            ],
            "category": "caption",
            "html": "<caption id='86' style='font-size:22px'>Table 5: Comparison of PoT against contemporary work PaL (Gao et al., 2022).</caption>",
            "id": 86,
            "page": 9,
            "text": "Table 5: Comparison of PoT against contemporary work PaL (Gao , 2022)."
        },
        {
            "bounding_box": [
                {
                    "x": 833,
                    "y": 705
                },
                {
                    "x": 1713,
                    "y": 705
                },
                {
                    "x": 1713,
                    "y": 946
                },
                {
                    "x": 833,
                    "y": 946
                }
            ],
            "category": "table",
            "html": "<table id='87' style='font-size:14px'><tr><td>Method</td><td>GSM8K</td><td>SVAMP</td><td>FinQA</td></tr><tr><td>PoT</td><td>71.6</td><td>85.2</td><td>64.5</td></tr><tr><td>PoT - Binding</td><td>60.2</td><td>83.8</td><td>61.6</td></tr><tr><td>PoT - MultiStep</td><td>45.8</td><td>81.9</td><td>58.9</td></tr></table>",
            "id": 87,
            "page": 9,
            "text": "Method GSM8K SVAMP FinQA  PoT 71.6 85.2 64.5  PoT - Binding 60.2 83.8 61.6  PoT - MultiStep 45.8 81.9"
        },
        {
            "bounding_box": [
                {
                    "x": 480,
                    "y": 979
                },
                {
                    "x": 2075,
                    "y": 979
                },
                {
                    "x": 2075,
                    "y": 1031
                },
                {
                    "x": 480,
                    "y": 1031
                }
            ],
            "category": "caption",
            "html": "<caption id='88' style='font-size:18px'>Table 6: Comparison between PoT and equation generation on three different datasets.</caption>",
            "id": 88,
            "page": 9,
            "text": "Table 6: Comparison between PoT and equation generation on three different datasets."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 1107
                },
                {
                    "x": 2254,
                    "y": 1107
                },
                {
                    "x": 2254,
                    "y": 1207
                },
                {
                    "x": 292,
                    "y": 1207
                }
            ],
            "category": "paragraph",
            "html": "<p id='89' style='font-size:16px'>larger. When K=2, the performance variance can be as large as 7% for both datasets. With more exemplars,<br>the performance becomes more stable.</p>",
            "id": 89,
            "page": 9,
            "text": "larger. When K=2, the performance variance can be as large as 7% for both datasets. With more exemplars, the performance becomes more stable."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 1277
                },
                {
                    "x": 2257,
                    "y": 1277
                },
                {
                    "x": 2257,
                    "y": 1427
                },
                {
                    "x": 292,
                    "y": 1427
                }
            ],
            "category": "paragraph",
            "html": "<p id='90' style='font-size:18px'>Comparison with PaL We also compare PoT with another more recent related approach like PaL (Gao<br>et al., 2022). According to to Table 5, we found that our method is in general better than PaL, especially<br>on SVAMP and ASDIV. Our results are 6% higher than their prompting method.</p>",
            "id": 90,
            "page": 9,
            "text": "Comparison with PaL We also compare PoT with another more recent related approach like PaL (Gao , 2022). According to to Table 5, we found that our method is in general better than PaL, especially on SVAMP and ASDIV. Our results are 6% higher than their prompting method."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 1496
                },
                {
                    "x": 2258,
                    "y": 1496
                },
                {
                    "x": 2258,
                    "y": 1947
                },
                {
                    "x": 291,
                    "y": 1947
                }
            ],
            "category": "paragraph",
            "html": "<p id='91' style='font-size:20px'>Semantic Binding and Multi-Step Reasoning The two core properties of 'program of thoughts' are:<br>(1) multiple steps: breaking down the thought process into the step-by-step program, (2) semantic binding:<br>associating semantic meaning to the variable names. To better understand how these two properties con-<br>tribute, we compared with two variants. One variant is to remove the semantic binding and simply use a,b,c<br>as the variable names. The other variant is to directly predict the final mathematical equation to compute<br>the results. We show our findings in Table 6. As can be seen, removing the binding will in general hurt the<br>model's performance. On more complex questions involving more variables like GSM8K, the performance<br>drop is larger. Similarly, prompting LLMs to directly generate the target equations is also very challenging.<br>Breaking down the target equation into multiple reasoning steps helps boost performance.</p>",
            "id": 91,
            "page": 9,
            "text": "Semantic Binding and Multi-Step Reasoning The two core properties of 'program of thoughts' are: (1) multiple steps: breaking down the thought process into the step-by-step program, (2) semantic binding: associating semantic meaning to the variable names. To better understand how these two properties contribute, we compared with two variants. One variant is to remove the semantic binding and simply use a,b,c as the variable names. The other variant is to directly predict the final mathematical equation to compute the results. We show our findings in Table 6. As can be seen, removing the binding will in general hurt the model's performance. On more complex questions involving more variables like GSM8K, the performance drop is larger. Similarly, prompting LLMs to directly generate the target equations is also very challenging. Breaking down the target equation into multiple reasoning steps helps boost performance."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 2013
                },
                {
                    "x": 2258,
                    "y": 2013
                },
                {
                    "x": 2258,
                    "y": 2466
                },
                {
                    "x": 292,
                    "y": 2466
                }
            ],
            "category": "paragraph",
            "html": "<p id='92' style='font-size:20px'>Breakdown Analysis We perform further analysis to determine which kinds of problems CoT and PoT<br>differ most in performance. We use AQuA (Ling et al., 2017) as our testbed for this. Specifically, we manually<br>classify the questions in AQuA into several categories including geometry, polynomial, symbolic, arithmetic,<br>combinatorics, linear equation, iterative and probability. We show the accuracy for each subcategory in Fig-<br>ure 6. The major categories are (1) linear equations, (2) arithmetic, (3) combinatorics, (4) probability, and<br>(5) iterative. The largest improvements of PoT are in the categories 'linear /polynomial equation', 'iterative',<br>'symbolic', and 'combinatorics'. These questions require more complex arithmetic or symbolic skills to solve.<br>In contrast, on 'arithmetic', 'probability', and 'geometric' questions, PoT and CoT perform similarly. Such<br>observation reflects our assumption that 'program' is more effective on more challenging problems.</p>",
            "id": 92,
            "page": 9,
            "text": "Breakdown Analysis We perform further analysis to determine which kinds of problems CoT and PoT differ most in performance. We use AQuA (Ling , 2017) as our testbed for this. Specifically, we manually classify the questions in AQuA into several categories including geometry, polynomial, symbolic, arithmetic, combinatorics, linear equation, iterative and probability. We show the accuracy for each subcategory in Figure 6. The major categories are (1) linear equations, (2) arithmetic, (3) combinatorics, (4) probability, and (5) iterative. The largest improvements of PoT are in the categories 'linear /polynomial equation', 'iterative', 'symbolic', and 'combinatorics'. These questions require more complex arithmetic or symbolic skills to solve. In contrast, on 'arithmetic', 'probability', and 'geometric' questions, PoT and CoT perform similarly. Such observation reflects our assumption that 'program' is more effective on more challenging problems."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 2553
                },
                {
                    "x": 2164,
                    "y": 2553
                },
                {
                    "x": 2164,
                    "y": 2983
                },
                {
                    "x": 291,
                    "y": 2983
                }
            ],
            "category": "figure",
            "html": "<figure><img id='93' style='font-size:14px' alt=\"IIPoT Ii CoT 86 88\n72\n66 65\n62\n56\n50 50\n40 40 40\n38\n20 20\n10\ngeo poly symb arith comb lin-eq iter prob\" data-coord=\"top-left:(291,2553); bottom-right:(2164,2983)\" /></figure>",
            "id": 93,
            "page": 9,
            "text": "IIPoT Ii CoT 86 88 72 66 65 62 56 50 50 40 40 40 38 20 20 10 geo poly symb arith comb lin-eq iter prob"
        },
        {
            "bounding_box": [
                {
                    "x": 532,
                    "y": 3026
                },
                {
                    "x": 2020,
                    "y": 3026
                },
                {
                    "x": 2020,
                    "y": 3076
                },
                {
                    "x": 532,
                    "y": 3076
                }
            ],
            "category": "caption",
            "html": "<caption id='94' style='font-size:18px'>Figure 6: PoT and CoT's breakdown accuracy across different types of questions.</caption>",
            "id": 94,
            "page": 9,
            "text": "Figure 6: PoT and CoT's breakdown accuracy across different types of questions."
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3135
                },
                {
                    "x": 1288,
                    "y": 3135
                },
                {
                    "x": 1288,
                    "y": 3170
                },
                {
                    "x": 1260,
                    "y": 3170
                }
            ],
            "category": "footer",
            "html": "<footer id='95' style='font-size:16px'>9</footer>",
            "id": 95,
            "page": 9,
            "text": "9"
        },
        {
            "bounding_box": [
                {
                    "x": 296,
                    "y": 111
                },
                {
                    "x": 1539,
                    "y": 111
                },
                {
                    "x": 1539,
                    "y": 157
                },
                {
                    "x": 296,
                    "y": 157
                }
            ],
            "category": "header",
            "html": "<header id='96' style='font-size:20px'>Published in Transactions on Machine Learning Research (10/2023)</header>",
            "id": 96,
            "page": 10,
            "text": "Published in Transactions on Machine Learning Research (10/2023)"
        },
        {
            "bounding_box": [
                {
                    "x": 353,
                    "y": 348
                },
                {
                    "x": 2154,
                    "y": 348
                },
                {
                    "x": 2154,
                    "y": 1486
                },
                {
                    "x": 353,
                    "y": 1486
                }
            ],
            "category": "figure",
            "html": "<figure><img id='97' style='font-size:14px' alt=\"Type 1 Error: Incorrect Value Grounding\nRead the following text and table, and then write code to answer a question:\n·· · Comparative information of the Company's overall revenues by end-use markets and geography for years ended June 30,\n2019, 2018 and 2017 were as follows:\nContext: ($ in millions) I 2019 I 2018 I 2017\nAerospace and Defense I $1,327.9 I $1,182.3 I $973.3\nMedical I 205.0 I 175.3 I 125.5\nDistribution I 136.4 I 131.7 I 118.7\nWhat was the change in Distribution in 2019 from 2018?\nQuestion: What is the change in the beginning balance between fiscal years 2019 and 2018?\ndistribution_in_2018 = 1317 The model generate the computation logic correctly,\ndistribution_in_2019 = 1364 however, the model accidentally assign 131.7 and 136.4 as\nOutput:\nans = distribution_in_2019 - distribution_in_2018 V 1317 and 1364. The model ground the value of relevant\nunits = 'million' V variables incorrectly.\nType 2 Error: Incorrect Logic Generation\nRead the following text and table, and then write code to answer a question:\n· · · Net operating losses and tax attributes generated by domestic and foreign entities in closed years and utilized in open years\nare subject to adjustment by the tax authorities.\nContext: - I 2019 I 2018\nBalance at the beginning of the fiscal year I $1,264 I $1,626\nLapse in statute of limitations I -29 I -58\nBalance at the end of the fiscal year I $1,258 I $1,264\nQuestion: What is the change in the beginning balance between fiscal years 2019 and 2018?\nbeginning_balance_2018 = 1626 The variables values are grounded correctly, however, the\nbeginning_balance_2019 = 1264 change of beginning balance should be computed by the\nOutput:\nans = beginning_balance_2018 - beginning_ balance_2019 X value of 2019 subtracted by the value of 2018. The model\nunits = 'thousand' V generate the computation logic incorrectly.\" data-coord=\"top-left:(353,348); bottom-right:(2154,1486)\" /></figure>",
            "id": 97,
            "page": 10,
            "text": "Type 1 Error: Incorrect Value Grounding Read the following text and table, and then write code to answer a question: ·· · Comparative information of the Company's overall revenues by end-use markets and geography for years ended June 30, 2019, 2018 and 2017 were as follows: Context: ($ in millions) I 2019 I 2018 I 2017 Aerospace and Defense I $1,327.9 I $1,182.3 I $973.3 Medical I 205.0 I 175.3 I 125.5 Distribution I 136.4 I 131.7 I 118.7 What was the change in Distribution in 2019 from 2018? Question: What is the change in the beginning balance between fiscal years 2019 and 2018? distribution_in_2018 = 1317 The model generate the computation logic correctly, distribution_in_2019 = 1364 however, the model accidentally assign 131.7 and 136.4 as Output: ans = distribution_in_2019 - distribution_in_2018 V 1317 and 1364. The model ground the value of relevant units = 'million' V variables incorrectly. Type 2 Error: Incorrect Logic Generation Read the following text and table, and then write code to answer a question: · · · Net operating losses and tax attributes generated by domestic and foreign entities in closed years and utilized in open years are subject to adjustment by the tax authorities. Context: - I 2019 I 2018 Balance at the beginning of the fiscal year I $1,264 I $1,626 Lapse in statute of limitations I -29 I -58 Balance at the end of the fiscal year I $1,258 I $1,264 Question: What is the change in the beginning balance between fiscal years 2019 and 2018? beginning_balance_2018 = 1626 The variables values are grounded correctly, however, the beginning_balance_2019 = 1264 change of beginning balance should be computed by the Output: ans = beginning_balance_2018 - beginning_ balance_2019 X value of 2019 subtracted by the value of 2018. The model units = 'thousand' V generate the computation logic incorrectly."
        },
        {
            "bounding_box": [
                {
                    "x": 641,
                    "y": 1526
                },
                {
                    "x": 1906,
                    "y": 1526
                },
                {
                    "x": 1906,
                    "y": 1577
                },
                {
                    "x": 641,
                    "y": 1577
                }
            ],
            "category": "caption",
            "html": "<caption id='98' style='font-size:18px'>Figure 7: Error cases on TAT-QA dev set using PoT-greedy method.</caption>",
            "id": 98,
            "page": 10,
            "text": "Figure 7: Error cases on TAT-QA dev set using PoT-greedy method."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 1633
                },
                {
                    "x": 2260,
                    "y": 1633
                },
                {
                    "x": 2260,
                    "y": 2185
                },
                {
                    "x": 290,
                    "y": 2185
                }
            ],
            "category": "paragraph",
            "html": "<p id='99' style='font-size:18px'>Error Analysis We considered two types of errors: (1) value grounding error, and (2) logic generation<br>error. The first type indicates that the model fails to assign correct values to the variables relevant to the<br>question. The second type indicates that the model fails to generate the correct computation process to<br>answer the question based on the defined variables. Figure 7 shows an example of each type of error. In<br>the upper example, the model fetches the value of the variables incorrectly while the computation logic is<br>correct. In the lower example, the model grounded relevant variables correctly but fails to generate proper<br>computation logic to answer the question. We manually examined the errors made in the TAT-QA results.<br>Among the 198 failure cases of numerical reasoning questions with the PoT (greedy) method, 47% have value<br>grounding errors and 33% have logic errors. In 15% both types of errors occurred and in 5% we believe the<br>answer is actually correct. We found that the majority of the errors are value grounding errors, which is also<br>common for other methods such as CoT.</p>",
            "id": 99,
            "page": 10,
            "text": "Error Analysis We considered two types of errors: (1) value grounding error, and (2) logic generation error. The first type indicates that the model fails to assign correct values to the variables relevant to the question. The second type indicates that the model fails to generate the correct computation process to answer the question based on the defined variables. Figure 7 shows an example of each type of error. In the upper example, the model fetches the value of the variables incorrectly while the computation logic is correct. In the lower example, the model grounded relevant variables correctly but fails to generate proper computation logic to answer the question. We manually examined the errors made in the TAT-QA results. Among the 198 failure cases of numerical reasoning questions with the PoT (greedy) method, 47% have value grounding errors and 33% have logic errors. In 15% both types of errors occurred and in 5% we believe the answer is actually correct. We found that the majority of the errors are value grounding errors, which is also common for other methods such as CoT."
        },
        {
            "bounding_box": [
                {
                    "x": 294,
                    "y": 2254
                },
                {
                    "x": 698,
                    "y": 2254
                },
                {
                    "x": 698,
                    "y": 2305
                },
                {
                    "x": 294,
                    "y": 2305
                }
            ],
            "category": "paragraph",
            "html": "<p id='100' style='font-size:22px'>4 Related Work</p>",
            "id": 100,
            "page": 10,
            "text": "4 Related Work"
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 2363
                },
                {
                    "x": 1028,
                    "y": 2363
                },
                {
                    "x": 1028,
                    "y": 2414
                },
                {
                    "x": 293,
                    "y": 2414
                }
            ],
            "category": "paragraph",
            "html": "<p id='101' style='font-size:20px'>4.1 Mathematical Reasoning in NLP</p>",
            "id": 101,
            "page": 10,
            "text": "4.1 Mathematical Reasoning in NLP"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 2454
                },
                {
                    "x": 2260,
                    "y": 2454
                },
                {
                    "x": 2260,
                    "y": 3056
                },
                {
                    "x": 291,
                    "y": 3056
                }
            ],
            "category": "paragraph",
            "html": "<p id='102' style='font-size:20px'>Mathematical reasoning skills are essential for general-purpose intelligent systems, which have attracted a<br>significant amount of attention from the community. Earlier, there have been studies in understanding NLP<br>models' capabilities to solve arithmetic/algebraic questions (Hosseini et al., 2014; Koncel-Kedziorski et al.,<br>2015; Roy & Roth, 2015; Ling et al., 2017; Roy & Roth, 2018). Recently, more challenging datasets (Dua<br>et al., 2019; Saxton et al., 2019; Miao et al., 2020; Amini et al., 2019; Hendrycks et al., 2021; Patel et al., 2021)<br>have been proposed to increase the difficulty, diversity or even adversarial robustness. LiLA (Mishra et al.,<br>2022) proposes to assemble a large set of mathematical datasets into a unified dataset. LiLA also annotates<br>Python programs as the generation target for solving mathematical problems. However, LiLA (Mishra et al.,<br>2022) is mostly focused on dataset unification. Our work aims to understand how to generate 'thoughtful<br>programs' to best elicit LLM's reasoning capability. Besides, we also investigate how to solve math problems<br>without any exemplars. Austin et al. (2021) propose to evaluate LLMs' capabilities to synthesize code on<br>two curated datasets MBPP and MathQA-Python.</p>",
            "id": 102,
            "page": 10,
            "text": "Mathematical reasoning skills are essential for general-purpose intelligent systems, which have attracted a significant amount of attention from the community. Earlier, there have been studies in understanding NLP models' capabilities to solve arithmetic/algebraic questions (Hosseini , 2014; Koncel-Kedziorski , 2015; Roy & Roth, 2015; Ling , 2017; Roy & Roth, 2018). Recently, more challenging datasets (Dua , 2019; Saxton , 2019; Miao , 2020; Amini , 2019; Hendrycks , 2021; Patel , 2021) have been proposed to increase the difficulty, diversity or even adversarial robustness. LiLA (Mishra , 2022) proposes to assemble a large set of mathematical datasets into a unified dataset. LiLA also annotates Python programs as the generation target for solving mathematical problems. However, LiLA (Mishra , 2022) is mostly focused on dataset unification. Our work aims to understand how to generate 'thoughtful programs' to best elicit LLM's reasoning capability. Besides, we also investigate how to solve math problems without any exemplars. Austin  (2021) propose to evaluate LLMs' capabilities to synthesize code on two curated datasets MBPP and MathQA-Python."
        },
        {
            "bounding_box": [
                {
                    "x": 1253,
                    "y": 3133
                },
                {
                    "x": 1299,
                    "y": 3133
                },
                {
                    "x": 1299,
                    "y": 3172
                },
                {
                    "x": 1253,
                    "y": 3172
                }
            ],
            "category": "footer",
            "html": "<footer id='103' style='font-size:16px'>10</footer>",
            "id": 103,
            "page": 10,
            "text": "10"
        },
        {
            "bounding_box": [
                {
                    "x": 296,
                    "y": 110
                },
                {
                    "x": 1539,
                    "y": 110
                },
                {
                    "x": 1539,
                    "y": 158
                },
                {
                    "x": 296,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='104' style='font-size:18px'>Published in Transactions on Machine Learning Research (10/2023)</header>",
            "id": 104,
            "page": 11,
            "text": "Published in Transactions on Machine Learning Research (10/2023)"
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 344
                },
                {
                    "x": 1000,
                    "y": 344
                },
                {
                    "x": 1000,
                    "y": 396
                },
                {
                    "x": 292,
                    "y": 396
                }
            ],
            "category": "paragraph",
            "html": "<p id='105' style='font-size:18px'>4.2 In-context Learning with LLMs</p>",
            "id": 105,
            "page": 11,
            "text": "4.2 In-context Learning with LLMs"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 432
                },
                {
                    "x": 2261,
                    "y": 432
                },
                {
                    "x": 2261,
                    "y": 884
                },
                {
                    "x": 291,
                    "y": 884
                }
            ],
            "category": "paragraph",
            "html": "<p id='106' style='font-size:18px'>GPT-3 (Brown et al., 2020) demonstrated a strong capability to perform few-shot predictions, where the<br>model is given a description of the task in natural language with few examples. Scaling model size, data,<br>and computing are crucial to enable this learning ability. Recently, Rae et al. (2021); Smith et al. (2022);<br>Chowdhery et al. (2022); Du et al. (2022) have proposed to train different types of LLMs with different<br>training recipes. The capability to follow few-shot exemplars to solve unseen tasks is not existent on smaller<br>LMs, but only emerge as the model scales up (Kaplan et al., 2020). Recently, there have been several<br>works (Xie et al., 2021; Min et al., 2022) aiming to understand how and why in-context learning works.<br>Another concurrent work similar to ours is BINDER (Cheng et al., 2022), which applies Codex to synthesize<br>'soft' SQL queries to answer questions from tables.</p>",
            "id": 106,
            "page": 11,
            "text": "GPT-3 (Brown , 2020) demonstrated a strong capability to perform few-shot predictions, where the model is given a description of the task in natural language with few examples. Scaling model size, data, and computing are crucial to enable this learning ability. Recently, Rae  (2021); Smith  (2022); Chowdhery  (2022); Du  (2022) have proposed to train different types of LLMs with different training recipes. The capability to follow few-shot exemplars to solve unseen tasks is not existent on smaller LMs, but only emerge as the model scales up (Kaplan , 2020). Recently, there have been several works (Xie , 2021; Min , 2022) aiming to understand how and why in-context learning works. Another concurrent work similar to ours is BINDER (Cheng , 2022), which applies Codex to synthesize 'soft' SQL queries to answer questions from tables."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 942
                },
                {
                    "x": 997,
                    "y": 942
                },
                {
                    "x": 997,
                    "y": 994
                },
                {
                    "x": 293,
                    "y": 994
                }
            ],
            "category": "paragraph",
            "html": "<p id='107' style='font-size:18px'>4.3 Chain of Reasoning with LLMs</p>",
            "id": 107,
            "page": 11,
            "text": "4.3 Chain of Reasoning with LLMs"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 1032
                },
                {
                    "x": 2259,
                    "y": 1032
                },
                {
                    "x": 2259,
                    "y": 1534
                },
                {
                    "x": 291,
                    "y": 1534
                }
            ],
            "category": "paragraph",
            "html": "<p id='108' style='font-size:16px'>Although LLMs have demonstrated remarkable success across a range of NLP tasks, their ability to reason<br>is often seen as a limitation. Recently, CoT (Wei et al., 2022; Kojima et al., 2022; Wang et al., 2022b)<br>was proposed to enable LLM's capability to perform reasoning tasks by demonstrating 'natural language<br>rationales'. Suzgun et al. (2022) have shown that CoT can already surpass human performance on challenging<br>BIG-Bench tasks. Later on, several other works (Drozdov et al., 2022; Zhou et al., 2022; Nye et al., 2021)<br>also propose different approaches to utilize LLMs to solve reasoning tasks by allowing intermediate steps.<br>ReAct Yao et al. (2022) propose to leverage external tools like search engine to enhance the LLM reasoning<br>skills. Our method can be seen as augmenting CoT with external tools (Python) to enable robust numerical<br>reasoning. Another contemporary work (Gao et al., 2022) was proposed at the same time as ours to adopt<br>hybrid text/code reasoning to address math questions.</p>",
            "id": 108,
            "page": 11,
            "text": "Although LLMs have demonstrated remarkable success across a range of NLP tasks, their ability to reason is often seen as a limitation. Recently, CoT (Wei , 2022; Kojima , 2022; Wang , 2022b) was proposed to enable LLM's capability to perform reasoning tasks by demonstrating 'natural language rationales'. Suzgun  (2022) have shown that CoT can already surpass human performance on challenging BIG-Bench tasks. Later on, several other works (Drozdov , 2022; Zhou , 2022; Nye , 2021) also propose different approaches to utilize LLMs to solve reasoning tasks by allowing intermediate steps. ReAct Yao  (2022) propose to leverage external tools like search engine to enhance the LLM reasoning skills. Our method can be seen as augmenting CoT with external tools (Python) to enable robust numerical reasoning. Another contemporary work (Gao , 2022) was proposed at the same time as ours to adopt hybrid text/code reasoning to address math questions."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 1591
                },
                {
                    "x": 1131,
                    "y": 1591
                },
                {
                    "x": 1131,
                    "y": 1644
                },
                {
                    "x": 292,
                    "y": 1644
                }
            ],
            "category": "paragraph",
            "html": "<p id='109' style='font-size:20px'>4.4 Discussion about Contemporary Work</p>",
            "id": 109,
            "page": 11,
            "text": "4.4 Discussion about Contemporary Work"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 1681
                },
                {
                    "x": 2258,
                    "y": 1681
                },
                {
                    "x": 2258,
                    "y": 1983
                },
                {
                    "x": 291,
                    "y": 1983
                }
            ],
            "category": "paragraph",
            "html": "<p id='110' style='font-size:18px'>Recently, there has been several follow-up work on top of PoT including self-critic (Gou et al., 2023), self-<br>eval (Xie et al., 2023), plan-and-solve (Wang et al., 2023a). These methods propose to enhance LLMs'<br>capabilities to solve math problems with PoT. self-critic (Gou et al., 2023) and self-eval (Xie et al., 2021)<br>both adopt self-evaluation to enhance the robustness of the generated program. plan-and-solve (Wang et al.,<br>2023a) instead adopt more detailed planning instruction to help LLMs create a high-level reasoning plan.<br>These methods all prove to bring decent improvements over PoT on different math reasoning datasets.</p>",
            "id": 110,
            "page": 11,
            "text": "Recently, there has been several follow-up work on top of PoT including self-critic (Gou , 2023), selfeval (Xie , 2023), plan-and-solve (Wang , 2023a). These methods propose to enhance LLMs' capabilities to solve math problems with PoT. self-critic (Gou , 2023) and self-eval (Xie , 2021) both adopt self-evaluation to enhance the robustness of the generated program. plan-and-solve (Wang , 2023a) instead adopt more detailed planning instruction to help LLMs create a high-level reasoning plan. These methods all prove to bring decent improvements over PoT on different math reasoning datasets."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 2004
                },
                {
                    "x": 2260,
                    "y": 2004
                },
                {
                    "x": 2260,
                    "y": 2257
                },
                {
                    "x": 289,
                    "y": 2257
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='111' style='font-size:16px'>Another line of work related to ours is Tool-use in transformer models (Schick et al., 2023; Paranjape et al.,<br>2023). These work propose to adopt different tools to help the language models ground on external world.<br>These work generalizes our Python program into more general API calls to include search engine, string<br>extraction, etc. By generalization, LLMs can unlock its capabilities to solve more complex reasoning and<br>grounding problems in real-world scenarios.</p>",
            "id": 111,
            "page": 11,
            "text": "Another line of work related to ours is Tool-use in transformer models (Schick , 2023; Paranjape , 2023). These work propose to adopt different tools to help the language models ground on external world. These work generalizes our Python program into more general API calls to include search engine, string extraction, etc. By generalization, LLMs can unlock its capabilities to solve more complex reasoning and grounding problems in real-world scenarios."
        },
        {
            "bounding_box": [
                {
                    "x": 294,
                    "y": 2320
                },
                {
                    "x": 623,
                    "y": 2320
                },
                {
                    "x": 623,
                    "y": 2376
                },
                {
                    "x": 294,
                    "y": 2376
                }
            ],
            "category": "paragraph",
            "html": "<p id='112' style='font-size:22px'>5 Discussion</p>",
            "id": 112,
            "page": 11,
            "text": "5 Discussion"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 2428
                },
                {
                    "x": 2259,
                    "y": 2428
                },
                {
                    "x": 2259,
                    "y": 2682
                },
                {
                    "x": 291,
                    "y": 2682
                }
            ],
            "category": "paragraph",
            "html": "<p id='113' style='font-size:14px'>In this work, we have verified that our prompting methods can work efficiently on numerical reasoning tasks<br>like math or finance problem solving. We also study how to combine PoT with CoT to combine the merits of<br>both prompting approaches. We believe PoT is suitable for problems which require highly symbolic reasoning<br>skills. For semantic reasoning tasks like commonsense reasoning (StrategyQA), we conjecture that PoT is<br>not the best option. In contrast, CoT can solve more broader reasoning tasks.</p>",
            "id": 113,
            "page": 11,
            "text": "In this work, we have verified that our prompting methods can work efficiently on numerical reasoning tasks like math or finance problem solving. We also study how to combine PoT with CoT to combine the merits of both prompting approaches. We believe PoT is suitable for problems which require highly symbolic reasoning skills. For semantic reasoning tasks like commonsense reasoning (StrategyQA), we conjecture that PoT is not the best option. In contrast, CoT can solve more broader reasoning tasks."
        },
        {
            "bounding_box": [
                {
                    "x": 294,
                    "y": 2744
                },
                {
                    "x": 654,
                    "y": 2744
                },
                {
                    "x": 654,
                    "y": 2801
                },
                {
                    "x": 294,
                    "y": 2801
                }
            ],
            "category": "paragraph",
            "html": "<p id='114' style='font-size:22px'>6 Conclusions</p>",
            "id": 114,
            "page": 11,
            "text": "6 Conclusions"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 2853
                },
                {
                    "x": 2260,
                    "y": 2853
                },
                {
                    "x": 2260,
                    "y": 3056
                },
                {
                    "x": 291,
                    "y": 3056
                }
            ],
            "category": "paragraph",
            "html": "<p id='115' style='font-size:16px'>In this work, we investigate how to disentangle computation from reasoning in solving numerical problems.<br>By 'program of thoughts' prompting, we are able to elicit LLMs' abilities to generate accurate programs to<br>express complex reasoning procedure, while also allows computation to be separately handled by an external<br>program interpreter. This approach is able to boost the performance of LLMs on several math datasets</p>",
            "id": 115,
            "page": 11,
            "text": "In this work, we investigate how to disentangle computation from reasoning in solving numerical problems. By 'program of thoughts' prompting, we are able to elicit LLMs' abilities to generate accurate programs to express complex reasoning procedure, while also allows computation to be separately handled by an external program interpreter. This approach is able to boost the performance of LLMs on several math datasets"
        },
        {
            "bounding_box": [
                {
                    "x": 1252,
                    "y": 3132
                },
                {
                    "x": 1296,
                    "y": 3132
                },
                {
                    "x": 1296,
                    "y": 3172
                },
                {
                    "x": 1252,
                    "y": 3172
                }
            ],
            "category": "footer",
            "html": "<footer id='116' style='font-size:14px'>11</footer>",
            "id": 116,
            "page": 11,
            "text": "11"
        },
        {
            "bounding_box": [
                {
                    "x": 296,
                    "y": 111
                },
                {
                    "x": 1538,
                    "y": 111
                },
                {
                    "x": 1538,
                    "y": 157
                },
                {
                    "x": 296,
                    "y": 157
                }
            ],
            "category": "header",
            "html": "<header id='117' style='font-size:16px'>Published in Transactions on Machine Learning Research (10/2023)</header>",
            "id": 117,
            "page": 12,
            "text": "Published in Transactions on Machine Learning Research (10/2023)"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 346
                },
                {
                    "x": 2257,
                    "y": 346
                },
                {
                    "x": 2257,
                    "y": 445
                },
                {
                    "x": 291,
                    "y": 445
                }
            ],
            "category": "paragraph",
            "html": "<p id='118' style='font-size:14px'>significantly. We believe our work can inspire more work to combine symbolic execution with LLMs to<br>achieve better performance on other symbolic reasoning tasks.</p>",
            "id": 118,
            "page": 12,
            "text": "significantly. We believe our work can inspire more work to combine symbolic execution with LLMs to achieve better performance on other symbolic reasoning tasks."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 509
                },
                {
                    "x": 559,
                    "y": 509
                },
                {
                    "x": 559,
                    "y": 562
                },
                {
                    "x": 293,
                    "y": 562
                }
            ],
            "category": "paragraph",
            "html": "<p id='119' style='font-size:22px'>Limitations</p>",
            "id": 119,
            "page": 12,
            "text": "Limitations"
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 614
                },
                {
                    "x": 2260,
                    "y": 614
                },
                {
                    "x": 2260,
                    "y": 1065
                },
                {
                    "x": 290,
                    "y": 1065
                }
            ],
            "category": "paragraph",
            "html": "<p id='120' style='font-size:16px'>Our work aims at combining LLM with symbolic execution to solve challenging math problems. PoT would<br>require execution of 'generated code' from LLMs, which could contain certain dangerous or risky code<br>snippets like 'import OS; os.rmdir()', etc. We have blocked the LLM from importing any additional modules<br>and restrict it to using the pre-defined modules. Such brutal-force blocking works reasonable for math QA,<br>however, for other unknown symbolic tasks, it might hurt the PoT's generalization. Another limitation is<br>that PoT still struggles with AQuA dataset with complex algebraic questions with only 58% accuracy. It's<br>mainly due to the diversity questions in AQuA, which the demonstration cannot possibly cover. Therefore,<br>the future research should discuss how to further prompt LLMs to generate code for highly diversified Math<br>questions.</p>",
            "id": 120,
            "page": 12,
            "text": "Our work aims at combining LLM with symbolic execution to solve challenging math problems. PoT would require execution of 'generated code' from LLMs, which could contain certain dangerous or risky code snippets like 'import OS; os.rmdir()', etc. We have blocked the LLM from importing any additional modules and restrict it to using the pre-defined modules. Such brutal-force blocking works reasonable for math QA, however, for other unknown symbolic tasks, it might hurt the PoT's generalization. Another limitation is that PoT still struggles with AQuA dataset with complex algebraic questions with only 58% accuracy. It's mainly due to the diversity questions in AQuA, which the demonstration cannot possibly cover. Therefore, the future research should discuss how to further prompt LLMs to generate code for highly diversified Math questions."
        },
        {
            "bounding_box": [
                {
                    "x": 294,
                    "y": 1126
                },
                {
                    "x": 551,
                    "y": 1126
                },
                {
                    "x": 551,
                    "y": 1182
                },
                {
                    "x": 294,
                    "y": 1182
                }
            ],
            "category": "paragraph",
            "html": "<p id='121' style='font-size:20px'>References</p>",
            "id": 121,
            "page": 12,
            "text": "References"
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 1208
                },
                {
                    "x": 2260,
                    "y": 1208
                },
                {
                    "x": 2260,
                    "y": 1409
                },
                {
                    "x": 293,
                    "y": 1409
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='122' style='font-size:18px'>Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi.<br>Mathqa: Towards interpretable math word problem solving with operation-based formalisms. In Pro-<br>ceedings of the 2019 Conference of the North American Chapter of the Association for Computational<br>Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 2357-2367, 2019.</p>",
            "id": 122,
            "page": 12,
            "text": "Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. Mathqa: Towards interpretable math word problem solving with operation-based formalisms. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 2357-2367, 2019."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 1442
                },
                {
                    "x": 2259,
                    "y": 1442
                },
                {
                    "x": 2259,
                    "y": 1590
                },
                {
                    "x": 293,
                    "y": 1590
                }
            ],
            "category": "paragraph",
            "html": "<p id='123' style='font-size:20px'>Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen<br>Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv<br>preprint arXiv:2108.07732, 2021.</p>",
            "id": 123,
            "page": 12,
            "text": "Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le,  Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021."
        },
        {
            "bounding_box": [
                {
                    "x": 294,
                    "y": 1624
                },
                {
                    "x": 2256,
                    "y": 1624
                },
                {
                    "x": 2256,
                    "y": 1773
                },
                {
                    "x": 294,
                    "y": 1773
                }
            ],
            "category": "paragraph",
            "html": "<p id='124' style='font-size:16px'>Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind<br>Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.<br>Advances in neural information processing systems, 33:1877-1901, 2020.</p>",
            "id": 124,
            "page": 12,
            "text": "Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,  Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 1808
                },
                {
                    "x": 2257,
                    "y": 1808
                },
                {
                    "x": 2257,
                    "y": 1957
                },
                {
                    "x": 293,
                    "y": 1957
                }
            ],
            "category": "paragraph",
            "html": "<p id='125' style='font-size:18px'>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan,<br>Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models<br>trained on code. arXiv preprint arXiv:2107.03374, 2021a.</p>",
            "id": 125,
            "page": 12,
            "text": "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman,  Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021a."
        },
        {
            "bounding_box": [
                {
                    "x": 294,
                    "y": 1989
                },
                {
                    "x": 2256,
                    "y": 1989
                },
                {
                    "x": 2256,
                    "y": 2087
                },
                {
                    "x": 294,
                    "y": 2087
                }
            ],
            "category": "paragraph",
            "html": "<p id='126' style='font-size:16px'>Wenhu Chen. Large language models are few (1)-shot table reasoners. arXiv preprint arXiv:2210.06710,<br>2022.</p>",
            "id": 126,
            "page": 12,
            "text": "Wenhu Chen. Large language models are few (1)-shot table reasoners. arXiv preprint arXiv:2210.06710, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 2122
                },
                {
                    "x": 2259,
                    "y": 2122
                },
                {
                    "x": 2259,
                    "y": 2322
                },
                {
                    "x": 292,
                    "y": 2322
                }
            ],
            "category": "paragraph",
            "html": "<p id='127' style='font-size:18px'>Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa,<br>Matt Beane, Ting-Hao Huang, Bryan R Routledge, et al. Finqa: A dataset of numerical reasoning<br>over financial data. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language<br>Processing, pp. 3697-3711, 2021b.</p>",
            "id": 127,
            "page": 12,
            "text": "Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan R Routledge,  Finqa: A dataset of numerical reasoning over financial data. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 3697-3711, 2021b."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 2355
                },
                {
                    "x": 2258,
                    "y": 2355
                },
                {
                    "x": 2258,
                    "y": 2504
                },
                {
                    "x": 293,
                    "y": 2504
                }
            ],
            "category": "paragraph",
            "html": "<p id='128' style='font-size:20px'>Zhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma, Sameena Shah, and William Yang Wang. Convfinqa:<br>Exploring the chain of numerical reasoning in conversational finance question answering. arXiv preprint<br>arXiv:2210.03849, 2022.</p>",
            "id": 128,
            "page": 12,
            "text": "Zhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma, Sameena Shah, and William Yang Wang. Convfinqa: Exploring the chain of numerical reasoning in conversational finance question answering. arXiv preprint arXiv:2210.03849, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 2538
                },
                {
                    "x": 2258,
                    "y": 2538
                },
                {
                    "x": 2258,
                    "y": 2688
                },
                {
                    "x": 293,
                    "y": 2688
                }
            ],
            "category": "paragraph",
            "html": "<p id='129' style='font-size:20px'>Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir<br>Radev, Mari Ostendorf, Luke Zettlemoyer, et al. Binding language models in symbolic languages. arXiv<br>preprint arXiv:2210.02875, 2022.</p>",
            "id": 129,
            "page": 12,
            "text": "Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer,  Binding language models in symbolic languages. arXiv preprint arXiv:2210.02875, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 295,
                    "y": 2722
                },
                {
                    "x": 2256,
                    "y": 2722
                },
                {
                    "x": 2256,
                    "y": 2871
                },
                {
                    "x": 295,
                    "y": 2871
                }
            ],
            "category": "paragraph",
            "html": "<p id='130' style='font-size:20px'>Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,<br>Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language<br>modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.</p>",
            "id": 130,
            "page": 12,
            "text": "Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann,  Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 294,
                    "y": 2903
                },
                {
                    "x": 2257,
                    "y": 2903
                },
                {
                    "x": 2257,
                    "y": 3052
                },
                {
                    "x": 294,
                    "y": 3052
                }
            ],
            "category": "paragraph",
            "html": "<p id='131' style='font-size:18px'>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse,<br>and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168,<br>2021.</p>",
            "id": 131,
            "page": 12,
            "text": "Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021."
        },
        {
            "bounding_box": [
                {
                    "x": 1252,
                    "y": 3133
                },
                {
                    "x": 1299,
                    "y": 3133
                },
                {
                    "x": 1299,
                    "y": 3172
                },
                {
                    "x": 1252,
                    "y": 3172
                }
            ],
            "category": "footer",
            "html": "<footer id='132' style='font-size:14px'>12</footer>",
            "id": 132,
            "page": 12,
            "text": "12"
        },
        {
            "bounding_box": [
                {
                    "x": 296,
                    "y": 111
                },
                {
                    "x": 1538,
                    "y": 111
                },
                {
                    "x": 1538,
                    "y": 157
                },
                {
                    "x": 296,
                    "y": 157
                }
            ],
            "category": "header",
            "html": "<header id='133' style='font-size:14px'>Published in Transactions on Machine Learning Research (10/2023)</header>",
            "id": 133,
            "page": 13,
            "text": "Published in Transactions on Machine Learning Research (10/2023)"
        },
        {
            "bounding_box": [
                {
                    "x": 294,
                    "y": 346
                },
                {
                    "x": 2258,
                    "y": 346
                },
                {
                    "x": 2258,
                    "y": 493
                },
                {
                    "x": 294,
                    "y": 493
                }
            ],
            "category": "paragraph",
            "html": "<p id='134' style='font-size:20px'>Andrew Drozdov, Nathanael Scharli, Ekin Akyurek, Nathan Scales, Xinying Song, Xinyun Chen, Olivier<br>Bousquet, and Denny Zhou. Compositional semantic parsing with large language models. arXiv preprint<br>arXiv:2209.15003, 2022.</p>",
            "id": 134,
            "page": 13,
            "text": "Andrew Drozdov, Nathanael Scharli, Ekin Akyurek, Nathan Scales, Xinying Song, Xinyun Chen, Olivier Bousquet, and Denny Zhou. Compositional semantic parsing with large language models. arXiv preprint arXiv:2209.15003, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 534
                },
                {
                    "x": 2257,
                    "y": 534
                },
                {
                    "x": 2257,
                    "y": 684
                },
                {
                    "x": 293,
                    "y": 684
                }
            ],
            "category": "paragraph",
            "html": "<p id='135' style='font-size:18px'>Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun,<br>Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language models with mixture-<br>of-experts. In International Conference on Machine Learning, pp. 5547-5569. PMLR, 2022.</p>",
            "id": 135,
            "page": 13,
            "text": "Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat,  Glam: Efficient scaling of language models with mixtureof-experts. In International Conference on Machine Learning, pp. 5547-5569. PMLR, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 294,
                    "y": 723
                },
                {
                    "x": 2259,
                    "y": 723
                },
                {
                    "x": 2259,
                    "y": 925
                },
                {
                    "x": 294,
                    "y": 925
                }
            ],
            "category": "paragraph",
            "html": "<p id='136' style='font-size:20px'>Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. Drop:<br>A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proceedings of the<br>2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human<br>Language Technologies, Volume 1 (Long and Short Papers), pp. 2368-2378, 2019.</p>",
            "id": 136,
            "page": 13,
            "text": "Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 2368-2378, 2019."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 965
                },
                {
                    "x": 2256,
                    "y": 965
                },
                {
                    "x": 2256,
                    "y": 1065
                },
                {
                    "x": 293,
                    "y": 1065
                }
            ],
            "category": "paragraph",
            "html": "<p id='137' style='font-size:22px'>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham<br>Neubig. Pal: Program-aided language models. arXiv preprint arXiu:2211.10435, 2022.</p>",
            "id": 137,
            "page": 13,
            "text": "Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. arXiv preprint arXiu:2211.10435, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 1104
                },
                {
                    "x": 2256,
                    "y": 1104
                },
                {
                    "x": 2256,
                    "y": 1252
                },
                {
                    "x": 292,
                    "y": 1252
                }
            ],
            "category": "paragraph",
            "html": "<p id='138' style='font-size:20px'>Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. Critic:<br>Large language models can self-correct with tool-interactive critiquing. arXiv preprint arXiv:2305.11738,<br>2023.</p>",
            "id": 138,
            "page": 13,
            "text": "Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. Critic: Large language models can self-correct with tool-interactive critiquing. arXiv preprint arXiv:2305.11738, 2023."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 1294
                },
                {
                    "x": 2259,
                    "y": 1294
                },
                {
                    "x": 2259,
                    "y": 1443
                },
                {
                    "x": 293,
                    "y": 1443
                }
            ],
            "category": "paragraph",
            "html": "<p id='139' style='font-size:18px'>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and<br>Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint<br>arXiv:2103.03874, 2021.</p>",
            "id": 139,
            "page": 13,
            "text": "Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021."
        },
        {
            "bounding_box": [
                {
                    "x": 294,
                    "y": 1484
                },
                {
                    "x": 2257,
                    "y": 1484
                },
                {
                    "x": 2257,
                    "y": 1585
                },
                {
                    "x": 294,
                    "y": 1585
                }
            ],
            "category": "paragraph",
            "html": "<p id='140' style='font-size:14px'>Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. Learning to solve<br>arithmetic word problems with verb categorization. In EMNLP, pp. 523-533, 2014.</p>",
            "id": 140,
            "page": 13,
            "text": "Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. Learning to solve arithmetic word problems with verb categorization. In EMNLP, pp. 523-533, 2014."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 1624
                },
                {
                    "x": 2257,
                    "y": 1624
                },
                {
                    "x": 2257,
                    "y": 1773
                },
                {
                    "x": 293,
                    "y": 1773
                }
            ],
            "category": "paragraph",
            "html": "<p id='141' style='font-size:18px'>Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray,<br>Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint<br>arXiv:2001.08361, 2020.</p>",
            "id": 141,
            "page": 13,
            "text": "Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 1814
                },
                {
                    "x": 2256,
                    "y": 1814
                },
                {
                    "x": 2256,
                    "y": 1914
                },
                {
                    "x": 293,
                    "y": 1914
                }
            ],
            "category": "paragraph",
            "html": "<p id='142' style='font-size:16px'>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language<br>models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.</p>",
            "id": 142,
            "page": 13,
            "text": "Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 1953
                },
                {
                    "x": 2257,
                    "y": 1953
                },
                {
                    "x": 2257,
                    "y": 2101
                },
                {
                    "x": 293,
                    "y": 2101
                }
            ],
            "category": "paragraph",
            "html": "<p id='143' style='font-size:20px'>Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena Dumas Ang. Pars-<br>ing algebraic word problems into equations. Transactions of the Association for Computational Linguistics,<br>3:585-597, 2015.</p>",
            "id": 143,
            "page": 13,
            "text": "Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena Dumas Ang. Parsing algebraic word problems into equations. Transactions of the Association for Computational Linguistics, 3:585-597, 2015."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 2144
                },
                {
                    "x": 2257,
                    "y": 2144
                },
                {
                    "x": 2257,
                    "y": 2294
                },
                {
                    "x": 293,
                    "y": 2294
                }
            ],
            "category": "paragraph",
            "html": "<p id='144' style='font-size:16px'>Fangyu Lei, Shizhu He, Xiang Li, Jun Zhao, and Kang Liu. Answering numerical reasoning questions in<br>table-text hybrid contents with graph-based encoder and tree-based decoder. In Proceedings of the 29th<br>International Conference on Computational Linguistics, pp. 1379-1390, 2022.</p>",
            "id": 144,
            "page": 13,
            "text": "Fangyu Lei, Shizhu He, Xiang Li, Jun Zhao, and Kang Liu. Answering numerical reasoning questions in table-text hybrid contents with graph-based encoder and tree-based decoder. In Proceedings of the 29th International Conference on Computational Linguistics, pp. 1379-1390, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 2333
                },
                {
                    "x": 2258,
                    "y": 2333
                },
                {
                    "x": 2258,
                    "y": 2483
                },
                {
                    "x": 292,
                    "y": 2483
                }
            ],
            "category": "paragraph",
            "html": "<p id='145' style='font-size:18px'>Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation:<br>Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the<br>Association for Computational Linguistics (Volume 1: Long Papers), pp. 158-167, 2017.</p>",
            "id": 145,
            "page": 13,
            "text": "Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 158-167, 2017."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 2523
                },
                {
                    "x": 2257,
                    "y": 2523
                },
                {
                    "x": 2257,
                    "y": 2672
                },
                {
                    "x": 293,
                    "y": 2672
                }
            ],
            "category": "paragraph",
            "html": "<p id='146' style='font-size:20px'>Pan Lu, Liang Qiu, Kai- Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and<br>Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning.<br>arXiv preprint arXiv:2209.14610, 2022.</p>",
            "id": 146,
            "page": 13,
            "text": "Pan Lu, Liang Qiu, Kai- Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning. arXiv preprint arXiv:2209.14610, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 2714
                },
                {
                    "x": 2257,
                    "y": 2714
                },
                {
                    "x": 2257,
                    "y": 2863
                },
                {
                    "x": 293,
                    "y": 2863
                }
            ],
            "category": "paragraph",
            "html": "<p id='147' style='font-size:16px'>Shen- Yun Miao, Chao- Chun Liang, and Keh- Yih Su. A diverse corpus for evaluating and developing english<br>math word problem solvers. In Proceedings of the 58th Annual Meeting of the Association for Computa-<br>tional Linguistics, pp. 975-984, 2020.</p>",
            "id": 147,
            "page": 13,
            "text": "Shen- Yun Miao, Chao- Chun Liang, and Keh- Yih Su. A diverse corpus for evaluating and developing english math word problem solvers. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 975-984, 2020."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 2903
                },
                {
                    "x": 2259,
                    "y": 2903
                },
                {
                    "x": 2259,
                    "y": 3053
                },
                {
                    "x": 293,
                    "y": 3053
                }
            ],
            "category": "paragraph",
            "html": "<p id='148' style='font-size:14px'>Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettle-<br>moyer. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint<br>arXiv:2202.12837, 2022.</p>",
            "id": 148,
            "page": 13,
            "text": "Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? arXiv preprint arXiv:2202.12837, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 1252,
                    "y": 3133
                },
                {
                    "x": 1299,
                    "y": 3133
                },
                {
                    "x": 1299,
                    "y": 3172
                },
                {
                    "x": 1252,
                    "y": 3172
                }
            ],
            "category": "footer",
            "html": "<footer id='149' style='font-size:14px'>13</footer>",
            "id": 149,
            "page": 13,
            "text": "13"
        },
        {
            "bounding_box": [
                {
                    "x": 296,
                    "y": 111
                },
                {
                    "x": 1538,
                    "y": 111
                },
                {
                    "x": 1538,
                    "y": 157
                },
                {
                    "x": 296,
                    "y": 157
                }
            ],
            "category": "header",
            "html": "<header id='150' style='font-size:16px'>Published in Transactions on Machine Learning Research (10/2023)</header>",
            "id": 150,
            "page": 14,
            "text": "Published in Transactions on Machine Learning Research (10/2023)"
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 345
                },
                {
                    "x": 2258,
                    "y": 345
                },
                {
                    "x": 2258,
                    "y": 545
                },
                {
                    "x": 293,
                    "y": 545
                }
            ],
            "category": "paragraph",
            "html": "<p id='151' style='font-size:20px'>Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpuro-<br>hit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, and Ashwin Kalyan. Lila: A unified benchmark for<br>mathematical reasoning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language<br>Processing (EMNLP), 2022.</p>",
            "id": 151,
            "page": 14,
            "text": "Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, and Ashwin Kalyan. Lila: A unified benchmark for mathematical reasoning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 294,
                    "y": 582
                },
                {
                    "x": 2258,
                    "y": 582
                },
                {
                    "x": 2258,
                    "y": 729
                },
                {
                    "x": 294,
                    "y": 729
                }
            ],
            "category": "paragraph",
            "html": "<p id='152' style='font-size:22px'>Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming<br>Xiong. Codegen: An open large language model for code with multi-turn program synthesis. arXiv preprint<br>arXiv:2203.13474, 2022.</p>",
            "id": 152,
            "page": 14,
            "text": "Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. Codegen: An open large language model for code with multi-turn program synthesis. arXiv preprint arXiv:2203.13474, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 767
                },
                {
                    "x": 2258,
                    "y": 767
                },
                {
                    "x": 2258,
                    "y": 917
                },
                {
                    "x": 293,
                    "y": 917
                }
            ],
            "category": "paragraph",
            "html": "<p id='153' style='font-size:20px'>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber,<br>David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for<br>intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021.</p>",
            "id": 153,
            "page": 14,
            "text": "Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan,  Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 955
                },
                {
                    "x": 1625,
                    "y": 955
                },
                {
                    "x": 1625,
                    "y": 1003
                },
                {
                    "x": 293,
                    "y": 1003
                }
            ],
            "category": "paragraph",
            "html": "<p id='154' style='font-size:20px'>OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.</p>",
            "id": 154,
            "page": 14,
            "text": "OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 1040
                },
                {
                    "x": 2259,
                    "y": 1040
                },
                {
                    "x": 2259,
                    "y": 1189
                },
                {
                    "x": 293,
                    "y": 1189
                }
            ],
            "category": "paragraph",
            "html": "<p id='155' style='font-size:20px'>Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, and<br>Marco Tulio Ribeiro. Art: Automatic multi-step reasoning and tool-use for large language models. arXiv<br>preprint arXiv:2303.09014, 2023.</p>",
            "id": 155,
            "page": 14,
            "text": "Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, and Marco Tulio Ribeiro. Art: Automatic multi-step reasoning and tool-use for large language models. arXiv preprint arXiv:2303.09014, 2023."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 1225
                },
                {
                    "x": 2259,
                    "y": 1225
                },
                {
                    "x": 2259,
                    "y": 1476
                },
                {
                    "x": 293,
                    "y": 1476
                }
            ],
            "category": "paragraph",
            "html": "<p id='156' style='font-size:20px'>Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple math word<br>problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for<br>Computational Linguistics: Human Language Technologies, pp. 2080-2094, Online, June 2021. Association<br>for Computational Linguistics. doi: 10.18663/y1/2021.naad-main.168. URL https : / /aclanthology.<br>org/2021 . naacl-main. 168.</p>",
            "id": 156,
            "page": 14,
            "text": "Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 2080-2094, Online, June 2021. Association for Computational Linguistics. doi: 10.18663/y1/2021.naad-main.168. URL https : / /aclanthology. org/2021 . naacl-main. 168."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 1511
                },
                {
                    "x": 2258,
                    "y": 1511
                },
                {
                    "x": 2258,
                    "y": 1662
                },
                {
                    "x": 291,
                    "y": 1662
                }
            ],
            "category": "paragraph",
            "html": "<p id='157' style='font-size:20px'>Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John<br>Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods,<br>analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021.</p>",
            "id": 157,
            "page": 14,
            "text": "Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young,  Scaling language models: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 1699
                },
                {
                    "x": 2257,
                    "y": 1699
                },
                {
                    "x": 2257,
                    "y": 1799
                },
                {
                    "x": 292,
                    "y": 1799
                }
            ],
            "category": "paragraph",
            "html": "<p id='158' style='font-size:18px'>Subhro Roy and Dan Roth. Solving general arithmetic word problems. In Proceedings of the 2015 Conference<br>on Empirical Methods in Natural Language Processing, pp. 1743-1752, 2015.</p>",
            "id": 158,
            "page": 14,
            "text": "Subhro Roy and Dan Roth. Solving general arithmetic word problems. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 1743-1752, 2015."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 1836
                },
                {
                    "x": 2260,
                    "y": 1836
                },
                {
                    "x": 2260,
                    "y": 1935
                },
                {
                    "x": 292,
                    "y": 1935
                }
            ],
            "category": "paragraph",
            "html": "<p id='159' style='font-size:20px'>Subhro Roy and Dan Roth. Mapping to declarative knowledge for word problem solving. Transactions of<br>the Association for Computational Linguistics, 6:159-172, 2018.</p>",
            "id": 159,
            "page": 14,
            "text": "Subhro Roy and Dan Roth. Mapping to declarative knowledge for word problem solving. Transactions of the Association for Computational Linguistics, 6:159-172, 2018."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 1972
                },
                {
                    "x": 2257,
                    "y": 1972
                },
                {
                    "x": 2257,
                    "y": 2072
                },
                {
                    "x": 292,
                    "y": 2072
                }
            ],
            "category": "paragraph",
            "html": "<p id='160' style='font-size:18px'>David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical reasoning<br>abilities of neural models. arXiv preprint arXiv:1904.01557, 2019.</p>",
            "id": 160,
            "page": 14,
            "text": "David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical reasoning abilities of neural models. arXiv preprint arXiv:1904.01557, 2019."
        },
        {
            "bounding_box": [
                {
                    "x": 294,
                    "y": 2108
                },
                {
                    "x": 2260,
                    "y": 2108
                },
                {
                    "x": 2260,
                    "y": 2257
                },
                {
                    "x": 294,
                    "y": 2257
                }
            ],
            "category": "paragraph",
            "html": "<p id='161' style='font-size:16px'>Timo Schick, Jane Dwivedi- Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola<br>Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv<br>preprint arXiv:2302.04 761, 2023.</p>",
            "id": 161,
            "page": 14,
            "text": "Timo Schick, Jane Dwivedi- Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04 761, 2023."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 2295
                },
                {
                    "x": 2258,
                    "y": 2295
                },
                {
                    "x": 2258,
                    "y": 2493
                },
                {
                    "x": 293,
                    "y": 2493
                }
            ],
            "category": "paragraph",
            "html": "<p id='162' style='font-size:22px'>Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper,<br>Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Using deepspeed and<br>megatron to train megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint<br>arXiv:2201.11990, 2022.</p>",
            "id": 162,
            "page": 14,
            "text": "Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti,  Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 2530
                },
                {
                    "x": 2257,
                    "y": 2530
                },
                {
                    "x": 2257,
                    "y": 2680
                },
                {
                    "x": 292,
                    "y": 2680
                }
            ],
            "category": "paragraph",
            "html": "<p id='163' style='font-size:20px'>Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung,<br>Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks and<br>whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.</p>",
            "id": 163,
            "page": 14,
            "text": "Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou,  Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 294,
                    "y": 2718
                },
                {
                    "x": 2257,
                    "y": 2718
                },
                {
                    "x": 2257,
                    "y": 2867
                },
                {
                    "x": 294,
                    "y": 2867
                }
            ],
            "category": "paragraph",
            "html": "<p id='164' style='font-size:20px'>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,<br>Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. arXiv<br>preprint arXiv:2201.08239, 2022.</p>",
            "id": 164,
            "page": 14,
            "text": "Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,  Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 294,
                    "y": 2902
                },
                {
                    "x": 2258,
                    "y": 2902
                },
                {
                    "x": 2258,
                    "y": 3054
                },
                {
                    "x": 294,
                    "y": 3054
                }
            ],
            "category": "paragraph",
            "html": "<p id='165' style='font-size:18px'>Bin Wang, Jiangzhou Ju, Yunlin Mao, Xin- Yu Dai, Shujian Huang, and Jiajun Chen. A numerical reasoning<br>question answering system with fine-grained retriever and the ensemble of multiple generators for finqa.<br>arXiv preprint arXiv:2206.08506, 2022a.</p>",
            "id": 165,
            "page": 14,
            "text": "Bin Wang, Jiangzhou Ju, Yunlin Mao, Xin- Yu Dai, Shujian Huang, and Jiajun Chen. A numerical reasoning question answering system with fine-grained retriever and the ensemble of multiple generators for finqa. arXiv preprint arXiv:2206.08506, 2022a."
        },
        {
            "bounding_box": [
                {
                    "x": 1252,
                    "y": 3133
                },
                {
                    "x": 1299,
                    "y": 3133
                },
                {
                    "x": 1299,
                    "y": 3172
                },
                {
                    "x": 1252,
                    "y": 3172
                }
            ],
            "category": "footer",
            "html": "<footer id='166' style='font-size:14px'>14</footer>",
            "id": 166,
            "page": 14,
            "text": "14"
        },
        {
            "bounding_box": [
                {
                    "x": 295,
                    "y": 110
                },
                {
                    "x": 1539,
                    "y": 110
                },
                {
                    "x": 1539,
                    "y": 158
                },
                {
                    "x": 295,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='167' style='font-size:16px'>Published in Transactions on Machine Learning Research (10/2023)</header>",
            "id": 167,
            "page": 15,
            "text": "Published in Transactions on Machine Learning Research (10/2023)"
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 342
                },
                {
                    "x": 2260,
                    "y": 342
                },
                {
                    "x": 2260,
                    "y": 496
                },
                {
                    "x": 292,
                    "y": 496
                }
            ],
            "category": "paragraph",
            "html": "<p id='168' style='font-size:20px'>Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka- Wei Lee, and Ee-Peng Lim. Plan-<br>and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. arXiv<br>preprint arXiv:2305.04091, 2023a.</p>",
            "id": 168,
            "page": 15,
            "text": "Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka- Wei Lee, and Ee-Peng Lim. Planand-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. arXiv preprint arXiv:2305.04091, 2023a."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 527
                },
                {
                    "x": 2258,
                    "y": 527
                },
                {
                    "x": 2258,
                    "y": 628
                },
                {
                    "x": 293,
                    "y": 628
                }
            ],
            "category": "paragraph",
            "html": "<p id='169' style='font-size:20px'>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistency improves<br>chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022b.</p>",
            "id": 169,
            "page": 15,
            "text": "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022b."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 658
                },
                {
                    "x": 2258,
                    "y": 658
                },
                {
                    "x": 2258,
                    "y": 808
                },
                {
                    "x": 293,
                    "y": 808
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='170' style='font-size:20px'>Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi DQ Bui, Junnan Li, and Steven CH Hoi. Codet5+:<br>Open code large language models for code understanding and generation. arXiv preprint arXiv:2305.07922,<br>2023b.</p>",
            "id": 170,
            "page": 15,
            "text": "Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi DQ Bui, Junnan Li, and Steven CH Hoi. Codet5+: Open code large language models for code understanding and generation. arXiv preprint arXiv:2305.07922, 2023b."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 843
                },
                {
                    "x": 2257,
                    "y": 843
                },
                {
                    "x": 2257,
                    "y": 944
                },
                {
                    "x": 293,
                    "y": 944
                }
            ],
            "category": "paragraph",
            "html": "<p id='171' style='font-size:18px'>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of<br>thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.</p>",
            "id": 171,
            "page": 15,
            "text": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 976
                },
                {
                    "x": 2259,
                    "y": 976
                },
                {
                    "x": 2259,
                    "y": 1076
                },
                {
                    "x": 292,
                    "y": 1076
                }
            ],
            "category": "paragraph",
            "html": "<p id='172' style='font-size:18px'>Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning<br>as implicit bayesian inference. In International Conference on Learning Representations, 2021.</p>",
            "id": 172,
            "page": 15,
            "text": "Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-context learning as implicit bayesian inference. In International Conference on Learning Representations, 2021."
        },
        {
            "bounding_box": [
                {
                    "x": 294,
                    "y": 1107
                },
                {
                    "x": 2257,
                    "y": 1107
                },
                {
                    "x": 2257,
                    "y": 1208
                },
                {
                    "x": 294,
                    "y": 1208
                }
            ],
            "category": "paragraph",
            "html": "<p id='173' style='font-size:18px'>Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min- Yen Kan, Junxian He, and Qizhe Xie. Decomposition<br>enhances reasoning via self-evaluation guided decoding. arXiv preprint arXiv:2305.00633, 2023.</p>",
            "id": 173,
            "page": 15,
            "text": "Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min- Yen Kan, Junxian He, and Qizhe Xie. Decomposition enhances reasoning via self-evaluation guided decoding. arXiv preprint arXiv:2305.00633, 2023."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 1241
                },
                {
                    "x": 2259,
                    "y": 1241
                },
                {
                    "x": 2259,
                    "y": 1391
                },
                {
                    "x": 293,
                    "y": 1391
                }
            ],
            "category": "paragraph",
            "html": "<p id='174' style='font-size:18px'>Shunyu Yao, Jeffrey Zhao, Dian Yu, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. React: Syner-<br>gizing reasoning and acting in language models. In NeurIPS 2022 Foundation Models for Decision Making<br>Workshop, 2022.</p>",
            "id": 174,
            "page": 15,
            "text": "Shunyu Yao, Jeffrey Zhao, Dian Yu, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In NeurIPS 2022 Foundation Models for Decision Making Workshop, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 1422
                },
                {
                    "x": 2259,
                    "y": 1422
                },
                {
                    "x": 2259,
                    "y": 1575
                },
                {
                    "x": 293,
                    "y": 1575
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='175' style='font-size:20px'>Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier<br>Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language<br>models. arXiv preprint arXiv:2205.10625, 2022.</p>",
            "id": 175,
            "page": 15,
            "text": "Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 1606
                },
                {
                    "x": 2261,
                    "y": 1606
                },
                {
                    "x": 2261,
                    "y": 1857
                },
                {
                    "x": 293,
                    "y": 1857
                }
            ],
            "category": "paragraph",
            "html": "<p id='176' style='font-size:18px'>Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and<br>Tat-Seng Chua. Tat-qa: A question answering benchmark on a hybrid of tabular and textual content in<br>finance. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and<br>the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp.<br>3277-3287, 2021.</p>",
            "id": 176,
            "page": 15,
            "text": "Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and Tat-Seng Chua. Tat-qa: A question answering benchmark on a hybrid of tabular and textual content in finance. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 3277-3287, 2021."
        },
        {
            "bounding_box": [
                {
                    "x": 1252,
                    "y": 3132
                },
                {
                    "x": 1300,
                    "y": 3132
                },
                {
                    "x": 1300,
                    "y": 3173
                },
                {
                    "x": 1252,
                    "y": 3173
                }
            ],
            "category": "footer",
            "html": "<footer id='177' style='font-size:14px'>15</footer>",
            "id": 177,
            "page": 15,
            "text": "15"
        },
        {
            "bounding_box": [
                {
                    "x": 295,
                    "y": 109
                },
                {
                    "x": 1540,
                    "y": 109
                },
                {
                    "x": 1540,
                    "y": 158
                },
                {
                    "x": 295,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='178' style='font-size:20px'>Published in Transactions on Machine Learning Research (10/2023)</header>",
            "id": 178,
            "page": 16,
            "text": "Published in Transactions on Machine Learning Research (10/2023)"
        },
        {
            "bounding_box": [
                {
                    "x": 294,
                    "y": 339
                },
                {
                    "x": 599,
                    "y": 339
                },
                {
                    "x": 599,
                    "y": 395
                },
                {
                    "x": 294,
                    "y": 395
                }
            ],
            "category": "paragraph",
            "html": "<p id='179' style='font-size:22px'>7 Appendix</p>",
            "id": 179,
            "page": 16,
            "text": "7 Appendix"
        },
        {
            "bounding_box": [
                {
                    "x": 294,
                    "y": 445
                },
                {
                    "x": 894,
                    "y": 445
                },
                {
                    "x": 894,
                    "y": 497
                },
                {
                    "x": 294,
                    "y": 497
                }
            ],
            "category": "paragraph",
            "html": "<p id='180' style='font-size:18px'>7.1 PoT as intermediate step</p>",
            "id": 180,
            "page": 16,
            "text": "7.1 PoT as intermediate step"
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 534
                },
                {
                    "x": 1071,
                    "y": 534
                },
                {
                    "x": 1071,
                    "y": 586
                },
                {
                    "x": 293,
                    "y": 586
                }
            ],
            "category": "paragraph",
            "html": "<p id='181' style='font-size:20px'>We demonstrate the workflow in Figure 8.</p>",
            "id": 181,
            "page": 16,
            "text": "We demonstrate the workflow in Figure 8."
        },
        {
            "bounding_box": [
                {
                    "x": 295,
                    "y": 604
                },
                {
                    "x": 2245,
                    "y": 604
                },
                {
                    "x": 2245,
                    "y": 991
                },
                {
                    "x": 295,
                    "y": 991
                }
            ],
            "category": "figure",
            "html": "<figure><img id='182' style='font-size:14px' alt=\"LLM\nKeep prompting CoT Answer\nLLM\nQuestion PoT Progarm Result\nEOS Answer\" data-coord=\"top-left:(295,604); bottom-right:(2245,991)\" /></figure>",
            "id": 182,
            "page": 16,
            "text": "LLM Keep prompting CoT Answer LLM Question PoT Progarm Result EOS Answer"
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 1031
                },
                {
                    "x": 2259,
                    "y": 1031
                },
                {
                    "x": 2259,
                    "y": 1133
                },
                {
                    "x": 290,
                    "y": 1133
                }
            ],
            "category": "paragraph",
            "html": "<p id='183' style='font-size:18px'>Figure 8: We adopt PoT to prompt language models to first generate an intermediate answer and then<br>continue to prompt large models to generate the final answer.</p>",
            "id": 183,
            "page": 16,
            "text": "Figure 8: We adopt PoT to prompt language models to first generate an intermediate answer and then continue to prompt large models to generate the final answer."
        },
        {
            "bounding_box": [
                {
                    "x": 294,
                    "y": 1152
                },
                {
                    "x": 972,
                    "y": 1152
                },
                {
                    "x": 972,
                    "y": 1203
                },
                {
                    "x": 294,
                    "y": 1203
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='184' style='font-size:18px'>We write the pseudo code as follows:</p>",
            "id": 184,
            "page": 16,
            "text": "We write the pseudo code as follows:"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 1226
                },
                {
                    "x": 1157,
                    "y": 1226
                },
                {
                    "x": 1157,
                    "y": 1659
                },
                {
                    "x": 291,
                    "y": 1659
                }
            ],
            "category": "paragraph",
            "html": "<p id='185' style='font-size:20px'># Function PoT( Input) -> Output<br># Input : question<br># Ouptut : program<br># Function Prompt ( Input) -> Output<br># Input : question + interme diate<br># Ouptut : answer<br>program = PoT ( question)<br>exec (program)<br>if isintance (ans dict ) :</p>",
            "id": 185,
            "page": 16,
            "text": "# Function PoT( Input) -> Output # Input : question # Ouptut : program # Function Prompt ( Input) -> Output # Input : question + interme diate # Ouptut : answer program = PoT ( question) exec (program) if isintance (ans dict ) :"
        },
        {
            "bounding_box": [
                {
                    "x": 704,
                    "y": 1652
                },
                {
                    "x": 717,
                    "y": 1652
                },
                {
                    "x": 717,
                    "y": 1673
                },
                {
                    "x": 704,
                    "y": 1673
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='186' style='font-size:14px'>,</p>",
            "id": 186,
            "page": 16,
            "text": ","
        },
        {
            "bounding_box": [
                {
                    "x": 393,
                    "y": 1674
                },
                {
                    "x": 1297,
                    "y": 1674
                },
                {
                    "x": 1297,
                    "y": 1876
                },
                {
                    "x": 393,
                    "y": 1876
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='187' style='font-size:16px'>ans = list (x . items ()) . pop (0)<br>, according_to_the_program : □<br>,<br>extra =<br>extra += ans [0] + , U=U , + ans [1]<br>pred = Prompt ( question + extra )</p>",
            "id": 187,
            "page": 16,
            "text": "ans = list (x . items ()) . pop (0) , according_to_the_program : □ , extra = extra += ans  + , U=U , + ans  pred = Prompt ( question + extra )"
        },
        {
            "bounding_box": [
                {
                    "x": 296,
                    "y": 1877
                },
                {
                    "x": 422,
                    "y": 1877
                },
                {
                    "x": 422,
                    "y": 1921
                },
                {
                    "x": 296,
                    "y": 1921
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='188' style='font-size:16px'>else :</p>",
            "id": 188,
            "page": 16,
            "text": "else :"
        },
        {
            "bounding_box": [
                {
                    "x": 396,
                    "y": 1928
                },
                {
                    "x": 650,
                    "y": 1928
                },
                {
                    "x": 650,
                    "y": 1974
                },
                {
                    "x": 396,
                    "y": 1974
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='189' style='font-size:14px'>pred = ans</p>",
            "id": 189,
            "page": 16,
            "text": "pred = ans"
        },
        {
            "bounding_box": [
                {
                    "x": 295,
                    "y": 1979
                },
                {
                    "x": 577,
                    "y": 1979
                },
                {
                    "x": 577,
                    "y": 2024
                },
                {
                    "x": 295,
                    "y": 2024
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='190' style='font-size:20px'>return pred</p>",
            "id": 190,
            "page": 16,
            "text": "return pred"
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 2072
                },
                {
                    "x": 2256,
                    "y": 2072
                },
                {
                    "x": 2256,
                    "y": 2175
                },
                {
                    "x": 290,
                    "y": 2175
                }
            ],
            "category": "paragraph",
            "html": "<p id='191' style='font-size:18px'>PoT as intermediate step is able to address more complex questions which require both symbolic and com-<br>monsense reasoning.</p>",
            "id": 191,
            "page": 16,
            "text": "PoT as intermediate step is able to address more complex questions which require both symbolic and commonsense reasoning."
        },
        {
            "bounding_box": [
                {
                    "x": 295,
                    "y": 2228
                },
                {
                    "x": 883,
                    "y": 2228
                },
                {
                    "x": 883,
                    "y": 2282
                },
                {
                    "x": 295,
                    "y": 2282
                }
            ],
            "category": "paragraph",
            "html": "<p id='192' style='font-size:22px'>7.2 Exemplars for Prompting</p>",
            "id": 192,
            "page": 16,
            "text": "7.2 Exemplars for Prompting"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 2318
                },
                {
                    "x": 2256,
                    "y": 2318
                },
                {
                    "x": 2256,
                    "y": 2421
                },
                {
                    "x": 291,
                    "y": 2421
                }
            ],
            "category": "paragraph",
            "html": "<p id='193' style='font-size:20px'>To enable better reproducibility, we also put our prompts and exemplars for GSM8K dataset and AQuA<br>dataset in the following pages:</p>",
            "id": 193,
            "page": 16,
            "text": "To enable better reproducibility, we also put our prompts and exemplars for GSM8K dataset and AQuA dataset in the following pages:"
        },
        {
            "bounding_box": [
                {
                    "x": 1252,
                    "y": 3132
                },
                {
                    "x": 1300,
                    "y": 3132
                },
                {
                    "x": 1300,
                    "y": 3173
                },
                {
                    "x": 1252,
                    "y": 3173
                }
            ],
            "category": "footer",
            "html": "<footer id='194' style='font-size:18px'>16</footer>",
            "id": 194,
            "page": 16,
            "text": "16"
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 303
                },
                {
                    "x": 2221,
                    "y": 303
                },
                {
                    "x": 2221,
                    "y": 809
                },
                {
                    "x": 290,
                    "y": 809
                }
            ],
            "category": "paragraph",
            "html": "<p id='195' style='font-size:18px'>Question: Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her<br>friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How<br>much in dollars does she make every day at the farmers' market?<br># Python code, return ans<br>total_eggs = 16<br>eaten_eggs = 3<br>baked_eggs = 4<br>sold_eggs = total_eggs - eaten_eggs - baked_eggs<br>dollars_per_egg = 2<br>ans = sold_eggs * dollars_per_egg</p>",
            "id": 195,
            "page": 17,
            "text": "Question: Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market? # Python code, return ans total_eggs = 16 eaten_eggs = 3 baked_eggs = 4 sold_eggs = total_eggs - eaten_eggs - baked_eggs dollars_per_egg = 2 ans = sold_eggs * dollars_per_egg"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 861
                },
                {
                    "x": 2213,
                    "y": 861
                },
                {
                    "x": 2213,
                    "y": 1118
                },
                {
                    "x": 291,
                    "y": 1118
                }
            ],
            "category": "paragraph",
            "html": "<p id='196' style='font-size:18px'>Question: A robe takes 2 bolts of blue fiber and half that much white fiber. How many bolts in total does it take?<br># Python code, return ans<br>bolts_of_blue_ fiber = 2<br>bolts_of_white_fiber = num_of_blue_fiber / 2<br>ans = bolts_of_ blue_fiber + bolts_of_white_fiber</p>",
            "id": 196,
            "page": 17,
            "text": "Question: A robe takes 2 bolts of blue fiber and half that much white fiber. How many bolts in total does it take? # Python code, return ans bolts_of_blue_ fiber = 2 bolts_of_white_fiber = num_of_blue_fiber / 2 ans = bolts_of_ blue_fiber + bolts_of_white_fiber"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 1168
                },
                {
                    "x": 2191,
                    "y": 1168
                },
                {
                    "x": 2191,
                    "y": 1576
                },
                {
                    "x": 291,
                    "y": 1576
                }
            ],
            "category": "paragraph",
            "html": "<p id='197' style='font-size:20px'>Question: Josh decides to try flipping a house. He buys a house for $80,000 and then puts in $50,000 in repairs.<br>This increased the value of the house by 150%. How much profit did he make?<br># Python code, return ans<br>cost_of_ original_house = 80000<br>increase_rate = 150 / 100<br>value_of_ house = (1 + increase_ rate) * cost_ of_ original_ house<br>cost_of_repair = 50000<br>ans = value_of_house - cost_of_repair - cost_of_original_ house</p>",
            "id": 197,
            "page": 17,
            "text": "Question: Josh decides to try flipping a house. He buys a house for $80,000 and then puts in $50,000 in repairs. This increased the value of the house by 150%. How much profit did he make? # Python code, return ans cost_of_ original_house = 80000 increase_rate = 150 / 100 value_of_ house = (1 + increase_ rate) * cost_ of_ original_ house cost_of_repair = 50000 ans = value_of_house - cost_of_repair - cost_of_original_ house"
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 1627
                },
                {
                    "x": 2242,
                    "y": 1627
                },
                {
                    "x": 2242,
                    "y": 2238
                },
                {
                    "x": 290,
                    "y": 2238
                }
            ],
            "category": "paragraph",
            "html": "<p id='198' style='font-size:20px'>Question: Every day, Wendi feeds each of her chickens three cups of mixed chicken feed, containing seeds,<br>mealworms and vegetables to help keep them healthy. She gives the chickens their feed in three separate meals.<br>In the morning, she gives her flock of chickens 15 cups of feed. In the afternoon, she gives her chickens another 25<br>cups of feed. How many cups of feed does she need to give her chickens in the final meal of the day if the size of<br>Wendi's flock is 20 chickens?<br># Python code, return ans<br>numb_of_chickens = 20<br>cups_for_each_chicken = 3<br>cups_for_all_chicken = num_of_ chickens * cups_for_ each_ chicken<br>cups_in_the_morning = 15<br>cups_in_the_afternoon = 25<br>ans = cups_for_all_chicken - cups_in_the_morning - cups_in_the_afternoon</p>",
            "id": 198,
            "page": 17,
            "text": "Question: Every day, Wendi feeds each of her chickens three cups of mixed chicken feed, containing seeds, mealworms and vegetables to help keep them healthy. She gives the chickens their feed in three separate meals. In the morning, she gives her flock of chickens 15 cups of feed. In the afternoon, she gives her chickens another 25 cups of feed. How many cups of feed does she need to give her chickens in the final meal of the day if the size of Wendi's flock is 20 chickens? # Python code, return ans numb_of_chickens = 20 cups_for_each_chicken = 3 cups_for_all_chicken = num_of_ chickens * cups_for_ each_ chicken cups_in_the_morning = 15 cups_in_the_afternoon = 25 ans = cups_for_all_chicken - cups_in_the_morning - cups_in_the_afternoon"
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 2285
                },
                {
                    "x": 2225,
                    "y": 2285
                },
                {
                    "x": 2225,
                    "y": 2391
                },
                {
                    "x": 289,
                    "y": 2391
                }
            ],
            "category": "paragraph",
            "html": "<p id='199' style='font-size:18px'>Question: Kylar went to the store to buy glasses for his new apartment. One glass costs $5, but every second glass<br>costs only 60% of the price. Kylar wants to buy 16 glasses. How much does he need to pay for them?</p>",
            "id": 199,
            "page": 17,
            "text": "Question: Kylar went to the store to buy glasses for his new apartment. One glass costs $5, but every second glass costs only 60% of the price. Kylar wants to buy 16 glasses. How much does he need to pay for them?"
        },
        {
            "bounding_box": [
                {
                    "x": 294,
                    "y": 2391
                },
                {
                    "x": 673,
                    "y": 2391
                },
                {
                    "x": 673,
                    "y": 2437
                },
                {
                    "x": 294,
                    "y": 2437
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='200' style='font-size:22px'># Python code, return</p>",
            "id": 200,
            "page": 17,
            "text": "# Python code, return"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 2414
                },
                {
                    "x": 777,
                    "y": 2414
                },
                {
                    "x": 777,
                    "y": 2696
                },
                {
                    "x": 291,
                    "y": 2696
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='201' style='font-size:14px'>ans<br>num_glasses = 16<br>first_glass_cost = 5<br>second_glass_ cost = 5 * 0.6<br>ans = 0<br>for i in range(num _glasses):</p>",
            "id": 201,
            "page": 17,
            "text": "ans num_glasses = 16 first_glass_cost = 5 second_glass_ cost = 5 * 0.6 ans = 0 for i in range(num _glasses):"
        },
        {
            "bounding_box": [
                {
                    "x": 328,
                    "y": 2696
                },
                {
                    "x": 545,
                    "y": 2696
                },
                {
                    "x": 545,
                    "y": 2740
                },
                {
                    "x": 328,
                    "y": 2740
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='202' style='font-size:14px'>ifi % 2 == 0:</p>",
            "id": 202,
            "page": 17,
            "text": "ifi % 2 == 0:"
        },
        {
            "bounding_box": [
                {
                    "x": 329,
                    "y": 2750
                },
                {
                    "x": 761,
                    "y": 2750
                },
                {
                    "x": 761,
                    "y": 2842
                },
                {
                    "x": 329,
                    "y": 2842
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='203' style='font-size:18px'>ans += first_glass_cost<br>else:</p>",
            "id": 203,
            "page": 17,
            "text": "ans += first_glass_cost else:"
        },
        {
            "bounding_box": [
                {
                    "x": 365,
                    "y": 2852
                },
                {
                    "x": 811,
                    "y": 2852
                },
                {
                    "x": 811,
                    "y": 2899
                },
                {
                    "x": 365,
                    "y": 2899
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='204' style='font-size:16px'>ans += second _glass_cost</p>",
            "id": 204,
            "page": 17,
            "text": "ans += second _glass_cost"
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 301
                },
                {
                    "x": 2233,
                    "y": 301
                },
                {
                    "x": 2233,
                    "y": 808
                },
                {
                    "x": 290,
                    "y": 808
                }
            ],
            "category": "paragraph",
            "html": "<p id='205' style='font-size:18px'>Question: Marissa is hiking a 12-mile trail. She took 1 hour to walk the first 4 miles, then another hour to walk the<br>next two miles. If she wants her average speed to be 4 miles per hour, what speed (in miles per hour) does she<br>need to walk the remaining distance?<br># Python code, return ans<br>average_mile_per_hour = 4<br>total_trail_ miles = 12<br>remaining_miles = total_trail_miles - 4 - 2<br>total_hours = total_trail_ miles / average_ mile _per_ hour<br>remaining_hours = total_ hours - 2<br>ans = remaining_miles / remaining_ hours</p>",
            "id": 205,
            "page": 18,
            "text": "Question: Marissa is hiking a 12-mile trail. She took 1 hour to walk the first 4 miles, then another hour to walk the next two miles. If she wants her average speed to be 4 miles per hour, what speed (in miles per hour) does she need to walk the remaining distance? # Python code, return ans average_mile_per_hour = 4 total_trail_ miles = 12 remaining_miles = total_trail_miles - 4 - 2 total_hours = total_trail_ miles / average_ mile _per_ hour remaining_hours = total_ hours - 2 ans = remaining_miles / remaining_ hours"
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 863
                },
                {
                    "x": 2237,
                    "y": 863
                },
                {
                    "x": 2237,
                    "y": 1375
                },
                {
                    "x": 292,
                    "y": 1375
                }
            ],
            "category": "paragraph",
            "html": "<p id='206' style='font-size:14px'>Question: Carlos is planting a lemon tree. The tree will cost $90 to plant. Each year it will grow 7 lemons, which he<br>can sell for $1.5 each. It costs $3 a year to water and feed the tree. How many years will it tak<br>e before he starts earning money on the lemon tree?<br># Python code, return ans<br>total_cost = 90<br>cost_of_ watering_and_feeding = 3<br>cost_of_ each_lemon = 1.5<br>num_ of_ Jemon_per_year = 7<br>ans = 0<br>while total_cost > 0:</p>",
            "id": 206,
            "page": 18,
            "text": "Question: Carlos is planting a lemon tree. The tree will cost $90 to plant. Each year it will grow 7 lemons, which he can sell for $1.5 each. It costs $3 a year to water and feed the tree. How many years will it tak e before he starts earning money on the lemon tree? # Python code, return ans total_cost = 90 cost_of_ watering_and_feeding = 3 cost_of_ each_lemon = 1.5 num_ of_ Jemon_per_year = 7 ans = 0 while total_cost > 0:"
        },
        {
            "bounding_box": [
                {
                    "x": 327,
                    "y": 1374
                },
                {
                    "x": 1389,
                    "y": 1374
                },
                {
                    "x": 1389,
                    "y": 1519
                },
                {
                    "x": 327,
                    "y": 1519
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='207' style='font-size:18px'>total_cost += cost_of_ watering_and_feeding<br>total_ cost -= num_of_ Jemon_per_year * cost_of_ each_ lemon<br>ans += 1</p>",
            "id": 207,
            "page": 18,
            "text": "total_cost += cost_of_ watering_and_feeding total_ cost -= num_of_ Jemon_per_year * cost_of_ each_ lemon ans += 1"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 1571
                },
                {
                    "x": 2254,
                    "y": 1571
                },
                {
                    "x": 2254,
                    "y": 2035
                },
                {
                    "x": 291,
                    "y": 2035
                }
            ],
            "category": "paragraph",
            "html": "<p id='208' style='font-size:14px'>Question: When Freda cooks canned tomatoes into sauce, they lose half their volume. Each 16 ounce can of<br>tomatoes that she uses contains three tomatoes. Freda`s last batch of tomato sauce made 32 ounces of sauce. How<br>many tomatoes did Freda use?<br># Python code, return ans<br>lose_rate = 0.5<br>num_tomato_comained_in_pe'__ounce_sauce = 3 / 16<br>ounce_sauce_in_last_batch = 32<br>num_tomato_in_last_ batch = ounce_sauce_in_last_batch * num_tomato_contained _in_per_ ounce_sauce<br>ans = num_tomato_in_last_batch / (1 - lose_rate)</p>",
            "id": 208,
            "page": 18,
            "text": "Question: When Freda cooks canned tomatoes into sauce, they lose half their volume. Each 16 ounce can of tomatoes that she uses contains three tomatoes. Freda`s last batch of tomato sauce made 32 ounces of sauce. How many tomatoes did Freda use? # Python code, return ans lose_rate = 0.5 num_tomato_comained_in_pe'__ounce_sauce = 3 / 16 ounce_sauce_in_last_batch = 32 num_tomato_in_last_ batch = ounce_sauce_in_last_batch * num_tomato_contained _in_per_ ounce_sauce ans = num_tomato_in_last_batch / (1 - lose_rate)"
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 2085
                },
                {
                    "x": 2250,
                    "y": 2085
                },
                {
                    "x": 2250,
                    "y": 2743
                },
                {
                    "x": 290,
                    "y": 2743
                }
            ],
            "category": "paragraph",
            "html": "<p id='209' style='font-size:14px'>Question: Jordan wanted to surprise her mom with a homemade birthday cake. From reading the instructions, she<br>knew it would take 20 minutes to make the cake batter and 30 minutes to bake the cake. The cake would require 2<br>hours to cool and an additional 10 minutes to frost the cake. If she plans to make the cake all on the same day,<br>what is the latest time of day that Jordan can start making the cake to be ready to serve it at 5:00 pm?<br># Python code, return ans<br>minutes_to_make_batter = 20<br>minutes_to_bake_cake = 30<br>minutes_to_cool_cake = 2 * 60<br>minutes_to_ frost_cake = 10<br>total_minutes = minutes_ to_make_ batter + minutes_to_ bake_ cake + minutes_to_cool_ cake +<br>minutes_to_ frost_cake<br>total_ hours = total_minutes / 60<br>ans = 5 - total_ hours</p>",
            "id": 209,
            "page": 18,
            "text": "Question: Jordan wanted to surprise her mom with a homemade birthday cake. From reading the instructions, she knew it would take 20 minutes to make the cake batter and 30 minutes to bake the cake. The cake would require 2 hours to cool and an additional 10 minutes to frost the cake. If she plans to make the cake all on the same day, what is the latest time of day that Jordan can start making the cake to be ready to serve it at 5:00 pm? # Python code, return ans minutes_to_make_batter = 20 minutes_to_bake_cake = 30 minutes_to_cool_cake = 2 * 60 minutes_to_ frost_cake = 10 total_minutes = minutes_ to_make_ batter + minutes_to_ bake_ cake + minutes_to_cool_ cake + minutes_to_ frost_cake total_ hours = total_minutes / 60 ans = 5 - total_ hours"
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 300
                },
                {
                    "x": 2111,
                    "y": 300
                },
                {
                    "x": 2111,
                    "y": 642
                },
                {
                    "x": 289,
                    "y": 642
                }
            ],
            "category": "paragraph",
            "html": "<p id='210' style='font-size:20px'># Write Python Code to solve the following questions. Store your result as a variable named 'ans'.<br>from sympy import Symbol<br>from sympy import simplify<br>import math<br>from sympy import solve_it<br># solve_it(equations, variable): solving the equations and return the variable value.</p>",
            "id": 210,
            "page": 19,
            "text": "# Write Python Code to solve the following questions. Store your result as a variable named 'ans'. from sympy import Symbol from sympy import simplify import math from sympy import solve_it # solve_it(equations, variable): solving the equations and return the variable value."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 693
                },
                {
                    "x": 2238,
                    "y": 693
                },
                {
                    "x": 2238,
                    "y": 1313
                },
                {
                    "x": 290,
                    "y": 1313
                }
            ],
            "category": "paragraph",
            "html": "<p id='211' style='font-size:18px'># Question: In a flight of 600 km, an aircraft was slowed down due to bad weather. Its average speed for<br>the trip was reduced by 200 km/hr and the time of flight increased by 30 minutes. The duration of the<br>flight is:<br># Answer option: ['A)1 hour', 'B)2 hours', 'C)3 hours', 'D)4 hours', 'E)5 hours']<br>duration = Symbol('duration', positive=True)<br>delay = 30 / 60<br>total_disntace = 600<br>original_speed = total_disntace / duration<br>reduced_speed = total_disntace / (duration + delay)<br>solution = solve_it(original_speed - reduced_speed - 200, duration)<br>ans = solution[duration]</p>",
            "id": 211,
            "page": 19,
            "text": "# Question: In a flight of 600 km, an aircraft was slowed down due to bad weather. Its average speed for the trip was reduced by 200 km/hr and the time of flight increased by 30 minutes. The duration of the flight is: # Answer option: ['A)1 hour', 'B)2 hours', 'C)3 hours', 'D)4 hours', 'E)5 hours'] duration = Symbol('duration', positive=True) delay = 30 / 60 total_disntace = 600 original_speed = total_disntace / duration reduced_speed = total_disntace / (duration + delay) solution = solve_it(original_speed - reduced_speed - 200, duration) ans = solution[duration]"
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 1367
                },
                {
                    "x": 2213,
                    "y": 1367
                },
                {
                    "x": 2213,
                    "y": 1817
                },
                {
                    "x": 290,
                    "y": 1817
                }
            ],
            "category": "paragraph",
            "html": "<p id='212' style='font-size:18px'># Question: M men agree to purchase a gift for Rs. D. If 3 men drop out how much more will each have<br>to contribute towards the purchase of the gift?<br># Answer options: ['A)D/(M-3)', 'B)MD/3', 'C)M/(D-3)', 'D)3D/(M2-3M)', 'E)None of these']<br>M = Symbol('M')<br>D = Symbol('D')<br>cost_before_dropout = D / M<br>cost_after_dropout = D / (M - 3)<br>ans=simplify)cost_after_dropout - cost_before_dropout)</p>",
            "id": 212,
            "page": 19,
            "text": "# Question: M men agree to purchase a gift for Rs. D. If 3 men drop out how much more will each have to contribute towards the purchase of the gift? # Answer options: ['A)D/(M-3)', 'B)MD/3', 'C)M/(D-3)', 'D)3D/(M2-3M)', 'E)None of these'] M = Symbol('M') D = Symbol('D') cost_before_dropout = D / M cost_after_dropout = D / (M - 3) ans=simplify)cost_after_dropout - cost_before_dropout)"
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 1868
                },
                {
                    "x": 2198,
                    "y": 1868
                },
                {
                    "x": 2198,
                    "y": 2374
                },
                {
                    "x": 290,
                    "y": 2374
                }
            ],
            "category": "paragraph",
            "html": "<p id='213' style='font-size:16px'># Question: A sum of money at simple interest amounts to Rs. 815 in 3 years and to Rs. 854 in 4 years.<br>The sum is:<br># Answer option: ['A)Rs. 650', 'B)Rs. 690', 'C)Rs. 698', 'D)Rs. 700', 'E)None of these']<br>deposit = Symbol('deposit', positive=True)<br>interest = Symbol('interest', positive=True)<br>money_in_3_years = deposit + 3 * interest<br>money_in_4_years = deposit + 4 * interest<br>solution = solve_it([money_in_3_years - 815, money_in_4_years - 854], [deposit, interest])<br>ans = solution[deposit]</p>",
            "id": 213,
            "page": 19,
            "text": "# Question: A sum of money at simple interest amounts to Rs. 815 in 3 years and to Rs. 854 in 4 years. The sum is: # Answer option: ['A)Rs. 650', 'B)Rs. 690', 'C)Rs. 698', 'D)Rs. 700', 'E)None of these'] deposit = Symbol('deposit', positive=True) interest = Symbol('interest', positive=True) money_in_3_years = deposit + 3 * interest money_in_4_years = deposit + 4 * interest solution = solve_it([money_in_3_years - 815, money_in_4_years - 854], [deposit, interest]) ans = solution[deposit]"
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 2432
                },
                {
                    "x": 2131,
                    "y": 2432
                },
                {
                    "x": 2131,
                    "y": 2658
                },
                {
                    "x": 290,
                    "y": 2658
                }
            ],
            "category": "paragraph",
            "html": "<p id='214' style='font-size:20px'># Question: Find out which of the following values is the multiple of X, if it is divisible by 9 and 12?<br># Answer option: ['A)36', 'B)15', 'C)17', 'D)5', 'E)7']<br>options = [36, 15, 17, 5, 7]<br>for option in options:</p>",
            "id": 214,
            "page": 19,
            "text": "# Question: Find out which of the following values is the multiple of X, if it is divisible by 9 and 12? # Answer option: ['A)36', 'B)15', 'C)17', 'D)5', 'E)7'] options =  for option in options:"
        },
        {
            "bounding_box": [
                {
                    "x": 332,
                    "y": 2657
                },
                {
                    "x": 1087,
                    "y": 2657
                },
                {
                    "x": 1087,
                    "y": 2820
                },
                {
                    "x": 332,
                    "y": 2820
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='215' style='font-size:14px'>if option % 9 == 0 and option % 12 == 0:<br>ans = option<br>break</p>",
            "id": 215,
            "page": 19,
            "text": "if option % 9 == 0 and option % 12 == 0: ans = option break"
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 299
                },
                {
                    "x": 2242,
                    "y": 299
                },
                {
                    "x": 2242,
                    "y": 862
                },
                {
                    "x": 289,
                    "y": 862
                }
            ],
            "category": "paragraph",
            "html": "<p id='216' style='font-size:14px'># Question: 35% of the employees of a company are men. 60% of the men in the company speak French<br>and 40% of the employees of the company speak French. What is % of the women in the company who<br>do not speak French?<br># Answer option: ['A)4%', 'B)10%', 'C)96%', 'D)90.12%', 'E)70.77%']<br>num_women = 65<br>men_speaking_french = 0.6 * 35<br>employees_speaking_french = 0.4 * 100<br>women_speaking_french = employees_speaking_french - men_speaking_ french<br>women_not_speaking_franchenum_womer - women_speaking_french<br>ans = women_not_speaking_french / num_women</p>",
            "id": 216,
            "page": 20,
            "text": "# Question: 35% of the employees of a company are men. 60% of the men in the company speak French and 40% of the employees of the company speak French. What is % of the women in the company who do not speak French? # Answer option: ['A)4%', 'B)10%', 'C)96%', 'D)90.12%', 'E)70.77%'] num_women = 65 men_speaking_french = 0.6 * 35 employees_speaking_french = 0.4 * 100 women_speaking_french = employees_speaking_french - men_speaking_ french women_not_speaking_franchenum_womer - women_speaking_french ans = women_not_speaking_french / num_women"
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 918
                },
                {
                    "x": 2188,
                    "y": 918
                },
                {
                    "x": 2188,
                    "y": 1479
                },
                {
                    "x": 289,
                    "y": 1479
                }
            ],
            "category": "paragraph",
            "html": "<p id='217' style='font-size:18px'># Question: In one hour, a boat goes 11 km/hr along the stream and 5 km/hr against the stream. The<br>speed of the boat in still water (in km/hr) is:<br># Answer option: ['A)4 kmph', 'B)5 kmph', 'C)6 kmph', 'D)7 kmph', 'E)8 kmph']<br>boat_speed = Symbol('boat_speed', positive=True)<br>stream_speed = Symbol('stream_speed', positive=True)<br>along_stream_speed = 11<br>against_stream_speed = 5<br>solution = solve_it([boat_speed + stream_speed - along_stream_speed, boat_speed - stream_speed -<br>against_stream_speed], [boat_speed, stream_speed])<br>ans = solution[boat_speed]</p>",
            "id": 217,
            "page": 20,
            "text": "# Question: In one hour, a boat goes 11 km/hr along the stream and 5 km/hr against the stream. The speed of the boat in still water (in km/hr) is: # Answer option: ['A)4 kmph', 'B)5 kmph', 'C)6 kmph', 'D)7 kmph', 'E)8 kmph'] boat_speed = Symbol('boat_speed', positive=True) stream_speed = Symbol('stream_speed', positive=True) along_stream_speed = 11 against_stream_speed = 5 solution = solve_it([boat_speed + stream_speed - along_stream_speed, boat_speed - stream_speed against_stream_speed], [boat_speed, stream_speed]) ans = solution[boat_speed]"
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 1533
                },
                {
                    "x": 2254,
                    "y": 1533
                },
                {
                    "x": 2254,
                    "y": 2036
                },
                {
                    "x": 289,
                    "y": 2036
                }
            ],
            "category": "paragraph",
            "html": "<p id='218' style='font-size:14px'># Question: The difference between simple interest and C.I. at the same rate for Rs.5000 for 2 years in<br>Rs.72. The rate of interest is?<br># Answer option: ['A)10%', 'B)12%', 'C)6%', 'D)8%', 'E)4%']<br>interest_rate = Symbol('interest_rate', positive=True)<br>amount = 5000<br>amount_with_simple_interest = amount * (1 + 2 * interest_rate / 100)<br>amount_with_compound_interest = amount * (1 + interest_rate / 100) ** 2<br>solution = soke_Itlamount_with_comcound_nterest - amount_with_simple_interest - 72, interest_rate)<br>ans = solution[interest_rate]</p>",
            "id": 218,
            "page": 20,
            "text": "# Question: The difference between simple interest and C.I. at the same rate for Rs.5000 for 2 years in Rs.72. The rate of interest is? # Answer option: ['A)10%', 'B)12%', 'C)6%', 'D)8%', 'E)4%'] interest_rate = Symbol('interest_rate', positive=True) amount = 5000 amount_with_simple_interest = amount * (1 + 2 * interest_rate / 100) amount_with_compound_interest = amount * (1 + interest_rate / 100) ** 2 solution = soke_Itlamount_with_comcound_nterest - amount_with_simple_interest - 72, interest_rate) ans = solution[interest_rate]"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 2105
                },
                {
                    "x": 2230,
                    "y": 2105
                },
                {
                    "x": 2230,
                    "y": 2597
                },
                {
                    "x": 288,
                    "y": 2597
                }
            ],
            "category": "paragraph",
            "html": "<p id='219' style='font-size:14px'># Question: The area of a rectangle is 15 square centimeters and the perimeter is 16 centimeters. What<br>are the dimensions of the rectangle?<br># Answer option: ['A)2&4', 'B)3&5', 'C)4&6', 'D)5&7', 'E)6&8']<br>width = Symbol('width', positive=True)<br>height = Symbol('height', positive=True)<br>area = 15<br>permimeter = 16<br>solution = solve_it([width * height - area, 2 * (width + height) - permimeter], [width, height])<br>ans = (solution[width], solution[height])</p>",
            "id": 219,
            "page": 20,
            "text": "# Question: The area of a rectangle is 15 square centimeters and the perimeter is 16 centimeters. What are the dimensions of the rectangle? # Answer option: ['A)2&4', 'B)3&5', 'C)4&6', 'D)5&7', 'E)6&8'] width = Symbol('width', positive=True) height = Symbol('height', positive=True) area = 15 permimeter = 16 solution = solve_it([width * height - area, 2 * (width + height) - permimeter], [width, height]) ans = (solution[width], solution[height])"
        }
    ]
}