{
    "id": "32adbe04-0f94-11ef-8828-426932df3dcf",
    "pdf_path": "./pdf/AI_VIT_X/2104.05832v1.pdf",
    "elements": [
        {
            "bounding_box": [
                {
                    "x": 529,
                    "y": 323
                },
                {
                    "x": 1952,
                    "y": 323
                },
                {
                    "x": 1952,
                    "y": 459
                },
                {
                    "x": 529,
                    "y": 459
                }
            ],
            "category": "paragraph",
            "html": "<p id='0' style='font-size:22px'>SPARTQA: A Textual Question Answering Benchmark<br>for Spatial Reasoning</p>",
            "id": 0,
            "page": 1,
            "text": "SPARTQA: A Textual Question Answering Benchmark for Spatial Reasoning"
        },
        {
            "bounding_box": [
                {
                    "x": 311,
                    "y": 546
                },
                {
                    "x": 2152,
                    "y": 546
                },
                {
                    "x": 2152,
                    "y": 610
                },
                {
                    "x": 311,
                    "y": 610
                }
            ],
            "category": "paragraph",
            "html": "<p id='1' style='font-size:20px'>Roshanak Mirzaee★ Hossein Rajaby Faghihi★ Qiang Ning * Parisa Kordjamshidi★</p>",
            "id": 1,
            "page": 1,
            "text": "Roshanak Mirzaee★ Hossein Rajaby Faghihi★ Qiang Ning * Parisa Kordjamshidi★"
        },
        {
            "bounding_box": [
                {
                    "x": 843,
                    "y": 606
                },
                {
                    "x": 1650,
                    "y": 606
                },
                {
                    "x": 1650,
                    "y": 662
                },
                {
                    "x": 843,
                    "y": 662
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='2' style='font-size:22px'>★Michigan State University Amazon</p>",
            "id": 2,
            "page": 1,
            "text": "★Michigan State University Amazon"
        },
        {
            "bounding_box": [
                {
                    "x": 431,
                    "y": 669
                },
                {
                    "x": 2069,
                    "y": 669
                },
                {
                    "x": 2069,
                    "y": 724
                },
                {
                    "x": 431,
                    "y": 724
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='3' style='font-size:16px'>{ mi rzaeem, ra jabyfa, kordj ams } @msu · edu qning@amazon · com</p>",
            "id": 3,
            "page": 1,
            "text": "{ mi rzaeem, ra jabyfa, kordj ams } @msu · edu qning@amazon · com"
        },
        {
            "bounding_box": [
                {
                    "x": 649,
                    "y": 889
                },
                {
                    "x": 848,
                    "y": 889
                },
                {
                    "x": 848,
                    "y": 944
                },
                {
                    "x": 649,
                    "y": 944
                }
            ],
            "category": "paragraph",
            "html": "<p id='4' style='font-size:20px'>Abstract</p>",
            "id": 4,
            "page": 1,
            "text": "Abstract"
        },
        {
            "bounding_box": [
                {
                    "x": 357,
                    "y": 985
                },
                {
                    "x": 1149,
                    "y": 985
                },
                {
                    "x": 1149,
                    "y": 1889
                },
                {
                    "x": 357,
                    "y": 1889
                }
            ],
            "category": "paragraph",
            "html": "<p id='5' style='font-size:16px'>This paper proposes a question-answering<br>(QA) benchmark for spatial reasoning on nat-<br>ural language text which contains more real-<br>istic spatial phenomena not covered by prior<br>work and is challenging for state-of-the-art<br>language models (LM). We propose a distant<br>supervision method to improve on this task.<br>Specifically, we design grammar and reason-<br>ing rules to automatically generate a spatial de-<br>scription of visual scenes and corresponding<br>QA pairs. Experiments show that further pre-<br>training LMs on these automatically generated<br>data significantly improves LMs' capability on<br>spatial understanding, which in turn helps to<br>better solve two external datasets, bAbI, and<br>boolQ. We hope that this work can foster inves-<br>tigations into more sophisticated models for<br>spatial reasoning over text.</p>",
            "id": 5,
            "page": 1,
            "text": "This paper proposes a question-answering (QA) benchmark for spatial reasoning on natural language text which contains more realistic spatial phenomena not covered by prior work and is challenging for state-of-the-art language models (LM). We propose a distant supervision method to improve on this task. Specifically, we design grammar and reasoning rules to automatically generate a spatial description of visual scenes and corresponding QA pairs. Experiments show that further pretraining LMs on these automatically generated data significantly improves LMs' capability on spatial understanding, which in turn helps to better solve two external datasets, bAbI, and boolQ. We hope that this work can foster investigations into more sophisticated models for spatial reasoning over text."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 1931
                },
                {
                    "x": 644,
                    "y": 1931
                },
                {
                    "x": 644,
                    "y": 1985
                },
                {
                    "x": 291,
                    "y": 1985
                }
            ],
            "category": "paragraph",
            "html": "<p id='6' style='font-size:20px'>1 Introduction</p>",
            "id": 6,
            "page": 1,
            "text": "1 Introduction"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 2020
                },
                {
                    "x": 1217,
                    "y": 2020
                },
                {
                    "x": 1217,
                    "y": 2639
                },
                {
                    "x": 287,
                    "y": 2639
                }
            ],
            "category": "paragraph",
            "html": "<p id='7' style='font-size:18px'>Spatial reasoning is a cognitive process based<br>on the construction of mental representations<br>for spatial objects, relations, and transforma-<br>tions (Clements and Battista, 1992), which is<br>necessary for many natural language understand-<br>ing (NLU) tasks such as natural language navi-<br>gation (Chen et al., 2019; Roman Roman et al.,<br>2020; Kim et al., 2020), human-machine interac-<br>tion (Landsiedel et al., 2017; Roman Roman et al.,<br>2020), dialogue systems (Udagawa et al., 2020),<br>and clinical analysis (Datta and Roberts, 2020).</p>",
            "id": 7,
            "page": 1,
            "text": "Spatial reasoning is a cognitive process based on the construction of mental representations for spatial objects, relations, and transformations (Clements and Battista, 1992), which is necessary for many natural language understanding (NLU) tasks such as natural language navigation (Chen , 2019; Roman Roman , 2020; Kim , 2020), human-machine interaction (Landsiedel , 2017; Roman Roman , 2020), dialogue systems (Udagawa , 2020), and clinical analysis (Datta and Roberts, 2020)."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 2644
                },
                {
                    "x": 1218,
                    "y": 2644
                },
                {
                    "x": 1218,
                    "y": 3150
                },
                {
                    "x": 288,
                    "y": 3150
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='8' style='font-size:18px'>Modern language models (LM), e.g., BERT (De-<br>vlin et al., 2019), ALBERT (Lan et al., 2020), and<br>XLNet (Yang et al., 2019) have seen great suc-<br>cesses in natural language processing (NLP). How-<br>ever, there has been limited investigation into spa-<br>tial reasoning capabilities of LMs. To the best of<br>our knowledge, bAbI (Weston et al., 2015) (Fig 9)<br>is the only dataset with direct textual spatial ques-<br>tion answering (QA) (Task 17), but it is synthetic</p>",
            "id": 8,
            "page": 1,
            "text": "Modern language models (LM), e.g., BERT (Devlin , 2019), ALBERT (Lan , 2020), and XLNet (Yang , 2019) have seen great successes in natural language processing (NLP). However, there has been limited investigation into spatial reasoning capabilities of LMs. To the best of our knowledge, bAbI (Weston , 2015) (Fig 9) is the only dataset with direct textual spatial question answering (QA) (Task 17), but it is synthetic"
        },
        {
            "bounding_box": [
                {
                    "x": 357,
                    "y": 3177
                },
                {
                    "x": 1134,
                    "y": 3177
                },
                {
                    "x": 1134,
                    "y": 3224
                },
                {
                    "x": 357,
                    "y": 3224
                }
            ],
            "category": "paragraph",
            "html": "<p id='9' style='font-size:14px'>* Work was done while at the Allen Institute for AI.</p>",
            "id": 9,
            "page": 1,
            "text": "* Work was done while at the Allen Institute for AI."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 891
                },
                {
                    "x": 2196,
                    "y": 891
                },
                {
                    "x": 2196,
                    "y": 1338
                },
                {
                    "x": 1267,
                    "y": 1338
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='10' style='font-size:18px'>and overly simplified: (1) The underlying scenes<br>are spatially simple, with only three objects and<br>relations only in four directions. (2) The stories<br>for these scenes are two short, templated sentences,<br>each describing a single relation between two ob-<br>jects. (3) The questions typically require up to<br>two-steps reasoning due to the simplicity of those<br>stories.</p>",
            "id": 10,
            "page": 1,
            "text": "and overly simplified: (1) The underlying scenes are spatially simple, with only three objects and relations only in four directions. (2) The stories for these scenes are two short, templated sentences, each describing a single relation between two objects. (3) The questions typically require up to two-steps reasoning due to the simplicity of those stories."
        },
        {
            "bounding_box": [
                {
                    "x": 1266,
                    "y": 1344
                },
                {
                    "x": 2196,
                    "y": 1344
                },
                {
                    "x": 2196,
                    "y": 1962
                },
                {
                    "x": 1266,
                    "y": 1962
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='11' style='font-size:18px'>To address these issues, this paper proposes a<br>new dataset, SPARTQA 1 (see Fig. 1). Specifically,<br>(1) SPARTQA is built on NLVR's (Suhr et al., 2017)<br>images containing more objects with richer spatial<br>structures (Fig. 1b). (2) SPARTQA 's stories are<br>more natural, have more sentences, and richer in<br>spatial relations in each sentence. (3) SPARTQA's<br>questions require deeper reasoning and have four<br>types: find relation (FR), find blocks (FB), choose<br>object (CO), and yes/no (YN), which allows for<br>more fine-grained analysis of models' capabilities.</p>",
            "id": 11,
            "page": 1,
            "text": "To address these issues, this paper proposes a new dataset, SPARTQA 1 (see Fig. 1). Specifically, (1) SPARTQA is built on NLVR's (Suhr , 2017) images containing more objects with richer spatial structures (Fig. 1b). (2) SPARTQA 's stories are more natural, have more sentences, and richer in spatial relations in each sentence. (3) SPARTQA's questions require deeper reasoning and have four types: find relation (FR), find blocks (FB), choose object (CO), and yes/no (YN), which allows for more fine-grained analysis of models' capabilities."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 1967
                },
                {
                    "x": 2197,
                    "y": 1967
                },
                {
                    "x": 2197,
                    "y": 2581
                },
                {
                    "x": 1267,
                    "y": 2581
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='12' style='font-size:18px'>We showed annotators random images from<br>NLVR, and instructed them to describe objects and<br>relationships not exhaustively at the cost of natu-<br>ralness (Sec. 3). In total, we obtained 1.1k unique<br>QA pair annotations on spatial reasoning, evenly<br>distributed among the aforementioned types. Simi-<br>lar to bAbI, we keep this dataset in relatively small<br>scale and suggest to use as little training data as<br>possible. Experiments show that modern LMs (e.g.,<br>BERT) do not perform well in this low-resource<br>setting.</p>",
            "id": 12,
            "page": 1,
            "text": "We showed annotators random images from NLVR, and instructed them to describe objects and relationships not exhaustively at the cost of naturalness (Sec. 3). In total, we obtained 1.1k unique QA pair annotations on spatial reasoning, evenly distributed among the aforementioned types. Similar to bAbI, we keep this dataset in relatively small scale and suggest to use as little training data as possible. Experiments show that modern LMs (e.g., BERT) do not perform well in this low-resource setting."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 2588
                },
                {
                    "x": 2197,
                    "y": 2588
                },
                {
                    "x": 2197,
                    "y": 3149
                },
                {
                    "x": 1267,
                    "y": 3149
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='13' style='font-size:16px'>This paper thus proposes a way to obtain distant<br>supervision signals for spatial reasoning (Sec. 4).<br>As spatial relationships are rarely mentioned in ex-<br>isting corpora, we take advantage of the fact that<br>spatial language is grounded to the geometry of vi-<br>sual scenes. We are able to automatically generate<br>stories for NLVR images (Suhr et al., 2017) via<br>our newly designed context free grammars (CFG)<br>and context-sensitive rules. In the process of story<br>generation, we store the information about all ob-</p>",
            "id": 13,
            "page": 1,
            "text": "This paper thus proposes a way to obtain distant supervision signals for spatial reasoning (Sec. 4). As spatial relationships are rarely mentioned in existing corpora, we take advantage of the fact that spatial language is grounded to the geometry of visual scenes. We are able to automatically generate stories for NLVR images (Suhr , 2017) via our newly designed context free grammars (CFG) and context-sensitive rules. In the process of story generation, we store the information about all ob-"
        },
        {
            "bounding_box": [
                {
                    "x": 1322,
                    "y": 3176
                },
                {
                    "x": 2109,
                    "y": 3176
                },
                {
                    "x": 2109,
                    "y": 3227
                },
                {
                    "x": 1322,
                    "y": 3227
                }
            ],
            "category": "paragraph",
            "html": "<p id='14' style='font-size:14px'>1SPAtial Reasoning on Textual Question Answering.</p>",
            "id": 14,
            "page": 1,
            "text": "1SPAtial Reasoning on Textual Question Answering."
        },
        {
            "bounding_box": [
                {
                    "x": 56,
                    "y": 1101
                },
                {
                    "x": 150,
                    "y": 1101
                },
                {
                    "x": 150,
                    "y": 2532
                },
                {
                    "x": 56,
                    "y": 2532
                }
            ],
            "category": "footer",
            "html": "<br><footer id='15' style='font-size:14px'>2021<br>Apr<br>12<br>[cs.CL]<br>arXiv:2104.05832v1</footer>",
            "id": 15,
            "page": 1,
            "text": "2021 Apr 12 [cs.CL] arXiv:2104.05832v1"
        },
        {
            "bounding_box": [
                {
                    "x": 318,
                    "y": 320
                },
                {
                    "x": 471,
                    "y": 320
                },
                {
                    "x": 471,
                    "y": 365
                },
                {
                    "x": 318,
                    "y": 365
                }
            ],
            "category": "paragraph",
            "html": "<p id='16' style='font-size:16px'>STORY:</p>",
            "id": 16,
            "page": 2,
            "text": "STORY:"
        },
        {
            "bounding_box": [
                {
                    "x": 315,
                    "y": 375
                },
                {
                    "x": 2165,
                    "y": 375
                },
                {
                    "x": 2165,
                    "y": 605
                },
                {
                    "x": 315,
                    "y": 605
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='17' style='font-size:16px'>We have three blocks, A, B and C. Block B is to the right of block C and it is below block A. Block A has two black<br>medium squares. Medium black square number one is below medium black square number two and a medium blue<br>square. It is touching the bottom edge of this block. The medium blue square is below medium black square number<br>two. Block B contains one medium black square. Block C contains one medium blue square and one medium black<br>square. The medium blue square is below the medium black square.</p>",
            "id": 17,
            "page": 2,
            "text": "We have three blocks, A, B and C. Block B is to the right of block C and it is below block A. Block A has two black medium squares. Medium black square number one is below medium black square number two and a medium blue square. It is touching the bottom edge of this block. The medium blue square is below medium black square number two. Block B contains one medium black square. Block C contains one medium blue square and one medium black square. The medium blue square is below the medium black square."
        },
        {
            "bounding_box": [
                {
                    "x": 321,
                    "y": 638
                },
                {
                    "x": 568,
                    "y": 638
                },
                {
                    "x": 568,
                    "y": 681
                },
                {
                    "x": 321,
                    "y": 681
                }
            ],
            "category": "paragraph",
            "html": "<p id='18' style='font-size:16px'>QUESTIONS:</p>",
            "id": 18,
            "page": 2,
            "text": "QUESTIONS:"
        },
        {
            "bounding_box": [
                {
                    "x": 317,
                    "y": 680
                },
                {
                    "x": 2169,
                    "y": 680
                },
                {
                    "x": 2169,
                    "y": 1114
                },
                {
                    "x": 317,
                    "y": 1114
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='19' style='font-size:14px'>FB: Which block(s) has a medium thing that is below a black square? A, B, C<br>FB: Which block(s) doesn't have any blue square that is to the left of a medium square? A, B<br>FR: What is the relation between the medium black square which is in block C and the medium square that is below a<br>medium black square that is touching the bottom edge of a block? Left<br>CO: Which object is above a medium black square? the medium black square which is in block C or medium black<br>square number two? medium black square number two<br>YN: Is there a square that is below medium square number two above all medium black squares that are touching the<br>bottom edge of a block? Yes</p>",
            "id": 19,
            "page": 2,
            "text": "FB: Which block(s) has a medium thing that is below a black square? A, B, C FB: Which block(s) doesn't have any blue square that is to the left of a medium square? A, B FR: What is the relation between the medium black square which is in block C and the medium square that is below a medium black square that is touching the bottom edge of a block? Left CO: Which object is above a medium black square? the medium black square which is in block C or medium black square number two? medium black square number two YN: Is there a square that is below medium square number two above all medium black squares that are touching the bottom edge of a block? Yes"
        },
        {
            "bounding_box": [
                {
                    "x": 753,
                    "y": 1169
                },
                {
                    "x": 1719,
                    "y": 1169
                },
                {
                    "x": 1719,
                    "y": 1214
                },
                {
                    "x": 753,
                    "y": 1214
                }
            ],
            "category": "caption",
            "html": "<caption id='20' style='font-size:16px'>(a) An example story and corresponding questions and answers.</caption>",
            "id": 20,
            "page": 2,
            "text": "(a) An example story and corresponding questions and answers."
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 1218
                },
                {
                    "x": 2173,
                    "y": 1218
                },
                {
                    "x": 2173,
                    "y": 1648
                },
                {
                    "x": 286,
                    "y": 1648
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='21' style='font-size:14px' alt=\"A\nDescribed image\nNLVR image\nC\nchoose some objects and B\nrelations randomly and add\nrelationship between blocks\" data-coord=\"top-left:(286,1218); bottom-right:(2173,1648)\" /></figure>",
            "id": 21,
            "page": 2,
            "text": "A Described image NLVR image C choose some objects and B relations randomly and add relationship between blocks"
        },
        {
            "bounding_box": [
                {
                    "x": 364,
                    "y": 1667
                },
                {
                    "x": 2107,
                    "y": 1667
                },
                {
                    "x": 2107,
                    "y": 1714
                },
                {
                    "x": 364,
                    "y": 1714
                }
            ],
            "category": "caption",
            "html": "<br><caption id='22' style='font-size:16px'>(b) An example NLVR image and the scene created in Fig. 1a, where the blocks in the NLVR image are rearranged.</caption>",
            "id": 22,
            "page": 2,
            "text": "(b) An example NLVR image and the scene created in Fig. 1a, where the blocks in the NLVR image are rearranged."
        },
        {
            "bounding_box": [
                {
                    "x": 624,
                    "y": 1752
                },
                {
                    "x": 1855,
                    "y": 1752
                },
                {
                    "x": 1855,
                    "y": 1804
                },
                {
                    "x": 624,
                    "y": 1804
                }
            ],
            "category": "paragraph",
            "html": "<p id='23' style='font-size:20px'>Figure 1: Example from SPARTQA (specifically from SPARTQA-AUTO)</p>",
            "id": 23,
            "page": 2,
            "text": "Figure 1: Example from SPARTQA (specifically from SPARTQA-AUTO)"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 1894
                },
                {
                    "x": 1215,
                    "y": 1894
                },
                {
                    "x": 1215,
                    "y": 2287
                },
                {
                    "x": 287,
                    "y": 2287
                }
            ],
            "category": "paragraph",
            "html": "<p id='24' style='font-size:20px'>jects and relationships, such that QA pairs can also<br>be generated automatically. In contrast to bAbI,<br>we use various spatial rules to infer new relation-<br>ships in these QA pairs, which requires more com-<br>plex reasoning capabilities. Hereafter, we call this<br>automatically-generated dataset SPARTQA-AUTO,<br>and the human-annotated one SPARTQA-HUMAN.</p>",
            "id": 24,
            "page": 2,
            "text": "jects and relationships, such that QA pairs can also be generated automatically. In contrast to bAbI, we use various spatial rules to infer new relationships in these QA pairs, which requires more complex reasoning capabilities. Hereafter, we call this automatically-generated dataset SPARTQA-AUTO, and the human-annotated one SPARTQA-HUMAN."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 2295
                },
                {
                    "x": 1214,
                    "y": 2295
                },
                {
                    "x": 1214,
                    "y": 2916
                },
                {
                    "x": 287,
                    "y": 2916
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='25' style='font-size:20px'>Experiments show that, by further pretraining on<br>SPARTQA-AUTO, we improve LMs' performance<br>on SPARTQA-HUMAN by a large margin.2 The<br>spatially-improved LMs also show stronger per-<br>formance on two external QA datasets, bAbI and<br>boolQ (Clark et al., 2019): BERT further pretrained<br>on SPARTQA-AUTO only requires half of the train-<br>ing data to achieve 99% accuracy on bAbI as com-<br>pared to the original BERT; on boolQ's develop-<br>ment set, this model shows better performance than<br>BERT, with 2.3% relative error reduction. 3</p>",
            "id": 25,
            "page": 2,
            "text": "Experiments show that, by further pretraining on SPARTQA-AUTO, we improve LMs' performance on SPARTQA-HUMAN by a large margin.2 The spatially-improved LMs also show stronger performance on two external QA datasets, bAbI and boolQ (Clark , 2019): BERT further pretrained on SPARTQA-AUTO only requires half of the training data to achieve 99% accuracy on bAbI as compared to the original BERT; on boolQ's development set, this model shows better performance than BERT, with 2.3% relative error reduction. 3"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 2970
                },
                {
                    "x": 1214,
                    "y": 2970
                },
                {
                    "x": 1214,
                    "y": 3161
                },
                {
                    "x": 288,
                    "y": 3161
                }
            ],
            "category": "paragraph",
            "html": "<p id='26' style='font-size:14px'>2Further pretraining LMs has become a common prac-<br>tice and baseline method for transferring knowledge between<br>tasks (Phang et al., 2018; Zhou et al., 2020). We leave more<br>advanced methods for future work.<br>3To the best of the leaderboard</p>",
            "id": 26,
            "page": 2,
            "text": "2Further pretraining LMs has become a common practice and baseline method for transferring knowledge between tasks (Phang , 2018; Zhou , 2020). We leave more advanced methods for future work. 3To the best of the leaderboard"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 3138
                },
                {
                    "x": 1211,
                    "y": 3138
                },
                {
                    "x": 1211,
                    "y": 3225
                },
                {
                    "x": 287,
                    "y": 3225
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='27' style='font-size:14px'>our knowledge, test set or<br>of boolQ has not been released yet.</p>",
            "id": 27,
            "page": 2,
            "text": "our knowledge, test set or of boolQ has not been released yet."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 1892
                },
                {
                    "x": 2198,
                    "y": 1892
                },
                {
                    "x": 2198,
                    "y": 2172
                },
                {
                    "x": 1268,
                    "y": 2172
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='28' style='font-size:20px'>Our contributions can be summarized as fol-<br>lows. First, we propose the first human-curated<br>benchmark, SPARTQA-HUMAN, for spatial rea-<br>soning with richer spatial phenomena than the prior<br>synthetic dataset bAbI (Task 17).</p>",
            "id": 28,
            "page": 2,
            "text": "Our contributions can be summarized as follows. First, we propose the first human-curated benchmark, SPARTQA-HUMAN, for spatial reasoning with richer spatial phenomena than the prior synthetic dataset bAbI (Task 17)."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 2180
                },
                {
                    "x": 2197,
                    "y": 2180
                },
                {
                    "x": 2197,
                    "y": 2457
                },
                {
                    "x": 1267,
                    "y": 2457
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='29' style='font-size:20px'>Second, we exploit the scene structure of images<br>and design novel CFGs and spatial reasoning rules<br>to automatically generate data (i.e., SPARTQA-<br>AUTO) to obtain distant supervision signals for<br>spatial reasoning over text.</p>",
            "id": 29,
            "page": 2,
            "text": "Second, we exploit the scene structure of images and design novel CFGs and spatial reasoning rules to automatically generate data (i.e., SPARTQAAUTO) to obtain distant supervision signals for spatial reasoning over text."
        },
        {
            "bounding_box": [
                {
                    "x": 1266,
                    "y": 2464
                },
                {
                    "x": 2198,
                    "y": 2464
                },
                {
                    "x": 2198,
                    "y": 2690
                },
                {
                    "x": 1266,
                    "y": 2690
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='30' style='font-size:18px'>Third, SPARTQA-AUTO proves to be a rich<br>source of spatial knowledge that improved the per-<br>formance of LMs on SPARTQA-HUMAN as well as<br>on different data domains such as bAbI and boolQ.</p>",
            "id": 30,
            "page": 2,
            "text": "Third, SPARTQA-AUTO proves to be a rich source of spatial knowledge that improved the performance of LMs on SPARTQA-HUMAN as well as on different data domains such as bAbI and boolQ."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 2736
                },
                {
                    "x": 1641,
                    "y": 2736
                },
                {
                    "x": 1641,
                    "y": 2793
                },
                {
                    "x": 1268,
                    "y": 2793
                }
            ],
            "category": "paragraph",
            "html": "<p id='31' style='font-size:22px'>2 Related work</p>",
            "id": 31,
            "page": 2,
            "text": "2 Related work"
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 2834
                },
                {
                    "x": 2199,
                    "y": 2834
                },
                {
                    "x": 2199,
                    "y": 3229
                },
                {
                    "x": 1267,
                    "y": 3229
                }
            ],
            "category": "paragraph",
            "html": "<p id='32' style='font-size:20px'>Question answering is a useful format to evalu-<br>ate machines' capability of reading comprehen-<br>sion (Gardner et al., 2019) and many recent works<br>have been implementing this strategy to test ma-<br>chines' understanding of linguistic formalisms: He<br>et al. (2015); Michael et al. (2018); Levy et al.<br>(2017); Jia et al. (2018); Ning et al. (2020); Du</p>",
            "id": 32,
            "page": 2,
            "text": "Question answering is a useful format to evaluate machines' capability of reading comprehension (Gardner , 2019) and many recent works have been implementing this strategy to test machines' understanding of linguistic formalisms: He  (2015); Michael  (2018); Levy  (2017); Jia  (2018); Ning  (2020); Du"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 301
                },
                {
                    "x": 1215,
                    "y": 301
                },
                {
                    "x": 1215,
                    "y": 636
                },
                {
                    "x": 287,
                    "y": 636
                }
            ],
            "category": "paragraph",
            "html": "<p id='33' style='font-size:20px'>and Cardie (2020). An important advantage of QA<br>is using natural language to annotate natural lan-<br>guage, thus having the flexibility to get annotations<br>on complex phenomena such as spatial reasoning.<br>However, spatial reasoning phenomena have been<br>covered minimally in the existing works.</p>",
            "id": 33,
            "page": 3,
            "text": "and Cardie (2020). An important advantage of QA is using natural language to annotate natural language, thus having the flexibility to get annotations on complex phenomena such as spatial reasoning. However, spatial reasoning phenomena have been covered minimally in the existing works."
        },
        {
            "bounding_box": [
                {
                    "x": 1497,
                    "y": 289
                },
                {
                    "x": 1968,
                    "y": 289
                },
                {
                    "x": 1968,
                    "y": 522
                },
                {
                    "x": 1497,
                    "y": 522
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='34' style='font-size:14px' alt=\"2\" data-coord=\"top-left:(1497,289); bottom-right:(1968,522)\" /></figure>",
            "id": 34,
            "page": 3,
            "text": "2"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 644
                },
                {
                    "x": 1213,
                    "y": 644
                },
                {
                    "x": 1213,
                    "y": 1259
                },
                {
                    "x": 287,
                    "y": 1259
                }
            ],
            "category": "paragraph",
            "html": "<p id='35' style='font-size:20px'>To the best of our knowledge, Task 17 of the<br>bAbI project (Weston et al., 2015) is the only QA<br>dataset focused on textual spatial reasoning (exam-<br>ples in Appendix F). However, bAbI is synthetic<br>and does not reflect the complexity of the spatial<br>reasoning in natural language. Solving Task 17<br>of bAbI typically does not require sophisticated<br>reasoning, which is an important capability empha-<br>sized by more recent works (e.g., Dua et al. (2019);<br>Khashabi et al. (2018); Yang et al. (2018); Dasigi<br>et al. (2019); Ning et al. (2020)).</p>",
            "id": 35,
            "page": 3,
            "text": "To the best of our knowledge, Task 17 of the bAbI project (Weston , 2015) is the only QA dataset focused on textual spatial reasoning (examples in Appendix F). However, bAbI is synthetic and does not reflect the complexity of the spatial reasoning in natural language. Solving Task 17 of bAbI typically does not require sophisticated reasoning, which is an important capability emphasized by more recent works (e.g., Dua  (2019); Khashabi  (2018); Yang  (2018); Dasigi  (2019); Ning  (2020))."
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 1270
                },
                {
                    "x": 1214,
                    "y": 1270
                },
                {
                    "x": 1214,
                    "y": 1941
                },
                {
                    "x": 286,
                    "y": 1941
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='36' style='font-size:20px'>Spatial reasoning is arguably more prominent in<br>multi-modal QA benchmarks, e.g., NLVR (Suhr<br>et al., 2017), VQA (Antol et al., 2015), GQA (Hud-<br>son and Manning, 2019), CLEVR (Johnson et al.,<br>2017). However, those spatial reasoning phenom-<br>ena are mostly expressed naturally through images,<br>while this paper focuses on studying spatial rea-<br>soning on natural language. Some other works on<br>visual-spatial reasoning are based on geographi-<br>cal information inside maps and diagrams (Huang<br>et al., 2019) and navigational instructions (Chen<br>et al., 2019; Anderson et al., 2018).</p>",
            "id": 36,
            "page": 3,
            "text": "Spatial reasoning is arguably more prominent in multi-modal QA benchmarks, e.g., NLVR (Suhr , 2017), VQA (Antol , 2015), GQA (Hudson and Manning, 2019), CLEVR (Johnson , 2017). However, those spatial reasoning phenomena are mostly expressed naturally through images, while this paper focuses on studying spatial reasoning on natural language. Some other works on visual-spatial reasoning are based on geographical information inside maps and diagrams (Huang , 2019) and navigational instructions (Chen , 2019; Anderson , 2018)."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 1951
                },
                {
                    "x": 1214,
                    "y": 1951
                },
                {
                    "x": 1214,
                    "y": 2343
                },
                {
                    "x": 288,
                    "y": 2343
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='37' style='font-size:18px'>As another approach to evaluate spatial reason-<br>ing capabilities of models, a dataset proposed in<br>Ghanimifard and Dobnik (2017) generates a syn-<br>thetic training set of spatial sentences and evaluates<br>the models' ability to generate spatial facts and sen-<br>tences containing composition and decomposition<br>of relations on grounded objects.</p>",
            "id": 37,
            "page": 3,
            "text": "As another approach to evaluate spatial reasoning capabilities of models, a dataset proposed in Ghanimifard and Dobnik (2017) generates a synthetic training set of spatial sentences and evaluates the models' ability to generate spatial facts and sentences containing composition and decomposition of relations on grounded objects."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 2394
                },
                {
                    "x": 793,
                    "y": 2394
                },
                {
                    "x": 793,
                    "y": 2451
                },
                {
                    "x": 288,
                    "y": 2451
                }
            ],
            "category": "paragraph",
            "html": "<p id='38' style='font-size:22px'>3 SPARTQA-HUMAN</p>",
            "id": 38,
            "page": 3,
            "text": "3 SPARTQA-HUMAN"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 2493
                },
                {
                    "x": 1214,
                    "y": 2493
                },
                {
                    "x": 1214,
                    "y": 2770
                },
                {
                    "x": 288,
                    "y": 2770
                }
            ],
            "category": "paragraph",
            "html": "<p id='39' style='font-size:18px'>To mitigate the aforementioned problems of Task<br>17 of bAbI, i.e., simple scenes, stories, and ques-<br>tions, we describe the data annotation process of<br>SPARTQA-HUMAN, and explain how those prob-<br>lems were addressed in this section.</p>",
            "id": 39,
            "page": 3,
            "text": "To mitigate the aforementioned problems of Task 17 of bAbI, i.e., simple scenes, stories, and questions, we describe the data annotation process of SPARTQA-HUMAN, and explain how those problems were addressed in this section."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 2779
                },
                {
                    "x": 1213,
                    "y": 2779
                },
                {
                    "x": 1213,
                    "y": 3227
                },
                {
                    "x": 288,
                    "y": 3227
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='40' style='font-size:18px'>First, we randomly selected a subset of NLVR<br>images, each of which has three blocks containing<br>multiple objects (see Fig 1b). The scenes shown by<br>these images are more complicated than those de-<br>scribed by bAbI because (1) there are more objects<br>in NLVR images; (2) the spatial relationships in<br>NLVR are not limited to just four relative directions<br>as objects are placed arbitrarily within blocks.</p>",
            "id": 40,
            "page": 3,
            "text": "First, we randomly selected a subset of NLVR images, each of which has three blocks containing multiple objects (see Fig 1b). The scenes shown by these images are more complicated than those described by bAbI because (1) there are more objects in NLVR images; (2) the spatial relationships in NLVR are not limited to just four relative directions as objects are placed arbitrarily within blocks."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 552
                },
                {
                    "x": 2197,
                    "y": 552
                },
                {
                    "x": 2197,
                    "y": 806
                },
                {
                    "x": 1267,
                    "y": 806
                }
            ],
            "category": "caption",
            "html": "<br><caption id='41' style='font-size:16px'>Figure 2: For \"A blue circle is above a big triangle. To<br>the left of the big triangle, there is a square,\" if the ques-<br>tion is: \"Is the square to the left of the blue circle?\", the<br>answer is neither Yes nor No. Thus, the correct answer<br>is \"Do not Know\" (DK) in our setting.</caption>",
            "id": 41,
            "page": 3,
            "text": "Figure 2: For \"A blue circle is above a big triangle. To the left of the big triangle, there is a square,\" if the question is: \"Is the square to the left of the blue circle?\", the answer is neither Yes nor No. Thus, the correct answer is \"Do not Know\" (DK) in our setting."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 897
                },
                {
                    "x": 2197,
                    "y": 897
                },
                {
                    "x": 2197,
                    "y": 1516
                },
                {
                    "x": 1267,
                    "y": 1516
                }
            ],
            "category": "paragraph",
            "html": "<p id='42' style='font-size:18px'>Second, two student volunteers produced tex-<br>tual description of those objects and their corre-<br>sponding spatial relationships based on these im-<br>ages. Since the blocks are always horizontally<br>aligned in each NLVR image, to allow for more<br>flexibility, annotators could also rearrange these<br>blocks (see Fig. 1a). Relationships between ob-<br>jects within the same block can take the forms of<br>relative direction (e.g., left or above), qualitative<br>distance (e.g., near or far), and topological relation-<br>ship (e.g., touching or containing).</p>",
            "id": 42,
            "page": 3,
            "text": "Second, two student volunteers produced textual description of those objects and their corresponding spatial relationships based on these images. Since the blocks are always horizontally aligned in each NLVR image, to allow for more flexibility, annotators could also rearrange these blocks (see Fig. 1a). Relationships between objects within the same block can take the forms of relative direction (e.g., left or above), qualitative distance (e.g., near or far), and topological relationship (e.g., touching or containing)."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 1522
                },
                {
                    "x": 2198,
                    "y": 1522
                },
                {
                    "x": 2198,
                    "y": 1855
                },
                {
                    "x": 1267,
                    "y": 1855
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='43' style='font-size:18px'>However, we instructed the annotators not to de-<br>scribe all objects and relationships, (1) to avoid un-<br>necessarily verbose stories, and (2) to intentionally<br>miss some information to enable more complex rea-<br>soning later. Therefore, annotators describe only a<br>random subset of blocks, objects, and relationships.</p>",
            "id": 43,
            "page": 3,
            "text": "However, we instructed the annotators not to describe all objects and relationships, (1) to avoid unnecessarily verbose stories, and (2) to intentionally miss some information to enable more complex reasoning later. Therefore, annotators describe only a random subset of blocks, objects, and relationships."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 1862
                },
                {
                    "x": 2197,
                    "y": 1862
                },
                {
                    "x": 2197,
                    "y": 2360
                },
                {
                    "x": 1267,
                    "y": 2360
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='44' style='font-size:18px'>To query more interesting phenomena, annota-<br>tors were then encouraged to write questions requir-<br>ing detecting relations and reasoning over them<br>using multiple spatial rules. A spatial rule can<br>be one of the transitivity (A → B, B → C ⇒<br>A → C), symmetry (A → B ⇒ B → A), con-<br>verse ((A, R, B) ⇒ (B, reverse(R), A)), inclu-<br>sion (obj1 in A), and exclusion (obj1 not in B)<br>rules.</p>",
            "id": 44,
            "page": 3,
            "text": "To query more interesting phenomena, annotators were then encouraged to write questions requiring detecting relations and reasoning over them using multiple spatial rules. A spatial rule can be one of the transitivity (A → B, B → C ⇒ A → C), symmetry (A → B ⇒ B → A), converse ((A, R, B) ⇒ (B, reverse(R), A)), inclusion (obj1 in A), and exclusion (obj1 not in B) rules."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 2370
                },
                {
                    "x": 2197,
                    "y": 2370
                },
                {
                    "x": 2197,
                    "y": 2758
                },
                {
                    "x": 1267,
                    "y": 2758
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='45' style='font-size:18px'>There are four types of questions (Q-TYPE). (1)<br>FR: find relation between two objects. (2) FB: find<br>the block that contains certain object(s). (3) CO:<br>choose between two objects mentioned in the ques-<br>tion that meets certain criteria. (4) YN: a yes/no<br>question that tests if a claim on spatial relationship<br>holds.</p>",
            "id": 45,
            "page": 3,
            "text": "There are four types of questions (Q-TYPE). (1) FR: find relation between two objects. (2) FB: find the block that contains certain object(s). (3) CO: choose between two objects mentioned in the question that meets certain criteria. (4) YN: a yes/no question that tests if a claim on spatial relationship holds."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 2765
                },
                {
                    "x": 2197,
                    "y": 2765
                },
                {
                    "x": 2197,
                    "y": 3102
                },
                {
                    "x": 1267,
                    "y": 3102
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='46' style='font-size:18px'>FB, FR, and CO questions are formulated as<br>multiple-choice questions4 and receive a list of can-<br>didate answers, and YN questions' answer is choos-<br>ing from Yes, No, or \"DK\" (Do not Know). The<br>\"DK\" option is due to the open-world assumption<br>of the stories, where if something is not described</p>",
            "id": 46,
            "page": 3,
            "text": "FB, FR, and CO questions are formulated as multiple-choice questions4 and receive a list of candidate answers, and YN questions' answer is choosing from Yes, No, or \"DK\" (Do not Know). The \"DK\" option is due to the open-world assumption of the stories, where if something is not described"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 3138
                },
                {
                    "x": 2196,
                    "y": 3138
                },
                {
                    "x": 2196,
                    "y": 3223
                },
                {
                    "x": 1268,
                    "y": 3223
                }
            ],
            "category": "paragraph",
            "html": "<p id='47' style='font-size:14px'>4CO can be considered as both single-choice and multiple-<br>choices question.</p>",
            "id": 47,
            "page": 3,
            "text": "4CO can be considered as both single-choice and multiplechoices question."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 285
                },
                {
                    "x": 1203,
                    "y": 285
                },
                {
                    "x": 1203,
                    "y": 674
                },
                {
                    "x": 290,
                    "y": 674
                }
            ],
            "category": "table",
            "html": "<table id='48' style='font-size:14px'><tr><td>Sets</td><td>FB</td><td>FR</td><td>YN</td><td>CO</td><td>Total</td></tr><tr><td>SPARTQA-HUMAN:</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Test</td><td>104</td><td>105</td><td>194</td><td>107</td><td>510</td></tr><tr><td>Train</td><td>154</td><td>149</td><td>162</td><td>151</td><td>616</td></tr><tr><td>SPARTQA-AUTO:</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Seen Test</td><td>3872</td><td>3712</td><td>3896</td><td>3594</td><td>15074</td></tr><tr><td>Unseen Test</td><td>3872</td><td>3721</td><td>3896</td><td>3598</td><td>15087</td></tr><tr><td>Dev</td><td>3842</td><td>3742</td><td>3860</td><td>3579</td><td>15023</td></tr><tr><td>Train</td><td>23654</td><td>23302</td><td>23968</td><td>22794</td><td>93673</td></tr></table>",
            "id": 48,
            "page": 4,
            "text": "Sets FB FR YN CO Total  SPARTQA-HUMAN:       Test 104 105 194 107 510  Train 154 149 162 151 616  SPARTQA-AUTO:       Seen Test 3872 3712 3896 3594 15074  Unseen Test 3872 3721 3896 3598 15087  Dev 3842 3742 3860 3579 15023  Train 23654 23302 23968 22794"
        },
        {
            "bounding_box": [
                {
                    "x": 388,
                    "y": 709
                },
                {
                    "x": 1110,
                    "y": 709
                },
                {
                    "x": 1110,
                    "y": 754
                },
                {
                    "x": 388,
                    "y": 754
                }
            ],
            "category": "caption",
            "html": "<caption id='49' style='font-size:18px'>Table 1: Number of questions per Q-TYPE</caption>",
            "id": 49,
            "page": 4,
            "text": "Table 1: Number of questions per Q-TYPE"
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 854
                },
                {
                    "x": 1211,
                    "y": 854
                },
                {
                    "x": 1211,
                    "y": 903
                },
                {
                    "x": 289,
                    "y": 903
                }
            ],
            "category": "paragraph",
            "html": "<p id='50' style='font-size:20px'>in the text, it is not considered as false (See Fig. 2).</p>",
            "id": 50,
            "page": 4,
            "text": "in the text, it is not considered as false (See Fig. 2)."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 900
                },
                {
                    "x": 1213,
                    "y": 900
                },
                {
                    "x": 1213,
                    "y": 1980
                },
                {
                    "x": 287,
                    "y": 1980
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='51' style='font-size:20px'>Finally, annotators were able to create 1.1k QA<br>pairs on spatial reasoning on the generated descrip-<br>tions, distributed among the aforementioned types.<br>We intentionally keep this data in a relatively small<br>scale due to two reasons. First, there has been some<br>consensus in our community that modern systems,<br>given their sufficiently large model capacities, can<br>easily find shortcuts and overfit a dataset if pro-<br>vided with a large training data (Gardner et al.,<br>2020; Sen and Saffari, 2020). Second, collecting<br>spatial reasoning QAs is very costly: The two an-<br>notators spent 45-60 mins on average to create a<br>single story with 8-16 QA pairs. We estimate that<br>SPARTQA-HUMAN costed about 100 human hours<br>in total. The expert performance on 100 examples<br>of SPARTQA-HUMAN's test set measured by their<br>accuracy of answering the questions is 92% across<br>four Q-TYPEs on average, indicating its high qual-<br>ity.</p>",
            "id": 51,
            "page": 4,
            "text": "Finally, annotators were able to create 1.1k QA pairs on spatial reasoning on the generated descriptions, distributed among the aforementioned types. We intentionally keep this data in a relatively small scale due to two reasons. First, there has been some consensus in our community that modern systems, given their sufficiently large model capacities, can easily find shortcuts and overfit a dataset if provided with a large training data (Gardner , 2020; Sen and Saffari, 2020). Second, collecting spatial reasoning QAs is very costly: The two annotators spent 45-60 mins on average to create a single story with 8-16 QA pairs. We estimate that SPARTQA-HUMAN costed about 100 human hours in total. The expert performance on 100 examples of SPARTQA-HUMAN's test set measured by their accuracy of answering the questions is 92% across four Q-TYPEs on average, indicating its high quality."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 2025
                },
                {
                    "x": 1195,
                    "y": 2025
                },
                {
                    "x": 1195,
                    "y": 2084
                },
                {
                    "x": 287,
                    "y": 2084
                }
            ],
            "category": "paragraph",
            "html": "<p id='52' style='font-size:22px'>4 Distant Supervision: SPARTQA-AUTO</p>",
            "id": 52,
            "page": 4,
            "text": "4 Distant Supervision: SPARTQA-AUTO"
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 2121
                },
                {
                    "x": 1214,
                    "y": 2121
                },
                {
                    "x": 1214,
                    "y": 3023
                },
                {
                    "x": 286,
                    "y": 3023
                }
            ],
            "category": "paragraph",
            "html": "<p id='53' style='font-size:20px'>Since human annotations are costly, it is impor-<br>tant to investigate ways to generate distant super-<br>vision signals for spatial reasoning. However, un-<br>like conventional distant supervision approaches<br>(e.g., Mintz et al. (2009); Zeng et al. (2015); Zhou<br>et al. (2020)) where distant supervision data can<br>be selected from large corpora by implementing<br>specialized filtering rules, spatial reasoning does<br>not appear often in existing corpora. Therefore,<br>similar to SPARTQA-HUMAN, we take advantage<br>of the ground truth of NLVR images, design CFGs<br>to generate stories, and use spatial reasoning rules<br>to ask and answer spatial reasoning questions. This<br>automatically generated data is called SPARTQA-<br>AUTO, and below we describe its generation pro-<br>cess in detail.</p>",
            "id": 53,
            "page": 4,
            "text": "Since human annotations are costly, it is important to investigate ways to generate distant supervision signals for spatial reasoning. However, unlike conventional distant supervision approaches (e.g., Mintz  (2009); Zeng  (2015); Zhou  (2020)) where distant supervision data can be selected from large corpora by implementing specialized filtering rules, spatial reasoning does not appear often in existing corpora. Therefore, similar to SPARTQA-HUMAN, we take advantage of the ground truth of NLVR images, design CFGs to generate stories, and use spatial reasoning rules to ask and answer spatial reasoning questions. This automatically generated data is called SPARTQAAUTO, and below we describe its generation process in detail."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 3060
                },
                {
                    "x": 1213,
                    "y": 3060
                },
                {
                    "x": 1213,
                    "y": 3229
                },
                {
                    "x": 287,
                    "y": 3229
                }
            ],
            "category": "paragraph",
            "html": "<p id='54' style='font-size:20px'>Story generation Since NLVR comes with struc-<br>tured descriptions of the ground truth locations<br>of those objects, we were able to choose random</p>",
            "id": 54,
            "page": 4,
            "text": "Story generation Since NLVR comes with structured descriptions of the ground truth locations of those objects, we were able to choose random"
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 303
                },
                {
                    "x": 2196,
                    "y": 303
                },
                {
                    "x": 2196,
                    "y": 634
                },
                {
                    "x": 1267,
                    "y": 634
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='55' style='font-size:20px'>blocks and objects from each image programmat-<br>ically. The benefit is two-fold. First, a random<br>selection of blocks and objects allows us to cre-<br>ate multiple stories for each image; second, this<br>randomness also creates spatial reasoning opportu-<br>nities with missing information.</p>",
            "id": 55,
            "page": 4,
            "text": "blocks and objects from each image programmatically. The benefit is two-fold. First, a random selection of blocks and objects allows us to create multiple stories for each image; second, this randomness also creates spatial reasoning opportunities with missing information."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 643
                },
                {
                    "x": 2195,
                    "y": 643
                },
                {
                    "x": 2195,
                    "y": 973
                },
                {
                    "x": 1267,
                    "y": 973
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='56' style='font-size:20px'>Once we decide on a set of blocks and objects<br>to be included, we determine their relationships:<br>Those relationships between blocks are generated<br>randomly; as for those between objects, we refer<br>to the ground truth of these images to determine<br>them.</p>",
            "id": 56,
            "page": 4,
            "text": "Once we decide on a set of blocks and objects to be included, we determine their relationships: Those relationships between blocks are generated randomly; as for those between objects, we refer to the ground truth of these images to determine them."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 981
                },
                {
                    "x": 2195,
                    "y": 981
                },
                {
                    "x": 2195,
                    "y": 1427
                },
                {
                    "x": 1268,
                    "y": 1427
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='57' style='font-size:20px'>Now we have a scene containing a set of blocks<br>and objects and their associated relationships. To<br>produce a story for this scene, we design CFGs to<br>produce natural language sentences that describe<br>those blocks/objects/relationships in various ex-<br>pressions (see Fig. 3 for two portions of our CFG<br>describing relative and nested relations between<br>objects).</p>",
            "id": 57,
            "page": 4,
            "text": "Now we have a scene containing a set of blocks and objects and their associated relationships. To produce a story for this scene, we design CFGs to produce natural language sentences that describe those blocks/objects/relationships in various expressions (see Fig. 3 for two portions of our CFG describing relative and nested relations between objects)."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 1478
                },
                {
                    "x": 2115,
                    "y": 1478
                },
                {
                    "x": 2115,
                    "y": 1525
                },
                {
                    "x": 1275,
                    "y": 1525
                }
            ],
            "category": "paragraph",
            "html": "<p id='58' style='font-size:18px'>The big black shape is above the medium triangle.</p>",
            "id": 58,
            "page": 4,
            "text": "The big black shape is above the medium triangle."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1558
                },
                {
                    "x": 2166,
                    "y": 1558
                },
                {
                    "x": 2166,
                    "y": 1598
                },
                {
                    "x": 1273,
                    "y": 1598
                }
            ],
            "category": "paragraph",
            "html": "<p id='59' style='font-size:14px'>S → <Article> <Object> is <Relation> <Article> <Object>.</p>",
            "id": 59,
            "page": 4,
            "text": "S → <Article> <Object> is <Relation> <Article> <Object>."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 1644
                },
                {
                    "x": 2073,
                    "y": 1644
                },
                {
                    "x": 2073,
                    "y": 1938
                },
                {
                    "x": 1274,
                    "y": 1938
                }
            ],
            "category": "paragraph",
            "html": "<p id='60' style='font-size:14px'>Article the I a<br>Relation above I left I<br>Object <Size> * <Color>* <Shape| Ind_shape><br>Size small I medium I big<br>Color yellow I blue I black<br>Shape square I triangle I circle<br>Ind shape shape I object | thing</p>",
            "id": 60,
            "page": 4,
            "text": "Article the I a Relation above I left I Object <Size> * <Color>* <Shape| Ind_shape> Size small I medium I big Color yellow I blue I black Shape square I triangle I circle Ind shape shape I object | thing"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2001
                },
                {
                    "x": 2185,
                    "y": 2001
                },
                {
                    "x": 2185,
                    "y": 2046
                },
                {
                    "x": 1272,
                    "y": 2046
                }
            ],
            "category": "paragraph",
            "html": "<p id='61' style='font-size:16px'>(a) Part of the grammar describing relations between objects</p>",
            "id": 61,
            "page": 4,
            "text": "(a) Part of the grammar describing relations between objects"
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 2057
                },
                {
                    "x": 2137,
                    "y": 2057
                },
                {
                    "x": 2137,
                    "y": 2162
                },
                {
                    "x": 1274,
                    "y": 2162
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='62' style='font-size:22px'>The big black shape is above the object that is<br>to the right of the medium triangle</p>",
            "id": 62,
            "page": 4,
            "text": "The big black shape is above the object that is to the right of the medium triangle"
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 2179
                },
                {
                    "x": 2142,
                    "y": 2179
                },
                {
                    "x": 2142,
                    "y": 2271
                },
                {
                    "x": 1275,
                    "y": 2271
                }
            ],
            "category": "paragraph",
            "html": "<p id='63' style='font-size:16px'>S → <Article> <Object> is <Relation> <Article><br><Object>.</p>",
            "id": 63,
            "page": 4,
            "text": "S → <Article> <Object> is <Relation> <Article> <Object>."
        },
        {
            "bounding_box": [
                {
                    "x": 1277,
                    "y": 2321
                },
                {
                    "x": 2153,
                    "y": 2321
                },
                {
                    "x": 2153,
                    "y": 2375
                },
                {
                    "x": 1277,
                    "y": 2375
                }
            ],
            "category": "paragraph",
            "html": "<p id='64' style='font-size:16px'>Object → <Size> * <Color>* <Shape| Ind shape></p>",
            "id": 64,
            "page": 4,
            "text": "Object → <Size> * <Color>* <Shape| Ind shape>"
        },
        {
            "bounding_box": [
                {
                    "x": 1451,
                    "y": 2369
                },
                {
                    "x": 2171,
                    "y": 2369
                },
                {
                    "x": 2171,
                    "y": 2420
                },
                {
                    "x": 1451,
                    "y": 2420
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='65' style='font-size:20px'><Ind _shape> that is <Relation> <Object></p>",
            "id": 65,
            "page": 4,
            "text": "<Ind _shape> that is <Relation> <Object>"
        },
        {
            "bounding_box": [
                {
                    "x": 1307,
                    "y": 2439
                },
                {
                    "x": 2151,
                    "y": 2439
                },
                {
                    "x": 2151,
                    "y": 2487
                },
                {
                    "x": 1307,
                    "y": 2487
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='66' style='font-size:16px'>(b) Part of the grammar describing nested relationships.</p>",
            "id": 66,
            "page": 4,
            "text": "(b) Part of the grammar describing nested relationships."
        },
        {
            "bounding_box": [
                {
                    "x": 1380,
                    "y": 2524
                },
                {
                    "x": 2080,
                    "y": 2524
                },
                {
                    "x": 2080,
                    "y": 2572
                },
                {
                    "x": 1380,
                    "y": 2572
                }
            ],
            "category": "paragraph",
            "html": "<p id='67' style='font-size:18px'>Figure 3: Two parts of our designed CFG</p>",
            "id": 67,
            "page": 4,
            "text": "Figure 3: Two parts of our designed CFG"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 2635
                },
                {
                    "x": 2195,
                    "y": 2635
                },
                {
                    "x": 2195,
                    "y": 3083
                },
                {
                    "x": 1268,
                    "y": 3083
                }
            ],
            "category": "paragraph",
            "html": "<p id='68' style='font-size:20px'>Being grounded to visual scenes guarantees spa-<br>tial coherency in a story, and using CFGs helps to<br>have correct sentences (grammatically) and various<br>expressions. We also design context-sensitive rules<br>to limited options for each CFG's variable based<br>on the chosen entities (e.g. black circle), or what is<br>described in the previous sentences (e.g. Block A<br>has a circle. The circle is below a triangle.)</p>",
            "id": 68,
            "page": 4,
            "text": "Being grounded to visual scenes guarantees spatial coherency in a story, and using CFGs helps to have correct sentences (grammatically) and various expressions. We also design context-sensitive rules to limited options for each CFG's variable based on the chosen entities (e.g. black circle), or what is described in the previous sentences (e.g. Block A has a circle. The circle is below a triangle.)"
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 3116
                },
                {
                    "x": 2194,
                    "y": 3116
                },
                {
                    "x": 2194,
                    "y": 3229
                },
                {
                    "x": 1271,
                    "y": 3229
                }
            ],
            "category": "paragraph",
            "html": "<p id='69' style='font-size:18px'>Question generation To generate questions<br>based on a passage, there are rule-based sys-</p>",
            "id": 69,
            "page": 4,
            "text": "Question generation To generate questions based on a passage, there are rule-based sys-"
        },
        {
            "bounding_box": [
                {
                    "x": 326,
                    "y": 292
                },
                {
                    "x": 1161,
                    "y": 292
                },
                {
                    "x": 1161,
                    "y": 533
                },
                {
                    "x": 326,
                    "y": 533
                }
            ],
            "category": "figure",
            "html": "<figure><img id='70' style='font-size:14px' alt=\"Obj1\nleft\n? (obj1 , obj4)\nleft 스 left => left\nObj4 Obj3 Obj2\nLeft (obj1 , obj2)\nTouching (obj2 , obj3) ~right = left\nLeft (obj1 , obj4)\nRight (obj4 , obj2) Obj4 Obj3\" data-coord=\"top-left:(326,292); bottom-right:(1161,533)\" /></figure>",
            "id": 70,
            "page": 5,
            "text": "Obj1 left ? (obj1 , obj4) left 스 left => left Obj4 Obj3 Obj2 Left (obj1 , obj2) Touching (obj2 , obj3) ~right = left Left (obj1 , obj4) Right (obj4 , obj2) Obj4 Obj3"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 569
                },
                {
                    "x": 1212,
                    "y": 569
                },
                {
                    "x": 1212,
                    "y": 868
                },
                {
                    "x": 287,
                    "y": 868
                }
            ],
            "category": "caption",
            "html": "<caption id='71' style='font-size:16px'>Figure 4: Find the implicit relation between obj1 and<br>obj4 by Transitivity rule. (1) Find a set of objects that<br>have a relation with obj1. Continue the same process<br>on the new set until obj4 is found. (2) Get the union<br>of the intermediate relations between these two objects<br>and it is the final answer.</caption>",
            "id": 71,
            "page": 5,
            "text": "Figure 4: Find the implicit relation between obj1 and obj4 by Transitivity rule. (1) Find a set of objects that have a relation with obj1. Continue the same process on the new set until obj4 is found. (2) Get the union of the intermediate relations between these two objects and it is the final answer."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 965
                },
                {
                    "x": 1213,
                    "y": 965
                },
                {
                    "x": 1213,
                    "y": 1410
                },
                {
                    "x": 287,
                    "y": 1410
                }
            ],
            "category": "paragraph",
            "html": "<p id='72' style='font-size:20px'>tems (Heilman and Smith, 2009; Labutov et al.,<br>2015), neural networks (Du et al., 2017), and their<br>combinations. (Dhole and Manning, 2020). How-<br>ever, in our approach, during generating each story,<br>the program stores the information about the enti-<br>ties and their relationships. Thus, without process-<br>ing the raw text, which is error-prone, we generate<br>questions by only looking at the stored data.</p>",
            "id": 72,
            "page": 5,
            "text": "tems (Heilman and Smith, 2009; Labutov , 2015), neural networks (Du , 2017), and their combinations. (Dhole and Manning, 2020). However, in our approach, during generating each story, the program stores the information about the entities and their relationships. Thus, without processing the raw text, which is error-prone, we generate questions by only looking at the stored data."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 1419
                },
                {
                    "x": 1214,
                    "y": 1419
                },
                {
                    "x": 1214,
                    "y": 1751
                },
                {
                    "x": 288,
                    "y": 1751
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='73' style='font-size:20px'>The question generation operates based on four<br>primary functionalities, Choose-objects, Describe-<br>objects, Find-all-relations, and Find-similar-<br>objects. These modules are responsible to control<br>the logical consistency, correctness, and the num-<br>ber of steps required for reasoning in each question.</p>",
            "id": 73,
            "page": 5,
            "text": "The question generation operates based on four primary functionalities, Choose-objects, Describeobjects, Find-all-relations, and Find-similarobjects. These modules are responsible to control the logical consistency, correctness, and the number of steps required for reasoning in each question."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 1759
                },
                {
                    "x": 1213,
                    "y": 1759
                },
                {
                    "x": 1213,
                    "y": 2034
                },
                {
                    "x": 288,
                    "y": 2034
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='74' style='font-size:18px'>Choose-objects randomly chooses up to three<br>objects from the set of possible objects in a story<br>under a set of constraints such as preventing selec-<br>tion of similar objects, or excluding objects with<br>relations that are directly mentioned in the text.</p>",
            "id": 74,
            "page": 5,
            "text": "Choose-objects randomly chooses up to three objects from the set of possible objects in a story under a set of constraints such as preventing selection of similar objects, or excluding objects with relations that are directly mentioned in the text."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 2042
                },
                {
                    "x": 1212,
                    "y": 2042
                },
                {
                    "x": 1212,
                    "y": 2714
                },
                {
                    "x": 287,
                    "y": 2714
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='75' style='font-size:20px'>Describe-Objects generates a mention phrase for<br>an object using parts of its full name (presented in<br>the story). The generated phrase is either point-<br>ing to a unique object or a group of objects such<br>as \"the big circle, \" \"big circles. \" To describe a<br>or<br>unique object, it chooses an attribute or a group<br>of attributes that apply to a unique object among<br>others in the story. To increase the steps of reason-<br>ing, the description may include the relationship of<br>the object to other objects instead of using a direct<br>unique description. For example, \"the circle which<br>is above the black triangle.</p>",
            "id": 75,
            "page": 5,
            "text": "Describe-Objects generates a mention phrase for an object using parts of its full name (presented in the story). The generated phrase is either pointing to a unique object or a group of objects such as \"the big circle, \" \"big circles. \" To describe a or unique object, it chooses an attribute or a group of attributes that apply to a unique object among others in the story. To increase the steps of reasoning, the description may include the relationship of the object to other objects instead of using a direct unique description. For example, \"the circle which is above the black triangle."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 2720
                },
                {
                    "x": 1213,
                    "y": 2720
                },
                {
                    "x": 1213,
                    "y": 3114
                },
                {
                    "x": 287,
                    "y": 3114
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='76' style='font-size:18px'>Find-all-relations completes the relationship<br>graph between objects by applying a set of spa-<br>tial rules such as transitivity, symmetry, converse,<br>inclusion, and exclusion on top of the direct rela-<br>tions described in the story. As shown in Fig. 4, it<br>does an exhaustive search over all combinations of<br>the relations that link two objects to each other.</p>",
            "id": 76,
            "page": 5,
            "text": "Find-all-relations completes the relationship graph between objects by applying a set of spatial rules such as transitivity, symmetry, converse, inclusion, and exclusion on top of the direct relations described in the story. As shown in Fig. 4, it does an exhaustive search over all combinations of the relations that link two objects to each other."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 3119
                },
                {
                    "x": 1213,
                    "y": 3119
                },
                {
                    "x": 1213,
                    "y": 3227
                },
                {
                    "x": 287,
                    "y": 3227
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='77' style='font-size:20px'>Find-similar-objects finds all the mentions<br>matching a description from the question to objects</p>",
            "id": 77,
            "page": 5,
            "text": "Find-similar-objects finds all the mentions matching a description from the question to objects"
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 303
                },
                {
                    "x": 2193,
                    "y": 303
                },
                {
                    "x": 2193,
                    "y": 521
                },
                {
                    "x": 1267,
                    "y": 521
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='78' style='font-size:20px'>in the story. For instance, for the question \"is there<br>any blue circle above the big blue triangle?\", this<br>module finds all the mentions in the story matching<br>the description \"a blue circle\".</p>",
            "id": 78,
            "page": 5,
            "text": "in the story. For instance, for the question \"is there any blue circle above the big blue triangle?\", this module finds all the mentions in the story matching the description \"a blue circle\"."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 533
                },
                {
                    "x": 2197,
                    "y": 533
                },
                {
                    "x": 2197,
                    "y": 2168
                },
                {
                    "x": 1267,
                    "y": 2168
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='79' style='font-size:20px'>Similar to the SPARTQA-HUMAN, we provide<br>four Q-TYPES FR, FB, CO, and YN. To gener-<br>ate FR questions, we choose two objects using<br>Choose-objects module and question their relation-<br>ships. The YN Q-TYPE is similar to FR, but the<br>question specifies one relationship of interest cho-<br>sen from all relation extracted by Find-all-relations<br>module to be questioned about the objects. Since<br>most of the time, Yes/No questions are simpler<br>problems, we make this question type more com-<br>plex by adding quantifiers (adding \"all\" and \"any\").<br>These quantifiers help to evaluates the models' ca-<br>pability to aggregate relations between more than<br>two objects in the story and do the reasoning over<br>all find relations to find the final answer. In FB<br>Q-TYPE, we mention an object by its indirect re-<br>lation to another object using the nested relation<br>in Describe-objects module and ask to find the<br>blocks containing or not containing this object. Fi-<br>nally, the CO question selects an anchor object<br>(Choose-objects) and specifies a relationship ( us-<br>ing Find-all-relations) in the question. Two other<br>objects are chosen as candidates to check whether<br>the specified relationship holds between them and<br>the anchor object. We tend to force the algorithm to<br>choose objects as candidates that at least have one<br>relationship to the anchor object. To see more de-<br>tails about different question' templates see Table<br>7 in the Appendix.</p>",
            "id": 79,
            "page": 5,
            "text": "Similar to the SPARTQA-HUMAN, we provide four Q-TYPES FR, FB, CO, and YN. To generate FR questions, we choose two objects using Choose-objects module and question their relationships. The YN Q-TYPE is similar to FR, but the question specifies one relationship of interest chosen from all relation extracted by Find-all-relations module to be questioned about the objects. Since most of the time, Yes/No questions are simpler problems, we make this question type more complex by adding quantifiers (adding \"all\" and \"any\"). These quantifiers help to evaluates the models' capability to aggregate relations between more than two objects in the story and do the reasoning over all find relations to find the final answer. In FB Q-TYPE, we mention an object by its indirect relation to another object using the nested relation in Describe-objects module and ask to find the blocks containing or not containing this object. Finally, the CO question selects an anchor object (Choose-objects) and specifies a relationship ( using Find-all-relations) in the question. Two other objects are chosen as candidates to check whether the specified relationship holds between them and the anchor object. We tend to force the algorithm to choose objects as candidates that at least have one relationship to the anchor object. To see more details about different question' templates see Table 7 in the Appendix."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 2220
                },
                {
                    "x": 2196,
                    "y": 2220
                },
                {
                    "x": 2196,
                    "y": 2444
                },
                {
                    "x": 1268,
                    "y": 2444
                }
            ],
            "category": "paragraph",
            "html": "<p id='80' style='font-size:20px'>Answer generation We compute all direct and<br>indirect relationships between objects using Find-<br>all-relations function and based on the Q-TYPES<br>generate the final answer.</p>",
            "id": 80,
            "page": 5,
            "text": "Answer generation We compute all direct and indirect relationships between objects using Findall-relations function and based on the Q-TYPES generate the final answer."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 2454
                },
                {
                    "x": 2195,
                    "y": 2454
                },
                {
                    "x": 2195,
                    "y": 2674
                },
                {
                    "x": 1267,
                    "y": 2674
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='81' style='font-size:16px'>For instance, in YN Q-TYPE if the asked relation<br>exists in the found relations, the answer is \"Yes\",<br>if the inverse relation exists it must be \"No\" , and<br>otherwise, it is \"DK\"5.</p>",
            "id": 81,
            "page": 5,
            "text": "For instance, in YN Q-TYPE if the asked relation exists in the found relations, the answer is \"Yes\", if the inverse relation exists it must be \"No\" , and otherwise, it is \"DK\"5."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 2736
                },
                {
                    "x": 1713,
                    "y": 2736
                },
                {
                    "x": 1713,
                    "y": 2788
                },
                {
                    "x": 1268,
                    "y": 2788
                }
            ],
            "category": "paragraph",
            "html": "<p id='82' style='font-size:22px'>4.1 Corpus Statistics</p>",
            "id": 82,
            "page": 5,
            "text": "4.1 Corpus Statistics"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 2820
                },
                {
                    "x": 2196,
                    "y": 2820
                },
                {
                    "x": 2196,
                    "y": 3042
                },
                {
                    "x": 1268,
                    "y": 3042
                }
            ],
            "category": "paragraph",
            "html": "<p id='83' style='font-size:18px'>We generate the train, dev, and test set splits based<br>on the same splits of the images in the NLVR<br>dataset. On average, each story contains 9 sen-<br>tences (Min:3, Max: 22) and 118 tokens (Min: 66,</p>",
            "id": 83,
            "page": 5,
            "text": "We generate the train, dev, and test set splits based on the same splits of the images in the NLVR dataset. On average, each story contains 9 sentences (Min:3, Max: 22) and 118 tokens (Min: 66,"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 3096
                },
                {
                    "x": 2193,
                    "y": 3096
                },
                {
                    "x": 2193,
                    "y": 3227
                },
                {
                    "x": 1268,
                    "y": 3227
                }
            ],
            "category": "footer",
            "html": "<footer id='84' style='font-size:14px'>5The SPARTQA-AUTO generation code and the file of<br>dataset are available at https : / / github . com/ HLR/<br>SpartQA_generation</footer>",
            "id": 84,
            "page": 5,
            "text": "5The SPARTQA-AUTO generation code and the file of dataset are available at https : / / github . com/ HLR/ SpartQA_generation"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 301
                },
                {
                    "x": 1216,
                    "y": 301
                },
                {
                    "x": 1216,
                    "y": 410
                },
                {
                    "x": 288,
                    "y": 410
                }
            ],
            "category": "paragraph",
            "html": "<p id='85' style='font-size:18px'>Max: 274). Also, the average tokens of each ques-<br>tion (on all Q-TYPE ) is 23 (Min:6, Max: 57).</p>",
            "id": 85,
            "page": 6,
            "text": "Max: 274). Also, the average tokens of each question (on all Q-TYPE ) is 23 (Min:6, Max: 57)."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 417
                },
                {
                    "x": 1212,
                    "y": 417
                },
                {
                    "x": 1212,
                    "y": 636
                },
                {
                    "x": 289,
                    "y": 636
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='86' style='font-size:16px'>Table 1 shows the total number of each question<br>type in SPARTQA-AUTO (Check Appendix to see<br>more statistic information about the labels in Tab<br>8.)</p>",
            "id": 86,
            "page": 6,
            "text": "Table 1 shows the total number of each question type in SPARTQA-AUTO (Check Appendix to see more statistic information about the labels in Tab 8.)"
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 678
                },
                {
                    "x": 1108,
                    "y": 678
                },
                {
                    "x": 1108,
                    "y": 793
                },
                {
                    "x": 289,
                    "y": 793
                }
            ],
            "category": "paragraph",
            "html": "<p id='87' style='font-size:20px'>5 Models for Spatial Reasoning over<br>Language</p>",
            "id": 87,
            "page": 6,
            "text": "5 Models for Spatial Reasoning over Language"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 826
                },
                {
                    "x": 1212,
                    "y": 826
                },
                {
                    "x": 1212,
                    "y": 1443
                },
                {
                    "x": 287,
                    "y": 1443
                }
            ],
            "category": "paragraph",
            "html": "<p id='88' style='font-size:18px'>This section describes the model architectures on<br>different Q-TYPES: FR, YN, FB, and CO. All Q-<br>TYPES can be cast into a sequence classification<br>task, and the three transformer-based LMs tested<br>in this paper, BERT (Devlin et al., 2019), ALBERT<br>(Lan et al., 2020), and XLNet (Yang et al., 2019),<br>can all handle this type of tasks by classifying the<br>representation of [CLS], a special token prepended<br>to each target sequence (see Appendix E). Depend-<br>ing on the Q-TYPE, the input sequence and how<br>we do inference may be different.</p>",
            "id": 88,
            "page": 6,
            "text": "This section describes the model architectures on different Q-TYPES: FR, YN, FB, and CO. All QTYPES can be cast into a sequence classification task, and the three transformer-based LMs tested in this paper, BERT (Devlin , 2019), ALBERT (Lan , 2020), and XLNet (Yang , 2019), can all handle this type of tasks by classifying the representation of [CLS], a special token prepended to each target sequence (see Appendix E). Depending on the Q-TYPE, the input sequence and how we do inference may be different."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 1447
                },
                {
                    "x": 1212,
                    "y": 1447
                },
                {
                    "x": 1212,
                    "y": 2005
                },
                {
                    "x": 287,
                    "y": 2005
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='89' style='font-size:16px'>FR and YN both have a predefined label set as<br>candidate answers, and their input sequences are<br>both the concatenation of a story and a question.<br>While the answer to a YN question is a single label<br>chosen from Yes, No, and DK, FR questions can<br>have multiple correct answers. Therefore, we treat<br>each candidate answer to FR as an independent<br>binary classification problem, and take the union<br>as the final answer. As for YN, we choose the label<br>with the highest confidence (Fig 8b).</p>",
            "id": 89,
            "page": 6,
            "text": "FR and YN both have a predefined label set as candidate answers, and their input sequences are both the concatenation of a story and a question. While the answer to a YN question is a single label chosen from Yes, No, and DK, FR questions can have multiple correct answers. Therefore, we treat each candidate answer to FR as an independent binary classification problem, and take the union as the final answer. As for YN, we choose the label with the highest confidence (Fig 8b)."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 2013
                },
                {
                    "x": 1213,
                    "y": 2013
                },
                {
                    "x": 1213,
                    "y": 2746
                },
                {
                    "x": 287,
                    "y": 2746
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='90' style='font-size:16px'>As the candidate answers to FB and CO are not<br>fixed and depend on each story and its question<br>the input sequences to these Q-TYPES are con-<br>catenated with each candidate answer. Since the<br>defined YN and FR model has moderately less ac-<br>curate results on FB and CO Q-TYPES, we add a<br>LSTM (Hochreiter and Schmidhuber, 1997) layer<br>to improve it. Hence, to find the final answer, we<br>run the model with each candidate answer and then<br>apply an LSTM layer on top of all token represen-<br>tations. Then, we use the last vector of the LSTM<br>outputs for classification (Fig 8a). The final an-<br>swers are selected based on Eq. (1).</p>",
            "id": 90,
            "page": 6,
            "text": "As the candidate answers to FB and CO are not fixed and depend on each story and its question the input sequences to these Q-TYPES are concatenated with each candidate answer. Since the defined YN and FR model has moderately less accurate results on FB and CO Q-TYPES, we add a LSTM (Hochreiter and Schmidhuber, 1997) layer to improve it. Hence, to find the final answer, we run the model with each candidate answer and then apply an LSTM layer on top of all token representations. Then, we use the last vector of the LSTM outputs for classification (Fig 8a). The final answers are selected based on Eq. (1)."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 304
                },
                {
                    "x": 2193,
                    "y": 304
                },
                {
                    "x": 2193,
                    "y": 525
                },
                {
                    "x": 1267,
                    "y": 525
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='91' style='font-size:16px'>where s is the story, Ci is the candidate answer, q is<br>the question, [] indicates the concatenation of the<br>listed vectors, and mi is tokens' number in Xi. The<br>parameter vector, W, is shared for all candidates.</p>",
            "id": 91,
            "page": 6,
            "text": "where s is the story, Ci is the candidate answer, q is the question, [] indicates the concatenation of the listed vectors, and mi is tokens' number in Xi. The parameter vector, W, is shared for all candidates."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 565
                },
                {
                    "x": 1831,
                    "y": 565
                },
                {
                    "x": 1831,
                    "y": 616
                },
                {
                    "x": 1268,
                    "y": 616
                }
            ],
            "category": "paragraph",
            "html": "<p id='92' style='font-size:18px'>5.1 Training and Inference</p>",
            "id": 92,
            "page": 6,
            "text": "5.1 Training and Inference"
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 637
                },
                {
                    "x": 2193,
                    "y": 637
                },
                {
                    "x": 2193,
                    "y": 914
                },
                {
                    "x": 1267,
                    "y": 914
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='93' style='font-size:16px'>We train the models based on the summation of<br>the cross-entropy losses of all binary classifiers in<br>the architecture. For FR and YN Q-TYPES, there<br>are multiple classifiers, while there is only one<br>classifier used for CO and FB Q-TYPES.</p>",
            "id": 93,
            "page": 6,
            "text": "We train the models based on the summation of the cross-entropy losses of all binary classifiers in the architecture. For FR and YN Q-TYPES, there are multiple classifiers, while there is only one classifier used for CO and FB Q-TYPES."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 923
                },
                {
                    "x": 2195,
                    "y": 923
                },
                {
                    "x": 2195,
                    "y": 1368
                },
                {
                    "x": 1267,
                    "y": 1368
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='94' style='font-size:16px'>We remove inconsistent answers in post-<br>processing for FR and YN Q-TYPES during in-<br>ference phase. For instance on FR, left and right<br>relations between two objects cannot be valid at<br>the same time. For YN, as there is only one valid<br>answer amongst the three candidates, we select the<br>candidate with the maximal predicted probability<br>of being the true answer.</p>",
            "id": 94,
            "page": 6,
            "text": "We remove inconsistent answers in postprocessing for FR and YN Q-TYPES during inference phase. For instance on FR, left and right relations between two objects cannot be valid at the same time. For YN, as there is only one valid answer amongst the three candidates, we select the candidate with the maximal predicted probability of being the true answer."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 1413
                },
                {
                    "x": 1625,
                    "y": 1413
                },
                {
                    "x": 1625,
                    "y": 1465
                },
                {
                    "x": 1270,
                    "y": 1465
                }
            ],
            "category": "paragraph",
            "html": "<p id='95' style='font-size:22px'>6 Experiments</p>",
            "id": 95,
            "page": 6,
            "text": "6 Experiments"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 1500
                },
                {
                    "x": 2197,
                    "y": 1500
                },
                {
                    "x": 2197,
                    "y": 2512
                },
                {
                    "x": 1268,
                    "y": 2512
                }
            ],
            "category": "paragraph",
            "html": "<p id='96' style='font-size:16px'>As fine-tuning LMs has become a common base-<br>line approach to knowledge transfer from a source<br>dataset to a target task, including but not limited<br>to Phang et al. (2018); Zhou et al. (2020); He et al.<br>(2020b), we study the capability of spatial reason-<br>ing of modern LMs, specifically BERT, ALBERT,<br>and XLNet, after fine-tuning them on SPARTQA-<br>AUTO. This fine-tuning process is also known as<br>further pretraining, to distinguish with the fine-<br>tuning process on one's target task. It is an open<br>problem to find out better transfer learning tech-<br>niques than simple further pretraining, as suggested<br>in He et al. (2020a); Khashabi et al. (2020), which<br>is beyond the scope of this work. All experi-<br>ments use the models proposed in Sec. 5. We<br>use AdamW (Loshchilov and Hutter, 2017) with<br>2 x 10-6 learning rate and Focal Loss (Lin et al.,<br>2017) with 2 = 2 for training all the models. 6</p>",
            "id": 96,
            "page": 6,
            "text": "As fine-tuning LMs has become a common baseline approach to knowledge transfer from a source dataset to a target task, including but not limited to Phang  (2018); Zhou  (2020); He  (2020b), we study the capability of spatial reasoning of modern LMs, specifically BERT, ALBERT, and XLNet, after fine-tuning them on SPARTQAAUTO. This fine-tuning process is also known as further pretraining, to distinguish with the finetuning process on one's target task. It is an open problem to find out better transfer learning techniques than simple further pretraining, as suggested in He  (2020a); Khashabi  (2020), which is beyond the scope of this work. All experiments use the models proposed in Sec. 5. We use AdamW (Loshchilov and Hutter, 2017) with 2 x 10-6 learning rate and Focal Loss (Lin , 2017) with 2 = 2 for training all the models. 6"
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 2555
                },
                {
                    "x": 2183,
                    "y": 2555
                },
                {
                    "x": 2183,
                    "y": 2664
                },
                {
                    "x": 1270,
                    "y": 2664
                }
            ],
            "category": "paragraph",
            "html": "<p id='97' style='font-size:20px'>6.1 Further pretraining on SPARTQA-AUTO<br>improves spatial reasoning</p>",
            "id": 97,
            "page": 6,
            "text": "6.1 Further pretraining on SPARTQA-AUTO improves spatial reasoning"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 2683
                },
                {
                    "x": 2195,
                    "y": 2683
                },
                {
                    "x": 2195,
                    "y": 3019
                },
                {
                    "x": 1268,
                    "y": 3019
                }
            ],
            "category": "paragraph",
            "html": "<p id='98' style='font-size:16px'>Table 2 shows performance on SPARTQA-HUMAN<br>in a low-resource setting, where 0.6k QA pairs<br>from SPARTQA-HUMAN are used for fine-tuning<br>these LMs and 0.5k for testing (see Table 1 for<br>information on this split). 7 During our annotation,<br>we found that the description of \"near to \" and \"far</p>",
            "id": 98,
            "page": 6,
            "text": "Table 2 shows performance on SPARTQA-HUMAN in a low-resource setting, where 0.6k QA pairs from SPARTQA-HUMAN are used for fine-tuning these LMs and 0.5k for testing (see Table 1 for information on this split). 7 During our annotation, we found that the description of \"near to \" and \"far"
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 3054
                },
                {
                    "x": 2189,
                    "y": 3054
                },
                {
                    "x": 2189,
                    "y": 3134
                },
                {
                    "x": 1271,
                    "y": 3134
                }
            ],
            "category": "paragraph",
            "html": "<p id='99' style='font-size:14px'>6All codes are available at https : / / github . com/<br>HLR / SpartQA-baselines</p>",
            "id": 99,
            "page": 6,
            "text": "6All codes are available at https : / / github . com/ HLR / SpartQA-baselines"
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 3142
                },
                {
                    "x": 2188,
                    "y": 3142
                },
                {
                    "x": 2188,
                    "y": 3223
                },
                {
                    "x": 1273,
                    "y": 3223
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='100' style='font-size:14px'>7Note this low-resource setting can also be viewed as a<br>spatial reasoning probe to these LMs (Tenney et al., 2019).</p>",
            "id": 100,
            "page": 6,
            "text": "7Note this low-resource setting can also be viewed as a spatial reasoning probe to these LMs (Tenney , 2019)."
        },
        {
            "bounding_box": [
                {
                    "x": 484,
                    "y": 284
                },
                {
                    "x": 1989,
                    "y": 284
                },
                {
                    "x": 1989,
                    "y": 705
                },
                {
                    "x": 484,
                    "y": 705
                }
            ],
            "category": "table",
            "html": "<table id='101' style='font-size:20px'><tr><td>#</td><td>Model</td><td>FB</td><td>FR</td><td>CO</td><td>YN</td><td>Avg</td></tr><tr><td>1</td><td>Majority</td><td>28.84</td><td>24.52</td><td>40.18</td><td>53.60</td><td>36.64</td></tr><tr><td>2</td><td>BERT</td><td>16.34</td><td>20</td><td>26.16</td><td>45.36</td><td>30.17</td></tr><tr><td>3</td><td>BERT (Stories only; MLM)</td><td>21.15</td><td>16.19</td><td>27.1</td><td>51.54</td><td>32.90</td></tr><tr><td>4</td><td>BERT (SPARTQA-AUTO; MLM)</td><td>19.23</td><td>29.54</td><td>32.71</td><td>47.42</td><td>34.88</td></tr><tr><td>5</td><td>BERT (SPARTQA-AUTO)</td><td>62.5</td><td>46.66</td><td>32.71</td><td>47.42</td><td>47.25</td></tr><tr><td>6</td><td>Human</td><td>91.66</td><td>95.23</td><td>91.66</td><td>90.69</td><td>92.31</td></tr></table>",
            "id": 101,
            "page": 7,
            "text": "# Model FB FR CO YN Avg  1 Majority 28.84 24.52 40.18 53.60 36.64  2 BERT 16.34 20 26.16 45.36 30.17  3 BERT (Stories only; MLM) 21.15 16.19 27.1 51.54 32.90  4 BERT (SPARTQA-AUTO; MLM) 19.23 29.54 32.71 47.42 34.88  5 BERT (SPARTQA-AUTO) 62.5 46.66 32.71 47.42 47.25  6 Human 91.66 95.23 91.66 90.69"
        },
        {
            "bounding_box": [
                {
                    "x": 285,
                    "y": 733
                },
                {
                    "x": 2197,
                    "y": 733
                },
                {
                    "x": 2197,
                    "y": 992
                },
                {
                    "x": 285,
                    "y": 992
                }
            ],
            "category": "caption",
            "html": "<caption id='102' style='font-size:16px'>Table 2: Further pretraining BERT on SPARTQA-AUTO improves accuracies on SPARTQA-HUMAN. All<br>systems are fine-tuned on the training data of SPARTQA-HUMAN, but Systems 3-5 are also further pretrained in<br>different ways. System 3: further pretrained on the stories from SPARTQA-AUTO as a masked language model<br>(MLM) task. System 4: further pretrained on both stories and QA annotations as MLM. System 5: the proposed<br>model that is further pretrained on SPARTQA-AUTO as a QA task. Avg: The micro-average on all four Q-TYPEs.</caption>",
            "id": 102,
            "page": 7,
            "text": "Table 2: Further pretraining BERT on SPARTQA-AUTO improves accuracies on SPARTQA-HUMAN. All systems are fine-tuned on the training data of SPARTQA-HUMAN, but Systems 3-5 are also further pretrained in different ways. System 3: further pretrained on the stories from SPARTQA-AUTO as a masked language model (MLM) task. System 4: further pretrained on both stories and QA annotations as MLM. System 5: the proposed model that is further pretrained on SPARTQA-AUTO as a QA task. Avg: The micro-average on all four Q-TYPEs."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 1078
                },
                {
                    "x": 1216,
                    "y": 1078
                },
                {
                    "x": 1216,
                    "y": 1243
                },
                {
                    "x": 287,
                    "y": 1243
                }
            ],
            "category": "paragraph",
            "html": "<p id='103' style='font-size:18px'>from\" varies largely between annotators. Therefore,<br>we ignore these two relations from FR Q-TYPE in<br>our evaluations.</p>",
            "id": 103,
            "page": 7,
            "text": "from\" varies largely between annotators. Therefore, we ignore these two relations from FR Q-TYPE in our evaluations."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 1249
                },
                {
                    "x": 1216,
                    "y": 1249
                },
                {
                    "x": 1216,
                    "y": 1640
                },
                {
                    "x": 288,
                    "y": 1640
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='104' style='font-size:20px'>In Table 2, System 5, BERT (SPARTQA-AUTO),<br>is the proposed method of further pretraining<br>BERT on SPARTQA-AUTO. We can see that<br>System 2, the original BERT, performs consis-<br>tently lower than System 5, indicating that hav-<br>ing SPARTQA-AUTO as a further pretraining task<br>improves BERT's spatial understanding.</p>",
            "id": 104,
            "page": 7,
            "text": "In Table 2, System 5, BERT (SPARTQA-AUTO), is the proposed method of further pretraining BERT on SPARTQA-AUTO. We can see that System 2, the original BERT, performs consistently lower than System 5, indicating that having SPARTQA-AUTO as a further pretraining task improves BERT's spatial understanding."
        },
        {
            "bounding_box": [
                {
                    "x": 363,
                    "y": 1675
                },
                {
                    "x": 1134,
                    "y": 1675
                },
                {
                    "x": 1134,
                    "y": 2033
                },
                {
                    "x": 363,
                    "y": 2033
                }
            ],
            "category": "table",
            "html": "<table id='105' style='font-size:22px'><tr><td>Model</td><td>F1</td></tr><tr><td>Majority</td><td>35</td></tr><tr><td>BERT</td><td>50</td></tr><tr><td>BERT (Stories only; MLM)</td><td>53</td></tr><tr><td>BERT (SPARTQA-AUTO; MLM)</td><td>48</td></tr><tr><td>BERT (SPARTQA-AUTO)</td><td>48</td></tr></table>",
            "id": 105,
            "page": 7,
            "text": "Model F1  Majority 35  BERT 50  BERT (Stories only; MLM) 53  BERT (SPARTQA-AUTO; MLM) 48  BERT (SPARTQA-AUTO)"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 2065
                },
                {
                    "x": 1214,
                    "y": 2065
                },
                {
                    "x": 1214,
                    "y": 2217
                },
                {
                    "x": 287,
                    "y": 2217
                }
            ],
            "category": "caption",
            "html": "<caption id='106' style='font-size:16px'>Table 3: Switching from accuracy in Table 2 to F1<br>shows that the models are all performing better than<br>the majority baseline on YN Q-TYPE.</caption>",
            "id": 106,
            "page": 7,
            "text": "Table 3: Switching from accuracy in Table 2 to F1 shows that the models are all performing better than the majority baseline on YN Q-TYPE."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 2270
                },
                {
                    "x": 1215,
                    "y": 2270
                },
                {
                    "x": 1215,
                    "y": 2773
                },
                {
                    "x": 287,
                    "y": 2773
                }
            ],
            "category": "paragraph",
            "html": "<p id='107' style='font-size:20px'>In addition, we implement another two baselines.<br>System 3, BERT (Stories only; MLM): further pre-<br>training BERT only on the stories of SPARTQA-<br>AUTO as a masked language model (MLM) task;<br>System 4, BERT (SPARTQA-AUTO; MLM): we<br>convert the QA pairs in SPARTQA-AUTO into tex-<br>tual statements and further pretrain BERT on the<br>text as an MLM (see Fig. 5 for an example conver-<br>sion).</p>",
            "id": 107,
            "page": 7,
            "text": "In addition, we implement another two baselines. System 3, BERT (Stories only; MLM): further pretraining BERT only on the stories of SPARTQAAUTO as a masked language model (MLM) task; System 4, BERT (SPARTQA-AUTO; MLM): we convert the QA pairs in SPARTQA-AUTO into textual statements and further pretrain BERT on the text as an MLM (see Fig. 5 for an example conversion)."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 2779
                },
                {
                    "x": 1213,
                    "y": 2779
                },
                {
                    "x": 1213,
                    "y": 2999
                },
                {
                    "x": 287,
                    "y": 2999
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='108' style='font-size:18px'>To convert each question and its answer into a<br>sentence, we utilize static templates for each ques-<br>tion type which removes the question words and<br>rearranges other parts into a sentence.</p>",
            "id": 108,
            "page": 7,
            "text": "To convert each question and its answer into a sentence, we utilize static templates for each question type which removes the question words and rearranges other parts into a sentence."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 3006
                },
                {
                    "x": 1214,
                    "y": 3006
                },
                {
                    "x": 1214,
                    "y": 3229
                },
                {
                    "x": 288,
                    "y": 3229
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='109' style='font-size:20px'>We can see that System 3 slightly improves over<br>System 2, an observation consistent with many<br>prior works that seeing more text generally helps<br>an LM (e.g., Gururangan et al. (2020)). The signif-</p>",
            "id": 109,
            "page": 7,
            "text": "We can see that System 3 slightly improves over System 2, an observation consistent with many prior works that seeing more text generally helps an LM (e.g., Gururangan  (2020)). The signif-"
        },
        {
            "bounding_box": [
                {
                    "x": 1339,
                    "y": 1083
                },
                {
                    "x": 2120,
                    "y": 1083
                },
                {
                    "x": 2120,
                    "y": 1300
                },
                {
                    "x": 1339,
                    "y": 1300
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='110' style='font-size:14px'>A big circle is above a triangle. A blue square is<br>below the triangle.<br>What is the relation between the circle and the<br>blue object?<br>Answer: Above</p>",
            "id": 110,
            "page": 7,
            "text": "A big circle is above a triangle. A blue square is below the triangle. What is the relation between the circle and the blue object? Answer: Above"
        },
        {
            "bounding_box": [
                {
                    "x": 1342,
                    "y": 1365
                },
                {
                    "x": 2116,
                    "y": 1365
                },
                {
                    "x": 2116,
                    "y": 1523
                },
                {
                    "x": 1342,
                    "y": 1523
                }
            ],
            "category": "paragraph",
            "html": "<p id='111' style='font-size:16px'>A big circle is above a triangle. A blue square is<br>below the triangle. The circle is [MASK] the blue<br>object.<br>Answer: Above</p>",
            "id": 111,
            "page": 7,
            "text": "A big circle is above a triangle. A blue square is below the triangle. The circle is [MASK] the blue object. Answer: Above"
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 1585
                },
                {
                    "x": 2195,
                    "y": 1585
                },
                {
                    "x": 2195,
                    "y": 1679
                },
                {
                    "x": 1270,
                    "y": 1679
                }
            ],
            "category": "caption",
            "html": "<caption id='112' style='font-size:16px'>Figure 5: Convert a triplet of (paragraph, question, an-<br>swer) into a single piece of text for the MLM task.</caption>",
            "id": 112,
            "page": 7,
            "text": "Figure 5: Convert a triplet of (paragraph, question, answer) into a single piece of text for the MLM task."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 1786
                },
                {
                    "x": 2196,
                    "y": 1786
                },
                {
                    "x": 2196,
                    "y": 2229
                },
                {
                    "x": 1269,
                    "y": 2229
                }
            ],
            "category": "paragraph",
            "html": "<p id='113' style='font-size:18px'>icant gap between System 3 and the proposed Sys-<br>tem 5 indicates that supervision signals come more<br>from our annotations in SPARTQA-AUTO rather<br>than from seeing more unannotated text. System 4<br>is another way to make use of the annotations in<br>SPARTQA-AUTO, but it is shown to be not as ef-<br>fective as further pretraining BERT on SPARTQA-<br>AUTO as a QA task.</p>",
            "id": 113,
            "page": 7,
            "text": "icant gap between System 3 and the proposed System 5 indicates that supervision signals come more from our annotations in SPARTQA-AUTO rather than from seeing more unannotated text. System 4 is another way to make use of the annotations in SPARTQA-AUTO, but it is shown to be not as effective as further pretraining BERT on SPARTQAAUTO as a QA task."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 2240
                },
                {
                    "x": 2197,
                    "y": 2240
                },
                {
                    "x": 2197,
                    "y": 3022
                },
                {
                    "x": 1268,
                    "y": 3022
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='114' style='font-size:20px'>While the proposed System 5 overall performs<br>better than the other three baseline systems, one ex-<br>ception is its accuracy on YN, which is lower than<br>that of System 3. Since all systems' YN accuracies<br>are also lower than the majority baseline8, we hy-<br>pothesize that this is due to imbalanced data. To<br>verify it, we compute the F1 score for YN Q-TYPE<br>in Table 3, where we see all systems effectively<br>achieve better scores than the majority baseline.<br>However, further pretraining BERT on SPARTQA-<br>AUTO still does not beat other baseline systems,<br>which implies that straightforward pretraining is<br>not necessarily helpful in capturing the complex<br>reasoning phenomena required by YN questions.</p>",
            "id": 114,
            "page": 7,
            "text": "While the proposed System 5 overall performs better than the other three baseline systems, one exception is its accuracy on YN, which is lower than that of System 3. Since all systems' YN accuracies are also lower than the majority baseline8, we hypothesize that this is due to imbalanced data. To verify it, we compute the F1 score for YN Q-TYPE in Table 3, where we see all systems effectively achieve better scores than the majority baseline. However, further pretraining BERT on SPARTQAAUTO still does not beat other baseline systems, which implies that straightforward pretraining is not necessarily helpful in capturing the complex reasoning phenomena required by YN questions."
        },
        {
            "bounding_box": [
                {
                    "x": 1316,
                    "y": 3034
                },
                {
                    "x": 2194,
                    "y": 3034
                },
                {
                    "x": 2194,
                    "y": 3085
                },
                {
                    "x": 1316,
                    "y": 3085
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='115' style='font-size:18px'>The human performance is evaluated on 100 ran-</p>",
            "id": 115,
            "page": 7,
            "text": "The human performance is evaluated on 100 ran-"
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 3140
                },
                {
                    "x": 2190,
                    "y": 3140
                },
                {
                    "x": 2190,
                    "y": 3221
                },
                {
                    "x": 1269,
                    "y": 3221
                }
            ],
            "category": "paragraph",
            "html": "<p id='116' style='font-size:14px'>8which predicts the label that is most common in each set<br>of SPARTQA</p>",
            "id": 116,
            "page": 7,
            "text": "8which predicts the label that is most common in each set of SPARTQA"
        },
        {
            "bounding_box": [
                {
                    "x": 295,
                    "y": 285
                },
                {
                    "x": 2183,
                    "y": 285
                },
                {
                    "x": 2183,
                    "y": 607
                },
                {
                    "x": 295,
                    "y": 607
                }
            ],
            "category": "table",
            "html": "<table id='117' style='font-size:14px'><tr><td rowspan=\"2\">#</td><td rowspan=\"2\">Models</td><td colspan=\"3\">FB</td><td colspan=\"3\">FR</td><td colspan=\"3\">CO</td><td colspan=\"3\">YN</td></tr><tr><td>Seen</td><td>Unseen</td><td>Human*</td><td>Seen</td><td>Unseen</td><td>Human*</td><td>Seen</td><td>Unseen</td><td>Human*</td><td>Seen</td><td>Unseen</td><td>Human *</td></tr><tr><td>1</td><td>Majority</td><td>48.70</td><td>48.70</td><td>28.84</td><td>40.81</td><td>40.81</td><td>24.52</td><td>20.59</td><td>20.38</td><td>40.18</td><td>49.94</td><td>49.91</td><td>53.60</td></tr><tr><td>2</td><td>BERT</td><td>87.13</td><td>69.38</td><td>62.5</td><td>85.68</td><td>73.71</td><td>46.66</td><td>71.44</td><td>61.09</td><td>32.71</td><td>78.29</td><td>76.81</td><td>47.42</td></tr><tr><td>3</td><td>ALBERT</td><td>97.66</td><td>83.53</td><td>56.73</td><td>91.61</td><td>83.70</td><td>44.76</td><td>95.20</td><td>84.55</td><td>49.53</td><td>79.38</td><td>75.05</td><td>41.75</td></tr><tr><td>4</td><td>XLNet</td><td>98.00</td><td>84.85</td><td>73.07</td><td>94.60</td><td>91.63</td><td>57.14</td><td>97.11</td><td>90.88</td><td>50.46</td><td>79.91</td><td>78.54</td><td>39.69</td></tr><tr><td>5</td><td>Human</td><td colspan=\"2\">85</td><td>91.66</td><td colspan=\"2\">90</td><td>95.23</td><td colspan=\"2\">94.44</td><td>91.66</td><td colspan=\"2\">90</td><td>90.69</td></tr></table>",
            "id": 117,
            "page": 8,
            "text": "# Models FB FR CO YN  Seen Unseen Human* Seen Unseen Human* Seen Unseen Human* Seen Unseen Human *  1 Majority 48.70 48.70 28.84 40.81 40.81 24.52 20.59 20.38 40.18 49.94 49.91 53.60  2 BERT 87.13 69.38 62.5 85.68 73.71 46.66 71.44 61.09 32.71 78.29 76.81 47.42  3 ALBERT 97.66 83.53 56.73 91.61 83.70 44.76 95.20 84.55 49.53 79.38 75.05 41.75  4 XLNet 98.00 84.85 73.07 94.60 91.63 57.14 97.11 90.88 50.46 79.91 78.54 39.69  5 Human 85 91.66 90 95.23 94.44 91.66 90"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 636
                },
                {
                    "x": 2197,
                    "y": 636
                },
                {
                    "x": 2197,
                    "y": 892
                },
                {
                    "x": 287,
                    "y": 892
                }
            ],
            "category": "caption",
            "html": "<caption id='118' style='font-size:14px'>Table 4: Spatial reasoning is challenging. We further pretrain three transformer-based LMs, BERT, ALBERT,<br>and XLNet, on SPARTQA-AUTO, and test their accuracy in three ways: Seen and Unseen are both from SPARTQA-<br>AUTO, where Unseen has applied minor modifications to its vocabulary; to get those Human columns, all models<br>are fine-tuned on SPARTQA-HUMAN's training data. Human performance on Seen and Unseen is the same since<br>the changes applied to Unseen does not affect human reasoning.</caption>",
            "id": 118,
            "page": 8,
            "text": "Table 4: Spatial reasoning is challenging. We further pretrain three transformer-based LMs, BERT, ALBERT, and XLNet, on SPARTQA-AUTO, and test their accuracy in three ways: Seen and Unseen are both from SPARTQAAUTO, where Unseen has applied minor modifications to its vocabulary; to get those Human columns, all models are fine-tuned on SPARTQA-HUMAN's training data. Human performance on Seen and Unseen is the same since the changes applied to Unseen does not affect human reasoning."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 981
                },
                {
                    "x": 1213,
                    "y": 981
                },
                {
                    "x": 1213,
                    "y": 1430
                },
                {
                    "x": 287,
                    "y": 1430
                }
            ],
            "category": "paragraph",
            "html": "<p id='119' style='font-size:18px'>dom questions from each SPARTQA-AUTO and<br>SPARTQA-HUMAN test set. The respondents are<br>graduate students that were trained by some exam-<br>ples of the dataset before answering the final ques-<br>tions. We can see from Table 2 that all systems'<br>performances fall behind human performance by<br>a large margin. We expand on the difficulty of<br>SPARTQA in the next subsection.</p>",
            "id": 119,
            "page": 8,
            "text": "dom questions from each SPARTQA-AUTO and SPARTQA-HUMAN test set. The respondents are graduate students that were trained by some examples of the dataset before answering the final questions. We can see from Table 2 that all systems' performances fall behind human performance by a large margin. We expand on the difficulty of SPARTQA in the next subsection."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 1496
                },
                {
                    "x": 881,
                    "y": 1496
                },
                {
                    "x": 881,
                    "y": 1552
                },
                {
                    "x": 287,
                    "y": 1552
                }
            ],
            "category": "paragraph",
            "html": "<p id='120' style='font-size:22px'>6.2 SPARTQA is challenging</p>",
            "id": 120,
            "page": 8,
            "text": "6.2 SPARTQA is challenging"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 1583
                },
                {
                    "x": 1214,
                    "y": 1583
                },
                {
                    "x": 1214,
                    "y": 2541
                },
                {
                    "x": 287,
                    "y": 2541
                }
            ],
            "category": "paragraph",
            "html": "<p id='121' style='font-size:18px'>In addition to BERT, we continue to test another<br>two LMs, ALBERT and XLNet (Table 5). We<br>further pretrain these LMs on SPARTQA-AUTO,<br>and test them on SPARTQA-HUMAN (the num-<br>bers of BERT are copied from Table 2) and two<br>held-out test sets of SPARTQA-AUTO, Seen and<br>Unseen. Note that when a system is tested against<br>SPARTQA-HUMAN, it is fine-tuned on SPARTQA-<br>HUMAN's training data following its further pre-<br>training on SPARTQA-AUTO. We use the unseen<br>set to test to what extent the baseline models use<br>shortcuts in the language surface. This set applies<br>minor modifications randomly on a number of sto-<br>ries and questions to change the names of shapes,<br>colors, sizes, and relationships in the vocabulary of<br>the stories, which do not influence the reasoning<br>steps (more details in Appendix C.1).</p>",
            "id": 121,
            "page": 8,
            "text": "In addition to BERT, we continue to test another two LMs, ALBERT and XLNet (Table 5). We further pretrain these LMs on SPARTQA-AUTO, and test them on SPARTQA-HUMAN (the numbers of BERT are copied from Table 2) and two held-out test sets of SPARTQA-AUTO, Seen and Unseen. Note that when a system is tested against SPARTQA-HUMAN, it is fine-tuned on SPARTQAHUMAN's training data following its further pretraining on SPARTQA-AUTO. We use the unseen set to test to what extent the baseline models use shortcuts in the language surface. This set applies minor modifications randomly on a number of stories and questions to change the names of shapes, colors, sizes, and relationships in the vocabulary of the stories, which do not influence the reasoning steps (more details in Appendix C.1)."
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 2553
                },
                {
                    "x": 1212,
                    "y": 2553
                },
                {
                    "x": 1212,
                    "y": 3230
                },
                {
                    "x": 286,
                    "y": 3230
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='122' style='font-size:18px'>All models perform worst in YN across all Q-<br>TYPES, which suggests that YN presents a more<br>complex phenomena, probably due to additional<br>quantifiers in the questions. XLNet performs<br>the best on all Q-TYPES except its accuracy on<br>SPARTQA-HUMAN's YN section. However, the<br>drops in Unseen and human suggest overfitting on<br>the training vocabulary. The low accuracies on hu-<br>man test set from all models show that solving this<br>benchmark is still a challenging problem and re-<br>quires more sophisticated methods like considering<br>spatial roles and relations extraction (Kordjamshidi</p>",
            "id": 122,
            "page": 8,
            "text": "All models perform worst in YN across all QTYPES, which suggests that YN presents a more complex phenomena, probably due to additional quantifiers in the questions. XLNet performs the best on all Q-TYPES except its accuracy on SPARTQA-HUMAN's YN section. However, the drops in Unseen and human suggest overfitting on the training vocabulary. The low accuracies on human test set from all models show that solving this benchmark is still a challenging problem and requires more sophisticated methods like considering spatial roles and relations extraction (Kordjamshidi"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 981
                },
                {
                    "x": 2194,
                    "y": 981
                },
                {
                    "x": 2194,
                    "y": 1087
                },
                {
                    "x": 1268,
                    "y": 1087
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='123' style='font-size:18px'>et al., 2010; Dan et al., 2020; Rahgooy et al., 2018)<br>to understand stories and questions better.</p>",
            "id": 123,
            "page": 8,
            "text": ", 2010; Dan , 2020; Rahgooy , 2018) to understand stories and questions better."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 1100
                },
                {
                    "x": 2198,
                    "y": 1100
                },
                {
                    "x": 2198,
                    "y": 1546
                },
                {
                    "x": 1268,
                    "y": 1546
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='124' style='font-size:18px'>To evaluate the reliability of the models, we also<br>provide two extra consistency and contrast test sets.<br>Consistency set is made by changing a part of the<br>question in a way that seeks for the same infor-<br>mation (Hudson and Manning, 2019; Suhr et al.,<br>2019). Given a pivot question and answer of a spe-<br>cific consistency set, answering other questions in<br>the set does not need extra reasoning over the story.</p>",
            "id": 124,
            "page": 8,
            "text": "To evaluate the reliability of the models, we also provide two extra consistency and contrast test sets. Consistency set is made by changing a part of the question in a way that seeks for the same information (Hudson and Manning, 2019; Suhr , 2019). Given a pivot question and answer of a specific consistency set, answering other questions in the set does not need extra reasoning over the story."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 1558
                },
                {
                    "x": 2194,
                    "y": 1558
                },
                {
                    "x": 2194,
                    "y": 2059
                },
                {
                    "x": 1267,
                    "y": 2059
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='125' style='font-size:18px'>Contrast set is made by minimal modification<br>in a question to change its answer (Gardner et al.,<br>2020). For contrast sets, there is a need to go back<br>to the story to find the new answer for the question's<br>minor variations (see Appendix C.2 for examples.)<br>The consistency and contrast sets are evaluated only<br>on the correctly predicted questions to check if the<br>actual understanding and reasoning occurs. This<br>ensures the reliability of the models.</p>",
            "id": 125,
            "page": 8,
            "text": "Contrast set is made by minimal modification in a question to change its answer (Gardner , 2020). For contrast sets, there is a need to go back to the story to find the new answer for the question's minor variations (see Appendix C.2 for examples.) The consistency and contrast sets are evaluated only on the correctly predicted questions to check if the actual understanding and reasoning occurs. This ensures the reliability of the models."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 2071
                },
                {
                    "x": 2194,
                    "y": 2071
                },
                {
                    "x": 2194,
                    "y": 2404
                },
                {
                    "x": 1267,
                    "y": 2404
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='126' style='font-size:16px'>Table 5 shows the result of this evaluation on<br>four Q-TYPEs of SPARTQA-AUTO, where we can<br>see, for another time, that the high scores on the<br>Seen test set are likely due to overfitting on training<br>data rather than correct detection of spatial terms<br>and reasoning over them.</p>",
            "id": 126,
            "page": 8,
            "text": "Table 5 shows the result of this evaluation on four Q-TYPEs of SPARTQA-AUTO, where we can see, for another time, that the high scores on the Seen test set are likely due to overfitting on training data rather than correct detection of spatial terms and reasoning over them."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 2464
                },
                {
                    "x": 1770,
                    "y": 2464
                },
                {
                    "x": 1770,
                    "y": 2515
                },
                {
                    "x": 1268,
                    "y": 2515
                }
            ],
            "category": "paragraph",
            "html": "<p id='127' style='font-size:18px'>6.3 Extrinsic evaluation</p>",
            "id": 127,
            "page": 8,
            "text": "6.3 Extrinsic evaluation"
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 2547
                },
                {
                    "x": 2194,
                    "y": 2547
                },
                {
                    "x": 2194,
                    "y": 2769
                },
                {
                    "x": 1267,
                    "y": 2769
                }
            ],
            "category": "paragraph",
            "html": "<p id='128' style='font-size:18px'>In this subsection, we take BERT as an example to<br>show, once pretrained on SPARTQA-AUTO, BERT<br>can achieve better performance on two extrinsic<br>evaluation datasets, namely bAbI and boolQ.</p>",
            "id": 128,
            "page": 8,
            "text": "In this subsection, we take BERT as an example to show, once pretrained on SPARTQA-AUTO, BERT can achieve better performance on two extrinsic evaluation datasets, namely bAbI and boolQ."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 2780
                },
                {
                    "x": 2196,
                    "y": 2780
                },
                {
                    "x": 2196,
                    "y": 3227
                },
                {
                    "x": 1267,
                    "y": 3227
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='129' style='font-size:20px'>We draw the learning curve on bAbI, using the<br>original BERT as a baseline and BERT further pre-<br>trained on SPARTQA-AUTO (Fig. 6). Although<br>both systems achieve perfect accuracy given large<br>enough training data (i.e., 5k and 10k), BERT<br>(SPARTQA-AUTO) is showing better scores given<br>less training data. Specifically, to achieve an accu-<br>racy of 99%, BERT (SPARTQA-AUTO) requires</p>",
            "id": 129,
            "page": 8,
            "text": "We draw the learning curve on bAbI, using the original BERT as a baseline and BERT further pretrained on SPARTQA-AUTO (Fig. 6). Although both systems achieve perfect accuracy given large enough training data (i.e., 5k and 10k), BERT (SPARTQA-AUTO) is showing better scores given less training data. Specifically, to achieve an accuracy of 99%, BERT (SPARTQA-AUTO) requires"
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 286
                },
                {
                    "x": 2180,
                    "y": 286
                },
                {
                    "x": 2180,
                    "y": 587
                },
                {
                    "x": 293,
                    "y": 587
                }
            ],
            "category": "table",
            "html": "<table id='130' style='font-size:18px'><tr><td rowspan=\"2\">Models</td><td colspan=\"5\">FB FR CO</td><td colspan=\"2\">YN</td></tr><tr><td>Consistency</td><td>Consistency</td><td>Contrast</td><td>Consistency</td><td>Contrast</td><td>Consistency</td><td>Contrast</td></tr><tr><td>BERT</td><td>69.44</td><td>76.13</td><td>42.47</td><td>16.99</td><td>15.58</td><td>48.07</td><td>71.41</td></tr><tr><td>AIBERT</td><td>84.77</td><td>82.42</td><td>41.69</td><td>58.42</td><td>62.51</td><td>48.78</td><td>69.19</td></tr><tr><td>XLNet</td><td>85.2</td><td>88.56</td><td>50</td><td>71.10</td><td>72.31</td><td>51.08</td><td>69.18</td></tr></table>",
            "id": 130,
            "page": 9,
            "text": "Models FB FR CO YN  Consistency Consistency Contrast Consistency Contrast Consistency Contrast  BERT 69.44 76.13 42.47 16.99 15.58 48.07 71.41  AIBERT 84.77 82.42 41.69 58.42 62.51 48.78 69.19  XLNet 85.2 88.56 50 71.10 72.31 51.08"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 621
                },
                {
                    "x": 2192,
                    "y": 621
                },
                {
                    "x": 2192,
                    "y": 721
                },
                {
                    "x": 288,
                    "y": 721
                }
            ],
            "category": "caption",
            "html": "<caption id='131' style='font-size:14px'>Table 5: Evaluation of consistency and semantic sensitivity of models in Table 4. All the results are on the correctly<br>predicted questions of Seen test set of SPARTQA-AUTO.</caption>",
            "id": 131,
            "page": 9,
            "text": "Table 5: Evaluation of consistency and semantic sensitivity of models in Table 4. All the results are on the correctly predicted questions of Seen test set of SPARTQA-AUTO."
        },
        {
            "bounding_box": [
                {
                    "x": 294,
                    "y": 807
                },
                {
                    "x": 1196,
                    "y": 807
                },
                {
                    "x": 1196,
                    "y": 1172
                },
                {
                    "x": 294,
                    "y": 1172
                }
            ],
            "category": "figure",
            "html": "<figure><img id='132' style='font-size:14px' alt=\"BERT BERT on SPARTQA-Auto\n99.1 99.399.6\n1\n100 100 100 100\n84.2\n9\nAccuracy\n74.9\n8\n7\n59.860.6 60.4\n6\n5\n100 500 1000 2000 5000 10000\nSize of training set\" data-coord=\"top-left:(294,807); bottom-right:(1196,1172)\" /></figure>",
            "id": 132,
            "page": 9,
            "text": "BERT BERT on SPARTQA-Auto 99.1 99.399.6 1 100 100 100 100 84.2 9 Accuracy 74.9 8 7 59.860.6 60.4 6 5 100 500 1000 2000 5000 10000 Size of training set"
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 1218
                },
                {
                    "x": 1212,
                    "y": 1218
                },
                {
                    "x": 1212,
                    "y": 1316
                },
                {
                    "x": 290,
                    "y": 1316
                }
            ],
            "category": "caption",
            "html": "<caption id='133' style='font-size:14px'>Figure 6: Learning curve of BERT and BERT further<br>pretrained on SPARTQA-AUTO on bAbI.</caption>",
            "id": 133,
            "page": 9,
            "text": "Figure 6: Learning curve of BERT and BERT further pretrained on SPARTQA-AUTO on bAbI."
        },
        {
            "bounding_box": [
                {
                    "x": 352,
                    "y": 1364
                },
                {
                    "x": 1148,
                    "y": 1364
                },
                {
                    "x": 1148,
                    "y": 1834
                },
                {
                    "x": 352,
                    "y": 1834
                }
            ],
            "category": "table",
            "html": "<table id='134' style='font-size:18px'><tr><td>Model</td><td>Accuracy</td></tr><tr><td>Majority baseline</td><td>62.2</td></tr><tr><td>Recurrent model (ReM)</td><td>62.2</td></tr><tr><td>ReM fine-tuned on SQuAD</td><td>69.8</td></tr><tr><td>ReM fine-tuned on QNLI</td><td>71.4</td></tr><tr><td>ReM fine-tuned on NQ</td><td>72.8</td></tr><tr><td>BERT (our setup)</td><td>71.9</td></tr><tr><td>BERT (SPARTQA-AUTO)</td><td>74.2</td></tr></table>",
            "id": 134,
            "page": 9,
            "text": "Model Accuracy  Majority baseline 62.2  Recurrent model (ReM) 62.2  ReM fine-tuned on SQuAD 69.8  ReM fine-tuned on QNLI 71.4  ReM fine-tuned on NQ 72.8  BERT (our setup) 71.9  BERT (SPARTQA-AUTO)"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 1869
                },
                {
                    "x": 1212,
                    "y": 1869
                },
                {
                    "x": 1212,
                    "y": 2121
                },
                {
                    "x": 287,
                    "y": 2121
                }
            ],
            "category": "paragraph",
            "html": "<p id='135' style='font-size:16px'>Table 6: System performances on the dev set of boolQ<br>(since the test set is not available to us). Top: numbers<br>reported in (Clark et al., 2019). Bottom: numbers from<br>our experiments. BERT (SPARTQA-AUTO): further<br>pretraining BERT on SPARTQA-AUTO as a QA task.</p>",
            "id": 135,
            "page": 9,
            "text": "Table 6: System performances on the dev set of boolQ (since the test set is not available to us). Top: numbers reported in (Clark , 2019). Bottom: numbers from our experiments. BERT (SPARTQA-AUTO): further pretraining BERT on SPARTQA-AUTO as a QA task."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 2210
                },
                {
                    "x": 1214,
                    "y": 2210
                },
                {
                    "x": 1214,
                    "y": 2377
                },
                {
                    "x": 287,
                    "y": 2377
                }
            ],
            "category": "paragraph",
            "html": "<p id='136' style='font-size:18px'>1k training examples, while BERT requires twice<br>as much. We also notice that BERT (SPARTQA-<br>AUTO) converges faster in our experiments.</p>",
            "id": 136,
            "page": 9,
            "text": "1k training examples, while BERT requires twice as much. We also notice that BERT (SPARTQAAUTO) converges faster in our experiments."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 2384
                },
                {
                    "x": 1213,
                    "y": 2384
                },
                {
                    "x": 1213,
                    "y": 2830
                },
                {
                    "x": 288,
                    "y": 2830
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='137' style='font-size:18px'>As another evaluation dataset, we chose boolQ<br>for two reasons. First, we needed a QA dataset<br>with Yes/No questions. To our knowledge boolQ<br>is the only available one used in the recent work.<br>Second, indeed, SPARTQA and boolQ are from dif-<br>ferent domains, however, boolQ needs multi-step<br>reasoning in which we wanted to see if SPARTQA<br>helps.</p>",
            "id": 137,
            "page": 9,
            "text": "As another evaluation dataset, we chose boolQ for two reasons. First, we needed a QA dataset with Yes/No questions. To our knowledge boolQ is the only available one used in the recent work. Second, indeed, SPARTQA and boolQ are from different domains, however, boolQ needs multi-step reasoning in which we wanted to see if SPARTQA helps."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 2836
                },
                {
                    "x": 1212,
                    "y": 2836
                },
                {
                    "x": 1212,
                    "y": 3230
                },
                {
                    "x": 287,
                    "y": 3230
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='138' style='font-size:18px'>Table 6 shows that further pretraining BERT on<br>SPARTQA-AUTO yields a better result than the<br>original BERT and those reported numbers in Clark<br>et al. (2019), which also tested on various distant<br>supervision signals such as SQuAD (Rajpurkar<br>et al., 2016), Google's Natural Question dataset<br>NQ (Kwiatkowski et al., 2019), and QNLI from</p>",
            "id": 138,
            "page": 9,
            "text": "Table 6 shows that further pretraining BERT on SPARTQA-AUTO yields a better result than the original BERT and those reported numbers in Clark  (2019), which also tested on various distant supervision signals such as SQuAD (Rajpurkar , 2016), Google's Natural Question dataset NQ (Kwiatkowski , 2019), and QNLI from"
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 813
                },
                {
                    "x": 1768,
                    "y": 813
                },
                {
                    "x": 1768,
                    "y": 862
                },
                {
                    "x": 1270,
                    "y": 862
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='139' style='font-size:18px'>GLUE (Wang et al., 2018).</p>",
            "id": 139,
            "page": 9,
            "text": "GLUE (Wang , 2018)."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 874
                },
                {
                    "x": 2198,
                    "y": 874
                },
                {
                    "x": 2198,
                    "y": 1262
                },
                {
                    "x": 1268,
                    "y": 1262
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='140' style='font-size:20px'>We observe that many of the boolQ examples<br>answered correctly by the BERT further pretrained<br>on SPARTQA-AUTO require multi-step reasoning.<br>Our hypothesis is that since solving SPARTQA-<br>AUTO questions needs multi-step reasoning, fine-<br>tuning BERT on SPARTQA-AUTO generally im-<br>proves this capability of the base model.</p>",
            "id": 140,
            "page": 9,
            "text": "We observe that many of the boolQ examples answered correctly by the BERT further pretrained on SPARTQA-AUTO require multi-step reasoning. Our hypothesis is that since solving SPARTQAAUTO questions needs multi-step reasoning, finetuning BERT on SPARTQA-AUTO generally improves this capability of the base model."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 1311
                },
                {
                    "x": 1592,
                    "y": 1311
                },
                {
                    "x": 1592,
                    "y": 1365
                },
                {
                    "x": 1270,
                    "y": 1365
                }
            ],
            "category": "paragraph",
            "html": "<p id='141' style='font-size:20px'>7 Conclusion</p>",
            "id": 141,
            "page": 9,
            "text": "7 Conclusion"
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 1405
                },
                {
                    "x": 2198,
                    "y": 1405
                },
                {
                    "x": 2198,
                    "y": 2366
                },
                {
                    "x": 1267,
                    "y": 2366
                }
            ],
            "category": "paragraph",
            "html": "<p id='142' style='font-size:18px'>Spatial reasoning is an important problem in natu-<br>ral language understanding. We propose the first<br>human-created QA benchmark on spatial reason-<br>ing, and experiments show that state-of-the-art pre-<br>trained language models (LM) do not have the capa-<br>bility to solve this task given limited training data,<br>while humans can solve those spatial reasoning<br>questions reliably. To improve LMs' capability on<br>this task, we propose to use hand-crafted grammar<br>and spatial reasoning rules to automatically gener-<br>ate a large corpus of spatial descriptions and cor-<br>responding question-answer annotations; further<br>pretraining LMs on this distant supervision dataset<br>significantly enhances their spatial language un-<br>derstanding and reasoning. We also show that a<br>spatially-improved LM can have better results on<br>two extrinsic datasets (bAbI and boolQ).</p>",
            "id": 142,
            "page": 9,
            "text": "Spatial reasoning is an important problem in natural language understanding. We propose the first human-created QA benchmark on spatial reasoning, and experiments show that state-of-the-art pretrained language models (LM) do not have the capability to solve this task given limited training data, while humans can solve those spatial reasoning questions reliably. To improve LMs' capability on this task, we propose to use hand-crafted grammar and spatial reasoning rules to automatically generate a large corpus of spatial descriptions and corresponding question-answer annotations; further pretraining LMs on this distant supervision dataset significantly enhances their spatial language understanding and reasoning. We also show that a spatially-improved LM can have better results on two extrinsic datasets (bAbI and boolQ)."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 2415
                },
                {
                    "x": 1689,
                    "y": 2415
                },
                {
                    "x": 1689,
                    "y": 2468
                },
                {
                    "x": 1271,
                    "y": 2468
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='143' style='font-size:22px'>Acknowledgements</p>",
            "id": 143,
            "page": 9,
            "text": "Acknowledgements"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 2505
                },
                {
                    "x": 2196,
                    "y": 2505
                },
                {
                    "x": 2196,
                    "y": 2898
                },
                {
                    "x": 1268,
                    "y": 2898
                }
            ],
            "category": "paragraph",
            "html": "<p id='144' style='font-size:18px'>This project is supported by National Science Foun-<br>dation (NSF) CAREER award #2028626 and (par-<br>tially) supported by the Office of Naval Research<br>grant #N00014-20-1-2005. We thank the reviewers<br>for their helpful comments to improve this paper<br>and Timothy Moran for his help in the human data<br>generation.</p>",
            "id": 144,
            "page": 9,
            "text": "This project is supported by National Science Foundation (NSF) CAREER award #2028626 and (partially) supported by the Office of Naval Research grant #N00014-20-1-2005. We thank the reviewers for their helpful comments to improve this paper and Timothy Moran for his help in the human data generation."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 3001
                },
                {
                    "x": 1512,
                    "y": 3001
                },
                {
                    "x": 1512,
                    "y": 3053
                },
                {
                    "x": 1272,
                    "y": 3053
                }
            ],
            "category": "paragraph",
            "html": "<p id='145' style='font-size:22px'>References</p>",
            "id": 145,
            "page": 9,
            "text": "References"
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 3084
                },
                {
                    "x": 2196,
                    "y": 3084
                },
                {
                    "x": 2196,
                    "y": 3224
                },
                {
                    "x": 1269,
                    "y": 3224
                }
            ],
            "category": "paragraph",
            "html": "<p id='146' style='font-size:16px'>Peter Anderson, Qi Wu, Damien Teney, Jake Bruce,<br>Mark Johnson, Niko Sunderhauf, Ian Reid, Stephen<br>Gould, and Anton van den Hengel. 2018. Vision-</p>",
            "id": 146,
            "page": 9,
            "text": "Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sunderhauf, Ian Reid, Stephen Gould, and Anton van den Hengel. 2018. Vision-"
        },
        {
            "bounding_box": [
                {
                    "x": 331,
                    "y": 305
                },
                {
                    "x": 1215,
                    "y": 305
                },
                {
                    "x": 1215,
                    "y": 534
                },
                {
                    "x": 331,
                    "y": 534
                }
            ],
            "category": "paragraph",
            "html": "<p id='147' style='font-size:18px'>and-language navigation: Interpreting visually-<br>grounded navigation instructions in real environ-<br>ments. In Proceedings of the IEEE Conference<br>on Computer Vision and Pattern Recognition, pages<br>3674-3683.</p>",
            "id": 147,
            "page": 10,
            "text": "and-language navigation: Interpreting visuallygrounded navigation instructions in real environments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3674-3683."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 572
                },
                {
                    "x": 1214,
                    "y": 572
                },
                {
                    "x": 1214,
                    "y": 805
                },
                {
                    "x": 288,
                    "y": 805
                }
            ],
            "category": "paragraph",
            "html": "<p id='148' style='font-size:16px'>Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-<br>garet Mitchell, Dhruv Batra, C Lawrence Zitnick,<br>and Devi Parikh. 2015. VQA: Visual question an-<br>swering. In Proceedings of the IEEE international<br>conference on computer vision, pages 2425-2433.</p>",
            "id": 148,
            "page": 10,
            "text": "Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. 2015. VQA: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 2425-2433."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 839
                },
                {
                    "x": 1213,
                    "y": 839
                },
                {
                    "x": 1213,
                    "y": 1118
                },
                {
                    "x": 291,
                    "y": 1118
                }
            ],
            "category": "paragraph",
            "html": "<p id='149' style='font-size:16px'>Howard Chen, Alane Suhr, Dipendra Misra, Noah<br>Snavely, and Yoav Artzi. 2019. TOUCHDOWN:<br>Natural language navigation and spatial reasoning<br>in visual street environments. In Proceedings of the<br>IEEE Conference on Computer Vision and Pattern<br>Recognition, pages 12538-12547.</p>",
            "id": 149,
            "page": 10,
            "text": "Howard Chen, Alane Suhr, Dipendra Misra, Noah Snavely, and Yoav Artzi. 2019. TOUCHDOWN: Natural language navigation and spatial reasoning in visual street environments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 12538-12547."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 1152
                },
                {
                    "x": 1213,
                    "y": 1152
                },
                {
                    "x": 1213,
                    "y": 1523
                },
                {
                    "x": 288,
                    "y": 1523
                }
            ],
            "category": "paragraph",
            "html": "<p id='150' style='font-size:20px'>Christopher Clark, Kenton Lee, Ming- Wei Chang,<br>Tom Kwiatkowski, Michael Collins, and Kristina<br>Toutanova. 2019. BoolQ: Exploring the surprising<br>difficulty of natural yes/no questions. In Proceed-<br>ings of the 2019 Conference of the North American<br>Chapter of the Association for Computational Lin-<br>guistics: Human Language Technologies, Volume 1<br>(Long and Short Papers), pages 2924-2936.</p>",
            "id": 150,
            "page": 10,
            "text": "Christopher Clark, Kenton Lee, Ming- Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2924-2936."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 1557
                },
                {
                    "x": 1214,
                    "y": 1557
                },
                {
                    "x": 1214,
                    "y": 1740
                },
                {
                    "x": 290,
                    "y": 1740
                }
            ],
            "category": "paragraph",
            "html": "<p id='151' style='font-size:16px'>Douglas H Clements and Michael T Battista. 1992. Ge-<br>ometry and spatial reasoning. Handbook of research<br>on mathematics teaching and learning, pages 420-<br>464.</p>",
            "id": 151,
            "page": 10,
            "text": "Douglas H Clements and Michael T Battista. 1992. Geometry and spatial reasoning. Handbook of research on mathematics teaching and learning, pages 420464."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 1779
                },
                {
                    "x": 1215,
                    "y": 1779
                },
                {
                    "x": 1215,
                    "y": 2101
                },
                {
                    "x": 289,
                    "y": 2101
                }
            ],
            "category": "paragraph",
            "html": "<p id='152' style='font-size:16px'>Soham Dan, Parisa Kordjamshidi, Julia Bonn, Archna<br>Bhatia, Zheng Cai, Martha Palmer, and Dan Roth.<br>2020. From spatial relations to spatial configura-<br>tions. In Proceedings of the 12th Language Re-<br>sources and Evaluation Conference, pages 5855-<br>5864, Marseille, France. European Language Re-<br>sources Association.</p>",
            "id": 152,
            "page": 10,
            "text": "Soham Dan, Parisa Kordjamshidi, Julia Bonn, Archna Bhatia, Zheng Cai, Martha Palmer, and Dan Roth. 2020. From spatial relations to spatial configurations. In Proceedings of the 12th Language Resources and Evaluation Conference, pages 58555864, Marseille, France. European Language Resources Association."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 2139
                },
                {
                    "x": 1213,
                    "y": 2139
                },
                {
                    "x": 1213,
                    "y": 2507
                },
                {
                    "x": 290,
                    "y": 2507
                }
            ],
            "category": "paragraph",
            "html": "<p id='153' style='font-size:16px'>Pradeep Dasigi, Nelson F. Liu, Ana Marasovic,<br>Noah A. Smith, and Matt Gardner. 2019. Quoref:<br>A reading comprehension dataset with questions re-<br>quiring coreferential reasoning. In Proceedings of<br>the 2019 Conference on Empirical Methods in Nat-<br>ural Language Processing and the 9th International<br>Joint Conference on Natural Language Processing<br>(EMNLP-IJCNLP), pages 5925-5932.</p>",
            "id": 153,
            "page": 10,
            "text": "Pradeep Dasigi, Nelson F. Liu, Ana Marasovic, Noah A. Smith, and Matt Gardner. 2019. Quoref: A reading comprehension dataset with questions requiring coreferential reasoning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5925-5932."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 2543
                },
                {
                    "x": 1213,
                    "y": 2543
                },
                {
                    "x": 1213,
                    "y": 2820
                },
                {
                    "x": 290,
                    "y": 2820
                }
            ],
            "category": "paragraph",
            "html": "<p id='154' style='font-size:20px'>Surabhi Datta and Kirk Roberts. 2020. A hybrid<br>deep learning approach for spatial trigger extraction<br>from radiology reports. In Proceedings of the Third<br>International Workshop on Spatial Language Un-<br>derstanding, pages 50-55, Online. Association for<br>Computational Linguistics.</p>",
            "id": 154,
            "page": 10,
            "text": "Surabhi Datta and Kirk Roberts. 2020. A hybrid deep learning approach for spatial trigger extraction from radiology reports. In Proceedings of the Third International Workshop on Spatial Language Understanding, pages 50-55, Online. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 2857
                },
                {
                    "x": 1214,
                    "y": 2857
                },
                {
                    "x": 1214,
                    "y": 3223
                },
                {
                    "x": 287,
                    "y": 3223
                }
            ],
            "category": "paragraph",
            "html": "<p id='155' style='font-size:18px'>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and<br>Kristina Toutanova. 2019. BERT: Pre-training of<br>deep bidirectional transformers for language under-<br>standing. In Proceedings of the 2019 Conference of<br>the North American Chapter of the Association for<br>Computational Linguistics: Human Language Tech-<br>nologies, Volume 1 (Long and Short Papers), pages<br>4171-4186.</p>",
            "id": 155,
            "page": 10,
            "text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 304
                },
                {
                    "x": 2196,
                    "y": 304
                },
                {
                    "x": 2196,
                    "y": 537
                },
                {
                    "x": 1270,
                    "y": 537
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='156' style='font-size:20px'>Kaustubh Dhole and Christopher D. Manning. 2020.<br>Syn-QG: Syntactic and shallow semantic rules for<br>question generation. In Proceedings of the 58th An-<br>nual Meeting of the Association for Computational<br>Linguistics, pages 752-765.</p>",
            "id": 156,
            "page": 10,
            "text": "Kaustubh Dhole and Christopher D. Manning. 2020. Syn-QG: Syntactic and shallow semantic rules for question generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 752-765."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 576
                },
                {
                    "x": 2194,
                    "y": 576
                },
                {
                    "x": 2194,
                    "y": 763
                },
                {
                    "x": 1270,
                    "y": 763
                }
            ],
            "category": "paragraph",
            "html": "<p id='157' style='font-size:18px'>Xinya Du and Claire Cardie. 2020. Event extraction by<br>answering (almost) natural questions. In Proceed-<br>ings of the 2020 Conference on Empirical Methods<br>in Natural Language Processing (EMNLP).</p>",
            "id": 157,
            "page": 10,
            "text": "Xinya Du and Claire Cardie. 2020. Event extraction by answering (almost) natural questions. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 804
                },
                {
                    "x": 2197,
                    "y": 804
                },
                {
                    "x": 2197,
                    "y": 1080
                },
                {
                    "x": 1271,
                    "y": 1080
                }
            ],
            "category": "paragraph",
            "html": "<p id='158' style='font-size:16px'>Xinya Du, Junru Shao, and Claire Cardie. 2017. Learn-<br>ing to Ask: Neural question generation for read-<br>ing comprehension. In Proceedings of the 55th An-<br>nual Meeting of the Association for Computational<br>Linguistics (Volume 1: Long Papers), pages 1342-<br>1352.</p>",
            "id": 158,
            "page": 10,
            "text": "Xinya Du, Junru Shao, and Claire Cardie. 2017. Learning to Ask: Neural question generation for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13421352."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 1123
                },
                {
                    "x": 2198,
                    "y": 1123
                },
                {
                    "x": 2198,
                    "y": 1496
                },
                {
                    "x": 1270,
                    "y": 1496
                }
            ],
            "category": "paragraph",
            "html": "<p id='159' style='font-size:20px'>Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel<br>Stanovsky, Sameer Singh, and Matt Gardner. 2019.<br>DROP: A reading comprehension benchmark requir-<br>ing discrete reasoning over paragraphs. In Proceed-<br>ings of the 2019 Conference of the North American<br>Chapter of the Association for Computational Lin-<br>guistics: Human Language Technologies, Volume 1<br>(Long and Short Papers), pages 2368-2378.</p>",
            "id": 159,
            "page": 10,
            "text": "Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2368-2378."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 1533
                },
                {
                    "x": 2196,
                    "y": 1533
                },
                {
                    "x": 2196,
                    "y": 2041
                },
                {
                    "x": 1271,
                    "y": 2041
                }
            ],
            "category": "paragraph",
            "html": "<p id='160' style='font-size:18px'>Matt Gardner, Yoav Artzi, Victoria Basmov, Jonathan<br>Berant, Ben Bogin, Sihao Chen, Pradeep Dasigi,<br>Dheeru Dua, Yanai Elazar, Ananth Gottumukkala,<br>Nitish Gupta, Hannaneh Hajishirzi, Gabriel Ilharco,<br>Daniel Khashabi, Kevin Lin, Jiangming Liu, Nel-<br>son F. Liu, Phoebe Mulcaire, Qiang Ning, Sameer<br>Singh, Noah A. Smith, Sanjay Subramanian, Reut<br>Tsarfaty, Eric Wallace, Ally Zhang, and Ben Zhou.<br>2020. Evaluating models' local decision boundaries<br>via contrast sets. In Findings of the Association for<br>Computational Linguistics: EMNLP 2020.</p>",
            "id": 160,
            "page": 10,
            "text": "Matt Gardner, Yoav Artzi, Victoria Basmov, Jonathan Berant, Ben Bogin, Sihao Chen, Pradeep Dasigi, Dheeru Dua, Yanai Elazar, Ananth Gottumukkala, Nitish Gupta, Hannaneh Hajishirzi, Gabriel Ilharco, Daniel Khashabi, Kevin Lin, Jiangming Liu, Nelson F. Liu, Phoebe Mulcaire, Qiang Ning, Sameer Singh, Noah A. Smith, Sanjay Subramanian, Reut Tsarfaty, Eric Wallace, Ally Zhang, and Ben Zhou. 2020. Evaluating models' local decision boundaries via contrast sets. In Findings of the Association for Computational Linguistics: EMNLP 2020."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 2083
                },
                {
                    "x": 2197,
                    "y": 2083
                },
                {
                    "x": 2197,
                    "y": 2266
                },
                {
                    "x": 1271,
                    "y": 2266
                }
            ],
            "category": "paragraph",
            "html": "<p id='161' style='font-size:14px'>Matt Gardner, Jonathan Berant, Hannaneh Hajishirzi,<br>Alon Talmor, and Sewon Min. 2019. Question An-<br>swering is a Format; when is it useful? ArXiv,<br>abs/1909.11291.</p>",
            "id": 161,
            "page": 10,
            "text": "Matt Gardner, Jonathan Berant, Hannaneh Hajishirzi, Alon Talmor, and Sewon Min. 2019. Question Answering is a Format; when is it useful? ArXiv, abs/1909.11291."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 2308
                },
                {
                    "x": 2198,
                    "y": 2308
                },
                {
                    "x": 2198,
                    "y": 2542
                },
                {
                    "x": 1271,
                    "y": 2542
                }
            ],
            "category": "paragraph",
            "html": "<p id='162' style='font-size:14px'>Mehdi Ghanimifard and Simon Dobnik. 2017. Learn-<br>ing to compose spatial relations with grounded neu-<br>ral language models. In IWCS 2017-12th Inter-<br>national Conference on Computational Semantics-<br>Long papers.</p>",
            "id": 162,
            "page": 10,
            "text": "Mehdi Ghanimifard and Simon Dobnik. 2017. Learning to compose spatial relations with grounded neural language models. In IWCS 2017-12th International Conference on Computational SemanticsLong papers."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 2584
                },
                {
                    "x": 2196,
                    "y": 2584
                },
                {
                    "x": 2196,
                    "y": 2905
                },
                {
                    "x": 1270,
                    "y": 2905
                }
            ],
            "category": "paragraph",
            "html": "<p id='163' style='font-size:20px'>Suchin Gururangan, Ana Marasovic, Swabha<br>Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,<br>and Noah A. Smith. 2020. Don't Stop Pretraining:<br>Adapt language models to domains and tasks. In<br>Proceedings of the 58th Annual Meeting of the<br>Association for Computational Linguistics, pages<br>8342-8360.</p>",
            "id": 163,
            "page": 10,
            "text": "Suchin Gururangan, Ana Marasovic, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020. Don't Stop Pretraining: Adapt language models to domains and tasks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8342-8360."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2948
                },
                {
                    "x": 2196,
                    "y": 2948
                },
                {
                    "x": 2196,
                    "y": 3224
                },
                {
                    "x": 1272,
                    "y": 3224
                }
            ],
            "category": "paragraph",
            "html": "<p id='164' style='font-size:18px'>Hangfeng He, Qiang Ning, and Dan Roth. 2020a.<br>QuASE: Question-answer driven sentence encoding.<br>In Proceedings of the 58th Annual Meeting of the<br>Association for Computational Linguistics, pages<br>8743-8758, Online. Association for Computational<br>Linguistics.</p>",
            "id": 164,
            "page": 10,
            "text": "Hangfeng He, Qiang Ning, and Dan Roth. 2020a. QuASE: Question-answer driven sentence encoding. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8743-8758, Online. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 304
                },
                {
                    "x": 1212,
                    "y": 304
                },
                {
                    "x": 1212,
                    "y": 445
                },
                {
                    "x": 288,
                    "y": 445
                }
            ],
            "category": "paragraph",
            "html": "<p id='165' style='font-size:20px'>Hangfeng He, Mingyuan Zhang, Qiang Ning, and Dan<br>Roth. 2020b. Foreshadowing the benefits of inciden-<br>tal supervision. arXiv preprint arXiv:2006.05500.</p>",
            "id": 165,
            "page": 11,
            "text": "Hangfeng He, Mingyuan Zhang, Qiang Ning, and Dan Roth. 2020b. Foreshadowing the benefits of incidental supervision. arXiv preprint arXiv:2006.05500."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 485
                },
                {
                    "x": 1214,
                    "y": 485
                },
                {
                    "x": 1214,
                    "y": 761
                },
                {
                    "x": 287,
                    "y": 761
                }
            ],
            "category": "paragraph",
            "html": "<p id='166' style='font-size:18px'>Luheng He, Mike Lewis, and Luke Zettlemoyer. 2015.<br>Question-Answer Driven Semantic Role Labeling:<br>Using natural language to annotate natural language.<br>In Proceedings of the 2015 Conference on Empiri-<br>cal Methods in Natural Language Processing, pages<br>643-653.</p>",
            "id": 166,
            "page": 11,
            "text": "Luheng He, Mike Lewis, and Luke Zettlemoyer. 2015. Question-Answer Driven Semantic Role Labeling: Using natural language to annotate natural language. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 643-653."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 804
                },
                {
                    "x": 1213,
                    "y": 804
                },
                {
                    "x": 1213,
                    "y": 1035
                },
                {
                    "x": 289,
                    "y": 1035
                }
            ],
            "category": "paragraph",
            "html": "<p id='167' style='font-size:14px'>Michael Heilman and Noah A Smith. 2009. Question<br>generation via overgenerating transformations and<br>ranking. Technical report, CARNEGIE-MELLON<br>UNIV PITTSBURGH PA LANGUAGE TECH-<br>NOLOGIES INST.</p>",
            "id": 167,
            "page": 11,
            "text": "Michael Heilman and Noah A Smith. 2009. Question generation via overgenerating transformations and ranking. Technical report, CARNEGIE-MELLON UNIV PITTSBURGH PA LANGUAGE TECHNOLOGIES INST."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 1079
                },
                {
                    "x": 1216,
                    "y": 1079
                },
                {
                    "x": 1216,
                    "y": 1217
                },
                {
                    "x": 287,
                    "y": 1217
                }
            ],
            "category": "paragraph",
            "html": "<p id='168' style='font-size:16px'>Sepp Hochreiter and Jurgen Schmidhuber. 1997.<br>Long short-term memory. Neural computation,<br>9(8):1735-1780.</p>",
            "id": 168,
            "page": 11,
            "text": "Sepp Hochreiter and Jurgen Schmidhuber. 1997. Long short-term memory. Neural computation, 9(8):1735-1780."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 1261
                },
                {
                    "x": 1214,
                    "y": 1261
                },
                {
                    "x": 1214,
                    "y": 1676
                },
                {
                    "x": 288,
                    "y": 1676
                }
            ],
            "category": "paragraph",
            "html": "<p id='169' style='font-size:18px'>Zixian Huang, Yulin Shen, Xiao Li, Yu'ang Wei, Gong<br>Cheng, Lin Zhou, Xinyu Dai, and Yuzhong Qu.<br>2019. GeoSQA: A benchmark for scenario-based<br>question answering in the geography domain at high<br>school level. In Proceedings of the 2019 Confer-<br>ence on Empirical Methods in Natural Language<br>Processing and the 9th International Joint Confer-<br>ence on Natural Language Processing (EMNLP-<br>IJCNLP), pages 5866-5871.</p>",
            "id": 169,
            "page": 11,
            "text": "Zixian Huang, Yulin Shen, Xiao Li, Yu'ang Wei, Gong Cheng, Lin Zhou, Xinyu Dai, and Yuzhong Qu. 2019. GeoSQA: A benchmark for scenario-based question answering in the geography domain at high school level. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pages 5866-5871."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 1716
                },
                {
                    "x": 1214,
                    "y": 1716
                },
                {
                    "x": 1214,
                    "y": 1949
                },
                {
                    "x": 289,
                    "y": 1949
                }
            ],
            "category": "paragraph",
            "html": "<p id='170' style='font-size:14px'>Drew A Hudson and Christopher D Manning. 2019.<br>GQA: A new dataset for real-world visual reason-<br>ing and compositional question answering. In Pro-<br>ceedings of the IEEE Conference on Computer Vi-<br>sion and Pattern Recognition, pages 6700-6709.</p>",
            "id": 170,
            "page": 11,
            "text": "Drew A Hudson and Christopher D Manning. 2019. GQA: A new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6700-6709."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 1990
                },
                {
                    "x": 1213,
                    "y": 1990
                },
                {
                    "x": 1213,
                    "y": 2222
                },
                {
                    "x": 289,
                    "y": 2222
                }
            ],
            "category": "paragraph",
            "html": "<p id='171' style='font-size:18px'>Zhen Jia, Abdalghani Abujabal, Rishiraj Saha Roy, Jan-<br>nik Strotgen, and Gerhard Weikum. 2018. Tem-<br>pQuestions: A benchmark for temporal question an-<br>swering. In Companion Proceedings of the The Web<br>Conference 2018, pages 1057-1062.</p>",
            "id": 171,
            "page": 11,
            "text": "Zhen Jia, Abdalghani Abujabal, Rishiraj Saha Roy, Jannik Strotgen, and Gerhard Weikum. 2018. TempQuestions: A benchmark for temporal question answering. In Companion Proceedings of the The Web Conference 2018, pages 1057-1062."
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 2263
                },
                {
                    "x": 1213,
                    "y": 2263
                },
                {
                    "x": 1213,
                    "y": 2584
                },
                {
                    "x": 286,
                    "y": 2584
                }
            ],
            "category": "paragraph",
            "html": "<p id='172' style='font-size:16px'>Justin Johnson, Bharath Hariharan, Laurens van der<br>Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross<br>Girshick. 2017. CLEVR: A diagnostic dataset for<br>compositional language and elementary visual rea-<br>soning. In Proceedings of the IEEE Conference<br>on Computer Vision and Pattern Recognition, pages<br>2901-2910.</p>",
            "id": 172,
            "page": 11,
            "text": "Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. 2017. CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2901-2910."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 2629
                },
                {
                    "x": 1213,
                    "y": 2629
                },
                {
                    "x": 1213,
                    "y": 2906
                },
                {
                    "x": 290,
                    "y": 2906
                }
            ],
            "category": "paragraph",
            "html": "<p id='173' style='font-size:20px'>Daniel Khashabi, Snigdha Chaturvedi, Michael Roth,<br>Shyam Upadhyay, and Dan Roth. 2018. Looking<br>Beyond the Surface:a challenge set for reading com-<br>prehension over multiple sentences. In Proceedings<br>of North American Chapter of the Association for<br>Computational Linguistics (NAACL).</p>",
            "id": 173,
            "page": 11,
            "text": "Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. 2018. Looking Beyond the Surface:a challenge set for reading comprehension over multiple sentences. In Proceedings of North American Chapter of the Association for Computational Linguistics (NAACL)."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 2948
                },
                {
                    "x": 1215,
                    "y": 2948
                },
                {
                    "x": 1215,
                    "y": 3226
                },
                {
                    "x": 290,
                    "y": 3226
                }
            ],
            "category": "paragraph",
            "html": "<p id='174' style='font-size:16px'>Daniel Khashabi, Sewon Min, Tushar Khot, Ashish<br>Sabharwal, Oyvind Tafjord, Peter Clark, and Han-<br>naneh Hajishirzi. 2020. UnifiedQA: Crossing for-<br>mat boundaries with a single QA system. In Find-<br>ings of the Association for Computational Linguis-<br>tics: EMNLP 2020, pages 1896-1907.</p>",
            "id": 174,
            "page": 11,
            "text": "Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. 2020. UnifiedQA: Crossing format boundaries with a single QA system. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1896-1907."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 303
                },
                {
                    "x": 2197,
                    "y": 303
                },
                {
                    "x": 2197,
                    "y": 628
                },
                {
                    "x": 1271,
                    "y": 628
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='175' style='font-size:18px'>Hyounghun Kim, Abhaysinh Zala, Graham Burri, Hao<br>Tan, and Mohit Bansal. 2020. ArraMon: A joint<br>navigation-assembly instruction interpretation task<br>in dynamic environments. In Findings of the Associ-<br>ation for Computational Linguistics: EMNLP 2020,<br>pages 3910-3927, Online. Association for Computa-<br>tional Linguistics.</p>",
            "id": 175,
            "page": 11,
            "text": "Hyounghun Kim, Abhaysinh Zala, Graham Burri, Hao Tan, and Mohit Bansal. 2020. ArraMon: A joint navigation-assembly instruction interpretation task in dynamic environments. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3910-3927, Online. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 669
                },
                {
                    "x": 2197,
                    "y": 669
                },
                {
                    "x": 2197,
                    "y": 991
                },
                {
                    "x": 1269,
                    "y": 991
                }
            ],
            "category": "paragraph",
            "html": "<p id='176' style='font-size:14px'>Parisa Kordjamshidi, Marie-Francine Moens, and Mar-<br>tijn van Otterlo. 2010. Spatial Role Labeling: Task<br>definition and annotation scheme. In Proceedings<br>of the Seventh conference on International Lan-<br>guage Resources and Evaluation (LREC'10), pages<br>413-420. European Language Resources Associa-<br>tion (ELRA).</p>",
            "id": 176,
            "page": 11,
            "text": "Parisa Kordjamshidi, Marie-Francine Moens, and Martijn van Otterlo. 2010. Spatial Role Labeling: Task definition and annotation scheme. In Proceedings of the Seventh conference on International Language Resources and Evaluation (LREC'10), pages 413-420. European Language Resources Association (ELRA)."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 1032
                },
                {
                    "x": 2197,
                    "y": 1032
                },
                {
                    "x": 2197,
                    "y": 1354
                },
                {
                    "x": 1270,
                    "y": 1354
                }
            ],
            "category": "paragraph",
            "html": "<p id='177' style='font-size:16px'>Tom Kwiatkowski, Jennimaria Palomaki, Olivia Red-<br>field, Michael Collins, Ankur Parikh, Chris Alberti,<br>Danielle Epstein, Illia Polosukhin, Jacob Devlin,<br>Kenton Lee, et al. 2019. Natural Questions: a bench-<br>mark for question answering research. Transactions<br>of the Association for Computational Linguistics,<br>7:453-466.</p>",
            "id": 177,
            "page": 11,
            "text": "Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee,  2019. Natural Questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453-466."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 1399
                },
                {
                    "x": 2198,
                    "y": 1399
                },
                {
                    "x": 2198,
                    "y": 1719
                },
                {
                    "x": 1270,
                    "y": 1719
                }
            ],
            "category": "paragraph",
            "html": "<p id='178' style='font-size:18px'>Igor Labutov, Sumit Basu, and Lucy Vanderwende.<br>2015. Deep questions without deep understanding.<br>In Proceedings of the 53rd Annual Meeting of the<br>Association for Computational Linguistics and the<br>7th International Joint Conference on Natural Lan-<br>guage Processing (Volume 1: Long Papers), pages<br>889-898.</p>",
            "id": 178,
            "page": 11,
            "text": "Igor Labutov, Sumit Basu, and Lucy Vanderwende. 2015. Deep questions without deep understanding. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 889-898."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 1763
                },
                {
                    "x": 2196,
                    "y": 1763
                },
                {
                    "x": 2196,
                    "y": 1996
                },
                {
                    "x": 1271,
                    "y": 1996
                }
            ],
            "category": "paragraph",
            "html": "<p id='179' style='font-size:18px'>Zhenzhong Lan, Mingda Chen, Sebastian Goodman,<br>Kevin Gimpel, Piyush Sharma, and Radu Soricut.<br>2020. ALBERT: A lite bert for self-supervised learn-<br>ing of language representations. In International<br>Conference on Learning Representations.</p>",
            "id": 179,
            "page": 11,
            "text": "Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2020. ALBERT: A lite bert for self-supervised learning of language representations. In International Conference on Learning Representations."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 2036
                },
                {
                    "x": 2196,
                    "y": 2036
                },
                {
                    "x": 2196,
                    "y": 2222
                },
                {
                    "x": 1271,
                    "y": 2222
                }
            ],
            "category": "paragraph",
            "html": "<p id='180' style='font-size:14px'>Christian Landsiedel, Verena Rieser, Matthew Walter,<br>and Dirk Wollherr. 2017. A review of spatial rea-<br>soning and interaction for real-world robotics. Ad-<br>vanced Robotics, 31(5):222-242.</p>",
            "id": 180,
            "page": 11,
            "text": "Christian Landsiedel, Verena Rieser, Matthew Walter, and Dirk Wollherr. 2017. A review of spatial reasoning and interaction for real-world robotics. Advanced Robotics, 31(5):222-242."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 2263
                },
                {
                    "x": 2194,
                    "y": 2263
                },
                {
                    "x": 2194,
                    "y": 2404
                },
                {
                    "x": 1270,
                    "y": 2404
                }
            ],
            "category": "paragraph",
            "html": "<p id='181' style='font-size:16px'>Omer Levy, Minjoon Seo, Eunsol Choi, and Luke<br>Zettlemoyer. 2017. Zero-Shot relation extraction via<br>reading comprehension. In CONLL, pages 333-342.</p>",
            "id": 181,
            "page": 11,
            "text": "Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. 2017. Zero-Shot relation extraction via reading comprehension. In CONLL, pages 333-342."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2445
                },
                {
                    "x": 2197,
                    "y": 2445
                },
                {
                    "x": 2197,
                    "y": 2675
                },
                {
                    "x": 1273,
                    "y": 2675
                }
            ],
            "category": "paragraph",
            "html": "<p id='182' style='font-size:16px'>Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming<br>He, and Piotr Dollar. 2017. Focal loss for dense ob-<br>ject detection. In Proceedings of the IEEE interna-<br>tional conference on computer vision, pages 2980-<br>2988.</p>",
            "id": 182,
            "page": 11,
            "text": "Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. 2017. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pages 29802988."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 2720
                },
                {
                    "x": 2196,
                    "y": 2720
                },
                {
                    "x": 2196,
                    "y": 2858
                },
                {
                    "x": 1270,
                    "y": 2858
                }
            ],
            "category": "paragraph",
            "html": "<p id='183' style='font-size:16px'>Ilya Loshchilov and Frank Hutter. 2017. Decou-<br>pled weight decay regularization. arXiv preprint<br>arXiv:1711.05101.</p>",
            "id": 183,
            "page": 11,
            "text": "Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2902
                },
                {
                    "x": 2197,
                    "y": 2902
                },
                {
                    "x": 2197,
                    "y": 3227
                },
                {
                    "x": 1272,
                    "y": 3227
                }
            ],
            "category": "paragraph",
            "html": "<p id='184' style='font-size:20px'>Julian Michael, Gabriel Stanovsky, Luheng He, Ido<br>Dagan, and Luke Zettlemoyer. 2018. Crowdsourc-<br>ing question-answer meaning representations. In<br>Proceedings of the 2018 Conference of the North<br>American Chapter of the Association for Computa-<br>tional Linguistics: Human Language Technologies,<br>Volume 2 (Short Papers), pages 560-568.</p>",
            "id": 184,
            "page": 11,
            "text": "Julian Michael, Gabriel Stanovsky, Luheng He, Ido Dagan, and Luke Zettlemoyer. 2018. Crowdsourcing question-answer meaning representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 560-568."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 302
                },
                {
                    "x": 1214,
                    "y": 302
                },
                {
                    "x": 1214,
                    "y": 626
                },
                {
                    "x": 288,
                    "y": 626
                }
            ],
            "category": "paragraph",
            "html": "<p id='185' style='font-size:18px'>Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-<br>sky. 2009. Distant supervision for relation extrac-<br>tion without labeled data. In Proceedings of the<br>Joint Conference of the 47th Annual Meeting of the<br>ACL and the 4th International Joint Conference on<br>Natural Language Processing of the AFNLP, pages<br>1003-1011.</p>",
            "id": 185,
            "page": 12,
            "text": "Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without labeled data. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 1003-1011."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 660
                },
                {
                    "x": 1213,
                    "y": 660
                },
                {
                    "x": 1213,
                    "y": 936
                },
                {
                    "x": 289,
                    "y": 936
                }
            ],
            "category": "paragraph",
            "html": "<p id='186' style='font-size:22px'>Qiang Ning, Hao Wu, Rujun Han, Nanyun Peng, Matt<br>Gardner, and Dan Roth. 2020. TORQUE: A reading<br>comprehension dataset of temporal ordering ques-<br>tions. In Proceedings of the 2020 Conference on<br>Empirical Methods in Natural Language Processing<br>(EMNLP), pages 1158-1172.</p>",
            "id": 186,
            "page": 12,
            "text": "Qiang Ning, Hao Wu, Rujun Han, Nanyun Peng, Matt Gardner, and Dan Roth. 2020. TORQUE: A reading comprehension dataset of temporal ordering questions. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1158-1172."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 968
                },
                {
                    "x": 1213,
                    "y": 968
                },
                {
                    "x": 1213,
                    "y": 1152
                },
                {
                    "x": 289,
                    "y": 1152
                }
            ],
            "category": "paragraph",
            "html": "<p id='187' style='font-size:16px'>Jason Phang, Thibault Fevry, and Samuel R Bowman.<br>2018. Sentence encoders on stilts: Supplementary<br>training on intermediate labeled-data tasks. arXiv<br>preprint arXiv:1811.01088.</p>",
            "id": 187,
            "page": 12,
            "text": "Jason Phang, Thibault Fevry, and Samuel R Bowman. 2018. Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks. arXiv preprint arXiv:1811.01088."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 1184
                },
                {
                    "x": 1213,
                    "y": 1184
                },
                {
                    "x": 1213,
                    "y": 1504
                },
                {
                    "x": 288,
                    "y": 1504
                }
            ],
            "category": "paragraph",
            "html": "<p id='188' style='font-size:20px'>Taher Rahgooy, Umar Manzoor, and Parisa Kord-<br>jamshidi. 2018. Visually guided spatial relation ex-<br>traction from text. In Proceedings of the 2018 Con-<br>ference of the North American Chapter of the Asso-<br>ciation for Computational Linguistics: Human Lan-<br>guage Technologies, Volume 2 (Short Papers), pages<br>788-794.</p>",
            "id": 188,
            "page": 12,
            "text": "Taher Rahgooy, Umar Manzoor, and Parisa Kordjamshidi. 2018. Visually guided spatial relation extraction from text. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 788-794."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 1541
                },
                {
                    "x": 1213,
                    "y": 1541
                },
                {
                    "x": 1213,
                    "y": 1772
                },
                {
                    "x": 289,
                    "y": 1772
                }
            ],
            "category": "paragraph",
            "html": "<p id='189' style='font-size:20px'>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and<br>Percy Liang. 2016. SQuAD: 100,000+ questions for<br>machine comprehension of text. In Proceedings of<br>the 2016 Conference on Empirical Methods in Natu-<br>ral Language Processing, pages 2383-2392.</p>",
            "id": 189,
            "page": 12,
            "text": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383-2392."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 1803
                },
                {
                    "x": 1214,
                    "y": 1803
                },
                {
                    "x": 1214,
                    "y": 2080
                },
                {
                    "x": 290,
                    "y": 2080
                }
            ],
            "category": "paragraph",
            "html": "<p id='190' style='font-size:20px'>Homero Roman Roman, Yonatan Bisk, Jesse Thoma-<br>son, Asli Celikyilmaz, and Jianfeng Gao. 2020.<br>RMM: A recursive mental model for dialogue nav-<br>igation. In Findings of the Association for Computa-<br>tional Linguistics: EMNLP 2020, pages 1732-1745,<br>Online. Association for Computational Linguistics.</p>",
            "id": 190,
            "page": 12,
            "text": "Homero Roman Roman, Yonatan Bisk, Jesse Thomason, Asli Celikyilmaz, and Jianfeng Gao. 2020. RMM: A recursive mental model for dialogue navigation. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1732-1745, Online. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 2112
                },
                {
                    "x": 1213,
                    "y": 2112
                },
                {
                    "x": 1213,
                    "y": 2340
                },
                {
                    "x": 289,
                    "y": 2340
                }
            ],
            "category": "paragraph",
            "html": "<p id='191' style='font-size:18px'>Priyanka Sen and Amir Saffari. 2020. What do models<br>learn from question answering datasets? In Proceed-<br>ings of the 2020 Conference on Empirical Methods<br>in Natural Language Processing (EMNLP), pages<br>2429-2438.</p>",
            "id": 191,
            "page": 12,
            "text": "Priyanka Sen and Amir Saffari. 2020. What do models learn from question answering datasets? In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2429-2438."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 2376
                },
                {
                    "x": 1214,
                    "y": 2376
                },
                {
                    "x": 1214,
                    "y": 2696
                },
                {
                    "x": 290,
                    "y": 2696
                }
            ],
            "category": "paragraph",
            "html": "<p id='192' style='font-size:18px'>Alane Suhr, Mike Lewis, James Yeh, and Yoav Artzi.<br>2017. A corpus of natural language for visual rea-<br>soning. In Proceedings of the 55th Annual Meet-<br>ing of the Associationfor Computational Linguistics<br>(Volume 2: Short Papers), pages 217-223, Vancou-<br>ver, Canada. Association for Computational Linguis-<br>tics.</p>",
            "id": 192,
            "page": 12,
            "text": "Alane Suhr, Mike Lewis, James Yeh, and Yoav Artzi. 2017. A corpus of natural language for visual reasoning. In Proceedings of the 55th Annual Meeting of the Associationfor Computational Linguistics (Volume 2: Short Papers), pages 217-223, Vancouver, Canada. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 2731
                },
                {
                    "x": 1215,
                    "y": 2731
                },
                {
                    "x": 1215,
                    "y": 3006
                },
                {
                    "x": 290,
                    "y": 3006
                }
            ],
            "category": "paragraph",
            "html": "<p id='193' style='font-size:20px'>Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang,<br>Huajun Bai, and Yoav Artzi. 2019. A corpus for<br>reasoning about natural language grounded in pho-<br>tographs. In Proceedings of the 57th Annual Meet-<br>ing ofthe Associationfor Computational Linguistics,<br>pages 6418-6428.</p>",
            "id": 193,
            "page": 12,
            "text": "Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, and Yoav Artzi. 2019. A corpus for reasoning about natural language grounded in photographs. In Proceedings of the 57th Annual Meeting ofthe Associationfor Computational Linguistics, pages 6418-6428."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 3040
                },
                {
                    "x": 1214,
                    "y": 3040
                },
                {
                    "x": 1214,
                    "y": 3226
                },
                {
                    "x": 289,
                    "y": 3226
                }
            ],
            "category": "paragraph",
            "html": "<p id='194' style='font-size:18px'>Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang,<br>Adam Poliak, R Thomas McCoy, Najoung Kim,<br>Benjamin Van Durme, Samuel R Bowman, Dipan-<br>jan Das, et al. 2019. What do you learn from</p>",
            "id": 194,
            "page": 12,
            "text": "Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam Poliak, R Thomas McCoy, Najoung Kim, Benjamin Van Durme, Samuel R Bowman, Dipanjan Das,  2019. What do you learn from"
        },
        {
            "bounding_box": [
                {
                    "x": 1314,
                    "y": 306
                },
                {
                    "x": 2197,
                    "y": 306
                },
                {
                    "x": 2197,
                    "y": 442
                },
                {
                    "x": 1314,
                    "y": 442
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='195' style='font-size:14px'>context? probing for sentence structure in con-<br>textualized word representations. arXiv preprint<br>arXiv:1905.06316.</p>",
            "id": 195,
            "page": 12,
            "text": "context? probing for sentence structure in contextualized word representations. arXiv preprint arXiv:1905.06316."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 477
                },
                {
                    "x": 2197,
                    "y": 477
                },
                {
                    "x": 2197,
                    "y": 757
                },
                {
                    "x": 1271,
                    "y": 757
                }
            ],
            "category": "paragraph",
            "html": "<p id='196' style='font-size:20px'>Takuma Udagawa, Takato Yamazaki, and Akiko<br>Aizawa. 2020. A linguistic analysis of visually<br>grounded dialogues based on spatial expressions. In<br>Findings of the Association for Computational Lin-<br>guistics: EMNLP 2020, pages 750-765, Online. As-<br>sociation for Computational Linguistics.</p>",
            "id": 196,
            "page": 12,
            "text": "Takuma Udagawa, Takato Yamazaki, and Akiko Aizawa. 2020. A linguistic analysis of visually grounded dialogues based on spatial expressions. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 750-765, Online. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 790
                },
                {
                    "x": 2199,
                    "y": 790
                },
                {
                    "x": 2199,
                    "y": 1113
                },
                {
                    "x": 1270,
                    "y": 1113
                }
            ],
            "category": "paragraph",
            "html": "<p id='197' style='font-size:20px'>Alex Wang, Amanpreet Singh, Julian Michael, Fe-<br>lix Hill, Omer Levy, and Samuel Bowman. 2018.<br>GLUE: A multi-task benchmark and analysis plat-<br>form for natural language understanding. In Pro-<br>ceedings of the 2018 EMNLP Workshop Black-<br>boxNLP: Analyzing and Interpreting Neural Net-<br>works for NLP, pages 353-355.</p>",
            "id": 197,
            "page": 12,
            "text": "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353-355."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 1148
                },
                {
                    "x": 2198,
                    "y": 1148
                },
                {
                    "x": 2198,
                    "y": 1378
                },
                {
                    "x": 1268,
                    "y": 1378
                }
            ],
            "category": "paragraph",
            "html": "<p id='198' style='font-size:16px'>Jason Weston, Antoine Bordes, Sumit Chopra, Alexan-<br>der M Rush, Bart van Merri�nboer, Armand Joulin,<br>and Tomas Mikolov. 2015. Towards ai-complete<br>question answering: A set of prerequisite toy tasks.<br>arXiv preprint arXiv:1502.05698.</p>",
            "id": 198,
            "page": 12,
            "text": "Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart van Merri�nboer, Armand Joulin, and Tomas Mikolov. 2015. Towards ai-complete question answering: A set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 1411
                },
                {
                    "x": 2198,
                    "y": 1411
                },
                {
                    "x": 2198,
                    "y": 1645
                },
                {
                    "x": 1269,
                    "y": 1645
                }
            ],
            "category": "paragraph",
            "html": "<p id='199' style='font-size:20px'>Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-<br>bonell, Russ R Salakhutdinov, and Quoc V Le. 2019.<br>XLNet: Generalized autoregressive pretraining for<br>language understanding. In Advances in neural in-<br>formation processing systems, pages 5754-5764.</p>",
            "id": 199,
            "page": 12,
            "text": "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. 2019. XLNet: Generalized autoregressive pretraining for language understanding. In Advances in neural information processing systems, pages 5754-5764."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 1679
                },
                {
                    "x": 2198,
                    "y": 1679
                },
                {
                    "x": 2198,
                    "y": 1909
                },
                {
                    "x": 1270,
                    "y": 1909
                }
            ],
            "category": "paragraph",
            "html": "<p id='200' style='font-size:22px'>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,<br>William Cohen, Ruslan Salakhutdinov, and Christo-<br>pher D. Manning. 2018. HotpotQA: A dataset for<br>diverse, explainable multi-hop question answering.<br>pages 2369-2380.</p>",
            "id": 200,
            "page": 12,
            "text": "Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. pages 2369-2380."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 1945
                },
                {
                    "x": 2198,
                    "y": 1945
                },
                {
                    "x": 2198,
                    "y": 2218
                },
                {
                    "x": 1271,
                    "y": 2218
                }
            ],
            "category": "paragraph",
            "html": "<p id='201' style='font-size:16px'>Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao.<br>2015. Distant supervision for relation extraction via<br>piecewise convolutional neural networks. In Pro-<br>ceedings of the 2015 conference on empirical meth-<br>ods in natural language processing, pages 1753-<br>1762.</p>",
            "id": 201,
            "page": 12,
            "text": "Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao. 2015. Distant supervision for relation extraction via piecewise convolutional neural networks. In Proceedings of the 2015 conference on empirical methods in natural language processing, pages 17531762."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 2254
                },
                {
                    "x": 2195,
                    "y": 2254
                },
                {
                    "x": 2195,
                    "y": 2489
                },
                {
                    "x": 1270,
                    "y": 2489
                }
            ],
            "category": "paragraph",
            "html": "<p id='202' style='font-size:20px'>Ben Zhou, Qiang Ning, Daniel Khashabi, and Dan<br>Roth. 2020. Temporal common sense acquisition<br>with minimal supervision. In Proceedings of the<br>58th Annual Meeting of the Association for Compu-<br>tational Linguistics, pages 7579-7589.</p>",
            "id": 202,
            "page": 12,
            "text": "Ben Zhou, Qiang Ning, Daniel Khashabi, and Dan Roth. 2020. Temporal common sense acquisition with minimal supervision. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7579-7589."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 298
                },
                {
                    "x": 1097,
                    "y": 298
                },
                {
                    "x": 1097,
                    "y": 409
                },
                {
                    "x": 288,
                    "y": 409
                }
            ],
            "category": "paragraph",
            "html": "<p id='203' style='font-size:22px'>A Question Templates and statistics<br>Information</p>",
            "id": 203,
            "page": 13,
            "text": "A Question Templates and statistics Information"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 444
                },
                {
                    "x": 1213,
                    "y": 444
                },
                {
                    "x": 1213,
                    "y": 836
                },
                {
                    "x": 288,
                    "y": 836
                }
            ],
            "category": "paragraph",
            "html": "<p id='204' style='font-size:20px'>Table 7 shows the templates used to create ques-<br>tions in SPARTQA-AUTO. The \"<object>\" is a<br>variable replaced by objects from the story (us-<br>ing Choose-objects and Describe-objects modules),<br>and the \"<relation>\" variable can be replaced by<br>the chosen relations between objects (using Find-<br>all-relations module).</p>",
            "id": 204,
            "page": 13,
            "text": "Table 7 shows the templates used to create questions in SPARTQA-AUTO. The \"<object>\" is a variable replaced by objects from the story (using Choose-objects and Describe-objects modules), and the \"<relation>\" variable can be replaced by the chosen relations between objects (using Findall-relations module)."
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 841
                },
                {
                    "x": 1212,
                    "y": 841
                },
                {
                    "x": 1212,
                    "y": 1401
                },
                {
                    "x": 286,
                    "y": 1401
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='205' style='font-size:18px'>The articles and the indefinite pronouns in each<br>template play an essential role in understanding<br>the question's objective. For example, \"Are all<br>blue circles near to a triangle?\" is different from<br>\"Are there any blue circles near to a triangle?\", and<br>\"Are there any blue circles near to all triangles?\".<br>Therefore, we check the uniqueness of the object<br>definition, using \"a\" or \"the\" in proper places and<br>randomly place the terms \"any\" or \"all\" in the YN<br>questions to generate different questions.</p>",
            "id": 205,
            "page": 13,
            "text": "The articles and the indefinite pronouns in each template play an essential role in understanding the question's objective. For example, \"Are all blue circles near to a triangle?\" is different from \"Are there any blue circles near to a triangle?\", and \"Are there any blue circles near to all triangles?\". Therefore, we check the uniqueness of the object definition, using \"a\" or \"the\" in proper places and randomly place the terms \"any\" or \"all\" in the YN questions to generate different questions."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 1406
                },
                {
                    "x": 1213,
                    "y": 1406
                },
                {
                    "x": 1213,
                    "y": 1570
                },
                {
                    "x": 288,
                    "y": 1570
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='206' style='font-size:16px'>Table 8 shows the percentage of correct labels in<br>train and test sets. In multi-choice Q-TYPES, more<br>than one label can be true.</p>",
            "id": 206,
            "page": 13,
            "text": "Table 8 shows the percentage of correct labels in train and test sets. In multi-choice Q-TYPES, more than one label can be true."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 1612
                },
                {
                    "x": 896,
                    "y": 1612
                },
                {
                    "x": 896,
                    "y": 1668
                },
                {
                    "x": 289,
                    "y": 1668
                }
            ],
            "category": "paragraph",
            "html": "<p id='207' style='font-size:20px'>B Sentences of the Dataset</p>",
            "id": 207,
            "page": 13,
            "text": "B Sentences of the Dataset"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 1700
                },
                {
                    "x": 1212,
                    "y": 1700
                },
                {
                    "x": 1212,
                    "y": 1926
                },
                {
                    "x": 287,
                    "y": 1926
                }
            ],
            "category": "paragraph",
            "html": "<p id='208' style='font-size:18px'>Table 10 shows some generated sentences in<br>SPARTQA-AUTO with some specific features that<br>challenge models to understand different forms of<br>relation description in spatial language.</p>",
            "id": 208,
            "page": 13,
            "text": "Table 10 shows some generated sentences in SPARTQA-AUTO with some specific features that challenge models to understand different forms of relation description in spatial language."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 1964
                },
                {
                    "x": 957,
                    "y": 1964
                },
                {
                    "x": 957,
                    "y": 2020
                },
                {
                    "x": 289,
                    "y": 2020
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='209' style='font-size:22px'>C Additional Evaluation Sets</p>",
            "id": 209,
            "page": 13,
            "text": "C Additional Evaluation Sets"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 2052
                },
                {
                    "x": 1213,
                    "y": 2052
                },
                {
                    "x": 1213,
                    "y": 2219
                },
                {
                    "x": 288,
                    "y": 2219
                }
            ],
            "category": "paragraph",
            "html": "<p id='210' style='font-size:16px'>Here we describe three extra evaluation sets pro-<br>vided with this dataset in more detail, including<br>unseen test, consistency, and contrast sets.</p>",
            "id": 210,
            "page": 13,
            "text": "Here we describe three extra evaluation sets provided with this dataset in more detail, including unseen test, consistency, and contrast sets."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 2257
                },
                {
                    "x": 850,
                    "y": 2257
                },
                {
                    "x": 850,
                    "y": 2310
                },
                {
                    "x": 290,
                    "y": 2310
                }
            ],
            "category": "paragraph",
            "html": "<p id='211' style='font-size:18px'>C.1 Unseen Evaluation Set</p>",
            "id": 211,
            "page": 13,
            "text": "C.1 Unseen Evaluation Set"
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 2330
                },
                {
                    "x": 1214,
                    "y": 2330
                },
                {
                    "x": 1214,
                    "y": 3006
                },
                {
                    "x": 286,
                    "y": 3006
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='212' style='font-size:18px'>We propose an unseen test set alongside the seen<br>test of SPARTQA-AUTO to check whether a model<br>is using shortcuts in the language surface by de-<br>scribing objects and relations with new vocabular-<br>ies in the samples. This set has minor modifications<br>that should not affect the performance of a consis-<br>tent and reliable model. The modifications are ran-<br>domly applied on a number of generated stories and<br>questions and include changing names of shapes,<br>colors, sizes, and relationships' names (describing<br>relationships using different language expressions).<br>The modification choices are described in Table 9.</p>",
            "id": 212,
            "page": 13,
            "text": "We propose an unseen test set alongside the seen test of SPARTQA-AUTO to check whether a model is using shortcuts in the language surface by describing objects and relations with new vocabularies in the samples. This set has minor modifications that should not affect the performance of a consistent and reliable model. The modifications are randomly applied on a number of generated stories and questions and include changing names of shapes, colors, sizes, and relationships' names (describing relationships using different language expressions). The modification choices are described in Table 9."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 3043
                },
                {
                    "x": 1135,
                    "y": 3043
                },
                {
                    "x": 1135,
                    "y": 3096
                },
                {
                    "x": 288,
                    "y": 3096
                }
            ],
            "category": "paragraph",
            "html": "<p id='213' style='font-size:20px'>C.2 Contrast and Consistency Evaluation</p>",
            "id": 213,
            "page": 13,
            "text": "C.2 Contrast and Consistency Evaluation"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 3117
                },
                {
                    "x": 1218,
                    "y": 3117
                },
                {
                    "x": 1218,
                    "y": 3227
                },
                {
                    "x": 287,
                    "y": 3227
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='214' style='font-size:18px'>For probing the consistency and semantic sensitiv-<br>ity of models, we provide two extra evaluation test</p>",
            "id": 214,
            "page": 13,
            "text": "For probing the consistency and semantic sensitivity of models, we provide two extra evaluation test"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 303
                },
                {
                    "x": 1859,
                    "y": 303
                },
                {
                    "x": 1859,
                    "y": 351
                },
                {
                    "x": 1268,
                    "y": 351
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='215' style='font-size:18px'>sets, Consistency and Contrast'.</p>",
            "id": 215,
            "page": 13,
            "text": "sets, Consistency and Contrast'."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 363
                },
                {
                    "x": 2196,
                    "y": 363
                },
                {
                    "x": 2196,
                    "y": 1149
                },
                {
                    "x": 1268,
                    "y": 1149
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='216' style='font-size:18px'>Consistency set is made by changing parts of<br>the question in a way that it still asks about the<br>same information (Hudson and Manning, 2019;<br>Suhr et al., 2019). For instance, for the question,<br>\"What is the relation between the blue circle and<br>the big shape? Left,\" we create a similar question<br>in the form of \"What is the relation between the big<br>shape and the blue circle? Right\". Answering these<br>questions around a pivot question is possible for<br>human without the need for extra reasoning over<br>the story and based on the main questions' answer.<br>Hence, the evaluation on this set shows that models<br>understand the real underlying semantics rather<br>than overfit on the structure of questions.</p>",
            "id": 216,
            "page": 13,
            "text": "Consistency set is made by changing parts of the question in a way that it still asks about the same information (Hudson and Manning, 2019; Suhr , 2019). For instance, for the question, \"What is the relation between the blue circle and the big shape? Left,\" we create a similar question in the form of \"What is the relation between the big shape and the blue circle? Right\". Answering these questions around a pivot question is possible for human without the need for extra reasoning over the story and based on the main questions' answer. Hence, the evaluation on this set shows that models understand the real underlying semantics rather than overfit on the structure of questions."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 1156
                },
                {
                    "x": 2195,
                    "y": 1156
                },
                {
                    "x": 2195,
                    "y": 1718
                },
                {
                    "x": 1268,
                    "y": 1718
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='217' style='font-size:18px'>Contrast set: This setis made by minor changes<br>in a question that changes the answer (Gardner<br>et al., 2020). As an instance, in the question \"Is<br>the blue circle below the black triangle? Yes,\" we<br>create a contrast question \"Is the blue circle below<br>all triangles? No\" by changing \"the black trinagle\"<br>to \"all triangles\". The evaluation on this set shows<br>the robustness of the model and its sensitivity to the<br>semantic changes when there are minor changes in<br>the language surface 10</p>",
            "id": 217,
            "page": 13,
            "text": "Contrast set: This setis made by minor changes in a question that changes the answer (Gardner , 2020). As an instance, in the question \"Is the blue circle below the black triangle? Yes,\" we create a contrast question \"Is the blue circle below all triangles? No\" by changing \"the black trinagle\" to \"all triangles\". The evaluation on this set shows the robustness of the model and its sensitivity to the semantic changes when there are minor changes in the language surface 10"
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 1770
                },
                {
                    "x": 1761,
                    "y": 1770
                },
                {
                    "x": 1761,
                    "y": 1825
                },
                {
                    "x": 1270,
                    "y": 1825
                }
            ],
            "category": "paragraph",
            "html": "<p id='218' style='font-size:20px'>D Extra Annotations</p>",
            "id": 218,
            "page": 13,
            "text": "D Extra Annotations"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 1868
                },
                {
                    "x": 2193,
                    "y": 1868
                },
                {
                    "x": 2193,
                    "y": 2089
                },
                {
                    "x": 1268,
                    "y": 2089
                }
            ],
            "category": "paragraph",
            "html": "<p id='219' style='font-size:18px'>Alongside the main SPARTQA-AUTO's stories and<br>questions we provided some extra annotation to<br>help the models to understand the spatial language<br>better.</p>",
            "id": 219,
            "page": 13,
            "text": "Alongside the main SPARTQA-AUTO's stories and questions we provided some extra annotation to help the models to understand the spatial language better."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 2142
                },
                {
                    "x": 2154,
                    "y": 2142
                },
                {
                    "x": 2154,
                    "y": 2198
                },
                {
                    "x": 1269,
                    "y": 2198
                }
            ],
            "category": "paragraph",
            "html": "<p id='220' style='font-size:20px'>D.1 Detailed Annotation and Scene-Graphs</p>",
            "id": 220,
            "page": 13,
            "text": "D.1 Detailed Annotation and Scene-Graphs"
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 2223
                },
                {
                    "x": 2196,
                    "y": 2223
                },
                {
                    "x": 2196,
                    "y": 2840
                },
                {
                    "x": 1269,
                    "y": 2840
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='221' style='font-size:18px'>Providing in-depth human annotations is quite ex-<br>pensive and time-consuming. In SPARTQA-AUTO,<br>we generated fine-grained scene-graph based on<br>the story. This scene-graph contains blocks' de-<br>scription, their relations, and the objects' attributes<br>alongside their direct relations with each other. The<br>scene-graphs can be used for the models to under-<br>stand all spatial relations directly mentioned in the<br>textual context. Figure 7 shows an example of this<br>scene-graph. The scene-graph can provide strong<br>supervision for question answering challenges and</p>",
            "id": 221,
            "page": 13,
            "text": "Providing in-depth human annotations is quite expensive and time-consuming. In SPARTQA-AUTO, we generated fine-grained scene-graph based on the story. This scene-graph contains blocks' description, their relations, and the objects' attributes alongside their direct relations with each other. The scene-graphs can be used for the models to understand all spatial relations directly mentioned in the textual context. Figure 7 shows an example of this scene-graph. The scene-graph can provide strong supervision for question answering challenges and"
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 2889
                },
                {
                    "x": 2193,
                    "y": 2889
                },
                {
                    "x": 2193,
                    "y": 2966
                },
                {
                    "x": 1270,
                    "y": 2966
                }
            ],
            "category": "paragraph",
            "html": "<p id='222' style='font-size:14px'>9for some questions, it is not possible to generate a com-<br>plementary set</p>",
            "id": 222,
            "page": 13,
            "text": "9for some questions, it is not possible to generate a complementary set"
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 2977
                },
                {
                    "x": 2195,
                    "y": 2977
                },
                {
                    "x": 2195,
                    "y": 3221
                },
                {
                    "x": 1269,
                    "y": 3221
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='223' style='font-size:14px'>10Based on the original contrast set paper, consistency and<br>contrast set should be generated manually to control the se-<br>mantic change. In our case that we are probing the spatial<br>language understanding of models, we must change parts that<br>affect spatial understanding, which can be implemented by<br>some static rules.</p>",
            "id": 223,
            "page": 13,
            "text": "10Based on the original contrast set paper, consistency and contrast set should be generated manually to control the semantic change. In our case that we are probing the spatial language understanding of models, we must change parts that affect spatial understanding, which can be implemented by some static rules."
        },
        {
            "bounding_box": [
                {
                    "x": 416,
                    "y": 281
                },
                {
                    "x": 2069,
                    "y": 281
                },
                {
                    "x": 2069,
                    "y": 992
                },
                {
                    "x": 416,
                    "y": 992
                }
            ],
            "category": "table",
            "html": "<table id='224' style='font-size:20px'><tr><td>Q-Type</td><td>Q-Templates</td><td>Candidate answer</td></tr><tr><td>FR</td><td>what is the relation between <object>and <object>?</td><td>Left, Right, Below, Above, Touching, Far from, Near to</td></tr><tr><td>CO</td><td>What is <relation >the <object>? an <objectl>or an <object2>? Which object is <relation >an <object>? the <object1>or the <object2>?</td><td>Object1, object2, Both, None</td></tr><tr><td>YN</td><td>Is (the I a )<object1><relation>(the a) <object2>? Is there any <object1>s <relation>all <object2>s?</td><td>Yes, No, Don't Know</td></tr><tr><td>FB</td><td>Which block has an <object>? Which block doesn't have an <object>?</td><td>Name of blocks, None</td></tr></table>",
            "id": 224,
            "page": 14,
            "text": "Q-Type Q-Templates Candidate answer  FR what is the relation between and ? Left, Right, Below, Above, Touching, Far from, Near to  CO What is the ? an or an ? Which object is an ? the or the ? Object1, object2, Both, None  YN Is (the I a )(the a) ? Is there any s all s? Yes, No, Don't Know  FB Which block has an ? Which block doesn't have an ?"
        },
        {
            "bounding_box": [
                {
                    "x": 878,
                    "y": 1022
                },
                {
                    "x": 1596,
                    "y": 1022
                },
                {
                    "x": 1596,
                    "y": 1074
                },
                {
                    "x": 878,
                    "y": 1074
                }
            ],
            "category": "caption",
            "html": "<caption id='225' style='font-size:16px'>Table 7: Questions and answers templates.</caption>",
            "id": 225,
            "page": 14,
            "text": "Table 7: Questions and answers templates."
        },
        {
            "bounding_box": [
                {
                    "x": 285,
                    "y": 1147
                },
                {
                    "x": 1214,
                    "y": 1147
                },
                {
                    "x": 1214,
                    "y": 1538
                },
                {
                    "x": 285,
                    "y": 1538
                }
            ],
            "category": "figure",
            "html": "<figure><img id='226' style='font-size:14px' alt=\"Mediun Mediun\nHas Left Has\nBlue 0 A C 0 Yellow\nCircle Below\nHas\nMediun Has\nB Below Triangle\nMediun\n0\nYellow\nBelow Has\n1 Black\nSquare\nSquare\nMediun\nYellow 1\nSquare\" data-coord=\"top-left:(285,1147); bottom-right:(1214,1538)\" /></figure>",
            "id": 226,
            "page": 14,
            "text": "Mediun Mediun Has Left Has Blue 0 A C 0 Yellow Circle Below Has Mediun Has B Below Triangle Mediun 0 Yellow Below Has 1 Black Square Square Mediun Yellow 1 Square"
        },
        {
            "bounding_box": [
                {
                    "x": 554,
                    "y": 1573
                },
                {
                    "x": 943,
                    "y": 1573
                },
                {
                    "x": 943,
                    "y": 1626
                },
                {
                    "x": 554,
                    "y": 1626
                }
            ],
            "category": "caption",
            "html": "<caption id='227' style='font-size:20px'>Figure 7: Scene-graph</caption>",
            "id": 227,
            "page": 14,
            "text": "Figure 7: Scene-graph"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 1721
                },
                {
                    "x": 1212,
                    "y": 1721
                },
                {
                    "x": 1212,
                    "y": 1831
                },
                {
                    "x": 287,
                    "y": 1831
                }
            ],
            "category": "paragraph",
            "html": "<p id='228' style='font-size:16px'>can be used to evaluate models based on their steps<br>of reasoning and decisions.</p>",
            "id": 228,
            "page": 14,
            "text": "can be used to evaluate models based on their steps of reasoning and decisions."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 1881
                },
                {
                    "x": 757,
                    "y": 1881
                },
                {
                    "x": 757,
                    "y": 1935
                },
                {
                    "x": 289,
                    "y": 1935
                }
            ],
            "category": "paragraph",
            "html": "<p id='229' style='font-size:20px'>D.2 SpRL Annotation</p>",
            "id": 229,
            "page": 14,
            "text": "D.2 SpRL Annotation"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 1960
                },
                {
                    "x": 1215,
                    "y": 1960
                },
                {
                    "x": 1215,
                    "y": 2469
                },
                {
                    "x": 288,
                    "y": 2469
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='230' style='font-size:18px'>We also provided spatial annotations for each sen-<br>tence and question, based on Spatial Role Labeling<br>(SpRL) annotation scheme (Kordjamshidi et al.,<br>2010)(Fig. 11). This annotation is generated by<br>hand-crafted rules during the main data generation.<br>SpRL is used for recognizing spatial expressions<br>and arguments in a sentence. This annotation is use-<br>ful for applications that need to detect and reason<br>about spatial expressions and arguments.</p>",
            "id": 230,
            "page": 14,
            "text": "We also provided spatial annotations for each sentence and question, based on Spatial Role Labeling (SpRL) annotation scheme (Kordjamshidi , 2010)(Fig. 11). This annotation is generated by hand-crafted rules during the main data generation. SpRL is used for recognizing spatial expressions and arguments in a sentence. This annotation is useful for applications that need to detect and reason about spatial expressions and arguments."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 2521
                },
                {
                    "x": 1088,
                    "y": 2521
                },
                {
                    "x": 1088,
                    "y": 2637
                },
                {
                    "x": 289,
                    "y": 2637
                }
            ],
            "category": "paragraph",
            "html": "<p id='231' style='font-size:22px'>E QA Language Models for Spatial<br>Reasoning over Text</p>",
            "id": 231,
            "page": 14,
            "text": "E QA Language Models for Spatial Reasoning over Text"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 2678
                },
                {
                    "x": 1213,
                    "y": 2678
                },
                {
                    "x": 1213,
                    "y": 2845
                },
                {
                    "x": 288,
                    "y": 2845
                }
            ],
            "category": "paragraph",
            "html": "<p id='232' style='font-size:18px'>Figures 8a and 8b depict the architecture used for<br>further fine-tuning language models on SPARTQA<br>described in section 5.</p>",
            "id": 232,
            "page": 14,
            "text": "Figures 8a and 8b depict the architecture used for further fine-tuning language models on SPARTQA described in section 5."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 2900
                },
                {
                    "x": 919,
                    "y": 2900
                },
                {
                    "x": 919,
                    "y": 2958
                },
                {
                    "x": 289,
                    "y": 2958
                }
            ],
            "category": "paragraph",
            "html": "<p id='233' style='font-size:22px'>F bAbI and boolQ Datasets</p>",
            "id": 233,
            "page": 14,
            "text": "F bAbI and boolQ Datasets"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 3000
                },
                {
                    "x": 1214,
                    "y": 3000
                },
                {
                    "x": 1214,
                    "y": 3107
                },
                {
                    "x": 287,
                    "y": 3107
                }
            ],
            "category": "paragraph",
            "html": "<p id='234' style='font-size:18px'>Figure 9 shows an example of the bAbI dataset (We-<br>ston et al., 2015) task 17.</p>",
            "id": 234,
            "page": 14,
            "text": "Figure 9 shows an example of the bAbI dataset (Weston , 2015) task 17."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 3117
                },
                {
                    "x": 1212,
                    "y": 3117
                },
                {
                    "x": 1212,
                    "y": 3227
                },
                {
                    "x": 288,
                    "y": 3227
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='235' style='font-size:16px'>To solve task 17 of bAbI , we implement two<br>SpRL+rule-based and neural network models. The</p>",
            "id": 235,
            "page": 14,
            "text": "To solve task 17 of bAbI , we implement two SpRL+rule-based and neural network models. The"
        },
        {
            "bounding_box": [
                {
                    "x": 1266,
                    "y": 1136
                },
                {
                    "x": 2193,
                    "y": 1136
                },
                {
                    "x": 2193,
                    "y": 2530
                },
                {
                    "x": 1266,
                    "y": 2530
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='236' style='font-size:14px' alt=\"Correct Answer y E {0,1}\nClassifier\nLSTM\nLast layer\nTCLS I C1 - 「SEP Q2 SEP Sm TSEP\nrepresentation\nLanguage Model\n(FB-CO)\nECLS EC FC ESEP Q2 FOr ESEP ESEP\nToken\nEmbedding CLS C1 Ck SEP Q1 Q2 Qn SEP S1 S2 Sm SEP\nSegment\n0 0 0 0 0 0 0 0 1 1 1\nEmbedding\nCandidate\nQuestion Story\noption\n(a) LMQA Architecture for CO and FB Q-TYPEs\nCorrect Answer y E {candidate answers}\nInference\nBod\nBo\nca\nBoolean classification\nca\ncandidate options 1\nLast layer\n「CLS Qn TSEP IS1 S2 IS3 Sm TSEP\nrepresentation\nLanguage Model\n(YN-FR)\nECLS =Qn ESEP ES1 S2 S3 -Sm ESEP\nToken\nEmbedding CLS Q1 Q2 Q3 Qn SEP S1 S2 S3 Sm SEP\nSegment\n0 0 0 0 0 0 1 1 1 1 1\nEmbedding\nQuestion Story\" data-coord=\"top-left:(1266,1136); bottom-right:(2193,2530)\" /></figure>",
            "id": 236,
            "page": 14,
            "text": "Correct Answer y E {0,1} Classifier LSTM Last layer TCLS I C1 - 「SEP Q2 SEP Sm TSEP representation Language Model (FB-CO) ECLS EC FC ESEP Q2 FOr ESEP ESEP Token Embedding CLS C1 Ck SEP Q1 Q2 Qn SEP S1 S2 Sm SEP Segment 0 0 0 0 0 0 0 0 1 1 1 Embedding Candidate Question Story option (a) LMQA Architecture for CO and FB Q-TYPEs Correct Answer y E {candidate answers} Inference Bod Bo ca Boolean classification ca candidate options 1 Last layer 「CLS Qn TSEP IS1 S2 IS3 Sm TSEP representation Language Model (YN-FR) ECLS =Qn ESEP ES1 S2 S3 -Sm ESEP Token Embedding CLS Q1 Q2 Q3 Qn SEP S1 S2 S3 Sm SEP Segment 0 0 0 0 0 0 1 1 1 1 1 Embedding Question Story"
        },
        {
            "bounding_box": [
                {
                    "x": 1352,
                    "y": 2539
                },
                {
                    "x": 2106,
                    "y": 2539
                },
                {
                    "x": 2106,
                    "y": 2583
                },
                {
                    "x": 1352,
                    "y": 2583
                }
            ],
            "category": "caption",
            "html": "<br><caption id='237' style='font-size:16px'>(b) LMQA Architecture for FR and YN Q-TYPES</caption>",
            "id": 237,
            "page": 14,
            "text": "(b) LMQA Architecture for FR and YN Q-TYPES"
        },
        {
            "bounding_box": [
                {
                    "x": 1317,
                    "y": 2625
                },
                {
                    "x": 2143,
                    "y": 2625
                },
                {
                    "x": 2143,
                    "y": 2673
                },
                {
                    "x": 1317,
                    "y": 2673
                }
            ],
            "category": "caption",
            "html": "<caption id='238' style='font-size:18px'>Figure 8: LMQA for Spatial Reasoning over Text</caption>",
            "id": 238,
            "page": 14,
            "text": "Figure 8: LMQA for Spatial Reasoning over Text"
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2762
                },
                {
                    "x": 2079,
                    "y": 2762
                },
                {
                    "x": 2079,
                    "y": 2850
                },
                {
                    "x": 1273,
                    "y": 2850
                }
            ],
            "category": "paragraph",
            "html": "<p id='239' style='font-size:14px'>\"The pink rectangle is below the red square.<br>The red square is below the blue square. , ,</p>",
            "id": 239,
            "page": 14,
            "text": "\"The pink rectangle is below the red square. The red square is below the blue square. , ,"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 2851
                },
                {
                    "x": 2148,
                    "y": 2851
                },
                {
                    "x": 2148,
                    "y": 2943
                },
                {
                    "x": 1279,
                    "y": 2943
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='240' style='font-size:16px'>1. Is the red square below the pink rectangle? No<br>2. Is the pink rectangle below the blue square? Yes</p>",
            "id": 240,
            "page": 14,
            "text": "1. Is the red square below the pink rectangle? No 2. Is the pink rectangle below the blue square? Yes"
        },
        {
            "bounding_box": [
                {
                    "x": 1332,
                    "y": 3007
                },
                {
                    "x": 2126,
                    "y": 3007
                },
                {
                    "x": 2126,
                    "y": 3057
                },
                {
                    "x": 1332,
                    "y": 3057
                }
            ],
            "category": "paragraph",
            "html": "<p id='241' style='font-size:16px'>Figure 9: An example of bAbI dataset, task 17.</p>",
            "id": 241,
            "page": 14,
            "text": "Figure 9: An example of bAbI dataset, task 17."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 3174
                },
                {
                    "x": 2195,
                    "y": 3174
                },
                {
                    "x": 2195,
                    "y": 3227
                },
                {
                    "x": 1270,
                    "y": 3227
                }
            ],
            "category": "paragraph",
            "html": "<p id='242' style='font-size:20px'>SpRL+rule-based model first, finds different spa-</p>",
            "id": 242,
            "page": 14,
            "text": "SpRL+rule-based model first, finds different spa-"
        },
        {
            "bounding_box": [
                {
                    "x": 303,
                    "y": 284
                },
                {
                    "x": 1197,
                    "y": 284
                },
                {
                    "x": 1197,
                    "y": 1476
                },
                {
                    "x": 303,
                    "y": 1476
                }
            ],
            "category": "table",
            "html": "<table id='243' style='font-size:18px'><tr><td>Q-TYPE</td><td>Candidate Answers</td><td>train</td><td>test</td></tr><tr><td rowspan=\"8\">FR (Multiple Choices)</td><td>Left</td><td>20.7</td><td>17.9</td></tr><tr><td>Right</td><td>21.4</td><td>16.7</td></tr><tr><td>Above</td><td>26.9</td><td>25.4</td></tr><tr><td>Below</td><td>37.2</td><td>42.9</td></tr><tr><td>Near to</td><td>5.8</td><td>2.9</td></tr><tr><td>Far from</td><td>1.3</td><td>0.56</td></tr><tr><td>Touching</td><td>0.57</td><td>0.27</td></tr><tr><td>DK</td><td>0.52</td><td>0.32</td></tr><tr><td rowspan=\"4\">FB (multiple Choices)</td><td>A</td><td>49.8</td><td>49.4</td></tr><tr><td>B</td><td>50.1</td><td>50</td></tr><tr><td>C</td><td>35.1</td><td>62</td></tr><tr><td>[]</td><td>7.1</td><td>90.5</td></tr><tr><td rowspan=\"4\">CO (Single choice)</td><td>Object1</td><td>25.4</td><td>26</td></tr><tr><td>Object2</td><td>25.3</td><td>24.9</td></tr><tr><td>Both</td><td>44.3</td><td>43.9</td></tr><tr><td>None</td><td>4.9</td><td>5.0</td></tr><tr><td rowspan=\"3\">YN (Single choice)</td><td>Yes</td><td>53.3</td><td>50.5</td></tr><tr><td>No</td><td>18.7</td><td>23.6</td></tr><tr><td>DK</td><td>27.8</td><td>25.9</td></tr></table>",
            "id": 243,
            "page": 15,
            "text": "Q-TYPE Candidate Answers train test  FR (Multiple Choices) Left 20.7 17.9  Right 21.4 16.7  Above 26.9 25.4  Below 37.2 42.9  Near to 5.8 2.9  Far from 1.3 0.56  Touching 0.57 0.27  DK 0.52 0.32  FB (multiple Choices) A 49.8 49.4  B 50.1 50  C 35.1 62  [] 7.1 90.5  CO (Single choice) Object1 25.4 26  Object2 25.3 24.9  Both 44.3 43.9  None 4.9 5.0  YN (Single choice) Yes 53.3 50.5  No 18.7 23.6  DK 27.8"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 1505
                },
                {
                    "x": 1212,
                    "y": 1505
                },
                {
                    "x": 1212,
                    "y": 2055
                },
                {
                    "x": 288,
                    "y": 2055
                }
            ],
            "category": "paragraph",
            "html": "<p id='244' style='font-size:16px'>Table 8: The percentage of each correct label in all sam-<br>ples. *The candidate answers for the FB Q-TYPE can<br>be varied, based on its story. **CO can be considered<br>as a multiple choice or single choice question. E.g.,<br>in \"which object is above the triangle? the blue cir-<br>cle or the black circle?\" you can consider two labels<br>with boolean classification on each \"blue circle\" and<br>\"black circle\" or consider it as a four labels classifica-<br>tion: \"blue circle, \" \"black circle,\" \"both of them, \" and<br>\"None of them. \" *** DK, None, [], all mean none of<br>the actual labels are correct.</p>",
            "id": 244,
            "page": 15,
            "text": "Table 8: The percentage of each correct label in all samples. *The candidate answers for the FB Q-TYPE can be varied, based on its story. **CO can be considered as a multiple choice or single choice question. E.g., in \"which object is above the triangle? the blue circle or the black circle?\" you can consider two labels with boolean classification on each \"blue circle\" and \"black circle\" or consider it as a four labels classification: \"blue circle, \" \"black circle,\" \"both of them, \" and \"None of them. \" *** DK, None, [], all mean none of the actual labels are correct."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 2152
                },
                {
                    "x": 1214,
                    "y": 2152
                },
                {
                    "x": 1214,
                    "y": 2545
                },
                {
                    "x": 287,
                    "y": 2545
                }
            ],
            "category": "paragraph",
            "html": "<p id='245' style='font-size:18px'>tial relation triplets (Landmark, Spatial-indicator,<br>trajector) for each fact in a story the applies spatial<br>rules over these extracted triplets and report all pos-<br>sible relations between two asked objects. Finally,<br>it checks whether the asked relation existed in the<br>find relation. This model solves task 17 of the bAbI<br>with 100% accuracy.</p>",
            "id": 245,
            "page": 15,
            "text": "tial relation triplets (Landmark, Spatial-indicator, trajector) for each fact in a story the applies spatial rules over these extracted triplets and report all possible relations between two asked objects. Finally, it checks whether the asked relation existed in the find relation. This model solves task 17 of the bAbI with 100% accuracy."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 2553
                },
                {
                    "x": 1214,
                    "y": 2553
                },
                {
                    "x": 1214,
                    "y": 3230
                },
                {
                    "x": 287,
                    "y": 3230
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='246' style='font-size:18px'>To implement the neural network approach, we<br>use huggingface implementation of pre-trained<br>BERT (Devlin et al., 2019). We apply a boolean<br>classifier on the output of \"[CLS]\" token from the<br>last layer of BERT model for each \"Yes\" and \"No\"<br>answers (the same as model used on YN question<br>types.) We use Adamw (Loshchilov and Hutter,<br>2017) optimizer and 2e - 6 learning rate with neg-<br>ative log-likelihood loss objective and train the<br>model on the 10k, 5k, 2k, 1k, 500, and 100 por-<br>tion of bAbI's training questions. The model yields<br>100% accuracy on 10k, and 5k and 99% accuracy</p>",
            "id": 246,
            "page": 15,
            "text": "To implement the neural network approach, we use huggingface implementation of pre-trained BERT (Devlin , 2019). We apply a boolean classifier on the output of \"[CLS]\" token from the last layer of BERT model for each \"Yes\" and \"No\" answers (the same as model used on YN question types.) We use Adamw (Loshchilov and Hutter, 2017) optimizer and 2e - 6 learning rate with negative log-likelihood loss objective and train the model on the 10k, 5k, 2k, 1k, 500, and 100 portion of bAbI's training questions. The model yields 100% accuracy on 10k, and 5k and 99% accuracy"
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 285
                },
                {
                    "x": 2182,
                    "y": 285
                },
                {
                    "x": 2182,
                    "y": 879
                },
                {
                    "x": 1274,
                    "y": 879
                }
            ],
            "category": "table",
            "html": "<br><table id='247' style='font-size:22px'><tr><td>Type</td><td>Original Set</td><td>Unseen Set</td></tr><tr><td>Shapes</td><td>Square, Circle, Triangle</td><td>Rectangle, Oval, Diamond</td></tr><tr><td>Relations</td><td>Left, Right, Above, Below</td><td>Left side, Right side, Top, Under</td></tr><tr><td>Colors</td><td>Yellow, Black, Below</td><td>Green, Red, White</td></tr><tr><td>Size</td><td>Small, Medium, Big</td><td>Little, Midsize, Large</td></tr></table>",
            "id": 247,
            "page": 15,
            "text": "Type Original Set Unseen Set  Shapes Square, Circle, Triangle Rectangle, Oval, Diamond  Relations Left, Right, Above, Below Left side, Right side, Top, Under  Colors Yellow, Black, Below Green, Red, White  Size Small, Medium, Big"
        },
        {
            "bounding_box": [
                {
                    "x": 1389,
                    "y": 910
                },
                {
                    "x": 2072,
                    "y": 910
                },
                {
                    "x": 2072,
                    "y": 957
                },
                {
                    "x": 1389,
                    "y": 957
                }
            ],
            "category": "caption",
            "html": "<caption id='248' style='font-size:14px'>Table 9: Modifications on the unseen set</caption>",
            "id": 248,
            "page": 15,
            "text": "Table 9: Modifications on the unseen set"
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 1056
                },
                {
                    "x": 1834,
                    "y": 1056
                },
                {
                    "x": 1834,
                    "y": 1101
                },
                {
                    "x": 1269,
                    "y": 1101
                }
            ],
            "category": "paragraph",
            "html": "<p id='249' style='font-size:20px'>on 2k and 1k training samples.</p>",
            "id": 249,
            "page": 15,
            "text": "on 2k and 1k training samples."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 1110
                },
                {
                    "x": 2194,
                    "y": 1110
                },
                {
                    "x": 2194,
                    "y": 1329
                },
                {
                    "x": 1269,
                    "y": 1329
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='250' style='font-size:18px'>Figure 10 shows an example of boolQ dataset.<br>To Answering the questions of this dataset, we use<br>the same setting as neural network model on bAbI<br>to further fine-tune BERT on boolQ.</p>",
            "id": 250,
            "page": 15,
            "text": "Figure 10 shows an example of boolQ dataset. To Answering the questions of this dataset, we use the same setting as neural network model on bAbI to further fine-tune BERT on boolQ."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 1376
                },
                {
                    "x": 2158,
                    "y": 1376
                },
                {
                    "x": 2158,
                    "y": 1840
                },
                {
                    "x": 1281,
                    "y": 1840
                }
            ],
            "category": "paragraph",
            "html": "<p id='251' style='font-size:14px'>Q: Has the UK been hit by a hurricane?<br>P: The Great Storm of 1987 was a violent extratropical<br>cyclone which caused casualties in England, France<br>and the Channel Islands ·<br>A: Yes. [An example event is given.]<br>Q: Does France have a Prime Minister and a President?<br>P: · · · The extent to which those decisions lie with the<br>Prime Minister or President depends upon · ..<br>A: Yes. [Both are mentioned, so it can be inferred both<br>exist.]</p>",
            "id": 251,
            "page": 15,
            "text": "Q: Has the UK been hit by a hurricane? P: The Great Storm of 1987 was a violent extratropical cyclone which caused casualties in England, France and the Channel Islands · A: Yes. [An example event is given.] Q: Does France have a Prime Minister and a President? P: · · · The extent to which those decisions lie with the Prime Minister or President depends upon · .. A: Yes. [Both are mentioned, so it can be inferred both exist.]"
        },
        {
            "bounding_box": [
                {
                    "x": 1385,
                    "y": 1896
                },
                {
                    "x": 2072,
                    "y": 1896
                },
                {
                    "x": 2072,
                    "y": 1945
                },
                {
                    "x": 1385,
                    "y": 1945
                }
            ],
            "category": "caption",
            "html": "<caption id='252' style='font-size:18px'>Figure 10: An example of boolQ dataset.</caption>",
            "id": 252,
            "page": 15,
            "text": "Figure 10: An example of boolQ dataset."
        },
        {
            "bounding_box": [
                {
                    "x": 344,
                    "y": 928
                },
                {
                    "x": 2147,
                    "y": 928
                },
                {
                    "x": 2147,
                    "y": 1033
                },
                {
                    "x": 344,
                    "y": 1033
                }
            ],
            "category": "paragraph",
            "html": "<p id='253' style='font-size:20px'>sentence: \"Medium blue square number one is touching the bottom edge of this block. \"<br>▼ spatial_description: [] 1 item</p>",
            "id": 253,
            "page": 16,
            "text": "sentence: \"Medium blue square number one is touching the bottom edge of this block. \" ▼ spatial_description: [] 1 item"
        },
        {
            "bounding_box": [
                {
                    "x": 397,
                    "y": 1094
                },
                {
                    "x": 648,
                    "y": 1094
                },
                {
                    "x": 648,
                    "y": 1137
                },
                {
                    "x": 397,
                    "y": 1137
                }
            ],
            "category": "paragraph",
            "html": "<p id='254' style='font-size:18px'>▼ trajector:</p>",
            "id": 254,
            "page": 16,
            "text": "▼ trajector:"
        },
        {
            "bounding_box": [
                {
                    "x": 365,
                    "y": 1044
                },
                {
                    "x": 449,
                    "y": 1044
                },
                {
                    "x": 449,
                    "y": 1079
                },
                {
                    "x": 365,
                    "y": 1079
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='255' style='font-size:14px'>▼ ⌀:</p>",
            "id": 255,
            "page": 16,
            "text": "▼ ⌀:"
        },
        {
            "bounding_box": [
                {
                    "x": 429,
                    "y": 1268
                },
                {
                    "x": 453,
                    "y": 1268
                },
                {
                    "x": 453,
                    "y": 1294
                },
                {
                    "x": 429,
                    "y": 1294
                }
            ],
            "category": "paragraph",
            "html": "<p id='256' style='font-size:14px'>▼</p>",
            "id": 256,
            "page": 16,
            "text": "▼"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1133
                },
                {
                    "x": 1286,
                    "y": 1133
                },
                {
                    "x": 1286,
                    "y": 1589
                },
                {
                    "x": 442,
                    "y": 1589
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='257' style='font-size:18px'>phrase: \"medium blue square number one\"<br>head: \"square\"<br>properties:<br>color: \"blue\"<br>size: \"medium\"<br>name : \"number one\"<br>number: \"\"<br>spatial_property: ・・・・・</p>",
            "id": 257,
            "page": 16,
            "text": "phrase: \"medium blue square number one\" head: \"square\" properties: color: \"blue\" size: \"medium\" name : \"number one\" number: \"\" spatial_property: ・・・・・"
        },
        {
            "bounding_box": [
                {
                    "x": 426,
                    "y": 1578
                },
                {
                    "x": 547,
                    "y": 1578
                },
                {
                    "x": 547,
                    "y": 1618
                },
                {
                    "x": 426,
                    "y": 1618
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='258' style='font-size:16px'>▼ SOT:</p>",
            "id": 258,
            "page": 16,
            "text": "▼ SOT:"
        },
        {
            "bounding_box": [
                {
                    "x": 493,
                    "y": 1632
                },
                {
                    "x": 712,
                    "y": 1632
                },
                {
                    "x": 712,
                    "y": 1726
                },
                {
                    "x": 493,
                    "y": 1726
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='259' style='font-size:20px'>start: 167<br>end: 195</p>",
            "id": 259,
            "page": 16,
            "text": "start: 167 end: 195"
        },
        {
            "bounding_box": [
                {
                    "x": 404,
                    "y": 1722
                },
                {
                    "x": 1291,
                    "y": 1722
                },
                {
                    "x": 1291,
                    "y": 2487
                },
                {
                    "x": 404,
                    "y": 2487
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='260' style='font-size:20px'>▼ landmark:<br>phrase: \"the bottom edge of this block\"<br>head: \"block\"<br>▶ properties:<br>spatial_property: \"the bottom edge\"<br>▶ SOT:<br>▼ spatial_indicator:<br>phrase: \"touching\"<br>spatial_value: \"TPP\"<br>g_type: \"Region\"<br>s_type: \"RCC8\"<br>polarity: false<br>F⌀R : \"Relative\"<br>▶ SOT:</p>",
            "id": 260,
            "page": 16,
            "text": "▼ landmark: phrase: \"the bottom edge of this block\" head: \"block\" ▶ properties: spatial_property: \"the bottom edge\" ▶ SOT: ▼ spatial_indicator: phrase: \"touching\" spatial_value: \"TPP\" g_type: \"Region\" s_type: \"RCC8\" polarity: false F⌀R : \"Relative\" ▶ SOT:"
        },
        {
            "bounding_box": [
                {
                    "x": 647,
                    "y": 2526
                },
                {
                    "x": 1828,
                    "y": 2526
                },
                {
                    "x": 1828,
                    "y": 2579
                },
                {
                    "x": 647,
                    "y": 2579
                }
            ],
            "category": "paragraph",
            "html": "<p id='261' style='font-size:22px'>Figure 11: SpRL annotation for an example sentence from SPARTQA.</p>",
            "id": 261,
            "page": 16,
            "text": "Figure 11: SpRL annotation for an example sentence from SPARTQA."
        },
        {
            "bounding_box": [
                {
                    "x": 331,
                    "y": 1096
                },
                {
                    "x": 2146,
                    "y": 1096
                },
                {
                    "x": 2146,
                    "y": 2313
                },
                {
                    "x": 331,
                    "y": 2313
                }
            ],
            "category": "table",
            "html": "<table id='262' style='font-size:18px'><tr><td>Examples</td><td>Features</td></tr><tr><td>Block A is above Block C and B.</td><td>Using conjunction to describe relation between more than two blocks.</td></tr><tr><td>The small circle is above the yellow square and the big black shape.</td><td>Using conjunction to describe relationships be- tween more than two objects.</td></tr><tr><td>The yellow square number one is to the right of and above the blue circle.</td><td>Using conjunction for more than one relation.</td></tr><tr><td>Block B has two medium yellow squares and two blue circles.</td><td>Describing a group of objects with the same properties. In the next sentences, they are men- tioned by an asigned number. For example, the blue circle number two.</td></tr><tr><td>The blue circle is below the object which is to the right of the big square.</td><td>Using nested relations between objects in their description.</td></tr><tr><td>A small blue circle is near to the big circle. It is to the left of the medium yellow square.</td><td>Using coreferences for an entity described in the previous sentences.</td></tr><tr><td>There is a block named A. One small yellow square is touching the bottom edge of this block.</td><td>The verb matches the number of the subject.</td></tr><tr><td>What is the relation between black object and a big circle?</td><td>Using shape, object, and thing, which are a gen- eral description of an object. It could be the \"black triangle\" or the \"black circle\" mentioned in the story.</td></tr></table>",
            "id": 262,
            "page": 17,
            "text": "Examples Features  Block A is above Block C and B. Using conjunction to describe relation between more than two blocks.  The small circle is above the yellow square and the big black shape. Using conjunction to describe relationships be- tween more than two objects.  The yellow square number one is to the right of and above the blue circle. Using conjunction for more than one relation.  Block B has two medium yellow squares and two blue circles. Describing a group of objects with the same properties. In the next sentences, they are men- tioned by an asigned number. For example, the blue circle number two.  The blue circle is below the object which is to the right of the big square. Using nested relations between objects in their description.  A small blue circle is near to the big circle. It is to the left of the medium yellow square. Using coreferences for an entity described in the previous sentences.  There is a block named A. One small yellow square is touching the bottom edge of this block. The verb matches the number of the subject.  What is the relation between black object and a big circle?"
        },
        {
            "bounding_box": [
                {
                    "x": 880,
                    "y": 2348
                },
                {
                    "x": 1599,
                    "y": 2348
                },
                {
                    "x": 1599,
                    "y": 2398
                },
                {
                    "x": 880,
                    "y": 2398
                }
            ],
            "category": "caption",
            "html": "<caption id='263' style='font-size:14px'>Table 10: Particular features of the dataset</caption>",
            "id": 263,
            "page": 17,
            "text": "Table 10: Particular features of the dataset"
        }
    ]
}