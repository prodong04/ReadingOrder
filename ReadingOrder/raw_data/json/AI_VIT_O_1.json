{
  "id": "629c90ba-0f92-11ef-8230-426932df3dcf",
  "pdf_path": "./pdf/AI_VIT_O/2111.06377v3.pdf",
  "elements": [
    {
      "bounding_box": [
        {
          "x": 566,
          "y": 425
        },
        {
          "x": 1909,
          "y": 425
        },
        {
          "x": 1909,
          "y": 492
        },
        {
          "x": 566,
          "y": 492
        }
      ],
      "category": "paragraph",
      "html": "<p id='0' style='font-size:22px'>Masked Autoencoders Are Scalable Vision Learners</p>",
      "id": 0,
      "page": 1,
      "text": "Masked Autoencoders Are Scalable Vision Learners"
    },
    {
      "bounding_box": [
        {
          "x": 358,
          "y": 550
        },
        {
          "x": 2120,
          "y": 550
        },
        {
          "x": 2120,
          "y": 774
        },
        {
          "x": 358,
          "y": 774
        }
      ],
      "category": "paragraph",
      "html": "<p id='1' style='font-size:20px'>Kaiming He*,† Xinlei Chen* Saining Xie Yanghao Li Piotr Dollar Ross Girshick<br>* equal technical contribution + project lead<br>Facebook AI Research (FAIR)</p>",
      "id": 1,
      "page": 1,
      "text": "Kaiming He*,† Xinlei Chen* Saining Xie Yanghao Li Piotr Dollar Ross Girshick\n* equal technical contribution + project lead\nFacebook AI Research (FAIR)"
    },
    {
      "bounding_box": [
        {
          "x": 603,
          "y": 844
        },
        {
          "x": 798,
          "y": 844
        },
        {
          "x": 798,
          "y": 896
        },
        {
          "x": 603,
          "y": 896
        }
      ],
      "category": "paragraph",
      "html": "<p id='2' style='font-size:22px'>Abstract</p>",
      "id": 2,
      "page": 1,
      "text": "Abstract"
    },
    {
      "bounding_box": [
        {
          "x": 198,
          "y": 948
        },
        {
          "x": 1199,
          "y": 948
        },
        {
          "x": 1199,
          "y": 1948
        },
        {
          "x": 198,
          "y": 1948
        }
      ],
      "category": "paragraph",
      "html": "<p id='3' style='font-size:18px'>This paper shows that masked autoencoders (MAE) are<br>scalable self-supervised learners for computer vision. Our<br>MAE approach is simple: we mask random patches of the<br>input image and reconstruct the missing pixels. It is based<br>on two core designs. First, we develop an asymmetric<br>encoder-decoder architecture, with an encoder that oper-<br>ates only on the visible subset of patches (without mask to-<br>kens), along with a lightweight decoder that reconstructs<br>the original image from the latent representation and mask<br>tokens. Second, we find that masking a high proportion<br>of the input image, e.g., 75%, yields a nontrivial and<br>meaningful self-supervisory task. Coupling these two de-<br>signs enables us to train large models efficiently and ef-<br>fectively: we accelerate training (by 3x or more) and im-<br>prove accuracy. Our scalable approach allows for learning<br>high-capacity models that generalize well: e.g., a vanilla<br>ViT-Huge model achieves the best accuracy (87.8%) among<br>methods that use only ImageNet-1K data. Transfer per-<br>formance in downstream tasks outperforms supervised pre-<br>training and shows promising scaling behavior.</p>",
      "id": 3,
      "page": 1,
      "text": "This paper shows that masked autoencoders (MAE) are\nscalable self-supervised learners for computer vision. Our\nMAE approach is simple: we mask random patches of the\ninput image and reconstruct the missing pixels. It is based\non two core designs. First, we develop an asymmetric\nencoder-decoder architecture, with an encoder that oper-\nates only on the visible subset of patches (without mask to-\nkens), along with a lightweight decoder that reconstructs\nthe original image from the latent representation and mask\ntokens. Second, we find that masking a high proportion\nof the input image, e.g., 75%, yields a nontrivial and\nmeaningful self-supervisory task. Coupling these two de-\nsigns enables us to train large models efficiently and ef-\nfectively: we accelerate training (by 3x or more) and im-\nprove accuracy. Our scalable approach allows for learning\nhigh-capacity models that generalize well: e.g., a vanilla\nViT-Huge model achieves the best accuracy (87.8%) among\nmethods that use only ImageNet-1K data. Transfer per-\nformance in downstream tasks outperforms supervised pre-\ntraining and shows promising scaling behavior."
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 2043
        },
        {
          "x": 531,
          "y": 2043
        },
        {
          "x": 531,
          "y": 2094
        },
        {
          "x": 204,
          "y": 2094
        }
      ],
      "category": "paragraph",
      "html": "<p id='4' style='font-size:20px'>1. Introduction</p>",
      "id": 4,
      "page": 1,
      "text": "1. Introduction"
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 2128
        },
        {
          "x": 1199,
          "y": 2128
        },
        {
          "x": 1199,
          "y": 2423
        },
        {
          "x": 200,
          "y": 2423
        }
      ],
      "category": "paragraph",
      "html": "<p id='5' style='font-size:18px'>Deep learning has witnessed an explosion of archi-<br>tectures of continuously growing capability and capacity<br>[33, 25, 57]. Aided by the rapid gains in hardware, mod-<br>els today can easily overfit one million images [13] and<br>begin to demand hundreds of millions of-often publicly<br>inaccessible-labeled images [16].</p>",
      "id": 5,
      "page": 1,
      "text": "Deep learning has witnessed an explosion of archi-\ntectures of continuously growing capability and capacity\n[33, 25, 57]. Aided by the rapid gains in hardware, mod-\nels today can easily overfit one million images [13] and\nbegin to demand hundreds of millions of-often publicly\ninaccessible-labeled images [16]."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 2430
        },
        {
          "x": 1199,
          "y": 2430
        },
        {
          "x": 1199,
          "y": 2824
        },
        {
          "x": 201,
          "y": 2824
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='6' style='font-size:18px'>This appetite for data has been successfully addressed in<br>natural language processing (NLP) by self-supervised pre-<br>training. The solutions, based on autoregressive language<br>modeling in GPT [47, 48, 4] and masked autoencoding in<br>BERT [14], are conceptually simple: they remove a portion<br>of the data and learn to predict the removed content. These<br>methods now enable training of generalizable NLP models<br>containing over one hundred billion parameters [4].</p>",
      "id": 6,
      "page": 1,
      "text": "This appetite for data has been successfully addressed in\nnatural language processing (NLP) by self-supervised pre-\ntraining. The solutions, based on autoregressive language\nmodeling in GPT [47, 48, 4] and masked autoencoding in\nBERT [14], are conceptually simple: they remove a portion\nof the data and learn to predict the removed content. These\nmethods now enable training of generalizable NLP models\ncontaining over one hundred billion parameters [4]."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 2830
        },
        {
          "x": 1199,
          "y": 2830
        },
        {
          "x": 1199,
          "y": 2976
        },
        {
          "x": 201,
          "y": 2976
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='7' style='font-size:18px'>The idea of masked autoencoders, a form of more gen-<br>eral denoising autoencoders [58], is natural and applicable<br>in computer vision as well. Indeed, closely related research</p>",
      "id": 7,
      "page": 1,
      "text": "The idea of masked autoencoders, a form of more gen-\neral denoising autoencoders [58], is natural and applicable\nin computer vision as well. Indeed, closely related research"
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 827
        },
        {
          "x": 2276,
          "y": 827
        },
        {
          "x": 2276,
          "y": 1389
        },
        {
          "x": 1280,
          "y": 1389
        }
      ],
      "category": "figure",
      "html": "<br><figure><img id='8' style='font-size:14px' alt=\"encoder decoder\ninput target\" data-coord=\"top-left:(1280,827); bottom-right:(2276,1389)\" /></figure>",
      "id": 8,
      "page": 1,
      "text": "encoder decoder\ninput target"
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 1416
        },
        {
          "x": 2277,
          "y": 1416
        },
        {
          "x": 2277,
          "y": 1785
        },
        {
          "x": 1279,
          "y": 1785
        }
      ],
      "category": "caption",
      "html": "<caption id='9' style='font-size:16px'>Figure 1. Our MAE architecture. During pre-training, a large<br>random subset of image patches (e.g., 75%) is masked out. The<br>encoder is applied to the small subset of visible patches. Mask<br>tokens are introduced after the encoder, and the full set of en-<br>coded patches and mask tokens is processed by a small decoder<br>that reconstructs the original image in pixels. After pre-training,<br>the decoder is discarded and the encoder is applied to uncorrupted<br>images (full sets of patches) for recognition tasks.</caption>",
      "id": 9,
      "page": 1,
      "text": "Figure 1. Our MAE architecture. During pre-training, a large\nrandom subset of image patches (e.g., 75%) is masked out. The\nencoder is applied to the small subset of visible patches. Mask\ntokens are introduced after the encoder, and the full set of en-\ncoded patches and mask tokens is processed by a small decoder\nthat reconstructs the original image in pixels. After pre-training,\nthe decoder is discarded and the encoder is applied to uncorrupted\nimages (full sets of patches) for recognition tasks."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 1877
        },
        {
          "x": 2276,
          "y": 1877
        },
        {
          "x": 2276,
          "y": 2174
        },
        {
          "x": 1280,
          "y": 2174
        }
      ],
      "category": "paragraph",
      "html": "<p id='10' style='font-size:18px'>in vision [59, 46] preceded BERT. However, despite signif-<br>icant interest in this idea following the success of BERT,<br>progress of autoencoding methods in vision lags behind<br>NLP. We ask: what makes masked autoencoding different<br>between vision and language? We attempt to answer this<br>question from the following perspectives:</p>",
      "id": 10,
      "page": 1,
      "text": "in vision [59, 46] preceded BERT. However, despite signif-\nicant interest in this idea following the success of BERT,\nprogress of autoencoding methods in vision lags behind\nNLP. We ask: what makes masked autoencoding different\nbetween vision and language? We attempt to answer this\nquestion from the following perspectives:"
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 2179
        },
        {
          "x": 2276,
          "y": 2179
        },
        {
          "x": 2276,
          "y": 2572
        },
        {
          "x": 1279,
          "y": 2572
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='11' style='font-size:16px'>(i) Until recently, architectures were different. In vision,<br>convolutional networks [34] were dominant over the last<br>decade [33]. Convolutions typically operate on regular grids<br>and itis not straightforward to integrate 'indicators' such as<br>mask tokens [14] or positional embeddings [57] into con-<br>volutional networks. This architectural gap, however, has<br>been addressed with the introduction of Vision Transform-<br>ers (ViT) [16] and should no longer present an obstacle.</p>",
      "id": 11,
      "page": 1,
      "text": "(i) Until recently, architectures were different. In vision,\nconvolutional networks [34] were dominant over the last\ndecade [33]. Convolutions typically operate on regular grids\nand itis not straightforward to integrate 'indicators' such as\nmask tokens [14] or positional embeddings [57] into con-\nvolutional networks. This architectural gap, however, has\nbeen addressed with the introduction of Vision Transform-\ners (ViT) [16] and should no longer present an obstacle."
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 2581
        },
        {
          "x": 2277,
          "y": 2581
        },
        {
          "x": 2277,
          "y": 2975
        },
        {
          "x": 1279,
          "y": 2975
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='12' style='font-size:18px'>(ii) Information density is different between language<br>and vision. Languages are human-generated signals that<br>are highly semantic and information-dense. When training<br>a model to predict only a few missing words per sentence,<br>this task appears to induce sophisticated language under-<br>standing. Images, on the contrary, are natural signals with<br>heavy spatial redundancy -e.g., a missing patch can be re-<br>covered from neighboring patches with little high-level un-</p>",
      "id": 12,
      "page": 1,
      "text": "(ii) Information density is different between language\nand vision. Languages are human-generated signals that\nare highly semantic and information-dense. When training\na model to predict only a few missing words per sentence,\nthis task appears to induce sophisticated language under-\nstanding. Images, on the contrary, are natural signals with\nheavy spatial redundancy -e.g., a missing patch can be re-\ncovered from neighboring patches with little high-level un-"
    },
    {
      "bounding_box": [
        {
          "x": 58,
          "y": 873
        },
        {
          "x": 151,
          "y": 873
        },
        {
          "x": 151,
          "y": 2329
        },
        {
          "x": 58,
          "y": 2329
        }
      ],
      "category": "footer",
      "html": "<br><footer id='13' style='font-size:14px'>2021<br>Dec<br>19<br>[cs.CV]<br>arXiv:2111.06377v3</footer>",
      "id": 13,
      "page": 1,
      "text": "2021\nDec\n19\n[cs.CV]\narXiv:2111.06377v3"
    },
    {
      "bounding_box": [
        {
          "x": 1226,
          "y": 3056
        },
        {
          "x": 1249,
          "y": 3056
        },
        {
          "x": 1249,
          "y": 3090
        },
        {
          "x": 1226,
          "y": 3090
        }
      ],
      "category": "footer",
      "html": "<footer id='14' style='font-size:14px'>1</footer>",
      "id": 14,
      "page": 1,
      "text": "1"
    },
    {
      "bounding_box": [
        {
          "x": 223,
          "y": 271
        },
        {
          "x": 2255,
          "y": 271
        },
        {
          "x": 2255,
          "y": 979
        },
        {
          "x": 223,
          "y": 979
        }
      ],
      "category": "figure",
      "html": "<figure><img id='15' style='font-size:14px' alt=\"XI\n//\n시\nVVIVI\" data-coord=\"top-left:(223,271); bottom-right:(2255,979)\" /></figure>",
      "id": 15,
      "page": 2,
      "text": "XI\n//\n시\nVVIVI"
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 992
        },
        {
          "x": 2273,
          "y": 992
        },
        {
          "x": 2273,
          "y": 1177
        },
        {
          "x": 201,
          "y": 1177
        }
      ],
      "category": "caption",
      "html": "<br><caption id='16' style='font-size:16px'>Figure 2. Example results on ImageNet validation images. For each triplet, we show the masked image (left), our MAE reconstruction t<br>(middle), and the ground-truth (right). The masking ratio is 80%, leaving only 39 out of 196 patches. More examples are in the appendix.<br>†As no loss is computed on visible patches, the model output on visible patches is qualitatively worse. One can simply overlay the output with the visible<br>patches to improve visual quality. We intentionally opt not to do this, so we can more comprehensively demonstrate the method's behavior.</caption>",
      "id": 16,
      "page": 2,
      "text": "Figure 2. Example results on ImageNet validation images. For each triplet, we show the masked image (left), our MAE reconstruction t\n(middle), and the ground-truth (right). The masking ratio is 80%, leaving only 39 out of 196 patches. More examples are in the appendix.\n†As no loss is computed on visible patches, the model output on visible patches is qualitatively worse. One can simply overlay the output with the visible\npatches to improve visual quality. We intentionally opt not to do this, so we can more comprehensively demonstrate the method's behavior."
    },
    {
      "bounding_box": [
        {
          "x": 220,
          "y": 1201
        },
        {
          "x": 2254,
          "y": 1201
        },
        {
          "x": 2254,
          "y": 1550
        },
        {
          "x": 220,
          "y": 1550
        }
      ],
      "category": "figure",
      "html": "<figure><img id='17' alt=\"\" data-coord=\"top-left:(220,1201); bottom-right:(2254,1550)\" /></figure>",
      "id": 17,
      "page": 2,
      "text": ""
    },
    {
      "bounding_box": [
        {
          "x": 205,
          "y": 1569
        },
        {
          "x": 2269,
          "y": 1569
        },
        {
          "x": 2269,
          "y": 1658
        },
        {
          "x": 205,
          "y": 1658
        }
      ],
      "category": "caption",
      "html": "<br><caption id='18' style='font-size:18px'>Figure 3. Example results on COCO validation images, using an MAE trained on ImageNet (the same model weights as in Figure 2).<br>Observe the reconstructions on the two right-most examples, which, although different from the ground truth, are semantically plausible.</caption>",
      "id": 18,
      "page": 2,
      "text": "Figure 3. Example results on COCO validation images, using an MAE trained on ImageNet (the same model weights as in Figure 2).\nObserve the reconstructions on the two right-most examples, which, although different from the ground truth, are semantically plausible."
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 1726
        },
        {
          "x": 1197,
          "y": 1726
        },
        {
          "x": 1197,
          "y": 2120
        },
        {
          "x": 203,
          "y": 2120
        }
      ],
      "category": "paragraph",
      "html": "<p id='19' style='font-size:20px'>derstanding of parts, objects, and scenes. To overcome this<br>difference and encourage learning useful features, we show<br>that a simple strategy works well in computer vision: mask-<br>ing a very high portion of random patches. This strategy<br>largely reduces redundancy and creates a challenging self-<br>supervisory task that requires holistic understanding beyond<br>low-level image statistics. To get a qualitative sense of our<br>reconstruction task, see Figures 2 - 4.</p>",
      "id": 19,
      "page": 2,
      "text": "derstanding of parts, objects, and scenes. To overcome this\ndifference and encourage learning useful features, we show\nthat a simple strategy works well in computer vision: mask-\ning a very high portion of random patches. This strategy\nlargely reduces redundancy and creates a challenging self-\nsupervisory task that requires holistic understanding beyond\nlow-level image statistics. To get a qualitative sense of our\nreconstruction task, see Figures 2 - 4."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 2128
        },
        {
          "x": 1198,
          "y": 2128
        },
        {
          "x": 1198,
          "y": 2621
        },
        {
          "x": 201,
          "y": 2621
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='20' style='font-size:20px'>(iii) The autoencoder's decoder, which maps the latent<br>representation back to the input, plays a different role be-<br>tween reconstructing text and images. In vision, the decoder<br>reconstructs pixels, hence its output is of a lower semantic<br>level than common recognition tasks. This is in contrast<br>to language, where the decoder predicts missing words that<br>contain rich semantic information. While in BERT the de-<br>coder can be trivial (an MLP) [14], we found that for im-<br>ages, the decoder design plays a key role in determining the<br>semantic level of the learned latent representations.</p>",
      "id": 20,
      "page": 2,
      "text": "(iii) The autoencoder's decoder, which maps the latent\nrepresentation back to the input, plays a different role be-\ntween reconstructing text and images. In vision, the decoder\nreconstructs pixels, hence its output is of a lower semantic\nlevel than common recognition tasks. This is in contrast\nto language, where the decoder predicts missing words that\ncontain rich semantic information. While in BERT the de-\ncoder can be trivial (an MLP) [14], we found that for im-\nages, the decoder design plays a key role in determining the\nsemantic level of the learned latent representations."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 2630
        },
        {
          "x": 1199,
          "y": 2630
        },
        {
          "x": 1199,
          "y": 2975
        },
        {
          "x": 202,
          "y": 2975
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='21' style='font-size:20px'>Driven by this analysis, we present a simple, effective,<br>and scalable form of a masked autoencoder (MAE) for<br>visual representation learning. Our MAE masks random<br>patches from the input image and reconstructs the missing<br>patches in the pixel space. It has an asymmetric encoder-<br>decoder design. Our encoder operates only on the visible<br>subset of patches (without mask tokens), and our decoder is</p>",
      "id": 21,
      "page": 2,
      "text": "Driven by this analysis, we present a simple, effective,\nand scalable form of a masked autoencoder (MAE) for\nvisual representation learning. Our MAE masks random\npatches from the input image and reconstructs the missing\npatches in the pixel space. It has an asymmetric encoder-\ndecoder design. Our encoder operates only on the visible\nsubset of patches (without mask tokens), and our decoder is"
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 1726
        },
        {
          "x": 2275,
          "y": 1726
        },
        {
          "x": 2275,
          "y": 2220
        },
        {
          "x": 1280,
          "y": 2220
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='22' style='font-size:20px'>lightweight and reconstructs the input from the latent rep-<br>resentation along with mask tokens (Figure 1). Shifting<br>the mask tokens to the small decoder in our asymmetric<br>encoder-decoder results in a large reduction in computation.<br>Under this design, a very high masking ratio (e.g., 75%) can<br>achieve a win-win scenario: it optimizes accuracy while al-<br>lowing the encoder to process only a small portion (e.g.,<br>25%) of patches. This can reduce overall pre-training time<br>by 3x or more and likewise reduce memory consumption,<br>enabling us to easily scale our MAE to large models.</p>",
      "id": 22,
      "page": 2,
      "text": "lightweight and reconstructs the input from the latent rep-\nresentation along with mask tokens (Figure 1). Shifting\nthe mask tokens to the small decoder in our asymmetric\nencoder-decoder results in a large reduction in computation.\nUnder this design, a very high masking ratio (e.g., 75%) can\nachieve a win-win scenario: it optimizes accuracy while al-\nlowing the encoder to process only a small portion (e.g.,\n25%) of patches. This can reduce overall pre-training time\nby 3x or more and likewise reduce memory consumption,\nenabling us to easily scale our MAE to large models."
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 2231
        },
        {
          "x": 2276,
          "y": 2231
        },
        {
          "x": 2276,
          "y": 2979
        },
        {
          "x": 1278,
          "y": 2979
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='23' style='font-size:22px'>Our MAE learns very high-capacity models that gen-<br>eralize well. With MAE pre-training, we can train data-<br>hungry models like ViT-Large/-Huge [16] on ImageNet-1K<br>with improved generalization performance. With a vanilla<br>ViT-Huge model, we achieve 87.8% accuracy when fine-<br>tuned on ImageNet-1K. This outperforms all previous re-<br>sults that use only ImageNet-1K data. We also evaluate<br>transfer learning on object detection, instance segmentation,<br>and semantic segmentation. In these tasks, our pre-training<br>achieves better results than its supervised pre-training coun-<br>terparts, and more importantly, we observe significant gains<br>by scaling up models. These observations are aligned<br>with those witnessed in self-supervised pre-training in NLP<br>[14, 47, 48, 4] and we hope that they will enable our field to<br>explore a similar trajectory.</p>",
      "id": 23,
      "page": 2,
      "text": "Our MAE learns very high-capacity models that gen-\neralize well. With MAE pre-training, we can train data-\nhungry models like ViT-Large/-Huge [16] on ImageNet-1K\nwith improved generalization performance. With a vanilla\nViT-Huge model, we achieve 87.8% accuracy when fine-\ntuned on ImageNet-1K. This outperforms all previous re-\nsults that use only ImageNet-1K data. We also evaluate\ntransfer learning on object detection, instance segmentation,\nand semantic segmentation. In these tasks, our pre-training\nachieves better results than its supervised pre-training coun-\nterparts, and more importantly, we observe significant gains\nby scaling up models. These observations are aligned\nwith those witnessed in self-supervised pre-training in NLP\n[14, 47, 48, 4] and we hope that they will enable our field to\nexplore a similar trajectory."
    },
    {
      "bounding_box": [
        {
          "x": 1225,
          "y": 3055
        },
        {
          "x": 1251,
          "y": 3055
        },
        {
          "x": 1251,
          "y": 3091
        },
        {
          "x": 1225,
          "y": 3091
        }
      ],
      "category": "footer",
      "html": "<footer id='24' style='font-size:20px'>2</footer>",
      "id": 24,
      "page": 2,
      "text": "2"
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 291
        },
        {
          "x": 1195,
          "y": 291
        },
        {
          "x": 1195,
          "y": 1054
        },
        {
          "x": 203,
          "y": 1054
        }
      ],
      "category": "figure",
      "html": "<figure><img id='25' style='font-size:14px' alt=\"W\n\n·\noriginal mask 75% mask 85% mask 95%\" data-coord=\"top-left:(203,291); bottom-right:(1195,1054)\" /></figure>",
      "id": 25,
      "page": 3,
      "text": "W\n\n·\noriginal mask 75% mask 85% mask 95%"
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 1076
        },
        {
          "x": 1200,
          "y": 1076
        },
        {
          "x": 1200,
          "y": 1260
        },
        {
          "x": 201,
          "y": 1260
        }
      ],
      "category": "caption",
      "html": "<caption id='26' style='font-size:14px'>Figure 4. Reconstructions of ImageNet validation images using<br>an MAE pre-trained with a masking ratio of 75% but applied on<br>inputs with higher masking ratios. The predictions differ plausibly<br>from the original images, showing that the method can generalize.</caption>",
      "id": 26,
      "page": 3,
      "text": "Figure 4. Reconstructions of ImageNet validation images using\nan MAE pre-trained with a masking ratio of 75% but applied on\ninputs with higher masking ratios. The predictions differ plausibly\nfrom the original images, showing that the method can generalize."
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 1368
        },
        {
          "x": 557,
          "y": 1368
        },
        {
          "x": 557,
          "y": 1416
        },
        {
          "x": 203,
          "y": 1416
        }
      ],
      "category": "paragraph",
      "html": "<p id='27' style='font-size:20px'>2. Related Work</p>",
      "id": 27,
      "page": 3,
      "text": "2. Related Work"
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 1479
        },
        {
          "x": 1198,
          "y": 1479
        },
        {
          "x": 1198,
          "y": 1874
        },
        {
          "x": 201,
          "y": 1874
        }
      ],
      "category": "paragraph",
      "html": "<p id='28' style='font-size:18px'>Masked language modeling and its autoregressive coun-<br>terparts, e.g., BERT [14] and GPT [47, 48, 4], are highly<br>successful methods for pre-training in NLP. These methods<br>hold out a portion of the input sequence and train models<br>to predict the missing content. These methods have been<br>shown to scale excellently [4] and a large abundance of ev-<br>idence indicates that these pre-trained representations gen-<br>eralize well to various downstream tasks.</p>",
      "id": 28,
      "page": 3,
      "text": "Masked language modeling and its autoregressive coun-\nterparts, e.g., BERT [14] and GPT [47, 48, 4], are highly\nsuccessful methods for pre-training in NLP. These methods\nhold out a portion of the input sequence and train models\nto predict the missing content. These methods have been\nshown to scale excellently [4] and a large abundance of ev-\nidence indicates that these pre-trained representations gen-\neralize well to various downstream tasks."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 1903
        },
        {
          "x": 1198,
          "y": 1903
        },
        {
          "x": 1198,
          "y": 2451
        },
        {
          "x": 201,
          "y": 2451
        }
      ],
      "category": "paragraph",
      "html": "<p id='29' style='font-size:16px'>Autoencoding is a classical method for learning representa-<br>tions. It has an encoder that maps an input to a latent repre-<br>sentation and a decoder that reconstructs the input. For ex-<br>ample, PCA and k-means are autoencoders [29]. Denoising<br>autoencoders (DAE) [58] are a class of autoencoders that<br>corrupt an input signal and learn to reconstruct the origi-<br>nal, uncorrupted signal. A series of methods can be thought<br>of as a generalized DAE under different corruptions, e.g.,<br>masking pixels [59, 46, 6] or removing color channels [70].<br>Our MAE is a form of denoising autoencoding, but different<br>from the classical DAE in numerous ways.</p>",
      "id": 29,
      "page": 3,
      "text": "Autoencoding is a classical method for learning representa-\ntions. It has an encoder that maps an input to a latent repre-\nsentation and a decoder that reconstructs the input. For ex-\nample, PCA and k-means are autoencoders [29]. Denoising\nautoencoders (DAE) [58] are a class of autoencoders that\ncorrupt an input signal and learn to reconstruct the origi-\nnal, uncorrupted signal. A series of methods can be thought\nof as a generalized DAE under different corruptions, e.g.,\nmasking pixels [59, 46, 6] or removing color channels [70].\nOur MAE is a form of denoising autoencoding, but different\nfrom the classical DAE in numerous ways."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 2476
        },
        {
          "x": 1198,
          "y": 2476
        },
        {
          "x": 1198,
          "y": 2977
        },
        {
          "x": 201,
          "y": 2977
        }
      ],
      "category": "paragraph",
      "html": "<p id='30' style='font-size:18px'>Masked image encoding methods learn representations<br>from images corrupted by masking. The pioneering work<br>of [59] presents masking as a noise type in DAE. Context<br>Encoder [46] inpaints large missing regions using convolu-<br>tional networks. Motivated by the success in NLP, related<br>recent methods [6, 16, 2] are based on Transformers [57].<br>iGPT [6] operates on sequences of pixels and predicts un-<br>known pixels. The ViT paper [16] studies masked patch<br>prediction for self-supervised learning. Most recently, BEiT<br>[2] proposes to predict discrete tokens [44, 50].</p>",
      "id": 30,
      "page": 3,
      "text": "Masked image encoding methods learn representations\nfrom images corrupted by masking. The pioneering work\nof [59] presents masking as a noise type in DAE. Context\nEncoder [46] inpaints large missing regions using convolu-\ntional networks. Motivated by the success in NLP, related\nrecent methods [6, 16, 2] are based on Transformers [57].\niGPT [6] operates on sequences of pixels and predicts un-\nknown pixels. The ViT paper [16] studies masked patch\nprediction for self-supervised learning. Most recently, BEiT\n[2] proposes to predict discrete tokens [44, 50]."
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 308
        },
        {
          "x": 2278,
          "y": 308
        },
        {
          "x": 2278,
          "y": 802
        },
        {
          "x": 1278,
          "y": 802
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='31' style='font-size:18px'>Self-supervised learning approaches have seen significant<br>interest in computer vision, often focusing on different pre-<br>text tasks for pre-training [15, 61, 42, 70, 45, 17]. Re-<br>cently, contrastive learning [3, 22] has been popular, e.g.,<br>[62, 43, 23, 7], which models image similarity and dis-<br>similarity (or only similarity [21, 8]) between two or more<br>views. Contrastive and related methods strongly depend on<br>data augmentation [7, 21, 8]. Autoencoding pursues a con-<br>ceptually different direction, and it exhibits different behav-<br>iors as we will present.</p>",
      "id": 31,
      "page": 3,
      "text": "Self-supervised learning approaches have seen significant\ninterest in computer vision, often focusing on different pre-\ntext tasks for pre-training [15, 61, 42, 70, 45, 17]. Re-\ncently, contrastive learning [3, 22] has been popular, e.g.,\n[62, 43, 23, 7], which models image similarity and dis-\nsimilarity (or only similarity [21, 8]) between two or more\nviews. Contrastive and related methods strongly depend on\ndata augmentation [7, 21, 8]. Autoencoding pursues a con-\nceptually different direction, and it exhibits different behav-\niors as we will present."
    },
    {
      "bounding_box": [
        {
          "x": 1282,
          "y": 877
        },
        {
          "x": 1549,
          "y": 877
        },
        {
          "x": 1549,
          "y": 930
        },
        {
          "x": 1282,
          "y": 930
        }
      ],
      "category": "paragraph",
      "html": "<p id='32' style='font-size:22px'>3. Approach</p>",
      "id": 32,
      "page": 3,
      "text": "3. Approach"
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 971
        },
        {
          "x": 2277,
          "y": 971
        },
        {
          "x": 2277,
          "y": 1516
        },
        {
          "x": 1278,
          "y": 1516
        }
      ],
      "category": "paragraph",
      "html": "<p id='33' style='font-size:16px'>Our masked autoencoder (MAE) is a simple autoencod-<br>ing approach that reconstructs the original signal given its<br>partial observation. Like all autoencoders, our approach<br>has an encoder that maps the observed signal to a latent<br>representation, and a decoder that reconstructs the origi-<br>nal signal from the latent representation. Unlike classical<br>autoencoders, we adopt an asymmetric design that allows<br>the encoder to operate only on the partial, observed signal<br>(without mask tokens) and a lightweight decoder that re-<br>constructs the full signal from the latent representation and<br>mask tokens. Figure 1 illustrates the idea, introduced next.</p>",
      "id": 33,
      "page": 3,
      "text": "Our masked autoencoder (MAE) is a simple autoencod-\ning approach that reconstructs the original signal given its\npartial observation. Like all autoencoders, our approach\nhas an encoder that maps the observed signal to a latent\nrepresentation, and a decoder that reconstructs the origi-\nnal signal from the latent representation. Unlike classical\nautoencoders, we adopt an asymmetric design that allows\nthe encoder to operate only on the partial, observed signal\n(without mask tokens) and a lightweight decoder that re-\nconstructs the full signal from the latent representation and\nmask tokens. Figure 1 illustrates the idea, introduced next."
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 1537
        },
        {
          "x": 2276,
          "y": 1537
        },
        {
          "x": 2276,
          "y": 1834
        },
        {
          "x": 1279,
          "y": 1834
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='34' style='font-size:18px'>Masking. Following ViT [16], we divide an image into reg-<br>ular non-overlapping patches. Then we sample a subset of<br>patches and mask (i.e., remove) the remaining ones. Our<br>sampling strategy is straightforward: we sample random<br>patches without replacement, following a uniform distribu-<br>tion. We simply refer to this as \"random sampling\".</p>",
      "id": 34,
      "page": 3,
      "text": "Masking. Following ViT [16], we divide an image into reg-\nular non-overlapping patches. Then we sample a subset of\npatches and mask (i.e., remove) the remaining ones. Our\nsampling strategy is straightforward: we sample random\npatches without replacement, following a uniform distribu-\ntion. We simply refer to this as \"random sampling\"."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 1845
        },
        {
          "x": 2278,
          "y": 1845
        },
        {
          "x": 2278,
          "y": 2237
        },
        {
          "x": 1280,
          "y": 2237
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='35' style='font-size:18px'>Random sampling with a high masking ratio (i.e., the ra-<br>tio of removed patches) largely eliminates redundancy, thus<br>creating a task that cannot be easily solved by extrapolation<br>from visible neighboring patches (see Figures 2 - 4). The<br>uniform distribution prevents a potential center bias (i.e.,<br>more masked patches near the image center). Finally, the<br>highly sparse input creates an opportunity for designing an<br>efficient encoder, introduced next.</p>",
      "id": 35,
      "page": 3,
      "text": "Random sampling with a high masking ratio (i.e., the ra-\ntio of removed patches) largely eliminates redundancy, thus\ncreating a task that cannot be easily solved by extrapolation\nfrom visible neighboring patches (see Figures 2 - 4). The\nuniform distribution prevents a potential center bias (i.e.,\nmore masked patches near the image center). Finally, the\nhighly sparse input creates an opportunity for designing an\nefficient encoder, introduced next."
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 2259
        },
        {
          "x": 2277,
          "y": 2259
        },
        {
          "x": 2277,
          "y": 2757
        },
        {
          "x": 1279,
          "y": 2757
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='36' style='font-size:16px'>MAE encoder. Our encoder is a ViT [16] but applied only<br>on visible, unmasked patches. Just as in a standard ViT, our<br>encoder embeds patches by a linear projection with added<br>positional embeddings, and then processes the resulting set<br>via a series of Transformer blocks. However, our encoder<br>only operates on a small subset (e.g., 25%) of the full set.<br>Masked patches are removed; no mask tokens are used.<br>This allows us to train very large encoders with only a frac-<br>tion of compute and memory. The full set is handled by a<br>lightweight decoder, described next.</p>",
      "id": 36,
      "page": 3,
      "text": "MAE encoder. Our encoder is a ViT [16] but applied only\non visible, unmasked patches. Just as in a standard ViT, our\nencoder embeds patches by a linear projection with added\npositional embeddings, and then processes the resulting set\nvia a series of Transformer blocks. However, our encoder\nonly operates on a small subset (e.g., 25%) of the full set.\nMasked patches are removed; no mask tokens are used.\nThis allows us to train very large encoders with only a frac-\ntion of compute and memory. The full set is handled by a\nlightweight decoder, described next."
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 2777
        },
        {
          "x": 2276,
          "y": 2777
        },
        {
          "x": 2276,
          "y": 2976
        },
        {
          "x": 1281,
          "y": 2976
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='37' style='font-size:16px'>MAE decoder. The input to the MAE decoder is the full<br>set of tokens consisting of (i) encoded visible patches, and<br>(ii) mask tokens. See Figure 1. Each mask token [14] is a<br>shared, learned vector that indicates the presence of a miss-</p>",
      "id": 37,
      "page": 3,
      "text": "MAE decoder. The input to the MAE decoder is the full\nset of tokens consisting of (i) encoded visible patches, and\n(ii) mask tokens. See Figure 1. Each mask token [14] is a\nshared, learned vector that indicates the presence of a miss-"
    },
    {
      "bounding_box": [
        {
          "x": 1224,
          "y": 3054
        },
        {
          "x": 1252,
          "y": 3054
        },
        {
          "x": 1252,
          "y": 3092
        },
        {
          "x": 1224,
          "y": 3092
        }
      ],
      "category": "footer",
      "html": "<footer id='38' style='font-size:16px'>3</footer>",
      "id": 38,
      "page": 3,
      "text": "3"
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 309
        },
        {
          "x": 1199,
          "y": 309
        },
        {
          "x": 1199,
          "y": 502
        },
        {
          "x": 201,
          "y": 502
        }
      ],
      "category": "paragraph",
      "html": "<p id='39' style='font-size:18px'>ing patch to be predicted. We add positional embeddings to<br>all tokens in this full set; without this, mask tokens would<br>have no information about their location in the image. The<br>decoder has another series of Transformer blocks.</p>",
      "id": 39,
      "page": 4,
      "text": "ing patch to be predicted. We add positional embeddings to\nall tokens in this full set; without this, mask tokens would\nhave no information about their location in the image. The\ndecoder has another series of Transformer blocks."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 520
        },
        {
          "x": 1198,
          "y": 520
        },
        {
          "x": 1198,
          "y": 1064
        },
        {
          "x": 201,
          "y": 1064
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='40' style='font-size:18px'>The MAE decoder is only used during pre-training to<br>perform the image reconstruction task (only the encoder<br>is used to produce image representations for recognition).<br>Therefore, the decoder architecture can be flexibly designed<br>in a manner that is independent of the encoder design. We<br>experiment with very small decoders, narrower and shal-<br>lower than the encoder. For example, our default decoder<br>has <10% computation per token vs. the encoder. With this<br>asymmetrical design, the full set of tokens are only pro-<br>cessed by the lightweight decoder, which significantly re-<br>duces pre-training time.</p>",
      "id": 40,
      "page": 4,
      "text": "The MAE decoder is only used during pre-training to\nperform the image reconstruction task (only the encoder\nis used to produce image representations for recognition).\nTherefore, the decoder architecture can be flexibly designed\nin a manner that is independent of the encoder design. We\nexperiment with very small decoders, narrower and shal-\nlower than the encoder. For example, our default decoder\nhas <10% computation per token vs. the encoder. With this\nasymmetrical design, the full set of tokens are only pro-\ncessed by the lightweight decoder, which significantly re-\nduces pre-training time."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 1094
        },
        {
          "x": 1199,
          "y": 1094
        },
        {
          "x": 1199,
          "y": 1590
        },
        {
          "x": 201,
          "y": 1590
        }
      ],
      "category": "paragraph",
      "html": "<p id='41' style='font-size:18px'>Reconstruction target. Our MAE reconstructs the input<br>by predicting the pixel values for each masked patch. Each<br>element in the decoder's output is a vector of pixel values<br>representing a patch. The last layer of the decoder is a lin-<br>ear projection whose number of output channels equals the<br>number of pixel values in a patch. The decoder's output is<br>reshaped to form a reconstructed image. Our loss function<br>computes the mean squared error (MSE) between the recon-<br>structed and original images in the pixel space. We compute<br>the loss only on masked patches, similar to BERT [14].</p>",
      "id": 41,
      "page": 4,
      "text": "Reconstruction target. Our MAE reconstructs the input\nby predicting the pixel values for each masked patch. Each\nelement in the decoder's output is a vector of pixel values\nrepresenting a patch. The last layer of the decoder is a lin-\near projection whose number of output channels equals the\nnumber of pixel values in a patch. The decoder's output is\nreshaped to form a reconstructed image. Our loss function\ncomputes the mean squared error (MSE) between the recon-\nstructed and original images in the pixel space. We compute\nthe loss only on masked patches, similar to BERT [14]."
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 1608
        },
        {
          "x": 1199,
          "y": 1608
        },
        {
          "x": 1199,
          "y": 1904
        },
        {
          "x": 203,
          "y": 1904
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='42' style='font-size:18px'>We also study a variant whose reconstruction target is<br>the normalized pixel values of each masked patch. Specif-<br>ically, we compute the mean and standard deviation of all<br>pixels in a patch and use them to normalize this patch. Us-<br>ing normalized pixels as the reconstruction target improves<br>representation quality in our experiments.</p>",
      "id": 42,
      "page": 4,
      "text": "We also study a variant whose reconstruction target is\nthe normalized pixel values of each masked patch. Specif-\nically, we compute the mean and standard deviation of all\npixels in a patch and use them to normalize this patch. Us-\ning normalized pixels as the reconstruction target improves\nrepresentation quality in our experiments."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 1935
        },
        {
          "x": 1198,
          "y": 1935
        },
        {
          "x": 1198,
          "y": 2731
        },
        {
          "x": 201,
          "y": 2731
        }
      ],
      "category": "paragraph",
      "html": "<p id='43' style='font-size:18px'>Simple implementation. Our MAE pre-training can be im-<br>plemented efficiently, and importantly, does not require any<br>specialized sparse operations. First we generate a token for<br>every input patch (by linear projection with an added po-<br>sitional embedding). Next we randomly shuffle the list of<br>tokens and remove the last portion of the list, based on the<br>masking ratio. This process produces a small subset of to-<br>kens for the encoder and is equivalent to sampling patches<br>without replacement. After encoding, we append a list of<br>mask tokens to the list of encoded patches, and unshuffle<br>this full list (inverting the random shuffle operation) to align<br>all tokens with their targets. The decoder is applied to this<br>full list (with positional embeddings added). As noted, no<br>sparse operations are needed. This simple implementation<br>introduces negligible overhead as the shuffling and unshuf-<br>fling operations are fast.</p>",
      "id": 43,
      "page": 4,
      "text": "Simple implementation. Our MAE pre-training can be im-\nplemented efficiently, and importantly, does not require any\nspecialized sparse operations. First we generate a token for\nevery input patch (by linear projection with an added po-\nsitional embedding). Next we randomly shuffle the list of\ntokens and remove the last portion of the list, based on the\nmasking ratio. This process produces a small subset of to-\nkens for the encoder and is equivalent to sampling patches\nwithout replacement. After encoding, we append a list of\nmask tokens to the list of encoded patches, and unshuffle\nthis full list (inverting the random shuffle operation) to align\nall tokens with their targets. The decoder is applied to this\nfull list (with positional embeddings added). As noted, no\nsparse operations are needed. This simple implementation\nintroduces negligible overhead as the shuffling and unshuf-\nfling operations are fast."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 2812
        },
        {
          "x": 1199,
          "y": 2812
        },
        {
          "x": 1199,
          "y": 2974
        },
        {
          "x": 201,
          "y": 2974
        }
      ],
      "category": "paragraph",
      "html": "<p id='44' style='font-size:14px'>1Computing the loss only on masked patches differs from traditional<br>denoising autoencoders [58] that compute the loss on all pixels. This<br>choice is purely result-driven: computing the loss on all pixels leads to<br>a slight decrease in accuracy (e.g., ~0.5%).</p>",
      "id": 44,
      "page": 4,
      "text": "1Computing the loss only on masked patches differs from traditional\ndenoising autoencoders [58] that compute the loss on all pixels. This\nchoice is purely result-driven: computing the loss on all pixels leads to\na slight decrease in accuracy (e.g., ~0.5%)."
    },
    {
      "bounding_box": [
        {
          "x": 1333,
          "y": 264
        },
        {
          "x": 2226,
          "y": 264
        },
        {
          "x": 2226,
          "y": 792
        },
        {
          "x": 1333,
          "y": 792
        }
      ],
      "category": "figure",
      "html": "<br><figure><img id='45' style='font-size:14px' alt=\"85.0\nfine-tuning 84.9 84.9 84.9\n85 84.7\n84.5\n84\n83.4 83.4\n83 2\n83.0\n83\n10 20 30 40 50 60 70 80 90\nmasking ratio (%)\n73.2 73.5\n71.8 71.8\nlinear probing 69.9\n70\n67.0\n66.1\n61.7\n58.9\n60\n54.6\n50\n10 20 30 40 50 60 70 80 90\nmasking ratio (%)\" data-coord=\"top-left:(1333,264); bottom-right:(2226,792)\" /></figure>",
      "id": 45,
      "page": 4,
      "text": "85.0\nfine-tuning 84.9 84.9 84.9\n85 84.7\n84.5\n84\n83.4 83.4\n83 2\n83.0\n83\n10 20 30 40 50 60 70 80 90\nmasking ratio (%)\n73.2 73.5\n71.8 71.8\nlinear probing 69.9\n70\n67.0\n66.1\n61.7\n58.9\n60\n54.6\n50\n10 20 30 40 50 60 70 80 90\nmasking ratio (%)"
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 804
        },
        {
          "x": 2277,
          "y": 804
        },
        {
          "x": 2277,
          "y": 944
        },
        {
          "x": 1281,
          "y": 944
        }
      ],
      "category": "caption",
      "html": "<br><caption id='46' style='font-size:16px'>Figure 5. Masking ratio. A high masking ratio (75%) works well<br>for both fine-tuning (top) and linear probing (bottom). The y-axes<br>are ImageNet-1K validation accuracy (%) in all plots in this paper.</caption>",
      "id": 46,
      "page": 4,
      "text": "Figure 5. Masking ratio. A high masking ratio (75%) works well\nfor both fine-tuning (top) and linear probing (bottom). The y-axes\nare ImageNet-1K validation accuracy (%) in all plots in this paper."
    },
    {
      "bounding_box": [
        {
          "x": 1282,
          "y": 996
        },
        {
          "x": 1830,
          "y": 996
        },
        {
          "x": 1830,
          "y": 1047
        },
        {
          "x": 1282,
          "y": 1047
        }
      ],
      "category": "paragraph",
      "html": "<p id='47' style='font-size:22px'>4. ImageNet Experiments</p>",
      "id": 47,
      "page": 4,
      "text": "4. ImageNet Experiments"
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 1079
        },
        {
          "x": 2277,
          "y": 1079
        },
        {
          "x": 2277,
          "y": 1324
        },
        {
          "x": 1280,
          "y": 1324
        }
      ],
      "category": "paragraph",
      "html": "<p id='48' style='font-size:20px'>We do self-supervised pre-training on the ImageNet-1K<br>(IN1K) [13] training set. Then we do supervised training to<br>evaluate the representations with (i) end-to-end fine-tuning<br>or (ii) linear probing. We report top-1 validation accuracy<br>of a single 224x224 crop. Details are in Appendix A.1.</p>",
      "id": 48,
      "page": 4,
      "text": "We do self-supervised pre-training on the ImageNet-1K\n(IN1K) [13] training set. Then we do supervised training to\nevaluate the representations with (i) end-to-end fine-tuning\nor (ii) linear probing. We report top-1 validation accuracy\nof a single 224x224 crop. Details are in Appendix A.1."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 1341
        },
        {
          "x": 2278,
          "y": 1341
        },
        {
          "x": 2278,
          "y": 1588
        },
        {
          "x": 1280,
          "y": 1588
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='49' style='font-size:18px'>Baseline: ViT-Large. We use ViT-Large (ViT-L/16) [16]<br>as the backbone in our ablation study. ViT-L is very big (an<br>order of magnitude bigger than ResNet-50 [25]) and tends<br>to overfit. The following is a comparison between ViT-L<br>trained from scratch vs. fine-tuned from our baseline MAE:</p>",
      "id": 49,
      "page": 4,
      "text": "Baseline: ViT-Large. We use ViT-Large (ViT-L/16) [16]\nas the backbone in our ablation study. ViT-L is very big (an\norder of magnitude bigger than ResNet-50 [25]) and tends\nto overfit. The following is a comparison between ViT-L\ntrained from scratch vs. fine-tuned from our baseline MAE:"
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 1711
        },
        {
          "x": 2275,
          "y": 1711
        },
        {
          "x": 2275,
          "y": 2010
        },
        {
          "x": 1280,
          "y": 2010
        }
      ],
      "category": "paragraph",
      "html": "<p id='50' style='font-size:20px'>We note that it is nontrivial to train supervised ViT-L from<br>scratch and a good recipe with strong regularization is<br>needed (82.5%, see Appendix A.2). Even so, our MAE pre-<br>training contributes a big improvement. Here fine-tuning is<br>only for 50 epochs (vs. 200 from scratch), implying that the<br>fine-tuning accuracy heavily depends on pre-training.</p>",
      "id": 50,
      "page": 4,
      "text": "We note that it is nontrivial to train supervised ViT-L from\nscratch and a good recipe with strong regularization is\nneeded (82.5%, see Appendix A.2). Even so, our MAE pre-\ntraining contributes a big improvement. Here fine-tuning is\nonly for 50 epochs (vs. 200 from scratch), implying that the\nfine-tuning accuracy heavily depends on pre-training."
    },
    {
      "bounding_box": [
        {
          "x": 1282,
          "y": 2040
        },
        {
          "x": 1687,
          "y": 2040
        },
        {
          "x": 1687,
          "y": 2086
        },
        {
          "x": 1282,
          "y": 2086
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='51' style='font-size:20px'>4.1. Main Properties</p>",
      "id": 51,
      "page": 4,
      "text": "4.1. Main Properties"
    },
    {
      "bounding_box": [
        {
          "x": 1282,
          "y": 2116
        },
        {
          "x": 2272,
          "y": 2116
        },
        {
          "x": 2272,
          "y": 2213
        },
        {
          "x": 1282,
          "y": 2213
        }
      ],
      "category": "paragraph",
      "html": "<p id='52' style='font-size:20px'>We ablate our MAE using the default settings in Table 1<br>(see caption). Several intriguing properties are observed.</p>",
      "id": 52,
      "page": 4,
      "text": "We ablate our MAE using the default settings in Table 1\n(see caption). Several intriguing properties are observed."
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 2230
        },
        {
          "x": 2275,
          "y": 2230
        },
        {
          "x": 2275,
          "y": 2573
        },
        {
          "x": 1281,
          "y": 2573
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='53' style='font-size:18px'>Masking ratio. Figure 5 shows the influence of the mask-<br>ing ratio. The optimal ratios are surprisingly high. The ra-<br>tio of 75% is good for both linear probing and fine-tuning.<br>This behavior is in contrast with BERT [14], whose typical<br>masking ratio is 15%. Our masking ratios are also much<br>higher than those in related works [6, 16, 2] in computer<br>vision (20% to 50%).</p>",
      "id": 53,
      "page": 4,
      "text": "Masking ratio. Figure 5 shows the influence of the mask-\ning ratio. The optimal ratios are surprisingly high. The ra-\ntio of 75% is good for both linear probing and fine-tuning.\nThis behavior is in contrast with BERT [14], whose typical\nmasking ratio is 15%. Our masking ratios are also much\nhigher than those in related works [6, 16, 2] in computer\nvision (20% to 50%)."
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 2580
        },
        {
          "x": 2277,
          "y": 2580
        },
        {
          "x": 2277,
          "y": 2873
        },
        {
          "x": 1281,
          "y": 2873
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='54' style='font-size:18px'>The model infers missing patches to produce different,<br>yet plausible, outputs (Figure 4). It makes sense of the<br>gestalt of objects and scenes, which cannot be simply com-<br>pleted by extending lines or textures. We hypothesize that<br>this reasoning-like behavior is linked to the learning of use-<br>ful representations.</p>",
      "id": 54,
      "page": 4,
      "text": "The model infers missing patches to produce different,\nyet plausible, outputs (Figure 4). It makes sense of the\ngestalt of objects and scenes, which cannot be simply com-\npleted by extending lines or textures. We hypothesize that\nthis reasoning-like behavior is linked to the learning of use-\nful representations."
    },
    {
      "bounding_box": [
        {
          "x": 1282,
          "y": 2879
        },
        {
          "x": 2275,
          "y": 2879
        },
        {
          "x": 2275,
          "y": 2976
        },
        {
          "x": 1282,
          "y": 2976
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='55' style='font-size:20px'>Figure 5 also shows that linear probing and fine-tuning<br>results follow different trends. For linear probing, the ac-</p>",
      "id": 55,
      "page": 4,
      "text": "Figure 5 also shows that linear probing and fine-tuning\nresults follow different trends. For linear probing, the ac-"
    },
    {
      "bounding_box": [
        {
          "x": 1225,
          "y": 3056
        },
        {
          "x": 1250,
          "y": 3056
        },
        {
          "x": 1250,
          "y": 3090
        },
        {
          "x": 1225,
          "y": 3090
        }
      ],
      "category": "footer",
      "html": "<footer id='56' style='font-size:16px'>4</footer>",
      "id": 56,
      "page": 4,
      "text": "4"
    },
    {
      "bounding_box": [
        {
          "x": 339,
          "y": 290
        },
        {
          "x": 724,
          "y": 290
        },
        {
          "x": 724,
          "y": 540
        },
        {
          "x": 339,
          "y": 540
        }
      ],
      "category": "table",
      "html": "<table id='57' style='font-size:14px'><tr><td>blocks</td><td>ft</td><td>lin</td></tr><tr><td>1</td><td>84.8</td><td>65.5</td></tr><tr><td>2</td><td>84.9</td><td>70.0</td></tr><tr><td>4</td><td>84.9</td><td>71.9</td></tr><tr><td>8</td><td>84.9</td><td>73.5</td></tr><tr><td>12</td><td>84.4</td><td>73.3</td></tr></table>",
      "id": 57,
      "page": 5,
      "text": "blocks ft lin\n 1 84.8 65.5\n 2 84.9 70.0\n 4 84.9 71.9\n 8 84.9 73.5\n 12 84.4"
    },
    {
      "bounding_box": [
        {
          "x": 212,
          "y": 558
        },
        {
          "x": 837,
          "y": 558
        },
        {
          "x": 837,
          "y": 632
        },
        {
          "x": 212,
          "y": 632
        }
      ],
      "category": "caption",
      "html": "<br><caption id='58' style='font-size:16px'>(a) Decoder depth. A deep decoder can im-<br>prove linear probing accuracy.</caption>",
      "id": 58,
      "page": 5,
      "text": "(a) Decoder depth. A deep decoder can im-\nprove linear probing accuracy."
    },
    {
      "bounding_box": [
        {
          "x": 245,
          "y": 652
        },
        {
          "x": 822,
          "y": 652
        },
        {
          "x": 822,
          "y": 862
        },
        {
          "x": 245,
          "y": 862
        }
      ],
      "category": "table",
      "html": "<br><table id='59' style='font-size:14px'><tr><td>case</td><td>ft</td><td>lin</td></tr><tr><td>pixel (w/o norm)</td><td>84.9</td><td>73.5</td></tr><tr><td>pixel (w/ norm)</td><td>85.4</td><td>73.9</td></tr><tr><td>PCA</td><td>84.6</td><td>72.3</td></tr><tr><td>dVAE token</td><td>85.3</td><td>71.6</td></tr></table>",
      "id": 59,
      "page": 5,
      "text": "case ft lin\n pixel (w/o norm) 84.9 73.5\n pixel (w/ norm) 85.4 73.9\n PCA 84.6 72.3\n dVAE token 85.3"
    },
    {
      "bounding_box": [
        {
          "x": 1050,
          "y": 289
        },
        {
          "x": 1433,
          "y": 289
        },
        {
          "x": 1433,
          "y": 541
        },
        {
          "x": 1050,
          "y": 541
        }
      ],
      "category": "table",
      "html": "<br><table id='60' style='font-size:14px'><tr><td>dim</td><td>ft</td><td>lin</td></tr><tr><td>128</td><td>84.9</td><td>69.1</td></tr><tr><td>256</td><td>84.8</td><td>71.3</td></tr><tr><td>512</td><td>84.9</td><td>73.5</td></tr><tr><td>768</td><td>84.4</td><td>73.1</td></tr><tr><td>1024</td><td>84.3</td><td>73.1</td></tr></table>",
      "id": 60,
      "page": 5,
      "text": "dim ft lin\n 128 84.9 69.1\n 256 84.8 71.3\n 512 84.9 73.5\n 768 84.4 73.1\n 1024 84.3"
    },
    {
      "bounding_box": [
        {
          "x": 926,
          "y": 558
        },
        {
          "x": 1552,
          "y": 558
        },
        {
          "x": 1552,
          "y": 630
        },
        {
          "x": 926,
          "y": 630
        }
      ],
      "category": "caption",
      "html": "<br><caption id='61' style='font-size:14px'>(b) Decoder width. The decoder can be nar-<br>rower than the encoder (1024-d).</caption>",
      "id": 61,
      "page": 5,
      "text": "(b) Decoder width. The decoder can be nar-\nrower than the encoder (1024-d)."
    },
    {
      "bounding_box": [
        {
          "x": 990,
          "y": 650
        },
        {
          "x": 1501,
          "y": 650
        },
        {
          "x": 1501,
          "y": 863
        },
        {
          "x": 990,
          "y": 863
        }
      ],
      "category": "table",
      "html": "<table id='62' style='font-size:14px'><tr><td>case</td><td>ft</td><td>lin</td></tr><tr><td>none</td><td>84.0</td><td>65.7</td></tr><tr><td>crop, fixed size</td><td>84.7</td><td>73.1</td></tr><tr><td>crop, rand size</td><td>84.9</td><td>73.5</td></tr><tr><td>crop + color jit</td><td>84.3</td><td>71.9</td></tr></table>",
      "id": 62,
      "page": 5,
      "text": "case ft lin\n none 84.0 65.7\n crop, fixed size 84.7 73.1\n crop, rand size 84.9 73.5\n crop + color jit 84.3"
    },
    {
      "bounding_box": [
        {
          "x": 1670,
          "y": 289
        },
        {
          "x": 2242,
          "y": 289
        },
        {
          "x": 2242,
          "y": 422
        },
        {
          "x": 1670,
          "y": 422
        }
      ],
      "category": "table",
      "html": "<br><table id='63' style='font-size:14px'><tr><td>case</td><td>ft</td><td>lin</td><td>FLOPs</td></tr><tr><td>encoder w/ [M]</td><td>84.2</td><td>59.6</td><td>3.3x</td></tr><tr><td>encoder w/o [M]</td><td>84.9</td><td>73.5</td><td>1x</td></tr></table>",
      "id": 63,
      "page": 5,
      "text": "case ft lin FLOPs\n encoder w/ [M] 84.2 59.6 3.3x\n encoder w/o [M] 84.9 73.5"
    },
    {
      "bounding_box": [
        {
          "x": 1637,
          "y": 557
        },
        {
          "x": 2264,
          "y": 557
        },
        {
          "x": 2264,
          "y": 631
        },
        {
          "x": 1637,
          "y": 631
        }
      ],
      "category": "caption",
      "html": "<caption id='64' style='font-size:14px'>(c) Mask token. An encoder without mask to-<br>kens is more accurate and faster (Table 2).</caption>",
      "id": 64,
      "page": 5,
      "text": "(c) Mask token. An encoder without mask to-\nkens is more accurate and faster (Table 2)."
    },
    {
      "bounding_box": [
        {
          "x": 1726,
          "y": 651
        },
        {
          "x": 2185,
          "y": 651
        },
        {
          "x": 2185,
          "y": 863
        },
        {
          "x": 1726,
          "y": 863
        }
      ],
      "category": "table",
      "html": "<table id='65' style='font-size:14px'><tr><td>case</td><td>ratio</td><td>ft</td><td>lin</td></tr><tr><td>random</td><td>75</td><td>84.9</td><td>73.5</td></tr><tr><td>block</td><td>50</td><td>83.9</td><td>72.3</td></tr><tr><td>block</td><td>75</td><td>82.8</td><td>63.9</td></tr><tr><td>grid</td><td>75</td><td>84.0</td><td>66.0</td></tr></table>",
      "id": 65,
      "page": 5,
      "text": "case ratio ft lin\n random 75 84.9 73.5\n block 50 83.9 72.3\n block 75 82.8 63.9\n grid 75 84.0"
    },
    {
      "bounding_box": [
        {
          "x": 210,
          "y": 877
        },
        {
          "x": 840,
          "y": 877
        },
        {
          "x": 840,
          "y": 952
        },
        {
          "x": 210,
          "y": 952
        }
      ],
      "category": "caption",
      "html": "<br><caption id='66' style='font-size:14px'>(d) Reconstruction target. Pixels as recon-<br>struction targets are effective.</caption>",
      "id": 66,
      "page": 5,
      "text": "(d) Reconstruction target. Pixels as recon-\nstruction targets are effective."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 992
        },
        {
          "x": 2278,
          "y": 992
        },
        {
          "x": 2278,
          "y": 1130
        },
        {
          "x": 202,
          "y": 1130
        }
      ],
      "category": "paragraph",
      "html": "<p id='67' style='font-size:18px'>Table 1. MAE ablation experiments with ViT-L/16 on ImageNet-1K. We report fine-tuning (ft) and linear probing (lin) accuracy (%). If<br>not specified, the default is: the decoder has depth 8 and width 512, the reconstruction target is unnormalized pixels, the data augmentation<br>is random resized cropping, the masking ratio is 75%, and the pre-training length is 800 epochs. Default settings are marked in gray</p>",
      "id": 67,
      "page": 5,
      "text": "Table 1. MAE ablation experiments with ViT-L/16 on ImageNet-1K. We report fine-tuning (ft) and linear probing (lin) accuracy (%). If\nnot specified, the default is: the decoder has depth 8 and width 512, the reconstruction target is unnormalized pixels, the data augmentation\nis random resized cropping, the masking ratio is 75%, and the pre-training length is 800 epochs. Default settings are marked in gray"
    },
    {
      "bounding_box": [
        {
          "x": 925,
          "y": 876
        },
        {
          "x": 1555,
          "y": 876
        },
        {
          "x": 1555,
          "y": 952
        },
        {
          "x": 925,
          "y": 952
        }
      ],
      "category": "caption",
      "html": "<br><caption id='68' style='font-size:14px'>(e) Data augmentation. Our MAE works with<br>minimal or no augmentation.</caption>",
      "id": 68,
      "page": 5,
      "text": "(e) Data augmentation. Our MAE works with\nminimal or no augmentation."
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 1208
        },
        {
          "x": 1199,
          "y": 1208
        },
        {
          "x": 1199,
          "y": 1500
        },
        {
          "x": 203,
          "y": 1500
        }
      ],
      "category": "paragraph",
      "html": "<p id='69' style='font-size:20px'>curacy increases steadily with the masking ratio until the<br>sweet point: the accuracy gap is up to ~20% (54.6% vs.<br>73.5%). For fine-tuning, the results are less sensitive to the<br>ratios, and a wide range of masking ratios (40-80%) work<br>well. All fine-tuning results in Figure 5 are better than train-<br>ing from scratch (82.5%).</p>",
      "id": 69,
      "page": 5,
      "text": "curacy increases steadily with the masking ratio until the\nsweet point: the accuracy gap is up to ~20% (54.6% vs.\n73.5%). For fine-tuning, the results are less sensitive to the\nratios, and a wide range of masking ratios (40-80%) work\nwell. All fine-tuning results in Figure 5 are better than train-\ning from scratch (82.5%)."
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 1523
        },
        {
          "x": 1195,
          "y": 1523
        },
        {
          "x": 1195,
          "y": 1615
        },
        {
          "x": 204,
          "y": 1615
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='70' style='font-size:18px'>Decoder design. Our MAE decoder can be flexibly de-<br>signed, as studied in Table 1a and 1b.</p>",
      "id": 70,
      "page": 5,
      "text": "Decoder design. Our MAE decoder can be flexibly de-\nsigned, as studied in Table 1a and 1b."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 1625
        },
        {
          "x": 1197,
          "y": 1625
        },
        {
          "x": 1197,
          "y": 2267
        },
        {
          "x": 202,
          "y": 2267
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='71' style='font-size:20px'>Table 1a varies the decoder depth (number of Trans-<br>former blocks). A sufficiently deep decoder is important<br>for linear probing. This can be explained by the gap be-<br>tween a pixel reconstruction task and a recognition task: the<br>last several layers in an autoencoder are more specialized<br>for reconstruction, but are less relevant for recognition. A<br>reasonably deep decoder can account for the reconstruction<br>specialization, leaving the latent representations at a more<br>abstract level. This design can yield up to 8% improvement<br>in linear probing (Table 1a, 'lin'). However, if fine-tuning<br>is used, the last layers of the encoder can be tuned to adapt<br>to the recognition task. The decoder depth is less influential<br>for improving fine-tuning (Table 1a, 'ft').</p>",
      "id": 71,
      "page": 5,
      "text": "Table 1a varies the decoder depth (number of Trans-\nformer blocks). A sufficiently deep decoder is important\nfor linear probing. This can be explained by the gap be-\ntween a pixel reconstruction task and a recognition task: the\nlast several layers in an autoencoder are more specialized\nfor reconstruction, but are less relevant for recognition. A\nreasonably deep decoder can account for the reconstruction\nspecialization, leaving the latent representations at a more\nabstract level. This design can yield up to 8% improvement\nin linear probing (Table 1a, 'lin'). However, if fine-tuning\nis used, the last layers of the encoder can be tuned to adapt\nto the recognition task. The decoder depth is less influential\nfor improving fine-tuning (Table 1a, 'ft')."
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 2275
        },
        {
          "x": 1198,
          "y": 2275
        },
        {
          "x": 1198,
          "y": 2519
        },
        {
          "x": 203,
          "y": 2519
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='72' style='font-size:18px'>Interestingly, our MAE with a single-block decoder can<br>perform strongly with fine-tuning (84.8%). Note that a sin-<br>gle Transformer block is the minimal requirement to propa-<br>gate information from visible tokens to mask tokens. Such<br>a small decoder can further speed up training.</p>",
      "id": 72,
      "page": 5,
      "text": "Interestingly, our MAE with a single-block decoder can\nperform strongly with fine-tuning (84.8%). Note that a sin-\ngle Transformer block is the minimal requirement to propa-\ngate information from visible tokens to mask tokens. Such\na small decoder can further speed up training."
    },
    {
      "bounding_box": [
        {
          "x": 1639,
          "y": 876
        },
        {
          "x": 2267,
          "y": 876
        },
        {
          "x": 2267,
          "y": 951
        },
        {
          "x": 1639,
          "y": 951
        }
      ],
      "category": "caption",
      "html": "<br><caption id='73' style='font-size:16px'>(f) Mask sampling. Random sampling works<br>the best. See Figure 6 for visualizations.</caption>",
      "id": 73,
      "page": 5,
      "text": "(f) Mask sampling. Random sampling works\nthe best. See Figure 6 for visualizations."
    },
    {
      "bounding_box": [
        {
          "x": 1354,
          "y": 1198
        },
        {
          "x": 2192,
          "y": 1198
        },
        {
          "x": 2192,
          "y": 1506
        },
        {
          "x": 1354,
          "y": 1506
        }
      ],
      "category": "table",
      "html": "<table id='74' style='font-size:14px'><tr><td>encoder</td><td>dec. depth</td><td>ft acc</td><td>hours</td><td>speedup</td></tr><tr><td>ViT-L, w/ [M]</td><td>8</td><td>84.2</td><td>42.4</td><td></td></tr><tr><td>ViT-L</td><td>8</td><td>84.9</td><td>15.4</td><td>2.8x</td></tr><tr><td>ViT-L</td><td>1</td><td>84.8</td><td>11.6</td><td>3.7x</td></tr><tr><td>ViT-H, w/ [M]</td><td>8</td><td>-</td><td>119.6†</td><td>-</td></tr><tr><td>ViT-H</td><td>8</td><td>85.8</td><td>34.5</td><td>3.5x</td></tr><tr><td>ViT-H</td><td>1</td><td>85.9</td><td>29.3</td><td>4.1x</td></tr></table>",
      "id": 74,
      "page": 5,
      "text": "encoder dec. depth ft acc hours speedup\n ViT-L, w/ [M] 8 84.2 42.4 \n ViT-L 8 84.9 15.4 2.8x\n ViT-L 1 84.8 11.6 3.7x\n ViT-H, w/ [M] 8 - 119.6† -\n ViT-H 8 85.8 34.5 3.5x\n ViT-H 1 85.9 29.3"
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 2527
        },
        {
          "x": 1198,
          "y": 2527
        },
        {
          "x": 1198,
          "y": 2721
        },
        {
          "x": 203,
          "y": 2721
        }
      ],
      "category": "paragraph",
      "html": "<p id='75' style='font-size:20px'>In Table 1b we study the decoder width (number of chan-<br>nels). We use 512-d by default, which performs well un-<br>der fine-tuning and linear probing. A narrower decoder also<br>works well with fine-tuning.</p>",
      "id": 75,
      "page": 5,
      "text": "In Table 1b we study the decoder width (number of chan-\nnels). We use 512-d by default, which performs well un-\nder fine-tuning and linear probing. A narrower decoder also\nworks well with fine-tuning."
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 2729
        },
        {
          "x": 1198,
          "y": 2729
        },
        {
          "x": 1198,
          "y": 2975
        },
        {
          "x": 203,
          "y": 2975
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='76' style='font-size:18px'>Overall, our default MAE decoder is lightweight. It has<br>8 blocks and a width of 512-d ( gray in Table 1). It only<br>has 9% FLOPs per token vs. ViT-L (24 blocks, 1024-d).<br>As such, while the decoder processes all tokens, it is still a<br>small fraction of the overall compute.</p>",
      "id": 76,
      "page": 5,
      "text": "Overall, our default MAE decoder is lightweight. It has\n8 blocks and a width of 512-d ( gray in Table 1). It only\nhas 9% FLOPs per token vs. ViT-L (24 blocks, 1024-d).\nAs such, while the decoder processes all tokens, it is still a\nsmall fraction of the overall compute."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 1524
        },
        {
          "x": 2278,
          "y": 1524
        },
        {
          "x": 2278,
          "y": 1754
        },
        {
          "x": 1280,
          "y": 1754
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='77' style='font-size:18px'>Table 2. Wall-clock time of our MAE training (800 epochs),<br>benchmarked in 128 TPU-v3 cores with TensorFlow. The speedup<br>is relative to the entry whose encoder has mask tokens (gray). The<br>decoder width is 512, and the mask ratio is 75%. t: This entry is<br>estimated by training ten epochs.</p>",
      "id": 77,
      "page": 5,
      "text": "Table 2. Wall-clock time of our MAE training (800 epochs),\nbenchmarked in 128 TPU-v3 cores with TensorFlow. The speedup\nis relative to the entry whose encoder has mask tokens (gray). The\ndecoder width is 512, and the mask ratio is 75%. t: This entry is\nestimated by training ten epochs."
    },
    {
      "bounding_box": [
        {
          "x": 1282,
          "y": 1830
        },
        {
          "x": 2276,
          "y": 1830
        },
        {
          "x": 2276,
          "y": 1975
        },
        {
          "x": 1282,
          "y": 1975
        }
      ],
      "category": "paragraph",
      "html": "<p id='78' style='font-size:20px'>Mask token. An important design of our MAE is to skip<br>the mask token [M] in the encoder and apply it later in the<br>lightweight decoder. Table 1c studies this design.</p>",
      "id": 78,
      "page": 5,
      "text": "Mask token. An important design of our MAE is to skip\nthe mask token [M] in the encoder and apply it later in the\nlightweight decoder. Table 1c studies this design."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 1983
        },
        {
          "x": 2277,
          "y": 1983
        },
        {
          "x": 2277,
          "y": 2376
        },
        {
          "x": 1280,
          "y": 2376
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='79' style='font-size:20px'>If the encoder uses mask tokens, it performs worse: its<br>accuracy drops by 14% in linear probing. In this case,<br>there is a gap between pre-training and deploying: this en-<br>coder has a large portion of mask tokens in its input in pre-<br>training, which does not exist in uncorrupted images. This<br>gap may degrade accuracy in deployment. By removing the<br>mask token from the encoder, we constrain the encoder to<br>always see real patches and thus improve accuracy.</p>",
      "id": 79,
      "page": 5,
      "text": "If the encoder uses mask tokens, it performs worse: its\naccuracy drops by 14% in linear probing. In this case,\nthere is a gap between pre-training and deploying: this en-\ncoder has a large portion of mask tokens in its input in pre-\ntraining, which does not exist in uncorrupted images. This\ngap may degrade accuracy in deployment. By removing the\nmask token from the encoder, we constrain the encoder to\nalways see real patches and thus improve accuracy."
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 2382
        },
        {
          "x": 2277,
          "y": 2382
        },
        {
          "x": 2277,
          "y": 2977
        },
        {
          "x": 1278,
          "y": 2977
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='80' style='font-size:22px'>Moreover, by skipping the mask token in the encoder,<br>we greatly reduce training computation. In Table 1c, we<br>reduce the overall training FLOPs by 3.3x. This leads to<br>a 2.8x wall-clock speedup in our implementation (see Ta-<br>ble 2). The wall-clock speedup is even bigger (3.5-4.1x),<br>for a smaller decoder (1-block), a larger encoder (ViT-H),<br>or both. Note that the speedup can be >4x for a masking<br>ratio of 75%, partially because the self-attention complexity<br>is quadratic. In addition, memory is greatly reduced, which<br>can enable training even larger models or speeding up more<br>by large-batch training. The time and memory efficiency<br>makes our MAE favorable for training very large models.</p>",
      "id": 80,
      "page": 5,
      "text": "Moreover, by skipping the mask token in the encoder,\nwe greatly reduce training computation. In Table 1c, we\nreduce the overall training FLOPs by 3.3x. This leads to\na 2.8x wall-clock speedup in our implementation (see Ta-\nble 2). The wall-clock speedup is even bigger (3.5-4.1x),\nfor a smaller decoder (1-block), a larger encoder (ViT-H),\nor both. Note that the speedup can be >4x for a masking\nratio of 75%, partially because the self-attention complexity\nis quadratic. In addition, memory is greatly reduced, which\ncan enable training even larger models or speeding up more\nby large-batch training. The time and memory efficiency\nmakes our MAE favorable for training very large models."
    },
    {
      "bounding_box": [
        {
          "x": 1228,
          "y": 3057
        },
        {
          "x": 1250,
          "y": 3057
        },
        {
          "x": 1250,
          "y": 3089
        },
        {
          "x": 1228,
          "y": 3089
        }
      ],
      "category": "footer",
      "html": "<footer id='81' style='font-size:18px'>5</footer>",
      "id": 81,
      "page": 5,
      "text": "5"
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 290
        },
        {
          "x": 1193,
          "y": 290
        },
        {
          "x": 1193,
          "y": 678
        },
        {
          "x": 200,
          "y": 678
        }
      ],
      "category": "figure",
      "html": "<figure><img id='82' style='font-size:14px' alt=\"random 75% block 50% grid 75%\" data-coord=\"top-left:(200,290); bottom-right:(1193,678)\" /></figure>",
      "id": 82,
      "page": 6,
      "text": "random 75% block 50% grid 75%"
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 706
        },
        {
          "x": 1199,
          "y": 706
        },
        {
          "x": 1199,
          "y": 1026
        },
        {
          "x": 201,
          "y": 1026
        }
      ],
      "category": "caption",
      "html": "<caption id='83' style='font-size:18px'>Figure 6. Mask sampling strategies determine the pretext task<br>difficulty, influencing reconstruction quality and representations<br>(Table 1f). Here each outputis from an MAE trained with the spec-<br>ified masking strategy. Left: random sampling (our default). Mid-<br>dle: block-wise sampling [2] that removes large random blocks.<br>Right: grid-wise sampling that keeps one of every four patches.<br>Images are from the validation set.</caption>",
      "id": 83,
      "page": 6,
      "text": "Figure 6. Mask sampling strategies determine the pretext task\ndifficulty, influencing reconstruction quality and representations\n(Table 1f). Here each outputis from an MAE trained with the spec-\nified masking strategy. Left: random sampling (our default). Mid-\ndle: block-wise sampling [2] that removes large random blocks.\nRight: grid-wise sampling that keeps one of every four patches.\nImages are from the validation set."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 1145
        },
        {
          "x": 1199,
          "y": 1145
        },
        {
          "x": 1199,
          "y": 1591
        },
        {
          "x": 202,
          "y": 1591
        }
      ],
      "category": "paragraph",
      "html": "<p id='84' style='font-size:20px'>Reconstruction target. We compare different reconstruc-<br>tion targets in Table 1d. Our results thus far are based on<br>pixels without (per-patch) normalization. Using pixels with<br>normalization improves accuracy. This per-patch normal-<br>ization enhances the contrast locally. In another variant, we<br>perform PCA in the patch space and use the largest PCA<br>coefficients (96 here) as the target. Doing SO degrades ac-<br>curacy. Both experiments suggest that the high-frequency<br>components are useful in our method.</p>",
      "id": 84,
      "page": 6,
      "text": "Reconstruction target. We compare different reconstruc-\ntion targets in Table 1d. Our results thus far are based on\npixels without (per-patch) normalization. Using pixels with\nnormalization improves accuracy. This per-patch normal-\nization enhances the contrast locally. In another variant, we\nperform PCA in the patch space and use the largest PCA\ncoefficients (96 here) as the target. Doing SO degrades ac-\ncuracy. Both experiments suggest that the high-frequency\ncomponents are useful in our method."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 1601
        },
        {
          "x": 1197,
          "y": 1601
        },
        {
          "x": 1197,
          "y": 2044
        },
        {
          "x": 201,
          "y": 2044
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='85' style='font-size:20px'>We also compare an MAE variant that predicts tokens,<br>the target used in BEiT [2]. Specifically for this variant,<br>we use the DALLE pre-trained dVAE [50] as the tokenizer,<br>following [2]. Here the MAE decoder predicts the token in-<br>dices using cross-entropy loss. This tokenization improves<br>fine-tuning accuracy by 0.4% vs. unnormalized pixels, but<br>has no advantage vs. normalized pixels. It also reduces lin-<br>ear probing accuracy. In §5 we further show that tokeniza-<br>tion is not necessary in transfer learning.</p>",
      "id": 85,
      "page": 6,
      "text": "We also compare an MAE variant that predicts tokens,\nthe target used in BEiT [2]. Specifically for this variant,\nwe use the DALLE pre-trained dVAE [50] as the tokenizer,\nfollowing [2]. Here the MAE decoder predicts the token in-\ndices using cross-entropy loss. This tokenization improves\nfine-tuning accuracy by 0.4% vs. unnormalized pixels, but\nhas no advantage vs. normalized pixels. It also reduces lin-\near probing accuracy. In §5 we further show that tokeniza-\ntion is not necessary in transfer learning."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 2054
        },
        {
          "x": 1198,
          "y": 2054
        },
        {
          "x": 1198,
          "y": 2348
        },
        {
          "x": 201,
          "y": 2348
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='86' style='font-size:20px'>Our pixel-based MAE is much simpler than tokeniza-<br>tion. The dVAE tokenizer requires one more pre-training<br>stage, which may depend on extra data (250M images [50]).<br>The dVAE encoder is a large convolutional network (40%<br>FLOPs of ViT-L) and adds nontrivial overhead. Using pix-<br>els does not suffer from these problems.</p>",
      "id": 86,
      "page": 6,
      "text": "Our pixel-based MAE is much simpler than tokeniza-\ntion. The dVAE tokenizer requires one more pre-training\nstage, which may depend on extra data (250M images [50]).\nThe dVAE encoder is a large convolutional network (40%\nFLOPs of ViT-L) and adds nontrivial overhead. Using pix-\nels does not suffer from these problems."
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 2372
        },
        {
          "x": 1196,
          "y": 2372
        },
        {
          "x": 1196,
          "y": 2465
        },
        {
          "x": 204,
          "y": 2465
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='87' style='font-size:18px'>Data augmentation. Table 1e studies the influence of data<br>augmentation on our MAE pre-training.</p>",
      "id": 87,
      "page": 6,
      "text": "Data augmentation. Table 1e studies the influence of data\naugmentation on our MAE pre-training."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 2476
        },
        {
          "x": 1197,
          "y": 2476
        },
        {
          "x": 1197,
          "y": 2670
        },
        {
          "x": 202,
          "y": 2670
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='88' style='font-size:20px'>Our MAE works well using cropping-only augmenta-<br>tion, either fixed-size or random-size (both having random<br>horizontal flipping). Adding color jittering degrades the re-<br>sults and SO we do not use it in other experiments.</p>",
      "id": 88,
      "page": 6,
      "text": "Our MAE works well using cropping-only augmenta-\ntion, either fixed-size or random-size (both having random\nhorizontal flipping). Adding color jittering degrades the re-\nsults and SO we do not use it in other experiments."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 2680
        },
        {
          "x": 1198,
          "y": 2680
        },
        {
          "x": 1198,
          "y": 2978
        },
        {
          "x": 201,
          "y": 2978
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='89' style='font-size:22px'>Surprisingly, our MAE behaves decently even if using<br>no data augmentation (only center-crop, no flipping). This<br>property is dramatically different from contrastive learning<br>and related methods [62, 23, 7, 21], which heavily rely<br>on data augmentation. It was observed [21] that using<br>cropping-only augmentation reduces the accuracy by 13%</p>",
      "id": 89,
      "page": 6,
      "text": "Surprisingly, our MAE behaves decently even if using\nno data augmentation (only center-crop, no flipping). This\nproperty is dramatically different from contrastive learning\nand related methods [62, 23, 7, 21], which heavily rely\non data augmentation. It was observed [21] that using\ncropping-only augmentation reduces the accuracy by 13%"
    },
    {
      "bounding_box": [
        {
          "x": 1293,
          "y": 298
        },
        {
          "x": 2275,
          "y": 298
        },
        {
          "x": 2275,
          "y": 881
        },
        {
          "x": 1293,
          "y": 881
        }
      ],
      "category": "figure",
      "html": "<br><figure><img id='90' style='font-size:14px' alt=\"85.1\n84.9\n85 fine-tuning\n84.3\n84\n83.3\n83\n82.3\n82\n100 200 400 800 1600\nepochs (log-scale)\n75.1\n73.5\n75 linear probing\n69.7\n70\n64.4\n65\n60 57.3\n100 200 400 800 1600\nepochs (log-scale)\" data-coord=\"top-left:(1293,298); bottom-right:(2275,881)\" /></figure>",
      "id": 90,
      "page": 6,
      "text": "85.1\n84.9\n85 fine-tuning\n84.3\n84\n83.3\n83\n82.3\n82\n100 200 400 800 1600\nepochs (log-scale)\n75.1\n73.5\n75 linear probing\n69.7\n70\n64.4\n65\n60 57.3\n100 200 400 800 1600\nepochs (log-scale)"
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 897
        },
        {
          "x": 2275,
          "y": 897
        },
        {
          "x": 2275,
          "y": 1034
        },
        {
          "x": 1279,
          "y": 1034
        }
      ],
      "category": "caption",
      "html": "<br><caption id='91' style='font-size:16px'>Figure 7. Training schedules. A longer training schedule gives a<br>noticeable improvement. Here each point is a full training sched-<br>ule. The model is ViT-L with the default setting in Table 1.</caption>",
      "id": 91,
      "page": 6,
      "text": "Figure 7. Training schedules. A longer training schedule gives a\nnoticeable improvement. Here each point is a full training sched-\nule. The model is ViT-L with the default setting in Table 1."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 1135
        },
        {
          "x": 2276,
          "y": 1135
        },
        {
          "x": 2276,
          "y": 1329
        },
        {
          "x": 1280,
          "y": 1329
        }
      ],
      "category": "paragraph",
      "html": "<p id='92' style='font-size:18px'>and 28% respectively for BYOL [21] and SimCLR [7]. In<br>addition, there is no evidence that contrastive learning can<br>work without augmentation: the two views of an image are<br>the same and can easily satisfy a trivial solution.</p>",
      "id": 92,
      "page": 6,
      "text": "and 28% respectively for BYOL [21] and SimCLR [7]. In\naddition, there is no evidence that contrastive learning can\nwork without augmentation: the two views of an image are\nthe same and can easily satisfy a trivial solution."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 1344
        },
        {
          "x": 2277,
          "y": 1344
        },
        {
          "x": 2277,
          "y": 1640
        },
        {
          "x": 1280,
          "y": 1640
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='93' style='font-size:20px'>In MAE, the role of data augmentation is mainly per-<br>formed by random masking (ablated next). The masks are<br>different for each iteration and so they generate new training<br>samples regardless of data augmentation. The pretext task<br>is made difficult by masking and requires less augmentation<br>to regularize training.</p>",
      "id": 93,
      "page": 6,
      "text": "In MAE, the role of data augmentation is mainly per-\nformed by random masking (ablated next). The masks are\ndifferent for each iteration and so they generate new training\nsamples regardless of data augmentation. The pretext task\nis made difficult by masking and requires less augmentation\nto regularize training."
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 1673
        },
        {
          "x": 2274,
          "y": 1673
        },
        {
          "x": 2274,
          "y": 1767
        },
        {
          "x": 1281,
          "y": 1767
        }
      ],
      "category": "paragraph",
      "html": "<p id='94' style='font-size:22px'>Mask sampling strategy. In Table 1f we compare different<br>mask sampling strategies, illustrated in Figure 6.</p>",
      "id": 94,
      "page": 6,
      "text": "Mask sampling strategy. In Table 1f we compare different\nmask sampling strategies, illustrated in Figure 6."
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 1783
        },
        {
          "x": 2277,
          "y": 1783
        },
        {
          "x": 2277,
          "y": 2077
        },
        {
          "x": 1281,
          "y": 2077
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='95' style='font-size:20px'>The block-wise masking strategy, proposed in [2], tends<br>to remove large blocks (Figure 6 middle). Our MAE with<br>block-wise masking works reasonably well at a ratio of<br>50%, but degrades at a ratio of 75%. This task is harder<br>than that of random sampling, as a higher training loss is<br>observed. The reconstruction is also blurrier.</p>",
      "id": 95,
      "page": 6,
      "text": "The block-wise masking strategy, proposed in [2], tends\nto remove large blocks (Figure 6 middle). Our MAE with\nblock-wise masking works reasonably well at a ratio of\n50%, but degrades at a ratio of 75%. This task is harder\nthan that of random sampling, as a higher training loss is\nobserved. The reconstruction is also blurrier."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 2092
        },
        {
          "x": 2276,
          "y": 2092
        },
        {
          "x": 2276,
          "y": 2288
        },
        {
          "x": 1280,
          "y": 2288
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='96' style='font-size:20px'>We also study grid-wise sampling, which regularly keeps<br>one of every four patches (Figure 6 right). This is an eas-<br>ier task and has lower training loss. The reconstruction is<br>sharper. However, the representation quality is lower.</p>",
      "id": 96,
      "page": 6,
      "text": "We also study grid-wise sampling, which regularly keeps\none of every four patches (Figure 6 right). This is an eas-\nier task and has lower training loss. The reconstruction is\nsharper. However, the representation quality is lower."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 2304
        },
        {
          "x": 2275,
          "y": 2304
        },
        {
          "x": 2275,
          "y": 2452
        },
        {
          "x": 1280,
          "y": 2452
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='97' style='font-size:22px'>Simple random sampling works the best for our MAE. It<br>allows for a higher masking ratio, which provides a greater<br>speedup benefit while also enjoying good accuracy.</p>",
      "id": 97,
      "page": 6,
      "text": "Simple random sampling works the best for our MAE. It\nallows for a higher masking ratio, which provides a greater\nspeedup benefit while also enjoying good accuracy."
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 2478
        },
        {
          "x": 2276,
          "y": 2478
        },
        {
          "x": 2276,
          "y": 2977
        },
        {
          "x": 1281,
          "y": 2977
        }
      ],
      "category": "paragraph",
      "html": "<p id='98' style='font-size:20px'>Training schedule. Our ablations thus far are based on<br>800-epoch pre-training. Figure 7 shows the influence of the<br>training schedule length. The accuracy improves steadily<br>with longer training. Indeed, we have not observed sat-<br>uration of linear probing accuracy even at 1600 epochs.<br>This behavior is unlike contrastive learning methods, e.g.,<br>MoCo v3 [9] saturates at 300 epochs for ViT-L. Note that<br>the MAE encoder only sees 25% of patches per epoch,<br>while in contrastive learning the encoder sees 200% (two-<br>crop) or even more (multi-crop) patches per epoch.</p>",
      "id": 98,
      "page": 6,
      "text": "Training schedule. Our ablations thus far are based on\n800-epoch pre-training. Figure 7 shows the influence of the\ntraining schedule length. The accuracy improves steadily\nwith longer training. Indeed, we have not observed sat-\nuration of linear probing accuracy even at 1600 epochs.\nThis behavior is unlike contrastive learning methods, e.g.,\nMoCo v3 [9] saturates at 300 epochs for ViT-L. Note that\nthe MAE encoder only sees 25% of patches per epoch,\nwhile in contrastive learning the encoder sees 200% (two-\ncrop) or even more (multi-crop) patches per epoch."
    },
    {
      "bounding_box": [
        {
          "x": 1225,
          "y": 3057
        },
        {
          "x": 1253,
          "y": 3057
        },
        {
          "x": 1253,
          "y": 3091
        },
        {
          "x": 1225,
          "y": 3091
        }
      ],
      "category": "footer",
      "html": "<footer id='99' style='font-size:18px'>6</footer>",
      "id": 99,
      "page": 6,
      "text": "6"
    },
    {
      "bounding_box": [
        {
          "x": 206,
          "y": 274
        },
        {
          "x": 1196,
          "y": 274
        },
        {
          "x": 1196,
          "y": 545
        },
        {
          "x": 206,
          "y": 545
        }
      ],
      "category": "table",
      "html": "<table id='100' style='font-size:14px'><tr><td>method</td><td>pre-train data</td><td>ViT-B</td><td>ViT-L</td><td>ViT-H</td><td>ViT-H448</td></tr><tr><td>scratch, our impl.</td><td></td><td>82.3</td><td>82.6</td><td>83.1</td><td></td></tr><tr><td>DINO [5]</td><td>IN1K</td><td>82.8</td><td>-</td><td>-</td><td>-</td></tr><tr><td>MoCo v3 [9]</td><td>INIK</td><td>83.2</td><td>84.1</td><td>-</td><td>-</td></tr><tr><td>BEiT [2]</td><td>IN1K+DALLE</td><td>83.2</td><td>85.2</td><td>-</td><td>-</td></tr><tr><td>MAE</td><td>INIK</td><td>83.6</td><td>85.9</td><td>86.9</td><td>87.8</td></tr></table>",
      "id": 100,
      "page": 7,
      "text": "method pre-train data ViT-B ViT-L ViT-H ViT-H448\n scratch, our impl.  82.3 82.6 83.1 \n DINO [5] IN1K 82.8 - - -\n MoCo v3 [9] INIK 83.2 84.1 - -\n BEiT [2] IN1K+DALLE 83.2 85.2 - -\n MAE INIK 83.6 85.9 86.9"
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 566
        },
        {
          "x": 1199,
          "y": 566
        },
        {
          "x": 1199,
          "y": 927
        },
        {
          "x": 203,
          "y": 927
        }
      ],
      "category": "paragraph",
      "html": "<p id='101' style='font-size:16px'>Table 3. Comparisons with previous results on ImageNet-<br>1K. The pre-training data is the ImageNet-1K training set (ex-<br>cept the tokenizer in BEiT was pre-trained on 250M DALLE data<br>[50]). All self-supervised methods are evaluated by end-to-end<br>fine-tuning. The ViT models are B/16, L/16, H/14 [16]. The best<br>for each column is underlined. All results are on an image size of<br>224, except for ViT-H with an extra result on 448. Here our MAE<br>reconstructs normalized pixels and is pre-trained for 1600 epochs.</p>",
      "id": 101,
      "page": 7,
      "text": "Table 3. Comparisons with previous results on ImageNet-\n1K. The pre-training data is the ImageNet-1K training set (ex-\ncept the tokenizer in BEiT was pre-trained on 250M DALLE data\n[50]). All self-supervised methods are evaluated by end-to-end\nfine-tuning. The ViT models are B/16, L/16, H/14 [16]. The best\nfor each column is underlined. All results are on an image size of\n224, except for ViT-H with an extra result on 448. Here our MAE\nreconstructs normalized pixels and is pre-trained for 1600 epochs."
    },
    {
      "bounding_box": [
        {
          "x": 220,
          "y": 946
        },
        {
          "x": 1182,
          "y": 946
        },
        {
          "x": 1182,
          "y": 1461
        },
        {
          "x": 220,
          "y": 1461
        }
      ],
      "category": "figure",
      "html": "<figure><img id='102' style='font-size:14px' alt=\"88\nViT-H/14\n86\nViT-L/16\n84\nViT-B/16\n82\n80\nMAE, INIK\nsupervised, INIK, our impl.\n78 supervised, INIK [16]\nsupervised, JFT300M [16]\n76\n0 200 400 600\nparams (M)\" data-coord=\"top-left:(220,946); bottom-right:(1182,1461)\" /></figure>",
      "id": 102,
      "page": 7,
      "text": "88\nViT-H/14\n86\nViT-L/16\n84\nViT-B/16\n82\n80\nMAE, INIK\nsupervised, INIK, our impl.\n78 supervised, INIK [16]\nsupervised, JFT300M [16]\n76\n0 200 400 600\nparams (M)"
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 1466
        },
        {
          "x": 1196,
          "y": 1466
        },
        {
          "x": 1196,
          "y": 1607
        },
        {
          "x": 203,
          "y": 1607
        }
      ],
      "category": "caption",
      "html": "<br><caption id='103' style='font-size:18px'>Figure 8. MAE pre-training vs. supervised pre-training, evalu-<br>ated by fine-tuning in ImageNet-1K (224 size). We compare with<br>the original ViT results [16] trained in IN1K or JFT300M.</caption>",
      "id": 103,
      "page": 7,
      "text": "Figure 8. MAE pre-training vs. supervised pre-training, evalu-\nated by fine-tuning in ImageNet-1K (224 size). We compare with\nthe original ViT results [16] trained in IN1K or JFT300M."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 1675
        },
        {
          "x": 982,
          "y": 1675
        },
        {
          "x": 982,
          "y": 1723
        },
        {
          "x": 202,
          "y": 1723
        }
      ],
      "category": "paragraph",
      "html": "<p id='104' style='font-size:22px'>4.2. Comparisons with Previous Results</p>",
      "id": 104,
      "page": 7,
      "text": "4.2. Comparisons with Previous Results"
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 1767
        },
        {
          "x": 1197,
          "y": 1767
        },
        {
          "x": 1197,
          "y": 2011
        },
        {
          "x": 203,
          "y": 2011
        }
      ],
      "category": "paragraph",
      "html": "<p id='105' style='font-size:20px'>Comparisons with self-supervised methods. In Table 3<br>we compare the fine-tuning results of self-supervised ViT<br>models. For ViT-B, all methods perform closely. For ViT-L,<br>the gaps among methods are bigger, suggesting that a chal-<br>lenge for bigger models is to reduce overfitting.</p>",
      "id": 105,
      "page": 7,
      "text": "Comparisons with self-supervised methods. In Table 3\nwe compare the fine-tuning results of self-supervised ViT\nmodels. For ViT-B, all methods perform closely. For ViT-L,\nthe gaps among methods are bigger, suggesting that a chal-\nlenge for bigger models is to reduce overfitting."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 2019
        },
        {
          "x": 1198,
          "y": 2019
        },
        {
          "x": 1198,
          "y": 2510
        },
        {
          "x": 202,
          "y": 2510
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='106' style='font-size:20px'>Our MAE can scale up easily and has shown steady im-<br>provement from bigger models. We obtain 86.9% accuracy<br>using ViT-H (224 size). By fine-tuning with a 448 size, we<br>achieve 87.8% accuracy, using only INIK data. The pre-<br>vious best accuracy, among all methods using only IN1K<br>data, is 87.1% (512 size) [67], based on advanced networks.<br>We improve over the state-of-the-art by a nontrivial margin<br>in the highly competitive benchmark of IN1K (no external<br>data). Our result is based on vanilla ViT, and we expect<br>advanced networks will perform better.</p>",
      "id": 106,
      "page": 7,
      "text": "Our MAE can scale up easily and has shown steady im-\nprovement from bigger models. We obtain 86.9% accuracy\nusing ViT-H (224 size). By fine-tuning with a 448 size, we\nachieve 87.8% accuracy, using only INIK data. The pre-\nvious best accuracy, among all methods using only IN1K\ndata, is 87.1% (512 size) [67], based on advanced networks.\nWe improve over the state-of-the-art by a nontrivial margin\nin the highly competitive benchmark of IN1K (no external\ndata). Our result is based on vanilla ViT, and we expect\nadvanced networks will perform better."
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 2516
        },
        {
          "x": 1198,
          "y": 2516
        },
        {
          "x": 1198,
          "y": 2861
        },
        {
          "x": 203,
          "y": 2861
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='107' style='font-size:18px'>Comparing with BEiT [2], our MAE is more accurate<br>while being simpler and faster. Our method reconstructs<br>pixels, in contrast to BEiT that predicts tokens: BEiT re-<br>ported a 1.8% degradation [2] when reconstructing pixels<br>with ViT-B.2 We do not need dVAE pre-training. More-<br>over, our MAE is considerably faster (3.5x per epoch) than<br>BEiT, for the reason as studied in Table 1c.</p>",
      "id": 107,
      "page": 7,
      "text": "Comparing with BEiT [2], our MAE is more accurate\nwhile being simpler and faster. Our method reconstructs\npixels, in contrast to BEiT that predicts tokens: BEiT re-\nported a 1.8% degradation [2] when reconstructing pixels\nwith ViT-B.2 We do not need dVAE pre-training. More-\nover, our MAE is considerably faster (3.5x per epoch) than\nBEiT, for the reason as studied in Table 1c."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 2891
        },
        {
          "x": 1197,
          "y": 2891
        },
        {
          "x": 1197,
          "y": 2971
        },
        {
          "x": 202,
          "y": 2971
        }
      ],
      "category": "paragraph",
      "html": "<p id='108' style='font-size:16px'>2We observed the degradation also in BEiT with ViT-L: it produces<br>85.2% (tokens) and 83.5% (pixels), reproduced from the official code.</p>",
      "id": 108,
      "page": 7,
      "text": "2We observed the degradation also in BEiT with ViT-L: it produces\n85.2% (tokens) and 83.5% (pixels), reproduced from the official code."
    },
    {
      "bounding_box": [
        {
          "x": 1333,
          "y": 268
        },
        {
          "x": 2230,
          "y": 268
        },
        {
          "x": 2230,
          "y": 719
        },
        {
          "x": 1333,
          "y": 719
        }
      ],
      "category": "figure",
      "html": "<br><figure><img id='109' style='font-size:14px' alt=\"85 84.2 84.4 84.6 84.7 84.9\n83.1\n84.1\n83.8\n83.2\n81⌀\n81.6 81.9\n80 80.8\n79.9\n7.6\n75\n73.5 MAE baseline\nMoCo v3\n70\n0 1 2 4 6 12 18 24\n# blocks fine-tuned\" data-coord=\"top-left:(1333,268); bottom-right:(2230,719)\" /></figure>",
      "id": 109,
      "page": 7,
      "text": "85 84.2 84.4 84.6 84.7 84.9\n83.1\n84.1\n83.8\n83.2\n81⌀\n81.6 81.9\n80 80.8\n79.9\n7.6\n75\n73.5 MAE baseline\nMoCo v3\n70\n0 1 2 4 6 12 18 24\n# blocks fine-tuned"
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 734
        },
        {
          "x": 2275,
          "y": 734
        },
        {
          "x": 2275,
          "y": 962
        },
        {
          "x": 1279,
          "y": 962
        }
      ],
      "category": "caption",
      "html": "<br><caption id='110' style='font-size:16px'>Figure 9. Partial fine-tuning results of ViT-L w.r.t. the number<br>of fine-tuned Transformer blocks under the default settings from<br>Table 1. Tuning 0 blocks is linear probing; 24 is full fine-tuning.<br>Our MAE representations are less linearly separable, but are con-<br>sistently better than MoCo v3 if one or more blocks are tuned.</caption>",
      "id": 110,
      "page": 7,
      "text": "Figure 9. Partial fine-tuning results of ViT-L w.r.t. the number\nof fine-tuned Transformer blocks under the default settings from\nTable 1. Tuning 0 blocks is linear probing; 24 is full fine-tuning.\nOur MAE representations are less linearly separable, but are con-\nsistently better than MoCo v3 if one or more blocks are tuned."
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 1012
        },
        {
          "x": 2277,
          "y": 1012
        },
        {
          "x": 2277,
          "y": 1308
        },
        {
          "x": 1279,
          "y": 1308
        }
      ],
      "category": "paragraph",
      "html": "<p id='111' style='font-size:18px'>The MAE models in Table 3 are pre-trained for 1600<br>epochs for better accuracy (Figure 7). Even so, our total<br>pre-training time is less than the other methods when trained<br>on the same hardware. For example, training ViT-L on 128<br>TPU-v3 cores, our MAE's training time is 31 hours for 1600<br>epochs and MoCo v3's is 36 hours for 300 epochs [9].</p>",
      "id": 111,
      "page": 7,
      "text": "The MAE models in Table 3 are pre-trained for 1600\nepochs for better accuracy (Figure 7). Even so, our total\npre-training time is less than the other methods when trained\non the same hardware. For example, training ViT-L on 128\nTPU-v3 cores, our MAE's training time is 31 hours for 1600\nepochs and MoCo v3's is 36 hours for 300 epochs [9]."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 1326
        },
        {
          "x": 2274,
          "y": 1326
        },
        {
          "x": 2274,
          "y": 1519
        },
        {
          "x": 1280,
          "y": 1519
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='112' style='font-size:20px'>Comparisons with supervised pre-training. In the origi-<br>nal ViT paper [16], ViT-L degrades when trained in IN1K.<br>Our implementation of supervised training (see A.2) works<br>better, but accuracy saturates. See Figure 8.</p>",
      "id": 112,
      "page": 7,
      "text": "Comparisons with supervised pre-training. In the origi-\nnal ViT paper [16], ViT-L degrades when trained in IN1K.\nOur implementation of supervised training (see A.2) works\nbetter, but accuracy saturates. See Figure 8."
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 1528
        },
        {
          "x": 2277,
          "y": 1528
        },
        {
          "x": 2277,
          "y": 1771
        },
        {
          "x": 1281,
          "y": 1771
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='113' style='font-size:18px'>Our MAE pre-training, using only IN1K, can general-<br>ize better: the gain over training from scratch is bigger for<br>higher-capacity models. It follows a trend similar to the<br>JFT-300M supervised pre-training in [16]. This compari-<br>son shows that our MAE can help scale up model sizes.</p>",
      "id": 113,
      "page": 7,
      "text": "Our MAE pre-training, using only IN1K, can general-\nize better: the gain over training from scratch is bigger for\nhigher-capacity models. It follows a trend similar to the\nJFT-300M supervised pre-training in [16]. This compari-\nson shows that our MAE can help scale up model sizes."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 1807
        },
        {
          "x": 1745,
          "y": 1807
        },
        {
          "x": 1745,
          "y": 1853
        },
        {
          "x": 1280,
          "y": 1853
        }
      ],
      "category": "paragraph",
      "html": "<p id='114' style='font-size:22px'>4.3. Partial Fine-tuning</p>",
      "id": 114,
      "page": 7,
      "text": "4.3. Partial Fine-tuning"
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 1880
        },
        {
          "x": 2277,
          "y": 1880
        },
        {
          "x": 2277,
          "y": 2277
        },
        {
          "x": 1280,
          "y": 2277
        }
      ],
      "category": "paragraph",
      "html": "<p id='115' style='font-size:20px'>Table 1 shows that linear probing and fine-tuning results<br>are largely uncorrelated. Linear probing has been a popular<br>protocol in the past few years; however, it misses the oppor-<br>tunity of pursuing strong but non-linear features-which is<br>indeed a strength of deep learning. As a middle ground, we<br>study a partial fine-tuning protocol: fine-tune the last sev-<br>eral layers while freezing the others. This protocol was also<br>used in early works, e.g., [65, 70, 42].</p>",
      "id": 115,
      "page": 7,
      "text": "Table 1 shows that linear probing and fine-tuning results\nare largely uncorrelated. Linear probing has been a popular\nprotocol in the past few years; however, it misses the oppor-\ntunity of pursuing strong but non-linear features-which is\nindeed a strength of deep learning. As a middle ground, we\nstudy a partial fine-tuning protocol: fine-tune the last sev-\neral layers while freezing the others. This protocol was also\nused in early works, e.g., [65, 70, 42]."
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 2282
        },
        {
          "x": 2276,
          "y": 2282
        },
        {
          "x": 2276,
          "y": 2625
        },
        {
          "x": 1281,
          "y": 2625
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='116' style='font-size:20px'>Figure 9 shows the results. Notably, fine-tuning only one<br>Transformer block boosts the accuracy significantly from<br>73.5% to 81.0%. Moreover, if we fine-tune only \"half\" of<br>the last block (i.e., its MLP sub-block), we can get 79.1%,<br>much better than linear probing. This variant is essentially<br>fine-tuning an MLP head. Fine-tuning a few blocks (e.g., 4<br>or 6) can achieve accuracy close to full fine-tuning.</p>",
      "id": 116,
      "page": 7,
      "text": "Figure 9 shows the results. Notably, fine-tuning only one\nTransformer block boosts the accuracy significantly from\n73.5% to 81.0%. Moreover, if we fine-tune only \"half\" of\nthe last block (i.e., its MLP sub-block), we can get 79.1%,\nmuch better than linear probing. This variant is essentially\nfine-tuning an MLP head. Fine-tuning a few blocks (e.g., 4\nor 6) can achieve accuracy close to full fine-tuning."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 2630
        },
        {
          "x": 2277,
          "y": 2630
        },
        {
          "x": 2277,
          "y": 2975
        },
        {
          "x": 1280,
          "y": 2975
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='117' style='font-size:18px'>In Figure 9 we also compare with MoCo v3 [9], a con-<br>trastive method with ViT-L results available. MoCo v3 has<br>higher linear probing accuracy; however, all of its partial<br>fine-tuning results are worse than MAE. The gap is 2.6%<br>when tuning 4 blocks. While the MAE representations are<br>less linearly separable, they are stronger non-linear features<br>and perform well when a non-linear head is tuned.</p>",
      "id": 117,
      "page": 7,
      "text": "In Figure 9 we also compare with MoCo v3 [9], a con-\ntrastive method with ViT-L results available. MoCo v3 has\nhigher linear probing accuracy; however, all of its partial\nfine-tuning results are worse than MAE. The gap is 2.6%\nwhen tuning 4 blocks. While the MAE representations are\nless linearly separable, they are stronger non-linear features\nand perform well when a non-linear head is tuned."
    },
    {
      "bounding_box": [
        {
          "x": 1226,
          "y": 3056
        },
        {
          "x": 1250,
          "y": 3056
        },
        {
          "x": 1250,
          "y": 3090
        },
        {
          "x": 1226,
          "y": 3090
        }
      ],
      "category": "footer",
      "html": "<footer id='118' style='font-size:16px'>7</footer>",
      "id": 118,
      "page": 7,
      "text": "7"
    },
    {
      "bounding_box": [
        {
          "x": 243,
          "y": 229
        },
        {
          "x": 1151,
          "y": 229
        },
        {
          "x": 1151,
          "y": 491
        },
        {
          "x": 243,
          "y": 491
        }
      ],
      "category": "table",
      "html": "<table id='119' style='font-size:14px'><tr><td></td><td></td><td colspan=\"2\">Apbox</td><td colspan=\"2\">APMASK</td></tr><tr><td>method</td><td>pre-train data</td><td>ViT-B</td><td>ViT-L</td><td>ViT-B</td><td>ViT-L</td></tr><tr><td>supervised</td><td>IN1K w/ labels</td><td>47.9</td><td>49.3</td><td>42.9</td><td>43.9</td></tr><tr><td>MoCo v3</td><td>IN1K</td><td>47.9</td><td>49.3</td><td>42.7</td><td>44.0</td></tr><tr><td>BEiT</td><td>IN1K+DALLE</td><td>49.8</td><td>53.3</td><td>44.4</td><td>47.1</td></tr><tr><td>MAE</td><td>INIK</td><td>50.3</td><td>53.3</td><td>44.9</td><td>47.2</td></tr></table>",
      "id": 119,
      "page": 8,
      "text": "Apbox APMASK\n method pre-train data ViT-B ViT-L ViT-B ViT-L\n supervised IN1K w/ labels 47.9 49.3 42.9 43.9\n MoCo v3 IN1K 47.9 49.3 42.7 44.0\n BEiT IN1K+DALLE 49.8 53.3 44.4 47.1\n MAE INIK 50.3 53.3 44.9"
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 510
        },
        {
          "x": 1198,
          "y": 510
        },
        {
          "x": 1198,
          "y": 690
        },
        {
          "x": 202,
          "y": 690
        }
      ],
      "category": "caption",
      "html": "<br><caption id='120' style='font-size:14px'>Table 4. COCO object detection and segmentation using a ViT<br>Mask R-CNN baseline. All entries are based on our implementa-<br>tion. Self-supervised entries use IN1K data without labels. Mask<br>AP follows a similar trend as box AP.</caption>",
      "id": 120,
      "page": 8,
      "text": "Table 4. COCO object detection and segmentation using a ViT\nMask R-CNN baseline. All entries are based on our implementa-\ntion. Self-supervised entries use IN1K data without labels. Mask\nAP follows a similar trend as box AP."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 747
        },
        {
          "x": 1198,
          "y": 747
        },
        {
          "x": 1198,
          "y": 1045
        },
        {
          "x": 202,
          "y": 1045
        }
      ],
      "category": "paragraph",
      "html": "<p id='121' style='font-size:18px'>These observations suggest that linear separability is not<br>the sole metric for evaluating representation quality. It has<br>also been observed (e.g., [8]) that linear probing is not well<br>correlated with transfer learning performance, e.g., for ob-<br>ject detection. To our knowledge, linear evaluation is not<br>often used in NLP for benchmarking pre-training.</p>",
      "id": 121,
      "page": 8,
      "text": "These observations suggest that linear separability is not\nthe sole metric for evaluating representation quality. It has\nalso been observed (e.g., [8]) that linear probing is not well\ncorrelated with transfer learning performance, e.g., for ob-\nject detection. To our knowledge, linear evaluation is not\noften used in NLP for benchmarking pre-training."
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 1091
        },
        {
          "x": 939,
          "y": 1091
        },
        {
          "x": 939,
          "y": 1144
        },
        {
          "x": 203,
          "y": 1144
        }
      ],
      "category": "paragraph",
      "html": "<p id='122' style='font-size:22px'>5. Transfer Learning Experiments</p>",
      "id": 122,
      "page": 8,
      "text": "5. Transfer Learning Experiments"
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 1175
        },
        {
          "x": 1198,
          "y": 1175
        },
        {
          "x": 1198,
          "y": 1269
        },
        {
          "x": 204,
          "y": 1269
        }
      ],
      "category": "paragraph",
      "html": "<p id='123' style='font-size:18px'>We evaluate transfer learning in downstream tasks using<br>the pre-trained models in Table 3.</p>",
      "id": 123,
      "page": 8,
      "text": "We evaluate transfer learning in downstream tasks using\nthe pre-trained models in Table 3."
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 1291
        },
        {
          "x": 1198,
          "y": 1291
        },
        {
          "x": 1198,
          "y": 1533
        },
        {
          "x": 203,
          "y": 1533
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='124' style='font-size:18px'>Object detection and segmentation. We fine-tune Mask<br>R-CNN [24] end-to-end on COCO [37]. The ViT backbone<br>is adapted for use with FPN [36] (see A.3). We apply this<br>approach for all entries in Table 4. We report box AP for<br>object detection and mask AP for instance segmentation.</p>",
      "id": 124,
      "page": 8,
      "text": "Object detection and segmentation. We fine-tune Mask\nR-CNN [24] end-to-end on COCO [37]. The ViT backbone\nis adapted for use with FPN [36] (see A.3). We apply this\napproach for all entries in Table 4. We report box AP for\nobject detection and mask AP for instance segmentation."
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 1541
        },
        {
          "x": 1199,
          "y": 1541
        },
        {
          "x": 1199,
          "y": 1832
        },
        {
          "x": 203,
          "y": 1832
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='125' style='font-size:20px'>Compared to supervised pre-training, our MAE performs<br>better under all configurations (Table 4). With the smaller<br>ViT-B, our MAE is 2.4 points higher than supervised pre-<br>training (50.3 vs. 47.9, Apbox). More significantly, with the<br>larger ViT-L, our MAE pre-training outperforms supervised<br>pre-training by 4.0 points (53.3 vs. 49.3).</p>",
      "id": 125,
      "page": 8,
      "text": "Compared to supervised pre-training, our MAE performs\nbetter under all configurations (Table 4). With the smaller\nViT-B, our MAE is 2.4 points higher than supervised pre-\ntraining (50.3 vs. 47.9, Apbox). More significantly, with the\nlarger ViT-L, our MAE pre-training outperforms supervised\npre-training by 4.0 points (53.3 vs. 49.3)."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 1840
        },
        {
          "x": 1198,
          "y": 1840
        },
        {
          "x": 1198,
          "y": 2032
        },
        {
          "x": 202,
          "y": 2032
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='126' style='font-size:16px'>The pixel-based MAE is better than or on par with the<br>token-based BEiT, while MAE is much simpler and faster.<br>Both MAE and BEiT are better than MoCo v3 and MoCo<br>v3 is on par with supervised pre-training.</p>",
      "id": 126,
      "page": 8,
      "text": "The pixel-based MAE is better than or on par with the\ntoken-based BEiT, while MAE is much simpler and faster.\nBoth MAE and BEiT are better than MoCo v3 and MoCo\nv3 is on par with supervised pre-training."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 2053
        },
        {
          "x": 1198,
          "y": 2053
        },
        {
          "x": 1198,
          "y": 2345
        },
        {
          "x": 202,
          "y": 2345
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='127' style='font-size:18px'>Semantic segmentation. We experiment on ADE20K [72]<br>using UperNet [63] (see A.4). Table 5 shows that our pre-<br>training significantly improves results over supervised pre-<br>training, e.g., by 3.7 points for ViT-L. Our pixel-based MAE<br>also outperforms the token-based BEiT. These observations<br>are consistent with those in COCO.</p>",
      "id": 127,
      "page": 8,
      "text": "Semantic segmentation. We experiment on ADE20K [72]\nusing UperNet [63] (see A.4). Table 5 shows that our pre-\ntraining significantly improves results over supervised pre-\ntraining, e.g., by 3.7 points for ViT-L. Our pixel-based MAE\nalso outperforms the token-based BEiT. These observations\nare consistent with those in COCO."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 2365
        },
        {
          "x": 1198,
          "y": 2365
        },
        {
          "x": 1198,
          "y": 2711
        },
        {
          "x": 202,
          "y": 2711
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='128' style='font-size:20px'>Classification tasks. Table 6 studies transfer learning on<br>the iNaturalists [56] and Places [71] tasks (see A.5). On<br>iNat, our method shows strong scaling behavior: accuracy<br>improves considerably with bigger models. Our results sur-<br>pass the previous best results by large margins. On Places,<br>our MAE outperforms the previous best results [19, 40],<br>which were obtained via pre-training on billions of images.</p>",
      "id": 128,
      "page": 8,
      "text": "Classification tasks. Table 6 studies transfer learning on\nthe iNaturalists [56] and Places [71] tasks (see A.5). On\niNat, our method shows strong scaling behavior: accuracy\nimproves considerably with bigger models. Our results sur-\npass the previous best results by large margins. On Places,\nour MAE outperforms the previous best results [19, 40],\nwhich were obtained via pre-training on billions of images."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 2728
        },
        {
          "x": 1198,
          "y": 2728
        },
        {
          "x": 1198,
          "y": 2975
        },
        {
          "x": 202,
          "y": 2975
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='129' style='font-size:18px'>Pixels vs. tokens. Table 7 compares pixels vs. tokens as the<br>MAE reconstruction target. While using dVAE tokens is<br>better than using unnormalized pixels, itis statistically sim-<br>ilar to using normalized pixels across all cases we tested. It<br>again shows that tokenization is not necessary for our MAE.</p>",
      "id": 129,
      "page": 8,
      "text": "Pixels vs. tokens. Table 7 compares pixels vs. tokens as the\nMAE reconstruction target. While using dVAE tokens is\nbetter than using unnormalized pixels, itis statistically sim-\nilar to using normalized pixels across all cases we tested. It\nagain shows that tokenization is not necessary for our MAE."
    },
    {
      "bounding_box": [
        {
          "x": 1396,
          "y": 274
        },
        {
          "x": 2152,
          "y": 274
        },
        {
          "x": 2152,
          "y": 489
        },
        {
          "x": 1396,
          "y": 489
        }
      ],
      "category": "table",
      "html": "<br><table id='130' style='font-size:14px'><tr><td>method</td><td>pre-train data</td><td>ViT-B</td><td>ViT-L</td></tr><tr><td>supervised</td><td>IN1K w/ labels</td><td>47.4</td><td>49.9</td></tr><tr><td>MoCo v3</td><td>IN1K</td><td>47.3</td><td>49.1</td></tr><tr><td>BEiT</td><td>IN1K+DALLE</td><td>47.1</td><td>53.3</td></tr><tr><td>MAE</td><td>INIK</td><td>48.1</td><td>53.6</td></tr></table>",
      "id": 130,
      "page": 8,
      "text": "method pre-train data ViT-B ViT-L\n supervised IN1K w/ labels 47.4 49.9\n MoCo v3 IN1K 47.3 49.1\n BEiT IN1K+DALLE 47.1 53.3\n MAE INIK 48.1"
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 509
        },
        {
          "x": 2277,
          "y": 509
        },
        {
          "x": 2277,
          "y": 688
        },
        {
          "x": 1280,
          "y": 688
        }
      ],
      "category": "caption",
      "html": "<br><caption id='131' style='font-size:16px'>Table 5. ADE20K semantic segmentation (mIoU) using Uper-<br>Net. BEiT results are reproduced using the official code. Other<br>entries are based on our implementation. Self-supervised entries<br>use IN1K data without labels.</caption>",
      "id": 131,
      "page": 8,
      "text": "Table 5. ADE20K semantic segmentation (mIoU) using Uper-\nNet. BEiT results are reproduced using the official code. Other\nentries are based on our implementation. Self-supervised entries\nuse IN1K data without labels."
    },
    {
      "bounding_box": [
        {
          "x": 1289,
          "y": 725
        },
        {
          "x": 2253,
          "y": 725
        },
        {
          "x": 2253,
          "y": 979
        },
        {
          "x": 1289,
          "y": 979
        }
      ],
      "category": "table",
      "html": "<table id='132' style='font-size:14px'><tr><td>dataset</td><td>ViT-B</td><td>ViT-L</td><td>ViT-H</td><td>ViT-H448</td><td>prev best</td></tr><tr><td>iNat 2017</td><td>70.5</td><td>75.7</td><td>79.3</td><td>83.4</td><td>75.4 [55]</td></tr><tr><td>iNat 2018</td><td>75.4</td><td>80.1</td><td>83.0</td><td>86.8</td><td>81.2 [54]</td></tr><tr><td>iNat 2019</td><td>80.5</td><td>83.4</td><td>85.7</td><td>88.3</td><td>84.1 [54]</td></tr><tr><td>Places205</td><td>63.9</td><td>65.8</td><td>65.9</td><td>66.8</td><td>66.0 [19]†</td></tr><tr><td>Places365</td><td>57.9</td><td>59.4</td><td>59.8</td><td>60.3</td><td>58.0 [40]‡</td></tr></table>",
      "id": 132,
      "page": 8,
      "text": "dataset ViT-B ViT-L ViT-H ViT-H448 prev best\n iNat 2017 70.5 75.7 79.3 83.4 75.4 [55]\n iNat 2018 75.4 80.1 83.0 86.8 81.2 [54]\n iNat 2019 80.5 83.4 85.7 88.3 84.1 [54]\n Places205 63.9 65.8 65.9 66.8 66.0 [19]†\n Places365 57.9 59.4 59.8 60.3"
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 997
        },
        {
          "x": 2276,
          "y": 997
        },
        {
          "x": 2276,
          "y": 1128
        },
        {
          "x": 1281,
          "y": 1128
        }
      ],
      "category": "caption",
      "html": "<br><caption id='133' style='font-size:14px'>Table 6. Transfer learning accuracy on classification datasets,<br>using MAE pre-trained on IN1K and then fine-tuned. We provide<br>system-level comparisons with the previous best results.</caption>",
      "id": 133,
      "page": 8,
      "text": "Table 6. Transfer learning accuracy on classification datasets,\nusing MAE pre-trained on IN1K and then fine-tuned. We provide\nsystem-level comparisons with the previous best results."
    },
    {
      "bounding_box": [
        {
          "x": 1288,
          "y": 1136
        },
        {
          "x": 2260,
          "y": 1136
        },
        {
          "x": 2260,
          "y": 1474
        },
        {
          "x": 1288,
          "y": 1474
        }
      ],
      "category": "table",
      "html": "<br><table id='134' style='font-size:14px'><tr><td colspan=\"8\">1: pre-trained on 1 billion images. I : pre-trained on 3.5 billion images.</td></tr><tr><td></td><td colspan=\"3\">INIK</td><td colspan=\"2\">COCO</td><td colspan=\"2\">ADE20K</td></tr><tr><td></td><td>ViT-B</td><td>ViT-L</td><td>ViT-H</td><td>ViT-B</td><td>ViT-L</td><td>ViT-B</td><td>ViT-L</td></tr><tr><td>pixel (w/o norm)</td><td>83.3</td><td>85.1</td><td>86.2</td><td>49.5</td><td>52.8</td><td>48.0</td><td>51.8</td></tr><tr><td>pixel (w/ norm)</td><td>83.6</td><td>85.9</td><td>86.9</td><td>50.3</td><td>53.3</td><td>48.1</td><td>53.6</td></tr><tr><td>dVAE token</td><td>83.6</td><td>85.7</td><td>86.9</td><td>50.3</td><td>53.2</td><td>48.1</td><td>53.4</td></tr><tr><td>△</td><td>0.0</td><td>-0.2</td><td>0.0</td><td>0.0</td><td>-0.1</td><td>0.0</td><td>-0.2</td></tr></table>",
      "id": 134,
      "page": 8,
      "text": "1: pre-trained on 1 billion images. I : pre-trained on 3.5 billion images.\n  INIK COCO ADE20K\n  ViT-B ViT-L ViT-H ViT-B ViT-L ViT-B ViT-L\n pixel (w/o norm) 83.3 85.1 86.2 49.5 52.8 48.0 51.8\n pixel (w/ norm) 83.6 85.9 86.9 50.3 53.3 48.1 53.6\n dVAE token 83.6 85.7 86.9 50.3 53.2 48.1 53.4\n △ 0.0 -0.2 0.0 0.0 -0.1 0.0"
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 1488
        },
        {
          "x": 2277,
          "y": 1488
        },
        {
          "x": 2277,
          "y": 1622
        },
        {
          "x": 1281,
          "y": 1622
        }
      ],
      "category": "caption",
      "html": "<br><caption id='135' style='font-size:14px'>Table 7. Pixels vs. tokens as the MAE reconstruction target. △ is<br>the difference between using dVAE tokens and using normalized<br>pixels. The difference is statistically insignificant.</caption>",
      "id": 135,
      "page": 8,
      "text": "Table 7. Pixels vs. tokens as the MAE reconstruction target. △ is\nthe difference between using dVAE tokens and using normalized\npixels. The difference is statistically insignificant."
    },
    {
      "bounding_box": [
        {
          "x": 1282,
          "y": 1695
        },
        {
          "x": 1906,
          "y": 1695
        },
        {
          "x": 1906,
          "y": 1745
        },
        {
          "x": 1282,
          "y": 1745
        }
      ],
      "category": "paragraph",
      "html": "<p id='136' style='font-size:22px'>6. Discussion and Conclusion</p>",
      "id": 136,
      "page": 8,
      "text": "6. Discussion and Conclusion"
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 1779
        },
        {
          "x": 2275,
          "y": 1779
        },
        {
          "x": 2275,
          "y": 2324
        },
        {
          "x": 1278,
          "y": 2324
        }
      ],
      "category": "paragraph",
      "html": "<p id='137' style='font-size:20px'>Simple algorithms that scale well are the core of deep<br>learning. In NLP, simple self-supervised learning methods<br>(e.g., [47, 14, 48, 4]) enable benefits from exponentially<br>scaling models. In computer vision, practical pre-training<br>paradigms are dominantly supervised (e.g. [33, 51, 25, 16])<br>despite progress in self-supervised learning. In this study,<br>we observe on ImageNet and in transfer learning that<br>an autoencoder-a simple self-supervised method similar<br>to techniques in NLP-provides scalable benefits. Self-<br>supervised learning in vision may now be embarking on a<br>similar trajectory as in NLP.</p>",
      "id": 137,
      "page": 8,
      "text": "Simple algorithms that scale well are the core of deep\nlearning. In NLP, simple self-supervised learning methods\n(e.g., [47, 14, 48, 4]) enable benefits from exponentially\nscaling models. In computer vision, practical pre-training\nparadigms are dominantly supervised (e.g. [33, 51, 25, 16])\ndespite progress in self-supervised learning. In this study,\nwe observe on ImageNet and in transfer learning that\nan autoencoder-a simple self-supervised method similar\nto techniques in NLP-provides scalable benefits. Self-\nsupervised learning in vision may now be embarking on a\nsimilar trajectory as in NLP."
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 2333
        },
        {
          "x": 2276,
          "y": 2333
        },
        {
          "x": 2276,
          "y": 2976
        },
        {
          "x": 1278,
          "y": 2976
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='138' style='font-size:18px'>On the other hand, we note that images and languages<br>are signals of a different nature and this difference must<br>be addressed carefully. Images are merely recorded light<br>without a semantic decomposition into the visual analogue<br>of words. Instead of attempting to remove objects, we re-<br>move random patches that most likely do not form a seman-<br>tic segment. Likewise, our MAE reconstructs pixels, which<br>are not semantic entities. Nevertheless, we observe (e.g.,<br>Figure 4) that our MAE infers complex, holistic reconstruc-<br>tions, suggesting it has learned numerous visual concepts,<br>i.e., semantics. We hypothesize that this behavior occurs<br>by way of a rich hidden representation inside the MAE. We<br>hope this perspective will inspire future work.</p>",
      "id": 138,
      "page": 8,
      "text": "On the other hand, we note that images and languages\nare signals of a different nature and this difference must\nbe addressed carefully. Images are merely recorded light\nwithout a semantic decomposition into the visual analogue\nof words. Instead of attempting to remove objects, we re-\nmove random patches that most likely do not form a seman-\ntic segment. Likewise, our MAE reconstructs pixels, which\nare not semantic entities. Nevertheless, we observe (e.g.,\nFigure 4) that our MAE infers complex, holistic reconstruc-\ntions, suggesting it has learned numerous visual concepts,\ni.e., semantics. We hypothesize that this behavior occurs\nby way of a rich hidden representation inside the MAE. We\nhope this perspective will inspire future work."
    },
    {
      "bounding_box": [
        {
          "x": 1225,
          "y": 3057
        },
        {
          "x": 1251,
          "y": 3057
        },
        {
          "x": 1251,
          "y": 3091
        },
        {
          "x": 1225,
          "y": 3091
        }
      ],
      "category": "footer",
      "html": "<footer id='139' style='font-size:18px'>8</footer>",
      "id": 139,
      "page": 8,
      "text": "8"
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 306
        },
        {
          "x": 1200,
          "y": 306
        },
        {
          "x": 1200,
          "y": 605
        },
        {
          "x": 200,
          "y": 605
        }
      ],
      "category": "paragraph",
      "html": "<p id='140' style='font-size:20px'>Broader impacts. The proposed method predicts content<br>based on learned statistics of the training dataset and as such<br>will reflect biases in those data, including ones with nega-<br>tive societal impacts. The model may generate inexistent<br>content. These issues warrant further research and consid-<br>eration when building upon this work to generate images.</p>",
      "id": 140,
      "page": 9,
      "text": "Broader impacts. The proposed method predicts content\nbased on learned statistics of the training dataset and as such\nwill reflect biases in those data, including ones with nega-\ntive societal impacts. The model may generate inexistent\ncontent. These issues warrant further research and consid-\neration when building upon this work to generate images."
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 652
        },
        {
          "x": 445,
          "y": 652
        },
        {
          "x": 445,
          "y": 698
        },
        {
          "x": 204,
          "y": 698
        }
      ],
      "category": "paragraph",
      "html": "<p id='141' style='font-size:22px'>References</p>",
      "id": 141,
      "page": 9,
      "text": "References"
    },
    {
      "bounding_box": [
        {
          "x": 214,
          "y": 725
        },
        {
          "x": 1198,
          "y": 725
        },
        {
          "x": 1198,
          "y": 2976
        },
        {
          "x": 214,
          "y": 2976
        }
      ],
      "category": "paragraph",
      "html": "<p id='142' style='font-size:16px'>[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer<br>normalization. arXiv:1607.06450, 2016.<br>[2] Hangbo Bao, Li Dong, and Furu Wei. BEiT: BERT pre-training<br>of image transformers. arXiv:2106.08254, 2021. Accessed in June<br>2021.<br>[3] Suzanna Becker and Geoffrey E Hinton. Self-organizing neural<br>network that discovers surfaces in random-dot stereograms. Na-<br>ture, 1992.<br>[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,<br>Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav<br>Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel<br>Herbert- Voss, Gretchen Krueger, Tom Henighan, Rewon Child,<br>Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris<br>Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Ben-<br>jamin Chess, Jack Clark, Christopher Berner, Sam McCandlish,<br>Alec Radford, Ilya Sutskever, and Dario Amodei. Language mod-<br>els are few-shot learners. In NeurIPS, 2020.<br>[5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien<br>Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties<br>in self-supervised vision transformers. In ICCV, 2021.<br>[6] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun,<br>David Luan, and Ilya Sutskever. Generative pretraining from pix-<br>els. In ICML, 2020.<br>[7] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey<br>Hinton. A simple framework for contrastive learning of visual rep-<br>resentations. In ICML, 2020.<br>[8] Xinlei Chen and Kaiming He. Exploring simple Siamese represen-<br>tation learning. In CVPR, 2021.<br>[9] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of<br>training self-supervised Vision Transformers. In ICCV, 2021.<br>[10] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D<br>Manning. ELECTRA: Pre-training text encoders as discriminators<br>rather than generators. In ICLR, 2020.<br>[11] Corinna Cortes and Vladimir Vapnik. Support-vector networks.<br>Machine learning, 1995.<br>[12] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc v Le. Ran-<br>daugment: Practical automated data augmentation with a reduced<br>search space. In CVPR Workshops, 2020.<br>[13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li<br>Fei-Fei. ImageNet: A large-scale hierarchical image database. In<br>CVPR, 2009.<br>[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina<br>Toutanova. BERT: Pre-training of deep bidirectional transformers<br>for language understanding. In NAACL, 2019.<br>[15] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised<br>visual representation learning by context prediction. In ICCV,<br>2015.<br>[16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk<br>Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa De-<br>hghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob</p>",
      "id": 142,
      "page": 9,
      "text": "[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer\nnormalization. arXiv:1607.06450, 2016.\n[2] Hangbo Bao, Li Dong, and Furu Wei. BEiT: BERT pre-training\nof image transformers. arXiv:2106.08254, 2021. Accessed in June\n2021.\n[3] Suzanna Becker and Geoffrey E Hinton. Self-organizing neural\nnetwork that discovers surfaces in random-dot stereograms. Na-\nture, 1992.\n[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,\nJared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav\nShyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert- Voss, Gretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris\nHesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Ben-\njamin Chess, Jack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei. Language mod-\nels are few-shot learners. In NeurIPS, 2020.\n[5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien\nMairal, Piotr Bojanowski, and Armand Joulin. Emerging properties\nin self-supervised vision transformers. In ICCV, 2021.\n[6] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun,\nDavid Luan, and Ilya Sutskever. Generative pretraining from pix-\nels. In ICML, 2020.\n[7] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey\nHinton. A simple framework for contrastive learning of visual rep-\nresentations. In ICML, 2020.\n[8] Xinlei Chen and Kaiming He. Exploring simple Siamese represen-\ntation learning. In CVPR, 2021.\n[9] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of\ntraining self-supervised Vision Transformers. In ICCV, 2021.\n[10] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D\nManning. ELECTRA: Pre-training text encoders as discriminators\nrather than generators. In ICLR, 2020.\n[11] Corinna Cortes and Vladimir Vapnik. Support-vector networks.\nMachine learning, 1995.\n[12] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc v Le. Ran-\ndaugment: Practical automated data augmentation with a reduced\nsearch space. In CVPR Workshops, 2020.\n[13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li\nFei-Fei. ImageNet: A large-scale hierarchical image database. In\nCVPR, 2009.\n[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: Pre-training of deep bidirectional transformers\nfor language understanding. In NAACL, 2019.\n[15] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised\nvisual representation learning by context prediction. In ICCV,\n2015.\n[16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk\nWeissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa De-\nhghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob"
    },
    {
      "bounding_box": [
        {
          "x": 1360,
          "y": 311
        },
        {
          "x": 2276,
          "y": 311
        },
        {
          "x": 2276,
          "y": 391
        },
        {
          "x": 1360,
          "y": 391
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='143' style='font-size:14px'>Uszkoreit, and Neil Houlsby. An image is worth 16x16 words:<br>Transformers for image recognition at scale. In ICLR, 2021.</p>",
      "id": 143,
      "page": 9,
      "text": "Uszkoreit, and Neil Houlsby. An image is worth 16x16 words:\nTransformers for image recognition at scale. In ICLR, 2021."
    },
    {
      "bounding_box": [
        {
          "x": 1277,
          "y": 403
        },
        {
          "x": 2287,
          "y": 403
        },
        {
          "x": 2287,
          "y": 2974
        },
        {
          "x": 1277,
          "y": 2974
        }
      ],
      "category": "paragraph",
      "html": "<p id='144' style='font-size:16px'>[17] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsuper-<br>vised representation learning by predicting image rotations. In<br>ICLR, 2018.<br>[18] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of<br>training deep feedforward neural networks. In AISTATS, 2010.<br>[19] Priya Goyal, Mathilde Caron, Benjamin Lefaudeux, Min Xu,<br>Pengchao Wang, Vivek Pai, Mannat Singh, Vitaliy Liptchinsky, Is-<br>han Misra, Armand Joulin, and Piotr Bojanowski. Self-supervised<br>pretraining of visual features in the wild. arXiv:2103.01988, 2021.<br>[20] Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, Lukasz<br>Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and<br>Kaiming He. Accurate, large minibatch SGD: Training ImageNet<br>in 1 hour. arXiv:1706.02677, 2017.<br>[21] Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec,<br>Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo<br>Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, Bilal<br>Piot, Koray Kavukcuoglu, Remi Munos, and Michal Valko. Boot-<br>strap your own latent - a new approach to self-supervised learning.<br>In NeurIPS, 2020.<br>[22] Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality<br>reduction by learning an invariant mapping. In CVPR, 2006.<br>[23] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Gir-<br>shick. Momentum contrast for unsupervised visual representation<br>learning. In CVPR, 2020.<br>[24] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick.<br>Mask R-CNN. In ICCV, 2017.<br>[25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep<br>residual learning for image recognition. In CVPR, 2016.<br>[26] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath,<br>Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak<br>Parajuli, Mike Guo, et al. The many faces of robustness: A critical<br>analysis of out-of-distribution generalization. In ICCV, 2021.<br>[27] Dan Hendrycks and Thomas Dietterich. Benchmarking neural net-<br>work robustness to common corruptions and perturbations. In<br>ICLR, 2019.<br>[28] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and<br>Dawn Song. Natural adversarial examples. In CVPR, 2021.<br>[29] Geoffrey E Hinton and Richard S Zemel. Autoencoders, minimum<br>description length, and helmholtz free energy. In NeurIPS, 1994.<br>[30] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Wein-<br>berger. Deep networks with stochastic depth. In ECCV, 2016.<br>[31] Sergey Ioffe and Christian Szegedy. Batch normalization: Accel-<br>erating deep network training by reducing internal covariate shift.<br>In ICML, 2015.<br>[32] Insoo Kim, Seungju Han, Ji-won Baek, Seong-Jin Park, Jae-Joon<br>Han, and Jinwoo Shin. Quality-agnostic image recognition via in-<br>vertible decoder. In CVPR, 2021.<br>[33] Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton. Imagenet clas-<br>sification with deep convolutional neural networks. In NeurIPS,<br>2012.<br>[34] Yann LeCun, Bernhard Boser, John S Denker, Donnie Hender-<br>son, Richard E Howard, Wayne Hubbard, and Lawrence D Jackel.<br>Backpropagation applied to handwritten zip code recognition. Neu-<br>ral computation, 1989.<br>[35] Yanghao Li, Saining Xie, Xinlei Chen, Piotr Dollar, Kaiming He,<br>and Ross Girshick. Benchmarking detection transfer learning with<br>vision transformers. In preparation, 2021.</p>",
      "id": 144,
      "page": 9,
      "text": "[17] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsuper-\nvised representation learning by predicting image rotations. In\nICLR, 2018.\n[18] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of\ntraining deep feedforward neural networks. In AISTATS, 2010.\n[19] Priya Goyal, Mathilde Caron, Benjamin Lefaudeux, Min Xu,\nPengchao Wang, Vivek Pai, Mannat Singh, Vitaliy Liptchinsky, Is-\nhan Misra, Armand Joulin, and Piotr Bojanowski. Self-supervised\npretraining of visual features in the wild. arXiv:2103.01988, 2021.\n[20] Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, Lukasz\nWesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and\nKaiming He. Accurate, large minibatch SGD: Training ImageNet\nin 1 hour. arXiv:1706.02677, 2017.\n[21] Jean-Bastien Grill, Florian Strub, Florent Altche, Corentin Tallec,\nPierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo\nAvila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, Bilal\nPiot, Koray Kavukcuoglu, Remi Munos, and Michal Valko. Boot-\nstrap your own latent - a new approach to self-supervised learning.\nIn NeurIPS, 2020.\n[22] Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality\nreduction by learning an invariant mapping. In CVPR, 2006.\n[23] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Gir-\nshick. Momentum contrast for unsupervised visual representation\nlearning. In CVPR, 2020.\n[24] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick.\nMask R-CNN. In ICCV, 2017.\n[25] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep\nresidual learning for image recognition. In CVPR, 2016.\n[26] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath,\nFrank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak\nParajuli, Mike Guo, et al. The many faces of robustness: A critical\nanalysis of out-of-distribution generalization. In ICCV, 2021.\n[27] Dan Hendrycks and Thomas Dietterich. Benchmarking neural net-\nwork robustness to common corruptions and perturbations. In\nICLR, 2019.\n[28] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and\nDawn Song. Natural adversarial examples. In CVPR, 2021.\n[29] Geoffrey E Hinton and Richard S Zemel. Autoencoders, minimum\ndescription length, and helmholtz free energy. In NeurIPS, 1994.\n[30] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Wein-\nberger. Deep networks with stochastic depth. In ECCV, 2016.\n[31] Sergey Ioffe and Christian Szegedy. Batch normalization: Accel-\nerating deep network training by reducing internal covariate shift.\nIn ICML, 2015.\n[32] Insoo Kim, Seungju Han, Ji-won Baek, Seong-Jin Park, Jae-Joon\nHan, and Jinwoo Shin. Quality-agnostic image recognition via in-\nvertible decoder. In CVPR, 2021.\n[33] Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton. Imagenet clas-\nsification with deep convolutional neural networks. In NeurIPS,\n2012.\n[34] Yann LeCun, Bernhard Boser, John S Denker, Donnie Hender-\nson, Richard E Howard, Wayne Hubbard, and Lawrence D Jackel.\nBackpropagation applied to handwritten zip code recognition. Neu-\nral computation, 1989.\n[35] Yanghao Li, Saining Xie, Xinlei Chen, Piotr Dollar, Kaiming He,\nand Ross Girshick. Benchmarking detection transfer learning with\nvision transformers. In preparation, 2021."
    },
    {
      "bounding_box": [
        {
          "x": 1226,
          "y": 3056
        },
        {
          "x": 1252,
          "y": 3056
        },
        {
          "x": 1252,
          "y": 3088
        },
        {
          "x": 1226,
          "y": 3088
        }
      ],
      "category": "footer",
      "html": "<footer id='145' style='font-size:18px'>9</footer>",
      "id": 145,
      "page": 9,
      "text": "9"
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 294
        },
        {
          "x": 1200,
          "y": 294
        },
        {
          "x": 1200,
          "y": 2972
        },
        {
          "x": 203,
          "y": 2972
        }
      ],
      "category": "paragraph",
      "html": "<p id='146' style='font-size:14px'>[36] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, Bharath<br>Hariharan, and Serge Belongie. Feature pyramid networks for ob-<br>ject detection. In CVPR, 2017.<br>[37] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro<br>Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Mi-<br>crosoft COCO: Common objects in context. In ECCV, 2014.<br>[38] Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradient de-<br>scent with warm restarts. In ICLR, 2017.<br>[39] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regu-<br>larization. In ICLR, 2019.<br>[40] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming<br>He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens<br>van der Maaten. Exploring the limits of weakly supervised pre-<br>training. In ECCV, 2018.<br>[41] Xiaofeng Mao, Gege Qi, Yuefeng Chen, Xiaodan Li, Ranjie Duan,<br>Shaokai Ye, Yuan He, and Hui Xue. Towards robust vision trans-<br>former. arXiv:2105.07926, 2021.<br>[42] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual<br>representations by solving jigsaw puzzles. In ECCV, 2016.<br>[43] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representa-<br>tion learning with contrastive predictive coding. arXiv:1807.03748,<br>2018.<br>[44] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neu-<br>ral discrete representation learning. In NeurIPS, 2017.<br>[45] Deepak Pathak, Ross Girshick, Piotr Dollar, Trevor Darrell, and<br>Bharath Hariharan. Learning features by watching objects move.<br>In CVPR, 2017.<br>[46] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell,<br>and Alexei A Efros. Context encoders: Feature learning by inpaint-<br>ing. In CVPR, 2016.<br>[47] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya<br>Sutskever. Improving language understanding by generative pre-<br>training. 2018.<br>[48] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario<br>Amodei, and Ilya Sutskever. Language models are unsupervised<br>multitask learners. 2019.<br>[49] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan<br>Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu.<br>Exploring the limits of transfer learning with a unified text-to-text<br>transformer. JMLR, 2020.<br>[50] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea<br>Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot<br>text-to-image generation. In ICML, 2021.<br>[51] Karen Simonyan and Andrew Zisserman. Very deep convolutional<br>networks for large-scale image recognition. In ICLR, 2015.<br>[52] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon<br>Shlens, and Zbigniew Wojna. Rethinking the inception architec-<br>ture for computer vision. In CVPR, 2016.<br>[53] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa,<br>Alexandre Sablayrolles, and Herve Jegou. Training data-efficient<br>image transformers & distillation through attention. In ICML,<br>2021.<br>[54] Hugo Touvron, Alexandre Sablayrolles, Matthijs Douze, Matthieu<br>Cord, and Herve Jegou. Grafit: Learning fine-grained image repre-<br>sentations with coarse labels. In ICCV, 2021.<br>[55] Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve Jegou.<br>Fixing the train-test resolution discrepancy. arXiv:1906.06423,<br>2019.</p>",
      "id": 146,
      "page": 10,
      "text": "[36] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, Bharath\nHariharan, and Serge Belongie. Feature pyramid networks for ob-\nject detection. In CVPR, 2017.\n[37] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro\nPerona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Mi-\ncrosoft COCO: Common objects in context. In ECCV, 2014.\n[38] Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradient de-\nscent with warm restarts. In ICLR, 2017.\n[39] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regu-\nlarization. In ICLR, 2019.\n[40] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming\nHe, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens\nvan der Maaten. Exploring the limits of weakly supervised pre-\ntraining. In ECCV, 2018.\n[41] Xiaofeng Mao, Gege Qi, Yuefeng Chen, Xiaodan Li, Ranjie Duan,\nShaokai Ye, Yuan He, and Hui Xue. Towards robust vision trans-\nformer. arXiv:2105.07926, 2021.\n[42] Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual\nrepresentations by solving jigsaw puzzles. In ECCV, 2016.\n[43] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representa-\ntion learning with contrastive predictive coding. arXiv:1807.03748,\n2018.\n[44] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neu-\nral discrete representation learning. In NeurIPS, 2017.\n[45] Deepak Pathak, Ross Girshick, Piotr Dollar, Trevor Darrell, and\nBharath Hariharan. Learning features by watching objects move.\nIn CVPR, 2017.\n[46] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell,\nand Alexei A Efros. Context encoders: Feature learning by inpaint-\ning. In CVPR, 2016.\n[47] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya\nSutskever. Improving language understanding by generative pre-\ntraining. 2018.\n[48] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\nAmodei, and Ilya Sutskever. Language models are unsupervised\nmultitask learners. 2019.\n[49] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan\nNarang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu.\nExploring the limits of transfer learning with a unified text-to-text\ntransformer. JMLR, 2020.\n[50] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea\nVoss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot\ntext-to-image generation. In ICML, 2021.\n[51] Karen Simonyan and Andrew Zisserman. Very deep convolutional\nnetworks for large-scale image recognition. In ICLR, 2015.\n[52] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon\nShlens, and Zbigniew Wojna. Rethinking the inception architec-\nture for computer vision. In CVPR, 2016.\n[53] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa,\nAlexandre Sablayrolles, and Herve Jegou. Training data-efficient\nimage transformers & distillation through attention. In ICML,\n2021.\n[54] Hugo Touvron, Alexandre Sablayrolles, Matthijs Douze, Matthieu\nCord, and Herve Jegou. Grafit: Learning fine-grained image repre-\nsentations with coarse labels. In ICCV, 2021.\n[55] Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve Jegou.\nFixing the train-test resolution discrepancy. arXiv:1906.06423,\n2019."
    },
    {
      "bounding_box": [
        {
          "x": 1275,
          "y": 304
        },
        {
          "x": 2286,
          "y": 304
        },
        {
          "x": 2286,
          "y": 2556
        },
        {
          "x": 1275,
          "y": 2556
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='147' style='font-size:14px'>[56] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen<br>Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and Serge Be-<br>longie. The iNaturalist species classification and detection dataset.<br>In CVPR, 2018.<br>[57] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,<br>Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin.<br>Attention is all you need. In NeurIPS, 2017.<br>[58] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-<br>Antoine Manzagol. Extracting and composing robust features with<br>denoising autoencoders. In ICML, 2008.<br>[59] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio,<br>Pierre-Antoine Manzagol, and Leon Bottou. Stacked denoising au-<br>toencoders: Learning useful representations in a deep network with<br>a local denoising criterion. JMLR, 2010.<br>[60] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing.<br>Learning robust global representations by penalizing local predic-<br>tive power. In NeurIPS, 2019.<br>[61] Xiaolong Wang and Abhinav Gupta. Unsupervised learning of vi-<br>sual representations using videos. In ICCV, 2015.<br>[62] Zhirong Wu, Yuanjun Xiong, Stella Yu, and Dahua Lin. Unsuper-<br>vised feature learning via non-parametric instance discrimination.<br>In CVPR, 2018.<br>[63] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian<br>Sun. Unified perceptual parsing for scene understanding. In ECCV,<br>2018.<br>[64] Tete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr Dollar,<br>and Ross Girshick. Early convolutions help transformers see better.<br>In NeurIPS, 2021.<br>[65] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How<br>transferable are features in deep neural networks? In NeurIPS,<br>2014.<br>[66] Yang You, Igor Gitman, and Boris Ginsburg. Large batch training<br>of convolutional networks. arXiv:1708.03888, 2017.<br>[67] Li Yuan, Qibin Hou, Zihang Jiang, Jiashi Feng, and Shuicheng Yan.<br>VOLO: Vision outlooker for visual recognition. arXiv:2106.13112,<br>2021.<br>[68] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun,<br>Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy<br>to train strong classifiers with localizable features. In ICCV, 2019.<br>[69] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David<br>Lopez-Paz. mixup: Beyond empirical risk minimization. In ICLR,<br>2018.<br>[70] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image<br>colorization. In ECCV, 2016.<br>[71] Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba,<br>and Aude Oliva. Learning deep features for scene recognition using<br>Places database. In NeurIPS, 2014.<br>[72] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler,<br>Adela Barriuso, and Antonio Torralba. Semantic understanding<br>of scenes through the ADE20K dataset. IJCV, 2019.</p>",
      "id": 147,
      "page": 10,
      "text": "[56] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen\nSun, Alex Shepard, Hartwig Adam, Pietro Perona, and Serge Be-\nlongie. The iNaturalist species classification and detection dataset.\nIn CVPR, 2018.\n[57] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\nLlion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin.\nAttention is all you need. In NeurIPS, 2017.\n[58] Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-\nAntoine Manzagol. Extracting and composing robust features with\ndenoising autoencoders. In ICML, 2008.\n[59] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio,\nPierre-Antoine Manzagol, and Leon Bottou. Stacked denoising au-\ntoencoders: Learning useful representations in a deep network with\na local denoising criterion. JMLR, 2010.\n[60] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing.\nLearning robust global representations by penalizing local predic-\ntive power. In NeurIPS, 2019.\n[61] Xiaolong Wang and Abhinav Gupta. Unsupervised learning of vi-\nsual representations using videos. In ICCV, 2015.\n[62] Zhirong Wu, Yuanjun Xiong, Stella Yu, and Dahua Lin. Unsuper-\nvised feature learning via non-parametric instance discrimination.\nIn CVPR, 2018.\n[63] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian\nSun. Unified perceptual parsing for scene understanding. In ECCV,\n2018.\n[64] Tete Xiao, Mannat Singh, Eric Mintun, Trevor Darrell, Piotr Dollar,\nand Ross Girshick. Early convolutions help transformers see better.\nIn NeurIPS, 2021.\n[65] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. How\ntransferable are features in deep neural networks? In NeurIPS,\n2014.\n[66] Yang You, Igor Gitman, and Boris Ginsburg. Large batch training\nof convolutional networks. arXiv:1708.03888, 2017.\n[67] Li Yuan, Qibin Hou, Zihang Jiang, Jiashi Feng, and Shuicheng Yan.\nVOLO: Vision outlooker for visual recognition. arXiv:2106.13112,\n2021.\n[68] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun,\nJunsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy\nto train strong classifiers with localizable features. In ICCV, 2019.\n[69] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David\nLopez-Paz. mixup: Beyond empirical risk minimization. In ICLR,\n2018.\n[70] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image\ncolorization. In ECCV, 2016.\n[71] Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba,\nand Aude Oliva. Learning deep features for scene recognition using\nPlaces database. In NeurIPS, 2014.\n[72] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler,\nAdela Barriuso, and Antonio Torralba. Semantic understanding\nof scenes through the ADE20K dataset. IJCV, 2019."
    },
    {
      "bounding_box": [
        {
          "x": 1221,
          "y": 3055
        },
        {
          "x": 1262,
          "y": 3055
        },
        {
          "x": 1262,
          "y": 3091
        },
        {
          "x": 1221,
          "y": 3091
        }
      ],
      "category": "footer",
      "html": "<footer id='148' style='font-size:18px'>10</footer>",
      "id": 148,
      "page": 10,
      "text": "10"
    },
    {
      "bounding_box": [
        {
          "x": 205,
          "y": 301
        },
        {
          "x": 773,
          "y": 301
        },
        {
          "x": 773,
          "y": 352
        },
        {
          "x": 205,
          "y": 352
        }
      ],
      "category": "paragraph",
      "html": "<p id='149' style='font-size:22px'>A. Implementation Details</p>",
      "id": 149,
      "page": 11,
      "text": "A. Implementation Details"
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 387
        },
        {
          "x": 753,
          "y": 387
        },
        {
          "x": 753,
          "y": 438
        },
        {
          "x": 204,
          "y": 438
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='150' style='font-size:22px'>A.1. ImageNet Experiments</p>",
      "id": 150,
      "page": 11,
      "text": "A.1. ImageNet Experiments"
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 484
        },
        {
          "x": 1199,
          "y": 484
        },
        {
          "x": 1199,
          "y": 981
        },
        {
          "x": 202,
          "y": 981
        }
      ],
      "category": "paragraph",
      "html": "<p id='151' style='font-size:16px'>ViT architecture. We follow the standard ViT architecture<br>[16]. It has a stack of Transformer blocks [57], and each<br>block consists of a multi-head self-attention block and an<br>MLP block, both having LayerNorm (LN) [1]. The encoder<br>ends with LN. As the MAE encoder and decoder have dif-<br>ferent width, we adopt a linear projection layer after the<br>encoder to match it. Our MAE adds positional embeddings<br>[57] (the sine-cosine version) to both the encoder and de-<br>coder inputs. Our MAE does not use relative position or<br>layer scaling (which are used in the code of [2]).</p>",
      "id": 151,
      "page": 11,
      "text": "ViT architecture. We follow the standard ViT architecture\n[16]. It has a stack of Transformer blocks [57], and each\nblock consists of a multi-head self-attention block and an\nMLP block, both having LayerNorm (LN) [1]. The encoder\nends with LN. As the MAE encoder and decoder have dif-\nferent width, we adopt a linear projection layer after the\nencoder to match it. Our MAE adds positional embeddings\n[57] (the sine-cosine version) to both the encoder and de-\ncoder inputs. Our MAE does not use relative position or\nlayer scaling (which are used in the code of [2])."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 991
        },
        {
          "x": 1198,
          "y": 991
        },
        {
          "x": 1198,
          "y": 1336
        },
        {
          "x": 202,
          "y": 1336
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='152' style='font-size:18px'>We extract features from the encoder output for fine-<br>tuning and linear probing. As ViT has a class token [16],<br>to adapt to this design, in our MAE pre-training we append<br>an auxiliary dummy token to the encoder input. This token<br>will be treated as the class token for training the classifier in<br>linear probing and fine-tuning. Our MAE works similarly<br>well without this token (with average pooling).</p>",
      "id": 152,
      "page": 11,
      "text": "We extract features from the encoder output for fine-\ntuning and linear probing. As ViT has a class token [16],\nto adapt to this design, in our MAE pre-training we append\nan auxiliary dummy token to the encoder input. This token\nwill be treated as the class token for training the classifier in\nlinear probing and fine-tuning. Our MAE works similarly\nwell without this token (with average pooling)."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 1357
        },
        {
          "x": 1199,
          "y": 1357
        },
        {
          "x": 1199,
          "y": 1605
        },
        {
          "x": 202,
          "y": 1605
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='153' style='font-size:18px'>Pre-training. The default setting is in Table 8. We do<br>not use color jittering, drop path, or gradient clip. We use<br>xavier_uniform [18] to initialize all Transformer blocks, fol-<br>lowing ViT's official code [16]. We use the linear lr scaling<br>rule [20]: lr = base_lr x batchsize / 256.</p>",
      "id": 153,
      "page": 11,
      "text": "Pre-training. The default setting is in Table 8. We do\nnot use color jittering, drop path, or gradient clip. We use\nxavier_uniform [18] to initialize all Transformer blocks, fol-\nlowing ViT's official code [16]. We use the linear lr scaling\nrule [20]: lr = base_lr x batchsize / 256."
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 1628
        },
        {
          "x": 1198,
          "y": 1628
        },
        {
          "x": 1198,
          "y": 1775
        },
        {
          "x": 203,
          "y": 1775
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='154' style='font-size:20px'>End-to-end fine-tuning. Our fine-tuning follows common<br>practice of supervised ViT training. The default setting is in<br>Table 9. We use layer-wise lr decay [10] following [2].</p>",
      "id": 154,
      "page": 11,
      "text": "End-to-end fine-tuning. Our fine-tuning follows common\npractice of supervised ViT training. The default setting is in\nTable 9. We use layer-wise lr decay [10] following [2]."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 1798
        },
        {
          "x": 1198,
          "y": 1798
        },
        {
          "x": 1198,
          "y": 2145
        },
        {
          "x": 202,
          "y": 2145
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='155' style='font-size:18px'>Linear probing. Our linear classifier training follows [9].<br>See Table 10. We observe that linear probing requires a very<br>different recipe than end-to-end fine-tuning. In particular,<br>regularization is in general harmful for linear probing. Fol-<br>lowing [9], we disable many common regularization strate-<br>gies: we do not use mixup [69], cutmix [68], drop path [30],<br>or color jittering, and we set weight decay as zero.</p>",
      "id": 155,
      "page": 11,
      "text": "Linear probing. Our linear classifier training follows [9].\nSee Table 10. We observe that linear probing requires a very\ndifferent recipe than end-to-end fine-tuning. In particular,\nregularization is in general harmful for linear probing. Fol-\nlowing [9], we disable many common regularization strate-\ngies: we do not use mixup [69], cutmix [68], drop path [30],\nor color jittering, and we set weight decay as zero."
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 2153
        },
        {
          "x": 1199,
          "y": 2153
        },
        {
          "x": 1199,
          "y": 2847
        },
        {
          "x": 200,
          "y": 2847
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='156' style='font-size:16px'>It is a common practice to normalize the classifier input<br>when training a classical linear classifier (e.g., SVM [11]).<br>Similarly, it is beneficial to normalize the pre-trained fea-<br>tures when training the linear probing classifier. Follow-<br>ing [15], we adopt an extra BatchNorm layer [31] without<br>affine transformation (affine=F alse). This layer is ap-<br>plied on the pre-trained features produced by the encoder,<br>and is before the linear classifier. We note that the layer<br>does not break the linear property, and it can be absorbed<br>into the linear classifier after training: it is essentially a re-<br>parameterized linear classifier. 3 Introducing this layer helps<br>calibrate the feature magnitudes across different variants in<br>our ablations, SO that they can use the same setting without<br>further lr search.</p>",
      "id": 156,
      "page": 11,
      "text": "It is a common practice to normalize the classifier input\nwhen training a classical linear classifier (e.g., SVM [11]).\nSimilarly, it is beneficial to normalize the pre-trained fea-\ntures when training the linear probing classifier. Follow-\ning [15], we adopt an extra BatchNorm layer [31] without\naffine transformation (affine=F alse). This layer is ap-\nplied on the pre-trained features produced by the encoder,\nand is before the linear classifier. We note that the layer\ndoes not break the linear property, and it can be absorbed\ninto the linear classifier after training: it is essentially a re-\nparameterized linear classifier. 3 Introducing this layer helps\ncalibrate the feature magnitudes across different variants in\nour ablations, SO that they can use the same setting without\nfurther lr search."
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 2893
        },
        {
          "x": 1198,
          "y": 2893
        },
        {
          "x": 1198,
          "y": 2970
        },
        {
          "x": 203,
          "y": 2970
        }
      ],
      "category": "paragraph",
      "html": "<p id='157' style='font-size:14px'>3 Alternatively, we can pre-compute the mean and std of the features<br>and use the normalized features to train linear classifiers.</p>",
      "id": 157,
      "page": 11,
      "text": "3 Alternatively, we can pre-compute the mean and std of the features\nand use the normalized features to train linear classifiers."
    },
    {
      "bounding_box": [
        {
          "x": 1379,
          "y": 295
        },
        {
          "x": 2169,
          "y": 295
        },
        {
          "x": 2169,
          "y": 615
        },
        {
          "x": 1379,
          "y": 615
        }
      ],
      "category": "table",
      "html": "<br><table id='158' style='font-size:14px'><tr><td>config</td><td>value</td></tr><tr><td>optimizer</td><td>Adam W [39]</td></tr><tr><td>base learning rate</td><td>1.5e-4</td></tr><tr><td>weight decay</td><td>0.05</td></tr><tr><td>optimizer momentum</td><td>B1, B2=0.9, 0.95 [6]</td></tr><tr><td>batch size</td><td>4096</td></tr><tr><td>learning rate schedule</td><td>cosine decay [38]</td></tr><tr><td>warmup epochs [20]</td><td>40</td></tr><tr><td>augmentation</td><td>RandomResizedCrop</td></tr></table>",
      "id": 158,
      "page": 11,
      "text": "config value\n optimizer Adam W [39]\n base learning rate 1.5e-4\n weight decay 0.05\n optimizer momentum B1, B2=0.9, 0.95 [6]\n batch size 4096\n learning rate schedule cosine decay [38]\n warmup epochs [20] 40\n augmentation"
    },
    {
      "bounding_box": [
        {
          "x": 1549,
          "y": 636
        },
        {
          "x": 2004,
          "y": 636
        },
        {
          "x": 2004,
          "y": 680
        },
        {
          "x": 1549,
          "y": 680
        }
      ],
      "category": "caption",
      "html": "<br><caption id='159' style='font-size:18px'>Table 8. Pre-training setting.</caption>",
      "id": 159,
      "page": 11,
      "text": "Table 8. Pre-training setting."
    },
    {
      "bounding_box": [
        {
          "x": 1383,
          "y": 721
        },
        {
          "x": 2169,
          "y": 721
        },
        {
          "x": 2169,
          "y": 1241
        },
        {
          "x": 1383,
          "y": 1241
        }
      ],
      "category": "table",
      "html": "<table id='160' style='font-size:14px'><tr><td>config</td><td>value</td></tr><tr><td>optimizer</td><td>AdamW</td></tr><tr><td>base learning rate</td><td>1e-3</td></tr><tr><td>weight decay</td><td>0.05</td></tr><tr><td>optimizer momentum</td><td>B1, B2=0.9, 0.999</td></tr><tr><td>layer-wise lr decay [10, 2]</td><td>0.75</td></tr><tr><td>batch size</td><td>1024</td></tr><tr><td>learning rate schedule</td><td>cosine decay</td></tr><tr><td>warmup epochs</td><td>5</td></tr><tr><td>training epochs</td><td>100 (B), 50 (L/H)</td></tr><tr><td>augmentation</td><td>RandAug (9, 0.5) [12]</td></tr><tr><td>label smoothing [52]</td><td>0.1</td></tr><tr><td>mixup [69]</td><td>0.8</td></tr><tr><td>cutmix [68]</td><td>1.0</td></tr><tr><td>drop path [30]</td><td>0.1 (B/L) 0.2 (H)</td></tr></table>",
      "id": 160,
      "page": 11,
      "text": "config value\n optimizer AdamW\n base learning rate 1e-3\n weight decay 0.05\n optimizer momentum B1, B2=0.9, 0.999\n layer-wise lr decay [10, 2] 0.75\n batch size 1024\n learning rate schedule cosine decay\n warmup epochs 5\n training epochs 100 (B), 50 (L/H)\n augmentation RandAug (9, 0.5) [12]\n label smoothing [52] 0.1\n mixup [69] 0.8\n cutmix [68] 1.0\n drop path [30]"
    },
    {
      "bounding_box": [
        {
          "x": 1466,
          "y": 1264
        },
        {
          "x": 2088,
          "y": 1264
        },
        {
          "x": 2088,
          "y": 1307
        },
        {
          "x": 1466,
          "y": 1307
        }
      ],
      "category": "caption",
      "html": "<br><caption id='161' style='font-size:18px'>Table 9. End-to-end fine-tuning setting.</caption>",
      "id": 161,
      "page": 11,
      "text": "Table 9. End-to-end fine-tuning setting."
    },
    {
      "bounding_box": [
        {
          "x": 1380,
          "y": 1345
        },
        {
          "x": 2172,
          "y": 1345
        },
        {
          "x": 2172,
          "y": 1697
        },
        {
          "x": 1380,
          "y": 1697
        }
      ],
      "category": "table",
      "html": "<table id='162' style='font-size:14px'><tr><td>config</td><td>value</td></tr><tr><td>optimizer</td><td>LARS [66]</td></tr><tr><td>base learning rate</td><td>0.1</td></tr><tr><td>weight decay</td><td>0</td></tr><tr><td>optimizer momentum</td><td>0.9</td></tr><tr><td>batch size</td><td>16384</td></tr><tr><td>learning rate schedule</td><td>cosine decay</td></tr><tr><td>warmup epochs</td><td>10</td></tr><tr><td>training epochs</td><td>90</td></tr><tr><td>augmentation</td><td>RandomResizedCrop</td></tr></table>",
      "id": 162,
      "page": 11,
      "text": "config value\n optimizer LARS [66]\n base learning rate 0.1\n weight decay 0\n optimizer momentum 0.9\n batch size 16384\n learning rate schedule cosine decay\n warmup epochs 10\n training epochs 90\n augmentation"
    },
    {
      "bounding_box": [
        {
          "x": 1284,
          "y": 1721
        },
        {
          "x": 2274,
          "y": 1721
        },
        {
          "x": 2274,
          "y": 1807
        },
        {
          "x": 1284,
          "y": 1807
        }
      ],
      "category": "caption",
      "html": "<caption id='163' style='font-size:16px'>Table 10. Linear probing setting. We use LARS with a large<br>batch for faster training; SGD works similarly with a 4096 batch.</caption>",
      "id": 163,
      "page": 11,
      "text": "Table 10. Linear probing setting. We use LARS with a large\nbatch for faster training; SGD works similarly with a 4096 batch."
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 1906
        },
        {
          "x": 2276,
          "y": 1906
        },
        {
          "x": 2276,
          "y": 2203
        },
        {
          "x": 1281,
          "y": 2203
        }
      ],
      "category": "paragraph",
      "html": "<p id='164' style='font-size:18px'>Partial fine-tuning. Our MAE partial fine-tuning (§4.3)<br>follows the setting in Table 9, except that we adjust the num-<br>ber of fine-tuning epochs. We observe that tuning fewer<br>blocks requires a longer schedule. We set the numbers of<br>fine-tuning epochs as {50, 100, 200} and use the optimal<br>one for each number of blocks tuned.</p>",
      "id": 164,
      "page": 11,
      "text": "Partial fine-tuning. Our MAE partial fine-tuning (§4.3)\nfollows the setting in Table 9, except that we adjust the num-\nber of fine-tuning epochs. We observe that tuning fewer\nblocks requires a longer schedule. We set the numbers of\nfine-tuning epochs as {50, 100, 200} and use the optimal\none for each number of blocks tuned."
    },
    {
      "bounding_box": [
        {
          "x": 1283,
          "y": 2248
        },
        {
          "x": 2219,
          "y": 2248
        },
        {
          "x": 2219,
          "y": 2296
        },
        {
          "x": 1283,
          "y": 2296
        }
      ],
      "category": "paragraph",
      "html": "<p id='165' style='font-size:20px'>A.2. Supervised Training ViT-L/H from Scratch</p>",
      "id": 165,
      "page": 11,
      "text": "A.2. Supervised Training ViT-L/H from Scratch"
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 2326
        },
        {
          "x": 2275,
          "y": 2326
        },
        {
          "x": 2275,
          "y": 2672
        },
        {
          "x": 1281,
          "y": 2672
        }
      ],
      "category": "paragraph",
      "html": "<p id='166' style='font-size:18px'>We find that it is nontrivial to train supervised ViT-L/H<br>from scratch on ImageNet-1K. The training is unstable.<br>While there have been strong baselines with publicly avail-<br>able implementations [53] for smaller models, the recipes<br>for the larger ViT-L/H are unexplored. Directly applying<br>the previous recipes to these larger models does not work.<br>A NaN loss is frequently observed during training.</p>",
      "id": 166,
      "page": 11,
      "text": "We find that it is nontrivial to train supervised ViT-L/H\nfrom scratch on ImageNet-1K. The training is unstable.\nWhile there have been strong baselines with publicly avail-\nable implementations [53] for smaller models, the recipes\nfor the larger ViT-L/H are unexplored. Directly applying\nthe previous recipes to these larger models does not work.\nA NaN loss is frequently observed during training."
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 2678
        },
        {
          "x": 2278,
          "y": 2678
        },
        {
          "x": 2278,
          "y": 2976
        },
        {
          "x": 1281,
          "y": 2976
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='167' style='font-size:18px'>We provide our recipe in Table 11. We use a wd of 0.3,<br>a large batch size of 4096, and a long warmup, following<br>the original ViT [16]. We use B2=0.95 following [6]. We<br>use the regularizations listed in Table 11 and disable others,<br>following [64]. All these choices are for improving training<br>stability. Our recipe can finish training with no NaN loss.</p>",
      "id": 167,
      "page": 11,
      "text": "We provide our recipe in Table 11. We use a wd of 0.3,\na large batch size of 4096, and a long warmup, following\nthe original ViT [16]. We use B2=0.95 following [6]. We\nuse the regularizations listed in Table 11 and disable others,\nfollowing [64]. All these choices are for improving training\nstability. Our recipe can finish training with no NaN loss."
    },
    {
      "bounding_box": [
        {
          "x": 1219,
          "y": 3056
        },
        {
          "x": 1258,
          "y": 3056
        },
        {
          "x": 1258,
          "y": 3091
        },
        {
          "x": 1219,
          "y": 3091
        }
      ],
      "category": "footer",
      "html": "<footer id='168' style='font-size:16px'>11</footer>",
      "id": 168,
      "page": 11,
      "text": "11"
    },
    {
      "bounding_box": [
        {
          "x": 309,
          "y": 293
        },
        {
          "x": 1089,
          "y": 293
        },
        {
          "x": 1089,
          "y": 818
        },
        {
          "x": 309,
          "y": 818
        }
      ],
      "category": "table",
      "html": "<table id='169' style='font-size:14px'><tr><td>config</td><td>value</td></tr><tr><td>optimizer</td><td>AdamW</td></tr><tr><td>base learning rate</td><td>1e-4</td></tr><tr><td>weight decay</td><td>0.3</td></tr><tr><td>optimizer momentum</td><td>B1, B2=0.9, 0.95</td></tr><tr><td>batch size</td><td>4096</td></tr><tr><td>learning rate schedule</td><td>cosine decay</td></tr><tr><td>warmup epochs</td><td>20</td></tr><tr><td>training epochs</td><td>300 (B), 200 (L/H)</td></tr><tr><td>augmentation</td><td>RandAug (9, 0.5) [12]</td></tr><tr><td>label smoothing [52]</td><td>0.1</td></tr><tr><td>mixup [69]</td><td>0.8</td></tr><tr><td>cutmix [68]</td><td>1.0</td></tr><tr><td>drop path [30]</td><td>0.1 (B), 0.2 (L/H)</td></tr><tr><td>exp. moving average (EMA)</td><td>0.9999</td></tr></table>",
      "id": 169,
      "page": 12,
      "text": "config value\n optimizer AdamW\n base learning rate 1e-4\n weight decay 0.3\n optimizer momentum B1, B2=0.9, 0.95\n batch size 4096\n learning rate schedule cosine decay\n warmup epochs 20\n training epochs 300 (B), 200 (L/H)\n augmentation RandAug (9, 0.5) [12]\n label smoothing [52] 0.1\n mixup [69] 0.8\n cutmix [68] 1.0\n drop path [30] 0.1 (B), 0.2 (L/H)\n exp. moving average (EMA)"
    },
    {
      "bounding_box": [
        {
          "x": 322,
          "y": 842
        },
        {
          "x": 1082,
          "y": 842
        },
        {
          "x": 1082,
          "y": 880
        },
        {
          "x": 322,
          "y": 880
        }
      ],
      "category": "caption",
      "html": "<br><caption id='170' style='font-size:16px'>Table 11. Supervised training ViT from scratch.</caption>",
      "id": 170,
      "page": 12,
      "text": "Table 11. Supervised training ViT from scratch."
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 967
        },
        {
          "x": 1199,
          "y": 967
        },
        {
          "x": 1199,
          "y": 1109
        },
        {
          "x": 203,
          "y": 1109
        }
      ],
      "category": "paragraph",
      "html": "<p id='171' style='font-size:18px'>The accuracy is 82.6% for ViT-L (81.5% w/o EMA), and<br>83.1% for ViT-H (80.9% w/o EMA). Both ViT-L and ViT-H<br>show an overfitting trend if not using EMA.</p>",
      "id": 171,
      "page": 12,
      "text": "The accuracy is 82.6% for ViT-L (81.5% w/o EMA), and\n83.1% for ViT-H (80.9% w/o EMA). Both ViT-L and ViT-H\nshow an overfitting trend if not using EMA."
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 1121
        },
        {
          "x": 1196,
          "y": 1121
        },
        {
          "x": 1196,
          "y": 1213
        },
        {
          "x": 203,
          "y": 1213
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='172' style='font-size:18px'>As a by-product, our recipe for ViT-B has 82.3% accu-<br>racy (82.1% w/o EMA), vs. 81.8% in [53].</p>",
      "id": 172,
      "page": 12,
      "text": "As a by-product, our recipe for ViT-B has 82.3% accu-\nracy (82.1% w/o EMA), vs. 81.8% in [53]."
    },
    {
      "bounding_box": [
        {
          "x": 205,
          "y": 1258
        },
        {
          "x": 1191,
          "y": 1258
        },
        {
          "x": 1191,
          "y": 1305
        },
        {
          "x": 205,
          "y": 1305
        }
      ],
      "category": "paragraph",
      "html": "<p id='173' style='font-size:20px'>A.3. Object Detection and Segmentation in COCO</p>",
      "id": 173,
      "page": 12,
      "text": "A.3. Object Detection and Segmentation in COCO"
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 1337
        },
        {
          "x": 1200,
          "y": 1337
        },
        {
          "x": 1200,
          "y": 1732
        },
        {
          "x": 201,
          "y": 1732
        }
      ],
      "category": "paragraph",
      "html": "<p id='174' style='font-size:18px'>We adapt the vanilla ViT for the use of an FPN backbone<br>[36] in Mask R-CNN [24]. ViT has a stack of Transformer<br>blocks that all produce feature maps at a single scale (e.g.,<br>stride 16). We equally divide this stack into 4 subsets and<br>apply convolutions to upsample or downsample the inter-<br>mediate feature maps for producing different scales (stride<br>4, 8, 16, or 32, the same as a standard ResNet [25]). FPN is<br>built on these multi-scale maps.</p>",
      "id": 174,
      "page": 12,
      "text": "We adapt the vanilla ViT for the use of an FPN backbone\n[36] in Mask R-CNN [24]. ViT has a stack of Transformer\nblocks that all produce feature maps at a single scale (e.g.,\nstride 16). We equally divide this stack into 4 subsets and\napply convolutions to upsample or downsample the inter-\nmediate feature maps for producing different scales (stride\n4, 8, 16, or 32, the same as a standard ResNet [25]). FPN is\nbuilt on these multi-scale maps."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 1742
        },
        {
          "x": 1199,
          "y": 1742
        },
        {
          "x": 1199,
          "y": 2087
        },
        {
          "x": 202,
          "y": 2087
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='175' style='font-size:20px'>For fair comparisons among different methods, we<br>search for hyper-parameters for each entry in Table 4 (in-<br>cluding all competitors). The hyper-parameters we search<br>for are the learning rate, weight decay, drop path rate, and<br>fine-tuning epochs. We will release code along with the<br>specific configurations. For full model and training details,<br>plus additional experiments, see [35].</p>",
      "id": 175,
      "page": 12,
      "text": "For fair comparisons among different methods, we\nsearch for hyper-parameters for each entry in Table 4 (in-\ncluding all competitors). The hyper-parameters we search\nfor are the learning rate, weight decay, drop path rate, and\nfine-tuning epochs. We will release code along with the\nspecific configurations. For full model and training details,\nplus additional experiments, see [35]."
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 2128
        },
        {
          "x": 996,
          "y": 2128
        },
        {
          "x": 996,
          "y": 2176
        },
        {
          "x": 203,
          "y": 2176
        }
      ],
      "category": "paragraph",
      "html": "<p id='176' style='font-size:22px'>A.4. Semantic Segmentation in ADE20K</p>",
      "id": 176,
      "page": 12,
      "text": "A.4. Semantic Segmentation in ADE20K"
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 2207
        },
        {
          "x": 1199,
          "y": 2207
        },
        {
          "x": 1199,
          "y": 2403
        },
        {
          "x": 202,
          "y": 2403
        }
      ],
      "category": "paragraph",
      "html": "<p id='177' style='font-size:18px'>We use UperNet [63] following the semantic segmenta-<br>tion code of [2]. We fine-tune end-to-end for 100 epochs<br>with a batch size of 16. We search for the optimal lr for<br>each entry in Table 5 (including all competitors).</p>",
      "id": 177,
      "page": 12,
      "text": "We use UperNet [63] following the semantic segmenta-\ntion code of [2]. We fine-tune end-to-end for 100 epochs\nwith a batch size of 16. We search for the optimal lr for\neach entry in Table 5 (including all competitors)."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 2411
        },
        {
          "x": 1199,
          "y": 2411
        },
        {
          "x": 1199,
          "y": 2706
        },
        {
          "x": 202,
          "y": 2706
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='178' style='font-size:18px'>The semantic segmentation code of [2] uses relative po-<br>sition bias [49]. Our MAE pre-training does not use it. For<br>fair comparison, we turn on relative position bias only dur-<br>ing transfer learning, initialized as zero. We note that our<br>BEiT reproduction uses relative position bias in both pre-<br>training and fine-tuning, following their code.</p>",
      "id": 178,
      "page": 12,
      "text": "The semantic segmentation code of [2] uses relative po-\nsition bias [49]. Our MAE pre-training does not use it. For\nfair comparison, we turn on relative position bias only dur-\ning transfer learning, initialized as zero. We note that our\nBEiT reproduction uses relative position bias in both pre-\ntraining and fine-tuning, following their code."
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 2748
        },
        {
          "x": 904,
          "y": 2748
        },
        {
          "x": 904,
          "y": 2796
        },
        {
          "x": 204,
          "y": 2796
        }
      ],
      "category": "paragraph",
      "html": "<p id='179' style='font-size:20px'>A.5. Additional Classification Tasks</p>",
      "id": 179,
      "page": 12,
      "text": "A.5. Additional Classification Tasks"
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 2828
        },
        {
          "x": 1199,
          "y": 2828
        },
        {
          "x": 1199,
          "y": 2974
        },
        {
          "x": 203,
          "y": 2974
        }
      ],
      "category": "paragraph",
      "html": "<p id='180' style='font-size:20px'>We follow the setting in Table 9 for iNaturalist and<br>Places fine-tuning (Table 6). We adjust the lr and fine-<br>tuning epochs for each individual dataset.</p>",
      "id": 180,
      "page": 12,
      "text": "We follow the setting in Table 9 for iNaturalist and\nPlaces fine-tuning (Table 6). We adjust the lr and fine-\ntuning epochs for each individual dataset."
    },
    {
      "bounding_box": [
        {
          "x": 1428,
          "y": 299
        },
        {
          "x": 2124,
          "y": 299
        },
        {
          "x": 2124,
          "y": 594
        },
        {
          "x": 1428,
          "y": 594
        }
      ],
      "category": "table",
      "html": "<br><table id='181' style='font-size:14px'><tr><td>method</td><td>model</td><td>params</td><td>acc</td></tr><tr><td>iGPT [6]</td><td>iGPT-L</td><td>1362 M</td><td>69.0</td></tr><tr><td>iGPT [6]</td><td>iGPT-XL</td><td>6801 M</td><td>72.0</td></tr><tr><td>BEiT [2]</td><td>ViT-L</td><td>304 M</td><td>52.1t</td></tr><tr><td>MAE</td><td>ViT-B</td><td>86 M</td><td>68.0</td></tr><tr><td>MAE</td><td>ViT-L</td><td>304 M</td><td>75.8</td></tr><tr><td>MAE</td><td>ViT-H</td><td>632 M</td><td>76.6</td></tr></table>",
      "id": 181,
      "page": 12,
      "text": "method model params acc\n iGPT [6] iGPT-L 1362 M 69.0\n iGPT [6] iGPT-XL 6801 M 72.0\n BEiT [2] ViT-L 304 M 52.1t\n MAE ViT-B 86 M 68.0\n MAE ViT-L 304 M 75.8\n MAE ViT-H 632 M"
    },
    {
      "bounding_box": [
        {
          "x": 1283,
          "y": 605
        },
        {
          "x": 2273,
          "y": 605
        },
        {
          "x": 2273,
          "y": 692
        },
        {
          "x": 1283,
          "y": 692
        }
      ],
      "category": "caption",
      "html": "<br><caption id='182' style='font-size:16px'>Table 12. Linear probing results of masked encoding methods.<br>Our fine-tuning results are in Table 3. t: our implementation.</caption>",
      "id": 182,
      "page": 12,
      "text": "Table 12. Linear probing results of masked encoding methods.\nOur fine-tuning results are in Table 3. t: our implementation."
    },
    {
      "bounding_box": [
        {
          "x": 1288,
          "y": 740
        },
        {
          "x": 2259,
          "y": 740
        },
        {
          "x": 2259,
          "y": 955
        },
        {
          "x": 1288,
          "y": 955
        }
      ],
      "category": "table",
      "html": "<table id='183' style='font-size:16px'><tr><td>dataset</td><td>ViT-B</td><td>ViT-L</td><td>ViT-H</td><td>ViT-H448</td><td>prev best</td></tr><tr><td>IN-Corruption ↓ [27]</td><td>51.7</td><td>41.8</td><td>33.8</td><td>36.8</td><td>42.5 [32]</td></tr><tr><td>IN-Adversarial [28]</td><td>35.9</td><td>57.1</td><td>68.2</td><td>76.7</td><td>35.8 [41]</td></tr><tr><td>IN-Rendition [26]</td><td>48.3</td><td>59.9</td><td>64.4</td><td>66.5</td><td>48.7 [41]</td></tr><tr><td>IN-Sketch [60]</td><td>34.5</td><td>45.3</td><td>49.6</td><td>50.9</td><td>36.0 [41]</td></tr></table>",
      "id": 183,
      "page": 12,
      "text": "dataset ViT-B ViT-L ViT-H ViT-H448 prev best\n IN-Corruption ↓ [27] 51.7 41.8 33.8 36.8 42.5 [32]\n IN-Adversarial [28] 35.9 57.1 68.2 76.7 35.8 [41]\n IN-Rendition [26] 48.3 59.9 64.4 66.5 48.7 [41]\n IN-Sketch [60] 34.5 45.3 49.6 50.9"
    },
    {
      "bounding_box": [
        {
          "x": 1297,
          "y": 959
        },
        {
          "x": 1935,
          "y": 959
        },
        {
          "x": 1935,
          "y": 1145
        },
        {
          "x": 1297,
          "y": 1145
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='184' style='font-size:14px'>our supervised training baselines:<br>IN-Corruption ↓ 45.8 42.3 41.3<br>IN-Adversarial 27.2 29.6 33.1<br>IN-Rendition 49.4 50.9 50.3<br>IN-Sketch 35.6 37.5 38.0</p>",
      "id": 184,
      "page": 12,
      "text": "our supervised training baselines:\nIN-Corruption ↓ 45.8 42.3 41.3\nIN-Adversarial 27.2 29.6 33.1\nIN-Rendition 49.4 50.9 50.3\nIN-Sketch 35.6 37.5 38.0"
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 1160
        },
        {
          "x": 2277,
          "y": 1160
        },
        {
          "x": 2277,
          "y": 1389
        },
        {
          "x": 1281,
          "y": 1389
        }
      ],
      "category": "caption",
      "html": "<br><caption id='185' style='font-size:16px'>Table 13. Robustness evaluation on ImageNet variants (top-1<br>accuracy, except for IN-C [27] which evaluates mean corruption<br>error). We test the same MAE models (Table 3) on different Im-<br>ageNet validation sets, without any specialized fine-tuning. We<br>provide system-level comparisons with the previous best results.</caption>",
      "id": 185,
      "page": 12,
      "text": "Table 13. Robustness evaluation on ImageNet variants (top-1\naccuracy, except for IN-C [27] which evaluates mean corruption\nerror). We test the same MAE models (Table 3) on different Im-\nageNet validation sets, without any specialized fine-tuning. We\nprovide system-level comparisons with the previous best results."
    },
    {
      "bounding_box": [
        {
          "x": 1283,
          "y": 1441
        },
        {
          "x": 2183,
          "y": 1441
        },
        {
          "x": 2183,
          "y": 1493
        },
        {
          "x": 1283,
          "y": 1493
        }
      ],
      "category": "paragraph",
      "html": "<p id='186' style='font-size:22px'>B. Comparison on Linear Probing Results</p>",
      "id": 186,
      "page": 12,
      "text": "B. Comparison on Linear Probing Results"
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 1525
        },
        {
          "x": 2277,
          "y": 1525
        },
        {
          "x": 2277,
          "y": 1870
        },
        {
          "x": 1280,
          "y": 1870
        }
      ],
      "category": "paragraph",
      "html": "<p id='187' style='font-size:20px'>In §4.3 we have shown that linear probing accuracy and<br>fine-tuning accuracy are largely uncorrelated and they have<br>different focuses about linear separability. We notice that<br>existing masked image encoding methods are generally less<br>competitive in linear probing (e.g., than contrastive learn-<br>ing). For completeness, in Table 12 we compare on linear<br>probing accuracy with masking-based methods.</p>",
      "id": 187,
      "page": 12,
      "text": "In §4.3 we have shown that linear probing accuracy and\nfine-tuning accuracy are largely uncorrelated and they have\ndifferent focuses about linear separability. We notice that\nexisting masked image encoding methods are generally less\ncompetitive in linear probing (e.g., than contrastive learn-\ning). For completeness, in Table 12 we compare on linear\nprobing accuracy with masking-based methods."
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 1877
        },
        {
          "x": 2277,
          "y": 1877
        },
        {
          "x": 2277,
          "y": 2121
        },
        {
          "x": 1281,
          "y": 2121
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='188' style='font-size:18px'>Our MAE with ViT-L has 75.8% linear probing accu-<br>racy. This is substantially better than previous masking-<br>based methods. On the other hand, it still lags behind con-<br>trastive methods under this protocol: e.g., MoCo v3 [9] has<br>77.6% linear probing accuracy for the ViT-L (Figure 9).</p>",
      "id": 188,
      "page": 12,
      "text": "Our MAE with ViT-L has 75.8% linear probing accu-\nracy. This is substantially better than previous masking-\nbased methods. On the other hand, it still lags behind con-\ntrastive methods under this protocol: e.g., MoCo v3 [9] has\n77.6% linear probing accuracy for the ViT-L (Figure 9)."
    },
    {
      "bounding_box": [
        {
          "x": 1283,
          "y": 2168
        },
        {
          "x": 2119,
          "y": 2168
        },
        {
          "x": 2119,
          "y": 2219
        },
        {
          "x": 1283,
          "y": 2219
        }
      ],
      "category": "paragraph",
      "html": "<p id='189' style='font-size:22px'>C. Robustness Evaluation on ImageNet</p>",
      "id": 189,
      "page": 12,
      "text": "C. Robustness Evaluation on ImageNet"
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 2251
        },
        {
          "x": 2276,
          "y": 2251
        },
        {
          "x": 2276,
          "y": 2697
        },
        {
          "x": 1279,
          "y": 2697
        }
      ],
      "category": "paragraph",
      "html": "<p id='190' style='font-size:18px'>In Table 13 we evaluate the robustness of our models on<br>different variants of ImageNet validation sets. We use the<br>same models fine-tuned on original ImageNet (Table 3) and<br>only run inference on the different validation sets, without<br>any specialized fine-tuning. Table 13 shows that our method<br>has strong scaling behavior: increasing the model sizes has<br>significant gains. Increasing the image size helps in all sets<br>but IN-C. Our results outperform the previous best results<br>(of specialized systems) by large margins.</p>",
      "id": 190,
      "page": 12,
      "text": "In Table 13 we evaluate the robustness of our models on\ndifferent variants of ImageNet validation sets. We use the\nsame models fine-tuned on original ImageNet (Table 3) and\nonly run inference on the different validation sets, without\nany specialized fine-tuning. Table 13 shows that our method\nhas strong scaling behavior: increasing the model sizes has\nsignificant gains. Increasing the image size helps in all sets\nbut IN-C. Our results outperform the previous best results\n(of specialized systems) by large margins."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 2703
        },
        {
          "x": 2278,
          "y": 2703
        },
        {
          "x": 2278,
          "y": 2897
        },
        {
          "x": 1280,
          "y": 2897
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='191' style='font-size:18px'>In contrast, supervised training performs much worse<br>(Table 13 bottom; models described in A.2). For example,<br>with ViT-H, our MAE pre-training is 35% better on IN-A<br>(68.2% VS 33.1%) than the supervised counterpart.</p>",
      "id": 191,
      "page": 12,
      "text": "In contrast, supervised training performs much worse\n(Table 13 bottom; models described in A.2). For example,\nwith ViT-H, our MAE pre-training is 35% better on IN-A\n(68.2% VS 33.1%) than the supervised counterpart."
    },
    {
      "bounding_box": [
        {
          "x": 1218,
          "y": 3052
        },
        {
          "x": 1263,
          "y": 3052
        },
        {
          "x": 1263,
          "y": 3093
        },
        {
          "x": 1218,
          "y": 3093
        }
      ],
      "category": "footer",
      "html": "<footer id='192' style='font-size:18px'>12</footer>",
      "id": 192,
      "page": 12,
      "text": "12"
    },
    {
      "bounding_box": [
        {
          "x": 225,
          "y": 190
        },
        {
          "x": 2257,
          "y": 190
        },
        {
          "x": 2257,
          "y": 2934
        },
        {
          "x": 225,
          "y": 2934
        }
      ],
      "category": "figure",
      "html": "<figure><img id='193' style='font-size:14px' alt=\"Congratulations\n-::\nROLAND\nGULAND\nGARROS 제품 Model 1319\" data-coord=\"top-left:(225,190); bottom-right:(2257,2934)\" /></figure>",
      "id": 193,
      "page": 13,
      "text": "Congratulations\n-::\nROLAND\nGULAND\nGARROS 제품 Model 1319"
    },
    {
      "bounding_box": [
        {
          "x": 199,
          "y": 2932
        },
        {
          "x": 2276,
          "y": 2932
        },
        {
          "x": 2276,
          "y": 3024
        },
        {
          "x": 199,
          "y": 3024
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='194' style='font-size:16px'>Figure 10. Uncurated random samples on ImageNet validation images. For each triplet, we show the masked image (left), our MAE<br>reconstruction (middle), and the ground-truth (right). The masking ratio is 75%.</p>",
      "id": 194,
      "page": 13,
      "text": "Figure 10. Uncurated random samples on ImageNet validation images. For each triplet, we show the masked image (left), our MAE\nreconstruction (middle), and the ground-truth (right). The masking ratio is 75%."
    },
    {
      "bounding_box": [
        {
          "x": 1216,
          "y": 3050
        },
        {
          "x": 1263,
          "y": 3050
        },
        {
          "x": 1263,
          "y": 3095
        },
        {
          "x": 1216,
          "y": 3095
        }
      ],
      "category": "footer",
      "html": "<br><footer id='195' style='font-size:20px'>13</footer>",
      "id": 195,
      "page": 13,
      "text": "13"
    },
    {
      "bounding_box": [
        {
          "x": 225,
          "y": 186
        },
        {
          "x": 2256,
          "y": 186
        },
        {
          "x": 2256,
          "y": 2930
        },
        {
          "x": 225,
          "y": 2930
        }
      ],
      "category": "figure",
      "html": "<figure><img id='196' style='font-size:14px' alt=\"next\nBOLL CHII BODACIOUS\" data-coord=\"top-left:(225,186); bottom-right:(2256,2930)\" /></figure>",
      "id": 196,
      "page": 14,
      "text": "next\nBOLL CHII BODACIOUS"
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 2932
        },
        {
          "x": 2275,
          "y": 2932
        },
        {
          "x": 2275,
          "y": 3023
        },
        {
          "x": 202,
          "y": 3023
        }
      ],
      "category": "caption",
      "html": "<br><caption id='197' style='font-size:18px'>Figure 11. Uncurated random samples on COCO validation images, using an MAE trained on ImageNet. For each triplet, we show the<br>masked image (left), our MAE reconstruction (middle), and the ground-truth (right). The masking ratio is 75%.</caption>",
      "id": 197,
      "page": 14,
      "text": "Figure 11. Uncurated random samples on COCO validation images, using an MAE trained on ImageNet. For each triplet, we show the\nmasked image (left), our MAE reconstruction (middle), and the ground-truth (right). The masking ratio is 75%."
    },
    {
      "bounding_box": [
        {
          "x": 1216,
          "y": 3052
        },
        {
          "x": 1263,
          "y": 3052
        },
        {
          "x": 1263,
          "y": 3094
        },
        {
          "x": 1216,
          "y": 3094
        }
      ],
      "category": "footer",
      "html": "<br><footer id='198' style='font-size:18px'>14</footer>",
      "id": 198,
      "page": 14,
      "text": "14"
    }
  ]
}