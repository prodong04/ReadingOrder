{
    "id": "329c0632-0f94-11ef-8828-426932df3dcf",
    "pdf_path": "./pdf/AI_VIT_X/1311.2524v5.pdf",
    "elements": [
        {
            "bounding_box": [
                {
                    "x": 207,
                    "y": 436
                },
                {
                    "x": 2274,
                    "y": 436
                },
                {
                    "x": 2274,
                    "y": 579
                },
                {
                    "x": 207,
                    "y": 579
                }
            ],
            "category": "paragraph",
            "html": "<p id='0' style='font-size:22px'>Rich feature hierarchies for accurate object detection and semantic segmentation<br>Tech report (v5)</p>",
            "id": 0,
            "page": 1,
            "text": "Rich feature hierarchies for accurate object detection and semantic segmentation Tech report (v5)"
        },
        {
            "bounding_box": [
                {
                    "x": 617,
                    "y": 679
                },
                {
                    "x": 1863,
                    "y": 679
                },
                {
                    "x": 1863,
                    "y": 789
                },
                {
                    "x": 617,
                    "y": 789
                }
            ],
            "category": "paragraph",
            "html": "<p id='1' style='font-size:20px'>Ross Girshick Jeff Donahue Trevor Darrell Jitendra Malik<br>UC Berkeley</p>",
            "id": 1,
            "page": 1,
            "text": "Ross Girshick Jeff Donahue Trevor Darrell Jitendra Malik UC Berkeley"
        },
        {
            "bounding_box": [
                {
                    "x": 736,
                    "y": 805
                },
                {
                    "x": 1745,
                    "y": 805
                },
                {
                    "x": 1745,
                    "y": 849
                },
                {
                    "x": 736,
                    "y": 849
                }
            ],
            "category": "paragraph",
            "html": "<p id='2' style='font-size:14px'>{rbg, jdonahue, trevor, malik}@eecs · berkeley · edu</p>",
            "id": 2,
            "page": 1,
            "text": "{rbg, jdonahue, trevor, malik}@eecs · berkeley · edu"
        },
        {
            "bounding_box": [
                {
                    "x": 603,
                    "y": 969
                },
                {
                    "x": 798,
                    "y": 969
                },
                {
                    "x": 798,
                    "y": 1018
                },
                {
                    "x": 603,
                    "y": 1018
                }
            ],
            "category": "paragraph",
            "html": "<p id='3' style='font-size:22px'>Abstract</p>",
            "id": 3,
            "page": 1,
            "text": "Abstract"
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 1076
                },
                {
                    "x": 1200,
                    "y": 1076
                },
                {
                    "x": 1200,
                    "y": 2174
                },
                {
                    "x": 199,
                    "y": 2174
                }
            ],
            "category": "paragraph",
            "html": "<p id='4' style='font-size:18px'>Object detection performance, as measured on the<br>canonical PASCAL VOC dataset, has plateaued in the last<br>few years. The best-performing methods are complex en-<br>semble systems that typically combine multiple low-level<br>image features with high-level context. In this paper, we<br>propose a simple and scalable detection algorithm that im-<br>proves mean average precision (mAP) by more than 30%<br>relative to the previous best result on VOC 2012-achieving<br>a mAP of 53.3%. Our approach combines two key insights:<br>(1) one can apply high-capacity convolutional neural net-<br>works (CNNs) to bottom-up region proposals in order to<br>localize and segment objects and (2) when labeled training<br>data is scarce, supervised pre-training for an auxiliary task,<br>followed by domain-specific fine-tuning, yields a significant<br>performance boost. Since we combine region proposals<br>with CNNs, we call our method R-CNN: Regions with CNN<br>features. We also compare R-CNN to OverFeat, a recently<br>proposed sliding-window detector based on a similar CNN<br>architecture. We find that R-CNN outperforms OverFeat<br>by a large margin on the 200-class ILSVRC2013 detection<br>dataset. Source code for the complete system is available at<br>http : / / www · CS berkeley . edu/ ~rbg/ rcnn.</p>",
            "id": 4,
            "page": 1,
            "text": "Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012-achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also compare R-CNN to OverFeat, a recently proposed sliding-window detector based on a similar CNN architecture. We find that R-CNN outperforms OverFeat by a large margin on the 200-class ILSVRC2013 detection dataset. Source code for the complete system is available at http : / / www · CS berkeley . edu/ ~rbg/ rcnn."
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 2237
                },
                {
                    "x": 531,
                    "y": 2237
                },
                {
                    "x": 531,
                    "y": 2287
                },
                {
                    "x": 204,
                    "y": 2287
                }
            ],
            "category": "paragraph",
            "html": "<p id='5' style='font-size:20px'>1. Introduction</p>",
            "id": 5,
            "page": 1,
            "text": "1. Introduction"
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 2327
                },
                {
                    "x": 1198,
                    "y": 2327
                },
                {
                    "x": 1198,
                    "y": 2721
                },
                {
                    "x": 203,
                    "y": 2721
                }
            ],
            "category": "paragraph",
            "html": "<p id='6' style='font-size:18px'>Features matter. The last decade of progress on various<br>visual recognition tasks has been based considerably on the<br>use of SIFT [29] and HOG [7]. But if we look at perfor-<br>mance on the canonical visual recognition task, PASCAL<br>VOC object detection [15], it is generally acknowledged<br>that progress has been slow during 2010-2012, with small<br>gains obtained by building ensemble systems and employ-<br>ing minor variants of successful methods.</p>",
            "id": 6,
            "page": 1,
            "text": "Features matter. The last decade of progress on various visual recognition tasks has been based considerably on the use of SIFT  and HOG . But if we look at performance on the canonical visual recognition task, PASCAL VOC object detection , it is generally acknowledged that progress has been slow during 2010-2012, with small gains obtained by building ensemble systems and employing minor variants of successful methods."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2729
                },
                {
                    "x": 1199,
                    "y": 2729
                },
                {
                    "x": 1199,
                    "y": 2976
                },
                {
                    "x": 202,
                    "y": 2976
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='7' style='font-size:18px'>SIFT and HOG are blockwise orientation histograms,<br>a representation we could associate roughly with complex<br>cells in V1, the first cortical area in the primate visual path-<br>way. But we also know that recognition occurs several<br>stages downstream, which suggests that there might be hier-</p>",
            "id": 7,
            "page": 1,
            "text": "SIFT and HOG are blockwise orientation histograms, a representation we could associate roughly with complex cells in V1, the first cortical area in the primate visual pathway. But we also know that recognition occurs several stages downstream, which suggests that there might be hier-"
        },
        {
            "bounding_box": [
                {
                    "x": 1485,
                    "y": 973
                },
                {
                    "x": 2088,
                    "y": 973
                },
                {
                    "x": 2088,
                    "y": 1018
                },
                {
                    "x": 1485,
                    "y": 1018
                }
            ],
            "category": "caption",
            "html": "<br><caption id='8' style='font-size:20px'>R-CNN: Regions with CNN features</caption>",
            "id": 8,
            "page": 1,
            "text": "R-CNN: Regions with CNN features"
        },
        {
            "bounding_box": [
                {
                    "x": 1285,
                    "y": 1026
                },
                {
                    "x": 2277,
                    "y": 1026
                },
                {
                    "x": 2277,
                    "y": 1308
                },
                {
                    "x": 1285,
                    "y": 1308
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='9' style='font-size:16px' alt=\"warped region aeroplane? no.\nperson? yes.\ntvmonitor? no.\n1. Input 2. Extract region 3. Compute 4. Classify\nimage proposals (~2k) CNN features regions\" data-coord=\"top-left:(1285,1026); bottom-right:(2277,1308)\" /></figure>",
            "id": 9,
            "page": 1,
            "text": "warped region aeroplane? no. person? yes. tvmonitor? no. 1. Input 2. Extract region 3. Compute 4. Classify image proposals (~2k) CNN features regions"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 1332
                },
                {
                    "x": 2275,
                    "y": 1332
                },
                {
                    "x": 2275,
                    "y": 1879
                },
                {
                    "x": 1279,
                    "y": 1879
                }
            ],
            "category": "paragraph",
            "html": "<p id='10' style='font-size:16px'>Figure 1: Object detection system overview. Our system (1)<br>takes an input image, (2) extracts around 2000 bottom-up region<br>proposals, (3) computes features for each proposal using a large<br>convolutional neural network (CNN), and then (4) classifies each<br>region using class-specific linear SVMs. R-CNN achieves a mean<br>average precision (mAP) of 53.7% on PASCAL VOC 2010. For<br>comparison, [39] reports 35.1% mAP using the same region pro-<br>posals, but with a spatial pyramid and bag-of-visual-words ap-<br>proach. The popular deformable part models perform at 33.4%.<br>On the 200-class ILSVRC2013 detection dataset, R-CNN's<br>mAP is 31.4%, a large improvement over OverFeat [34], which<br>had the previous best result at 24.3%.</p>",
            "id": 10,
            "page": 1,
            "text": "Figure 1: Object detection system overview. Our system (1) takes an input image, (2) extracts around 2000 bottom-up region proposals, (3) computes features for each proposal using a large convolutional neural network (CNN), and then (4) classifies each region using class-specific linear SVMs. R-CNN achieves a mean average precision (mAP) of 53.7% on PASCAL VOC 2010. For comparison,  reports 35.1% mAP using the same region proposals, but with a spatial pyramid and bag-of-visual-words approach. The popular deformable part models perform at 33.4%. On the 200-class ILSVRC2013 detection dataset, R-CNN's mAP is 31.4%, a large improvement over OverFeat , which had the previous best result at 24.3%."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 1932
                },
                {
                    "x": 2274,
                    "y": 1932
                },
                {
                    "x": 2274,
                    "y": 2024
                },
                {
                    "x": 1281,
                    "y": 2024
                }
            ],
            "category": "paragraph",
            "html": "<p id='11' style='font-size:18px'>archical, multi-stage processes for computing features that<br>are even more informative for visual recognition.</p>",
            "id": 11,
            "page": 1,
            "text": "archical, multi-stage processes for computing features that are even more informative for visual recognition."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2033
                },
                {
                    "x": 2275,
                    "y": 2033
                },
                {
                    "x": 2275,
                    "y": 2472
                },
                {
                    "x": 1280,
                    "y": 2472
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='12' style='font-size:18px'>Fukushima's \"neocognitron\" [19], a biologically-<br>inspired hierarchical and shift-invariant model for pattern<br>recognition, was an early attempt at just such a process.<br>The neocognitron, however, lacked a supervised training<br>algorithm. Building on Rumelhart et al. [33], LeCun et<br>al. [26] showed that stochastic gradient descent via back-<br>propagation was effective for training convolutional neural<br>networks (CNNs), a class of models that extend the neocog-<br>nitron.</p>",
            "id": 12,
            "page": 1,
            "text": "Fukushima's \"neocognitron\" , a biologicallyinspired hierarchical and shift-invariant model for pattern recognition, was an early attempt at just such a process. The neocognitron, however, lacked a supervised training algorithm. Building on Rumelhart  , LeCun   showed that stochastic gradient descent via backpropagation was effective for training convolutional neural networks (CNNs), a class of models that extend the neocognitron."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2480
                },
                {
                    "x": 2275,
                    "y": 2480
                },
                {
                    "x": 2275,
                    "y": 2924
                },
                {
                    "x": 1280,
                    "y": 2924
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='13' style='font-size:20px'>CNNs saw heavy use in the 1990s (e.g., [27]), but then<br>fell out of fashion with the rise of support vector machines.<br>In 2012, Krizhevsky et al. [25] rekindled interest in CNNs<br>by showing substantially higher image classification accu-<br>racy on the ImageNet Large Scale Visual Recognition Chal-<br>lenge (ILSVRC) [9, 10]. Their success resulted from train-<br>ing a large CNN on 1.2 million labeled images, together<br>with a few twists on LeCun's CNN (e.g., max(x, 0) rectify-<br>ing non-linearities and \"dropout\" regularization).</p>",
            "id": 13,
            "page": 1,
            "text": "CNNs saw heavy use in the 1990s (e.g., ), but then fell out of fashion with the rise of support vector machines. In 2012, Krizhevsky   rekindled interest in CNNs by showing substantially higher image classification accuracy on the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) . Their success resulted from training a large CNN on 1.2 million labeled images, together with a few twists on LeCun's CNN (e.g., max(x, 0) rectifying non-linearities and \"dropout\" regularization)."
        },
        {
            "bounding_box": [
                {
                    "x": 1332,
                    "y": 2929
                },
                {
                    "x": 2271,
                    "y": 2929
                },
                {
                    "x": 2271,
                    "y": 2976
                },
                {
                    "x": 1332,
                    "y": 2976
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='14' style='font-size:20px'>The significance of the ImageNet result was vigorously</p>",
            "id": 14,
            "page": 1,
            "text": "The significance of the ImageNet result was vigorously"
        },
        {
            "bounding_box": [
                {
                    "x": 59,
                    "y": 908
                },
                {
                    "x": 149,
                    "y": 908
                },
                {
                    "x": 149,
                    "y": 2310
                },
                {
                    "x": 59,
                    "y": 2310
                }
            ],
            "category": "footer",
            "html": "<br><footer id='15' style='font-size:14px'>2014<br>Oct<br>22<br>[cs.CV]<br>arXiv:1311.2524v5</footer>",
            "id": 15,
            "page": 1,
            "text": "2014 Oct 22 [cs.CV] arXiv:1311.2524v5"
        },
        {
            "bounding_box": [
                {
                    "x": 1223,
                    "y": 3056
                },
                {
                    "x": 1250,
                    "y": 3056
                },
                {
                    "x": 1250,
                    "y": 3092
                },
                {
                    "x": 1223,
                    "y": 3092
                }
            ],
            "category": "footer",
            "html": "<footer id='16' style='font-size:16px'>1</footer>",
            "id": 16,
            "page": 1,
            "text": "1"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 307
                },
                {
                    "x": 1199,
                    "y": 307
                },
                {
                    "x": 1199,
                    "y": 503
                },
                {
                    "x": 201,
                    "y": 503
                }
            ],
            "category": "paragraph",
            "html": "<p id='17' style='font-size:14px'>debated during the ILSVRC 2012 workshop. The central<br>issue can be distilled to the following: To what extent do<br>the CNN classification results on ImageNet generalize to<br>object detection results on the PASCAL VOC Challenge?</p>",
            "id": 17,
            "page": 2,
            "text": "debated during the ILSVRC 2012 workshop. The central issue can be distilled to the following: To what extent do the CNN classification results on ImageNet generalize to object detection results on the PASCAL VOC Challenge?"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 515
                },
                {
                    "x": 1200,
                    "y": 515
                },
                {
                    "x": 1200,
                    "y": 907
                },
                {
                    "x": 202,
                    "y": 907
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='18' style='font-size:14px'>We answer this question by bridging the gap between<br>image classification and object detection. This paper is the<br>first to show that a CNN can lead to dramatically higher ob-<br>ject detection performance on PASCAL VOC as compared<br>to systems based on simpler HOG-like features. To achieve<br>this result, we focused on two problems: localizing objects<br>with a deep network and training a high-capacity model<br>with only a small quantity of annotated detection data.</p>",
            "id": 18,
            "page": 2,
            "text": "We answer this question by bridging the gap between image classification and object detection. This paper is the first to show that a CNN can lead to dramatically higher object detection performance on PASCAL VOC as compared to systems based on simpler HOG-like features. To achieve this result, we focused on two problems: localizing objects with a deep network and training a high-capacity model with only a small quantity of annotated detection data."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 921
                },
                {
                    "x": 1199,
                    "y": 921
                },
                {
                    "x": 1199,
                    "y": 1812
                },
                {
                    "x": 201,
                    "y": 1812
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='19' style='font-size:16px'>Unlike image classification, detection requires localiz-<br>ing (likely many) objects within an image. One approach<br>frames localization as a regression problem. However, work<br>from Szegedy et al. [38], concurrent with our own, indi-<br>cates that this strategy may not fare well in practice (they<br>report a mAP of 30.5% on VOC 2007 compared to the<br>58.5% achieved by our method). An alternative is to build a<br>sliding-window detector. CNNs have been used in this way<br>for at least two decades, typically on constrained object cat-<br>egories, such as faces [32, 40] and pedestrians [35]. In order<br>to maintain high spatial resolution, these CNNs typically<br>only have two convolutional and pooling layers. We also<br>considered adopting a sliding-window approach. However,<br>units high up in our network, which has five convolutional<br>layers, have very large receptive fields (195 x 195 pixels)<br>and strides (32 x 32 pixels) in the input image, which makes<br>precise localization within the sliding-window paradigm an<br>open technical challenge.</p>",
            "id": 19,
            "page": 2,
            "text": "Unlike image classification, detection requires localizing (likely many) objects within an image. One approach frames localization as a regression problem. However, work from Szegedy  , concurrent with our own, indicates that this strategy may not fare well in practice (they report a mAP of 30.5% on VOC 2007 compared to the 58.5% achieved by our method). An alternative is to build a sliding-window detector. CNNs have been used in this way for at least two decades, typically on constrained object categories, such as faces  and pedestrians . In order to maintain high spatial resolution, these CNNs typically only have two convolutional and pooling layers. We also considered adopting a sliding-window approach. However, units high up in our network, which has five convolutional layers, have very large receptive fields (195 x 195 pixels) and strides (32 x 32 pixels) in the input image, which makes precise localization within the sliding-window paradigm an open technical challenge."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1821
                },
                {
                    "x": 1199,
                    "y": 1821
                },
                {
                    "x": 1199,
                    "y": 2514
                },
                {
                    "x": 201,
                    "y": 2514
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='20' style='font-size:16px'>Instead, we solve the CNN localization problem by oper-<br>ating within the \"recognition using regions\" paradigm [21],<br>which has been successful for both object detection [39] and<br>semantic segmentation [5]. At test time, our method gener-<br>ates around 2000 category-independent region proposals for<br>the input image, extracts a fixed-length feature vector from<br>each proposal using a CNN, and then classifies each region<br>with category-specific linear SVMs. We use a simple tech-<br>nique (affine image warping) to compute a fixed-size CNN<br>input from each region proposal, regardless of the region's<br>shape. Figure 1 presents an overview of our method and<br>highlights some of our results. Since our system combines<br>region proposals with CNNs, we dub the method R-CNN:<br>Regions with CNN features.</p>",
            "id": 20,
            "page": 2,
            "text": "Instead, we solve the CNN localization problem by operating within the \"recognition using regions\" paradigm , which has been successful for both object detection  and semantic segmentation . At test time, our method generates around 2000 category-independent region proposals for the input image, extracts a fixed-length feature vector from each proposal using a CNN, and then classifies each region with category-specific linear SVMs. We use a simple technique (affine image warping) to compute a fixed-size CNN input from each region proposal, regardless of the region's shape. Figure 1 presents an overview of our method and highlights some of our results. Since our system combines region proposals with CNNs, we dub the method R-CNN: Regions with CNN features."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2523
                },
                {
                    "x": 1200,
                    "y": 2523
                },
                {
                    "x": 1200,
                    "y": 2917
                },
                {
                    "x": 202,
                    "y": 2917
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='21' style='font-size:14px'>In this updated version of this paper, we provide a head-<br>to-head comparison of R-CNN and the recently proposed<br>OverFeat [34] detection system by running R-CNN on the<br>200-class ILSVRC2013 detection dataset. OverFeat uses a<br>sliding-window CNN for detection and until now was the<br>best performing method on ILSVRC2013 detection. We<br>show that R-CNN significantly outperforms OverFeat, with<br>a mAP of 31.4% versus 24.3%.</p>",
            "id": 21,
            "page": 2,
            "text": "In this updated version of this paper, we provide a headto-head comparison of R-CNN and the recently proposed OverFeat  detection system by running R-CNN on the 200-class ILSVRC2013 detection dataset. OverFeat uses a sliding-window CNN for detection and until now was the best performing method on ILSVRC2013 detection. We show that R-CNN significantly outperforms OverFeat, with a mAP of 31.4% versus 24.3%."
        },
        {
            "bounding_box": [
                {
                    "x": 252,
                    "y": 2929
                },
                {
                    "x": 1197,
                    "y": 2929
                },
                {
                    "x": 1197,
                    "y": 2973
                },
                {
                    "x": 252,
                    "y": 2973
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='22' style='font-size:14px'>A second challenge faced in detection is that labeled data</p>",
            "id": 22,
            "page": 2,
            "text": "A second challenge faced in detection is that labeled data"
        },
        {
            "bounding_box": [
                {
                    "x": 1277,
                    "y": 309
                },
                {
                    "x": 2277,
                    "y": 309
                },
                {
                    "x": 2277,
                    "y": 1248
                },
                {
                    "x": 1277,
                    "y": 1248
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='23' style='font-size:16px'>is scarce and the amount currently available is insufficient<br>for training a large CNN. The conventional solution to this<br>problem is to use unsupervised pre-training, followed by su-<br>pervised fine-tuning (e.g., [35]). The second principle con-<br>tribution of this paper is to show that supervised pre-training<br>on a large auxiliary dataset (ILSVRC), followed by domain-<br>specific fine-tuning on a small dataset (PASCAL), is an<br>effective paradigm for learning high-capacity CNNs when<br>data is scarce. In our experiments, fine-tuning for detection<br>improves mAP performance by 8 percentage points. After<br>fine-tuning, our system achieves a mAP of 54% on VOC<br>2010 compared to 33% for the highly-tuned, HOG-based<br>deformable part model (DPM) [17, 20]. We also point read-<br>ers to contemporaneous work by Donahue et al. [12], who<br>show that Krizhevsky's CNN can be used (without fine-<br>tuning) as a blackbox feature extractor, yielding excellent<br>performance on several recognition tasks including scene<br>classification, fine-grained sub-categorization, and domain<br>adaptation.</p>",
            "id": 23,
            "page": 2,
            "text": "is scarce and the amount currently available is insufficient for training a large CNN. The conventional solution to this problem is to use unsupervised pre-training, followed by supervised fine-tuning (e.g., ). The second principle contribution of this paper is to show that supervised pre-training on a large auxiliary dataset (ILSVRC), followed by domainspecific fine-tuning on a small dataset (PASCAL), is an effective paradigm for learning high-capacity CNNs when data is scarce. In our experiments, fine-tuning for detection improves mAP performance by 8 percentage points. After fine-tuning, our system achieves a mAP of 54% on VOC 2010 compared to 33% for the highly-tuned, HOG-based deformable part model (DPM) . We also point readers to contemporaneous work by Donahue  , who show that Krizhevsky's CNN can be used (without finetuning) as a blackbox feature extractor, yielding excellent performance on several recognition tasks including scene classification, fine-grained sub-categorization, and domain adaptation."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 1254
                },
                {
                    "x": 2276,
                    "y": 1254
                },
                {
                    "x": 2276,
                    "y": 1551
                },
                {
                    "x": 1281,
                    "y": 1551
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='24' style='font-size:16px'>Our system is also quite efficient. The only class-specific<br>computations are a reasonably small matrix-vector product<br>and greedy non-maximum suppression. This computational<br>property follows from features that are shared across all cat-<br>egories and that are also two orders of magnitude lower-<br>dimensional than previously used region features (cf. [39]).</p>",
            "id": 24,
            "page": 2,
            "text": "Our system is also quite efficient. The only class-specific computations are a reasonably small matrix-vector product and greedy non-maximum suppression. This computational property follows from features that are shared across all categories and that are also two orders of magnitude lowerdimensional than previously used region features (cf. )."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1552
                },
                {
                    "x": 2277,
                    "y": 1552
                },
                {
                    "x": 2277,
                    "y": 1846
                },
                {
                    "x": 1280,
                    "y": 1846
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='25' style='font-size:14px'>Understanding the failure modes of our approach is also<br>critical for improving it, and SO we report results from the<br>detection analysis tool of Hoiem et al. [23]. As an im-<br>mediate consequence of this analysis, we demonstrate that<br>a simple bounding-box regression method significantly re-<br>duces mislocalizations, which are the dominant error mode.</p>",
            "id": 25,
            "page": 2,
            "text": "Understanding the failure modes of our approach is also critical for improving it, and SO we report results from the detection analysis tool of Hoiem  . As an immediate consequence of this analysis, we demonstrate that a simple bounding-box regression method significantly reduces mislocalizations, which are the dominant error mode."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 1852
                },
                {
                    "x": 2278,
                    "y": 1852
                },
                {
                    "x": 2278,
                    "y": 2147
                },
                {
                    "x": 1281,
                    "y": 2147
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='26' style='font-size:14px'>Before developing technical details, we note that because<br>R-CNN operates on regions it is natural to extend it to the<br>task of semantic segmentation. With minor modifications,<br>we also achieve competitive results on the PASCAL VOC<br>segmentation task, with an average segmentation accuracy<br>of 47.9% on the VOC 2011 test set.</p>",
            "id": 26,
            "page": 2,
            "text": "Before developing technical details, we note that because R-CNN operates on regions it is natural to extend it to the task of semantic segmentation. With minor modifications, we also achieve competitive results on the PASCAL VOC segmentation task, with an average segmentation accuracy of 47.9% on the VOC 2011 test set."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 2190
                },
                {
                    "x": 1968,
                    "y": 2190
                },
                {
                    "x": 1968,
                    "y": 2242
                },
                {
                    "x": 1281,
                    "y": 2242
                }
            ],
            "category": "paragraph",
            "html": "<p id='27' style='font-size:22px'>2. Object detection with R-CNN</p>",
            "id": 27,
            "page": 2,
            "text": "2. Object detection with R-CNN"
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 2274
                },
                {
                    "x": 2276,
                    "y": 2274
                },
                {
                    "x": 2276,
                    "y": 2770
                },
                {
                    "x": 1281,
                    "y": 2770
                }
            ],
            "category": "paragraph",
            "html": "<p id='28' style='font-size:14px'>Our object detection system consists of three modules.<br>The first generates category-independent region proposals.<br>These proposals define the set of candidate detections avail-<br>able to our detector. The second module is a large convo-<br>lutional neural network that extracts a fixed-length feature<br>vector from each region. The third module is a set of class-<br>specific linear SVMs. In this section, we present our design<br>decisions for each module, describe their test-time usage,<br>detail how their parameters are learned, and show detection<br>results on PASCAL VOC 2010-12 and on ILSVRC2013.</p>",
            "id": 28,
            "page": 2,
            "text": "Our object detection system consists of three modules. The first generates category-independent region proposals. These proposals define the set of candidate detections available to our detector. The second module is a large convolutional neural network that extracts a fixed-length feature vector from each region. The third module is a set of classspecific linear SVMs. In this section, we present our design decisions for each module, describe their test-time usage, detail how their parameters are learned, and show detection results on PASCAL VOC 2010-12 and on ILSVRC2013."
        },
        {
            "bounding_box": [
                {
                    "x": 1282,
                    "y": 2800
                },
                {
                    "x": 1655,
                    "y": 2800
                },
                {
                    "x": 1655,
                    "y": 2849
                },
                {
                    "x": 1282,
                    "y": 2849
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='29' style='font-size:20px'>2.1. Module design</p>",
            "id": 29,
            "page": 2,
            "text": "2.1. Module design"
        },
        {
            "bounding_box": [
                {
                    "x": 1282,
                    "y": 2879
                },
                {
                    "x": 2273,
                    "y": 2879
                },
                {
                    "x": 2273,
                    "y": 2979
                },
                {
                    "x": 1282,
                    "y": 2979
                }
            ],
            "category": "paragraph",
            "html": "<p id='30' style='font-size:18px'>Region proposals. A variety of recent papers offer meth-<br>ods for generating category-independent region proposals.</p>",
            "id": 30,
            "page": 2,
            "text": "Region proposals. A variety of recent papers offer methods for generating category-independent region proposals."
        },
        {
            "bounding_box": [
                {
                    "x": 1224,
                    "y": 3055
                },
                {
                    "x": 1252,
                    "y": 3055
                },
                {
                    "x": 1252,
                    "y": 3093
                },
                {
                    "x": 1224,
                    "y": 3093
                }
            ],
            "category": "footer",
            "html": "<footer id='31' style='font-size:14px'>2</footer>",
            "id": 31,
            "page": 2,
            "text": "2"
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 297
                },
                {
                    "x": 1213,
                    "y": 297
                },
                {
                    "x": 1213,
                    "y": 541
                },
                {
                    "x": 203,
                    "y": 541
                }
            ],
            "category": "figure",
            "html": "<figure><img id='32' style='font-size:14px' alt=\"aeroplane bicycle bird car\" data-coord=\"top-left:(203,297); bottom-right:(1213,541)\" /></figure>",
            "id": 32,
            "page": 3,
            "text": "aeroplane bicycle bird car"
        },
        {
            "bounding_box": [
                {
                    "x": 248,
                    "y": 571
                },
                {
                    "x": 1150,
                    "y": 571
                },
                {
                    "x": 1150,
                    "y": 612
                },
                {
                    "x": 248,
                    "y": 612
                }
            ],
            "category": "caption",
            "html": "<caption id='33' style='font-size:18px'>Figure 2: Warped training samples from VOC 2007 train.</caption>",
            "id": 33,
            "page": 3,
            "text": "Figure 2: Warped training samples from VOC 2007 train."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 665
                },
                {
                    "x": 1199,
                    "y": 665
                },
                {
                    "x": 1199,
                    "y": 1110
                },
                {
                    "x": 201,
                    "y": 1110
                }
            ],
            "category": "paragraph",
            "html": "<p id='34' style='font-size:18px'>Examples include: objectness [1], selective search [39],<br>category-independent object proposals [14], constrained<br>parametric min-cuts (CPMC) [5], multi-scale combinatorial<br>grouping [3], and Cire�an et al. [6], who detect mitotic cells<br>by applying a CNN to regularly-spaced square crops, which<br>are a special case of region proposals. While R-CNN is ag-<br>nostic to the particular region proposal method, we use se-<br>lective search to enable a controlled comparison with prior<br>detection work (e.g., [39, 41]).</p>",
            "id": 34,
            "page": 3,
            "text": "Examples include: objectness , selective search , category-independent object proposals , constrained parametric min-cuts (CPMC) , multi-scale combinatorial grouping , and Cire�an  , who detect mitotic cells by applying a CNN to regularly-spaced square crops, which are a special case of region proposals. While R-CNN is agnostic to the particular region proposal method, we use selective search to enable a controlled comparison with prior detection work (e.g., )."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 1142
                },
                {
                    "x": 1199,
                    "y": 1142
                },
                {
                    "x": 1199,
                    "y": 1487
                },
                {
                    "x": 202,
                    "y": 1487
                }
            ],
            "category": "paragraph",
            "html": "<p id='35' style='font-size:18px'>Feature extraction. We extract a 4096-dimensional fea-<br>ture vector from each region proposal using the Caffe [24]<br>implementation of the CNN described by Krizhevsky et<br>al. [25]. Features are computed by forward propagating<br>a mean-subtracted 227 x 227 RGB image through five con-<br>volutional layers and two fully connected layers. We refer<br>readers to [24, 25] for more network architecture details.</p>",
            "id": 35,
            "page": 3,
            "text": "Feature extraction. We extract a 4096-dimensional feature vector from each region proposal using the Caffe  implementation of the CNN described by Krizhevsky  . Features are computed by forward propagating a mean-subtracted 227 x 227 RGB image through five convolutional layers and two fully connected layers. We refer readers to  for more network architecture details."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 1495
                },
                {
                    "x": 1199,
                    "y": 1495
                },
                {
                    "x": 1199,
                    "y": 2139
                },
                {
                    "x": 200,
                    "y": 2139
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='36' style='font-size:16px'>In order to compute features for a region proposal, we<br>must first convert the image data in that region into a form<br>that is compatible with the CNN (its architecture requires<br>inputs of a fixed 227 x 227 pixel size). Of the many possi-<br>ble transformations of our arbitrary-shaped regions, we opt<br>for the simplest. Regardless of the size or aspect ratio of the<br>candidate region, we warp all pixels in a tight bounding box<br>around it to the required size. Prior to warping, we dilate the<br>tight bounding box SO that at the warped size there are ex-<br>actly p pixels of warped image context around the original<br>box (we use p = 16). Figure 2 shows a random sampling<br>of warped training regions. Alternatives to warping are dis-<br>cussed in Appendix A.</p>",
            "id": 36,
            "page": 3,
            "text": "In order to compute features for a region proposal, we must first convert the image data in that region into a form that is compatible with the CNN (its architecture requires inputs of a fixed 227 x 227 pixel size). Of the many possible transformations of our arbitrary-shaped regions, we opt for the simplest. Regardless of the size or aspect ratio of the candidate region, we warp all pixels in a tight bounding box around it to the required size. Prior to warping, we dilate the tight bounding box SO that at the warped size there are exactly p pixels of warped image context around the original box (we use p = 16). Figure 2 shows a random sampling of warped training regions. Alternatives to warping are discussed in Appendix A."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2172
                },
                {
                    "x": 660,
                    "y": 2172
                },
                {
                    "x": 660,
                    "y": 2219
                },
                {
                    "x": 202,
                    "y": 2219
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='37' style='font-size:20px'>2.2. Test-time detection</p>",
            "id": 37,
            "page": 3,
            "text": "2.2. Test-time detection"
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 2251
                },
                {
                    "x": 1200,
                    "y": 2251
                },
                {
                    "x": 1200,
                    "y": 2795
                },
                {
                    "x": 200,
                    "y": 2795
                }
            ],
            "category": "paragraph",
            "html": "<p id='38' style='font-size:16px'>At test time, we run selective search on the test image<br>to extract around 2000 region proposals (we use selective<br>search's \"fast mode\" in all experiments). We warp each<br>proposal and forward propagate it through the CNN in or-<br>der to compute features. Then, for each class, we score<br>each extracted feature vector using the SVM trained for that<br>class. Given all scored regions in an image, we apply a<br>greedy non-maximum suppression (for each class indepen-<br>dently) that rejects a region if it has an intersection-over-<br>union (IoU) overlap with a higher scoring selected region<br>larger than a learned threshold.</p>",
            "id": 38,
            "page": 3,
            "text": "At test time, we run selective search on the test image to extract around 2000 region proposals (we use selective search's \"fast mode\" in all experiments). We warp each proposal and forward propagate it through the CNN in order to compute features. Then, for each class, we score each extracted feature vector using the SVM trained for that class. Given all scored regions in an image, we apply a greedy non-maximum suppression (for each class independently) that rejects a region if it has an intersection-overunion (IoU) overlap with a higher scoring selected region larger than a learned threshold."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2828
                },
                {
                    "x": 1198,
                    "y": 2828
                },
                {
                    "x": 1198,
                    "y": 2976
                },
                {
                    "x": 202,
                    "y": 2976
                }
            ],
            "category": "paragraph",
            "html": "<p id='39' style='font-size:16px'>Run-time analysis. Two properties make detection effi-<br>cient. First, all CNN parameters are shared across all cate-<br>gories. Second, the feature vectors computed by the CNN</p>",
            "id": 39,
            "page": 3,
            "text": "Run-time analysis. Two properties make detection efficient. First, all CNN parameters are shared across all categories. Second, the feature vectors computed by the CNN"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 310
                },
                {
                    "x": 2277,
                    "y": 310
                },
                {
                    "x": 2277,
                    "y": 551
                },
                {
                    "x": 1279,
                    "y": 551
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='40' style='font-size:16px'>are low-dimensional when compared to other common ap-<br>proaches, such as spatial pyramids with bag-of-visual-word<br>encodings. The features used in the UVA detection system<br>[39], for example, are two orders of magnitude larger than<br>ours (360k VS. 4k-dimensional).</p>",
            "id": 40,
            "page": 3,
            "text": "are low-dimensional when compared to other common approaches, such as spatial pyramids with bag-of-visual-word encodings. The features used in the UVA detection system , for example, are two orders of magnitude larger than ours (360k VS. 4k-dimensional)."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 558
                },
                {
                    "x": 2277,
                    "y": 558
                },
                {
                    "x": 2277,
                    "y": 998
                },
                {
                    "x": 1280,
                    "y": 998
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='41' style='font-size:16px'>The result of such sharing is that the time spent com-<br>puting region proposals and features (13s/image on a GPU<br>or 53s/image on a CPU) is amortized over all classes. The<br>only class-specific computations are dot products between<br>features and SVM weights and non-maximum suppression.<br>In practice, all dot products for an image are batched into<br>a single matrix-matrix product. The feature matrix is typi-<br>cally 2000 x 4096 and the SVM weight matrix is 4096 x N,<br>where N is the number of classes.</p>",
            "id": 41,
            "page": 3,
            "text": "The result of such sharing is that the time spent computing region proposals and features (13s/image on a GPU or 53s/image on a CPU) is amortized over all classes. The only class-specific computations are dot products between features and SVM weights and non-maximum suppression. In practice, all dot products for an image are batched into a single matrix-matrix product. The feature matrix is typically 2000 x 4096 and the SVM weight matrix is 4096 x N, where N is the number of classes."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1006
                },
                {
                    "x": 2277,
                    "y": 1006
                },
                {
                    "x": 2277,
                    "y": 1498
                },
                {
                    "x": 1280,
                    "y": 1498
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='42' style='font-size:18px'>This analysis shows that R-CNN can scale to thousands<br>of object classes without resorting to approximate tech-<br>niques, such as hashing. Even if there were 100k classes,<br>the resulting matrix multiplication takes only 10 seconds on<br>a modern multi-core CPU. This efficiency is not merely the<br>result of using region proposals and shared features. The<br>UVA system, due to its high-dimensional features, would<br>be two orders of magnitude slower while requiring 134GB<br>of memory just to store 100k linear predictors, compared to<br>just 1.5GB for our lower-dimensional features.</p>",
            "id": 42,
            "page": 3,
            "text": "This analysis shows that R-CNN can scale to thousands of object classes without resorting to approximate techniques, such as hashing. Even if there were 100k classes, the resulting matrix multiplication takes only 10 seconds on a modern multi-core CPU. This efficiency is not merely the result of using region proposals and shared features. The UVA system, due to its high-dimensional features, would be two orders of magnitude slower while requiring 134GB of memory just to store 100k linear predictors, compared to just 1.5GB for our lower-dimensional features."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1503
                },
                {
                    "x": 2278,
                    "y": 1503
                },
                {
                    "x": 2278,
                    "y": 1848
                },
                {
                    "x": 1280,
                    "y": 1848
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='43' style='font-size:16px'>It is also interesting to contrast R-CNN with the recent<br>work from Dean et al. on scalable detection using DPMs<br>and hashing [8]. They report a mAP of around 16% on VOC<br>2007 at a run-time of 5 minutes per image when introducing<br>10k distractor classes. With our approach, 10k detectors can<br>run in about a minute on a CPU, and because no approxi-<br>mations are made mAP would remain at 59% (Section 3.2).</p>",
            "id": 43,
            "page": 3,
            "text": "It is also interesting to contrast R-CNN with the recent work from Dean  on scalable detection using DPMs and hashing . They report a mAP of around 16% on VOC 2007 at a run-time of 5 minutes per image when introducing 10k distractor classes. With our approach, 10k detectors can run in about a minute on a CPU, and because no approximations are made mAP would remain at 59% (Section 3.2)."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 1877
                },
                {
                    "x": 1541,
                    "y": 1877
                },
                {
                    "x": 1541,
                    "y": 1925
                },
                {
                    "x": 1281,
                    "y": 1925
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='44' style='font-size:22px'>2.3. Training</p>",
            "id": 44,
            "page": 3,
            "text": "2.3. Training"
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1955
                },
                {
                    "x": 2278,
                    "y": 1955
                },
                {
                    "x": 2278,
                    "y": 2451
                },
                {
                    "x": 1280,
                    "y": 2451
                }
            ],
            "category": "paragraph",
            "html": "<p id='45' style='font-size:18px'>Supervised pre-training. We discriminatively pre-trained<br>the CNN on a large auxiliary dataset (ILSVRC2012 clas-<br>sification) using image-level annotations only (bounding-<br>box labels are not available for this data). Pre-training<br>was performed using the open source Caffe CNN library<br>[24]. In brief, our CNN nearly matches the performance<br>of Krizhevsky et al. [25], obtaining a top-1 error rate 2.2<br>percentage points higher on the ILSVRC2012 classification<br>validation set. This discrepancy is due to simplifications in<br>the training process.</p>",
            "id": 45,
            "page": 3,
            "text": "Supervised pre-training. We discriminatively pre-trained the CNN on a large auxiliary dataset (ILSVRC2012 classification) using image-level annotations only (boundingbox labels are not available for this data). Pre-training was performed using the open source Caffe CNN library . In brief, our CNN nearly matches the performance of Krizhevsky  , obtaining a top-1 error rate 2.2 percentage points higher on the ILSVRC2012 classification validation set. This discrepancy is due to simplifications in the training process."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2476
                },
                {
                    "x": 2277,
                    "y": 2476
                },
                {
                    "x": 2277,
                    "y": 2977
                },
                {
                    "x": 1280,
                    "y": 2977
                }
            ],
            "category": "paragraph",
            "html": "<p id='46' style='font-size:20px'>Domain-specific fine-tuning. To adapt our CNN to the<br>new task (detection) and the new domain (warped proposal<br>windows), we continue stochastic gradient descent (SGD)<br>training of the CNN parameters using only warped region<br>proposals. Aside from replacing the CNN's ImageNet-<br>specific 1000-way classification layer with a randomly ini-<br>tialized (N + 1)-way classification layer (where N is the<br>number of object classes, plus 1 for background), the CNN<br>architecture is unchanged. For VOC, N = 20 and for<br>ILSVRC2013, N = 200. We treat all region proposals with</p>",
            "id": 46,
            "page": 3,
            "text": "Domain-specific fine-tuning. To adapt our CNN to the new task (detection) and the new domain (warped proposal windows), we continue stochastic gradient descent (SGD) training of the CNN parameters using only warped region proposals. Aside from replacing the CNN's ImageNetspecific 1000-way classification layer with a randomly initialized (N + 1)-way classification layer (where N is the number of object classes, plus 1 for background), the CNN architecture is unchanged. For VOC, N = 20 and for ILSVRC2013, N = 200. We treat all region proposals with"
        },
        {
            "bounding_box": [
                {
                    "x": 1226,
                    "y": 3056
                },
                {
                    "x": 1250,
                    "y": 3056
                },
                {
                    "x": 1250,
                    "y": 3089
                },
                {
                    "x": 1226,
                    "y": 3089
                }
            ],
            "category": "footer",
            "html": "<footer id='47' style='font-size:14px'>3</footer>",
            "id": 47,
            "page": 3,
            "text": "3"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 305
                },
                {
                    "x": 1199,
                    "y": 305
                },
                {
                    "x": 1199,
                    "y": 753
                },
                {
                    "x": 201,
                    "y": 753
                }
            ],
            "category": "paragraph",
            "html": "<p id='48' style='font-size:14px'>≥ 0.5 IoU overlap with a ground-truth box as positives for<br>that box's class and the rest as negatives. We start SGD at<br>a learning rate of 0.001 (1/10th of the initial pre-training<br>rate), which allows fine-tuning to make progress while not<br>clobbering the initialization. In each SGD iteration, we uni-<br>formly sample 32 positive windows (over all classes) and<br>96 background windows to construct a mini-batch of size<br>128. We bias the sampling towards positive windows be-<br>cause they are extremely rare compared to background.</p>",
            "id": 48,
            "page": 4,
            "text": "≥ 0.5 IoU overlap with a ground-truth box as positives for that box's class and the rest as negatives. We start SGD at a learning rate of 0.001 (1/10th of the initial pre-training rate), which allows fine-tuning to make progress while not clobbering the initialization. In each SGD iteration, we uniformly sample 32 positive windows (over all classes) and 96 background windows to construct a mini-batch of size 128. We bias the sampling towards positive windows because they are extremely rare compared to background."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 780
                },
                {
                    "x": 1199,
                    "y": 780
                },
                {
                    "x": 1199,
                    "y": 1477
                },
                {
                    "x": 200,
                    "y": 1477
                }
            ],
            "category": "paragraph",
            "html": "<p id='49' style='font-size:14px'>Object category classifiers. Consider training a binary<br>classifier to detect cars. It's clear that an image region<br>tightly enclosing a car should be a positive example. Simi-<br>larly, it's clear that a background region, which has nothing<br>to do with cars, should be a negative example. Less clear<br>is how to label a region that partially overlaps a car. We re-<br>solve this issue with an IoU overlap threshold, below which<br>regions are defined as negatives. The overlap threshold, 0.3,<br>was selected by a grid search over {0, 0.1, · · · , 0.5} on a<br>validation set. We found that selecting this threshold care-<br>fully is important. Setting it to 0.5, as in [39], decreased<br>mAP by 5 points. Similarly, setting it to 0 decreased mAP<br>by 4 points. Positive examples are defined simply to be the<br>ground-truth bounding boxes for each class.</p>",
            "id": 49,
            "page": 4,
            "text": "Object category classifiers. Consider training a binary classifier to detect cars. It's clear that an image region tightly enclosing a car should be a positive example. Similarly, it's clear that a background region, which has nothing to do with cars, should be a negative example. Less clear is how to label a region that partially overlaps a car. We resolve this issue with an IoU overlap threshold, below which regions are defined as negatives. The overlap threshold, 0.3, was selected by a grid search over {0, 0.1, · · · , 0.5} on a validation set. We found that selecting this threshold carefully is important. Setting it to 0.5, as in , decreased mAP by 5 points. Similarly, setting it to 0 decreased mAP by 4 points. Positive examples are defined simply to be the ground-truth bounding boxes for each class."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1480
                },
                {
                    "x": 1200,
                    "y": 1480
                },
                {
                    "x": 1200,
                    "y": 1775
                },
                {
                    "x": 201,
                    "y": 1775
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='50' style='font-size:16px'>Once features are extracted and training labels are ap-<br>plied, we optimize one linear SVM per class. Since the<br>training data is too large to fit in memory, we adopt the<br>standard hard negative mining method [17, 37]. Hard neg-<br>ative mining converges quickly and in practice mAP stops<br>increasing after only a single pass over all images.</p>",
            "id": 50,
            "page": 4,
            "text": "Once features are extracted and training labels are applied, we optimize one linear SVM per class. Since the training data is too large to fit in memory, we adopt the standard hard negative mining method . Hard negative mining converges quickly and in practice mAP stops increasing after only a single pass over all images."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1780
                },
                {
                    "x": 1199,
                    "y": 1780
                },
                {
                    "x": 1199,
                    "y": 2024
                },
                {
                    "x": 201,
                    "y": 2024
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='51' style='font-size:18px'>In Appendix B we discuss why the positive and negative<br>examples are defined differently in fine-tuning versus SVM<br>training. We also discuss the trade-offs involved in training<br>detection SVMs rather than simply using the outputs from<br>the final softmax layer of the fine-tuned CNN.</p>",
            "id": 51,
            "page": 4,
            "text": "In Appendix B we discuss why the positive and negative examples are defined differently in fine-tuning versus SVM training. We also discuss the trade-offs involved in training detection SVMs rather than simply using the outputs from the final softmax layer of the fine-tuned CNN."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2052
                },
                {
                    "x": 959,
                    "y": 2052
                },
                {
                    "x": 959,
                    "y": 2100
                },
                {
                    "x": 202,
                    "y": 2100
                }
            ],
            "category": "paragraph",
            "html": "<p id='52' style='font-size:20px'>2.4. Results on PASCAL VOC 2010-12</p>",
            "id": 52,
            "page": 4,
            "text": "2.4. Results on PASCAL VOC 2010-12"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2130
                },
                {
                    "x": 1199,
                    "y": 2130
                },
                {
                    "x": 1199,
                    "y": 2524
                },
                {
                    "x": 202,
                    "y": 2524
                }
            ],
            "category": "paragraph",
            "html": "<p id='53' style='font-size:14px'>Following the PASCAL VOC best practices [15], we<br>validated all design decisions and hyperparameters on the<br>VOC 2007 dataset (Section 3.2). For final results on the<br>VOC 2010-12 datasets, we fine-tuned the CNN on VOC<br>2012 train and optimized our detection SVMs on VOC 2012<br>trainval. We submitted test results to the evaluation server<br>only once for each of the two major algorithm variants (with<br>and without bounding-box regression).</p>",
            "id": 53,
            "page": 4,
            "text": "Following the PASCAL VOC best practices , we validated all design decisions and hyperparameters on the VOC 2007 dataset (Section 3.2). For final results on the VOC 2010-12 datasets, we fine-tuned the CNN on VOC 2012 train and optimized our detection SVMs on VOC 2012 trainval. We submitted test results to the evaluation server only once for each of the two major algorithm variants (with and without bounding-box regression)."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2531
                },
                {
                    "x": 1200,
                    "y": 2531
                },
                {
                    "x": 1200,
                    "y": 2978
                },
                {
                    "x": 202,
                    "y": 2978
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='54' style='font-size:14px'>Table 1 shows complete results on VOC 2010. We com-<br>pare our method against four strong baselines, including<br>SegDPM [18], which combines DPM detectors with the<br>output of a semantic segmentation system [4] and uses ad-<br>ditional inter-detector context and image-classifier rescor-<br>ing. The most germane comparison is to the UVA system<br>from Uijlings et al. [39], since our systems use the same re-<br>gion proposal algorithm. To classify regions, their method<br>builds a four-level spatial pyramid and populates it with</p>",
            "id": 54,
            "page": 4,
            "text": "Table 1 shows complete results on VOC 2010. We compare our method against four strong baselines, including SegDPM , which combines DPM detectors with the output of a semantic segmentation system  and uses additional inter-detector context and image-classifier rescoring. The most germane comparison is to the UVA system from Uijlings  , since our systems use the same region proposal algorithm. To classify regions, their method builds a four-level spatial pyramid and populates it with"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 306
                },
                {
                    "x": 2277,
                    "y": 306
                },
                {
                    "x": 2277,
                    "y": 704
                },
                {
                    "x": 1279,
                    "y": 704
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='55' style='font-size:16px'>densely sampled SIFT, Extended OpponentSIFT, and RGB-<br>SIFT descriptors, each vector quantized with 4000-word<br>codebooks. Classification is performed with a histogram<br>intersection kernel SVM. Compared to their multi-feature,<br>non-linear kernel SVM approach, we achieve a large im-<br>provement in mAP, from 35.1% to 53.7% mAP, while also<br>being much faster (Section 2.2). Our method achieves sim-<br>ilar performance (53.3% mAP) on VOC 2011/12 test.</p>",
            "id": 55,
            "page": 4,
            "text": "densely sampled SIFT, Extended OpponentSIFT, and RGBSIFT descriptors, each vector quantized with 4000-word codebooks. Classification is performed with a histogram intersection kernel SVM. Compared to their multi-feature, non-linear kernel SVM approach, we achieve a large improvement in mAP, from 35.1% to 53.7% mAP, while also being much faster (Section 2.2). Our method achieves similar performance (53.3% mAP) on VOC 2011/12 test."
        },
        {
            "bounding_box": [
                {
                    "x": 1282,
                    "y": 752
                },
                {
                    "x": 2035,
                    "y": 752
                },
                {
                    "x": 2035,
                    "y": 799
                },
                {
                    "x": 1282,
                    "y": 799
                }
            ],
            "category": "paragraph",
            "html": "<p id='56' style='font-size:20px'>2.5. Results on ILSVRC2013 detection</p>",
            "id": 56,
            "page": 4,
            "text": "2.5. Results on ILSVRC2013 detection"
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 834
                },
                {
                    "x": 2277,
                    "y": 834
                },
                {
                    "x": 2277,
                    "y": 1132
                },
                {
                    "x": 1278,
                    "y": 1132
                }
            ],
            "category": "paragraph",
            "html": "<p id='57' style='font-size:14px'>We ran R-CNN on the 200-class ILSVRC2013 detection<br>dataset using the same system hyperparameters that we used<br>for PASCAL VOC. We followed the same protocol of sub-<br>mitting test results to the ILSVRC2013 evaluation server<br>only twice, once with and once without bounding-box re-<br>gression.</p>",
            "id": 57,
            "page": 4,
            "text": "We ran R-CNN on the 200-class ILSVRC2013 detection dataset using the same system hyperparameters that we used for PASCAL VOC. We followed the same protocol of submitting test results to the ILSVRC2013 evaluation server only twice, once with and once without bounding-box regression."
        },
        {
            "bounding_box": [
                {
                    "x": 1277,
                    "y": 1140
                },
                {
                    "x": 2277,
                    "y": 1140
                },
                {
                    "x": 2277,
                    "y": 1736
                },
                {
                    "x": 1277,
                    "y": 1736
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='58' style='font-size:14px'>Figure 3 compares R-CNN to the entries in the ILSVRC<br>2013 competition and to the post-competition OverFeat re-<br>sult [34]. R-CNN achieves a mAP of 31.4%, which is sig-<br>nificantly ahead of the second-best result of 24.3% from<br>OverFeat. To give a sense of the AP distribution over<br>classes, box plots are also presented and a table of per-<br>class APs follows at the end of the paper in Table 8. Most<br>of the competing submissions (OverFeat, NEC-MU, UvA-<br>Euvision, Toronto A, and UIUC-IFP) used convolutional<br>neural networks, indicating that there is significant nuance<br>in how CNNs can be applied to object detection, leading to<br>greatly varying outcomes.</p>",
            "id": 58,
            "page": 4,
            "text": "Figure 3 compares R-CNN to the entries in the ILSVRC 2013 competition and to the post-competition OverFeat result . R-CNN achieves a mAP of 31.4%, which is significantly ahead of the second-best result of 24.3% from OverFeat. To give a sense of the AP distribution over classes, box plots are also presented and a table of perclass APs follows at the end of the paper in Table 8. Most of the competing submissions (OverFeat, NEC-MU, UvAEuvision, Toronto A, and UIUC-IFP) used convolutional neural networks, indicating that there is significant nuance in how CNNs can be applied to object detection, leading to greatly varying outcomes."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1744
                },
                {
                    "x": 2277,
                    "y": 1744
                },
                {
                    "x": 2277,
                    "y": 1891
                },
                {
                    "x": 1280,
                    "y": 1891
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='59' style='font-size:14px'>In Section 4, we give an overview of the ILSVRC2013<br>detection dataset and provide details about choices that we<br>made when running R-CNN on it.</p>",
            "id": 59,
            "page": 4,
            "text": "In Section 4, we give an overview of the ILSVRC2013 detection dataset and provide details about choices that we made when running R-CNN on it."
        },
        {
            "bounding_box": [
                {
                    "x": 1282,
                    "y": 1954
                },
                {
                    "x": 2250,
                    "y": 1954
                },
                {
                    "x": 2250,
                    "y": 2092
                },
                {
                    "x": 1282,
                    "y": 2092
                }
            ],
            "category": "paragraph",
            "html": "<p id='60' style='font-size:22px'>3. Visualization, ablation, and modes of error<br>3.1. Visualizing learned features</p>",
            "id": 60,
            "page": 4,
            "text": "3. Visualization, ablation, and modes of error 3.1. Visualizing learned features"
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2124
                },
                {
                    "x": 2276,
                    "y": 2124
                },
                {
                    "x": 2276,
                    "y": 2470
                },
                {
                    "x": 1280,
                    "y": 2470
                }
            ],
            "category": "paragraph",
            "html": "<p id='61' style='font-size:16px'>First-layer filters can be visualized directly and are easy<br>to understand [25]. They capture oriented edges and oppo-<br>nent colors. Understanding the subsequent layers is more<br>challenging. Zeiler and Fergus present a visually attrac-<br>tive deconvolutional approach in [42]. We propose a simple<br>(and complementary) non-parametric method that directly<br>shows what the network learned.</p>",
            "id": 61,
            "page": 4,
            "text": "First-layer filters can be visualized directly and are easy to understand . They capture oriented edges and opponent colors. Understanding the subsequent layers is more challenging. Zeiler and Fergus present a visually attractive deconvolutional approach in . We propose a simple (and complementary) non-parametric method that directly shows what the network learned."
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 2478
                },
                {
                    "x": 2278,
                    "y": 2478
                },
                {
                    "x": 2278,
                    "y": 2978
                },
                {
                    "x": 1279,
                    "y": 2978
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='62' style='font-size:14px'>The idea is to single out a particular unit (feature) in the<br>network and use it as if it were an object detector in its own<br>right. That is, we compute the unit's activations on a large<br>set of held-out region proposals (about 10 million), sort the<br>proposals from highest to lowest activation, perform non-<br>maximum suppression, and then display the top-scoring re-<br>gions. Our method lets the selected unit \"speak for itself\"<br>by showing exactly which inputs it fires on. We avoid aver-<br>aging in order to see different visual modes and gain insight<br>into the invariances computed by the unit.</p>",
            "id": 62,
            "page": 4,
            "text": "The idea is to single out a particular unit (feature) in the network and use it as if it were an object detector in its own right. That is, we compute the unit's activations on a large set of held-out region proposals (about 10 million), sort the proposals from highest to lowest activation, perform nonmaximum suppression, and then display the top-scoring regions. Our method lets the selected unit \"speak for itself\" by showing exactly which inputs it fires on. We avoid averaging in order to see different visual modes and gain insight into the invariances computed by the unit."
        },
        {
            "bounding_box": [
                {
                    "x": 1225,
                    "y": 3056
                },
                {
                    "x": 1251,
                    "y": 3056
                },
                {
                    "x": 1251,
                    "y": 3088
                },
                {
                    "x": 1225,
                    "y": 3088
                }
            ],
            "category": "footer",
            "html": "<footer id='63' style='font-size:14px'>4</footer>",
            "id": 63,
            "page": 4,
            "text": "4"
        },
        {
            "bounding_box": [
                {
                    "x": 206,
                    "y": 298
                },
                {
                    "x": 2274,
                    "y": 298
                },
                {
                    "x": 2274,
                    "y": 639
                },
                {
                    "x": 206,
                    "y": 639
                }
            ],
            "category": "paragraph",
            "html": "<p id='64' style='font-size:16px'>VOC 2010 test aero bike bird boat bottle bus car cat chair COW table dog horse mbike person plant sheep sofa train tv mAP<br>DPM v5 [20]† 49.2 53.8 13.1 15.3 35.5 53.4 49.7 27.0 17.2 28.8 14.7 17.8 46.4 51.2 47.7 10.8 34.2 20.7 43.8 38.3 33.4<br>UVA [39] 56.2 42.4 15.3 12.6 21.8 49.3 36.8 46.1 12.9 32.1 30.0 36.5 43.5 52.9 32.9 15.3 41.1 31.8 47.0 44.8 35.1<br>Regionlets [41] 65.0 48.9 25.9 24.6 24.5 56.1 54.5 51.2 17.0 28.9 30.2 35.8 40.2 55.7 43.5 14.3 43.9 32.6 54.0 45.9 39.7<br>SegDPM [18]+ 61.4 53.4 25.6 25.2 35.5 51.7 50.6 50.8 19.3 33.8 26.8 40.4 48.3 54.4 47.1 14.8 38.7 35.0 52.8 43.1 40.4<br>R-CNN 67.1 64.1 46.7 32.0 30.5 56.4 57.2 65.9 27.0 47.3 40.9 66.6 57.8 65.9 53.6 26.7 56.5 38.1 52.8 50.2 50.2<br>R-CNN BB 71.8 65.8 53.0 36.8 35.9 59.7 60.0 69.9 27.9 50.6 41.4 70.0 62.0 69.0 58.1 29.5 59.4 39.3 61.2 52.4 53.7</p>",
            "id": 64,
            "page": 5,
            "text": "VOC 2010 test aero bike bird boat bottle bus car cat chair COW table dog horse mbike person plant sheep sofa train tv mAP DPM v5 † 49.2 53.8 13.1 15.3 35.5 53.4 49.7 27.0 17.2 28.8 14.7 17.8 46.4 51.2 47.7 10.8 34.2 20.7 43.8 38.3 33.4 UVA  56.2 42.4 15.3 12.6 21.8 49.3 36.8 46.1 12.9 32.1 30.0 36.5 43.5 52.9 32.9 15.3 41.1 31.8 47.0 44.8 35.1 Regionlets  65.0 48.9 25.9 24.6 24.5 56.1 54.5 51.2 17.0 28.9 30.2 35.8 40.2 55.7 43.5 14.3 43.9 32.6 54.0 45.9 39.7 SegDPM + 61.4 53.4 25.6 25.2 35.5 51.7 50.6 50.8 19.3 33.8 26.8 40.4 48.3 54.4 47.1 14.8 38.7 35.0 52.8 43.1 40.4 R-CNN 67.1 64.1 46.7 32.0 30.5 56.4 57.2 65.9 27.0 47.3 40.9 66.6 57.8 65.9 53.6 26.7 56.5 38.1 52.8 50.2 50.2 R-CNN BB 71.8 65.8 53.0 36.8 35.9 59.7 60.0 69.9 27.9 50.6 41.4 70.0 62.0 69.0 58.1 29.5 59.4 39.3 61.2 52.4 53.7"
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 657
                },
                {
                    "x": 2277,
                    "y": 657
                },
                {
                    "x": 2277,
                    "y": 794
                },
                {
                    "x": 203,
                    "y": 794
                }
            ],
            "category": "caption",
            "html": "<br><caption id='65' style='font-size:20px'>Table 1: Detection average precision (%) on VOC 2010 test. R-CNN is most directly comparable to UVA and Regionlets since all<br>methods use selective search region proposals. Bounding-box regression (BB) is described in Section C. At publication time, SegDPM<br>was the top-performer on the PASCAL VOC leaderboard. +DPM and SegDPM use context rescoring not used by the other methods.</caption>",
            "id": 65,
            "page": 5,
            "text": "Table 1: Detection average precision (%) on VOC 2010 test. R-CNN is most directly comparable to UVA and Regionlets since all methods use selective search region proposals. Bounding-box regression (BB) is described in Section C. At publication time, SegDPM was the top-performer on the PASCAL VOC leaderboard. +DPM and SegDPM use context rescoring not used by the other methods."
        },
        {
            "bounding_box": [
                {
                    "x": 209,
                    "y": 853
                },
                {
                    "x": 2249,
                    "y": 853
                },
                {
                    "x": 2249,
                    "y": 1602
                },
                {
                    "x": 209,
                    "y": 1602
                }
            ],
            "category": "figure",
            "html": "<figure><img id='66' style='font-size:14px' alt=\"ILSVRC2013 detection test set mAP ILSVRC2013 detection test set class AP box plots\n100\n*R-CNN BB 31.4%\n% 90\n*OverFeat (2) 24.3% .m 80\n(AP)\nUvA-Euvision 22.6% 70\n60\nprecision\n*NEC-MU 20.9%\n50\n*OverFeat (1) 19.4%\n40\nToronto A 11.5% 30\naverage\n20\nSYSU_Vision 10.5%\n10\nGPU_UCLA 9.8%\n0\nUvA-Euvision\n(1)\nDelta 6.1%\nGPU_UCLA\n*NEC-MU\nUIUC-IFP\ncompetition result BB\n*OverFeat\nA\n*R-CNN\nUIUC-IFP 1.0% post competition result Vision\nToronto\nSYSU\nDelta\n0 20 40 60 80 100\nmean average precision (mAP) in %\" data-coord=\"top-left:(209,853); bottom-right:(2249,1602)\" /></figure>",
            "id": 66,
            "page": 5,
            "text": "ILSVRC2013 detection test set mAP ILSVRC2013 detection test set class AP box plots 100 *R-CNN BB 31.4% % 90 *OverFeat (2) 24.3% .m 80 (AP) UvA-Euvision 22.6% 70 60 precision *NEC-MU 20.9% 50 *OverFeat (1) 19.4% 40 Toronto A 11.5% 30 average 20 SYSU_Vision 10.5% 10 GPU_UCLA 9.8% 0 UvA-Euvision (1) Delta 6.1% GPU_UCLA *NEC-MU UIUC-IFP competition result BB *OverFeat A *R-CNN UIUC-IFP 1.0% post competition result Vision Toronto SYSU Delta 0 20 40 60 80 100 mean average precision (mAP) in %"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1633
                },
                {
                    "x": 2277,
                    "y": 1633
                },
                {
                    "x": 2277,
                    "y": 1911
                },
                {
                    "x": 201,
                    "y": 1911
                }
            ],
            "category": "caption",
            "html": "<caption id='67' style='font-size:18px'>Figure 3: (Left) Mean average precision on the ILSVRC2013 detection test set. Methods preceeded by * use outside training data<br>(images and labels from the ILSVRC classification dataset in all cases). (Right) Box plots for the 200 average precision values per<br>method. A box plot for the post-competition OverFeat result is not shown because per-class APs are not yet available (per-class APs for<br>R-CNN are in Table 8 and also included in the tech report source uploaded to arXiv.org; see R-CNN-ILSVRC2013-APs · txt). The red<br>line marks the median AP, the box bottom and top are the 25th and 75th percentiles. The whiskers extend to the min and max AP of each<br>method. Each AP is plotted as a green dot over the whiskers (best viewed digitally with zoom).</caption>",
            "id": 67,
            "page": 5,
            "text": "Figure 3: (Left) Mean average precision on the ILSVRC2013 detection test set. Methods preceeded by * use outside training data (images and labels from the ILSVRC classification dataset in all cases). (Right) Box plots for the 200 average precision values per method. A box plot for the post-competition OverFeat result is not shown because per-class APs are not yet available (per-class APs for R-CNN are in Table 8 and also included in the tech report source uploaded to arXiv.org; see R-CNN-ILSVRC2013-APs · txt). The red line marks the median AP, the box bottom and top are the 25th and 75th percentiles. The whiskers extend to the min and max AP of each method. Each AP is plotted as a green dot over the whiskers (best viewed digitally with zoom)."
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 1954
                },
                {
                    "x": 2274,
                    "y": 1954
                },
                {
                    "x": 2274,
                    "y": 2766
                },
                {
                    "x": 204,
                    "y": 2766
                }
            ],
            "category": "figure",
            "html": "<figure><img id='68' style='font-size:14px' alt=\"1.0 1.0 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9\nAHMA\n �\n1.0 0.9 0.9 0.8 0.8 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.6 0.6\n1.0 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7■ 0.7 0.7 0.7 0.7 0.6 0.6\nuv Terns 0.7 0.7 0.7 0.7\n心\n1.0 0.9 0.8 0.814 0.8 0.7 0.7L 0.7 0.7 0.7 0.7\nEURODATA OMAT and 2006 eeds\nRESTAUR\nWHYDOE\n2006\n2006 2005 RESTAUR\nSOUP\ncom.au\n9000 Saunier\nO\n1.0 1.0 0.9 0.9 0.9 0.8 0.8 0.8 0.85 0.8 0.8 0.8 0.8 0.8 0.8 0.8\n9-01\n빠 999\n1.0 0.9 0.8 0.8 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\" data-coord=\"top-left:(204,1954); bottom-right:(2274,2766)\" /></figure>",
            "id": 68,
            "page": 5,
            "text": "1.0 1.0 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 0.9 AHMA  � 1.0 0.9 0.9 0.8 0.8 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.6 0.6 1.0 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7■ 0.7 0.7 0.7 0.7 0.6 0.6 uv Terns 0.7 0.7 0.7 0.7 心 1.0 0.9 0.8 0.814 0.8 0.7 0.7L 0.7 0.7 0.7 0.7 EURODATA OMAT and 2006 eeds RESTAUR WHYDOE 2006 2006 2005 RESTAUR SOUP com.au 9000 Saunier O 1.0 1.0 0.9 0.9 0.9 0.8 0.8 0.8 0.85 0.8 0.8 0.8 0.8 0.8 0.8 0.8 9-01 빠 999 1.0 0.9 0.8 0.8 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7"
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 2779
                },
                {
                    "x": 2272,
                    "y": 2779
                },
                {
                    "x": 2272,
                    "y": 2875
                },
                {
                    "x": 203,
                    "y": 2875
                }
            ],
            "category": "caption",
            "html": "<br><caption id='69' style='font-size:20px'>Figure 4: Top regions for six pool5 units. Receptive fields and activation values are drawn in white. Some units are aligned to concepts,<br>such as people (row 1) or text (4). Other units capture texture and material properties, such as dot arrays (2) and specular reflections (6).</caption>",
            "id": 69,
            "page": 5,
            "text": "Figure 4: Top regions for six pool5 units. Receptive fields and activation values are drawn in white. Some units are aligned to concepts, such as people (row 1) or text (4). Other units capture texture and material properties, such as dot arrays (2) and specular reflections (6)."
        },
        {
            "bounding_box": [
                {
                    "x": 1225,
                    "y": 3054
                },
                {
                    "x": 1253,
                    "y": 3054
                },
                {
                    "x": 1253,
                    "y": 3092
                },
                {
                    "x": 1225,
                    "y": 3092
                }
            ],
            "category": "footer",
            "html": "<footer id='70' style='font-size:22px'>5</footer>",
            "id": 70,
            "page": 5,
            "text": "5"
        },
        {
            "bounding_box": [
                {
                    "x": 207,
                    "y": 292
                },
                {
                    "x": 2270,
                    "y": 292
                },
                {
                    "x": 2270,
                    "y": 830
                },
                {
                    "x": 207,
                    "y": 830
                }
            ],
            "category": "table",
            "html": "<table id='71' style='font-size:14px'><tr><td>VOC 2007 test</td><td>aero</td><td>bike</td><td>bird</td><td>boat</td><td>bottle</td><td>bus</td><td>car</td><td>cat</td><td>chair</td><td>COW</td><td>table</td><td>dog</td><td>horse</td><td>mbike</td><td>person</td><td>plant</td><td>sheep</td><td>sofa</td><td>train tv</td><td></td><td>mAP</td></tr><tr><td>R-CNN pool5</td><td>51.8</td><td>60.2</td><td>36.4</td><td>27.8</td><td>23.2</td><td>52.8</td><td>60.6</td><td>49.2</td><td>18.3</td><td>47.8</td><td>44.3</td><td>40.8</td><td>56.6</td><td>58.7</td><td>42.4</td><td>23.4</td><td>46.1</td><td>36.7</td><td>51.3</td><td>55.7</td><td>44.2</td></tr><tr><td>R-CNN fc6</td><td>59.3</td><td>61.8</td><td>43.1</td><td>34.0</td><td>25.1</td><td>53.1</td><td>60.6</td><td>52.8</td><td>21.7</td><td>47.8</td><td>42.7</td><td>47.8</td><td>52.5</td><td>58.5</td><td>44.6</td><td>25.6</td><td>48.3</td><td>34.0</td><td>53.1</td><td>58.0</td><td>46.2</td></tr><tr><td>R-CNN fc7</td><td>57.6</td><td>57.9</td><td>38.5</td><td>31.8</td><td>23.7</td><td>51.2</td><td>58.9</td><td>51.4</td><td>20.0</td><td>50.5</td><td>40.9</td><td>46.0</td><td>51.6</td><td>55.9</td><td>43.3</td><td>23.3</td><td>48.1</td><td>35.3</td><td>51.0</td><td>57.4</td><td>44.7</td></tr><tr><td>R-CNN FT pool5</td><td>58.2</td><td>63.3</td><td>37.9</td><td>27.6</td><td>26.1</td><td>54.1</td><td>66.9</td><td>51.4</td><td>26.7</td><td>55.5</td><td>43.4</td><td>43.1</td><td>57.7</td><td>59.0</td><td>45.8</td><td>28.1</td><td>50.8</td><td>40.6</td><td>53.1</td><td>56.4</td><td>47.3</td></tr><tr><td>R-CNN FT fc6</td><td>63.5</td><td>66.0</td><td>47.9</td><td>37.7</td><td>29.9</td><td>62.5</td><td>70.2</td><td>60.2</td><td>32.0</td><td>57.9</td><td>47.0</td><td>53.5</td><td>60.1</td><td>64.2</td><td>52.2</td><td>31.3</td><td>55.0</td><td>50.0</td><td>57.7</td><td>63.0</td><td>53.1</td></tr><tr><td>R-CNN FT fc7</td><td>64.2</td><td>69.7</td><td>50.0</td><td>41.9</td><td>32.0</td><td>62.6</td><td>71.0</td><td>60.7</td><td>32.7</td><td>58.5</td><td>46.5</td><td>56.1</td><td>60.6</td><td>66.8</td><td>54.2</td><td>31.5</td><td>52.8</td><td>48.9</td><td>57.9</td><td>64.7</td><td>54.2</td></tr><tr><td>R-CNN FT fc7 BB</td><td>68.1</td><td>72.8</td><td>56.8</td><td>43.0</td><td>36.8</td><td>66.3</td><td>74.2</td><td>67.6</td><td>34.4</td><td>63.5</td><td>54.5</td><td>61.2</td><td>69.1</td><td>68.6</td><td>58.7</td><td>33.4</td><td>62.9</td><td>51.1</td><td>62.5</td><td>64.8</td><td>58.5</td></tr><tr><td>DPM v5 [20]</td><td>33.2</td><td>60.3</td><td>10.2</td><td>16.1</td><td>27.3</td><td>54.3</td><td>58.2</td><td>23.0</td><td>20.0</td><td>24.1</td><td>26.7</td><td>12.7</td><td>58.1</td><td>48.2</td><td>43.2</td><td>12.0</td><td>21.1</td><td>36.1</td><td>46.0</td><td>43.5</td><td>33.7</td></tr><tr><td>DPM ST [28]</td><td>23.8</td><td>58.2</td><td>10.5</td><td>8.5</td><td>27.1</td><td>50.4</td><td>52.0</td><td>7.3</td><td>19.2</td><td>22.8</td><td>18.1</td><td>8.0</td><td>55.9</td><td>44.8</td><td>32.4</td><td>13.3</td><td>15.9</td><td>22.8</td><td>46.2</td><td>44.9</td><td>29.1</td></tr><tr><td>DPM HSC [31]</td><td>32.2</td><td>58.3</td><td>11.5</td><td>16.3</td><td>30.6</td><td>49.9</td><td>54.8</td><td>23.5</td><td>21.5</td><td>27.7</td><td>34.0</td><td>13.7</td><td>58.1</td><td>51.6</td><td>39.9</td><td>12.4</td><td>23.5</td><td>34.4</td><td>47.4</td><td>45.2</td><td>34.3</td></tr></table>",
            "id": 71,
            "page": 6,
            "text": "VOC 2007 test aero bike bird boat bottle bus car cat chair COW table dog horse mbike person plant sheep sofa train tv  mAP  R-CNN pool5 51.8 60.2 36.4 27.8 23.2 52.8 60.6 49.2 18.3 47.8 44.3 40.8 56.6 58.7 42.4 23.4 46.1 36.7 51.3 55.7 44.2  R-CNN fc6 59.3 61.8 43.1 34.0 25.1 53.1 60.6 52.8 21.7 47.8 42.7 47.8 52.5 58.5 44.6 25.6 48.3 34.0 53.1 58.0 46.2  R-CNN fc7 57.6 57.9 38.5 31.8 23.7 51.2 58.9 51.4 20.0 50.5 40.9 46.0 51.6 55.9 43.3 23.3 48.1 35.3 51.0 57.4 44.7  R-CNN FT pool5 58.2 63.3 37.9 27.6 26.1 54.1 66.9 51.4 26.7 55.5 43.4 43.1 57.7 59.0 45.8 28.1 50.8 40.6 53.1 56.4 47.3  R-CNN FT fc6 63.5 66.0 47.9 37.7 29.9 62.5 70.2 60.2 32.0 57.9 47.0 53.5 60.1 64.2 52.2 31.3 55.0 50.0 57.7 63.0 53.1  R-CNN FT fc7 64.2 69.7 50.0 41.9 32.0 62.6 71.0 60.7 32.7 58.5 46.5 56.1 60.6 66.8 54.2 31.5 52.8 48.9 57.9 64.7 54.2  R-CNN FT fc7 BB 68.1 72.8 56.8 43.0 36.8 66.3 74.2 67.6 34.4 63.5 54.5 61.2 69.1 68.6 58.7 33.4 62.9 51.1 62.5 64.8 58.5  DPM v5  33.2 60.3 10.2 16.1 27.3 54.3 58.2 23.0 20.0 24.1 26.7 12.7 58.1 48.2 43.2 12.0 21.1 36.1 46.0 43.5 33.7  DPM ST  23.8 58.2 10.5 8.5 27.1 50.4 52.0 7.3 19.2 22.8 18.1 8.0 55.9 44.8 32.4 13.3 15.9 22.8 46.2 44.9 29.1  DPM HSC  32.2 58.3 11.5 16.3 30.6 49.9 54.8 23.5 21.5 27.7 34.0 13.7 58.1 51.6 39.9 12.4 23.5 34.4 47.4 45.2"
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 844
                },
                {
                    "x": 2276,
                    "y": 844
                },
                {
                    "x": 2276,
                    "y": 1032
                },
                {
                    "x": 203,
                    "y": 1032
                }
            ],
            "category": "caption",
            "html": "<br><caption id='72' style='font-size:14px'>Table 2: Detection average precision (%) on VOC 2007 test. Rows 1-3 show R-CNN performance without fine-tuning. Rows 4-6 show<br>results for the CNN pre-trained on ILSVRC 2012 and then fine-tuned (FT) on VOC 2007 trainval. Row 7 includes a simple bounding-box<br>regression (BB) stage that reduces localization errors (Section C). Rows 8-10 present DPM methods as a strong baseline. The first uses<br>only HOG, while the next two use different feature learning approaches to augment or replace HOG.</caption>",
            "id": 72,
            "page": 6,
            "text": "Table 2: Detection average precision (%) on VOC 2007 test. Rows 1-3 show R-CNN performance without fine-tuning. Rows 4-6 show results for the CNN pre-trained on ILSVRC 2012 and then fine-tuned (FT) on VOC 2007 trainval. Row 7 includes a simple bounding-box regression (BB) stage that reduces localization errors (Section C). Rows 8-10 present DPM methods as a strong baseline. The first uses only HOG, while the next two use different feature learning approaches to augment or replace HOG."
        },
        {
            "bounding_box": [
                {
                    "x": 207,
                    "y": 1079
                },
                {
                    "x": 2270,
                    "y": 1079
                },
                {
                    "x": 2270,
                    "y": 1317
                },
                {
                    "x": 207,
                    "y": 1317
                }
            ],
            "category": "table",
            "html": "<table id='73' style='font-size:14px'><tr><td>VOC 2007 test</td><td>aero</td><td>bike</td><td>bird</td><td>boat</td><td>bottle</td><td>bus</td><td>car</td><td>cat</td><td>chair</td><td>COW</td><td>table</td><td>dog</td><td>horse</td><td>mbike</td><td>person</td><td>plant</td><td>sheep</td><td>sofa</td><td>train</td><td>tv</td><td>mAP</td></tr><tr><td>R-CNN T-Net</td><td>64.2</td><td>69.7</td><td>50.0</td><td>41.9</td><td>32.0</td><td>62.6</td><td>71.0</td><td>60.7</td><td>32.7</td><td>58.5</td><td>46.5</td><td>56.1</td><td>60.6</td><td>66.8</td><td>54.2</td><td>31.5</td><td>52.8</td><td>48.9</td><td>57.9</td><td>64.7</td><td>54.2</td></tr><tr><td>R-CNN T-Net BB</td><td>68.1</td><td>72.8</td><td>56.8</td><td>43.0</td><td>36.8</td><td>66.3</td><td>74.2</td><td>67.6</td><td>34.4</td><td>63.5</td><td>54.5</td><td>61.2</td><td>69.1</td><td>68.6</td><td>58.7</td><td>33.4</td><td>62.9</td><td>51.1</td><td>62.5</td><td>64.8</td><td>58.5</td></tr><tr><td>R-CNN O-Net</td><td>71.6</td><td>73.5</td><td>58.1</td><td>42.2</td><td>39.4</td><td>70.7</td><td>76.0</td><td>74.5</td><td>38.7</td><td>71.0</td><td>56.9</td><td>74.5</td><td>67.9</td><td>69.6</td><td>59.3</td><td>35.7</td><td>62.1</td><td>64.0</td><td>66.5</td><td>71.2</td><td>62.2</td></tr><tr><td>R-CNN O-Net BB</td><td>73.4</td><td>77.0</td><td>63.4</td><td>45.4</td><td>44.6</td><td>75.1</td><td>78.1</td><td>79.8</td><td>40.5</td><td>73.7</td><td>62.2</td><td>79.4</td><td>78.1</td><td>73.1</td><td>64.2</td><td>35.6</td><td>66.8</td><td>67.2</td><td>70.4</td><td>71.1</td><td>66.0</td></tr></table>",
            "id": 73,
            "page": 6,
            "text": "VOC 2007 test aero bike bird boat bottle bus car cat chair COW table dog horse mbike person plant sheep sofa train tv mAP  R-CNN T-Net 64.2 69.7 50.0 41.9 32.0 62.6 71.0 60.7 32.7 58.5 46.5 56.1 60.6 66.8 54.2 31.5 52.8 48.9 57.9 64.7 54.2  R-CNN T-Net BB 68.1 72.8 56.8 43.0 36.8 66.3 74.2 67.6 34.4 63.5 54.5 61.2 69.1 68.6 58.7 33.4 62.9 51.1 62.5 64.8 58.5  R-CNN O-Net 71.6 73.5 58.1 42.2 39.4 70.7 76.0 74.5 38.7 71.0 56.9 74.5 67.9 69.6 59.3 35.7 62.1 64.0 66.5 71.2 62.2  R-CNN O-Net BB 73.4 77.0 63.4 45.4 44.6 75.1 78.1 79.8 40.5 73.7 62.2 79.4 78.1 73.1 64.2 35.6 66.8 67.2 70.4 71.1"
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 1338
                },
                {
                    "x": 2278,
                    "y": 1338
                },
                {
                    "x": 2278,
                    "y": 1474
                },
                {
                    "x": 203,
                    "y": 1474
                }
            ],
            "category": "caption",
            "html": "<br><caption id='74' style='font-size:16px'>Table 3: Detection average precision (%) on VOC 2007 test for two different CNN architectures. The first two rows are results from<br>Table 2 using Krizhevsky et al. 's architecture (T-Net). Rows three and four use the recently proposed 16-layer architecture from Simonyan<br>and Zisserman (O-Net) [43].</caption>",
            "id": 74,
            "page": 6,
            "text": "Table 3: Detection average precision (%) on VOC 2007 test for two different CNN architectures. The first two rows are results from Table 2 using Krizhevsky  's architecture (T-Net). Rows three and four use the recently proposed 16-layer architecture from Simonyan and Zisserman (O-Net) ."
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 1524
                },
                {
                    "x": 1198,
                    "y": 1524
                },
                {
                    "x": 1198,
                    "y": 1869
                },
                {
                    "x": 203,
                    "y": 1869
                }
            ],
            "category": "paragraph",
            "html": "<p id='75' style='font-size:20px'>We visualize units from layer pool5, which is the max-<br>pooled output of the network's fifth and final convolutional<br>layer. The pool5 feature map is 6 x 6 x 256 = 9216-<br>dimensional. Ignoring boundary effects, each pool5 unit has<br>a receptive field of 195 x 195 pixels in the original 227 x 227<br>pixel input. A central pool5 unit has a nearly global view,<br>while one near the edge has a smaller, clipped support.</p>",
            "id": 75,
            "page": 6,
            "text": "We visualize units from layer pool5, which is the maxpooled output of the network's fifth and final convolutional layer. The pool5 feature map is 6 x 6 x 256 = 9216dimensional. Ignoring boundary effects, each pool5 unit has a receptive field of 195 x 195 pixels in the original 227 x 227 pixel input. A central pool5 unit has a nearly global view, while one near the edge has a smaller, clipped support."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1873
                },
                {
                    "x": 1198,
                    "y": 1873
                },
                {
                    "x": 1198,
                    "y": 2619
                },
                {
                    "x": 201,
                    "y": 2619
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='76' style='font-size:18px'>Each row in Figure 4 displays the top 16 activations for<br>a pool5 unit from a CNN that we fine-tuned on VOC 2007<br>trainval. Six of the 256 functionally unique units are visu-<br>alized (Appendix D includes more). These units were se-<br>lected to show a representative sample of what the network<br>learns. In the second row, we see a unit that fires on dog<br>faces and dot arrays. The unit corresponding to the third row<br>is a red blob detector. There are also detectors for human<br>faces and more abstract patterns such as text and triangular<br>structures with windows. The network appears to learn a<br>representation that combines a small number of class-tuned<br>features together with a distributed representation of shape,<br>texture, color, and material properties. The subsequent fully<br>connected layer fc6 has the ability to model a large set of<br>compositions of these rich features.</p>",
            "id": 76,
            "page": 6,
            "text": "Each row in Figure 4 displays the top 16 activations for a pool5 unit from a CNN that we fine-tuned on VOC 2007 trainval. Six of the 256 functionally unique units are visualized (Appendix D includes more). These units were selected to show a representative sample of what the network learns. In the second row, we see a unit that fires on dog faces and dot arrays. The unit corresponding to the third row is a red blob detector. There are also detectors for human faces and more abstract patterns such as text and triangular structures with windows. The network appears to learn a representation that combines a small number of class-tuned features together with a distributed representation of shape, texture, color, and material properties. The subsequent fully connected layer fc6 has the ability to model a large set of compositions of these rich features."
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 2650
                },
                {
                    "x": 609,
                    "y": 2650
                },
                {
                    "x": 609,
                    "y": 2699
                },
                {
                    "x": 203,
                    "y": 2699
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='77' style='font-size:22px'>3.2. Ablation studies</p>",
            "id": 77,
            "page": 6,
            "text": "3.2. Ablation studies"
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 2727
                },
                {
                    "x": 1198,
                    "y": 2727
                },
                {
                    "x": 1198,
                    "y": 2974
                },
                {
                    "x": 203,
                    "y": 2974
                }
            ],
            "category": "paragraph",
            "html": "<p id='78' style='font-size:20px'>Performance layer-by-layer, without fine-tuning. To un-<br>derstand which layers are critical for detection performance,<br>we analyzed results on the VOC 2007 dataset for each of the<br>CNN's last three layers. Layer pool5 was briefly described<br>in Section 3.1. The final two layers are summarized below.</p>",
            "id": 78,
            "page": 6,
            "text": "Performance layer-by-layer, without fine-tuning. To understand which layers are critical for detection performance, we analyzed results on the VOC 2007 dataset for each of the CNN's last three layers. Layer pool5 was briefly described in Section 3.1. The final two layers are summarized below."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 1524
                },
                {
                    "x": 2275,
                    "y": 1524
                },
                {
                    "x": 2275,
                    "y": 1770
                },
                {
                    "x": 1281,
                    "y": 1770
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='79' style='font-size:20px'>Layer fc6 is fully connected to pool5. To compute fea-<br>tures, it multiplies a 4096 x 9216 weight matrix by the pool5<br>feature map (reshaped as a 9216-dimensional vector) and<br>then adds a vector of biases. This intermediate vector is<br>component-wise half-wave rectified (x ← max(0, x)).</p>",
            "id": 79,
            "page": 6,
            "text": "Layer fc6 is fully connected to pool5. To compute features, it multiplies a 4096 x 9216 weight matrix by the pool5 feature map (reshaped as a 9216-dimensional vector) and then adds a vector of biases. This intermediate vector is component-wise half-wave rectified (x ← max(0, x))."
        },
        {
            "bounding_box": [
                {
                    "x": 1282,
                    "y": 1781
                },
                {
                    "x": 2276,
                    "y": 1781
                },
                {
                    "x": 2276,
                    "y": 1972
                },
                {
                    "x": 1282,
                    "y": 1972
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='80' style='font-size:22px'>Layer fc7 is the final layer of the network. It is imple-<br>mented by multiplying the features computed by fc6 by a<br>4096 x 4096 weight matrix, and similarly adding a vector<br>of biases and applying half-wave rectification.</p>",
            "id": 80,
            "page": 6,
            "text": "Layer fc7 is the final layer of the network. It is implemented by multiplying the features computed by fc6 by a 4096 x 4096 weight matrix, and similarly adding a vector of biases and applying half-wave rectification."
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 1982
                },
                {
                    "x": 2275,
                    "y": 1982
                },
                {
                    "x": 2275,
                    "y": 2827
                },
                {
                    "x": 1278,
                    "y": 2827
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='81' style='font-size:20px'>We start by looking at results from the CNN without<br>fine-tuning on PASCAL, i.e. all CNN parameters were<br>pre-trained on ILSVRC 2012 only. Analyzing performance<br>layer-by-layer (Table 2 rows 1-3) reveals that features from<br>fc7 generalize worse than features from fc6. This means<br>that 29%, or about 16.8 million, of the CNN's parameters<br>can be removed without degrading mAP. More surprising is<br>that removing both fc7 and fc6 produces quite good results<br>even though pool5 features are computed using only 6% of<br>the CNN's parameters. Much of the CNN's representational<br>power comes from its convolutional layers, rather than from<br>the much larger densely connected layers. This finding sug-<br>gests potential utility in computing a dense feature map, in<br>the sense of HOG, of an arbitrary-sized image by using only<br>the convolutional layers of the CNN. This representation<br>would enable experimentation with sliding-window detec-<br>tors, including DPM, on top of pool5 features.</p>",
            "id": 81,
            "page": 6,
            "text": "We start by looking at results from the CNN without fine-tuning on PASCAL, i.e. all CNN parameters were pre-trained on ILSVRC 2012 only. Analyzing performance layer-by-layer (Table 2 rows 1-3) reveals that features from fc7 generalize worse than features from fc6. This means that 29%, or about 16.8 million, of the CNN's parameters can be removed without degrading mAP. More surprising is that removing both fc7 and fc6 produces quite good results even though pool5 features are computed using only 6% of the CNN's parameters. Much of the CNN's representational power comes from its convolutional layers, rather than from the much larger densely connected layers. This finding suggests potential utility in computing a dense feature map, in the sense of HOG, of an arbitrary-sized image by using only the convolutional layers of the CNN. This representation would enable experimentation with sliding-window detectors, including DPM, on top of pool5 features."
        },
        {
            "bounding_box": [
                {
                    "x": 1283,
                    "y": 2878
                },
                {
                    "x": 2275,
                    "y": 2878
                },
                {
                    "x": 2275,
                    "y": 2978
                },
                {
                    "x": 1283,
                    "y": 2978
                }
            ],
            "category": "paragraph",
            "html": "<p id='82' style='font-size:20px'>Performance layer-by-layer, with fine-tuning. We now<br>look at results from our CNN after having fine-tuned its pa-</p>",
            "id": 82,
            "page": 6,
            "text": "Performance layer-by-layer, with fine-tuning. We now look at results from our CNN after having fine-tuned its pa-"
        },
        {
            "bounding_box": [
                {
                    "x": 1226,
                    "y": 3057
                },
                {
                    "x": 1254,
                    "y": 3057
                },
                {
                    "x": 1254,
                    "y": 3091
                },
                {
                    "x": 1226,
                    "y": 3091
                }
            ],
            "category": "footer",
            "html": "<footer id='83' style='font-size:16px'>6</footer>",
            "id": 83,
            "page": 6,
            "text": "6"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 307
                },
                {
                    "x": 1199,
                    "y": 307
                },
                {
                    "x": 1199,
                    "y": 654
                },
                {
                    "x": 201,
                    "y": 654
                }
            ],
            "category": "paragraph",
            "html": "<p id='84' style='font-size:18px'>rameters on VOC 2007 trainval. The improvement is strik-<br>ing (Table 2 rows 4-6): fine-tuning increases mAP by 8.0<br>percentage points to 54.2%. The boost from fine-tuning is<br>much larger for fc6 and fc7 than for pool5, which suggests<br>that the pool5 features learned from ImageNet are general<br>and that most of the improvement is gained from learning<br>domain-specific non-linear classifiers on top of them.</p>",
            "id": 84,
            "page": 7,
            "text": "rameters on VOC 2007 trainval. The improvement is striking (Table 2 rows 4-6): fine-tuning increases mAP by 8.0 percentage points to 54.2%. The boost from fine-tuning is much larger for fc6 and fc7 than for pool5, which suggests that the pool5 features learned from ImageNet are general and that most of the improvement is gained from learning domain-specific non-linear classifiers on top of them."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 702
                },
                {
                    "x": 1196,
                    "y": 702
                },
                {
                    "x": 1196,
                    "y": 947
                },
                {
                    "x": 201,
                    "y": 947
                }
            ],
            "category": "paragraph",
            "html": "<p id='85' style='font-size:16px'>Comparison to recent feature learning methods. Rela-<br>tively few feature learning methods have been tried on PAS-<br>CAL VOC detection. We look at two recent approaches that<br>build on deformable part models. For reference, we also in-<br>clude results for the standard HOG-based DPM [20].</p>",
            "id": 85,
            "page": 7,
            "text": "Comparison to recent feature learning methods. Relatively few feature learning methods have been tried on PASCAL VOC detection. We look at two recent approaches that build on deformable part models. For reference, we also include results for the standard HOG-based DPM ."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 956
                },
                {
                    "x": 1197,
                    "y": 956
                },
                {
                    "x": 1197,
                    "y": 1301
                },
                {
                    "x": 201,
                    "y": 1301
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='86' style='font-size:16px'>The first DPM feature learning method, DPM ST [28],<br>augments HOG features with histograms of \"sketch token\"<br>probabilities. Intuitively, a sketch token is a tight distri-<br>bution of contours passing through the center of an image<br>patch. Sketch token probabilities are computed at each pixel<br>by a random forest that was trained to classify 35 x 35 pixel<br>patches into one of 150 sketch tokens or background.</p>",
            "id": 86,
            "page": 7,
            "text": "The first DPM feature learning method, DPM ST , augments HOG features with histograms of \"sketch token\" probabilities. Intuitively, a sketch token is a tight distribution of contours passing through the center of an image patch. Sketch token probabilities are computed at each pixel by a random forest that was trained to classify 35 x 35 pixel patches into one of 150 sketch tokens or background."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 1308
                },
                {
                    "x": 1198,
                    "y": 1308
                },
                {
                    "x": 1198,
                    "y": 1652
                },
                {
                    "x": 202,
                    "y": 1652
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='87' style='font-size:18px'>The second method, DPM HSC [31], replaces HOG with<br>histograms of sparse codes (HSC). To compute an HSC,<br>sparse code activations are solved for at each pixel using<br>a learned dictionary of 100 7 x 7 pixel (grayscale) atoms.<br>The resulting activations are rectified in three ways (full and<br>both half-waves), spatially pooled, unit l2 normalized, and<br>then power transformed (x ← sign(x)|x|�).</p>",
            "id": 87,
            "page": 7,
            "text": "The second method, DPM HSC , replaces HOG with histograms of sparse codes (HSC). To compute an HSC, sparse code activations are solved for at each pixel using a learned dictionary of 100 7 x 7 pixel (grayscale) atoms. The resulting activations are rectified in three ways (full and both half-waves), spatially pooled, unit l2 normalized, and then power transformed (x ← sign(x)|x|�)."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1661
                },
                {
                    "x": 1199,
                    "y": 1661
                },
                {
                    "x": 1199,
                    "y": 2256
                },
                {
                    "x": 201,
                    "y": 2256
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='88' style='font-size:18px'>All R-CNN variants strongly outperform the three DPM<br>baselines (Table 2 rows 8-10), including the two that use<br>feature learning. Compared to the latest version of DPM,<br>which uses only HOG features, our mAP is more than 20<br>percentage points higher: 54.2% VS. 33.7%-a 61% rela-<br>tive improvement. The combination of HOG and sketch to-<br>kens yields 2.5 mAP points over HOG alone, while HSC<br>improves over HOG by 4 mAP points (when compared<br>internally to their private DPM baselines-both use non-<br>public implementations of DPM that underperform the open<br>source version [20]). These methods achieve mAPs of<br>29.1% and 34.3%, respectively.</p>",
            "id": 88,
            "page": 7,
            "text": "All R-CNN variants strongly outperform the three DPM baselines (Table 2 rows 8-10), including the two that use feature learning. Compared to the latest version of DPM, which uses only HOG features, our mAP is more than 20 percentage points higher: 54.2% VS. 33.7%-a 61% relative improvement. The combination of HOG and sketch tokens yields 2.5 mAP points over HOG alone, while HSC improves over HOG by 4 mAP points (when compared internally to their private DPM baselines-both use nonpublic implementations of DPM that underperform the open source version ). These methods achieve mAPs of 29.1% and 34.3%, respectively."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2298
                },
                {
                    "x": 728,
                    "y": 2298
                },
                {
                    "x": 728,
                    "y": 2346
                },
                {
                    "x": 202,
                    "y": 2346
                }
            ],
            "category": "paragraph",
            "html": "<p id='89' style='font-size:20px'>3.3. Network architectures</p>",
            "id": 89,
            "page": 7,
            "text": "3.3. Network architectures"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2377
                },
                {
                    "x": 1199,
                    "y": 2377
                },
                {
                    "x": 1199,
                    "y": 2974
                },
                {
                    "x": 201,
                    "y": 2974
                }
            ],
            "category": "paragraph",
            "html": "<p id='90' style='font-size:16px'>Most results in this paper use the network architecture<br>from Krizhevsky et al. [25]. However, we have found that<br>the choice of architecture has a large effect on R-CNN de-<br>tection performance. In Table 3 we show results on VOC<br>2007 test using the 16-layer deep network recently proposed<br>by Simonyan and Zisserman [43]. This network was one of<br>the top performers in the recent ILSVRC 2014 classifica-<br>tion challenge. The network has a homogeneous structure<br>consisting of 13 layers of 3 x 3 convolution kernels, with<br>five max pooling layers interspersed, and topped with three<br>fully-connected layers. We refer to this network as \"O-Net\"<br>for OxfordNet and the baseline as \"T-Net\" for TorontoNet.</p>",
            "id": 90,
            "page": 7,
            "text": "Most results in this paper use the network architecture from Krizhevsky  . However, we have found that the choice of architecture has a large effect on R-CNN detection performance. In Table 3 we show results on VOC 2007 test using the 16-layer deep network recently proposed by Simonyan and Zisserman . This network was one of the top performers in the recent ILSVRC 2014 classification challenge. The network has a homogeneous structure consisting of 13 layers of 3 x 3 convolution kernels, with five max pooling layers interspersed, and topped with three fully-connected layers. We refer to this network as \"O-Net\" for OxfordNet and the baseline as \"T-Net\" for TorontoNet."
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 306
                },
                {
                    "x": 2277,
                    "y": 306
                },
                {
                    "x": 2277,
                    "y": 902
                },
                {
                    "x": 1278,
                    "y": 902
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='91' style='font-size:16px'>To use O-Net in R-CNN, we downloaded the pub-<br>licly available pre-trained network weights for the<br>VGG_ILSVRC_1 6_layers model from the Caffe Model<br>Zoo. 1 We then fine-tuned the network using the same pro-<br>tocol as we used for T-Net. The only difference was to use<br>smaller minibatches (24 examples) as required in order to<br>fit within GPU memory. The results in Table 3 show that R-<br>CNN with O-Net substantially outperforms R-CNN with T-<br>Net, increasing mAP from 58.5% to 66.0%. However there<br>is a considerable drawback in terms of compute time, with<br>the forward pass of O-Net taking roughly 7 times longer<br>than T-Net.</p>",
            "id": 91,
            "page": 7,
            "text": "To use O-Net in R-CNN, we downloaded the publicly available pre-trained network weights for the VGG_ILSVRC_1 6_layers model from the Caffe Model Zoo. 1 We then fine-tuned the network using the same protocol as we used for T-Net. The only difference was to use smaller minibatches (24 examples) as required in order to fit within GPU memory. The results in Table 3 show that RCNN with O-Net substantially outperforms R-CNN with TNet, increasing mAP from 58.5% to 66.0%. However there is a considerable drawback in terms of compute time, with the forward pass of O-Net taking roughly 7 times longer than T-Net."
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 936
                },
                {
                    "x": 1836,
                    "y": 936
                },
                {
                    "x": 1836,
                    "y": 985
                },
                {
                    "x": 1279,
                    "y": 985
                }
            ],
            "category": "paragraph",
            "html": "<p id='92' style='font-size:20px'>3.4. Detection error analysis</p>",
            "id": 92,
            "page": 7,
            "text": "3.4. Detection error analysis"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 1012
                },
                {
                    "x": 2278,
                    "y": 1012
                },
                {
                    "x": 2278,
                    "y": 1507
                },
                {
                    "x": 1279,
                    "y": 1507
                }
            ],
            "category": "paragraph",
            "html": "<p id='93' style='font-size:16px'>We applied the excellent detection analysis tool from<br>Hoiem et al. [23] in order to reveal our method's error<br>modes, understand how fine-tuning changes them, and to<br>see how our error types compare with DPM. A full sum-<br>mary of the analysis tool is beyond the scope of this pa-<br>per and we encourage readers to consult [23] to understand<br>some finer details (such as \"normalized AP\"). Since the<br>analysis is best absorbed in the context of the associated<br>plots, we present the discussion within the captions of Fig-<br>ure 5 and Figure 6.</p>",
            "id": 93,
            "page": 7,
            "text": "We applied the excellent detection analysis tool from Hoiem   in order to reveal our method's error modes, understand how fine-tuning changes them, and to see how our error types compare with DPM. A full summary of the analysis tool is beyond the scope of this paper and we encourage readers to consult  to understand some finer details (such as \"normalized AP\"). Since the analysis is best absorbed in the context of the associated plots, we present the discussion within the captions of Figure 5 and Figure 6."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 1541
                },
                {
                    "x": 1855,
                    "y": 1541
                },
                {
                    "x": 1855,
                    "y": 1591
                },
                {
                    "x": 1281,
                    "y": 1591
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='94' style='font-size:22px'>3.5. Bounding-box regression</p>",
            "id": 94,
            "page": 7,
            "text": "3.5. Bounding-box regression"
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 1619
                },
                {
                    "x": 2277,
                    "y": 1619
                },
                {
                    "x": 2277,
                    "y": 2063
                },
                {
                    "x": 1281,
                    "y": 2063
                }
            ],
            "category": "paragraph",
            "html": "<p id='95' style='font-size:16px'>Based on the error analysis, we implemented a sim-<br>ple method to reduce localization errors. Inspired by the<br>bounding-box regression employed in DPM [17], we train a<br>linear regression model to predict a new detection window<br>given the pool5 features for a selective search region pro-<br>posal. Full details are given in Appendix C. Results in Ta-<br>ble 1, Table 2, and Figure 5 show that this simple approach<br>fixes a large number of mislocalized detections, boosting<br>mAP by 3 to 4 points.</p>",
            "id": 95,
            "page": 7,
            "text": "Based on the error analysis, we implemented a simple method to reduce localization errors. Inspired by the bounding-box regression employed in DPM , we train a linear regression model to predict a new detection window given the pool5 features for a selective search region proposal. Full details are given in Appendix C. Results in Table 1, Table 2, and Figure 5 show that this simple approach fixes a large number of mislocalized detections, boosting mAP by 3 to 4 points."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 2098
                },
                {
                    "x": 1728,
                    "y": 2098
                },
                {
                    "x": 1728,
                    "y": 2146
                },
                {
                    "x": 1281,
                    "y": 2146
                }
            ],
            "category": "paragraph",
            "html": "<p id='96' style='font-size:20px'>3.6. Qualitative results</p>",
            "id": 96,
            "page": 7,
            "text": "3.6. Qualitative results"
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2174
                },
                {
                    "x": 2277,
                    "y": 2174
                },
                {
                    "x": 2277,
                    "y": 2674
                },
                {
                    "x": 1280,
                    "y": 2674
                }
            ],
            "category": "paragraph",
            "html": "<p id='97' style='font-size:16px'>Qualitative detection results on ILSVRC2013 are pre-<br>sented in Figure 8 and Figure 9 at the end of the paper. Each<br>image was sampled randomly from the val2 set and all de-<br>tections from all detectors with a precision greater than 0.5<br>are shown. Note that these are not curated and give a re-<br>alistic impression of the detectors in action. More qualita-<br>tive results are presented in Figure 10 and Figure 11, but<br>these have been curated. We selected each image because it<br>contained interesting, surprising, or amusing results. Here,<br>also, all detections at precision greater than 0.5 are shown.</p>",
            "id": 97,
            "page": 7,
            "text": "Qualitative detection results on ILSVRC2013 are presented in Figure 8 and Figure 9 at the end of the paper. Each image was sampled randomly from the val2 set and all detections from all detectors with a precision greater than 0.5 are shown. Note that these are not curated and give a realistic impression of the detectors in action. More qualitative results are presented in Figure 10 and Figure 11, but these have been curated. We selected each image because it contained interesting, surprising, or amusing results. Here, also, all detections at precision greater than 0.5 are shown."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 2717
                },
                {
                    "x": 2092,
                    "y": 2717
                },
                {
                    "x": 2092,
                    "y": 2768
                },
                {
                    "x": 1281,
                    "y": 2768
                }
            ],
            "category": "paragraph",
            "html": "<p id='98' style='font-size:22px'>4. The ILSVRC2013 detection dataset</p>",
            "id": 98,
            "page": 7,
            "text": "4. The ILSVRC2013 detection dataset"
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 2801
                },
                {
                    "x": 2275,
                    "y": 2801
                },
                {
                    "x": 2275,
                    "y": 2899
                },
                {
                    "x": 1281,
                    "y": 2899
                }
            ],
            "category": "paragraph",
            "html": "<p id='99' style='font-size:16px'>In Section 2 we presented results on the ILSVRC2013<br>detection dataset. This dataset is less homogeneous than</p>",
            "id": 99,
            "page": 7,
            "text": "In Section 2 we presented results on the ILSVRC2013 detection dataset. This dataset is less homogeneous than"
        },
        {
            "bounding_box": [
                {
                    "x": 1225,
                    "y": 3055
                },
                {
                    "x": 1251,
                    "y": 3055
                },
                {
                    "x": 1251,
                    "y": 3090
                },
                {
                    "x": 1225,
                    "y": 3090
                }
            ],
            "category": "footer",
            "html": "<footer id='100' style='font-size:14px'>7</footer>",
            "id": 100,
            "page": 7,
            "text": "7"
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 279
                },
                {
                    "x": 2275,
                    "y": 279
                },
                {
                    "x": 2275,
                    "y": 687
                },
                {
                    "x": 199,
                    "y": 687
                }
            ],
            "category": "figure",
            "html": "<figure><img id='101' style='font-size:14px' alt=\"R-CNN fc6: sensitivity and impact R-CNN FT fc7: sensitivity and impact R-CNN FT fc7 BB: sensitivity and impact DPM voc-release5: sensitivity and impact\n0.8 0.8 0.8 0.786 0.779 0.8\n0.766\n0.720 0.723 0.731 0.720\n0.709\n0.701 0.685 T 0.676\n0.677 0.672\nAP 0.6 0.612 0.606 0.609 0.6 0.593 0.6 0.6\n0.634 0.633\n0.557 0.542\nnormalized\n0.516\n0.498  0.484 0.487\n0.453 0.453\n0.442. 0.429\n0.4 0.420 0.4 0.4 0.385 0.368\n0.4 0.391-0.388\n0.344-0.351\n0.335 0.325 I0.339T 0.347\n0.297\n0.244\n0.2 0.212 0.201 0.2 0.179 0.2 0.211 0.2 0.216\n0.132 0.126 0.137\n0.094\n↓ 0.056\n0 0 0 0\nOCC trn size asp view part OCC trn size asp view part OCC trn size asp view part OCC trn size asp view part\" data-coord=\"top-left:(199,279); bottom-right:(2275,687)\" /></figure>",
            "id": 101,
            "page": 8,
            "text": "R-CNN fc6: sensitivity and impact R-CNN FT fc7: sensitivity and impact R-CNN FT fc7 BB: sensitivity and impact DPM voc-release5: sensitivity and impact 0.8 0.8 0.8 0.786 0.779 0.8 0.766 0.720 0.723 0.731 0.720 0.709 0.701 0.685 T 0.676 0.677 0.672 AP 0.6 0.612 0.606 0.609 0.6 0.593 0.6 0.6 0.634 0.633 0.557 0.542 normalized 0.516 0.498  0.484 0.487 0.453 0.453 0.442. 0.429 0.4 0.420 0.4 0.4 0.385 0.368 0.4 0.391-0.388 0.344-0.351 0.335 0.325 I0.339T 0.347 0.297 0.244 0.2 0.212 0.201 0.2 0.179 0.2 0.211 0.2 0.216 0.132 0.126 0.137 0.094 ↓ 0.056 0 0 0 0 OCC trn size asp view part OCC trn size asp view part OCC trn size asp view part OCC trn size asp view part"
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 696
                },
                {
                    "x": 2278,
                    "y": 696
                },
                {
                    "x": 2278,
                    "y": 1024
                },
                {
                    "x": 200,
                    "y": 1024
                }
            ],
            "category": "caption",
            "html": "<br><caption id='102' style='font-size:18px'>Figure 6: Sensitivity to object characteristics. Each plot shows the mean (over classes) normalized AP (see [23]) for the highest and<br>lowest performing subsets within six different object characteristics (occlusion, truncation, bounding-box area, aspect ratio, viewpoint, part<br>visibility). We show plots for our method (R-CNN) with and without fine-tuning (FT) and bounding-box regression (BB) as well as for<br>DPM voc-release5. Overall, fine-tuning does not reduce sensitivity (the difference between max and min), but does substantially improve<br>both the highest and lowest performing subsets for nearly all characteristics. This indicates that fine-tuning does more than simply improve<br>the lowest performing subsets for aspect ratio and bounding-box area, as one might conjecture based on how we warp network inputs.<br>Instead, fine-tuning improves robustness for all characteristics including occlusion, truncation, viewpoint, and part visibility.</caption>",
            "id": 102,
            "page": 8,
            "text": "Figure 6: Sensitivity to object characteristics. Each plot shows the mean (over classes) normalized AP (see ) for the highest and lowest performing subsets within six different object characteristics (occlusion, truncation, bounding-box area, aspect ratio, viewpoint, part visibility). We show plots for our method (R-CNN) with and without fine-tuning (FT) and bounding-box regression (BB) as well as for DPM voc-release5. Overall, fine-tuning does not reduce sensitivity (the difference between max and min), but does substantially improve both the highest and lowest performing subsets for nearly all characteristics. This indicates that fine-tuning does more than simply improve the lowest performing subsets for aspect ratio and bounding-box area, as one might conjecture based on how we warp network inputs. Instead, fine-tuning improves robustness for all characteristics including occlusion, truncation, viewpoint, and part visibility."
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 1053
                },
                {
                    "x": 1203,
                    "y": 1053
                },
                {
                    "x": 1203,
                    "y": 1800
                },
                {
                    "x": 203,
                    "y": 1800
                }
            ],
            "category": "figure",
            "html": "<figure><img id='103' style='font-size:14px' alt=\"R-CNN fc6: animals R-CNN FT fc7: animals R-CNN FT fc7 BB: animals\n100 100 100\ntype\n80 80 80\neach\nof 60 60 60\npercentage\n40 40 40\nLoc Loc Loc\nSim Sim Sim\n20 20 20\nOth Oth Oth\nBG BG BG\n25 100 400 1600 6400 25 100 400 1600 6400 25 100 400 1600 6400\ntotal false positives total false positives total false positives\nR-CNN fc6: furniture R-CNN FT fc7: furniture R-CNN FT fc7 BB: furniture\n100 100 100\ntype\n80 80 80\neach\nof 60 60 60\npercentage\n40 40 40\nLoc Loc Loc\nSim Sim Sim\n20 20 20\nOth Oth Oth\nBG BG BG\n25 100 400 1600 6400 25 100 400 1600 6400 25 100 400 1600 6400\ntotal false positives total false positives total false positives\" data-coord=\"top-left:(203,1053); bottom-right:(1203,1800)\" /></figure>",
            "id": 103,
            "page": 8,
            "text": "R-CNN fc6: animals R-CNN FT fc7: animals R-CNN FT fc7 BB: animals 100 100 100 type 80 80 80 each of 60 60 60 percentage 40 40 40 Loc Loc Loc Sim Sim Sim 20 20 20 Oth Oth Oth BG BG BG 25 100 400 1600 6400 25 100 400 1600 6400 25 100 400 1600 6400 total false positives total false positives total false positives R-CNN fc6: furniture R-CNN FT fc7: furniture R-CNN FT fc7 BB: furniture 100 100 100 type 80 80 80 each of 60 60 60 percentage 40 40 40 Loc Loc Loc Sim Sim Sim 20 20 20 Oth Oth Oth BG BG BG 25 100 400 1600 6400 25 100 400 1600 6400 25 100 400 1600 6400 total false positives total false positives total false positives"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1817
                },
                {
                    "x": 1200,
                    "y": 1817
                },
                {
                    "x": 1200,
                    "y": 2503
                },
                {
                    "x": 201,
                    "y": 2503
                }
            ],
            "category": "caption",
            "html": "<br><caption id='104' style='font-size:16px'>Figure 5: Distribution of top-ranked false positive (FP) types.<br>Each plot shows the evolving distribution of FP types as more FPs<br>are considered in order of decreasing score. Each FP is catego-<br>rized into 1 of 4 types: Loc-poor localization (a detection with<br>an IoU overlap with the correct class between 0.1 and 0.5, or a du-<br>plicate); Sim-confusion with a similar category; Oth-confusion<br>with a dissimilar object category; BG-a FP that fired on back-<br>ground. Compared with DPM (see [23]), significantly more of<br>our errors result from poor localization, rather than confusion with<br>background or other object classes, indicating that the CNN fea-<br>tures are much more discriminative than HOG. Loose localiza-<br>tion likely results from our use of bottom-up region proposals and<br>the positional invariance learned from pre-training the CNN for<br>whole-image classification. Column three shows how our simple<br>bounding-box regression method fixes many localization errors.</caption>",
            "id": 104,
            "page": 8,
            "text": "Figure 5: Distribution of top-ranked false positive (FP) types. Each plot shows the evolving distribution of FP types as more FPs are considered in order of decreasing score. Each FP is categorized into 1 of 4 types: Loc-poor localization (a detection with an IoU overlap with the correct class between 0.1 and 0.5, or a duplicate); Sim-confusion with a similar category; Oth-confusion with a dissimilar object category; BG-a FP that fired on background. Compared with DPM (see ), significantly more of our errors result from poor localization, rather than confusion with background or other object classes, indicating that the CNN features are much more discriminative than HOG. Loose localization likely results from our use of bottom-up region proposals and the positional invariance learned from pre-training the CNN for whole-image classification. Column three shows how our simple bounding-box regression method fixes many localization errors."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2556
                },
                {
                    "x": 1198,
                    "y": 2556
                },
                {
                    "x": 1198,
                    "y": 2702
                },
                {
                    "x": 202,
                    "y": 2702
                }
            ],
            "category": "paragraph",
            "html": "<p id='105' style='font-size:18px'>PASCAL VOC, requiring choices about how to use it. Since<br>these decisions are non-trivial, we cover them in this sec-<br>tion.</p>",
            "id": 105,
            "page": 8,
            "text": "PASCAL VOC, requiring choices about how to use it. Since these decisions are non-trivial, we cover them in this section."
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 2748
                },
                {
                    "x": 623,
                    "y": 2748
                },
                {
                    "x": 623,
                    "y": 2795
                },
                {
                    "x": 204,
                    "y": 2795
                }
            ],
            "category": "paragraph",
            "html": "<p id='106' style='font-size:22px'>4.1. Dataset overview</p>",
            "id": 106,
            "page": 8,
            "text": "4.1. Dataset overview"
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 2828
                },
                {
                    "x": 1199,
                    "y": 2828
                },
                {
                    "x": 1199,
                    "y": 2975
                },
                {
                    "x": 204,
                    "y": 2975
                }
            ],
            "category": "paragraph",
            "html": "<p id='107' style='font-size:20px'>The ILSVRC2013 detection dataset is split into three<br>sets: train (395,918), val (20,121), and test (40,152), where<br>the number of images in each set is in parentheses. The</p>",
            "id": 107,
            "page": 8,
            "text": "The ILSVRC2013 detection dataset is split into three sets: train (395,918), val (20,121), and test (40,152), where the number of images in each set is in parentheses. The"
        },
        {
            "bounding_box": [
                {
                    "x": 1277,
                    "y": 1073
                },
                {
                    "x": 2277,
                    "y": 1073
                },
                {
                    "x": 2277,
                    "y": 2017
                },
                {
                    "x": 1277,
                    "y": 2017
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='108' style='font-size:20px'>val and test splits are drawn from the same image distribu-<br>tion. These images are scene-like and similar in complexity<br>(number of objects, amount of clutter, pose variability, etc.)<br>to PASCAL VOC images. The val and test splits are exhaus-<br>tively annotated, meaning that in each image all instances<br>from all 200 classes are labeled with bounding boxes. The<br>train set, in contrast, is drawn from the ILSVRC2013 clas-<br>sification image distribution. These images have more vari-<br>able complexity with a skew towards images of a single cen-<br>tered object. Unlike val and test, the train images (due to<br>their large number) are not exhaustively annotated. In any<br>given train image, instances from the 200 classes may or<br>may not be labeled. In addition to these image sets, each<br>class has an extra set of negative images. Negative images<br>are manually checked to validate that they do not contain<br>any instances of their associated class. The negative im-<br>age sets were not used in this work. More information on<br>how ILSVRC was collected and annotated can be found in<br>[11, 36].</p>",
            "id": 108,
            "page": 8,
            "text": "val and test splits are drawn from the same image distribution. These images are scene-like and similar in complexity (number of objects, amount of clutter, pose variability, etc.) to PASCAL VOC images. The val and test splits are exhaustively annotated, meaning that in each image all instances from all 200 classes are labeled with bounding boxes. The train set, in contrast, is drawn from the ILSVRC2013 classification image distribution. These images have more variable complexity with a skew towards images of a single centered object. Unlike val and test, the train images (due to their large number) are not exhaustively annotated. In any given train image, instances from the 200 classes may or may not be labeled. In addition to these image sets, each class has an extra set of negative images. Negative images are manually checked to validate that they do not contain any instances of their associated class. The negative image sets were not used in this work. More information on how ILSVRC was collected and annotated can be found in ."
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 2048
                },
                {
                    "x": 2277,
                    "y": 2048
                },
                {
                    "x": 2277,
                    "y": 2497
                },
                {
                    "x": 1279,
                    "y": 2497
                }
            ],
            "category": "paragraph",
            "html": "<p id='109' style='font-size:18px'>The nature of these splits presents a number of choices<br>for training R-CNN. The train images cannot be used for<br>hard negative mining, because annotations are not exhaus-<br>tive. Where should negative examples come from? Also,<br>the train images have different statistics than val and test.<br>Should the train images be used at all, and if so, to what<br>extent? While we have not thoroughly evaluated a large<br>number of choices, we present what seemed like the most<br>obvious path based on previous experience.</p>",
            "id": 109,
            "page": 8,
            "text": "The nature of these splits presents a number of choices for training R-CNN. The train images cannot be used for hard negative mining, because annotations are not exhaustive. Where should negative examples come from? Also, the train images have different statistics than val and test. Should the train images be used at all, and if so, to what extent? While we have not thoroughly evaluated a large number of choices, we present what seemed like the most obvious path based on previous experience."
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 2529
                },
                {
                    "x": 2277,
                    "y": 2529
                },
                {
                    "x": 2277,
                    "y": 2977
                },
                {
                    "x": 1279,
                    "y": 2977
                }
            ],
            "category": "paragraph",
            "html": "<p id='110' style='font-size:18px'>Our general strategy is to rely heavily on the val set and<br>use some of the train images as an auxiliary source of pos-<br>itive examples. To use val for both training and valida-<br>tion, we split it into roughly equally sized \"val1 , , and \"val2 , ,<br>sets. Since some classes have very few examples in val (the<br>smallest has only 31 and half have fewer than 110), it is<br>important to produce an approximately class-balanced par-<br>tition. To do this, a large number of candidate splits were<br>generated and the one with the smallest maximum relative</p>",
            "id": 110,
            "page": 8,
            "text": "Our general strategy is to rely heavily on the val set and use some of the train images as an auxiliary source of positive examples. To use val for both training and validation, we split it into roughly equally sized \"val1 , , and \"val2 , , sets. Since some classes have very few examples in val (the smallest has only 31 and half have fewer than 110), it is important to produce an approximately class-balanced partition. To do this, a large number of candidate splits were generated and the one with the smallest maximum relative"
        },
        {
            "bounding_box": [
                {
                    "x": 1226,
                    "y": 3057
                },
                {
                    "x": 1251,
                    "y": 3057
                },
                {
                    "x": 1251,
                    "y": 3091
                },
                {
                    "x": 1226,
                    "y": 3091
                }
            ],
            "category": "footer",
            "html": "<footer id='111' style='font-size:18px'>8</footer>",
            "id": 111,
            "page": 8,
            "text": "8"
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 301
                },
                {
                    "x": 1200,
                    "y": 301
                },
                {
                    "x": 1200,
                    "y": 753
                },
                {
                    "x": 200,
                    "y": 753
                }
            ],
            "category": "paragraph",
            "html": "<p id='112' style='font-size:16px'>class imbalance was selected. 2 Each candidate split was<br>generated by clustering val images using their class counts<br>as features, followed by a randomized local search that may<br>improve the split balance. The particular split used here has<br>a maximum relative imbalance of about 11% and a median<br>relative imbalance of 4%. The val1/val2 split and code used<br>to produce them will be publicly available to allow other re-<br>searchers to compare their methods on the val splits used in<br>this report.</p>",
            "id": 112,
            "page": 9,
            "text": "class imbalance was selected. 2 Each candidate split was generated by clustering val images using their class counts as features, followed by a randomized local search that may improve the split balance. The particular split used here has a maximum relative imbalance of about 11% and a median relative imbalance of 4%. The val1/val2 split and code used to produce them will be publicly available to allow other researchers to compare their methods on the val splits used in this report."
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 797
                },
                {
                    "x": 628,
                    "y": 797
                },
                {
                    "x": 628,
                    "y": 846
                },
                {
                    "x": 203,
                    "y": 846
                }
            ],
            "category": "paragraph",
            "html": "<p id='113' style='font-size:22px'>4.2. Region proposals</p>",
            "id": 113,
            "page": 9,
            "text": "4.2. Region proposals"
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 879
                },
                {
                    "x": 1200,
                    "y": 879
                },
                {
                    "x": 1200,
                    "y": 1626
                },
                {
                    "x": 199,
                    "y": 1626
                }
            ],
            "category": "paragraph",
            "html": "<p id='114' style='font-size:16px'>We followed the same region proposal approach that was<br>used for detection on PASCAL. Selective search [39] was<br>run in \"fast mode\" on each image in val1, val2, and test (but<br>not on images in train). One minor modification was re-<br>quired to deal with the fact that selective search is not scale<br>invariant and SO the number of regions produced depends<br>on the image resolution. ILSVRC image sizes range from<br>very small to a few that are several mega-pixels, and SO we<br>resized each image to a fixed width (500 pixels) before run-<br>ning selective search. On val, selective search resulted in an<br>average of 2403 region proposals per image with a 91.6%<br>recall of all ground-truth bounding boxes (at 0.5 IoU thresh-<br>old). This recall is notably lower than in PASCAL, where<br>itis approximately 98%, indicating significant room for im-<br>provement in the region proposal stage.</p>",
            "id": 114,
            "page": 9,
            "text": "We followed the same region proposal approach that was used for detection on PASCAL. Selective search  was run in \"fast mode\" on each image in val1, val2, and test (but not on images in train). One minor modification was required to deal with the fact that selective search is not scale invariant and SO the number of regions produced depends on the image resolution. ILSVRC image sizes range from very small to a few that are several mega-pixels, and SO we resized each image to a fixed width (500 pixels) before running selective search. On val, selective search resulted in an average of 2403 region proposals per image with a 91.6% recall of all ground-truth bounding boxes (at 0.5 IoU threshold). This recall is notably lower than in PASCAL, where itis approximately 98%, indicating significant room for improvement in the region proposal stage."
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 1667
                },
                {
                    "x": 558,
                    "y": 1667
                },
                {
                    "x": 558,
                    "y": 1715
                },
                {
                    "x": 203,
                    "y": 1715
                }
            ],
            "category": "paragraph",
            "html": "<p id='115' style='font-size:20px'>4.3. Training data</p>",
            "id": 115,
            "page": 9,
            "text": "4.3. Training data"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1748
                },
                {
                    "x": 1200,
                    "y": 1748
                },
                {
                    "x": 1200,
                    "y": 2142
                },
                {
                    "x": 201,
                    "y": 2142
                }
            ],
            "category": "paragraph",
            "html": "<p id='116' style='font-size:16px'>For training data, we formed a set of images and boxes<br>that includes all selective search and ground-truth boxes<br>from val1 together with up to N ground-truth boxes per<br>class from train (if a class has fewer than N ground-truth<br>boxes in train, then we take all of them). We'll call this<br>dataset of images and boxes val1 +trainN. In an ablation<br>study, we show mAP on val2 for N E {0, 500, 1000} (Sec-<br>tion 4.5).</p>",
            "id": 116,
            "page": 9,
            "text": "For training data, we formed a set of images and boxes that includes all selective search and ground-truth boxes from val1 together with up to N ground-truth boxes per class from train (if a class has fewer than N ground-truth boxes in train, then we take all of them). We'll call this dataset of images and boxes val1 +trainN. In an ablation study, we show mAP on val2 for N E {0, 500, 1000} (Section 4.5)."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 2151
                },
                {
                    "x": 1199,
                    "y": 2151
                },
                {
                    "x": 1199,
                    "y": 2848
                },
                {
                    "x": 200,
                    "y": 2848
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='117' style='font-size:18px'>Training data is required for three procedures in R-CNN:<br>(1) CNN fine-tuning, (2) detector SVM training, and (3)<br>bounding-box regressor training. CNN fine-tuning was run<br>for 50k SGD iteration on val1 +trainN using the exact same<br>settings as were used for PASCAL. Fine-tuning on a sin-<br>gle NVIDIA Tesla K20 took 13 hours using Caffe. For<br>SVM training, all ground-truth boxes from val1 +trainN<br>were used as positive examples for their respective classes.<br>Hard negative mining was performed on a randomly se-<br>lected subset of 5000 images from val1. An initial experi-<br>ment indicated that mining negatives from all of val1, versus<br>a 5000 image subset (roughly half of it), resulted in only a<br>0.5 percentage point drop in mAP, while cutting SVM train-<br>ing time in half. No negative examples were taken from</p>",
            "id": 117,
            "page": 9,
            "text": "Training data is required for three procedures in R-CNN: (1) CNN fine-tuning, (2) detector SVM training, and (3) bounding-box regressor training. CNN fine-tuning was run for 50k SGD iteration on val1 +trainN using the exact same settings as were used for PASCAL. Fine-tuning on a single NVIDIA Tesla K20 took 13 hours using Caffe. For SVM training, all ground-truth boxes from val1 +trainN were used as positive examples for their respective classes. Hard negative mining was performed on a randomly selected subset of 5000 images from val1. An initial experiment indicated that mining negatives from all of val1, versus a 5000 image subset (roughly half of it), resulted in only a 0.5 percentage point drop in mAP, while cutting SVM training time in half. No negative examples were taken from"
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 2892
                },
                {
                    "x": 1198,
                    "y": 2892
                },
                {
                    "x": 1198,
                    "y": 2971
                },
                {
                    "x": 203,
                    "y": 2971
                }
            ],
            "category": "paragraph",
            "html": "<p id='118' style='font-size:14px'>2Relative imbalance is measured as |a - b/(a + 6) where a and 6 are<br>class counts in each half of the split.</p>",
            "id": 118,
            "page": 9,
            "text": "2Relative imbalance is measured as |a - b/(a + 6) where a and 6 are class counts in each half of the split."
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 309
                },
                {
                    "x": 2276,
                    "y": 309
                },
                {
                    "x": 2276,
                    "y": 454
                },
                {
                    "x": 1279,
                    "y": 454
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='119' style='font-size:14px'>train because the annotations are not exhaustive. The ex-<br>tra sets of verified negative images were not used. The<br>bounding-box regressors were trained on val1.</p>",
            "id": 119,
            "page": 9,
            "text": "train because the annotations are not exhaustive. The extra sets of verified negative images were not used. The bounding-box regressors were trained on val1."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 488
                },
                {
                    "x": 1869,
                    "y": 488
                },
                {
                    "x": 1869,
                    "y": 536
                },
                {
                    "x": 1281,
                    "y": 536
                }
            ],
            "category": "paragraph",
            "html": "<p id='120' style='font-size:20px'>4.4. Validation and evaluation</p>",
            "id": 120,
            "page": 9,
            "text": "4.4. Validation and evaluation"
        },
        {
            "bounding_box": [
                {
                    "x": 1277,
                    "y": 565
                },
                {
                    "x": 2279,
                    "y": 565
                },
                {
                    "x": 2279,
                    "y": 1560
                },
                {
                    "x": 1277,
                    "y": 1560
                }
            ],
            "category": "paragraph",
            "html": "<p id='121' style='font-size:18px'>Before submitting results to the evaluation server, we<br>validated data usage choices and the effect of fine-tuning<br>and bounding-box regression on the val2 set using the train-<br>ing data described above. All system hyperparameters (e.g.,<br>SVM C hyperparameters, padding used in region warp-<br>ing, NMS thresholds, bounding-box regression hyperpa-<br>rameters) were fixed at the same values used for PAS-<br>CAL. Undoubtedly some of these hyperparameter choices<br>are slightly suboptimal for ILSVRC, however the goal of<br>this work was to produce a preliminary R-CNN result on<br>ILSVRC without extensive dataset tuning. After selecting<br>the best choices on val2, we submitted exactly two result<br>files to the ILSVRC2013 evaluation server. The first sub-<br>mission was without bounding-box regression and the sec-<br>ond submission was with bounding-box regression. For<br>these submissions, we expanded the SVM and bounding-<br>box regressor training sets to use val+train1k and val, re-<br>spectively. We used the CNN that was fine-tuned on<br>val1 +train1k to avoid re-running fine-tuning and feature<br>computation.</p>",
            "id": 121,
            "page": 9,
            "text": "Before submitting results to the evaluation server, we validated data usage choices and the effect of fine-tuning and bounding-box regression on the val2 set using the training data described above. All system hyperparameters (e.g., SVM C hyperparameters, padding used in region warping, NMS thresholds, bounding-box regression hyperparameters) were fixed at the same values used for PASCAL. Undoubtedly some of these hyperparameter choices are slightly suboptimal for ILSVRC, however the goal of this work was to produce a preliminary R-CNN result on ILSVRC without extensive dataset tuning. After selecting the best choices on val2, we submitted exactly two result files to the ILSVRC2013 evaluation server. The first submission was without bounding-box regression and the second submission was with bounding-box regression. For these submissions, we expanded the SVM and boundingbox regressor training sets to use val+train1k and val, respectively. We used the CNN that was fine-tuned on val1 +train1k to avoid re-running fine-tuning and feature computation."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1595
                },
                {
                    "x": 1656,
                    "y": 1595
                },
                {
                    "x": 1656,
                    "y": 1643
                },
                {
                    "x": 1280,
                    "y": 1643
                }
            ],
            "category": "paragraph",
            "html": "<p id='122' style='font-size:20px'>4.5. Ablation study</p>",
            "id": 122,
            "page": 9,
            "text": "4.5. Ablation study"
        },
        {
            "bounding_box": [
                {
                    "x": 1277,
                    "y": 1670
                },
                {
                    "x": 2278,
                    "y": 1670
                },
                {
                    "x": 2278,
                    "y": 2716
                },
                {
                    "x": 1277,
                    "y": 2716
                }
            ],
            "category": "paragraph",
            "html": "<p id='123' style='font-size:18px'>Table 4 shows an ablation study of the effects of differ-<br>ent amounts of training data, fine-tuning, and bounding-<br>box regression. A first observation is that mAP on val2<br>matches mAP on test very closely. This gives us confi-<br>dence that mAP on val2 is a good indicator of test set per-<br>formance. The first result, 20.9%, is what R-CNN achieves<br>using a CNN pre-trained on the ILSVRC2012 classifica-<br>tion dataset (no fine-tuning) and given access to the small<br>amount of training data in val1 (recall that half of the classes<br>in val1 have between 15 and 55 examples). Expanding<br>the training set to val1 +trainN improves performance to<br>24.1%, with essentially no difference between N = 500<br>and N = 1000. Fine-tuning the CNN using examples from<br>just val1 gives a modest improvement to 26.5%, however<br>there is likely significant overfitting due to the small number<br>of positive training examples. Expanding the fine-tuning<br>set to val1 +train1k, which adds up to 1000 positive exam-<br>ples per class from the train set, helps significantly, boosting<br>mAP to 29.7%. Bounding-box regression improves results<br>to 31.0%, which is a smaller relative gain that what was ob-<br>served in PASCAL.</p>",
            "id": 123,
            "page": 9,
            "text": "Table 4 shows an ablation study of the effects of different amounts of training data, fine-tuning, and boundingbox regression. A first observation is that mAP on val2 matches mAP on test very closely. This gives us confidence that mAP on val2 is a good indicator of test set performance. The first result, 20.9%, is what R-CNN achieves using a CNN pre-trained on the ILSVRC2012 classification dataset (no fine-tuning) and given access to the small amount of training data in val1 (recall that half of the classes in val1 have between 15 and 55 examples). Expanding the training set to val1 +trainN improves performance to 24.1%, with essentially no difference between N = 500 and N = 1000. Fine-tuning the CNN using examples from just val1 gives a modest improvement to 26.5%, however there is likely significant overfitting due to the small number of positive training examples. Expanding the fine-tuning set to val1 +train1k, which adds up to 1000 positive examples per class from the train set, helps significantly, boosting mAP to 29.7%. Bounding-box regression improves results to 31.0%, which is a smaller relative gain that what was observed in PASCAL."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 2751
                },
                {
                    "x": 1859,
                    "y": 2751
                },
                {
                    "x": 1859,
                    "y": 2799
                },
                {
                    "x": 1281,
                    "y": 2799
                }
            ],
            "category": "paragraph",
            "html": "<p id='124' style='font-size:22px'>4.6. Relationship to OverFeat</p>",
            "id": 124,
            "page": 9,
            "text": "4.6. Relationship to OverFeat"
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 2827
                },
                {
                    "x": 2276,
                    "y": 2827
                },
                {
                    "x": 2276,
                    "y": 2976
                },
                {
                    "x": 1281,
                    "y": 2976
                }
            ],
            "category": "paragraph",
            "html": "<p id='125' style='font-size:16px'>There is an interesting relationship between R-CNN and<br>OverFeat: OverFeat can be seen (roughly) as a special case<br>of R-CNN. If one were to replace selective search region</p>",
            "id": 125,
            "page": 9,
            "text": "There is an interesting relationship between R-CNN and OverFeat: OverFeat can be seen (roughly) as a special case of R-CNN. If one were to replace selective search region"
        },
        {
            "bounding_box": [
                {
                    "x": 1225,
                    "y": 3056
                },
                {
                    "x": 1251,
                    "y": 3056
                },
                {
                    "x": 1251,
                    "y": 3089
                },
                {
                    "x": 1225,
                    "y": 3089
                }
            ],
            "category": "footer",
            "html": "<footer id='126' style='font-size:14px'>9</footer>",
            "id": 126,
            "page": 9,
            "text": "9"
        },
        {
            "bounding_box": [
                {
                    "x": 206,
                    "y": 297
                },
                {
                    "x": 2272,
                    "y": 297
                },
                {
                    "x": 2272,
                    "y": 703
                },
                {
                    "x": 206,
                    "y": 703
                }
            ],
            "category": "figure",
            "html": "<figure><img id='127' style='font-size:20px' alt=\"test set val2 val2 val2 val2 val2 val2 test test\nSVM training set val1 val1 +train.5k val1 +train1k val1 +train1k val1 +train1k val1 +train1k val+train1k val+train1k\nCNN fine-tuning set n/a n/a n/a val1 val1 +train1k val1 +train1k val1 +train1k val1 +train1k\nbbox reg set n/a n/a n/a n/a n/a val1 n/a val\nCNN feature layer fc6 fc6 fc6 fc7 fc7 fc7 fc7 fc7\nmAP 20.9 24.1 24.1 26.5 29.7 31.0 30.2 31.4\nmedian AP 17.7 21.0 21.4 24.8 29.2 29.6 29.0 30.3\" data-coord=\"top-left:(206,297); bottom-right:(2272,703)\" /></figure>",
            "id": 127,
            "page": 10,
            "text": "test set val2 val2 val2 val2 val2 val2 test test SVM training set val1 val1 +train.5k val1 +train1k val1 +train1k val1 +train1k val1 +train1k val+train1k val+train1k CNN fine-tuning set n/a n/a n/a val1 val1 +train1k val1 +train1k val1 +train1k val1 +train1k bbox reg set n/a n/a n/a n/a n/a val1 n/a val CNN feature layer fc6 fc6 fc6 fc7 fc7 fc7 fc7 fc7 mAP 20.9 24.1 24.1 26.5 29.7 31.0 30.2 31.4 median AP 17.7 21.0 21.4 24.8 29.2 29.6 29.0 30.3"
        },
        {
            "bounding_box": [
                {
                    "x": 451,
                    "y": 722
                },
                {
                    "x": 2027,
                    "y": 722
                },
                {
                    "x": 2027,
                    "y": 768
                },
                {
                    "x": 451,
                    "y": 768
                }
            ],
            "category": "caption",
            "html": "<br><caption id='128' style='font-size:16px'>Table 4: ILSVRC2013 ablation study of data usage choices, fine-tuning, and bounding-box regression.</caption>",
            "id": 128,
            "page": 10,
            "text": "Table 4: ILSVRC2013 ablation study of data usage choices, fine-tuning, and bounding-box regression."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 858
                },
                {
                    "x": 1199,
                    "y": 858
                },
                {
                    "x": 1199,
                    "y": 1607
                },
                {
                    "x": 200,
                    "y": 1607
                }
            ],
            "category": "paragraph",
            "html": "<p id='129' style='font-size:20px'>proposals with a multi-scale pyramid of regular square re-<br>gions and change the per-class bounding-box regressors to<br>a single bounding-box regressor, then the systems would<br>be very similar (modulo some potentially significant differ-<br>ences in how they are trained: CNN detection fine-tuning,<br>using SVMs, etc.). It is worth noting that OverFeat has<br>a significant speed advantage over R-CNN: it is about 9x<br>faster, based on a figure of 2 seconds per image quoted from<br>[34]. This speed comes from the fact that OverFeat's slid-<br>ing windows (i.e., region proposals) are not warped at the<br>image level and therefore computation can be easily shared<br>between overlapping windows. Sharing is implemented by<br>running the entire network in a convolutional fashion over<br>arbitrary-sized inputs. Speeding up R-CNN should be pos-<br>sible in a variety of ways and remains as future work.</p>",
            "id": 129,
            "page": 10,
            "text": "proposals with a multi-scale pyramid of regular square regions and change the per-class bounding-box regressors to a single bounding-box regressor, then the systems would be very similar (modulo some potentially significant differences in how they are trained: CNN detection fine-tuning, using SVMs, etc.). It is worth noting that OverFeat has a significant speed advantage over R-CNN: it is about 9x faster, based on a figure of 2 seconds per image quoted from . This speed comes from the fact that OverFeat's sliding windows (i.e., region proposals) are not warped at the image level and therefore computation can be easily shared between overlapping windows. Sharing is implemented by running the entire network in a convolutional fashion over arbitrary-sized inputs. Speeding up R-CNN should be possible in a variety of ways and remains as future work."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 1654
                },
                {
                    "x": 752,
                    "y": 1654
                },
                {
                    "x": 752,
                    "y": 1709
                },
                {
                    "x": 202,
                    "y": 1709
                }
            ],
            "category": "paragraph",
            "html": "<p id='130' style='font-size:22px'>5. Semantic segmentation</p>",
            "id": 130,
            "page": 10,
            "text": "5. Semantic segmentation"
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 1741
                },
                {
                    "x": 1199,
                    "y": 1741
                },
                {
                    "x": 1199,
                    "y": 2485
                },
                {
                    "x": 200,
                    "y": 2485
                }
            ],
            "category": "paragraph",
            "html": "<p id='131' style='font-size:20px'>Region classification is a standard technique for seman-<br>tic segmentation, allowing us to easily apply R-CNN to the<br>PASCAL VOC segmentation challenge. To facilitate a di-<br>rect comparison with the current leading semantic segmen-<br>tation system (called O2P for \"second-order pooling\") [4],<br>we work within their open source framework. O2P uses<br>CPMC to generate 150 region proposals per image and then<br>predicts the quality of each region, for each class, using<br>support vector regression (SVR). The high performance of<br>their approach is due to the quality of the CPMC regions<br>and the powerful second-order pooling of multiple feature<br>types (enriched variants of SIFT and LBP). We also note<br>that Farabet et al. [16] recently demonstrated good results<br>on several dense scene labeling datasets (not including PAS-<br>CAL) using a CNN as a multi-scale per-pixel classifier.</p>",
            "id": 131,
            "page": 10,
            "text": "Region classification is a standard technique for semantic segmentation, allowing us to easily apply R-CNN to the PASCAL VOC segmentation challenge. To facilitate a direct comparison with the current leading semantic segmentation system (called O2P for \"second-order pooling\") , we work within their open source framework. O2P uses CPMC to generate 150 region proposals per image and then predicts the quality of each region, for each class, using support vector regression (SVR). The high performance of their approach is due to the quality of the CPMC regions and the powerful second-order pooling of multiple feature types (enriched variants of SIFT and LBP). We also note that Farabet   recently demonstrated good results on several dense scene labeling datasets (not including PASCAL) using a CNN as a multi-scale per-pixel classifier."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2493
                },
                {
                    "x": 1199,
                    "y": 2493
                },
                {
                    "x": 1199,
                    "y": 2737
                },
                {
                    "x": 201,
                    "y": 2737
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='132' style='font-size:16px'>We follow [2, 4] and extend the PASCAL segmentation<br>training set to include the extra annotations made available<br>by Hariharan et al. [22]. Design decisions and hyperparam-<br>eters were cross-validated on the VOC 2011 validation set.<br>Final test results were evaluated only once.</p>",
            "id": 132,
            "page": 10,
            "text": "We follow  and extend the PASCAL segmentation training set to include the extra annotations made available by Hariharan  . Design decisions and hyperparameters were cross-validated on the VOC 2011 validation set. Final test results were evaluated only once."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2776
                },
                {
                    "x": 1199,
                    "y": 2776
                },
                {
                    "x": 1199,
                    "y": 2977
                },
                {
                    "x": 202,
                    "y": 2977
                }
            ],
            "category": "paragraph",
            "html": "<p id='133' style='font-size:20px'>CNN features for segmentation. We evaluate three strate-<br>gies for computing features on CPMC regions, all of which<br>begin by warping the rectangular window around the re-<br>gion to 227 x 227. The first strategy (full) ignores the re-</p>",
            "id": 133,
            "page": 10,
            "text": "CNN features for segmentation. We evaluate three strategies for computing features on CPMC regions, all of which begin by warping the rectangular window around the region to 227 x 227. The first strategy (full) ignores the re-"
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 858
                },
                {
                    "x": 2277,
                    "y": 858
                },
                {
                    "x": 2277,
                    "y": 1406
                },
                {
                    "x": 1278,
                    "y": 1406
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='134' style='font-size:20px'>gion's shape and computes CNN features directly on the<br>warped window, exactly as we did for detection. However,<br>these features ignore the non-rectangular shape of the re-<br>gion. Two regions might have very similar bounding boxes<br>while having very little overlap. Therefore, the second strat-<br>egy (fg) computes CNN features only on a region's fore-<br>ground mask. We replace the background with the mean<br>input SO that background regions are zero after mean sub-<br>traction. The third strategy (full+fg) simply concatenates<br>the full and fg features; our experiments validate their com-<br>plementarity.</p>",
            "id": 134,
            "page": 10,
            "text": "gion's shape and computes CNN features directly on the warped window, exactly as we did for detection. However, these features ignore the non-rectangular shape of the region. Two regions might have very similar bounding boxes while having very little overlap. Therefore, the second strategy (fg) computes CNN features only on a region's foreground mask. We replace the background with the mean input SO that background regions are zero after mean subtraction. The third strategy (full+fg) simply concatenates the full and fg features; our experiments validate their complementarity."
        },
        {
            "bounding_box": [
                {
                    "x": 1323,
                    "y": 1462
                },
                {
                    "x": 2223,
                    "y": 1462
                },
                {
                    "x": 2223,
                    "y": 1604
                },
                {
                    "x": 1323,
                    "y": 1604
                }
            ],
            "category": "table",
            "html": "<table id='135' style='font-size:18px'><tr><td></td><td colspan=\"2\">full R-CNN</td><td colspan=\"2\">fg R-CNN</td><td colspan=\"2\">full+fg R-CNN</td></tr><tr><td>O2P [4]</td><td>fc6</td><td>fc7</td><td>fc6</td><td>fc7</td><td>fc6</td><td>fc7</td></tr><tr><td>46.4</td><td>43.0</td><td>42.5</td><td>43.7</td><td>42.1</td><td>47.9</td><td>45.8</td></tr></table>",
            "id": 135,
            "page": 10,
            "text": "full R-CNN fg R-CNN full+fg R-CNN  O2P  fc6 fc7 fc6 fc7 fc6 fc7  46.4 43.0 42.5 43.7 42.1 47.9"
        },
        {
            "bounding_box": [
                {
                    "x": 1283,
                    "y": 1629
                },
                {
                    "x": 2276,
                    "y": 1629
                },
                {
                    "x": 2276,
                    "y": 1762
                },
                {
                    "x": 1283,
                    "y": 1762
                }
            ],
            "category": "caption",
            "html": "<caption id='136' style='font-size:14px'>Table 5: Segmentation mean accuracy (%) on VOC 2011 vali-<br>dation. Column 1 presents O2P; 2-7 use our CNN pre-trained on<br>ILSVRC 2012.</caption>",
            "id": 136,
            "page": 10,
            "text": "Table 5: Segmentation mean accuracy (%) on VOC 2011 validation. Column 1 presents O2P; 2-7 use our CNN pre-trained on ILSVRC 2012."
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 1870
                },
                {
                    "x": 2277,
                    "y": 1870
                },
                {
                    "x": 2277,
                    "y": 2567
                },
                {
                    "x": 1278,
                    "y": 2567
                }
            ],
            "category": "paragraph",
            "html": "<p id='137' style='font-size:20px'>Results on VOC 2011. Table 5 shows a summary of our<br>results on the VOC 2011 validation set compared with O2P.<br>(See Appendix E for complete per-category results.) Within<br>each feature computation strategy, layer fc6 always outper-<br>forms fc7 and the following discussion refers to the fc6 fea-<br>tures. The fg strategy slightly outperforms full, indicating<br>that the masked region shape provides a stronger signal,<br>matching our intuition. However, full+fg achieves an aver-<br>age accuracy of 47.9%, our best result by a margin of 4.2%<br>(also modestly outperforming O2P), indicating that the con-<br>text provided by the full features is highly informative even<br>given the fg features. Notably, training the 20 SVRs on our<br>full+fg features takes an hour on a single core, compared to<br>10+ hours for training on O2P features.</p>",
            "id": 137,
            "page": 10,
            "text": "Results on VOC 2011. Table 5 shows a summary of our results on the VOC 2011 validation set compared with O2P. (See Appendix E for complete per-category results.) Within each feature computation strategy, layer fc6 always outperforms fc7 and the following discussion refers to the fc6 features. The fg strategy slightly outperforms full, indicating that the masked region shape provides a stronger signal, matching our intuition. However, full+fg achieves an average accuracy of 47.9%, our best result by a margin of 4.2% (also modestly outperforming O2P), indicating that the context provided by the full features is highly informative even given the fg features. Notably, training the 20 SVRs on our full+fg features takes an hour on a single core, compared to 10+ hours for training on O2P features."
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 2578
                },
                {
                    "x": 2277,
                    "y": 2578
                },
                {
                    "x": 2277,
                    "y": 2977
                },
                {
                    "x": 1279,
                    "y": 2977
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='138' style='font-size:20px'>In Table 6 we present results on the VOC 2011 test<br>set, comparing our best-performing method, fc6 (full+fg),<br>against two strong baselines. Our method achieves the high-<br>est segmentation accuracy for 11 out of 21 categories, and<br>the highest overall segmentation accuracy of 47.9%, aver-<br>aged across categories (but likely ties with the O2P result<br>under any reasonable margin of error). Still better perfor-<br>mance could likely be achieved by fine-tuning.</p>",
            "id": 138,
            "page": 10,
            "text": "In Table 6 we present results on the VOC 2011 test set, comparing our best-performing method, fc6 (full+fg), against two strong baselines. Our method achieves the highest segmentation accuracy for 11 out of 21 categories, and the highest overall segmentation accuracy of 47.9%, averaged across categories (but likely ties with the O2P result under any reasonable margin of error). Still better performance could likely be achieved by fine-tuning."
        },
        {
            "bounding_box": [
                {
                    "x": 1219,
                    "y": 3055
                },
                {
                    "x": 1263,
                    "y": 3055
                },
                {
                    "x": 1263,
                    "y": 3092
                },
                {
                    "x": 1219,
                    "y": 3092
                }
            ],
            "category": "footer",
            "html": "<footer id='139' style='font-size:18px'>10</footer>",
            "id": 139,
            "page": 10,
            "text": "10"
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 292
                },
                {
                    "x": 2272,
                    "y": 292
                },
                {
                    "x": 2272,
                    "y": 473
                },
                {
                    "x": 203,
                    "y": 473
                }
            ],
            "category": "table",
            "html": "<table id='140' style='font-size:14px'><tr><td>VOC 2011 test</td><td>bg</td><td>aero</td><td>bike</td><td>bird</td><td>boat</td><td>bottle</td><td>bus</td><td>car</td><td>cat</td><td>chair</td><td>COW</td><td>table</td><td>dog</td><td>horse</td><td>mbike</td><td>person</td><td>plant</td><td>sheep</td><td>sofa</td><td>train</td><td>tv</td><td></td><td>mean</td></tr><tr><td>R&P [2]</td><td>83.4</td><td>46.8</td><td>18.9</td><td>36.6</td><td>31.2</td><td>42.7</td><td>57.3</td><td>47.4</td><td>44.1</td><td>8.1</td><td>39.4</td><td>36.1</td><td>36.3</td><td></td><td>49.5</td><td>48.3</td><td>50.7</td><td>26.3</td><td>47.2</td><td>22.1</td><td>42.0</td><td>43.2</td><td>40.8</td></tr><tr><td>O2P [4]</td><td>85.4</td><td>69.7</td><td>22.3</td><td>45.2</td><td>44.4</td><td>46.9</td><td>66.7</td><td>57.8</td><td>56.2</td><td>13.5</td><td>46.1</td><td>32.3</td><td>41.2</td><td>59.1</td><td></td><td>55.3</td><td>51.0</td><td>36.2</td><td>50.4</td><td>27.8</td><td>46.9</td><td>44.6</td><td>47.6</td></tr><tr><td>ours (full+fg R-CNN fc6)</td><td>84.2</td><td>66.9</td><td>23.7</td><td>58.3</td><td>37.4</td><td>55.4</td><td>73.3</td><td>58.7</td><td>56.5</td><td>9.7</td><td>45.5</td><td>29.5</td><td>49.3</td><td></td><td>40.1</td><td>57.8</td><td>53.9</td><td>33.8</td><td>60.7</td><td>22.7</td><td>47.1</td><td>41.3</td><td>47.9</td></tr></table>",
            "id": 140,
            "page": 11,
            "text": "VOC 2011 test bg aero bike bird boat bottle bus car cat chair COW table dog horse mbike person plant sheep sofa train tv  mean  R&P  83.4 46.8 18.9 36.6 31.2 42.7 57.3 47.4 44.1 8.1 39.4 36.1 36.3  49.5 48.3 50.7 26.3 47.2 22.1 42.0 43.2 40.8  O2P  85.4 69.7 22.3 45.2 44.4 46.9 66.7 57.8 56.2 13.5 46.1 32.3 41.2 59.1  55.3 51.0 36.2 50.4 27.8 46.9 44.6 47.6  ours (full+fg R-CNN fc6) 84.2 66.9 23.7 58.3 37.4 55.4 73.3 58.7 56.5 9.7 45.5 29.5 49.3  40.1 57.8 53.9 33.8 60.7 22.7 47.1 41.3"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 496
                },
                {
                    "x": 2275,
                    "y": 496
                },
                {
                    "x": 2275,
                    "y": 631
                },
                {
                    "x": 202,
                    "y": 631
                }
            ],
            "category": "caption",
            "html": "<br><caption id='141' style='font-size:16px'>Table 6: Segmentation accuracy (%) on VOC 2011 test. We compare against two strong baselines: the \"Regions and Parts\" (R&P)<br>method of [2] and the second-order pooling (O2P) method of [4]. Without any fine-tuning, our CNN achieves top segmentation perfor-<br>mance, outperforming R&P and roughly matching O2P.</caption>",
            "id": 141,
            "page": 11,
            "text": "Table 6: Segmentation accuracy (%) on VOC 2011 test. We compare against two strong baselines: the \"Regions and Parts\" (R&P) method of  and the second-order pooling (O2P) method of . Without any fine-tuning, our CNN achieves top segmentation performance, outperforming R&P and roughly matching O2P."
        },
        {
            "bounding_box": [
                {
                    "x": 205,
                    "y": 678
                },
                {
                    "x": 499,
                    "y": 678
                },
                {
                    "x": 499,
                    "y": 725
                },
                {
                    "x": 205,
                    "y": 725
                }
            ],
            "category": "paragraph",
            "html": "<p id='142' style='font-size:20px'>6. Conclusion</p>",
            "id": 142,
            "page": 11,
            "text": "6. Conclusion"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 762
                },
                {
                    "x": 1197,
                    "y": 762
                },
                {
                    "x": 1197,
                    "y": 1106
                },
                {
                    "x": 202,
                    "y": 1106
                }
            ],
            "category": "paragraph",
            "html": "<p id='143' style='font-size:18px'>In recent years, object detection performance had stag-<br>nated. The best performing systems were complex en-<br>sembles combining multiple low-level image features with<br>high-level context from object detectors and scene classi-<br>fiers. This paper presents a simple and scalable object de-<br>tection algorithm that gives a 30% relative improvement<br>over the best previous results on PASCAL VOC 2012.</p>",
            "id": 143,
            "page": 11,
            "text": "In recent years, object detection performance had stagnated. The best performing systems were complex ensembles combining multiple low-level image features with high-level context from object detectors and scene classifiers. This paper presents a simple and scalable object detection algorithm that gives a 30% relative improvement over the best previous results on PASCAL VOC 2012."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 1116
                },
                {
                    "x": 1198,
                    "y": 1116
                },
                {
                    "x": 1198,
                    "y": 1706
                },
                {
                    "x": 200,
                    "y": 1706
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='144' style='font-size:18px'>We achieved this performance through two insights. The<br>first is to apply high-capacity convolutional neural net-<br>works to bottom-up region proposals in order to localize<br>and segment objects. The second is a paradigm for train-<br>ing large CNNs when labeled training data is scarce. We<br>show that it is highly effective to pre-train the network- -<br>with supervision-for a auxiliary task with abundant data<br>(image classification) and then to fine-tune the network for<br>the target task where data is scarce (detection). We conjec-<br>ture that the \"supervised pre-training/domain-specific fine-<br>tuning\" paradigm will be highly effective for a variety of<br>data-scarce vision problems.</p>",
            "id": 144,
            "page": 11,
            "text": "We achieved this performance through two insights. The first is to apply high-capacity convolutional neural networks to bottom-up region proposals in order to localize and segment objects. The second is a paradigm for training large CNNs when labeled training data is scarce. We show that it is highly effective to pre-train the network- with supervision-for a auxiliary task with abundant data (image classification) and then to fine-tune the network for the target task where data is scarce (detection). We conjecture that the \"supervised pre-training/domain-specific finetuning\" paradigm will be highly effective for a variety of data-scarce vision problems."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 1715
                },
                {
                    "x": 1198,
                    "y": 1715
                },
                {
                    "x": 1198,
                    "y": 2009
                },
                {
                    "x": 202,
                    "y": 2009
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='145' style='font-size:18px'>We conclude by noting that it is significant that we<br>achieved these results by using a combination of classi-<br>cal tools from computer vision and deep learning (bottom-<br>up region proposals and convolutional neural networks).<br>Rather than opposing lines of scientific inquiry, the two are<br>natural and inevitable partners.</p>",
            "id": 145,
            "page": 11,
            "text": "We conclude by noting that it is significant that we achieved these results by using a combination of classical tools from computer vision and deep learning (bottomup region proposals and convolutional neural networks). Rather than opposing lines of scientific inquiry, the two are natural and inevitable partners."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2052
                },
                {
                    "x": 1198,
                    "y": 2052
                },
                {
                    "x": 1198,
                    "y": 2348
                },
                {
                    "x": 201,
                    "y": 2348
                }
            ],
            "category": "paragraph",
            "html": "<p id='146' style='font-size:18px'>Acknowledgments. This research was supported in part<br>by DARPA Mind's Eye and MSEE programs, by NSF<br>awards IIS-0905647, IIS-1134072, and IIS-1212798,<br>MURI N000014-10-1-0933, and by support from Toyota.<br>The GPUs used in this research were generously donated<br>by the NVIDIA Corporation.</p>",
            "id": 146,
            "page": 11,
            "text": "Acknowledgments. This research was supported in part by DARPA Mind's Eye and MSEE programs, by NSF awards IIS-0905647, IIS-1134072, and IIS-1212798, MURI N000014-10-1-0933, and by support from Toyota. The GPUs used in this research were generously donated by the NVIDIA Corporation."
        },
        {
            "bounding_box": [
                {
                    "x": 205,
                    "y": 2428
                },
                {
                    "x": 459,
                    "y": 2428
                },
                {
                    "x": 459,
                    "y": 2489
                },
                {
                    "x": 205,
                    "y": 2489
                }
            ],
            "category": "paragraph",
            "html": "<p id='147' style='font-size:22px'>Appendix</p>",
            "id": 147,
            "page": 11,
            "text": "Appendix"
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 2541
                },
                {
                    "x": 968,
                    "y": 2541
                },
                {
                    "x": 968,
                    "y": 2593
                },
                {
                    "x": 204,
                    "y": 2593
                }
            ],
            "category": "paragraph",
            "html": "<p id='148' style='font-size:20px'>A. Object proposal transformations</p>",
            "id": 148,
            "page": 11,
            "text": "A. Object proposal transformations"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2628
                },
                {
                    "x": 1198,
                    "y": 2628
                },
                {
                    "x": 1198,
                    "y": 2872
                },
                {
                    "x": 202,
                    "y": 2872
                }
            ],
            "category": "paragraph",
            "html": "<p id='149' style='font-size:18px'>The convolutional neural network used in this work re-<br>quires a fixed-size input of 227 x 227 pixels. For detec-<br>tion, we consider object proposals that are arbitrary image<br>rectangles. We evaluated two approaches for transforming<br>object proposals into valid CNN inputs.</p>",
            "id": 149,
            "page": 11,
            "text": "The convolutional neural network used in this work requires a fixed-size input of 227 x 227 pixels. For detection, we consider object proposals that are arbitrary image rectangles. We evaluated two approaches for transforming object proposals into valid CNN inputs."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2880
                },
                {
                    "x": 1196,
                    "y": 2880
                },
                {
                    "x": 1196,
                    "y": 2976
                },
                {
                    "x": 202,
                    "y": 2976
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='150' style='font-size:18px'>The first method (\"tightest square with context\") en-<br>closes each object proposal inside the tightest square and</p>",
            "id": 150,
            "page": 11,
            "text": "The first method (\"tightest square with context\") encloses each object proposal inside the tightest square and"
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 667
                },
                {
                    "x": 2278,
                    "y": 667
                },
                {
                    "x": 2278,
                    "y": 1255
                },
                {
                    "x": 1278,
                    "y": 1255
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='151' style='font-size:16px' alt=\"(A) (B) (C) (D) (A) (B) (C) (D)\" data-coord=\"top-left:(1278,667); bottom-right:(2278,1255)\" /></figure>",
            "id": 151,
            "page": 11,
            "text": "(A) (B) (C) (D) (A) (B) (C) (D)"
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 1281
                },
                {
                    "x": 2277,
                    "y": 1281
                },
                {
                    "x": 2277,
                    "y": 1603
                },
                {
                    "x": 1278,
                    "y": 1603
                }
            ],
            "category": "caption",
            "html": "<caption id='152' style='font-size:14px'>Figure 7: Different object proposal transformations. (A) the<br>original object proposal at its actual scale relative to the trans-<br>formed CNN inputs; (B) tightest square with context; (C) tight-<br>est square without context; (D) warp. Within each column and<br>example proposal, the top row corresponds to p = 0 pixels of con-<br>text padding while the bottom row has p = 16 pixels of context<br>padding.</caption>",
            "id": 152,
            "page": 11,
            "text": "Figure 7: Different object proposal transformations. (A) the original object proposal at its actual scale relative to the transformed CNN inputs; (B) tightest square with context; (C) tightest square without context; (D) warp. Within each column and example proposal, the top row corresponds to p = 0 pixels of context padding while the bottom row has p = 16 pixels of context padding."
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 1753
                },
                {
                    "x": 2276,
                    "y": 1753
                },
                {
                    "x": 2276,
                    "y": 2197
                },
                {
                    "x": 1279,
                    "y": 2197
                }
            ],
            "category": "paragraph",
            "html": "<p id='153' style='font-size:18px'>then scales (isotropically) the image contained in that<br>square to the CNN input size. Figure 7 column (B) shows<br>this transformation. A variant on this method (\"tightest<br>square without context\") excludes the image content that<br>surrounds the original object proposal. Figure 7 column<br>(C) shows this transformation. The second method (\"warp\")<br>anisotropically scales each object proposal to the CNN in-<br>put size. Figure 7 column (D) shows the warp transforma-<br>tion.</p>",
            "id": 153,
            "page": 11,
            "text": "then scales (isotropically) the image contained in that square to the CNN input size. Figure 7 column (B) shows this transformation. A variant on this method (\"tightest square without context\") excludes the image content that surrounds the original object proposal. Figure 7 column (C) shows this transformation. The second method (\"warp\") anisotropically scales each object proposal to the CNN input size. Figure 7 column (D) shows the warp transformation."
        },
        {
            "bounding_box": [
                {
                    "x": 1277,
                    "y": 2229
                },
                {
                    "x": 2277,
                    "y": 2229
                },
                {
                    "x": 2277,
                    "y": 2976
                },
                {
                    "x": 1277,
                    "y": 2976
                }
            ],
            "category": "paragraph",
            "html": "<p id='154' style='font-size:18px'>For each of these transformations, we also consider in-<br>cluding additional image context around the original object<br>proposal. The amount of context padding (p) is defined as a<br>border size around the original object proposal in the trans-<br>formed input coordinate frame. Figure 7 shows p = 0 pix-<br>els in the top row of each example and p = 16 pixels in<br>the bottom row. In all methods, if the source rectangle ex-<br>tends beyond the image, the missing data is replaced with<br>the image mean (which is then subtracted before inputing<br>the image into the CNN). A pilot set of experiments showed<br>that warping with context padding (p = 16 pixels) outper-<br>formed the alternatives by a large margin (3-5 mAP points).<br>Obviously more alternatives are possible, including using<br>replication instead of mean padding. Exhaustive evaluation<br>of these alternatives is left as future work.</p>",
            "id": 154,
            "page": 11,
            "text": "For each of these transformations, we also consider including additional image context around the original object proposal. The amount of context padding (p) is defined as a border size around the original object proposal in the transformed input coordinate frame. Figure 7 shows p = 0 pixels in the top row of each example and p = 16 pixels in the bottom row. In all methods, if the source rectangle extends beyond the image, the missing data is replaced with the image mean (which is then subtracted before inputing the image into the CNN). A pilot set of experiments showed that warping with context padding (p = 16 pixels) outperformed the alternatives by a large margin (3-5 mAP points). Obviously more alternatives are possible, including using replication instead of mean padding. Exhaustive evaluation of these alternatives is left as future work."
        },
        {
            "bounding_box": [
                {
                    "x": 1217,
                    "y": 3052
                },
                {
                    "x": 1260,
                    "y": 3052
                },
                {
                    "x": 1260,
                    "y": 3096
                },
                {
                    "x": 1217,
                    "y": 3096
                }
            ],
            "category": "footer",
            "html": "<footer id='155' style='font-size:16px'>11</footer>",
            "id": 155,
            "page": 11,
            "text": "11"
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 302
                },
                {
                    "x": 1178,
                    "y": 302
                },
                {
                    "x": 1178,
                    "y": 354
                },
                {
                    "x": 203,
                    "y": 354
                }
            ],
            "category": "paragraph",
            "html": "<p id='156' style='font-size:20px'>B. Positive VS. negative examples and softmax</p>",
            "id": 156,
            "page": 12,
            "text": "B. Positive VS. negative examples and softmax"
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 385
                },
                {
                    "x": 1198,
                    "y": 385
                },
                {
                    "x": 1198,
                    "y": 1133
                },
                {
                    "x": 200,
                    "y": 1133
                }
            ],
            "category": "paragraph",
            "html": "<p id='157' style='font-size:14px'>Two design choices warrant further discussion. The first<br>is: Why are positive and negative examples defined differ-<br>ently for fine-tuning the CNN versus training the object de-<br>tection SVMs? To review the definitions briefly, for fine-<br>tuning we map each object proposal to the ground-truth in-<br>stance with which it has maximum IoU overlap (if any) and<br>label it as a positive for the matched ground-truth class if the<br>IoU is at least 0.5. All other proposals are labeled \"back-<br>ground\" (i.e., negative examples for all classes). For train-<br>ing SVMs, in contrast, we take only the ground-truth boxes<br>as positive examples for their respective classes and label<br>proposals with less than 0.3 IoU overlap with all instances<br>of a class as a negative for that class. Proposals that fall<br>into the grey zone (more than 0.3 IoU overlap, but are not<br>ground truth) are ignored.</p>",
            "id": 157,
            "page": 12,
            "text": "Two design choices warrant further discussion. The first is: Why are positive and negative examples defined differently for fine-tuning the CNN versus training the object detection SVMs? To review the definitions briefly, for finetuning we map each object proposal to the ground-truth instance with which it has maximum IoU overlap (if any) and label it as a positive for the matched ground-truth class if the IoU is at least 0.5. All other proposals are labeled \"background\" (i.e., negative examples for all classes). For training SVMs, in contrast, we take only the ground-truth boxes as positive examples for their respective classes and label proposals with less than 0.3 IoU overlap with all instances of a class as a negative for that class. Proposals that fall into the grey zone (more than 0.3 IoU overlap, but are not ground truth) are ignored."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 1135
                },
                {
                    "x": 1199,
                    "y": 1135
                },
                {
                    "x": 1199,
                    "y": 1727
                },
                {
                    "x": 200,
                    "y": 1727
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='158' style='font-size:14px'>Historically speaking, we arrived at these definitions be-<br>cause we started by training SVMs on features computed<br>by the ImageNet pre-trained CNN, and SO fine-tuning was<br>not a consideration at that point in time. In that setup, we<br>found that our particular label definition for training SVMs<br>was optimal within the set of options we evaluated (which<br>included the setting we now use for fine-tuning). When we<br>started using fine-tuning, we initially used the same positive<br>and negative example definition as we were using for SVM<br>training. However, we found that results were much worse<br>than those obtained using our current definition of positives<br>and negatives.</p>",
            "id": 158,
            "page": 12,
            "text": "Historically speaking, we arrived at these definitions because we started by training SVMs on features computed by the ImageNet pre-trained CNN, and SO fine-tuning was not a consideration at that point in time. In that setup, we found that our particular label definition for training SVMs was optimal within the set of options we evaluated (which included the setting we now use for fine-tuning). When we started using fine-tuning, we initially used the same positive and negative example definition as we were using for SVM training. However, we found that results were much worse than those obtained using our current definition of positives and negatives."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1733
                },
                {
                    "x": 1198,
                    "y": 1733
                },
                {
                    "x": 1198,
                    "y": 2276
                },
                {
                    "x": 201,
                    "y": 2276
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='159' style='font-size:14px'>Our hypothesis is that this difference in how positives<br>and negatives are defined is not fundamentally important<br>and arises from the fact that fine-tuning data is limited.<br>Our current scheme introduces many \"jittered\" examples<br>(those proposals with overlap between 0.5 and 1, but not<br>ground truth), which expands the number of positive exam-<br>ples by approximately 30x. We conjecture that this large<br>set is needed when fine-tuning the entire network to avoid<br>overfitting. However, we also note that using these jittered<br>examples is likely suboptimal because the network is not<br>being fine-tuned for precise localization.</p>",
            "id": 159,
            "page": 12,
            "text": "Our hypothesis is that this difference in how positives and negatives are defined is not fundamentally important and arises from the fact that fine-tuning data is limited. Our current scheme introduces many \"jittered\" examples (those proposals with overlap between 0.5 and 1, but not ground truth), which expands the number of positive examples by approximately 30x. We conjecture that this large set is needed when fine-tuning the entire network to avoid overfitting. However, we also note that using these jittered examples is likely suboptimal because the network is not being fine-tuned for precise localization."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 2281
                },
                {
                    "x": 1199,
                    "y": 2281
                },
                {
                    "x": 1199,
                    "y": 2874
                },
                {
                    "x": 200,
                    "y": 2874
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='160' style='font-size:14px'>This leads to the second issue: Why, after fine-tuning,<br>train SVMs at all? It would be cleaner to simply apply the<br>last layer of the fine-tuned network, which is a 21-way soft-<br>max regression classifier, as the object detector. We tried<br>this and found that performance on VOC 2007 dropped<br>from 54.2% to 50.9% mAP. This performance drop likely<br>arises from a combination of several factors including that<br>the definition of positive examples used in fine-tuning does<br>not emphasize precise localization and the softmax classi-<br>fier was trained on randomly sampled negative examples<br>rather than on the subset of \"hard negatives\" used for SVM<br>training.</p>",
            "id": 160,
            "page": 12,
            "text": "This leads to the second issue: Why, after fine-tuning, train SVMs at all? It would be cleaner to simply apply the last layer of the fine-tuned network, which is a 21-way softmax regression classifier, as the object detector. We tried this and found that performance on VOC 2007 dropped from 54.2% to 50.9% mAP. This performance drop likely arises from a combination of several factors including that the definition of positive examples used in fine-tuning does not emphasize precise localization and the softmax classifier was trained on randomly sampled negative examples rather than on the subset of \"hard negatives\" used for SVM training."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2879
                },
                {
                    "x": 1198,
                    "y": 2879
                },
                {
                    "x": 1198,
                    "y": 2975
                },
                {
                    "x": 202,
                    "y": 2975
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='161' style='font-size:14px'>This result shows that it's possible to obtain close to<br>the same level of performance without training SVMs af-</p>",
            "id": 161,
            "page": 12,
            "text": "This result shows that it's possible to obtain close to the same level of performance without training SVMs af-"
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 307
                },
                {
                    "x": 2277,
                    "y": 307
                },
                {
                    "x": 2277,
                    "y": 505
                },
                {
                    "x": 1280,
                    "y": 505
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='162' style='font-size:14px'>ter fine-tuning. We conjecture that with some additional<br>tweaks to fine-tuning the remaining performance gap may<br>be closed. If true, this would simplify and speed up R-CNN<br>training with no loss in detection performance.</p>",
            "id": 162,
            "page": 12,
            "text": "ter fine-tuning. We conjecture that with some additional tweaks to fine-tuning the remaining performance gap may be closed. If true, this would simplify and speed up R-CNN training with no loss in detection performance."
        },
        {
            "bounding_box": [
                {
                    "x": 1282,
                    "y": 568
                },
                {
                    "x": 1882,
                    "y": 568
                },
                {
                    "x": 1882,
                    "y": 622
                },
                {
                    "x": 1282,
                    "y": 622
                }
            ],
            "category": "paragraph",
            "html": "<p id='163' style='font-size:22px'>C. Bounding-box regression</p>",
            "id": 163,
            "page": 12,
            "text": "C. Bounding-box regression"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 655
                },
                {
                    "x": 2277,
                    "y": 655
                },
                {
                    "x": 2277,
                    "y": 1151
                },
                {
                    "x": 1279,
                    "y": 1151
                }
            ],
            "category": "paragraph",
            "html": "<p id='164' style='font-size:14px'>We use a simple bounding-box regression stage to im-<br>prove localization performance. After scoring each selec-<br>tive search proposal with a class-specific detection SVM,<br>we predict a new bounding box for the detection using a<br>class-specific bounding-box regressor. This is similar in<br>spirit to the bounding-box regression used in deformable<br>part models [17]. The primary difference between the two<br>approaches is that here we regress from features computed<br>by the CNN, rather than from geometric features computed<br>on the inferred DPM part locations.</p>",
            "id": 164,
            "page": 12,
            "text": "We use a simple bounding-box regression stage to improve localization performance. After scoring each selective search proposal with a class-specific detection SVM, we predict a new bounding box for the detection using a class-specific bounding-box regressor. This is similar in spirit to the bounding-box regression used in deformable part models . The primary difference between the two approaches is that here we regress from features computed by the CNN, rather than from geometric features computed on the inferred DPM part locations."
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 1160
                },
                {
                    "x": 2276,
                    "y": 1160
                },
                {
                    "x": 2276,
                    "y": 1609
                },
                {
                    "x": 1279,
                    "y": 1609
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='165' style='font-size:14px'>The input to our training algorithm is a set of N train-<br>ing pairs {(Pi, , Gi)}i=1,...,N, where Pi = (Pi, Piy, Piw , Ph)<br>specifies the pixel coordinates of the center of proposal Pi's<br>bounding box together with Pi's width and height in pixels.<br>Hence forth, we drop the superscript i unless it is needed.<br>Each ground-truth bounding box G is specified in the same<br>way: G = (Gx, Gy, Gw, Gh). Our goal is to learn a trans-<br>formation that maps a proposed box P to a ground-truth box<br>G.</p>",
            "id": 165,
            "page": 12,
            "text": "The input to our training algorithm is a set of N training pairs {(Pi, , Gi)}i=1,...,N, where Pi = (Pi, Piy, Piw , Ph) specifies the pixel coordinates of the center of proposal Pi's bounding box together with Pi's width and height in pixels. Hence forth, we drop the superscript i unless it is needed. Each ground-truth bounding box G is specified in the same way: G = (Gx, Gy, Gw, Gh). Our goal is to learn a transformation that maps a proposed box P to a ground-truth box G."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1619
                },
                {
                    "x": 2278,
                    "y": 1619
                },
                {
                    "x": 2278,
                    "y": 2016
                },
                {
                    "x": 1280,
                    "y": 2016
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='166' style='font-size:14px'>We parameterize the transformation in terms of four<br>functions dx (P), dy(P), dw (P), and dh(P). The first<br>two specify a scale-invariant translation of the center of<br>P's bounding box, while the second two specify log-space<br>translations of the width and height of P's bounding box.<br>After learning these functions, we can transform an input<br>proposal P into a predicted ground-truth box G by apply-<br>ing the transformation</p>",
            "id": 166,
            "page": 12,
            "text": "We parameterize the transformation in terms of four functions dx (P), dy(P), dw (P), and dh(P). The first two specify a scale-invariant translation of the center of P's bounding box, while the second two specify log-space translations of the width and height of P's bounding box. After learning these functions, we can transform an input proposal P into a predicted ground-truth box G by applying the transformation"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 2419
                },
                {
                    "x": 2277,
                    "y": 2419
                },
                {
                    "x": 2277,
                    "y": 2770
                },
                {
                    "x": 1279,
                    "y": 2770
                }
            ],
            "category": "paragraph",
            "html": "<p id='167' style='font-size:16px'>Each function d* (P) (where ★ is one of x,y, h, w) is<br>modeled as a linear function of the pool5 features of pro-<br>posal P, denoted by �5 (P). (The dependence of ⌀5 (P)<br>on the image data is implicitly assumed.) Thus we have<br>d* (P) = WT ⌀5 (P), where W* is a vector of learnable<br>model parameters. We learn W* by optimizing the regu-<br>larized least squares objective (ridge regression):</p>",
            "id": 167,
            "page": 12,
            "text": "Each function d* (P) (where ★ is one of x,y, h, w) is modeled as a linear function of the pool5 features of proposal P, denoted by �5 (P). (The dependence of ⌀5 (P) on the image data is implicitly assumed.) Thus we have d* (P) = WT ⌀5 (P), where W* is a vector of learnable model parameters. We learn W* by optimizing the regularized least squares objective (ridge regression):"
        },
        {
            "bounding_box": [
                {
                    "x": 1218,
                    "y": 3052
                },
                {
                    "x": 1263,
                    "y": 3052
                },
                {
                    "x": 1263,
                    "y": 3095
                },
                {
                    "x": 1218,
                    "y": 3095
                }
            ],
            "category": "footer",
            "html": "<footer id='168' style='font-size:14px'>12</footer>",
            "id": 168,
            "page": 12,
            "text": "12"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 308
                },
                {
                    "x": 1196,
                    "y": 308
                },
                {
                    "x": 1196,
                    "y": 400
                },
                {
                    "x": 201,
                    "y": 400
                }
            ],
            "category": "paragraph",
            "html": "<p id='169' style='font-size:16px'>The regression targets t* for the training pair (P, G) are de-<br>fined as</p>",
            "id": 169,
            "page": 13,
            "text": "The regression targets t* for the training pair (P, G) are defined as"
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 774
                },
                {
                    "x": 1197,
                    "y": 774
                },
                {
                    "x": 1197,
                    "y": 867
                },
                {
                    "x": 203,
                    "y": 867
                }
            ],
            "category": "paragraph",
            "html": "<p id='170' style='font-size:16px'>As a standard regularized least squares problem, this can be<br>solved efficiently in closed form.</p>",
            "id": 170,
            "page": 13,
            "text": "As a standard regularized least squares problem, this can be solved efficiently in closed form."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 879
                },
                {
                    "x": 1200,
                    "y": 879
                },
                {
                    "x": 1200,
                    "y": 1672
                },
                {
                    "x": 200,
                    "y": 1672
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='171' style='font-size:16px'>We found two subtle issues while implementing<br>bounding-box regression. The first is that regularization<br>is important: we set 入 = 1000 based on a validation set.<br>The second issue is that care must be taken when selecting<br>which training pairs (P, G) to use. Intuitively, if P is far<br>from all ground-truth boxes, then the task of transforming<br>P to a ground-truth box G does not make sense. Using ex-<br>amples like P would lead to a hopeless learning problem.<br>Therefore, we only learn from a proposal P if it is nearby<br>at least one ground-truth box. We implement \"nearness\" by<br>assigning P to the ground-truth box G with which it has<br>maximum IoU overlap (in case it overlaps more than one) if<br>and only if the overlap is greater than a threshold (which we<br>set to 0.6 using a validation set). All unassigned proposals<br>are discarded. We do this once for each object class in order<br>to learn a set of class-specific bounding-box regressors.</p>",
            "id": 171,
            "page": 13,
            "text": "We found two subtle issues while implementing bounding-box regression. The first is that regularization is important: we set 入 = 1000 based on a validation set. The second issue is that care must be taken when selecting which training pairs (P, G) to use. Intuitively, if P is far from all ground-truth boxes, then the task of transforming P to a ground-truth box G does not make sense. Using examples like P would lead to a hopeless learning problem. Therefore, we only learn from a proposal P if it is nearby at least one ground-truth box. We implement \"nearness\" by assigning P to the ground-truth box G with which it has maximum IoU overlap (in case it overlaps more than one) if and only if the overlap is greater than a threshold (which we set to 0.6 using a validation set). All unassigned proposals are discarded. We do this once for each object class in order to learn a set of class-specific bounding-box regressors."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1682
                },
                {
                    "x": 1200,
                    "y": 1682
                },
                {
                    "x": 1200,
                    "y": 1975
                },
                {
                    "x": 201,
                    "y": 1975
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='172' style='font-size:16px'>At test time, we score each proposal and predict its new<br>detection window only once. In principle, we could iterate<br>this procedure (i.e., re-score the newly predicted bounding<br>box, and then predict a new bounding box from it, and SO<br>on). However, we found that iterating does not improve<br>results.</p>",
            "id": 172,
            "page": 13,
            "text": "At test time, we score each proposal and predict its new detection window only once. In principle, we could iterate this procedure (i.e., re-score the newly predicted bounding box, and then predict a new bounding box from it, and SO on). However, we found that iterating does not improve results."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2036
                },
                {
                    "x": 964,
                    "y": 2036
                },
                {
                    "x": 964,
                    "y": 2089
                },
                {
                    "x": 202,
                    "y": 2089
                }
            ],
            "category": "paragraph",
            "html": "<p id='173' style='font-size:20px'>D. Additional feature visualizations</p>",
            "id": 173,
            "page": 13,
            "text": "D. Additional feature visualizations"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2125
                },
                {
                    "x": 1198,
                    "y": 2125
                },
                {
                    "x": 1198,
                    "y": 2322
                },
                {
                    "x": 201,
                    "y": 2322
                }
            ],
            "category": "paragraph",
            "html": "<p id='174' style='font-size:16px'>Figure 12 shows additional visualizations for 20 pool5<br>units. For each unit, we show the 24 region proposals that<br>maximally activate that unit out of the full set of approxi-<br>mately 10 million regions in all of VOC 2007 test.</p>",
            "id": 174,
            "page": 13,
            "text": "Figure 12 shows additional visualizations for 20 pool5 units. For each unit, we show the 24 region proposals that maximally activate that unit out of the full set of approximately 10 million regions in all of VOC 2007 test."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 2332
                },
                {
                    "x": 1200,
                    "y": 2332
                },
                {
                    "x": 1200,
                    "y": 2578
                },
                {
                    "x": 200,
                    "y": 2578
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='175' style='font-size:18px'>We label each unit by its (y, X, channel) position in the<br>6 x 6 x 256 dimensional pool5 feature map. Within each<br>channel, the CNN computes exactly the same function of<br>the input region, with the (y, x) position changing only the<br>receptive field.</p>",
            "id": 175,
            "page": 13,
            "text": "We label each unit by its (y, X, channel) position in the 6 x 6 x 256 dimensional pool5 feature map. Within each channel, the CNN computes exactly the same function of the input region, with the (y, x) position changing only the receptive field."
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 2638
                },
                {
                    "x": 988,
                    "y": 2638
                },
                {
                    "x": 988,
                    "y": 2692
                },
                {
                    "x": 203,
                    "y": 2692
                }
            ],
            "category": "paragraph",
            "html": "<p id='176' style='font-size:20px'>E. Per-category segmentation results</p>",
            "id": 176,
            "page": 13,
            "text": "E. Per-category segmentation results"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2728
                },
                {
                    "x": 1199,
                    "y": 2728
                },
                {
                    "x": 1199,
                    "y": 2975
                },
                {
                    "x": 201,
                    "y": 2975
                }
            ],
            "category": "paragraph",
            "html": "<p id='177' style='font-size:14px'>In Table 7 we show the per-category segmentation ac-<br>curacy on VOC 2011 val for each of our six segmentation<br>methods in addition to the O2P method [4]. These results<br>show which methods are strongest across each of the 20<br>PASCAL classes, plus the background class.</p>",
            "id": 177,
            "page": 13,
            "text": "In Table 7 we show the per-category segmentation accuracy on VOC 2011 val for each of our six segmentation methods in addition to the O2P method . These results show which methods are strongest across each of the 20 PASCAL classes, plus the background class."
        },
        {
            "bounding_box": [
                {
                    "x": 1282,
                    "y": 302
                },
                {
                    "x": 2131,
                    "y": 302
                },
                {
                    "x": 2131,
                    "y": 354
                },
                {
                    "x": 1282,
                    "y": 354
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='178' style='font-size:22px'>F. Analysis of cross-dataset redundancy</p>",
            "id": 178,
            "page": 13,
            "text": "F. Analysis of cross-dataset redundancy"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 386
                },
                {
                    "x": 2277,
                    "y": 386
                },
                {
                    "x": 2277,
                    "y": 879
                },
                {
                    "x": 1279,
                    "y": 879
                }
            ],
            "category": "paragraph",
            "html": "<p id='179' style='font-size:16px'>One concern when training on an auxiliary dataset is that<br>there might be redundancy between it and the test set. Even<br>though the tasks of object detection and whole-image clas-<br>sification are substantially different, making such cross-set<br>redundancy much less worrisome, we still conducted a thor-<br>ough investigation that quantifies the extent to which PAS-<br>CAL test images are contained within the ILSVRC 2012<br>training and validation sets. Our findings may be useful to<br>researchers who are interested in using ILSVRC 2012 as<br>training data for the PASCAL image classification task.</p>",
            "id": 179,
            "page": 13,
            "text": "One concern when training on an auxiliary dataset is that there might be redundancy between it and the test set. Even though the tasks of object detection and whole-image classification are substantially different, making such cross-set redundancy much less worrisome, we still conducted a thorough investigation that quantifies the extent to which PASCAL test images are contained within the ILSVRC 2012 training and validation sets. Our findings may be useful to researchers who are interested in using ILSVRC 2012 as training data for the PASCAL image classification task."
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 885
                },
                {
                    "x": 2277,
                    "y": 885
                },
                {
                    "x": 2277,
                    "y": 1229
                },
                {
                    "x": 1278,
                    "y": 1229
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='180' style='font-size:16px'>We performed two checks for duplicate (and near-<br>duplicate) images. The first test is based on exact matches<br>of flickr image IDs, which are included in the VOC 2007<br>test annotations (these IDs are intentionally kept secret for<br>subsequent PASCAL test sets). All PASCAL images, and<br>about half of ILSVRC, were collected from flickr.com. This<br>check turned up 31 matches out of 4952 (0.63%).</p>",
            "id": 180,
            "page": 13,
            "text": "We performed two checks for duplicate (and nearduplicate) images. The first test is based on exact matches of flickr image IDs, which are included in the VOC 2007 test annotations (these IDs are intentionally kept secret for subsequent PASCAL test sets). All PASCAL images, and about half of ILSVRC, were collected from flickr.com. This check turned up 31 matches out of 4952 (0.63%)."
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 1233
                },
                {
                    "x": 2278,
                    "y": 1233
                },
                {
                    "x": 2278,
                    "y": 1526
                },
                {
                    "x": 1279,
                    "y": 1526
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='181' style='font-size:16px'>The second check uses GIST [30] descriptor matching,<br>which was shown in [13] to have excellent performance at<br>near-duplicate image detection in large (> 1 million) image<br>collections. Following [13], we computed GIST descrip-<br>tors on warped 32 x 32 pixel versions of all ILSVRC 2012<br>trainval and PASCAL 2007 test images.</p>",
            "id": 181,
            "page": 13,
            "text": "The second check uses GIST  descriptor matching, which was shown in  to have excellent performance at near-duplicate image detection in large (> 1 million) image collections. Following , we computed GIST descriptors on warped 32 x 32 pixel versions of all ILSVRC 2012 trainval and PASCAL 2007 test images."
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 1534
                },
                {
                    "x": 2277,
                    "y": 1534
                },
                {
                    "x": 2277,
                    "y": 2125
                },
                {
                    "x": 1278,
                    "y": 2125
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='182' style='font-size:16px'>Euclidean distance nearest-neighbor matching of GIST<br>descriptors revealed 38 near-duplicate images (including all<br>31 found by flickr ID matching). The matches tend to vary<br>slightly in JPEG compression level and resolution, and to a<br>lesser extent cropping. These findings show that the overlap<br>is small, less than 1%. For VOC 2012, because flickr IDs<br>are not available, we used the GIST matching method only.<br>Based on GIST matches, 1.5% of VOC 2012 test images<br>are in ILSVRC 2012 trainval. The slightly higher rate for<br>VOC 2012 is likely due to the fact that the two datasets<br>were collected closer together in time than VOC 2007 and<br>ILSVRC 2012 were.</p>",
            "id": 182,
            "page": 13,
            "text": "Euclidean distance nearest-neighbor matching of GIST descriptors revealed 38 near-duplicate images (including all 31 found by flickr ID matching). The matches tend to vary slightly in JPEG compression level and resolution, and to a lesser extent cropping. These findings show that the overlap is small, less than 1%. For VOC 2012, because flickr IDs are not available, we used the GIST matching method only. Based on GIST matches, 1.5% of VOC 2012 test images are in ILSVRC 2012 trainval. The slightly higher rate for VOC 2012 is likely due to the fact that the two datasets were collected closer together in time than VOC 2007 and ILSVRC 2012 were."
        },
        {
            "bounding_box": [
                {
                    "x": 1282,
                    "y": 2169
                },
                {
                    "x": 1800,
                    "y": 2169
                },
                {
                    "x": 1800,
                    "y": 2223
                },
                {
                    "x": 1282,
                    "y": 2223
                }
            ],
            "category": "paragraph",
            "html": "<p id='183' style='font-size:20px'>G. Document changelog</p>",
            "id": 183,
            "page": 13,
            "text": "G. Document changelog"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 2251
                },
                {
                    "x": 2276,
                    "y": 2251
                },
                {
                    "x": 2276,
                    "y": 2401
                },
                {
                    "x": 1279,
                    "y": 2401
                }
            ],
            "category": "paragraph",
            "html": "<p id='184' style='font-size:16px'>This document tracks the progress of R-CNN. To help<br>readers understand how it has changed over time, here's a<br>brief changelog describing the revisions.</p>",
            "id": 184,
            "page": 13,
            "text": "This document tracks the progress of R-CNN. To help readers understand how it has changed over time, here's a brief changelog describing the revisions."
        },
        {
            "bounding_box": [
                {
                    "x": 1283,
                    "y": 2429
                },
                {
                    "x": 1594,
                    "y": 2429
                },
                {
                    "x": 1594,
                    "y": 2473
                },
                {
                    "x": 1283,
                    "y": 2473
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='185' style='font-size:14px'>v1 Initial version.</p>",
            "id": 185,
            "page": 13,
            "text": "v1 Initial version."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2502
                },
                {
                    "x": 2277,
                    "y": 2502
                },
                {
                    "x": 2277,
                    "y": 2798
                },
                {
                    "x": 1280,
                    "y": 2798
                }
            ],
            "category": "paragraph",
            "html": "<p id='186' style='font-size:18px'>v2 CVPR 2014 camera-ready revision. Includes substan-<br>tial improvements in detection performance brought about<br>by (1) starting fine-tuning from a higher learning rate (0.001<br>instead of 0.0001), (2) using context padding when prepar-<br>ing CNN inputs, and (3) bounding-box regression to fix lo-<br>calization errors.</p>",
            "id": 186,
            "page": 13,
            "text": "v2 CVPR 2014 camera-ready revision. Includes substantial improvements in detection performance brought about by (1) starting fine-tuning from a higher learning rate (0.001 instead of 0.0001), (2) using context padding when preparing CNN inputs, and (3) bounding-box regression to fix localization errors."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 2829
                },
                {
                    "x": 2276,
                    "y": 2829
                },
                {
                    "x": 2276,
                    "y": 2975
                },
                {
                    "x": 1281,
                    "y": 2975
                }
            ],
            "category": "paragraph",
            "html": "<p id='187' style='font-size:16px'>v3 Results on the ILSVRC2013 detection dataset and com-<br>parison with OverFeat were integrated into several sections<br>(primarily Section 2 and Section 4).</p>",
            "id": 187,
            "page": 13,
            "text": "v3 Results on the ILSVRC2013 detection dataset and comparison with OverFeat were integrated into several sections (primarily Section 2 and Section 4)."
        },
        {
            "bounding_box": [
                {
                    "x": 1218,
                    "y": 3052
                },
                {
                    "x": 1263,
                    "y": 3052
                },
                {
                    "x": 1263,
                    "y": 3094
                },
                {
                    "x": 1218,
                    "y": 3094
                }
            ],
            "category": "footer",
            "html": "<footer id='188' style='font-size:16px'>13</footer>",
            "id": 188,
            "page": 13,
            "text": "13"
        },
        {
            "bounding_box": [
                {
                    "x": 205,
                    "y": 294
                },
                {
                    "x": 2271,
                    "y": 294
                },
                {
                    "x": 2271,
                    "y": 670
                },
                {
                    "x": 205,
                    "y": 670
                }
            ],
            "category": "table",
            "html": "<table id='189' style='font-size:14px'><tr><td>VOC 2011 val</td><td>bg</td><td>aero</td><td>bike</td><td>bird</td><td>boat</td><td>bottle</td><td>bus</td><td>car</td><td>cat</td><td>chair</td><td>COW</td><td>table</td><td>dog</td><td>horse</td><td>mbike</td><td>person</td><td>plant</td><td>sheep</td><td>sofa</td><td>train</td><td>tv</td><td>mean</td></tr><tr><td>O2P [4]</td><td>84.0</td><td>69.0</td><td>21.7</td><td>47.7</td><td>42.2</td><td>42.4</td><td>64.7</td><td>65.8</td><td>57.4</td><td>12.9</td><td>37.4</td><td>20.5</td><td>43.7</td><td>35.7</td><td>52.7</td><td>51.0</td><td>35.8</td><td>51.0</td><td>28.4</td><td>59.8</td><td>49.7</td><td>46.4</td></tr><tr><td>full R-CNN fc6</td><td>81.3</td><td>56.2</td><td>23.9</td><td>42.9</td><td>40.7</td><td>38.8</td><td>59.2</td><td>56.5</td><td>53.2</td><td>11.4</td><td>34.6</td><td>16.7</td><td>48.1</td><td>37.0</td><td>51.4</td><td>46.0</td><td>31.5</td><td>44.0</td><td>24.3</td><td>53.7</td><td>51.1</td><td>43.0</td></tr><tr><td>full R-CNN fc7</td><td>81.0</td><td>52.8</td><td>25.1</td><td>43.8</td><td>40.5</td><td>42.7</td><td>55.4</td><td>57.7</td><td>51.3</td><td>8.7</td><td>32.5</td><td>11.5</td><td>48.1</td><td>37.0</td><td>50.5</td><td>46.4</td><td>30.2</td><td>42.1</td><td>21.2</td><td>57.7</td><td>56.0</td><td>42.5</td></tr><tr><td>fg R-CNN fc6</td><td>81.4</td><td>54.1</td><td>21.1</td><td>40.6</td><td>38.7</td><td>53.6</td><td>59.9</td><td>57.2</td><td>52.5</td><td>9.1</td><td>36.5</td><td>23.6</td><td>46.4</td><td>38.1</td><td>53.2</td><td>51.3</td><td>32.2</td><td>38.7</td><td>29.0</td><td>53.0</td><td>47.5</td><td>43.7</td></tr><tr><td>fg R-CNN fc7</td><td>80.9</td><td>50.1</td><td>20.0</td><td>40.2</td><td>34.1</td><td>40.9</td><td>59.7</td><td>59.8</td><td>52.7</td><td>7.3</td><td>32.1</td><td>14.3</td><td>48.8</td><td>42.9</td><td>54.0</td><td>48.6</td><td>28.9</td><td>42.6</td><td>24.9</td><td>52.2</td><td>48.8</td><td>42.1</td></tr><tr><td>full+fg R-CNN fc6</td><td>83.1</td><td>60.4</td><td>23.2</td><td>48.4</td><td>47.3</td><td>52.6</td><td>61.6</td><td>60.6</td><td>59.1</td><td>10.8</td><td>45.8</td><td>20.9</td><td>57.7</td><td>43.3</td><td>57.4</td><td>52.9</td><td>34.7</td><td>48.7</td><td>28.1</td><td>60.0</td><td>48.6</td><td>47.9</td></tr><tr><td>full+fg R-CNN fc7</td><td>82.3</td><td>56.7</td><td>20.6</td><td>49.9</td><td>44.2</td><td>43.6</td><td>59.3</td><td>61.3</td><td>57.8</td><td>7.7</td><td>38.4</td><td>15.1</td><td>53.4</td><td>43.7</td><td>50.8</td><td>52.0</td><td>34.1</td><td>47.8</td><td>24.7</td><td>60.1</td><td>55.2</td><td>45.7</td></tr></table>",
            "id": 189,
            "page": 14,
            "text": "VOC 2011 val bg aero bike bird boat bottle bus car cat chair COW table dog horse mbike person plant sheep sofa train tv mean  O2P  84.0 69.0 21.7 47.7 42.2 42.4 64.7 65.8 57.4 12.9 37.4 20.5 43.7 35.7 52.7 51.0 35.8 51.0 28.4 59.8 49.7 46.4  full R-CNN fc6 81.3 56.2 23.9 42.9 40.7 38.8 59.2 56.5 53.2 11.4 34.6 16.7 48.1 37.0 51.4 46.0 31.5 44.0 24.3 53.7 51.1 43.0  full R-CNN fc7 81.0 52.8 25.1 43.8 40.5 42.7 55.4 57.7 51.3 8.7 32.5 11.5 48.1 37.0 50.5 46.4 30.2 42.1 21.2 57.7 56.0 42.5  fg R-CNN fc6 81.4 54.1 21.1 40.6 38.7 53.6 59.9 57.2 52.5 9.1 36.5 23.6 46.4 38.1 53.2 51.3 32.2 38.7 29.0 53.0 47.5 43.7  fg R-CNN fc7 80.9 50.1 20.0 40.2 34.1 40.9 59.7 59.8 52.7 7.3 32.1 14.3 48.8 42.9 54.0 48.6 28.9 42.6 24.9 52.2 48.8 42.1  full+fg R-CNN fc6 83.1 60.4 23.2 48.4 47.3 52.6 61.6 60.6 59.1 10.8 45.8 20.9 57.7 43.3 57.4 52.9 34.7 48.7 28.1 60.0 48.6 47.9  full+fg R-CNN fc7 82.3 56.7 20.6 49.9 44.2 43.6 59.3 61.3 57.8 7.7 38.4 15.1 53.4 43.7 50.8 52.0 34.1 47.8 24.7 60.1 55.2"
        },
        {
            "bounding_box": [
                {
                    "x": 616,
                    "y": 693
                },
                {
                    "x": 1861,
                    "y": 693
                },
                {
                    "x": 1861,
                    "y": 737
                },
                {
                    "x": 616,
                    "y": 737
                }
            ],
            "category": "caption",
            "html": "<br><caption id='190' style='font-size:14px'>Table 7: Per-category segmentation accuracy (%) on the VOC 2011 validation set.</caption>",
            "id": 190,
            "page": 14,
            "text": "Table 7: Per-category segmentation accuracy (%) on the VOC 2011 validation set."
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 827
                },
                {
                    "x": 1198,
                    "y": 827
                },
                {
                    "x": 1198,
                    "y": 977
                },
                {
                    "x": 203,
                    "y": 977
                }
            ],
            "category": "paragraph",
            "html": "<p id='191' style='font-size:18px'>v4 The softmax VS. SVM results in Appendix B contained<br>an error, which has been fixed. We thank Sergio Guadar-<br>rama for helping to identify this issue.</p>",
            "id": 191,
            "page": 14,
            "text": "v4 The softmax VS. SVM results in Appendix B contained an error, which has been fixed. We thank Sergio Guadarrama for helping to identify this issue."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 1008
                },
                {
                    "x": 1199,
                    "y": 1008
                },
                {
                    "x": 1199,
                    "y": 1153
                },
                {
                    "x": 202,
                    "y": 1153
                }
            ],
            "category": "paragraph",
            "html": "<p id='192' style='font-size:20px'>v5 Added results using the new 16-layer network architec-<br>ture from Simonyan and Zisserman [43] to Section 3.3 and<br>Table 3.</p>",
            "id": 192,
            "page": 14,
            "text": "v5 Added results using the new 16-layer network architecture from Simonyan and Zisserman  to Section 3.3 and Table 3."
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 1200
                },
                {
                    "x": 445,
                    "y": 1200
                },
                {
                    "x": 445,
                    "y": 1252
                },
                {
                    "x": 204,
                    "y": 1252
                }
            ],
            "category": "paragraph",
            "html": "<p id='193' style='font-size:22px'>References</p>",
            "id": 193,
            "page": 14,
            "text": "References"
        },
        {
            "bounding_box": [
                {
                    "x": 210,
                    "y": 1265
                },
                {
                    "x": 1201,
                    "y": 1265
                },
                {
                    "x": 1201,
                    "y": 2973
                },
                {
                    "x": 210,
                    "y": 2973
                }
            ],
            "category": "paragraph",
            "html": "<p id='194' style='font-size:16px'>[1] B. Alexe, T. Deselaers, and V. Ferrari. Measuring the object-<br>ness of image windows. TPAMI, 2012. 2<br>[2] P. Arbelaez, B. Hariharan, C. Gu, S. Gupta, L. Bourdev, and<br>J. Malik. Semantic segmentation using regions and parts. In<br>CVPR, 2012. 10, 11<br>[3] P. Arbel�ez, J. Pont-Tuset, J. Barron, F. Marques, and J. Ma-<br>lik. Multiscale combinatorial grouping. In CVPR, 2014. 3<br>[4] J. Carreira, R. Caseiro, J. Batista, and C. Sminchisescu. Se-<br>mantic segmentation with second-order pooling. In ECCV,<br>2012. 4, 10, 11, 13, 14<br>[5] J. Carreira and C. Sminchisescu. CPMC: Automatic ob-<br>ject segmentation using constrained parametric min-cuts.<br>TPAMI, 2012. 2, 3<br>[6] D. Cire�an, A. Giusti, L. Gambardella, and J. Schmidhu-<br>ber. Mitosis detection in breast cancer histology images with<br>deep neural networks. In MICCAI, 2013. 3<br>[7] N. Dalal and B. Triggs. Histograms of oriented gradients for<br>human detection. In CVPR, 2005. 1<br>[8] T. Dean, M. A. Ruzon, M. Segal, J. Shlens, S. Vijaya-<br>narasimhan, and J. Yagnik. Fast, accurate detection of<br>100,000 object classes on a single machine. In CVPR, 2013.<br>3<br>[9] J. Deng, A. Berg, S. Satheesh, H. Su, A. Khosla, and L. Fei-<br>Fei. ImageNet Large Scale Visual Recognition Competition<br>2012 (ILSVRC2012). http: / /www · image-net · org/<br>challenges/LSVRC/2012/ 1<br>[10] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-<br>Fei. ImageNet: A large-scale hierarchical image database.<br>In CVPR, 2009. 1<br>[11] J. Deng, 0. Russakovsky, J. Krause, M. Bernstein, A. C.<br>Berg, and L. Fei-Fei. Scalable multi-label annotation. In<br>CHI, 2014. 8<br>[12] J. Donahue, Y. Jia, 0. Vinyals, J. Hoffman, N. Zhang,<br>E. Tzeng, and T. Darrell. DeCAF: A Deep Convolutional<br>Activation Feature for Generic Visual Recognition. In ICML,<br>2014. 2</p>",
            "id": 194,
            "page": 14,
            "text": " B. Alexe, T. Deselaers, and V. Ferrari. Measuring the objectness of image windows. TPAMI, 2012. 2  P. Arbelaez, B. Hariharan, C. Gu, S. Gupta, L. Bourdev, and J. Malik. Semantic segmentation using regions and parts. In CVPR, 2012. 10, 11  P. Arbel�ez, J. Pont-Tuset, J. Barron, F. Marques, and J. Malik. Multiscale combinatorial grouping. In CVPR, 2014. 3  J. Carreira, R. Caseiro, J. Batista, and C. Sminchisescu. Semantic segmentation with second-order pooling. In ECCV, 2012. 4, 10, 11, 13, 14  J. Carreira and C. Sminchisescu. CPMC: Automatic object segmentation using constrained parametric min-cuts. TPAMI, 2012. 2, 3  D. Cire�an, A. Giusti, L. Gambardella, and J. Schmidhuber. Mitosis detection in breast cancer histology images with deep neural networks. In MICCAI, 2013. 3  N. Dalal and B. Triggs. Histograms of oriented gradients for human detection. In CVPR, 2005. 1  T. Dean, M. A. Ruzon, M. Segal, J. Shlens, S. Vijayanarasimhan, and J. Yagnik. Fast, accurate detection of 100,000 object classes on a single machine. In CVPR, 2013. 3  J. Deng, A. Berg, S. Satheesh, H. Su, A. Khosla, and L. FeiFei. ImageNet Large Scale Visual Recognition Competition 2012 (ILSVRC2012). http: / /www · image-net · org/ challenges/LSVRC/2012/ 1  J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. FeiFei. ImageNet: A large-scale hierarchical image database. In CVPR, 2009. 1  J. Deng, 0. Russakovsky, J. Krause, M. Bernstein, A. C. Berg, and L. Fei-Fei. Scalable multi-label annotation. In CHI, 2014. 8  J. Donahue, Y. Jia, 0. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell. DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition. In ICML, 2014. 2"
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 826
                },
                {
                    "x": 2284,
                    "y": 826
                },
                {
                    "x": 2284,
                    "y": 2976
                },
                {
                    "x": 1280,
                    "y": 2976
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='195' style='font-size:14px'>[13] M. Douze, H. Jegou, H. Sandhawalia, L. Amsaleg, and<br>C. Schmid. Evaluation of gist descriptors for web-scale im-<br>age search. In Proc. ofthe ACM International Conference on<br>Image and Video Retrieval, 2009. 13<br>[14] I. Endres and D. Hoiem. Category independent object pro-<br>posals. In ECCV, 2010. 3<br>[15] M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and<br>A. Zisserman. The PASCAL Visual Object Classes (VOC)<br>Challenge. IJCV, 2010. 1 4<br>[16] C. Farabet, C. Couprie, L. Najman, and Y. LeCun. Learning<br>hierarchical features for scene labeling. TPAMI, 2013. 10<br>[17] P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ra-<br>manan. Object detection with discriminatively trained part<br>based models. TPAMI, 2010. 2, 4, 7, 12<br>[18] S. Fidler, R. Mottaghi, A. Yuille, and R. Urtasun. Bottom-up<br>segmentation for top-down detection. In CVPR, 2013. 4, 5<br>[19] K. Fukushima. Neocognitron: A self-organizing neu-<br>ral network model for a mechanism of pattern recogni-<br>tion unaffected by shift in position. Biological cybernetics,<br>36(4):193-202, 1980. 1<br>[20] R. Girshick, P. Felzenszwalb, and D. McAllester. Discrimi-<br>natively trained deformable part models, release 5. http:<br>/ /www · CS · berkeley · edu/ ~ rbg/latent-v5/. 2,<br>5, 6, 7<br>[21] C. Gu, J. J. Lim, P. Arbelaez, and J. Malik. Recognition<br>using regions. In CVPR, 2009. 2<br>[22] B. Hariharan, P. Arbelaez, L. Bourdev, S. Maji, and J. Malik.<br>Semantic contours from inverse detectors. In ICCV, 2011.<br>10<br>[23] D. Hoiem, Y. Chodpathumwan, and Q. Dai. Diagnosing error<br>in object detectors. In ECCV. 2012. 2, 7, 8<br>[24] Y. Jia. Caffe: An open source convolutional archi-<br>tecture for fast feature embedding. http : / / caffe.<br>berkeleyvision. org/, 2013. 3<br>[25] A. Krizhevsky, I. Sutskever, and G. Hinton. ImageNet clas-<br>sification with deep convolutional neural networks. In NIPS,<br>2012. 1, 3, 4, 7<br>[26] Y. LeCun, B. Boser, J. Denker, D. Henderson, R. Howard,<br>W. Hubbard, and L. Jackel. Backpropagation applied to<br>handwritten zip code recognition. Neural Comp., 1989. 1<br>[27] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-<br>based learning applied to document recognition. Proc. of the<br>IEEE, 1998. 1<br>[28] J. J. Lim, C. L. Zitnick, and P. Dollar. Sketch tokens: A<br>learned mid-level representation for contour and object de-<br>tection. In CVPR, 2013. 6, 7</p>",
            "id": 195,
            "page": 14,
            "text": " M. Douze, H. Jegou, H. Sandhawalia, L. Amsaleg, and C. Schmid. Evaluation of gist descriptors for web-scale image search. In Proc. ofthe ACM International Conference on Image and Video Retrieval, 2009. 13  I. Endres and D. Hoiem. Category independent object proposals. In ECCV, 2010. 3  M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The PASCAL Visual Object Classes (VOC) Challenge. IJCV, 2010. 1 4  C. Farabet, C. Couprie, L. Najman, and Y. LeCun. Learning hierarchical features for scene labeling. TPAMI, 2013. 10  P. Felzenszwalb, R. Girshick, D. McAllester, and D. Ramanan. Object detection with discriminatively trained part based models. TPAMI, 2010. 2, 4, 7, 12  S. Fidler, R. Mottaghi, A. Yuille, and R. Urtasun. Bottom-up segmentation for top-down detection. In CVPR, 2013. 4, 5  K. Fukushima. Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Biological cybernetics, 36(4):193-202, 1980. 1  R. Girshick, P. Felzenszwalb, and D. McAllester. Discriminatively trained deformable part models, release 5. http: / /www · CS · berkeley · edu/ ~ rbg/latent-v5/. 2, 5, 6, 7  C. Gu, J. J. Lim, P. Arbelaez, and J. Malik. Recognition using regions. In CVPR, 2009. 2  B. Hariharan, P. Arbelaez, L. Bourdev, S. Maji, and J. Malik. Semantic contours from inverse detectors. In ICCV, 2011. 10  D. Hoiem, Y. Chodpathumwan, and Q. Dai. Diagnosing error in object detectors. In ECCV. 2012. 2, 7, 8  Y. Jia. Caffe: An open source convolutional architecture for fast feature embedding. http : / / caffe. berkeleyvision. org/, 2013. 3  A. Krizhevsky, I. Sutskever, and G. Hinton. ImageNet classification with deep convolutional neural networks. In NIPS, 2012. 1, 3, 4, 7  Y. LeCun, B. Boser, J. Denker, D. Henderson, R. Howard, W. Hubbard, and L. Jackel. Backpropagation applied to handwritten zip code recognition. Neural Comp., 1989. 1  Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradientbased learning applied to document recognition. Proc. of the IEEE, 1998. 1  J. J. Lim, C. L. Zitnick, and P. Dollar. Sketch tokens: A learned mid-level representation for contour and object detection. In CVPR, 2013. 6, 7"
        },
        {
            "bounding_box": [
                {
                    "x": 1219,
                    "y": 3054
                },
                {
                    "x": 1262,
                    "y": 3054
                },
                {
                    "x": 1262,
                    "y": 3091
                },
                {
                    "x": 1219,
                    "y": 3091
                }
            ],
            "category": "footer",
            "html": "<footer id='196' style='font-size:18px'>14</footer>",
            "id": 196,
            "page": 14,
            "text": "14"
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 285
                },
                {
                    "x": 2276,
                    "y": 285
                },
                {
                    "x": 2276,
                    "y": 2645
                },
                {
                    "x": 203,
                    "y": 2645
                }
            ],
            "category": "table",
            "html": "<table id='197' style='font-size:20px'><tr><td>class</td><td>AP</td><td>class</td><td>AP</td><td>class</td><td>AP</td><td>class</td><td>AP</td><td>class</td><td>AP</td></tr><tr><td>accordion</td><td>50.8</td><td>centipede</td><td>30.4</td><td>hair spray</td><td>13.8</td><td>pencil box</td><td>11.4</td><td>snowplow</td><td>69.2</td></tr><tr><td>airplane</td><td>50.0</td><td>chain saw</td><td>14.1</td><td>hamburger</td><td>34.2</td><td>pencil sharpener</td><td>9.0</td><td>soap dispenser</td><td>16.8</td></tr><tr><td>ant</td><td>31.8</td><td>chair</td><td>19.5</td><td>hammer</td><td>9.9</td><td>perfume</td><td>32.8</td><td>soccer ball</td><td>43.7</td></tr><tr><td>antelope</td><td>53.8</td><td>chime</td><td>24.6</td><td>hamster</td><td>46.0</td><td>person</td><td>41.7</td><td>sofa</td><td>16.3</td></tr><tr><td>apple</td><td>30.9</td><td>cocktail shaker</td><td>46.2</td><td>harmonica</td><td>12.6</td><td>piano</td><td>20.5</td><td>spatula</td><td>6.8</td></tr><tr><td>armadillo</td><td>54.0</td><td>coffee maker</td><td>21.5</td><td>harp</td><td>50.4</td><td>pineapple</td><td>22.6</td><td>squirrel</td><td>31.3</td></tr><tr><td>artichoke</td><td>45.0</td><td>computer keyboard</td><td>39.6</td><td>hat with a wide brim</td><td>40.5</td><td>ping-pong ball</td><td>21.0</td><td>starfish</td><td>45.1</td></tr><tr><td>axe</td><td>11.8</td><td>computer mouse</td><td>21.2</td><td>head cabbage</td><td>17.4</td><td>pitcher</td><td>19.2</td><td>stethoscope</td><td>18.3</td></tr><tr><td>baby bed</td><td>42.0</td><td>corkscrew</td><td>24.2</td><td>helmet</td><td>33.4</td><td>pizza</td><td>43.7</td><td>stove</td><td>8.1</td></tr><tr><td>backpack</td><td>2.8</td><td>cream</td><td>29.9</td><td>hippopotamus</td><td>38.0</td><td>plastic bag</td><td>6.4</td><td>strainer</td><td>9.9</td></tr><tr><td>bagel</td><td>37.5</td><td>croquet ball</td><td>30.0</td><td>horizontal bar</td><td>7.0</td><td>plate rack</td><td>15.2</td><td>strawberry</td><td>26.8</td></tr><tr><td>balance beam</td><td>32.6</td><td>crutch</td><td>23.7</td><td>horse</td><td>41.7</td><td>pomegranate</td><td>32.0</td><td>stretcher</td><td>13.2</td></tr><tr><td>banana</td><td>21.9</td><td>cucumber</td><td>22.8</td><td>hotdog</td><td>28.7</td><td>popsicle</td><td>21.2</td><td>sunglasses</td><td>18.8</td></tr><tr><td>band aid</td><td>17.4</td><td>cup or mug</td><td>34.0</td><td>iPod</td><td>59.2</td><td>porcupine</td><td>37.2</td><td>swimming trunks</td><td>9.1</td></tr><tr><td>banjo</td><td>55.3</td><td>diaper</td><td>10.1</td><td>isopod</td><td>19.5</td><td>power drill</td><td>7.9</td><td>swine</td><td>45.3</td></tr><tr><td>baseball</td><td>41.8</td><td>digital clock</td><td>18.5</td><td>jellyfish</td><td>23.7</td><td>pretzel</td><td>24.8</td><td>syringe</td><td>5.7</td></tr><tr><td>basketball</td><td>65.3</td><td>dishwasher</td><td>19.9</td><td>koala bear</td><td>44.3</td><td>printer</td><td>21.3</td><td>table</td><td>21.7</td></tr><tr><td>bathing cap</td><td>37.2</td><td>dog</td><td>76.8</td><td>ladle</td><td>3.0</td><td>puck</td><td>14.1</td><td>tape player</td><td>21.4</td></tr><tr><td>beaker</td><td>11.3</td><td>domestic cat</td><td>44.1</td><td>ladybug</td><td>58.4</td><td>punching bag</td><td>29.4</td><td>tennis ball</td><td>59.1</td></tr><tr><td>bear</td><td>62.7</td><td>dragonfly</td><td>27.8</td><td>lamp</td><td>9.1</td><td>purse</td><td>8.0</td><td>tick</td><td>42.6</td></tr><tr><td>bee</td><td>52.9</td><td>drum</td><td>19.9</td><td>laptop</td><td>35.4</td><td>rabbit</td><td>71.0</td><td>tie</td><td>24.6</td></tr><tr><td>bell pepper</td><td>38.8</td><td>dumbbell</td><td>14.1</td><td>lemon</td><td>33.3</td><td>racket</td><td>16.2</td><td>tiger</td><td>61.8</td></tr><tr><td>bench</td><td>12.7</td><td>electric fan</td><td>35.0</td><td>lion</td><td>51.3</td><td>ray</td><td>41.1</td><td>toaster</td><td>29.2</td></tr><tr><td>bicycle</td><td>41.1</td><td>elephant</td><td>56.4</td><td>lipstick</td><td>23.1</td><td>red panda</td><td>61.1</td><td>traffic light</td><td>24.7</td></tr><tr><td>binder</td><td>6.2</td><td>face powder</td><td>22.1</td><td>lizard</td><td>38.9</td><td>refrigerator</td><td>14.0</td><td>train</td><td>60.8</td></tr><tr><td>bird</td><td>70.9</td><td>fig</td><td>44.5</td><td>lobster</td><td>32.4</td><td>remote control</td><td>41.6</td><td>trombone</td><td>13.8</td></tr><tr><td>bookshelf</td><td>19.3</td><td>filing cabinet</td><td>20.6</td><td>maillot</td><td>31.0</td><td>rubber eraser</td><td>2.5</td><td>trumpet</td><td>14.4</td></tr><tr><td>bow tie</td><td>38.8</td><td>flower pot</td><td>20.2</td><td>maraca</td><td>30.1</td><td>rugby ball</td><td>34.5</td><td>turtle</td><td>59.1</td></tr><tr><td>bow</td><td>9.0</td><td>flute</td><td>4.9</td><td>microphone</td><td>4.0</td><td>ruler</td><td>11.5</td><td>tv or monitor</td><td>41.7</td></tr><tr><td>bowl</td><td>26.7</td><td>fox</td><td>59.3</td><td>microwave</td><td>40.1</td><td>salt or pepper shaker</td><td>24.6</td><td>unicycle</td><td>27.2</td></tr><tr><td>brassiere</td><td>31.2</td><td>french horn</td><td>24.2</td><td>milk can</td><td>33.3</td><td>saxophone</td><td>40.8</td><td>vacuum</td><td>19.5</td></tr><tr><td>burrito</td><td>25.7</td><td>frog</td><td>64.1</td><td>miniskirt</td><td>14.9</td><td>scorpion</td><td>57.3</td><td>violin</td><td>13.7</td></tr><tr><td>bus</td><td>57.5</td><td>frying pan</td><td>21.5</td><td>monkey</td><td>49.6</td><td>screwdriver</td><td>10.6</td><td>volleyball</td><td>59.7</td></tr><tr><td>butterfly</td><td>88.5</td><td>giant panda</td><td>42.5</td><td>motorcycle</td><td>42.2</td><td>seal</td><td>20.9</td><td>waffle iron</td><td>24.0</td></tr><tr><td>camel</td><td>37.6</td><td>goldfish</td><td>28.6</td><td>mushroom</td><td>31.8</td><td>sheep</td><td>48.9</td><td>washer</td><td>39.8</td></tr><tr><td>can opener</td><td>28.9</td><td>golf ball</td><td>51.3</td><td>nail</td><td>4.5</td><td>ski</td><td>9.0</td><td>water bottle</td><td>8.1</td></tr><tr><td>car</td><td>44.5</td><td>golfcart</td><td>47.9</td><td>neck brace</td><td>31.6</td><td>skunk</td><td>57.9</td><td>watercraft</td><td>40.9</td></tr><tr><td>cart</td><td>48.0</td><td>guacamole</td><td>32.3</td><td>oboe</td><td>27.5</td><td>snail</td><td>36.2</td><td>whale</td><td>48.6</td></tr><tr><td>cattle</td><td>32.3</td><td>guitar</td><td>33.1</td><td>orange</td><td>38.8</td><td>snake</td><td>33.8</td><td>wine bottle</td><td>31.2</td></tr><tr><td>cello</td><td>28.9</td><td>hair dryer</td><td>13.0</td><td>otter</td><td>22.2</td><td>snowmobile</td><td>58.8</td><td>zebra</td><td>49.6</td></tr></table>",
            "id": 197,
            "page": 15,
            "text": "class AP class AP class AP class AP class AP  accordion 50.8 centipede 30.4 hair spray 13.8 pencil box 11.4 snowplow 69.2  airplane 50.0 chain saw 14.1 hamburger 34.2 pencil sharpener 9.0 soap dispenser 16.8  ant 31.8 chair 19.5 hammer 9.9 perfume 32.8 soccer ball 43.7  antelope 53.8 chime 24.6 hamster 46.0 person 41.7 sofa 16.3  apple 30.9 cocktail shaker 46.2 harmonica 12.6 piano 20.5 spatula 6.8  armadillo 54.0 coffee maker 21.5 harp 50.4 pineapple 22.6 squirrel 31.3  artichoke 45.0 computer keyboard 39.6 hat with a wide brim 40.5 ping-pong ball 21.0 starfish 45.1  axe 11.8 computer mouse 21.2 head cabbage 17.4 pitcher 19.2 stethoscope 18.3  baby bed 42.0 corkscrew 24.2 helmet 33.4 pizza 43.7 stove 8.1  backpack 2.8 cream 29.9 hippopotamus 38.0 plastic bag 6.4 strainer 9.9  bagel 37.5 croquet ball 30.0 horizontal bar 7.0 plate rack 15.2 strawberry 26.8  balance beam 32.6 crutch 23.7 horse 41.7 pomegranate 32.0 stretcher 13.2  banana 21.9 cucumber 22.8 hotdog 28.7 popsicle 21.2 sunglasses 18.8  band aid 17.4 cup or mug 34.0 iPod 59.2 porcupine 37.2 swimming trunks 9.1  banjo 55.3 diaper 10.1 isopod 19.5 power drill 7.9 swine 45.3  baseball 41.8 digital clock 18.5 jellyfish 23.7 pretzel 24.8 syringe 5.7  basketball 65.3 dishwasher 19.9 koala bear 44.3 printer 21.3 table 21.7  bathing cap 37.2 dog 76.8 ladle 3.0 puck 14.1 tape player 21.4  beaker 11.3 domestic cat 44.1 ladybug 58.4 punching bag 29.4 tennis ball 59.1  bear 62.7 dragonfly 27.8 lamp 9.1 purse 8.0 tick 42.6  bee 52.9 drum 19.9 laptop 35.4 rabbit 71.0 tie 24.6  bell pepper 38.8 dumbbell 14.1 lemon 33.3 racket 16.2 tiger 61.8  bench 12.7 electric fan 35.0 lion 51.3 ray 41.1 toaster 29.2  bicycle 41.1 elephant 56.4 lipstick 23.1 red panda 61.1 traffic light 24.7  binder 6.2 face powder 22.1 lizard 38.9 refrigerator 14.0 train 60.8  bird 70.9 fig 44.5 lobster 32.4 remote control 41.6 trombone 13.8  bookshelf 19.3 filing cabinet 20.6 maillot 31.0 rubber eraser 2.5 trumpet 14.4  bow tie 38.8 flower pot 20.2 maraca 30.1 rugby ball 34.5 turtle 59.1  bow 9.0 flute 4.9 microphone 4.0 ruler 11.5 tv or monitor 41.7  bowl 26.7 fox 59.3 microwave 40.1 salt or pepper shaker 24.6 unicycle 27.2  brassiere 31.2 french horn 24.2 milk can 33.3 saxophone 40.8 vacuum 19.5  burrito 25.7 frog 64.1 miniskirt 14.9 scorpion 57.3 violin 13.7  bus 57.5 frying pan 21.5 monkey 49.6 screwdriver 10.6 volleyball 59.7  butterfly 88.5 giant panda 42.5 motorcycle 42.2 seal 20.9 waffle iron 24.0  camel 37.6 goldfish 28.6 mushroom 31.8 sheep 48.9 washer 39.8  can opener 28.9 golf ball 51.3 nail 4.5 ski 9.0 water bottle 8.1  car 44.5 golfcart 47.9 neck brace 31.6 skunk 57.9 watercraft 40.9  cart 48.0 guacamole 32.3 oboe 27.5 snail 36.2 whale 48.6  cattle 32.3 guitar 33.1 orange 38.8 snake 33.8 wine bottle 31.2  cello 28.9 hair dryer 13.0 otter 22.2 snowmobile 58.8 zebra"
        },
        {
            "bounding_box": [
                {
                    "x": 636,
                    "y": 2658
                },
                {
                    "x": 1839,
                    "y": 2658
                },
                {
                    "x": 1839,
                    "y": 2702
                },
                {
                    "x": 636,
                    "y": 2702
                }
            ],
            "category": "caption",
            "html": "<br><caption id='198' style='font-size:14px'>Table 8: Per-class average precision (%) on the ILSVRC2013 detection test set.</caption>",
            "id": 198,
            "page": 15,
            "text": "Table 8: Per-class average precision (%) on the ILSVRC2013 detection test set."
        },
        {
            "bounding_box": [
                {
                    "x": 209,
                    "y": 2789
                },
                {
                    "x": 1301,
                    "y": 2789
                },
                {
                    "x": 1301,
                    "y": 2982
                },
                {
                    "x": 209,
                    "y": 2982
                }
            ],
            "category": "paragraph",
            "html": "<p id='199' style='font-size:18px'>[29] D. Lowe. Distinctive image features from scale-invariant<br>keypoints. IJCV, 2004. 1<br>[30] A. Oliva and A. Torralba. Modeling the shape of the scene:</p>",
            "id": 199,
            "page": 15,
            "text": " D. Lowe. Distinctive image features from scale-invariant keypoints. IJCV, 2004. 1  A. Oliva and A. Torralba. Modeling the shape of the scene:"
        },
        {
            "bounding_box": [
                {
                    "x": 1363,
                    "y": 2797
                },
                {
                    "x": 2276,
                    "y": 2797
                },
                {
                    "x": 2276,
                    "y": 2841
                },
                {
                    "x": 1363,
                    "y": 2841
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='200' style='font-size:18px'>A holistic representation of the spatial envelope. IJCV, 2001.</p>",
            "id": 200,
            "page": 15,
            "text": "A holistic representation of the spatial envelope. IJCV, 2001."
        },
        {
            "bounding_box": [
                {
                    "x": 1367,
                    "y": 2844
                },
                {
                    "x": 1412,
                    "y": 2844
                },
                {
                    "x": 1412,
                    "y": 2880
                },
                {
                    "x": 1367,
                    "y": 2880
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='201' style='font-size:14px'>13</p>",
            "id": 201,
            "page": 15,
            "text": "13"
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 2930
                },
                {
                    "x": 2279,
                    "y": 2930
                },
                {
                    "x": 2279,
                    "y": 2975
                },
                {
                    "x": 1281,
                    "y": 2975
                }
            ],
            "category": "paragraph",
            "html": "<p id='202' style='font-size:16px'>[31] X. Ren and D. Ramanan. Histograms of sparse codes for</p>",
            "id": 202,
            "page": 15,
            "text": " X. Ren and D. Ramanan. Histograms of sparse codes for"
        },
        {
            "bounding_box": [
                {
                    "x": 1217,
                    "y": 3052
                },
                {
                    "x": 1265,
                    "y": 3052
                },
                {
                    "x": 1265,
                    "y": 3094
                },
                {
                    "x": 1217,
                    "y": 3094
                }
            ],
            "category": "footer",
            "html": "<footer id='203' style='font-size:20px'>15</footer>",
            "id": 203,
            "page": 15,
            "text": "15"
        },
        {
            "bounding_box": [
                {
                    "x": 239,
                    "y": 288
                },
                {
                    "x": 2253,
                    "y": 288
                },
                {
                    "x": 2253,
                    "y": 2737
                },
                {
                    "x": 239,
                    "y": 2737
                }
            ],
            "category": "figure",
            "html": "<figure><img id='204' style='font-size:14px' alt=\"cocktail shaker 0.56\nperson 0.88 helmet 0.65\ndog 0.95\nperson 0.72 dog 0. 9y\ndog 0.97\ndog 0 .85 0.57 bird 0.63\ndog\ndog 0.64\nlemon 0.79\nlemon0.70\nA E3\nlemon 0.50\nperson 0.82\nTERRA\nbird 0.96\ndoa 0 66\ndomestic cat 0.57\ndog 0.61\nperson 0.75 52\nsnowmobile 0 83 motorcycle 0 65\nsnowmobile 0.83 person 0.58\nbow tie 0. 86 bird 0.61\nperson 0.87\nsofa 0.71 dog0.91 ladybug 1.00\ndog 0.77\ndog 0.95\ndog 0.55\npretzel 0.78\nSUZUKI\n日本进入 bird 0.98\ncar 0.63:ar0.96\ncar0.66 person 0 .52\nwatercraft 1 .00 bird 0.91\nbird 0.99\nperson 0.65\ncar 0.96\nwatercraft - 69 person 0 .52 bird0.75\nperson 0 .58\nmown 0.65\n[*][*]\nRNLI17-25\n東京路線\n[*][*][*]\narmadillo 0.56\ntrain 1.00\nflower pot 0 62 d dog 0.56 train 0.53\narmadillo 1.00\ndog 0.98\ndog 0.92\nswine 0.88\nbird 0.93 bird 1.00\nbutterfly 0.96\nantelope 0.53 tvor monitor 0.8?\nperson 0.90 IV or monitor -\n业 tv or monitor 0.54\nsnake0.70 mushroom 0.93\nbell pepper 0.54\nturtle 0.54\nflower pot 0.62 bell pepper 0.62\nbell pepper 0.81\nOrangeBell\nruler / .00\n WAND 이상 고등비행비용 The 가까요?\ndog 0.97\nperson 0 .58\nlipstick 0.61\nlipstick 0.80\nbird0.89\nsoccer ball 0.90\nhttp://juzh orao\" data-coord=\"top-left:(239,288); bottom-right:(2253,2737)\" /></figure>",
            "id": 204,
            "page": 16,
            "text": "cocktail shaker 0.56 person 0.88 helmet 0.65 dog 0.95 person 0.72 dog 0. 9y dog 0.97 dog 0 .85 0.57 bird 0.63 dog dog 0.64 lemon 0.79 lemon0.70 A E3 lemon 0.50 person 0.82 TERRA bird 0.96 doa 0 66 domestic cat 0.57 dog 0.61 person 0.75 52 snowmobile 0 83 motorcycle 0 65 snowmobile 0.83 person 0.58 bow tie 0. 86 bird 0.61 person 0.87 sofa 0.71 dog0.91 ladybug 1.00 dog 0.77 dog 0.95 dog 0.55 pretzel 0.78 SUZUKI 日本进入 bird 0.98 car 0.63:ar0.96 car0.66 person 0 .52 watercraft 1 .00 bird 0.91 bird 0.99 person 0.65 car 0.96 watercraft - 69 person 0 .52 bird0.75 person 0 .58 mown 0.65 [*][*] RNLI17-25 東京路線 [*][*][*] armadillo 0.56 train 1.00 flower pot 0 62 d dog 0.56 train 0.53 armadillo 1.00 dog 0.98 dog 0.92 swine 0.88 bird 0.93 bird 1.00 butterfly 0.96 antelope 0.53 tvor monitor 0.8? person 0.90 IV or monitor 业 tv or monitor 0.54 snake0.70 mushroom 0.93 bell pepper 0.54 turtle 0.54 flower pot 0.62 bell pepper 0.62 bell pepper 0.81 OrangeBell ruler / .00  WAND 이상 고등비행비용 The 가까요? dog 0.97 person 0 .58 lipstick 0.61 lipstick 0.80 bird0.89 soccer ball 0.90 http://juzh orao"
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 2786
                },
                {
                    "x": 2276,
                    "y": 2786
                },
                {
                    "x": 2276,
                    "y": 2929
                },
                {
                    "x": 199,
                    "y": 2929
                }
            ],
            "category": "caption",
            "html": "<caption id='205' style='font-size:16px'>Figure 8: Example detections on the val2 set from the configuration that achieved 31.0% mAP on val2. Each image was sampled randomly<br>(these are not curated). All detections at precision greater than 0.5 are shown. Each detection is labeled with the predicted class and the<br>precision value of that detection from the detector's precision-recall curve. Viewing digitally with zoom is recommended.</caption>",
            "id": 205,
            "page": 16,
            "text": "Figure 8: Example detections on the val2 set from the configuration that achieved 31.0% mAP on val2. Each image was sampled randomly (these are not curated). All detections at precision greater than 0.5 are shown. Each detection is labeled with the predicted class and the precision value of that detection from the detector's precision-recall curve. Viewing digitally with zoom is recommended."
        },
        {
            "bounding_box": [
                {
                    "x": 1216,
                    "y": 3052
                },
                {
                    "x": 1265,
                    "y": 3052
                },
                {
                    "x": 1265,
                    "y": 3095
                },
                {
                    "x": 1216,
                    "y": 3095
                }
            ],
            "category": "footer",
            "html": "<footer id='206' style='font-size:20px'>16</footer>",
            "id": 206,
            "page": 16,
            "text": "16"
        },
        {
            "bounding_box": [
                {
                    "x": 207,
                    "y": 288
                },
                {
                    "x": 2272,
                    "y": 288
                },
                {
                    "x": 2272,
                    "y": 2712
                },
                {
                    "x": 207,
                    "y": 2712
                }
            ],
            "category": "figure",
            "html": "<figure><img id='207' style='font-size:14px' alt=\"helm^* n 6 watercraft 0.55\npitcher 0.57\nmonkey 0.97\ntable 0.60\nhat with a wide brim 0.78 pira 0.52\nperson 0 86\ndog 0.98\ntable 0.68\nshutterstoc\n1623057\nperson 0.88\nperson 0.87\nsunglasses 0.51\nperson 0.51\ncar 0.61\ndog 0.97\nswine 0 50 ey0.87\nmonkey 0.81\ndog 0.55 0.94\ndog\ndog 0.97\nhat with a wide brim 0.96\nperson 0 .77\nsnake 0.74\ndog 0.93\njulieDANIELLE\ntable 0.54 person 0.52\nperson 0.85 zebra 0. .55\nzebra 0.83\nzebra 0 80\nzebra 0.52\npretzel 0.69 BAS dog 0.71\nladybug 0.90\nguacamole 0.64\nperson 0.58\nperson 0.85\ndog 0.98 with a wide brim dog 0.98\nperson 0.73\nperson 0.81 elephant 1.00\nbird0.99\nPHOTOS COM\ncomputer keyboard 0.52 dog 0.97 dog 0.92\nbird 0.94\nDay ILRI/ 8L5\nYanalliium Jialimal Schart\ncart 1.00\nperson 0.87 91 leach othusin your exanhie person 0.77\nperson 0.57\nchair 0.64 79 person 0.52 Shillines butterfly 0.98\nAnthmuic\n645 5 d Alnas\n32 ろん 19 10종 Nater\nQiullA\n1 10\n+539 +\nmodulator\n6754\n8365\nX9 embention\n1189\nperson 0.91 person 0.75\nperson 0.73\nbird 0 83\nbird 1.00\nstethoscope 0.83\nperson 0.61\nbird 0.78\" data-coord=\"top-left:(207,288); bottom-right:(2272,2712)\" /></figure>",
            "id": 207,
            "page": 17,
            "text": "helm^* n 6 watercraft 0.55 pitcher 0.57 monkey 0.97 table 0.60 hat with a wide brim 0.78 pira 0.52 person 0 86 dog 0.98 table 0.68 shutterstoc 1623057 person 0.88 person 0.87 sunglasses 0.51 person 0.51 car 0.61 dog 0.97 swine 0 50 ey0.87 monkey 0.81 dog 0.55 0.94 dog dog 0.97 hat with a wide brim 0.96 person 0 .77 snake 0.74 dog 0.93 julieDANIELLE table 0.54 person 0.52 person 0.85 zebra 0. .55 zebra 0.83 zebra 0 80 zebra 0.52 pretzel 0.69 BAS dog 0.71 ladybug 0.90 guacamole 0.64 person 0.58 person 0.85 dog 0.98 with a wide brim dog 0.98 person 0.73 person 0.81 elephant 1.00 bird0.99 PHOTOS COM computer keyboard 0.52 dog 0.97 dog 0.92 bird 0.94 Day ILRI/ 8L5 Yanalliium Jialimal Schart cart 1.00 person 0.87 91 leach othusin your exanhie person 0.77 person 0.57 chair 0.64 79 person 0.52 Shillines butterfly 0.98 Anthmuic 645 5 d Alnas 32 ろん 19 10종 Nater QiullA 1 10 +539 + modulator 6754 8365 X9 embention 1189 person 0.91 person 0.75 person 0.73 bird 0 83 bird 1.00 stethoscope 0.83 person 0.61 bird 0.78"
        },
        {
            "bounding_box": [
                {
                    "x": 301,
                    "y": 2765
                },
                {
                    "x": 2176,
                    "y": 2765
                },
                {
                    "x": 2176,
                    "y": 2813
                },
                {
                    "x": 301,
                    "y": 2813
                }
            ],
            "category": "caption",
            "html": "<caption id='208' style='font-size:16px'>Figure 9: More randomly selected examples. See Figure 8 caption for details. Viewing digitally with zoom is recommended.</caption>",
            "id": 208,
            "page": 17,
            "text": "Figure 9: More randomly selected examples. See Figure 8 caption for details. Viewing digitally with zoom is recommended."
        },
        {
            "bounding_box": [
                {
                    "x": 1216,
                    "y": 3051
                },
                {
                    "x": 1264,
                    "y": 3051
                },
                {
                    "x": 1264,
                    "y": 3095
                },
                {
                    "x": 1216,
                    "y": 3095
                }
            ],
            "category": "footer",
            "html": "<footer id='209' style='font-size:20px'>17</footer>",
            "id": 209,
            "page": 17,
            "text": "17"
        },
        {
            "bounding_box": [
                {
                    "x": 239,
                    "y": 288
                },
                {
                    "x": 2245,
                    "y": 288
                },
                {
                    "x": 2245,
                    "y": 2742
                },
                {
                    "x": 239,
                    "y": 2742
                }
            ],
            "category": "figure",
            "html": "<figure><img id='210' style='font-size:14px' alt=\"person 0 73\norange 0.73\nperson 0.51\npineapple 1 .00 bowl 0.63\ntennis ball 0 600le1.00\norange 0.71\nperson 0.81 motorcycle 0.64\nperson 0.57 orange 0.78 lemon 0.86\nperson 0.53\nbage 0.57\nNoney\nVAKET\nMA\nlamp 0.61\nsoccer ball 0 67\ngolt ball 0.81\nbee 0.85 person 0.52\ngolf ball 0.51 jellyfish 0.71 dumbbell 1.00\nguil Dall 0.79\nbowl 0.54\ngolf (\ngolf ball 0.76 lemon 0.53\ngolf ball 1.00 hamburger 0.78\ngolfball O 60 table 0.59\ngolf ball 1.00 \ngoldfish 0.76\nperson 0.85 head cabbage 0. .75 microwave 0.60\nperson 0.57\nguitar 1 .00\nhead cabbage 0.83 tick0.64\nguitar 1 .00\nmicrophone 1.00\nguitar 0.88\ntable 0.53\ndog 0.74\ntable 0.63 keyboard 0.78\ncomputer\nFRIAS\nSERGIO\n:01DWd 09/02/2010\nperson 0.81\nperson 0.92\ndog 0.98\nrabbit 1.00\ntennis ball 0.67\nlemon 0.80\nwatercraft 0.86\nsunglasses 0. 52\nmilk can 1.00\nmilk can 1.00 0.87 antelope 0.74\nperson\n dog 0.87\nbookshelf 0.50 horse 0. 78\n16 cattle 0.81\npomegranate 1.00\ngiant panda 0.61\nchair 0.86\ntvor monitor 0.52\ndog 0.88\nhird 0 94\nantelope 0.68 0.60\nsnake\nchair0.86\nperson 0.79\ndog 0.98 0.76\nsnake\nlamp 0.65 watercraft 0.91\namp 0.86\nfox0.81\nfox 1 .00g 0.88\nmonkey 1.00\nmonkey 1.00 元\nmonkey 0.52\ntable 0.83\nmonkey 0.88\nPOI8\ntv or monitortv or monitor 0.54 8109\nmonkey 0.90\ntv or monitor 0.58\ntable0.62\nwatercraft 0.56 .8\nperson 0\ndragonfly 0.70 electric fan 0.83\nbird0.69\nhamburger 0.60\nhamburger 0.72\ndragonfly 0.60\ncup or mug 0.72\nbird 0.95\nisopod 0.56\nstarfish 0.67\nbird 0.78\nHOTOCOURT soccer ball 0.63\nelhelmet 0.64 '8월 electric fan 1.00\" data-coord=\"top-left:(239,288); bottom-right:(2245,2742)\" /></figure>",
            "id": 210,
            "page": 18,
            "text": "person 0 73 orange 0.73 person 0.51 pineapple 1 .00 bowl 0.63 tennis ball 0 600le1.00 orange 0.71 person 0.81 motorcycle 0.64 person 0.57 orange 0.78 lemon 0.86 person 0.53 bage 0.57 Noney VAKET MA lamp 0.61 soccer ball 0 67 golt ball 0.81 bee 0.85 person 0.52 golf ball 0.51 jellyfish 0.71 dumbbell 1.00 guil Dall 0.79 bowl 0.54 golf ( golf ball 0.76 lemon 0.53 golf ball 1.00 hamburger 0.78 golfball O 60 table 0.59 golf ball 1.00  goldfish 0.76 person 0.85 head cabbage 0. .75 microwave 0.60 person 0.57 guitar 1 .00 head cabbage 0.83 tick0.64 guitar 1 .00 microphone 1.00 guitar 0.88 table 0.53 dog 0.74 table 0.63 keyboard 0.78 computer FRIAS SERGIO :01DWd 09/02/2010 person 0.81 person 0.92 dog 0.98 rabbit 1.00 tennis ball 0.67 lemon 0.80 watercraft 0.86 sunglasses 0. 52 milk can 1.00 milk can 1.00 0.87 antelope 0.74 person  dog 0.87 bookshelf 0.50 horse 0. 78 16 cattle 0.81 pomegranate 1.00 giant panda 0.61 chair 0.86 tvor monitor 0.52 dog 0.88 hird 0 94 antelope 0.68 0.60 snake chair0.86 person 0.79 dog 0.98 0.76 snake lamp 0.65 watercraft 0.91 amp 0.86 fox0.81 fox 1 .00g 0.88 monkey 1.00 monkey 1.00 元 monkey 0.52 table 0.83 monkey 0.88 POI8 tv or monitortv or monitor 0.54 8109 monkey 0.90 tv or monitor 0.58 table0.62 watercraft 0.56 .8 person 0 dragonfly 0.70 electric fan 0.83 bird0.69 hamburger 0.60 hamburger 0.72 dragonfly 0.60 cup or mug 0.72 bird 0.95 isopod 0.56 starfish 0.67 bird 0.78 HOTOCOURT soccer ball 0.63 elhelmet 0.64 '8월 electric fan 1.00"
        },
        {
            "bounding_box": [
                {
                    "x": 198,
                    "y": 2785
                },
                {
                    "x": 2277,
                    "y": 2785
                },
                {
                    "x": 2277,
                    "y": 2882
                },
                {
                    "x": 198,
                    "y": 2882
                }
            ],
            "category": "caption",
            "html": "<caption id='211' style='font-size:16px'>Figure 10: Curated examples. Each image was selected because we found it impressive, surprising, interesting, or amusing. Viewing<br>digitally with zoom is recommended.</caption>",
            "id": 211,
            "page": 18,
            "text": "Figure 10: Curated examples. Each image was selected because we found it impressive, surprising, interesting, or amusing. Viewing digitally with zoom is recommended."
        },
        {
            "bounding_box": [
                {
                    "x": 1217,
                    "y": 3053
                },
                {
                    "x": 1264,
                    "y": 3053
                },
                {
                    "x": 1264,
                    "y": 3093
                },
                {
                    "x": 1217,
                    "y": 3093
                }
            ],
            "category": "footer",
            "html": "<footer id='212' style='font-size:20px'>18</footer>",
            "id": 212,
            "page": 18,
            "text": "18"
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 311
                },
                {
                    "x": 864,
                    "y": 311
                },
                {
                    "x": 864,
                    "y": 349
                },
                {
                    "x": 286,
                    "y": 349
                }
            ],
            "category": "paragraph",
            "html": "<p id='213' style='font-size:18px'>object detection. In CVPR, 2013. 6, 7</p>",
            "id": 213,
            "page": 19,
            "text": "object detection. In CVPR, 2013. 6, 7"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 353
                },
                {
                    "x": 1202,
                    "y": 353
                },
                {
                    "x": 1202,
                    "y": 1957
                },
                {
                    "x": 201,
                    "y": 1957
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='214' style='font-size:14px'>[32] H. A. Rowley, S. Baluja, and T. Kanade. Neural network-<br>based face detection. TPAMI, 1998. 2<br>[33] D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learn-<br>ing internal representations by error propagation. Parallel<br>Distributed Processing, 1:318-362, 1986. 1<br>[34] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus,<br>and Y. LeCun. OverFeat: Integrated Recognition, Localiza-<br>tion and Detection using Convolutional Networks. In ICLR,<br>2014. 1, 2, 4, 10<br>[35] P. Sermanet, K. Kavukcuoglu, S. Chintala, and Y. LeCun.<br>Pedestrian detection with unsupervised multi-stage feature<br>learning. In CVPR, 2013. 2<br>[36] H. Su, J. Deng, and L. Fei-Fei. Crowdsourcing annotations<br>for visual object detection. In AAAI Technical Report, 4th<br>Human Computation Workshop, 2012. 8<br>[37] K. Sung and T. Poggio. Example-based learning for view-<br>based human face detection. Technical Report A.I. Memo<br>No. 1521, Massachussets Institute of Technology, 1994. 4<br>[38] C. Szegedy, A. Toshev, and D. Erhan. Deep neural networks<br>for object detection. In NIPS, 2013. 2<br>[39] J. Uijlings, K. van de Sande, T. Gevers, and A. Smeulders.<br>Selective search for object recognition. IJCV, 2013. 1, 2, 3,<br>4, 5, 9<br>[40] R. Vaillant, C. Monrocq, and Y. LeCun. Original approach<br>for the localisation of objects in images. IEE Proc on Vision,<br>Image, and Signal Processing, 1994. 2<br>[41] X. Wang, M. Yang, S. Zhu, and Y. Lin. Regionlets for generic<br>object detection. In ICCV, 2013. 3, 5<br>[42] M. Zeiler, G. Taylor, and R. Fergus. Adaptive deconvolu-<br>tional networks for mid and high level feature learning. In<br>CVPR, 2011. 4<br>[43] K. Simonyan and A. Zisserman. Very Deep Convolu-<br>tional Networks for Large-Scale Image Recognition. arXiv<br>preprint, arXiv:1409.1556, 2014. 6, 7, 14</p>",
            "id": 214,
            "page": 19,
            "text": " H. A. Rowley, S. Baluja, and T. Kanade. Neural networkbased face detection. TPAMI, 1998. 2  D. E. Rumelhart, G. E. Hinton, and R. J. Williams. Learning internal representations by error propagation. Parallel Distributed Processing, 1:318-362, 1986. 1  P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun. OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks. In ICLR, 2014. 1, 2, 4, 10  P. Sermanet, K. Kavukcuoglu, S. Chintala, and Y. LeCun. Pedestrian detection with unsupervised multi-stage feature learning. In CVPR, 2013. 2  H. Su, J. Deng, and L. Fei-Fei. Crowdsourcing annotations for visual object detection. In AAAI Technical Report, 4th Human Computation Workshop, 2012. 8  K. Sung and T. Poggio. Example-based learning for viewbased human face detection. Technical Report A.I. Memo No. 1521, Massachussets Institute of Technology, 1994. 4  C. Szegedy, A. Toshev, and D. Erhan. Deep neural networks for object detection. In NIPS, 2013. 2  J. Uijlings, K. van de Sande, T. Gevers, and A. Smeulders. Selective search for object recognition. IJCV, 2013. 1, 2, 3, 4, 5, 9  R. Vaillant, C. Monrocq, and Y. LeCun. Original approach for the localisation of objects in images. IEE Proc on Vision, Image, and Signal Processing, 1994. 2  X. Wang, M. Yang, S. Zhu, and Y. Lin. Regionlets for generic object detection. In ICCV, 2013. 3, 5  M. Zeiler, G. Taylor, and R. Fergus. Adaptive deconvolutional networks for mid and high level feature learning. In CVPR, 2011. 4  K. Simonyan and A. Zisserman. Very Deep Convolutional Networks for Large-Scale Image Recognition. arXiv preprint, arXiv:1409.1556, 2014. 6, 7, 14"
        },
        {
            "bounding_box": [
                {
                    "x": 1217,
                    "y": 3050
                },
                {
                    "x": 1264,
                    "y": 3050
                },
                {
                    "x": 1264,
                    "y": 3095
                },
                {
                    "x": 1217,
                    "y": 3095
                }
            ],
            "category": "footer",
            "html": "<footer id='215' style='font-size:18px'>19</footer>",
            "id": 215,
            "page": 19,
            "text": "19"
        },
        {
            "bounding_box": [
                {
                    "x": 284,
                    "y": 287
                },
                {
                    "x": 2213,
                    "y": 287
                },
                {
                    "x": 2213,
                    "y": 2910
                },
                {
                    "x": 284,
                    "y": 2910
                }
            ],
            "category": "figure",
            "html": "<figure><img id='216' style='font-size:14px' alt=\"person 0.82\nsnake 0.76 person 0.94\nperson 0 person 0.60\npeperson 0.67\ngoldfish 0.76 stethoscope 0 .56\nhird0 79\nfrog 0.78 goldfish 0.76 goldfish 0.58\ntable 0.81\nwatercraft 0 .55\nperson 0.94 person 0.80\ntv or monitor 0.82\njellyfish 0.67\nperson 0.55 0 68\nperson\nlemon 0.52 0. 79 person 0.59\nperson\nperson 0.65\nperson 0.52\nlizard0.58\nperson0.61\nperson 0.82 dog 0.60 person 0.88\nperson 0.79\ncomputer keyboard 0.81\nbaseball 00\nperson 0.74\nperson 0.69\n福島 山  J.79\nperson 0.94 person person 0.58person 0.79\nvolleyball 0.70 pineapple 1.00\nperson 0 80\n代替 Iperson 0.54 person 0.56\nperson 0.94 person 0.66\nperson 0.84 person 0.59 person 0.94 person 0.94\nperson 0.95\nperson 0.95\ntable 0.82 person 0 person0.69\nbrassie e 0.71\nchair 0.50\n2 swimming trunks 0.56\nrugby ball 0.91\nperson 0.92\nbaseball 0.75 tiger 1.00\nperson\ntiger 0.59\nhelmet 0.74 dog 0.98\nvacuum 1.00\ndog 0.93\nhimal ni 55\ntiger 0.67\nRIGOHRICOH person 0.94 person 0.75\nperson 0.65\nminiskirt 0 64\nperson 0.53\nski0.80\nski0.80\n2009ⒸGennaroPascale vika\nDOW 0.52\nperson 0.78\nperson 0 82\nwww.allsonsshop.com\nhirry 0 56 awberry 0.79\nwhale 1.00 strawberry 0.70\nburrito 0.54\nperson 0.92\nperson 0.92\nchair 0.53\nall 0 croquet ball 0.91\nmushroom 0.571\nJoyeux\nNoel\nplastic bag 0 62 tv or monitor 0.57\nwatercraft 0.87\nplastic bag 0.62\ndog 0.94\ncart 0. 80 person 0. 53\nperson 0 .79\nwhale 0 .88\nwatercraft0.91\ncar0.70\nwatercraft 0.58\nantelope 0.63\nante\nbird 0.59 antelope 1.00\nperson 0.54 erson 0.881.89 traffic light 0.79\n79\nantelope 0.63\nhorizontal bar 1.00 0\nperson 0.82 antelope 0.73 person 0.80\nfox 0.57\nperson 0.56 cucumber 0.53\nantelope 0.94\ncucumber 0.52\nhelmet 0.69\nperson 0.82 orange 0.56\nperson 0.90\ndog 0.97 orange 0.66\nbird 0.96 bird0.64\nhorse 0.92\nbird0.89\nbird0.53\ndog 0.98 bird 0.523\nsnake 0.64\nbird0.5697\nperson 0.72 horse 0.69\nbird 0.94\norange 0.66)range 0.79\norange 0.59\norange 0.71\n�JOSEFINE STENUDD\nperson 0.83 elephant0.60\nperson 0 .82\nguitar 1.00\nperson 0 .54 person 0.74\nperson 0.83 person 0 .80\ncar 1 00 car 0.97\nperson 0 .90\ndog 0.85\nbicycle 0.92\nlog 0.86\ndog 0 dog 0.50 dog D 98\" data-coord=\"top-left:(284,287); bottom-right:(2213,2910)\" /></figure>",
            "id": 216,
            "page": 20,
            "text": "person 0.82 snake 0.76 person 0.94 person 0 person 0.60 peperson 0.67 goldfish 0.76 stethoscope 0 .56 hird0 79 frog 0.78 goldfish 0.76 goldfish 0.58 table 0.81 watercraft 0 .55 person 0.94 person 0.80 tv or monitor 0.82 jellyfish 0.67 person 0.55 0 68 person lemon 0.52 0. 79 person 0.59 person person 0.65 person 0.52 lizard0.58 person0.61 person 0.82 dog 0.60 person 0.88 person 0.79 computer keyboard 0.81 baseball 00 person 0.74 person 0.69 福島 山  J.79 person 0.94 person person 0.58person 0.79 volleyball 0.70 pineapple 1.00 person 0 80 代替 Iperson 0.54 person 0.56 person 0.94 person 0.66 person 0.84 person 0.59 person 0.94 person 0.94 person 0.95 person 0.95 table 0.82 person 0 person0.69 brassie e 0.71 chair 0.50 2 swimming trunks 0.56 rugby ball 0.91 person 0.92 baseball 0.75 tiger 1.00 person tiger 0.59 helmet 0.74 dog 0.98 vacuum 1.00 dog 0.93 himal ni 55 tiger 0.67 RIGOHRICOH person 0.94 person 0.75 person 0.65 miniskirt 0 64 person 0.53 ski0.80 ski0.80 2009ⒸGennaroPascale vika DOW 0.52 person 0.78 person 0 82 www.allsonsshop.com hirry 0 56 awberry 0.79 whale 1.00 strawberry 0.70 burrito 0.54 person 0.92 person 0.92 chair 0.53 all 0 croquet ball 0.91 mushroom 0.571 Joyeux Noel plastic bag 0 62 tv or monitor 0.57 watercraft 0.87 plastic bag 0.62 dog 0.94 cart 0. 80 person 0. 53 person 0 .79 whale 0 .88 watercraft0.91 car0.70 watercraft 0.58 antelope 0.63 ante bird 0.59 antelope 1.00 person 0.54 erson 0.881.89 traffic light 0.79 79 antelope 0.63 horizontal bar 1.00 0 person 0.82 antelope 0.73 person 0.80 fox 0.57 person 0.56 cucumber 0.53 antelope 0.94 cucumber 0.52 helmet 0.69 person 0.82 orange 0.56 person 0.90 dog 0.97 orange 0.66 bird 0.96 bird0.64 horse 0.92 bird0.89 bird0.53 dog 0.98 bird 0.523 snake 0.64 bird0.5697 person 0.72 horse 0.69 bird 0.94 orange 0.66)range 0.79 orange 0.59 orange 0.71 �JOSEFINE STENUDD person 0.83 elephant0.60 person 0 .82 guitar 1.00 person 0 .54 person 0.74 person 0.83 person 0 .80 car 1 00 car 0.97 person 0 .90 dog 0.85 bicycle 0.92 log 0.86 dog 0 dog 0.50 dog D 98"
        },
        {
            "bounding_box": [
                {
                    "x": 362,
                    "y": 2959
                },
                {
                    "x": 2118,
                    "y": 2959
                },
                {
                    "x": 2118,
                    "y": 3004
                },
                {
                    "x": 362,
                    "y": 3004
                }
            ],
            "category": "caption",
            "html": "<caption id='217' style='font-size:16px'>Figure 11: More curated examples. See Figure 10 caption for details. Viewing digitally with zoom is recommended.</caption>",
            "id": 217,
            "page": 20,
            "text": "Figure 11: More curated examples. See Figure 10 caption for details. Viewing digitally with zoom is recommended."
        },
        {
            "bounding_box": [
                {
                    "x": 1215,
                    "y": 3054
                },
                {
                    "x": 1264,
                    "y": 3054
                },
                {
                    "x": 1264,
                    "y": 3092
                },
                {
                    "x": 1215,
                    "y": 3092
                }
            ],
            "category": "footer",
            "html": "<footer id='218' style='font-size:20px'>20</footer>",
            "id": 218,
            "page": 20,
            "text": "20"
        },
        {
            "bounding_box": [
                {
                    "x": 253,
                    "y": 317
                },
                {
                    "x": 2207,
                    "y": 317
                },
                {
                    "x": 2207,
                    "y": 2778
                },
                {
                    "x": 253,
                    "y": 2778
                }
            ],
            "category": "figure",
            "html": "<figure><img id='219' style='font-size:14px' alt=\"pool5 feature: (3,3,1) (top 1 - 24) pool5 feature: (3,3,2) (top 1 - 24)\n1.0 10.9 10.8 |0.8 10.7 |0.7 10.7 |0.7 0.7 0.7 I0.7 |0.7 1.0 0.9 10.9 10.9 0.9 10.8 10.8 0.7 I0.7 0.7 0.7 I0.7\n0.7 0.7 0.6 0.6년 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\npool5 feature: (3,3,3) (top 1 - 24) pool5 feature: (3,3,4) (top 1 - 24)\n0.9 10.8 10.8 10.8 0.8 0.8 10.8 10.7 |0.7 10.7 10.6F 10.6 0.9 0.8 0.7 10.7 |0.7 10.7 10.7 10.7 10.7 0.7-10.7 10.7\n0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.7 0.7 0.7 0.7 0.7 0.7 0.7 10.6 0.6 0.6 0.6 0.6\npool5 feature: (3,3,5) (top 1 - 24) pool5 feature: (3,3,6) (top 1 - 24)\n0.9 I0.8 10.8 |0.8 |0.8 10.8 10.8 0.8 10.7 0.7 0.7 10.7 0.9 10.8 |0.8 10.8 10.8 10.7 |0.7 |0.7 10.7 10.7 10.7 |0.7\n0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7\npool5 feature: (3,3,7) (top 1 - 24) pool5 feature: (3,3,8) (top 1 - 24)\n0.9 10.8 0.8 10.8 |0.8 10.8 10.7 0.7 10.7 10.7 10.7 10.7 0.9 0.8 10.8 0.8 10.8 |0.8 10.8 0.7 10.7 0.7 10.7 10.7\n0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.6 0.6 0.6 0.6 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 -\npool5 feature: (3,3,9) (top 1 - 24) pool5 feature: (3,3,10) (top 1 - 24)\n0.8 10.8 10.8 0.7 0.7 0.7 0.7 |0.7 i0.7 10.7 10.7 0.7 0.9 10.8 0.8 10.7 I0.6 0.6 |0.6 |0.6 10.6 I0.6 10.6 10.6\n0.7 0.7mm 0.7 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 T0.6 0.6 0.6 0.6 0.6 0.5 0.5\npool5 feature: (3,3,11) (top 1 - 24) pool5 feature: (3,3,12) (top 1 - 24)\n0.7 0.7 0.7 0.7 0.7 |0.6 |0.6 |0.6 10.6 0.6 10.6 0.6 0.9 10.8 10.7一 10.7 0.7 ]0.7 0.7 |0.7 10.7 10.7 0.7 0.7\n0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.7 0.6 0.6- 0.6 0.6 0.6 0.6 0.6- ·0.6 0.6 0.6 0.6\n--------\npool5 feature: (3,3,13) (top 1 - 24) pool5 feature: (3,3,14) (top 1 - 24)\n0.9 10.9 10.8 0.8 |0.8 10.8 10.8 0.8 10.8 0.8 10.8 10.8 0.9 10.9 10.9 0.8 10.8 10.8 0.8 0.8 10.8 0.8 10.8 10.8\n0.8 0.8- 0.8 0.8 0.8 0.8 0.8- 0.8 0.8 0.8 0.8 0.8 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7mm. 0.7 0.7 0.7\npool5 feature: (3,3,15) (top 1 - 24) pool5 feature: (3,3,16) (top 1 - 24)\n0.8m |0.8 10.8 10.8 10.8 0.8 10.8 10.8 0.8 0.8 10.8 10.8 0.9 |0.8 10.8 0.7 10.7 10.7 0.7 0.7 |0.7 10.7 |0.7 |0.7\n0.7 0.7 0.7 0.7 0.7m 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6\npool5 feature: (3,3,17) (top 1 - 24) pool5 feature: (3,3,18) (top 1 - 24)\n0.9 10.9 10.8 I0.8 |0.8 10.8 |0.7 10.7 10.7 10.7 |0.7 0.7 0.8 10.7 10.7 10.7 0.7 i0.7 0.7 10.7 10.7 10.7 ]0.6 10.6\n0.7 10.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6\npool5 feature: (3,3,19) (top 1 - 24) pool5 feature: (3,3,20) (top 1 - 24)\n0.9 10.8 10.8 |0.7 0.7 10.7 10.7 10.7 I0.7 0.6 10.6 10.6 1.0 10.9 10.7 0.7 10.7 0.7 10.7 10.7 10.6 I0.6 10.6 0.6\n0.6 0.6 0.6 0.6 0.6 0.6 0.6■ 0.6 0.6 0.6 0.6n 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6\" data-coord=\"top-left:(253,317); bottom-right:(2207,2778)\" /></figure>",
            "id": 219,
            "page": 21,
            "text": "pool5 feature: (3,3,1) (top 1 - 24) pool5 feature: (3,3,2) (top 1 - 24) 1.0 10.9 10.8 |0.8 10.7 |0.7 10.7 |0.7 0.7 0.7 I0.7 |0.7 1.0 0.9 10.9 10.9 0.9 10.8 10.8 0.7 I0.7 0.7 0.7 I0.7 0.7 0.7 0.6 0.6년 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 pool5 feature: (3,3,3) (top 1 - 24) pool5 feature: (3,3,4) (top 1 - 24) 0.9 10.8 10.8 10.8 0.8 0.8 10.8 10.7 |0.7 10.7 10.6F 10.6 0.9 0.8 0.7 10.7 |0.7 10.7 10.7 10.7 10.7 0.7-10.7 10.7 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.7 0.7 0.7 0.7 0.7 0.7 0.7 10.6 0.6 0.6 0.6 0.6 pool5 feature: (3,3,5) (top 1 - 24) pool5 feature: (3,3,6) (top 1 - 24) 0.9 I0.8 10.8 |0.8 |0.8 10.8 10.8 0.8 10.7 0.7 0.7 10.7 0.9 10.8 |0.8 10.8 10.8 10.7 |0.7 |0.7 10.7 10.7 10.7 |0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 pool5 feature: (3,3,7) (top 1 - 24) pool5 feature: (3,3,8) (top 1 - 24) 0.9 10.8 0.8 10.8 |0.8 10.8 10.7 0.7 10.7 10.7 10.7 10.7 0.9 0.8 10.8 0.8 10.8 |0.8 10.8 0.7 10.7 0.7 10.7 10.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.6 0.6 0.6 0.6 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 pool5 feature: (3,3,9) (top 1 - 24) pool5 feature: (3,3,10) (top 1 - 24) 0.8 10.8 10.8 0.7 0.7 0.7 0.7 |0.7 i0.7 10.7 10.7 0.7 0.9 10.8 0.8 10.7 I0.6 0.6 |0.6 |0.6 10.6 I0.6 10.6 10.6 0.7 0.7mm 0.7 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 T0.6 0.6 0.6 0.6 0.6 0.5 0.5 pool5 feature: (3,3,11) (top 1 - 24) pool5 feature: (3,3,12) (top 1 - 24) 0.7 0.7 0.7 0.7 0.7 |0.6 |0.6 |0.6 10.6 0.6 10.6 0.6 0.9 10.8 10.7一 10.7 0.7 ]0.7 0.7 |0.7 10.7 10.7 0.7 0.7 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.7 0.6 0.6- 0.6 0.6 0.6 0.6 0.6- ·0.6 0.6 0.6 0.6 -------pool5 feature: (3,3,13) (top 1 - 24) pool5 feature: (3,3,14) (top 1 - 24) 0.9 10.9 10.8 0.8 |0.8 10.8 10.8 0.8 10.8 0.8 10.8 10.8 0.9 10.9 10.9 0.8 10.8 10.8 0.8 0.8 10.8 0.8 10.8 10.8 0.8 0.8- 0.8 0.8 0.8 0.8 0.8- 0.8 0.8 0.8 0.8 0.8 0.8 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7mm. 0.7 0.7 0.7 pool5 feature: (3,3,15) (top 1 - 24) pool5 feature: (3,3,16) (top 1 - 24) 0.8m |0.8 10.8 10.8 10.8 0.8 10.8 10.8 0.8 0.8 10.8 10.8 0.9 |0.8 10.8 0.7 10.7 10.7 0.7 0.7 |0.7 10.7 |0.7 |0.7 0.7 0.7 0.7 0.7 0.7m 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 pool5 feature: (3,3,17) (top 1 - 24) pool5 feature: (3,3,18) (top 1 - 24) 0.9 10.9 10.8 I0.8 |0.8 10.8 |0.7 10.7 10.7 10.7 |0.7 0.7 0.8 10.7 10.7 10.7 0.7 i0.7 0.7 10.7 10.7 10.7 ]0.6 10.6 0.7 10.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.7 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 pool5 feature: (3,3,19) (top 1 - 24) pool5 feature: (3,3,20) (top 1 - 24) 0.9 10.8 10.8 |0.7 0.7 10.7 10.7 10.7 I0.7 0.6 10.6 10.6 1.0 10.9 10.7 0.7 10.7 0.7 10.7 10.7 10.6 I0.6 10.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6■ 0.6 0.6 0.6 0.6n 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2787
                },
                {
                    "x": 2273,
                    "y": 2787
                },
                {
                    "x": 2273,
                    "y": 2974
                },
                {
                    "x": 202,
                    "y": 2974
                }
            ],
            "category": "caption",
            "html": "<br><caption id='220' style='font-size:16px'>Figure 12: We show the 24 region proposals, out of the approximately 10 million regions in VOC 2007 test, that most strongly<br>activate each of 20 units. Each montage is labeled by the unit's (y, x, channel) position in the 6 x 6 x 256 dimensional pool5 feature map.<br>Each image region is drawn with an overlay of the unit's receptive field in white. The activation value (which we normalize by dividing by<br>the max activation value over all units in a channel) is shown in the receptive field's upper-left corner. Best viewed digitally with zoom.</caption>",
            "id": 220,
            "page": 21,
            "text": "Figure 12: We show the 24 region proposals, out of the approximately 10 million regions in VOC 2007 test, that most strongly activate each of 20 units. Each montage is labeled by the unit's (y, x, channel) position in the 6 x 6 x 256 dimensional pool5 feature map. Each image region is drawn with an overlay of the unit's receptive field in white. The activation value (which we normalize by dividing by the max activation value over all units in a channel) is shown in the receptive field's upper-left corner. Best viewed digitally with zoom."
        },
        {
            "bounding_box": [
                {
                    "x": 1215,
                    "y": 3054
                },
                {
                    "x": 1259,
                    "y": 3054
                },
                {
                    "x": 1259,
                    "y": 3093
                },
                {
                    "x": 1215,
                    "y": 3093
                }
            ],
            "category": "footer",
            "html": "<footer id='221' style='font-size:20px'>21</footer>",
            "id": 221,
            "page": 21,
            "text": "21"
        }
    ]
}