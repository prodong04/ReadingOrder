{
  "id": "62a0e034-0f92-11ef-8230-426932df3dcf",
  "pdf_path": "./pdf/AI_VIT_O/2304.02643v1.pdf",
  "elements": [
    {
      "bounding_box": [
        {
          "x": 947,
          "y": 413
        },
        {
          "x": 1528,
          "y": 413
        },
        {
          "x": 1528,
          "y": 487
        },
        {
          "x": 947,
          "y": 487
        }
      ],
      "category": "paragraph",
      "html": "<p id='0' style='font-size:22px'>Segment Anything</p>",
      "id": 0,
      "page": 1,
      "text": "Segment Anything"
    },
    {
      "bounding_box": [
        {
          "x": 282,
          "y": 517
        },
        {
          "x": 2187,
          "y": 517
        },
        {
          "x": 2187,
          "y": 754
        },
        {
          "x": 282,
          "y": 754
        }
      ],
      "category": "paragraph",
      "html": "<p id='1' style='font-size:18px'>Alexander Kirillov1,2,4 Eric Mintun2 Nikhila Ravi1,2 Hanzi Mao2 Chloe Rolland3 Laura Gustafson3<br>Tete Xiao3 Spencer Whitehead Alexander C. Berg Wan-Yen Lo Piotr Dollar4 Ross Girshick4<br>1 project lead 2joint first author 3 equal contribution 4directional lead<br>Meta AI Research, FAIR</p>",
      "id": 1,
      "page": 1,
      "text": "Alexander Kirillov1,2,4 Eric Mintun2 Nikhila Ravi1,2 Hanzi Mao2 Chloe Rolland3 Laura Gustafson3\nTete Xiao3 Spencer Whitehead Alexander C. Berg Wan-Yen Lo Piotr Dollar4 Ross Girshick4\n1 project lead 2joint first author 3 equal contribution 4directional lead\nMeta AI Research, FAIR"
    },
    {
      "bounding_box": [
        {
          "x": 233,
          "y": 808
        },
        {
          "x": 2263,
          "y": 808
        },
        {
          "x": 2263,
          "y": 1317
        },
        {
          "x": 233,
          "y": 1317
        }
      ],
      "category": "figure",
      "html": "<figure><img id='2' style='font-size:14px' alt=\"valid mask valid mask annotate\nlightweight mask decoder model data\ntrain\nmodel\nimage Segment Anything 1B (SA-1B):\nencoder\n· 1+ billion masks\nprompt\n· 11 million images\ncat with encoder\nblack ears · privacy respecting\n· licensed images\nsegmentation prompt image prompt image\n(a) Task: promptable segmentation (b) Model: Segment Anything Model (SAM) (c) Data: data engine (top) & dataset (bottom)\" data-coord=\"top-left:(233,808); bottom-right:(2263,1317)\" /></figure>",
      "id": 2,
      "page": 1,
      "text": "valid mask valid mask annotate\nlightweight mask decoder model data\ntrain\nmodel\nimage Segment Anything 1B (SA-1B):\nencoder\n· 1+ billion masks\nprompt\n· 11 million images\ncat with encoder\nblack ears · privacy respecting\n· licensed images\nsegmentation prompt image prompt image\n(a) Task: promptable segmentation (b) Model: Segment Anything Model (SAM) (c) Data: data engine (top) & dataset (bottom)"
    },
    {
      "bounding_box": [
        {
          "x": 198,
          "y": 1338
        },
        {
          "x": 2277,
          "y": 1338
        },
        {
          "x": 2277,
          "y": 1490
        },
        {
          "x": 198,
          "y": 1490
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='3' style='font-size:16px'>Figure 1: We aim to build a foundation model for segmentation by introducing three interconnected components: a prompt-<br>able segmentation task, a segmentation model (SAM) that powers data annotation and enables zero-shot transfer to a range<br>of tasks via prompt engineering, and a data engine for collecting SA-1B, our dataset of over 1 billion masks.</p>",
      "id": 3,
      "page": 1,
      "text": "Figure 1: We aim to build a foundation model for segmentation by introducing three interconnected components: a prompt-\nable segmentation task, a segmentation model (SAM) that powers data annotation and enables zero-shot transfer to a range\nof tasks via prompt engineering, and a data engine for collecting SA-1B, our dataset of over 1 billion masks."
    },
    {
      "bounding_box": [
        {
          "x": 604,
          "y": 1548
        },
        {
          "x": 797,
          "y": 1548
        },
        {
          "x": 797,
          "y": 1599
        },
        {
          "x": 604,
          "y": 1599
        }
      ],
      "category": "paragraph",
      "html": "<p id='4' style='font-size:22px'>Abstract</p>",
      "id": 4,
      "page": 1,
      "text": "Abstract"
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 1618
        },
        {
          "x": 1198,
          "y": 1618
        },
        {
          "x": 1198,
          "y": 2319
        },
        {
          "x": 200,
          "y": 2319
        }
      ],
      "category": "paragraph",
      "html": "<p id='5' style='font-size:18px'>We introduce the Segment Anything (SA) project: a new<br>task, model, and datasetfor image segmentation. Using our<br>efficient model in a data collection loop, we built the largest<br>segmentation dataset to date (by far), with over 1 billion<br>masks on 11M licensed and privacy respecting images. The<br>model is designed and trained to be promptable, SO it can<br>transfer zero-shot to new image distributions and tasks. We<br>evaluate its capabilities on numerous tasks and find that<br>its zero-shot performance is impressive - often competitive<br>with or even superior to prior fully supervised results. We<br>are releasing the Segment Anything Model (SAM) and cor-<br>responding dataset (SA-1B) of 1B masks and 11M images at<br>https://segment-anything.com to foster research into foun-<br>dation models for computer vision.</p>",
      "id": 5,
      "page": 1,
      "text": "We introduce the Segment Anything (SA) project: a new\ntask, model, and datasetfor image segmentation. Using our\nefficient model in a data collection loop, we built the largest\nsegmentation dataset to date (by far), with over 1 billion\nmasks on 11M licensed and privacy respecting images. The\nmodel is designed and trained to be promptable, SO it can\ntransfer zero-shot to new image distributions and tasks. We\nevaluate its capabilities on numerous tasks and find that\nits zero-shot performance is impressive - often competitive\nwith or even superior to prior fully supervised results. We\nare releasing the Segment Anything Model (SAM) and cor-\nresponding dataset (SA-1B) of 1B masks and 11M images at\nhttps://segment-anything.com to foster research into foun-\ndation models for computer vision."
    },
    {
      "bounding_box": [
        {
          "x": 205,
          "y": 2390
        },
        {
          "x": 531,
          "y": 2390
        },
        {
          "x": 531,
          "y": 2441
        },
        {
          "x": 205,
          "y": 2441
        }
      ],
      "category": "paragraph",
      "html": "<p id='6' style='font-size:20px'>1. Introduction</p>",
      "id": 6,
      "page": 1,
      "text": "1. Introduction"
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 2477
        },
        {
          "x": 1200,
          "y": 2477
        },
        {
          "x": 1200,
          "y": 2978
        },
        {
          "x": 202,
          "y": 2978
        }
      ],
      "category": "paragraph",
      "html": "<p id='7' style='font-size:18px'>Large language models pre-trained on web-scale datasets<br>are revolutionizing NLP with strong zero-shot and few-shot<br>generalization [10]. These \"foundation models\" [8] can<br>generalize to tasks and data distributions beyond those seen<br>during training. This capability is often implemented with<br>prompt engineering in which hand-crafted text is used to<br>prompt the language model to generate a valid textual re-<br>sponse for the task at hand. When scaled and trained with<br>abundant text corpora from the web, these models' zero and<br>few-shot performance compares surprisingly well to (even</p>",
      "id": 7,
      "page": 1,
      "text": "Large language models pre-trained on web-scale datasets\nare revolutionizing NLP with strong zero-shot and few-shot\ngeneralization [10]. These \"foundation models\" [8] can\ngeneralize to tasks and data distributions beyond those seen\nduring training. This capability is often implemented with\nprompt engineering in which hand-crafted text is used to\nprompt the language model to generate a valid textual re-\nsponse for the task at hand. When scaled and trained with\nabundant text corpora from the web, these models' zero and\nfew-shot performance compares surprisingly well to (even"
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 1557
        },
        {
          "x": 2273,
          "y": 1557
        },
        {
          "x": 2273,
          "y": 1699
        },
        {
          "x": 1280,
          "y": 1699
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='8' style='font-size:20px'>matching in some cases) fine-tuned models [10, 21]. Empir-<br>ical trends show this behavior improving with model scale,<br>dataset size, and total training compute [56, 10, 21, 51].</p>",
      "id": 8,
      "page": 1,
      "text": "matching in some cases) fine-tuned models [10, 21]. Empir-\nical trends show this behavior improving with model scale,\ndataset size, and total training compute [56, 10, 21, 51]."
    },
    {
      "bounding_box": [
        {
          "x": 1277,
          "y": 1704
        },
        {
          "x": 2277,
          "y": 1704
        },
        {
          "x": 2277,
          "y": 2345
        },
        {
          "x": 1277,
          "y": 2345
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='9' style='font-size:18px'>Foundation models have also been explored in computer<br>vision, albeit to a lesser extent. Perhaps the most promi-<br>nent illustration aligns paired text and images from the web.<br>For example, CLIP [82] and ALIGN [55] use contrastive<br>learning to train text and image encoders that align the two<br>modalities. Once trained, engineered text prompts enable<br>zero-shot generalization to novel visual concepts and data<br>distributions. Such encoders also compose effectively with<br>other modules to enable downstream tasks, such as image<br>generation (e.g., DALL·E [83]). While much progress has<br>been made on vision and language encoders, computer vi-<br>sion includes a wide range of problems beyond this scope,<br>and for many of these, abundant training data does not exist.</p>",
      "id": 9,
      "page": 1,
      "text": "Foundation models have also been explored in computer\nvision, albeit to a lesser extent. Perhaps the most promi-\nnent illustration aligns paired text and images from the web.\nFor example, CLIP [82] and ALIGN [55] use contrastive\nlearning to train text and image encoders that align the two\nmodalities. Once trained, engineered text prompts enable\nzero-shot generalization to novel visual concepts and data\ndistributions. Such encoders also compose effectively with\nother modules to enable downstream tasks, such as image\ngeneration (e.g., DALL·E [83]). While much progress has\nbeen made on vision and language encoders, computer vi-\nsion includes a wide range of problems beyond this scope,\nand for many of these, abundant training data does not exist."
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 2351
        },
        {
          "x": 2276,
          "y": 2351
        },
        {
          "x": 2276,
          "y": 2646
        },
        {
          "x": 1278,
          "y": 2646
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='10' style='font-size:16px'>In this work, our goal is to build a foundation model for<br>image segmentation. That is, we seek to develop a prompt-<br>able model and pre-train it on a broad dataset using a task<br>that enables powerful generalization. With this model, we<br>aim to solve a range of downstream segmentation problems<br>on new data distributions using prompt engineering.</p>",
      "id": 10,
      "page": 1,
      "text": "In this work, our goal is to build a foundation model for\nimage segmentation. That is, we seek to develop a prompt-\nable model and pre-train it on a broad dataset using a task\nthat enables powerful generalization. With this model, we\naim to solve a range of downstream segmentation problems\non new data distributions using prompt engineering."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 2653
        },
        {
          "x": 2277,
          "y": 2653
        },
        {
          "x": 2277,
          "y": 2799
        },
        {
          "x": 1280,
          "y": 2799
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='11' style='font-size:18px'>The success of this plan hinges on three components:<br>task, model, and data. To develop them, we address the<br>following questions about image segmentation:</p>",
      "id": 11,
      "page": 1,
      "text": "The success of this plan hinges on three components:\ntask, model, and data. To develop them, we address the\nfollowing questions about image segmentation:"
    },
    {
      "bounding_box": [
        {
          "x": 1310,
          "y": 2816
        },
        {
          "x": 2170,
          "y": 2816
        },
        {
          "x": 2170,
          "y": 2975
        },
        {
          "x": 1310,
          "y": 2975
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='12' style='font-size:16px'>1. What task will enable zero-shot generalization?<br>2. What is the corresponding model architecture?<br>3. What data can power this task and model?</p>",
      "id": 12,
      "page": 1,
      "text": "1. What task will enable zero-shot generalization?\n2. What is the corresponding model architecture?\n3. What data can power this task and model?"
    },
    {
      "bounding_box": [
        {
          "x": 1225,
          "y": 3055
        },
        {
          "x": 1250,
          "y": 3055
        },
        {
          "x": 1250,
          "y": 3092
        },
        {
          "x": 1225,
          "y": 3092
        }
      ],
      "category": "footer",
      "html": "<footer id='13' style='font-size:16px'>1</footer>",
      "id": 13,
      "page": 1,
      "text": "1"
    },
    {
      "bounding_box": [
        {
          "x": 64,
          "y": 882
        },
        {
          "x": 151,
          "y": 882
        },
        {
          "x": 151,
          "y": 2322
        },
        {
          "x": 64,
          "y": 2322
        }
      ],
      "category": "footer",
      "html": "<br><footer id='14' style='font-size:14px'>2023<br>Apr<br>5<br>[cs.CV]<br>arXiv:2304.02643v1</footer>",
      "id": 14,
      "page": 1,
      "text": "2023\nApr\n5\n[cs.CV]\narXiv:2304.02643v1"
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 305
        },
        {
          "x": 1199,
          "y": 305
        },
        {
          "x": 1199,
          "y": 1053
        },
        {
          "x": 200,
          "y": 1053
        }
      ],
      "category": "paragraph",
      "html": "<p id='15' style='font-size:16px'>These questions are entangled and require a comprehen-<br>sive solution. We start by defining a promptable segmenta-<br>tion task that is general enough to provide a powerful pre-<br>training objective and to enable a wide range of downstream<br>applications. This task requires a model that supports flex-<br>ible prompting and can output segmentation masks in real-<br>time when prompted to allow for interactive use. To train<br>our model, we need a diverse, large-scale source of data.<br>Unfortunately, there is no web-scale data source for seg-<br>mentation; to address this, we build a \"data engine\" , i.e.,<br>we iterate between using our efficient model to assist in data<br>collection and using the newly collected data to improve the<br>model. We introduce each interconnected component next,<br>followed by the dataset we created and the experiments that<br>demonstrate the effectiveness of our approach.</p>",
      "id": 15,
      "page": 2,
      "text": "These questions are entangled and require a comprehen-\nsive solution. We start by defining a promptable segmenta-\ntion task that is general enough to provide a powerful pre-\ntraining objective and to enable a wide range of downstream\napplications. This task requires a model that supports flex-\nible prompting and can output segmentation masks in real-\ntime when prompted to allow for interactive use. To train\nour model, we need a diverse, large-scale source of data.\nUnfortunately, there is no web-scale data source for seg-\nmentation; to address this, we build a \"data engine\" , i.e.,\nwe iterate between using our efficient model to assist in data\ncollection and using the newly collected data to improve the\nmodel. We introduce each interconnected component next,\nfollowed by the dataset we created and the experiments that\ndemonstrate the effectiveness of our approach."
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 1067
        },
        {
          "x": 1199,
          "y": 1067
        },
        {
          "x": 1199,
          "y": 1914
        },
        {
          "x": 200,
          "y": 1914
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='16' style='font-size:16px'>Task (§2). In NLP and more recently computer vision,<br>foundation models are a promising development that can<br>perform zero-shot and few-shot learning for new datasets<br>and tasks often by using \"prompting\" techniques. Inspired<br>by this line of work, we propose the promptable segmen-<br>tation task, where the goal is to return a valid segmenta-<br>tion mask given any segmentation prompt (see Fig. 1a). A<br>prompt simply specifies what to segment in an image, e.g.,<br>a prompt can include spatial or text information identifying<br>an object. The requirement of a valid output mask means<br>that even when a prompt is ambiguous and could refer to<br>multiple objects (for example, a point on a shirt may in-<br>dicate either the shirt or the person wearing it), the output<br>should be a reasonable mask for at least one of those ob-<br>jects. We use the promptable segmentation task as both a<br>pre-training objective and to solve general downstream seg-<br>mentation tasks via prompt engineering.</p>",
      "id": 16,
      "page": 2,
      "text": "Task (§2). In NLP and more recently computer vision,\nfoundation models are a promising development that can\nperform zero-shot and few-shot learning for new datasets\nand tasks often by using \"prompting\" techniques. Inspired\nby this line of work, we propose the promptable segmen-\ntation task, where the goal is to return a valid segmenta-\ntion mask given any segmentation prompt (see Fig. 1a). A\nprompt simply specifies what to segment in an image, e.g.,\na prompt can include spatial or text information identifying\nan object. The requirement of a valid output mask means\nthat even when a prompt is ambiguous and could refer to\nmultiple objects (for example, a point on a shirt may in-\ndicate either the shirt or the person wearing it), the output\nshould be a reasonable mask for at least one of those ob-\njects. We use the promptable segmentation task as both a\npre-training objective and to solve general downstream seg-\nmentation tasks via prompt engineering."
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 1926
        },
        {
          "x": 1199,
          "y": 1926
        },
        {
          "x": 1199,
          "y": 2978
        },
        {
          "x": 200,
          "y": 2978
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='17' style='font-size:16px'>Model (§3). The promptable segmentation task and the goal<br>of real-world use impose constraints on the model architec-<br>ture. In particular, the model must support flexible prompts,<br>needs to compute masks in amortized real-time to allow in-<br>teractive use, and must be ambiguity-aware. Surprisingly,<br>we find that a simple design satisfies all three constraints:<br>a powerful image encoder computes an image embedding,<br>a prompt encoder embeds prompts, and then the two infor-<br>mation sources are combined in a lightweight mask decoder<br>that predicts segmentation masks. We refer to this model as<br>the Segment Anything Model, or SAM (see Fig. 1b). By<br>separating SAM into an image encoder and a fast prompt<br>encoder / mask decoder, the same image embedding can<br>be reused (and its cost amortized) with different prompts.<br>Given an image embedding, the prompt encoder and mask<br>decoder predict a mask from a prompt in ~50ms in a web<br>browser. We focus on point, box, and mask prompts, and<br>also present initial results with free-form text prompts. To<br>make SAM ambiguity-aware, we design it to predict mul-<br>tiple masks for a single prompt allowing SAM to naturally<br>handle ambiguity, such as the shirt vs. person example.</p>",
      "id": 17,
      "page": 2,
      "text": "Model (§3). The promptable segmentation task and the goal\nof real-world use impose constraints on the model architec-\nture. In particular, the model must support flexible prompts,\nneeds to compute masks in amortized real-time to allow in-\nteractive use, and must be ambiguity-aware. Surprisingly,\nwe find that a simple design satisfies all three constraints:\na powerful image encoder computes an image embedding,\na prompt encoder embeds prompts, and then the two infor-\nmation sources are combined in a lightweight mask decoder\nthat predicts segmentation masks. We refer to this model as\nthe Segment Anything Model, or SAM (see Fig. 1b). By\nseparating SAM into an image encoder and a fast prompt\nencoder / mask decoder, the same image embedding can\nbe reused (and its cost amortized) with different prompts.\nGiven an image embedding, the prompt encoder and mask\ndecoder predict a mask from a prompt in ~50ms in a web\nbrowser. We focus on point, box, and mask prompts, and\nalso present initial results with free-form text prompts. To\nmake SAM ambiguity-aware, we design it to predict mul-\ntiple masks for a single prompt allowing SAM to naturally\nhandle ambiguity, such as the shirt vs. person example."
    },
    {
      "bounding_box": [
        {
          "x": 1277,
          "y": 306
        },
        {
          "x": 2278,
          "y": 306
        },
        {
          "x": 2278,
          "y": 1204
        },
        {
          "x": 1277,
          "y": 1204
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='18' style='font-size:16px'>Data engine (§4). To achieve strong generalization to new<br>data distributions, we found it necessary to train SAM on<br>a large and diverse set of masks, beyond any segmenta-<br>tion dataset that already exists. While a typical approach<br>for foundation models is to obtain data online [82], masks<br>are not naturally abundant and thus we need an alternative<br>strategy. Our solution is to build a \"data engine\" , i.e., we<br>co-develop our model with model-in-the-loop dataset an-<br>notation (see Fig. 1c). Our data engine has three stages:<br>assisted-manual, semi-automatic, and fully automatic. In<br>the first stage, SAM assists annotators in annotating masks,<br>similar to a classic interactive segmentation setup. In the<br>second stage, SAM can automatically generate masks for<br>a subset of objects by prompting it with likely object lo-<br>cations and annotators focus on annotating the remaining<br>objects, helping increase mask diversity. In the final stage,<br>we prompt SAM with a regular grid of foreground points,<br>yielding on average ~100 high-quality masks per image.</p>",
      "id": 18,
      "page": 2,
      "text": "Data engine (§4). To achieve strong generalization to new\ndata distributions, we found it necessary to train SAM on\na large and diverse set of masks, beyond any segmenta-\ntion dataset that already exists. While a typical approach\nfor foundation models is to obtain data online [82], masks\nare not naturally abundant and thus we need an alternative\nstrategy. Our solution is to build a \"data engine\" , i.e., we\nco-develop our model with model-in-the-loop dataset an-\nnotation (see Fig. 1c). Our data engine has three stages:\nassisted-manual, semi-automatic, and fully automatic. In\nthe first stage, SAM assists annotators in annotating masks,\nsimilar to a classic interactive segmentation setup. In the\nsecond stage, SAM can automatically generate masks for\na subset of objects by prompting it with likely object lo-\ncations and annotators focus on annotating the remaining\nobjects, helping increase mask diversity. In the final stage,\nwe prompt SAM with a regular grid of foreground points,\nyielding on average ~100 high-quality masks per image."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 1222
        },
        {
          "x": 2277,
          "y": 1222
        },
        {
          "x": 2277,
          "y": 1670
        },
        {
          "x": 1280,
          "y": 1670
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='19' style='font-size:20px'>Dataset (§5). Our final dataset, SA-1B, includes more than<br>1B masks from 11M licensed and privacy-preserving im-<br>ages (see Fig. 2). SA-1B, collected fully automatically us-<br>ing the final stage of our data engine, has 400x more masks<br>than any existing segmentation dataset [66, 44, 117, 60],<br>and as we verify extensively, the masks are of high quality<br>and diversity. Beyond its use in training SAM to be robust<br>and general, we hope SA-1B becomes a valuable resource<br>for research aiming to build new foundation models.</p>",
      "id": 19,
      "page": 2,
      "text": "Dataset (§5). Our final dataset, SA-1B, includes more than\n1B masks from 11M licensed and privacy-preserving im-\nages (see Fig. 2). SA-1B, collected fully automatically us-\ning the final stage of our data engine, has 400x more masks\nthan any existing segmentation dataset [66, 44, 117, 60],\nand as we verify extensively, the masks are of high quality\nand diversity. Beyond its use in training SAM to be robust\nand general, we hope SA-1B becomes a valuable resource\nfor research aiming to build new foundation models."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 1691
        },
        {
          "x": 2277,
          "y": 1691
        },
        {
          "x": 2277,
          "y": 2038
        },
        {
          "x": 1280,
          "y": 2038
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='20' style='font-size:16px'>Responsible AI (§6). We study and report on potential fair-<br>ness concerns and biases when using SA-1B and SAM. Im-<br>ages in SA-1B span a geographically and economically di-<br>verse set of countries and we found that SAM performs sim-<br>ilarly across different groups of people. Together, we hope<br>this will make our work more equitable for real-world use<br>cases. We provide model and dataset cards in the appendix.</p>",
      "id": 20,
      "page": 2,
      "text": "Responsible AI (§6). We study and report on potential fair-\nness concerns and biases when using SA-1B and SAM. Im-\nages in SA-1B span a geographically and economically di-\nverse set of countries and we found that SAM performs sim-\nilarly across different groups of people. Together, we hope\nthis will make our work more equitable for real-world use\ncases. We provide model and dataset cards in the appendix."
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 2059
        },
        {
          "x": 2277,
          "y": 2059
        },
        {
          "x": 2277,
          "y": 2757
        },
        {
          "x": 1278,
          "y": 2757
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='21' style='font-size:20px'>Experiments (§7). We extensively evaluate SAM. First, us-<br>ing a diverse new suite of 23 segmentation datasets, we find<br>that SAM produces high-quality masks from a single fore-<br>ground point, often only slightly below that of the manu-<br>ally annotated ground truth. Second, we find consistently<br>strong quantitative and qualitative results on a variety of<br>downstream tasks under a zero-shot transfer protocol using<br>prompt engineering, including edge detection, object pro-<br>posal generation, instance segmentation, and a preliminary<br>exploration of text-to-mask prediction. These results sug-<br>gest that SAM can be used out-of-the-box with prompt en-<br>gineering to solve a variety of tasks involving object and<br>image distributions beyond SAM's training data. Neverthe-<br>less, room for improvement remains, as we discuss in §8.</p>",
      "id": 21,
      "page": 2,
      "text": "Experiments (§7). We extensively evaluate SAM. First, us-\ning a diverse new suite of 23 segmentation datasets, we find\nthat SAM produces high-quality masks from a single fore-\nground point, often only slightly below that of the manu-\nally annotated ground truth. Second, we find consistently\nstrong quantitative and qualitative results on a variety of\ndownstream tasks under a zero-shot transfer protocol using\nprompt engineering, including edge detection, object pro-\nposal generation, instance segmentation, and a preliminary\nexploration of text-to-mask prediction. These results sug-\ngest that SAM can be used out-of-the-box with prompt en-\ngineering to solve a variety of tasks involving object and\nimage distributions beyond SAM's training data. Neverthe-\nless, room for improvement remains, as we discuss in §8."
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 2776
        },
        {
          "x": 2277,
          "y": 2776
        },
        {
          "x": 2277,
          "y": 2976
        },
        {
          "x": 1281,
          "y": 2976
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='22' style='font-size:16px'>Release. We are releasing the SA-1B dataset for research<br>purposes and making SAM available under a permissive<br>open license (Apache 2.0) at https://segment-anything.com.<br>We also showcase SAM's capabilities with an online demo.</p>",
      "id": 22,
      "page": 2,
      "text": "Release. We are releasing the SA-1B dataset for research\npurposes and making SAM available under a permissive\nopen license (Apache 2.0) at https://segment-anything.com.\nWe also showcase SAM's capabilities with an online demo."
    },
    {
      "bounding_box": [
        {
          "x": 1225,
          "y": 3053
        },
        {
          "x": 1251,
          "y": 3053
        },
        {
          "x": 1251,
          "y": 3092
        },
        {
          "x": 1225,
          "y": 3092
        }
      ],
      "category": "footer",
      "html": "<footer id='23' style='font-size:14px'>2</footer>",
      "id": 23,
      "page": 2,
      "text": "2"
    },
    {
      "bounding_box": [
        {
          "x": 206,
          "y": 284
        },
        {
          "x": 2276,
          "y": 284
        },
        {
          "x": 2276,
          "y": 2716
        },
        {
          "x": 206,
          "y": 2716
        }
      ],
      "category": "figure",
      "html": "<figure><img id='24' style='font-size:14px' alt=\"masks\n<50\nmasks\n50-100\nmasks\n100-200\nmasks\n200-300\nmasks\nOWN\n300-400\nmasks\n400-500\nmasks\n>500\" data-coord=\"top-left:(206,284); bottom-right:(2276,2716)\" /></figure>",
      "id": 24,
      "page": 3,
      "text": "masks\n<50\nmasks\n50-100\nmasks\n100-200\nmasks\n200-300\nmasks\nOWN\n300-400\nmasks\n400-500\nmasks\n>500"
    },
    {
      "bounding_box": [
        {
          "x": 198,
          "y": 2754
        },
        {
          "x": 2277,
          "y": 2754
        },
        {
          "x": 2277,
          "y": 2958
        },
        {
          "x": 198,
          "y": 2958
        }
      ],
      "category": "caption",
      "html": "<caption id='25' style='font-size:20px'>Figure 2: Example images with overlaid masks from our newly introduced dataset, SA-1B. SA-1B contains 11M diverse,<br>high-resolution, licensed, and privacy protecting images and 1.1B high-quality segmentation masks. These masks were<br>annotated fully automatically by SAM, and as we verify by human ratings and numerous experiments, are of high quality and<br>diversity. We group images by number of masks per image for visualization (there are ~100 masks per image on average).</caption>",
      "id": 25,
      "page": 3,
      "text": "Figure 2: Example images with overlaid masks from our newly introduced dataset, SA-1B. SA-1B contains 11M diverse,\nhigh-resolution, licensed, and privacy protecting images and 1.1B high-quality segmentation masks. These masks were\nannotated fully automatically by SAM, and as we verify by human ratings and numerous experiments, are of high quality and\ndiversity. We group images by number of masks per image for visualization (there are ~100 masks per image on average)."
    },
    {
      "bounding_box": [
        {
          "x": 1222,
          "y": 3053
        },
        {
          "x": 1253,
          "y": 3053
        },
        {
          "x": 1253,
          "y": 3093
        },
        {
          "x": 1222,
          "y": 3093
        }
      ],
      "category": "footer",
      "html": "<footer id='26' style='font-size:16px'>3</footer>",
      "id": 26,
      "page": 3,
      "text": "3"
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 302
        },
        {
          "x": 768,
          "y": 302
        },
        {
          "x": 768,
          "y": 353
        },
        {
          "x": 203,
          "y": 353
        }
      ],
      "category": "paragraph",
      "html": "<p id='27' style='font-size:20px'>2. Segment Anything Task</p>",
      "id": 27,
      "page": 4,
      "text": "2. Segment Anything Task"
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 394
        },
        {
          "x": 1200,
          "y": 394
        },
        {
          "x": 1200,
          "y": 640
        },
        {
          "x": 201,
          "y": 640
        }
      ],
      "category": "paragraph",
      "html": "<p id='28' style='font-size:14px'>We take inspiration from NLP, where the next token pre-<br>diction task is used for foundation model pre-training and<br>to solve diverse downstream tasks via prompt engineer-<br>ing [10]. To build a foundation model for segmentation,<br>we aim to define a task with analogous capabilities.</p>",
      "id": 28,
      "page": 4,
      "text": "We take inspiration from NLP, where the next token pre-\ndiction task is used for foundation model pre-training and\nto solve diverse downstream tasks via prompt engineer-\ning [10]. To build a foundation model for segmentation,\nwe aim to define a task with analogous capabilities."
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 665
        },
        {
          "x": 1200,
          "y": 665
        },
        {
          "x": 1200,
          "y": 1411
        },
        {
          "x": 200,
          "y": 1411
        }
      ],
      "category": "paragraph",
      "html": "<p id='29' style='font-size:14px'>Task. We start by translating the idea of a prompt from NLP<br>to segmentation, where a prompt can be a set of foreground<br>/ background points, a rough box or mask, free-form text,<br>or, in general, any information indicating what to segment<br>in an image. The promptable segmentation task, then, is to<br>return a valid segmentation mask given any prompt. The re-<br>quirement of a \"valid\" mask simply means that even when<br>a prompt is ambiguous and could refer to multiple objects<br>(e.g., recall the shirt vs. person example, and see Fig. 3),<br>the output should be a reasonable mask for at least one of<br>those objects. This requirementis similar to expecting a lan-<br>guage model to output a coherent response to an ambiguous<br>prompt. We choose this task because it leads to a natural<br>pre-training algorithm and a general method for zero-shot<br>transfer to downstream segmentation tasks via prompting.</p>",
      "id": 29,
      "page": 4,
      "text": "Task. We start by translating the idea of a prompt from NLP\nto segmentation, where a prompt can be a set of foreground\n/ background points, a rough box or mask, free-form text,\nor, in general, any information indicating what to segment\nin an image. The promptable segmentation task, then, is to\nreturn a valid segmentation mask given any prompt. The re-\nquirement of a \"valid\" mask simply means that even when\na prompt is ambiguous and could refer to multiple objects\n(e.g., recall the shirt vs. person example, and see Fig. 3),\nthe output should be a reasonable mask for at least one of\nthose objects. This requirementis similar to expecting a lan-\nguage model to output a coherent response to an ambiguous\nprompt. We choose this task because it leads to a natural\npre-training algorithm and a general method for zero-shot\ntransfer to downstream segmentation tasks via prompting."
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 1434
        },
        {
          "x": 1199,
          "y": 1434
        },
        {
          "x": 1199,
          "y": 2131
        },
        {
          "x": 200,
          "y": 2131
        }
      ],
      "category": "paragraph",
      "html": "<p id='30' style='font-size:14px'>Pre-training. The promptable segmentation task suggests a<br>natural pre-training algorithm that simulates a sequence of<br>prompts (e.g., points, boxes, masks) for each training sam-<br>ple and compares the model's mask predictions against the<br>ground truth. We adapt this method from interactive seg-<br>mentation [109, 70], although unlike interactive segmenta-<br>tion whose aim is to eventually predict a valid mask after<br>enough user input, our aim is to always predict a valid mask<br>for any prompt even when the prompt is ambiguous. This<br>ensures that a pre-trained model is effective in use cases that<br>involve ambiguity, including automatic annotation as re-<br>quired by our data engine §4. We note that performing well<br>at this task is challenging and requires specialized modeling<br>and training loss choices, which we discuss in §3.</p>",
      "id": 30,
      "page": 4,
      "text": "Pre-training. The promptable segmentation task suggests a\nnatural pre-training algorithm that simulates a sequence of\nprompts (e.g., points, boxes, masks) for each training sam-\nple and compares the model's mask predictions against the\nground truth. We adapt this method from interactive seg-\nmentation [109, 70], although unlike interactive segmenta-\ntion whose aim is to eventually predict a valid mask after\nenough user input, our aim is to always predict a valid mask\nfor any prompt even when the prompt is ambiguous. This\nensures that a pre-trained model is effective in use cases that\ninvolve ambiguity, including automatic annotation as re-\nquired by our data engine §4. We note that performing well\nat this task is challenging and requires specialized modeling\nand training loss choices, which we discuss in §3."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 2156
        },
        {
          "x": 1199,
          "y": 2156
        },
        {
          "x": 1199,
          "y": 2653
        },
        {
          "x": 202,
          "y": 2653
        }
      ],
      "category": "paragraph",
      "html": "<p id='31' style='font-size:14px'>Zero-shot transfer. Intuitively, our pre-training task en-<br>dows the model with the ability to respond appropriately to<br>any prompt at inference time, and thus downstream tasks<br>can be solved by engineering appropriate prompts. For ex-<br>ample, if one has a bounding box detector for cats, cat in-<br>stance segmentation can be solved by providing the detec-<br>tor's box output as a prompt to our model. In general, a wide<br>array of practical segmentation tasks can be cast as prompt-<br>ing. In addition to automatic dataset labeling, we explore<br>five diverse example tasks in our experiments in §7.</p>",
      "id": 31,
      "page": 4,
      "text": "Zero-shot transfer. Intuitively, our pre-training task en-\ndows the model with the ability to respond appropriately to\nany prompt at inference time, and thus downstream tasks\ncan be solved by engineering appropriate prompts. For ex-\nample, if one has a bounding box detector for cats, cat in-\nstance segmentation can be solved by providing the detec-\ntor's box output as a prompt to our model. In general, a wide\narray of practical segmentation tasks can be cast as prompt-\ning. In addition to automatic dataset labeling, we explore\nfive diverse example tasks in our experiments in §7."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 2677
        },
        {
          "x": 1199,
          "y": 2677
        },
        {
          "x": 1199,
          "y": 2978
        },
        {
          "x": 202,
          "y": 2978
        }
      ],
      "category": "paragraph",
      "html": "<p id='32' style='font-size:16px'>Related tasks. Segmentation is a broad field: there's in-<br>teractive segmentation [57, 109], edge detection [3], su-<br>per pixelization [85], object proposal generation [2], fore-<br>ground segmentation [94], semantic segmentation [90], in-<br>stance segmentation [66], panoptic segmentation [59], etc.<br>The goal of our promptable segmentation task is to produce</p>",
      "id": 32,
      "page": 4,
      "text": "Related tasks. Segmentation is a broad field: there's in-\nteractive segmentation [57, 109], edge detection [3], su-\nper pixelization [85], object proposal generation [2], fore-\nground segmentation [94], semantic segmentation [90], in-\nstance segmentation [66], panoptic segmentation [59], etc.\nThe goal of our promptable segmentation task is to produce"
    },
    {
      "bounding_box": [
        {
          "x": 1297,
          "y": 293
        },
        {
          "x": 2258,
          "y": 293
        },
        {
          "x": 2258,
          "y": 1359
        },
        {
          "x": 1297,
          "y": 1359
        }
      ],
      "category": "figure",
      "html": "<br><figure><img id='33' style='font-size:22px' alt=\"DURICH\nZ\nOURICH\nDURICH\nZ\" data-coord=\"top-left:(1297,293); bottom-right:(2258,1359)\" /></figure>",
      "id": 33,
      "page": 4,
      "text": "DURICH\nZ\nOURICH\nDURICH\nZ"
    },
    {
      "bounding_box": [
        {
          "x": 1283,
          "y": 1384
        },
        {
          "x": 2272,
          "y": 1384
        },
        {
          "x": 2272,
          "y": 1484
        },
        {
          "x": 1283,
          "y": 1484
        }
      ],
      "category": "caption",
      "html": "<caption id='34' style='font-size:18px'>Figure 3: Each column shows 3 valid masks generated by<br>SAM from a single ambiguous point prompt (green circle).</caption>",
      "id": 34,
      "page": 4,
      "text": "Figure 3: Each column shows 3 valid masks generated by\nSAM from a single ambiguous point prompt (green circle)."
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 1565
        },
        {
          "x": 2277,
          "y": 1565
        },
        {
          "x": 2277,
          "y": 2211
        },
        {
          "x": 1278,
          "y": 2211
        }
      ],
      "category": "paragraph",
      "html": "<p id='35' style='font-size:14px'>a broadly capable model that can adapt to many (though<br>not all) existing and new segmentation tasks via prompt<br>engineering. This capability is a form of task generaliza-<br>tion [26]. Note that this is different than previous work on<br>multi-task segmentation systems. In a multi-task system, a<br>single model performs a fixed set of tasks, e.g., joint seman-<br>tic, instance, and panoptic segmentation [114, 19, 54], but<br>the training and test tasks are the same. An important dis-<br>tinction in our work is that a model trained for promptable<br>segmentation can perform a new, different task at inference<br>time by acting as a component in a larger system, e.g., to<br>perform instance segmentation, a promptable segmentation<br>model is combined with an existing object detector.</p>",
      "id": 35,
      "page": 4,
      "text": "a broadly capable model that can adapt to many (though\nnot all) existing and new segmentation tasks via prompt\nengineering. This capability is a form of task generaliza-\ntion [26]. Note that this is different than previous work on\nmulti-task segmentation systems. In a multi-task system, a\nsingle model performs a fixed set of tasks, e.g., joint seman-\ntic, instance, and panoptic segmentation [114, 19, 54], but\nthe training and test tasks are the same. An important dis-\ntinction in our work is that a model trained for promptable\nsegmentation can perform a new, different task at inference\ntime by acting as a component in a larger system, e.g., to\nperform instance segmentation, a promptable segmentation\nmodel is combined with an existing object detector."
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 2229
        },
        {
          "x": 2277,
          "y": 2229
        },
        {
          "x": 2277,
          "y": 2978
        },
        {
          "x": 1278,
          "y": 2978
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='36' style='font-size:14px'>Discussion. Prompting and composition are powerful tools<br>that enable a single model to be used in extensible ways, po-<br>tentially to accomplish tasks unknown at the time of model<br>design. This approach is analogous to how other founda-<br>tion models are used, e.g., how CLIP [82] is the text-image<br>alignment component of the DALL·E [83] image generation<br>system. We anticipate that composable system design, pow-<br>ered by techniques such as prompt engineering, will enable<br>a wider variety of applications than systems trained specif-<br>ically for a fixed set of tasks. It's also interesting to com-<br>pare promptable and interactive segmentation through the<br>lens of composition: while interactive segmentation mod-<br>els are designed with human users in mind, a model trained<br>for promptable segmentation can also be composed into a<br>larger algorithmic system as we will demonstrate.</p>",
      "id": 36,
      "page": 4,
      "text": "Discussion. Prompting and composition are powerful tools\nthat enable a single model to be used in extensible ways, po-\ntentially to accomplish tasks unknown at the time of model\ndesign. This approach is analogous to how other founda-\ntion models are used, e.g., how CLIP [82] is the text-image\nalignment component of the DALL·E [83] image generation\nsystem. We anticipate that composable system design, pow-\nered by techniques such as prompt engineering, will enable\na wider variety of applications than systems trained specif-\nically for a fixed set of tasks. It's also interesting to com-\npare promptable and interactive segmentation through the\nlens of composition: while interactive segmentation mod-\nels are designed with human users in mind, a model trained\nfor promptable segmentation can also be composed into a\nlarger algorithmic system as we will demonstrate."
    },
    {
      "bounding_box": [
        {
          "x": 1225,
          "y": 3056
        },
        {
          "x": 1251,
          "y": 3056
        },
        {
          "x": 1251,
          "y": 3088
        },
        {
          "x": 1225,
          "y": 3088
        }
      ],
      "category": "footer",
      "html": "<footer id='37' style='font-size:14px'>4</footer>",
      "id": 37,
      "page": 4,
      "text": "4"
    },
    {
      "bounding_box": [
        {
          "x": 211,
          "y": 279
        },
        {
          "x": 2266,
          "y": 279
        },
        {
          "x": 2266,
          "y": 709
        },
        {
          "x": 211,
          "y": 709
        }
      ],
      "category": "figure",
      "html": "<figure><img id='38' style='font-size:14px' alt=\"score\n,\nmask decoder\nimage\nencoder , score\nconv prompt encoder\nimage , score\nimage mask points box text\nembedding\nvalid masks\" data-coord=\"top-left:(211,279); bottom-right:(2266,709)\" /></figure>",
      "id": 38,
      "page": 5,
      "text": "score\n,\nmask decoder\nimage\nencoder , score\nconv prompt encoder\nimage , score\nimage mask points box text\nembedding\nvalid masks"
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 730
        },
        {
          "x": 2277,
          "y": 730
        },
        {
          "x": 2277,
          "y": 884
        },
        {
          "x": 200,
          "y": 884
        }
      ],
      "category": "caption",
      "html": "<br><caption id='39' style='font-size:18px'>Figure 4: Segment Anything Model (SAM) overview. A heavyweight image encoder outputs an image embedding that can<br>then be efficiently queried by a variety of input prompts to produce object masks at amortized real-time speed. For ambiguous<br>prompts corresponding to more than one object, SAM can output multiple valid masks and associated confidence scores.</caption>",
      "id": 39,
      "page": 5,
      "text": "Figure 4: Segment Anything Model (SAM) overview. A heavyweight image encoder outputs an image embedding that can\nthen be efficiently queried by a variety of input prompts to produce object masks at amortized real-time speed. For ambiguous\nprompts corresponding to more than one object, SAM can output multiple valid masks and associated confidence scores."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 953
        },
        {
          "x": 805,
          "y": 953
        },
        {
          "x": 805,
          "y": 1007
        },
        {
          "x": 201,
          "y": 1007
        }
      ],
      "category": "paragraph",
      "html": "<p id='40' style='font-size:20px'>3. Segment Anything Model</p>",
      "id": 40,
      "page": 5,
      "text": "3. Segment Anything Model"
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 1045
        },
        {
          "x": 1201,
          "y": 1045
        },
        {
          "x": 1201,
          "y": 1394
        },
        {
          "x": 201,
          "y": 1394
        }
      ],
      "category": "paragraph",
      "html": "<p id='41' style='font-size:18px'>We next describe the Segment Anything Model (SAM)<br>for promptable segmentation. SAM has three components,<br>illustrated in Fig. 4: an image encoder, a flexible prompt<br>encoder, and a fast mask decoder. We build on Transformer<br>vision models [14, 33, 20, 62] with specific tradeoffs for<br>(amortized) real-time performance. We describe these com-<br>ponents at a high-level here, with details in §A.</p>",
      "id": 41,
      "page": 5,
      "text": "We next describe the Segment Anything Model (SAM)\nfor promptable segmentation. SAM has three components,\nillustrated in Fig. 4: an image encoder, a flexible prompt\nencoder, and a fast mask decoder. We build on Transformer\nvision models [14, 33, 20, 62] with specific tradeoffs for\n(amortized) real-time performance. We describe these com-\nponents at a high-level here, with details in §A."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 1416
        },
        {
          "x": 1199,
          "y": 1416
        },
        {
          "x": 1199,
          "y": 1665
        },
        {
          "x": 202,
          "y": 1665
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='42' style='font-size:18px'>Image encoder. Motivated by scalability and powerful pre-<br>training methods, we use an MAE [47] pre-trained Vision<br>Transformer (ViT) [33] minimally adapted to process high<br>resolution inputs [62]. The image encoder runs once per<br>image and can be applied prior to prompting the model.</p>",
      "id": 42,
      "page": 5,
      "text": "Image encoder. Motivated by scalability and powerful pre-\ntraining methods, we use an MAE [47] pre-trained Vision\nTransformer (ViT) [33] minimally adapted to process high\nresolution inputs [62]. The image encoder runs once per\nimage and can be applied prior to prompting the model."
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 1684
        },
        {
          "x": 1199,
          "y": 1684
        },
        {
          "x": 1199,
          "y": 2035
        },
        {
          "x": 200,
          "y": 2035
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='43' style='font-size:18px'>Prompt encoder. We consider two sets of prompts: sparse<br>(points, boxes, text) and dense (masks). We represent<br>points and boxes by positional encodings [95] summed with<br>learned embeddings for each prompt type and free-form text<br>with an off-the-shelf text encoder from CLIP [82]. Dense<br>prompts (i.e., masks) are embedded using convolutions and<br>summed element-wise with the image embedding.</p>",
      "id": 43,
      "page": 5,
      "text": "Prompt encoder. We consider two sets of prompts: sparse\n(points, boxes, text) and dense (masks). We represent\npoints and boxes by positional encodings [95] summed with\nlearned embeddings for each prompt type and free-form text\nwith an off-the-shelf text encoder from CLIP [82]. Dense\nprompts (i.e., masks) are embedded using convolutions and\nsummed element-wise with the image embedding."
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 2056
        },
        {
          "x": 1200,
          "y": 2056
        },
        {
          "x": 1200,
          "y": 2607
        },
        {
          "x": 200,
          "y": 2607
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='44' style='font-size:18px'>Mask decoder. The mask decoder efficiently maps the im-<br>age embedding, prompt embeddings, and an output token<br>to a mask. This design, inspired by [14, 20], employs a<br>modification of a Transformer decoder block [103] followed<br>by a dynamic mask prediction head. Our modified decoder<br>block uses prompt self-attention and cross-attention in two<br>directions (prompt-to-image embedding and vice-versa) to<br>update all embeddings. After running two blocks, we up-<br>sample the image embedding and an MLP maps the output<br>token to a dynamic linear classifier, which then computes<br>the mask foreground probability at each image location.</p>",
      "id": 44,
      "page": 5,
      "text": "Mask decoder. The mask decoder efficiently maps the im-\nage embedding, prompt embeddings, and an output token\nto a mask. This design, inspired by [14, 20], employs a\nmodification of a Transformer decoder block [103] followed\nby a dynamic mask prediction head. Our modified decoder\nblock uses prompt self-attention and cross-attention in two\ndirections (prompt-to-image embedding and vice-versa) to\nupdate all embeddings. After running two blocks, we up-\nsample the image embedding and an MLP maps the output\ntoken to a dynamic linear classifier, which then computes\nthe mask foreground probability at each image location."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 2626
        },
        {
          "x": 1199,
          "y": 2626
        },
        {
          "x": 1199,
          "y": 2978
        },
        {
          "x": 201,
          "y": 2978
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='45' style='font-size:18px'>Resolving ambiguity. With one output, the model will av-<br>erage multiple valid masks if given an ambiguous prompt.<br>To address this, we modify the model to predict multiple<br>output masks for a single prompt (see Fig. 3). We found<br>3 mask outputs is sufficient to address most common cases<br>(nested masks are often at most three deep: whole, part, and<br>subpart). During training, we backprop only the minimum</p>",
      "id": 45,
      "page": 5,
      "text": "Resolving ambiguity. With one output, the model will av-\nerage multiple valid masks if given an ambiguous prompt.\nTo address this, we modify the model to predict multiple\noutput masks for a single prompt (see Fig. 3). We found\n3 mask outputs is sufficient to address most common cases\n(nested masks are often at most three deep: whole, part, and\nsubpart). During training, we backprop only the minimum"
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 959
        },
        {
          "x": 2276,
          "y": 959
        },
        {
          "x": 2276,
          "y": 1057
        },
        {
          "x": 1279,
          "y": 1057
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='46' style='font-size:16px'>loss [15, 45, 64] over masks. To rank masks, the model pre-<br>dicts a confidence score (i.e., estimated IoU) for each mask.</p>",
      "id": 46,
      "page": 5,
      "text": "loss [15, 45, 64] over masks. To rank masks, the model pre-\ndicts a confidence score (i.e., estimated IoU) for each mask."
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 1073
        },
        {
          "x": 2276,
          "y": 1073
        },
        {
          "x": 2276,
          "y": 1321
        },
        {
          "x": 1279,
          "y": 1321
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='47' style='font-size:18px'>Efficiency. The overall model design is largely motivated<br>by efficiency. Given a precomputed image embedding, the<br>prompt encoder and mask decoder run in a web browser, on<br>CPU, in ~50ms. This runtime performance enables seam-<br>less, real-time interactive prompting of our model.</p>",
      "id": 47,
      "page": 5,
      "text": "Efficiency. The overall model design is largely motivated\nby efficiency. Given a precomputed image embedding, the\nprompt encoder and mask decoder run in a web browser, on\nCPU, in ~50ms. This runtime performance enables seam-\nless, real-time interactive prompting of our model."
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 1338
        },
        {
          "x": 2278,
          "y": 1338
        },
        {
          "x": 2278,
          "y": 1688
        },
        {
          "x": 1279,
          "y": 1688
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='48' style='font-size:18px'>Losses and training. We supervise mask prediction with<br>the linear combination of focal loss [65] and dice loss [73]<br>used in [14]. We train for the promptable segmentation task<br>using a mixture of geometric prompts (for text prompts see<br>§7.5). Following [92, 37], we simulate an interactive setup<br>by randomly sampling prompts in 11 rounds per mask, al-<br>lowing SAM to integrate seamlessly into our data engine.</p>",
      "id": 48,
      "page": 5,
      "text": "Losses and training. We supervise mask prediction with\nthe linear combination of focal loss [65] and dice loss [73]\nused in [14]. We train for the promptable segmentation task\nusing a mixture of geometric prompts (for text prompts see\n§7.5). Following [92, 37], we simulate an interactive setup\nby randomly sampling prompts in 11 rounds per mask, al-\nlowing SAM to integrate seamlessly into our data engine."
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 1732
        },
        {
          "x": 2012,
          "y": 1732
        },
        {
          "x": 2012,
          "y": 1785
        },
        {
          "x": 1281,
          "y": 1785
        }
      ],
      "category": "paragraph",
      "html": "<p id='49' style='font-size:22px'>4. Segment Anything Data Engine</p>",
      "id": 49,
      "page": 5,
      "text": "4. Segment Anything Data Engine"
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 1818
        },
        {
          "x": 2277,
          "y": 1818
        },
        {
          "x": 2277,
          "y": 2211
        },
        {
          "x": 1280,
          "y": 2211
        }
      ],
      "category": "paragraph",
      "html": "<p id='50' style='font-size:14px'>As segmentation masks are not abundant on the inter-<br>net, we built a data engine to enable the collection of our<br>1.1B mask dataset, SA-1B. The data engine has three<br>stages: (1) a model-assisted manual annotation stage, (2) a<br>semi-automatic stage with a mix of automatically predicted<br>masks and model-assisted annotation, and (3) a fully auto-<br>matic stage in which our model generates masks without<br>annotator input. We go into details of each next.</p>",
      "id": 50,
      "page": 5,
      "text": "As segmentation masks are not abundant on the inter-\nnet, we built a data engine to enable the collection of our\n1.1B mask dataset, SA-1B. The data engine has three\nstages: (1) a model-assisted manual annotation stage, (2) a\nsemi-automatic stage with a mix of automatically predicted\nmasks and model-assisted annotation, and (3) a fully auto-\nmatic stage in which our model generates masks without\nannotator input. We go into details of each next."
    },
    {
      "bounding_box": [
        {
          "x": 1277,
          "y": 2229
        },
        {
          "x": 2276,
          "y": 2229
        },
        {
          "x": 2276,
          "y": 2978
        },
        {
          "x": 1277,
          "y": 2978
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='51' style='font-size:16px'>Assisted-manual stage. In the first stage, resembling clas-<br>sic interactive segmentation, a team of professional annota-<br>tors labeled masks by clicking foreground / background ob-<br>ject points using a browser-based interactive segmentation<br>tool powered by SAM. Masks could be refined using pixel-<br>precise \"brush\" and \"eraser\" tools. Our model-assisted an-<br>notation runs in real-time directly inside a browser (using<br>precomputed image embeddings) enabling a truly interac-<br>tive experience. We did not impose semantic constraints for<br>labeling objects, and annotators freely labeled both \"stuff\"<br>and \"things\" [1]. We suggested annotators label objects<br>they could name or describe, but did not collect these names<br>or descriptions. Annotators were asked to label objects in<br>order of prominence and were encouraged to proceed to the<br>next image once a mask took over 30 seconds to annotate.</p>",
      "id": 51,
      "page": 5,
      "text": "Assisted-manual stage. In the first stage, resembling clas-\nsic interactive segmentation, a team of professional annota-\ntors labeled masks by clicking foreground / background ob-\nject points using a browser-based interactive segmentation\ntool powered by SAM. Masks could be refined using pixel-\nprecise \"brush\" and \"eraser\" tools. Our model-assisted an-\nnotation runs in real-time directly inside a browser (using\nprecomputed image embeddings) enabling a truly interac-\ntive experience. We did not impose semantic constraints for\nlabeling objects, and annotators freely labeled both \"stuff\"\nand \"things\" [1]. We suggested annotators label objects\nthey could name or describe, but did not collect these names\nor descriptions. Annotators were asked to label objects in\norder of prominence and were encouraged to proceed to the\nnext image once a mask took over 30 seconds to annotate."
    },
    {
      "bounding_box": [
        {
          "x": 1226,
          "y": 3054
        },
        {
          "x": 1251,
          "y": 3054
        },
        {
          "x": 1251,
          "y": 3092
        },
        {
          "x": 1226,
          "y": 3092
        }
      ],
      "category": "footer",
      "html": "<footer id='52' style='font-size:16px'>5</footer>",
      "id": 52,
      "page": 5,
      "text": "5"
    },
    {
      "bounding_box": [
        {
          "x": 199,
          "y": 307
        },
        {
          "x": 1199,
          "y": 307
        },
        {
          "x": 1199,
          "y": 952
        },
        {
          "x": 199,
          "y": 952
        }
      ],
      "category": "paragraph",
      "html": "<p id='53' style='font-size:18px'>At the start of this stage, SAM was trained using com-<br>mon public segmentation datasets. After sufficient data an-<br>notation, SAM was retrained using only newly annotated<br>masks. As more masks were collected, the image encoder<br>was scaled from ViT-B to ViT-H and other architectural de-<br>tails evolved; in total we retrained our model 6 times. Av-<br>erage annotation time per mask decreased from 34 to 14<br>seconds as the model improved. We note that 14 seconds<br>is 6.5x faster than mask annotation for COCO [66] and<br>only 2x slower than bounding-box labeling with extreme<br>points [76, 71]. As SAM improved, the average number of<br>masks per image increased from 20 to 44 masks. Overall,<br>we collected 4.3M masks from 120k images in this stage.</p>",
      "id": 53,
      "page": 6,
      "text": "At the start of this stage, SAM was trained using com-\nmon public segmentation datasets. After sufficient data an-\nnotation, SAM was retrained using only newly annotated\nmasks. As more masks were collected, the image encoder\nwas scaled from ViT-B to ViT-H and other architectural de-\ntails evolved; in total we retrained our model 6 times. Av-\nerage annotation time per mask decreased from 34 to 14\nseconds as the model improved. We note that 14 seconds\nis 6.5x faster than mask annotation for COCO [66] and\nonly 2x slower than bounding-box labeling with extreme\npoints [76, 71]. As SAM improved, the average number of\nmasks per image increased from 20 to 44 masks. Overall,\nwe collected 4.3M masks from 120k images in this stage."
    },
    {
      "bounding_box": [
        {
          "x": 199,
          "y": 968
        },
        {
          "x": 1198,
          "y": 968
        },
        {
          "x": 1198,
          "y": 1765
        },
        {
          "x": 199,
          "y": 1765
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='54' style='font-size:18px'>Semi-automatic stage. In this stage, we aimed to increase<br>the diversity of masks in order to improve our model's<br>ability to segment anything. To focus annotators on less<br>prominent objects, we first automatically detected confident<br>masks. Then we presented annotators with images prefilled<br>with these masks and asked them to annotate any additional<br>unannotated objects. To detect confident masks, we trained<br>a bounding box detector [84] on all first stage masks using a<br>generic \"object\" category. During this stage we collected an<br>additional 5.9M masks in 180k images (for a total of 10.2M<br>masks). As in the first stage, we periodically retrained our<br>model on newly collected data (5 times). Average annota-<br>tion time per mask went back up to 34 seconds (excluding<br>the automatic masks) as these objects were more challeng-<br>ing to label. The average number of masks per image went<br>from 44 to 72 masks (including the automatic masks).</p>",
      "id": 54,
      "page": 6,
      "text": "Semi-automatic stage. In this stage, we aimed to increase\nthe diversity of masks in order to improve our model's\nability to segment anything. To focus annotators on less\nprominent objects, we first automatically detected confident\nmasks. Then we presented annotators with images prefilled\nwith these masks and asked them to annotate any additional\nunannotated objects. To detect confident masks, we trained\na bounding box detector [84] on all first stage masks using a\ngeneric \"object\" category. During this stage we collected an\nadditional 5.9M masks in 180k images (for a total of 10.2M\nmasks). As in the first stage, we periodically retrained our\nmodel on newly collected data (5 times). Average annota-\ntion time per mask went back up to 34 seconds (excluding\nthe automatic masks) as these objects were more challeng-\ning to label. The average number of masks per image went\nfrom 44 to 72 masks (including the automatic masks)."
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 1777
        },
        {
          "x": 1199,
          "y": 1777
        },
        {
          "x": 1199,
          "y": 2977
        },
        {
          "x": 200,
          "y": 2977
        }
      ],
      "category": "paragraph",
      "html": "<p id='55' style='font-size:20px'>Fully automatic stage. In the final stage, annotation was<br>fully automatic. This was feasible due to two major en-<br>hancements to our model. First, at the start of this stage, we<br>had collected enough masks to greatly improve the model,<br>including the diverse masks from the previous stage. Sec-<br>ond, by this stage we had developed the ambiguity-aware<br>model, which allowed us to predict valid masks even in am-<br>biguous cases. Specifically, we prompted the model with a<br>32x32 regular grid of points and for each point predicted<br>a set of masks that may correspond to valid objects. With<br>the ambiguity-aware model, if a point lies on a part or sub-<br>part, our model will return the subpart, part, and whole ob-<br>ject. The IoU prediction module of our model is used to se-<br>lect confident masks; moreover, we identified and selected<br>only stable masks (we consider a mask stable if threshold-<br>ing the probability map at 0.5 - 8 and 0.5 + 8 results in<br>similar masks). Finally, after selecting the confident and<br>stable masks, we applied non-maximal suppression (NMS)<br>to filter duplicates. To further improve the quality of smaller<br>masks, we also processed multiple overlapping zoomed-in<br>image crops. For further details of this stage, see §B. We<br>applied fully automatic mask generation to all 11M images<br>in our dataset, producing a total of 1. 1B high-quality masks.<br>We describe and analyze the resulting dataset, SA-1B, next.</p>",
      "id": 55,
      "page": 6,
      "text": "Fully automatic stage. In the final stage, annotation was\nfully automatic. This was feasible due to two major en-\nhancements to our model. First, at the start of this stage, we\nhad collected enough masks to greatly improve the model,\nincluding the diverse masks from the previous stage. Sec-\nond, by this stage we had developed the ambiguity-aware\nmodel, which allowed us to predict valid masks even in am-\nbiguous cases. Specifically, we prompted the model with a\n32x32 regular grid of points and for each point predicted\na set of masks that may correspond to valid objects. With\nthe ambiguity-aware model, if a point lies on a part or sub-\npart, our model will return the subpart, part, and whole ob-\nject. The IoU prediction module of our model is used to se-\nlect confident masks; moreover, we identified and selected\nonly stable masks (we consider a mask stable if threshold-\ning the probability map at 0.5 - 8 and 0.5 + 8 results in\nsimilar masks). Finally, after selecting the confident and\nstable masks, we applied non-maximal suppression (NMS)\nto filter duplicates. To further improve the quality of smaller\nmasks, we also processed multiple overlapping zoomed-in\nimage crops. For further details of this stage, see §B. We\napplied fully automatic mask generation to all 11M images\nin our dataset, producing a total of 1. 1B high-quality masks.\nWe describe and analyze the resulting dataset, SA-1B, next."
    },
    {
      "bounding_box": [
        {
          "x": 1296,
          "y": 295
        },
        {
          "x": 2257,
          "y": 295
        },
        {
          "x": 2257,
          "y": 504
        },
        {
          "x": 1296,
          "y": 504
        }
      ],
      "category": "figure",
      "html": "<br><figure><img id='56' style='font-size:14px' alt=\"SA-1B LVIS v1 COCO ADE20K Open Images\" data-coord=\"top-left:(1296,295); bottom-right:(2257,504)\" /></figure>",
      "id": 56,
      "page": 6,
      "text": "SA-1B LVIS v1 COCO ADE20K Open Images"
    },
    {
      "bounding_box": [
        {
          "x": 1284,
          "y": 531
        },
        {
          "x": 2267,
          "y": 531
        },
        {
          "x": 2267,
          "y": 577
        },
        {
          "x": 1284,
          "y": 577
        }
      ],
      "category": "caption",
      "html": "<caption id='57' style='font-size:20px'>Figure 5: Image-size normalized mask center distributions.</caption>",
      "id": 57,
      "page": 6,
      "text": "Figure 5: Image-size normalized mask center distributions."
    },
    {
      "bounding_box": [
        {
          "x": 1283,
          "y": 646
        },
        {
          "x": 1908,
          "y": 646
        },
        {
          "x": 1908,
          "y": 698
        },
        {
          "x": 1283,
          "y": 698
        }
      ],
      "category": "paragraph",
      "html": "<p id='58' style='font-size:22px'>5. Segment Anything Dataset</p>",
      "id": 58,
      "page": 6,
      "text": "5. Segment Anything Dataset"
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 733
        },
        {
          "x": 2278,
          "y": 733
        },
        {
          "x": 2278,
          "y": 1179
        },
        {
          "x": 1279,
          "y": 1179
        }
      ],
      "category": "paragraph",
      "html": "<p id='59' style='font-size:20px'>Our dataset, SA-1B, consists of 11M diverse, high-<br>resolution, licensed, and privacy protecting images and<br>1.1B high-quality segmentation masks collected with our<br>data engine. We compare SA-1B with existing datasets<br>and analyze mask quality and properties. We are releasing<br>SA-1B to aid future development of foundation models for<br>computer vision. We note that SA-1B will be released un-<br>der a favorable license agreement for certain research uses<br>and with protections for researchers.</p>",
      "id": 59,
      "page": 6,
      "text": "Our dataset, SA-1B, consists of 11M diverse, high-\nresolution, licensed, and privacy protecting images and\n1.1B high-quality segmentation masks collected with our\ndata engine. We compare SA-1B with existing datasets\nand analyze mask quality and properties. We are releasing\nSA-1B to aid future development of foundation models for\ncomputer vision. We note that SA-1B will be released un-\nder a favorable license agreement for certain research uses\nand with protections for researchers."
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 1198
        },
        {
          "x": 2276,
          "y": 1198
        },
        {
          "x": 2276,
          "y": 1745
        },
        {
          "x": 1278,
          "y": 1745
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='60' style='font-size:20px'>Images. We licensed a new set of 11M images from a<br>provider that works directly with photographers. These im-<br>ages are high resolution (3300 x4950 pixels on average),<br>and the resulting data size can present accessibility and stor-<br>age challenges. Therefore, we are releasing downsampled<br>images with their shortest side set to 1500 pixels. Even af-<br>ter downsampling, our images are significantly higher reso-<br>lution than many existing vision datasets (e.g., COCO [66]<br>images are ~480x 640 pixels). Note that most models today<br>operate on much lower resolution inputs. Faces and vehicle<br>license plates have been blurred in the released images.</p>",
      "id": 60,
      "page": 6,
      "text": "Images. We licensed a new set of 11M images from a\nprovider that works directly with photographers. These im-\nages are high resolution (3300 x4950 pixels on average),\nand the resulting data size can present accessibility and stor-\nage challenges. Therefore, we are releasing downsampled\nimages with their shortest side set to 1500 pixels. Even af-\nter downsampling, our images are significantly higher reso-\nlution than many existing vision datasets (e.g., COCO [66]\nimages are ~480x 640 pixels). Note that most models today\noperate on much lower resolution inputs. Faces and vehicle\nlicense plates have been blurred in the released images."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 1761
        },
        {
          "x": 2277,
          "y": 1761
        },
        {
          "x": 2277,
          "y": 2260
        },
        {
          "x": 1280,
          "y": 2260
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='61' style='font-size:20px'>Masks. Our data engine produced 1.1B masks, 99.1% of<br>which were generated fully automatically. Therefore, the<br>quality of the automatic masks is centrally important. We<br>compare them directly to professional annotations and look<br>at how various mask properties compare to prominent seg-<br>mentation datasets. Our main conclusion, as borne out in<br>the analysis below and the experiments in §7, is that our<br>automatic masks are high quality and effective for training<br>models. Motivated by these findings, SA-1B only includes<br>automatically generated masks.</p>",
      "id": 61,
      "page": 6,
      "text": "Masks. Our data engine produced 1.1B masks, 99.1% of\nwhich were generated fully automatically. Therefore, the\nquality of the automatic masks is centrally important. We\ncompare them directly to professional annotations and look\nat how various mask properties compare to prominent seg-\nmentation datasets. Our main conclusion, as borne out in\nthe analysis below and the experiments in §7, is that our\nautomatic masks are high quality and effective for training\nmodels. Motivated by these findings, SA-1B only includes\nautomatically generated masks."
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 2278
        },
        {
          "x": 2276,
          "y": 2278
        },
        {
          "x": 2276,
          "y": 2978
        },
        {
          "x": 1278,
          "y": 2978
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='62' style='font-size:20px'>Mask quality. To estimate mask quality, we randomly sam-<br>pled 500 images (~50k masks) and asked our professional<br>annotators to improve the quality of all masks in these im-<br>ages. Annotators did so using our model and pixel-precise<br>\"brush\" and \"eraser\" editing tools. This procedure resulted<br>in pairs of automatically predicted and professionally cor-<br>rected masks. We computed IoU between each pair and<br>found that 94% of pairs have greater than 90% IoU (and<br>97% of pairs have greater than 75% IoU). For comparison,<br>prior work estimates inter-annotator consistency at 85-91%<br>IoU [44, 60]. Our experiments in §7 confirm by human rat-<br>ings that mask quality is high relative to a variety of datasets<br>and that training our model on automatic masks is nearly as<br>good as using all masks produced by the data engine.</p>",
      "id": 62,
      "page": 6,
      "text": "Mask quality. To estimate mask quality, we randomly sam-\npled 500 images (~50k masks) and asked our professional\nannotators to improve the quality of all masks in these im-\nages. Annotators did so using our model and pixel-precise\n\"brush\" and \"eraser\" editing tools. This procedure resulted\nin pairs of automatically predicted and professionally cor-\nrected masks. We computed IoU between each pair and\nfound that 94% of pairs have greater than 90% IoU (and\n97% of pairs have greater than 75% IoU). For comparison,\nprior work estimates inter-annotator consistency at 85-91%\nIoU [44, 60]. Our experiments in §7 confirm by human rat-\nings that mask quality is high relative to a variety of datasets\nand that training our model on automatic masks is nearly as\ngood as using all masks produced by the data engine."
    },
    {
      "bounding_box": [
        {
          "x": 1226,
          "y": 3057
        },
        {
          "x": 1253,
          "y": 3057
        },
        {
          "x": 1253,
          "y": 3091
        },
        {
          "x": 1226,
          "y": 3091
        }
      ],
      "category": "footer",
      "html": "<footer id='63' style='font-size:16px'>6</footer>",
      "id": 63,
      "page": 6,
      "text": "6"
    },
    {
      "bounding_box": [
        {
          "x": 215,
          "y": 294
        },
        {
          "x": 2244,
          "y": 294
        },
        {
          "x": 2244,
          "y": 674
        },
        {
          "x": 215,
          "y": 674
        }
      ],
      "category": "figure",
      "html": "<figure><img id='64' style='font-size:14px' alt=\"SA-1B LVIS v1 COCO ADE20K Open Images\n11M images 0.120M images 0.123M images 0.028M images 1M images\n1129M (1.1B) masks 1.5M masks 0.9M masks 0.7M masks 2.7M masks\nmasks\nimages\n15\n80\n10°\nof 10\nof 40 of masks\nPercent 10-2\n0 0\n<10 11-50 51-100 101-200 >200 0.00 0.25 0.50 0.75 0.0 0.2 0.4 0.6 0.8\nNumber of masks per image Percent\nRelative segmentation mask size Percent 5\nConcavity\" data-coord=\"top-left:(215,294); bottom-right:(2244,674)\" /></figure>",
      "id": 64,
      "page": 7,
      "text": "SA-1B LVIS v1 COCO ADE20K Open Images\n11M images 0.120M images 0.123M images 0.028M images 1M images\n1129M (1.1B) masks 1.5M masks 0.9M masks 0.7M masks 2.7M masks\nmasks\nimages\n15\n80\n10°\nof 10\nof 40 of masks\nPercent 10-2\n0 0\n<10 11-50 51-100 101-200 >200 0.00 0.25 0.50 0.75 0.0 0.2 0.4 0.6 0.8\nNumber of masks per image Percent\nRelative segmentation mask size Percent 5\nConcavity"
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 686
        },
        {
          "x": 2272,
          "y": 686
        },
        {
          "x": 2272,
          "y": 782
        },
        {
          "x": 202,
          "y": 782
        }
      ],
      "category": "caption",
      "html": "<br><caption id='65' style='font-size:20px'>Figure 6: Dataset mask properties. The legend references the number of images and masks in each dataset. Note, that SA-1B<br>has 11 x more images and 400x more masks than the largest existing segmentation dataset Open Images [60].</caption>",
      "id": 65,
      "page": 7,
      "text": "Figure 6: Dataset mask properties. The legend references the number of images and masks in each dataset. Note, that SA-1B\nhas 11 x more images and 400x more masks than the largest existing segmentation dataset Open Images [60]."
    },
    {
      "bounding_box": [
        {
          "x": 207,
          "y": 803
        },
        {
          "x": 2260,
          "y": 803
        },
        {
          "x": 2260,
          "y": 1153
        },
        {
          "x": 207,
          "y": 1153
        }
      ],
      "category": "figure",
      "html": "<figure><img id='66' style='font-size:14px' alt=\"800k Asia & Oceania\nAfrica\nPer country country\n600k\nEurope\nimage count per\nNorth America\n400k\n≥ 100k Latin America & Caribbean\n< 100k images\n< 10k of 200k\nNumber\n0\n< 1k MAKE GBR DEU ������������������ BLR ROU KOR ARE  HKG CHE ISR SGP NOH BEL HRV BGR PHL KAZ MEX NOR �������\n50 most common countries (ISO codes)\" data-coord=\"top-left:(207,803); bottom-right:(2260,1153)\" /></figure>",
      "id": 66,
      "page": 7,
      "text": "800k Asia & Oceania\nAfrica\nPer country country\n600k\nEurope\nimage count per\nNorth America\n400k\n≥ 100k Latin America & Caribbean\n< 100k images\n< 10k of 200k\nNumber\n0\n< 1k MAKE GBR DEU ������������������ BLR ROU KOR ARE  HKG CHE ISR SGP NOH BEL HRV BGR PHL KAZ MEX NOR �������\n50 most common countries (ISO codes)"
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 1168
        },
        {
          "x": 2274,
          "y": 1168
        },
        {
          "x": 2274,
          "y": 1268
        },
        {
          "x": 203,
          "y": 1268
        }
      ],
      "category": "caption",
      "html": "<br><caption id='67' style='font-size:18px'>Figure 7: Estimated geographic distribution of SA-1B images. Most of the world's countries have more than 1000 images in<br>SA-1B, and the three countries with the most images are from different parts of the world.</caption>",
      "id": 67,
      "page": 7,
      "text": "Figure 7: Estimated geographic distribution of SA-1B images. Most of the world's countries have more than 1000 images in\nSA-1B, and the three countries with the most images are from different parts of the world."
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 1306
        },
        {
          "x": 1199,
          "y": 1306
        },
        {
          "x": 1199,
          "y": 2558
        },
        {
          "x": 200,
          "y": 2558
        }
      ],
      "category": "paragraph",
      "html": "<p id='68' style='font-size:20px'>Mask properties. In Fig. 5 we plot the spatial distribution<br>of object centers in SA-1B compared to the largest existing<br>segmentation datasets. Common photographer biases are<br>present in all datasets. We observe that SA-1B has greater<br>coverage of image corners compared to LVIS v1 [44] and<br>ADE20K [117], the two most similarly distributed datasets,<br>while COCO [66] and Open Images V5 [60] have a more<br>prominent center bias. In Fig. 6 (legend) we compare these<br>datasets by size. SA-1B has 11 x more images and 400x<br>more masks than the second largest, Open Images. On av-<br>erage, it has 36x more masks per image than Open Images.<br>The closest dataset in this respect, ADE20K, still has 3.5x<br>fewer masks per image. Fig. 6 (left) plots the masks-per-<br>image distribution. Next, we look at image-relative mask<br>size (square root of the mask area divided by image area)<br>in Fig. 6 (middle). As expected, since our dataset has more<br>masks per image, it also tends to include a greater percent-<br>age of small and medium relative-size masks. Finally, to<br>analyze shape complexity, we look at mask concavity (1<br>minus mask area divided by area of mask's convex hull) in<br>Fig. 6 (right). Since shape complexity is correlated with<br>mask size, we control for the datasets' mask size distribu-<br>tions by first performing stratified sampling from binned<br>mask sizes. We observe that the concavity distribution of<br>our masks is broadly similar to that of other datasets.</p>",
      "id": 68,
      "page": 7,
      "text": "Mask properties. In Fig. 5 we plot the spatial distribution\nof object centers in SA-1B compared to the largest existing\nsegmentation datasets. Common photographer biases are\npresent in all datasets. We observe that SA-1B has greater\ncoverage of image corners compared to LVIS v1 [44] and\nADE20K [117], the two most similarly distributed datasets,\nwhile COCO [66] and Open Images V5 [60] have a more\nprominent center bias. In Fig. 6 (legend) we compare these\ndatasets by size. SA-1B has 11 x more images and 400x\nmore masks than the second largest, Open Images. On av-\nerage, it has 36x more masks per image than Open Images.\nThe closest dataset in this respect, ADE20K, still has 3.5x\nfewer masks per image. Fig. 6 (left) plots the masks-per-\nimage distribution. Next, we look at image-relative mask\nsize (square root of the mask area divided by image area)\nin Fig. 6 (middle). As expected, since our dataset has more\nmasks per image, it also tends to include a greater percent-\nage of small and medium relative-size masks. Finally, to\nanalyze shape complexity, we look at mask concavity (1\nminus mask area divided by area of mask's convex hull) in\nFig. 6 (right). Since shape complexity is correlated with\nmask size, we control for the datasets' mask size distribu-\ntions by first performing stratified sampling from binned\nmask sizes. We observe that the concavity distribution of\nour masks is broadly similar to that of other datasets."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 2595
        },
        {
          "x": 951,
          "y": 2595
        },
        {
          "x": 951,
          "y": 2647
        },
        {
          "x": 202,
          "y": 2647
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='69' style='font-size:22px'>6. Segment Anything RAI Analysis</p>",
      "id": 69,
      "page": 7,
      "text": "6. Segment Anything RAI Analysis"
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 2679
        },
        {
          "x": 1199,
          "y": 2679
        },
        {
          "x": 1199,
          "y": 2975
        },
        {
          "x": 202,
          "y": 2975
        }
      ],
      "category": "paragraph",
      "html": "<p id='70' style='font-size:18px'>We next perform a Responsible AI (RAI) analysis of our<br>work by investigating potential fairness concerns and bi-<br>ases when using SA-1B and SAM. We focus on the geo-<br>graphic and income distribution of SA-1B and fairness of<br>SAM across protected attributes of people. We also provide<br>dataset, data annotation, and model cards in §F.</p>",
      "id": 70,
      "page": 7,
      "text": "We next perform a Responsible AI (RAI) analysis of our\nwork by investigating potential fairness concerns and bi-\nases when using SA-1B and SAM. We focus on the geo-\ngraphic and income distribution of SA-1B and fairness of\nSAM across protected attributes of people. We also provide\ndataset, data annotation, and model cards in §F."
    },
    {
      "bounding_box": [
        {
          "x": 1283,
          "y": 1298
        },
        {
          "x": 2278,
          "y": 1298
        },
        {
          "x": 2278,
          "y": 1798
        },
        {
          "x": 1283,
          "y": 1798
        }
      ],
      "category": "table",
      "html": "<br><table id='71' style='font-size:16px'><tr><td colspan=\"2\"></td><td colspan=\"2\">SA-1B</td><td colspan=\"3\">% images</td></tr><tr><td colspan=\"2\"># countries</td><td>#imgs</td><td>#masks</td><td>SA-1B</td><td>COCO</td><td>O.I.</td></tr><tr><td>Africa</td><td>54</td><td>300k</td><td>28M</td><td>2.8%</td><td>3.0%</td><td>1.7%</td></tr><tr><td>Asia & Oceania</td><td>70</td><td>3.9M</td><td>423M</td><td>36.2%</td><td>11.4%</td><td>14.3%</td></tr><tr><td>Europe</td><td>47</td><td>5.4M</td><td>540M</td><td>49.8%</td><td>34.2%</td><td>36.2%</td></tr><tr><td>Latin America & Carib.</td><td>42</td><td>380k</td><td>36M</td><td>3.5%</td><td>3.1%</td><td>5.0%</td></tr><tr><td>North America</td><td>4</td><td>830k</td><td>80M</td><td>7.7%</td><td>48.3%</td><td>42.8%</td></tr><tr><td>high income countries</td><td>81</td><td>5.8M</td><td>598M</td><td>54.0%</td><td>89.1%</td><td>87.5%</td></tr><tr><td>middle income countries</td><td>108</td><td>4.9M</td><td>499M</td><td>45.0%</td><td>10.5%</td><td>12.0%</td></tr><tr><td>low income countries</td><td>28</td><td>100k</td><td>9.4M</td><td>0.9%</td><td>0.4%</td><td>0.5%</td></tr></table>",
      "id": 71,
      "page": 7,
      "text": "SA-1B % images\n # countries #imgs #masks SA-1B COCO O.I.\n Africa 54 300k 28M 2.8% 3.0% 1.7%\n Asia & Oceania 70 3.9M 423M 36.2% 11.4% 14.3%\n Europe 47 5.4M 540M 49.8% 34.2% 36.2%\n Latin America & Carib. 42 380k 36M 3.5% 3.1% 5.0%\n North America 4 830k 80M 7.7% 48.3% 42.8%\n high income countries 81 5.8M 598M 54.0% 89.1% 87.5%\n middle income countries 108 4.9M 499M 45.0% 10.5% 12.0%\n low income countries 28 100k 9.4M 0.9% 0.4%"
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 1817
        },
        {
          "x": 2275,
          "y": 1817
        },
        {
          "x": 2275,
          "y": 2064
        },
        {
          "x": 1280,
          "y": 2064
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='72' style='font-size:18px'>Table 1: Comparison of geographic and income representa-<br>tion. SA-1B has higher representation in Europe and Asia &<br>Oceania as well as middle income countries. Images from<br>Africa, Latin America & Caribbean, as well as low income<br>countries, are underrepresented in all datasets.</p>",
      "id": 72,
      "page": 7,
      "text": "Table 1: Comparison of geographic and income representa-\ntion. SA-1B has higher representation in Europe and Asia &\nOceania as well as middle income countries. Images from\nAfrica, Latin America & Caribbean, as well as low income\ncountries, are underrepresented in all datasets."
    },
    {
      "bounding_box": [
        {
          "x": 1277,
          "y": 2176
        },
        {
          "x": 2278,
          "y": 2176
        },
        {
          "x": 2278,
          "y": 2979
        },
        {
          "x": 1277,
          "y": 2979
        }
      ],
      "category": "paragraph",
      "html": "<p id='73' style='font-size:18px'>Geographic and income representation. We infer the<br>country images were photographed in using standard meth-<br>ods (see §C). In Fig. 7 we visualize the per-country image<br>counts in SA-1B (left) and the 50 countries with the most<br>images (right). We note that the top-three countries are<br>from different parts of the world. Next, in Table 1 we com-<br>pare the geographic and income representation of SA-1B,<br>COCO [66], and Open Images [60]. SA-1B has a substan-<br>tially higher percentage of images in Europe and Asia &<br>Oceania as well as in middle income countries. All datasets<br>underrepresent Africa as well as low income countries. We<br>note that in SA-1B, all regions, including Africa, have at<br>least 28 million masks, 10x more than the total number of<br>masks of any previous dataset. Finally, we observe that the<br>average number of masks per image (not shown) is fairly<br>consistent across region and income (94-108 per image).</p>",
      "id": 73,
      "page": 7,
      "text": "Geographic and income representation. We infer the\ncountry images were photographed in using standard meth-\nods (see §C). In Fig. 7 we visualize the per-country image\ncounts in SA-1B (left) and the 50 countries with the most\nimages (right). We note that the top-three countries are\nfrom different parts of the world. Next, in Table 1 we com-\npare the geographic and income representation of SA-1B,\nCOCO [66], and Open Images [60]. SA-1B has a substan-\ntially higher percentage of images in Europe and Asia &\nOceania as well as in middle income countries. All datasets\nunderrepresent Africa as well as low income countries. We\nnote that in SA-1B, all regions, including Africa, have at\nleast 28 million masks, 10x more than the total number of\nmasks of any previous dataset. Finally, we observe that the\naverage number of masks per image (not shown) is fairly\nconsistent across region and income (94-108 per image)."
    },
    {
      "bounding_box": [
        {
          "x": 1224,
          "y": 3055
        },
        {
          "x": 1252,
          "y": 3055
        },
        {
          "x": 1252,
          "y": 3090
        },
        {
          "x": 1224,
          "y": 3090
        }
      ],
      "category": "footer",
      "html": "<footer id='74' style='font-size:16px'>7</footer>",
      "id": 74,
      "page": 7,
      "text": "7"
    },
    {
      "bounding_box": [
        {
          "x": 209,
          "y": 294
        },
        {
          "x": 1190,
          "y": 294
        },
        {
          "x": 1190,
          "y": 732
        },
        {
          "x": 209,
          "y": 732
        }
      ],
      "category": "table",
      "html": "<table id='75' style='font-size:14px'><tr><td></td><td colspan=\"2\">mIoU at</td><td colspan=\"3\">mIoU at</td></tr><tr><td></td><td>1 point</td><td>3 points</td><td colspan=\"2\">1 point</td><td>3 points</td></tr><tr><td colspan=\"3\">perceived gender presentation</td><td colspan=\"3\">perceived skin tone</td></tr><tr><td>feminine</td><td>54.4 ±1.7</td><td>90.4 ±0.6</td><td>1</td><td>52.9 ±2.2</td><td>91.0 ±0.9</td></tr><tr><td>masculine</td><td>55.7 ±1.7</td><td>90.1 ±0.6</td><td>2</td><td>51.5 ±1.4</td><td>91.1 ±0.5</td></tr><tr><td colspan=\"3\">perceived age group</td><td>3</td><td>52.2 ±1.9</td><td>91.4 ±0.7</td></tr><tr><td>older</td><td>62.9 ±6.7</td><td>92.6 ±1.3</td><td>4</td><td>51.5 ±2.7</td><td>91.7 ±1.0</td></tr><tr><td>middle</td><td>54.5 ±1.3</td><td>90.2 ±0.5</td><td>5</td><td>52.4 ±4.2</td><td>92.5 ±1.4</td></tr><tr><td>young</td><td>54.2 ±2.2</td><td>91.2 ±0.7</td><td>6</td><td>56.7 ±6.3</td><td>91.2 ±2.4</td></tr></table>",
      "id": 75,
      "page": 8,
      "text": "mIoU at mIoU at\n  1 point 3 points 1 point 3 points\n perceived gender presentation perceived skin tone\n feminine 54.4 ±1.7 90.4 ±0.6 1 52.9 ±2.2 91.0 ±0.9\n masculine 55.7 ±1.7 90.1 ±0.6 2 51.5 ±1.4 91.1 ±0.5\n perceived age group 3 52.2 ±1.9 91.4 ±0.7\n older 62.9 ±6.7 92.6 ±1.3 4 51.5 ±2.7 91.7 ±1.0\n middle 54.5 ±1.3 90.2 ±0.5 5 52.4 ±4.2 92.5 ±1.4\n young 54.2 ±2.2 91.2 ±0.7 6 56.7 ±6.3"
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 746
        },
        {
          "x": 1197,
          "y": 746
        },
        {
          "x": 1197,
          "y": 947
        },
        {
          "x": 201,
          "y": 947
        }
      ],
      "category": "caption",
      "html": "<br><caption id='76' style='font-size:16px'>Table 2: SAM's performance segmenting people across per-<br>ceived gender presentation, age group, and skin tone. 95%<br>confidence intervals are shown. Within each grouping, all<br>confidence intervals overlap except older vs. middle.</caption>",
      "id": 76,
      "page": 8,
      "text": "Table 2: SAM's performance segmenting people across per-\nceived gender presentation, age group, and skin tone. 95%\nconfidence intervals are shown. Within each grouping, all\nconfidence intervals overlap except older vs. middle."
    },
    {
      "bounding_box": [
        {
          "x": 199,
          "y": 998
        },
        {
          "x": 1198,
          "y": 998
        },
        {
          "x": 1198,
          "y": 2601
        },
        {
          "x": 199,
          "y": 2601
        }
      ],
      "category": "paragraph",
      "html": "<p id='77' style='font-size:14px'>Fairness in segmenting people. We investigate potential<br>fairness concerns across perceived gender presentation, per-<br>ceived age group, and perceived skin tone by measuring<br>the performance discrepancy of SAM between groups. We<br>use the More Inclusive Annotations for People (MIAP) [87]<br>dataset for gender presentation and age and a proprietary<br>dataset for skin tone (see §C). Our evaluation uses simu-<br>lated interactive segmentation with random sampling of 1<br>and 3 points (see §D). Table 2 (top left) shows results for<br>perceived gender presentation. We note that females have<br>been shown to be underrepresented in detection and seg-<br>mentation datasets [115], but observe that SAM performs<br>similarly across groups. We repeat the analysis for per-<br>ceived age in Table 2 (bottom left), noting that those who<br>are perceived to be younger and older have been shown to<br>be underrepresented in large-scale datasets [110]. SAM per-<br>forms best on those who are perceived older (although the<br>confidence interval is large). Finally, we repeat the anal-<br>ysis for perceived skin tone in Table 2 (right), noting that<br>those with lighter apparent skin tones have been shown to<br>be overrepresented and those with darker skin tones under-<br>represented in large-scale datasets [110]. As MIAP does<br>not contain perceived skin tone annotations, we use a pro-<br>prietary dataset that contains annotations for the perceived<br>Fitzpatrick skin type [36], which ranges from 1 (lightest<br>skin tone) to 6 (darkest skin tone). While the means vary<br>somewhat, we do not find a significant difference across<br>groups. We believe our findings stem from the nature of<br>the task, and acknowledge biases may arise when SAM is<br>used as a component in larger systems. Finally, in §C we<br>extend the analysis to segmenting clothing where we find<br>an indication of bias across perceived gender presentation.</p>",
      "id": 77,
      "page": 8,
      "text": "Fairness in segmenting people. We investigate potential\nfairness concerns across perceived gender presentation, per-\nceived age group, and perceived skin tone by measuring\nthe performance discrepancy of SAM between groups. We\nuse the More Inclusive Annotations for People (MIAP) [87]\ndataset for gender presentation and age and a proprietary\ndataset for skin tone (see §C). Our evaluation uses simu-\nlated interactive segmentation with random sampling of 1\nand 3 points (see §D). Table 2 (top left) shows results for\nperceived gender presentation. We note that females have\nbeen shown to be underrepresented in detection and seg-\nmentation datasets [115], but observe that SAM performs\nsimilarly across groups. We repeat the analysis for per-\nceived age in Table 2 (bottom left), noting that those who\nare perceived to be younger and older have been shown to\nbe underrepresented in large-scale datasets [110]. SAM per-\nforms best on those who are perceived older (although the\nconfidence interval is large). Finally, we repeat the anal-\nysis for perceived skin tone in Table 2 (right), noting that\nthose with lighter apparent skin tones have been shown to\nbe overrepresented and those with darker skin tones under-\nrepresented in large-scale datasets [110]. As MIAP does\nnot contain perceived skin tone annotations, we use a pro-\nprietary dataset that contains annotations for the perceived\nFitzpatrick skin type [36], which ranges from 1 (lightest\nskin tone) to 6 (darkest skin tone). While the means vary\nsomewhat, we do not find a significant difference across\ngroups. We believe our findings stem from the nature of\nthe task, and acknowledge biases may arise when SAM is\nused as a component in larger systems. Finally, in §C we\nextend the analysis to segmenting clothing where we find\nan indication of bias across perceived gender presentation."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 2642
        },
        {
          "x": 958,
          "y": 2642
        },
        {
          "x": 958,
          "y": 2696
        },
        {
          "x": 201,
          "y": 2696
        }
      ],
      "category": "paragraph",
      "html": "<p id='78' style='font-size:22px'>7. Zero-Shot Transfer Experiments</p>",
      "id": 78,
      "page": 8,
      "text": "7. Zero-Shot Transfer Experiments"
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 2729
        },
        {
          "x": 1199,
          "y": 2729
        },
        {
          "x": 1199,
          "y": 2975
        },
        {
          "x": 202,
          "y": 2975
        }
      ],
      "category": "paragraph",
      "html": "<p id='79' style='font-size:14px'>In this section, we present zero-shot transfer experiments<br>with SAM, the Segment Anything Model. We consider five<br>tasks, four of which differ significantly from the promptable<br>segmentation task used to train SAM. These experiments<br>evaluate SAM on datasets and tasks that were not seen dur-</p>",
      "id": 79,
      "page": 8,
      "text": "In this section, we present zero-shot transfer experiments\nwith SAM, the Segment Anything Model. We consider five\ntasks, four of which differ significantly from the promptable\nsegmentation task used to train SAM. These experiments\nevaluate SAM on datasets and tasks that were not seen dur-"
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 308
        },
        {
          "x": 2276,
          "y": 308
        },
        {
          "x": 2276,
          "y": 502
        },
        {
          "x": 1279,
          "y": 502
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='80' style='font-size:16px'>ing training (our usage of \"zero-shot transfer\" follows its<br>usage in CLIP [82]). The datasets may include novel image<br>distributions, such as underwater or ego-centric images (e.g.<br>Fig. 8) that, to our knowledge, do not appear in SA-1B.</p>",
      "id": 80,
      "page": 8,
      "text": "ing training (our usage of \"zero-shot transfer\" follows its\nusage in CLIP [82]). The datasets may include novel image\ndistributions, such as underwater or ego-centric images (e.g.\nFig. 8) that, to our knowledge, do not appear in SA-1B."
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 508
        },
        {
          "x": 2275,
          "y": 508
        },
        {
          "x": 2275,
          "y": 1250
        },
        {
          "x": 1278,
          "y": 1250
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='81' style='font-size:18px'>Our experiments begin by testing the core goal of<br>promptable segmentation: producing a valid mask from any<br>prompt. We emphasize the challenging scenario of a single<br>foreground point prompt, since it is more likely to be am-<br>biguous than other more specific prompts. Next, we present<br>a sequence of experiments that traverse low, mid, and high-<br>level image understanding and roughly parallel the histori-<br>cal development of the field. Specifically, we prompt SAM<br>to (1) perform edge detection, (2) segment everything, i.e.<br>object proposal generation, (3) segment detected objects,<br>i.e. instance segmentation, and (4), as a proof-of-concept, to<br>segment objects from free-form text. These four tasks dif-<br>fer significantly from the promptable segmentation task that<br>SAM was trained on and are implemented via prompt engi-<br>neering. Our experiments conclude with an ablation study.</p>",
      "id": 81,
      "page": 8,
      "text": "Our experiments begin by testing the core goal of\npromptable segmentation: producing a valid mask from any\nprompt. We emphasize the challenging scenario of a single\nforeground point prompt, since it is more likely to be am-\nbiguous than other more specific prompts. Next, we present\na sequence of experiments that traverse low, mid, and high-\nlevel image understanding and roughly parallel the histori-\ncal development of the field. Specifically, we prompt SAM\nto (1) perform edge detection, (2) segment everything, i.e.\nobject proposal generation, (3) segment detected objects,\ni.e. instance segmentation, and (4), as a proof-of-concept, to\nsegment objects from free-form text. These four tasks dif-\nfer significantly from the promptable segmentation task that\nSAM was trained on and are implemented via prompt engi-\nneering. Our experiments conclude with an ablation study."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 1266
        },
        {
          "x": 2277,
          "y": 1266
        },
        {
          "x": 2277,
          "y": 1565
        },
        {
          "x": 1280,
          "y": 1565
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='82' style='font-size:16px'>Implementation. Unless otherwise specified: (1) SAM<br>uses an MAE [47] pre-trained ViT-H [33] image encoder<br>and (2) SAM was trained on SA-1B, noting that this dataset<br>includes only automatically generated masks from the final<br>stage of our data engine. For all other model and training<br>details, such as hyperparameters, refer to §A.</p>",
      "id": 82,
      "page": 8,
      "text": "Implementation. Unless otherwise specified: (1) SAM\nuses an MAE [47] pre-trained ViT-H [33] image encoder\nand (2) SAM was trained on SA-1B, noting that this dataset\nincludes only automatically generated masks from the final\nstage of our data engine. For all other model and training\ndetails, such as hyperparameters, refer to §A."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 1598
        },
        {
          "x": 2271,
          "y": 1598
        },
        {
          "x": 2271,
          "y": 1647
        },
        {
          "x": 1280,
          "y": 1647
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='83' style='font-size:20px'>7.1. Zero-Shot Single Point Valid Mask Evaluation</p>",
      "id": 83,
      "page": 8,
      "text": "7.1. Zero-Shot Single Point Valid Mask Evaluation"
    },
    {
      "bounding_box": [
        {
          "x": 1282,
          "y": 1665
        },
        {
          "x": 2276,
          "y": 1665
        },
        {
          "x": 2276,
          "y": 2111
        },
        {
          "x": 1282,
          "y": 2111
        }
      ],
      "category": "paragraph",
      "html": "<p id='84' style='font-size:14px'>Task. We evaluate segmenting an object from a single fore-<br>ground point. This task is ill-posed as one point can refer<br>to multiple objects. Ground truth masks in most datasets<br>do not enumerate all possible masks, which can make au-<br>tomatic metrics unreliable. Therefore, we supplement the<br>standard mIoU metric (i.e., the mean of all IoUs between<br>predicted and ground truth masks) with a human study in<br>which annotators rate mask quality from 1 (nonsense) to 10<br>(pixel-perfect). See §D. 1 , §E, and §G for additional details.</p>",
      "id": 84,
      "page": 8,
      "text": "Task. We evaluate segmenting an object from a single fore-\nground point. This task is ill-posed as one point can refer\nto multiple objects. Ground truth masks in most datasets\ndo not enumerate all possible masks, which can make au-\ntomatic metrics unreliable. Therefore, we supplement the\nstandard mIoU metric (i.e., the mean of all IoUs between\npredicted and ground truth masks) with a human study in\nwhich annotators rate mask quality from 1 (nonsense) to 10\n(pixel-perfect). See §D. 1 , §E, and §G for additional details."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 2115
        },
        {
          "x": 2277,
          "y": 2115
        },
        {
          "x": 2277,
          "y": 2561
        },
        {
          "x": 1280,
          "y": 2561
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='85' style='font-size:16px'>By default, we sample points from the \"center\" of ground<br>truth masks (at a maximal value of the mask's interior dis-<br>tance transform), following the standard evaluation proto-<br>col in interactive segmentation [92]. Since SAM is capable<br>of predicting multiple masks, we evaluate only the model's<br>most confident mask by default. The baselines are all<br>single-mask methods. We compare mainly to RITM [92],<br>a strong interactive segmenter that performs best on our<br>benchmark compared to other strong baselines [67, 18].</p>",
      "id": 85,
      "page": 8,
      "text": "By default, we sample points from the \"center\" of ground\ntruth masks (at a maximal value of the mask's interior dis-\ntance transform), following the standard evaluation proto-\ncol in interactive segmentation [92]. Since SAM is capable\nof predicting multiple masks, we evaluate only the model's\nmost confident mask by default. The baselines are all\nsingle-mask methods. We compare mainly to RITM [92],\na strong interactive segmenter that performs best on our\nbenchmark compared to other strong baselines [67, 18]."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 2577
        },
        {
          "x": 2276,
          "y": 2577
        },
        {
          "x": 2276,
          "y": 2976
        },
        {
          "x": 1280,
          "y": 2976
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='86' style='font-size:14px'>Datasets. We use a newly compiled suite of 23 datasets<br>with diverse image distributions. Fig. 8 lists the datasets<br>and shows a sample from each one (see appendix Table 7 for<br>more details). We use all 23 datasets for mIoU evaluation.<br>For the human study, we use the subset listed in Fig. 9b<br>(due to the resource requirements of such studies). This<br>subset includes both datasets for which SAM outperforms<br>and underperforms RITM according to automatic metrics.</p>",
      "id": 86,
      "page": 8,
      "text": "Datasets. We use a newly compiled suite of 23 datasets\nwith diverse image distributions. Fig. 8 lists the datasets\nand shows a sample from each one (see appendix Table 7 for\nmore details). We use all 23 datasets for mIoU evaluation.\nFor the human study, we use the subset listed in Fig. 9b\n(due to the resource requirements of such studies). This\nsubset includes both datasets for which SAM outperforms\nand underperforms RITM according to automatic metrics."
    },
    {
      "bounding_box": [
        {
          "x": 1226,
          "y": 3056
        },
        {
          "x": 1251,
          "y": 3056
        },
        {
          "x": 1251,
          "y": 3091
        },
        {
          "x": 1226,
          "y": 3091
        }
      ],
      "category": "footer",
      "html": "<footer id='87' style='font-size:14px'>8</footer>",
      "id": 87,
      "page": 8,
      "text": "8"
    },
    {
      "bounding_box": [
        {
          "x": 218,
          "y": 283
        },
        {
          "x": 2259,
          "y": 283
        },
        {
          "x": 2259,
          "y": 939
        },
        {
          "x": 218,
          "y": 939
        }
      ],
      "category": "figure",
      "html": "<figure><img id='88' style='font-size:16px' alt=\"ADE20K [117] BBBC038v1 [12] Cityscapes [25 DOORS [80] DRAM [24] EgoHOS [1 13] GTEA [34, 63] Hypersim [86]\nIBD [17] iShape I 1 1 1 LVIS [44] NDD20 [100] NDISPark [22, 23] OVIS [81] PPDLS [74] Plittersdorf [46]\nIS\nSTREETS [91] TimberSeg [38] TrashCan [52] VISOR [28, 27] WoodScape [112] PIDRay [104] ZeroWaste-f [6]\n|4/D7/00 14:22:17 11:2\nCOD ITS 214 주점점 Do 생님\nⒸJAMSTEC\" data-coord=\"top-left:(218,283); bottom-right:(2259,939)\" /></figure>",
      "id": 88,
      "page": 9,
      "text": "ADE20K [117] BBBC038v1 [12] Cityscapes [25 DOORS [80] DRAM [24] EgoHOS [1 13] GTEA [34, 63] Hypersim [86]\nIBD [17] iShape I 1 1 1 LVIS [44] NDD20 [100] NDISPark [22, 23] OVIS [81] PPDLS [74] Plittersdorf [46]\nIS\nSTREETS [91] TimberSeg [38] TrashCan [52] VISOR [28, 27] WoodScape [112] PIDRay [104] ZeroWaste-f [6]\n|4/D7/00 14:22:17 11:2\nCOD ITS 214 주점점 Do 생님\nⒸJAMSTEC"
    },
    {
      "bounding_box": [
        {
          "x": 272,
          "y": 959
        },
        {
          "x": 2204,
          "y": 959
        },
        {
          "x": 2204,
          "y": 1003
        },
        {
          "x": 272,
          "y": 1003
        }
      ],
      "category": "caption",
      "html": "<br><caption id='89' style='font-size:22px'>Figure 8: Samples from the 23 diverse segmentation datasets used to evaluate SAM's zero-shot transfer capabilities.</caption>",
      "id": 89,
      "page": 9,
      "text": "Figure 8: Samples from the 23 diverse segmentation datasets used to evaluate SAM's zero-shot transfer capabilities."
    },
    {
      "bounding_box": [
        {
          "x": 206,
          "y": 1057
        },
        {
          "x": 2264,
          "y": 1057
        },
        {
          "x": 2264,
          "y": 1733
        },
        {
          "x": 206,
          "y": 1733
        }
      ],
      "category": "figure",
      "html": "<figure><img id='90' style='font-size:14px' alt=\"PPDLS [74] +46.9 9\nBBBC038v1 [12] +44.7 · Ground Truth\nDOORS [80] +41.1 rating\nmask\n· SAM\nTimberSeg [38] +28.9 7\nNDD20 [100] +21.1 . SAM - single output\nLVIS [44] +18.5 · RITM\nSTREETS [91] +17.3\n5\nZeroWaste-f[6] +9.1 Avg. 重\niShape [111] +8.8 LVIS VISOR DRAM IBD NDD20 OVIS iShape\nADE20K [117] +7.8 Datasets\nOVIS [81] +7.0\nHypersim [86] +6.1 (b) Mask quality ratings by human annotators\nNDISPark [22, 23] +2.7\nVISOR [28, 27] +1.8\nPlittersdorf [46] +1.5 SAM (oracle) SAM (oracle)\nEgoHOS [113] +0.8\nV\nIBD [17] -0.3 75 75\nx\nWoodScape [112] -0.6\nSAM datasets)\nCityscapes [25] 2.0 datasets)\nPIDRay [104] 5.8 RITM (23\n50\n50\nDRAM [24] 6.5 (23\nmloU\nTrashCan [52] -15.0 SimpleClick\nGTEA [34, 63] -21.4 FocalClick\n-20 0 +20 +40 mloU\n1 2 3 5 9 1 2 3 5 9\nIoU delta at 1 center point Number of points Number of points\n(a) SAM vs. RITM [92] on 23 datasets (c) Center points (default) (d) Random points\" data-coord=\"top-left:(206,1057); bottom-right:(2264,1733)\" /></figure>",
      "id": 90,
      "page": 9,
      "text": "PPDLS [74] +46.9 9\nBBBC038v1 [12] +44.7 · Ground Truth\nDOORS [80] +41.1 rating\nmask\n· SAM\nTimberSeg [38] +28.9 7\nNDD20 [100] +21.1 . SAM - single output\nLVIS [44] +18.5 · RITM\nSTREETS [91] +17.3\n5\nZeroWaste-f[6] +9.1 Avg. 重\niShape [111] +8.8 LVIS VISOR DRAM IBD NDD20 OVIS iShape\nADE20K [117] +7.8 Datasets\nOVIS [81] +7.0\nHypersim [86] +6.1 (b) Mask quality ratings by human annotators\nNDISPark [22, 23] +2.7\nVISOR [28, 27] +1.8\nPlittersdorf [46] +1.5 SAM (oracle) SAM (oracle)\nEgoHOS [113] +0.8\nV\nIBD [17] -0.3 75 75\nx\nWoodScape [112] -0.6\nSAM datasets)\nCityscapes [25] 2.0 datasets)\nPIDRay [104] 5.8 RITM (23\n50\n50\nDRAM [24] 6.5 (23\nmloU\nTrashCan [52] -15.0 SimpleClick\nGTEA [34, 63] -21.4 FocalClick\n-20 0 +20 +40 mloU\n1 2 3 5 9 1 2 3 5 9\nIoU delta at 1 center point Number of points Number of points\n(a) SAM vs. RITM [92] on 23 datasets (c) Center points (default) (d) Random points"
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 1755
        },
        {
          "x": 2276,
          "y": 1755
        },
        {
          "x": 2276,
          "y": 2005
        },
        {
          "x": 200,
          "y": 2005
        }
      ],
      "category": "caption",
      "html": "<br><caption id='91' style='font-size:22px'>Figure 9: Point to mask evaluation on 23 datasets. (a) Mean IoU of SAM and the strongest single point segmenter, RITM [92].<br>Due to ambiguity, a single mask may not match ground truth; circles show \"oracle\" results of the most relevant of SAM's 3<br>predictions. (b) Per-dataset comparison of mask quality ratings by annotators from 1 (worst) to 10 (best). All methods use<br>the ground truth mask center as the prompt. (c, d) mloU with varying number of points. SAM significantly outperforms prior<br>interactive segmenters with 1 point and is on par with more points. Low absolute mIoU at 1 point is the result of ambiguity.</caption>",
      "id": 91,
      "page": 9,
      "text": "Figure 9: Point to mask evaluation on 23 datasets. (a) Mean IoU of SAM and the strongest single point segmenter, RITM [92].\nDue to ambiguity, a single mask may not match ground truth; circles show \"oracle\" results of the most relevant of SAM's 3\npredictions. (b) Per-dataset comparison of mask quality ratings by annotators from 1 (worst) to 10 (best). All methods use\nthe ground truth mask center as the prompt. (c, d) mloU with varying number of points. SAM significantly outperforms prior\ninteractive segmenters with 1 point and is on par with more points. Low absolute mIoU at 1 point is the result of ambiguity."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 2068
        },
        {
          "x": 1198,
          "y": 2068
        },
        {
          "x": 1198,
          "y": 2565
        },
        {
          "x": 201,
          "y": 2565
        }
      ],
      "category": "paragraph",
      "html": "<p id='92' style='font-size:20px'>Results. First, we look at automatic evaluation on the full<br>suite of 23 datasets using mIoU. We compare per-dataset<br>results in Fig. 9a against RITM. SAM yields higher re-<br>sults on 16 of the 23 datasets, by as much as ~47 IoU. We<br>also present an \"oracle\" result, in which the most relevant<br>of SAM's 3 masks is selected by comparing them to the<br>ground truth, rather than selecting the most confident mask.<br>This reveals the impact of ambiguity on automatic evalu-<br>ation. In particular, with the oracle to perform ambiguity<br>resolution, SAM outperforms RITM on all datasets.</p>",
      "id": 92,
      "page": 9,
      "text": "Results. First, we look at automatic evaluation on the full\nsuite of 23 datasets using mIoU. We compare per-dataset\nresults in Fig. 9a against RITM. SAM yields higher re-\nsults on 16 of the 23 datasets, by as much as ~47 IoU. We\nalso present an \"oracle\" result, in which the most relevant\nof SAM's 3 masks is selected by comparing them to the\nground truth, rather than selecting the most confident mask.\nThis reveals the impact of ambiguity on automatic evalu-\nation. In particular, with the oracle to perform ambiguity\nresolution, SAM outperforms RITM on all datasets."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 2581
        },
        {
          "x": 1198,
          "y": 2581
        },
        {
          "x": 1198,
          "y": 2976
        },
        {
          "x": 202,
          "y": 2976
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='93' style='font-size:22px'>Results of the human study are presented in Fig. 9b. Er-<br>ror bars are 95% confidence intervals for mean mask rat-<br>ings (all differences are significant; see §E for details). We<br>observe that the annotators consistently rate the quality of<br>SAM's masks substantially higher than the strongest base-<br>line, RITM. An ablated, \"ambiguity-unaware\" version of<br>SAM with a single output mask has consistently lower rat-<br>ings, though still higher than RITM. SAM's mean ratings</p>",
      "id": 93,
      "page": 9,
      "text": "Results of the human study are presented in Fig. 9b. Er-\nror bars are 95% confidence intervals for mean mask rat-\nings (all differences are significant; see §E for details). We\nobserve that the annotators consistently rate the quality of\nSAM's masks substantially higher than the strongest base-\nline, RITM. An ablated, \"ambiguity-unaware\" version of\nSAM with a single output mask has consistently lower rat-\nings, though still higher than RITM. SAM's mean ratings"
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 2071
        },
        {
          "x": 2275,
          "y": 2071
        },
        {
          "x": 2275,
          "y": 2466
        },
        {
          "x": 1279,
          "y": 2466
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='94' style='font-size:20px'>fall between 7 and 9, which corresponds to the qualitative<br>rating guideline: \"A high score (7-9): The object is identi-<br>fiable and errors are small and rare (e.g., missing a small,<br>heavily obscured disconnected component, ...). \" These re-<br>sults indicate that SAM has learned to segment valid masks<br>from a single point. Note that for datasets like DRAM and<br>IBD, where SAM is worse on automatic metrics, it receives<br>consistently higher ratings in the human study.</p>",
      "id": 94,
      "page": 9,
      "text": "fall between 7 and 9, which corresponds to the qualitative\nrating guideline: \"A high score (7-9): The object is identi-\nfiable and errors are small and rare (e.g., missing a small,\nheavily obscured disconnected component, ...). \" These re-\nsults indicate that SAM has learned to segment valid masks\nfrom a single point. Note that for datasets like DRAM and\nIBD, where SAM is worse on automatic metrics, it receives\nconsistently higher ratings in the human study."
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 2479
        },
        {
          "x": 2276,
          "y": 2479
        },
        {
          "x": 2276,
          "y": 2977
        },
        {
          "x": 1279,
          "y": 2977
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='95' style='font-size:22px'>Fig. 9c shows additional baselines, SimpleClick [67] and<br>FocalClick [18], which obtain lower single point perfor-<br>mance than RITM and SAM. As the number of points in-<br>creases from 1 to 9, we observe that the gap between meth-<br>ods decreases. This is expected as the task becomes easier;<br>also, SAM is not optimized for the very high IoU regime.<br>Finally, in Fig. 9d we replace the default center point sam-<br>pling with random point sampling. We observe that the gap<br>between SAM and the baselines grows and SAM is able to<br>achieve comparable results under either sampling method.</p>",
      "id": 95,
      "page": 9,
      "text": "Fig. 9c shows additional baselines, SimpleClick [67] and\nFocalClick [18], which obtain lower single point perfor-\nmance than RITM and SAM. As the number of points in-\ncreases from 1 to 9, we observe that the gap between meth-\nods decreases. This is expected as the task becomes easier;\nalso, SAM is not optimized for the very high IoU regime.\nFinally, in Fig. 9d we replace the default center point sam-\npling with random point sampling. We observe that the gap\nbetween SAM and the baselines grows and SAM is able to\nachieve comparable results under either sampling method."
    },
    {
      "bounding_box": [
        {
          "x": 1224,
          "y": 3055
        },
        {
          "x": 1252,
          "y": 3055
        },
        {
          "x": 1252,
          "y": 3090
        },
        {
          "x": 1224,
          "y": 3090
        }
      ],
      "category": "footer",
      "html": "<footer id='96' style='font-size:18px'>9</footer>",
      "id": 96,
      "page": 9,
      "text": "9"
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 288
        },
        {
          "x": 1195,
          "y": 288
        },
        {
          "x": 1195,
          "y": 773
        },
        {
          "x": 203,
          "y": 773
        }
      ],
      "category": "figure",
      "html": "<figure><img id='97' style='font-size:14px' alt=\"image ground truth SAM\" data-coord=\"top-left:(203,288); bottom-right:(1195,773)\" /></figure>",
      "id": 97,
      "page": 10,
      "text": "image ground truth SAM"
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 784
        },
        {
          "x": 1199,
          "y": 784
        },
        {
          "x": 1199,
          "y": 932
        },
        {
          "x": 203,
          "y": 932
        }
      ],
      "category": "caption",
      "html": "<br><caption id='98' style='font-size:16px'>Figure 10: Zero-shot edge prediction on BSDS500. SAM<br>was not trained to predict edge maps nor did it have access<br>to BSDS images or annotations during training.</caption>",
      "id": 98,
      "page": 10,
      "text": "Figure 10: Zero-shot edge prediction on BSDS500. SAM\nwas not trained to predict edge maps nor did it have access\nto BSDS images or annotations during training."
    },
    {
      "bounding_box": [
        {
          "x": 207,
          "y": 981
        },
        {
          "x": 1186,
          "y": 981
        },
        {
          "x": 1186,
          "y": 1337
        },
        {
          "x": 207,
          "y": 1337
        }
      ],
      "category": "table",
      "html": "<table id='99' style='font-size:14px'><tr><td>method</td><td>year</td><td>ODS</td><td>OIS</td><td>AP</td><td>R50</td></tr><tr><td>HED [108]</td><td>2015</td><td>.788</td><td>.808</td><td>.840</td><td>.923</td></tr><tr><td>EDETR [79]</td><td>2022</td><td>.840</td><td>.858</td><td>.896</td><td>.930</td></tr><tr><td colspan=\"6\">zero-shot transfer methods:</td></tr><tr><td>Sobel filter</td><td>1968</td><td>.539</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Canny [13]</td><td>1986</td><td>.600</td><td>.640</td><td>.580</td><td>-</td></tr><tr><td>Felz-Hutt [35]</td><td>2004</td><td>.610</td><td>.640</td><td>.560</td><td>-</td></tr><tr><td>SAM</td><td>2023</td><td>.768</td><td>.786</td><td>.794</td><td>.928</td></tr></table>",
      "id": 99,
      "page": 10,
      "text": "method year ODS OIS AP R50\n HED [108] 2015 .788 .808 .840 .923\n EDETR [79] 2022 .840 .858 .896 .930\n zero-shot transfer methods:\n Sobel filter 1968 .539 - - -\n Canny [13] 1986 .600 .640 .580 -\n Felz-Hutt [35] 2004 .610 .640 .560 -\n SAM 2023 .768 .786 .794"
    },
    {
      "bounding_box": [
        {
          "x": 208,
          "y": 1354
        },
        {
          "x": 1189,
          "y": 1354
        },
        {
          "x": 1189,
          "y": 1398
        },
        {
          "x": 208,
          "y": 1398
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='100' style='font-size:16px'>Table 3: Zero-shot transfer to edge detection on BSDS500.</p>",
      "id": 100,
      "page": 10,
      "text": "Table 3: Zero-shot transfer to edge detection on BSDS500."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 1493
        },
        {
          "x": 794,
          "y": 1493
        },
        {
          "x": 794,
          "y": 1542
        },
        {
          "x": 202,
          "y": 1542
        }
      ],
      "category": "paragraph",
      "html": "<p id='101' style='font-size:20px'>7.2. Zero-Shot Edge Detection</p>",
      "id": 101,
      "page": 10,
      "text": "7.2. Zero-Shot Edge Detection"
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 1588
        },
        {
          "x": 1197,
          "y": 1588
        },
        {
          "x": 1197,
          "y": 2034
        },
        {
          "x": 202,
          "y": 2034
        }
      ],
      "category": "paragraph",
      "html": "<p id='102' style='font-size:20px'>Approach. We evaluate SAM on the classic low-level task<br>of edge detection using BSDS500 [72, 3]. We use a sim-<br>plified version of our automatic mask generation pipeline.<br>Specifically, we prompt SAM with a 16x 16 regular grid of<br>foreground points resulting in 768 predicted masks (3 per<br>point). Redundant masks are removed by NMS. Then, edge<br>maps are computed using Sobel filtering of unthresholded<br>mask probability maps and standard lightweight postpro-<br>cessing, including edge NMS (see §D.2 for details).</p>",
      "id": 102,
      "page": 10,
      "text": "Approach. We evaluate SAM on the classic low-level task\nof edge detection using BSDS500 [72, 3]. We use a sim-\nplified version of our automatic mask generation pipeline.\nSpecifically, we prompt SAM with a 16x 16 regular grid of\nforeground points resulting in 768 predicted masks (3 per\npoint). Redundant masks are removed by NMS. Then, edge\nmaps are computed using Sobel filtering of unthresholded\nmask probability maps and standard lightweight postpro-\ncessing, including edge NMS (see §D.2 for details)."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 2049
        },
        {
          "x": 1198,
          "y": 2049
        },
        {
          "x": 1198,
          "y": 2700
        },
        {
          "x": 201,
          "y": 2700
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='103' style='font-size:18px'>Results. We visualize representative edge maps in Fig. 10<br>(see Fig. 15 for more). Qualitatively, we observe that even<br>though SAM was not trained for edge detection, it produces<br>reasonable edge maps. Compared to the ground truth, SAM<br>predicts more edges, including sensible ones that are not an-<br>notated in BSDS500. This bias is reflected quantitatively in<br>Table 3: recall at 50% precision (R50) is high, at the cost of<br>precision. SAM naturally lags behind state-of-the-art meth-<br>ods that learn the biases of BSDS500, i.e., which edges to<br>suppress. Nevertheless, SAM performs well compared to<br>pioneering deep learning methods such as HED [108] (also<br>trained on BSDS500) and significantly better than prior,<br>though admittedly outdated, zero-shot transfer methods.</p>",
      "id": 103,
      "page": 10,
      "text": "Results. We visualize representative edge maps in Fig. 10\n(see Fig. 15 for more). Qualitatively, we observe that even\nthough SAM was not trained for edge detection, it produces\nreasonable edge maps. Compared to the ground truth, SAM\npredicts more edges, including sensible ones that are not an-\nnotated in BSDS500. This bias is reflected quantitatively in\nTable 3: recall at 50% precision (R50) is high, at the cost of\nprecision. SAM naturally lags behind state-of-the-art meth-\nods that learn the biases of BSDS500, i.e., which edges to\nsuppress. Nevertheless, SAM performs well compared to\npioneering deep learning methods such as HED [108] (also\ntrained on BSDS500) and significantly better than prior,\nthough admittedly outdated, zero-shot transfer methods."
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 2734
        },
        {
          "x": 833,
          "y": 2734
        },
        {
          "x": 833,
          "y": 2784
        },
        {
          "x": 204,
          "y": 2784
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='104' style='font-size:22px'>7.3. Zero-Shot Object Proposals</p>",
      "id": 104,
      "page": 10,
      "text": "7.3. Zero-Shot Object Proposals"
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 2827
        },
        {
          "x": 1198,
          "y": 2827
        },
        {
          "x": 1198,
          "y": 2976
        },
        {
          "x": 202,
          "y": 2976
        }
      ],
      "category": "paragraph",
      "html": "<p id='105' style='font-size:18px'>Approach. Next, we evaluate SAM on the mid-level task<br>of object proposal generation [2, 102]. This task has played<br>an important role in object detection research, serving as an</p>",
      "id": 105,
      "page": 10,
      "text": "Approach. Next, we evaluate SAM on the mid-level task\nof object proposal generation [2, 102]. This task has played\nan important role in object detection research, serving as an"
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 296
        },
        {
          "x": 2270,
          "y": 296
        },
        {
          "x": 2270,
          "y": 567
        },
        {
          "x": 1278,
          "y": 567
        }
      ],
      "category": "table",
      "html": "<br><table id='106' style='font-size:14px'><tr><td></td><td colspan=\"7\">mask AR @1000</td></tr><tr><td>method</td><td>all</td><td>small</td><td>med.</td><td>large</td><td>freq.</td><td>com.</td><td>rare</td></tr><tr><td>ViTDet-H [62]</td><td>63.0</td><td>51.7</td><td>80.8</td><td>87.0</td><td>63.1</td><td>63.3</td><td>58.3</td></tr><tr><td colspan=\"8\">zero-shot transfer methods:</td></tr><tr><td>SAM - single out.</td><td>54.9</td><td>42.8</td><td>76.7</td><td>74.4</td><td>54.7</td><td>59.8</td><td>62.0</td></tr><tr><td>SAM</td><td>59.3</td><td>45.5</td><td>81.6</td><td>86.9</td><td>59.1</td><td>63.9</td><td>65.8</td></tr></table>",
      "id": 106,
      "page": 10,
      "text": "mask AR @1000\n method all small med. large freq. com. rare\n ViTDet-H [62] 63.0 51.7 80.8 87.0 63.1 63.3 58.3\n zero-shot transfer methods:\n SAM - single out. 54.9 42.8 76.7 74.4 54.7 59.8 62.0\n SAM 59.3 45.5 81.6 86.9 59.1 63.9"
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 581
        },
        {
          "x": 2275,
          "y": 581
        },
        {
          "x": 2275,
          "y": 728
        },
        {
          "x": 1279,
          "y": 728
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='107' style='font-size:16px'>Table 4: Object proposal generation on LVIS v1. SAM is<br>applied zero-shot, i.e. it was not trained for object proposal<br>generation nor did it access LVIS images or annotations.</p>",
      "id": 107,
      "page": 10,
      "text": "Table 4: Object proposal generation on LVIS v1. SAM is\napplied zero-shot, i.e. it was not trained for object proposal\ngeneration nor did it access LVIS images or annotations."
    },
    {
      "bounding_box": [
        {
          "x": 1282,
          "y": 825
        },
        {
          "x": 2275,
          "y": 825
        },
        {
          "x": 2275,
          "y": 1018
        },
        {
          "x": 1282,
          "y": 1018
        }
      ],
      "category": "paragraph",
      "html": "<p id='108' style='font-size:18px'>intermediate step in pioneering systems (e.g., [102, 41, 84]).<br>To generate object proposals, we run a slightly modified<br>version of our automatic mask generation pipeline and out-<br>put the masks as proposals (see §D.3 for details).</p>",
      "id": 108,
      "page": 10,
      "text": "intermediate step in pioneering systems (e.g., [102, 41, 84]).\nTo generate object proposals, we run a slightly modified\nversion of our automatic mask generation pipeline and out-\nput the masks as proposals (see §D.3 for details)."
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 1027
        },
        {
          "x": 2276,
          "y": 1027
        },
        {
          "x": 2276,
          "y": 1418
        },
        {
          "x": 1279,
          "y": 1418
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='109' style='font-size:18px'>We compute the standard average recall (AR) metric on<br>LVIS v1 [44]. We focus on LVIS because its large number<br>of categories presents a challenging test. We compare to<br>a strong baseline implemented as a ViTDet [62] detector<br>(with cascade Mask R-CNN [48, 11] ViT-H). We note that<br>this \"baseline\" corresponds to the \"Detector Masquerading<br>as Proposal generator\" (DMP) method [16] that was shown<br>to game AR, making it a truly demanding comparison.</p>",
      "id": 109,
      "page": 10,
      "text": "We compute the standard average recall (AR) metric on\nLVIS v1 [44]. We focus on LVIS because its large number\nof categories presents a challenging test. We compare to\na strong baseline implemented as a ViTDet [62] detector\n(with cascade Mask R-CNN [48, 11] ViT-H). We note that\nthis \"baseline\" corresponds to the \"Detector Masquerading\nas Proposal generator\" (DMP) method [16] that was shown\nto game AR, making it a truly demanding comparison."
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 1435
        },
        {
          "x": 2276,
          "y": 1435
        },
        {
          "x": 2276,
          "y": 2035
        },
        {
          "x": 1279,
          "y": 2035
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='110' style='font-size:16px'>Results. In Table 4 we see unsurprisingly that using the<br>detections from ViTDet-H as object proposals (i.e., the<br>DMP method [16] that games AR) performs the best over-<br>all. However, SAM does remarkably well on several met-<br>rics. Notably, it outperforms ViTDet-H on medium and<br>large objects, as well as rare and common objects. In fact,<br>SAM only underperforms ViTDet-H on small objects and<br>frequent objects, where ViTDet-H can easily learn LVIS-<br>specific annotation biases since it was trained on LVIS, un-<br>like SAM. We also compare against an ablated ambiguity-<br>unaware version of SAM (\"single out.\"), which performs<br>significantly worse than SAM on all AR metrics.</p>",
      "id": 110,
      "page": 10,
      "text": "Results. In Table 4 we see unsurprisingly that using the\ndetections from ViTDet-H as object proposals (i.e., the\nDMP method [16] that games AR) performs the best over-\nall. However, SAM does remarkably well on several met-\nrics. Notably, it outperforms ViTDet-H on medium and\nlarge objects, as well as rare and common objects. In fact,\nSAM only underperforms ViTDet-H on small objects and\nfrequent objects, where ViTDet-H can easily learn LVIS-\nspecific annotation biases since it was trained on LVIS, un-\nlike SAM. We also compare against an ablated ambiguity-\nunaware version of SAM (\"single out.\"), which performs\nsignificantly worse than SAM on all AR metrics."
    },
    {
      "bounding_box": [
        {
          "x": 1282,
          "y": 2070
        },
        {
          "x": 2021,
          "y": 2070
        },
        {
          "x": 2021,
          "y": 2120
        },
        {
          "x": 1282,
          "y": 2120
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='111' style='font-size:20px'>7.4. Zero-Shot Instance Segmentation</p>",
      "id": 111,
      "page": 10,
      "text": "7.4. Zero-Shot Instance Segmentation"
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 2164
        },
        {
          "x": 2275,
          "y": 2164
        },
        {
          "x": 2275,
          "y": 2410
        },
        {
          "x": 1281,
          "y": 2410
        }
      ],
      "category": "paragraph",
      "html": "<p id='112' style='font-size:18px'>Approach. Moving to higher-level vision, we use SAM<br>as the segmentation module of an instance segmenter. The<br>implementation is simple: we run a object detector (the<br>ViTDet used before) and prompt SAM with its output<br>boxes. This illustrates composing SAM in a larger system.</p>",
      "id": 112,
      "page": 10,
      "text": "Approach. Moving to higher-level vision, we use SAM\nas the segmentation module of an instance segmenter. The\nimplementation is simple: we run a object detector (the\nViTDet used before) and prompt SAM with its output\nboxes. This illustrates composing SAM in a larger system."
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 2427
        },
        {
          "x": 2275,
          "y": 2427
        },
        {
          "x": 2275,
          "y": 2977
        },
        {
          "x": 1279,
          "y": 2977
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='113' style='font-size:18px'>Results. We compare the masks predicted by SAM and<br>ViTDet on COCO and LVIS in Table 5. Looking at the<br>mask AP metric we observe gaps on both datasets, where<br>SAM is reasonably close, though certainly behind ViTDet.<br>By visualizing outputs, we observed that SAM masks are<br>often qualitatively better than those of ViTDet, with crisper<br>boundaries (see §D.4 and Fig. 16). To investigate this ob-<br>servation, we conducted an additional human study asking<br>annotators to rate the ViTDet masks and SAM masks on the<br>1 to 10 quality scale used before. In Fig. 11 we observe that<br>SAM consistently outperforms ViTDet in the human study.</p>",
      "id": 113,
      "page": 10,
      "text": "Results. We compare the masks predicted by SAM and\nViTDet on COCO and LVIS in Table 5. Looking at the\nmask AP metric we observe gaps on both datasets, where\nSAM is reasonably close, though certainly behind ViTDet.\nBy visualizing outputs, we observed that SAM masks are\noften qualitatively better than those of ViTDet, with crisper\nboundaries (see §D.4 and Fig. 16). To investigate this ob-\nservation, we conducted an additional human study asking\nannotators to rate the ViTDet masks and SAM masks on the\n1 to 10 quality scale used before. In Fig. 11 we observe that\nSAM consistently outperforms ViTDet in the human study."
    },
    {
      "bounding_box": [
        {
          "x": 1220,
          "y": 3056
        },
        {
          "x": 1262,
          "y": 3056
        },
        {
          "x": 1262,
          "y": 3090
        },
        {
          "x": 1220,
          "y": 3090
        }
      ],
      "category": "footer",
      "html": "<footer id='114' style='font-size:16px'>10</footer>",
      "id": 114,
      "page": 10,
      "text": "10"
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 536
        },
        {
          "x": 1198,
          "y": 536
        },
        {
          "x": 1198,
          "y": 785
        },
        {
          "x": 202,
          "y": 785
        }
      ],
      "category": "paragraph",
      "html": "<p id='115' style='font-size:18px'>Table 5: Instance segmentation results. SAM is prompted<br>with ViTDet boxes to do zero-shot segmentation. The fully-<br>supervised ViTDet outperforms SAM, but the gap shrinks<br>on the higher-quality LVIS masks. Interestingly, SAM out-<br>performs ViTDet according to human ratings (see Fig. 11).</p>",
      "id": 115,
      "page": 11,
      "text": "Table 5: Instance segmentation results. SAM is prompted\nwith ViTDet boxes to do zero-shot segmentation. The fully-\nsupervised ViTDet outperforms SAM, but the gap shrinks\non the higher-quality LVIS masks. Interestingly, SAM out-\nperforms ViTDet according to human ratings (see Fig. 11)."
    },
    {
      "bounding_box": [
        {
          "x": 207,
          "y": 826
        },
        {
          "x": 1174,
          "y": 826
        },
        {
          "x": 1174,
          "y": 1143
        },
        {
          "x": 207,
          "y": 1143
        }
      ],
      "category": "figure",
      "html": "<figure><img id='116' style='font-size:14px' alt=\"ratings\n8.6 土 0.06, LVIS GT\n40\nof 8.1 土 0.07, SAM\nPercent 7.9± 0.08, ViTDet-H\n20\n7.6±0.12, COCO GT\n0\n1 2 3 4 5 6 7 8 9 10\nMask quality rating\" data-coord=\"top-left:(207,826); bottom-right:(1174,1143)\" /></figure>",
      "id": 116,
      "page": 11,
      "text": "ratings\n8.6 土 0.06, LVIS GT\n40\nof 8.1 土 0.07, SAM\nPercent 7.9± 0.08, ViTDet-H\n20\n7.6±0.12, COCO GT\n0\n1 2 3 4 5 6 7 8 9 10\nMask quality rating"
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 1155
        },
        {
          "x": 1198,
          "y": 1155
        },
        {
          "x": 1198,
          "y": 1504
        },
        {
          "x": 201,
          "y": 1504
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='117' style='font-size:20px'>Figure 11: Mask quality rating distribution from our human<br>study for ViTDet and SAM, both applied to LVIS ground<br>truth boxes. We also report LVIS and COCO ground truth<br>quality. The legend shows rating means and 95% confi-<br>dence intervals. Despite its lower AP (Table 5), SAM has<br>higher ratings than ViTDet, suggesting that ViTDet exploits<br>biases in the COCO and LVIS training data.</p>",
      "id": 117,
      "page": 11,
      "text": "Figure 11: Mask quality rating distribution from our human\nstudy for ViTDet and SAM, both applied to LVIS ground\ntruth boxes. We also report LVIS and COCO ground truth\nquality. The legend shows rating means and 95% confi-\ndence intervals. Despite its lower AP (Table 5), SAM has\nhigher ratings than ViTDet, suggesting that ViTDet exploits\nbiases in the COCO and LVIS training data."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 1559
        },
        {
          "x": 1199,
          "y": 1559
        },
        {
          "x": 1199,
          "y": 2055
        },
        {
          "x": 202,
          "y": 2055
        }
      ],
      "category": "paragraph",
      "html": "<p id='118' style='font-size:18px'>We hypothesize that on COCO, where the mask AP gap<br>is larger and the ground truth quality is relatively low (as<br>borne out by the human study), ViTDet learns the specific<br>biases of COCO masks. SAM, being a zero-shot method,<br>is unable to exploit these (generally undesirable) biases.<br>The LVIS dataset has higher quality ground truth, but there<br>are still specific idiosyncrasies (e.g., masks do not contain<br>holes, they are simple polygons by construction) and biases<br>for modal vs. amodal masks. Again, SAM is not trained to<br>learn these biases, while ViTDet can exploit them.</p>",
      "id": 118,
      "page": 11,
      "text": "We hypothesize that on COCO, where the mask AP gap\nis larger and the ground truth quality is relatively low (as\nborne out by the human study), ViTDet learns the specific\nbiases of COCO masks. SAM, being a zero-shot method,\nis unable to exploit these (generally undesirable) biases.\nThe LVIS dataset has higher quality ground truth, but there\nare still specific idiosyncrasies (e.g., masks do not contain\nholes, they are simple polygons by construction) and biases\nfor modal vs. amodal masks. Again, SAM is not trained to\nlearn these biases, while ViTDet can exploit them."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 2087
        },
        {
          "x": 761,
          "y": 2087
        },
        {
          "x": 761,
          "y": 2137
        },
        {
          "x": 202,
          "y": 2137
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='119' style='font-size:22px'>7.5. Zero-Shot Text-to-Mask</p>",
      "id": 119,
      "page": 11,
      "text": "7.5. Zero-Shot Text-to-Mask"
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 2178
        },
        {
          "x": 1198,
          "y": 2178
        },
        {
          "x": 1198,
          "y": 2978
        },
        {
          "x": 200,
          "y": 2978
        }
      ],
      "category": "paragraph",
      "html": "<p id='120' style='font-size:16px'>Approach. Finally, we consider an even higher-level task:<br>segmenting objects from free-form text. This experiment<br>is a proof-of-concept of SAM's ability to process text<br>prompts. While we used the exact same SAM in all prior<br>experiments, for this one SAM's training procedure is mod-<br>ified to make it text-aware, but in a way that does not require<br>new text annotations. Specifically, for each manually col-<br>lected mask with area larger than 1002 we extract the CLIP<br>image embedding. Then, during training, we prompt SAM<br>with the extracted CLIP image embeddings as its first in-<br>teraction. The key observation here is that because CLIP's<br>image embeddings are trained to align with its text embed-<br>dings, we can train with image embeddings, but use text<br>embeddings for inference. That is, at inference time we run<br>text through CLIP's text encoder and then give the resulting<br>text embedding as a prompt to SAM (see §D.5 for details).</p>",
      "id": 120,
      "page": 11,
      "text": "Approach. Finally, we consider an even higher-level task:\nsegmenting objects from free-form text. This experiment\nis a proof-of-concept of SAM's ability to process text\nprompts. While we used the exact same SAM in all prior\nexperiments, for this one SAM's training procedure is mod-\nified to make it text-aware, but in a way that does not require\nnew text annotations. Specifically, for each manually col-\nlected mask with area larger than 1002 we extract the CLIP\nimage embedding. Then, during training, we prompt SAM\nwith the extracted CLIP image embeddings as its first in-\nteraction. The key observation here is that because CLIP's\nimage embeddings are trained to align with its text embed-\ndings, we can train with image embeddings, but use text\nembeddings for inference. That is, at inference time we run\ntext through CLIP's text encoder and then give the resulting\ntext embedding as a prompt to SAM (see §D.5 for details)."
    },
    {
      "bounding_box": [
        {
          "x": 1286,
          "y": 290
        },
        {
          "x": 2271,
          "y": 290
        },
        {
          "x": 2271,
          "y": 1006
        },
        {
          "x": 1286,
          "y": 1006
        }
      ],
      "category": "figure",
      "html": "<br><figure><img id='121' style='font-size:14px' alt=\"a wheel 'beaver tooth grille'\n'a wiper' a wiper' + point\n'wipers 'wipers' + point\" data-coord=\"top-left:(1286,290); bottom-right:(2271,1006)\" /></figure>",
      "id": 121,
      "page": 11,
      "text": "a wheel \"beaver tooth grille\"\n\"a wiper' a wiper\" + point\n\"wipers \"wipers\" + point"
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 1014
        },
        {
          "x": 2276,
          "y": 1014
        },
        {
          "x": 2276,
          "y": 1161
        },
        {
          "x": 1280,
          "y": 1161
        }
      ],
      "category": "caption",
      "html": "<br><caption id='122' style='font-size:16px'>Figure 12: Zero-shot text-to-mask. SAM can work with<br>simple and nuanced text prompts. When SAM fails to make<br>a correct prediction, an additional point prompt can help.</caption>",
      "id": 122,
      "page": 11,
      "text": "Figure 12: Zero-shot text-to-mask. SAM can work with\nsimple and nuanced text prompts. When SAM fails to make\na correct prediction, an additional point prompt can help."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 1189
        },
        {
          "x": 2276,
          "y": 1189
        },
        {
          "x": 2276,
          "y": 1439
        },
        {
          "x": 1280,
          "y": 1439
        }
      ],
      "category": "paragraph",
      "html": "<p id='123' style='font-size:16px'>Results. We show qualitative results in Fig. 12. SAM<br>can segment objects based on simple text prompts like \"a<br>wheel\" as well as phrases like \"beaver tooth grille\". When<br>SAM fails to pick the right object from a text prompt only,<br>an additional point often fixes the prediction, similar to [31].</p>",
      "id": 123,
      "page": 11,
      "text": "Results. We show qualitative results in Fig. 12. SAM\ncan segment objects based on simple text prompts like \"a\nwheel\" as well as phrases like \"beaver tooth grille\". When\nSAM fails to pick the right object from a text prompt only,\nan additional point often fixes the prediction, similar to [31]."
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 1467
        },
        {
          "x": 1559,
          "y": 1467
        },
        {
          "x": 1559,
          "y": 1514
        },
        {
          "x": 1281,
          "y": 1514
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='124' style='font-size:20px'>7.6. Ablations</p>",
      "id": 124,
      "page": 11,
      "text": "7.6. Ablations"
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 1535
        },
        {
          "x": 2277,
          "y": 1535
        },
        {
          "x": 2277,
          "y": 1979
        },
        {
          "x": 1280,
          "y": 1979
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='125' style='font-size:16px'>We perform several ablations on our 23 dataset suite with<br>the single center point prompt protocol. Recall that a sin-<br>gle point may be ambiguous and that ambiguity may not<br>be represented in the ground truth, which contains only a<br>single mask per point. Since SAM is operating in a zero-<br>shot transfer setting there can be systematic biases between<br>SAM's top-ranked mask vs. the masks resulting from data<br>annotation guidelines. We therefore additionally report the<br>best mask with respect to the ground truth (\"oracle\").</p>",
      "id": 125,
      "page": 11,
      "text": "We perform several ablations on our 23 dataset suite with\nthe single center point prompt protocol. Recall that a sin-\ngle point may be ambiguous and that ambiguity may not\nbe represented in the ground truth, which contains only a\nsingle mask per point. Since SAM is operating in a zero-\nshot transfer setting there can be systematic biases between\nSAM's top-ranked mask vs. the masks resulting from data\nannotation guidelines. We therefore additionally report the\nbest mask with respect to the ground truth (\"oracle\")."
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 1982
        },
        {
          "x": 2275,
          "y": 1982
        },
        {
          "x": 2275,
          "y": 2576
        },
        {
          "x": 1278,
          "y": 2576
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='126' style='font-size:18px'>Fig. 13 (left) plots SAM's performance when trained on<br>cumulative data from the data engine stages. We observe<br>that each stage increases mIoU. When training with all three<br>stages, the automatic masks vastly outnumber the manual<br>and semi-automatic masks. To address this, we found that<br>oversampling the manual and semi-automatic masks during<br>training by 10x gave best results. This setup complicates<br>training. We therefore tested a fourth setup that uses only<br>the automatically generated masks. With this data, SAM<br>performs only marginally lower than using all data (~0.5<br>mIoU). Therefore, by default we use only the automatically<br>generated masks to simplify the training setup.</p>",
      "id": 126,
      "page": 11,
      "text": "Fig. 13 (left) plots SAM's performance when trained on\ncumulative data from the data engine stages. We observe\nthat each stage increases mIoU. When training with all three\nstages, the automatic masks vastly outnumber the manual\nand semi-automatic masks. To address this, we found that\noversampling the manual and semi-automatic masks during\ntraining by 10x gave best results. This setup complicates\ntraining. We therefore tested a fourth setup that uses only\nthe automatically generated masks. With this data, SAM\nperforms only marginally lower than using all data (~0.5\nmIoU). Therefore, by default we use only the automatically\ngenerated masks to simplify the training setup."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 2580
        },
        {
          "x": 2276,
          "y": 2580
        },
        {
          "x": 2276,
          "y": 2977
        },
        {
          "x": 1280,
          "y": 2977
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='127' style='font-size:18px'>In Fig. 13 (middle) we look at the impact of data volume.<br>The full SA-1B contains 11M images, which we uniformly<br>subsample to 1M and 0.1M for this ablation. At 0.1M im-<br>ages, we observe a large mIoU decline under all settings.<br>However, with 1M images, about 10% of the full dataset,<br>we observe results comparable to using the full dataset.<br>This data regime, which still includes approximately 100M<br>masks, may be a practical setting for many use cases.</p>",
      "id": 127,
      "page": 11,
      "text": "In Fig. 13 (middle) we look at the impact of data volume.\nThe full SA-1B contains 11M images, which we uniformly\nsubsample to 1M and 0.1M for this ablation. At 0.1M im-\nages, we observe a large mIoU decline under all settings.\nHowever, with 1M images, about 10% of the full dataset,\nwe observe results comparable to using the full dataset.\nThis data regime, which still includes approximately 100M\nmasks, may be a practical setting for many use cases."
    },
    {
      "bounding_box": [
        {
          "x": 1217,
          "y": 3052
        },
        {
          "x": 1260,
          "y": 3052
        },
        {
          "x": 1260,
          "y": 3095
        },
        {
          "x": 1217,
          "y": 3095
        }
      ],
      "category": "footer",
      "html": "<footer id='128' style='font-size:16px'>11</footer>",
      "id": 128,
      "page": 11,
      "text": "11"
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 299
        },
        {
          "x": 2265,
          "y": 299
        },
        {
          "x": 2265,
          "y": 624
        },
        {
          "x": 204,
          "y": 624
        }
      ],
      "category": "figure",
      "html": "<figure><img id='129' style='font-size:14px' alt=\"5 points\n1 point (oracle) (oracle)\ndatasets)\npoint\n1 point 80\n70\n3 points\n65\n(23 datasets)\n& 60 datasets) 70\n& 75\nmloU\nmloU 50 1 point\n1 point (oracle) 60\n70 2 points\n91M 308M 636M\nmanual + semi + automatic automatic mloU\nautomatic only 0.1M 1M 11M ViT-B ViT-L ViT-H\nTraining data stages Training images Number of parameters\" data-coord=\"top-left:(204,299); bottom-right:(2265,624)\" /></figure>",
      "id": 129,
      "page": 12,
      "text": "5 points\n1 point (oracle) (oracle)\ndatasets)\npoint\n1 point 80\n70\n3 points\n65\n(23 datasets)\n& 60 datasets) 70\n& 75\nmloU\nmloU 50 1 point\n1 point (oracle) 60\n70 2 points\n91M 308M 636M\nmanual + semi + automatic automatic mloU\nautomatic only 0.1M 1M 11M ViT-B ViT-L ViT-H\nTraining data stages Training images Number of parameters"
    },
    {
      "bounding_box": [
        {
          "x": 199,
          "y": 641
        },
        {
          "x": 2277,
          "y": 641
        },
        {
          "x": 2277,
          "y": 893
        },
        {
          "x": 199,
          "y": 893
        }
      ],
      "category": "caption",
      "html": "<br><caption id='130' style='font-size:18px'>Figure 13: Ablation studies of our data engine stages, image encoder scaling, and training data scaling. (Left) Each data<br>engine stage leads to improvements on our 23 dataset suite, and training with only the automatic data (our default) yields<br>similar results to using data from all three stages. (Middle) SAM trained with ~10% of SA-1B and full SA-1B is comparable.<br>We train with all 11M images by default, but using 1M images is a reasonable practical setting. (Right) Scaling SAM's image<br>encoder shows meaningful, yet saturating gains. Nevertheless, smaller image encoders may be preferred in certain settings.</caption>",
      "id": 130,
      "page": 12,
      "text": "Figure 13: Ablation studies of our data engine stages, image encoder scaling, and training data scaling. (Left) Each data\nengine stage leads to improvements on our 23 dataset suite, and training with only the automatic data (our default) yields\nsimilar results to using data from all three stages. (Middle) SAM trained with ~10% of SA-1B and full SA-1B is comparable.\nWe train with all 11M images by default, but using 1M images is a reasonable practical setting. (Right) Scaling SAM's image\nencoder shows meaningful, yet saturating gains. Nevertheless, smaller image encoders may be preferred in certain settings."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 944
        },
        {
          "x": 1199,
          "y": 944
        },
        {
          "x": 1199,
          "y": 1145
        },
        {
          "x": 202,
          "y": 1145
        }
      ],
      "category": "paragraph",
      "html": "<p id='131' style='font-size:18px'>Finally, Fig. 13 (right) shows results with ViT-B, ViT-L,<br>and ViT-H image encoders. ViT-H improves substantially<br>over ViT-B, but has only marginal gains over ViT-L. Further<br>image encoder scaling does not appear fruitful at this time.</p>",
      "id": 131,
      "page": 12,
      "text": "Finally, Fig. 13 (right) shows results with ViT-B, ViT-L,\nand ViT-H image encoders. ViT-H improves substantially\nover ViT-B, but has only marginal gains over ViT-L. Further\nimage encoder scaling does not appear fruitful at this time."
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 1181
        },
        {
          "x": 485,
          "y": 1181
        },
        {
          "x": 485,
          "y": 1232
        },
        {
          "x": 203,
          "y": 1232
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='132' style='font-size:20px'>8. Discussion</p>",
      "id": 132,
      "page": 12,
      "text": "8. Discussion"
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 1267
        },
        {
          "x": 1198,
          "y": 1267
        },
        {
          "x": 1198,
          "y": 2166
        },
        {
          "x": 200,
          "y": 2166
        }
      ],
      "category": "paragraph",
      "html": "<p id='133' style='font-size:16px'>Foundation models. Pre-trained models have been adapted<br>to downstream tasks since the early days of machine learn-<br>ing [99]. This paradigm has become increasingly impor-<br>tant in recent years with a growing emphasis on scale, and<br>such models have recently been (re-)branded as \"founda-<br>tion models\": i.e. models that are \"trained on broad data<br>at scale and are adaptable to a wide range of downstream<br>tasks\" [8]. Our work correlates well with this definition,<br>though we note that a foundation model for image segmen-<br>tation is an inherently limited scope, since it represents an<br>important, yet fractional, subset of computer vision. We<br>also contrast one aspect of our approach with [8], which<br>emphasizes the role of self-supervised learning in founda-<br>tion models. While our model is initialized with a self-<br>supervised technique (MAE [47]), the vast majority of its<br>capabilities come from large-scale supervised training. In<br>cases where data engines can scale available annotations,<br>like ours, supervised training provides an effective solution.</p>",
      "id": 133,
      "page": 12,
      "text": "Foundation models. Pre-trained models have been adapted\nto downstream tasks since the early days of machine learn-\ning [99]. This paradigm has become increasingly impor-\ntant in recent years with a growing emphasis on scale, and\nsuch models have recently been (re-)branded as \"founda-\ntion models\": i.e. models that are \"trained on broad data\nat scale and are adaptable to a wide range of downstream\ntasks\" [8]. Our work correlates well with this definition,\nthough we note that a foundation model for image segmen-\ntation is an inherently limited scope, since it represents an\nimportant, yet fractional, subset of computer vision. We\nalso contrast one aspect of our approach with [8], which\nemphasizes the role of self-supervised learning in founda-\ntion models. While our model is initialized with a self-\nsupervised technique (MAE [47]), the vast majority of its\ncapabilities come from large-scale supervised training. In\ncases where data engines can scale available annotations,\nlike ours, supervised training provides an effective solution."
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 2178
        },
        {
          "x": 1198,
          "y": 2178
        },
        {
          "x": 1198,
          "y": 2978
        },
        {
          "x": 200,
          "y": 2978
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='134' style='font-size:16px'>Compositionality. Pre-trained models can power new ca-<br>pabilities even beyond ones imagined at the moment of<br>training. One prominent example is how CLIP [82] is used<br>as a component in larger systems, such as DALL·E [83].<br>Our goal is to make this kind of composition straightfor-<br>ward with SAM. We aim to achieve this by requiring SAM<br>to predict a valid mask for a wide range of segmentation<br>prompts. The effect is to create a reliable interface between<br>SAM and other components. For example, MCC [106] can<br>easily use SAM to segment an object of interest and achieve<br>strong generalization to unseen objects for 3D reconstruc-<br>tion from a single RGB-D image. In another example, SAM<br>can be prompted with gaze points detected by a wearable<br>device, enabling new applications. Thanks to SAM's abil-<br>ity to generalize to new domains like ego-centric images,<br>such systems work without need for additional training.</p>",
      "id": 134,
      "page": 12,
      "text": "Compositionality. Pre-trained models can power new ca-\npabilities even beyond ones imagined at the moment of\ntraining. One prominent example is how CLIP [82] is used\nas a component in larger systems, such as DALL·E [83].\nOur goal is to make this kind of composition straightfor-\nward with SAM. We aim to achieve this by requiring SAM\nto predict a valid mask for a wide range of segmentation\nprompts. The effect is to create a reliable interface between\nSAM and other components. For example, MCC [106] can\neasily use SAM to segment an object of interest and achieve\nstrong generalization to unseen objects for 3D reconstruc-\ntion from a single RGB-D image. In another example, SAM\ncan be prompted with gaze points detected by a wearable\ndevice, enabling new applications. Thanks to SAM's abil-\nity to generalize to new domains like ego-centric images,\nsuch systems work without need for additional training."
    },
    {
      "bounding_box": [
        {
          "x": 1277,
          "y": 943
        },
        {
          "x": 2277,
          "y": 943
        },
        {
          "x": 2277,
          "y": 1842
        },
        {
          "x": 1277,
          "y": 1842
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='135' style='font-size:16px'>Limitations. While SAM performs well in general, it is<br>not perfect. It can miss fine structures, hallucinates small<br>disconnected components at times, and does not produce<br>boundaries as crisply as more computationally intensive<br>methods that \"zoom-in' , [18]. In general, we expect<br>, e.g.<br>dedicated interactive segmentation methods to outperform<br>SAM when many points are provided, e.g. [67]. Unlike<br>these methods, SAM is designed for generality and breadth<br>of use rather than high IoU interactive segmentation. More-<br>over, SAM can process prompts in real-time, but neverthe-<br>less SAM's overall performance is not real-time when using<br>a heavy image encoder. Our foray into the text-to-mask task<br>is exploratory and not entirely robust, although we believe<br>it can be improved with more effort. While SAM can per-<br>form many tasks, itis unclear how to design simple prompts<br>that implement semantic and panoptic segmentation. Fi-<br>nally, there are domain-specific tools, such as [7], that we<br>expect to outperform SAM in their respective domains.</p>",
      "id": 135,
      "page": 12,
      "text": "Limitations. While SAM performs well in general, it is\nnot perfect. It can miss fine structures, hallucinates small\ndisconnected components at times, and does not produce\nboundaries as crisply as more computationally intensive\nmethods that \"zoom-in' , [18]. In general, we expect\n, e.g.\ndedicated interactive segmentation methods to outperform\nSAM when many points are provided, e.g. [67]. Unlike\nthese methods, SAM is designed for generality and breadth\nof use rather than high IoU interactive segmentation. More-\nover, SAM can process prompts in real-time, but neverthe-\nless SAM's overall performance is not real-time when using\na heavy image encoder. Our foray into the text-to-mask task\nis exploratory and not entirely robust, although we believe\nit can be improved with more effort. While SAM can per-\nform many tasks, itis unclear how to design simple prompts\nthat implement semantic and panoptic segmentation. Fi-\nnally, there are domain-specific tools, such as [7], that we\nexpect to outperform SAM in their respective domains."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 1862
        },
        {
          "x": 2277,
          "y": 1862
        },
        {
          "x": 2277,
          "y": 2308
        },
        {
          "x": 1280,
          "y": 2308
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='136' style='font-size:16px'>Conclusion. The Segment Anything project is an attempt to<br>lift image segmentation into the era of foundation models.<br>Our principal contributions are a new task (promptable seg-<br>mentation), model (SAM), and dataset (SA-1B) that make<br>this leap possible. Whether SAM achieves the status of a<br>foundation model remains to be seen by how it is used in<br>the community, but regardless we expect the perspective of<br>this work, the release of over 1B masks, and our promptable<br>segmentation model will help pave the path ahead.</p>",
      "id": 136,
      "page": 12,
      "text": "Conclusion. The Segment Anything project is an attempt to\nlift image segmentation into the era of foundation models.\nOur principal contributions are a new task (promptable seg-\nmentation), model (SAM), and dataset (SA-1B) that make\nthis leap possible. Whether SAM achieves the status of a\nfoundation model remains to be seen by how it is used in\nthe community, but regardless we expect the perspective of\nthis work, the release of over 1B masks, and our promptable\nsegmentation model will help pave the path ahead."
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 2330
        },
        {
          "x": 2275,
          "y": 2330
        },
        {
          "x": 2275,
          "y": 2978
        },
        {
          "x": 1278,
          "y": 2978
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='137' style='font-size:18px'>Acknowledgments. We would like to thank Aaron Ad-<br>cock and Jitendra Malik for helpful discussion. We thank<br>Vaibhav Aggarwal and Yanghao Li for help with scal-<br>ing the model. We thank Cheng-Yang Fu, Jiabo Hu, and<br>Robert Kuo for help with data annotation platform. We<br>thank Allen Goodman and Bram Wasti for help in optimiz-<br>ing web-version of our model. Finally, we thank Morteza<br>Behrooz, Ashley Gabriel, Ahuva Goldstand, Sumanth Gur-<br>ram, Somya Jain, Devansh Kukreja, Joshua Lane, Lilian<br>Luong, Mallika Malhotra, William Ngan, Omkar Parkhi,<br>Nikhil Raina, Dirk Rowe, Neil Sejoor, Vanessa Stark, Bala<br>Varadarajan, and Zachary Winstrom for their help in mak-<br>ing the demo, dataset viewer, and other assets and tooling.</p>",
      "id": 137,
      "page": 12,
      "text": "Acknowledgments. We would like to thank Aaron Ad-\ncock and Jitendra Malik for helpful discussion. We thank\nVaibhav Aggarwal and Yanghao Li for help with scal-\ning the model. We thank Cheng-Yang Fu, Jiabo Hu, and\nRobert Kuo for help with data annotation platform. We\nthank Allen Goodman and Bram Wasti for help in optimiz-\ning web-version of our model. Finally, we thank Morteza\nBehrooz, Ashley Gabriel, Ahuva Goldstand, Sumanth Gur-\nram, Somya Jain, Devansh Kukreja, Joshua Lane, Lilian\nLuong, Mallika Malhotra, William Ngan, Omkar Parkhi,\nNikhil Raina, Dirk Rowe, Neil Sejoor, Vanessa Stark, Bala\nVaradarajan, and Zachary Winstrom for their help in mak-\ning the demo, dataset viewer, and other assets and tooling."
    },
    {
      "bounding_box": [
        {
          "x": 1218,
          "y": 3052
        },
        {
          "x": 1264,
          "y": 3052
        },
        {
          "x": 1264,
          "y": 3094
        },
        {
          "x": 1218,
          "y": 3094
        }
      ],
      "category": "footer",
      "html": "<footer id='138' style='font-size:16px'>12</footer>",
      "id": 138,
      "page": 12,
      "text": "12"
    },
    {
      "bounding_box": [
        {
          "x": 205,
          "y": 303
        },
        {
          "x": 444,
          "y": 303
        },
        {
          "x": 444,
          "y": 352
        },
        {
          "x": 205,
          "y": 352
        }
      ],
      "category": "paragraph",
      "html": "<p id='139' style='font-size:20px'>References</p>",
      "id": 139,
      "page": 13,
      "text": "References"
    },
    {
      "bounding_box": [
        {
          "x": 231,
          "y": 369
        },
        {
          "x": 1200,
          "y": 369
        },
        {
          "x": 1200,
          "y": 2973
        },
        {
          "x": 231,
          "y": 2973
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='140' style='font-size:14px'>[1] Edward H Adelson. On seeing stuff: the perception of materials by<br>humans and machines. Human vision and electronic imaging VI,<br>2001. 5<br>[2] Bogdan Alexe, Thomas Deselaers, and Vittorio Ferrari. What is an<br>object? CVPR, 2010. 4, 10<br>[3] Pablo Arbelaez, Michael Maire, Charless Fowlkes, and Jitendra<br>Malik. Contour detection and hierarchical image segmentation.<br>TPAMI, 2010. 4, 10, 21, 28<br>[4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer<br>normalization. arXiv: 1607. 06450, 2016. 16<br>[5] Hangbo Bao, Li Dong, and Furu Wei. BEiT: BERT pre-training of<br>image transformers. arXiv:2106.08254, 2021. 17<br>[6] Dina Bashkirova, Mohamed Abdelfattah, Ziliang Zhu, James Akl,<br>Fadi Alladkani, Ping Hu, Vitaly Ablavsky, Berk Calli, Sarah Adel<br>Bargal, and Kate Saenko. ZeroWaste dataset: Towards deformable<br>object segmentation in cluttered scenes. CVPR, 2022. 9, 20<br>[7] Stuart Berg, Dominik Kutra, Thorben Kroeger, Christoph N.<br>Straehle, Bernhard X. Kausler, Carsten Haubold, Martin Schiegg,<br>Janez Ales, Thorsten Beier, Markus Rudy, Kemal Eren, Jaime I.<br>Cervantes, Buote Xu, Fynn Beuttenmueller, Adrian Wolny, Chong<br>Zhang, Ullrich Koethe, Fred A. Hamprecht, and Anna Kreshuk.<br>ilastik: interactive machine learning for (bio)image analysis. Na-<br>ture Methods, 2019. 12<br>[8] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman,<br>Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette<br>Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportu-<br>nities and risks of foundation models. arXiv:2108.07258, 2021. 1,<br>12<br>[9] Gustav Bredell, Christine Tanner, and Ender Konukoglu. Iterative<br>interaction training for segmentation editing networks. MICCAI,<br>2018. 17<br>[10] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,<br>Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav<br>Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel<br>Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child,<br>Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris<br>Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Ben-<br>jamin Chess, Jack Clark, Christopher Berner, Sam McCandlish,<br>Alec Radford, Ilya Sutskever, and Dario Amodei. Language models<br>are few-shot learners. NeurIPS, 2020. 1, 4<br>[11] Zhaowei Cai and Nuno Vasconcelos. Cascade R-CNN: Delving into<br>high quality object detection. CVPR, 2018. 10<br>[12] Juan C. Caicedo, Allen Goodman, Kyle W. Karhohs, Beth A. Ci-<br>mini, Jeanelle Ackerman, Marzieh Haghighi, CherKeng Heng, Tim<br>Becker, Minh Doan, Claire McQuin, Mohammad Rohban, Shan-<br>tanu Singh, and Anne E. Carpenter. Nucleus segmentation across<br>imaging experiments: the 2018 data science bowl. Nature Methods,<br>2019. 9, 19, 20<br>[13] John Canny. A computational approach to edge detection. TPAMI,<br>1986. 10, 21<br>[14] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas<br>Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end<br>object detection with Transformers. ECCV, 2020. 5, 16, 17<br>[15] Guillaume Charpiat, Matthias Hofmann, and Bernhard Scholkopf.<br>Automatic image colorization via multimodal predictions. ECCV,<br>2008. 5, 17<br>[16] Neelima Chavali, Harsh Agrawal, Aroma Mahendru, and Dhruv<br>Batra. Object-proposal evaluation protocol is' gameable'. CVPR,<br>2016. 10, 21<br>[17] Jiazhou Chen, Yanghui Xu, Shufang Lu, Ronghua Liang, and Lian-<br>gliang Nan. 3D instance segmentation of MVS buildings. IEEE<br>Transactions on Geoscience and Remote Sensing, 2022. 9, 19, 20,<br>23, 24<br>[18] Xi Chen, Zhiyan Zhao, Yilei Zhang, Manni Duan, Donglian Qi, and<br>Hengshuang Zhao. FocalClick: towards practical interactive image<br>segmentation. CVPR, 2022. 8, 9, 12, 19</p>",
      "id": 140,
      "page": 13,
      "text": "[1] Edward H Adelson. On seeing stuff: the perception of materials by\nhumans and machines. Human vision and electronic imaging VI,\n2001. 5\n[2] Bogdan Alexe, Thomas Deselaers, and Vittorio Ferrari. What is an\nobject? CVPR, 2010. 4, 10\n[3] Pablo Arbelaez, Michael Maire, Charless Fowlkes, and Jitendra\nMalik. Contour detection and hierarchical image segmentation.\nTPAMI, 2010. 4, 10, 21, 28\n[4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer\nnormalization. arXiv: 1607. 06450, 2016. 16\n[5] Hangbo Bao, Li Dong, and Furu Wei. BEiT: BERT pre-training of\nimage transformers. arXiv:2106.08254, 2021. 17\n[6] Dina Bashkirova, Mohamed Abdelfattah, Ziliang Zhu, James Akl,\nFadi Alladkani, Ping Hu, Vitaly Ablavsky, Berk Calli, Sarah Adel\nBargal, and Kate Saenko. ZeroWaste dataset: Towards deformable\nobject segmentation in cluttered scenes. CVPR, 2022. 9, 20\n[7] Stuart Berg, Dominik Kutra, Thorben Kroeger, Christoph N.\nStraehle, Bernhard X. Kausler, Carsten Haubold, Martin Schiegg,\nJanez Ales, Thorsten Beier, Markus Rudy, Kemal Eren, Jaime I.\nCervantes, Buote Xu, Fynn Beuttenmueller, Adrian Wolny, Chong\nZhang, Ullrich Koethe, Fred A. Hamprecht, and Anna Kreshuk.\nilastik: interactive machine learning for (bio)image analysis. Na-\nture Methods, 2019. 12\n[8] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman,\nSimran Arora, Sydney von Arx, Michael S Bernstein, Jeannette\nBohg, Antoine Bosselut, Emma Brunskill, et al. On the opportu-\nnities and risks of foundation models. arXiv:2108.07258, 2021. 1,\n12\n[9] Gustav Bredell, Christine Tanner, and Ender Konukoglu. Iterative\ninteraction training for segmentation editing networks. MICCAI,\n2018. 17\n[10] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,\nJared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav\nShyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris\nHesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Ben-\njamin Chess, Jack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei. Language models\nare few-shot learners. NeurIPS, 2020. 1, 4\n[11] Zhaowei Cai and Nuno Vasconcelos. Cascade R-CNN: Delving into\nhigh quality object detection. CVPR, 2018. 10\n[12] Juan C. Caicedo, Allen Goodman, Kyle W. Karhohs, Beth A. Ci-\nmini, Jeanelle Ackerman, Marzieh Haghighi, CherKeng Heng, Tim\nBecker, Minh Doan, Claire McQuin, Mohammad Rohban, Shan-\ntanu Singh, and Anne E. Carpenter. Nucleus segmentation across\nimaging experiments: the 2018 data science bowl. Nature Methods,\n2019. 9, 19, 20\n[13] John Canny. A computational approach to edge detection. TPAMI,\n1986. 10, 21\n[14] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end\nobject detection with Transformers. ECCV, 2020. 5, 16, 17\n[15] Guillaume Charpiat, Matthias Hofmann, and Bernhard Scholkopf.\nAutomatic image colorization via multimodal predictions. ECCV,\n2008. 5, 17\n[16] Neelima Chavali, Harsh Agrawal, Aroma Mahendru, and Dhruv\nBatra. Object-proposal evaluation protocol is' gameable'. CVPR,\n2016. 10, 21\n[17] Jiazhou Chen, Yanghui Xu, Shufang Lu, Ronghua Liang, and Lian-\ngliang Nan. 3D instance segmentation of MVS buildings. IEEE\nTransactions on Geoscience and Remote Sensing, 2022. 9, 19, 20,\n23, 24\n[18] Xi Chen, Zhiyan Zhao, Yilei Zhang, Manni Duan, Donglian Qi, and\nHengshuang Zhao. FocalClick: towards practical interactive image\nsegmentation. CVPR, 2022. 8, 9, 12, 19"
    },
    {
      "bounding_box": [
        {
          "x": 1297,
          "y": 299
        },
        {
          "x": 2289,
          "y": 299
        },
        {
          "x": 2289,
          "y": 2969
        },
        {
          "x": 1297,
          "y": 2969
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='141' style='font-size:14px'>[19] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kir-<br>illov, and Rohit Girdhar. Masked-attention mask transformer for<br>universal image segmentation. CVPR, 2022. 4<br>[20] Bowen Cheng, Alex Schwing, and Alexander Kirillov. Per-<br>pixel classification is not all you need for semantic segmentation.<br>NeurIPS, 2021. 5, 16, 17<br>[21] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten<br>Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won<br>Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM: Scaling<br>language modeling with pathways. arXiv:2204. 02311, 2022. 1<br>[22] Luca Ciampi, Carlos Santiago, Joao Costeira, Claudio Gennaro, and<br>Giuseppe Amato. Domain adaptation for traffic density estimation.<br>International Joint Conference on Computer Vision, Imaging and<br>Computer Graphics Theory and Applications, 2021. 9, 20<br>[23] Luca Ciampi, Carlos Santiago, Joao Costeira, Claudio Gennaro, and<br>Giuseppe Amato. Night and day instance segmented park (NDIS-<br>Park) dataset: a collection of images taken by day and by night for<br>vehicle detection, segmentation and counting in parking areas. Zen-<br>odo, 2022. 9, 20<br>[24] Nadav Cohen, Yael Newman, and Ariel Shamir. Semantic segmen-<br>tation in art paintings. Computer Graphics Forum, 2022. 9, 19, 20,<br>23, 24<br>[25] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld,<br>Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth,<br>and Bernt Schiele. The Cityscapes dataset for semantic urban scene<br>understanding. CVPR, 2016. 9, 19, 20<br>[26] Bruno da Silva, George Konidaris, and Andrew Barto. Learning<br>parameterized skills. ICML, 2012. 4<br>[27] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino<br>Furnari, Jian Ma, Evangelos Kazakos, Davide Moltisanti, Jonathan<br>Munro, Toby Perrett, Will Price, and Michael Wray. Rescaling<br>egocentric vision: Collection, pipeline and challenges for EPIC-<br>KITCHENS-100. IJCV, 2022. 9, 20, 23, 24<br>[28] Ahmad Darkhalil, Dandan Shan, Bin Zhu, Jian Ma, Amlan Kar,<br>Richard Higgins, Sanja Fidler, David Fouhey, and Dima Damen.<br>EPIC-KITCHENS VISOR benchmark: Video segmentations and<br>object relations. NeurIPS, 2022. 9, 19, 20, 23, 24<br>[29] Terrance De Vries, Ishan Misra, Changhan Wang, and Laurens<br>Van der Maaten. Does object recognition work for everyone? CVPR<br>workshops, 2019. 18<br>[30] Mark Diaz, Ian Kivlichan, Rachel Rosen, Dylan Baker, Razvan<br>Amironesei, Vinodkumar Prabhakaran, and Emily Denton. Crowd-<br>WorkSheets: Accounting for individual and collective identities un-<br>derlying crowdsourced dataset annotation. ACM Conference on<br>Fairness, Accountability, and Transparency, 2022. 25<br>[31] Henghui Ding, Scott Cohen, Brian Price, and Xudong Jiang.<br>PhraseClick: toward achieving flexible interactive segmentation by<br>phrase and click. ECCV, 2020. 11<br>[32] Piotr Dollar and C Lawrence Zitnick. Fast edge detection using<br>structured forests. TPAMI, 2014. 21<br>[33] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk<br>Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa De-<br>hghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob<br>Uszkoreit, and Neil Houlsby. An image is worth 16x16 words:<br>Transformers for image recognition at scale. ICLR, 2021. 5, 8,<br>16<br>[34] Alireza Fathi, Xiaofeng Ren, and James M. Rehg. Learning to rec-<br>ognize objects in egocentric activities. CVPR, 2011. 9, 19, 20<br>[35] Pedro F Felzenszwalb and Daniel P Huttenlocher. Efficient graph-<br>based image segmentation. IJCV, 2004. 10<br>[36] Thomas B. Fitzpatrick. The validity and practicality of sun-reactive<br>skin types i through vi. Archives of Dermatology, 1988. 8<br>[37] Marco Forte, Brian Price, Scott Cohen, Ning Xu, and Fran�ois<br>Pitie. Getting to 99% accuracy in interactive segmentation.<br>arXiv:2003.07932, 2020. 5, 17<br>[38] Jean-Michel Fortin, Olivier Gamache, Vincent Grondin, Fran�ois<br>Pomerleau, and Philippe Giguere. Instance segmentation for au-<br>tonomous log grasping in forestry operations. IROS, 2022. 9, 20</p>",
      "id": 141,
      "page": 13,
      "text": "[19] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kir-\nillov, and Rohit Girdhar. Masked-attention mask transformer for\nuniversal image segmentation. CVPR, 2022. 4\n[20] Bowen Cheng, Alex Schwing, and Alexander Kirillov. Per-\npixel classification is not all you need for semantic segmentation.\nNeurIPS, 2021. 5, 16, 17\n[21] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten\nBosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won\nChung, Charles Sutton, Sebastian Gehrmann, et al. PaLM: Scaling\nlanguage modeling with pathways. arXiv:2204. 02311, 2022. 1\n[22] Luca Ciampi, Carlos Santiago, Joao Costeira, Claudio Gennaro, and\nGiuseppe Amato. Domain adaptation for traffic density estimation.\nInternational Joint Conference on Computer Vision, Imaging and\nComputer Graphics Theory and Applications, 2021. 9, 20\n[23] Luca Ciampi, Carlos Santiago, Joao Costeira, Claudio Gennaro, and\nGiuseppe Amato. Night and day instance segmented park (NDIS-\nPark) dataset: a collection of images taken by day and by night for\nvehicle detection, segmentation and counting in parking areas. Zen-\nodo, 2022. 9, 20\n[24] Nadav Cohen, Yael Newman, and Ariel Shamir. Semantic segmen-\ntation in art paintings. Computer Graphics Forum, 2022. 9, 19, 20,\n23, 24\n[25] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld,\nMarkus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth,\nand Bernt Schiele. The Cityscapes dataset for semantic urban scene\nunderstanding. CVPR, 2016. 9, 19, 20\n[26] Bruno da Silva, George Konidaris, and Andrew Barto. Learning\nparameterized skills. ICML, 2012. 4\n[27] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino\nFurnari, Jian Ma, Evangelos Kazakos, Davide Moltisanti, Jonathan\nMunro, Toby Perrett, Will Price, and Michael Wray. Rescaling\negocentric vision: Collection, pipeline and challenges for EPIC-\nKITCHENS-100. IJCV, 2022. 9, 20, 23, 24\n[28] Ahmad Darkhalil, Dandan Shan, Bin Zhu, Jian Ma, Amlan Kar,\nRichard Higgins, Sanja Fidler, David Fouhey, and Dima Damen.\nEPIC-KITCHENS VISOR benchmark: Video segmentations and\nobject relations. NeurIPS, 2022. 9, 19, 20, 23, 24\n[29] Terrance De Vries, Ishan Misra, Changhan Wang, and Laurens\nVan der Maaten. Does object recognition work for everyone? CVPR\nworkshops, 2019. 18\n[30] Mark Diaz, Ian Kivlichan, Rachel Rosen, Dylan Baker, Razvan\nAmironesei, Vinodkumar Prabhakaran, and Emily Denton. Crowd-\nWorkSheets: Accounting for individual and collective identities un-\nderlying crowdsourced dataset annotation. ACM Conference on\nFairness, Accountability, and Transparency, 2022. 25\n[31] Henghui Ding, Scott Cohen, Brian Price, and Xudong Jiang.\nPhraseClick: toward achieving flexible interactive segmentation by\nphrase and click. ECCV, 2020. 11\n[32] Piotr Dollar and C Lawrence Zitnick. Fast edge detection using\nstructured forests. TPAMI, 2014. 21\n[33] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk\nWeissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa De-\nhghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob\nUszkoreit, and Neil Houlsby. An image is worth 16x16 words:\nTransformers for image recognition at scale. ICLR, 2021. 5, 8,\n16\n[34] Alireza Fathi, Xiaofeng Ren, and James M. Rehg. Learning to rec-\nognize objects in egocentric activities. CVPR, 2011. 9, 19, 20\n[35] Pedro F Felzenszwalb and Daniel P Huttenlocher. Efficient graph-\nbased image segmentation. IJCV, 2004. 10\n[36] Thomas B. Fitzpatrick. The validity and practicality of sun-reactive\nskin types i through vi. Archives of Dermatology, 1988. 8\n[37] Marco Forte, Brian Price, Scott Cohen, Ning Xu, and Fran�ois\nPitie. Getting to 99% accuracy in interactive segmentation.\narXiv:2003.07932, 2020. 5, 17\n[38] Jean-Michel Fortin, Olivier Gamache, Vincent Grondin, Fran�ois\nPomerleau, and Philippe Giguere. Instance segmentation for au-\ntonomous log grasping in forestry operations. IROS, 2022. 9, 20"
    },
    {
      "bounding_box": [
        {
          "x": 1219,
          "y": 3052
        },
        {
          "x": 1262,
          "y": 3052
        },
        {
          "x": 1262,
          "y": 3093
        },
        {
          "x": 1219,
          "y": 3093
        }
      ],
      "category": "footer",
      "html": "<footer id='142' style='font-size:16px'>13</footer>",
      "id": 142,
      "page": 13,
      "text": "13"
    },
    {
      "bounding_box": [
        {
          "x": 227,
          "y": 299
        },
        {
          "x": 1202,
          "y": 299
        },
        {
          "x": 1202,
          "y": 2978
        },
        {
          "x": 227,
          "y": 2978
        }
      ],
      "category": "paragraph",
      "html": "<p id='143' style='font-size:16px'>[39] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jen-<br>nifer Wortman Vaughan, Hanna Wallach, Hal Daume Iii, and Kate<br>Crawford. Datasheets for datasets. Communications of the ACM,<br>2021. 25<br>[40] Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin,<br>Ekin D Cubuk, Quoc V Le, and Barret Zoph. Simple copy-paste is a<br>strong data augmentation method for instance segmentation. CVPR,<br>2021. 16, 18, 22<br>[41] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik.<br>Rich feature hierarchies for accurate object detection and semantic<br>segmentation. CVPR, 2014. 10<br>[42] Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, Lukasz<br>Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and<br>Kaiming He. Accurate, large minibatch SGD: Training ImageNet<br>in 1 hour. arXiv:1706.02677, 2017. 17<br>[43] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary<br>Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger,<br>Hao Jiang, Miao Liu, Xingyu Liu, Miguel Martin, Tushar Na-<br>garajan, Ilija Radosavovic, Santhosh Kumar Ramakrishnan, Fiona<br>Ryan, Jayant Sharma, Michael Wray, Mengmeng Xu, Eric Zhong-<br>cong Xu, Chen Zhao, Siddhant Bansal, Dhruv Batra, Vincent Car-<br>tillier, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli,<br>Christoph Feichtenhofer, Adriano Fragomeni, Qichen Fu, Chris-<br>tian Fuegen, Abrham Gebreselasie, Cristina Gonzalez, James Hillis,<br>Xuhua Huang, Yifei Huang, Wenqi Jia, Weslie Khoo, Jachym Ko-<br>lar, Satwik Kottur, Anurag Kumar, Federico Landini, Chao Li,<br>Yanghao Li, Zhenqiang Li, Karttikeya Mangalam, Raghava Mod-<br>hugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, Will<br>Price, Paola Ruiz Puentes, Merey Ramazanova, Leda Sari, Kiran<br>Somasundaram, Audrey Southerland, Yusuke Sugano, Ruijie Tao,<br>Minh Vo, Yuchen Wang, Xindi Wu, Takuma Yagi, Yunyi Zhu,<br>Pablo Arbelaez, David Crandall, Dima Damen, Giovanni Maria<br>Farinella, Bernard Ghanem, Vamsi Krishna Ithapu, C. V. Jawahar,<br>Hanbyul Joo, Kris Kitani, Haizhou Li, Richard Newcombe, Aude<br>Oliva, Hyun Soo Park, James M. Rehg, Yoichi Sato, Jianbo Shi,<br>Mike Zheng Shou, Antonio Torralba, Lorenzo Torresani, Mingfei<br>Yan, and Jitendra Malik. Ego4D: Around the World in 3,000 Hours<br>of Egocentric Video. CVPR, 2022. 20<br>[44] Agrim Gupta, Piotr Dollar, and Ross Girshick. LVIS: A dataset for<br>large vocabulary instance segmentation. CVPR, 2019. 2, 6, 7, 9, 10,<br>11, 19, 20, 21, 24<br>[45] Abner Guzman-Rivera, Dhruv Batra, and Pushmeet Kohli. Multiple<br>choice learning: Learning to produce multiple structured outputs.<br>NeurIPS, 2012. 5, 17<br>[46] Timm Haucke, Hjalmar S. K�hl, and Volker Steinhage.<br>SOCRATES: Introducing depth in visual wildlife monitoring using<br>stereo vision. Sensors, 2022. 9, 20<br>[47] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar,<br>and Ross Girshick. Masked autoencoders are scalable vision learn-<br>ers. CVPR, 2022. 5, 8, 12, 16, 17<br>[48] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick.<br>Mask R-CNN. ICCV, 2017. 10<br>[49] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep<br>residual learning for image recognition. CVPR, 2016. 16<br>[50] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units<br>(gelus). arXiv:1606.08415, 2016. 16<br>[51] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena<br>Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas,<br>Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training<br>compute-optimal large language models. arXiv:2203.15556, 2022.<br>1<br>[52] Jungseok Hong, Michael Fulton, and Junaed Sattar. TrashCan: A<br>semantically-segmented dataset towards visual detection of marine<br>debris. arXiv:2007.08097, 2020. 9, 19, 20<br>[53] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Wein-<br>berger. Deep networks with stochastic depth. ECCV, 2016. 17<br>[54] Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita Orlov,<br>and Humphrey Shi. Oneformer: One transformer to rule universal<br>image segmentation. arXiv:2211.06220, 2022. 4</p>",
      "id": 143,
      "page": 14,
      "text": "[39] Timnit Gebru, Jamie Morgenstern, Briana Vecchione, Jen-\nnifer Wortman Vaughan, Hanna Wallach, Hal Daume Iii, and Kate\nCrawford. Datasheets for datasets. Communications of the ACM,\n2021. 25\n[40] Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin,\nEkin D Cubuk, Quoc V Le, and Barret Zoph. Simple copy-paste is a\nstrong data augmentation method for instance segmentation. CVPR,\n2021. 16, 18, 22\n[41] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik.\nRich feature hierarchies for accurate object detection and semantic\nsegmentation. CVPR, 2014. 10\n[42] Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, Lukasz\nWesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and\nKaiming He. Accurate, large minibatch SGD: Training ImageNet\nin 1 hour. arXiv:1706.02677, 2017. 17\n[43] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary\nChavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger,\nHao Jiang, Miao Liu, Xingyu Liu, Miguel Martin, Tushar Na-\ngarajan, Ilija Radosavovic, Santhosh Kumar Ramakrishnan, Fiona\nRyan, Jayant Sharma, Michael Wray, Mengmeng Xu, Eric Zhong-\ncong Xu, Chen Zhao, Siddhant Bansal, Dhruv Batra, Vincent Car-\ntillier, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli,\nChristoph Feichtenhofer, Adriano Fragomeni, Qichen Fu, Chris-\ntian Fuegen, Abrham Gebreselasie, Cristina Gonzalez, James Hillis,\nXuhua Huang, Yifei Huang, Wenqi Jia, Weslie Khoo, Jachym Ko-\nlar, Satwik Kottur, Anurag Kumar, Federico Landini, Chao Li,\nYanghao Li, Zhenqiang Li, Karttikeya Mangalam, Raghava Mod-\nhugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, Will\nPrice, Paola Ruiz Puentes, Merey Ramazanova, Leda Sari, Kiran\nSomasundaram, Audrey Southerland, Yusuke Sugano, Ruijie Tao,\nMinh Vo, Yuchen Wang, Xindi Wu, Takuma Yagi, Yunyi Zhu,\nPablo Arbelaez, David Crandall, Dima Damen, Giovanni Maria\nFarinella, Bernard Ghanem, Vamsi Krishna Ithapu, C. V. Jawahar,\nHanbyul Joo, Kris Kitani, Haizhou Li, Richard Newcombe, Aude\nOliva, Hyun Soo Park, James M. Rehg, Yoichi Sato, Jianbo Shi,\nMike Zheng Shou, Antonio Torralba, Lorenzo Torresani, Mingfei\nYan, and Jitendra Malik. Ego4D: Around the World in 3,000 Hours\nof Egocentric Video. CVPR, 2022. 20\n[44] Agrim Gupta, Piotr Dollar, and Ross Girshick. LVIS: A dataset for\nlarge vocabulary instance segmentation. CVPR, 2019. 2, 6, 7, 9, 10,\n11, 19, 20, 21, 24\n[45] Abner Guzman-Rivera, Dhruv Batra, and Pushmeet Kohli. Multiple\nchoice learning: Learning to produce multiple structured outputs.\nNeurIPS, 2012. 5, 17\n[46] Timm Haucke, Hjalmar S. K�hl, and Volker Steinhage.\nSOCRATES: Introducing depth in visual wildlife monitoring using\nstereo vision. Sensors, 2022. 9, 20\n[47] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar,\nand Ross Girshick. Masked autoencoders are scalable vision learn-\ners. CVPR, 2022. 5, 8, 12, 16, 17\n[48] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick.\nMask R-CNN. ICCV, 2017. 10\n[49] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep\nresidual learning for image recognition. CVPR, 2016. 16\n[50] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units\n(gelus). arXiv:1606.08415, 2016. 16\n[51] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena\nBuchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas,\nLisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training\ncompute-optimal large language models. arXiv:2203.15556, 2022.\n1\n[52] Jungseok Hong, Michael Fulton, and Junaed Sattar. TrashCan: A\nsemantically-segmented dataset towards visual detection of marine\ndebris. arXiv:2007.08097, 2020. 9, 19, 20\n[53] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Wein-\nberger. Deep networks with stochastic depth. ECCV, 2016. 17\n[54] Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita Orlov,\nand Humphrey Shi. Oneformer: One transformer to rule universal\nimage segmentation. arXiv:2211.06220, 2022. 4"
    },
    {
      "bounding_box": [
        {
          "x": 1295,
          "y": 305
        },
        {
          "x": 2290,
          "y": 305
        },
        {
          "x": 2290,
          "y": 2972
        },
        {
          "x": 1295,
          "y": 2972
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='144' style='font-size:14px'>[55] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,<br>Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig.<br>Scaling up visual and vision-language representation learning with<br>noisy text supervision. ICML, 2021. 1<br>[56] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown,<br>Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey<br>Wu, and Dario Amodei. Scaling laws for neural language models.<br>arXiv:2001.08361, 2020. 1<br>[57] Michael Kass, Andrew Witkin, and Demetri Terzopoulos. Snakes:<br>Active contour models. IJCV, 1988. 4<br>[58] Dahun Kim, Tsung-Yi Lin, Anelia Angelova, In So Kweon, and<br>Weicheng Kuo. Learning open-world object proposals without<br>learning to classify. IEEE Robotics and Automation Letters, 2022.<br>21<br>[59] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother,<br>and Piotr Dollar. Panoptic segmentation. CVPR, 2019. 4<br>[60] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan<br>Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo<br>Malloci, Alexander Kolesnikov, Tom Duerig, and Vittorio Ferrari.<br>The open images dataset v4: Unified image classification, object<br>detection, and visual relationship detection at scale. IJCV, 2020. 2,<br>6, 7, 18, 19<br>[61] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and<br>Thomas Dandres. Quantifying the carbon emissions of machine<br>learning. arXiv:1910.09700, 2019. 28<br>[62] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Explor-<br>ing plain vision transformer backbones for object detection. ECCV,<br>2022. 5, 10, 11, 16, 21, 23, 24<br>[63] Yin Li, Zhefan Ye, and James M. Rehg. Delving into egocentric<br>actions. CVPR, 2015. 9, 20<br>[64] Zhuwen Li, Qifeng Chen, and Vladlen Koltun. Interactive image<br>segmentation with latent diversity. CVPR, 2018. 5, 17, 19<br>[65] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr<br>Dollar. Focal loss for dense object detection. ICCV, 2017. 5, 17<br>[66] Tsung- Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro<br>Perona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Mi-<br>crosoft COCO: Common objects in context. ECCV, 2014. 2, 4, 6,<br>7, 11, 18, 19, 20<br>[67] Qin Liu, Zhenlin Xu, Gedas Bertasius, and Marc Niethammer. Sim-<br>pleClick: Interactive image segmentation with simple vision trans-<br>formers. arXiv:2210.11006, 2022. 8, 9, 12, 19<br>[68] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regu-<br>larization. ICLR, 2019. 17<br>[69] Cathy H Lucas, Daniel OB Jones, Catherine J Hollyhead, Robert H<br>Condon, Carlos M Duarte, William M Graham, Kelly L Robinson,<br>Kylie A Pitt, Mark Schildhauer, and Jim Regetz. Gelatinous ZOO-<br>plankton biomass in the global oceans: geographic variation and<br>environmental drivers. Global Ecology and Biogeography, 2014.<br>20<br>[70] Sabarinath Mahadevan, Paul Voigtlaender, and Bastian Leibe. Iter-<br>atively trained interactive segmentation. BMVC, 2018. 4, 17<br>[71] Kevis-Kokitsi Maninis, Sergi Caelles, Jordi Pont-Tuset, and Luc<br>Van Gool. Deep extreme cut: From extreme points to object seg-<br>mentation. CVPR, 2018. 6<br>[72] David Martin, Charless Fowlkes, Doron Tal, and Jitendra Malik.<br>A database of human segmented natural images and its applica-<br>tion to evaluating segmentation algorithms and measuring ecologi-<br>cal statistics. ICCV, 2001. 10, 21, 28<br>[73] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-Net:<br>Fully convolutional neural networks for volumetric medical image<br>segmentation. 3DV, 2016. 5, 17<br>[74] Massimo Minervini, Andreas Fischbach, Hanno Scharr, and<br>Sotirios A. Tsaftaris. Finely-grained annotated datasets for image-<br>based plant phenotyping. Pattern Recognition Letters, 2016. 9, 20<br>[75] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes,<br>Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Debo-<br>rah Raji, and Timnit Gebru. Model cards for model reporting. Pro-<br>ceedings of the conference on fairness, accountability, and trans-<br>parency, 2019. 25, 28</p>",
      "id": 144,
      "page": 14,
      "text": "[55] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,\nHieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig.\nScaling up visual and vision-language representation learning with\nnoisy text supervision. ICML, 2021. 1\n[56] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown,\nBenjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey\nWu, and Dario Amodei. Scaling laws for neural language models.\narXiv:2001.08361, 2020. 1\n[57] Michael Kass, Andrew Witkin, and Demetri Terzopoulos. Snakes:\nActive contour models. IJCV, 1988. 4\n[58] Dahun Kim, Tsung-Yi Lin, Anelia Angelova, In So Kweon, and\nWeicheng Kuo. Learning open-world object proposals without\nlearning to classify. IEEE Robotics and Automation Letters, 2022.\n21\n[59] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother,\nand Piotr Dollar. Panoptic segmentation. CVPR, 2019. 4\n[60] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan\nKrasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo\nMalloci, Alexander Kolesnikov, Tom Duerig, and Vittorio Ferrari.\nThe open images dataset v4: Unified image classification, object\ndetection, and visual relationship detection at scale. IJCV, 2020. 2,\n6, 7, 18, 19\n[61] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and\nThomas Dandres. Quantifying the carbon emissions of machine\nlearning. arXiv:1910.09700, 2019. 28\n[62] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Explor-\ning plain vision transformer backbones for object detection. ECCV,\n2022. 5, 10, 11, 16, 21, 23, 24\n[63] Yin Li, Zhefan Ye, and James M. Rehg. Delving into egocentric\nactions. CVPR, 2015. 9, 20\n[64] Zhuwen Li, Qifeng Chen, and Vladlen Koltun. Interactive image\nsegmentation with latent diversity. CVPR, 2018. 5, 17, 19\n[65] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr\nDollar. Focal loss for dense object detection. ICCV, 2017. 5, 17\n[66] Tsung- Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro\nPerona, Deva Ramanan, Piotr Dollar, and C Lawrence Zitnick. Mi-\ncrosoft COCO: Common objects in context. ECCV, 2014. 2, 4, 6,\n7, 11, 18, 19, 20\n[67] Qin Liu, Zhenlin Xu, Gedas Bertasius, and Marc Niethammer. Sim-\npleClick: Interactive image segmentation with simple vision trans-\nformers. arXiv:2210.11006, 2022. 8, 9, 12, 19\n[68] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regu-\nlarization. ICLR, 2019. 17\n[69] Cathy H Lucas, Daniel OB Jones, Catherine J Hollyhead, Robert H\nCondon, Carlos M Duarte, William M Graham, Kelly L Robinson,\nKylie A Pitt, Mark Schildhauer, and Jim Regetz. Gelatinous ZOO-\nplankton biomass in the global oceans: geographic variation and\nenvironmental drivers. Global Ecology and Biogeography, 2014.\n20\n[70] Sabarinath Mahadevan, Paul Voigtlaender, and Bastian Leibe. Iter-\natively trained interactive segmentation. BMVC, 2018. 4, 17\n[71] Kevis-Kokitsi Maninis, Sergi Caelles, Jordi Pont-Tuset, and Luc\nVan Gool. Deep extreme cut: From extreme points to object seg-\nmentation. CVPR, 2018. 6\n[72] David Martin, Charless Fowlkes, Doron Tal, and Jitendra Malik.\nA database of human segmented natural images and its applica-\ntion to evaluating segmentation algorithms and measuring ecologi-\ncal statistics. ICCV, 2001. 10, 21, 28\n[73] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-Net:\nFully convolutional neural networks for volumetric medical image\nsegmentation. 3DV, 2016. 5, 17\n[74] Massimo Minervini, Andreas Fischbach, Hanno Scharr, and\nSotirios A. Tsaftaris. Finely-grained annotated datasets for image-\nbased plant phenotyping. Pattern Recognition Letters, 2016. 9, 20\n[75] Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes,\nLucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Debo-\nrah Raji, and Timnit Gebru. Model cards for model reporting. Pro-\nceedings of the conference on fairness, accountability, and trans-\nparency, 2019. 25, 28"
    },
    {
      "bounding_box": [
        {
          "x": 1220,
          "y": 3055
        },
        {
          "x": 1261,
          "y": 3055
        },
        {
          "x": 1261,
          "y": 3090
        },
        {
          "x": 1220,
          "y": 3090
        }
      ],
      "category": "footer",
      "html": "<footer id='145' style='font-size:20px'>14</footer>",
      "id": 145,
      "page": 14,
      "text": "14"
    },
    {
      "bounding_box": [
        {
          "x": 225,
          "y": 294
        },
        {
          "x": 1201,
          "y": 294
        },
        {
          "x": 1201,
          "y": 2975
        },
        {
          "x": 225,
          "y": 2975
        }
      ],
      "category": "paragraph",
      "html": "<p id='146' style='font-size:14px'>[76] Dim P Papadopoulos, Jasper RR Uijlings, Frank Keller, and Vittorio<br>Ferrari. Extreme clicking for efficient object annotation. ICCV,<br>2017. 6<br>[77] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-<br>Miquel Munguia, Daniel Rothchild, David So, Maud Texier, and<br>Jeff Dean. Carbon emissions and large neural network training.<br>arXiv:2104.10350, 2021. 28<br>[78] Matthew E Peters, Waleed Ammar, Chandra Bhagavatula, and Rus-<br>sell Power. Semi-supervised sequence tagging with bidirectional<br>language models. Proceedings of the 55th Annual Meeting of the<br>Association for Computational Linguistics, 2017. 18<br>[79] Mengyang Pu, Yaping Huang, Yuming Liu, Qingji Guan, and<br>Haibin Ling. EDTER: Edge detection with transformer. CVPR,<br>2022. 10<br>[80] Mattia Pugliatti and Francesco Topputo. DOORS: Dataset fOr<br>bOuldeRs Segmentation. Zenodo, 2022. 9, 20<br>[81] Jiyang Qi, Yan Gao, Yao Hu, Xinggang Wang, Xiaoyu Liu, Xiang<br>Bai, Serge Belongie, Alan Yuille, Philip Torr, and Song Bai. Oc-<br>cluded video instance segmentation: A benchmark. ICCV, 2022. 9,<br>20, 23, 24<br>[82] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,<br>Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell,<br>Pamela Mishkin, Jack Clark, et al. Learning transferable visual<br>models from natural language supervision. ICML, 2021. 1, 2, 4, 5,<br>8, 12, 16, 22<br>[83] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea<br>Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-<br>to-image generation. ICML, 2021. 1, 4, 12<br>[84] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster<br>R-CNN: Towards real-time object detection with region proposal<br>networks. NeurIPS, 2015. 6, 10<br>[85] Xiaofeng Ren and Jitendra Malik. Learning a classification model<br>for segmentation. ICCV, 2003. 4<br>[86] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar,<br>Miguel Angel Bautista, Nathan Paczan, Russ Webb, and Joshua M.<br>Susskind. Hypersim: A photorealistic synthetic dataset for holistic<br>indoor scene understanding. ICCV, 2021. 9, 19, 20<br>[87] Candice Schumann, Susanna Ricco, Utsav Prabhu, Vittorio Ferrari,<br>and Caroline Pantofaru. A step toward more inclusive people anno-<br>tations for fairness. Proceedings ofthe 2021 AAAI/ACM Conference<br>on AI, Ethics, and Society, 2021. 8, 19<br>[88] Sefik Ilkin Serengil and Alper Ozpinar. LightFace: A hybrid deep<br>face recognition framework. ASYU, 2020. 26<br>[89] Sefik Ilkin Serengil and Alper Ozpinar. HyperExtended LightFace:<br>A facial attribute analysis framework. ICEET, 2021. 26<br>[90] Jamie Shotton, John Winn, Carsten Rother, and Antonio Crimin-<br>isi. TextonBoost: Joint appearance, shape and context modeling for<br>mulit-class object recognition and segmentation. ECCV, 2006. 4<br>[91] Corey Snyder and Minh Do. STREETS: A novel camera network<br>dataset for traffic flow. NeurIPS, 2019. 9, 20<br>[92] Konstantin Sofiiuk, Ilya A Petrov, and Anton Konushin. Reviving<br>iterative training with mask guidance for interactive segmentation.<br>ICIP, 2022. 5, 8, 9, 17, 19, 23, 24, 28<br>[93] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya<br>Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to<br>prevent neural networks from overfitting. The Journal of Machine<br>Learning Research, 2014. 16<br>[94] Chris Stauffer and W Eric L Grimson. Adaptive background mix-<br>ture models for real-time tracking. CVPR, 1999. 4<br>[95] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara<br>Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ra-<br>mamoorthi, Jonathan Barron, and Ren Ng. Fourier features let net-<br>works learn high frequency functions in low dimensional domains.<br>NeurIPS, 2020. 5, 16<br>[96] Yansong Tang, Yi Tian, Jiwen Lu, Jianjiang Feng, and Jie Zhou.<br>Action recognition in RGB-D egocentric videos. ICIP, 2017. 20</p>",
      "id": 146,
      "page": 15,
      "text": "[76] Dim P Papadopoulos, Jasper RR Uijlings, Frank Keller, and Vittorio\nFerrari. Extreme clicking for efficient object annotation. ICCV,\n2017. 6\n[77] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-\nMiquel Munguia, Daniel Rothchild, David So, Maud Texier, and\nJeff Dean. Carbon emissions and large neural network training.\narXiv:2104.10350, 2021. 28\n[78] Matthew E Peters, Waleed Ammar, Chandra Bhagavatula, and Rus-\nsell Power. Semi-supervised sequence tagging with bidirectional\nlanguage models. Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics, 2017. 18\n[79] Mengyang Pu, Yaping Huang, Yuming Liu, Qingji Guan, and\nHaibin Ling. EDTER: Edge detection with transformer. CVPR,\n2022. 10\n[80] Mattia Pugliatti and Francesco Topputo. DOORS: Dataset fOr\nbOuldeRs Segmentation. Zenodo, 2022. 9, 20\n[81] Jiyang Qi, Yan Gao, Yao Hu, Xinggang Wang, Xiaoyu Liu, Xiang\nBai, Serge Belongie, Alan Yuille, Philip Torr, and Song Bai. Oc-\ncluded video instance segmentation: A benchmark. ICCV, 2022. 9,\n20, 23, 24\n[82] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,\nGabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell,\nPamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. ICML, 2021. 1, 2, 4, 5,\n8, 12, 16, 22\n[83] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea\nVoss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-\nto-image generation. ICML, 2021. 1, 4, 12\n[84] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster\nR-CNN: Towards real-time object detection with region proposal\nnetworks. NeurIPS, 2015. 6, 10\n[85] Xiaofeng Ren and Jitendra Malik. Learning a classification model\nfor segmentation. ICCV, 2003. 4\n[86] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar,\nMiguel Angel Bautista, Nathan Paczan, Russ Webb, and Joshua M.\nSusskind. Hypersim: A photorealistic synthetic dataset for holistic\nindoor scene understanding. ICCV, 2021. 9, 19, 20\n[87] Candice Schumann, Susanna Ricco, Utsav Prabhu, Vittorio Ferrari,\nand Caroline Pantofaru. A step toward more inclusive people anno-\ntations for fairness. Proceedings ofthe 2021 AAAI/ACM Conference\non AI, Ethics, and Society, 2021. 8, 19\n[88] Sefik Ilkin Serengil and Alper Ozpinar. LightFace: A hybrid deep\nface recognition framework. ASYU, 2020. 26\n[89] Sefik Ilkin Serengil and Alper Ozpinar. HyperExtended LightFace:\nA facial attribute analysis framework. ICEET, 2021. 26\n[90] Jamie Shotton, John Winn, Carsten Rother, and Antonio Crimin-\nisi. TextonBoost: Joint appearance, shape and context modeling for\nmulit-class object recognition and segmentation. ECCV, 2006. 4\n[91] Corey Snyder and Minh Do. STREETS: A novel camera network\ndataset for traffic flow. NeurIPS, 2019. 9, 20\n[92] Konstantin Sofiiuk, Ilya A Petrov, and Anton Konushin. Reviving\niterative training with mask guidance for interactive segmentation.\nICIP, 2022. 5, 8, 9, 17, 19, 23, 24, 28\n[93] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya\nSutskever, and Ruslan Salakhutdinov. Dropout: A simple way to\nprevent neural networks from overfitting. The Journal of Machine\nLearning Research, 2014. 16\n[94] Chris Stauffer and W Eric L Grimson. Adaptive background mix-\nture models for real-time tracking. CVPR, 1999. 4\n[95] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara\nFridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ra-\nmamoorthi, Jonathan Barron, and Ren Ng. Fourier features let net-\nworks learn high frequency functions in low dimensional domains.\nNeurIPS, 2020. 5, 16\n[96] Yansong Tang, Yi Tian, Jiwen Lu, Jianjiang Feng, and Jie Zhou.\nAction recognition in RGB-D egocentric videos. ICIP, 2017. 20"
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 303
        },
        {
          "x": 2288,
          "y": 303
        },
        {
          "x": 2288,
          "y": 2971
        },
        {
          "x": 1281,
          "y": 2971
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='147' style='font-size:14px'>[97] Yansong Tang, Zian Wang, Jiwen Lu, Jianjiang Feng, and Jie Zhou.<br>Multi-stream deep neural networks for RGB-D egocentric action<br>recognition. IEEE Transactions on Circuits and Systems for Video<br>Technology, 2019. 20<br>[98] The World Bank. The world by income and regions,<br>2022. https://canaopics.workbank.cog/wordedevelopment-<br>indicators/the-woild-by-ncome-and-regioal 18<br>[99] Sebastian Thrun. Is learning the n-th thing any easier than learning<br>the first? NeurIPS, 1995. 12<br>[100] Cameron Trotter, Georgia Atkinson, Matt Sharpe, Kirsten Richard-<br>son, A. Stephen McGough, Nick Wright, Ben Burville, and Per<br>Berggren. NDD20: A large-scale few-shot dolphin dataset for<br>coarse and fine-grained categorisation. arXiv:2005.13359, 2020.<br>9, 19, 20, 23, 24<br>[101] United States Environmental Protection Agency. Greenhouse Gas<br>Equivalencies Calculator. https://www.epa.govienergy/greenhouse-<br>gas-equivalencies-calculator, 2022. 28<br>[102] Koen EA van de Sande, Jasper RR Uijlings, Theo Gevers, and<br>Arnold WM Smeulders. Segmentation as selective search for ob-<br>ject recognition. ICCV, 2011. 10<br>[103] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,<br>Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin.<br>Attention is all you need. NeurIPS, 2017. 5, 16<br>[104] Boying Wang, Libo Zhang, Longyin Wen, Xianglong Liu, and Yan-<br>jun Wu. Towards real-world prohibited item detection: A large-<br>scale x-ray benchmark. CVPR, 2021. 9, 19, 20<br>[105] Weiyao Wang, Matt Feiszli, Heng Wang, Jitendra Malik, and<br>Du Tran. Open-world instance segmentation: Exploiting pseudo<br>ground truth from learned pairwise affinity. CVPR, 2022. 21<br>[106] Chao- Yuan Wu, Justin Johnson, Jitendra Malik, Christoph Feicht-<br>enhofer, and Georgia Gkioxari. Multiview compressive coding for<br>3D reconstruction. CVPR, 2023. 12<br>[107] Jianxiong Xiao, James Hays, Krista Ehinger, Aude Oliva, and An-<br>tonio Torralba. SUN database: Large-scale scene recognition from<br>abbey to zoo. CVPR, 2010. 20<br>[108] Saining Xie and Zhuowen Tu. Holistically-nested edge detection.<br>ICCV, 2015. 10<br>[109] Ning Xu, Brian Price, Scott Cohen, Jimei Yang, and Thomas S<br>Huang. Deep interactive object selection. CVPR, 2016. 4, 19<br>[110] Kaiyu Yang, Klint Qinami, Li Fei-Fei, Jia Deng, and Olga Rus-<br>sakovsky. Towards fairer datasets: Filtering and balancing the dis-<br>tribution of the people subtree in the imagenet hierarchy. Proceed-<br>ings of the 2020 conference on fairness, accountability, and trans-<br>parency, 2020. 8<br>[111] Lei Yang, Yan Zi Wei, Yisheng HE, Wei Sun, Zhenhang Huang,<br>Haibin Huang, and Haoqiang Fan. iShape: A first step towards<br>irregular shape instance segmentation. arXiv:2109.15068, 2021. 9,<br>20, 23, 24<br>[112] Senthil Yogamani, Ciaran Hughes, Jonathan Horgan, Ganesh Sistu,<br>Padraig Varley, Derek O'Dea, Michal Uricar, Stefan Milz, Mar-<br>tin Simon, Karl Amende, et al. WoodScape: A multi-task, multi-<br>camera fisheye dataset for autonomous driving. ICCV, 2019. 9,<br>20<br>[113] Lingzhi Zhang, Shenghao Zhou, Simon Stent, and Jianbo Shi. Fine-<br>grained egocentric hand-object segmentation: Dataset, model, and<br>applications. ECCV, 2022. 9, 19, 20<br>[114] Wenwei Zhang, Jiangmiao Pang, Kai Chen, and Chen Change Loy.<br>K-Net: Towards unified image segmentation. NeurIPS, 2021. 4<br>[115] Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-<br>Wei Chang. Men also like shopping: Reducing gender bias ampli-<br>fication using corpus-level constraints. arXiv:1707.09457, 2017. 8<br>[116] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and An-<br>tonio Torralba. Places: A 10 million image database for scene<br>recognition. TPAMI, 2017. 20<br>[117] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler,<br>Adela Barriuso, and Antonio Torralba. Semantic understanding of<br>scenes through the ADE20K dataset. IJCV, 2019. 2, 7, 9, 20</p>",
      "id": 147,
      "page": 15,
      "text": "[97] Yansong Tang, Zian Wang, Jiwen Lu, Jianjiang Feng, and Jie Zhou.\nMulti-stream deep neural networks for RGB-D egocentric action\nrecognition. IEEE Transactions on Circuits and Systems for Video\nTechnology, 2019. 20\n[98] The World Bank. The world by income and regions,\n2022. https://canaopics.workbank.cog/wordedevelopment-\nindicators/the-woild-by-ncome-and-regioal 18\n[99] Sebastian Thrun. Is learning the n-th thing any easier than learning\nthe first? NeurIPS, 1995. 12\n[100] Cameron Trotter, Georgia Atkinson, Matt Sharpe, Kirsten Richard-\nson, A. Stephen McGough, Nick Wright, Ben Burville, and Per\nBerggren. NDD20: A large-scale few-shot dolphin dataset for\ncoarse and fine-grained categorisation. arXiv:2005.13359, 2020.\n9, 19, 20, 23, 24\n[101] United States Environmental Protection Agency. Greenhouse Gas\nEquivalencies Calculator. https://www.epa.govienergy/greenhouse-\ngas-equivalencies-calculator, 2022. 28\n[102] Koen EA van de Sande, Jasper RR Uijlings, Theo Gevers, and\nArnold WM Smeulders. Segmentation as selective search for ob-\nject recognition. ICCV, 2011. 10\n[103] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\nLlion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin.\nAttention is all you need. NeurIPS, 2017. 5, 16\n[104] Boying Wang, Libo Zhang, Longyin Wen, Xianglong Liu, and Yan-\njun Wu. Towards real-world prohibited item detection: A large-\nscale x-ray benchmark. CVPR, 2021. 9, 19, 20\n[105] Weiyao Wang, Matt Feiszli, Heng Wang, Jitendra Malik, and\nDu Tran. Open-world instance segmentation: Exploiting pseudo\nground truth from learned pairwise affinity. CVPR, 2022. 21\n[106] Chao- Yuan Wu, Justin Johnson, Jitendra Malik, Christoph Feicht-\nenhofer, and Georgia Gkioxari. Multiview compressive coding for\n3D reconstruction. CVPR, 2023. 12\n[107] Jianxiong Xiao, James Hays, Krista Ehinger, Aude Oliva, and An-\ntonio Torralba. SUN database: Large-scale scene recognition from\nabbey to zoo. CVPR, 2010. 20\n[108] Saining Xie and Zhuowen Tu. Holistically-nested edge detection.\nICCV, 2015. 10\n[109] Ning Xu, Brian Price, Scott Cohen, Jimei Yang, and Thomas S\nHuang. Deep interactive object selection. CVPR, 2016. 4, 19\n[110] Kaiyu Yang, Klint Qinami, Li Fei-Fei, Jia Deng, and Olga Rus-\nsakovsky. Towards fairer datasets: Filtering and balancing the dis-\ntribution of the people subtree in the imagenet hierarchy. Proceed-\nings of the 2020 conference on fairness, accountability, and trans-\nparency, 2020. 8\n[111] Lei Yang, Yan Zi Wei, Yisheng HE, Wei Sun, Zhenhang Huang,\nHaibin Huang, and Haoqiang Fan. iShape: A first step towards\nirregular shape instance segmentation. arXiv:2109.15068, 2021. 9,\n20, 23, 24\n[112] Senthil Yogamani, Ciaran Hughes, Jonathan Horgan, Ganesh Sistu,\nPadraig Varley, Derek O'Dea, Michal Uricar, Stefan Milz, Mar-\ntin Simon, Karl Amende, et al. WoodScape: A multi-task, multi-\ncamera fisheye dataset for autonomous driving. ICCV, 2019. 9,\n20\n[113] Lingzhi Zhang, Shenghao Zhou, Simon Stent, and Jianbo Shi. Fine-\ngrained egocentric hand-object segmentation: Dataset, model, and\napplications. ECCV, 2022. 9, 19, 20\n[114] Wenwei Zhang, Jiangmiao Pang, Kai Chen, and Chen Change Loy.\nK-Net: Towards unified image segmentation. NeurIPS, 2021. 4\n[115] Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-\nWei Chang. Men also like shopping: Reducing gender bias ampli-\nfication using corpus-level constraints. arXiv:1707.09457, 2017. 8\n[116] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and An-\ntonio Torralba. Places: A 10 million image database for scene\nrecognition. TPAMI, 2017. 20\n[117] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler,\nAdela Barriuso, and Antonio Torralba. Semantic understanding of\nscenes through the ADE20K dataset. IJCV, 2019. 2, 7, 9, 20"
    },
    {
      "bounding_box": [
        {
          "x": 1219,
          "y": 3052
        },
        {
          "x": 1263,
          "y": 3052
        },
        {
          "x": 1263,
          "y": 3093
        },
        {
          "x": 1219,
          "y": 3093
        }
      ],
      "category": "footer",
      "html": "<footer id='148' style='font-size:18px'>15</footer>",
      "id": 148,
      "page": 15,
      "text": "15"
    },
    {
      "bounding_box": [
        {
          "x": 205,
          "y": 303
        },
        {
          "x": 418,
          "y": 303
        },
        {
          "x": 418,
          "y": 354
        },
        {
          "x": 205,
          "y": 354
        }
      ],
      "category": "paragraph",
      "html": "<p id='149' style='font-size:22px'>Appendix</p>",
      "id": 149,
      "page": 16,
      "text": "Appendix"
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 399
        },
        {
          "x": 527,
          "y": 399
        },
        {
          "x": 527,
          "y": 444
        },
        {
          "x": 204,
          "y": 444
        }
      ],
      "category": "paragraph",
      "html": "<p id='150' style='font-size:16px'>Table of contents:</p>",
      "id": 150,
      "page": 16,
      "text": "Table of contents:"
    },
    {
      "bounding_box": [
        {
          "x": 248,
          "y": 463
        },
        {
          "x": 1090,
          "y": 463
        },
        {
          "x": 1090,
          "y": 877
        },
        {
          "x": 248,
          "y": 877
        }
      ],
      "category": "paragraph",
      "html": "<p id='151' style='font-size:14px'>· §A: Segment Anything Model and Task Details<br>· §B: Automatic Mask Generation Details<br>· §C: RAI Additional Details<br>· §D: Experiment Implementation Details<br>· §E: Human Study Experimental Design<br>· §F: Dataset, Annotation, and Model Cards<br>· §G: Annotation Guidelines</p>",
      "id": 151,
      "page": 16,
      "text": "· §A: Segment Anything Model and Task Details\n· §B: Automatic Mask Generation Details\n· §C: RAI Additional Details\n· §D: Experiment Implementation Details\n· §E: Human Study Experimental Design\n· §F: Dataset, Annotation, and Model Cards\n· §G: Annotation Guidelines"
    },
    {
      "bounding_box": [
        {
          "x": 206,
          "y": 923
        },
        {
          "x": 1181,
          "y": 923
        },
        {
          "x": 1181,
          "y": 972
        },
        {
          "x": 206,
          "y": 972
        }
      ],
      "category": "paragraph",
      "html": "<p id='152' style='font-size:20px'>A. Segment Anything Model and Task Details</p>",
      "id": 152,
      "page": 16,
      "text": "A. Segment Anything Model and Task Details"
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 1020
        },
        {
          "x": 1198,
          "y": 1020
        },
        {
          "x": 1198,
          "y": 1614
        },
        {
          "x": 201,
          "y": 1614
        }
      ],
      "category": "paragraph",
      "html": "<p id='153' style='font-size:18px'>Image encoder. In general, the image encoder can be any<br>network that outputs a Cx H x W image embedding. Mo-<br>tivated by scalability and access to strong pre-training, we<br>use an MAE [47] pre-trained Vision Transformer (ViT) [33]<br>with minimal adaptations to process high resolution inputs,<br>specifically a ViT-H/16 with 14x 14 windowed attention<br>and four equally-spaced global attention blocks, follow-<br>ing [62]. The image encoder's output is a 16x downscaled<br>embedding of the input image. Since our runtime goal is to<br>process each prompt in real-time, we can afford a high num-<br>ber of image encoder FLOPs because they are computed<br>only once per image, not per prompt.</p>",
      "id": 153,
      "page": 16,
      "text": "Image encoder. In general, the image encoder can be any\nnetwork that outputs a Cx H x W image embedding. Mo-\ntivated by scalability and access to strong pre-training, we\nuse an MAE [47] pre-trained Vision Transformer (ViT) [33]\nwith minimal adaptations to process high resolution inputs,\nspecifically a ViT-H/16 with 14x 14 windowed attention\nand four equally-spaced global attention blocks, follow-\ning [62]. The image encoder's output is a 16x downscaled\nembedding of the input image. Since our runtime goal is to\nprocess each prompt in real-time, we can afford a high num-\nber of image encoder FLOPs because they are computed\nonly once per image, not per prompt."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 1621
        },
        {
          "x": 1198,
          "y": 1621
        },
        {
          "x": 1198,
          "y": 1963
        },
        {
          "x": 202,
          "y": 1963
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='154' style='font-size:16px'>Following standard practices (e.g., [40]), we use an in-<br>put resolution of 1024x 1024 obtained by rescaling the im-<br>age and padding the shorter side. The image embedding<br>is therefore 64x 64. To reduce the channel dimension, fol-<br>lowing [62], we use a 1 x 1 convolution to get to 256 chan-<br>nels, followed by a 3x3 convolution also with 256 channels.<br>Each convolution is followed by a layer normalization [4].</p>",
      "id": 154,
      "page": 16,
      "text": "Following standard practices (e.g., [40]), we use an in-\nput resolution of 1024x 1024 obtained by rescaling the im-\nage and padding the shorter side. The image embedding\nis therefore 64x 64. To reduce the channel dimension, fol-\nlowing [62], we use a 1 x 1 convolution to get to 256 chan-\nnels, followed by a 3x3 convolution also with 256 channels.\nEach convolution is followed by a layer normalization [4]."
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 1980
        },
        {
          "x": 1198,
          "y": 1980
        },
        {
          "x": 1198,
          "y": 2624
        },
        {
          "x": 200,
          "y": 2624
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='155' style='font-size:16px'>Prompt encoder. Sparse prompts are mapped to 256-<br>dimensional vectorial embeddings as follows. A point is<br>represented as the sum of a positional encoding [95] of the<br>point's location and one of two learned embeddings that in-<br>dicate if the point is either in the foreground or background.<br>A box is represented by an embedding pair: (1) the posi-<br>tional encoding of its top-left corner summed with a learned<br>embedding representing \"top-left corner\" and (2) the same<br>structure but using a learned embedding indicating \"bottom-<br>right corner\". Finally, to represent free-form text we use the<br>text encoder from CLIP [82] (any text encoder is possible in<br>general). We focus on geometric prompts for the remainder<br>of this section and discuss text prompts in depth in §D.5.</p>",
      "id": 155,
      "page": 16,
      "text": "Prompt encoder. Sparse prompts are mapped to 256-\ndimensional vectorial embeddings as follows. A point is\nrepresented as the sum of a positional encoding [95] of the\npoint's location and one of two learned embeddings that in-\ndicate if the point is either in the foreground or background.\nA box is represented by an embedding pair: (1) the posi-\ntional encoding of its top-left corner summed with a learned\nembedding representing \"top-left corner\" and (2) the same\nstructure but using a learned embedding indicating \"bottom-\nright corner\". Finally, to represent free-form text we use the\ntext encoder from CLIP [82] (any text encoder is possible in\ngeneral). We focus on geometric prompts for the remainder\nof this section and discuss text prompts in depth in §D.5."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 2632
        },
        {
          "x": 1199,
          "y": 2632
        },
        {
          "x": 1199,
          "y": 2974
        },
        {
          "x": 201,
          "y": 2974
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='156' style='font-size:16px'>Dense prompts (i.e., masks) have a spatial correspon-<br>dence with the image. We input masks at a 4x lower res-<br>olution than the input image, then downscale an additional<br>4x using two 2x2, stride-2 convolutions with output chan-<br>nels 4 and 16, respectively. A final 1 x 1 convolution maps<br>the channel dimension to 256. Each layer is separated by<br>GELU activations [50] and layer normalization. The mask</p>",
      "id": 156,
      "page": 16,
      "text": "Dense prompts (i.e., masks) have a spatial correspon-\ndence with the image. We input masks at a 4x lower res-\nolution than the input image, then downscale an additional\n4x using two 2x2, stride-2 convolutions with output chan-\nnels 4 and 16, respectively. A final 1 x 1 convolution maps\nthe channel dimension to 256. Each layer is separated by\nGELU activations [50] and layer normalization. The mask"
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 293
        },
        {
          "x": 2271,
          "y": 293
        },
        {
          "x": 2271,
          "y": 633
        },
        {
          "x": 1278,
          "y": 633
        }
      ],
      "category": "figure",
      "html": "<br><figure><img id='157' style='font-size:14px' alt=\"image x2\ndot product\nembedding image to token attn. 2x per mask\n(256x64x64) masks\nconv.\ntrans.\nmlp\noutput\ntoken\ntoken to image attn. per mask\ntoken mlp\noutput tokens IoU\nto image output\n+\nprompt tokens self attn. attn. token IoU\nmlp\nscores\n(Ntokensx256) mask decoder\" data-coord=\"top-left:(1278,293); bottom-right:(2271,633)\" /></figure>",
      "id": 157,
      "page": 16,
      "text": "image x2\ndot product\nembedding image to token attn. 2x per mask\n(256x64x64) masks\nconv.\ntrans.\nmlp\noutput\ntoken\ntoken to image attn. per mask\ntoken mlp\noutput tokens IoU\nto image output\n+\nprompt tokens self attn. attn. token IoU\nmlp\nscores\n(Ntokensx256) mask decoder"
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 649
        },
        {
          "x": 2276,
          "y": 649
        },
        {
          "x": 2276,
          "y": 1097
        },
        {
          "x": 1280,
          "y": 1097
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='158' style='font-size:18px'>Figure 14: Details of the lightweight mask decoder. A<br>two-layer decoder updates both the image embedding and<br>prompt tokens via cross-attention. Then the image embed-<br>ding is upscaled, from which the updated output tokens are<br>used to dynamically predict masks. (Not illustrated for fig-<br>ure clarity: At every attention layer, positional encodings<br>are added to the image embedding, and the entire original<br>prompt token (including position encoding) is re-added to<br>the token queries and keys.)</p>",
      "id": 158,
      "page": 16,
      "text": "Figure 14: Details of the lightweight mask decoder. A\ntwo-layer decoder updates both the image embedding and\nprompt tokens via cross-attention. Then the image embed-\nding is upscaled, from which the updated output tokens are\nused to dynamically predict masks. (Not illustrated for fig-\nure clarity: At every attention layer, positional encodings\nare added to the image embedding, and the entire original\nprompt token (including position encoding) is re-added to\nthe token queries and keys.)"
    },
    {
      "bounding_box": [
        {
          "x": 1282,
          "y": 1151
        },
        {
          "x": 2276,
          "y": 1151
        },
        {
          "x": 2276,
          "y": 1296
        },
        {
          "x": 1282,
          "y": 1296
        }
      ],
      "category": "paragraph",
      "html": "<p id='159' style='font-size:18px'>and image embedding are then added element-wise. If there<br>is no mask prompt, a learned embedding representing \"no<br>mask\" is added to each image embedding location.</p>",
      "id": 159,
      "page": 16,
      "text": "and image embedding are then added element-wise. If there\nis no mask prompt, a learned embedding representing \"no\nmask\" is added to each image embedding location."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 1318
        },
        {
          "x": 2276,
          "y": 1318
        },
        {
          "x": 2276,
          "y": 1813
        },
        {
          "x": 1280,
          "y": 1813
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='160' style='font-size:18px'>Lightweight mask decoder. This module efficiently maps<br>the image embedding and a set of prompt embeddings to an<br>output mask. To combine these inputs, we take inspiration<br>from Transformer segmentation models [14, 20] and modify<br>a standard Transformer decoder [103]. Before applying our<br>decoder, we first insert into the set of prompt embeddings<br>a learned output token embedding that will be used at the<br>decoder's output, analogous to the [class ] token in [33].<br>For simplicity, we refer to these embeddings (not including<br>the image embedding) collectively as \"tokens\".</p>",
      "id": 160,
      "page": 16,
      "text": "Lightweight mask decoder. This module efficiently maps\nthe image embedding and a set of prompt embeddings to an\noutput mask. To combine these inputs, we take inspiration\nfrom Transformer segmentation models [14, 20] and modify\na standard Transformer decoder [103]. Before applying our\ndecoder, we first insert into the set of prompt embeddings\na learned output token embedding that will be used at the\ndecoder's output, analogous to the [class ] token in [33].\nFor simplicity, we refer to these embeddings (not including\nthe image embedding) collectively as \"tokens\"."
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 1823
        },
        {
          "x": 2276,
          "y": 1823
        },
        {
          "x": 2276,
          "y": 2466
        },
        {
          "x": 1278,
          "y": 2466
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='161' style='font-size:18px'>Our decoder design is shown in Fig. 14. Each decoder<br>layer performs 4 steps: (1) self-attention on the tokens, (2)<br>cross-attention from tokens (as queries) to the image em-<br>bedding, (3) a point-wise MLP updates each token, and (4)<br>cross-attention from the image embedding (as queries) to<br>tokens. This last step updates the image embedding with<br>prompt information. During cross-attention, the image em-<br>bedding is treated as a set of 642 256-dimensional vectors.<br>Each self/cross-attention and MLP has a residual connec-<br>tion [49], layer normalization, and a dropout [93] of 0.1 at<br>training. The next decoder layer takes the updated tokens<br>and the updated image embedding from the previous layer.<br>We use a two-layer decoder.</p>",
      "id": 161,
      "page": 16,
      "text": "Our decoder design is shown in Fig. 14. Each decoder\nlayer performs 4 steps: (1) self-attention on the tokens, (2)\ncross-attention from tokens (as queries) to the image em-\nbedding, (3) a point-wise MLP updates each token, and (4)\ncross-attention from the image embedding (as queries) to\ntokens. This last step updates the image embedding with\nprompt information. During cross-attention, the image em-\nbedding is treated as a set of 642 256-dimensional vectors.\nEach self/cross-attention and MLP has a residual connec-\ntion [49], layer normalization, and a dropout [93] of 0.1 at\ntraining. The next decoder layer takes the updated tokens\nand the updated image embedding from the previous layer.\nWe use a two-layer decoder."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 2475
        },
        {
          "x": 2277,
          "y": 2475
        },
        {
          "x": 2277,
          "y": 2871
        },
        {
          "x": 1280,
          "y": 2871
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='162' style='font-size:16px'>To ensure the decoder has access to critical geometric in-<br>formation the positional encodings are added to the image<br>embedding whenever they participate in an attention layer.<br>Additionally, the entire original prompt tokens (including<br>their positional encodings) are re-added to the updated to-<br>kens whenever they participate in an attention layer. This<br>allows for a strong dependence on both the prompt token's<br>geometric location and type.</p>",
      "id": 162,
      "page": 16,
      "text": "To ensure the decoder has access to critical geometric in-\nformation the positional encodings are added to the image\nembedding whenever they participate in an attention layer.\nAdditionally, the entire original prompt tokens (including\ntheir positional encodings) are re-added to the updated to-\nkens whenever they participate in an attention layer. This\nallows for a strong dependence on both the prompt token's\ngeometric location and type."
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 2880
        },
        {
          "x": 2274,
          "y": 2880
        },
        {
          "x": 2274,
          "y": 2976
        },
        {
          "x": 1281,
          "y": 2976
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='163' style='font-size:18px'>After running the decoder, we upsample the updated im-<br>age embedding by 4x with two transposed convolutional</p>",
      "id": 163,
      "page": 16,
      "text": "After running the decoder, we upsample the updated im-\nage embedding by 4x with two transposed convolutional"
    },
    {
      "bounding_box": [
        {
          "x": 1220,
          "y": 3056
        },
        {
          "x": 1263,
          "y": 3056
        },
        {
          "x": 1263,
          "y": 3091
        },
        {
          "x": 1220,
          "y": 3091
        }
      ],
      "category": "footer",
      "html": "<footer id='164' style='font-size:14px'>16</footer>",
      "id": 164,
      "page": 16,
      "text": "16"
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 306
        },
        {
          "x": 1198,
          "y": 306
        },
        {
          "x": 1198,
          "y": 651
        },
        {
          "x": 201,
          "y": 651
        }
      ],
      "category": "paragraph",
      "html": "<p id='165' style='font-size:14px'>layers (now it's downscaled 4x relative to the input image).<br>Then, the tokens attend once more to the image embedding<br>and we pass the updated output token embedding to a small<br>3-layer MLP that outputs a vector matching the channel di-<br>mension of the upscaled image embedding. Finally, we pre-<br>dict a mask with a spatially point-wise product between the<br>upscaled image embedding and the MLP's output.</p>",
      "id": 165,
      "page": 17,
      "text": "layers (now it's downscaled 4x relative to the input image).\nThen, the tokens attend once more to the image embedding\nand we pass the updated output token embedding to a small\n3-layer MLP that outputs a vector matching the channel di-\nmension of the upscaled image embedding. Finally, we pre-\ndict a mask with a spatially point-wise product between the\nupscaled image embedding and the MLP's output."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 657
        },
        {
          "x": 1198,
          "y": 657
        },
        {
          "x": 1198,
          "y": 1049
        },
        {
          "x": 201,
          "y": 1049
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='166' style='font-size:14px'>The transformer uses an embedding dimension of 256.<br>The transformer MLP blocks have a large internal dimen-<br>sion of 2048, but the MLP is applied only to the prompt to-<br>kens for which there are relatively few (rarely greater than<br>20). However, in cross-attention layers where we have a<br>64x 64 image embedding, we reduce the channel dimension<br>of the queries, keys, and values by 2x to 128 for computa-<br>tional efficiency. All attention layers use 8 heads.</p>",
      "id": 166,
      "page": 17,
      "text": "The transformer uses an embedding dimension of 256.\nThe transformer MLP blocks have a large internal dimen-\nsion of 2048, but the MLP is applied only to the prompt to-\nkens for which there are relatively few (rarely greater than\n20). However, in cross-attention layers where we have a\n64x 64 image embedding, we reduce the channel dimension\nof the queries, keys, and values by 2x to 128 for computa-\ntional efficiency. All attention layers use 8 heads."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 1056
        },
        {
          "x": 1198,
          "y": 1056
        },
        {
          "x": 1198,
          "y": 1250
        },
        {
          "x": 201,
          "y": 1250
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='167' style='font-size:18px'>The transposed convolutions used to upscale the output<br>image embedding are 2x2, stride 2 with output channel di-<br>mensions of 64 and 32 and have GELU activations. They<br>are separated by layer normalization.</p>",
      "id": 167,
      "page": 17,
      "text": "The transposed convolutions used to upscale the output\nimage embedding are 2x2, stride 2 with output channel di-\nmensions of 64 and 32 and have GELU activations. They\nare separated by layer normalization."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 1268
        },
        {
          "x": 1198,
          "y": 1268
        },
        {
          "x": 1198,
          "y": 2112
        },
        {
          "x": 201,
          "y": 2112
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='168' style='font-size:18px'>Making the model ambiguity-aware. As described, a sin-<br>gle input prompt may be ambiguous in the sense that it cor-<br>responds to multiple valid masks, and the model will learn<br>to average over these masks. We eliminate this problem<br>with a simple modification: instead of predicting a single<br>mask, we use a small number of output tokens and predict<br>multiple masks simultaneously. By default we predict three<br>masks, since we observe that three layers (whole, part, and<br>subpart) are often enough to describe nested masks. During<br>training, we compute the loss (described shortly) between<br>the ground truth and each of the predicted masks, but only<br>backpropagate from the lowest loss. This is a common tech-<br>nique used for models with multiple outputs [15, 45, 64].<br>For use in applications, we'd like to rank predicted masks,<br>SO we add a small head (operating on an additional output<br>token) that estimates the IoU between each predicted mask<br>and the object it covers.</p>",
      "id": 168,
      "page": 17,
      "text": "Making the model ambiguity-aware. As described, a sin-\ngle input prompt may be ambiguous in the sense that it cor-\nresponds to multiple valid masks, and the model will learn\nto average over these masks. We eliminate this problem\nwith a simple modification: instead of predicting a single\nmask, we use a small number of output tokens and predict\nmultiple masks simultaneously. By default we predict three\nmasks, since we observe that three layers (whole, part, and\nsubpart) are often enough to describe nested masks. During\ntraining, we compute the loss (described shortly) between\nthe ground truth and each of the predicted masks, but only\nbackpropagate from the lowest loss. This is a common tech-\nnique used for models with multiple outputs [15, 45, 64].\nFor use in applications, we'd like to rank predicted masks,\nSO we add a small head (operating on an additional output\ntoken) that estimates the IoU between each predicted mask\nand the object it covers."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 2116
        },
        {
          "x": 1199,
          "y": 2116
        },
        {
          "x": 1199,
          "y": 2563
        },
        {
          "x": 201,
          "y": 2563
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='169' style='font-size:14px'>Ambiguity is much rarer with multiple prompts and the<br>three output masks will usually become similar. To mini-<br>mize computation of degenerate losses at training and en-<br>sure the single unambiguous mask receives a regular gradi-<br>ent signal, we only predict a single mask when more than<br>one prompt is given. This is accomplished by adding a<br>fourth output token for an additional mask prediction. This<br>fourth mask is never returned for a single prompt and is the<br>only mask returned for multiple prompts.</p>",
      "id": 169,
      "page": 17,
      "text": "Ambiguity is much rarer with multiple prompts and the\nthree output masks will usually become similar. To mini-\nmize computation of degenerate losses at training and en-\nsure the single unambiguous mask receives a regular gradi-\nent signal, we only predict a single mask when more than\none prompt is given. This is accomplished by adding a\nfourth output token for an additional mask prediction. This\nfourth mask is never returned for a single prompt and is the\nonly mask returned for multiple prompts."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 2578
        },
        {
          "x": 1198,
          "y": 2578
        },
        {
          "x": 1198,
          "y": 2976
        },
        {
          "x": 201,
          "y": 2976
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='170' style='font-size:14px'>Losses. We supervise mask prediction with a linear combi-<br>nation of focal loss [65] and dice loss [73] in a 20:1 ratio of<br>focal loss to dice loss, following [20, 14]. Unlike [20, 14],<br>we observe that auxiliary deep supervision after each de-<br>coder layer is unhelpful. The IoU prediction head is trained<br>with mean-square-error loss between the IoU prediction and<br>the predicted mask's IoU with the ground truth mask. It is<br>added to the mask loss with a constant scaling factor of 1.0.</p>",
      "id": 170,
      "page": 17,
      "text": "Losses. We supervise mask prediction with a linear combi-\nnation of focal loss [65] and dice loss [73] in a 20:1 ratio of\nfocal loss to dice loss, following [20, 14]. Unlike [20, 14],\nwe observe that auxiliary deep supervision after each de-\ncoder layer is unhelpful. The IoU prediction head is trained\nwith mean-square-error loss between the IoU prediction and\nthe predicted mask's IoU with the ground truth mask. It is\nadded to the mask loss with a constant scaling factor of 1.0."
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 304
        },
        {
          "x": 2276,
          "y": 304
        },
        {
          "x": 2276,
          "y": 902
        },
        {
          "x": 1278,
          "y": 902
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='171' style='font-size:14px'>Training algorithm. Following recent approaches [92, 37],<br>we simulate an interactive segmentation setup during train-<br>ing. First, with equal probability either a foreground point<br>or bounding box is selected randomly for the target mask.<br>Points are sampled uniformly from the ground truth mask.<br>Boxes are taken as the ground truth mask's bounding box,<br>with random noise added in each coordinate with standard<br>deviation equal to 10% of the box sidelength, to a maxi-<br>mum of 20 pixels. This noise profile is a reasonable com-<br>promise between applications like instance segmentation,<br>which produce a tight box around the target object, and in-<br>teractive segmentation, where a user may draw a loose box.</p>",
      "id": 171,
      "page": 17,
      "text": "Training algorithm. Following recent approaches [92, 37],\nwe simulate an interactive segmentation setup during train-\ning. First, with equal probability either a foreground point\nor bounding box is selected randomly for the target mask.\nPoints are sampled uniformly from the ground truth mask.\nBoxes are taken as the ground truth mask's bounding box,\nwith random noise added in each coordinate with standard\ndeviation equal to 10% of the box sidelength, to a maxi-\nmum of 20 pixels. This noise profile is a reasonable com-\npromise between applications like instance segmentation,\nwhich produce a tight box around the target object, and in-\nteractive segmentation, where a user may draw a loose box."
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 911
        },
        {
          "x": 2276,
          "y": 911
        },
        {
          "x": 2276,
          "y": 1505
        },
        {
          "x": 1278,
          "y": 1505
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='172' style='font-size:14px'>After making a prediction from this first prompt, subse-<br>quent points are selected uniformly from the error region<br>between the previous mask prediction and the ground truth<br>mask. Each new point is foreground or background if the er-<br>ror region is a false negative or false positive, respectively.<br>We also supply the mask prediction from the previous it-<br>eration as an additional prompt to our model. To provide<br>the next iteration with maximal information, we supply the<br>unthresholded mask logits instead of the binarized mask.<br>When multiple masks are returned, the mask passed to the<br>next iteration and used to sample the next point is the one<br>with the highest predicted IoU.</p>",
      "id": 172,
      "page": 17,
      "text": "After making a prediction from this first prompt, subse-\nquent points are selected uniformly from the error region\nbetween the previous mask prediction and the ground truth\nmask. Each new point is foreground or background if the er-\nror region is a false negative or false positive, respectively.\nWe also supply the mask prediction from the previous it-\neration as an additional prompt to our model. To provide\nthe next iteration with maximal information, we supply the\nunthresholded mask logits instead of the binarized mask.\nWhen multiple masks are returned, the mask passed to the\nnext iteration and used to sample the next point is the one\nwith the highest predicted IoU."
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 1512
        },
        {
          "x": 2277,
          "y": 1512
        },
        {
          "x": 2277,
          "y": 2309
        },
        {
          "x": 1278,
          "y": 2309
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='173' style='font-size:14px'>We find diminishing returns after 8 iteratively sampled<br>points (we have tested up to 16). Additionally, to encour-<br>age the model to benefit from the supplied mask, we also<br>use two more iterations where no additional points are sam-<br>pled. One of these iterations is randomly inserted among the<br>8 iteratively sampled points, and the other is always at the<br>end. This gives 11 total iterations: one sampled initial in-<br>put prompt, 8 iteratively sampled points, and two iterations<br>where no new external information is supplied to the model<br>SO it can learn to refine its own mask predictions. We note<br>that using a relatively large number of iterations is possible<br>because our lightweight mask decoder requires less than 1 %<br>of the image encoder's compute and, therefore, each itera-<br>tion adds only a small overhead. This is unlike previous<br>interactive methods that perform only one or a few interac-<br>tive steps per optimizer update [70, 9, 37, 92].</p>",
      "id": 173,
      "page": 17,
      "text": "We find diminishing returns after 8 iteratively sampled\npoints (we have tested up to 16). Additionally, to encour-\nage the model to benefit from the supplied mask, we also\nuse two more iterations where no additional points are sam-\npled. One of these iterations is randomly inserted among the\n8 iteratively sampled points, and the other is always at the\nend. This gives 11 total iterations: one sampled initial in-\nput prompt, 8 iteratively sampled points, and two iterations\nwhere no new external information is supplied to the model\nSO it can learn to refine its own mask predictions. We note\nthat using a relatively large number of iterations is possible\nbecause our lightweight mask decoder requires less than 1 %\nof the image encoder's compute and, therefore, each itera-\ntion adds only a small overhead. This is unlike previous\ninteractive methods that perform only one or a few interac-\ntive steps per optimizer update [70, 9, 37, 92]."
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 2329
        },
        {
          "x": 2276,
          "y": 2329
        },
        {
          "x": 2276,
          "y": 2977
        },
        {
          "x": 1278,
          "y": 2977
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='174' style='font-size:18px'>Training recipe. We use the AdamW [68] optimizer (B1 =<br>0.9, B2 = 0.999) and a linear learning rate warmup [42] for<br>250 iterations and a step-wise learning rate decay schedule.<br>The initial learning rate (lr), after warmup, is 8e-4 We<br>train for 90k iterations (~2 SA-1B epochs) and decrease the<br>lr by a factor of 10 at 60k iterations and again at 86666 it-<br>erations. The batch size is 256 images. To regularize SAM,<br>we set weight decay (wd) to 0.1 and apply drop path [53]<br>(dp) with a rate of 0.4. We use a layer-wise learning rate<br>decay [5] (ld) of 0.8. No data augmentation is applied. We<br>initialize SAM from an MAE [47] pre-trained ViT-H. We<br>distribute training across 256 GPUs, due to the large image<br>encoder and 1024x 1024 input size. To limit GPU mem-</p>",
      "id": 174,
      "page": 17,
      "text": "Training recipe. We use the AdamW [68] optimizer (B1 =\n0.9, B2 = 0.999) and a linear learning rate warmup [42] for\n250 iterations and a step-wise learning rate decay schedule.\nThe initial learning rate (lr), after warmup, is 8e-4 We\ntrain for 90k iterations (~2 SA-1B epochs) and decrease the\nlr by a factor of 10 at 60k iterations and again at 86666 it-\nerations. The batch size is 256 images. To regularize SAM,\nwe set weight decay (wd) to 0.1 and apply drop path [53]\n(dp) with a rate of 0.4. We use a layer-wise learning rate\ndecay [5] (ld) of 0.8. No data augmentation is applied. We\ninitialize SAM from an MAE [47] pre-trained ViT-H. We\ndistribute training across 256 GPUs, due to the large image\nencoder and 1024x 1024 input size. To limit GPU mem-"
    },
    {
      "bounding_box": [
        {
          "x": 1217,
          "y": 3052
        },
        {
          "x": 1264,
          "y": 3052
        },
        {
          "x": 1264,
          "y": 3094
        },
        {
          "x": 1217,
          "y": 3094
        }
      ],
      "category": "footer",
      "html": "<footer id='175' style='font-size:14px'>17</footer>",
      "id": 175,
      "page": 17,
      "text": "17"
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 309
        },
        {
          "x": 1199,
          "y": 309
        },
        {
          "x": 1199,
          "y": 502
        },
        {
          "x": 200,
          "y": 502
        }
      ],
      "category": "paragraph",
      "html": "<p id='176' style='font-size:16px'>ory usage, we train with up to 64 randomly sampled masks<br>per GPU. Additionally, we find that lightly filtering SA-1B<br>masks to discard any that cover more than 90% of the image<br>qualitatively improves results.</p>",
      "id": 176,
      "page": 18,
      "text": "ory usage, we train with up to 64 randomly sampled masks\nper GPU. Additionally, we find that lightly filtering SA-1B\nmasks to discard any that cover more than 90% of the image\nqualitatively improves results."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 509
        },
        {
          "x": 1200,
          "y": 509
        },
        {
          "x": 1200,
          "y": 1003
        },
        {
          "x": 201,
          "y": 1003
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='177' style='font-size:16px'>For ablations and others variations on training (e.g., text-<br>to-mask §D.5), we deviate from the default recipe above as<br>follows. When training with data from the first and sec-<br>ond data engine stages only, we augment the input with<br>large-scale jitter [40] with a scale range of [0.1, 2.0]. In-<br>tuitively, data augmentation may be helpful when training<br>data is more limited. To train ViT-B and ViT-L, we use<br>180k iterations with batch size 128 distributed across 128<br>GPUs. We set lr = 8e-4/4e-4, ld = 0.6/0.8, wd = 0.1, and<br>dp = 0.6/0.4 for ViT-B/L, respectively.</p>",
      "id": 177,
      "page": 18,
      "text": "For ablations and others variations on training (e.g., text-\nto-mask §D.5), we deviate from the default recipe above as\nfollows. When training with data from the first and sec-\nond data engine stages only, we augment the input with\nlarge-scale jitter [40] with a scale range of [0.1, 2.0]. In-\ntuitively, data augmentation may be helpful when training\ndata is more limited. To train ViT-B and ViT-L, we use\n180k iterations with batch size 128 distributed across 128\nGPUs. We set lr = 8e-4/4e-4, ld = 0.6/0.8, wd = 0.1, and\ndp = 0.6/0.4 for ViT-B/L, respectively."
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 1052
        },
        {
          "x": 1036,
          "y": 1052
        },
        {
          "x": 1036,
          "y": 1104
        },
        {
          "x": 204,
          "y": 1104
        }
      ],
      "category": "paragraph",
      "html": "<p id='178' style='font-size:22px'>B. Automatic Mask Generation Details</p>",
      "id": 178,
      "page": 18,
      "text": "B. Automatic Mask Generation Details"
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 1136
        },
        {
          "x": 1198,
          "y": 1136
        },
        {
          "x": 1198,
          "y": 1234
        },
        {
          "x": 203,
          "y": 1234
        }
      ],
      "category": "paragraph",
      "html": "<p id='179' style='font-size:14px'>Here we discuss details of the data engine's fully auto-<br>matic stage that was used to generate the released SA-1B.</p>",
      "id": 179,
      "page": 18,
      "text": "Here we discuss details of the data engine's fully auto-\nmatic stage that was used to generate the released SA-1B."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 1251
        },
        {
          "x": 1199,
          "y": 1251
        },
        {
          "x": 1199,
          "y": 1998
        },
        {
          "x": 201,
          "y": 1998
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='180' style='font-size:16px'>Cropping. Masks were generated from a regular grid of<br>32x32 points on the full image and 20 additional zoomed-<br>in image crops arising from 2x2 and 4x4 partially over-<br>lapping windows using 16x 16 and 8x8 regular point grids,<br>respectively. The original high-resolution images were used<br>for cropping (this was the only time we used them). We re-<br>moved masks that touch the inner boundaries of the crops.<br>We applied standard greedy box-based NMS (boxes were<br>used for efficiency) in two phases: first within each crop and<br>second across crops. When applying NMS within a crop,<br>we used the model's predicted IoU to rank masks. When<br>applying NMS across crops, we ranked masks from most<br>zoomed-in (i.e., from a 4x4 crop) to least zoomed-in (i.e.,<br>the original image), based on their source crop. In both<br>cases, we used an NMS threshold of 0.7.</p>",
      "id": 180,
      "page": 18,
      "text": "Cropping. Masks were generated from a regular grid of\n32x32 points on the full image and 20 additional zoomed-\nin image crops arising from 2x2 and 4x4 partially over-\nlapping windows using 16x 16 and 8x8 regular point grids,\nrespectively. The original high-resolution images were used\nfor cropping (this was the only time we used them). We re-\nmoved masks that touch the inner boundaries of the crops.\nWe applied standard greedy box-based NMS (boxes were\nused for efficiency) in two phases: first within each crop and\nsecond across crops. When applying NMS within a crop,\nwe used the model's predicted IoU to rank masks. When\napplying NMS across crops, we ranked masks from most\nzoomed-in (i.e., from a 4x4 crop) to least zoomed-in (i.e.,\nthe original image), based on their source crop. In both\ncases, we used an NMS threshold of 0.7."
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 2012
        },
        {
          "x": 1199,
          "y": 2012
        },
        {
          "x": 1199,
          "y": 2762
        },
        {
          "x": 200,
          "y": 2762
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='181' style='font-size:18px'>Filtering. We used three filters to increase mask qual-<br>ity. First, to keep only confident masks we filtered by the<br>model's predicted IoU score at a threshold of 88.0. Second,<br>to keep only stable masks we compared two binary masks<br>resulting from the same underlying soft mask by threshold-<br>ing it at different values. We kept the prediction (i.e., the<br>binary mask resulting from thresholding logits at 0) only if<br>the IoU between its pair of -1 and +1 thresholded masks was<br>equal to or greater than 95.0. Third, we noticed that occa-<br>sionally an automatic mask would cover the entire image.<br>These masks were generally uninteresting, and we filtered<br>them by removing masks that covered 95% or more of an<br>image. All filtering thresholds were selected to achieve both<br>a large number of masks and high mask quality as judged by<br>professional annotators using the method described in §5.</p>",
      "id": 181,
      "page": 18,
      "text": "Filtering. We used three filters to increase mask qual-\nity. First, to keep only confident masks we filtered by the\nmodel's predicted IoU score at a threshold of 88.0. Second,\nto keep only stable masks we compared two binary masks\nresulting from the same underlying soft mask by threshold-\ning it at different values. We kept the prediction (i.e., the\nbinary mask resulting from thresholding logits at 0) only if\nthe IoU between its pair of -1 and +1 thresholded masks was\nequal to or greater than 95.0. Third, we noticed that occa-\nsionally an automatic mask would cover the entire image.\nThese masks were generally uninteresting, and we filtered\nthem by removing masks that covered 95% or more of an\nimage. All filtering thresholds were selected to achieve both\na large number of masks and high mask quality as judged by\nprofessional annotators using the method described in §5."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 2777
        },
        {
          "x": 1199,
          "y": 2777
        },
        {
          "x": 1199,
          "y": 2975
        },
        {
          "x": 201,
          "y": 2975
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='182' style='font-size:16px'>Postprocessing. We observed two error types that are eas-<br>ily mitigated with postprocessing. First, an estimated 4%<br>of masks include small, spurious components. To address<br>these, we removed connected components with area less</p>",
      "id": 182,
      "page": 18,
      "text": "Postprocessing. We observed two error types that are eas-\nily mitigated with postprocessing. First, an estimated 4%\nof masks include small, spurious components. To address\nthese, we removed connected components with area less"
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 307
        },
        {
          "x": 2278,
          "y": 307
        },
        {
          "x": 2278,
          "y": 554
        },
        {
          "x": 1279,
          "y": 554
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='183' style='font-size:16px'>than 100 pixels (including removing entire masks if the<br>largest component is below this threshold). Second, another<br>estimated 4% of masks include small, spurious holes. To<br>address these, we filled holes with area less than 100 pixels.<br>Holes were identified as components of inverted masks.</p>",
      "id": 183,
      "page": 18,
      "text": "than 100 pixels (including removing entire masks if the\nlargest component is below this threshold). Second, another\nestimated 4% of masks include small, spurious holes. To\naddress these, we filled holes with area less than 100 pixels.\nHoles were identified as components of inverted masks."
    },
    {
      "bounding_box": [
        {
          "x": 1277,
          "y": 575
        },
        {
          "x": 2278,
          "y": 575
        },
        {
          "x": 2278,
          "y": 1269
        },
        {
          "x": 1277,
          "y": 1269
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='184' style='font-size:18px'>Automatic mask generation model. We trained a special<br>version of SAM for fully automatic mask generation that<br>sacrifices some inference speed for improved mask gener-<br>ation properties. We note the differences between our de-<br>fault SAM and the one used for data generation here: it<br>was trained on manual and semi-automatic data only, it was<br>trained for longer (177656 iterations instead of 90k) with<br>large-scale jitter data augmentation [40], simulated interac-<br>tive training used only point and mask prompts (no boxes)<br>and sampled only 4 points per mask during training (reduc-<br>ing from our default of 9 to 4 sped up training iterations<br>and had no impact on 1-point performance, though it would<br>harm mIoU if evaluating with more points), and finally the<br>mask decoder used 3 layers instead of 2.</p>",
      "id": 184,
      "page": 18,
      "text": "Automatic mask generation model. We trained a special\nversion of SAM for fully automatic mask generation that\nsacrifices some inference speed for improved mask gener-\nation properties. We note the differences between our de-\nfault SAM and the one used for data generation here: it\nwas trained on manual and semi-automatic data only, it was\ntrained for longer (177656 iterations instead of 90k) with\nlarge-scale jitter data augmentation [40], simulated interac-\ntive training used only point and mask prompts (no boxes)\nand sampled only 4 points per mask during training (reduc-\ning from our default of 9 to 4 sped up training iterations\nand had no impact on 1-point performance, though it would\nharm mIoU if evaluating with more points), and finally the\nmask decoder used 3 layers instead of 2."
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 1289
        },
        {
          "x": 2273,
          "y": 1289
        },
        {
          "x": 2273,
          "y": 1387
        },
        {
          "x": 1281,
          "y": 1387
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='185' style='font-size:20px'>SA-1B examples. We show SA-1B samples in Fig. 2. For<br>more examples, please see our dataset explorer.</p>",
      "id": 185,
      "page": 18,
      "text": "SA-1B examples. We show SA-1B samples in Fig. 2. For\nmore examples, please see our dataset explorer."
    },
    {
      "bounding_box": [
        {
          "x": 1283,
          "y": 1442
        },
        {
          "x": 1843,
          "y": 1442
        },
        {
          "x": 1843,
          "y": 1495
        },
        {
          "x": 1283,
          "y": 1495
        }
      ],
      "category": "paragraph",
      "html": "<p id='186' style='font-size:22px'>C. RAI Additional Details</p>",
      "id": 186,
      "page": 18,
      "text": "C. RAI Additional Details"
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 1544
        },
        {
          "x": 2278,
          "y": 1544
        },
        {
          "x": 2278,
          "y": 2242
        },
        {
          "x": 1278,
          "y": 2242
        }
      ],
      "category": "paragraph",
      "html": "<p id='187' style='font-size:16px'>Inferring geographic information for SA-1B. While the<br>images in SA-1B are not geo-tagged, each image has a cap-<br>tion describing its contents and where it was taken. We infer<br>approximate image geo-locations from these captions using<br>an Elmo-based named entity recognition model [78]. Each<br>extracted location entity is mapped to every matching coun-<br>try, province, and city. Captions are mapped to a single<br>country by first considering the matching countries, then<br>provinces, and finally cities. We note that there are ambigu-<br>ities and potential for biases with this method (e.g., \"Geor-<br>gia\" may refer to the country or the US state). As such, we<br>use the extracted locations to analyze the dataset as a whole,<br>but do not release the inferred locations. The captions will<br>not be released publicly as required by the image provider.</p>",
      "id": 187,
      "page": 18,
      "text": "Inferring geographic information for SA-1B. While the\nimages in SA-1B are not geo-tagged, each image has a cap-\ntion describing its contents and where it was taken. We infer\napproximate image geo-locations from these captions using\nan Elmo-based named entity recognition model [78]. Each\nextracted location entity is mapped to every matching coun-\ntry, province, and city. Captions are mapped to a single\ncountry by first considering the matching countries, then\nprovinces, and finally cities. We note that there are ambigu-\nities and potential for biases with this method (e.g., \"Geor-\ngia\" may refer to the country or the US state). As such, we\nuse the extracted locations to analyze the dataset as a whole,\nbut do not release the inferred locations. The captions will\nnot be released publicly as required by the image provider."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 2261
        },
        {
          "x": 2278,
          "y": 2261
        },
        {
          "x": 2278,
          "y": 2758
        },
        {
          "x": 1280,
          "y": 2758
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='188' style='font-size:20px'>Inferring geographic information for COCO and Open<br>Images. The COCO [66] and Open Images [60] datasets<br>do not provide geo-locations. Following [29], we retrieve<br>geographic metadata using the Flickr API. We retrieved<br>locations for 24% of the COCO training set (19,562 im-<br>ages) and for Open Images we retrieved 18% of the train-<br>ing set (493,517 images, after only considering images with<br>masks). We note that the geographic information is approx-<br>imate, and the sample of images with this information may<br>not fully match the full dataset distribution.</p>",
      "id": 188,
      "page": 18,
      "text": "Inferring geographic information for COCO and Open\nImages. The COCO [66] and Open Images [60] datasets\ndo not provide geo-locations. Following [29], we retrieve\ngeographic metadata using the Flickr API. We retrieved\nlocations for 24% of the COCO training set (19,562 im-\nages) and for Open Images we retrieved 18% of the train-\ning set (493,517 images, after only considering images with\nmasks). We note that the geographic information is approx-\nimate, and the sample of images with this information may\nnot fully match the full dataset distribution."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 2776
        },
        {
          "x": 2277,
          "y": 2776
        },
        {
          "x": 2277,
          "y": 2977
        },
        {
          "x": 1280,
          "y": 2977
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='189' style='font-size:18px'>Inferring income information. We use each image's in-<br>ferred country to look up its income level using the levels<br>defined by The World Bank [98]. We collapse the upper-<br>middle and lower-middle levels into a single middle level.</p>",
      "id": 189,
      "page": 18,
      "text": "Inferring income information. We use each image's in-\nferred country to look up its income level using the levels\ndefined by The World Bank [98]. We collapse the upper-\nmiddle and lower-middle levels into a single middle level."
    },
    {
      "bounding_box": [
        {
          "x": 1218,
          "y": 3052
        },
        {
          "x": 1263,
          "y": 3052
        },
        {
          "x": 1263,
          "y": 3094
        },
        {
          "x": 1218,
          "y": 3094
        }
      ],
      "category": "footer",
      "html": "<footer id='190' style='font-size:14px'>18</footer>",
      "id": 190,
      "page": 18,
      "text": "18"
    },
    {
      "bounding_box": [
        {
          "x": 206,
          "y": 295
        },
        {
          "x": 1193,
          "y": 295
        },
        {
          "x": 1193,
          "y": 566
        },
        {
          "x": 206,
          "y": 566
        }
      ],
      "category": "paragraph",
      "html": "<p id='191' style='font-size:14px'>mloU at mloU at<br>1 point 3 points 1 point 3 points<br>perceived gender presentation perceived age group<br>feminine 76.3 ±1.1 90.7 ±0.5 older 81.9 ±3.8 92.8 ±1.6<br>masculine 81.0 ±1.2 92.3 ±0.4 middle 78.2 ±0.8 91.3 ±0.3<br>young 77.3 ±2.7 91.5 ±0.9</p>",
      "id": 191,
      "page": 19,
      "text": "mloU at mloU at\n1 point 3 points 1 point 3 points\nperceived gender presentation perceived age group\nfeminine 76.3 ±1.1 90.7 ±0.5 older 81.9 ±3.8 92.8 ±1.6\nmasculine 81.0 ±1.2 92.3 ±0.4 middle 78.2 ±0.8 91.3 ±0.3\nyoung 77.3 ±2.7 91.5 ±0.9"
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 584
        },
        {
          "x": 1198,
          "y": 584
        },
        {
          "x": 1198,
          "y": 784
        },
        {
          "x": 202,
          "y": 784
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='192' style='font-size:20px'>Table 6: SAM's performance segmenting clothing across<br>perceived gender presentation and age group. The intervals<br>for perceived gender are disjoint, with mIoU for masculine<br>being higher. Confidence intervals for age group overlap.</p>",
      "id": 192,
      "page": 19,
      "text": "Table 6: SAM's performance segmenting clothing across\nperceived gender presentation and age group. The intervals\nfor perceived gender are disjoint, with mIoU for masculine\nbeing higher. Confidence intervals for age group overlap."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 846
        },
        {
          "x": 1198,
          "y": 846
        },
        {
          "x": 1198,
          "y": 1396
        },
        {
          "x": 201,
          "y": 1396
        }
      ],
      "category": "paragraph",
      "html": "<p id='193' style='font-size:18px'>Fairness in segmenting people. To investigate SAM's fair-<br>ness at segmenting people we use the More Inclusive Anno-<br>tations for People (MIAP) [87] test set annotations for Open<br>Images [60], which allows us to compare SAM's perfor-<br>mance across perceived gender presentation and perceived<br>age group. MIAP provides box annotations, while we need<br>ground truth masks for this analysis. To get ground truth<br>masks, we select each person-category mask from Open<br>Images if its corresponding bounding box is within a 1%<br>margin (based on relative box side lengths) of an annotated<br>bounding box in MIAP, resulting in 3.9k masks.</p>",
      "id": 193,
      "page": 19,
      "text": "Fairness in segmenting people. To investigate SAM's fair-\nness at segmenting people we use the More Inclusive Anno-\ntations for People (MIAP) [87] test set annotations for Open\nImages [60], which allows us to compare SAM's perfor-\nmance across perceived gender presentation and perceived\nage group. MIAP provides box annotations, while we need\nground truth masks for this analysis. To get ground truth\nmasks, we select each person-category mask from Open\nImages if its corresponding bounding box is within a 1%\nmargin (based on relative box side lengths) of an annotated\nbounding box in MIAP, resulting in 3.9k masks."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 1410
        },
        {
          "x": 1198,
          "y": 1410
        },
        {
          "x": 1198,
          "y": 2159
        },
        {
          "x": 201,
          "y": 2159
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='194' style='font-size:16px'>Fairness in segmenting clothing. We extend our analysis<br>from §6 to clothing segmentation. We look at SAM's per-<br>formance on clothing relative to the attributes of those wear-<br>ing the clothes. We use all 6.5k ground truth masks from<br>Open Images that have a category under the clothing super-<br>class and reside within a person box from MIAP. In Table 6<br>we compare performance across perceived gender presenta-<br>tion and age group. We find that SAM is better at segment-<br>ing clothing on those who present predominantly mascu-<br>line, with disjoint 95% confidence intervals. The gap closes<br>when moving from 1 to 3 point evaluation. Differences for<br>perceived age group are not significant. Our results indicate<br>there is a bias when segmenting clothing across perceived<br>gender presentation with a one point prompt, and we en-<br>courage users of SAM to be mindful of this limitation.</p>",
      "id": 194,
      "page": 19,
      "text": "Fairness in segmenting clothing. We extend our analysis\nfrom §6 to clothing segmentation. We look at SAM's per-\nformance on clothing relative to the attributes of those wear-\ning the clothes. We use all 6.5k ground truth masks from\nOpen Images that have a category under the clothing super-\nclass and reside within a person box from MIAP. In Table 6\nwe compare performance across perceived gender presenta-\ntion and age group. We find that SAM is better at segment-\ning clothing on those who present predominantly mascu-\nline, with disjoint 95% confidence intervals. The gap closes\nwhen moving from 1 to 3 point evaluation. Differences for\nperceived age group are not significant. Our results indicate\nthere is a bias when segmenting clothing across perceived\ngender presentation with a one point prompt, and we en-\ncourage users of SAM to be mindful of this limitation."
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 2204
        },
        {
          "x": 1197,
          "y": 2204
        },
        {
          "x": 1197,
          "y": 2337
        },
        {
          "x": 204,
          "y": 2337
        }
      ],
      "category": "paragraph",
      "html": "<p id='195' style='font-size:22px'>D. Experiment Implementation Details<br>D.1. Zero-Shot Single Point Valid Mask Evaluation</p>",
      "id": 195,
      "page": 19,
      "text": "D. Experiment Implementation Details\nD.1. Zero-Shot Single Point Valid Mask Evaluation"
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 2377
        },
        {
          "x": 1197,
          "y": 2377
        },
        {
          "x": 1197,
          "y": 2976
        },
        {
          "x": 201,
          "y": 2976
        }
      ],
      "category": "paragraph",
      "html": "<p id='196' style='font-size:18px'>Datasets. We built a new segmentation benchmark to eval-<br>uate the zero-shot transfer capabilities of our model using a<br>suite of 23 diverse segmentation datasets from prior work.<br>A description of each dataset is given in Table 7. For exam-<br>ples, see main text Fig. 8. This suite covers a range of do-<br>mains including egocentric [34, 28, 113], microscopy [12],<br>X-ray [104], underwater [52, 100], aerial [17], simula-<br>tion [86], driving [25], and painting [24] images. For ef-<br>ficient evaluation we subsampled datasets with more than<br>15k masks. Specifically, we randomly picked images SO<br>that the total number of masks in the sampled images was<br>~10k. We blurred faces of people in all the datasets.</p>",
      "id": 196,
      "page": 19,
      "text": "Datasets. We built a new segmentation benchmark to eval-\nuate the zero-shot transfer capabilities of our model using a\nsuite of 23 diverse segmentation datasets from prior work.\nA description of each dataset is given in Table 7. For exam-\nples, see main text Fig. 8. This suite covers a range of do-\nmains including egocentric [34, 28, 113], microscopy [12],\nX-ray [104], underwater [52, 100], aerial [17], simula-\ntion [86], driving [25], and painting [24] images. For ef-\nficient evaluation we subsampled datasets with more than\n15k masks. Specifically, we randomly picked images SO\nthat the total number of masks in the sampled images was\n~10k. We blurred faces of people in all the datasets."
    },
    {
      "bounding_box": [
        {
          "x": 1277,
          "y": 305
        },
        {
          "x": 2277,
          "y": 305
        },
        {
          "x": 2277,
          "y": 904
        },
        {
          "x": 1277,
          "y": 904
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='197' style='font-size:18px'>Point sampling. Our default point sampling follows stan-<br>dard practice in interactive segmentation [109, 64, 92]. The<br>first point is chosen deterministically as the point farthest<br>from the object boundary. Each subsequent point is the<br>farthest from the boundary of the error region between<br>ground truth and the previous prediction. Some experiments<br>(where specified) use a more challenging sampling strategy<br>in which the first point is a random point, rather than a deter-<br>ministically selected \"center\" point. Each subsequent point<br>is selected as described above. This setting better reflects<br>use cases in which the first point is not reliably near the<br>center of the mask, such as prompting from eye gaze.</p>",
      "id": 197,
      "page": 19,
      "text": "Point sampling. Our default point sampling follows stan-\ndard practice in interactive segmentation [109, 64, 92]. The\nfirst point is chosen deterministically as the point farthest\nfrom the object boundary. Each subsequent point is the\nfarthest from the boundary of the error region between\nground truth and the previous prediction. Some experiments\n(where specified) use a more challenging sampling strategy\nin which the first point is a random point, rather than a deter-\nministically selected \"center\" point. Each subsequent point\nis selected as described above. This setting better reflects\nuse cases in which the first point is not reliably near the\ncenter of the mask, such as prompting from eye gaze."
    },
    {
      "bounding_box": [
        {
          "x": 1277,
          "y": 930
        },
        {
          "x": 2277,
          "y": 930
        },
        {
          "x": 2277,
          "y": 1678
        },
        {
          "x": 1277,
          "y": 1678
        }
      ],
      "category": "paragraph",
      "html": "<p id='198' style='font-size:16px'>Evaluation. We measure IoU between a prediction after<br>N point prompts and a ground truth mask, where N =<br>{1, 2, 3, 5, 9} and points are sampled iteratively with either<br>of the strategies described above. The per-dataset mIoU is<br>the per-mask IoU averaged across all objects in the dataset.<br>Finally, we report the top-line metric by averaging the per-<br>dataset mIoUs across all 23 datasets. Our evaluation differs<br>from the standard interactive segmentation evaluation pro-<br>tocol which measures the average number of points needed<br>to achieve X% IoU, with up to 20 points. We focus on pre-<br>dictions after just one, or possibly a few points, since many<br>of our use cases involve a single or very few prompts. Given<br>our application focus, which requires real-time prompt pro-<br>cessing, we expect the best interactive segmentation models<br>to outperform SAM when using a large number of points.</p>",
      "id": 198,
      "page": 19,
      "text": "Evaluation. We measure IoU between a prediction after\nN point prompts and a ground truth mask, where N =\n{1, 2, 3, 5, 9} and points are sampled iteratively with either\nof the strategies described above. The per-dataset mIoU is\nthe per-mask IoU averaged across all objects in the dataset.\nFinally, we report the top-line metric by averaging the per-\ndataset mIoUs across all 23 datasets. Our evaluation differs\nfrom the standard interactive segmentation evaluation pro-\ntocol which measures the average number of points needed\nto achieve X% IoU, with up to 20 points. We focus on pre-\ndictions after just one, or possibly a few points, since many\nof our use cases involve a single or very few prompts. Given\nour application focus, which requires real-time prompt pro-\ncessing, we expect the best interactive segmentation models\nto outperform SAM when using a large number of points."
    },
    {
      "bounding_box": [
        {
          "x": 1277,
          "y": 1702
        },
        {
          "x": 2278,
          "y": 1702
        },
        {
          "x": 2278,
          "y": 2550
        },
        {
          "x": 1277,
          "y": 2550
        }
      ],
      "category": "paragraph",
      "html": "<p id='199' style='font-size:14px'>Baselines. We use three recent strong interactive base-<br>lines: RITM [92], FocalClick [18], and SimpleClick [67].<br>For each, we use the largest models trained on the broad-<br>est datasets publicly released by the authors. For RITM,<br>we use HRNet32 IT-M trained on the combination of<br>COCO [66] and LVIS [44] introduced by the authors.<br>For FocalClick, we use SegF ormerB3-S2 trained on a<br>\"combined dataset\" that includes 8 different segmentation<br>datasets [18]. For SimpleClick, we use ViT-H448 trained<br>on a combination of COCO and LVIS. We follow the sug-<br>gested default strategies for data pre-processing (i.e., data<br>augmentations or image resizing) and do not change or<br>adapt any parameters for our evaluation. In our experi-<br>ments, we observe that RITM outperforms other baselines<br>on our 23 dataset suite with 1 point evaluation. Therefore,<br>we use RITM as the default baseline. When evaluating with<br>more points we report results for all baselines.</p>",
      "id": 199,
      "page": 19,
      "text": "Baselines. We use three recent strong interactive base-\nlines: RITM [92], FocalClick [18], and SimpleClick [67].\nFor each, we use the largest models trained on the broad-\nest datasets publicly released by the authors. For RITM,\nwe use HRNet32 IT-M trained on the combination of\nCOCO [66] and LVIS [44] introduced by the authors.\nFor FocalClick, we use SegF ormerB3-S2 trained on a\n\"combined dataset\" that includes 8 different segmentation\ndatasets [18]. For SimpleClick, we use ViT-H448 trained\non a combination of COCO and LVIS. We follow the sug-\ngested default strategies for data pre-processing (i.e., data\naugmentations or image resizing) and do not change or\nadapt any parameters for our evaluation. In our experi-\nments, we observe that RITM outperforms other baselines\non our 23 dataset suite with 1 point evaluation. Therefore,\nwe use RITM as the default baseline. When evaluating with\nmore points we report results for all baselines."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 2577
        },
        {
          "x": 2277,
          "y": 2577
        },
        {
          "x": 2277,
          "y": 2977
        },
        {
          "x": 1280,
          "y": 2977
        }
      ],
      "category": "paragraph",
      "html": "<p id='200' style='font-size:18px'>Single point ambiguity and oracle evaluation. In addition<br>to IoU after N points prompts, we report SAM's \"oracle\"<br>performance at 1 point by evaluating the predicted mask that<br>best matches ground truth from amongst SAM's three pre-<br>dictions (rather than using the one that SAM itself ranks<br>first, as we do by default). This protocol addresses possible<br>single point prompt ambiguity by relaxing the requirement<br>to guess the one right mask among several valid objects.</p>",
      "id": 200,
      "page": 19,
      "text": "Single point ambiguity and oracle evaluation. In addition\nto IoU after N points prompts, we report SAM's \"oracle\"\nperformance at 1 point by evaluating the predicted mask that\nbest matches ground truth from amongst SAM's three pre-\ndictions (rather than using the one that SAM itself ranks\nfirst, as we do by default). This protocol addresses possible\nsingle point prompt ambiguity by relaxing the requirement\nto guess the one right mask among several valid objects."
    },
    {
      "bounding_box": [
        {
          "x": 1218,
          "y": 3052
        },
        {
          "x": 1264,
          "y": 3052
        },
        {
          "x": 1264,
          "y": 3094
        },
        {
          "x": 1218,
          "y": 3094
        }
      ],
      "category": "footer",
      "html": "<footer id='201' style='font-size:16px'>19</footer>",
      "id": 201,
      "page": 19,
      "text": "19"
    },
    {
      "bounding_box": [
        {
          "x": 214,
          "y": 347
        },
        {
          "x": 2279,
          "y": 347
        },
        {
          "x": 2279,
          "y": 2645
        },
        {
          "x": 214,
          "y": 2645
        }
      ],
      "category": "table",
      "html": "<table id='202' style='font-size:14px'><tr><td>dataset</td><td>abbreviation & link</td><td>image type</td><td>description</td><td>mask type</td><td>source split</td><td># images sampled</td><td># masks sampled</td></tr><tr><td>Plant Phenotyping Datasets Leaf Segmentation [74]</td><td>PPDLS</td><td>Plants</td><td>Leaf segmentation for images of tobacco and ara plants.</td><td>Instance</td><td>N/A</td><td>182</td><td>2347</td></tr><tr><td>BBBC038v1 from Broad Bioimage Benchmark Collection [12]</td><td>BBBC038v1</td><td>Microscopy</td><td>Biological images of cells in a variety of settings testing robustness in nuclei segmentation.</td><td>Instance</td><td>Train</td><td>227</td><td>10506</td></tr><tr><td>Dataset fOr bOuldeRs Segmentation [80]</td><td>DOORS</td><td>Boulders</td><td>Segmentation masks of single boulders positioned on the surface of a spherical mesh.</td><td>Instance</td><td>DS1</td><td>10000</td><td>10000</td></tr><tr><td>TimberSeg 1.0 [38]</td><td>TimberSeg</td><td>Logs</td><td>Segmentation masks of individual logs in piles of timber in various environments and conditions. Images are taken from an operator's point-of-view.</td><td>Instance</td><td>N/A</td><td>220</td><td>2487</td></tr><tr><td>Northumberland Dolphin Dataset 2020 [100]</td><td>NDD20</td><td>Underwater</td><td>Segmentation masks of two different dolphin species in images taken above and under water.</td><td>Instance</td><td>N/A</td><td>4402</td><td>6100</td></tr><tr><td>Large Vocabulary Instance Segmentation [44]</td><td>LVIS</td><td>Scenes</td><td>Additional annotations for the COCO [66] dataset to enable the study of long-tailed object detection and segmentation.</td><td>Instance</td><td>Validation (v0.5)</td><td>945</td><td>9642</td></tr><tr><td>STREETS [91]</td><td>STREETS</td><td>Traffic camera</td><td>Segmentation masks of cars in traffic camera footage.</td><td>Instance</td><td>N/A</td><td>819</td><td>9854</td></tr><tr><td>ZeroWaste-f [6]</td><td>Zero Waste-f</td><td>Recycling</td><td>Segmentation masks in cluttered scenes of deformed recycling waste.</td><td>Instance</td><td>Train</td><td>2947</td><td>6155</td></tr><tr><td>iShape [111]</td><td>iShape</td><td>Irregular shapes</td><td>Segmentation masks of irregular shapes like antennas, logs, fences, and hangers.</td><td>Instance</td><td>Validation</td><td>754</td><td>9742</td></tr><tr><td>ADE20K [117]</td><td>ADE20K</td><td>Scenes</td><td>Object and part segmentation masks for images from SUN [107] and Places [116] datasets.</td><td>Instance</td><td>Validation</td><td>302</td><td>10128</td></tr><tr><td>Occluded Video Instance Segmentation [81]</td><td>OVIS</td><td>Occlusions</td><td>Instance segmentation masks in videos, focusing on objects that are occluded.</td><td>Instance</td><td>Train</td><td>2044</td><td>10011</td></tr><tr><td>Hypersim [86]</td><td>Hypersim</td><td>Simulation</td><td>Photorealistic synthetic dataset of indoor scenes with instance masks.</td><td>Instance</td><td>Evermotion archinteriors volumes 1-55 excluding 20,25,40,49</td><td>338</td><td>9445</td></tr><tr><td>Night and Day Instance Segmented Park [22, 23]</td><td>NDISPark</td><td>Parking lots</td><td>Images of parking lots from video footage taken at day and night during different weather conditions and camera angles for vehicle segmentation.</td><td>Instance</td><td>Train</td><td>111</td><td>2577</td></tr><tr><td>EPIC-KITCHENS VISOR [28,27]</td><td>VISOR</td><td>Egocentric</td><td>Segmentation masks for hands and active objects in ego-centric video from the cooking dataset EPIC-KITCHENS [27].</td><td>Instance</td><td>Validation</td><td>1864</td><td>10141</td></tr><tr><td>Plittersdorf dataset [46]</td><td>Plittersdorf</td><td>Stereo images</td><td>Segmentation masks of wildlife in images taken with the SOCRATES stereo camera trap.</td><td>Instance</td><td>Train, validation, test</td><td>187</td><td>546</td></tr><tr><td>Egocentric Hand-Object Segmentation [113]</td><td>EgoHOS</td><td>Egocentric</td><td>Fine-grained egocentric hand-object segmentation dataset. Dataset contains mask annotations for existing datasets.</td><td>Instance</td><td>Train (including only Ego4D [43] and THU-READ [97, 96])</td><td>2940</td><td>9961</td></tr><tr><td>InstanceBuilding 2D [17]</td><td>IBD</td><td>Drones</td><td>High-resolution drone UAV images annotated with roof instance segmentation masks.</td><td>Instance</td><td>Train (2D annotations)</td><td>467</td><td>11953</td></tr><tr><td>WoodScape [112]</td><td>WoodScape</td><td>Fisheye driving</td><td>Fisheye driving dataset with segmentation masks. Images are taken from four surround-view cameras.</td><td>Instance</td><td>Set 1</td><td>107</td><td>10266</td></tr><tr><td>Cityscapes [25]</td><td>Cityscapes</td><td>Driving</td><td>Stereo video of street scenes with segmentation masks.</td><td>Panoptic</td><td>Validation</td><td>293</td><td>9973</td></tr><tr><td>PIDray [104]</td><td>PIDRay</td><td>X-ray</td><td>Segmentation masks of prohibited items in X-ray images of baggage.</td><td>Instance</td><td>Test (hard)</td><td>3733</td><td>8892</td></tr><tr><td>Diverse Realism in Art Movements [24]</td><td>DRAM</td><td>Paintings</td><td>Domain adaptation dataset for semantic segmentation of art paintings.</td><td>Semantic</td><td>Test</td><td>718</td><td>1179</td></tr><tr><td>TrashCan [52]</td><td>TrashCan</td><td>Underwater</td><td>Segmentation masks of trash in images taken by underwater ROVs. Images are sourced from the J-EDI [69] dataset.</td><td>Instance</td><td>Train (instance task)</td><td>5936</td><td>9540</td></tr><tr><td>Georgia Tech Egocentric Activity Datasets [34, 63]</td><td>GTEA</td><td>Egocentric</td><td>Videos are composed of four different subjects performing seven types of daily activities with segmentation masks of hands.</td><td>Instance</td><td>Train (segmenting hands task)</td><td>652</td><td>1208</td></tr></table>",
      "id": 202,
      "page": 20,
      "text": "dataset abbreviation & link image type description mask type source split # images sampled # masks sampled\n Plant Phenotyping Datasets Leaf Segmentation [74] PPDLS Plants Leaf segmentation for images of tobacco and ara plants. Instance N/A 182 2347\n BBBC038v1 from Broad Bioimage Benchmark Collection [12] BBBC038v1 Microscopy Biological images of cells in a variety of settings testing robustness in nuclei segmentation. Instance Train 227 10506\n Dataset fOr bOuldeRs Segmentation [80] DOORS Boulders Segmentation masks of single boulders positioned on the surface of a spherical mesh. Instance DS1 10000 10000\n TimberSeg 1.0 [38] TimberSeg Logs Segmentation masks of individual logs in piles of timber in various environments and conditions. Images are taken from an operator's point-of-view. Instance N/A 220 2487\n Northumberland Dolphin Dataset 2020 [100] NDD20 Underwater Segmentation masks of two different dolphin species in images taken above and under water. Instance N/A 4402 6100\n Large Vocabulary Instance Segmentation [44] LVIS Scenes Additional annotations for the COCO [66] dataset to enable the study of long-tailed object detection and segmentation. Instance Validation (v0.5) 945 9642\n STREETS [91] STREETS Traffic camera Segmentation masks of cars in traffic camera footage. Instance N/A 819 9854\n ZeroWaste-f [6] Zero Waste-f Recycling Segmentation masks in cluttered scenes of deformed recycling waste. Instance Train 2947 6155\n iShape [111] iShape Irregular shapes Segmentation masks of irregular shapes like antennas, logs, fences, and hangers. Instance Validation 754 9742\n ADE20K [117] ADE20K Scenes Object and part segmentation masks for images from SUN [107] and Places [116] datasets. Instance Validation 302 10128\n Occluded Video Instance Segmentation [81] OVIS Occlusions Instance segmentation masks in videos, focusing on objects that are occluded. Instance Train 2044 10011\n Hypersim [86] Hypersim Simulation Photorealistic synthetic dataset of indoor scenes with instance masks. Instance Evermotion archinteriors volumes 1-55 excluding 20,25,40,49 338 9445\n Night and Day Instance Segmented Park [22, 23] NDISPark Parking lots Images of parking lots from video footage taken at day and night during different weather conditions and camera angles for vehicle segmentation. Instance Train 111 2577\n EPIC-KITCHENS VISOR [28,27] VISOR Egocentric Segmentation masks for hands and active objects in ego-centric video from the cooking dataset EPIC-KITCHENS [27]. Instance Validation 1864 10141\n Plittersdorf dataset [46] Plittersdorf Stereo images Segmentation masks of wildlife in images taken with the SOCRATES stereo camera trap. Instance Train, validation, test 187 546\n Egocentric Hand-Object Segmentation [113] EgoHOS Egocentric Fine-grained egocentric hand-object segmentation dataset. Dataset contains mask annotations for existing datasets. Instance Train (including only Ego4D [43] and THU-READ [97, 96]) 2940 9961\n InstanceBuilding 2D [17] IBD Drones High-resolution drone UAV images annotated with roof instance segmentation masks. Instance Train (2D annotations) 467 11953\n WoodScape [112] WoodScape Fisheye driving Fisheye driving dataset with segmentation masks. Images are taken from four surround-view cameras. Instance Set 1 107 10266\n Cityscapes [25] Cityscapes Driving Stereo video of street scenes with segmentation masks. Panoptic Validation 293 9973\n PIDray [104] PIDRay X-ray Segmentation masks of prohibited items in X-ray images of baggage. Instance Test (hard) 3733 8892\n Diverse Realism in Art Movements [24] DRAM Paintings Domain adaptation dataset for semantic segmentation of art paintings. Semantic Test 718 1179\n TrashCan [52] TrashCan Underwater Segmentation masks of trash in images taken by underwater ROVs. Images are sourced from the J-EDI [69] dataset. Instance Train (instance task) 5936 9540\n Georgia Tech Egocentric Activity Datasets [34, 63] GTEA Egocentric Videos are composed of four different subjects performing seven types of daily activities with segmentation masks of hands. Instance Train (segmenting hands task) 652"
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 2674
        },
        {
          "x": 2278,
          "y": 2674
        },
        {
          "x": 2278,
          "y": 2828
        },
        {
          "x": 202,
          "y": 2828
        }
      ],
      "category": "caption",
      "html": "<caption id='203' style='font-size:20px'>Table 7: Segmentation datasets used to evaluate zero-shot segmentation with point prompts. The 23 datasets cover a broad<br>range of domains; see column \"image type\". To make our evaluation efficient, we subsample datasets that have more than<br>15k masks. Specifically, we randomly sampled images SO that the total number of masks in the images is ~10k.</caption>",
      "id": 203,
      "page": 20,
      "text": "Table 7: Segmentation datasets used to evaluate zero-shot segmentation with point prompts. The 23 datasets cover a broad\nrange of domains; see column \"image type\". To make our evaluation efficient, we subsample datasets that have more than\n15k masks. Specifically, we randomly sampled images SO that the total number of masks in the images is ~10k."
    },
    {
      "bounding_box": [
        {
          "x": 1216,
          "y": 3053
        },
        {
          "x": 1264,
          "y": 3053
        },
        {
          "x": 1264,
          "y": 3091
        },
        {
          "x": 1216,
          "y": 3091
        }
      ],
      "category": "footer",
      "html": "<footer id='204' style='font-size:16px'>20</footer>",
      "id": 204,
      "page": 20,
      "text": "20"
    },
    {
      "bounding_box": [
        {
          "x": 199,
          "y": 293
        },
        {
          "x": 2274,
          "y": 293
        },
        {
          "x": 2274,
          "y": 801
        },
        {
          "x": 199,
          "y": 801
        }
      ],
      "category": "figure",
      "html": "<figure><img id='205' style='font-size:14px' alt=\"image ground truth SAM image ground truth SAM\nU\" data-coord=\"top-left:(199,293); bottom-right:(2274,801)\" /></figure>",
      "id": 205,
      "page": 21,
      "text": "image ground truth SAM image ground truth SAM\nU"
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 827
        },
        {
          "x": 2276,
          "y": 827
        },
        {
          "x": 2276,
          "y": 927
        },
        {
          "x": 202,
          "y": 927
        }
      ],
      "category": "caption",
      "html": "<caption id='206' style='font-size:16px'>Figure 15: Additional visualizations of zero-shot edge predictions on BSDS500. Recall that SAM was not trained to predict<br>edge maps and did not have access to BSDS images and annotations during training.</caption>",
      "id": 206,
      "page": 21,
      "text": "Figure 15: Additional visualizations of zero-shot edge predictions on BSDS500. Recall that SAM was not trained to predict\nedge maps and did not have access to BSDS images and annotations during training."
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 990
        },
        {
          "x": 804,
          "y": 990
        },
        {
          "x": 804,
          "y": 1039
        },
        {
          "x": 204,
          "y": 1039
        }
      ],
      "category": "paragraph",
      "html": "<p id='207' style='font-size:20px'>D.2. Zero-Shot Edge Detection</p>",
      "id": 207,
      "page": 21,
      "text": "D.2. Zero-Shot Edge Detection"
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 1083
        },
        {
          "x": 1200,
          "y": 1083
        },
        {
          "x": 1200,
          "y": 1430
        },
        {
          "x": 201,
          "y": 1430
        }
      ],
      "category": "paragraph",
      "html": "<p id='208' style='font-size:18px'>Dataset and metrics. We perform zero-shot edge detection<br>experiments on BSDS500 [72, 3]. The ground truth for each<br>image comes from the manual annotations of five different<br>subjects. We report results on the 200 image test subset<br>using the four standard metrics for edge detection [3, 32]:<br>optimal dataset scale (ODS), optimal image scale (OIS), av-<br>erage precision (AP), and recall at 50% precision (R50).</p>",
      "id": 208,
      "page": 21,
      "text": "Dataset and metrics. We perform zero-shot edge detection\nexperiments on BSDS500 [72, 3]. The ground truth for each\nimage comes from the manual annotations of five different\nsubjects. We report results on the 200 image test subset\nusing the four standard metrics for edge detection [3, 32]:\noptimal dataset scale (ODS), optimal image scale (OIS), av-\nerage precision (AP), and recall at 50% precision (R50)."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 1445
        },
        {
          "x": 1199,
          "y": 1445
        },
        {
          "x": 1199,
          "y": 1993
        },
        {
          "x": 201,
          "y": 1993
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='209' style='font-size:16px'>Method. For zero-shot transfer, we use a simplified ver-<br>sion of our automatic mask generation pipeline. We prompt<br>SAM with a 16x16 regular grid of foreground points,<br>which yields 768 predicted masks (three per point). We do<br>not filter by predicted IoU or stability. Redundant masks<br>are removed by NMS. Then we apply a Sobel filter to the<br>remaining masks' unthresholded probability maps and set<br>values to zero if they do not intersect with the outer bound-<br>ary pixels of a mask. Finally, we take a pixel-wise max over<br>all the predictions, linearly normalize the result to [0,1], and<br>apply edge NMS [13] to thin the edges.</p>",
      "id": 209,
      "page": 21,
      "text": "Method. For zero-shot transfer, we use a simplified ver-\nsion of our automatic mask generation pipeline. We prompt\nSAM with a 16x16 regular grid of foreground points,\nwhich yields 768 predicted masks (three per point). We do\nnot filter by predicted IoU or stability. Redundant masks\nare removed by NMS. Then we apply a Sobel filter to the\nremaining masks' unthresholded probability maps and set\nvalues to zero if they do not intersect with the outer bound-\nary pixels of a mask. Finally, we take a pixel-wise max over\nall the predictions, linearly normalize the result to [0,1], and\napply edge NMS [13] to thin the edges."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 2009
        },
        {
          "x": 1199,
          "y": 2009
        },
        {
          "x": 1199,
          "y": 2456
        },
        {
          "x": 202,
          "y": 2456
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='210' style='font-size:16px'>Visualizations. In Fig. 15, we show additional examples<br>of zero-shot edge predictions from SAM. These qualitative<br>examples further illustrate how SAM tends to output sensi-<br>ble edge maps, despite not being trained for edge detection.<br>We see that the edges can align well with the human anno-<br>tations. Although, as previously mentioned, since SAM is<br>not trained for edge detection it does not learn the biases of<br>the BSDS500 dataset and often outputs more edges than are<br>present in the ground truth annotations.</p>",
      "id": 210,
      "page": 21,
      "text": "Visualizations. In Fig. 15, we show additional examples\nof zero-shot edge predictions from SAM. These qualitative\nexamples further illustrate how SAM tends to output sensi-\nble edge maps, despite not being trained for edge detection.\nWe see that the edges can align well with the human anno-\ntations. Although, as previously mentioned, since SAM is\nnot trained for edge detection it does not learn the biases of\nthe BSDS500 dataset and often outputs more edges than are\npresent in the ground truth annotations."
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 2486
        },
        {
          "x": 841,
          "y": 2486
        },
        {
          "x": 841,
          "y": 2535
        },
        {
          "x": 204,
          "y": 2535
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='211' style='font-size:22px'>D.3. Zero-Shot Object Proposals</p>",
      "id": 211,
      "page": 21,
      "text": "D.3. Zero-Shot Object Proposals"
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 2578
        },
        {
          "x": 1199,
          "y": 2578
        },
        {
          "x": 1199,
          "y": 2976
        },
        {
          "x": 202,
          "y": 2976
        }
      ],
      "category": "paragraph",
      "html": "<p id='212' style='font-size:16px'>Dataset and metrics. We report the standard average recall<br>(AR) metric for masks at 1000 proposals on the LVIS v1<br>validation set [44]. Since LVIS has high-quality masks for<br>1203 object classes, it provides a challenging test for ob-<br>ject proposal generation. We focus on AR@ 1000 due to the<br>open-world nature of our model, which will likely produce<br>many valid masks outside even the 1203 classes in LVIS. To<br>measure performance on frequent, common, and rare cate-</p>",
      "id": 212,
      "page": 21,
      "text": "Dataset and metrics. We report the standard average recall\n(AR) metric for masks at 1000 proposals on the LVIS v1\nvalidation set [44]. Since LVIS has high-quality masks for\n1203 object classes, it provides a challenging test for ob-\nject proposal generation. We focus on AR@ 1000 due to the\nopen-world nature of our model, which will likely produce\nmany valid masks outside even the 1203 classes in LVIS. To\nmeasure performance on frequent, common, and rare cate-"
    },
    {
      "bounding_box": [
        {
          "x": 1282,
          "y": 995
        },
        {
          "x": 2276,
          "y": 995
        },
        {
          "x": 2276,
          "y": 1092
        },
        {
          "x": 1282,
          "y": 1092
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='213' style='font-size:16px'>gories, we use AR@ 1000 but measured against a ground<br>truth set containing just the corresponding LVIS categories.</p>",
      "id": 213,
      "page": 21,
      "text": "gories, we use AR@ 1000 but measured against a ground\ntruth set containing just the corresponding LVIS categories."
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 1142
        },
        {
          "x": 2277,
          "y": 1142
        },
        {
          "x": 2277,
          "y": 1543
        },
        {
          "x": 1279,
          "y": 1543
        }
      ],
      "category": "paragraph",
      "html": "<p id='214' style='font-size:14px'>Baseline. We use cascade ViTDet-H as a baseline, the<br>strongest model from [62] by AP on LVIS. As noted in the<br>main text, an object detector trained in-domain can \"game\"<br>AR [16] and is expected to be a stronger baseline than other<br>models that focus on open-world proposals or segmenta-<br>tion [58, 105]. To produce 1000 proposals, we disable score<br>thresholding in the three cascade stages and as raise the<br>maximum number of predictions per stage to 1000.</p>",
      "id": 214,
      "page": 21,
      "text": "Baseline. We use cascade ViTDet-H as a baseline, the\nstrongest model from [62] by AP on LVIS. As noted in the\nmain text, an object detector trained in-domain can \"game\"\nAR [16] and is expected to be a stronger baseline than other\nmodels that focus on open-world proposals or segmenta-\ntion [58, 105]. To produce 1000 proposals, we disable score\nthresholding in the three cascade stages and as raise the\nmaximum number of predictions per stage to 1000."
    },
    {
      "bounding_box": [
        {
          "x": 1277,
          "y": 1592
        },
        {
          "x": 2278,
          "y": 1592
        },
        {
          "x": 2278,
          "y": 2194
        },
        {
          "x": 1277,
          "y": 2194
        }
      ],
      "category": "paragraph",
      "html": "<p id='215' style='font-size:16px'>Method. We use a modified version of SAM's automatic<br>mask generation pipeline for zero-shot transfer. First, to<br>make inference time comparable to that of ViTDet we do<br>not process image crops. Second, we remove filtering by<br>predicted IoU and stability. This leaves two tunable param-<br>eters to get ~ 1000 masks per image: the input point grid and<br>the NMS threshold duplicate mask suppression. We choose<br>a 64 x64 point grid and an NMS threshold of 0.9, which<br>produces ~900 masks per image on average. At evaluation,<br>if greater than 1000 masks have been proposed in an im-<br>age, they are ranked by the average of their confidence and<br>stability scores, then truncated to the top 1000 proposals.</p>",
      "id": 215,
      "page": 21,
      "text": "Method. We use a modified version of SAM's automatic\nmask generation pipeline for zero-shot transfer. First, to\nmake inference time comparable to that of ViTDet we do\nnot process image crops. Second, we remove filtering by\npredicted IoU and stability. This leaves two tunable param-\neters to get ~ 1000 masks per image: the input point grid and\nthe NMS threshold duplicate mask suppression. We choose\na 64 x64 point grid and an NMS threshold of 0.9, which\nproduces ~900 masks per image on average. At evaluation,\nif greater than 1000 masks have been proposed in an im-\nage, they are ranked by the average of their confidence and\nstability scores, then truncated to the top 1000 proposals."
    },
    {
      "bounding_box": [
        {
          "x": 1277,
          "y": 2228
        },
        {
          "x": 2277,
          "y": 2228
        },
        {
          "x": 2277,
          "y": 2977
        },
        {
          "x": 1277,
          "y": 2977
        }
      ],
      "category": "paragraph",
      "html": "<p id='216' style='font-size:16px'>We hypothesize that SAM's ability to output multiple<br>masks is especially valuable for this task, since recall should<br>benefit from proposals generated at multiple scales from<br>a single input point. To test this, we compare to an ab-<br>lated version SAM that only outputs a single mask instead<br>of three (SAM - single-output). Since this model produces<br>fewer masks, we further increase the number of points sam-<br>pled and NMS threshold to 128 x 128 and 0.95, respectively,<br>obtaining ~950 masks per image on average. Additionally,<br>single-output SAM does not produce the IoU score used<br>to rank masks for NMS in the automatic mask generation<br>pipeline, SO instead masks are ranked randomly. Testing<br>suggests this has similar performance to more sophisticated<br>methods of ranking masks, such as using the max logit value<br>of the mask as a proxy for model confidence.</p>",
      "id": 216,
      "page": 21,
      "text": "We hypothesize that SAM's ability to output multiple\nmasks is especially valuable for this task, since recall should\nbenefit from proposals generated at multiple scales from\na single input point. To test this, we compare to an ab-\nlated version SAM that only outputs a single mask instead\nof three (SAM - single-output). Since this model produces\nfewer masks, we further increase the number of points sam-\npled and NMS threshold to 128 x 128 and 0.95, respectively,\nobtaining ~950 masks per image on average. Additionally,\nsingle-output SAM does not produce the IoU score used\nto rank masks for NMS in the automatic mask generation\npipeline, SO instead masks are ranked randomly. Testing\nsuggests this has similar performance to more sophisticated\nmethods of ranking masks, such as using the max logit value\nof the mask as a proxy for model confidence."
    },
    {
      "bounding_box": [
        {
          "x": 1216,
          "y": 3054
        },
        {
          "x": 1258,
          "y": 3054
        },
        {
          "x": 1258,
          "y": 3093
        },
        {
          "x": 1216,
          "y": 3093
        }
      ],
      "category": "footer",
      "html": "<footer id='217' style='font-size:16px'>21</footer>",
      "id": 217,
      "page": 21,
      "text": "21"
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 288
        },
        {
          "x": 2276,
          "y": 288
        },
        {
          "x": 2276,
          "y": 822
        },
        {
          "x": 200,
          "y": 822
        }
      ],
      "category": "figure",
      "html": "<figure><img id='218' style='font-size:14px' alt=\"ground truth ViTDet SAM ground truth ViTDet SAM\" data-coord=\"top-left:(200,288); bottom-right:(2276,822)\" /></figure>",
      "id": 218,
      "page": 22,
      "text": "ground truth ViTDet SAM ground truth ViTDet SAM"
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 838
        },
        {
          "x": 2275,
          "y": 838
        },
        {
          "x": 2275,
          "y": 988
        },
        {
          "x": 202,
          "y": 988
        }
      ],
      "category": "caption",
      "html": "<br><caption id='219' style='font-size:16px'>Figure 16: Zero-shot instance segmentation on LVIS v1. SAM produces higher quality masks than ViTDet. As a zero-shot<br>model, SAM does not have the opportunity to learn specific training data biases; see top-right as an example where SAM<br>makes a modal prediction, whereas the ground truth in LVIS is amodal given that mask annotations in LVIS have no holes.</caption>",
      "id": 219,
      "page": 22,
      "text": "Figure 16: Zero-shot instance segmentation on LVIS v1. SAM produces higher quality masks than ViTDet. As a zero-shot\nmodel, SAM does not have the opportunity to learn specific training data biases; see top-right as an example where SAM\nmakes a modal prediction, whereas the ground truth in LVIS is amodal given that mask annotations in LVIS have no holes."
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 1040
        },
        {
          "x": 951,
          "y": 1040
        },
        {
          "x": 951,
          "y": 1090
        },
        {
          "x": 204,
          "y": 1090
        }
      ],
      "category": "paragraph",
      "html": "<p id='220' style='font-size:22px'>D.4. Zero-Shot Instance Segmentation</p>",
      "id": 220,
      "page": 22,
      "text": "D.4. Zero-Shot Instance Segmentation"
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 1131
        },
        {
          "x": 1199,
          "y": 1131
        },
        {
          "x": 1199,
          "y": 1835
        },
        {
          "x": 200,
          "y": 1835
        }
      ],
      "category": "paragraph",
      "html": "<p id='221' style='font-size:16px'>Method. For zero-shot instance segmentation, we prompt<br>SAM with the boxes output by a fully-supervised ViTDet-H<br>on COCO and LVIS v1 validation splits. We apply an ad-<br>ditional mask refinement iteration by feeding the most con-<br>fident predicted mask, together with the box prompt, back<br>to the mask decoder to produce the final prediction. We<br>show zero-shot instance segmentations predicted on LVIS<br>in Fig. 16. Compared to ViTDet, SAM tends to produce<br>higher quality masks with cleaner boundaries. We confirm<br>this observation with human studies in §7.4. Note that as a<br>zero-shot model, SAM is not able to learn annotation biases<br>in a dataset. For instance, we see that SAM makes a valid<br>modal prediction for the plate, whereas LVIS masks cannot<br>contain holes by design SO the plate is annotated amodally.</p>",
      "id": 221,
      "page": 22,
      "text": "Method. For zero-shot instance segmentation, we prompt\nSAM with the boxes output by a fully-supervised ViTDet-H\non COCO and LVIS v1 validation splits. We apply an ad-\nditional mask refinement iteration by feeding the most con-\nfident predicted mask, together with the box prompt, back\nto the mask decoder to produce the final prediction. We\nshow zero-shot instance segmentations predicted on LVIS\nin Fig. 16. Compared to ViTDet, SAM tends to produce\nhigher quality masks with cleaner boundaries. We confirm\nthis observation with human studies in §7.4. Note that as a\nzero-shot model, SAM is not able to learn annotation biases\nin a dataset. For instance, we see that SAM makes a valid\nmodal prediction for the plate, whereas LVIS masks cannot\ncontain holes by design SO the plate is annotated amodally."
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 1869
        },
        {
          "x": 771,
          "y": 1869
        },
        {
          "x": 771,
          "y": 1918
        },
        {
          "x": 204,
          "y": 1918
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='222' style='font-size:20px'>D.5. Zero-Shot Text-to-Mask</p>",
      "id": 222,
      "page": 22,
      "text": "D.5. Zero-Shot Text-to-Mask"
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 1963
        },
        {
          "x": 1199,
          "y": 1963
        },
        {
          "x": 1199,
          "y": 2361
        },
        {
          "x": 202,
          "y": 2361
        }
      ],
      "category": "paragraph",
      "html": "<p id='223' style='font-size:18px'>Model and training. We use the largest publicly available<br>CLIP model [82] (ViT-L/ 1 4@33 6px) to compute text<br>and image embeddings, which we l2 normalize prior to use.<br>To train SAM, we use masks from the first two stages of our<br>data engine. Moreover, we discard all masks with an area<br>smaller than 1002 pixels. We train this model with large-<br>scale jitter [40] for 120k iterations with batch size 128. All<br>other training parameters follow our default settings.</p>",
      "id": 223,
      "page": 22,
      "text": "Model and training. We use the largest publicly available\nCLIP model [82] (ViT-L/ 1 4@33 6px) to compute text\nand image embeddings, which we l2 normalize prior to use.\nTo train SAM, we use masks from the first two stages of our\ndata engine. Moreover, we discard all masks with an area\nsmaller than 1002 pixels. We train this model with large-\nscale jitter [40] for 120k iterations with batch size 128. All\nother training parameters follow our default settings."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 2380
        },
        {
          "x": 1198,
          "y": 2380
        },
        {
          "x": 1198,
          "y": 2977
        },
        {
          "x": 201,
          "y": 2977
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='224' style='font-size:16px'>Generating training prompts. To extract an input prompt<br>we first expand the bounding box around each mask by a<br>random factor from 1 x to 2x, square-crop the expanded<br>box to maintain its aspect ratio, and resize it to 336x336<br>pixels. Before feeding the crop to the CLIP image encoder,<br>with 50% probability we zero-out pixels outside the mask.<br>To ensure the embedding focuses on the object, we use<br>masked attention in the last layer to restrict attention from<br>the output token to the image positions inside the mask. Fi-<br>nally, our prompt is the output token embedding. For train-<br>ing we supply the CLIP-based prompt first, followed by ad-<br>ditional iterative point prompts to refine the prediction.</p>",
      "id": 224,
      "page": 22,
      "text": "Generating training prompts. To extract an input prompt\nwe first expand the bounding box around each mask by a\nrandom factor from 1 x to 2x, square-crop the expanded\nbox to maintain its aspect ratio, and resize it to 336x336\npixels. Before feeding the crop to the CLIP image encoder,\nwith 50% probability we zero-out pixels outside the mask.\nTo ensure the embedding focuses on the object, we use\nmasked attention in the last layer to restrict attention from\nthe output token to the image positions inside the mask. Fi-\nnally, our prompt is the output token embedding. For train-\ning we supply the CLIP-based prompt first, followed by ad-\nditional iterative point prompts to refine the prediction."
    },
    {
      "bounding_box": [
        {
          "x": 1282,
          "y": 1030
        },
        {
          "x": 2273,
          "y": 1030
        },
        {
          "x": 2273,
          "y": 1622
        },
        {
          "x": 1282,
          "y": 1622
        }
      ],
      "category": "figure",
      "html": "<br><figure><img id='225' style='font-size:14px' alt=\"420\nL20\" data-coord=\"top-left:(1282,1030); bottom-right:(2273,1622)\" /></figure>",
      "id": 225,
      "page": 22,
      "text": "420\nL20"
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 1643
        },
        {
          "x": 2277,
          "y": 1643
        },
        {
          "x": 2277,
          "y": 1990
        },
        {
          "x": 1279,
          "y": 1990
        }
      ],
      "category": "caption",
      "html": "<caption id='226' style='font-size:16px'>Figure 17: Visualization of thresholding the similarities of<br>mask embeddings from SAM's latent space. A query is in-<br>dicated by the magenta box; top row shows matches at a low<br>threshold, bottom row at a high threshold. The most similar<br>mask embeddings in the same image can often be seman-<br>tically similar to the query mask embedding, even though<br>SAM is not trained with explicit semantic supervision.</caption>",
      "id": 226,
      "page": 22,
      "text": "Figure 17: Visualization of thresholding the similarities of\nmask embeddings from SAM's latent space. A query is in-\ndicated by the magenta box; top row shows matches at a low\nthreshold, bottom row at a high threshold. The most similar\nmask embeddings in the same image can often be seman-\ntically similar to the query mask embedding, even though\nSAM is not trained with explicit semantic supervision."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 2067
        },
        {
          "x": 2276,
          "y": 2067
        },
        {
          "x": 2276,
          "y": 2319
        },
        {
          "x": 1280,
          "y": 2319
        }
      ],
      "category": "paragraph",
      "html": "<p id='227' style='font-size:16px'>Inference. During inference we use the CLIP text encoder<br>without any modifications to create a prompt for SAM. We<br>rely on the fact that text and image embeddings are aligned<br>by CLIP, which allows us to train without any explicit text<br>supervision while using text-based prompts for inference.</p>",
      "id": 227,
      "page": 22,
      "text": "Inference. During inference we use the CLIP text encoder\nwithout any modifications to create a prompt for SAM. We\nrely on the fact that text and image embeddings are aligned\nby CLIP, which allows us to train without any explicit text\nsupervision while using text-based prompts for inference."
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 2351
        },
        {
          "x": 2035,
          "y": 2351
        },
        {
          "x": 2035,
          "y": 2400
        },
        {
          "x": 1281,
          "y": 2400
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='228' style='font-size:22px'>D.6. Probing the Latent Space of SAM</p>",
      "id": 228,
      "page": 22,
      "text": "D.6. Probing the Latent Space of SAM"
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 2428
        },
        {
          "x": 2276,
          "y": 2428
        },
        {
          "x": 2276,
          "y": 2978
        },
        {
          "x": 1278,
          "y": 2978
        }
      ],
      "category": "paragraph",
      "html": "<p id='229' style='font-size:18px'>Finally, we perform an initial investigation to qualita-<br>tively probe the latent space learned by SAM. In particu-<br>lar, we are interested in whether SAM is able to capture any<br>semantics in its representation even though is not trained<br>with explicit semantic supervision. To do SO, we compute<br>mask embeddings by extracting an image embedding from<br>SAM from an image crop around a mask and its horizon-<br>tally flipped version, multiplying the image embedding by<br>the binary mask, and averaging over spatial locations. In<br>Fig. 17, we show 3 examples of a query mask and similar<br>masks (in the latent space) in the same image. We observe</p>",
      "id": 229,
      "page": 22,
      "text": "Finally, we perform an initial investigation to qualita-\ntively probe the latent space learned by SAM. In particu-\nlar, we are interested in whether SAM is able to capture any\nsemantics in its representation even though is not trained\nwith explicit semantic supervision. To do SO, we compute\nmask embeddings by extracting an image embedding from\nSAM from an image crop around a mask and its horizon-\ntally flipped version, multiplying the image embedding by\nthe binary mask, and averaging over spatial locations. In\nFig. 17, we show 3 examples of a query mask and similar\nmasks (in the latent space) in the same image. We observe"
    },
    {
      "bounding_box": [
        {
          "x": 1216,
          "y": 3055
        },
        {
          "x": 1262,
          "y": 3055
        },
        {
          "x": 1262,
          "y": 3091
        },
        {
          "x": 1216,
          "y": 3091
        }
      ],
      "category": "footer",
      "html": "<footer id='230' style='font-size:16px'>22</footer>",
      "id": 230,
      "page": 22,
      "text": "22"
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 308
        },
        {
          "x": 1200,
          "y": 308
        },
        {
          "x": 1200,
          "y": 604
        },
        {
          "x": 200,
          "y": 604
        }
      ],
      "category": "paragraph",
      "html": "<p id='231' style='font-size:18px'>that the nearest neighbors for each query show some, albeit<br>imperfect, shape and semantic similarity. Although these<br>results are preliminary, they indicate that the representations<br>from SAM may be useful for a variety of purposes, such as<br>further data labeling, understanding the contents of datasets,<br>or as features for downstream tasks.</p>",
      "id": 231,
      "page": 23,
      "text": "that the nearest neighbors for each query show some, albeit\nimperfect, shape and semantic similarity. Although these\nresults are preliminary, they indicate that the representations\nfrom SAM may be useful for a variety of purposes, such as\nfurther data labeling, understanding the contents of datasets,\nor as features for downstream tasks."
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 653
        },
        {
          "x": 1026,
          "y": 653
        },
        {
          "x": 1026,
          "y": 704
        },
        {
          "x": 204,
          "y": 704
        }
      ],
      "category": "paragraph",
      "html": "<p id='232' style='font-size:22px'>E. Human Study Experimental Design</p>",
      "id": 232,
      "page": 23,
      "text": "E. Human Study Experimental Design"
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 739
        },
        {
          "x": 1200,
          "y": 739
        },
        {
          "x": 1200,
          "y": 1483
        },
        {
          "x": 200,
          "y": 1483
        }
      ],
      "category": "paragraph",
      "html": "<p id='233' style='font-size:16px'>Here we describe details of the human study used to eval-<br>uate mask quality in §7.1 and §7.4. The purpose of the<br>human study is to address two limitations of using IoU to<br>ground truth as a measure of predicted mask quality. The<br>first limitation is that, for ambiguous inputs such as a single<br>point, the model may be strongly penalized for returning a<br>valid mask of a different object than the ground truth. The<br>second limitation is that ground truth masks may include<br>various biases, such as systematic errors in the edge qual-<br>ity or decisions to modally or amodally segment occluding<br>objects. A model trained in-domain can learn these biases<br>and obtain a higher IoU without necessarily producing bet-<br>ter masks. Human review can obtain a measure of mask<br>quality independent of an underlying ground truth mask in<br>order to alleviate these issues.</p>",
      "id": 233,
      "page": 23,
      "text": "Here we describe details of the human study used to eval-\nuate mask quality in §7.1 and §7.4. The purpose of the\nhuman study is to address two limitations of using IoU to\nground truth as a measure of predicted mask quality. The\nfirst limitation is that, for ambiguous inputs such as a single\npoint, the model may be strongly penalized for returning a\nvalid mask of a different object than the ground truth. The\nsecond limitation is that ground truth masks may include\nvarious biases, such as systematic errors in the edge qual-\nity or decisions to modally or amodally segment occluding\nobjects. A model trained in-domain can learn these biases\nand obtain a higher IoU without necessarily producing bet-\nter masks. Human review can obtain a measure of mask\nquality independent of an underlying ground truth mask in\norder to alleviate these issues."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 1501
        },
        {
          "x": 1198,
          "y": 1501
        },
        {
          "x": 1198,
          "y": 1946
        },
        {
          "x": 201,
          "y": 1946
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='234' style='font-size:18px'>Models. For single-point evaluation, we use RITM [92],<br>single-output SAM, and SAM to test two hypotheses. First,<br>we hypothesize that SAM produces visually higher quality<br>masks than baseline interactive segmentation models when<br>given a single point, even when metrics such as IoU with<br>ground truth do not reveal this. Second, we hypothesize<br>that SAM's ability to disambiguate masks improves mask<br>quality for single point inputs, since single output SAM may<br>return masks that average over ambiguous masks.</p>",
      "id": 234,
      "page": 23,
      "text": "Models. For single-point evaluation, we use RITM [92],\nsingle-output SAM, and SAM to test two hypotheses. First,\nwe hypothesize that SAM produces visually higher quality\nmasks than baseline interactive segmentation models when\ngiven a single point, even when metrics such as IoU with\nground truth do not reveal this. Second, we hypothesize\nthat SAM's ability to disambiguate masks improves mask\nquality for single point inputs, since single output SAM may\nreturn masks that average over ambiguous masks."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 1952
        },
        {
          "x": 1199,
          "y": 1952
        },
        {
          "x": 1199,
          "y": 2194
        },
        {
          "x": 201,
          "y": 2194
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='235' style='font-size:16px'>For instance segmentation experiments, we evaluate cas-<br>cade ViTDet-H [62] and SAM in order to test the hypothesis<br>that SAM produces visually higher quality masks, even ifit<br>obtains a lower AP due to the inability to learn specific an-<br>notation biases of the validation dataset.</p>",
      "id": 235,
      "page": 23,
      "text": "For instance segmentation experiments, we evaluate cas-\ncade ViTDet-H [62] and SAM in order to test the hypothesis\nthat SAM produces visually higher quality masks, even ifit\nobtains a lower AP due to the inability to learn specific an-\nnotation biases of the validation dataset."
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 2212
        },
        {
          "x": 1198,
          "y": 2212
        },
        {
          "x": 1198,
          "y": 2760
        },
        {
          "x": 200,
          "y": 2760
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='236' style='font-size:18px'>Datasets. For single-point experiments, we select 7 datasets<br>from our set of 23 datasets, since the full suite is too large<br>for human review. We choose LVIS v0.5 [17], VISOR [28,<br>27], DRAM [24], IBD [17], NDD20 [100], OVIS [81], and<br>iShape [111], which provide a diverse collection of images,<br>including scene-level, ego-centric, drawn, overhead, under-<br>water, and synthetic imagery. Additionally, this set includes<br>datasets both where SAM outperforms RITM with IoU met-<br>rics and vice-versa. For instance segmentation experiments,<br>we use the LVIS v1 validation set, allowing for direct com-<br>parison to ViTDet, which was trained on LVIS.</p>",
      "id": 236,
      "page": 23,
      "text": "Datasets. For single-point experiments, we select 7 datasets\nfrom our set of 23 datasets, since the full suite is too large\nfor human review. We choose LVIS v0.5 [17], VISOR [28,\n27], DRAM [24], IBD [17], NDD20 [100], OVIS [81], and\niShape [111], which provide a diverse collection of images,\nincluding scene-level, ego-centric, drawn, overhead, under-\nwater, and synthetic imagery. Additionally, this set includes\ndatasets both where SAM outperforms RITM with IoU met-\nrics and vice-versa. For instance segmentation experiments,\nwe use the LVIS v1 validation set, allowing for direct com-\nparison to ViTDet, which was trained on LVIS."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 2778
        },
        {
          "x": 1199,
          "y": 2778
        },
        {
          "x": 1199,
          "y": 2975
        },
        {
          "x": 201,
          "y": 2975
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='237' style='font-size:14px'>Methodology. We presented masks generated by the mod-<br>els to professional annotators and asked them to rate each<br>mask using provided guidelines (see §G for the complete<br>guidelines). Annotators were sourced from the same com-</p>",
      "id": 237,
      "page": 23,
      "text": "Methodology. We presented masks generated by the mod-\nels to professional annotators and asked them to rate each\nmask using provided guidelines (see §G for the complete\nguidelines). Annotators were sourced from the same com-"
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 309
        },
        {
          "x": 2277,
          "y": 309
        },
        {
          "x": 2277,
          "y": 703
        },
        {
          "x": 1279,
          "y": 703
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='238' style='font-size:16px'>pany that collected manually annotated masks for the data<br>engine. An annotator was provided access to an image, the<br>predicted mask of a single model, and the input to the model<br>(either a single point or single box) and asked to judge the<br>mask on three criterion: Does the mask correspond to a<br>valid object? Does the mask have a clean boundary? and<br>Does the mask correspond to the input? They then submit-<br>ted a rating from 1-10 indicating the overall mask quality.</p>",
      "id": 238,
      "page": 23,
      "text": "pany that collected manually annotated masks for the data\nengine. An annotator was provided access to an image, the\npredicted mask of a single model, and the input to the model\n(either a single point or single box) and asked to judge the\nmask on three criterion: Does the mask correspond to a\nvalid object? Does the mask have a clean boundary? and\nDoes the mask correspond to the input? They then submit-\nted a rating from 1-10 indicating the overall mask quality."
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 714
        },
        {
          "x": 2277,
          "y": 714
        },
        {
          "x": 2277,
          "y": 1208
        },
        {
          "x": 1279,
          "y": 1208
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='239' style='font-size:16px'>A score of 1 indicates a mask that corresponds to no ob-<br>ject at all; a low score (2-4) indicates that the mask has huge<br>errors, such including huge regions of other objects or hav-<br>ing large areas of nonsensical boundaries; a middle score<br>(5-6) indicates masks that are mostly sensible but still have<br>significant semantic or boundary errors; a high score (7-<br>9) indicates masks with only minor boundary errors; and a<br>score of 10 is for masks with no visible errors. Annotators<br>were provided with five different views, each designed to<br>help identify different error types.</p>",
      "id": 239,
      "page": 23,
      "text": "A score of 1 indicates a mask that corresponds to no ob-\nject at all; a low score (2-4) indicates that the mask has huge\nerrors, such including huge regions of other objects or hav-\ning large areas of nonsensical boundaries; a middle score\n(5-6) indicates masks that are mostly sensible but still have\nsignificant semantic or boundary errors; a high score (7-\n9) indicates masks with only minor boundary errors; and a\nscore of 10 is for masks with no visible errors. Annotators\nwere provided with five different views, each designed to\nhelp identify different error types."
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 1220
        },
        {
          "x": 2277,
          "y": 1220
        },
        {
          "x": 2277,
          "y": 1964
        },
        {
          "x": 1278,
          "y": 1964
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='240' style='font-size:16px'>For single point experiments, 1000 masks per dataset<br>were selected randomly from the same subsets used for<br>benchmarking zero-shot interactive segmentation (see §D.1<br>for details on these subsets). The model input was the cen-<br>termost point, calculated as the largest value of the distance<br>transform from the edge of the mask. For instance seg-<br>mentation experiments, 1000 masks were selected from the<br>LVIS v1 validation set, and the model input was the LVIS<br>ground truth box. In all experiments, masks with a size<br>smaller than 242 pixels were excluded from sampling, to<br>prevent showing raters a mask that was too small to judge<br>accurately. For both memory and display reasons, large im-<br>ages were rescaled to have a max side-length of 2000 before<br>predicting a mask. In all experiments, the same inputs were<br>fed to each model to produce a predicted mask.</p>",
      "id": 240,
      "page": 23,
      "text": "For single point experiments, 1000 masks per dataset\nwere selected randomly from the same subsets used for\nbenchmarking zero-shot interactive segmentation (see §D.1\nfor details on these subsets). The model input was the cen-\ntermost point, calculated as the largest value of the distance\ntransform from the edge of the mask. For instance seg-\nmentation experiments, 1000 masks were selected from the\nLVIS v1 validation set, and the model input was the LVIS\nground truth box. In all experiments, masks with a size\nsmaller than 242 pixels were excluded from sampling, to\nprevent showing raters a mask that was too small to judge\naccurately. For both memory and display reasons, large im-\nages were rescaled to have a max side-length of 2000 before\npredicting a mask. In all experiments, the same inputs were\nfed to each model to produce a predicted mask."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 1974
        },
        {
          "x": 2278,
          "y": 1974
        },
        {
          "x": 2278,
          "y": 2270
        },
        {
          "x": 1280,
          "y": 2270
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='241' style='font-size:20px'>For comparison, the ground truth masks from each<br>dataset were also submitted for rating. For single-point<br>experiments, this gave 4000 total rating jobs per dataset<br>(1000 masks each for RITM, SAM single-output, SAM,<br>and ground truth); for instance segmentation experiments,<br>it gave 3000 total jobs (ViTDet, SAM, and ground truth).</p>",
      "id": 241,
      "page": 23,
      "text": "For comparison, the ground truth masks from each\ndataset were also submitted for rating. For single-point\nexperiments, this gave 4000 total rating jobs per dataset\n(1000 masks each for RITM, SAM single-output, SAM,\nand ground truth); for instance segmentation experiments,\nit gave 3000 total jobs (ViTDet, SAM, and ground truth)."
    },
    {
      "bounding_box": [
        {
          "x": 1277,
          "y": 2280
        },
        {
          "x": 2277,
          "y": 2280
        },
        {
          "x": 2277,
          "y": 2975
        },
        {
          "x": 1277,
          "y": 2975
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='242' style='font-size:16px'>For each dataset, these jobs were inserted with random<br>ordering into a queue from which 30 annotators drew jobs.<br>In initial testing of the review study, we provided each job to<br>five different annotators and found reasonable consistency<br>in scores: the average standard deviation in score over the<br>five annotators was 0.83. Additionally, the annotation com-<br>pany deployed quality assurance testers who spot checked<br>a fraction of results for extreme departures from the guide-<br>lines. Thus for our experiments each job (i.e., rating one<br>mask in one image) was completed by only a single anno-<br>tator. Average time spent per annotator per job was 90 sec-<br>onds, longer than our initial target of 30 seconds, but still<br>sufficiently fast to collect a large number of ratings on each<br>of the 7 selected datasets.</p>",
      "id": 242,
      "page": 23,
      "text": "For each dataset, these jobs were inserted with random\nordering into a queue from which 30 annotators drew jobs.\nIn initial testing of the review study, we provided each job to\nfive different annotators and found reasonable consistency\nin scores: the average standard deviation in score over the\nfive annotators was 0.83. Additionally, the annotation com-\npany deployed quality assurance testers who spot checked\na fraction of results for extreme departures from the guide-\nlines. Thus for our experiments each job (i.e., rating one\nmask in one image) was completed by only a single anno-\ntator. Average time spent per annotator per job was 90 sec-\nonds, longer than our initial target of 30 seconds, but still\nsufficiently fast to collect a large number of ratings on each\nof the 7 selected datasets."
    },
    {
      "bounding_box": [
        {
          "x": 1216,
          "y": 3054
        },
        {
          "x": 1262,
          "y": 3054
        },
        {
          "x": 1262,
          "y": 3091
        },
        {
          "x": 1216,
          "y": 3091
        }
      ],
      "category": "footer",
      "html": "<footer id='243' style='font-size:14px'>23</footer>",
      "id": 243,
      "page": 23,
      "text": "23"
    },
    {
      "bounding_box": [
        {
          "x": 197,
          "y": 337
        },
        {
          "x": 2260,
          "y": 337
        },
        {
          "x": 2260,
          "y": 1768
        },
        {
          "x": 197,
          "y": 1768
        }
      ],
      "category": "figure",
      "html": "<figure><img id='244' style='font-size:14px' alt=\"ratings\n6.5±0.15, RITM 8.1 ±0.10, SAM 6.3±0.16, RITM 8.3±0.09, SAM\n40\n40 7.7±0.12, SAM - single output 8.5 ±0.09, GT 7.5±0.13, SAM - single output 8.5 ±0.13, GT\nof ratings\nof\n20\nPercent\n20\n0 Percent\n0\n1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10\nMask quality rating Mask quality rating\n(a) LVIS v0.5 [17] (b) VISOR [28, 27]\nratings\nratings\n40 5.9±0.14, RITM 7.7±0.13, SAM 40 7.1±0.12, RITM 8.3 ±0.08, SAM\n6.8±0.15, SAM - single output 8.0±0.15, GT 7.9±0.11, SAM - single output 8.4±0.09, GT\nof\n20 of\n20\nPercent\n0\n0 Percent\n1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10\nMask quality rating Mask quality rating\n(c) DRAM [24] (d) IBD [17]\n6.4±0.17, RITM 8.6±0.10, SAM 40 6.1±0.15, RITM 7.2±0.13, SAM\n40 - single output 8.8 ± 0.09, GT\n8.2±0.11, SAM - single output 8.9 ±0.06, GT 7.7±0.12, SAM\nof ratings\nof ratings\n20\nPercent\n20\n0\n0 Percent\n1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10\nMask quality rating Mask quality rating\n(e) NDD20 [100] (f) OVIS [81]\n4.9± 0.16, RITM 7.1±0.15, SAM\n40\n6.2±0.17, SAM - single output 9.3±0.06, GT\nof ratings\nPercent 20\n0\n1 2 3 4 5 6 7 8 9 10\nMask quality rating\n(g) iShape [111]\" data-coord=\"top-left:(197,337); bottom-right:(2260,1768)\" /></figure>",
      "id": 244,
      "page": 24,
      "text": "ratings\n6.5±0.15, RITM 8.1 ±0.10, SAM 6.3±0.16, RITM 8.3±0.09, SAM\n40\n40 7.7±0.12, SAM - single output 8.5 ±0.09, GT 7.5±0.13, SAM - single output 8.5 ±0.13, GT\nof ratings\nof\n20\nPercent\n20\n0 Percent\n0\n1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10\nMask quality rating Mask quality rating\n(a) LVIS v0.5 [17] (b) VISOR [28, 27]\nratings\nratings\n40 5.9±0.14, RITM 7.7±0.13, SAM 40 7.1±0.12, RITM 8.3 ±0.08, SAM\n6.8±0.15, SAM - single output 8.0±0.15, GT 7.9±0.11, SAM - single output 8.4±0.09, GT\nof\n20 of\n20\nPercent\n0\n0 Percent\n1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10\nMask quality rating Mask quality rating\n(c) DRAM [24] (d) IBD [17]\n6.4±0.17, RITM 8.6±0.10, SAM 40 6.1±0.15, RITM 7.2±0.13, SAM\n40 - single output 8.8 ± 0.09, GT\n8.2±0.11, SAM - single output 8.9 ±0.06, GT 7.7±0.12, SAM\nof ratings\nof ratings\n20\nPercent\n20\n0\n0 Percent\n1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10\nMask quality rating Mask quality rating\n(e) NDD20 [100] (f) OVIS [81]\n4.9± 0.16, RITM 7.1±0.15, SAM\n40\n6.2±0.17, SAM - single output 9.3±0.06, GT\nof ratings\nPercent 20\n0\n1 2 3 4 5 6 7 8 9 10\nMask quality rating\n(g) iShape [111]"
    },
    {
      "bounding_box": [
        {
          "x": 497,
          "y": 1808
        },
        {
          "x": 1978,
          "y": 1808
        },
        {
          "x": 1978,
          "y": 1857
        },
        {
          "x": 497,
          "y": 1857
        }
      ],
      "category": "caption",
      "html": "<caption id='245' style='font-size:22px'>Figure 18: Mask quality rating distributions by dataset from our human evaluation study.</caption>",
      "id": 245,
      "page": 24,
      "text": "Figure 18: Mask quality rating distributions by dataset from our human evaluation study."
    },
    {
      "bounding_box": [
        {
          "x": 211,
          "y": 1934
        },
        {
          "x": 1174,
          "y": 1934
        },
        {
          "x": 1174,
          "y": 2464
        },
        {
          "x": 211,
          "y": 2464
        }
      ],
      "category": "table",
      "html": "<table id='246' style='font-size:14px'><tr><td></td><td colspan=\"2\">SAM > baseline</td><td colspan=\"2\">SAM > SAM single out.</td></tr><tr><td>dataset</td><td>p-value</td><td>CI99 (△�)</td><td>p-value</td><td>CI99(△�)</td></tr><tr><td colspan=\"5\">point input (RITM [92] baseline):</td></tr><tr><td>LVIS v0.5 [44]</td><td>4e-69</td><td>(1.40, 1.84)</td><td>2e-11</td><td>(0.29, 0.64)</td></tr><tr><td>VISOR [28, 27]</td><td>7e-98</td><td>(1.81, 2.24)</td><td>7e-26</td><td>(0.58, 0.94)</td></tr><tr><td>DRAM [24]</td><td>1e-76</td><td>(1.54, 2.00)</td><td>2e-24</td><td>(0.62, 1.03)</td></tr><tr><td>IBD [17]</td><td>2e-57</td><td>(1.03, 1.39)</td><td>1e-15</td><td>(0.32, 0.62)</td></tr><tr><td>NDD20 [100]</td><td>2e-86</td><td>(1.88, 2.37)</td><td>5e-08</td><td>(0.19, 0.55)</td></tr><tr><td>OVIS [81]</td><td>2e-64</td><td>(1.38, 1.84)</td><td>3e-10</td><td>(0.27, 0.63)</td></tr><tr><td>iShape [111]</td><td>2e-88</td><td>(1.97, 2.47)</td><td>7e-23</td><td>(0.65, 1.10)</td></tr><tr><td colspan=\"5\">box input (ViTDet-H [62] baseline):</td></tr><tr><td>LVIS v1 [44]</td><td>2e-05</td><td>(0.11, 0.42)</td><td>I N/A</td><td>N/A</td></tr></table>",
      "id": 246,
      "page": 24,
      "text": "SAM > baseline SAM > SAM single out.\n dataset p-value CI99 (△�) p-value CI99(△�)\n point input (RITM [92] baseline):\n LVIS v0.5 [44] 4e-69 (1.40, 1.84) 2e-11 (0.29, 0.64)\n VISOR [28, 27] 7e-98 (1.81, 2.24) 7e-26 (0.58, 0.94)\n DRAM [24] 1e-76 (1.54, 2.00) 2e-24 (0.62, 1.03)\n IBD [17] 2e-57 (1.03, 1.39) 1e-15 (0.32, 0.62)\n NDD20 [100] 2e-86 (1.88, 2.37) 5e-08 (0.19, 0.55)\n OVIS [81] 2e-64 (1.38, 1.84) 3e-10 (0.27, 0.63)\n iShape [111] 2e-88 (1.97, 2.47) 7e-23 (0.65, 1.10)\n box input (ViTDet-H [62] baseline):\n LVIS v1 [44] 2e-05 (0.11, 0.42) I N/A"
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 2481
        },
        {
          "x": 1199,
          "y": 2481
        },
        {
          "x": 1199,
          "y": 2778
        },
        {
          "x": 204,
          "y": 2778
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='247' style='font-size:18px'>Table 8: Statistical tests showing significance that SAM has<br>higher mask quality ratings than baseline and single-output<br>SAM. P-values are calculated by paired t-test, while confi-<br>dence intervals for the difference in mean scores are calcu-<br>lated by paired bootstrap on 10k samples. All p-values are<br>significant, and all confidence intervals exclude zero.</p>",
      "id": 247,
      "page": 24,
      "text": "Table 8: Statistical tests showing significance that SAM has\nhigher mask quality ratings than baseline and single-output\nSAM. P-values are calculated by paired t-test, while confi-\ndence intervals for the difference in mean scores are calcu-\nlated by paired bootstrap on 10k samples. All p-values are\nsignificant, and all confidence intervals exclude zero."
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 2876
        },
        {
          "x": 1198,
          "y": 2876
        },
        {
          "x": 1198,
          "y": 2977
        },
        {
          "x": 204,
          "y": 2977
        }
      ],
      "category": "caption",
      "html": "<caption id='248' style='font-size:20px'>Results. Fig. 18 shows histograms over ratings for each<br>dataset in the single-point experiments. We run statistical</caption>",
      "id": 248,
      "page": 24,
      "text": "Results. Fig. 18 shows histograms over ratings for each\ndataset in the single-point experiments. We run statistical"
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 1947
        },
        {
          "x": 2277,
          "y": 1947
        },
        {
          "x": 2277,
          "y": 2394
        },
        {
          "x": 1278,
          "y": 2394
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='249' style='font-size:16px'>tests for two hypotheses: (1) that SAM gets higher scores<br>than the baseline model (RITM or ViTDet) and (2) that<br>SAM gets higher scores than single-output SAM. P-values<br>are calculated via a paired t-test on the means of the model<br>scores, which we supplement with a paired bootstrap test on<br>10k samples to find the 99% confidence interval for the dif-<br>ference of means. Table 8 shows p-values and confidence<br>intervals for these tests. All statistical tests are strongly sig-<br>nificant, and all confidence intervals exclude zero.</p>",
      "id": 249,
      "page": 24,
      "text": "tests for two hypotheses: (1) that SAM gets higher scores\nthan the baseline model (RITM or ViTDet) and (2) that\nSAM gets higher scores than single-output SAM. P-values\nare calculated via a paired t-test on the means of the model\nscores, which we supplement with a paired bootstrap test on\n10k samples to find the 99% confidence interval for the dif-\nference of means. Table 8 shows p-values and confidence\nintervals for these tests. All statistical tests are strongly sig-\nnificant, and all confidence intervals exclude zero."
    },
    {
      "bounding_box": [
        {
          "x": 1277,
          "y": 2428
        },
        {
          "x": 2278,
          "y": 2428
        },
        {
          "x": 2278,
          "y": 2977
        },
        {
          "x": 1277,
          "y": 2977
        }
      ],
      "category": "paragraph",
      "html": "<p id='250' style='font-size:16px'>For instance segmentation, Fig. 11 of the main text<br>shows the histogram for ratings. To compare to COCO<br>ground truth, we additionally include 794 ratings of COCO<br>ground truth masks that were collected during our testing of<br>the human review process. These masks were presented to<br>raters using an identical setup as the LVIS results. For fair<br>comparison, results for LVIS in Fig. 11 were subsampled<br>to the same 794 inputs for each model and ground truth.<br>For Table 8, the full 1000 ratings are used to run statistical<br>tests, which show that SAM's mask quality improvement<br>over ViTDet is statistically significant.</p>",
      "id": 250,
      "page": 24,
      "text": "For instance segmentation, Fig. 11 of the main text\nshows the histogram for ratings. To compare to COCO\nground truth, we additionally include 794 ratings of COCO\nground truth masks that were collected during our testing of\nthe human review process. These masks were presented to\nraters using an identical setup as the LVIS results. For fair\ncomparison, results for LVIS in Fig. 11 were subsampled\nto the same 794 inputs for each model and ground truth.\nFor Table 8, the full 1000 ratings are used to run statistical\ntests, which show that SAM's mask quality improvement\nover ViTDet is statistically significant."
    },
    {
      "bounding_box": [
        {
          "x": 1216,
          "y": 3055
        },
        {
          "x": 1262,
          "y": 3055
        },
        {
          "x": 1262,
          "y": 3092
        },
        {
          "x": 1216,
          "y": 3092
        }
      ],
      "category": "footer",
      "html": "<footer id='251' style='font-size:16px'>24</footer>",
      "id": 251,
      "page": 24,
      "text": "24"
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 301
        },
        {
          "x": 1092,
          "y": 301
        },
        {
          "x": 1092,
          "y": 351
        },
        {
          "x": 204,
          "y": 351
        }
      ],
      "category": "paragraph",
      "html": "<p id='252' style='font-size:22px'>F. Dataset, Annotation, and Model Cards</p>",
      "id": 252,
      "page": 25,
      "text": "F. Dataset, Annotation, and Model Cards"
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 384
        },
        {
          "x": 1199,
          "y": 384
        },
        {
          "x": 1199,
          "y": 684
        },
        {
          "x": 200,
          "y": 684
        }
      ],
      "category": "paragraph",
      "html": "<p id='253' style='font-size:18px'>In §F.1 we provide a Dataset Card for SA-1B, follow-<br>ing [39], in a list of questions and answers. Next, we pro-<br>vide a Data Annotation Card in §F.2 for the first two stages<br>of our data engine described in §4, following CrowdWork-<br>Sheets [30], again as a list of questions and answers. We<br>provide a Model Card following [75] in Table 9.</p>",
      "id": 253,
      "page": 25,
      "text": "In §F.1 we provide a Dataset Card for SA-1B, follow-\ning [39], in a list of questions and answers. Next, we pro-\nvide a Data Annotation Card in §F.2 for the first two stages\nof our data engine described in §4, following CrowdWork-\nSheets [30], again as a list of questions and answers. We\nprovide a Model Card following [75] in Table 9."
    },
    {
      "bounding_box": [
        {
          "x": 205,
          "y": 721
        },
        {
          "x": 757,
          "y": 721
        },
        {
          "x": 757,
          "y": 767
        },
        {
          "x": 205,
          "y": 767
        }
      ],
      "category": "paragraph",
      "html": "<p id='254' style='font-size:20px'>F.1. Dataset Card for SA-1B</p>",
      "id": 254,
      "page": 25,
      "text": "F.1. Dataset Card for SA-1B"
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 807
        },
        {
          "x": 359,
          "y": 807
        },
        {
          "x": 359,
          "y": 841
        },
        {
          "x": 204,
          "y": 841
        }
      ],
      "category": "paragraph",
      "html": "<p id='255' style='font-size:16px'>Motivation</p>",
      "id": 255,
      "page": 25,
      "text": "Motivation"
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 856
        },
        {
          "x": 1200,
          "y": 856
        },
        {
          "x": 1200,
          "y": 1541
        },
        {
          "x": 204,
          "y": 1541
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='256' style='font-size:14px'>1. For what purpose was the dataset created? Was there a specific task in<br>mind? Was there a specific gap that needed to be filled? Please provide a<br>description. The contributions of our dataset to the vision community are<br>fourfold: (1) We release a dataset of 11M images and 1.1B masks, by far the<br>largest segmentation dataset to date. (2) The dataset we release is privacy<br>protecting: we have blurred faces and license plates in all images. (3) The<br>dataset is licensed under a broad set of terms of use which can be found<br>at https://affacebook.com/dases/sagmanianything (4) The data is more<br>geographically diverse than its predecessors, and we hope it will bring the<br>community one step closer to creating fairer and more equitable models.<br>2. Who created the dataset (e.g., which team, research group) and on behalf<br>of which entity (e.g., company, institution, organization)? The dataset was<br>created by the FAIR team of Meta AI. The underlying images were collected<br>and licensed from a third party photo company.<br>3. Who funded the creation of the dataset? If there is an associated grant,<br>please provide the name of the grantor and the grant name and number.<br>Meta AI funded the creation of the dataset.<br>4. Any other comments? No.</p>",
      "id": 256,
      "page": 25,
      "text": "1. For what purpose was the dataset created? Was there a specific task in\nmind? Was there a specific gap that needed to be filled? Please provide a\ndescription. The contributions of our dataset to the vision community are\nfourfold: (1) We release a dataset of 11M images and 1.1B masks, by far the\nlargest segmentation dataset to date. (2) The dataset we release is privacy\nprotecting: we have blurred faces and license plates in all images. (3) The\ndataset is licensed under a broad set of terms of use which can be found\nat https://affacebook.com/dases/sagmanianything (4) The data is more\ngeographically diverse than its predecessors, and we hope it will bring the\ncommunity one step closer to creating fairer and more equitable models.\n2. Who created the dataset (e.g., which team, research group) and on behalf\nof which entity (e.g., company, institution, organization)? The dataset was\ncreated by the FAIR team of Meta AI. The underlying images were collected\nand licensed from a third party photo company.\n3. Who funded the creation of the dataset? If there is an associated grant,\nplease provide the name of the grantor and the grant name and number.\nMeta AI funded the creation of the dataset.\n4. Any other comments? No."
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 1571
        },
        {
          "x": 382,
          "y": 1571
        },
        {
          "x": 382,
          "y": 1605
        },
        {
          "x": 204,
          "y": 1605
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='257' style='font-size:18px'>Composition</p>",
      "id": 257,
      "page": 25,
      "text": "Composition"
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 1610
        },
        {
          "x": 1199,
          "y": 1610
        },
        {
          "x": 1199,
          "y": 2979
        },
        {
          "x": 204,
          "y": 2979
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='258' style='font-size:14px'>1. What do the instances that comprise the dataset represent (e.g., documents,<br>photos, people, countries)? Are there multiple types of instances (e.g.,<br>movies, users, and ratings; people and interactions between them; nodes<br>and edges)? Please provide a description. All of the instances in the dataset<br>are photos. The photos vary in subject matter; common themes of the photo<br>include: locations, objects, scenes. All of the photos are distinct, however<br>there are some sets of photos that were taken of the same subject matter.<br>2. How many instances are there in total (of each type, if appropriate)? There<br>are 11 million images.<br>3. Does the dataset contain all possible instances or is it a sample (not nec-<br>essarily random) of instances from a larger set? If the dataset is a sample,<br>then what is the larger set? Is the sample representative of the larger set<br>(e.g., geographic coverage)? If so, please describe how this representa-<br>tiveness was validated/verified. If it is not representative of the larger set,<br>please describe why not (e.g., to cover a more diverse range of instances,<br>because instances were withheld or unavailable). The dataset is composed<br>of images licensed from a photo provider. The dataset contains all instances<br>licensed. The images are photos, i.e. not artwork, although there are a few<br>exceptions. The dataset includes all generated masks for each image in the<br>dataset. We withheld ~2k randomly selected images for testing purposes.<br>4. What data does each instance consist of? \"Raw\" data (e.g., unprocessed<br>text or images) or features? In either case, please provide a description.<br>Each instance in the dataset is an image. The images were processed to blur<br>faces and license plates to protect the identities of those in the image.<br>5. Is there a label or target associated with each instance? Ifso, please provide<br>a description. Each image is annotated with masks. There are no categories<br>or text associated with the masks. The average image has ~100 masks, and<br>there are ~ 1.1B masks in total.<br>6. Is any information missing from individual instances? If so, please provide<br>a description, explaining why this information is missing (e.g., because it<br>was unavailable). This does not include intentionally removed information,<br>but might include, e.g., redacted text. Yes. Each image is accompanied by<br>a short caption that describes the content and place of the photo in a free<br>form text. Per our agreement with the photo provider we are not allowed to<br>release these captions. However, we use them in our paper to analyze the<br>geographical distribution of the dataset.</p>",
      "id": 258,
      "page": 25,
      "text": "1. What do the instances that comprise the dataset represent (e.g., documents,\nphotos, people, countries)? Are there multiple types of instances (e.g.,\nmovies, users, and ratings; people and interactions between them; nodes\nand edges)? Please provide a description. All of the instances in the dataset\nare photos. The photos vary in subject matter; common themes of the photo\ninclude: locations, objects, scenes. All of the photos are distinct, however\nthere are some sets of photos that were taken of the same subject matter.\n2. How many instances are there in total (of each type, if appropriate)? There\nare 11 million images.\n3. Does the dataset contain all possible instances or is it a sample (not nec-\nessarily random) of instances from a larger set? If the dataset is a sample,\nthen what is the larger set? Is the sample representative of the larger set\n(e.g., geographic coverage)? If so, please describe how this representa-\ntiveness was validated/verified. If it is not representative of the larger set,\nplease describe why not (e.g., to cover a more diverse range of instances,\nbecause instances were withheld or unavailable). The dataset is composed\nof images licensed from a photo provider. The dataset contains all instances\nlicensed. The images are photos, i.e. not artwork, although there are a few\nexceptions. The dataset includes all generated masks for each image in the\ndataset. We withheld ~2k randomly selected images for testing purposes.\n4. What data does each instance consist of? \"Raw\" data (e.g., unprocessed\ntext or images) or features? In either case, please provide a description.\nEach instance in the dataset is an image. The images were processed to blur\nfaces and license plates to protect the identities of those in the image.\n5. Is there a label or target associated with each instance? Ifso, please provide\na description. Each image is annotated with masks. There are no categories\nor text associated with the masks. The average image has ~100 masks, and\nthere are ~ 1.1B masks in total.\n6. Is any information missing from individual instances? If so, please provide\na description, explaining why this information is missing (e.g., because it\nwas unavailable). This does not include intentionally removed information,\nbut might include, e.g., redacted text. Yes. Each image is accompanied by\na short caption that describes the content and place of the photo in a free\nform text. Per our agreement with the photo provider we are not allowed to\nrelease these captions. However, we use them in our paper to analyze the\ngeographical distribution of the dataset."
    },
    {
      "bounding_box": [
        {
          "x": 1269,
          "y": 329
        },
        {
          "x": 2285,
          "y": 329
        },
        {
          "x": 2285,
          "y": 2607
        },
        {
          "x": 1269,
          "y": 2607
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='259' style='font-size:14px'>7. Are relationships between individual instances made explicit (e.g., users<br>movie ratings, social network links)? If so, please describe how these rela-<br>tionships are made explicit. No, there are no known relationships between<br>instances in the dataset.<br>8. Are there any errors, sources of noise, or redundancies in the dataset? If<br>so, please provide a description. Errors: The masks are generated by a<br>segmentation model, so there may be errors or inconsistencies in the masks.<br>Redundancies: While no two images are the same, there are instances of<br>images of the same subject taken close together in time.<br>9. Is the dataset self-contained, or does it link to or otherwise rely on external<br>resources (e.g., websites, tweets, other datasets)? If it links to or relies on<br>external resources, a) are there guarantees that they will exist, and remain<br>constant, over time; b) are there official archival versions of the complete<br>dataset (i.e., including the external resources as they existed at the time<br>the dataset was created); c) are there any restrictions (e.g., licenses, fees)<br>associated with any of the external resources that might apply to a dataset<br>consumer? Please provide descriptions of all external resources and any<br>restrictions associated with them, as well as links or other access points, as<br>appropriate. The dataset is self-contained.<br>10. Does the dataset contain data that might be considered confidential (e.g.,<br>data that is protected by legal privilege or by doctor-patient confidentiality,<br>data that includes the content of individuals' non-public communications)?<br>If so, please provide a description. No.<br>11. Does the dataset contain data that, if viewed directly, might be offensive,<br>insulting, threatening, or might otherwise cause anxiety? If so, please de-<br>scribe why. We have two safety measures to prevent objectionable content:<br>(1) Photos are licensed from a photo provider and had to meet the terms of<br>service of the photo provider. We requested that all objectionable content<br>be filtered from the images we licensed. (2) If a user observes objectionable<br>image(s) in the dataset, we invite them to report the image(s) at segment-<br>anything@ meta.com for removal. Despite the measures taken, we observe<br>that a small portion of images contains scenes of protests or other gatherings<br>that focus on a diverse spectrum of religious beliefs or political opinions that<br>may be offensive. We were not able to produce a filtering strategy that re-<br>moves all such images and rely on users to report this type of content.<br>12. Does the dataset identify any subpopulations (e.g., by age, gender)? If so,<br>please describe how these subpopulations are identified and provide a de-<br>scription of their respective distributions within the dataset. The dataset<br>does not identify any subpopulations of the people in the photos.<br>13. Is it possible to identify individuals (i.e., one or more natural persons), ei-<br>ther directly or indirectly (i.e., in combination with other data) from the<br>dataset? If so, please describe how. No. Images were subjected to a face<br>blurring model to remove any personally identifiable information. If a user<br>observes any anonymization issue, we invite them to report the issue and<br>the image id(s) at segment-anything@meta.com.<br>14. Does the dataset contain data that might be considered sensitive in any way<br>(e.g., data that reveals race or ethnic origins, sexual orientations, religious<br>beliefs, political opinions or union memberships, or locations; financial or<br>health data; biometric or genetic data; forms of government identification,<br>such as social security numbers; criminal history)? If so, please provide<br>a description. The dataset contains scenes of protests, or other gatherings<br>that may suggest religious beliefs, political opinions or union memberships.<br>However, the faces of all people in the dataset have been anonymized via<br>facial blurring, so itis not possible to identify any person in the dataset.<br>15. Any other comments? No.<br>Collection Process<br>1. How was the data associated with each instance acquired? Was the data<br>directly observable (e.g., raw text, movie ratings), reported by subjects (e.g.,<br>survey responses), or indirectly inferred/derived from other data (e.g., part-<br>of-speech tags, model-based guesses for age or language)? If the data was</p>",
      "id": 259,
      "page": 25,
      "text": "7. Are relationships between individual instances made explicit (e.g., users\nmovie ratings, social network links)? If so, please describe how these rela-\ntionships are made explicit. No, there are no known relationships between\ninstances in the dataset.\n8. Are there any errors, sources of noise, or redundancies in the dataset? If\nso, please provide a description. Errors: The masks are generated by a\nsegmentation model, so there may be errors or inconsistencies in the masks.\nRedundancies: While no two images are the same, there are instances of\nimages of the same subject taken close together in time.\n9. Is the dataset self-contained, or does it link to or otherwise rely on external\nresources (e.g., websites, tweets, other datasets)? If it links to or relies on\nexternal resources, a) are there guarantees that they will exist, and remain\nconstant, over time; b) are there official archival versions of the complete\ndataset (i.e., including the external resources as they existed at the time\nthe dataset was created); c) are there any restrictions (e.g., licenses, fees)\nassociated with any of the external resources that might apply to a dataset\nconsumer? Please provide descriptions of all external resources and any\nrestrictions associated with them, as well as links or other access points, as\nappropriate. The dataset is self-contained.\n10. Does the dataset contain data that might be considered confidential (e.g.,\ndata that is protected by legal privilege or by doctor-patient confidentiality,\ndata that includes the content of individuals' non-public communications)?\nIf so, please provide a description. No.\n11. Does the dataset contain data that, if viewed directly, might be offensive,\ninsulting, threatening, or might otherwise cause anxiety? If so, please de-\nscribe why. We have two safety measures to prevent objectionable content:\n(1) Photos are licensed from a photo provider and had to meet the terms of\nservice of the photo provider. We requested that all objectionable content\nbe filtered from the images we licensed. (2) If a user observes objectionable\nimage(s) in the dataset, we invite them to report the image(s) at segment-\nanything@ meta.com for removal. Despite the measures taken, we observe\nthat a small portion of images contains scenes of protests or other gatherings\nthat focus on a diverse spectrum of religious beliefs or political opinions that\nmay be offensive. We were not able to produce a filtering strategy that re-\nmoves all such images and rely on users to report this type of content.\n12. Does the dataset identify any subpopulations (e.g., by age, gender)? If so,\nplease describe how these subpopulations are identified and provide a de-\nscription of their respective distributions within the dataset. The dataset\ndoes not identify any subpopulations of the people in the photos.\n13. Is it possible to identify individuals (i.e., one or more natural persons), ei-\nther directly or indirectly (i.e., in combination with other data) from the\ndataset? If so, please describe how. No. Images were subjected to a face\nblurring model to remove any personally identifiable information. If a user\nobserves any anonymization issue, we invite them to report the issue and\nthe image id(s) at segment-anything@meta.com.\n14. Does the dataset contain data that might be considered sensitive in any way\n(e.g., data that reveals race or ethnic origins, sexual orientations, religious\nbeliefs, political opinions or union memberships, or locations; financial or\nhealth data; biometric or genetic data; forms of government identification,\nsuch as social security numbers; criminal history)? If so, please provide\na description. The dataset contains scenes of protests, or other gatherings\nthat may suggest religious beliefs, political opinions or union memberships.\nHowever, the faces of all people in the dataset have been anonymized via\nfacial blurring, so itis not possible to identify any person in the dataset.\n15. Any other comments? No.\nCollection Process\n1. How was the data associated with each instance acquired? Was the data\ndirectly observable (e.g., raw text, movie ratings), reported by subjects (e.g.,\nsurvey responses), or indirectly inferred/derived from other data (e.g., part-\nof-speech tags, model-based guesses for age or language)? If the data was"
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 2460
        },
        {
          "x": 2276,
          "y": 2460
        },
        {
          "x": 2276,
          "y": 2974
        },
        {
          "x": 1279,
          "y": 2974
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='260' style='font-size:14px'>reported by subjects or indirectly inferred/derived from other data, was the<br>data validated/verified? If so, please describe how. The released masks<br>associated with each image were automatically inferred by our segmentation<br>model, SAM. The masks that were collected using model-assisted manual<br>annotation will not be released. Quality was validated as described in §5.<br>2. What mechanisms or procedures were used to collect the data (e.g., hard-<br>ware apparatuses or sensors, manual human curation, software programs,<br>software APIs)? How were these mechanisms or procedures validated? The<br>images in the dataset are licensed from an image provider. They are all pho-<br>tos taken by photographers with different cameras.</p>",
      "id": 260,
      "page": 25,
      "text": "reported by subjects or indirectly inferred/derived from other data, was the\ndata validated/verified? If so, please describe how. The released masks\nassociated with each image were automatically inferred by our segmentation\nmodel, SAM. The masks that were collected using model-assisted manual\nannotation will not be released. Quality was validated as described in §5.\n2. What mechanisms or procedures were used to collect the data (e.g., hard-\nware apparatuses or sensors, manual human curation, software programs,\nsoftware APIs)? How were these mechanisms or procedures validated? The\nimages in the dataset are licensed from an image provider. They are all pho-\ntos taken by photographers with different cameras."
    },
    {
      "bounding_box": [
        {
          "x": 1217,
          "y": 3054
        },
        {
          "x": 1262,
          "y": 3054
        },
        {
          "x": 1262,
          "y": 3091
        },
        {
          "x": 1217,
          "y": 3091
        }
      ],
      "category": "footer",
      "html": "<footer id='261' style='font-size:18px'>25</footer>",
      "id": 261,
      "page": 25,
      "text": "25"
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 319
        },
        {
          "x": 1199,
          "y": 319
        },
        {
          "x": 1199,
          "y": 2407
        },
        {
          "x": 201,
          "y": 2407
        }
      ],
      "category": "paragraph",
      "html": "<p id='262' style='font-size:14px'>3. If the dataset is a sample from a larger set, what was the sampling strategy<br>(e.g., deterministic, probabilistic with specific sampling probabilities)? We<br>withheld ~2k randomly selected images for testing purposes. The rest of<br>the licensed images are included in the dataset.<br>4. Who was involved in the data collection process (e.g., students, crowdwork-<br>ers, contractors) and how were they compensated (e.g., how much were<br>crowdworkers paid)? The released masks were automatically inferred by<br>SAM. For details on our model-assisted manual annotation process see our<br>Data Annotation Card in §F.2. Note these masks will not be released.<br>5. Over what timeframe was the data collected? Does this timeframe match<br>the creation timeframe ofthe data associated with the instances (e.g., recent<br>crawl of old news articles)? If not, please describe the timeframe in which<br>the data associated with the instances was created. The licensed photos<br>vary in their date taken over a wide range of years up to 2022.<br>6. Were any ethical review processes conducted (e.g., by an institutional re-<br>view board)? If so, please provide a description of these review processes,<br>including the outcomes, as well as a link or other access point to any sup-<br>porting documentation. Ifthe dataset does not relate to people, you may skip<br>the remaining questions in this section. We underwent an internal privacy<br>review to evaluate and determine how to mitigate any potential risks with<br>respect to the privacy of people in the photos. Blurring faces and license<br>plates protects the privacy of the people in the photos.<br>7. Did you collect the data from the individuals in question directly, or obtain<br>it via third parties or other sources (e.g., websites)? We licensed the data<br>from a third party photo provider.<br>8. Were the individuals in question notified about the data collection? If so,<br>please describe (or show with screenshots or other information) how no-<br>tice was provided, and provide a link or other access point to, or other-<br>wise reproduce, the exact language of the notification itself. The images<br>are licensed from a third party who provided appropriate representations<br>regarding the collection of any notices and consents as required from indi-<br>viduals. In addition, all identifiable information (e.g. faces, license plates)<br>was blurred. Under the terms of the dataset license it is prohibited to attempt<br>to identify or associate an image with a particular individual.<br>9. Did the individuals in question consent to the collection and use of their<br>data? If so, please describe (or show with screenshots or other informa-<br>tion) how consent was requested and provided, and provide a link or other<br>access point to, or otherwise reproduce, the exact language to which the<br>individuals consented. The images are licensed from a third party who pro-<br>vided appropriate representations regarding the collection of any notices and<br>consents as required from individuals. In addition, all identifiable informa-<br>tion (e.g. faces, license plates) was blurred from all images. For avoidance<br>of doubt, under the terms of the dataset license it is prohibited to attempt to<br>identify or associate an image with a particular individual.<br>10. If consent was obtained, were the consenting individuals provided with a<br>mechanism to revoke their consent in the future or for certain uses? If<br>so, please provide a description, as well as a link or other access point<br>to the mechanism (if appropriate). We invite users to report at segment-<br>anything@meta.com for image(s) removal.<br>11. Has an analysis of the potential impact of the dataset and its use on data<br>subjects (e.g., a data protection impact analysis) been conducted? If so,<br>please provide a description of this analysis, including the outcomes, as<br>well as a link or other access point to any supporting documentation. To<br>eliminate any potential impact on people whose photos are included in the<br>dataset, identifiable information (faces, license plates) has been blurred.<br>12. Any other comments? No.</p>",
      "id": 262,
      "page": 26,
      "text": "3. If the dataset is a sample from a larger set, what was the sampling strategy\n(e.g., deterministic, probabilistic with specific sampling probabilities)? We\nwithheld ~2k randomly selected images for testing purposes. The rest of\nthe licensed images are included in the dataset.\n4. Who was involved in the data collection process (e.g., students, crowdwork-\ners, contractors) and how were they compensated (e.g., how much were\ncrowdworkers paid)? The released masks were automatically inferred by\nSAM. For details on our model-assisted manual annotation process see our\nData Annotation Card in §F.2. Note these masks will not be released.\n5. Over what timeframe was the data collected? Does this timeframe match\nthe creation timeframe ofthe data associated with the instances (e.g., recent\ncrawl of old news articles)? If not, please describe the timeframe in which\nthe data associated with the instances was created. The licensed photos\nvary in their date taken over a wide range of years up to 2022.\n6. Were any ethical review processes conducted (e.g., by an institutional re-\nview board)? If so, please provide a description of these review processes,\nincluding the outcomes, as well as a link or other access point to any sup-\nporting documentation. Ifthe dataset does not relate to people, you may skip\nthe remaining questions in this section. We underwent an internal privacy\nreview to evaluate and determine how to mitigate any potential risks with\nrespect to the privacy of people in the photos. Blurring faces and license\nplates protects the privacy of the people in the photos.\n7. Did you collect the data from the individuals in question directly, or obtain\nit via third parties or other sources (e.g., websites)? We licensed the data\nfrom a third party photo provider.\n8. Were the individuals in question notified about the data collection? If so,\nplease describe (or show with screenshots or other information) how no-\ntice was provided, and provide a link or other access point to, or other-\nwise reproduce, the exact language of the notification itself. The images\nare licensed from a third party who provided appropriate representations\nregarding the collection of any notices and consents as required from indi-\nviduals. In addition, all identifiable information (e.g. faces, license plates)\nwas blurred. Under the terms of the dataset license it is prohibited to attempt\nto identify or associate an image with a particular individual.\n9. Did the individuals in question consent to the collection and use of their\ndata? If so, please describe (or show with screenshots or other informa-\ntion) how consent was requested and provided, and provide a link or other\naccess point to, or otherwise reproduce, the exact language to which the\nindividuals consented. The images are licensed from a third party who pro-\nvided appropriate representations regarding the collection of any notices and\nconsents as required from individuals. In addition, all identifiable informa-\ntion (e.g. faces, license plates) was blurred from all images. For avoidance\nof doubt, under the terms of the dataset license it is prohibited to attempt to\nidentify or associate an image with a particular individual.\n10. If consent was obtained, were the consenting individuals provided with a\nmechanism to revoke their consent in the future or for certain uses? If\nso, please provide a description, as well as a link or other access point\nto the mechanism (if appropriate). We invite users to report at segment-\nanything@meta.com for image(s) removal.\n11. Has an analysis of the potential impact of the dataset and its use on data\nsubjects (e.g., a data protection impact analysis) been conducted? If so,\nplease provide a description of this analysis, including the outcomes, as\nwell as a link or other access point to any supporting documentation. To\neliminate any potential impact on people whose photos are included in the\ndataset, identifiable information (faces, license plates) has been blurred.\n12. Any other comments? No."
    },
    {
      "bounding_box": [
        {
          "x": 1330,
          "y": 315
        },
        {
          "x": 2273,
          "y": 315
        },
        {
          "x": 2273,
          "y": 384
        },
        {
          "x": 1330,
          "y": 384
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='263' style='font-size:14px'>RetinaFace [88, 89] model (https://githuh.com/serengilretinaface) to detect<br>faces. The model used to blur license plates has not been made public.</p>",
      "id": 263,
      "page": 26,
      "text": "RetinaFace [88, 89] model (https://githuh.com/serengilretinaface) to detect\nfaces. The model used to blur license plates has not been made public."
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 2439
        },
        {
          "x": 683,
          "y": 2439
        },
        {
          "x": 683,
          "y": 2473
        },
        {
          "x": 203,
          "y": 2473
        }
      ],
      "category": "paragraph",
      "html": "<p id='264' style='font-size:20px'>Preprocessing / Cleaning / Labeling</p>",
      "id": 264,
      "page": 26,
      "text": "Preprocessing / Cleaning / Labeling"
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 2484
        },
        {
          "x": 1199,
          "y": 2484
        },
        {
          "x": 1199,
          "y": 2973
        },
        {
          "x": 202,
          "y": 2973
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='265' style='font-size:14px'>1. Was any preprocessing / cleaning / labeling of the data done (e.g., dis-<br>cretization or bucketing, tokenization, part-of-speech tagging, SIFT fea-<br>ture extraction, removal of instances, processing of missing values)? If so,<br>please provide a description. If not, you may skip the remaining questions<br>in this section. We resized the high-resolution licensed images such that<br>the shorter side is 1500 pixels and only processed the images to remove any<br>identifiable and personal information from the photos (faces, license plates).<br>2. Was the \"raw\" data saved in addition to the preprocessed/cleaned/labeled<br>data (e.g., to support unanticipatedfuture uses)? Ifso, please provide a link<br>or other access point to the \"raw\" data. No, as we removed the data for<br>safety reasons and to respect privacy, we do not release the unaltered photos.<br>3. Is the software that was used to preprocess/clean/label the data avail-<br>able? If so, please provide a link or other access point. We used the</p>",
      "id": 265,
      "page": 26,
      "text": "1. Was any preprocessing / cleaning / labeling of the data done (e.g., dis-\ncretization or bucketing, tokenization, part-of-speech tagging, SIFT fea-\nture extraction, removal of instances, processing of missing values)? If so,\nplease provide a description. If not, you may skip the remaining questions\nin this section. We resized the high-resolution licensed images such that\nthe shorter side is 1500 pixels and only processed the images to remove any\nidentifiable and personal information from the photos (faces, license plates).\n2. Was the \"raw\" data saved in addition to the preprocessed/cleaned/labeled\ndata (e.g., to support unanticipatedfuture uses)? Ifso, please provide a link\nor other access point to the \"raw\" data. No, as we removed the data for\nsafety reasons and to respect privacy, we do not release the unaltered photos.\n3. Is the software that was used to preprocess/clean/label the data avail-\nable? If so, please provide a link or other access point. We used the"
    },
    {
      "bounding_box": [
        {
          "x": 1284,
          "y": 413
        },
        {
          "x": 1352,
          "y": 413
        },
        {
          "x": 1352,
          "y": 444
        },
        {
          "x": 1284,
          "y": 444
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='266' style='font-size:16px'>Uses</p>",
      "id": 266,
      "page": 26,
      "text": "Uses"
    },
    {
      "bounding_box": [
        {
          "x": 1285,
          "y": 455
        },
        {
          "x": 2275,
          "y": 455
        },
        {
          "x": 2275,
          "y": 1384
        },
        {
          "x": 1285,
          "y": 1384
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='267' style='font-size:14px'>1. Has the dataset been used for any tasks already? If so, please provide a<br>description. The dataset was used to train our segmentation model, SAM.<br>2. Is there a repository that links to any or all papers or systems that use the<br>dataset? If so, please provide a link or other access point. No. However, all<br>users of the dataset must cite it, so its use is trackable via citation explorers.<br>3. What (other) tasks could the dataset be used for? We intend the dataset<br>to be a large-scale segmentation dataset. However, we invite the research<br>community to gather additional annotations for the dataset.<br>4. Is there anything about the composition of the dataset or the way it was<br>collected and preprocessed/cleaned/labeled that might impact future uses?<br>For example, is there anything that a dataset consumer might need to know<br>to avoid uses that could result in unfair treatment of individuals or groups<br>(e.g., stereotyping, quality of service issues) or other risks or harms (e.g.,<br>legal risks, financial harms)? If so, please provide a description. Is there<br>anything a dataset consumer could do to mitigate these risks or harms? We<br>have an analysis of the approximate geographic and income level coverage<br>of our dataset in §6. While we believe our dataset to be more representative<br>than most of the publicly existing datasets at this time, we acknowledge<br>that we do not have parity across all groups, and we encourage users to be<br>mindful of potential biases their models have learned using this dataset.<br>5. Are there tasks for which the dataset should not be used? If so, please pro-<br>vide a description. Full terms of use for the dataset including prohibited use<br>cases can be found at https://affacebook.com/dravetsagnagmanyfing<br>6. Any other comments? No.</p>",
      "id": 267,
      "page": 26,
      "text": "1. Has the dataset been used for any tasks already? If so, please provide a\ndescription. The dataset was used to train our segmentation model, SAM.\n2. Is there a repository that links to any or all papers or systems that use the\ndataset? If so, please provide a link or other access point. No. However, all\nusers of the dataset must cite it, so its use is trackable via citation explorers.\n3. What (other) tasks could the dataset be used for? We intend the dataset\nto be a large-scale segmentation dataset. However, we invite the research\ncommunity to gather additional annotations for the dataset.\n4. Is there anything about the composition of the dataset or the way it was\ncollected and preprocessed/cleaned/labeled that might impact future uses?\nFor example, is there anything that a dataset consumer might need to know\nto avoid uses that could result in unfair treatment of individuals or groups\n(e.g., stereotyping, quality of service issues) or other risks or harms (e.g.,\nlegal risks, financial harms)? If so, please provide a description. Is there\nanything a dataset consumer could do to mitigate these risks or harms? We\nhave an analysis of the approximate geographic and income level coverage\nof our dataset in §6. While we believe our dataset to be more representative\nthan most of the publicly existing datasets at this time, we acknowledge\nthat we do not have parity across all groups, and we encourage users to be\nmindful of potential biases their models have learned using this dataset.\n5. Are there tasks for which the dataset should not be used? If so, please pro-\nvide a description. Full terms of use for the dataset including prohibited use\ncases can be found at https://affacebook.com/dravetsagnagmanyfing\n6. Any other comments? No."
    },
    {
      "bounding_box": [
        {
          "x": 1282,
          "y": 1408
        },
        {
          "x": 1453,
          "y": 1408
        },
        {
          "x": 1453,
          "y": 1441
        },
        {
          "x": 1282,
          "y": 1441
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='268' style='font-size:18px'>Distribution</p>",
      "id": 268,
      "page": 26,
      "text": "Distribution"
    },
    {
      "bounding_box": [
        {
          "x": 1287,
          "y": 1453
        },
        {
          "x": 2275,
          "y": 1453
        },
        {
          "x": 2275,
          "y": 2534
        },
        {
          "x": 1287,
          "y": 2534
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='269' style='font-size:14px'>1. Will the dataset be distributed to third parties outside of the entity (e.g.,<br>company, institution, organization) on behalf of which the dataset was cre-<br>ated? If so, please provide a description. The dataset will be available for<br>the research community.<br>2. How will the dataset will be distributed (e.g., tarball on website, API,<br>GitHub)? Does the dataset have a digital object identifier (DOI)? The<br>dataset is available at https://affacebook.com/dravets/sagmen-anyhing<br>3. When will the dataset be distributed? The dataset will be released in 2023.<br>4. Will the dataset be distributed under a copyright or other intellectual<br>property (IP) license, and/or under applicable terms of use (ToU)? If<br>so, please describe this license and/or ToU, and provide a link or other<br>access point to, or otherwise reproduce, any relevant licensing terms<br>or ToU, as well as any fees associated with these restrictions. Yes.<br>The license agreement and terms of use for the dataset can be found at<br>https://aufacebook.com/daisett/wagmant-angying Users must agree to the<br>terms of use before downloading or using the dataset.<br>5. Have any third parties imposed IP-based or other restrictions on the data<br>associated with the instances? If so, please describe these restrictions, and<br>provide a link or other access point to, or otherwise reproduce, any relevant<br>licensing terms, as well as any fees associated with these restrictions. Full<br>terms of use and restrictions on use of the SA-1B dataset can be found at<br>https://aufacebook.com/dazaseas/sagmanianying.<br>6. Do any export controls or other regulatory restrictions apply to the dataset<br>or to individual instances? If so, please describe these restrictions, and pro-<br>vide a link or other access point to, or otherwise reproduce, any supporting<br>documentation. The license and restrictions on use of the SA-1B dataset<br>can be found at https://affacebook.com/datasets/sagmen-inyfing<br>7. Any other comments? No.</p>",
      "id": 269,
      "page": 26,
      "text": "1. Will the dataset be distributed to third parties outside of the entity (e.g.,\ncompany, institution, organization) on behalf of which the dataset was cre-\nated? If so, please provide a description. The dataset will be available for\nthe research community.\n2. How will the dataset will be distributed (e.g., tarball on website, API,\nGitHub)? Does the dataset have a digital object identifier (DOI)? The\ndataset is available at https://affacebook.com/dravets/sagmen-anyhing\n3. When will the dataset be distributed? The dataset will be released in 2023.\n4. Will the dataset be distributed under a copyright or other intellectual\nproperty (IP) license, and/or under applicable terms of use (ToU)? If\nso, please describe this license and/or ToU, and provide a link or other\naccess point to, or otherwise reproduce, any relevant licensing terms\nor ToU, as well as any fees associated with these restrictions. Yes.\nThe license agreement and terms of use for the dataset can be found at\nhttps://aufacebook.com/daisett/wagmant-angying Users must agree to the\nterms of use before downloading or using the dataset.\n5. Have any third parties imposed IP-based or other restrictions on the data\nassociated with the instances? If so, please describe these restrictions, and\nprovide a link or other access point to, or otherwise reproduce, any relevant\nlicensing terms, as well as any fees associated with these restrictions. Full\nterms of use and restrictions on use of the SA-1B dataset can be found at\nhttps://aufacebook.com/dazaseas/sagmanianying.\n6. Do any export controls or other regulatory restrictions apply to the dataset\nor to individual instances? If so, please describe these restrictions, and pro-\nvide a link or other access point to, or otherwise reproduce, any supporting\ndocumentation. The license and restrictions on use of the SA-1B dataset\ncan be found at https://affacebook.com/datasets/sagmen-inyfing\n7. Any other comments? No."
    },
    {
      "bounding_box": [
        {
          "x": 1283,
          "y": 2559
        },
        {
          "x": 1461,
          "y": 2559
        },
        {
          "x": 1461,
          "y": 2592
        },
        {
          "x": 1283,
          "y": 2592
        }
      ],
      "category": "caption",
      "html": "<br><caption id='270' style='font-size:18px'>Maintenance</caption>",
      "id": 270,
      "page": 26,
      "text": "Maintenance"
    },
    {
      "bounding_box": [
        {
          "x": 1286,
          "y": 2607
        },
        {
          "x": 2276,
          "y": 2607
        },
        {
          "x": 2276,
          "y": 2972
        },
        {
          "x": 1286,
          "y": 2972
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='271' style='font-size:14px'>1. Who will be supporting/hosting/maintaining the dataset? The dataset will<br>be hosted at https://ailscebook.com/danaets/sagment-onyting and main-<br>tained by Meta AI.<br>2. How can the owner/curator/manager ofthe dataset be contacted (e.g., email<br>address)? Please email segment-anything@meta.com.<br>3. Is there an erratum? If so, please provide a link or other access point. No.<br>4. Will the dataset be updated (e.g., to correct labeling errors, add new in-<br>stances, delete instances)? If so, please describe how often, by whom, and<br>how updates will be communicated to dataset consumers (e.g., mailing list,</p>",
      "id": 271,
      "page": 26,
      "text": "1. Who will be supporting/hosting/maintaining the dataset? The dataset will\nbe hosted at https://ailscebook.com/danaets/sagment-onyting and main-\ntained by Meta AI.\n2. How can the owner/curator/manager ofthe dataset be contacted (e.g., email\naddress)? Please email segment-anything@meta.com.\n3. Is there an erratum? If so, please provide a link or other access point. No.\n4. Will the dataset be updated (e.g., to correct labeling errors, add new in-\nstances, delete instances)? If so, please describe how often, by whom, and\nhow updates will be communicated to dataset consumers (e.g., mailing list,"
    },
    {
      "bounding_box": [
        {
          "x": 1218,
          "y": 3055
        },
        {
          "x": 1262,
          "y": 3055
        },
        {
          "x": 1262,
          "y": 3091
        },
        {
          "x": 1218,
          "y": 3091
        }
      ],
      "category": "footer",
      "html": "<footer id='272' style='font-size:22px'>26</footer>",
      "id": 272,
      "page": 26,
      "text": "26"
    },
    {
      "bounding_box": [
        {
          "x": 251,
          "y": 315
        },
        {
          "x": 1197,
          "y": 315
        },
        {
          "x": 1197,
          "y": 384
        },
        {
          "x": 251,
          "y": 384
        }
      ],
      "category": "paragraph",
      "html": "<p id='273' style='font-size:14px'>GitHub)? To aid reproducibility of research using SA-1B, the only updates<br>will be to remove reported images.</p>",
      "id": 273,
      "page": 27,
      "text": "GitHub)? To aid reproducibility of research using SA-1B, the only updates\nwill be to remove reported images."
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 399
        },
        {
          "x": 1198,
          "y": 399
        },
        {
          "x": 1198,
          "y": 1130
        },
        {
          "x": 204,
          "y": 1130
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='274' style='font-size:14px'>5. Ifthe dataset relates to people, are there applicable limits on the retention of<br>the data associated with the instances (e.g., were the individuals in question<br>told that their data would be retained for a fixed period of time and then<br>deleted)? If so, please describe these limits and explain how they will be<br>enforced. There are no limits on data retention. We took measures to remove<br>personally identifiable information from any images of people. Users may<br>report content for potential removal here: segment-anything@meta.com.<br>6. Will older versions of the dataset continue to be sup-<br>ported/hosted/maintained? If so, please describe how. If not, please<br>describe how its obsolescence will be communicated to dataset consumers.<br>No, as the only updates will be to remove potentially harmful content, we<br>will not keep older versions with the content.<br>7. If others want to extend/augment/build on/contribute to the dataset, is there<br>a mechanism for them to do so? If so, please provide a description. Will<br>these contributions be validated/verified? If so, please describe how. If not,<br>why not? Is there a process for communicating/distributing these contribu-<br>tions to dataset consumers? If so, please provide a description. We encour-<br>age users to gather further annotations for SA-1B. Any users who generate<br>annotations will be liable for hosting and distributing their annotations.</p>",
      "id": 274,
      "page": 27,
      "text": "5. Ifthe dataset relates to people, are there applicable limits on the retention of\nthe data associated with the instances (e.g., were the individuals in question\ntold that their data would be retained for a fixed period of time and then\ndeleted)? If so, please describe these limits and explain how they will be\nenforced. There are no limits on data retention. We took measures to remove\npersonally identifiable information from any images of people. Users may\nreport content for potential removal here: segment-anything@meta.com.\n6. Will older versions of the dataset continue to be sup-\nported/hosted/maintained? If so, please describe how. If not, please\ndescribe how its obsolescence will be communicated to dataset consumers.\nNo, as the only updates will be to remove potentially harmful content, we\nwill not keep older versions with the content.\n7. If others want to extend/augment/build on/contribute to the dataset, is there\na mechanism for them to do so? If so, please provide a description. Will\nthese contributions be validated/verified? If so, please describe how. If not,\nwhy not? Is there a process for communicating/distributing these contribu-\ntions to dataset consumers? If so, please provide a description. We encour-\nage users to gather further annotations for SA-1B. Any users who generate\nannotations will be liable for hosting and distributing their annotations."
    },
    {
      "bounding_box": [
        {
          "x": 205,
          "y": 1123
        },
        {
          "x": 585,
          "y": 1123
        },
        {
          "x": 585,
          "y": 1159
        },
        {
          "x": 205,
          "y": 1159
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='275' style='font-size:14px'>8. Any other comments? No.</p>",
      "id": 275,
      "page": 27,
      "text": "8. Any other comments? No."
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 1200
        },
        {
          "x": 731,
          "y": 1200
        },
        {
          "x": 731,
          "y": 1246
        },
        {
          "x": 203,
          "y": 1246
        }
      ],
      "category": "paragraph",
      "html": "<p id='276' style='font-size:22px'>F.2. Data Annotation Card</p>",
      "id": 276,
      "page": 27,
      "text": "F.2. Data Annotation Card"
    },
    {
      "bounding_box": [
        {
          "x": 205,
          "y": 1284
        },
        {
          "x": 450,
          "y": 1284
        },
        {
          "x": 450,
          "y": 1320
        },
        {
          "x": 205,
          "y": 1320
        }
      ],
      "category": "paragraph",
      "html": "<p id='277' style='font-size:16px'>Task Formulation</p>",
      "id": 277,
      "page": 27,
      "text": "Task Formulation"
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 1324
        },
        {
          "x": 1199,
          "y": 1324
        },
        {
          "x": 1199,
          "y": 2720
        },
        {
          "x": 203,
          "y": 2720
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='278' style='font-size:14px'>1. At a high level, what are the subjective aspects of your task? Segmenting<br>objects present in an image is inherently a subjective task. For instance,<br>one annotator may segment two boots as one mask, whereas another may<br>segment each boot separately. Depending on annotators's skills, the quality<br>of the mask and the number of masks per image are different between an-<br>notators. Despite these subjective aspects of the task, we believed efficient<br>annotation was possible as the data was annotated in a per-mask fashion<br>with the main focus on the diversity of the data rather than completeness.<br>2. What assumptions do you make about annotators? Our annotators worked<br>full time on our annotation task with very small attrition rate. This made<br>it possible to train the annotators providing feedback and answering their<br>questions on a regular basis. Specifically: (1) By giving a clear understand-<br>ing of the goals of this work and providing clear guidelines, including vi-<br>suals and video recordings of the tasks, annotators had enough context to<br>understand and perform the tasks reasonably. (2) Sharing objectives and<br>key results and meeting weekly with annotators increased the likelihood<br>that annotators improved annotation quality and quantity over time.<br>3. How did you choose the specific wording of your task instructions? What<br>steps, if any, were taken to verify the clarity oftask instructions and wording<br>for annotators? As our task was annotating images, the annotation guide-<br>lines included visual examples. Our research team completed 30 annotation<br>tasks to identify any obvious challenges using the annotation tool, collec-<br>tively decide how to handle complex cases, and refine the guidelines. The<br>research team met with the annotators weekly for feedback sessions. Videos<br>of the research team performing the task were shared live with the annota-<br>tors, followed by Q&A sessions. Annotators were able to give feedback on<br>unclear aspects, both during the feedback session and asynchronously.<br>4. What, if any, risks did your task pose for annotators and were they informed<br>of the risks prior to engagement with the task? No identified risks. Images<br>were filtered for objectionable content prior to the annotation phase.<br>5. What are the precise instructions that were provided to annotators? We<br>provide only high-level instructions: Given an image, we aim at segment-<br>ing every possible object. Annotators generate a mask for every potential<br>object they can identify. An object can be segmented using our interactive<br>segmentation tool either by using corrective foreground/background clicks<br>to add/remove parts of the mask or by drawing a bounding box around the<br>object. Masks can be refined using pixel-precise tools.</p>",
      "id": 278,
      "page": 27,
      "text": "1. At a high level, what are the subjective aspects of your task? Segmenting\nobjects present in an image is inherently a subjective task. For instance,\none annotator may segment two boots as one mask, whereas another may\nsegment each boot separately. Depending on annotators's skills, the quality\nof the mask and the number of masks per image are different between an-\nnotators. Despite these subjective aspects of the task, we believed efficient\nannotation was possible as the data was annotated in a per-mask fashion\nwith the main focus on the diversity of the data rather than completeness.\n2. What assumptions do you make about annotators? Our annotators worked\nfull time on our annotation task with very small attrition rate. This made\nit possible to train the annotators providing feedback and answering their\nquestions on a regular basis. Specifically: (1) By giving a clear understand-\ning of the goals of this work and providing clear guidelines, including vi-\nsuals and video recordings of the tasks, annotators had enough context to\nunderstand and perform the tasks reasonably. (2) Sharing objectives and\nkey results and meeting weekly with annotators increased the likelihood\nthat annotators improved annotation quality and quantity over time.\n3. How did you choose the specific wording of your task instructions? What\nsteps, if any, were taken to verify the clarity oftask instructions and wording\nfor annotators? As our task was annotating images, the annotation guide-\nlines included visual examples. Our research team completed 30 annotation\ntasks to identify any obvious challenges using the annotation tool, collec-\ntively decide how to handle complex cases, and refine the guidelines. The\nresearch team met with the annotators weekly for feedback sessions. Videos\nof the research team performing the task were shared live with the annota-\ntors, followed by Q&A sessions. Annotators were able to give feedback on\nunclear aspects, both during the feedback session and asynchronously.\n4. What, if any, risks did your task pose for annotators and were they informed\nof the risks prior to engagement with the task? No identified risks. Images\nwere filtered for objectionable content prior to the annotation phase.\n5. What are the precise instructions that were provided to annotators? We\nprovide only high-level instructions: Given an image, we aim at segment-\ning every possible object. Annotators generate a mask for every potential\nobject they can identify. An object can be segmented using our interactive\nsegmentation tool either by using corrective foreground/background clicks\nto add/remove parts of the mask or by drawing a bounding box around the\nobject. Masks can be refined using pixel-precise tools."
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 2728
        },
        {
          "x": 501,
          "y": 2728
        },
        {
          "x": 501,
          "y": 2762
        },
        {
          "x": 203,
          "y": 2762
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='279' style='font-size:18px'>Selecting Annotations</p>",
      "id": 279,
      "page": 27,
      "text": "Selecting Annotations"
    },
    {
      "bounding_box": [
        {
          "x": 206,
          "y": 2776
        },
        {
          "x": 1199,
          "y": 2776
        },
        {
          "x": 1199,
          "y": 2974
        },
        {
          "x": 206,
          "y": 2974
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='280' style='font-size:14px'>1. Are there certain perspectives that should be privileged? If so, how did you<br>seek these perspectives out? We chose to work with annotators that have<br>worked on other vision annotation tasks before.<br>2. Are there certain perspectives that would be harmful to include? If so, how<br>did you screen these perspectives out? No.</p>",
      "id": 280,
      "page": 27,
      "text": "1. Are there certain perspectives that should be privileged? If so, how did you\nseek these perspectives out? We chose to work with annotators that have\nworked on other vision annotation tasks before.\n2. Are there certain perspectives that would be harmful to include? If so, how\ndid you screen these perspectives out? No."
    },
    {
      "bounding_box": [
        {
          "x": 1287,
          "y": 315
        },
        {
          "x": 2275,
          "y": 315
        },
        {
          "x": 2275,
          "y": 870
        },
        {
          "x": 1287,
          "y": 870
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='281' style='font-size:14px'>3. Were sociodemographic characteristics used to select annotators for your<br>task? If so, please detail the process. No.<br>4. Ifyou have any aggregated socio-demographic statistics about your anno-<br>tator pool, please describe. Do you have reason to believe that sociode-<br>mographic characteristics of annotators may have impacted how they an-<br>notated the data? Why or why not? We worked with 130 annotators. The<br>annotators were all based in Kenya. We do not believe sociodemographic<br>characteristics of annotators meaningfully impacted the annotated data.<br>5. Consider the intended context of use of the dataset and the individuals<br>and communities that may be impacted by a model trained on this dataset.<br>Are these communities represented in your annotator pool? The Segment<br>Anything 1B (SA-1B) dataset is to be used for research purposes only.<br>The SA-1B dataset is one of the most geographically diverse segmentation<br>dataset, as discussed in §6. In addition, we analyze the responsible AI axes<br>of a model trained on the dataset in §6.</p>",
      "id": 281,
      "page": 27,
      "text": "3. Were sociodemographic characteristics used to select annotators for your\ntask? If so, please detail the process. No.\n4. Ifyou have any aggregated socio-demographic statistics about your anno-\ntator pool, please describe. Do you have reason to believe that sociode-\nmographic characteristics of annotators may have impacted how they an-\nnotated the data? Why or why not? We worked with 130 annotators. The\nannotators were all based in Kenya. We do not believe sociodemographic\ncharacteristics of annotators meaningfully impacted the annotated data.\n5. Consider the intended context of use of the dataset and the individuals\nand communities that may be impacted by a model trained on this dataset.\nAre these communities represented in your annotator pool? The Segment\nAnything 1B (SA-1B) dataset is to be used for research purposes only.\nThe SA-1B dataset is one of the most geographically diverse segmentation\ndataset, as discussed in §6. In addition, we analyze the responsible AI axes\nof a model trained on the dataset in §6."
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 894
        },
        {
          "x": 1779,
          "y": 894
        },
        {
          "x": 1779,
          "y": 931
        },
        {
          "x": 1281,
          "y": 931
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='282' style='font-size:16px'>Platform and Infrastructure Choices</p>",
      "id": 282,
      "page": 27,
      "text": "Platform and Infrastructure Choices"
    },
    {
      "bounding_box": [
        {
          "x": 1288,
          "y": 938
        },
        {
          "x": 2275,
          "y": 938
        },
        {
          "x": 2275,
          "y": 1599
        },
        {
          "x": 1288,
          "y": 1599
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='283' style='font-size:14px'>1. What annotation platform did you utilize? At a high level, what considera-<br>tions informed your decision to choose this platform? Did the chosen plat-<br>form sufficiently meet the requirements you outlined for annotator pools?<br>Are any aspects not covered? We used a proprietary annotation platform.<br>2. What, if any, communication channels did your chosen platform offer to<br>facilitate communication with annotators? How did this channel of com-<br>munication influence the annotation process and/or resulting annotations?<br>We manually reviewed annotations and shared feedback with the annotators<br>on a weekly basis. We communicated common mistakes or inconsisten-<br>cies and the corresponding corrections. In addition, the annotators were<br>given feedback for improvements daily by the annotation QA team. Out-<br>side the weekly feedback sessions, annotators had access to a spreadsheet<br>and chat group to facilitate communication with the research team. This<br>process greatly improved the average speed and quality of the annotations.<br>3. How much were annotators compensated? Did you consider any partic-<br>ular pay standards, when determining their compensation? If so, please<br>describe. Annotators were compensated with an hourly wage set by the<br>vendor. The vendor is a Certified B Corporation.</p>",
      "id": 283,
      "page": 27,
      "text": "1. What annotation platform did you utilize? At a high level, what considera-\ntions informed your decision to choose this platform? Did the chosen plat-\nform sufficiently meet the requirements you outlined for annotator pools?\nAre any aspects not covered? We used a proprietary annotation platform.\n2. What, if any, communication channels did your chosen platform offer to\nfacilitate communication with annotators? How did this channel of com-\nmunication influence the annotation process and/or resulting annotations?\nWe manually reviewed annotations and shared feedback with the annotators\non a weekly basis. We communicated common mistakes or inconsisten-\ncies and the corresponding corrections. In addition, the annotators were\ngiven feedback for improvements daily by the annotation QA team. Out-\nside the weekly feedback sessions, annotators had access to a spreadsheet\nand chat group to facilitate communication with the research team. This\nprocess greatly improved the average speed and quality of the annotations.\n3. How much were annotators compensated? Did you consider any partic-\nular pay standards, when determining their compensation? If so, please\ndescribe. Annotators were compensated with an hourly wage set by the\nvendor. The vendor is a Certified B Corporation."
    },
    {
      "bounding_box": [
        {
          "x": 1282,
          "y": 1625
        },
        {
          "x": 1722,
          "y": 1625
        },
        {
          "x": 1722,
          "y": 1662
        },
        {
          "x": 1282,
          "y": 1662
        }
      ],
      "category": "paragraph",
      "html": "<p id='284' style='font-size:16px'>Dataset Analysis and Evaluation</p>",
      "id": 284,
      "page": 27,
      "text": "Dataset Analysis and Evaluation"
    },
    {
      "bounding_box": [
        {
          "x": 1289,
          "y": 1673
        },
        {
          "x": 2276,
          "y": 1673
        },
        {
          "x": 2276,
          "y": 2297
        },
        {
          "x": 1289,
          "y": 2297
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='285' style='font-size:14px'>1. How do you define the quality of annotations in your context, and how did<br>you assess the quality in the dataset you constructed? Annotators were first<br>placed into training. They followed a 1-day training session led by the ven-<br>dor and then were asked to annotate a large number of examples from a<br>training queue. Annotators graduated from training to production after the<br>vendor QA team, in collaboration with the research team, manually spot-<br>checked the annotator's masks to ensure quality. On average, annotators<br>spent one week in training before graduating. Production quality assess-<br>ment followed a similar process: the vendor QA team and the research team<br>manually reviewed the annotations weekly, sharing feedback weekly.<br>2. Have you conducted any analysis on disagreement patterns? If so, what<br>analyses did you use and what were the major findings? Did you analyze<br>potential sources of disagreement? We pointed out common mistakes dur-<br>ing weekly meetings with the annotators.<br>3. How do the individual annotator responses relate to the final labels released<br>in the dataset? The annotations were only used to train early versions of the<br>SAM model and we do not currently plan to release them.</p>",
      "id": 285,
      "page": 27,
      "text": "1. How do you define the quality of annotations in your context, and how did\nyou assess the quality in the dataset you constructed? Annotators were first\nplaced into training. They followed a 1-day training session led by the ven-\ndor and then were asked to annotate a large number of examples from a\ntraining queue. Annotators graduated from training to production after the\nvendor QA team, in collaboration with the research team, manually spot-\nchecked the annotator's masks to ensure quality. On average, annotators\nspent one week in training before graduating. Production quality assess-\nment followed a similar process: the vendor QA team and the research team\nmanually reviewed the annotations weekly, sharing feedback weekly.\n2. Have you conducted any analysis on disagreement patterns? If so, what\nanalyses did you use and what were the major findings? Did you analyze\npotential sources of disagreement? We pointed out common mistakes dur-\ning weekly meetings with the annotators.\n3. How do the individual annotator responses relate to the final labels released\nin the dataset? The annotations were only used to train early versions of the\nSAM model and we do not currently plan to release them."
    },
    {
      "bounding_box": [
        {
          "x": 1282,
          "y": 2321
        },
        {
          "x": 1736,
          "y": 2321
        },
        {
          "x": 1736,
          "y": 2356
        },
        {
          "x": 1282,
          "y": 2356
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='286' style='font-size:14px'>Dataset Release and Maintenance</p>",
      "id": 286,
      "page": 27,
      "text": "Dataset Release and Maintenance"
    },
    {
      "bounding_box": [
        {
          "x": 1289,
          "y": 2368
        },
        {
          "x": 2276,
          "y": 2368
        },
        {
          "x": 2276,
          "y": 2952
        },
        {
          "x": 1289,
          "y": 2952
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='287' style='font-size:14px'>1. Do you have reason to believe the annotations in this dataset may change<br>over time? Do you plan to update your dataset? No, except to remove<br>objectionable images.<br>2. Are there any conditions or definitions that, if changed, could impact the<br>utility ofyour dataset? We do not believe so.<br>3. Will you attempt to track, impose limitations on, or otherwise influence how<br>your dataset is used? If so, how? The SA-1B dataset will be released under<br>a license agreement allowing use for certain research purposes and protec-<br>tions for researchers. Researchers must agree to the terms of the license<br>agreement to access the dataset.<br>4. Were annotators informed about how the data is externalized? If changes to<br>the dataset are made, will they be informed? No, we do not plan to release<br>the manual annotations at the moment.<br>5. Is there a process by which annotators can later choose to withdraw their<br>data from the dataset? If so, please detail. No.</p>",
      "id": 287,
      "page": 27,
      "text": "1. Do you have reason to believe the annotations in this dataset may change\nover time? Do you plan to update your dataset? No, except to remove\nobjectionable images.\n2. Are there any conditions or definitions that, if changed, could impact the\nutility ofyour dataset? We do not believe so.\n3. Will you attempt to track, impose limitations on, or otherwise influence how\nyour dataset is used? If so, how? The SA-1B dataset will be released under\na license agreement allowing use for certain research purposes and protec-\ntions for researchers. Researchers must agree to the terms of the license\nagreement to access the dataset.\n4. Were annotators informed about how the data is externalized? If changes to\nthe dataset are made, will they be informed? No, we do not plan to release\nthe manual annotations at the moment.\n5. Is there a process by which annotators can later choose to withdraw their\ndata from the dataset? If so, please detail. No."
    },
    {
      "bounding_box": [
        {
          "x": 1218,
          "y": 3055
        },
        {
          "x": 1262,
          "y": 3055
        },
        {
          "x": 1262,
          "y": 3090
        },
        {
          "x": 1218,
          "y": 3090
        }
      ],
      "category": "footer",
      "html": "<footer id='288' style='font-size:20px'>27</footer>",
      "id": 288,
      "page": 27,
      "text": "27"
    },
    {
      "bounding_box": [
        {
          "x": 230,
          "y": 294
        },
        {
          "x": 457,
          "y": 294
        },
        {
          "x": 457,
          "y": 333
        },
        {
          "x": 230,
          "y": 333
        }
      ],
      "category": "caption",
      "html": "<caption id='289' style='font-size:16px'>Model Overview</caption>",
      "id": 289,
      "page": 28,
      "text": "Model Overview"
    },
    {
      "bounding_box": [
        {
          "x": 230,
          "y": 311
        },
        {
          "x": 2142,
          "y": 311
        },
        {
          "x": 2142,
          "y": 2567
        },
        {
          "x": 230,
          "y": 2567
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='290' style='font-size:14px'>Name SAM or Segment Anything Model<br>Version 1.0<br>Date 2023<br>Organization The FAIR team of Meta AI<br>Mode type Promptable segmentation model<br>Architecture See §3<br>Repository https://gibot.com/facebodiresachikegreenting<br>Citation http://www.com/prebookcomificalice/vendanthing<br>License Apache 2.0<br>Intended Use<br>Primary intended uses SAM is intended to be used for any prompt-based segmentation task. We explored its use in segmenting objects<br>from a point (§7.1), edge detection (§7.2), segmenting all objects (§7.3), and segmenting detected objects (§7.4).<br>We explored how SAM can integrate with other vision models to segment objects from text (§7.5).<br>Primary intended users SAM was primarily developed for research. The license for SAM can be found at<br>http://ghhub.com/flicebookresexchiegmentanything<br>Out-of-scope use cases See terms of use for SAM found at https://ghti.com/Kacebookrascondexgrangranything.gt See Use Cases under<br>Ethical Considerations.<br>Caveats and recommendations SAM has impressive zero-shot performance across a wide range of tasks. We note, however, that in the zero-shot<br>setting there may be multiple valid ground truth masks for a given input. We recommend users take this into<br>consideration when using SAM for zero-shot segmentation. SAM can miss fine structures and can hallucinate<br>small disconnected components. See §8 for a discussion of limitations.<br>Relevant Factors<br>Groups SAM was designed to segment any object. This includes stuff and things.<br>Instrumentation and environment We benchmarked SAM on a diverse set of datasets and found that SAM can handle a variety of visual data including<br>simulations, paintings, underwater images, microscopy images, driving data, stereo images, fish-eye images. See<br>§D.1 and Table 7 for information on the benchmarks used.<br>Metrics<br>Model performance measures We evaluated SAM on a variety of metrics based on the downstream task in our experiments.<br>· mIoU: We used the mean intersection-over-union after a given number of prompts to evaluate the segmen-<br>tation quality of a mask when prompted with points.<br>· Human evaluation: We performed a human study (detailed in §E) to evaluate the real world performance<br>of SAM. We compared the masks generated by SAM to a baseline state-of-the-art interactive segmentation<br>model, RITM [92], using a perceptual quality scale from 1 to 10.<br>· AP: We used average precision to evaluate instance segmentation for a given box and edge detection.<br>· AR@ 1000: We used average recall to evaluate object proposal generation.<br>· ODS, OIS, AP, R50: We used the standard edge detection evaluation metrics from BSDS500 [72, 3].<br>Evaluation Data<br>Data sources See §D.1.<br>Training Data<br>Data source I See Data Card in §F.1.<br>Ethical Considerations<br>Data We trained SAM on licensed images. The images were filtered for objectionable content by the provider, but we<br>acknowledge the possibility of false negatives. We performed a geographic analysis of the SA-1B dataset in §6.<br>While SA-1B is more geographically diverse than many of its predecessors, we acknowledge that some geographic<br>regions and economic groups are underrepresented.<br>Cost and impact of compute SAM was trained on 256 A100 GPUS for 68 hours. We acknowledge the environmental impact and cost of training<br>large scale models. The environmental impact of training the released SAM model is approximately 6963 kWh<br>resulting in an estimated 2.8 metric tons of carbon dioxide given the specific data center used, using the calculation<br>described in [77] and the ML CO2 Impact calculator [61]. This is equivalent to ~7k miles driven by the average<br>gasoline-powered passenger vehicle in the US [101]. We released the SAM models to both reduce the need for<br>retraining and lower the barrier to entry for large scale vision research.<br>Risks and harms We evaluated SAM for fairness in §6. Downstream use cases of SAM will create their own potential for biases<br>and fairness concerns. As such we recommend users run their own fairness evaluation when using SAM for their<br>specific use case.<br>Use cases We implore users to use their best judgement for downstream use of the model.</p>",
      "id": 290,
      "page": 28,
      "text": "Name SAM or Segment Anything Model\nVersion 1.0\nDate 2023\nOrganization The FAIR team of Meta AI\nMode type Promptable segmentation model\nArchitecture See §3\nRepository https://gibot.com/facebodiresachikegreenting\nCitation http://www.com/prebookcomificalice/vendanthing\nLicense Apache 2.0\nIntended Use\nPrimary intended uses SAM is intended to be used for any prompt-based segmentation task. We explored its use in segmenting objects\nfrom a point (§7.1), edge detection (§7.2), segmenting all objects (§7.3), and segmenting detected objects (§7.4).\nWe explored how SAM can integrate with other vision models to segment objects from text (§7.5).\nPrimary intended users SAM was primarily developed for research. The license for SAM can be found at\nhttp://ghhub.com/flicebookresexchiegmentanything\nOut-of-scope use cases See terms of use for SAM found at https://ghti.com/Kacebookrascondexgrangranything.gt See Use Cases under\nEthical Considerations.\nCaveats and recommendations SAM has impressive zero-shot performance across a wide range of tasks. We note, however, that in the zero-shot\nsetting there may be multiple valid ground truth masks for a given input. We recommend users take this into\nconsideration when using SAM for zero-shot segmentation. SAM can miss fine structures and can hallucinate\nsmall disconnected components. See §8 for a discussion of limitations.\nRelevant Factors\nGroups SAM was designed to segment any object. This includes stuff and things.\nInstrumentation and environment We benchmarked SAM on a diverse set of datasets and found that SAM can handle a variety of visual data including\nsimulations, paintings, underwater images, microscopy images, driving data, stereo images, fish-eye images. See\n§D.1 and Table 7 for information on the benchmarks used.\nMetrics\nModel performance measures We evaluated SAM on a variety of metrics based on the downstream task in our experiments.\n· mIoU: We used the mean intersection-over-union after a given number of prompts to evaluate the segmen-\ntation quality of a mask when prompted with points.\n· Human evaluation: We performed a human study (detailed in §E) to evaluate the real world performance\nof SAM. We compared the masks generated by SAM to a baseline state-of-the-art interactive segmentation\nmodel, RITM [92], using a perceptual quality scale from 1 to 10.\n· AP: We used average precision to evaluate instance segmentation for a given box and edge detection.\n· AR@ 1000: We used average recall to evaluate object proposal generation.\n· ODS, OIS, AP, R50: We used the standard edge detection evaluation metrics from BSDS500 [72, 3].\nEvaluation Data\nData sources See §D.1.\nTraining Data\nData source I See Data Card in §F.1.\nEthical Considerations\nData We trained SAM on licensed images. The images were filtered for objectionable content by the provider, but we\nacknowledge the possibility of false negatives. We performed a geographic analysis of the SA-1B dataset in §6.\nWhile SA-1B is more geographically diverse than many of its predecessors, we acknowledge that some geographic\nregions and economic groups are underrepresented.\nCost and impact of compute SAM was trained on 256 A100 GPUS for 68 hours. We acknowledge the environmental impact and cost of training\nlarge scale models. The environmental impact of training the released SAM model is approximately 6963 kWh\nresulting in an estimated 2.8 metric tons of carbon dioxide given the specific data center used, using the calculation\ndescribed in [77] and the ML CO2 Impact calculator [61]. This is equivalent to ~7k miles driven by the average\ngasoline-powered passenger vehicle in the US [101]. We released the SAM models to both reduce the need for\nretraining and lower the barrier to entry for large scale vision research.\nRisks and harms We evaluated SAM for fairness in §6. Downstream use cases of SAM will create their own potential for biases\nand fairness concerns. As such we recommend users run their own fairness evaluation when using SAM for their\nspecific use case.\nUse cases We implore users to use their best judgement for downstream use of the model."
    },
    {
      "bounding_box": [
        {
          "x": 636,
          "y": 2575
        },
        {
          "x": 1843,
          "y": 2575
        },
        {
          "x": 1843,
          "y": 2628
        },
        {
          "x": 636,
          "y": 2628
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='291' style='font-size:20px'>Table 9: Model Card for SAM, following the procedure detailed in [75].</p>",
      "id": 291,
      "page": 28,
      "text": "Table 9: Model Card for SAM, following the procedure detailed in [75]."
    },
    {
      "bounding_box": [
        {
          "x": 1218,
          "y": 3056
        },
        {
          "x": 1261,
          "y": 3056
        },
        {
          "x": 1261,
          "y": 3090
        },
        {
          "x": 1218,
          "y": 3090
        }
      ],
      "category": "footer",
      "html": "<footer id='292' style='font-size:18px'>28</footer>",
      "id": 292,
      "page": 28,
      "text": "28"
    },
    {
      "bounding_box": [
        {
          "x": 191,
          "y": 277
        },
        {
          "x": 2294,
          "y": 277
        },
        {
          "x": 2294,
          "y": 2555
        },
        {
          "x": 191,
          "y": 2555
        }
      ],
      "category": "figure",
      "html": "<figure><img id='293' style='font-size:14px' alt=\"We have several modelsthat, when provided with a click or a box as input, output a mask. We would\nlike to compare the quality of these models by rating the quality of their masks on many examples.\nThe interface will be different than for regular mask annotation.\n· Each job reviews one mask in one image.\n· On the right, there will be five image thumbnails in two rows. Each thumbnail can be moused-\nover to show the image at a larger size. Clicking on the thumbnailwill make itfull screen, and\nclicking again will return to the original screen.\n- The images show the same mask in five different views. On the top row: (left) the image\nwithout the mask, (middle) the mask overlaid on the image, and (right) the mask alone. On\nthe bottom row: (left) a zoomed in view ofthe object without a mask, and (right) a zoomed\nin view of the mask overlaid on the image. These views are provided to make it easy to see\ndifferent types of mask errors.\nThe mask will be in red when overlaid on the image.\nWhen shown by itself, the mask is yellow, and the background is purple.\nEach image willinclude eithera blue dot or a blue and white box. This is the input to the\nmodel, as if you had clicked at this location or drawn this box.\n· On the left, there are buttons labeled 1-10. This is used to rate the quality of the shown mask.\nObjective and Setup Example interface page. There will be five images on the Mouse over an image to show the full image. Click on an image to make it full screen. The arrows will cy-\nright and a question box on the left. cle between images. Click again to return to previous view.\nし\nThe first image on the top row shows the image without a The second image on the top row shows the mask for the The third image on the top row shows the mask only. The The first image on the bottom row shows a zoomed in view\nmask. A blue point will be on the object of interest, or a object in red. mask is in yellow and the background is purple. of the object without a mask.\nblue and white box will surround it.\nDoes the mask correspond to an actual object?\n· Valid objects can include:\nWhat we would like you to do for each job:\nEntire single objects (such as a person, shirt, or tree)\n· Please aim to spend up to 30 seconds perjob. Logical parts of objects (a chair leg, a car door, a tabletop)\n· Mouse-over or click each of the three images of the mask on the right to get a sense of the Collections of objects (a stack of books, acrowd of people)\nquality of the mask. The thumbnail 一 too small to judge a mask. do not judge a mask by the 'Stuff' (the ground, the sky).\nthumbnail alone. Each image can provide a different signal on possible mask errors:\nExample errors a mask may have. The severity of these errors may be minor or major:\n- The unzoomed image can give context for the mask: does this mask correspond to an actual\n- Include a piece of another object (the mask of a person including the arm of a nearby\nobject?\nperson)\nThe mask-only image can show if the mask has small holes or separated, incorrect pixels.\n- Miss part of an object (the mask covers only one part of a building obscured by a tree in\nThe zoomed image can show if the mask boundaries make sense.\nthe foreground),\nJudge the qualityof the mask on three criterion. Examples will follow. - Combine two unrelated things (a single mask covers both a mug and a pen on a desk)\nInclude an arbitrary part of a collection for a point input (a point 의 on one apple, but\nDoes the mask correspond to an actual object?\nthe mask covers three apples in a pile of many apples). If a box surrounds an arbitrary\nDoes the mask have a good boundary?\ncollection, it is not an error to provide a mask for these objects.\nDoes the mask correspond to the provided point or box?\nIfyou are unsure, a good rule-of-thumb is: can you name the object in question? However,\nRate the quality of the mask on a scale of 1-10 using the drop-down box on the left.\nsome things that are hard to name may still be good objects (an unusual component of a\n· Next are details and examples for judging mask quality according to the three criterion. These machine, something at the edge of the image for which itis hard to determine whatitis).\nare just examples and other cases may come up, please use your best judgment when deter-\nmining if something is a good mask.\nThe second image on the bottom row shows a zoomed in On the left are buttons to rate the mask quality, with selec- Task Judging Mask Quality (1 of 3)\nview of the object with a mask. The mask is in red. tions 1-10.\nDoes the mask correspond to the provided point or box?\n· For points:\nThe point needs to be on the mask.\nDoes the mask have a good boundary? The size or position of the object with respect to the point does not matter (a point on\nsomeone s gloved hand can correspond to the glove or to the entire person. both are valid\n· Errors in the boundary can include:\nmasks).\nIncorrect holes in the mask\nFor boxes:\nIncorrect pixels included separated from the main part of the mask\nPoor edge quality, where the mask does not exactly match the edge of the object. The object needs to be the best object that is the size of the box (if a box IS around some-\nFailure to consistently handle obscuring foreground objects (a mask that covers obscuring one's entire head but the mask is of their hair, this is an error: their hair is in the box but is\nobjects is fine, and a mask that doesn't cover obscuring objects is fine, but one that does not the correct object).\nsome of both has an error) - Ifthe box clearly corresponds to a given object but is slightly smaller than it, it is okay if\nPixelation of a small mask is not an error, as long as the mask still matches the edges of the mask goes slightly outside a box (if a box around a person misses their extended hand,\nthe object. the mask can still include their hand even ifthe mask goes outside the box).\nJudging Mask Quality (2 of3) Judging Mask Quality (3 of 3) Example error of 'Include a piece of another object' : The Example error of 'Missing a part of an object': the mask is\nelephant mask contains a piece of another nearby elephant. missing a disconnected part of the object: the back half of\nthe zebra, and the right portion of the plate.\nI\nExample error of 'Include an arbitrary part of a collection': Example error for 'Incorrect holes in the mask' : This mask Example error for 'Incorrect pixels included separated from Example error for 'Poor edge quality' : The mask has poor\nIn top top image, the point is on one orange rind, but the has holes in the upper left and on the left sides (black ar- the main part of the mask' : The 'mask only' view reveals a edge quality, both along the edge of the umbrella, as well as\nmask covers two orange rinds. This is a mask error: the rows). These holes are much easier to see on the 'mask few stray incorrect pixels on the clock face. along the thin pole.\nmask covers an arbitrary number of objects in the collection, only' image.\nand should either cover one orange rind or all of them. In\" data-coord=\"top-left:(191,277); bottom-right:(2294,2555)\" /></figure>",
      "id": 293,
      "page": 29,
      "text": "We have several modelsthat, when provided with a click or a box as input, output a mask. We would\nlike to compare the quality of these models by rating the quality of their masks on many examples.\nThe interface will be different than for regular mask annotation.\n· Each job reviews one mask in one image.\n· On the right, there will be five image thumbnails in two rows. Each thumbnail can be moused-\nover to show the image at a larger size. Clicking on the thumbnailwill make itfull screen, and\nclicking again will return to the original screen.\n- The images show the same mask in five different views. On the top row: (left) the image\nwithout the mask, (middle) the mask overlaid on the image, and (right) the mask alone. On\nthe bottom row: (left) a zoomed in view ofthe object without a mask, and (right) a zoomed\nin view of the mask overlaid on the image. These views are provided to make it easy to see\ndifferent types of mask errors.\nThe mask will be in red when overlaid on the image.\nWhen shown by itself, the mask is yellow, and the background is purple.\nEach image willinclude eithera blue dot or a blue and white box. This is the input to the\nmodel, as if you had clicked at this location or drawn this box.\n· On the left, there are buttons labeled 1-10. This is used to rate the quality of the shown mask.\nObjective and Setup Example interface page. There will be five images on the Mouse over an image to show the full image. Click on an image to make it full screen. The arrows will cy-\nright and a question box on the left. cle between images. Click again to return to previous view.\nし\nThe first image on the top row shows the image without a The second image on the top row shows the mask for the The third image on the top row shows the mask only. The The first image on the bottom row shows a zoomed in view\nmask. A blue point will be on the object of interest, or a object in red. mask is in yellow and the background is purple. of the object without a mask.\nblue and white box will surround it.\nDoes the mask correspond to an actual object?\n· Valid objects can include:\nWhat we would like you to do for each job:\nEntire single objects (such as a person, shirt, or tree)\n· Please aim to spend up to 30 seconds perjob. Logical parts of objects (a chair leg, a car door, a tabletop)\n· Mouse-over or click each of the three images of the mask on the right to get a sense of the Collections of objects (a stack of books, acrowd of people)\nquality of the mask. The thumbnail 一 too small to judge a mask. do not judge a mask by the 'Stuff' (the ground, the sky).\nthumbnail alone. Each image can provide a different signal on possible mask errors:\nExample errors a mask may have. The severity of these errors may be minor or major:\n- The unzoomed image can give context for the mask: does this mask correspond to an actual\n- Include a piece of another object (the mask of a person including the arm of a nearby\nobject?\nperson)\nThe mask-only image can show if the mask has small holes or separated, incorrect pixels.\n- Miss part of an object (the mask covers only one part of a building obscured by a tree in\nThe zoomed image can show if the mask boundaries make sense.\nthe foreground),\nJudge the qualityof the mask on three criterion. Examples will follow. - Combine two unrelated things (a single mask covers both a mug and a pen on a desk)\nInclude an arbitrary part of a collection for a point input (a point 의 on one apple, but\nDoes the mask correspond to an actual object?\nthe mask covers three apples in a pile of many apples). If a box surrounds an arbitrary\nDoes the mask have a good boundary?\ncollection, it is not an error to provide a mask for these objects.\nDoes the mask correspond to the provided point or box?\nIfyou are unsure, a good rule-of-thumb is: can you name the object in question? However,\nRate the quality of the mask on a scale of 1-10 using the drop-down box on the left.\nsome things that are hard to name may still be good objects (an unusual component of a\n· Next are details and examples for judging mask quality according to the three criterion. These machine, something at the edge of the image for which itis hard to determine whatitis).\nare just examples and other cases may come up, please use your best judgment when deter-\nmining if something is a good mask.\nThe second image on the bottom row shows a zoomed in On the left are buttons to rate the mask quality, with selec- Task Judging Mask Quality (1 of 3)\nview of the object with a mask. The mask is in red. tions 1-10.\nDoes the mask correspond to the provided point or box?\n· For points:\nThe point needs to be on the mask.\nDoes the mask have a good boundary? The size or position of the object with respect to the point does not matter (a point on\nsomeone s gloved hand can correspond to the glove or to the entire person. both are valid\n· Errors in the boundary can include:\nmasks).\nIncorrect holes in the mask\nFor boxes:\nIncorrect pixels included separated from the main part of the mask\nPoor edge quality, where the mask does not exactly match the edge of the object. The object needs to be the best object that is the size of the box (if a box IS around some-\nFailure to consistently handle obscuring foreground objects (a mask that covers obscuring one's entire head but the mask is of their hair, this is an error: their hair is in the box but is\nobjects is fine, and a mask that doesn't cover obscuring objects is fine, but one that does not the correct object).\nsome of both has an error) - Ifthe box clearly corresponds to a given object but is slightly smaller than it, it is okay if\nPixelation of a small mask is not an error, as long as the mask still matches the edges of the mask goes slightly outside a box (if a box around a person misses their extended hand,\nthe object. the mask can still include their hand even ifthe mask goes outside the box).\nJudging Mask Quality (2 of3) Judging Mask Quality (3 of 3) Example error of 'Include a piece of another object' : The Example error of 'Missing a part of an object': the mask is\nelephant mask contains a piece of another nearby elephant. missing a disconnected part of the object: the back half of\nthe zebra, and the right portion of the plate.\nI\nExample error of 'Include an arbitrary part of a collection': Example error for 'Incorrect holes in the mask' : This mask Example error for 'Incorrect pixels included separated from Example error for 'Poor edge quality' : The mask has poor\nIn top top image, the point is on one orange rind, but the has holes in the upper left and on the left sides (black ar- the main part of the mask' : The 'mask only' view reveals a edge quality, both along the edge of the umbrella, as well as\nmask covers two orange rinds. This is a mask error: the rows). These holes are much easier to see on the 'mask few stray incorrect pixels on the clock face. along the thin pole.\nmask covers an arbitrary number of objects in the collection, only' image.\nand should either cover one orange rind or all of them. In"
    },
    {
      "bounding_box": [
        {
          "x": 205,
          "y": 2548
        },
        {
          "x": 707,
          "y": 2548
        },
        {
          "x": 707,
          "y": 2574
        },
        {
          "x": 205,
          "y": 2574
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='294' style='font-size:16px'>the bottom image, the box is around both vegetables. Since</p>",
      "id": 294,
      "page": 29,
      "text": "the bottom image, the box is around both vegetables. Since"
    },
    {
      "bounding_box": [
        {
          "x": 205,
          "y": 2573
        },
        {
          "x": 678,
          "y": 2573
        },
        {
          "x": 678,
          "y": 2596
        },
        {
          "x": 205,
          "y": 2596
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='295' style='font-size:14px'>this is the best match to the box, this is not a mask error.</p>",
      "id": 295,
      "page": 29,
      "text": "this is the best match to the box, this is not a mask error."
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 2628
        },
        {
          "x": 2274,
          "y": 2628
        },
        {
          "x": 2274,
          "y": 2731
        },
        {
          "x": 204,
          "y": 2731
        }
      ],
      "category": "paragraph",
      "html": "<p id='296' style='font-size:18px'>Figure 19: Here we provide the complete guidelines given to annotations for the human review of mask quality. Some images<br>been edited slightly and faces have been blurred to enable release. Best viewed with zoom (part 1 of 2).</p>",
      "id": 296,
      "page": 29,
      "text": "Figure 19: Here we provide the complete guidelines given to annotations for the human review of mask quality. Some images\nbeen edited slightly and faces have been blurred to enable release. Best viewed with zoom (part 1 of 2)."
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 2791
        },
        {
          "x": 757,
          "y": 2791
        },
        {
          "x": 757,
          "y": 2844
        },
        {
          "x": 204,
          "y": 2844
        }
      ],
      "category": "paragraph",
      "html": "<p id='297' style='font-size:22px'>G. Annotation Guidelines</p>",
      "id": 297,
      "page": 29,
      "text": "G. Annotation Guidelines"
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 2875
        },
        {
          "x": 1202,
          "y": 2875
        },
        {
          "x": 1202,
          "y": 2980
        },
        {
          "x": 202,
          "y": 2980
        }
      ],
      "category": "paragraph",
      "html": "<p id='298' style='font-size:20px'>We provide the complete guidelines given to annotations<br>for the human review of mask quality in Fig. 19 and Fig. 20.</p>",
      "id": 298,
      "page": 29,
      "text": "We provide the complete guidelines given to annotations\nfor the human review of mask quality in Fig. 19 and Fig. 20."
    },
    {
      "bounding_box": [
        {
          "x": 1217,
          "y": 3053
        },
        {
          "x": 1262,
          "y": 3053
        },
        {
          "x": 1262,
          "y": 3092
        },
        {
          "x": 1217,
          "y": 3092
        }
      ],
      "category": "footer",
      "html": "<footer id='299' style='font-size:18px'>29</footer>",
      "id": 299,
      "page": 29,
      "text": "29"
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 382
        },
        {
          "x": 704,
          "y": 382
        },
        {
          "x": 704,
          "y": 695
        },
        {
          "x": 203,
          "y": 695
        }
      ],
      "category": "figure",
      "html": "<figure><img id='300' alt=\"\" data-coord=\"top-left:(203,382); bottom-right:(704,695)\" /></figure>",
      "id": 300,
      "page": 30,
      "text": ""
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 709
        },
        {
          "x": 707,
          "y": 709
        },
        {
          "x": 707,
          "y": 787
        },
        {
          "x": 203,
          "y": 787
        }
      ],
      "category": "caption",
      "html": "<caption id='301' style='font-size:18px'>Example for 'Combine two unrelated things : The point in-<br>dicates the lizard, but the mask covers both the lizard and a<br>bird. This is a mask error.</caption>",
      "id": 301,
      "page": 30,
      "text": "Example for 'Combine two unrelated things : The point in-\ndicates the lizard, but the mask covers both the lizard and a\nbird. This is a mask error."
    },
    {
      "bounding_box": [
        {
          "x": 728,
          "y": 381
        },
        {
          "x": 1225,
          "y": 381
        },
        {
          "x": 1225,
          "y": 690
        },
        {
          "x": 728,
          "y": 690
        }
      ],
      "category": "figure",
      "html": "<br><figure><img id='302' alt=\"\" data-coord=\"top-left:(728,381); bottom-right:(1225,690)\" /></figure>",
      "id": 302,
      "page": 30,
      "text": ""
    },
    {
      "bounding_box": [
        {
          "x": 726,
          "y": 708
        },
        {
          "x": 1229,
          "y": 708
        },
        {
          "x": 1229,
          "y": 836
        },
        {
          "x": 726,
          "y": 836
        }
      ],
      "category": "caption",
      "html": "<caption id='303' style='font-size:18px'>Example error for 'Failure to consistently handle obscuring<br>foreground objects' : The pole on the right (blue arrow) is<br>excluded from the mask, while the pole on the left is in-<br>cluded in the object (black arrow). The mask should either<br>include or exclude both of these.</caption>",
      "id": 303,
      "page": 30,
      "text": "Example error for 'Failure to consistently handle obscuring\nforeground objects' : The pole on the right (blue arrow) is\nexcluded from the mask, while the pole on the left is in-\ncluded in the object (black arrow). The mask should either\ninclude or exclude both of these."
    },
    {
      "bounding_box": [
        {
          "x": 1253,
          "y": 382
        },
        {
          "x": 1747,
          "y": 382
        },
        {
          "x": 1747,
          "y": 694
        },
        {
          "x": 1253,
          "y": 694
        }
      ],
      "category": "figure",
      "html": "<br><figure><img id='304' alt=\"\" data-coord=\"top-left:(1253,382); bottom-right:(1747,694)\" /></figure>",
      "id": 304,
      "page": 30,
      "text": ""
    },
    {
      "bounding_box": [
        {
          "x": 1249,
          "y": 709
        },
        {
          "x": 1755,
          "y": 709
        },
        {
          "x": 1755,
          "y": 837
        },
        {
          "x": 1249,
          "y": 837
        }
      ],
      "category": "caption",
      "html": "<caption id='305' style='font-size:18px'>Example of 'Pixelation of a small mask' : this mask has an<br>imperfect boundary, since it extends beyond the object at<br>the black arrow. However, the 'blocky' pattern of the mask<br>is not an error, since, when zoomed in this much, the image<br>is also blocky the same way.</caption>",
      "id": 305,
      "page": 30,
      "text": "Example of 'Pixelation of a small mask' : this mask has an\nimperfect boundary, since it extends beyond the object at\nthe black arrow. However, the 'blocky' pattern of the mask\nis not an error, since, when zoomed in this much, the image\nis also blocky the same way."
    },
    {
      "bounding_box": [
        {
          "x": 1777,
          "y": 383
        },
        {
          "x": 2271,
          "y": 383
        },
        {
          "x": 2271,
          "y": 693
        },
        {
          "x": 1777,
          "y": 693
        }
      ],
      "category": "figure",
      "html": "<br><figure><img id='306' alt=\"\" data-coord=\"top-left:(1777,383); bottom-right:(2271,693)\" /></figure>",
      "id": 306,
      "page": 30,
      "text": ""
    },
    {
      "bounding_box": [
        {
          "x": 1771,
          "y": 710
        },
        {
          "x": 2277,
          "y": 710
        },
        {
          "x": 2277,
          "y": 785
        },
        {
          "x": 1771,
          "y": 785
        }
      ],
      "category": "caption",
      "html": "<caption id='307' style='font-size:18px'>Example error for consistency with the provided point: The<br>mask does not agree with the blue point, so this is a mask<br>error.</caption>",
      "id": 307,
      "page": 30,
      "text": "Example error for consistency with the provided point: The\nmask does not agree with the blue point, so this is a mask\nerror."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 884
        },
        {
          "x": 704,
          "y": 884
        },
        {
          "x": 704,
          "y": 1199
        },
        {
          "x": 201,
          "y": 1199
        }
      ],
      "category": "figure",
      "html": "<figure><img id='308' alt=\"\" data-coord=\"top-left:(201,884); bottom-right:(704,1199)\" /></figure>",
      "id": 308,
      "page": 30,
      "text": ""
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 1212
        },
        {
          "x": 709,
          "y": 1212
        },
        {
          "x": 709,
          "y": 1313
        },
        {
          "x": 204,
          "y": 1313
        }
      ],
      "category": "caption",
      "html": "<caption id='309' style='font-size:18px'>Example for consistency with the provided point: For this<br>input point, but the logo (left) and the container (right) are<br>valid objects, since the blue point lies on both of them. Nei-<br>ther mask has a mask error.</caption>",
      "id": 309,
      "page": 30,
      "text": "Example for consistency with the provided point: For this\ninput point, but the logo (left) and the container (right) are\nvalid objects, since the blue point lies on both of them. Nei-\nther mask has a mask error."
    },
    {
      "bounding_box": [
        {
          "x": 730,
          "y": 883
        },
        {
          "x": 1225,
          "y": 883
        },
        {
          "x": 1225,
          "y": 1197
        },
        {
          "x": 730,
          "y": 1197
        }
      ],
      "category": "figure",
      "html": "<br><figure><img id='310' style='font-size:22px' alt=\"A\" data-coord=\"top-left:(730,883); bottom-right:(1225,1197)\" /></figure>",
      "id": 310,
      "page": 30,
      "text": "A"
    },
    {
      "bounding_box": [
        {
          "x": 726,
          "y": 1210
        },
        {
          "x": 1229,
          "y": 1210
        },
        {
          "x": 1229,
          "y": 1289
        },
        {
          "x": 726,
          "y": 1289
        }
      ],
      "category": "caption",
      "html": "<caption id='311' style='font-size:18px'>Example for consistency with a box: The box surrounds the<br>bowl of oranges, but the mask is only of a single orange.<br>This is a mask error.</caption>",
      "id": 311,
      "page": 30,
      "text": "Example for consistency with a box: The box surrounds the\nbowl of oranges, but the mask is only of a single orange.\nThis is a mask error."
    },
    {
      "bounding_box": [
        {
          "x": 1251,
          "y": 883
        },
        {
          "x": 1750,
          "y": 883
        },
        {
          "x": 1750,
          "y": 1196
        },
        {
          "x": 1251,
          "y": 1196
        }
      ],
      "category": "figure",
      "html": "<br><figure><img id='312' alt=\"\" data-coord=\"top-left:(1251,883); bottom-right:(1750,1196)\" /></figure>",
      "id": 312,
      "page": 30,
      "text": ""
    },
    {
      "bounding_box": [
        {
          "x": 1249,
          "y": 1211
        },
        {
          "x": 1755,
          "y": 1211
        },
        {
          "x": 1755,
          "y": 1289
        },
        {
          "x": 1249,
          "y": 1289
        }
      ],
      "category": "caption",
      "html": "<caption id='313' style='font-size:18px'>Example for consistency with a box: The box's shape fits<br>the zebra. Even though the mask extends slightly outside<br>the box to include the zebra 's left leg, this is not an error.</caption>",
      "id": 313,
      "page": 30,
      "text": "Example for consistency with a box: The box's shape fits\nthe zebra. Even though the mask extends slightly outside\nthe box to include the zebra 's left leg, this is not an error."
    },
    {
      "bounding_box": [
        {
          "x": 1779,
          "y": 976
        },
        {
          "x": 1786,
          "y": 976
        },
        {
          "x": 1786,
          "y": 984
        },
        {
          "x": 1779,
          "y": 984
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='314' style='font-size:14px'>·</p>",
      "id": 314,
      "page": 30,
      "text": "·"
    },
    {
      "bounding_box": [
        {
          "x": 1779,
          "y": 1011
        },
        {
          "x": 1786,
          "y": 1011
        },
        {
          "x": 1786,
          "y": 1019
        },
        {
          "x": 1779,
          "y": 1019
        }
      ],
      "category": "paragraph",
      "html": "<p id='315' style='font-size:14px'>·</p>",
      "id": 315,
      "page": 30,
      "text": "·"
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 1360
        },
        {
          "x": 703,
          "y": 1360
        },
        {
          "x": 703,
          "y": 1672
        },
        {
          "x": 203,
          "y": 1672
        }
      ],
      "category": "figure",
      "html": "<figure><img id='316' alt=\"\" data-coord=\"top-left:(203,1360); bottom-right:(703,1672)\" /></figure>",
      "id": 316,
      "page": 30,
      "text": ""
    },
    {
      "bounding_box": [
        {
          "x": 1779,
          "y": 1060
        },
        {
          "x": 1786,
          "y": 1060
        },
        {
          "x": 1786,
          "y": 1068
        },
        {
          "x": 1779,
          "y": 1068
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='317' style='font-size:14px'>·</p>",
      "id": 317,
      "page": 30,
      "text": "·"
    },
    {
      "bounding_box": [
        {
          "x": 1783,
          "y": 909
        },
        {
          "x": 2275,
          "y": 909
        },
        {
          "x": 2275,
          "y": 1185
        },
        {
          "x": 1783,
          "y": 1185
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='318' style='font-size:16px'>Overall mask quality is subjective. each of the above errors may hurt mask qualityonly a little or a<br>lot, depending on how large the error is. Please use your best judgment when choosing mask scores,<br>and try to stay consistent from mask-to-mask. Here are some general guidelines for what different<br>scores should correspond to:<br>A score of 1: It is not possible to tell what object this mask corresponds to. This includes the<br>case that there is no mask visible at all.<br>A low score (2-4): The object is mostly identifiable, but the mask quality is extremely poor<br>(e.g. large regions of the mask cover other objects: large regions of the object missing: ex-<br>tremely splotchy mask boundaries that cut through the middle of the object).<br>A mid score (5-6): The object is identifiable and the boundary is mostly correct, but there<br>are major errors (missing a significant disconnected part of the object: containing a significant<br>part of another object: very poor boundary quality in one area of the object but not the entire<br>object).<br>A high score (7-9): The object is identifiable and errors are small and rare (missing a small,<br>heavily obscured disconnected component, having small regions where the mask boundary<br>does not quite match the object boundary).<br>A score of 10: The mask is pixel-perfect: it has no identifiable errors at all.</p>",
      "id": 318,
      "page": 30,
      "text": "Overall mask quality is subjective. each of the above errors may hurt mask qualityonly a little or a\nlot, depending on how large the error is. Please use your best judgment when choosing mask scores,\nand try to stay consistent from mask-to-mask. Here are some general guidelines for what different\nscores should correspond to:\nA score of 1: It is not possible to tell what object this mask corresponds to. This includes the\ncase that there is no mask visible at all.\nA low score (2-4): The object is mostly identifiable, but the mask quality is extremely poor\n(e.g. large regions of the mask cover other objects: large regions of the object missing: ex-\ntremely splotchy mask boundaries that cut through the middle of the object).\nA mid score (5-6): The object is identifiable and the boundary is mostly correct, but there\nare major errors (missing a significant disconnected part of the object: containing a significant\npart of another object: very poor boundary quality in one area of the object but not the entire\nobject).\nA high score (7-9): The object is identifiable and errors are small and rare (missing a small,\nheavily obscured disconnected component, having small regions where the mask boundary\ndoes not quite match the object boundary).\nA score of 10: The mask is pixel-perfect: it has no identifiable errors at all."
    },
    {
      "bounding_box": [
        {
          "x": 1778,
          "y": 1123
        },
        {
          "x": 1786,
          "y": 1123
        },
        {
          "x": 1786,
          "y": 1132
        },
        {
          "x": 1778,
          "y": 1132
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='319' style='font-size:14px'>·</p>",
      "id": 319,
      "page": 30,
      "text": "·"
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 1688
        },
        {
          "x": 707,
          "y": 1688
        },
        {
          "x": 707,
          "y": 1741
        },
        {
          "x": 203,
          "y": 1741
        }
      ],
      "category": "caption",
      "html": "<caption id='320' style='font-size:18px'>Example of a mask with a score of 1: It is not clear what<br>object this mask corresponds to.</caption>",
      "id": 320,
      "page": 30,
      "text": "Example of a mask with a score of 1: It is not clear what\nobject this mask corresponds to."
    },
    {
      "bounding_box": [
        {
          "x": 728,
          "y": 1361
        },
        {
          "x": 1225,
          "y": 1361
        },
        {
          "x": 1225,
          "y": 1671
        },
        {
          "x": 728,
          "y": 1671
        }
      ],
      "category": "figure",
      "html": "<br><figure><img id='321' alt=\"\" data-coord=\"top-left:(728,1361); bottom-right:(1225,1671)\" /></figure>",
      "id": 321,
      "page": 30,
      "text": ""
    },
    {
      "bounding_box": [
        {
          "x": 1779,
          "y": 1172
        },
        {
          "x": 1786,
          "y": 1172
        },
        {
          "x": 1786,
          "y": 1180
        },
        {
          "x": 1779,
          "y": 1180
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='322' style='font-size:14px'>·</p>",
      "id": 322,
      "page": 30,
      "text": "·"
    },
    {
      "bounding_box": [
        {
          "x": 727,
          "y": 1689
        },
        {
          "x": 1230,
          "y": 1689
        },
        {
          "x": 1230,
          "y": 1764
        },
        {
          "x": 727,
          "y": 1764
        }
      ],
      "category": "caption",
      "html": "<caption id='323' style='font-size:18px'>Example of a mask with a low score (2-4): The main ob-<br>ject is identifiable, but the mask includes a large, incorrect<br>portion of another object.</caption>",
      "id": 323,
      "page": 30,
      "text": "Example of a mask with a low score (2-4): The main ob-\nject is identifiable, but the mask includes a large, incorrect\nportion of another object."
    },
    {
      "bounding_box": [
        {
          "x": 1960,
          "y": 1212
        },
        {
          "x": 2085,
          "y": 1212
        },
        {
          "x": 2085,
          "y": 1237
        },
        {
          "x": 1960,
          "y": 1237
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='324' style='font-size:20px'>Mask Scoring</p>",
      "id": 324,
      "page": 30,
      "text": "Mask Scoring"
    },
    {
      "bounding_box": [
        {
          "x": 1251,
          "y": 1361
        },
        {
          "x": 1747,
          "y": 1361
        },
        {
          "x": 1747,
          "y": 1670
        },
        {
          "x": 1251,
          "y": 1670
        }
      ],
      "category": "figure",
      "html": "<figure><img id='325' alt=\"\" data-coord=\"top-left:(1251,1361); bottom-right:(1747,1670)\" /></figure>",
      "id": 325,
      "page": 30,
      "text": ""
    },
    {
      "bounding_box": [
        {
          "x": 1249,
          "y": 1687
        },
        {
          "x": 1752,
          "y": 1687
        },
        {
          "x": 1752,
          "y": 1764
        },
        {
          "x": 1249,
          "y": 1764
        }
      ],
      "category": "caption",
      "html": "<caption id='326' style='font-size:18px'>Example of a mask with a low score (2-4): The main ob-<br>ject is identifiable, but a large, random part of the object is<br>missing.</caption>",
      "id": 326,
      "page": 30,
      "text": "Example of a mask with a low score (2-4): The main ob-\nject is identifiable, but a large, random part of the object is\nmissing."
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 1814
        },
        {
          "x": 703,
          "y": 1814
        },
        {
          "x": 703,
          "y": 2124
        },
        {
          "x": 203,
          "y": 2124
        }
      ],
      "category": "figure",
      "html": "<figure><img id='327' alt=\"\" data-coord=\"top-left:(203,1814); bottom-right:(703,2124)\" /></figure>",
      "id": 327,
      "page": 30,
      "text": ""
    },
    {
      "bounding_box": [
        {
          "x": 1777,
          "y": 1367
        },
        {
          "x": 2273,
          "y": 1367
        },
        {
          "x": 2273,
          "y": 1671
        },
        {
          "x": 1777,
          "y": 1671
        }
      ],
      "category": "figure",
      "html": "<br><figure><img id='328' alt=\"\" data-coord=\"top-left:(1777,1367); bottom-right:(2273,1671)\" /></figure>",
      "id": 328,
      "page": 30,
      "text": ""
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 2138
        },
        {
          "x": 709,
          "y": 2138
        },
        {
          "x": 709,
          "y": 2217
        },
        {
          "x": 203,
          "y": 2217
        }
      ],
      "category": "caption",
      "html": "<caption id='329' style='font-size:18px'>Example of a mask with a medium score (5-6): The mask<br>clearly corresponds to the plate, but the boundary with the<br>waffle is quite poor.</caption>",
      "id": 329,
      "page": 30,
      "text": "Example of a mask with a medium score (5-6): The mask\nclearly corresponds to the plate, but the boundary with the\nwaffle is quite poor."
    },
    {
      "bounding_box": [
        {
          "x": 1770,
          "y": 1687
        },
        {
          "x": 2276,
          "y": 1687
        },
        {
          "x": 2276,
          "y": 1764
        },
        {
          "x": 1770,
          "y": 1764
        }
      ],
      "category": "caption",
      "html": "<br><caption id='330' style='font-size:18px'>Example of a mask with a low-to-medium score (4-5): The<br>object is identifiable and the edges are all correct, but the<br>mask incorrectly includes the hand of the person on the left.</caption>",
      "id": 330,
      "page": 30,
      "text": "Example of a mask with a low-to-medium score (4-5): The\nobject is identifiable and the edges are all correct, but the\nmask incorrectly includes the hand of the person on the left."
    },
    {
      "bounding_box": [
        {
          "x": 741,
          "y": 1810
        },
        {
          "x": 1225,
          "y": 1810
        },
        {
          "x": 1225,
          "y": 2125
        },
        {
          "x": 741,
          "y": 2125
        }
      ],
      "category": "figure",
      "html": "<figure><img id='331' alt=\"\" data-coord=\"top-left:(741,1810); bottom-right:(1225,2125)\" /></figure>",
      "id": 331,
      "page": 30,
      "text": ""
    },
    {
      "bounding_box": [
        {
          "x": 725,
          "y": 2139
        },
        {
          "x": 1229,
          "y": 2139
        },
        {
          "x": 1229,
          "y": 2266
        },
        {
          "x": 725,
          "y": 2266
        }
      ],
      "category": "caption",
      "html": "<caption id='332' style='font-size:18px'>Example of a mask with a medium score (5-6): the object<br>is easy to identify, and most of the edges make sense. How-<br>ever, there is a significant disconnected part (their arm inside<br>the frame) that is mostly missing, as well as splotchy pixels<br>in this region.</caption>",
      "id": 332,
      "page": 30,
      "text": "Example of a mask with a medium score (5-6): the object\nis easy to identify, and most of the edges make sense. How-\never, there is a significant disconnected part (their arm inside\nthe frame) that is mostly missing, as well as splotchy pixels\nin this region."
    },
    {
      "bounding_box": [
        {
          "x": 1250,
          "y": 1812
        },
        {
          "x": 1746,
          "y": 1812
        },
        {
          "x": 1746,
          "y": 2124
        },
        {
          "x": 1250,
          "y": 2124
        }
      ],
      "category": "figure",
      "html": "<br><figure><img id='333' alt=\"\" data-coord=\"top-left:(1250,1812); bottom-right:(1746,2124)\" /></figure>",
      "id": 333,
      "page": 30,
      "text": ""
    },
    {
      "bounding_box": [
        {
          "x": 1249,
          "y": 2138
        },
        {
          "x": 1755,
          "y": 2138
        },
        {
          "x": 1755,
          "y": 2217
        },
        {
          "x": 1249,
          "y": 2217
        }
      ],
      "category": "caption",
      "html": "<caption id='334' style='font-size:18px'>Example of a mask with a medium-to-high score (6-8): the<br>mask has two small-ish regions of poor boundary, at the top<br>of the mask and on the bottom right.</caption>",
      "id": 334,
      "page": 30,
      "text": "Example of a mask with a medium-to-high score (6-8): the\nmask has two small-ish regions of poor boundary, at the top\nof the mask and on the bottom right."
    },
    {
      "bounding_box": [
        {
          "x": 1779,
          "y": 1812
        },
        {
          "x": 2272,
          "y": 1812
        },
        {
          "x": 2272,
          "y": 2123
        },
        {
          "x": 1779,
          "y": 2123
        }
      ],
      "category": "figure",
      "html": "<br><figure><img id='335' alt=\"\" data-coord=\"top-left:(1779,1812); bottom-right:(2272,2123)\" /></figure>",
      "id": 335,
      "page": 30,
      "text": ""
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 2316
        },
        {
          "x": 702,
          "y": 2316
        },
        {
          "x": 702,
          "y": 2625
        },
        {
          "x": 203,
          "y": 2625
        }
      ],
      "category": "figure",
      "html": "<figure><img id='336' alt=\"\" data-coord=\"top-left:(203,2316); bottom-right:(702,2625)\" /></figure>",
      "id": 336,
      "page": 30,
      "text": ""
    },
    {
      "bounding_box": [
        {
          "x": 718,
          "y": 2312
        },
        {
          "x": 1223,
          "y": 2312
        },
        {
          "x": 1223,
          "y": 2626
        },
        {
          "x": 718,
          "y": 2626
        }
      ],
      "category": "figure",
      "html": "<br><figure><img id='337' alt=\"\" data-coord=\"top-left:(718,2312); bottom-right:(1223,2626)\" /></figure>",
      "id": 337,
      "page": 30,
      "text": ""
    },
    {
      "bounding_box": [
        {
          "x": 1771,
          "y": 2138
        },
        {
          "x": 2277,
          "y": 2138
        },
        {
          "x": 2277,
          "y": 2242
        },
        {
          "x": 1771,
          "y": 2242
        }
      ],
      "category": "caption",
      "html": "<br><caption id='338' style='font-size:16px'>Example of a mask with a medium-to-high score (6-8): The<br>wreath is a valid object that is the size of the box (the entire<br>wreath + clock would also be a valid object). However, there<br>are incorrect stray mask pixels on the clock.</caption>",
      "id": 338,
      "page": 30,
      "text": "Example of a mask with a medium-to-high score (6-8): The\nwreath is a valid object that is the size of the box (the entire\nwreath + clock would also be a valid object). However, there\nare incorrect stray mask pixels on the clock."
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 2641
        },
        {
          "x": 707,
          "y": 2641
        },
        {
          "x": 707,
          "y": 2741
        },
        {
          "x": 204,
          "y": 2741
        }
      ],
      "category": "paragraph",
      "html": "<p id='339' style='font-size:18px'>Example of a mask with a high score (7-9): The boundary of<br>the horse is almost entirely correct, except for the right side<br>of its back leg. The mask consistently includes all of the<br>equipment that horse is wearing, and has logical boundaries.</p>",
      "id": 339,
      "page": 30,
      "text": "Example of a mask with a high score (7-9): The boundary of\nthe horse is almost entirely correct, except for the right side\nof its back leg. The mask consistently includes all of the\nequipment that horse is wearing, and has logical boundaries."
    },
    {
      "bounding_box": [
        {
          "x": 727,
          "y": 2642
        },
        {
          "x": 1230,
          "y": 2642
        },
        {
          "x": 1230,
          "y": 2741
        },
        {
          "x": 727,
          "y": 2741
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='340' style='font-size:18px'>Example of a mask with a very high score (~9): There are<br>only minor errors around the edge of the mask. The blocky<br>'pixelation' is not an error, since the image is also blocky at<br>this scale.</p>",
      "id": 340,
      "page": 30,
      "text": "Example of a mask with a very high score (~9): There are\nonly minor errors around the edge of the mask. The blocky\n'pixelation' is not an error, since the image is also blocky at\nthis scale."
    },
    {
      "bounding_box": [
        {
          "x": 1250,
          "y": 2642
        },
        {
          "x": 1752,
          "y": 2642
        },
        {
          "x": 1752,
          "y": 2692
        },
        {
          "x": 1250,
          "y": 2692
        }
      ],
      "category": "caption",
      "html": "<br><caption id='341' style='font-size:18px'>Example of a mask with a very high score (9-10): the mask<br>has only very minor errors in the edge on the bottom right.</caption>",
      "id": 341,
      "page": 30,
      "text": "Example of a mask with a very high score (9-10): the mask\nhas only very minor errors in the edge on the bottom right."
    },
    {
      "bounding_box": [
        {
          "x": 1251,
          "y": 2313
        },
        {
          "x": 1749,
          "y": 2313
        },
        {
          "x": 1749,
          "y": 2626
        },
        {
          "x": 1251,
          "y": 2626
        }
      ],
      "category": "figure",
      "html": "<br><figure><img id='342' alt=\"\" data-coord=\"top-left:(1251,2313); bottom-right:(1749,2626)\" /></figure>",
      "id": 342,
      "page": 30,
      "text": ""
    },
    {
      "bounding_box": [
        {
          "x": 1778,
          "y": 2313
        },
        {
          "x": 2273,
          "y": 2313
        },
        {
          "x": 2273,
          "y": 2627
        },
        {
          "x": 1778,
          "y": 2627
        }
      ],
      "category": "figure",
      "html": "<br><figure><img id='343' alt=\"\" data-coord=\"top-left:(1778,2313); bottom-right:(2273,2627)\" /></figure>",
      "id": 343,
      "page": 30,
      "text": ""
    },
    {
      "bounding_box": [
        {
          "x": 1772,
          "y": 2642
        },
        {
          "x": 2275,
          "y": 2642
        },
        {
          "x": 2275,
          "y": 2692
        },
        {
          "x": 1772,
          "y": 2692
        }
      ],
      "category": "caption",
      "html": "<caption id='344' style='font-size:18px'>Example of a mask with a very high score (9-10): There are<br>only minor errors around the edge of the mask.</caption>",
      "id": 344,
      "page": 30,
      "text": "Example of a mask with a very high score (9-10): There are\nonly minor errors around the edge of the mask."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 2773
        },
        {
          "x": 2274,
          "y": 2773
        },
        {
          "x": 2274,
          "y": 2875
        },
        {
          "x": 201,
          "y": 2875
        }
      ],
      "category": "paragraph",
      "html": "<p id='345' style='font-size:20px'>Figure 20: Here we provide the complete guidelines given to annotations for the human review of mask quality. Some images<br>been edited slightly and faces have been blurred to enable release. Best viewed with zoom (part 2 of 2).</p>",
      "id": 345,
      "page": 30,
      "text": "Figure 20: Here we provide the complete guidelines given to annotations for the human review of mask quality. Some images\nbeen edited slightly and faces have been blurred to enable release. Best viewed with zoom (part 2 of 2)."
    },
    {
      "bounding_box": [
        {
          "x": 1220,
          "y": 3056
        },
        {
          "x": 1261,
          "y": 3056
        },
        {
          "x": 1261,
          "y": 3089
        },
        {
          "x": 1220,
          "y": 3089
        }
      ],
      "category": "footer",
      "html": "<footer id='346' style='font-size:20px'>30</footer>",
      "id": 346,
      "page": 30,
      "text": "30"
    }
  ]
}