{
    "id": "32b942c4-0f94-11ef-8828-426932df3dcf",
    "pdf_path": "./pdf/AI_VIT_X/1711.09020v3.pdf",
    "elements": [
        {
            "bounding_box": [
                {
                    "x": 563,
                    "y": 436
                },
                {
                    "x": 1917,
                    "y": 436
                },
                {
                    "x": 1917,
                    "y": 578
                },
                {
                    "x": 563,
                    "y": 578
                }
            ],
            "category": "paragraph",
            "html": "<p id='0' style='font-size:22px'>StarGAN: Unified Generative Adversarial Networks<br>for Multi-Domain Image-to-Image Translation</p>",
            "id": 0,
            "page": 1,
            "text": "StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation"
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 671
                },
                {
                    "x": 2258,
                    "y": 671
                },
                {
                    "x": 2258,
                    "y": 883
                },
                {
                    "x": 225,
                    "y": 883
                }
            ],
            "category": "paragraph",
            "html": "<p id='1' style='font-size:20px'>Yunjey Choi1,2 Minje Choi1,2 Munyoung Kim2,3 Jung-Woo Ha2 Sunghun Kim2,4 Jaegul Ch001,2<br>1 Korea University 2 Clova AI Research, NAVER Corp.<br>3 The College of New Jersey 4 Hong Kong University of Science & Technology</p>",
            "id": 1,
            "page": 1,
            "text": "Yunjey Choi1,2 Minje Choi1,2 Munyoung Kim2,3 Jung-Woo Ha2 Sunghun Kim2,4 Jaegul Ch001,2 1 Korea University 2 Clova AI Research, NAVER Corp. 3 The College of New Jersey 4 Hong Kong University of Science & Technology"
        },
        {
            "bounding_box": [
                {
                    "x": 209,
                    "y": 960
                },
                {
                    "x": 2269,
                    "y": 960
                },
                {
                    "x": 2269,
                    "y": 1894
                },
                {
                    "x": 209,
                    "y": 1894
                }
            ],
            "category": "figure",
            "html": "<figure><img id='2' style='font-size:14px' alt=\"Input Blond hair Gender Aged Pale skin Input Angry Happy Fearful\n.LLA STORY\nLLA STORY\nLLA STORY\nLLA STORY\nLLA STORY\n- =\nRELLA S RELLA S RELLA S RELLA S RELLA S\nU UI U\nE\nO 0 0 0\" data-coord=\"top-left:(209,960); bottom-right:(2269,1894)\" /></figure>",
            "id": 2,
            "page": 1,
            "text": "Input Blond hair Gender Aged Pale skin Input Angry Happy Fearful .LLA STORY LLA STORY LLA STORY LLA STORY LLA STORY - = RELLA S RELLA S RELLA S RELLA S RELLA S U UI U E O 0 0 0"
        },
        {
            "bounding_box": [
                {
                    "x": 205,
                    "y": 1907
                },
                {
                    "x": 2276,
                    "y": 1907
                },
                {
                    "x": 2276,
                    "y": 2043
                },
                {
                    "x": 205,
                    "y": 2043
                }
            ],
            "category": "caption",
            "html": "<br><caption id='3' style='font-size:16px'>Figure 1. Multi-domain image-to-image translation results on the CelebA dataset via transferring knowledge learned from the RaFD dataset.<br>The first and sixth columns show input images while the remaining columns are images generated by StarGAN. Note that the images are<br>generated by a single generator network, and facial expression labels such as angry, happy, and fearful are from RaFD, not CelebA.</caption>",
            "id": 3,
            "page": 1,
            "text": "Figure 1. Multi-domain image-to-image translation results on the CelebA dataset via transferring knowledge learned from the RaFD dataset. The first and sixth columns show input images while the remaining columns are images generated by StarGAN. Note that the images are generated by a single generator network, and facial expression labels such as angry, happy, and fearful are from RaFD, not CelebA."
        },
        {
            "bounding_box": [
                {
                    "x": 601,
                    "y": 2084
                },
                {
                    "x": 799,
                    "y": 2084
                },
                {
                    "x": 799,
                    "y": 2137
                },
                {
                    "x": 601,
                    "y": 2137
                }
            ],
            "category": "paragraph",
            "html": "<p id='4' style='font-size:22px'>Abstract</p>",
            "id": 4,
            "page": 1,
            "text": "Abstract"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2179
                },
                {
                    "x": 1199,
                    "y": 2179
                },
                {
                    "x": 1199,
                    "y": 2979
                },
                {
                    "x": 201,
                    "y": 2979
                }
            ],
            "category": "paragraph",
            "html": "<p id='5' style='font-size:18px'>Recent studies have shown remarkable success in image-<br>to-image translation for two domains. However, existing<br>approaches have limited scalability and robustness in han-<br>dling more than two domains, since different models should<br>be built independently for every pair of image domains. To<br>address this limitation, we propose StarGAN, a novel and<br>scalable approach that can perform image-to-image trans-<br>lations for multiple domains using only a single model.<br>Such a unified model architecture ofStarGAN allows simul-<br>taneous training of multiple datasets with different domains<br>within a single network. This leads to StarGAN's superior<br>quality of translated images compared to existing models as<br>well as the novel capability offlexibly translating an input<br>image to any desired target domain. We empirically demon-<br>strate the effectiveness of our approach on a facial attribute<br>transfer and a facial expression synthesis tasks.</p>",
            "id": 5,
            "page": 1,
            "text": "Recent studies have shown remarkable success in imageto-image translation for two domains. However, existing approaches have limited scalability and robustness in handling more than two domains, since different models should be built independently for every pair of image domains. To address this limitation, we propose StarGAN, a novel and scalable approach that can perform image-to-image translations for multiple domains using only a single model. Such a unified model architecture ofStarGAN allows simultaneous training of multiple datasets with different domains within a single network. This leads to StarGAN's superior quality of translated images compared to existing models as well as the novel capability offlexibly translating an input image to any desired target domain. We empirically demonstrate the effectiveness of our approach on a facial attribute transfer and a facial expression synthesis tasks."
        },
        {
            "bounding_box": [
                {
                    "x": 1282,
                    "y": 2082
                },
                {
                    "x": 1611,
                    "y": 2082
                },
                {
                    "x": 1611,
                    "y": 2133
                },
                {
                    "x": 1282,
                    "y": 2133
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='6' style='font-size:20px'>1. Introduction</p>",
            "id": 6,
            "page": 1,
            "text": "1. Introduction"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 2174
                },
                {
                    "x": 2276,
                    "y": 2174
                },
                {
                    "x": 2276,
                    "y": 2570
                },
                {
                    "x": 1279,
                    "y": 2570
                }
            ],
            "category": "paragraph",
            "html": "<p id='7' style='font-size:18px'>The task of image-to-image translation is to change a<br>particular aspect of a given image to another, e.g., changing<br>the facial expression of a person from smiling to frowning<br>(see Fig. 1). This task has experienced significant improve-<br>ments following the introduction of generative adversarial<br>networks (GANs), with results ranging from changing hair<br>color [9], reconstructing photos from edge maps [7], and<br>changing the seasons of scenery images [33].</p>",
            "id": 7,
            "page": 1,
            "text": "The task of image-to-image translation is to change a particular aspect of a given image to another, e.g., changing the facial expression of a person from smiling to frowning (see Fig. 1). This task has experienced significant improvements following the introduction of generative adversarial networks (GANs), with results ranging from changing hair color , reconstructing photos from edge maps , and changing the seasons of scenery images ."
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 2580
                },
                {
                    "x": 2276,
                    "y": 2580
                },
                {
                    "x": 2276,
                    "y": 2976
                },
                {
                    "x": 1279,
                    "y": 2976
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='8' style='font-size:18px'>Given training data from two different domains, these<br>models learn to translate images from one domain to the<br>other. We denote the terms attribute as a meaningful fea-<br>ture inherent in an image such as hair color, gender or age,<br>and attribute value as a particular value of an attribute, e.g.,<br>black/blond/brown for hair color or male/female for gender.<br>We further denote domain as a set of images sharing the<br>same attribute value. For example, images of women can</p>",
            "id": 8,
            "page": 1,
            "text": "Given training data from two different domains, these models learn to translate images from one domain to the other. We denote the terms attribute as a meaningful feature inherent in an image such as hair color, gender or age, and attribute value as a particular value of an attribute, e.g., black/blond/brown for hair color or male/female for gender. We further denote domain as a set of images sharing the same attribute value. For example, images of women can"
        },
        {
            "bounding_box": [
                {
                    "x": 65,
                    "y": 863
                },
                {
                    "x": 149,
                    "y": 863
                },
                {
                    "x": 149,
                    "y": 2338
                },
                {
                    "x": 65,
                    "y": 2338
                }
            ],
            "category": "footer",
            "html": "<br><footer id='9' style='font-size:14px'>2018<br>Sep<br>21<br>[cs.CV]<br>arXiv:1711.09020v3</footer>",
            "id": 9,
            "page": 1,
            "text": "2018 Sep 21 [cs.CV] arXiv:1711.09020v3"
        },
        {
            "bounding_box": [
                {
                    "x": 1225,
                    "y": 3057
                },
                {
                    "x": 1249,
                    "y": 3057
                },
                {
                    "x": 1249,
                    "y": 3090
                },
                {
                    "x": 1225,
                    "y": 3090
                }
            ],
            "category": "footer",
            "html": "<footer id='10' style='font-size:16px'>1</footer>",
            "id": 10,
            "page": 1,
            "text": "1"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 311
                },
                {
                    "x": 1195,
                    "y": 311
                },
                {
                    "x": 1195,
                    "y": 351
                },
                {
                    "x": 202,
                    "y": 351
                }
            ],
            "category": "paragraph",
            "html": "<p id='11' style='font-size:14px'>represent one domain while those of men represent another.</p>",
            "id": 11,
            "page": 2,
            "text": "represent one domain while those of men represent another."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 370
                },
                {
                    "x": 1199,
                    "y": 370
                },
                {
                    "x": 1199,
                    "y": 1106
                },
                {
                    "x": 200,
                    "y": 1106
                }
            ],
            "category": "paragraph",
            "html": "<p id='12' style='font-size:18px'>Several image datasets come with a number of labeled<br>attributes. For instance, the CelebA[19] dataset contains 40<br>labels related to facial attributes such as hair color, gender,<br>and age, and the RaFD [13] dataset has 8 labels for facial<br>expressions such as 'happy' , 'angry' and 'sad' These set-<br>tings enable us to perform more interesting tasks, namely<br>multi-domain image-to-image translation, where we change<br>images according to attributes from multiple domains. The<br>first five columns in Fig. 1 show how a CelebA image can<br>be translated according to any of the four domains, 'blond<br>hair' , 'gender', 'aged' , and 'pale skin' We can further ex-<br>tend to training multiple domains from different datasets,<br>such as jointly training CelebA and RaFD images to change<br>a CelebA image's facial expression using features learned<br>by training on RaFD, as in the rightmost columns of Fig. 1.</p>",
            "id": 12,
            "page": 2,
            "text": "Several image datasets come with a number of labeled attributes. For instance, the CelebA dataset contains 40 labels related to facial attributes such as hair color, gender, and age, and the RaFD  dataset has 8 labels for facial expressions such as 'happy' , 'angry' and 'sad' These settings enable us to perform more interesting tasks, namely multi-domain image-to-image translation, where we change images according to attributes from multiple domains. The first five columns in Fig. 1 show how a CelebA image can be translated according to any of the four domains, 'blond hair' , 'gender', 'aged' , and 'pale skin' We can further extend to training multiple domains from different datasets, such as jointly training CelebA and RaFD images to change a CelebA image's facial expression using features learned by training on RaFD, as in the rightmost columns of Fig. 1."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1122
                },
                {
                    "x": 1199,
                    "y": 1122
                },
                {
                    "x": 1199,
                    "y": 1865
                },
                {
                    "x": 201,
                    "y": 1865
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='13' style='font-size:20px'>However, existing models are both inefficient and inef-<br>fective in such multi-domain image translation tasks. Their<br>inefficiency results from the fact that in order to learn all<br>mappings among k domains, k(k-1) generators have to<br>be trained. Fig. 2 (a) illustrates how twelve distinct gener-<br>ator networks have to be trained to translate images among<br>four different domains. Meanwhile, they are ineffective that<br>even though there exist global features that can be learned<br>from images of all domains such as face shapes, each gen-<br>erator cannot fully utilize the entire training data and only<br>can learn from two domains out of k. Failure to fully uti-<br>lize training data is likely to limit the quality of generated<br>images. Furthermore, they are incapable of jointly train-<br>ing domains from different datasets because each dataset is<br>partially labeled, which we further discuss in Section 3.2.</p>",
            "id": 13,
            "page": 2,
            "text": "However, existing models are both inefficient and ineffective in such multi-domain image translation tasks. Their inefficiency results from the fact that in order to learn all mappings among k domains, k(k-1) generators have to be trained. Fig. 2 (a) illustrates how twelve distinct generator networks have to be trained to translate images among four different domains. Meanwhile, they are ineffective that even though there exist global features that can be learned from images of all domains such as face shapes, each generator cannot fully utilize the entire training data and only can learn from two domains out of k. Failure to fully utilize training data is likely to limit the quality of generated images. Furthermore, they are incapable of jointly training domains from different datasets because each dataset is partially labeled, which we further discuss in Section 3.2."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 1874
                },
                {
                    "x": 1199,
                    "y": 1874
                },
                {
                    "x": 1199,
                    "y": 2620
                },
                {
                    "x": 200,
                    "y": 2620
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='14' style='font-size:20px'>As a solution to such problems we propose StarGAN, a<br>novel and scalable approach capable of learning mappings<br>among multiple domains. As demonstrated in Fig. 2 (b), our<br>model takes in training data of multiple domains, and learns<br>the mappings between all available domains using only a<br>single generator. The idea is simple. Instead of learning<br>a fixed translation (e.g., black-to-blond hair), our generator<br>takes in as inputs both image and domain information, and<br>learns to flexibly translate the image into the correspond-<br>ing domain. We use a label (e.g., binary or one-hot vector)<br>to represent domain information. During training, we ran-<br>domly generate a target domain label and train the model to<br>flexibly translate an input image into the target domain. By<br>doing so, we can control the domain label and translate the<br>image into any desired domain at testing phase.</p>",
            "id": 14,
            "page": 2,
            "text": "As a solution to such problems we propose StarGAN, a novel and scalable approach capable of learning mappings among multiple domains. As demonstrated in Fig. 2 (b), our model takes in training data of multiple domains, and learns the mappings between all available domains using only a single generator. The idea is simple. Instead of learning a fixed translation (e.g., black-to-blond hair), our generator takes in as inputs both image and domain information, and learns to flexibly translate the image into the corresponding domain. We use a label (e.g., binary or one-hot vector) to represent domain information. During training, we randomly generate a target domain label and train the model to flexibly translate an input image into the target domain. By doing so, we can control the domain label and translate the image into any desired domain at testing phase."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2628
                },
                {
                    "x": 1199,
                    "y": 2628
                },
                {
                    "x": 1199,
                    "y": 2978
                },
                {
                    "x": 202,
                    "y": 2978
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='15' style='font-size:18px'>We also introduce a simple but effective approach that<br>enables joint training between domains of different datasets<br>by adding a mask vector to the domain label. Our proposed<br>method ensures that the model can ignore unknown labels<br>and focus on the label provided by a particular dataset. In<br>this manner, our model can perform well on tasks such<br>as synthesizing facial expressions of CelebA images us-</p>",
            "id": 15,
            "page": 2,
            "text": "We also introduce a simple but effective approach that enables joint training between domains of different datasets by adding a mask vector to the domain label. Our proposed method ensures that the model can ignore unknown labels and focus on the label provided by a particular dataset. In this manner, our model can perform well on tasks such as synthesizing facial expressions of CelebA images us-"
        },
        {
            "bounding_box": [
                {
                    "x": 1286,
                    "y": 303
                },
                {
                    "x": 2269,
                    "y": 303
                },
                {
                    "x": 2269,
                    "y": 857
                },
                {
                    "x": 1286,
                    "y": 857
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='16' style='font-size:16px' alt=\"(a) Cross-domain models (b) StarGAN\n1\n1 ← G21 G12 2\nG31 G42\nG41\nG32 5 2\nG24 G13\nG14 G23\n4 G34 G43 3\n4 3\" data-coord=\"top-left:(1286,303); bottom-right:(2269,857)\" /></figure>",
            "id": 16,
            "page": 2,
            "text": "(a) Cross-domain models (b) StarGAN 1 1 ← G21 G12 2 G31 G42 G41 G32 5 2 G24 G13 G14 G23 4 G34 G43 3 4 3"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 866
                },
                {
                    "x": 2277,
                    "y": 866
                },
                {
                    "x": 2277,
                    "y": 1142
                },
                {
                    "x": 1279,
                    "y": 1142
                }
            ],
            "category": "caption",
            "html": "<br><caption id='17' style='font-size:16px'>Figure 2. Comparison between cross-domain models and our pro-<br>posed model, StarGAN. (a) To handle multiple domains, cross-<br>domain models should be built for every pair of image domains.<br>(b) StarGAN is capable of learning mappings among multiple do-<br>mains using a single generator. The figure represents a star topol-<br>ogy connecting multi-domains.</caption>",
            "id": 17,
            "page": 2,
            "text": "Figure 2. Comparison between cross-domain models and our proposed model, StarGAN. (a) To handle multiple domains, crossdomain models should be built for every pair of image domains. (b) StarGAN is capable of learning mappings among multiple domains using a single generator. The figure represents a star topology connecting multi-domains."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1222
                },
                {
                    "x": 2275,
                    "y": 1222
                },
                {
                    "x": 2275,
                    "y": 1417
                },
                {
                    "x": 1280,
                    "y": 1417
                }
            ],
            "category": "paragraph",
            "html": "<p id='18' style='font-size:18px'>ing features learned from RaFD, as shown in the right-<br>most columns of Fig. 1. As far as our knowledge goes, our<br>work is the first to successfully perform multi-domain im-<br>age translation across different datasets.</p>",
            "id": 18,
            "page": 2,
            "text": "ing features learned from RaFD, as shown in the rightmost columns of Fig. 1. As far as our knowledge goes, our work is the first to successfully perform multi-domain image translation across different datasets."
        },
        {
            "bounding_box": [
                {
                    "x": 1332,
                    "y": 1438
                },
                {
                    "x": 2020,
                    "y": 1438
                },
                {
                    "x": 2020,
                    "y": 1483
                },
                {
                    "x": 1332,
                    "y": 1483
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='19' style='font-size:14px'>Overall, our contributions are as follows:</p>",
            "id": 19,
            "page": 2,
            "text": "Overall, our contributions are as follows:"
        },
        {
            "bounding_box": [
                {
                    "x": 1326,
                    "y": 1515
                },
                {
                    "x": 2277,
                    "y": 1515
                },
                {
                    "x": 2277,
                    "y": 2171
                },
                {
                    "x": 1326,
                    "y": 2171
                }
            ],
            "category": "paragraph",
            "html": "<p id='20' style='font-size:18px'>● We propose StarGAN, a novel generative adversarial<br>network that learns the mappings among multiple do-<br>mains using only a single generator and a discrimina-<br>tor, training effectively from images of all domains.<br>● We demonstrate how we can successfully learn multi-<br>domain image translation between multiple datasets by<br>utilizing a mask vector method that enables StarGAN<br>to control all available domain labels.<br>● We provide both qualitative and quantitative results on<br>facial attribute transfer and facial expression synthe-<br>sis tasks using StarGAN, showing its superiority over<br>baseline models.</p>",
            "id": 20,
            "page": 2,
            "text": "● We propose StarGAN, a novel generative adversarial network that learns the mappings among multiple domains using only a single generator and a discriminator, training effectively from images of all domains. ● We demonstrate how we can successfully learn multidomain image translation between multiple datasets by utilizing a mask vector method that enables StarGAN to control all available domain labels. ● We provide both qualitative and quantitative results on facial attribute transfer and facial expression synthesis tasks using StarGAN, showing its superiority over baseline models."
        },
        {
            "bounding_box": [
                {
                    "x": 1282,
                    "y": 2222
                },
                {
                    "x": 1635,
                    "y": 2222
                },
                {
                    "x": 1635,
                    "y": 2272
                },
                {
                    "x": 1282,
                    "y": 2272
                }
            ],
            "category": "paragraph",
            "html": "<p id='21' style='font-size:22px'>2. Related Work</p>",
            "id": 21,
            "page": 2,
            "text": "2. Related Work"
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 2307
                },
                {
                    "x": 2276,
                    "y": 2307
                },
                {
                    "x": 2276,
                    "y": 2852
                },
                {
                    "x": 1278,
                    "y": 2852
                }
            ],
            "category": "paragraph",
            "html": "<p id='22' style='font-size:20px'>Generative Adversarial Networks. Generative adversar-<br>ial networks (GANs) [3] have shown remarkable results<br>in various computer vision tasks such as image generation<br>[6, 24, 32, 8], image translation [7, 9, 33], super-resolution<br>imaging [14], and face image synthesis [10, 16, 26, 31]. A<br>typical GAN model consists of two modules: a discrimina-<br>tor and a generator. The discriminator learns to distinguish<br>between real and fake samples, while the generator learns to<br>generate fake samples that are indistinguishable from real<br>samples. Our approach also leverages the adversarial loss<br>to make the generated images as realistic as possible.</p>",
            "id": 22,
            "page": 2,
            "text": "Generative Adversarial Networks. Generative adversarial networks (GANs)  have shown remarkable results in various computer vision tasks such as image generation , image translation , super-resolution imaging , and face image synthesis . A typical GAN model consists of two modules: a discriminator and a generator. The discriminator learns to distinguish between real and fake samples, while the generator learns to generate fake samples that are indistinguishable from real samples. Our approach also leverages the adversarial loss to make the generated images as realistic as possible."
        },
        {
            "bounding_box": [
                {
                    "x": 1282,
                    "y": 2877
                },
                {
                    "x": 2274,
                    "y": 2877
                },
                {
                    "x": 2274,
                    "y": 2978
                },
                {
                    "x": 1282,
                    "y": 2978
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='23' style='font-size:18px'>Conditional GANs. GAN-based conditional image gener-<br>ation has also been actively studied. Prior studies have pro-</p>",
            "id": 23,
            "page": 2,
            "text": "Conditional GANs. GAN-based conditional image generation has also been actively studied. Prior studies have pro-"
        },
        {
            "bounding_box": [
                {
                    "x": 1223,
                    "y": 3054
                },
                {
                    "x": 1252,
                    "y": 3054
                },
                {
                    "x": 1252,
                    "y": 3092
                },
                {
                    "x": 1223,
                    "y": 3092
                }
            ],
            "category": "footer",
            "html": "<footer id='24' style='font-size:18px'>2</footer>",
            "id": 24,
            "page": 2,
            "text": "2"
        },
        {
            "bounding_box": [
                {
                    "x": 219,
                    "y": 303
                },
                {
                    "x": 2261,
                    "y": 303
                },
                {
                    "x": 2261,
                    "y": 1112
                },
                {
                    "x": 219,
                    "y": 1112
                }
            ],
            "category": "figure",
            "html": "<figure><img id='25' style='font-size:14px' alt=\"(a) Training the discriminator (b) Original-to-target domain (c) Target-to-original domain (d) Fooling the discriminator\nDepth-wise concatenation\nOriginal\nReal image Fake image Fake image Fake image Fake image\ndomain\n↑ ↓ ↓\n(1) (2)\nG G\n(1), (2) (1) ↑ ↓\nDomain Reconstructed Domain\nReal / Fake\nReal / Fake Target domain Input image image classification\nclassification\nDepth-wise concatenation\" data-coord=\"top-left:(219,303); bottom-right:(2261,1112)\" /></figure>",
            "id": 25,
            "page": 3,
            "text": "(a) Training the discriminator (b) Original-to-target domain (c) Target-to-original domain (d) Fooling the discriminator Depth-wise concatenation Original Real image Fake image Fake image Fake image Fake image domain ↑ ↓ ↓ (1) (2) G G (1), (2) (1) ↑ ↓ Domain Reconstructed Domain Real / Fake Real / Fake Target domain Input image image classification classification Depth-wise concatenation"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1141
                },
                {
                    "x": 2279,
                    "y": 1141
                },
                {
                    "x": 2279,
                    "y": 1372
                },
                {
                    "x": 201,
                    "y": 1372
                }
            ],
            "category": "caption",
            "html": "<caption id='26' style='font-size:14px'>Figure 3. Overview of StarGAN, consisting of two modules, a discriminator D and a generator G. (a) D learns to distinguish between<br>real and fake images and classify the real images to its corresponding domain. (b) G takes in as input both the image and target domain<br>label and generates an fake image. The target domain label is spatially replicated and concatenated with the input image. (c) G tries to<br>reconstruct the original image from the fake image given the original domain label. (d) G tries to generate images indistinguishable from<br>real images and classifiable as target domain by D.</caption>",
            "id": 26,
            "page": 3,
            "text": "Figure 3. Overview of StarGAN, consisting of two modules, a discriminator D and a generator G. (a) D learns to distinguish between real and fake images and classify the real images to its corresponding domain. (b) G takes in as input both the image and target domain label and generates an fake image. The target domain label is spatially replicated and concatenated with the input image. (c) G tries to reconstruct the original image from the fake image given the original domain label. (d) G tries to generate images indistinguishable from real images and classifiable as target domain by D."
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 1456
                },
                {
                    "x": 1199,
                    "y": 1456
                },
                {
                    "x": 1199,
                    "y": 1953
                },
                {
                    "x": 203,
                    "y": 1953
                }
            ],
            "category": "paragraph",
            "html": "<p id='27' style='font-size:16px'>vided both the discriminator and generator with class infor-<br>mation in order to generate samples conditioned on the class<br>[20, 21, 22]. Other recent approaches focused on generating<br>particular images highly relevant to a given text description<br>[25, 30]. The idea of conditional image generation has also<br>been successfully applied to domain transfer [9, 28], super-<br>resolution imaging[14], and photo editing [2, 27]. In this<br>paper, we propose a scalable GAN framework that can flex-<br>ibly steer the image translation to various target domains,<br>by providing conditional domain information.</p>",
            "id": 27,
            "page": 3,
            "text": "vided both the discriminator and generator with class information in order to generate samples conditioned on the class . Other recent approaches focused on generating particular images highly relevant to a given text description . The idea of conditional image generation has also been successfully applied to domain transfer , superresolution imaging, and photo editing . In this paper, we propose a scalable GAN framework that can flexibly steer the image translation to various target domains, by providing conditional domain information."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 1978
                },
                {
                    "x": 1199,
                    "y": 1978
                },
                {
                    "x": 1199,
                    "y": 2978
                },
                {
                    "x": 200,
                    "y": 2978
                }
            ],
            "category": "paragraph",
            "html": "<p id='28' style='font-size:16px'>Image-to-Image Translation. Recent work have achieved<br>impressive results in image-to-image translation [7, 9, 17,<br>33]. For instance, pix2pix [7] learns this task in a super-<br>vised manner using cGANs[20]. It combines an adver-<br>sarial loss with a L1 loss, thus requires paired data sam-<br>ples. To alleviate the problem of obtaining data pairs, un-<br>paired image-to-image translation frameworks [9, 17, 33]<br>have been proposed. UNIT [17] combines variational au-<br>toencoders (VAEs) [12] with CoGAN [18], a GAN frame-<br>work where two generators share weights to learn the joint<br>distribution of images in cross domains. CycleGAN [33]<br>and DiscoGAN [9] preserve key attributes between the in-<br>put and the translated image by utilizing a cycle consistency<br>loss. However, all these frameworks are only capable of<br>learning the relations between two different domains at a<br>time. Their approaches have limited scalability in handling<br>multiple domains since different models should be trained<br>for each pair of domains. Unlike the aforementioned ap-<br>proaches, our framework can learn the relations among mul-<br>tiple domains using only a single model.</p>",
            "id": 28,
            "page": 3,
            "text": "Image-to-Image Translation. Recent work have achieved impressive results in image-to-image translation . For instance, pix2pix  learns this task in a supervised manner using cGANs. It combines an adversarial loss with a L1 loss, thus requires paired data samples. To alleviate the problem of obtaining data pairs, unpaired image-to-image translation frameworks  have been proposed. UNIT  combines variational autoencoders (VAEs)  with CoGAN , a GAN framework where two generators share weights to learn the joint distribution of images in cross domains. CycleGAN  and DiscoGAN  preserve key attributes between the input and the translated image by utilizing a cycle consistency loss. However, all these frameworks are only capable of learning the relations between two different domains at a time. Their approaches have limited scalability in handling multiple domains since different models should be trained for each pair of domains. Unlike the aforementioned approaches, our framework can learn the relations among multiple domains using only a single model."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1450
                },
                {
                    "x": 2160,
                    "y": 1450
                },
                {
                    "x": 2160,
                    "y": 1501
                },
                {
                    "x": 1280,
                    "y": 1501
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='29' style='font-size:22px'>3. Star Generative Adversarial Networks</p>",
            "id": 29,
            "page": 3,
            "text": "3. Star Generative Adversarial Networks"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 1535
                },
                {
                    "x": 2277,
                    "y": 1535
                },
                {
                    "x": 2277,
                    "y": 1781
                },
                {
                    "x": 1279,
                    "y": 1781
                }
            ],
            "category": "paragraph",
            "html": "<p id='30' style='font-size:16px'>We first describe our proposed StarGAN, a framework to<br>address multi-domain image-to-image translation within a<br>single dataset. Then, we discuss how StarGAN incorporates<br>multiple datasets containing different label sets to flexibly<br>perform image translations using any of these labels.</p>",
            "id": 30,
            "page": 3,
            "text": "We first describe our proposed StarGAN, a framework to address multi-domain image-to-image translation within a single dataset. Then, we discuss how StarGAN incorporates multiple datasets containing different label sets to flexibly perform image translations using any of these labels."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 1812
                },
                {
                    "x": 2207,
                    "y": 1812
                },
                {
                    "x": 2207,
                    "y": 1859
                },
                {
                    "x": 1281,
                    "y": 1859
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='31' style='font-size:20px'>3.1. Multi-Domain Image-to-Image Translation</p>",
            "id": 31,
            "page": 3,
            "text": "3.1. Multi-Domain Image-to-Image Translation"
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 1889
                },
                {
                    "x": 2276,
                    "y": 1889
                },
                {
                    "x": 2276,
                    "y": 2435
                },
                {
                    "x": 1278,
                    "y": 2435
                }
            ],
            "category": "paragraph",
            "html": "<p id='32' style='font-size:14px'>Our goal is to train a single generator G that learns map-<br>pings among multiple domains. To achieve this, we train G<br>to translate an input image x into an output image y condi-<br>tioned on the target domain label c, G(x, c) → y. We ran-<br>domly generate the target domain label c SO that G learns<br>to flexibly translate the input image. We also introduce an<br>auxiliary classifier [22] that allows a single discriminator to<br>control multiple domains. That is, our discriminator pro-<br>duces probability distributions over both sources and do-<br>main labels, D : x → {Dsrc(x), Dcls (x)}. Fig. 3 illustrates<br>the training process of our proposed approach.</p>",
            "id": 32,
            "page": 3,
            "text": "Our goal is to train a single generator G that learns mappings among multiple domains. To achieve this, we train G to translate an input image x into an output image y conditioned on the target domain label c, G(x, c) → y. We randomly generate the target domain label c SO that G learns to flexibly translate the input image. We also introduce an auxiliary classifier  that allows a single discriminator to control multiple domains. That is, our discriminator produces probability distributions over both sources and domain labels, D : x → {Dsrc(x), Dcls (x)}. Fig. 3 illustrates the training process of our proposed approach."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 2457
                },
                {
                    "x": 2273,
                    "y": 2457
                },
                {
                    "x": 2273,
                    "y": 2553
                },
                {
                    "x": 1281,
                    "y": 2553
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='33' style='font-size:14px'>Adversarial Loss. To make the generated images indistin-<br>guishable from real images, we adopt an adversarial loss</p>",
            "id": 33,
            "page": 3,
            "text": "Adversarial Loss. To make the generated images indistinguishable from real images, we adopt an adversarial loss"
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2727
                },
                {
                    "x": 2277,
                    "y": 2727
                },
                {
                    "x": 2277,
                    "y": 2977
                },
                {
                    "x": 1280,
                    "y": 2977
                }
            ],
            "category": "paragraph",
            "html": "<p id='34' style='font-size:14px'>where G generates an image G(x, c) conditioned on both<br>the input image x and the target domain label c, while D<br>tries to distinguish between real and fake images. In this<br>paper, we refer to the term Dsrc(x) as a probability distri-<br>bution over sources given by D. The generator G tries to</p>",
            "id": 34,
            "page": 3,
            "text": "where G generates an image G(x, c) conditioned on both the input image x and the target domain label c, while D tries to distinguish between real and fake images. In this paper, we refer to the term Dsrc(x) as a probability distribution over sources given by D. The generator G tries to"
        },
        {
            "bounding_box": [
                {
                    "x": 1224,
                    "y": 3054
                },
                {
                    "x": 1252,
                    "y": 3054
                },
                {
                    "x": 1252,
                    "y": 3092
                },
                {
                    "x": 1224,
                    "y": 3092
                }
            ],
            "category": "footer",
            "html": "<footer id='35' style='font-size:14px'>3</footer>",
            "id": 35,
            "page": 3,
            "text": "3"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 311
                },
                {
                    "x": 1196,
                    "y": 311
                },
                {
                    "x": 1196,
                    "y": 400
                },
                {
                    "x": 202,
                    "y": 400
                }
            ],
            "category": "paragraph",
            "html": "<p id='36' style='font-size:14px'>minimize this objective, while the discriminator D tries to<br>maximize it.</p>",
            "id": 36,
            "page": 4,
            "text": "minimize this objective, while the discriminator D tries to maximize it."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 427
                },
                {
                    "x": 1200,
                    "y": 427
                },
                {
                    "x": 1200,
                    "y": 920
                },
                {
                    "x": 201,
                    "y": 920
                }
            ],
            "category": "paragraph",
            "html": "<p id='37' style='font-size:16px'>Domain Classification Loss. For a given input image x<br>and a target domain label c, our goal is to translate x into<br>an output image y, which is properly classified to the target<br>domain c. To achieve this condition, we add an auxiliary<br>classifier on top of D and impose the domain classification<br>loss when optimizing both D and G. Thatis, we decompose<br>the objective into two terms: a domain classification loss of<br>real images used to optimize D, and a domain classification<br>loss of fake images used to optimize G. In detail, the former<br>is defined as</p>",
            "id": 37,
            "page": 4,
            "text": "Domain Classification Loss. For a given input image x and a target domain label c, our goal is to translate x into an output image y, which is properly classified to the target domain c. To achieve this condition, we add an auxiliary classifier on top of D and impose the domain classification loss when optimizing both D and G. Thatis, we decompose the objective into two terms: a domain classification loss of real images used to optimize D, and a domain classification loss of fake images used to optimize G. In detail, the former is defined as"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1030
                },
                {
                    "x": 1199,
                    "y": 1030
                },
                {
                    "x": 1199,
                    "y": 1374
                },
                {
                    "x": 201,
                    "y": 1374
                }
            ],
            "category": "paragraph",
            "html": "<p id='38' style='font-size:16px'>where the term Dcls (c'|x) represents a probability distribu-<br>tion over domain labels computed by D. By minimizing<br>this objective, D learns to classify a real image x to its cor-<br>responding original domain c'. We assume that the input<br>image and domain label pair (x, c') is given by the training<br>data. On the other hand, the loss function for the domain<br>classification of fake images is defined as</p>",
            "id": 38,
            "page": 4,
            "text": "where the term Dcls (c'|x) represents a probability distribution over domain labels computed by D. By minimizing this objective, D learns to classify a real image x to its corresponding original domain c'. We assume that the input image and domain label pair (x, c') is given by the training data. On the other hand, the loss function for the domain classification of fake images is defined as"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 1484
                },
                {
                    "x": 1196,
                    "y": 1484
                },
                {
                    "x": 1196,
                    "y": 1578
                },
                {
                    "x": 202,
                    "y": 1578
                }
            ],
            "category": "paragraph",
            "html": "<p id='39' style='font-size:14px'>In other words, G tries to minimize this objective to gener-<br>ate images that can be classified as the target domain c.</p>",
            "id": 39,
            "page": 4,
            "text": "In other words, G tries to minimize this objective to generate images that can be classified as the target domain c."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 1602
                },
                {
                    "x": 1199,
                    "y": 1602
                },
                {
                    "x": 1199,
                    "y": 1998
                },
                {
                    "x": 202,
                    "y": 1998
                }
            ],
            "category": "paragraph",
            "html": "<p id='40' style='font-size:16px'>Reconstruction Loss. By minimizing the adversarial and<br>classification losses, G is trained to generate images that<br>are realistic and classified to its correct target domain. How-<br>ever, minimizing the losses (Eqs. (1) and (3)) does not guar-<br>antee that translated images preserve the content of its input<br>images while changing only the domain-related part of the<br>inputs. To alleviate this problem, we apply a cycle consis-<br>tency loss [9, 33] to the generator, defined as</p>",
            "id": 40,
            "page": 4,
            "text": "Reconstruction Loss. By minimizing the adversarial and classification losses, G is trained to generate images that are realistic and classified to its correct target domain. However, minimizing the losses (Eqs. (1) and (3)) does not guarantee that translated images preserve the content of its input images while changing only the domain-related part of the inputs. To alleviate this problem, we apply a cycle consistency loss  to the generator, defined as"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2108
                },
                {
                    "x": 1198,
                    "y": 2108
                },
                {
                    "x": 1198,
                    "y": 2450
                },
                {
                    "x": 202,
                    "y": 2450
                }
            ],
            "category": "paragraph",
            "html": "<p id='41' style='font-size:16px'>where G takes in the translated image G(x, c) and the origi-<br>nal domain label c' as input and tries to reconstruct the orig-<br>inal image x. We adopt the L1 norm as our reconstruction<br>loss. Note that we use a single generator twice, first to trans-<br>late an original image into an image in the target domain<br>and then to reconstruct the original image from the trans-<br>lated image.</p>",
            "id": 41,
            "page": 4,
            "text": "where G takes in the translated image G(x, c) and the original domain label c' as input and tries to reconstruct the original image x. We adopt the L1 norm as our reconstruction loss. Note that we use a single generator twice, first to translate an original image into an image in the target domain and then to reconstruct the original image from the translated image."
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 2475
                },
                {
                    "x": 1196,
                    "y": 2475
                },
                {
                    "x": 1196,
                    "y": 2569
                },
                {
                    "x": 204,
                    "y": 2569
                }
            ],
            "category": "paragraph",
            "html": "<p id='42' style='font-size:16px'>Full Objective. Finally, the objective functions to optimize<br>G and D are written, respectively, as</p>",
            "id": 42,
            "page": 4,
            "text": "Full Objective. Finally, the objective functions to optimize G and D are written, respectively, as"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2752
                },
                {
                    "x": 1197,
                    "y": 2752
                },
                {
                    "x": 1197,
                    "y": 2946
                },
                {
                    "x": 202,
                    "y": 2946
                }
            ],
            "category": "paragraph",
            "html": "<p id='43' style='font-size:16px'>where 入cls and 入rec are hyper-parameters that control the<br>relative importance of domain classification and reconstruc-<br>tion losses, respectively, compared to the adversarial loss.<br>We use 入cls = 1 and 入rec = 10 in all of our experiments.</p>",
            "id": 43,
            "page": 4,
            "text": "where 入cls and 入rec are hyper-parameters that control the relative importance of domain classification and reconstruction losses, respectively, compared to the adversarial loss. We use 入cls = 1 and 入rec = 10 in all of our experiments."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 305
                },
                {
                    "x": 1991,
                    "y": 305
                },
                {
                    "x": 1991,
                    "y": 352
                },
                {
                    "x": 1281,
                    "y": 352
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='44' style='font-size:20px'>3.2. Training with Multiple Datasets</p>",
            "id": 44,
            "page": 4,
            "text": "3.2. Training with Multiple Datasets"
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 395
                },
                {
                    "x": 2277,
                    "y": 395
                },
                {
                    "x": 2277,
                    "y": 1041
                },
                {
                    "x": 1278,
                    "y": 1041
                }
            ],
            "category": "paragraph",
            "html": "<p id='45' style='font-size:16px'>An important advantage of StarGAN is that it simulta-<br>neously incorporates multiple datasets containing different<br>types of labels, SO that StarGAN can control all the labels<br>at the test phase. An issue when learning from multiple<br>datasets, however, is that the label information is only par-<br>tially known to each dataset. In the case of CelebA [19] and<br>RaFD [13], while the former contains labels for attributes<br>such as hair color and gender, it does not have any labels<br>for facial expressions such as 'happy' and 'angry' , and vice<br>versa for the latter. This is problematic because the com-<br>plete information on the label vector c' is required when<br>reconstructing the input image x from the translated image<br>G(x, c) (See Eq. (4)).</p>",
            "id": 45,
            "page": 4,
            "text": "An important advantage of StarGAN is that it simultaneously incorporates multiple datasets containing different types of labels, SO that StarGAN can control all the labels at the test phase. An issue when learning from multiple datasets, however, is that the label information is only partially known to each dataset. In the case of CelebA  and RaFD , while the former contains labels for attributes such as hair color and gender, it does not have any labels for facial expressions such as 'happy' and 'angry' , and vice versa for the latter. This is problematic because the complete information on the label vector c' is required when reconstructing the input image x from the translated image G(x, c) (See Eq. (4))."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1108
                },
                {
                    "x": 2276,
                    "y": 1108
                },
                {
                    "x": 2276,
                    "y": 1454
                },
                {
                    "x": 1280,
                    "y": 1454
                }
            ],
            "category": "paragraph",
            "html": "<p id='46' style='font-size:14px'>Mask Vector. To alleviate this problem, we introduce a<br>mask vector m that allows StarGAN to ignore unspecified<br>labels and focus on the explicitly known label provided by<br>a particular dataset. In StarGAN, we use an n-dimensional<br>one-hot vector to represent m, with n being the number of<br>datasets. In addition, we define a unified version of the label<br>as a vector</p>",
            "id": 46,
            "page": 4,
            "text": "Mask Vector. To alleviate this problem, we introduce a mask vector m that allows StarGAN to ignore unspecified labels and focus on the explicitly known label provided by a particular dataset. In StarGAN, we use an n-dimensional one-hot vector to represent m, with n being the number of datasets. In addition, we define a unified version of the label as a vector"
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1615
                },
                {
                    "x": 2277,
                    "y": 1615
                },
                {
                    "x": 2277,
                    "y": 1963
                },
                {
                    "x": 1280,
                    "y": 1963
                }
            ],
            "category": "paragraph",
            "html": "<p id='47' style='font-size:14px'>where [·] refers to concatenation, and Ci represents a vector<br>for the labels of the i-th dataset. The vector of the known<br>label Ci can be represented as either a binary vector for bi-<br>nary attributes or a one-hot vector for categorical attributes.<br>For the remaining n-1 unknown labels we simply assign<br>zero values. In our experiments, we utilize the CelebA and<br>RaFD datasets, where n is two.</p>",
            "id": 47,
            "page": 4,
            "text": "where [·] refers to concatenation, and Ci represents a vector for the labels of the i-th dataset. The vector of the known label Ci can be represented as either a binary vector for binary attributes or a one-hot vector for categorical attributes. For the remaining n-1 unknown labels we simply assign zero values. In our experiments, we utilize the CelebA and RaFD datasets, where n is two."
        },
        {
            "bounding_box": [
                {
                    "x": 1277,
                    "y": 2030
                },
                {
                    "x": 2277,
                    "y": 2030
                },
                {
                    "x": 2277,
                    "y": 2975
                },
                {
                    "x": 1277,
                    "y": 2975
                }
            ],
            "category": "paragraph",
            "html": "<p id='48' style='font-size:16px'>Training Strategy. When training StarGAN with multiple<br>datasets, we use the domain label c defined in Eq. (7) as in-<br>put to the generator. By doing SO, the generator learns to<br>ignore the unspecified labels, which are zero vectors, and<br>focus on the explicitly given label. The structure of the gen-<br>erator is exactly the same as in training with a single dataset,<br>except for the dimension of the input label c. On the other<br>hand, we extend the auxiliary classifier of the discrimina-<br>tor to generate probability distributions over labels for all<br>datasets. Then, we train the model in a multi-task learning<br>setting, where the discriminator tries to minimize only the<br>classification error associated to the known label. For ex-<br>ample, when training with images in CelebA, the discrimi-<br>nator minimizes only classification errors for labels related<br>to CelebA attributes, and not facial expressions related to<br>RaFD. Under these settings, by alternating between CelebA<br>and RaFD the discriminator learns all of the discriminative<br>features for both datasets, and the generator learns to con-<br>trol all the labels in both datasets.</p>",
            "id": 48,
            "page": 4,
            "text": "Training Strategy. When training StarGAN with multiple datasets, we use the domain label c defined in Eq. (7) as input to the generator. By doing SO, the generator learns to ignore the unspecified labels, which are zero vectors, and focus on the explicitly given label. The structure of the generator is exactly the same as in training with a single dataset, except for the dimension of the input label c. On the other hand, we extend the auxiliary classifier of the discriminator to generate probability distributions over labels for all datasets. Then, we train the model in a multi-task learning setting, where the discriminator tries to minimize only the classification error associated to the known label. For example, when training with images in CelebA, the discriminator minimizes only classification errors for labels related to CelebA attributes, and not facial expressions related to RaFD. Under these settings, by alternating between CelebA and RaFD the discriminator learns all of the discriminative features for both datasets, and the generator learns to control all the labels in both datasets."
        },
        {
            "bounding_box": [
                {
                    "x": 1225,
                    "y": 3055
                },
                {
                    "x": 1250,
                    "y": 3055
                },
                {
                    "x": 1250,
                    "y": 3089
                },
                {
                    "x": 1225,
                    "y": 3089
                }
            ],
            "category": "footer",
            "html": "<footer id='49' style='font-size:14px'>4</footer>",
            "id": 49,
            "page": 4,
            "text": "4"
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 285
                },
                {
                    "x": 2273,
                    "y": 285
                },
                {
                    "x": 2273,
                    "y": 1193
                },
                {
                    "x": 204,
                    "y": 1193
                }
            ],
            "category": "figure",
            "html": "<figure><img id='50' style='font-size:14px' alt=\"Input Black hair Blond hair Gender Aged H+G H+A G+A H+G+A\nDIAT\nCycleGAN\nIcGAN\nStarGAN\" data-coord=\"top-left:(204,285); bottom-right:(2273,1193)\" /></figure>",
            "id": 50,
            "page": 5,
            "text": "Input Black hair Blond hair Gender Aged H+G H+A G+A H+G+A DIAT CycleGAN IcGAN StarGAN"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1205
                },
                {
                    "x": 2274,
                    "y": 1205
                },
                {
                    "x": 2274,
                    "y": 1297
                },
                {
                    "x": 201,
                    "y": 1297
                }
            ],
            "category": "caption",
            "html": "<br><caption id='51' style='font-size:14px'>Figure 4. Facial attribute transfer results on the CelebA dataset. The first column shows the input image, next four columns show the single<br>attribute transfer results, and rightmost columns show the multi-attribute transfer results. H: Hair color, G: Gender, A: Aged.</caption>",
            "id": 51,
            "page": 5,
            "text": "Figure 4. Facial attribute transfer results on the CelebA dataset. The first column shows the input image, next four columns show the single attribute transfer results, and rightmost columns show the multi-attribute transfer results. H: Hair color, G: Gender, A: Aged."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 1375
                },
                {
                    "x": 602,
                    "y": 1375
                },
                {
                    "x": 602,
                    "y": 1429
                },
                {
                    "x": 202,
                    "y": 1429
                }
            ],
            "category": "paragraph",
            "html": "<p id='52' style='font-size:20px'>4. Implementation</p>",
            "id": 52,
            "page": 5,
            "text": "4. Implementation"
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 1460
                },
                {
                    "x": 1199,
                    "y": 1460
                },
                {
                    "x": 1199,
                    "y": 1656
                },
                {
                    "x": 203,
                    "y": 1656
                }
            ],
            "category": "paragraph",
            "html": "<p id='53' style='font-size:18px'>Improved GAN Training. To stabilize the training process<br>and generate higher quality images, we replace Eq. (1) with<br>Wasserstein GAN objective with gradient penalty [1, 4] de-<br>fined as</p>",
            "id": 53,
            "page": 5,
            "text": "Improved GAN Training. To stabilize the training process and generate higher quality images, we replace Eq. (1) with Wasserstein GAN objective with gradient penalty  defined as"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1837
                },
                {
                    "x": 1197,
                    "y": 1837
                },
                {
                    "x": 1197,
                    "y": 1979
                },
                {
                    "x": 201,
                    "y": 1979
                }
            ],
            "category": "paragraph",
            "html": "<p id='54' style='font-size:16px'>where x is sampled uniformly along a straight line between<br>a pair of a real and a generated images. We use 入gp = 10<br>for all experiments.</p>",
            "id": 54,
            "page": 5,
            "text": "where x is sampled uniformly along a straight line between a pair of a real and a generated images. We use 入gp = 10 for all experiments."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2004
                },
                {
                    "x": 1199,
                    "y": 2004
                },
                {
                    "x": 1199,
                    "y": 2552
                },
                {
                    "x": 201,
                    "y": 2552
                }
            ],
            "category": "paragraph",
            "html": "<p id='55' style='font-size:18px'>Network Architecture. Adapted from CycleGAN [33],<br>StarGAN has the generator network composed of two con-<br>volutional layers with the stride size of two for downsam-<br>pling, six residual blocks [5], and two transposed convolu-<br>tional layers with the stride size of two for upsampling. We<br>use instance normalization [29] for the generator but no nor-<br>malization for the discriminator. We leverage PatchGANs<br>[7, 15, 33] for the discriminator network, which classifies<br>whether local image patches are real or fake. See the ap-<br>pendix (Section 7.2) for more details about the network ar-<br>chitecture.</p>",
            "id": 55,
            "page": 5,
            "text": "Network Architecture. Adapted from CycleGAN , StarGAN has the generator network composed of two convolutional layers with the stride size of two for downsampling, six residual blocks , and two transposed convolutional layers with the stride size of two for upsampling. We use instance normalization  for the generator but no normalization for the discriminator. We leverage PatchGANs  for the discriminator network, which classifies whether local image patches are real or fake. See the appendix (Section 7.2) for more details about the network architecture."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2597
                },
                {
                    "x": 533,
                    "y": 2597
                },
                {
                    "x": 533,
                    "y": 2646
                },
                {
                    "x": 202,
                    "y": 2646
                }
            ],
            "category": "paragraph",
            "html": "<p id='56' style='font-size:22px'>5. Experiments</p>",
            "id": 56,
            "page": 5,
            "text": "5. Experiments"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2679
                },
                {
                    "x": 1199,
                    "y": 2679
                },
                {
                    "x": 1199,
                    "y": 2976
                },
                {
                    "x": 202,
                    "y": 2976
                }
            ],
            "category": "paragraph",
            "html": "<p id='57' style='font-size:16px'>In this section, we first compare StarGAN against recent<br>methods on facial attribute transfer by conducting user stud-<br>ies. Next, we perform a classification experiment on fa-<br>cial expression synthesis. Lastly, we demonstrate empirical<br>results that StarGAN can learn image-to-image translation<br>from multiple datasets. All our experiments were conducted</p>",
            "id": 57,
            "page": 5,
            "text": "In this section, we first compare StarGAN against recent methods on facial attribute transfer by conducting user studies. Next, we perform a classification experiment on facial expression synthesis. Lastly, we demonstrate empirical results that StarGAN can learn image-to-image translation from multiple datasets. All our experiments were conducted"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 1383
                },
                {
                    "x": 2275,
                    "y": 1383
                },
                {
                    "x": 2275,
                    "y": 1479
                },
                {
                    "x": 1279,
                    "y": 1479
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='58' style='font-size:18px'>by using the model output from unseen images during the<br>training phase.</p>",
            "id": 58,
            "page": 5,
            "text": "by using the model output from unseen images during the training phase."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1510
                },
                {
                    "x": 1686,
                    "y": 1510
                },
                {
                    "x": 1686,
                    "y": 1557
                },
                {
                    "x": 1280,
                    "y": 1557
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='59' style='font-size:18px'>5.1. Baseline Models</p>",
            "id": 59,
            "page": 5,
            "text": "5.1. Baseline Models"
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1588
                },
                {
                    "x": 2278,
                    "y": 1588
                },
                {
                    "x": 2278,
                    "y": 1884
                },
                {
                    "x": 1280,
                    "y": 1884
                }
            ],
            "category": "paragraph",
            "html": "<p id='60' style='font-size:18px'>As our baseline models, we adopt DIAT [16] and Cycle-<br>GAN [33], both of which performs image-to-image trans-<br>lation between two different domains. For comparison, we<br>trained these models multiple times for every pair of two<br>different domains. We also adopt IcGAN [23] as a baseline<br>which can perform attribute transfer using a cGAN [22].</p>",
            "id": 60,
            "page": 5,
            "text": "As our baseline models, we adopt DIAT  and CycleGAN , both of which performs image-to-image translation between two different domains. For comparison, we trained these models multiple times for every pair of two different domains. We also adopt IcGAN  as a baseline which can perform attribute transfer using a cGAN ."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1909
                },
                {
                    "x": 2277,
                    "y": 1909
                },
                {
                    "x": 2277,
                    "y": 2207
                },
                {
                    "x": 1280,
                    "y": 2207
                }
            ],
            "category": "paragraph",
            "html": "<p id='61' style='font-size:16px'>DIAT uses an adversarial loss to learn the mapping from<br>x E X to y E Y, where x and y are face images in two<br>different domains X and Y, respectively. This method has<br>a regularization term on the mapping as IIx - F(G(x))lI1<br>to preserve identity features of the source image, where F<br>is a feature extractor pretrained on a face recognition task.</p>",
            "id": 61,
            "page": 5,
            "text": "DIAT uses an adversarial loss to learn the mapping from x E X to y E Y, where x and y are face images in two different domains X and Y, respectively. This method has a regularization term on the mapping as IIx - F(G(x))lI1 to preserve identity features of the source image, where F is a feature extractor pretrained on a face recognition task."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 2233
                },
                {
                    "x": 2277,
                    "y": 2233
                },
                {
                    "x": 2277,
                    "y": 2528
                },
                {
                    "x": 1281,
                    "y": 2528
                }
            ],
            "category": "paragraph",
            "html": "<p id='62' style='font-size:18px'>CycleGAN also uses an adversarial loss to learn the map-<br>ping between two different domains X and Y. This method<br>regularizes the mapping via cycle consistency losses,<br>I|x - (GYX(GXY(x)))||1 and lly - (Gxy(Gyx(y)))||1.<br>This method requires two generators and discriminators for<br>each pair of two different domains.</p>",
            "id": 62,
            "page": 5,
            "text": "CycleGAN also uses an adversarial loss to learn the mapping between two different domains X and Y. This method regularizes the mapping via cycle consistency losses, I|x - (GYX(GXY(x)))||1 and lly - (Gxy(Gyx(y)))||1. This method requires two generators and discriminators for each pair of two different domains."
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 2555
                },
                {
                    "x": 2278,
                    "y": 2555
                },
                {
                    "x": 2278,
                    "y": 2950
                },
                {
                    "x": 1278,
                    "y": 2950
                }
            ],
            "category": "paragraph",
            "html": "<p id='63' style='font-size:16px'>IcGAN combines an encoder with a cGAN [22] model.<br>cGAN learns the mapping G : {z,c} → x that generates<br>an image x conditioned on both the latent vector z and the<br>conditional vector c. In addition, IcGAN introduces an en-<br>coder to learn the inverse mappings of cGAN, Ez : x → z<br>and Ec : x → c. This allows IcGAN to synthesis images<br>by only changing the conditional vector and preserving the<br>latent vector.</p>",
            "id": 63,
            "page": 5,
            "text": "IcGAN combines an encoder with a cGAN  model. cGAN learns the mapping G : {z,c} → x that generates an image x conditioned on both the latent vector z and the conditional vector c. In addition, IcGAN introduces an encoder to learn the inverse mappings of cGAN, Ez : x → z and Ec : x → c. This allows IcGAN to synthesis images by only changing the conditional vector and preserving the latent vector."
        },
        {
            "bounding_box": [
                {
                    "x": 1225,
                    "y": 3055
                },
                {
                    "x": 1252,
                    "y": 3055
                },
                {
                    "x": 1252,
                    "y": 3092
                },
                {
                    "x": 1225,
                    "y": 3092
                }
            ],
            "category": "footer",
            "html": "<footer id='64' style='font-size:16px'>5</footer>",
            "id": 64,
            "page": 5,
            "text": "5"
        },
        {
            "bounding_box": [
                {
                    "x": 197,
                    "y": 282
                },
                {
                    "x": 2273,
                    "y": 282
                },
                {
                    "x": 2273,
                    "y": 1301
                },
                {
                    "x": 197,
                    "y": 1301
                }
            ],
            "category": "figure",
            "html": "<figure><img id='65' style='font-size:14px' alt=\"Input Angry Contemptuous Disgusted Fearful Happy Sad Surprised\nDIAT\nCycleGAN\nIcGAN\nStarGAN\" data-coord=\"top-left:(197,282); bottom-right:(2273,1301)\" /></figure>",
            "id": 65,
            "page": 6,
            "text": "Input Angry Contemptuous Disgusted Fearful Happy Sad Surprised DIAT CycleGAN IcGAN StarGAN"
        },
        {
            "bounding_box": [
                {
                    "x": 742,
                    "y": 1317
                },
                {
                    "x": 1731,
                    "y": 1317
                },
                {
                    "x": 1731,
                    "y": 1357
                },
                {
                    "x": 742,
                    "y": 1357
                }
            ],
            "category": "caption",
            "html": "<br><caption id='66' style='font-size:14px'>Figure 5. Facial expression synthesis results on the RaFD dataset.</caption>",
            "id": 66,
            "page": 6,
            "text": "Figure 5. Facial expression synthesis results on the RaFD dataset."
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 1412
                },
                {
                    "x": 457,
                    "y": 1412
                },
                {
                    "x": 457,
                    "y": 1458
                },
                {
                    "x": 203,
                    "y": 1458
                }
            ],
            "category": "paragraph",
            "html": "<p id='67' style='font-size:20px'>5.2. Datasets</p>",
            "id": 67,
            "page": 6,
            "text": "5.2. Datasets"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 1492
                },
                {
                    "x": 1198,
                    "y": 1492
                },
                {
                    "x": 1198,
                    "y": 1891
                },
                {
                    "x": 202,
                    "y": 1891
                }
            ],
            "category": "paragraph",
            "html": "<p id='68' style='font-size:18px'>CelebA. The CelebFaces Attributes (CelebA) dataset [19]<br>contains 202,599 face images of celebrities, each annotated<br>with 40 binary attributes. We crop the initial 178 x 218 size<br>images to 178 x 178, then resize them as 128 x 128. We ran-<br>domly select 2,000 images as test set and use all remaining<br>images for training data. We construct seven domains using<br>the following attributes: hair color (black, blond, brown),<br>gender (male/female), and age (young/old).</p>",
            "id": 68,
            "page": 6,
            "text": "CelebA. The CelebFaces Attributes (CelebA) dataset  contains 202,599 face images of celebrities, each annotated with 40 binary attributes. We crop the initial 178 x 218 size images to 178 x 178, then resize them as 128 x 128. We randomly select 2,000 images as test set and use all remaining images for training data. We construct seven domains using the following attributes: hair color (black, blond, brown), gender (male/female), and age (young/old)."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 1925
                },
                {
                    "x": 1199,
                    "y": 1925
                },
                {
                    "x": 1199,
                    "y": 2223
                },
                {
                    "x": 202,
                    "y": 2223
                }
            ],
            "category": "paragraph",
            "html": "<p id='69' style='font-size:16px'>RaFD. The Radboud Faces Database (RaFD) [13] consists<br>of 4,824 images collected from 67 participants. Each partic-<br>ipant makes eight facial expressions in three different gaze<br>directions, which are captured from three different angles.<br>We crop the images to 256 x 256, where the faces are cen-<br>tered, and then resize them to 128 x 128.</p>",
            "id": 69,
            "page": 6,
            "text": "RaFD. The Radboud Faces Database (RaFD)  consists of 4,824 images collected from 67 participants. Each participant makes eight facial expressions in three different gaze directions, which are captured from three different angles. We crop the images to 256 x 256, where the faces are centered, and then resize them to 128 x 128."
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 2298
                },
                {
                    "x": 461,
                    "y": 2298
                },
                {
                    "x": 461,
                    "y": 2347
                },
                {
                    "x": 203,
                    "y": 2347
                }
            ],
            "category": "paragraph",
            "html": "<p id='70' style='font-size:22px'>5.3. Training</p>",
            "id": 70,
            "page": 6,
            "text": "5.3. Training"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2378
                },
                {
                    "x": 1199,
                    "y": 2378
                },
                {
                    "x": 1199,
                    "y": 2977
                },
                {
                    "x": 201,
                    "y": 2977
                }
            ],
            "category": "paragraph",
            "html": "<p id='71' style='font-size:16px'>All models are trained using Adam [11] with B1 = 0.5<br>and B2 = 0.999. For data augmentation we flip the im-<br>ages horizontally with a probability of 0.5. We perform one<br>generator update after five discriminator updates as in [4].<br>The batch size is set to 16 for all experiments. For experi-<br>ments on CelebA, we train all models with a learning rate of<br>0.0001 for the first 10 epochs and linearly decay the learn-<br>ing rate to 0 over the next 10 epochs. To compensate for the<br>lack of data, when training with RaFD we train all models<br>for 100 epochs with a learning rate of 0.0001 and apply the<br>same decaying strategy over the next 100 epochs. Training<br>takes about one day on a single NVIDIA Tesla M40 GPU.</p>",
            "id": 71,
            "page": 6,
            "text": "All models are trained using Adam  with B1 = 0.5 and B2 = 0.999. For data augmentation we flip the images horizontally with a probability of 0.5. We perform one generator update after five discriminator updates as in . The batch size is set to 16 for all experiments. For experiments on CelebA, we train all models with a learning rate of 0.0001 for the first 10 epochs and linearly decay the learning rate to 0 over the next 10 epochs. To compensate for the lack of data, when training with RaFD we train all models for 100 epochs with a learning rate of 0.0001 and apply the same decaying strategy over the next 100 epochs. Training takes about one day on a single NVIDIA Tesla M40 GPU."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 1413
                },
                {
                    "x": 2002,
                    "y": 1413
                },
                {
                    "x": 2002,
                    "y": 1460
                },
                {
                    "x": 1281,
                    "y": 1460
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='72' style='font-size:20px'>5.4. Experimental Results on CelebA</p>",
            "id": 72,
            "page": 6,
            "text": "5.4. Experimental Results on CelebA"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 1490
                },
                {
                    "x": 2278,
                    "y": 1490
                },
                {
                    "x": 2278,
                    "y": 1836
                },
                {
                    "x": 1279,
                    "y": 1836
                }
            ],
            "category": "paragraph",
            "html": "<p id='73' style='font-size:16px'>We first compare our proposed method to the baseline<br>models on a single and multi-attribute transfer tasks. We<br>train the cross-domain models such as DIAT and Cycle-<br>GAN multiple times considering all possible attribute value<br>pairs. In the case of DIAT and CycleGAN, we perform<br>multi-step translations to synthesize multiple attributes (e.g.<br>transferring a gender attribute after changing a hair color).</p>",
            "id": 73,
            "page": 6,
            "text": "We first compare our proposed method to the baseline models on a single and multi-attribute transfer tasks. We train the cross-domain models such as DIAT and CycleGAN multiple times considering all possible attribute value pairs. In the case of DIAT and CycleGAN, we perform multi-step translations to synthesize multiple attributes (e.g. transferring a gender attribute after changing a hair color)."
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 1859
                },
                {
                    "x": 2276,
                    "y": 1859
                },
                {
                    "x": 2276,
                    "y": 2455
                },
                {
                    "x": 1278,
                    "y": 2455
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='74' style='font-size:16px'>Qualitative evaluation. Fig. 4 shows the facial attribute<br>transfer results on CelebA. We observed that our method<br>provides a higher visual quality of translation results on test<br>data compared to the cross-domain models. One possible<br>reason is the regularization effect of StarGAN through a<br>multi-task learning framework. In other words, rather than<br>training a model to perform a fixed translation (e.g., brown-<br>to-blond hair), which is prone to overfitting, we train our<br>model to flexibly translate images according to the labels<br>of the target domain. This allows our model to learn reli-<br>able features universally applicable to multiple domains of<br>images with different facial attribute values.</p>",
            "id": 74,
            "page": 6,
            "text": "Qualitative evaluation. Fig. 4 shows the facial attribute transfer results on CelebA. We observed that our method provides a higher visual quality of translation results on test data compared to the cross-domain models. One possible reason is the regularization effect of StarGAN through a multi-task learning framework. In other words, rather than training a model to perform a fixed translation (e.g., brownto-blond hair), which is prone to overfitting, we train our model to flexibly translate images according to the labels of the target domain. This allows our model to learn reliable features universally applicable to multiple domains of images with different facial attribute values."
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 2460
                },
                {
                    "x": 2277,
                    "y": 2460
                },
                {
                    "x": 2277,
                    "y": 2753
                },
                {
                    "x": 1279,
                    "y": 2753
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='75' style='font-size:16px'>Furthermore, compared to IcGAN, our model demon-<br>strates an advantage in preserving the facial identity feature<br>of an input. We conjecture that this is because our method<br>maintains the spatial information by using activation maps<br>from the convolutional layer as latent representation, rather<br>than just a low-dimensional latent vector as in IcGAN.</p>",
            "id": 75,
            "page": 6,
            "text": "Furthermore, compared to IcGAN, our model demonstrates an advantage in preserving the facial identity feature of an input. We conjecture that this is because our method maintains the spatial information by using activation maps from the convolutional layer as latent representation, rather than just a low-dimensional latent vector as in IcGAN."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2778
                },
                {
                    "x": 2276,
                    "y": 2778
                },
                {
                    "x": 2276,
                    "y": 2977
                },
                {
                    "x": 1280,
                    "y": 2977
                }
            ],
            "category": "paragraph",
            "html": "<p id='76' style='font-size:16px'>Quantitative evaluation protocol. For quantitative evalu-<br>ations, we performed two user studies in a survey format<br>using Amazon Mechanical Turk (AMT) to assess single<br>and multiple attribute transfer tasks. Given an input im-</p>",
            "id": 76,
            "page": 6,
            "text": "Quantitative evaluation protocol. For quantitative evaluations, we performed two user studies in a survey format using Amazon Mechanical Turk (AMT) to assess single and multiple attribute transfer tasks. Given an input im-"
        },
        {
            "bounding_box": [
                {
                    "x": 1224,
                    "y": 3057
                },
                {
                    "x": 1254,
                    "y": 3057
                },
                {
                    "x": 1254,
                    "y": 3092
                },
                {
                    "x": 1224,
                    "y": 3092
                }
            ],
            "category": "footer",
            "html": "<footer id='77' style='font-size:14px'>6</footer>",
            "id": 77,
            "page": 6,
            "text": "6"
        },
        {
            "bounding_box": [
                {
                    "x": 209,
                    "y": 299
                },
                {
                    "x": 2271,
                    "y": 299
                },
                {
                    "x": 2271,
                    "y": 878
                },
                {
                    "x": 209,
                    "y": 878
                }
            ],
            "category": "figure",
            "html": "<figure><img id='78' style='font-size:20px' alt=\"Input Angry Disgusted Fearful Happy Neutral Sad\nStarGAN\n(SNG)\nStarGAN\n(JNT)\" data-coord=\"top-left:(209,299); bottom-right:(2271,878)\" /></figure>",
            "id": 78,
            "page": 7,
            "text": "Input Angry Disgusted Fearful Happy Neutral Sad StarGAN (SNG) StarGAN (JNT)"
        },
        {
            "bounding_box": [
                {
                    "x": 475,
                    "y": 892
                },
                {
                    "x": 1999,
                    "y": 892
                },
                {
                    "x": 1999,
                    "y": 934
                },
                {
                    "x": 475,
                    "y": 934
                }
            ],
            "category": "caption",
            "html": "<br><caption id='79' style='font-size:16px'>Figure 6. Facial expression synthesis results of StarGAN-SNG and StarGAN-JNT on CelebA dataset.</caption>",
            "id": 79,
            "page": 7,
            "text": "Figure 6. Facial expression synthesis results of StarGAN-SNG and StarGAN-JNT on CelebA dataset."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 992
                },
                {
                    "x": 1199,
                    "y": 992
                },
                {
                    "x": 1199,
                    "y": 1638
                },
                {
                    "x": 200,
                    "y": 1638
                }
            ],
            "category": "paragraph",
            "html": "<p id='80' style='font-size:18px'>age, the Turkers were instructed to choose the best gener-<br>ated image based on perceptual realism, quality of transfer<br>in attribute(s), and preservation of a figure's original iden-<br>tity. The options were four randomly shuffled images gen-<br>erated from four different methods. The generated images<br>in one study have a single attribute transfer in either hair<br>color (black, blond, brown), gender, or age. In another<br>study, the generated images involve a combination of at-<br>tribute transfers. Each Turker was asked 30 to 40 questions<br>with a few simple yet logical questions for validating hu-<br>man effort. The number of validated Turkers in each user<br>study is 146 and 100 in single and multiple transfer tasks,<br>respectively.</p>",
            "id": 80,
            "page": 7,
            "text": "age, the Turkers were instructed to choose the best generated image based on perceptual realism, quality of transfer in attribute(s), and preservation of a figure's original identity. The options were four randomly shuffled images generated from four different methods. The generated images in one study have a single attribute transfer in either hair color (black, blond, brown), gender, or age. In another study, the generated images involve a combination of attribute transfers. Each Turker was asked 30 to 40 questions with a few simple yet logical questions for validating human effort. The number of validated Turkers in each user study is 146 and 100 in single and multiple transfer tasks, respectively."
        },
        {
            "bounding_box": [
                {
                    "x": 230,
                    "y": 1693
                },
                {
                    "x": 1167,
                    "y": 1693
                },
                {
                    "x": 1167,
                    "y": 1955
                },
                {
                    "x": 230,
                    "y": 1955
                }
            ],
            "category": "table",
            "html": "<table id='81' style='font-size:18px'><tr><td>Method</td><td>Hair color</td><td>Gender</td><td>Aged</td></tr><tr><td>DIAT</td><td>9.3%</td><td>31.4%</td><td>6.9%</td></tr><tr><td>CycleGAN</td><td>20.0%</td><td>16.6%</td><td>13.3%</td></tr><tr><td>IcGAN</td><td>4.5%</td><td>12.9%</td><td>9.2%</td></tr><tr><td>StarGAN</td><td>66.2%</td><td>39.1%</td><td>70.6%</td></tr></table>",
            "id": 81,
            "page": 7,
            "text": "Method Hair color Gender Aged  DIAT 9.3% 31.4% 6.9%  CycleGAN 20.0% 16.6% 13.3%  IcGAN 4.5% 12.9% 9.2%  StarGAN 66.2% 39.1%"
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 1989
                },
                {
                    "x": 1198,
                    "y": 1989
                },
                {
                    "x": 1198,
                    "y": 2079
                },
                {
                    "x": 204,
                    "y": 2079
                }
            ],
            "category": "caption",
            "html": "<caption id='82' style='font-size:14px'>Table 1. AMT perceptual evaluation for ranking different models<br>on a single attribute transfer task. Each column sums to 100%.</caption>",
            "id": 82,
            "page": 7,
            "text": "Table 1. AMT perceptual evaluation for ranking different models on a single attribute transfer task. Each column sums to 100%."
        },
        {
            "bounding_box": [
                {
                    "x": 238,
                    "y": 2132
                },
                {
                    "x": 1159,
                    "y": 2132
                },
                {
                    "x": 1159,
                    "y": 2427
                },
                {
                    "x": 238,
                    "y": 2427
                }
            ],
            "category": "table",
            "html": "<table id='83' style='font-size:18px'><tr><td>Method</td><td>H+G</td><td>H+A</td><td>G+A</td><td>H+G+A</td></tr><tr><td>DIAT</td><td>20.4%</td><td>15.6%</td><td>18.7%</td><td>15.6%</td></tr><tr><td>CycleGAN</td><td>14.0%</td><td>12.0%</td><td>11.2%</td><td>11.9%</td></tr><tr><td>IcGAN</td><td>18.2%</td><td>10.9%</td><td>20.3%</td><td>20.3%</td></tr><tr><td>StarGAN</td><td>47.4%</td><td>61.5%</td><td>49.8%</td><td>52.2%</td></tr><tr><td>a</td><td></td><td></td><td></td><td></td></tr></table>",
            "id": 83,
            "page": 7,
            "text": "Method H+G H+A G+A H+G+A  DIAT 20.4% 15.6% 18.7% 15.6%  CycleGAN 14.0% 12.0% 11.2% 11.9%  IcGAN 18.2% 10.9% 20.3% 20.3%  StarGAN 47.4% 61.5% 49.8% 52.2%  a"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2476
                },
                {
                    "x": 1198,
                    "y": 2476
                },
                {
                    "x": 1198,
                    "y": 2612
                },
                {
                    "x": 202,
                    "y": 2612
                }
            ],
            "category": "paragraph",
            "html": "<p id='84' style='font-size:14px'>Table 2. AMT perceptual evaluation for ranking different models<br>on a multi-attribute transfer task. H: Hair color; G: Gender; A:<br>Aged.</p>",
            "id": 84,
            "page": 7,
            "text": "Table 2. AMT perceptual evaluation for ranking different models on a multi-attribute transfer task. H: Hair color; G: Gender; A: Aged."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2628
                },
                {
                    "x": 1200,
                    "y": 2628
                },
                {
                    "x": 1200,
                    "y": 2975
                },
                {
                    "x": 201,
                    "y": 2975
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='85' style='font-size:16px'>Quantitative results. Tables 1 and 2 show the results of<br>our AMT experiment on single- and multi-attribute trans-<br>fer tasks, respectively. StarGAN obtained the majority of<br>votes for best transferring attributes in all cases. In the case<br>of gender changes in Table 1, the voting difference between<br>our model and other models was marginal, e.g., 39.1% for<br>StarGAN VS. 31.4% for DIAT. However, in multi-attribute</p>",
            "id": 85,
            "page": 7,
            "text": "Quantitative results. Tables 1 and 2 show the results of our AMT experiment on single- and multi-attribute transfer tasks, respectively. StarGAN obtained the majority of votes for best transferring attributes in all cases. In the case of gender changes in Table 1, the voting difference between our model and other models was marginal, e.g., 39.1% for StarGAN VS. 31.4% for DIAT. However, in multi-attribute"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 992
                },
                {
                    "x": 2276,
                    "y": 992
                },
                {
                    "x": 2276,
                    "y": 1387
                },
                {
                    "x": 1279,
                    "y": 1387
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='86' style='font-size:20px'>changes, e.g., the 'G+A' case in Table 2, the performance<br>difference becomes significant, e.g., 49.8% for StarGAN VS.<br>20.3% for IcGAN), clearly showing the advantages of Star-<br>GAN in more complicated, multi-attribute transfer tasks.<br>This is because unlike the other methods, StarGAN can han-<br>dle image translation involving multiple attribute changes<br>by randomly generating a target domain label in the train-<br>ing phase.</p>",
            "id": 86,
            "page": 7,
            "text": "changes, e.g., the 'G+A' case in Table 2, the performance difference becomes significant, e.g., 49.8% for StarGAN VS. 20.3% for IcGAN), clearly showing the advantages of StarGAN in more complicated, multi-attribute transfer tasks. This is because unlike the other methods, StarGAN can handle image translation involving multiple attribute changes by randomly generating a target domain label in the training phase."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 1415
                },
                {
                    "x": 1980,
                    "y": 1415
                },
                {
                    "x": 1980,
                    "y": 1463
                },
                {
                    "x": 1281,
                    "y": 1463
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='87' style='font-size:22px'>5.5. Experimental Results on RaFD</p>",
            "id": 87,
            "page": 7,
            "text": "5.5. Experimental Results on RaFD"
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 1492
                },
                {
                    "x": 2278,
                    "y": 1492
                },
                {
                    "x": 2278,
                    "y": 1739
                },
                {
                    "x": 1281,
                    "y": 1739
                }
            ],
            "category": "paragraph",
            "html": "<p id='88' style='font-size:16px'>We next train our model on the RaFD dataset to learn the<br>task of synthesizing facial expressions. To compare Star-<br>GAN and baseline models, we fix the input domain as the<br>'neutral' expression, but the target domain varies among the<br>seven remaining expressions.</p>",
            "id": 88,
            "page": 7,
            "text": "We next train our model on the RaFD dataset to learn the task of synthesizing facial expressions. To compare StarGAN and baseline models, we fix the input domain as the 'neutral' expression, but the target domain varies among the seven remaining expressions."
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 1762
                },
                {
                    "x": 2277,
                    "y": 1762
                },
                {
                    "x": 2277,
                    "y": 2156
                },
                {
                    "x": 1279,
                    "y": 2156
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='89' style='font-size:18px'>Qualitative evaluation. As seen in Fig. 5, StarGAN clearly<br>generates the most natural-looking expressions while prop-<br>erly maintaining the personal identity and facial features of<br>the input. While DIAT and CycleGAN mostly preserve the<br>identity of the input, many of their results are shown blurry<br>and do not maintain the degree of sharpness as seen in the<br>input. IcGAN even fails to preserve the personal identity in<br>the image by generating male images.</p>",
            "id": 89,
            "page": 7,
            "text": "Qualitative evaluation. As seen in Fig. 5, StarGAN clearly generates the most natural-looking expressions while properly maintaining the personal identity and facial features of the input. While DIAT and CycleGAN mostly preserve the identity of the input, many of their results are shown blurry and do not maintain the degree of sharpness as seen in the input. IcGAN even fails to preserve the personal identity in the image by generating male images."
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 2162
                },
                {
                    "x": 2277,
                    "y": 2162
                },
                {
                    "x": 2277,
                    "y": 2607
                },
                {
                    "x": 1279,
                    "y": 2607
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='90' style='font-size:18px'>We believe that the superiority of StarGAN in the image<br>quality is due to its implicit data augmentation effect from<br>a multi-task learning setting. RaFD images contain a rela-<br>tively small size of samples, e.g., 500 images per domain.<br>When trained on two domains, DIAT and CycleGAN can<br>only use 1,000 training images at a time, but StarGAN can<br>use 4,000 images in total from all the available domains for<br>its training. This allows StarGAN to properly learn how to<br>maintain the quality and sharpness of the generated output.</p>",
            "id": 90,
            "page": 7,
            "text": "We believe that the superiority of StarGAN in the image quality is due to its implicit data augmentation effect from a multi-task learning setting. RaFD images contain a relatively small size of samples, e.g., 500 images per domain. When trained on two domains, DIAT and CycleGAN can only use 1,000 training images at a time, but StarGAN can use 4,000 images in total from all the available domains for its training. This allows StarGAN to properly learn how to maintain the quality and sharpness of the generated output."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2628
                },
                {
                    "x": 2277,
                    "y": 2628
                },
                {
                    "x": 2277,
                    "y": 2976
                },
                {
                    "x": 1280,
                    "y": 2976
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='91' style='font-size:18px'>Quantitative evaluation. For a quantitative evaluation, we<br>compute the classification error of a facial expression on<br>synthesized images. We trained a facial expression clas-<br>sifier on the RaFD dataset (90%/10% splitting for training<br>and test sets) using a ResNet-18 architecture [5], resulting<br>in a near-perfect accuracy of 99.55%. We then trained each<br>of image translation models using the same training set and</p>",
            "id": 91,
            "page": 7,
            "text": "Quantitative evaluation. For a quantitative evaluation, we compute the classification error of a facial expression on synthesized images. We trained a facial expression classifier on the RaFD dataset (90%/10% splitting for training and test sets) using a ResNet-18 architecture , resulting in a near-perfect accuracy of 99.55%. We then trained each of image translation models using the same training set and"
        },
        {
            "bounding_box": [
                {
                    "x": 1224,
                    "y": 3056
                },
                {
                    "x": 1252,
                    "y": 3056
                },
                {
                    "x": 1252,
                    "y": 3089
                },
                {
                    "x": 1224,
                    "y": 3089
                }
            ],
            "category": "footer",
            "html": "<footer id='92' style='font-size:14px'>7</footer>",
            "id": 92,
            "page": 7,
            "text": "7"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 309
                },
                {
                    "x": 1199,
                    "y": 309
                },
                {
                    "x": 1199,
                    "y": 603
                },
                {
                    "x": 202,
                    "y": 603
                }
            ],
            "category": "paragraph",
            "html": "<p id='93' style='font-size:14px'>performed image translation on the same, unseen test set.<br>Finally, we classified the expression of these translated im-<br>ages using the above-mentioned classifier. As can be seen in<br>Table 3, our model achieves the lowest classification error,<br>indicating that our model produces the most realistic facial<br>expressions among all the methods compared.</p>",
            "id": 93,
            "page": 8,
            "text": "performed image translation on the same, unseen test set. Finally, we classified the expression of these translated images using the above-mentioned classifier. As can be seen in Table 3, our model achieves the lowest classification error, indicating that our model produces the most realistic facial expressions among all the methods compared."
        },
        {
            "bounding_box": [
                {
                    "x": 230,
                    "y": 649
                },
                {
                    "x": 1169,
                    "y": 649
                },
                {
                    "x": 1169,
                    "y": 954
                },
                {
                    "x": 230,
                    "y": 954
                }
            ],
            "category": "table",
            "html": "<table id='94' style='font-size:14px'><tr><td>Method</td><td>Classification error</td><td># of parameters</td></tr><tr><td>DIAT</td><td>4.10</td><td>52.6M x 7</td></tr><tr><td>CycleGAN</td><td>5.99</td><td>52.6M x 14</td></tr><tr><td>IcGAN</td><td>8.07</td><td>67.8M x 1</td></tr><tr><td>StarGAN</td><td>2.12</td><td>53.2M x 1</td></tr><tr><td>Real images</td><td>0.45</td><td>-</td></tr></table>",
            "id": 94,
            "page": 8,
            "text": "Method Classification error # of parameters  DIAT 4.10 52.6M x 7  CycleGAN 5.99 52.6M x 14  IcGAN 8.07 67.8M x 1  StarGAN 2.12 53.2M x 1  Real images 0.45"
        },
        {
            "bounding_box": [
                {
                    "x": 205,
                    "y": 996
                },
                {
                    "x": 1196,
                    "y": 996
                },
                {
                    "x": 1196,
                    "y": 1081
                },
                {
                    "x": 205,
                    "y": 1081
                }
            ],
            "category": "caption",
            "html": "<caption id='95' style='font-size:14px'>Table 3. Classification errors [%] and the number of parameters on<br>the RaFD dataset.</caption>",
            "id": 95,
            "page": 8,
            "text": "Table 3. Classification errors [%] and the number of parameters on the RaFD dataset."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1136
                },
                {
                    "x": 1200,
                    "y": 1136
                },
                {
                    "x": 1200,
                    "y": 1630
                },
                {
                    "x": 201,
                    "y": 1630
                }
            ],
            "category": "paragraph",
            "html": "<p id='96' style='font-size:18px'>Another important advantage of our model is the scala-<br>bility in terms of the number of parameters required. The<br>last column in Table 3 shows that the number of parameters<br>required to learn all translations by StarGAN is seven times<br>smaller than that of DIAT and fourteen times smaller than<br>that of CycleGAN. This is because StarGAN requires only<br>a single generator and discriminator pair, regardless of the<br>number of domains, while in the case of cross-domain mod-<br>els such as CycleGAN, a completely different model should<br>be trained for each source-target domain pair.</p>",
            "id": 96,
            "page": 8,
            "text": "Another important advantage of our model is the scalability in terms of the number of parameters required. The last column in Table 3 shows that the number of parameters required to learn all translations by StarGAN is seven times smaller than that of DIAT and fourteen times smaller than that of CycleGAN. This is because StarGAN requires only a single generator and discriminator pair, regardless of the number of domains, while in the case of cross-domain models such as CycleGAN, a completely different model should be trained for each source-target domain pair."
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 1668
                },
                {
                    "x": 1070,
                    "y": 1668
                },
                {
                    "x": 1070,
                    "y": 1717
                },
                {
                    "x": 203,
                    "y": 1717
                }
            ],
            "category": "paragraph",
            "html": "<p id='97' style='font-size:20px'>5.6. Experimental Results on CelebA +RaFD</p>",
            "id": 97,
            "page": 8,
            "text": "5.6. Experimental Results on CelebA +RaFD"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1748
                },
                {
                    "x": 1200,
                    "y": 1748
                },
                {
                    "x": 1200,
                    "y": 2142
                },
                {
                    "x": 201,
                    "y": 2142
                }
            ],
            "category": "paragraph",
            "html": "<p id='98' style='font-size:16px'>Finally, we empirically demonstrate that our model can<br>learn not only from multiple domains within a single<br>dataset, but also from multiple datasets. We train our model<br>jointly on the CelebA and RaFD datasets using the mask<br>vector (see Section 3.2). To distinguish between the model<br>trained only on RaFD and the model trained on both CelebA<br>and RaFD, we denote the former as StarGAN-SNG (single)<br>and the latter as StarGAN-JNT (joint).</p>",
            "id": 98,
            "page": 8,
            "text": "Finally, we empirically demonstrate that our model can learn not only from multiple domains within a single dataset, but also from multiple datasets. We train our model jointly on the CelebA and RaFD datasets using the mask vector (see Section 3.2). To distinguish between the model trained only on RaFD and the model trained on both CelebA and RaFD, we denote the former as StarGAN-SNG (single) and the latter as StarGAN-JNT (joint)."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2162
                },
                {
                    "x": 1199,
                    "y": 2162
                },
                {
                    "x": 1199,
                    "y": 2808
                },
                {
                    "x": 201,
                    "y": 2808
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='99' style='font-size:18px'>Effects of joint training. Fig. 6 shows qualitative com-<br>parisons between StarGAN-SNG and StarGAN-JNT, where<br>the task is to synthesize facial expressions of images in<br>CelebA. StarGAN-JNT exhibits emotional expressions with<br>high visual quality, while StarGAN-SNG generates reason-<br>able but blurry images with gray backgrounds. This differ-<br>ence is due to the fact that StarGAN-JNT learns to translate<br>CelebA images during training but not StarGAN-SNG. In<br>other words, StarGAN-JNT can leverage both datasets to<br>improve shared low-level tasks such facial keypoint detec-<br>tion and segmentation. By utilizing both CelebA and RaFD,<br>StarGAN-JNT can improve these low-level tasks, which is<br>beneficial to learning facial expression synthesis.</p>",
            "id": 99,
            "page": 8,
            "text": "Effects of joint training. Fig. 6 shows qualitative comparisons between StarGAN-SNG and StarGAN-JNT, where the task is to synthesize facial expressions of images in CelebA. StarGAN-JNT exhibits emotional expressions with high visual quality, while StarGAN-SNG generates reasonable but blurry images with gray backgrounds. This difference is due to the fact that StarGAN-JNT learns to translate CelebA images during training but not StarGAN-SNG. In other words, StarGAN-JNT can leverage both datasets to improve shared low-level tasks such facial keypoint detection and segmentation. By utilizing both CelebA and RaFD, StarGAN-JNT can improve these low-level tasks, which is beneficial to learning facial expression synthesis."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2828
                },
                {
                    "x": 1200,
                    "y": 2828
                },
                {
                    "x": 1200,
                    "y": 2975
                },
                {
                    "x": 202,
                    "y": 2975
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='100' style='font-size:16px'>Learned role of mask vector. In this experiment, we gave a<br>one-hot vector c by setting the dimension of a particular fa-<br>cial expression (available from the second dataset, RaFD) to</p>",
            "id": 100,
            "page": 8,
            "text": "Learned role of mask vector. In this experiment, we gave a one-hot vector c by setting the dimension of a particular facial expression (available from the second dataset, RaFD) to"
        },
        {
            "bounding_box": [
                {
                    "x": 1284,
                    "y": 290
                },
                {
                    "x": 2274,
                    "y": 290
                },
                {
                    "x": 2274,
                    "y": 825
                },
                {
                    "x": 1284,
                    "y": 825
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='101' style='font-size:20px' alt=\"Input Disgusted Fearful Happy\" data-coord=\"top-left:(1284,290); bottom-right:(2274,825)\" /></figure>",
            "id": 101,
            "page": 8,
            "text": "Input Disgusted Fearful Happy"
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 836
                },
                {
                    "x": 2276,
                    "y": 836
                },
                {
                    "x": 2276,
                    "y": 1020
                },
                {
                    "x": 1280,
                    "y": 1020
                }
            ],
            "category": "caption",
            "html": "<br><caption id='102' style='font-size:14px'>Figure 7. Learned role of the mask vector. All images are gener-<br>ated by StarGAN-JNT. The first row shows the result of applying<br>the proper mask vector, and the last row shows the result of apply-<br>ing the wrong mask vector.</caption>",
            "id": 102,
            "page": 8,
            "text": "Figure 7. Learned role of the mask vector. All images are generated by StarGAN-JNT. The first row shows the result of applying the proper mask vector, and the last row shows the result of applying the wrong mask vector."
        },
        {
            "bounding_box": [
                {
                    "x": 1277,
                    "y": 1080
                },
                {
                    "x": 2278,
                    "y": 1080
                },
                {
                    "x": 2278,
                    "y": 1821
                },
                {
                    "x": 1277,
                    "y": 1821
                }
            ],
            "category": "paragraph",
            "html": "<p id='103' style='font-size:16px'>one. In this case, since the label associated with the second<br>data set is explicitly given, the proper mask vector would be<br>[0, 1]. Fig. 7 shows the case where this proper mask vector<br>was given and the opposite case where a wrong mask vector<br>of [1, 0] was given. When the wrong mask vector was used,<br>StarGAN-JNT fails to synthesize facial expressions, and it<br>manipulates the age of the input image. This is because the<br>model ignores the facial expression label as unknown and<br>treats the facial attribute label as valid by the mask vector.<br>Note that since one of the facial attributes is 'young', the<br>model translates the image from young to old when it takes<br>in a zero vector as input. From this behavior, we can con-<br>firm that StarGAN properly learned the intended role of a<br>mask vector in image-to-image translations when involving<br>all the labels from multiple datasets altogether.</p>",
            "id": 103,
            "page": 8,
            "text": "one. In this case, since the label associated with the second data set is explicitly given, the proper mask vector would be . Fig. 7 shows the case where this proper mask vector was given and the opposite case where a wrong mask vector of  was given. When the wrong mask vector was used, StarGAN-JNT fails to synthesize facial expressions, and it manipulates the age of the input image. This is because the model ignores the facial expression label as unknown and treats the facial attribute label as valid by the mask vector. Note that since one of the facial attributes is 'young', the model translates the image from young to old when it takes in a zero vector as input. From this behavior, we can confirm that StarGAN properly learned the intended role of a mask vector in image-to-image translations when involving all the labels from multiple datasets altogether."
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 1869
                },
                {
                    "x": 1578,
                    "y": 1869
                },
                {
                    "x": 1578,
                    "y": 1918
                },
                {
                    "x": 1279,
                    "y": 1918
                }
            ],
            "category": "paragraph",
            "html": "<p id='104' style='font-size:22px'>6. Conclusion</p>",
            "id": 104,
            "page": 8,
            "text": "6. Conclusion"
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 1952
                },
                {
                    "x": 2279,
                    "y": 1952
                },
                {
                    "x": 2279,
                    "y": 2547
                },
                {
                    "x": 1278,
                    "y": 2547
                }
            ],
            "category": "paragraph",
            "html": "<p id='105' style='font-size:18px'>In this paper, we proposed StarGAN, a scalable image-<br>to-image translation model among multiple domains using<br>a single generator and a discriminator. Besides the advan-<br>tages in scalability, StarGAN generated images of higher<br>visual quality compared to existing methods [16, 23, 33],<br>owing to the generalization capability behind the multi-task<br>learning setting. In addition, the use of the proposed simple<br>mask vector enables StarGAN to utilize multiple datasets<br>with different sets of domain labels, thus handling all avail-<br>able labels from them. We hope our work to enable users<br>to develop interesting image translation applications across<br>multiple domains.</p>",
            "id": 105,
            "page": 8,
            "text": "In this paper, we proposed StarGAN, a scalable imageto-image translation model among multiple domains using a single generator and a discriminator. Besides the advantages in scalability, StarGAN generated images of higher visual quality compared to existing methods , owing to the generalization capability behind the multi-task learning setting. In addition, the use of the proposed simple mask vector enables StarGAN to utilize multiple datasets with different sets of domain labels, thus handling all available labels from them. We hope our work to enable users to develop interesting image translation applications across multiple domains."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2580
                },
                {
                    "x": 2278,
                    "y": 2580
                },
                {
                    "x": 2278,
                    "y": 2975
                },
                {
                    "x": 1280,
                    "y": 2975
                }
            ],
            "category": "paragraph",
            "html": "<p id='106' style='font-size:18px'>Acknowledgements. This work was mainly done while the<br>first author did a research internship at Clova AI Research,<br>NAVER. We thank all the researchers at NAVER, especially<br>Donghyun Kwak, for insightful discussions. This work was<br>partially supported by the National Research Foundation<br>of Korea (NRF) grant funded by the Korean government<br>(MSIP) (No. NRF2016R1C1B2015924). Jaegul Choo is<br>the corresponding author.</p>",
            "id": 106,
            "page": 8,
            "text": "Acknowledgements. This work was mainly done while the first author did a research internship at Clova AI Research, NAVER. We thank all the researchers at NAVER, especially Donghyun Kwak, for insightful discussions. This work was partially supported by the National Research Foundation of Korea (NRF) grant funded by the Korean government (MSIP) (No. NRF2016R1C1B2015924). Jaegul Choo is the corresponding author."
        },
        {
            "bounding_box": [
                {
                    "x": 1227,
                    "y": 3056
                },
                {
                    "x": 1251,
                    "y": 3056
                },
                {
                    "x": 1251,
                    "y": 3092
                },
                {
                    "x": 1227,
                    "y": 3092
                }
            ],
            "category": "footer",
            "html": "<footer id='107' style='font-size:16px'>8</footer>",
            "id": 107,
            "page": 8,
            "text": "8"
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 301
                },
                {
                    "x": 445,
                    "y": 301
                },
                {
                    "x": 445,
                    "y": 352
                },
                {
                    "x": 204,
                    "y": 352
                }
            ],
            "category": "paragraph",
            "html": "<p id='108' style='font-size:20px'>References</p>",
            "id": 108,
            "page": 9,
            "text": "References"
        },
        {
            "bounding_box": [
                {
                    "x": 213,
                    "y": 371
                },
                {
                    "x": 1201,
                    "y": 371
                },
                {
                    "x": 1201,
                    "y": 2979
                },
                {
                    "x": 213,
                    "y": 2979
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='109' style='font-size:14px'>[1] M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein gen-<br>erative adversarial networks. In Proceedings of the 34th In-<br>ternational Conference on Machine Learning (ICML), pages<br>214-223, 2017. 5<br>[2] A. Brock, T. Lim, J. M. Ritchie, and N. Weston. Neural<br>photo editing with introspective adversarial networks. arXiv<br>preprint arXiv: 1609.07093, 2016. 3<br>[3] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,<br>D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Gen-<br>erative adversarial nets. In Advances in Neural Information<br>Processing Systems (NIPS), pages 2672-2680, 2014. 2<br>[4] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and<br>A. Courville. Improved training of wasserstein gans. arXiv<br>preprint arXiv:1704.00028, 2017. 5, 6<br>[5] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning<br>for image recognition. In Proceedings of the IEEE confer-<br>ence on Computer Vision and Pattern Recognition (CVPR),<br>pages 770-778, 2016. 5, 7<br>[6] X. Huang, Y. Li, 0. Poursaeed, J. Hopcroft, and S. Be-<br>longie. Stacked generative adversarial networks. In The<br>IEEE Conference on Computer Vision and Pattern Recog-<br>nition (CVPR), July 2017. 2<br>[7] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image<br>translation with conditional adversarial networks. In Pro-<br>ceedings of the IEEE Conference on Computer Vision and<br>Pattern Recognition (CVPR), 2017. 1, 2, 3, 5<br>[8] T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive<br>growing of gans for improved quality, stability, and variation.<br>arXiv preprint arXiv:1710.10196, 2017. 2<br>[9] T. Kim, M. Cha, H. Kim, J. K. Lee, and J. Kim. Learning to<br>discover cross-domain relations with generative adversarial<br>networks. In Proceedings of the 34th International Confer-<br>ence on Machine Learning (ICML), pages 1857-1865, 2017.<br>1, 2, 3, 4<br>[10] T. Kim, B. Kim, M. Cha, and J. Kim. Unsupervised visual<br>attribute transfer with reconfigurable generative adversarial<br>networks. arXiv preprint arXiv:1707.09798, 2017. 2<br>[11] D. Kingma and J. Ba. Adam: A method for stochastic opti-<br>mization. arXiv preprint arXiv:1412.6980, 2014. 6<br>[12] D. P. Kingma and M. Welling. Auto-encoding variational<br>bayes. In Proceedings of the 2nd International Conference<br>on Learning Representations (ICLR), 2014. 3<br>[13] 0. Langner, R. Dotsch, G. Bijlstra, D. H. Wigboldus, S. T.<br>Hawk, and A. Van Knippenberg. Presentation and valida-<br>tion of the radboud faces database. Cognition and Emotion,<br>24(8):1377-1388, 2010. 2, 4, 6<br>[14] C. Ledig, L. Theis, F. Huszar, J. Caballero, A. Cunning-<br>ham, A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, and<br>W. Shi. Photo-realistic single image super-resolution using a<br>generative adversarial network. In The IEEE Conference on<br>Computer Vision and Pattern Recognition (CVPR), 2017. 2,<br>3<br>[15] C. Li and M. Wand. Precomputed real-time texture synthesis<br>with markovian generative adversarial networks. In Proceed-<br>ings of the 14th European Conference on Computer Vision<br>(ECCV), pages 702-716, 2016. 5</p>",
            "id": 109,
            "page": 9,
            "text": " M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein generative adversarial networks. In Proceedings of the 34th International Conference on Machine Learning (ICML), pages 214-223, 2017. 5  A. Brock, T. Lim, J. M. Ritchie, and N. Weston. Neural photo editing with introspective adversarial networks. arXiv preprint arXiv: 1609.07093, 2016. 3  I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems (NIPS), pages 2672-2680, 2014. 2  I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. Courville. Improved training of wasserstein gans. arXiv preprint arXiv:1704.00028, 2017. 5, 6  K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR), pages 770-778, 2016. 5, 7  X. Huang, Y. Li, 0. Poursaeed, J. Hopcroft, and S. Belongie. Stacked generative adversarial networks. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017. 2  P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 1, 2, 3, 5  T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017. 2  T. Kim, M. Cha, H. Kim, J. K. Lee, and J. Kim. Learning to discover cross-domain relations with generative adversarial networks. In Proceedings of the 34th International Conference on Machine Learning (ICML), pages 1857-1865, 2017. 1, 2, 3, 4  T. Kim, B. Kim, M. Cha, and J. Kim. Unsupervised visual attribute transfer with reconfigurable generative adversarial networks. arXiv preprint arXiv:1707.09798, 2017. 2  D. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 6  D. P. Kingma and M. Welling. Auto-encoding variational bayes. In Proceedings of the 2nd International Conference on Learning Representations (ICLR), 2014. 3  0. Langner, R. Dotsch, G. Bijlstra, D. H. Wigboldus, S. T. Hawk, and A. Van Knippenberg. Presentation and validation of the radboud faces database. Cognition and Emotion, 24(8):1377-1388, 2010. 2, 4, 6  C. Ledig, L. Theis, F. Huszar, J. Caballero, A. Cunningham, A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, and W. Shi. Photo-realistic single image super-resolution using a generative adversarial network. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 2, 3  C. Li and M. Wand. Precomputed real-time texture synthesis with markovian generative adversarial networks. In Proceedings of the 14th European Conference on Computer Vision (ECCV), pages 702-716, 2016. 5"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 305
                },
                {
                    "x": 2291,
                    "y": 305
                },
                {
                    "x": 2291,
                    "y": 2974
                },
                {
                    "x": 1279,
                    "y": 2974
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='110' style='font-size:14px'>[16] M. Li, W. Zuo, and D. Zhang. Deep identity-aware transfer<br>of facial attributes. arXiv preprint arXiv:1610.05586, 2016.<br>2, 5, 8<br>[17] M.-Y. Liu, T. Breuel, and J. Kautz. Unsupervised<br>image-to-image translation networks. arXiv preprint<br>arXiv:1703.00848, 2017. 3<br>[18] M.-Y. Liu and 0. Tuzel. Coupled generative adversarial net-<br>works. In Advances in Neural Information Processing Sys-<br>tems (NIPS), pages 469-477, 2016. 3<br>[19] Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face<br>attributes in the wild. In Proceedings of the IEEE Interna-<br>tional Conference on Computer Vision (ICCV), 2015. 2, 4,<br>6<br>[20] M. Mirza and S. Osindero. Conditional generative adversar-<br>ial nets. arXiv preprint arXiv:1411.1784, 2014. 3<br>[21] A. Odena. Semi-supervised learning with generative adver-<br>sarial networks. arXiv preprint arXiv: 1606.01583, 2016. 3<br>[22] A. Odena, C. Olah, and J. Shlens. Conditional image<br>synthesis with auxiliary classifier gans. arXiv preprint<br>arXiv:1610.09585, 2016. 3, 5<br>[23] G. Perarnau, J. van de Weijer, B. Raducanu, and J. M.<br>Alvarez. Invertible conditional gans for image editing. arXiv<br>preprint arXiv:1611.06355, 2016. 5, 8<br>[24] A. Radford, L. Metz, and S. Chintala. Unsupervised repre-<br>sentation learning with deep convolutional generative adver-<br>sarial networks. arXiv preprint arXiv:1511.06434, 2015. 2<br>[25] S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and<br>H. Lee. Generative adversarial text to image synthesis. arXiv<br>preprint arXiv:1605.05396, 2016. 3<br>[26] W. Shen and R. Liu. Learning residual images for face at-<br>tribute manipulation. In The IEEE Conference on Computer<br>Vision and Pattern Recognition (CVPR), 2017. 2<br>[27] Z. Shu, E. Yumer, S. Hadap, K. Sunkavalli, E. Shechtman,<br>and D. Samaras. Neural face editing with intrinsic image<br>disentangling. In The IEEE Conference on Computer Vision<br>and Pattern Recognition (CVPR), 2017. 3<br>[28] Y. Taigman, A. Polyak, and L. Wolf. Unsupervised cross-<br>domain image generation. In 5th International Conference<br>on Learning Representations (ICLR), 2017. 3<br>[29] D. Ulyanov, A. Vedaldi, and V. Lempitsky. Instance normal-<br>ization: The missing ingredient for fast stylization. arXiv<br>preprint arXiv:1607.08022, 2016. 5<br>[30] H. Zhang, T. Xu, H. Li, S. Zhang, X. Huang, X. Wang, and<br>D. Metaxas. Stackgan: Text to photo-realistic image syn-<br>thesis with stacked generative adversarial networks. arXiv<br>preprint arXiv:1612.03242, 2016. 3<br>[31] Z. Zhang, Y. Song, and H. Qi. Age progression/regression<br>by conditional adversarial autoencoder. In The IEEE Confer-<br>ence on Computer Vision and Pattern Recognition (CVPR),<br>July 2017. 2<br>[32] J. Zhao, M. Mathieu, and Y. LeCun. Energy-based genera-<br>tive adversarial network. In 5th International Conference on<br>Learning Representations (ICLR), 2017. 2<br>[33] J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-<br>to-image translation using cycle-consistent adversarial net-<br>works. In Proceedings of the IEEE International Conference<br>on Computer Vision (ICCV), 2017. 1, 2, 3, 4, 5, 8</p>",
            "id": 110,
            "page": 9,
            "text": " M. Li, W. Zuo, and D. Zhang. Deep identity-aware transfer of facial attributes. arXiv preprint arXiv:1610.05586, 2016. 2, 5, 8  M.-Y. Liu, T. Breuel, and J. Kautz. Unsupervised image-to-image translation networks. arXiv preprint arXiv:1703.00848, 2017. 3  M.-Y. Liu and 0. Tuzel. Coupled generative adversarial networks. In Advances in Neural Information Processing Systems (NIPS), pages 469-477, 2016. 3  Z. Liu, P. Luo, X. Wang, and X. Tang. Deep learning face attributes in the wild. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2015. 2, 4, 6  M. Mirza and S. Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784, 2014. 3  A. Odena. Semi-supervised learning with generative adversarial networks. arXiv preprint arXiv: 1606.01583, 2016. 3  A. Odena, C. Olah, and J. Shlens. Conditional image synthesis with auxiliary classifier gans. arXiv preprint arXiv:1610.09585, 2016. 3, 5  G. Perarnau, J. van de Weijer, B. Raducanu, and J. M. Alvarez. Invertible conditional gans for image editing. arXiv preprint arXiv:1611.06355, 2016. 5, 8  A. Radford, L. Metz, and S. Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434, 2015. 2  S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and H. Lee. Generative adversarial text to image synthesis. arXiv preprint arXiv:1605.05396, 2016. 3  W. Shen and R. Liu. Learning residual images for face attribute manipulation. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 2  Z. Shu, E. Yumer, S. Hadap, K. Sunkavalli, E. Shechtman, and D. Samaras. Neural face editing with intrinsic image disentangling. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 3  Y. Taigman, A. Polyak, and L. Wolf. Unsupervised crossdomain image generation. In 5th International Conference on Learning Representations (ICLR), 2017. 3  D. Ulyanov, A. Vedaldi, and V. Lempitsky. Instance normalization: The missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016. 5  H. Zhang, T. Xu, H. Li, S. Zhang, X. Huang, X. Wang, and D. Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks. arXiv preprint arXiv:1612.03242, 2016. 3  Z. Zhang, Y. Song, and H. Qi. Age progression/regression by conditional adversarial autoencoder. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017. 2  J. Zhao, M. Mathieu, and Y. LeCun. Energy-based generative adversarial network. In 5th International Conference on Learning Representations (ICLR), 2017. 2  J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired imageto-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2017. 1, 2, 3, 4, 5, 8"
        },
        {
            "bounding_box": [
                {
                    "x": 1225,
                    "y": 3056
                },
                {
                    "x": 1252,
                    "y": 3056
                },
                {
                    "x": 1252,
                    "y": 3089
                },
                {
                    "x": 1225,
                    "y": 3089
                }
            ],
            "category": "footer",
            "html": "<footer id='111' style='font-size:16px'>9</footer>",
            "id": 111,
            "page": 9,
            "text": "9"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 301
                },
                {
                    "x": 468,
                    "y": 301
                },
                {
                    "x": 468,
                    "y": 356
                },
                {
                    "x": 202,
                    "y": 356
                }
            ],
            "category": "paragraph",
            "html": "<p id='112' style='font-size:22px'>7. Appendix</p>",
            "id": 112,
            "page": 10,
            "text": "7. Appendix"
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 381
                },
                {
                    "x": 914,
                    "y": 381
                },
                {
                    "x": 914,
                    "y": 430
                },
                {
                    "x": 203,
                    "y": 430
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='113' style='font-size:20px'>7.1. Training with Multiple Datasets</p>",
            "id": 113,
            "page": 10,
            "text": "7.1. Training with Multiple Datasets"
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 458
                },
                {
                    "x": 2279,
                    "y": 458
                },
                {
                    "x": 2279,
                    "y": 657
                },
                {
                    "x": 200,
                    "y": 657
                }
            ],
            "category": "paragraph",
            "html": "<p id='114' style='font-size:18px'>Fig. 8 shows an overview of StarGAN when learning from both the CelebA and RaFD datasets. As can be seen at the<br>top of the figure, the label for CelebA contains binary attributes (Black, Blond, Brown, Male, and Young), while the label<br>for RaFD provides information on categorical attributes (Angry, Fearful, Happy, Sad, and Disgusted). The mask vector is a<br>two-dimensional one-hot vector which indicates whether the CelebA or RaFD label is valid.</p>",
            "id": 114,
            "page": 10,
            "text": "Fig. 8 shows an overview of StarGAN when learning from both the CelebA and RaFD datasets. As can be seen at the top of the figure, the label for CelebA contains binary attributes (Black, Blond, Brown, Male, and Young), while the label for RaFD provides information on categorical attributes (Angry, Fearful, Happy, Sad, and Disgusted). The mask vector is a two-dimensional one-hot vector which indicates whether the CelebA or RaFD label is valid."
        },
        {
            "bounding_box": [
                {
                    "x": 255,
                    "y": 684
                },
                {
                    "x": 2233,
                    "y": 684
                },
                {
                    "x": 2233,
                    "y": 2600
                },
                {
                    "x": 255,
                    "y": 2600
                }
            ],
            "category": "figure",
            "html": "<figure><img id='115' style='font-size:14px' alt=\"CelebA label RaFD label Mask vector\nBlack / Blond / Brown / Male / Young Angry / Fearful / Happy / Sad / Disgusted CelebA / RaFD\n(a) Training the discriminator (b) Original-to-target domain (c) Target-to-original domain (d) Fooling the discriminator\nOutput image and original domain label\nReal image Fake image\n0 0 1 0 1\n0 0 0\n1 0\n↓\n(1) (2)\nG G\n↑\n(1),(2)\n(1)\n1 0 0 1 1 Real? 1 0 0 1 1 ? ? ? ? ?\nReal? 0 0 1 0 1 ? ? ? ? ?\nCelebA label RaFD label 0 0 0 CelebA label RaFD label\n(1) when training with real images 1 0\n(2) when training with fake images Reconstructed image\nInput image and target domain label\nTraining with CelebA\nTraining with RaFD\n(e) Training the discriminator (f) Original-to-target domain (g) Target-to-original domain (h) Fooling the discriminator\nOutput image and original domain label\nReal image Fake image\n0 0 0 0 0\n1 0 0\n0 1\n(1) (2)\nG G\nD\n(1), (2) (1) ↑\n0 0 0 Real? ? ? ? ? ? 0 0 1 0 0\nReal? ? ? ? ? ? 1 0 0 0 0\nCelebA label RaFD label 0 0 1 0 0 CelebA label RaFD label\n(1) when training with real images 0 1\n(2) when training with fake images\nInput image and target domain label Reconstructed image\" data-coord=\"top-left:(255,684); bottom-right:(2233,2600)\" /></figure>",
            "id": 115,
            "page": 10,
            "text": "CelebA label RaFD label Mask vector Black / Blond / Brown / Male / Young Angry / Fearful / Happy / Sad / Disgusted CelebA / RaFD (a) Training the discriminator (b) Original-to-target domain (c) Target-to-original domain (d) Fooling the discriminator Output image and original domain label Real image Fake image 0 0 1 0 1 0 0 0 1 0 ↓ (1) (2) G G ↑ (1),(2) (1) 1 0 0 1 1 Real? 1 0 0 1 1 ? ? ? ? ? Real? 0 0 1 0 1 ? ? ? ? ? CelebA label RaFD label 0 0 0 CelebA label RaFD label (1) when training with real images 1 0 (2) when training with fake images Reconstructed image Input image and target domain label Training with CelebA Training with RaFD (e) Training the discriminator (f) Original-to-target domain (g) Target-to-original domain (h) Fooling the discriminator Output image and original domain label Real image Fake image 0 0 0 0 0 1 0 0 0 1 (1) (2) G G D (1), (2) (1) ↑ 0 0 0 Real? ? ? ? ? ? 0 0 1 0 0 Real? ? ? ? ? ? 1 0 0 0 0 CelebA label RaFD label 0 0 1 0 0 CelebA label RaFD label (1) when training with real images 0 1 (2) when training with fake images Input image and target domain label Reconstructed image"
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 2638
                },
                {
                    "x": 2279,
                    "y": 2638
                },
                {
                    "x": 2279,
                    "y": 2918
                },
                {
                    "x": 199,
                    "y": 2918
                }
            ],
            "category": "caption",
            "html": "<caption id='116' style='font-size:16px'>Figure 8. Overview of StarGAN when training with both CelebA and RaFD. (a) ~ (d) shows the training process using CelebA, and (e) ~<br>(h) shows the training process using RaFD. (a), (e) The discriminator D learns to distinguish between real and fake images and minimize<br>the classification error only for the known label. (b), (c), (f), (g) When the mask vector (purple) is [1, 0], the generator G learns to focus on<br>the CelebA label (yellow) and ignore the RaFD label (green) to perform image-to-image translation, and vice versa when the mask vector<br>is [0, 1]. (d), (h) G tries to generate images that are both indistinguishable from real images and classifiable by D as belonging to the target<br>domain.</caption>",
            "id": 116,
            "page": 10,
            "text": "Figure 8. Overview of StarGAN when training with both CelebA and RaFD. (a) ~ (d) shows the training process using CelebA, and (e) ~ (h) shows the training process using RaFD. (a), (e) The discriminator D learns to distinguish between real and fake images and minimize the classification error only for the known label. (b), (c), (f), (g) When the mask vector (purple) is , the generator G learns to focus on the CelebA label (yellow) and ignore the RaFD label (green) to perform image-to-image translation, and vice versa when the mask vector is . (d), (h) G tries to generate images that are both indistinguishable from real images and classifiable by D as belonging to the target domain."
        },
        {
            "bounding_box": [
                {
                    "x": 1218,
                    "y": 3053
                },
                {
                    "x": 1264,
                    "y": 3053
                },
                {
                    "x": 1264,
                    "y": 3093
                },
                {
                    "x": 1218,
                    "y": 3093
                }
            ],
            "category": "footer",
            "html": "<footer id='117' style='font-size:18px'>10</footer>",
            "id": 117,
            "page": 10,
            "text": "10"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 303
                },
                {
                    "x": 721,
                    "y": 303
                },
                {
                    "x": 721,
                    "y": 353
                },
                {
                    "x": 202,
                    "y": 353
                }
            ],
            "category": "paragraph",
            "html": "<p id='118' style='font-size:20px'>7.2. Network Architecture</p>",
            "id": 118,
            "page": 11,
            "text": "7.2. Network Architecture"
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 377
                },
                {
                    "x": 2281,
                    "y": 377
                },
                {
                    "x": 2281,
                    "y": 628
                },
                {
                    "x": 200,
                    "y": 628
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='119' style='font-size:18px'>The network architectures of StarGAN are shown in Table 4 and 5. For the generator network, we use instance normal-<br>ization in all layers except the last output layer. For the discriminator network, we use Leaky ReLU with a negative slope of<br>0.01. There are some notations; nd: the number of domain, nc: the dimension of domain labels (nd + 2 when training with<br>both the CelebA and RaFD datasets, otherwise same as nd), N: the number of output channels, K: kernel size, S: stride size,<br>P: padding size, IN: instance normalization.</p>",
            "id": 119,
            "page": 11,
            "text": "The network architectures of StarGAN are shown in Table 4 and 5. For the generator network, we use instance normalization in all layers except the last output layer. For the discriminator network, we use Leaky ReLU with a negative slope of 0.01. There are some notations; nd: the number of domain, nc: the dimension of domain labels (nd + 2 when training with both the CelebA and RaFD datasets, otherwise same as nd), N: the number of output channels, K: kernel size, S: stride size, P: padding size, IN: instance normalization."
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 705
                },
                {
                    "x": 2257,
                    "y": 705
                },
                {
                    "x": 2257,
                    "y": 1834
                },
                {
                    "x": 222,
                    "y": 1834
                }
            ],
            "category": "table",
            "html": "<table id='120' style='font-size:22px'><tr><td>Part</td><td>Input → Output Shape</td><td>Layer Information</td></tr><tr><td rowspan=\"3\">Down-sampling</td><td>(h,w,3+ nc) → (h,w, 64)</td><td>CONV-(N64, K7x7, S1,P3), IN, ReLU</td></tr><tr><td>(h,w,64) → (슬, 쁠, 128)</td><td>CONV-(N128, K4x4, S2, P1), IN, ReLU</td></tr><tr><td>(슬, 쁠, 128) → (물, w4, 256)</td><td>CONV-(N256, K4x4, S2, P1), IN, ReLU</td></tr><tr><td rowspan=\"6\">Bottleneck</td><td>(문 , w, 256) → (7, 314 256)</td><td>Residual Block: CONV-(N256, K3x3, S1,P1), IN, ReLU</td></tr><tr><td>(문 , w4, 256) → (물, w4, 256)</td><td>Residual Block: CONV-(N256, K3x3, S1,P1), IN, ReLU</td></tr><tr><td>(문 , w4 , 256) → (물, w4 , 256)</td><td>Residual Block: CONV-(N256, K3x3, S1,P1), IN, ReLU</td></tr><tr><td>(7 , ww ・ 256) → (물, w4, 256)</td><td>Residual Block: CONV-(N256, K3x3, S1,P1), IN, ReLU</td></tr><tr><td>(문 , w4 , 256) → (물, 314 256)</td><td>Residual Block: CONV-(N256, K3x3, S1,P1), IN, ReLU</td></tr><tr><td>(물, 4,256) → (물, 314 256)</td><td>Residual Block: CONV-(N256, K3x3,S1,P1), IN, ReLU</td></tr><tr><td rowspan=\"3\">Up-sampling</td><td>(7, w4, 256) → (슬, 쁠, 128)</td><td>DECONV-(N128, K4x4, S2, P1), IN, ReLU</td></tr><tr><td>(슬, 쁠, 128) → (h,w,64)</td><td>DECONV-(N64, K4x4, S2,P1), IN, ReLU</td></tr><tr><td>(h,w,64) → (h,w,3)</td><td>CONV-(N3, K7x7, S1, P3), Tanh</td></tr></table>",
            "id": 120,
            "page": 11,
            "text": "Part Input → Output Shape Layer Information  Down-sampling (h,w,3+ nc) → (h,w, 64) CONV-(N64, K7x7, S1,P3), IN, ReLU  (h,w,64) → (슬, 쁠, 128) CONV-(N128, K4x4, S2, P1), IN, ReLU  (슬, 쁠, 128) → (물, w4, 256) CONV-(N256, K4x4, S2, P1), IN, ReLU  Bottleneck (문 , w, 256) → (7, 314 256) Residual Block: CONV-(N256, K3x3, S1,P1), IN, ReLU  (문 , w4, 256) → (물, w4, 256) Residual Block: CONV-(N256, K3x3, S1,P1), IN, ReLU  (문 , w4 , 256) → (물, w4 , 256) Residual Block: CONV-(N256, K3x3, S1,P1), IN, ReLU  (7 , ww ・ 256) → (물, w4, 256) Residual Block: CONV-(N256, K3x3, S1,P1), IN, ReLU  (문 , w4 , 256) → (물, 314 256) Residual Block: CONV-(N256, K3x3, S1,P1), IN, ReLU  (물, 4,256) → (물, 314 256) Residual Block: CONV-(N256, K3x3,S1,P1), IN, ReLU  Up-sampling (7, w4, 256) → (슬, 쁠, 128) DECONV-(N128, K4x4, S2, P1), IN, ReLU  (슬, 쁠, 128) → (h,w,64) DECONV-(N64, K4x4, S2,P1), IN, ReLU  (h,w,64) → (h,w,3)"
        },
        {
            "bounding_box": [
                {
                    "x": 934,
                    "y": 1867
                },
                {
                    "x": 1545,
                    "y": 1867
                },
                {
                    "x": 1545,
                    "y": 1913
                },
                {
                    "x": 934,
                    "y": 1913
                }
            ],
            "category": "caption",
            "html": "<caption id='121' style='font-size:14px'>Table 4. Generator network architecture</caption>",
            "id": 121,
            "page": 11,
            "text": "Table 4. Generator network architecture"
        },
        {
            "bounding_box": [
                {
                    "x": 226,
                    "y": 2035
                },
                {
                    "x": 2251,
                    "y": 2035
                },
                {
                    "x": 2251,
                    "y": 2823
                },
                {
                    "x": 226,
                    "y": 2823
                }
            ],
            "category": "table",
            "html": "<table id='122' style='font-size:22px'><tr><td>Layer</td><td>Input → Output Shape</td><td>Layer Information</td></tr><tr><td>Input Layer</td><td>(h,w,3) → (슬, 쁠, 64)</td><td>CONV-(N64, K4x4, S2, P1), Leaky ReLU</td></tr><tr><td>Hidden Layer</td><td>(슬, 2,64) → (7, w4, 128)</td><td>CONV-(N128, K4x4, S2, P1), Leaky ReLU</td></tr><tr><td>Hidden Layer</td><td>(문 , w, 128) → (승, �, 256)</td><td>CONV-(N256, K4x4, S2, P1), Leaky ReLU</td></tr><tr><td>Hidden Layer</td><td>w 512) (승 , (0,256) → (음, 16,</td><td>CONV-(N512, K4x4, S2, P1), Leaky ReLU</td></tr><tr><td>Hidden Layer</td><td>(음, 16,512) → (�, w 1024) 32'</td><td>CONV-(N1024, K4x4, S2,P1), Leaky ReLU</td></tr><tr><td>Hidden Layer</td><td>h 332 , 1024) → ( h 64, 2048) ( 32, 64,</td><td>CONV-(N2048, K4x4, S2,P1), Leaky ReLU</td></tr><tr><td>Output Layer (Dsrc)</td><td>(습, (2048) → (승, 64, 1)</td><td>CONV-(N1, K3x3,S1,P1)</td></tr><tr><td>Output Layer (Dcls)</td><td>( he , 64 , 2048) → (1,1,nd)</td><td>CONV-(N(nd), K64x64, S1, PO)</td></tr></table>",
            "id": 122,
            "page": 11,
            "text": "Layer Input → Output Shape Layer Information  Input Layer (h,w,3) → (슬, 쁠, 64) CONV-(N64, K4x4, S2, P1), Leaky ReLU  Hidden Layer (슬, 2,64) → (7, w4, 128) CONV-(N128, K4x4, S2, P1), Leaky ReLU  Hidden Layer (문 , w, 128) → (승, �, 256) CONV-(N256, K4x4, S2, P1), Leaky ReLU  Hidden Layer w 512) (승 , (0,256) → (음, 16, CONV-(N512, K4x4, S2, P1), Leaky ReLU  Hidden Layer (음, 16,512) → (�, w 1024) 32' CONV-(N1024, K4x4, S2,P1), Leaky ReLU  Hidden Layer h 332 , 1024) → ( h 64, 2048) ( 32, 64, CONV-(N2048, K4x4, S2,P1), Leaky ReLU  Output Layer (Dsrc) (습, (2048) → (승, 64, 1) CONV-(N1, K3x3,S1,P1)  Output Layer (Dcls) ( he , 64 , 2048) → (1,1,nd)"
        },
        {
            "bounding_box": [
                {
                    "x": 907,
                    "y": 2856
                },
                {
                    "x": 1571,
                    "y": 2856
                },
                {
                    "x": 1571,
                    "y": 2900
                },
                {
                    "x": 907,
                    "y": 2900
                }
            ],
            "category": "caption",
            "html": "<caption id='123' style='font-size:14px'>Table 5. Discriminator network architecture</caption>",
            "id": 123,
            "page": 11,
            "text": "Table 5. Discriminator network architecture"
        },
        {
            "bounding_box": [
                {
                    "x": 1217,
                    "y": 3052
                },
                {
                    "x": 1262,
                    "y": 3052
                },
                {
                    "x": 1262,
                    "y": 3094
                },
                {
                    "x": 1217,
                    "y": 3094
                }
            ],
            "category": "footer",
            "html": "<footer id='124' style='font-size:16px'>11</footer>",
            "id": 124,
            "page": 11,
            "text": "11"
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 305
                },
                {
                    "x": 883,
                    "y": 305
                },
                {
                    "x": 883,
                    "y": 353
                },
                {
                    "x": 204,
                    "y": 353
                }
            ],
            "category": "paragraph",
            "html": "<p id='125' style='font-size:20px'>7.3. Additional Qualitative Results</p>",
            "id": 125,
            "page": 12,
            "text": "7.3. Additional Qualitative Results"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 385
                },
                {
                    "x": 2276,
                    "y": 385
                },
                {
                    "x": 2276,
                    "y": 535
                },
                {
                    "x": 201,
                    "y": 535
                }
            ],
            "category": "paragraph",
            "html": "<p id='126' style='font-size:18px'>Figs. 9, 10, 11, and 12 show additional images with 256 x 256 resolutions generated by StarGAN. All images were gener-<br>ated by a single generator trained on both the CelebA and RaFD datasets. We trained StarGAN on a single NVIDIA Pascal<br>M40 GPU for seven days.</p>",
            "id": 126,
            "page": 12,
            "text": "Figs. 9, 10, 11, and 12 show additional images with 256 x 256 resolutions generated by StarGAN. All images were generated by a single generator trained on both the CelebA and RaFD datasets. We trained StarGAN on a single NVIDIA Pascal M40 GPU for seven days."
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 589
                },
                {
                    "x": 2277,
                    "y": 589
                },
                {
                    "x": 2277,
                    "y": 2872
                },
                {
                    "x": 199,
                    "y": 2872
                }
            ],
            "category": "figure",
            "html": "<figure><img id='127' style='font-size:14px' alt=\"ress ress ess ress ress res: ress ress ress ress\nP P P P\nIOSTRA MOSTRA MOSTRAI! HOSTRA MOSTRA MOSTRA MOSTRAI MOSTRA RIOSTRA MOSTRA\n'ARTECIN 'ARTECR 'ARTECIP -ARTECIA ~ARTECR -ARTECO ~ARTECH 'ARTECI ARTECIA -ARTECIA\nBiennale Biennalo Biennal Hiennal\nOD OD OD OD OD OD OD OD OD OD\n프로 SHIS MUS\" data-coord=\"top-left:(199,589); bottom-right:(2277,2872)\" /></figure>",
            "id": 127,
            "page": 12,
            "text": "ress ress ess ress ress res: ress ress ress ress P P P P IOSTRA MOSTRA MOSTRAI! HOSTRA MOSTRA MOSTRA MOSTRAI MOSTRA RIOSTRA MOSTRA \"ARTECIN \"ARTECR \"ARTECIP -ARTECIA ~ARTECR -ARTECO ~ARTECH \"ARTECI ARTECIA -ARTECIA Biennale Biennalo Biennal Hiennal OD OD OD OD OD OD OD OD OD OD 프로 SHIS MUS"
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 2876
                },
                {
                    "x": 2274,
                    "y": 2876
                },
                {
                    "x": 2274,
                    "y": 2967
                },
                {
                    "x": 204,
                    "y": 2967
                }
            ],
            "category": "caption",
            "html": "<br><caption id='128' style='font-size:16px'>Figure 9. Single and multiple attribute transfer on CelebA (Input, Black hair, Blond hair, Brown hair, Gender, Aged, Hair color + Gender,<br>Hair color + Aged, Gender + Aged, Hair color + Gender + Aged).</caption>",
            "id": 128,
            "page": 12,
            "text": "Figure 9. Single and multiple attribute transfer on CelebA (Input, Black hair, Blond hair, Brown hair, Gender, Aged, Hair color + Gender, Hair color + Aged, Gender + Aged, Hair color + Gender + Aged)."
        },
        {
            "bounding_box": [
                {
                    "x": 1216,
                    "y": 3052
                },
                {
                    "x": 1264,
                    "y": 3052
                },
                {
                    "x": 1264,
                    "y": 3095
                },
                {
                    "x": 1216,
                    "y": 3095
                }
            ],
            "category": "footer",
            "html": "<footer id='129' style='font-size:18px'>12</footer>",
            "id": 129,
            "page": 12,
            "text": "12"
        },
        {
            "bounding_box": [
                {
                    "x": 198,
                    "y": 291
                },
                {
                    "x": 2279,
                    "y": 291
                },
                {
                    "x": 2279,
                    "y": 2817
                },
                {
                    "x": 198,
                    "y": 2817
                }
            ],
            "category": "figure",
            "html": "<figure><img id='130' style='font-size:14px' alt=\"U read ady ady OU ready\nwom DY wom DY wom DY wom DY wom DY wom DY wom DY wom DY wom DY\nY! Y! Y! Y! Y! Y! Y!\nButt Butt Butt Butt Butt Butt Butt Butt Butt\" data-coord=\"top-left:(198,291); bottom-right:(2279,2817)\" /></figure>",
            "id": 130,
            "page": 13,
            "text": "U read ady ady OU ready wom DY wom DY wom DY wom DY wom DY wom DY wom DY wom DY wom DY Y! Y! Y! Y! Y! Y! Y! Butt Butt Butt Butt Butt Butt Butt Butt Butt"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2833
                },
                {
                    "x": 2247,
                    "y": 2833
                },
                {
                    "x": 2247,
                    "y": 2878
                },
                {
                    "x": 201,
                    "y": 2878
                }
            ],
            "category": "caption",
            "html": "<br><caption id='131' style='font-size:18px'>Figure 10. Single attribute transfer on CelebA (Input, Black hair, Blond hair, Brown hair, Gender, Mouth, Pale skin, Rose cheek, Aged).</caption>",
            "id": 131,
            "page": 13,
            "text": "Figure 10. Single attribute transfer on CelebA (Input, Black hair, Blond hair, Brown hair, Gender, Mouth, Pale skin, Rose cheek, Aged)."
        },
        {
            "bounding_box": [
                {
                    "x": 1217,
                    "y": 3052
                },
                {
                    "x": 1263,
                    "y": 3052
                },
                {
                    "x": 1263,
                    "y": 3093
                },
                {
                    "x": 1217,
                    "y": 3093
                }
            ],
            "category": "footer",
            "html": "<footer id='132' style='font-size:18px'>13</footer>",
            "id": 132,
            "page": 13,
            "text": "13"
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 332
                },
                {
                    "x": 2279,
                    "y": 332
                },
                {
                    "x": 2279,
                    "y": 2868
                },
                {
                    "x": 193,
                    "y": 2868
                }
            ],
            "category": "figure",
            "html": "<figure><img id='133' alt=\"\" data-coord=\"top-left:(193,332); bottom-right:(2279,2868)\" /></figure>",
            "id": 133,
            "page": 14,
            "text": ""
        },
        {
            "bounding_box": [
                {
                    "x": 208,
                    "y": 2875
                },
                {
                    "x": 2244,
                    "y": 2875
                },
                {
                    "x": 2244,
                    "y": 2923
                },
                {
                    "x": 208,
                    "y": 2923
                }
            ],
            "category": "caption",
            "html": "<br><caption id='134' style='font-size:18px'>Figure 11. Emotional expression synthesis on RaFD (Input, Angry, Contemptuous, Disgusted, Fearful, Happy, Neutral, Sad, Surprised ).</caption>",
            "id": 134,
            "page": 14,
            "text": "Figure 11. Emotional expression synthesis on RaFD (Input, Angry, Contemptuous, Disgusted, Fearful, Happy, Neutral, Sad, Surprised )."
        },
        {
            "bounding_box": [
                {
                    "x": 1218,
                    "y": 3055
                },
                {
                    "x": 1263,
                    "y": 3055
                },
                {
                    "x": 1263,
                    "y": 3092
                },
                {
                    "x": 1218,
                    "y": 3092
                }
            ],
            "category": "footer",
            "html": "<footer id='135' style='font-size:14px'>14</footer>",
            "id": 135,
            "page": 14,
            "text": "14"
        },
        {
            "bounding_box": [
                {
                    "x": 191,
                    "y": 321
                },
                {
                    "x": 2281,
                    "y": 321
                },
                {
                    "x": 2281,
                    "y": 2861
                },
                {
                    "x": 191,
                    "y": 2861
                }
            ],
            "category": "figure",
            "html": "<figure><img id='136' style='font-size:14px' alt=\"R R\n아 아\nR R R R R R R R\" data-coord=\"top-left:(191,321); bottom-right:(2281,2861)\" /></figure>",
            "id": 136,
            "page": 15,
            "text": "R R 아 아 R R R R R R R R"
        },
        {
            "bounding_box": [
                {
                    "x": 207,
                    "y": 2877
                },
                {
                    "x": 2258,
                    "y": 2877
                },
                {
                    "x": 2258,
                    "y": 2921
                },
                {
                    "x": 207,
                    "y": 2921
                }
            ],
            "category": "caption",
            "html": "<br><caption id='137' style='font-size:20px'>Figure 12. Emotional expression synthesis on CelebA (Input, Angry, Contemptuous, Disgusted, Fearful, Happy, Neutral, Sad, Surprised).</caption>",
            "id": 137,
            "page": 15,
            "text": "Figure 12. Emotional expression synthesis on CelebA (Input, Angry, Contemptuous, Disgusted, Fearful, Happy, Neutral, Sad, Surprised)."
        },
        {
            "bounding_box": [
                {
                    "x": 1218,
                    "y": 3054
                },
                {
                    "x": 1263,
                    "y": 3054
                },
                {
                    "x": 1263,
                    "y": 3091
                },
                {
                    "x": 1218,
                    "y": 3091
                }
            ],
            "category": "footer",
            "html": "<footer id='138' style='font-size:16px'>15</footer>",
            "id": 138,
            "page": 15,
            "text": "15"
        }
    ]
}