{
    "id": "32a7df34-0f94-11ef-8828-426932df3dcf",
    "pdf_path": "./pdf/AI_VIT_X/2109.00122v3.pdf",
    "elements": [
        {
            "bounding_box": [
                {
                    "x": 370,
                    "y": 281
                },
                {
                    "x": 2119,
                    "y": 281
                },
                {
                    "x": 2119,
                    "y": 555
                },
                {
                    "x": 370,
                    "y": 555
                }
            ],
            "category": "paragraph",
            "html": "<p id='0' style='font-size:22px'>FINQA: A Dataset of Numerical Reasoning over Financial Data<br>Zhiyu Chen1, Wenhu Chen1, Charese Smiley2, Sameena Shah2,<br>Iana Boroval , Dylan Langdon1 , Reema Moussa1 , Matt Beane1 . Ting-Hao Huang3,<br>Bryan Routledge4 and William Yang Wang1</p>",
            "id": 0,
            "page": 1,
            "text": "FINQA: A Dataset of Numerical Reasoning over Financial Data Zhiyu Chen1, Wenhu Chen1, Charese Smiley2, Sameena Shah2, Iana Boroval , Dylan Langdon1 , Reema Moussa1 , Matt Beane1 . Ting-Hao Huang3, Bryan Routledge4 and William Yang Wang1"
        },
        {
            "bounding_box": [
                {
                    "x": 837,
                    "y": 550
                },
                {
                    "x": 1656,
                    "y": 550
                },
                {
                    "x": 1656,
                    "y": 620
                },
                {
                    "x": 837,
                    "y": 620
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='1' style='font-size:22px'>1University of California, Santa Barbara</p>",
            "id": 1,
            "page": 1,
            "text": "1University of California, Santa Barbara"
        },
        {
            "bounding_box": [
                {
                    "x": 770,
                    "y": 601
                },
                {
                    "x": 1710,
                    "y": 601
                },
                {
                    "x": 1710,
                    "y": 844
                },
                {
                    "x": 770,
                    "y": 844
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='2' style='font-size:20px'>2J.P. Morgan<br>3Pennsylvania State University<br>4Carnegie Mellon University<br>{ zhi yuchen, wi lli am } @cs · ucsb · edu</p>",
            "id": 2,
            "page": 1,
            "text": "2J.P. Morgan 3Pennsylvania State University 4Carnegie Mellon University { zhi yuchen, wi lli am } @cs · ucsb · edu"
        },
        {
            "bounding_box": [
                {
                    "x": 650,
                    "y": 889
                },
                {
                    "x": 848,
                    "y": 889
                },
                {
                    "x": 848,
                    "y": 942
                },
                {
                    "x": 650,
                    "y": 942
                }
            ],
            "category": "paragraph",
            "html": "<p id='3' style='font-size:16px'>Abstract</p>",
            "id": 3,
            "page": 1,
            "text": "Abstract"
        },
        {
            "bounding_box": [
                {
                    "x": 355,
                    "y": 963
                },
                {
                    "x": 1146,
                    "y": 963
                },
                {
                    "x": 1146,
                    "y": 2316
                },
                {
                    "x": 355,
                    "y": 2316
                }
            ],
            "category": "paragraph",
            "html": "<p id='4' style='font-size:14px'>The sheer volume of financial statements<br>makes it difficult for humans to access and an-<br>alyze a business's financials. Robust numeri-<br>cal reasoning likewise faces unique challenges<br>in this domain. In this work, we focus on<br>answering deep questions over financial data,<br>aiming to automate the analysis of a large cor-<br>pus of financial documents. In contrast to ex-<br>isting tasks on general domain, the finance do-<br>main includes complex numerical reasoning<br>and understanding of heterogeneous represen-<br>tations. To facilitate analytical progress, we<br>propose a new large-scale dataset, FINQA,<br>with Question-Answering pairs over Financial<br>reports, written by financial experts. We also<br>annotate the gold reasoning programs to en-<br>sure full explainability. We further introduce<br>baselines and conduct comprehensive experi-<br>ments in our dataset. The results demonstrate<br>that popular, large, pre-trained models fall far<br>short of expert humans in acquiring finance<br>knowledge and in complex multi-step numer-<br>ical reasoning on that knowledge. Our dataset<br>the first of its kind - should therefore en-<br>able significant, new community research into<br>complex application domains. The dataset and<br>code are publicly available1.</p>",
            "id": 4,
            "page": 1,
            "text": "The sheer volume of financial statements makes it difficult for humans to access and analyze a business's financials. Robust numerical reasoning likewise faces unique challenges in this domain. In this work, we focus on answering deep questions over financial data, aiming to automate the analysis of a large corpus of financial documents. In contrast to existing tasks on general domain, the finance domain includes complex numerical reasoning and understanding of heterogeneous representations. To facilitate analytical progress, we propose a new large-scale dataset, FINQA, with Question-Answering pairs over Financial reports, written by financial experts. We also annotate the gold reasoning programs to ensure full explainability. We further introduce baselines and conduct comprehensive experiments in our dataset. The results demonstrate that popular, large, pre-trained models fall far short of expert humans in acquiring finance knowledge and in complex multi-step numerical reasoning on that knowledge. Our dataset the first of its kind - should therefore enable significant, new community research into complex application domains. The dataset and code are publicly available1."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 2335
                },
                {
                    "x": 647,
                    "y": 2335
                },
                {
                    "x": 647,
                    "y": 2392
                },
                {
                    "x": 290,
                    "y": 2392
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='5' style='font-size:20px'>1 Introduction</p>",
            "id": 5,
            "page": 1,
            "text": "1 Introduction"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 2417
                },
                {
                    "x": 1215,
                    "y": 2417
                },
                {
                    "x": 1215,
                    "y": 3156
                },
                {
                    "x": 287,
                    "y": 3156
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='6' style='font-size:18px'>Financial analysis is a critical means of assessing<br>business performance, and the consequences of<br>poor analysis can involve costs of billions of dol-<br>lars (Jerven, 2013; MacKenzie, 2008). To facilitate<br>high quality, timely decision making, profession-<br>als such as analysts or investors perform<br>complex quantitative analysis to select informa-<br>tion from financial reports. Such analysis demands<br>advanced expertise in reasoning among heteroge-<br>neous (structured and unstructured) data sources<br>and performing complex numerical reasoning, for<br>example, comparing financial ratios of profitabil-<br>ity or growth. These challenges are compounded</p>",
            "id": 6,
            "page": 1,
            "text": "Financial analysis is a critical means of assessing business performance, and the consequences of poor analysis can involve costs of billions of dollars (Jerven, 2013; MacKenzie, 2008). To facilitate high quality, timely decision making, professionals such as analysts or investors perform complex quantitative analysis to select information from financial reports. Such analysis demands advanced expertise in reasoning among heterogeneous (structured and unstructured) data sources and performing complex numerical reasoning, for example, comparing financial ratios of profitability or growth. These challenges are compounded"
        },
        {
            "bounding_box": [
                {
                    "x": 339,
                    "y": 3174
                },
                {
                    "x": 1088,
                    "y": 3174
                },
                {
                    "x": 1088,
                    "y": 3230
                },
                {
                    "x": 339,
                    "y": 3230
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='7' style='font-size:16px'>1https: / / github. com/ czyssrs/FinQA</p>",
            "id": 7,
            "page": 1,
            "text": "1https: / / github. com/ czyssrs/FinQA"
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 891
                },
                {
                    "x": 2196,
                    "y": 891
                },
                {
                    "x": 2196,
                    "y": 1283
                },
                {
                    "x": 1267,
                    "y": 1283
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='8' style='font-size:16px'>by an exponentially expanding collection of com-<br>pany financial documents (MacKenzie et al., 2012;<br>Lange et al., 2016) such that it is genuinely unclear<br>whether dedicated human effort can produce fiscal<br>analysis of sufficient quality for current decision<br>making. This poses an interesting question: can we<br>automate such deep analysis of financial data?</p>",
            "id": 8,
            "page": 1,
            "text": "by an exponentially expanding collection of company financial documents (MacKenzie , 2012; Lange , 2016) such that it is genuinely unclear whether dedicated human effort can produce fiscal analysis of sufficient quality for current decision making. This poses an interesting question: can we automate such deep analysis of financial data?"
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 1297
                },
                {
                    "x": 2199,
                    "y": 1297
                },
                {
                    "x": 2199,
                    "y": 2542
                },
                {
                    "x": 1267,
                    "y": 2542
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='9' style='font-size:16px'>A few NLP studies in Question Answering<br>(QA) explored the numerical reasoning capabilities<br>needed to answer questions correctly. For exam-<br>ple, the DROP dataset (Dua et al., 2019) focused<br>on Wikipedia-based questions that require numer-<br>ical reasoning, e.g., \"Where did Charles travel to<br>first, Castile or Barcelona?\" needs a comparison<br>between the times of two events. However, most<br>prior work only targeted the general domain, where<br>the questions involve much less calculation (mostly<br>one-step calculation) than that of the financial do-<br>main. Financial QA is more challenging than clas-<br>sic QA (Rajpurkar et al., 2018; Yang et al., 2018)<br>because it requires the system to spot relevant in-<br>formation across heterogeneous sources, such as<br>tables and unstructured texts, and then create a<br>numerical reasoning path to connect all the infor-<br>mation. It also takes substantial knowledge to ask<br>meaningful financial questions. It is not clear how<br>well the large language models, which performed<br>well for general-domain QA, can be adapted to<br>answer realistic, complex financial questions.</p>",
            "id": 9,
            "page": 1,
            "text": "A few NLP studies in Question Answering (QA) explored the numerical reasoning capabilities needed to answer questions correctly. For example, the DROP dataset (Dua , 2019) focused on Wikipedia-based questions that require numerical reasoning, e.g., \"Where did Charles travel to first, Castile or Barcelona?\" needs a comparison between the times of two events. However, most prior work only targeted the general domain, where the questions involve much less calculation (mostly one-step calculation) than that of the financial domain. Financial QA is more challenging than classic QA (Rajpurkar , 2018; Yang , 2018) because it requires the system to spot relevant information across heterogeneous sources, such as tables and unstructured texts, and then create a numerical reasoning path to connect all the information. It also takes substantial knowledge to ask meaningful financial questions. It is not clear how well the large language models, which performed well for general-domain QA, can be adapted to answer realistic, complex financial questions."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 2554
                },
                {
                    "x": 2200,
                    "y": 2554
                },
                {
                    "x": 2200,
                    "y": 3230
                },
                {
                    "x": 1267,
                    "y": 3230
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='10' style='font-size:16px'>This paper introduces FINQA, a expert-<br>annotated dataset that contains 8,281 financial QA<br>pairs, along with their numerical reasoning pro-<br>cesses. Eleven finance professionals collectively<br>constructed FINQA based on the earnings reports<br>of S&P 500 companies (Zheng et al., 2021). The<br>questions in FINQA, such as \"Considering the<br>weighted average fair value of options, what was<br>the change of shares vested from 2005 to 2006?\"<br>(Figure 1) and \"What was the net change in tax<br>positions in 2014?\", require information from both<br>tables and unstructured texts to answer. The reason-</p>",
            "id": 10,
            "page": 1,
            "text": "This paper introduces FINQA, a expertannotated dataset that contains 8,281 financial QA pairs, along with their numerical reasoning processes. Eleven finance professionals collectively constructed FINQA based on the earnings reports of S&P 500 companies (Zheng , 2021). The questions in FINQA, such as \"Considering the weighted average fair value of options, what was the change of shares vested from 2005 to 2006?\" (Figure 1) and \"What was the net change in tax positions in 2014?\", require information from both tables and unstructured texts to answer. The reason-"
        },
        {
            "bounding_box": [
                {
                    "x": 57,
                    "y": 1087
                },
                {
                    "x": 147,
                    "y": 1087
                },
                {
                    "x": 147,
                    "y": 2517
                },
                {
                    "x": 57,
                    "y": 2517
                }
            ],
            "category": "footer",
            "html": "<br><footer id='11' style='font-size:14px'>2022<br>May<br>L<br>[cs.CL]<br>arxi7:1199012.12.0</footer>",
            "id": 11,
            "page": 1,
            "text": "2022 May L [cs.CL] arxi7:1199012.12.0"
        },
        {
            "bounding_box": [
                {
                    "x": 319,
                    "y": 299
                },
                {
                    "x": 979,
                    "y": 299
                },
                {
                    "x": 979,
                    "y": 337
                },
                {
                    "x": 319,
                    "y": 337
                }
            ],
            "category": "paragraph",
            "html": "<p id='12' style='font-size:16px'>Page 91 from the annual reports of GRMN (Garmin Ltd.)</p>",
            "id": 12,
            "page": 2,
            "text": "Page 91 from the annual reports of GRMN (Garmin Ltd.)"
        },
        {
            "bounding_box": [
                {
                    "x": 317,
                    "y": 335
                },
                {
                    "x": 1174,
                    "y": 335
                },
                {
                    "x": 1174,
                    "y": 375
                },
                {
                    "x": 317,
                    "y": 375
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='13' style='font-size:14px'>The fair value for these options was estimated at the date of grant using a</p>",
            "id": 13,
            "page": 2,
            "text": "The fair value for these options was estimated at the date of grant using a"
        },
        {
            "bounding_box": [
                {
                    "x": 319,
                    "y": 372
                },
                {
                    "x": 1306,
                    "y": 372
                },
                {
                    "x": 1306,
                    "y": 410
                },
                {
                    "x": 319,
                    "y": 410
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='14' style='font-size:16px'>Black-Scholes option pricing model with the following weighted-average assumptions</p>",
            "id": 14,
            "page": 2,
            "text": "Black-Scholes option pricing model with the following weighted-average assumptions"
        },
        {
            "bounding_box": [
                {
                    "x": 316,
                    "y": 409
                },
                {
                    "x": 621,
                    "y": 409
                },
                {
                    "x": 621,
                    "y": 444
                },
                {
                    "x": 316,
                    "y": 444
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='15' style='font-size:14px'>for 2006, 2005 and 2004:</p>",
            "id": 15,
            "page": 2,
            "text": "for 2006, 2005 and 2004:"
        },
        {
            "bounding_box": [
                {
                    "x": 298,
                    "y": 430
                },
                {
                    "x": 1288,
                    "y": 430
                },
                {
                    "x": 1288,
                    "y": 766
                },
                {
                    "x": 298,
                    "y": 766
                }
            ],
            "category": "table",
            "html": "<br><table id='16' style='font-size:16px'><tr><td></td><td>2006</td><td>2005</td><td>2004</td></tr><tr><td>Weighted average fair value of options granted</td><td>$20.01</td><td>$9.48</td><td>$7.28</td></tr><tr><td>Expected volatility</td><td>0.3534</td><td>0.3224</td><td>0.3577</td></tr><tr><td>Distribution yield</td><td>1.00%</td><td>0.98%</td><td>1.30%</td></tr><tr><td>Expected life of options in years</td><td>6.3</td><td>6.3</td><td>6.3</td></tr><tr><td>Risk-free interest rate</td><td>5%</td><td>4%</td><td>4%</td></tr></table>",
            "id": 16,
            "page": 2,
            "text": "2006 2005 2004  Weighted average fair value of options granted $20.01 $9.48 $7.28  Expected volatility 0.3534 0.3224 0.3577  Distribution yield 1.00% 0.98% 1.30%  Expected life of options in years 6.3 6.3 6.3  Risk-free interest rate 5% 4%"
        },
        {
            "bounding_box": [
                {
                    "x": 321,
                    "y": 768
                },
                {
                    "x": 1291,
                    "y": 768
                },
                {
                    "x": 1291,
                    "y": 912
                },
                {
                    "x": 321,
                    "y": 912
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='17' style='font-size:14px'>The total fair value of shares vested during 2006, 2005, and 2004 was $9,413,<br>$8,249, and $6,418 respectively. The aggregate intrinsic values of options<br>outstanding and exercisable at December 30, 2006 were $204. 1 million and $100.2<br>million, respectively. ( abbreviate 10 sentences )</p>",
            "id": 17,
            "page": 2,
            "text": "The total fair value of shares vested during 2006, 2005, and 2004 was $9,413, $8,249, and $6,418 respectively. The aggregate intrinsic values of options outstanding and exercisable at December 30, 2006 were $204. 1 million and $100.2 million, respectively. ( abbreviate 10 sentences )"
        },
        {
            "bounding_box": [
                {
                    "x": 1360,
                    "y": 338
                },
                {
                    "x": 2189,
                    "y": 338
                },
                {
                    "x": 2189,
                    "y": 897
                },
                {
                    "x": 1360,
                    "y": 897
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='18' style='font-size:16px' alt=\"Question: Considering the weighted average fair value of options ,\nwhat was the change of shares vested from 2005 to 2006?\nAnswer: - 400\nCalculations:\n9413 8249\n( ) - ( ) = - 400\n20.01 9.48\nProgram:\ndivide ( 9413, 20.01 ) divide ( 8249, 9.48 )\nsubstract ( #0, #1 )\" data-coord=\"top-left:(1360,338); bottom-right:(2189,897)\" /></figure>",
            "id": 18,
            "page": 2,
            "text": "Question: Considering the weighted average fair value of options , what was the change of shares vested from 2005 to 2006? Answer: - 400 Calculations: 9413 8249 ( ) - ( ) = - 400 20.01 9.48 Program: divide ( 9413, 20.01 ) divide ( 8249, 9.48 ) substract ( #0, #1 )"
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 955
                },
                {
                    "x": 2190,
                    "y": 955
                },
                {
                    "x": 2190,
                    "y": 1040
                },
                {
                    "x": 293,
                    "y": 1040
                }
            ],
            "category": "caption",
            "html": "<caption id='19' style='font-size:18px'>Figure 1: An example from FINQA: The system needs to learn how to calculate the number of shares, then select relevant<br>numbers from both the table and the text to generate the reasoning program to get the answer.</caption>",
            "id": 19,
            "page": 2,
            "text": "Figure 1: An example from FINQA: The system needs to learn how to calculate the number of shares, then select relevant numbers from both the table and the text to generate the reasoning program to get the answer."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 1132
                },
                {
                    "x": 1213,
                    "y": 1132
                },
                {
                    "x": 1213,
                    "y": 1466
                },
                {
                    "x": 288,
                    "y": 1466
                }
            ],
            "category": "paragraph",
            "html": "<p id='20' style='font-size:20px'>ing processes answering these questions are made<br>of many common calculations in financial analysis,<br>such as addition, comparison, and table aggrega-<br>tion. To the best of our knowledge, FINQA is the<br>first dataset of its kind to tackle complicated QA<br>tasks based on the real-world financial documents.</p>",
            "id": 20,
            "page": 2,
            "text": "ing processes answering these questions are made of many common calculations in financial analysis, such as addition, comparison, and table aggregation. To the best of our knowledge, FINQA is the first dataset of its kind to tackle complicated QA tasks based on the real-world financial documents."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 1481
                },
                {
                    "x": 1215,
                    "y": 1481
                },
                {
                    "x": 1215,
                    "y": 2156
                },
                {
                    "x": 288,
                    "y": 2156
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='21' style='font-size:20px'>We propose a retriever-generator QA framework<br>to first retrieve supporting facts from financial re-<br>ports, then to generate executable reasoning pro-<br>grams to answer the questions. Equipped with pre-<br>trained language models, such as BERT (Devlin<br>et al., 2019) and RoBERTa (Liu et al., 2019), our<br>proposed approach outperforms all other baselines<br>and achieves an execution accuracy of 65.05%.<br>Although our system outperforms the non-expert<br>crowd (50.68%), the significant accuracy gap be-<br>tween the model and human experts (91.16%) mo-<br>tivates the need for future research.</p>",
            "id": 21,
            "page": 2,
            "text": "We propose a retriever-generator QA framework to first retrieve supporting facts from financial reports, then to generate executable reasoning programs to answer the questions. Equipped with pretrained language models, such as BERT (Devlin , 2019) and RoBERTa (Liu , 2019), our proposed approach outperforms all other baselines and achieves an execution accuracy of 65.05%. Although our system outperforms the non-expert crowd (50.68%), the significant accuracy gap between the model and human experts (91.16%) motivates the need for future research."
        },
        {
            "bounding_box": [
                {
                    "x": 333,
                    "y": 2171
                },
                {
                    "x": 1213,
                    "y": 2171
                },
                {
                    "x": 1213,
                    "y": 2222
                },
                {
                    "x": 333,
                    "y": 2222
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='22' style='font-size:18px'>The main contribution of this work is three-fold:</p>",
            "id": 22,
            "page": 2,
            "text": "The main contribution of this work is three-fold:"
        },
        {
            "bounding_box": [
                {
                    "x": 343,
                    "y": 2322
                },
                {
                    "x": 1214,
                    "y": 2322
                },
                {
                    "x": 1214,
                    "y": 2660
                },
                {
                    "x": 343,
                    "y": 2660
                }
            ],
            "category": "paragraph",
            "html": "<p id='23' style='font-size:20px'>● We propose the task of QA over financial data<br>to assist financial analysis. The task empha-<br>sizes an important phenomenon for the NLP<br>community to study and analyze how the cur-<br>rent pre-trained models perform on complex<br>and specialized domains.</p>",
            "id": 23,
            "page": 2,
            "text": "● We propose the task of QA over financial data to assist financial analysis. The task emphasizes an important phenomenon for the NLP community to study and analyze how the current pre-trained models perform on complex and specialized domains."
        },
        {
            "bounding_box": [
                {
                    "x": 343,
                    "y": 2751
                },
                {
                    "x": 1214,
                    "y": 2751
                },
                {
                    "x": 1214,
                    "y": 2973
                },
                {
                    "x": 343,
                    "y": 2973
                }
            ],
            "category": "paragraph",
            "html": "<p id='24' style='font-size:20px'>● We construct a new large-scale dataset,<br>FINQA, with 8,281 examples written by finan-<br>cial experts, with fully annotated numerical<br>reasoning programs.</p>",
            "id": 24,
            "page": 2,
            "text": "● We construct a new large-scale dataset, FINQA, with 8,281 examples written by financial experts, with fully annotated numerical reasoning programs."
        },
        {
            "bounding_box": [
                {
                    "x": 347,
                    "y": 3060
                },
                {
                    "x": 1215,
                    "y": 3060
                },
                {
                    "x": 1215,
                    "y": 3228
                },
                {
                    "x": 347,
                    "y": 3228
                }
            ],
            "category": "paragraph",
            "html": "<p id='25' style='font-size:18px'>● We experiment on various baselines and find<br>that the models are still far behind expert per-<br>formance, strongly motivating future research.</p>",
            "id": 25,
            "page": 2,
            "text": "● We experiment on various baselines and find that the models are still far behind expert performance, strongly motivating future research."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 1128
                },
                {
                    "x": 1649,
                    "y": 1128
                },
                {
                    "x": 1649,
                    "y": 1178
                },
                {
                    "x": 1271,
                    "y": 1178
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='26' style='font-size:22px'>2 Related Work</p>",
            "id": 26,
            "page": 2,
            "text": "2 Related Work"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 1214
                },
                {
                    "x": 2197,
                    "y": 1214
                },
                {
                    "x": 2197,
                    "y": 2799
                },
                {
                    "x": 1268,
                    "y": 2799
                }
            ],
            "category": "paragraph",
            "html": "<p id='27' style='font-size:20px'>Questions Answering. There have been several<br>QA datasets involving numerical understandings<br>and calculations. The major source is from struc-<br>tured tables or knowledge bases, owning the nature<br>to succinctly organize numerical information. Pop-<br>ular datasets include Complex WebQuestions (Tal-<br>mor and Berant, 2018), WikiTableQuestions (Pa-<br>supat and Liang, 2015), Spider (Yu et al., 2018),<br>TabFact (Chen et al., 2020b), etc. For reading com-<br>prehension, the dataset most related to ours is the<br>DROP dataset (Dua et al., 2019), which applies<br>simple calculations over texts. The top methods on<br>DROP typically use specific prediction heads for<br>each kind of calculation. HybridQA (Chen et al.,<br>2020c) targets QA over both the table and the text,<br>but not with the focus of numerical reasoning. All<br>these existing datasets are built upon the general do-<br>main (mostly based on Wikipedia). In contrast, our<br>dataset focus on the finance domain, which demon-<br>strates much more complex nature in numerical<br>reasoning questions, combining both the structured<br>tables and unstructured texts. Another kind of QA<br>datasets related to ours is the math word problem<br>datasets, like MaWPS (Koncel-Kedziorski et al.,<br>2016), MathQA (Amini et al., 2019). The task is to<br>generate the solution programs given a short input<br>math problem. Existing models include (Kim et al.,<br>2020; Chen et al., 2020a,d), etc.</p>",
            "id": 27,
            "page": 2,
            "text": "Questions Answering. There have been several QA datasets involving numerical understandings and calculations. The major source is from structured tables or knowledge bases, owning the nature to succinctly organize numerical information. Popular datasets include Complex WebQuestions (Talmor and Berant, 2018), WikiTableQuestions (Pasupat and Liang, 2015), Spider (Yu , 2018), TabFact (Chen , 2020b), etc. For reading comprehension, the dataset most related to ours is the DROP dataset (Dua , 2019), which applies simple calculations over texts. The top methods on DROP typically use specific prediction heads for each kind of calculation. HybridQA (Chen , 2020c) targets QA over both the table and the text, but not with the focus of numerical reasoning. All these existing datasets are built upon the general domain (mostly based on Wikipedia). In contrast, our dataset focus on the finance domain, which demonstrates much more complex nature in numerical reasoning questions, combining both the structured tables and unstructured texts. Another kind of QA datasets related to ours is the math word problem datasets, like MaWPS (Koncel-Kedziorski , 2016), MathQA (Amini , 2019). The task is to generate the solution programs given a short input math problem. Existing models include (Kim , 2020; Chen , 2020a,d), etc."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 2833
                },
                {
                    "x": 2197,
                    "y": 2833
                },
                {
                    "x": 2197,
                    "y": 3229
                },
                {
                    "x": 1268,
                    "y": 3229
                }
            ],
            "category": "paragraph",
            "html": "<p id='28' style='font-size:22px'>Financial NLP. Financial NLP has become one<br>of the major application domains attracting grow-<br>ing attentions. Previous works in finance domain<br>include risk management to detect fraud (Han et al.,<br>2018; Wang et al., 2019; Nourbakhsh and Bang,<br>2019), sentiment analysis to assist market predic-<br>tion (Day and Lee, 2016; Wang et al., 2013; Akhtar</p>",
            "id": 28,
            "page": 2,
            "text": "Financial NLP. Financial NLP has become one of the major application domains attracting growing attentions. Previous works in finance domain include risk management to detect fraud (Han , 2018; Wang , 2019; Nourbakhsh and Bang, 2019), sentiment analysis to assist market prediction (Day and Lee, 2016; Wang , 2013; Akhtar"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 302
                },
                {
                    "x": 1212,
                    "y": 302
                },
                {
                    "x": 1212,
                    "y": 809
                },
                {
                    "x": 287,
                    "y": 809
                }
            ],
            "category": "paragraph",
            "html": "<p id='29' style='font-size:14px'>et al., 2017), opinionated Question Answering (Liu<br>et al., 2020), such as the FiQA2 dataset built from<br>forums and social media. Recent works attempt to<br>develop pre-trained models specialized for finance<br>domain (Yang et al., 2020; Araci, 2019), and the<br>downstream tasks are mostly sentiment classifica-<br>tions. To the best of our knowledge, there is no<br>previous work and dataset on building QA systems<br>of numerical reasoning on financial reports.</p>",
            "id": 29,
            "page": 3,
            "text": ", 2017), opinionated Question Answering (Liu , 2020), such as the FiQA2 dataset built from forums and social media. Recent works attempt to develop pre-trained models specialized for finance domain (Yang , 2020; Araci, 2019), and the downstream tasks are mostly sentiment classifications. To the best of our knowledge, there is no previous work and dataset on building QA systems of numerical reasoning on financial reports."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 853
                },
                {
                    "x": 697,
                    "y": 853
                },
                {
                    "x": 697,
                    "y": 901
                },
                {
                    "x": 289,
                    "y": 901
                }
            ],
            "category": "paragraph",
            "html": "<p id='30' style='font-size:16px'>3 Task Definition</p>",
            "id": 30,
            "page": 3,
            "text": "3 Task Definition"
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 939
                },
                {
                    "x": 1212,
                    "y": 939
                },
                {
                    "x": 1212,
                    "y": 1333
                },
                {
                    "x": 289,
                    "y": 1333
                }
            ],
            "category": "paragraph",
            "html": "<p id='31' style='font-size:14px'>Problem Formulation. Presented with a finan-<br>cial report consisting of textual contents E and<br>structured table T, given a question Q, the<br>task is to generate the reasoning program G =<br>{wo, w1, ...wn}, where Wi is the program tokens<br>defined by domain specific language (DSL), then it<br>is executed to get the answer A:</p>",
            "id": 31,
            "page": 3,
            "text": "Problem Formulation. Presented with a financial report consisting of textual contents E and structured table T, given a question Q, the task is to generate the reasoning program G = {wo, w1, ...wn}, where Wi is the program tokens defined by domain specific language (DSL), then it is executed to get the answer A:"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 1497
                },
                {
                    "x": 1214,
                    "y": 1497
                },
                {
                    "x": 1214,
                    "y": 2005
                },
                {
                    "x": 287,
                    "y": 2005
                }
            ],
            "category": "paragraph",
            "html": "<p id='32' style='font-size:14px'>Where {Gi} is all the correct programs to evaluate<br>to the answer. For financial tables, there is typi-<br>cally a description header (blue header in Figure 1),<br>which often gives the timing information; and each<br>row has its name on the left. Some of the financial<br>tables may demonstrate more complicated layouts,<br>e.g., nested structures. As a first step for this di-<br>rection, in this paper we only focus on the regular<br>layout cases for simplicity.</p>",
            "id": 32,
            "page": 3,
            "text": "Where {Gi} is all the correct programs to evaluate to the answer. For financial tables, there is typically a description header (blue header in Figure 1), which often gives the timing information; and each row has its name on the left. Some of the financial tables may demonstrate more complicated layouts, e.g., nested structures. As a first step for this direction, in this paper we only focus on the regular layout cases for simplicity."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 2036
                },
                {
                    "x": 1212,
                    "y": 2036
                },
                {
                    "x": 1212,
                    "y": 2263
                },
                {
                    "x": 288,
                    "y": 2263
                }
            ],
            "category": "paragraph",
            "html": "<p id='33' style='font-size:14px'>Domain Specific Language. In this work, we<br>use DSL consisting of mathematical operations<br>and table operations as executable programs. The<br>program consists of a sequence of operations:</p>",
            "id": 33,
            "page": 3,
            "text": "Domain Specific Language. In this work, we use DSL consisting of mathematical operations and table operations as executable programs. The program consists of a sequence of operations:"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 2414
                },
                {
                    "x": 1213,
                    "y": 2414
                },
                {
                    "x": 1213,
                    "y": 3148
                },
                {
                    "x": 287,
                    "y": 3148
                }
            ],
            "category": "paragraph",
            "html": "<p id='34' style='font-size:14px'>Each operation takes a list of arguments<br>argsn. On consulting with financial experts,<br>as most of the accounting and financial val-<br>uation theory primarily include linear algebra,<br>we include 10 common types of operations in<br>our dataset. There are 6 mathematical opera-<br>tions: add, subtract, multiply, divide,<br>greater, exp, and 4 table aggregation opera-<br>tions table-max, table-min, table-sum,<br>table-average, that apply aggregation opera-<br>tions on table rows. The mathematical operations<br>take arguments of either numbers from the given<br>reports, or a numerical result from a previous step;</p>",
            "id": 34,
            "page": 3,
            "text": "Each operation takes a list of arguments argsn. On consulting with financial experts, as most of the accounting and financial valuation theory primarily include linear algebra, we include 10 common types of operations in our dataset. There are 6 mathematical operations: add, subtract, multiply, divide, greater, exp, and 4 table aggregation operations table-max, table-min, table-sum, table-average, that apply aggregation operations on table rows. The mathematical operations take arguments of either numbers from the given reports, or a numerical result from a previous step;"
        },
        {
            "bounding_box": [
                {
                    "x": 340,
                    "y": 3177
                },
                {
                    "x": 966,
                    "y": 3177
                },
                {
                    "x": 966,
                    "y": 3225
                },
                {
                    "x": 340,
                    "y": 3225
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='35' style='font-size:22px'>Phttps://sies.google.com/view/fiqa/home</p>",
            "id": 35,
            "page": 3,
            "text": "Phttps://sies.google.com/view/fiqa/home"
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 300
                },
                {
                    "x": 2195,
                    "y": 300
                },
                {
                    "x": 2195,
                    "y": 808
                },
                {
                    "x": 1267,
                    "y": 808
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='36' style='font-size:14px'>The table operations take arguments of table row<br>names. We use the special token #n to denote the<br>result from the nth step. For example, in Figure 1,<br>the program consists of 3 steps; The first and the<br>second division steps take arguments from the table<br>and the text, respectively, then the third step sub-<br>tracts the results from the two previous steps. Refer<br>to Appendix A for more details of the operations<br>and the grammars.</p>",
            "id": 36,
            "page": 3,
            "text": "The table operations take arguments of table row names. We use the special token #n to denote the result from the nth step. For example, in Figure 1, the program consists of 3 steps; The first and the second division steps take arguments from the table and the text, respectively, then the third step subtracts the results from the two previous steps. Refer to Appendix A for more details of the operations and the grammars."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 837
                },
                {
                    "x": 2197,
                    "y": 837
                },
                {
                    "x": 2197,
                    "y": 1684
                },
                {
                    "x": 1267,
                    "y": 1684
                }
            ],
            "category": "paragraph",
            "html": "<p id='37' style='font-size:14px'>Evaluations. Previous studies on QA with nu-<br>merical reasoning only evaluate the execution ac-<br>curacy, i.e., the final results from the generated<br>programs, such as DROP (Dua et al., 2019) and<br>MathQA (Amini et al., 2019). However, the ap-<br>plications for the finance domain generally pose<br>much higher requirements of explainability and<br>transparency. Therefore, we also provide the gold<br>programs for our dataset. Besides execution accu-<br>racy, we also propose to evaluate the accuracy of<br>the generated programs. Specifically, we replace all<br>the arguments in a program with symbols, and then<br>we evaluate if two symbolic programs are mathe-<br>matically equivalent. For example, the following<br>two programs are equivalent programs:</p>",
            "id": 37,
            "page": 3,
            "text": "Evaluations. Previous studies on QA with numerical reasoning only evaluate the execution accuracy, i.e., the final results from the generated programs, such as DROP (Dua , 2019) and MathQA (Amini , 2019). However, the applications for the finance domain generally pose much higher requirements of explainability and transparency. Therefore, we also provide the gold programs for our dataset. Besides execution accuracy, we also propose to evaluate the accuracy of the generated programs. Specifically, we replace all the arguments in a program with symbols, and then we evaluate if two symbolic programs are mathematically equivalent. For example, the following two programs are equivalent programs:"
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 1871
                },
                {
                    "x": 2195,
                    "y": 1871
                },
                {
                    "x": 2195,
                    "y": 2152
                },
                {
                    "x": 1267,
                    "y": 2152
                }
            ],
            "category": "paragraph",
            "html": "<p id='38' style='font-size:14px'>Note that execution accuracy tends to overestimate<br>the performance because sometimes the model just<br>hit the correct answer by chance; While program ac-<br>curacy tends to produce false negatives since some<br>questions may have multiple correct programs.</p>",
            "id": 38,
            "page": 3,
            "text": "Note that execution accuracy tends to overestimate the performance because sometimes the model just hit the correct answer by chance; While program accuracy tends to produce false negatives since some questions may have multiple correct programs."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 2192
                },
                {
                    "x": 1784,
                    "y": 2192
                },
                {
                    "x": 1784,
                    "y": 2246
                },
                {
                    "x": 1268,
                    "y": 2246
                }
            ],
            "category": "paragraph",
            "html": "<p id='39' style='font-size:18px'>4 The FINQA Dataset</p>",
            "id": 39,
            "page": 3,
            "text": "4 The FINQA Dataset"
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 2279
                },
                {
                    "x": 1720,
                    "y": 2279
                },
                {
                    "x": 1720,
                    "y": 2332
                },
                {
                    "x": 1270,
                    "y": 2332
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='40' style='font-size:16px'>4.1 Data Preparation</p>",
            "id": 40,
            "page": 3,
            "text": "4.1 Data Preparation"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 2353
                },
                {
                    "x": 2196,
                    "y": 2353
                },
                {
                    "x": 2196,
                    "y": 2801
                },
                {
                    "x": 1268,
                    "y": 2801
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='41' style='font-size:14px'>Data Source. We develop FINQA based on the<br>publicly available earnings reports of S&P 500<br>companies from 1999 to 2019, collected in the<br>FinTabNet dataset (Zheng et al., 2021). An earn-<br>ings report is a set of pages in a PDF file that out-<br>lines the financials of a company, which usually<br>contains tables and texts. The FinTabNet dataset<br>has annotated the tables in each report.</p>",
            "id": 41,
            "page": 3,
            "text": "Data Source. We develop FINQA based on the publicly available earnings reports of S&P 500 companies from 1999 to 2019, collected in the FinTabNet dataset (Zheng , 2021). An earnings report is a set of pages in a PDF file that outlines the financials of a company, which usually contains tables and texts. The FinTabNet dataset has annotated the tables in each report."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 2832
                },
                {
                    "x": 2196,
                    "y": 2832
                },
                {
                    "x": 2196,
                    "y": 3229
                },
                {
                    "x": 1268,
                    "y": 3229
                }
            ],
            "category": "paragraph",
            "html": "<p id='42' style='font-size:14px'>Data Filtering. Realistic earnings reports con-<br>tain many tables not suitable for numerical reason-<br>ing tasks. Equipped with the table annotations in<br>FinTabNet, we filter the data as follows: First, we<br>extract the pages in earnings reports with at most<br>one table. Second, we exclude the tables with over<br>20 rows, over 2 description headers, or with other</p>",
            "id": 42,
            "page": 3,
            "text": "Data Filtering. Realistic earnings reports contain many tables not suitable for numerical reasoning tasks. Equipped with the table annotations in FinTabNet, we filter the data as follows: First, we extract the pages in earnings reports with at most one table. Second, we exclude the tables with over 20 rows, over 2 description headers, or with other"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 303
                },
                {
                    "x": 1213,
                    "y": 303
                },
                {
                    "x": 1213,
                    "y": 752
                },
                {
                    "x": 288,
                    "y": 752
                }
            ],
            "category": "paragraph",
            "html": "<p id='43' style='font-size:18px'>complex nested structures. We also exclude the ta-<br>bles with tedious contents, such as catalogs, which<br>is common in FinTabNet. As stated in §3, these<br>over-complicated tables are out of the scope of this<br>work. Finally, for the tables with 2 description<br>headers, we merge them into a single header to<br>simplify the representations. As a result, a total of<br>12,719 pages were selected for further annotation.</p>",
            "id": 43,
            "page": 4,
            "text": "complex nested structures. We also exclude the tables with tedious contents, such as catalogs, which is common in FinTabNet. As stated in §3, these over-complicated tables are out of the scope of this work. Finally, for the tables with 2 description headers, we merge them into a single header to simplify the representations. As a result, a total of 12,719 pages were selected for further annotation."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 792
                },
                {
                    "x": 834,
                    "y": 792
                },
                {
                    "x": 834,
                    "y": 844
                },
                {
                    "x": 288,
                    "y": 844
                }
            ],
            "category": "paragraph",
            "html": "<p id='44' style='font-size:18px'>4.2 Annotation Procedure</p>",
            "id": 44,
            "page": 4,
            "text": "4.2 Annotation Procedure"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 865
                },
                {
                    "x": 1212,
                    "y": 865
                },
                {
                    "x": 1212,
                    "y": 1429
                },
                {
                    "x": 287,
                    "y": 1429
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='45' style='font-size:20px'>Recruiting Expert Annotators. We postjob ads<br>on UpWork3 and hire eleven US-based experts with<br>professional finance backgrounds (CPAs, MBAs,<br>etc.) Each hire is interviewed using four exam-<br>ple report pages and asked to compose example<br>Q&A pairs. After hiring, each annotator first goes<br>through a training session to learn the task and the<br>annotation interface (Appendix D). When the work-<br>ers fully master the annotation process, we launch<br>the official batches for them to work on.</p>",
            "id": 45,
            "page": 4,
            "text": "Recruiting Expert Annotators. We postjob ads on UpWork3 and hire eleven US-based experts with professional finance backgrounds (CPAs, MBAs, etc.) Each hire is interviewed using four example report pages and asked to compose example Q&A pairs. After hiring, each annotator first goes through a training session to learn the task and the annotation interface (Appendix D). When the workers fully master the annotation process, we launch the official batches for them to work on."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 1435
                },
                {
                    "x": 1212,
                    "y": 1435
                },
                {
                    "x": 1212,
                    "y": 1767
                },
                {
                    "x": 287,
                    "y": 1767
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='46' style='font-size:18px'>An annotator can compose up to two questions<br>for each given report page or skip if it is hard to<br>compose any meaningful question. We pay around<br>$2.0 for each question, which leads to an average<br>hourly wage of $35.0. The whole data collection<br>took around eight weeks.</p>",
            "id": 46,
            "page": 4,
            "text": "An annotator can compose up to two questions for each given report page or skip if it is hard to compose any meaningful question. We pay around $2.0 for each question, which leads to an average hourly wage of $35.0. The whole data collection took around eight weeks."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 1775
                },
                {
                    "x": 1213,
                    "y": 1775
                },
                {
                    "x": 1213,
                    "y": 2390
                },
                {
                    "x": 287,
                    "y": 2390
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='47' style='font-size:20px'>We do not use popular micro-task platforms,<br>such as Amazon Mechanical Turk (MTurk), be-<br>cause our preliminary studies show that many<br>MTurk workers can not perform this task effec-<br>tively. Our experiment with MTurk workers in S 4.3<br>further echo this observation. As most existing QA<br>datasets were constructed by MTurk workers (Yang<br>et al., 2018; Dua et al., 2019; Chen et al., 2020c),<br>it requires substantial domain-specific knowledge<br>to compose meaningful questions that are hard for<br>computers to answer.</p>",
            "id": 47,
            "page": 4,
            "text": "We do not use popular micro-task platforms, such as Amazon Mechanical Turk (MTurk), because our preliminary studies show that many MTurk workers can not perform this task effectively. Our experiment with MTurk workers in S 4.3 further echo this observation. As most existing QA datasets were constructed by MTurk workers (Yang , 2018; Dua , 2019; Chen , 2020c), it requires substantial domain-specific knowledge to compose meaningful questions that are hard for computers to answer."
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 2424
                },
                {
                    "x": 1213,
                    "y": 2424
                },
                {
                    "x": 1213,
                    "y": 3105
                },
                {
                    "x": 286,
                    "y": 3105
                }
            ],
            "category": "paragraph",
            "html": "<p id='48' style='font-size:18px'>Annotation Task Design. For each page se-<br>lected in §4.1, the annotators are asked to (i) write<br>a meaningful financial question, (ii) compose a rea-<br>soning program to answer the question, and (iii) to<br>annotate the supporting fact. Each page is assigned<br>to one or two experts for annotation. We detail<br>each part as follows. (I) Financial question: For<br>a given page of earnings reports, the annotators are<br>asked first to compose a question that is \"meaning-<br>ful for financial analysis or learning insights of the<br>company financial reports\" and require numerical<br>calculations to answer. We encourage the experts</p>",
            "id": 48,
            "page": 4,
            "text": "Annotation Task Design. For each page selected in §4.1, the annotators are asked to (i) write a meaningful financial question, (ii) compose a reasoning program to answer the question, and (iii) to annotate the supporting fact. Each page is assigned to one or two experts for annotation. We detail each part as follows. (I) Financial question: For a given page of earnings reports, the annotators are asked first to compose a question that is \"meaningful for financial analysis or learning insights of the company financial reports\" and require numerical calculations to answer. We encourage the experts"
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 3136
                },
                {
                    "x": 1216,
                    "y": 3136
                },
                {
                    "x": 1216,
                    "y": 3224
                },
                {
                    "x": 289,
                    "y": 3224
                }
            ],
            "category": "paragraph",
            "html": "<p id='49' style='font-size:14px'>3UpWork (www.upwork.com) is a platform where re-<br>questers can recruit skilled freelancers.</p>",
            "id": 49,
            "page": 4,
            "text": "3UpWork (www.upwork.com) is a platform where requesters can recruit skilled freelancers."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 303
                },
                {
                    "x": 2198,
                    "y": 303
                },
                {
                    "x": 2198,
                    "y": 1374
                },
                {
                    "x": 1267,
                    "y": 1374
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='50' style='font-size:16px'>to write questions that require the information from<br>both the text and the table to answer. (II) Reason-<br>ing program: After providing the question, the<br>annotators are then asked to elaborate the oper-<br>ation steps to answer the question. Specifically,<br>they compose a maximum of 5 steps of operation,<br>where each operation has four slots: \"operation\",<br>\"argument1\", , \"argument2\", and \"result\". The \"op-<br>eration\" is one of the ten predefined operations<br>described in §3. An \"argument\" is a number or a<br>table's row name, either from the report or a previ-<br>ous step's result. For operations that only use one<br>argument, such as table aggregation, workers can<br>leave argument2 blank. The annotation interface<br>(see Appendix D) automatically validates the in-<br>puts to ensure correctness. (III) Supporting fact:<br>We also ask the annotators to mark all the sentences<br>in the text and the table rows that contain the infor-<br>mation needed to answer the question.</p>",
            "id": 50,
            "page": 4,
            "text": "to write questions that require the information from both the text and the table to answer. (II) Reasoning program: After providing the question, the annotators are then asked to elaborate the operation steps to answer the question. Specifically, they compose a maximum of 5 steps of operation, where each operation has four slots: \"operation\", \"argument1\", , \"argument2\", and \"result\". The \"operation\" is one of the ten predefined operations described in §3. An \"argument\" is a number or a table's row name, either from the report or a previous step's result. For operations that only use one argument, such as table aggregation, workers can leave argument2 blank. The annotation interface (see Appendix D) automatically validates the inputs to ensure correctness. (III) Supporting fact: We also ask the annotators to mark all the sentences in the text and the table rows that contain the information needed to answer the question."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 1407
                },
                {
                    "x": 1867,
                    "y": 1407
                },
                {
                    "x": 1867,
                    "y": 1460
                },
                {
                    "x": 1267,
                    "y": 1460
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='51' style='font-size:22px'>4.3 Data Quality Assessment</p>",
            "id": 51,
            "page": 4,
            "text": "4.3 Data Quality Assessment"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 1481
                },
                {
                    "x": 2197,
                    "y": 1481
                },
                {
                    "x": 2197,
                    "y": 2382
                },
                {
                    "x": 1268,
                    "y": 2382
                }
            ],
            "category": "paragraph",
            "html": "<p id='52' style='font-size:18px'>External experts answer FINQA questions with<br>a high accuracy and a high inter-annotator<br>agreement. To validate the quality of the anno-<br>tations, as well as to set up human expert perfor-<br>mance upper bound, we hire another two financial<br>professionals on UpWork. We randomly sample<br>200 examples from our dataset, and ask the pro-<br>fessionals to answer the questions as well as write<br>the operation steps, following the same procedure<br>as in the dataset construction. The payment is<br>$2.0 per question. For execution accuracy, they<br>reach 92.25% and 90.06%, respectively (mean<br>91.16%). For program accuracy, they reach 89.44%<br>and 85.53% (mean = 87.49%). The agreements be-<br>tween the two annotators are 92.65% for execution<br>accuracy, and 86.76% for program accuracy.</p>",
            "id": 52,
            "page": 4,
            "text": "External experts answer FINQA questions with a high accuracy and a high inter-annotator agreement. To validate the quality of the annotations, as well as to set up human expert performance upper bound, we hire another two financial professionals on UpWork. We randomly sample 200 examples from our dataset, and ask the professionals to answer the questions as well as write the operation steps, following the same procedure as in the dataset construction. The payment is $2.0 per question. For execution accuracy, they reach 92.25% and 90.06%, respectively (mean 91.16%). For program accuracy, they reach 89.44% and 85.53% (mean = 87.49%). The agreements between the two annotators are 92.65% for execution accuracy, and 86.76% for program accuracy."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 2410
                },
                {
                    "x": 2195,
                    "y": 2410
                },
                {
                    "x": 2195,
                    "y": 3031
                },
                {
                    "x": 1268,
                    "y": 3031
                }
            ],
            "category": "paragraph",
            "html": "<p id='53' style='font-size:18px'>Non-expert crowd workers answer FINQA<br>questions with a low accuracy. We also test<br>how well non-expert MTurk workers can answer<br>FINQA questions. We distribute the samples to<br>MTurk4 and take the similar process to distribute<br>each example to two workers. We end up with<br>an average execution accuracy of 50.68% and a<br>program accuracy of 48.17%, which is far below<br>the expert performance; the agreement rate is only<br>around 60%. These results echo our preliminary<br>study's observations for MTurk workers in §4.2.</p>",
            "id": 53,
            "page": 4,
            "text": "Non-expert crowd workers answer FINQA questions with a low accuracy. We also test how well non-expert MTurk workers can answer FINQA questions. We distribute the samples to MTurk4 and take the similar process to distribute each example to two workers. We end up with an average execution accuracy of 50.68% and a program accuracy of 48.17%, which is far below the expert performance; the agreement rate is only around 60%. These results echo our preliminary study's observations for MTurk workers in §4.2."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 3054
                },
                {
                    "x": 2195,
                    "y": 3054
                },
                {
                    "x": 2195,
                    "y": 3225
                },
                {
                    "x": 1267,
                    "y": 3225
                }
            ],
            "category": "paragraph",
            "html": "<p id='54' style='font-size:14px'>4Three built-in worker qualifications are used: HIT Ap-<br>proval Rate (≥95%), Number of Approved HITs (≥ 3000),<br>and Locale (US Only) Qualification. We do not select any<br>profession constraints. We pay $2.0 for each question.</p>",
            "id": 54,
            "page": 4,
            "text": "4Three built-in worker qualifications are used: HIT Approval Rate (≥95%), Number of Approved HITs (≥ 3000), and Locale (US Only) Qualification. We do not select any profession constraints. We pay $2.0 for each question."
        },
        {
            "bounding_box": [
                {
                    "x": 322,
                    "y": 294
                },
                {
                    "x": 1166,
                    "y": 294
                },
                {
                    "x": 1166,
                    "y": 741
                },
                {
                    "x": 322,
                    "y": 741
                }
            ],
            "category": "table",
            "html": "<table id='55' style='font-size:16px'><tr><td>Examples (Q&A pairs with program, fact)</td><td>8,281</td></tr><tr><td>Report pages</td><td>2,789</td></tr><tr><td>Vocabulary</td><td>22.3k</td></tr><tr><td>Avg. # sentences in input text</td><td>24.32</td></tr><tr><td>Avg. # tokens in input text</td><td>628.11</td></tr><tr><td>Avg. # rows in input table</td><td>6.36</td></tr><tr><td>Avg. # tokens in input table</td><td>59.42</td></tr><tr><td>Avg. # tokens in all inputs (text & table)</td><td>687.53</td></tr><tr><td>Max. # tokens in all inputs (text & table)</td><td>2,679</td></tr><tr><td>Avg. question length</td><td>16.63</td></tr></table>",
            "id": 55,
            "page": 5,
            "text": "Examples (Q&A pairs with program, fact) 8,281  Report pages 2,789  Vocabulary 22.3k  Avg. # sentences in input text 24.32  Avg. # tokens in input text 628.11  Avg. # rows in input table 6.36  Avg. # tokens in input table 59.42  Avg. # tokens in all inputs (text & table) 687.53  Max. # tokens in all inputs (text & table) 2,679  Avg. question length"
        },
        {
            "bounding_box": [
                {
                    "x": 523,
                    "y": 781
                },
                {
                    "x": 971,
                    "y": 781
                },
                {
                    "x": 971,
                    "y": 822
                },
                {
                    "x": 523,
                    "y": 822
                }
            ],
            "category": "caption",
            "html": "<caption id='56' style='font-size:16px'>Table 1: Statistics of FINQA.</caption>",
            "id": 56,
            "page": 5,
            "text": "Table 1: Statistics of FINQA."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 948
                },
                {
                    "x": 673,
                    "y": 948
                },
                {
                    "x": 673,
                    "y": 1003
                },
                {
                    "x": 288,
                    "y": 1003
                }
            ],
            "category": "paragraph",
            "html": "<p id='57' style='font-size:22px'>4.4 Data Analysis</p>",
            "id": 57,
            "page": 5,
            "text": "4.4 Data Analysis"
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 1034
                },
                {
                    "x": 1213,
                    "y": 1034
                },
                {
                    "x": 1213,
                    "y": 1372
                },
                {
                    "x": 289,
                    "y": 1372
                }
            ],
            "category": "paragraph",
            "html": "<p id='58' style='font-size:20px'>FINQA contains 8,281 examples. The data is re-<br>leased as training (6,251), validation (883), and<br>test (1,147) following an 75%/10%/15% split. The<br>three sets do not have overlapping input reports.<br>We quantitatively analyze some key properties of<br>FINQA. Table 1 shows the general statistics.</p>",
            "id": 58,
            "page": 5,
            "text": "FINQA contains 8,281 examples. The data is released as training (6,251), validation (883), and test (1,147) following an 75%/10%/15% split. The three sets do not have overlapping input reports. We quantitatively analyze some key properties of FINQA. Table 1 shows the general statistics."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 1428
                },
                {
                    "x": 1214,
                    "y": 1428
                },
                {
                    "x": 1214,
                    "y": 2220
                },
                {
                    "x": 287,
                    "y": 2220
                }
            ],
            "category": "paragraph",
            "html": "<p id='59' style='font-size:18px'>Statistics of Supporting Facts. In FINQA,<br>23.42% of the questions only require the informa-<br>tion in the text to answer; 62.43% of the questions<br>only require the information in the table to answer;<br>and 14.15% need both the text and table to an-<br>swer. Meanwhile, 46.30% of the examples have<br>one sentence or one table row as the fact; 42.63%<br>has two pieces of facts; and 11.07% has more than<br>two pieces of facts. For the examples with more<br>than one piece of fact, we also calculate the max-<br>imum distances between all the same example's<br>facts. 55.48% has a maximum distance of 3 or less<br>sentences ; 24.35% has a maximum distance of 4-6<br>sentences; and 20.17% has over 6 sentences.</p>",
            "id": 59,
            "page": 5,
            "text": "Statistics of Supporting Facts. In FINQA, 23.42% of the questions only require the information in the text to answer; 62.43% of the questions only require the information in the table to answer; and 14.15% need both the text and table to answer. Meanwhile, 46.30% of the examples have one sentence or one table row as the fact; 42.63% has two pieces of facts; and 11.07% has more than two pieces of facts. For the examples with more than one piece of fact, we also calculate the maximum distances between all the same example's facts. 55.48% has a maximum distance of 3 or less sentences ; 24.35% has a maximum distance of 4-6 sentences; and 20.17% has over 6 sentences."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 2275
                },
                {
                    "x": 1214,
                    "y": 2275
                },
                {
                    "x": 1214,
                    "y": 2782
                },
                {
                    "x": 288,
                    "y": 2782
                }
            ],
            "category": "paragraph",
            "html": "<p id='60' style='font-size:20px'>Statistics of Reasoning Programs. In the pro-<br>grams, the most frequent operations, add,<br>subtract, multiply, and divide, have the<br>distributions of 14.98%, 28.20%, 5.82%, and<br>45.29%, respectively. The operation division<br>has the highest frequency, as calculating ratios is<br>common in financial analysis. In FINQA, 59.10%<br>of the programs have 1 step, 32.71% have 2 steps,<br>and the rest 8.19% have 3 or more steps.</p>",
            "id": 60,
            "page": 5,
            "text": "Statistics of Reasoning Programs. In the programs, the most frequent operations, add, subtract, multiply, and divide, have the distributions of 14.98%, 28.20%, 5.82%, and 45.29%, respectively. The operation division has the highest frequency, as calculating ratios is common in financial analysis. In FINQA, 59.10% of the programs have 1 step, 32.71% have 2 steps, and the rest 8.19% have 3 or more steps."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 2847
                },
                {
                    "x": 735,
                    "y": 2847
                },
                {
                    "x": 735,
                    "y": 2905
                },
                {
                    "x": 288,
                    "y": 2905
                }
            ],
            "category": "paragraph",
            "html": "<p id='61' style='font-size:22px'>5 Baseline Systems</p>",
            "id": 61,
            "page": 5,
            "text": "5 Baseline Systems"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 2954
                },
                {
                    "x": 1216,
                    "y": 2954
                },
                {
                    "x": 1216,
                    "y": 3122
                },
                {
                    "x": 288,
                    "y": 3122
                }
            ],
            "category": "paragraph",
            "html": "<p id='62' style='font-size:18px'>In this section, we first describe our main base-<br>line framework FinQANet in §5.1, and then we<br>introduce other baselines in §5.2.</p>",
            "id": 62,
            "page": 5,
            "text": "In this section, we first describe our main baseline framework FinQANet in §5.1, and then we introduce other baselines in §5.2."
        },
        {
            "bounding_box": [
                {
                    "x": 340,
                    "y": 3177
                },
                {
                    "x": 1124,
                    "y": 3177
                },
                {
                    "x": 1124,
                    "y": 3224
                },
                {
                    "x": 340,
                    "y": 3224
                }
            ],
            "category": "paragraph",
            "html": "<p id='63' style='font-size:14px'>5For tables, we consider one row as one \"sentence\".</p>",
            "id": 63,
            "page": 5,
            "text": "5For tables, we consider one row as one \"sentence\"."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 288
                },
                {
                    "x": 2187,
                    "y": 288
                },
                {
                    "x": 2187,
                    "y": 725
                },
                {
                    "x": 1274,
                    "y": 725
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='64' style='font-size:14px' alt=\"Financial Report\nThe fair value for these options was estimated at the date\nof grant using a Black-Scholes option pricing model with\nthe following weighted-average assumptions for 2006, Retrieved Facts\n2005 and 2004:\n- - -\n\n- I\nThe total fair value of shares vested during\n2006, 2005 was $9,413, $8,249 respectively.\n\nThe total fair value of shares vested during 2006_2005 was\n$9,413, $8,249 respectively.\nThe aggregate intrinsic values of options outstanding and\nexercisable at December 30, 2006 were $204.1 million and $100.2\nmillion, respectively.\" data-coord=\"top-left:(1274,288); bottom-right:(2187,725)\" /></figure>",
            "id": 64,
            "page": 5,
            "text": "Financial Report The fair value for these options was estimated at the date of grant using a Black-Scholes option pricing model with the following weighted-average assumptions for 2006, Retrieved Facts 2005 and 2004: - -  - I The total fair value of shares vested during 2006, 2005 was $9,413, $8,249 respectively.  The total fair value of shares vested during 2006_2005 was $9,413, $8,249 respectively. The aggregate intrinsic values of options outstanding and exercisable at December 30, 2006 were $204.1 million and $100.2 million, respectively."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 763
                },
                {
                    "x": 2191,
                    "y": 763
                },
                {
                    "x": 2191,
                    "y": 846
                },
                {
                    "x": 1271,
                    "y": 846
                }
            ],
            "category": "caption",
            "html": "<caption id='65' style='font-size:16px'>Figure 2: The retriever retrieves supporting facts (text sen-<br>tences or table rows) from the input financial report.</caption>",
            "id": 65,
            "page": 5,
            "text": "Figure 2: The retriever retrieves supporting facts (text sentences or table rows) from the input financial report."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 939
                },
                {
                    "x": 1912,
                    "y": 939
                },
                {
                    "x": 1912,
                    "y": 990
                },
                {
                    "x": 1268,
                    "y": 990
                }
            ],
            "category": "paragraph",
            "html": "<p id='66' style='font-size:20px'>5.1 The FinQANet Framework</p>",
            "id": 66,
            "page": 5,
            "text": "5.1 The FinQANet Framework"
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 1015
                },
                {
                    "x": 2196,
                    "y": 1015
                },
                {
                    "x": 2196,
                    "y": 1241
                },
                {
                    "x": 1269,
                    "y": 1241
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='67' style='font-size:18px'>As a preliminary attempt on FINQA, we propose<br>FinQANet, with a retriever to first retrieve the sup-<br>porting facts from the input financial report, then a<br>generator to generate the program to get the answer.</p>",
            "id": 67,
            "page": 5,
            "text": "As a preliminary attempt on FINQA, we propose FinQANet, with a retriever to first retrieve the supporting facts from the input financial report, then a generator to generate the program to get the answer."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 1272
                },
                {
                    "x": 2197,
                    "y": 1272
                },
                {
                    "x": 2197,
                    "y": 2687
                },
                {
                    "x": 1268,
                    "y": 2687
                }
            ],
            "category": "paragraph",
            "html": "<p id='68' style='font-size:18px'>Retriever The full page of the financial report<br>can go beyond 2,000 tokens, which cannot be<br>coped with the current popular QA models (De-<br>vlin et al., 2019). Therefore we first retrieve the<br>supporting facts from the input report. For the<br>tables, we use templates to turn each row into sen-<br>tences. For example, the last row of the table in<br>Figure 1 is represented as 'the risk-free interest<br>, We concatenate each sup-<br>rate of 2006 is 5%; ·· · ·<br>porting fact with the question and train a classifier<br>using pre-trained LMs like BERT (Devlin et al.,<br>2019). Then we take the top n retrieved facts, re-<br>ordered as they appear in the input report. This<br>set of retriever results will serve as the input to the<br>second phase. Figure 2 illustrates the retrieving<br>procedure. Another common strategy is sliding<br>window (Alberti et al., 2019). We take the sliding<br>window of a fixed size with a stride to go through<br>the report, then the windows containing all the sup-<br>porting facts are marked as positive. However, we<br>observe in the experiments that the length of the<br>input to the program generator in the second phase<br>greatly influences the performance. The perfor-<br>mance of using sliding window falls far behind the<br>previous method.</p>",
            "id": 68,
            "page": 5,
            "text": "Retriever The full page of the financial report can go beyond 2,000 tokens, which cannot be coped with the current popular QA models (Devlin , 2019). Therefore we first retrieve the supporting facts from the input report. For the tables, we use templates to turn each row into sentences. For example, the last row of the table in Figure 1 is represented as 'the risk-free interest , We concatenate each suprate of 2006 is 5%; ·· · · porting fact with the question and train a classifier using pre-trained LMs like BERT (Devlin , 2019). Then we take the top n retrieved facts, reordered as they appear in the input report. This set of retriever results will serve as the input to the second phase. Figure 2 illustrates the retrieving procedure. Another common strategy is sliding window (Alberti , 2019). We take the sliding window of a fixed size with a stride to go through the report, then the windows containing all the supporting facts are marked as positive. However, we observe in the experiments that the length of the input to the program generator in the second phase greatly influences the performance. The performance of using sliding window falls far behind the previous method."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 2720
                },
                {
                    "x": 2196,
                    "y": 2720
                },
                {
                    "x": 2196,
                    "y": 3230
                },
                {
                    "x": 1268,
                    "y": 3230
                }
            ],
            "category": "paragraph",
            "html": "<p id='69' style='font-size:18px'>Program Generator Given the retrieved sup-<br>porting facts from the retriever, the program gen-<br>erator aims to generate the executable program to<br>answer the question. Figure 3 gives an overview of<br>the program generator. The generated tokens come<br>from 3 sources: 1) The input passage (retriever out-<br>put) and the question tokens {ei}, like the numbers<br>or the table row names. 2) The special tokens {si}<br>from the DSL, like the function names, predefined</p>",
            "id": 69,
            "page": 5,
            "text": "Program Generator Given the retrieved supporting facts from the retriever, the program generator aims to generate the executable program to answer the question. Figure 3 gives an overview of the program generator. The generated tokens come from 3 sources: 1) The input passage (retriever output) and the question tokens {ei}, like the numbers or the table row names. 2) The special tokens {si} from the DSL, like the function names, predefined"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 283
                },
                {
                    "x": 2195,
                    "y": 283
                },
                {
                    "x": 2195,
                    "y": 699
                },
                {
                    "x": 288,
                    "y": 699
                }
            ],
            "category": "figure",
            "html": "<figure><img id='70' style='font-size:14px' alt=\"Step memory embeddings Output space Predicted token Step memory embeddings Update memory Step memory embeddings\nConcat\n#0 #1 #0 #1 · #0 #1\nSpecial token embeddings\nAttentions ··· ···\nadd( ) ···\nInput embeddings\nLSTM\n... was $ 9413 decoder\n↑ ↑\nInput encoder\n↑ ↑ ↑ ↑ add( 9413 8249 ) divide( #0 8249 )\nwas $ 9413\" data-coord=\"top-left:(288,283); bottom-right:(2195,699)\" /></figure>",
            "id": 70,
            "page": 6,
            "text": "Step memory embeddings Output space Predicted token Step memory embeddings Update memory Step memory embeddings Concat #0 #1 #0 #1 · #0 #1 Special token embeddings Attentions ··· ··· add( ) ··· Input embeddings LSTM ... was $ 9413 decoder ↑ ↑ Input encoder ↑ ↑ ↑ ↑ add( 9413 8249 ) divide( #0 8249 ) was $ 9413"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 733
                },
                {
                    "x": 2193,
                    "y": 733
                },
                {
                    "x": 2193,
                    "y": 863
                },
                {
                    "x": 288,
                    "y": 863
                }
            ],
            "category": "caption",
            "html": "<caption id='71' style='font-size:14px'>Figure 3: The program generator. The retriever results and the question are first encoded using pre-trained LMs. At each<br>decoding step, the model can generate from the numbers or table row names from the input, the special tokens in the DSL, or the<br>step memory tokens. At the end of the generation of each operation step, we update the step memory token embeddings.</caption>",
            "id": 71,
            "page": 6,
            "text": "Figure 3: The program generator. The retriever results and the question are first encoded using pre-trained LMs. At each decoding step, the model can generate from the numbers or table row names from the input, the special tokens in the DSL, or the step memory tokens. At the end of the generation of each operation step, we update the step memory token embeddings."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 952
                },
                {
                    "x": 1211,
                    "y": 952
                },
                {
                    "x": 1211,
                    "y": 1399
                },
                {
                    "x": 287,
                    "y": 1399
                }
            ],
            "category": "paragraph",
            "html": "<p id='72' style='font-size:18px'>constants, etc. 3) The step memory tokens {mi}<br>to denote the results from previous steps, like #0,<br>#1 , etc. We first use pre-trained LMs to encode<br>{ei}, denote the output embeddings as {he}. The<br>embeddings of the special tokens and the step mem-<br>ory tokens are randomly initialized and denoted as<br>{hs} and {hm} respectively. Denote all the token<br>embeddings H = [he; hs; hm].</p>",
            "id": 72,
            "page": 6,
            "text": "constants, etc. 3) The step memory tokens {mi} to denote the results from previous steps, like #0, #1 , etc. We first use pre-trained LMs to encode {ei}, denote the output embeddings as {he}. The embeddings of the special tokens and the step memory tokens are randomly initialized and denoted as {hs} and {hm} respectively. Denote all the token embeddings H = [he; hs; hm]."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 1405
                },
                {
                    "x": 1212,
                    "y": 1405
                },
                {
                    "x": 1212,
                    "y": 1738
                },
                {
                    "x": 288,
                    "y": 1738
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='73' style='font-size:16px'>An LSTM is used for decoding. At each decod-<br>ing step T, the program token embeddings H are<br>fed as the input; The decoder output hT is used<br>to calculate the attention vector attp and atth over<br>the input and the decoding history. Then a context<br>vector CT combines all the contextual information:</p>",
            "id": 73,
            "page": 6,
            "text": "An LSTM is used for decoding. At each decoding step T, the program token embeddings H are fed as the input; The decoder output hT is used to calculate the attention vector attp and atth over the input and the decoding history. Then a context vector CT combines all the contextual information:"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 1881
                },
                {
                    "x": 1211,
                    "y": 1881
                },
                {
                    "x": 1211,
                    "y": 1992
                },
                {
                    "x": 288,
                    "y": 1992
                }
            ],
            "category": "paragraph",
            "html": "<p id='74' style='font-size:18px'>Meanwhile, another attention vector attp over the<br>input is applied to all the token embeddings:</p>",
            "id": 74,
            "page": 6,
            "text": "Meanwhile, another attention vector attp over the input is applied to all the token embeddings:"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 2143
                },
                {
                    "x": 1211,
                    "y": 2143
                },
                {
                    "x": 1211,
                    "y": 2536
                },
                {
                    "x": 288,
                    "y": 2536
                }
            ],
            "category": "paragraph",
            "html": "<p id='75' style='font-size:16px'>Different from other program tokens, the step mem-<br>ory tokens {mi} imply the reasoning path of the<br>program. To make use of such structure informa-<br>tion, at each decoding step indicating the end of<br>one operation[args] unit, i.e., the step to generate<br>the ending parentheses in our DSL, we compute<br>another context vector aT:</p>",
            "id": 75,
            "page": 6,
            "text": "Different from other program tokens, the step memory tokens {mi} imply the reasoning path of the program. To make use of such structure information, at each decoding step indicating the end of one operation[args] unit, i.e., the step to generate the ending parentheses in our DSL, we compute another context vector aT:"
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 2687
                },
                {
                    "x": 1215,
                    "y": 2687
                },
                {
                    "x": 1215,
                    "y": 2793
                },
                {
                    "x": 289,
                    "y": 2793
                }
            ],
            "category": "paragraph",
            "html": "<p id='76' style='font-size:16px'>Then the step memory token embedding corre-<br>sponding to the current step is updated as aT.</p>",
            "id": 76,
            "page": 6,
            "text": "Then the step memory token embedding corresponding to the current step is updated as aT."
        },
        {
            "bounding_box": [
                {
                    "x": 336,
                    "y": 2801
                },
                {
                    "x": 1045,
                    "y": 2801
                },
                {
                    "x": 1045,
                    "y": 2849
                },
                {
                    "x": 336,
                    "y": 2849
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='77' style='font-size:18px'>The final prediction is calculated with:</p>",
            "id": 77,
            "page": 6,
            "text": "The final prediction is calculated with:"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 3004
                },
                {
                    "x": 1211,
                    "y": 3004
                },
                {
                    "x": 1211,
                    "y": 3230
                },
                {
                    "x": 288,
                    "y": 3230
                }
            ],
            "category": "paragraph",
            "html": "<p id='78' style='font-size:16px'>During inference time, based on the grammar of<br>the DSL, we use masks at each decoding step to<br>ensure the structural correctness of the generated<br>programs. In the retriever phase, we take the top</p>",
            "id": 78,
            "page": 6,
            "text": "During inference time, based on the grammar of the DSL, we use masks at each decoding step to ensure the structural correctness of the generated programs. In the retriever phase, we take the top"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 953
                },
                {
                    "x": 2193,
                    "y": 953
                },
                {
                    "x": 2193,
                    "y": 1229
                },
                {
                    "x": 1268,
                    "y": 1229
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='79' style='font-size:16px'>n retrieved results as the input to the program gen-<br>erator. Therefore, for the training of the program<br>generator, we use the retriever result on the training<br>set (combined with the gold facts if there is any<br>wrong prediction) as the input.</p>",
            "id": 79,
            "page": 6,
            "text": "n retrieved results as the input to the program generator. Therefore, for the training of the program generator, we use the retriever result on the training set (combined with the gold facts if there is any wrong prediction) as the input."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 1273
                },
                {
                    "x": 1688,
                    "y": 1273
                },
                {
                    "x": 1688,
                    "y": 1320
                },
                {
                    "x": 1271,
                    "y": 1320
                }
            ],
            "category": "paragraph",
            "html": "<p id='80' style='font-size:16px'>5.2 Other Baselines</p>",
            "id": 80,
            "page": 6,
            "text": "5.2 Other Baselines"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 1347
                },
                {
                    "x": 2194,
                    "y": 1347
                },
                {
                    "x": 2194,
                    "y": 1683
                },
                {
                    "x": 1268,
                    "y": 1683
                }
            ],
            "category": "paragraph",
            "html": "<p id='81' style='font-size:16px'>TF-IDF + Single Op. We use TF-IDF to retrieve<br>the top 2 sentences from the input report. Since the<br>most common case in our dataset is one-step pro-<br>gram and the most common operation is division,<br>we take the first number from each sentence and<br>apply the division operation.</p>",
            "id": 81,
            "page": 6,
            "text": "TF-IDF + Single Op. We use TF-IDF to retrieve the top 2 sentences from the input report. Since the most common case in our dataset is one-step program and the most common operation is division, we take the first number from each sentence and apply the division operation."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 1718
                },
                {
                    "x": 2195,
                    "y": 1718
                },
                {
                    "x": 2195,
                    "y": 1942
                },
                {
                    "x": 1268,
                    "y": 1942
                }
            ],
            "category": "paragraph",
            "html": "<p id='82' style='font-size:16px'>Retriever + Direct Generation. To demonstrate<br>the necessity of generating the reasoning programs,<br>we keep the architecture the same as our model, but<br>directly generating the final results.</p>",
            "id": 82,
            "page": 6,
            "text": "Retriever + Direct Generation. To demonstrate the necessity of generating the reasoning programs, we keep the architecture the same as our model, but directly generating the final results."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 1978
                },
                {
                    "x": 2195,
                    "y": 1978
                },
                {
                    "x": 2195,
                    "y": 2257
                },
                {
                    "x": 1268,
                    "y": 2257
                }
            ],
            "category": "paragraph",
            "html": "<p id='83' style='font-size:16px'>Retriever + Seq2seq. We use a Seq2seq architec-<br>ture for the generator, similar to the Seq2seq base-<br>line in the MathQA dataset (Amini et al., 2019). A<br>bi-LSTM is used for encoding the input, and then<br>an LSTM is used for decoding with attention.</p>",
            "id": 83,
            "page": 6,
            "text": "Retriever + Seq2seq. We use a Seq2seq architecture for the generator, similar to the Seq2seq baseline in the MathQA dataset (Amini , 2019). A bi-LSTM is used for encoding the input, and then an LSTM is used for decoding with attention."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 2293
                },
                {
                    "x": 2196,
                    "y": 2293
                },
                {
                    "x": 2196,
                    "y": 3026
                },
                {
                    "x": 1268,
                    "y": 3026
                }
            ],
            "category": "paragraph",
            "html": "<p id='84' style='font-size:16px'>Retriever + NeRd. The Neural Symbolic<br>Reader(NeRd) (Chen et al., 2020d) is also a pointer-<br>generator based model for program generation,<br>with the state of the art results on the MathQA<br>dataset (Amini et al., 2019). Different from ours,<br>it directly learns the program with nested format<br>as a sequence, i.e., without the step memory to-<br>kens. This way the model is able to learn the pro-<br>gram structures as patterns from very large-scale<br>data (~40k for MathQA), but may fail on learning<br>the reasoning paths. We keep the retriever part<br>the same and compare with the generator part to<br>demonstrate the usefulness of structure learning.</p>",
            "id": 84,
            "page": 6,
            "text": "Retriever + NeRd. The Neural Symbolic Reader(NeRd) (Chen , 2020d) is also a pointergenerator based model for program generation, with the state of the art results on the MathQA dataset (Amini , 2019). Different from ours, it directly learns the program with nested format as a sequence, i.e., without the step memory tokens. This way the model is able to learn the program structures as patterns from very large-scale data (~40k for MathQA), but may fail on learning the reasoning paths. We keep the retriever part the same and compare with the generator part to demonstrate the usefulness of structure learning."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 3060
                },
                {
                    "x": 2192,
                    "y": 3060
                },
                {
                    "x": 2192,
                    "y": 3229
                },
                {
                    "x": 1268,
                    "y": 3229
                }
            ],
            "category": "paragraph",
            "html": "<p id='85' style='font-size:16px'>Pre-Trained Longformer. There are also works<br>on modeling very long documents with thousands<br>of characters, with the attention mechanism that</p>",
            "id": 85,
            "page": 6,
            "text": "Pre-Trained Longformer. There are also works on modeling very long documents with thousands of characters, with the attention mechanism that"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 302
                },
                {
                    "x": 1216,
                    "y": 302
                },
                {
                    "x": 1216,
                    "y": 751
                },
                {
                    "x": 288,
                    "y": 751
                }
            ],
            "category": "paragraph",
            "html": "<p id='86' style='font-size:16px'>scales linearly with sequence length, like the Long-<br>former (Beltagy et al., 2020). To demonstrate the<br>necessity of breaking up into the pipeline of re-<br>triever and program generator, we remove the re-<br>triever and directly use the pre-trained Longformer<br>as the input encoder in the program generator, and<br>encode the whole report. The table rows are lin-<br>earized similar as in §5.1.</p>",
            "id": 86,
            "page": 7,
            "text": "scales linearly with sequence length, like the Longformer (Beltagy , 2020). To demonstrate the necessity of breaking up into the pipeline of retriever and program generator, we remove the retriever and directly use the pre-trained Longformer as the input encoder in the program generator, and encode the whole report. The table rows are linearized similar as in §5.1."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 803
                },
                {
                    "x": 835,
                    "y": 803
                },
                {
                    "x": 835,
                    "y": 859
                },
                {
                    "x": 288,
                    "y": 859
                }
            ],
            "category": "paragraph",
            "html": "<p id='87' style='font-size:20px'>6 Experimental Results</p>",
            "id": 87,
            "page": 7,
            "text": "6 Experimental Results"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 901
                },
                {
                    "x": 1215,
                    "y": 901
                },
                {
                    "x": 1215,
                    "y": 1690
                },
                {
                    "x": 287,
                    "y": 1690
                }
            ],
            "category": "paragraph",
            "html": "<p id='88' style='font-size:16px'>Experiment Setups. For the retriever, we use<br>BERT-base as the classifier (other pre-trained mod-<br>els perform similarly). Since most of the examples<br>in our dataset have 1 or 2 facts, and we find that<br>longer inputs lower the performance of the pro-<br>gram generator, we take the top 3 ranked facts as<br>the retriever results. For the program generator, we<br>experiment on using BERT (Devlin et al., 2019),<br>RoBERTa (Liu et al., 2019), and FinBert (Araci,<br>2019) as the encoder, to test the performances of<br>popular large pre-trained models. For all models,<br>we use the Adam optimizer (Kingma and Ba, 2015).<br>Check Appendix B for more details of training and<br>parameter settings.</p>",
            "id": 88,
            "page": 7,
            "text": "Experiment Setups. For the retriever, we use BERT-base as the classifier (other pre-trained models perform similarly). Since most of the examples in our dataset have 1 or 2 facts, and we find that longer inputs lower the performance of the program generator, we take the top 3 ranked facts as the retriever results. For the program generator, we experiment on using BERT (Devlin , 2019), RoBERTa (Liu , 2019), and FinBert (Araci, 2019) as the encoder, to test the performances of popular large pre-trained models. For all models, we use the Adam optimizer (Kingma and Ba, 2015). Check Appendix B for more details of training and parameter settings."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 1739
                },
                {
                    "x": 870,
                    "y": 1739
                },
                {
                    "x": 870,
                    "y": 1791
                },
                {
                    "x": 288,
                    "y": 1791
                }
            ],
            "category": "paragraph",
            "html": "<p id='89' style='font-size:18px'>6.1 QA Model Performance</p>",
            "id": 89,
            "page": 7,
            "text": "6.1 QA Model Performance"
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 1813
                },
                {
                    "x": 1214,
                    "y": 1813
                },
                {
                    "x": 1214,
                    "y": 3232
                },
                {
                    "x": 286,
                    "y": 3232
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='90' style='font-size:18px'>Table 2 presents the results for all the baseline sys-<br>tems. We evaluate the execution accuracy (exe acc)<br>and program accuracy (prog acc) as explained in<br>§3. For the BERT-based retriever, we have 89.66%<br>recall for the top 3 retrieved facts and 93.63% re-<br>call for the top 5. Using TF-IDF results in 82.91%<br>recall for the top 5 facts. We use the same retriever<br>results for all retriever-generator based models.<br>Directly generating the execution results gives near-<br>zero scores, which indicates the necessity of gen-<br>erating the reasoning programs. If without using<br>the retriever-generator pipeline, but directly apply-<br>ing an end-to-end pre-trained Longformer model,<br>the performance falls far behind. Because longer<br>inputs have more numbers which put more confu-<br>sions on the program generator and thus make it<br>harder to learn. Generally, the program generators<br>using pre-trained models perform much better than<br>the Seq2seq baseline, as there is language model-<br>ing knowledge that can also be used for the finance<br>domain. And larger pre-trained models give better<br>performance, as they tend to see more financial<br>corpus during their pre-training. FinBert (Araci,<br>2019) is a pre-trained model for the finance domain;<br>its main downstream tasks are sentiment analysis.</p>",
            "id": 90,
            "page": 7,
            "text": "Table 2 presents the results for all the baseline systems. We evaluate the execution accuracy (exe acc) and program accuracy (prog acc) as explained in §3. For the BERT-based retriever, we have 89.66% recall for the top 3 retrieved facts and 93.63% recall for the top 5. Using TF-IDF results in 82.91% recall for the top 5 facts. We use the same retriever results for all retriever-generator based models. Directly generating the execution results gives nearzero scores, which indicates the necessity of generating the reasoning programs. If without using the retriever-generator pipeline, but directly applying an end-to-end pre-trained Longformer model, the performance falls far behind. Because longer inputs have more numbers which put more confusions on the program generator and thus make it harder to learn. Generally, the program generators using pre-trained models perform much better than the Seq2seq baseline, as there is language modeling knowledge that can also be used for the finance domain. And larger pre-trained models give better performance, as they tend to see more financial corpus during their pre-training. FinBert (Araci, 2019) is a pre-trained model for the finance domain; its main downstream tasks are sentiment analysis."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 290
                },
                {
                    "x": 2183,
                    "y": 290
                },
                {
                    "x": 2183,
                    "y": 1209
                },
                {
                    "x": 1270,
                    "y": 1209
                }
            ],
            "category": "table",
            "html": "<br><table id='91' style='font-size:14px'><tr><td>Baselines</td><td>Exe Acc</td><td>Prog Acc</td></tr><tr><td>TF-IDF + Single Op</td><td>1.01</td><td>0.90</td></tr><tr><td>Retriever + Direct Generation</td><td>0.30</td><td>-</td></tr><tr><td>Pre-Trained Longformer (base)</td><td>21.90</td><td>20.48</td></tr><tr><td>Retriever + Seq2seq</td><td>19.71</td><td>18.38</td></tr><tr><td>Retriever + NeRd (BERT-base)</td><td>48.57</td><td>46.76</td></tr><tr><td>FinQANet (FinBert)</td><td>50.10</td><td>47.52</td></tr><tr><td>FinQANet (BERT-base)</td><td>50.00</td><td>48.00</td></tr><tr><td>FinQANet (BERT-large)</td><td>53.52</td><td>51.62</td></tr><tr><td>FinQANet (RoBERTa-base)</td><td>56.10</td><td>54.38</td></tr><tr><td>FinQANet (RoBERTa-large)</td><td>61.24</td><td>58.86</td></tr><tr><td>FinQANet-Gold (RoBERTa-large)</td><td>70.00</td><td>68.76</td></tr><tr><td>Human Expert Performance</td><td>91.16</td><td>87.49</td></tr><tr><td>General Crowd Performance</td><td>50.68</td><td>48.17</td></tr></table>",
            "id": 91,
            "page": 7,
            "text": "Baselines Exe Acc Prog Acc  TF-IDF + Single Op 1.01 0.90  Retriever + Direct Generation 0.30  Pre-Trained Longformer (base) 21.90 20.48  Retriever + Seq2seq 19.71 18.38  Retriever + NeRd (BERT-base) 48.57 46.76  FinQANet (FinBert) 50.10 47.52  FinQANet (BERT-base) 50.00 48.00  FinQANet (BERT-large) 53.52 51.62  FinQANet (RoBERTa-base) 56.10 54.38  FinQANet (RoBERTa-large) 61.24 58.86  FinQANet-Gold (RoBERTa-large) 70.00 68.76  Human Expert Performance 91.16 87.49  General Crowd Performance 50.68"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 1239
                },
                {
                    "x": 2194,
                    "y": 1239
                },
                {
                    "x": 2194,
                    "y": 1450
                },
                {
                    "x": 1268,
                    "y": 1450
                }
            ],
            "category": "caption",
            "html": "<caption id='92' style='font-size:14px'>Table 2: The execution accuracy (Exe Acc) and program<br>accuracy (Prog Acc) for all the models. Although our best<br>system (61.24%) outperforms the non-expert crowd (50.68%),<br>the significant accuracy gap between the model and human<br>experts (91.16%) motivates the need for future research.</caption>",
            "id": 92,
            "page": 7,
            "text": "Table 2: The execution accuracy (Exe Acc) and program accuracy (Prog Acc) for all the models. Although our best system (61.24%) outperforms the non-expert crowd (50.68%), the significant accuracy gap between the model and human experts (91.16%) motivates the need for future research."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 1515
                },
                {
                    "x": 2198,
                    "y": 1515
                },
                {
                    "x": 2198,
                    "y": 2249
                },
                {
                    "x": 1268,
                    "y": 2249
                }
            ],
            "category": "paragraph",
            "html": "<p id='93' style='font-size:18px'>The performance of using FinBert is no better than<br>BERT-large, mostly because its pre-training cor-<br>pus is limited (~30M words from news articles).<br>Comparing FinQANet with the retriever + NeRd<br>baseline (Chen et al., 2020d), it shows the improve-<br>ments from learning the logical structure of the pro-<br>grams. We also run the program generator using<br>the gold retriever result, shown as FinQANet-Gold.<br>Another interesting observation is the comparisons<br>with human performances. While there is still a<br>large gap from the human expert upper bound, the<br>best performing model already surpasses the gen-<br>eral crowd performance.</p>",
            "id": 93,
            "page": 7,
            "text": "The performance of using FinBert is no better than BERT-large, mostly because its pre-training corpus is limited (~30M words from news articles). Comparing FinQANet with the retriever + NeRd baseline (Chen , 2020d), it shows the improvements from learning the logical structure of the programs. We also run the program generator using the gold retriever result, shown as FinQANet-Gold. Another interesting observation is the comparisons with human performances. While there is still a large gap from the human expert upper bound, the best performing model already surpasses the general crowd performance."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 2293
                },
                {
                    "x": 1867,
                    "y": 2293
                },
                {
                    "x": 1867,
                    "y": 2345
                },
                {
                    "x": 1269,
                    "y": 2345
                }
            ],
            "category": "paragraph",
            "html": "<p id='94' style='font-size:18px'>6.2 Performance Breakdown</p>",
            "id": 94,
            "page": 7,
            "text": "6.2 Performance Breakdown"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 2368
                },
                {
                    "x": 2192,
                    "y": 2368
                },
                {
                    "x": 2192,
                    "y": 2533
                },
                {
                    "x": 1268,
                    "y": 2533
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='95' style='font-size:16px'>We conduct a set of performance breakdowns using<br>the FinQANet (RoBERTa-large) model. Table 3<br>shows all the results.</p>",
            "id": 95,
            "page": 7,
            "text": "We conduct a set of performance breakdowns using the FinQANet (RoBERTa-large) model. Table 3 shows all the results."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 2573
                },
                {
                    "x": 2193,
                    "y": 2573
                },
                {
                    "x": 2193,
                    "y": 2851
                },
                {
                    "x": 1268,
                    "y": 2851
                }
            ],
            "category": "paragraph",
            "html": "<p id='96' style='font-size:18px'>Necessity of using both table and text. We run<br>inferences taking facts only from a single source<br>from the retriever. Inferences on individual source<br>(table-only: 45.81%, text-only: 15.80%) are both<br>far behind the full results (61.24%).</p>",
            "id": 96,
            "page": 7,
            "text": "Necessity of using both table and text. We run inferences taking facts only from a single source from the retriever. Inferences on individual source (table-only: 45.81%, text-only: 15.80%) are both far behind the full results (61.24%)."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 2888
                },
                {
                    "x": 2196,
                    "y": 2888
                },
                {
                    "x": 2196,
                    "y": 3228
                },
                {
                    "x": 1268,
                    "y": 3228
                }
            ],
            "category": "paragraph",
            "html": "<p id='97' style='font-size:16px'>The model performs the best on the table-only<br>questions. The model performs the best on table-<br>only questions (67.38%). Tables tend to have more<br>unified structures and might be easier for the model<br>to learn. Table 3 also shows that the questions<br>involving both tables and texts are the most chal-</p>",
            "id": 97,
            "page": 7,
            "text": "The model performs the best on the table-only questions. The model performs the best on tableonly questions (67.38%). Tables tend to have more unified structures and might be easier for the model to learn. Table 3 also shows that the questions involving both tables and texts are the most chal-"
        },
        {
            "bounding_box": [
                {
                    "x": 298,
                    "y": 289
                },
                {
                    "x": 2188,
                    "y": 289
                },
                {
                    "x": 2188,
                    "y": 938
                },
                {
                    "x": 298,
                    "y": 938
                }
            ],
            "category": "table",
            "html": "<table id='98' style='font-size:14px'><tr><td>Error case (1)</td><td colspan=\"3\">Gold supporting facts: text sentence(s) [1] additionally , we have other committed and uncommitted credit lines of $ 746 million with major international banks and financial institutions to support our general global funding needs , including with respect to bank supported letters of credit, performance bonds and guarantees [2] approximately $ 554 million of these credit lines were available for use as of year-end 2016</td><td>Question: what is the amount of credit lines that has been drawn in millions as of year-end 2016? Gold program: subtract(746, 554) Predicted program: multiply(554, const_ 1000000)</td></tr><tr><td rowspan=\"4\">Error case (2)</td><td colspan=\"3\">Gold supporting facts: table row(s)</td><td rowspan=\"4\">Question: what is the percentage change in the total fair value of non-vested shares from 2009 to 2010? Gold program: multiply(762, 42), multiply(713, 42), subtract(#1, #0), divide(#2, #0) Predicted program: subtract(713, 762), divide(#0, 762)</td></tr><tr><td></td><td>shares</td><td>weighted average grant-date fair value</td></tr><tr><td>non-vested at may 31 2009</td><td>762</td><td>42</td></tr><tr><td>non-vested at may 31 2010</td><td>713</td><td>42</td></tr><tr><td>Error case (3)</td><td colspan=\"3\">Gold supporting facts: text sentence(s) [1] we maintained a $ 1.4 billion senior credit facility with various financial institutions , including the $ 420.5 million term loan and a $ 945.5 million revolving credit facility ·</td><td>Question: what is the estimated percentage of revolving credit facility in relation with the total senior credit facility in millions? Gold program: multiply(1.4, const_ 1000), divide(945.5, #0) Predicted program: divide(945.5, const_1000)</td></tr></table>",
            "id": 98,
            "page": 8,
            "text": "Error case (1) Gold supporting facts: text sentence(s)  additionally , we have other committed and uncommitted credit lines of $ 746 million with major international banks and financial institutions to support our general global funding needs , including with respect to bank supported letters of credit, performance bonds and guarantees  approximately $ 554 million of these credit lines were available for use as of year-end 2016 Question: what is the amount of credit lines that has been drawn in millions as of year-end 2016? Gold program: subtract(746, 554) Predicted program: multiply(554, const_ 1000000)  Error case (2) Gold supporting facts: table row(s) Question: what is the percentage change in the total fair value of non-vested shares from 2009 to 2010? Gold program: multiply(762, 42), multiply(713, 42), subtract(#1, #0), divide(#2, #0) Predicted program: subtract(713, 762), divide(#0, 762)   shares weighted average grant-date fair value  non-vested at may 31 2009 762 42  non-vested at may 31 2010 713 42  Error case (3) Gold supporting facts: text sentence(s)  we maintained a $ 1.4 billion senior credit facility with various financial institutions , including the $ 420.5 million term loan and a $ 945.5 million revolving credit facility ·"
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 974
                },
                {
                    "x": 2195,
                    "y": 974
                },
                {
                    "x": 2195,
                    "y": 1145
                },
                {
                    "x": 289,
                    "y": 1145
                }
            ],
            "category": "caption",
            "html": "<caption id='99' style='font-size:14px'>Figure 4: Error cases. In these examples, the retriever results all correctly cover the gold facts; thus we only present the gold facts,<br>gold program, and the predicted program to study the errors of the program generator. We give more error cases in Appendix<br>C, including the cases for the retriever errors. Example 1: The financial knowledge to calculate the 'credit lines that has been<br>drawn' · Example 2: Complex reasoning of 4 steps. Example 3: Number unit conversion between 'billion' and 'million'</caption>",
            "id": 99,
            "page": 8,
            "text": "Figure 4: Error cases. In these examples, the retriever results all correctly cover the gold facts; thus we only present the gold facts, gold program, and the predicted program to study the errors of the program generator. We give more error cases in Appendix C, including the cases for the retriever errors. Example 1: The financial knowledge to calculate the 'credit lines that has been drawn' · Example 2: Complex reasoning of 4 steps. Example 3: Number unit conversion between 'billion' and 'million'"
        },
        {
            "bounding_box": [
                {
                    "x": 296,
                    "y": 1219
                },
                {
                    "x": 1201,
                    "y": 1219
                },
                {
                    "x": 1201,
                    "y": 2172
                },
                {
                    "x": 296,
                    "y": 2172
                }
            ],
            "category": "table",
            "html": "<table id='100' style='font-size:16px'><tr><td>Methods</td><td>Exe Acc</td><td>Prog Acc</td></tr><tr><td>full results</td><td>61.24</td><td>58.86</td></tr><tr><td colspan=\"3\">Necessity of table and text</td></tr><tr><td>table-only inference</td><td>45.81</td><td>43.62</td></tr><tr><td>text-only inference</td><td>15.80</td><td>15.33</td></tr><tr><td colspan=\"3\">Performances on table and text</td></tr><tr><td>table-only questions</td><td>67.38</td><td>64.48</td></tr><tr><td>text-only questions</td><td>54.86</td><td>53.70</td></tr><tr><td>table-text questions</td><td>43.80</td><td>41.61</td></tr><tr><td colspan=\"3\">Performances regarding program steps</td></tr><tr><td>1 step programs</td><td>67.61</td><td>65.28</td></tr><tr><td>2 step programs</td><td>59.08</td><td>56.37</td></tr><tr><td>>2 step programs</td><td>22.78</td><td>21.52</td></tr><tr><td>Programs with constants</td><td>43.88</td><td>39.80</td></tr></table>",
            "id": 100,
            "page": 8,
            "text": "Methods Exe Acc Prog Acc  full results 61.24 58.86  Necessity of table and text  table-only inference 45.81 43.62  text-only inference 15.80 15.33  Performances on table and text  table-only questions 67.38 64.48  text-only questions 54.86 53.70  table-text questions 43.80 41.61  Performances regarding program steps  1 step programs 67.61 65.28  2 step programs 59.08 56.37  >2 step programs 22.78 21.52  Programs with constants 43.88"
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 2202
                },
                {
                    "x": 1212,
                    "y": 2202
                },
                {
                    "x": 1212,
                    "y": 2456
                },
                {
                    "x": 289,
                    "y": 2456
                }
            ],
            "category": "paragraph",
            "html": "<p id='101' style='font-size:14px'>Table 3: Performance breakdown of FinQANet (RoBERTa-<br>large). The model benefits from using both table and text, as<br>inferences on individual source yield much lower performance.<br>FinQANet is better at answering table-only questions, and the<br>questions that require more steps to solve are indeed more<br>challenging to the model.</p>",
            "id": 101,
            "page": 8,
            "text": "Table 3: Performance breakdown of FinQANet (RoBERTalarge). The model benefits from using both table and text, as inferences on individual source yield much lower performance. FinQANet is better at answering table-only questions, and the questions that require more steps to solve are indeed more challenging to the model."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 2577
                },
                {
                    "x": 980,
                    "y": 2577
                },
                {
                    "x": 980,
                    "y": 2626
                },
                {
                    "x": 289,
                    "y": 2626
                }
            ],
            "category": "paragraph",
            "html": "<p id='102' style='font-size:20px'>lenging ones for the model (43.80%).</p>",
            "id": 102,
            "page": 8,
            "text": "lenging ones for the model (43.80%)."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 2676
                },
                {
                    "x": 1215,
                    "y": 2676
                },
                {
                    "x": 1215,
                    "y": 2957
                },
                {
                    "x": 287,
                    "y": 2957
                }
            ],
            "category": "paragraph",
            "html": "<p id='103' style='font-size:16px'>Questions that need more than two steps to an-<br>swer are challenging. The model has a low ac-<br>curacy (22.78%) on the questions that need three<br>or more steps. Meanwhile, not surprisingly, the<br>questions that require only one step are the easiest.</p>",
            "id": 103,
            "page": 8,
            "text": "Questions that need more than two steps to answer are challenging. The model has a low accuracy (22.78%) on the questions that need three or more steps. Meanwhile, not surprisingly, the questions that require only one step are the easiest."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 3000
                },
                {
                    "x": 1212,
                    "y": 3000
                },
                {
                    "x": 1212,
                    "y": 3231
                },
                {
                    "x": 288,
                    "y": 3231
                }
            ],
            "category": "paragraph",
            "html": "<p id='104' style='font-size:16px'>Constants in programs. Many programs in<br>FINQA contain constants as arguments. A constant<br>is often used to convert an English number word<br>to another. For example, we need first to use the</p>",
            "id": 104,
            "page": 8,
            "text": "Constants in programs. Many programs in FINQA contain constants as arguments. A constant is often used to convert an English number word to another. For example, we need first to use the"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 1234
                },
                {
                    "x": 2197,
                    "y": 1234
                },
                {
                    "x": 2197,
                    "y": 1852
                },
                {
                    "x": 1268,
                    "y": 1852
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='105' style='font-size:18px'>constant \"1,000\" to convert \"1.5 billion\" to \"1,500<br>million\" so that it can be added with \"50 million\".<br>A constant is also used to explicate the implicit<br>numbers hidden in the language. For example, to<br>calculate \"the average for the year 2012, 2013, and<br>2014\", the program needs to use the constant \"3\" as<br>the denominator, which is not mentioned explicitly<br>in the text. As shown in Table 3, the programs with<br>constants yield great challenges for our model, as<br>the performance (43.88%) is much lower than that<br>of the whole set (61.24%).</p>",
            "id": 105,
            "page": 8,
            "text": "constant \"1,000\" to convert \"1.5 billion\" to \"1,500 million\" so that it can be added with \"50 million\". A constant is also used to explicate the implicit numbers hidden in the language. For example, to calculate \"the average for the year 2012, 2013, and 2014\", the program needs to use the constant \"3\" as the denominator, which is not mentioned explicitly in the text. As shown in Table 3, the programs with constants yield great challenges for our model, as the performance (43.88%) is much lower than that of the whole set (61.24%)."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 1895
                },
                {
                    "x": 1671,
                    "y": 1895
                },
                {
                    "x": 1671,
                    "y": 1944
                },
                {
                    "x": 1270,
                    "y": 1944
                }
            ],
            "category": "paragraph",
            "html": "<p id='106' style='font-size:20px'>6.3 Error Analysis</p>",
            "id": 106,
            "page": 8,
            "text": "6.3 Error Analysis"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 1967
                },
                {
                    "x": 2198,
                    "y": 1967
                },
                {
                    "x": 2198,
                    "y": 2758
                },
                {
                    "x": 1268,
                    "y": 2758
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='107' style='font-size:18px'>We sample 50 error cases from the results of the<br>FinQANet (RoBERTa-large) model and analyze<br>them manually. 15% of the errors are caused by<br>the retriever, e.g., missing facts. Half of the rest are<br>due to the lack of financial knowledge, such as the<br>meaning of some terminology. And the rest half<br>are primarily numerical reasoning errors, includ-<br>ing complex programs with multiple steps, numeri-<br>cal unit conversions, or resolving the ordering and<br>matching of the numbers and the years. Many error<br>cases involve both the numerical reasoning prob-<br>lems and misunderstandings of financial knowl-<br>edge. We show three representative error cases in<br>Figure 4.</p>",
            "id": 107,
            "page": 8,
            "text": "We sample 50 error cases from the results of the FinQANet (RoBERTa-large) model and analyze them manually. 15% of the errors are caused by the retriever, e.g., missing facts. Half of the rest are due to the lack of financial knowledge, such as the meaning of some terminology. And the rest half are primarily numerical reasoning errors, including complex programs with multiple steps, numerical unit conversions, or resolving the ordering and matching of the numbers and the years. Many error cases involve both the numerical reasoning problems and misunderstandings of financial knowledge. We show three representative error cases in Figure 4."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 2800
                },
                {
                    "x": 1977,
                    "y": 2800
                },
                {
                    "x": 1977,
                    "y": 2854
                },
                {
                    "x": 1270,
                    "y": 2854
                }
            ],
            "category": "paragraph",
            "html": "<p id='108' style='font-size:22px'>7 Conclusion and Future Work</p>",
            "id": 108,
            "page": 8,
            "text": "7 Conclusion and Future Work"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 2889
                },
                {
                    "x": 2196,
                    "y": 2889
                },
                {
                    "x": 2196,
                    "y": 3229
                },
                {
                    "x": 1268,
                    "y": 3229
                }
            ],
            "category": "paragraph",
            "html": "<p id='109' style='font-size:18px'>This paper introduces FINQA, a new expert-<br>annotated QA dataset that aims to tackle numerical<br>reasoning over real-world financial data. The ques-<br>tions in FINQA pose great challenge for existing<br>models to resolve domain-specific knowledge, as<br>well as to acquire complex numerical reasoning</p>",
            "id": 109,
            "page": 8,
            "text": "This paper introduces FINQA, a new expertannotated QA dataset that aims to tackle numerical reasoning over real-world financial data. The questions in FINQA pose great challenge for existing models to resolve domain-specific knowledge, as well as to acquire complex numerical reasoning"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 300
                },
                {
                    "x": 1214,
                    "y": 300
                },
                {
                    "x": 1214,
                    "y": 810
                },
                {
                    "x": 288,
                    "y": 810
                }
            ],
            "category": "paragraph",
            "html": "<p id='110' style='font-size:16px'>abilities. We propose baseline frameworks and con-<br>duct comprehensive experiments and analysis. The<br>results show that current large pre-trained models<br>still fall far behind the human expert performance.<br>This encourages potential future work on develop-<br>ing pre-training tasks for such realistic, complex<br>application domains. We believe FINQA should<br>serve as a valuable resource for the research com-<br>munity.</p>",
            "id": 110,
            "page": 9,
            "text": "abilities. We propose baseline frameworks and conduct comprehensive experiments and analysis. The results show that current large pre-trained models still fall far behind the human expert performance. This encourages potential future work on developing pre-training tasks for such realistic, complex application domains. We believe FINQA should serve as a valuable resource for the research community."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 855
                },
                {
                    "x": 861,
                    "y": 855
                },
                {
                    "x": 861,
                    "y": 911
                },
                {
                    "x": 288,
                    "y": 911
                }
            ],
            "category": "paragraph",
            "html": "<p id='111' style='font-size:18px'>8 Ethical Considerations</p>",
            "id": 111,
            "page": 9,
            "text": "8 Ethical Considerations"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 948
                },
                {
                    "x": 1212,
                    "y": 948
                },
                {
                    "x": 1212,
                    "y": 1511
                },
                {
                    "x": 287,
                    "y": 1511
                }
            ],
            "category": "paragraph",
            "html": "<p id='112' style='font-size:16px'>Data Access and Licensing. We develop<br>FINQA based on the publicly available earnings<br>reports of S&P 500 companies from 1999 to 2019,<br>collected in the FinTabNet dataset (Zheng et al.,<br>2021). The FinTabNet dataset is publicly available<br>under the CDLA-Permissive° license, which<br>permits us to create additional annotations on top<br>of the data (\"Enhanced Data\" , §1.5 of CDLA)<br>and publish the annotations (\"Publish\", §1.9 of<br>CDLA).</p>",
            "id": 112,
            "page": 9,
            "text": "Data Access and Licensing. We develop FINQA based on the publicly available earnings reports of S&P 500 companies from 1999 to 2019, collected in the FinTabNet dataset (Zheng , 2021). The FinTabNet dataset is publicly available under the CDLA-Permissive° license, which permits us to create additional annotations on top of the data (\"Enhanced Data\" , §1.5 of CDLA) and publish the annotations (\"Publish\", §1.9 of CDLA)."
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 1560
                },
                {
                    "x": 1212,
                    "y": 1560
                },
                {
                    "x": 1212,
                    "y": 2511
                },
                {
                    "x": 286,
                    "y": 2511
                }
            ],
            "category": "paragraph",
            "html": "<p id='113' style='font-size:16px'>Dataset Collection Process and Conditions.<br>For the annotation of our FINQA dataset on Up-<br>work, we first launch interviews of the task intro-<br>duction with 4 example questions, which is paid as<br>$30, for them to try a few examples to get informed<br>and familiar with the task. Then based on their con-<br>sents to continue working on the large-scale job,<br>we discuss with the workers to reach agreements<br>on the compensation before starting the large-scale<br>job. We pay around $2.0 per question, and the<br>hourly rates are discussed and agreed upon with<br>both sides based on the working speed of differ-<br>ent workers. Among all eleven US-based hires,<br>the average hourly rate is $35.0, and the minimum<br>and maximum hourly rates are $20 and $50, re-<br>spectively. The evaluation tasks follow the similar<br>procedure, and each question is paid as $2.0.</p>",
            "id": 113,
            "page": 9,
            "text": "Dataset Collection Process and Conditions. For the annotation of our FINQA dataset on Upwork, we first launch interviews of the task introduction with 4 example questions, which is paid as $30, for them to try a few examples to get informed and familiar with the task. Then based on their consents to continue working on the large-scale job, we discuss with the workers to reach agreements on the compensation before starting the large-scale job. We pay around $2.0 per question, and the hourly rates are discussed and agreed upon with both sides based on the working speed of different workers. Among all eleven US-based hires, the average hourly rate is $35.0, and the minimum and maximum hourly rates are $20 and $50, respectively. The evaluation tasks follow the similar procedure, and each question is paid as $2.0."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 2547
                },
                {
                    "x": 1212,
                    "y": 2547
                },
                {
                    "x": 1212,
                    "y": 2887
                },
                {
                    "x": 287,
                    "y": 2887
                }
            ],
            "category": "paragraph",
            "html": "<p id='114' style='font-size:16px'>IRB (Institutional Review Board) Approval.<br>This projectis approved by our Institutional Review<br>Board (IRB). The systems trained using our dataset<br>are primarily intended to be used as augmenting<br>human decision-making in financial analysis, but<br>not as a replacement of human experts.</p>",
            "id": 114,
            "page": 9,
            "text": "IRB (Institutional Review Board) Approval. This projectis approved by our Institutional Review Board (IRB). The systems trained using our dataset are primarily intended to be used as augmenting human decision-making in financial analysis, but not as a replacement of human experts."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 2935
                },
                {
                    "x": 669,
                    "y": 2935
                },
                {
                    "x": 669,
                    "y": 2992
                },
                {
                    "x": 288,
                    "y": 2992
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='115' style='font-size:22px'>Acknowledgment</p>",
            "id": 115,
            "page": 9,
            "text": "Acknowledgment"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 3029
                },
                {
                    "x": 1213,
                    "y": 3029
                },
                {
                    "x": 1213,
                    "y": 3141
                },
                {
                    "x": 287,
                    "y": 3141
                }
            ],
            "category": "paragraph",
            "html": "<p id='116' style='font-size:14px'>We thank the anonymous reviewers for their<br>thoughtful comments. This research was supported</p>",
            "id": 116,
            "page": 9,
            "text": "We thank the anonymous reviewers for their thoughtful comments. This research was supported"
        },
        {
            "bounding_box": [
                {
                    "x": 340,
                    "y": 3176
                },
                {
                    "x": 1087,
                    "y": 3176
                },
                {
                    "x": 1087,
                    "y": 3226
                },
                {
                    "x": 340,
                    "y": 3226
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='117' style='font-size:20px'>6CDLA-Permissive: https://cdla.dev/sharing-1-0/</p>",
            "id": 117,
            "page": 9,
            "text": "6CDLA-Permissive: https://cdla.dev/sharing-1-0/"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 301
                },
                {
                    "x": 2197,
                    "y": 301
                },
                {
                    "x": 2197,
                    "y": 526
                },
                {
                    "x": 1268,
                    "y": 526
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='118' style='font-size:16px'>by the J.P. Morgan Faculty research award. The au-<br>thors are solely responsible for the contents of the<br>paper and the opinions expressed in this publication<br>do not reflect those of the funding agencies.</p>",
            "id": 118,
            "page": 9,
            "text": "by the J.P. Morgan Faculty research award. The authors are solely responsible for the contents of the paper and the opinions expressed in this publication do not reflect those of the funding agencies."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 621
                },
                {
                    "x": 1512,
                    "y": 621
                },
                {
                    "x": 1512,
                    "y": 674
                },
                {
                    "x": 1269,
                    "y": 674
                }
            ],
            "category": "paragraph",
            "html": "<p id='119' style='font-size:20px'>References</p>",
            "id": 119,
            "page": 9,
            "text": "References"
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 701
                },
                {
                    "x": 2197,
                    "y": 701
                },
                {
                    "x": 2197,
                    "y": 1116
                },
                {
                    "x": 1270,
                    "y": 1116
                }
            ],
            "category": "paragraph",
            "html": "<p id='120' style='font-size:16px'>Md. Shad Akhtar, Abhishek Kumar, Deepanway<br>Ghosal, Asif Ekbal, and Pushpak Bhattacharyya.<br>2017. A multilayer perceptron based ensemble<br>technique for fine-grained financial sentiment anal-<br>ysis. In Proceedings of the 2017 Conference on<br>Empirical Methods in Natural Language Processing,<br>EMNLP 2017, Copenhagen, Denmark, September 9-<br>11, 2017, pages 540-546. Association for Computa-<br>tional Linguistics.</p>",
            "id": 120,
            "page": 9,
            "text": "Md. Shad Akhtar, Abhishek Kumar, Deepanway Ghosal, Asif Ekbal, and Pushpak Bhattacharyya. 2017. A multilayer perceptron based ensemble technique for fine-grained financial sentiment analysis. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017, Copenhagen, Denmark, September 911, 2017, pages 540-546. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 1154
                },
                {
                    "x": 2197,
                    "y": 1154
                },
                {
                    "x": 2197,
                    "y": 1292
                },
                {
                    "x": 1269,
                    "y": 1292
                }
            ],
            "category": "paragraph",
            "html": "<p id='121' style='font-size:14px'>Chris Alberti, Kenton Lee, and Michael Collins. 2019.<br>A BERT baseline for the natural questions. CoRR,<br>abs/1901.08634.</p>",
            "id": 121,
            "page": 9,
            "text": "Chris Alberti, Kenton Lee, and Michael Collins. 2019. A BERT baseline for the natural questions. CoRR, abs/1901.08634."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 1330
                },
                {
                    "x": 2197,
                    "y": 1330
                },
                {
                    "x": 2197,
                    "y": 1837
                },
                {
                    "x": 1270,
                    "y": 1837
                }
            ],
            "category": "paragraph",
            "html": "<p id='122' style='font-size:16px'>Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik<br>Koncel-Kedziorski, Yejin Choi, and Hannaneh Ha-<br>jishirzi. 2019. Mathqa: Towards interpretable math<br>word problem solving with operation-based for-<br>malisms. In Proceedings of the 2019 Conference<br>of the North American Chapter of the Association<br>for Computational Linguistics: Human Language<br>Technologies, NAACL-HLT 2019, Minneapolis, MN,<br>USA, June 2-7, 2019, Volume 1 (Long and Short Pa-<br>pers), pages 2357-2367. Association for Computa-<br>tional Linguistics.</p>",
            "id": 122,
            "page": 9,
            "text": "Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019. Mathqa: Towards interpretable math word problem solving with operation-based formalisms. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 2357-2367. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 1873
                },
                {
                    "x": 2195,
                    "y": 1873
                },
                {
                    "x": 2195,
                    "y": 2013
                },
                {
                    "x": 1268,
                    "y": 2013
                }
            ],
            "category": "paragraph",
            "html": "<p id='123' style='font-size:14px'>Dogu Araci. 2019. Finbert: Financial sentiment<br>analysis with pre-trained language models. CoRR,<br>abs/1908.10063.</p>",
            "id": 123,
            "page": 9,
            "text": "Dogu Araci. 2019. Finbert: Financial sentiment analysis with pre-trained language models. CoRR, abs/1908.10063."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 2052
                },
                {
                    "x": 2197,
                    "y": 2052
                },
                {
                    "x": 2197,
                    "y": 2189
                },
                {
                    "x": 1268,
                    "y": 2189
                }
            ],
            "category": "paragraph",
            "html": "<p id='124' style='font-size:14px'>Iz Beltagy, Matthew E. Peters, and Arman Cohan.<br>2020. Longformer: The long-document transformer.<br>CoRR, abs/2004.05150.</p>",
            "id": 124,
            "page": 9,
            "text": "Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. CoRR, abs/2004.05150."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 2227
                },
                {
                    "x": 2196,
                    "y": 2227
                },
                {
                    "x": 2196,
                    "y": 2640
                },
                {
                    "x": 1270,
                    "y": 2640
                }
            ],
            "category": "paragraph",
            "html": "<p id='125' style='font-size:16px'>Kezhen Chen, Qiuyuan Huang, Hamid Palangi, Paul<br>Smolensky, Kenneth D. Forbus, and Jianfeng Gao.<br>2020a. Mapping natural-language problems to<br>formal-language solutions using structured neural<br>representations. In Proceedings of the 37th Inter-<br>national Conference on Machine Learning, ICML<br>2020, 13-18 July 2020, Virtual Event, volume 119 of<br>Proceedings of Machine Learning Research, pages<br>1566-1575. PMLR.</p>",
            "id": 125,
            "page": 9,
            "text": "Kezhen Chen, Qiuyuan Huang, Hamid Palangi, Paul Smolensky, Kenneth D. Forbus, and Jianfeng Gao. 2020a. Mapping natural-language problems to formal-language solutions using structured neural representations. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 1566-1575. PMLR."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 2678
                },
                {
                    "x": 2196,
                    "y": 2678
                },
                {
                    "x": 2196,
                    "y": 3001
                },
                {
                    "x": 1271,
                    "y": 3001
                }
            ],
            "category": "paragraph",
            "html": "<p id='126' style='font-size:16px'>Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai<br>Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, and<br>William Yang Wang. 2020b. Tabfact: A large-scale<br>dataset for table-based fact verification. In 8th Inter-<br>national Conference on Learning Representations,<br>ICLR 2020, Addis Ababa, Ethiopia, April 26-30,<br>2020. OpenReview.net.</p>",
            "id": 126,
            "page": 9,
            "text": "Wenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, and William Yang Wang. 2020b. Tabfact: A large-scale dataset for table-based fact verification. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 3040
                },
                {
                    "x": 2198,
                    "y": 3040
                },
                {
                    "x": 2198,
                    "y": 3226
                },
                {
                    "x": 1271,
                    "y": 3226
                }
            ],
            "category": "paragraph",
            "html": "<p id='127' style='font-size:16px'>Wenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan<br>Xiong, Hong Wang, and William Yang Wang. 2020c.<br>Hybridqa: A dataset of multi-hop question answer-<br>ing over tabular and textual data. In Proceedings of</p>",
            "id": 127,
            "page": 9,
            "text": "Wenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong, Hong Wang, and William Yang Wang. 2020c. Hybridqa: A dataset of multi-hop question answering over tabular and textual data. In Proceedings of"
        },
        {
            "bounding_box": [
                {
                    "x": 332,
                    "y": 305
                },
                {
                    "x": 1214,
                    "y": 305
                },
                {
                    "x": 1214,
                    "y": 491
                },
                {
                    "x": 332,
                    "y": 491
                }
            ],
            "category": "paragraph",
            "html": "<p id='128' style='font-size:16px'>the 2020 Conference on Empirical Methods in Nat-<br>ural Language Processing: Findings, EMNLP 2020,<br>Online Event, 16-20 November 2020, pages 1026-<br>1036. Association for Computational Linguistics.</p>",
            "id": 128,
            "page": 10,
            "text": "the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, EMNLP 2020, Online Event, 16-20 November 2020, pages 10261036. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 531
                },
                {
                    "x": 1213,
                    "y": 531
                },
                {
                    "x": 1213,
                    "y": 856
                },
                {
                    "x": 289,
                    "y": 856
                }
            ],
            "category": "paragraph",
            "html": "<p id='129' style='font-size:18px'>Xinyun Chen, Chen Liang, Adams Wei Yu, Denny<br>Zhou, Dawn Song, and Quoc V. Le. 2020d. Neu-<br>ral symbolic reader: Scalable integration of dis-<br>tributed and symbolic representations for reading<br>comprehension. In 8th International Conference on<br>Learning Representations, ICLR 2020, Addis Ababa,<br>Ethiopia, April 26-30, 2020. OpenReview.net.</p>",
            "id": 129,
            "page": 10,
            "text": "Xinyun Chen, Chen Liang, Adams Wei Yu, Denny Zhou, Dawn Song, and Quoc V. Le. 2020d. Neural symbolic reader: Scalable integration of distributed and symbolic representations for reading comprehension. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 894
                },
                {
                    "x": 1214,
                    "y": 894
                },
                {
                    "x": 1214,
                    "y": 1220
                },
                {
                    "x": 289,
                    "y": 1220
                }
            ],
            "category": "paragraph",
            "html": "<p id='130' style='font-size:16px'>Min-Yuh Day and Chia-Chou Lee. 2016. Deep learn-<br>ing for financial sentiment analysis on finance news<br>providers. In 2016 IEEE/ACM International Confer-<br>ence on Advances in Social Networks Analysis and<br>Mining, ASONAM 2016, San Francisco, CA, USA,<br>August 18-21, 2016, pages 1127-1134. IEEE Com-<br>puter Society.</p>",
            "id": 130,
            "page": 10,
            "text": "Min-Yuh Day and Chia-Chou Lee. 2016. Deep learning for financial sentiment analysis on finance news providers. In 2016 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, ASONAM 2016, San Francisco, CA, USA, August 18-21, 2016, pages 1127-1134. IEEE Computer Society."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 1260
                },
                {
                    "x": 1213,
                    "y": 1260
                },
                {
                    "x": 1213,
                    "y": 1722
                },
                {
                    "x": 289,
                    "y": 1722
                }
            ],
            "category": "paragraph",
            "html": "<p id='131' style='font-size:18px'>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and<br>Kristina Toutanova. 2019. BERT: pre-training of<br>deep bidirectional transformers for language under-<br>standing. In Proceedings of the 2019 Conference<br>of the North American Chapter of the Association<br>for Computational Linguistics: Human Language<br>Technologies, NAACL-HLT 2019, Minneapolis, MN,<br>USA, June 2-7, 2019, Volume 1 (Long and Short Pa-<br>pers), pages 4171-4186. Association for Computa-<br>tional Linguistics.</p>",
            "id": 131,
            "page": 10,
            "text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4171-4186. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 1762
                },
                {
                    "x": 1214,
                    "y": 1762
                },
                {
                    "x": 1214,
                    "y": 2223
                },
                {
                    "x": 290,
                    "y": 2223
                }
            ],
            "category": "paragraph",
            "html": "<p id='132' style='font-size:18px'>Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel<br>Stanovsky, Sameer Singh, and Matt Gardner. 2019.<br>DROP: A reading comprehension benchmark requir-<br>ing discrete reasoning over paragraphs. In Proceed-<br>ings of the 2019 Conference of the North American<br>Chapter of the Association for Computational Lin-<br>guistics: Human Language Technologies, NAACL-<br>HLT 2019, Minneapolis, MN, USA, June 2-7, 2019,<br>Volume 1 (Long and Short Papers), pages 2368-<br>2378. Association for Computational Linguistics.</p>",
            "id": 132,
            "page": 10,
            "text": "Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACLHLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 23682378. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 2263
                },
                {
                    "x": 1215,
                    "y": 2263
                },
                {
                    "x": 1215,
                    "y": 2632
                },
                {
                    "x": 287,
                    "y": 2632
                }
            ],
            "category": "paragraph",
            "html": "<p id='133' style='font-size:20px'>Jingguang Han, Utsab Barman, Jer Hayes, Jinhua Du,<br>Edward Burgin, and Dadong Wan. 2018. Nextgen<br>AML: distributed deep learning based language tech-<br>nologies to augment anti money laundering inves-<br>tigation. In Proceedings of ACL 2018, Melbourne,<br>Australia, July 15-20, 2018, System Demonstrations,<br>pages 37-42. Association for Computational Lin-<br>guistics.</p>",
            "id": 133,
            "page": 10,
            "text": "Jingguang Han, Utsab Barman, Jer Hayes, Jinhua Du, Edward Burgin, and Dadong Wan. 2018. Nextgen AML: distributed deep learning based language technologies to augment anti money laundering investigation. In Proceedings of ACL 2018, Melbourne, Australia, July 15-20, 2018, System Demonstrations, pages 37-42. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 2674
                },
                {
                    "x": 1214,
                    "y": 2674
                },
                {
                    "x": 1214,
                    "y": 2814
                },
                {
                    "x": 289,
                    "y": 2814
                }
            ],
            "category": "paragraph",
            "html": "<p id='134' style='font-size:14px'>Morten Jerven. 2013. Poor numbers: how we are mis-<br>led by African development statistics and what to do<br>about it. Cornell University Press.</p>",
            "id": 134,
            "page": 10,
            "text": "Morten Jerven. 2013. Poor numbers: how we are misled by African development statistics and what to do about it. Cornell University Press."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 2856
                },
                {
                    "x": 1214,
                    "y": 2856
                },
                {
                    "x": 1214,
                    "y": 3224
                },
                {
                    "x": 289,
                    "y": 3224
                }
            ],
            "category": "paragraph",
            "html": "<p id='135' style='font-size:16px'>Bugeun Kim, Kyung Seo Ki, Donggeon Lee, and Gah-<br>gene Gweon. 2020. Point to the expression: Solv-<br>ing algebraic word problems using the expression-<br>pointer transformer model. In Proceedings of the<br>2020 Conference on Empirical Methods in Natu-<br>ral Language Processing, EMNLP 2020, Online,<br>November 16-20, 2020, pages 3768-3779. Associ-<br>ation for Computational Linguistics.</p>",
            "id": 135,
            "page": 10,
            "text": "Bugeun Kim, Kyung Seo Ki, Donggeon Lee, and Gahgene Gweon. 2020. Point to the expression: Solving algebraic word problems using the expressionpointer transformer model. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 3768-3779. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 304
                },
                {
                    "x": 2197,
                    "y": 304
                },
                {
                    "x": 2197,
                    "y": 536
                },
                {
                    "x": 1271,
                    "y": 536
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='136' style='font-size:16px'>Diederik P. Kingma and Jimmy Ba. 2015. Adam: A<br>method for stochastic optimization. In 3rd Inter-<br>national Conference on Learning Representations,<br>ICLR 2015, San Diego, CA, USA, May 7-9, 2015,<br>Conference Track Proceedings.</p>",
            "id": 136,
            "page": 10,
            "text": "Diederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 567
                },
                {
                    "x": 2197,
                    "y": 567
                },
                {
                    "x": 2197,
                    "y": 981
                },
                {
                    "x": 1269,
                    "y": 981
                }
            ],
            "category": "paragraph",
            "html": "<p id='137' style='font-size:18px'>Rik Koncel-Kedziorski, Subhro Roy, Aida Amini,<br>Nate Kushman, and Hannaneh Hajishirzi. 2016.<br>MAWPS: A math word problem repository. In<br>NAACL HLT 2016, The 2016 Conference of the<br>North American Chapter of the Association for Com-<br>putational Linguistics: Human Language Technolo-<br>gies, San Diego California, USA, June 12-17, 2016,<br>pages 1152-1157. The Association for Computa-<br>tional Linguistics.</p>",
            "id": 137,
            "page": 10,
            "text": "Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. 2016. MAWPS: A math word problem repository. In NAACL HLT 2016, The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, San Diego California, USA, June 12-17, 2016, pages 1152-1157. The Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 1014
                },
                {
                    "x": 2195,
                    "y": 1014
                },
                {
                    "x": 2195,
                    "y": 1243
                },
                {
                    "x": 1271,
                    "y": 1243
                }
            ],
            "category": "paragraph",
            "html": "<p id='138' style='font-size:22px'>Ann-Christina Lange, Marc Lenglet, and Robert<br>Seyfert. 2016. Cultures of high-frequency trading:<br>Mapping the landscape of algorithmic developments<br>in contemporary financial markets. Economy and<br>Society, 45(2):149-165.</p>",
            "id": 138,
            "page": 10,
            "text": "Ann-Christina Lange, Marc Lenglet, and Robert Seyfert. 2016. Cultures of high-frequency trading: Mapping the landscape of algorithmic developments in contemporary financial markets. Economy and Society, 45(2):149-165."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1276
                },
                {
                    "x": 2197,
                    "y": 1276
                },
                {
                    "x": 2197,
                    "y": 1506
                },
                {
                    "x": 1272,
                    "y": 1506
                }
            ],
            "category": "paragraph",
            "html": "<p id='139' style='font-size:20px'>Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-<br>dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,<br>Luke Zettlemoyer, and Veselin Stoyanov. 2019.<br>Roberta: A robustly optimized BERT pretraining ap-<br>proach. CoRR, abs/1907.11692.</p>",
            "id": 139,
            "page": 10,
            "text": "Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 1541
                },
                {
                    "x": 2197,
                    "y": 1541
                },
                {
                    "x": 2197,
                    "y": 1817
                },
                {
                    "x": 1270,
                    "y": 1817
                }
            ],
            "category": "paragraph",
            "html": "<p id='140' style='font-size:18px'>Zhuang Liu, Degen Huang, Kaiyu Huang, Zhuang Li,<br>and Jun Zhao. 2020. Finbert: A pre-trained finan-<br>cial language representation model for financial text<br>mining. In Proceedings of the Twenty-Ninth Inter-<br>national Joint Conference on Artificial Intelligence,<br>IJCAI 2020, pages 4513-4519. ijcai.org.</p>",
            "id": 140,
            "page": 10,
            "text": "Zhuang Liu, Degen Huang, Kaiyu Huang, Zhuang Li, and Jun Zhao. 2020. Finbert: A pre-trained financial language representation model for financial text mining. In Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI 2020, pages 4513-4519. ijcai.org."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 1848
                },
                {
                    "x": 2197,
                    "y": 1848
                },
                {
                    "x": 2197,
                    "y": 1943
                },
                {
                    "x": 1269,
                    "y": 1943
                }
            ],
            "category": "paragraph",
            "html": "<p id='141' style='font-size:14px'>Donald MacKenzie. 2008. An engine, not a camera:<br>How financial models shape markets. Mit Press.</p>",
            "id": 141,
            "page": 10,
            "text": "Donald MacKenzie. 2008. An engine, not a camera: How financial models shape markets. Mit Press."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 1975
                },
                {
                    "x": 2194,
                    "y": 1975
                },
                {
                    "x": 2194,
                    "y": 2204
                },
                {
                    "x": 1271,
                    "y": 2204
                }
            ],
            "category": "paragraph",
            "html": "<p id='142' style='font-size:20px'>Donald MacKenzie, Daniel Beunza, Yuval Millo, and<br>Juan Pablo Pardo-Guerra. 2012. Drilling through<br>the allegheny mountains: Liquidity, materiality and<br>high-frequency trading. Journal of cultural econ-<br>omy, 5(3):279-296.</p>",
            "id": 142,
            "page": 10,
            "text": "Donald MacKenzie, Daniel Beunza, Yuval Millo, and Juan Pablo Pardo-Guerra. 2012. Drilling through the allegheny mountains: Liquidity, materiality and high-frequency trading. Journal of cultural economy, 5(3):279-296."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 2240
                },
                {
                    "x": 2194,
                    "y": 2240
                },
                {
                    "x": 2194,
                    "y": 2422
                },
                {
                    "x": 1270,
                    "y": 2422
                }
            ],
            "category": "paragraph",
            "html": "<p id='143' style='font-size:16px'>Armineh Nourbakhsh and Grace Bang. 2019. A<br>framework for anomaly detection using language<br>modeling, and its applications to finance. CoRR,<br>abs/1908.09156.</p>",
            "id": 143,
            "page": 10,
            "text": "Armineh Nourbakhsh and Grace Bang. 2019. A framework for anomaly detection using language modeling, and its applications to finance. CoRR, abs/1908.09156."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 2456
                },
                {
                    "x": 2197,
                    "y": 2456
                },
                {
                    "x": 2197,
                    "y": 2870
                },
                {
                    "x": 1271,
                    "y": 2870
                }
            ],
            "category": "paragraph",
            "html": "<p id='144' style='font-size:18px'>Panupong Pasupat and Percy Liang. 2015. Composi-<br>tional semantic parsing on semi-structured tables. In<br>Proceedings of the 53rd Annual Meeting of the Asso-<br>ciationfor Computational Linguistics and the 7th In-<br>ternational Joint Conference on Natural Language<br>Processing of the Asian Federation of Natural Lan-<br>guage Processing, ACL 2015, July 26-31, 2015, Bei-<br>jing, China, Volume 1: Long Papers, pages 1470-<br>1480. The Association for Computer Linguistics.</p>",
            "id": 144,
            "page": 10,
            "text": "Panupong Pasupat and Percy Liang. 2015. Compositional semantic parsing on semi-structured tables. In Proceedings of the 53rd Annual Meeting of the Associationfor Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL 2015, July 26-31, 2015, Beijing, China, Volume 1: Long Papers, pages 14701480. The Association for Computer Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 2903
                },
                {
                    "x": 2198,
                    "y": 2903
                },
                {
                    "x": 2198,
                    "y": 3225
                },
                {
                    "x": 1270,
                    "y": 3225
                }
            ],
            "category": "paragraph",
            "html": "<p id='145' style='font-size:18px'>Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018.<br>Know what you don't know: Unanswerable ques-<br>tions for squad. In Proceedings of the 56th Annual<br>Meeting of the Association for Computational Lin-<br>guistics, ACL 2018, Melbourne, Australia, July 15-<br>20, 2018, Volume 2: Short Papers, pages 784-789.<br>Association for Computational Linguistics.</p>",
            "id": 145,
            "page": 10,
            "text": "Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don't know: Unanswerable questions for squad. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 1520, 2018, Volume 2: Short Papers, pages 784-789. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 303
                },
                {
                    "x": 1214,
                    "y": 303
                },
                {
                    "x": 1214,
                    "y": 674
                },
                {
                    "x": 289,
                    "y": 674
                }
            ],
            "category": "paragraph",
            "html": "<p id='146' style='font-size:16px'>Alon Talmor and Jonathan Berant. 2018. The web as<br>a knowledge-base for answering complex questions.<br>In Proceedings of the 2018 Conference of the North<br>American Chapter of the Association for Computa-<br>tional Linguistics: Human Language Technologies,<br>NAACL-HLT 2018, New Orleans, Louisiana, USA,<br>June 1-6, 2018, Volume 1 (Long Papers), pages 641-<br>651. Association for Computational Linguistics.</p>",
            "id": 146,
            "page": 11,
            "text": "Alon Talmor and Jonathan Berant. 2018. The web as a knowledge-base for answering complex questions. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long Papers), pages 641651. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 716
                },
                {
                    "x": 1214,
                    "y": 716
                },
                {
                    "x": 1214,
                    "y": 1043
                },
                {
                    "x": 287,
                    "y": 1043
                }
            ],
            "category": "paragraph",
            "html": "<p id='147' style='font-size:14px'>Chuan-Ju Wang, Ming-Feng Tsai, Tse Liu, and Chin-<br>Ting Chang. 2013. Financial sentiment analysis<br>for risk prediction. In Sixth International Joint<br>Conference on Natural Language Processing, IJC-<br>NLP 2013, Nagoya, Japan, October 14-18, 2013,<br>pages 802-808. Asian Federation of Natural Lan-<br>guage Processing / ACL.</p>",
            "id": 147,
            "page": 11,
            "text": "Chuan-Ju Wang, Ming-Feng Tsai, Tse Liu, and ChinTing Chang. 2013. Financial sentiment analysis for risk prediction. In Sixth International Joint Conference on Natural Language Processing, IJCNLP 2013, Nagoya, Japan, October 14-18, 2013, pages 802-808. Asian Federation of Natural Language Processing / ACL."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 1084
                },
                {
                    "x": 1215,
                    "y": 1084
                },
                {
                    "x": 1215,
                    "y": 1501
                },
                {
                    "x": 289,
                    "y": 1501
                }
            ],
            "category": "paragraph",
            "html": "<p id='148' style='font-size:16px'>Weikang Wang, Jiajun Zhang, Qian Li, Chengqing<br>Zong, and Zhifei Li. 2019. Are you for real? de-<br>tecting identity fraud via dialogue interactions. In<br>Proceedings of the 2019 Conference on Empirical<br>Methods in Natural Language Processing and the<br>9th International Joint Conference on Natural Lan-<br>guage Processing, EMNLP-IJCNLP 2019, Hong<br>Kong, China, November 3-7, 2019, pages 1762-<br>1771. Association for Computational Linguistics.</p>",
            "id": 148,
            "page": 11,
            "text": "Weikang Wang, Jiajun Zhang, Qian Li, Chengqing Zong, and Zhifei Li. 2019. Are you for real? detecting identity fraud via dialogue interactions. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 17621771. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 1544
                },
                {
                    "x": 1213,
                    "y": 1544
                },
                {
                    "x": 1213,
                    "y": 1687
                },
                {
                    "x": 290,
                    "y": 1687
                }
            ],
            "category": "paragraph",
            "html": "<p id='149' style='font-size:16px'>Yi Yang, Mark Christopher Siy Uy, and Allen Huang.<br>2020. Finbert: A pretrained language model for fi-<br>nancial communications. CoRR, abs/2006.08097.</p>",
            "id": 149,
            "page": 11,
            "text": "Yi Yang, Mark Christopher Siy Uy, and Allen Huang. 2020. Finbert: A pretrained language model for financial communications. CoRR, abs/2006.08097."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 1729
                },
                {
                    "x": 1214,
                    "y": 1729
                },
                {
                    "x": 1214,
                    "y": 2147
                },
                {
                    "x": 288,
                    "y": 2147
                }
            ],
            "category": "paragraph",
            "html": "<p id='150' style='font-size:16px'>Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-<br>gio, William W. Cohen, Ruslan Salakhutdinov, and<br>Christopher D. Manning. 2018. Hotpotqa: A dataset<br>for diverse, explainable multi-hop question answer-<br>ing. In Proceedings of the 2018 Conference on<br>Empirical Methods in Natural Language Process-<br>ing, Brussels, Belgium, October 31 - November 4,<br>2018, pages 2369-2380. Association for Computa-<br>tional Linguistics.</p>",
            "id": 150,
            "page": 11,
            "text": "Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. Hotpotqa: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 2369-2380. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 2188
                },
                {
                    "x": 1214,
                    "y": 2188
                },
                {
                    "x": 1214,
                    "y": 2694
                },
                {
                    "x": 287,
                    "y": 2694
                }
            ],
            "category": "paragraph",
            "html": "<p id='151' style='font-size:14px'>Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,<br>Dongxu Wang, Zifan Li, James Ma, Irene Li,<br>Qingning Yao, Shanelle Roman, Zilin Zhang, and<br>Dragomir R. Radev. 2018. Spider: A large-<br>scale human-labeled dataset for complex and cross-<br>domain semantic parsing and text-to-sql task. In<br>Proceedings of the 2018 Conference on Empirical<br>Methods in Natural Language Processing, Brussels,<br>Belgium, October 31 - November 4, 2018, pages<br>3911-3921. Association for Computational Linguis-<br>tics.</p>",
            "id": 151,
            "page": 11,
            "text": "Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir R. Radev. 2018. Spider: A largescale human-labeled dataset for complex and crossdomain semantic parsing and text-to-sql task. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pages 3911-3921. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 2739
                },
                {
                    "x": 1214,
                    "y": 2739
                },
                {
                    "x": 1214,
                    "y": 3021
                },
                {
                    "x": 289,
                    "y": 3021
                }
            ],
            "category": "paragraph",
            "html": "<p id='152' style='font-size:16px'>Xinyi Zheng, Doug Burdick, Lucian Popa, Peter<br>Zhong, and Nancy Xin Ru Wang. 2021. Global ta-<br>ble extractor (gte): A framework for joint table iden-<br>tification and cell structure recognition using visual<br>context. Winter Conference for Applications in Com-<br>puter Vision (WACV).</p>",
            "id": 152,
            "page": 11,
            "text": "Xinyi Zheng, Doug Burdick, Lucian Popa, Peter Zhong, and Nancy Xin Ru Wang. 2021. Global table extractor (gte): A framework for joint table identification and cell structure recognition using visual context. Winter Conference for Applications in Computer Vision (WACV)."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 3078
                },
                {
                    "x": 1044,
                    "y": 3078
                },
                {
                    "x": 1044,
                    "y": 3132
                },
                {
                    "x": 288,
                    "y": 3132
                }
            ],
            "category": "paragraph",
            "html": "<p id='153' style='font-size:20px'>Appendix A: Operation Definitions</p>",
            "id": 153,
            "page": 11,
            "text": "Appendix A: Operation Definitions"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 3174
                },
                {
                    "x": 1041,
                    "y": 3174
                },
                {
                    "x": 1041,
                    "y": 3224
                },
                {
                    "x": 288,
                    "y": 3224
                }
            ],
            "category": "paragraph",
            "html": "<p id='154' style='font-size:16px'>We describe all the operations in Table 4.</p>",
            "id": 154,
            "page": 11,
            "text": "We describe all the operations in Table 4."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 301
                },
                {
                    "x": 1972,
                    "y": 301
                },
                {
                    "x": 1972,
                    "y": 353
                },
                {
                    "x": 1271,
                    "y": 353
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='155' style='font-size:20px'>Appendix B: Experiment Details</p>",
            "id": 155,
            "page": 11,
            "text": "Appendix B: Experiment Details"
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 389
                },
                {
                    "x": 2198,
                    "y": 389
                },
                {
                    "x": 2198,
                    "y": 1788
                },
                {
                    "x": 1269,
                    "y": 1788
                }
            ],
            "category": "paragraph",
            "html": "<p id='156' style='font-size:16px'>All the validation results of the baselines are shown<br>in Table 5. The trainings of all models are con-<br>ducted on TITAN RTX GPUs. All the implementa-<br>tion and pre-trained models are based on the hug-<br>gingface transformers library. We use the Adam<br>optimizer (Kingma and Ba, 2015). The parameter<br>settings are the following:<br>Retriever The learning rate is set as 3e-5, with<br>batch size of 16.<br>TF-IDF + Single Op We use the TF-IDF from the<br>Scikit-learn library.<br>FinQANet The learning rate is set as 1e-5. For<br>Bert-base, Roberta-base, and finBert we use batch<br>size of 32; For Bert-large and RoBerta-large we use<br>batch size of 16 due to GPU memory constraints.<br>Retriever + Seq2seq A bidirectional LSTM is<br>used for encoding the input, then an LSTM is used<br>for decoding with attention. Learning rate is set as<br>1e-3, hidden size as 100.<br>Retriever + NeRd The parameter settings are the<br>same as FinQANet.<br>Pre-Trained Longformer We truncate the maxi-<br>mum input length as 2,000. The learning rate is set<br>as 2e-5, with batch size of 16 due to GPU memory<br>constraints.</p>",
            "id": 156,
            "page": 11,
            "text": "All the validation results of the baselines are shown in Table 5. The trainings of all models are conducted on TITAN RTX GPUs. All the implementation and pre-trained models are based on the huggingface transformers library. We use the Adam optimizer (Kingma and Ba, 2015). The parameter settings are the following: Retriever The learning rate is set as 3e-5, with batch size of 16. TF-IDF + Single Op We use the TF-IDF from the Scikit-learn library. FinQANet The learning rate is set as 1e-5. For Bert-base, Roberta-base, and finBert we use batch size of 32; For Bert-large and RoBerta-large we use batch size of 16 due to GPU memory constraints. Retriever + Seq2seq A bidirectional LSTM is used for encoding the input, then an LSTM is used for decoding with attention. Learning rate is set as 1e-3, hidden size as 100. Retriever + NeRd The parameter settings are the same as FinQANet. Pre-Trained Longformer We truncate the maximum input length as 2,000. The learning rate is set as 2e-5, with batch size of 16 due to GPU memory constraints."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 1804
                },
                {
                    "x": 2189,
                    "y": 1804
                },
                {
                    "x": 2189,
                    "y": 1906
                },
                {
                    "x": 1269,
                    "y": 1906
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='157' style='font-size:14px'>For more modeling details refer to our released<br>code.</p>",
            "id": 157,
            "page": 11,
            "text": "For more modeling details refer to our released code."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1954
                },
                {
                    "x": 1833,
                    "y": 1954
                },
                {
                    "x": 1833,
                    "y": 2008
                },
                {
                    "x": 1272,
                    "y": 2008
                }
            ],
            "category": "paragraph",
            "html": "<p id='158' style='font-size:18px'>Appendix C: Case Studies</p>",
            "id": 158,
            "page": 11,
            "text": "Appendix C: Case Studies"
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 2041
                },
                {
                    "x": 2195,
                    "y": 2041
                },
                {
                    "x": 2195,
                    "y": 2208
                },
                {
                    "x": 1269,
                    "y": 2208
                }
            ],
            "category": "paragraph",
            "html": "<p id='159' style='font-size:16px'>Here we provide more case studies with the full in-<br>put reports. For all the examples the gold evidence<br>is highlighted in blue.</p>",
            "id": 159,
            "page": 11,
            "text": "Here we provide more case studies with the full input reports. For all the examples the gold evidence is highlighted in blue."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 2253
                },
                {
                    "x": 2011,
                    "y": 2253
                },
                {
                    "x": 2011,
                    "y": 2308
                },
                {
                    "x": 1271,
                    "y": 2308
                }
            ],
            "category": "paragraph",
            "html": "<p id='160' style='font-size:18px'>Appendix D: Annotation Interface</p>",
            "id": 160,
            "page": 11,
            "text": "Appendix D: Annotation Interface"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 2339
                },
                {
                    "x": 2195,
                    "y": 2339
                },
                {
                    "x": 2195,
                    "y": 2736
                },
                {
                    "x": 1268,
                    "y": 2736
                }
            ],
            "category": "paragraph",
            "html": "<p id='161' style='font-size:16px'>We use Turkle7 to build our annotation platform,<br>which is a Django-based web application that can<br>run in a local server. Figure 7 and Figure 8 show<br>our annotation interface. After the annotators finish<br>one example, they will use the validation check<br>button to automatically check the validity of their<br>inputs.</p>",
            "id": 161,
            "page": 11,
            "text": "We use Turkle7 to build our annotation platform, which is a Django-based web application that can run in a local server. Figure 7 and Figure 8 show our annotation interface. After the annotators finish one example, they will use the validation check button to automatically check the validity of their inputs."
        },
        {
            "bounding_box": [
                {
                    "x": 1323,
                    "y": 3177
                },
                {
                    "x": 1818,
                    "y": 3177
                },
                {
                    "x": 1818,
                    "y": 3224
                },
                {
                    "x": 1323,
                    "y": 3224
                }
            ],
            "category": "paragraph",
            "html": "<p id='162' style='font-size:22px'>7https://github.com/hltooe/turkle</p>",
            "id": 162,
            "page": 11,
            "text": "7https://github.com/hltooe/turkle"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 287
                },
                {
                    "x": 2182,
                    "y": 287
                },
                {
                    "x": 2182,
                    "y": 1249
                },
                {
                    "x": 291,
                    "y": 1249
                }
            ],
            "category": "table",
            "html": "<table id='163' style='font-size:20px'><tr><td>Name</td><td>Arguments</td><td>Output</td><td>Description</td></tr><tr><td>add</td><td>number1, number2</td><td>number</td><td>add two numbers: number1 + number2</td></tr><tr><td>subtract</td><td>number1, number2</td><td>number</td><td>subtract two numbers: number1 - number2</td></tr><tr><td>multiply</td><td>numberl, number2</td><td>number</td><td>multiply two numbers: number1 · number2</td></tr><tr><td>divide</td><td>number1, number2</td><td>number</td><td>multiply two numbers: number1 /number2</td></tr><tr><td>exp</td><td>number1, number2</td><td>number</td><td>exponential: number1 number2</td></tr><tr><td>greater</td><td>numberl, number2</td><td>bool</td><td>comparison: number1 > number2</td></tr><tr><td>table-sum</td><td>table header</td><td>number</td><td>the summation of one table row</td></tr><tr><td>table-average</td><td>table header</td><td>number</td><td>the average of one table row</td></tr><tr><td>table-max</td><td>table header</td><td>number</td><td>the maximum number of one table row</td></tr><tr><td>table-min</td><td>table header</td><td>number</td><td>the minimum number of one table row</td></tr></table>",
            "id": 163,
            "page": 12,
            "text": "Name Arguments Output Description  add number1, number2 number add two numbers: number1 + number2  subtract number1, number2 number subtract two numbers: number1 - number2  multiply numberl, number2 number multiply two numbers: number1 · number2  divide number1, number2 number multiply two numbers: number1 /number2  exp number1, number2 number exponential: number1 number2  greater numberl, number2 bool comparison: number1 > number2  table-sum table header number the summation of one table row  table-average table header number the average of one table row  table-max table header number the maximum number of one table row  table-min table header number"
        },
        {
            "bounding_box": [
                {
                    "x": 960,
                    "y": 1279
                },
                {
                    "x": 1520,
                    "y": 1279
                },
                {
                    "x": 1520,
                    "y": 1321
                },
                {
                    "x": 960,
                    "y": 1321
                }
            ],
            "category": "caption",
            "html": "<caption id='164' style='font-size:18px'>Table 4: Definitions of all operations</caption>",
            "id": 164,
            "page": 12,
            "text": "Table 4: Definitions of all operations"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 1846
                },
                {
                    "x": 1201,
                    "y": 1846
                },
                {
                    "x": 1201,
                    "y": 2670
                },
                {
                    "x": 291,
                    "y": 2670
                }
            ],
            "category": "table",
            "html": "<table id='165' style='font-size:16px'><tr><td>Baselines</td><td>Execution Accuracy (%)</td><td>Program Accuracy (%)</td></tr><tr><td>TF-IDF + Single Op</td><td>1.65</td><td>1.65</td></tr><tr><td>Retriever + Direct Generation</td><td>0.87</td><td>-</td></tr><tr><td>Pre-Trained Longformer (base)</td><td>23.83</td><td>22.56</td></tr><tr><td>Retriever + Seq2seq</td><td>18.76</td><td>17.52</td></tr><tr><td>Retriever + NeRd (BERT-base)</td><td>47.53</td><td>45.37</td></tr><tr><td>FinQANet (FinBert)</td><td>46.64</td><td>44.11</td></tr><tr><td>FinQANet (BERT-base)</td><td>49.91</td><td>47.15</td></tr><tr><td>FinQANet (BERT-large)</td><td>53.86</td><td>50.95</td></tr><tr><td>FinQANet (RoBerta-base)</td><td>56.27</td><td>53.49</td></tr><tr><td>FinQANet (RoBerta-large)</td><td>61.22</td><td>58.05</td></tr></table>",
            "id": 165,
            "page": 12,
            "text": "Baselines Execution Accuracy (%) Program Accuracy (%)  TF-IDF + Single Op 1.65 1.65  Retriever + Direct Generation 0.87  Pre-Trained Longformer (base) 23.83 22.56  Retriever + Seq2seq 18.76 17.52  Retriever + NeRd (BERT-base) 47.53 45.37  FinQANet (FinBert) 46.64 44.11  FinQANet (BERT-base) 49.91 47.15  FinQANet (BERT-large) 53.86 50.95  FinQANet (RoBerta-base) 56.27 53.49  FinQANet (RoBerta-large) 61.22"
        },
        {
            "bounding_box": [
                {
                    "x": 493,
                    "y": 2701
                },
                {
                    "x": 1002,
                    "y": 2701
                },
                {
                    "x": 1002,
                    "y": 2742
                },
                {
                    "x": 493,
                    "y": 2742
                }
            ],
            "category": "caption",
            "html": "<caption id='166' style='font-size:14px'>Table 5: Results on validation set</caption>",
            "id": 166,
            "page": 12,
            "text": "Table 5: Results on validation set"
        },
        {
            "bounding_box": [
                {
                    "x": 344,
                    "y": 510
                },
                {
                    "x": 774,
                    "y": 510
                },
                {
                    "x": 774,
                    "y": 541
                },
                {
                    "x": 344,
                    "y": 541
                }
            ],
            "category": "paragraph",
            "html": "<p id='167' style='font-size:20px'>Input Report AWK/2014/page_ 121.pdf</p>",
            "id": 167,
            "page": 13,
            "text": "Input Report AWK/2014/page_ 121.pdf"
        },
        {
            "bounding_box": [
                {
                    "x": 340,
                    "y": 550
                },
                {
                    "x": 2123,
                    "y": 550
                },
                {
                    "x": 2123,
                    "y": 708
                },
                {
                    "x": 340,
                    "y": 708
                }
            ],
            "category": "paragraph",
            "html": "<p id='168' style='font-size:16px'>(abbreviate 20 sentences)... the ppaca effectively changes the tax treatment of federal subsidies paid to sponsors of retiree health benefit plans that provide a benefit<br>that is at least actuarially equivalent to the benefits under medicare part d · the acts effectively make the subsidy payments taxable in tax years beginning after december<br>31 , 2012 and as a result , the company followed its original accounting for the underfunded status of the other postretirement benefits for the medicare part d adjustment<br>and recorded a reduction in deferred tax assets and an increase in its regulatory assets amounting to $ 6348 and $ 6241 at december 31 , 2014 and 2013 , respectively ·<br>the following table summarizes the changes in the company 2019s gross liability , excluding interest and penalties , for unrecognized tax benefits:</p>",
            "id": 168,
            "page": 13,
            "text": "(abbreviate 20 sentences)... the ppaca effectively changes the tax treatment of federal subsidies paid to sponsors of retiree health benefit plans that provide a benefit that is at least actuarially equivalent to the benefits under medicare part d · the acts effectively make the subsidy payments taxable in tax years beginning after december 31 , 2012 and as a result , the company followed its original accounting for the underfunded status of the other postretirement benefits for the medicare part d adjustment and recorded a reduction in deferred tax assets and an increase in its regulatory assets amounting to $ 6348 and $ 6241 at december 31 , 2014 and 2013 , respectively · the following table summarizes the changes in the company 2019s gross liability , excluding interest and penalties , for unrecognized tax benefits:"
        },
        {
            "bounding_box": [
                {
                    "x": 346,
                    "y": 717
                },
                {
                    "x": 1217,
                    "y": 717
                },
                {
                    "x": 1217,
                    "y": 1016
                },
                {
                    "x": 346,
                    "y": 1016
                }
            ],
            "category": "table",
            "html": "<br><table id='169' style='font-size:16px'><tr><td>balance at january 1 2013</td><td>$ 180993</td></tr><tr><td>increases in current period tax position</td><td>27229</td></tr><tr><td>decreases in prior period measurement of tax positions</td><td>-30275 ( 30275 )</td></tr><tr><td>balance at december 31 2013</td><td>$ 177947</td></tr><tr><td>increases in current period tax positions</td><td>53818</td></tr><tr><td>decreases in prior period measurement of tax positions</td><td>-36528 ( 36528 )</td></tr><tr><td>balance at december 31 2014</td><td>$ 195237</td></tr></table>",
            "id": 169,
            "page": 13,
            "text": "balance at january 1 2013 $ 180993  increases in current period tax position 27229  decreases in prior period measurement of tax positions -30275 ( 30275 )  balance at december 31 2013 $ 177947  increases in current period tax positions 53818  decreases in prior period measurement of tax positions -36528 ( 36528 )  balance at december 31 2014"
        },
        {
            "bounding_box": [
                {
                    "x": 344,
                    "y": 1027
                },
                {
                    "x": 2116,
                    "y": 1027
                },
                {
                    "x": 2116,
                    "y": 1098
                },
                {
                    "x": 344,
                    "y": 1098
                }
            ],
            "category": "paragraph",
            "html": "<p id='170' style='font-size:14px'>the total balance in the table above does not include interest and penalties of $ 157 and $ 242 as of december 31 , 2014 and 2013 , respectively , which is recorded as a<br>component of income tax expense ·</p>",
            "id": 170,
            "page": 13,
            "text": "the total balance in the table above does not include interest and penalties of $ 157 and $ 242 as of december 31 , 2014 and 2013 , respectively , which is recorded as a component of income tax expense ·"
        },
        {
            "bounding_box": [
                {
                    "x": 343,
                    "y": 1180
                },
                {
                    "x": 1033,
                    "y": 1180
                },
                {
                    "x": 1033,
                    "y": 1247
                },
                {
                    "x": 343,
                    "y": 1247
                }
            ],
            "category": "paragraph",
            "html": "<p id='171' style='font-size:18px'>Question: what was the net change in tax positions in 2014?<br>Gold program: add(53818, -36528), add(#0, 157)</p>",
            "id": 171,
            "page": 13,
            "text": "Question: what was the net change in tax positions in 2014? Gold program: add(53818, -36528), add(#0, 157)"
        },
        {
            "bounding_box": [
                {
                    "x": 343,
                    "y": 1281
                },
                {
                    "x": 575,
                    "y": 1281
                },
                {
                    "x": 575,
                    "y": 1309
                },
                {
                    "x": 343,
                    "y": 1309
                }
            ],
            "category": "paragraph",
            "html": "<p id='172' style='font-size:18px'>Retrieved evidence:</p>",
            "id": 172,
            "page": 13,
            "text": "Retrieved evidence:"
        },
        {
            "bounding_box": [
                {
                    "x": 341,
                    "y": 1299
                },
                {
                    "x": 1006,
                    "y": 1299
                },
                {
                    "x": 1006,
                    "y": 1478
                },
                {
                    "x": 341,
                    "y": 1478
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='173' style='font-size:16px'>[1] table row: increases in current period tax positions: 27229 ;<br>[2] table row: increases in current period tax positions: 53818 ;<br>[3] table row: balance at december 31 2014: $ 195237 ;<br>Predicted program:<br>subtract(27229, 53818)</p>",
            "id": 173,
            "page": 13,
            "text": " table row: increases in current period tax positions: 27229 ;  table row: increases in current period tax positions: 53818 ;  table row: balance at december 31 2014: $ 195237 ; Predicted program: subtract(27229, 53818)"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 1564
                },
                {
                    "x": 2196,
                    "y": 1564
                },
                {
                    "x": 2196,
                    "y": 1780
                },
                {
                    "x": 288,
                    "y": 1780
                }
            ],
            "category": "paragraph",
            "html": "<p id='174' style='font-size:22px'>Figure 5: Error case study 1: The net change in the tax position is the sum of the increase and the decrease plus the penalties and<br>interest. The model lacks this finance knowledge, thus the retriever fails to retrieve the correct table rows and sentences. Another<br>challenging point is the table understanding, since in this case, it's hard to distinguish the retrieved two table rows for the year<br>2013 or 2014, using our method that regards each table row as basic unit. The model needs to look at the full table to get this<br>global information.</p>",
            "id": 174,
            "page": 13,
            "text": "Figure 5: Error case study 1: The net change in the tax position is the sum of the increase and the decrease plus the penalties and interest. The model lacks this finance knowledge, thus the retriever fails to retrieve the correct table rows and sentences. Another challenging point is the table understanding, since in this case, it's hard to distinguish the retrieved two table rows for the year 2013 or 2014, using our method that regards each table row as basic unit. The model needs to look at the full table to get this global information."
        },
        {
            "bounding_box": [
                {
                    "x": 337,
                    "y": 2197
                },
                {
                    "x": 740,
                    "y": 2197
                },
                {
                    "x": 740,
                    "y": 2232
                },
                {
                    "x": 337,
                    "y": 2232
                }
            ],
            "category": "paragraph",
            "html": "<p id='175' style='font-size:20px'>Input Report K/2013/page_23.pdf-1</p>",
            "id": 175,
            "page": 13,
            "text": "Input Report K/2013/page_23.pdf-1"
        },
        {
            "bounding_box": [
                {
                    "x": 341,
                    "y": 2231
                },
                {
                    "x": 2071,
                    "y": 2231
                },
                {
                    "x": 2071,
                    "y": 2331
                },
                {
                    "x": 341,
                    "y": 2331
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='176' style='font-size:16px'>(abbreviate 12 sentences)... underlying gross margin declined by 180 basis points in 2012 as a result of cost inflation , net of cost savings , and the lower margin<br>structure of the pringles business · underlying sga% ( sga % ) was consistent with 2011 · our underlying gross profit , underlying sga , and underlying operating profit<br>measures are reconciled to the most comparable gaap measure as follows:</p>",
            "id": 176,
            "page": 13,
            "text": "(abbreviate 12 sentences)... underlying gross margin declined by 180 basis points in 2012 as a result of cost inflation , net of cost savings , and the lower margin structure of the pringles business · underlying sga% ( sga % ) was consistent with 2011 · our underlying gross profit , underlying sga , and underlying operating profit measures are reconciled to the most comparable gaap measure as follows:"
        },
        {
            "bounding_box": [
                {
                    "x": 340,
                    "y": 2331
                },
                {
                    "x": 1212,
                    "y": 2331
                },
                {
                    "x": 1212,
                    "y": 2511
                },
                {
                    "x": 340,
                    "y": 2511
                }
            ],
            "category": "table",
            "html": "<br><table id='177' style='font-size:14px'><tr><td>( dollars in millions )</td><td>2013</td><td>2012</td><td>2011</td></tr><tr><td>reported gross profit ( a )</td><td>$ 6103</td><td>$ 5434</td><td>$ 5152</td></tr><tr><td colspan=\"4\">· · · abbreviate 10 rows ...</td></tr><tr><td>underlying operating profit ( d )</td><td>$ 2098</td><td>$ 2014</td><td>$ 2109</td></tr></table>",
            "id": 177,
            "page": 13,
            "text": "( dollars in millions ) 2013 2012 2011  reported gross profit ( a ) $ 6103 $ 5434 $ 5152  · · · abbreviate 10 rows ...  underlying operating profit ( d ) $ 2098 $ 2014"
        },
        {
            "bounding_box": [
                {
                    "x": 341,
                    "y": 2586
                },
                {
                    "x": 1534,
                    "y": 2586
                },
                {
                    "x": 1534,
                    "y": 2654
                },
                {
                    "x": 341,
                    "y": 2654
                }
            ],
            "category": "paragraph",
            "html": "<p id='178' style='font-size:16px'>Question: if 2014 underlying operating profit increases at the same pace as 2013 , what would it be , in millions?<br>Gold program: divide(2098, 2014), multiply(2098, #0)</p>",
            "id": 178,
            "page": 13,
            "text": "Question: if 2014 underlying operating profit increases at the same pace as 2013 , what would it be , in millions? Gold program: divide(2098, 2014), multiply(2098, #0)"
        },
        {
            "bounding_box": [
                {
                    "x": 343,
                    "y": 2685
                },
                {
                    "x": 460,
                    "y": 2685
                },
                {
                    "x": 460,
                    "y": 2717
                },
                {
                    "x": 343,
                    "y": 2717
                }
            ],
            "category": "paragraph",
            "html": "<p id='179' style='font-size:20px'>Retrieved</p>",
            "id": 179,
            "page": 13,
            "text": "Retrieved"
        },
        {
            "bounding_box": [
                {
                    "x": 339,
                    "y": 2701
                },
                {
                    "x": 2122,
                    "y": 2701
                },
                {
                    "x": 2122,
                    "y": 2947
                },
                {
                    "x": 339,
                    "y": 2947
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='180' style='font-size:16px'>evidence:<br>[1] underlying gross margin declined by 110 basis points in 2013 due to the impact of inflation , net of productivity savings , lower operating leverage due to lower sales<br>volume , and the impact of the lower margin structure of the pringles business<br>[2] table row: ( dollars in millions ) The underlying operating profit ( d ) of 2013 is $ 2098 ; The underlying operating profit ( d ) of 2012 is $ 2014 ; The underlying operating<br>profit ( d ) of 2011 is $ 2109 ;<br>[3] during 2013 , we recorded $ 42 million of charges associated with cost reduction initiatives .<br>Predicted program:<br>divide(2098, 2098), multiply(2098, #0)</p>",
            "id": 180,
            "page": 13,
            "text": "evidence:  underlying gross margin declined by 110 basis points in 2013 due to the impact of inflation , net of productivity savings , lower operating leverage due to lower sales volume , and the impact of the lower margin structure of the pringles business  table row: ( dollars in millions ) The underlying operating profit ( d ) of 2013 is $ 2098 ; The underlying operating profit ( d ) of 2012 is $ 2014 ; The underlying operating profit ( d ) of 2011 is $ 2109 ;  during 2013 , we recorded $ 42 million of charges associated with cost reduction initiatives . Predicted program: divide(2098, 2098), multiply(2098, #0)"
        },
        {
            "bounding_box": [
                {
                    "x": 781,
                    "y": 2994
                },
                {
                    "x": 1692,
                    "y": 2994
                },
                {
                    "x": 1692,
                    "y": 3043
                },
                {
                    "x": 781,
                    "y": 3043
                }
            ],
            "category": "paragraph",
            "html": "<p id='181' style='font-size:22px'>Figure 6: Error case study 2: Complex numerical reasoning.</p>",
            "id": 181,
            "page": 13,
            "text": "Figure 6: Error case study 2: Complex numerical reasoning."
        },
        {
            "bounding_box": [
                {
                    "x": 328,
                    "y": 476
                },
                {
                    "x": 2162,
                    "y": 476
                },
                {
                    "x": 2162,
                    "y": 1811
                },
                {
                    "x": 328,
                    "y": 1811
                }
            ],
            "category": "figure",
            "html": "<figure><img id='182' style='font-size:14px' alt=\"Turkle Admin Stats Help Logged in as Change Password - Logout\nProject: FinanceQA / Batch: Batch 2 - □ Auto-accept next Task Return Task Skip Task Expires in 23:59\n[9]: The reserve for losses under these programs totaled $33 million and $43 million as of December 31, 2013 and December 31, 2012, respectively, and is included in Other\nliabilities on our Consolidated Balance Sheet.\n[10]: If payment is required under these programs, we would not have a contractual interest in the collateral underlying the mortgage loans on which losses occurred, although\nthe value of the collateral is taken into account in determining our share of such losses.\n[11]: Our exposure and activity associated with these recourse obligations are reported in the Corporate & Institutional Banking segment.\n[12]: Table 152: Analysis of Commercial Mortgage Recourse Obligations.\n1 In millions 2013 2012\n2 January 1 $43 $47\n3 Reserve adjustments net (9) 4\n4 Losses - loan repurchases and settlements (1) (8)\n5 December 31 $33 $43\n[13]: RESIDENTIAL MORTGAGE LOAN AND HOME EQUITY REPURCHASE OBLIGATIONS While residential mortgage loans are sold on a non-recourse basis, we assume\ncertain loan repurchase obligations associated with mortgage loans we have sold to investors.\n[14]: These loan repurchase obligations primarily relate to situations where PNC is alleged to have breached certain origination covenants and representations and warranties\nmade to purchasers of the loans in the respective purchase and sale agreements.\n[15]: For additional information on loan sales see Note 3 Loan Sale and Servicing Activities and Variable Interest Entities.\n[16]: Our historical exposure and activity associated with Agency securitization repurchase obligations has primarily been related to transactions with FNMA and FHLMC, as\nindemnification and repurchase losses associated with FHA and VA-insured and uninsured loans pooled in GNMA securitizations historically have been minimal.\n[17]: Repurchase obligation activity associated with residential mortgages is reported in the Residential Mortgage Banking segment.\n[18]: In the fourth quarter of 2013, PNC reached agreements with both FNMA and FHLMC to resolve their repurchase claims with respect to loans sold between 2000 and 2008.\n[19]: PNC paid a total of $191 million related to these settlements.\nFigure 7: Annotation interface: Display report.\" data-coord=\"top-left:(328,476); bottom-right:(2162,1811)\" /></figure>",
            "id": 182,
            "page": 14,
            "text": "Turkle Admin Stats Help Logged in as Change Password - Logout Project: FinanceQA / Batch: Batch 2 - □ Auto-accept next Task Return Task Skip Task Expires in 23:59 : The reserve for losses under these programs totaled $33 million and $43 million as of December 31, 2013 and December 31, 2012, respectively, and is included in Other liabilities on our Consolidated Balance Sheet. : If payment is required under these programs, we would not have a contractual interest in the collateral underlying the mortgage loans on which losses occurred, although the value of the collateral is taken into account in determining our share of such losses. : Our exposure and activity associated with these recourse obligations are reported in the Corporate & Institutional Banking segment. : Table 152: Analysis of Commercial Mortgage Recourse Obligations. 1 In millions 2013 2012 2 January 1 $43 $47 3 Reserve adjustments net (9) 4 4 Losses - loan repurchases and settlements (1) (8) 5 December 31 $33 $43 : RESIDENTIAL MORTGAGE LOAN AND HOME EQUITY REPURCHASE OBLIGATIONS While residential mortgage loans are sold on a non-recourse basis, we assume certain loan repurchase obligations associated with mortgage loans we have sold to investors. : These loan repurchase obligations primarily relate to situations where PNC is alleged to have breached certain origination covenants and representations and warranties made to purchasers of the loans in the respective purchase and sale agreements. : For additional information on loan sales see Note 3 Loan Sale and Servicing Activities and Variable Interest Entities. : Our historical exposure and activity associated with Agency securitization repurchase obligations has primarily been related to transactions with FNMA and FHLMC, as indemnification and repurchase losses associated with FHA and VA-insured and uninsured loans pooled in GNMA securitizations historically have been minimal. : Repurchase obligation activity associated with residential mortgages is reported in the Residential Mortgage Banking segment. : In the fourth quarter of 2013, PNC reached agreements with both FNMA and FHLMC to resolve their repurchase claims with respect to loans sold between 2000 and 2008. : PNC paid a total of $191 million related to these settlements. Figure 7: Annotation interface: Display report."
        },
        {
            "bounding_box": [
                {
                    "x": 326,
                    "y": 1905
                },
                {
                    "x": 2114,
                    "y": 1905
                },
                {
                    "x": 2114,
                    "y": 2971
                },
                {
                    "x": 326,
                    "y": 2971
                }
            ],
            "category": "figure",
            "html": "<figure><img id='183' style='font-size:14px' alt=\"Turkle Admin Stats Help Logged in as Change Password - Logout\nProject: FinanceQA / Batch: Batch 2 - □ Auto-accept next Task Return Task Skip Task Expires in 23:55\nQuestion 2:\nQuestion\nAnswer\nText line(s) involved, separated by comma\nTable row(s) involved, separated by comma\nCalculation Step 1:\nOperator: click to select - V\nFirst argument Second argument Result\nCalculation Step 2:\nOperator: click to select - V\nFirst argument Second argument Result\nCalculation Step 3:\nOperator: clickto select - - V\nFirst argument Second argument Result\nCalculation Step 4:\nOperator: click to select - V\nFirst argument Second argument Result\nCalculation Step 5:\nOperator: click to select V\nFirst argument Second argument Result\nOther explanation for question 2:\nOther explanation for question 2\nValidation Check\nSubmit\" data-coord=\"top-left:(326,1905); bottom-right:(2114,2971)\" /></figure>",
            "id": 183,
            "page": 14,
            "text": "Turkle Admin Stats Help Logged in as Change Password - Logout Project: FinanceQA / Batch: Batch 2 - □ Auto-accept next Task Return Task Skip Task Expires in 23:55 Question 2: Question Answer Text line(s) involved, separated by comma Table row(s) involved, separated by comma Calculation Step 1: Operator: click to select - V First argument Second argument Result Calculation Step 2: Operator: click to select - V First argument Second argument Result Calculation Step 3: Operator: clickto select - - V First argument Second argument Result Calculation Step 4: Operator: click to select - V First argument Second argument Result Calculation Step 5: Operator: click to select V First argument Second argument Result Other explanation for question 2: Other explanation for question 2 Validation Check Submit"
        },
        {
            "bounding_box": [
                {
                    "x": 825,
                    "y": 3016
                },
                {
                    "x": 1651,
                    "y": 3016
                },
                {
                    "x": 1651,
                    "y": 3069
                },
                {
                    "x": 825,
                    "y": 3069
                }
            ],
            "category": "caption",
            "html": "<caption id='184' style='font-size:18px'>Figure 8: Annotation interface: Annotator input fields.</caption>",
            "id": 184,
            "page": 14,
            "text": "Figure 8: Annotation interface: Annotator input fields."
        }
    ]
}