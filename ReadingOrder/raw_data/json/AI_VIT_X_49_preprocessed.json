{
    "id": "329806c2-0f94-11ef-8828-426932df3dcf",
    "pdf_path": "./pdf/AI_VIT_X/1312.6114v11.pdf",
    "elements": [
        {
            "bounding_box": [
                {
                    "x": 758,
                    "y": 453
                },
                {
                    "x": 1789,
                    "y": 453
                },
                {
                    "x": 1789,
                    "y": 534
                },
                {
                    "x": 758,
                    "y": 534
                }
            ],
            "category": "paragraph",
            "html": "<p id='0' style='font-size:22px'>Auto-Encoding Variational Bayes</p>",
            "id": 0,
            "page": 1,
            "text": "Auto-Encoding Variational Bayes"
        },
        {
            "bounding_box": [
                {
                    "x": 655,
                    "y": 705
                },
                {
                    "x": 1896,
                    "y": 705
                },
                {
                    "x": 1896,
                    "y": 893
                },
                {
                    "x": 655,
                    "y": 893
                }
            ],
            "category": "paragraph",
            "html": "<p id='1' style='font-size:18px'>Diederik P. Kingma Max Welling<br>Machine Learning Group Machine Learning Group<br>Universiteit van Amsterdam Universiteit van Amsterdam<br>dpkingma@gmail · com welling · max@ gmail · com</p>",
            "id": 1,
            "page": 1,
            "text": "Diederik P. Kingma Max Welling Machine Learning Group Machine Learning Group Universiteit van Amsterdam Universiteit van Amsterdam dpkingma@gmail · com welling · max@ gmail · com"
        },
        {
            "bounding_box": [
                {
                    "x": 1176,
                    "y": 1009
                },
                {
                    "x": 1373,
                    "y": 1009
                },
                {
                    "x": 1373,
                    "y": 1065
                },
                {
                    "x": 1176,
                    "y": 1065
                }
            ],
            "category": "paragraph",
            "html": "<p id='2' style='font-size:20px'>Abstract</p>",
            "id": 2,
            "page": 1,
            "text": "Abstract"
        },
        {
            "bounding_box": [
                {
                    "x": 591,
                    "y": 1110
                },
                {
                    "x": 1962,
                    "y": 1110
                },
                {
                    "x": 1962,
                    "y": 1668
                },
                {
                    "x": 591,
                    "y": 1668
                }
            ],
            "category": "paragraph",
            "html": "<p id='3' style='font-size:18px'>How can we perform efficient inference and learning in directed probabilistic<br>models, in the presence of continuous latent variables with intractable posterior<br>distributions, and large datasets? We introduce a stochastic variational inference<br>and learning algorithm that scales to large datasets and, under some mild differ-<br>entiability conditions, even works in the intractable case. Our contributions are<br>two-fold. First, we show that a reparameterization of the variational lower bound<br>yields a lower bound estimator that can be straightforwardly optimized using stan-<br>dard stochastic gradient methods. Second, we show that for i.i.d. datasets with<br>continuous latent variables per datapoint, posterior inference can be made espe-<br>cially efficient by fitting an approximate inference model (also called a recogni-<br>tion model) to the intractable posterior using the proposed lower bound estimator.<br>Theoretical advantages are reflected in experimental results.</p>",
            "id": 3,
            "page": 1,
            "text": "How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 1746
                },
                {
                    "x": 798,
                    "y": 1746
                },
                {
                    "x": 798,
                    "y": 1801
                },
                {
                    "x": 445,
                    "y": 1801
                }
            ],
            "category": "paragraph",
            "html": "<p id='4' style='font-size:18px'>1 Introduction</p>",
            "id": 4,
            "page": 1,
            "text": "1 Introduction"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1847
                },
                {
                    "x": 2109,
                    "y": 1847
                },
                {
                    "x": 2109,
                    "y": 2269
                },
                {
                    "x": 441,
                    "y": 2269
                }
            ],
            "category": "paragraph",
            "html": "<p id='5' style='font-size:18px'>How can we perform efficient approximate inference and learning with directed probabilistic models<br>whose continuous latent variables and/or parameters have intractable posterior distributions? The<br>variational Bayesian (VB) approach involves the optimization of an approximation to the intractable<br>posterior. Unfortunately, the common mean-field approach requires analytical solutions of expecta-<br>tions w.r.t. the approximate posterior, which are also intractable in the general case. We show how a<br>reparameterization of the variational lower bound yields a simple differentiable unbiased estimator<br>of the lower bound; this SGVB (Stochastic Gradient Variational Bayes) estimator can be used for ef-<br>ficient approximate posterior inference in almost any model with continuous latent variables and/or<br>parameters, and is straightforward to optimize using standard stochastic gradient ascent techniques.</p>",
            "id": 5,
            "page": 1,
            "text": "How can we perform efficient approximate inference and learning with directed probabilistic models whose continuous latent variables and/or parameters have intractable posterior distributions? The variational Bayesian (VB) approach involves the optimization of an approximation to the intractable posterior. Unfortunately, the common mean-field approach requires analytical solutions of expectations w.r.t. the approximate posterior, which are also intractable in the general case. We show how a reparameterization of the variational lower bound yields a simple differentiable unbiased estimator of the lower bound; this SGVB (Stochastic Gradient Variational Bayes) estimator can be used for efficient approximate posterior inference in almost any model with continuous latent variables and/or parameters, and is straightforward to optimize using standard stochastic gradient ascent techniques."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2284
                },
                {
                    "x": 2110,
                    "y": 2284
                },
                {
                    "x": 2110,
                    "y": 2657
                },
                {
                    "x": 441,
                    "y": 2657
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='6' style='font-size:18px'>For the case of an i.i.d. dataset and continuous latent variables per datapoint, we propose the Auto-<br>Encoding VB (AEVB) algorithm. In the AEVB algorithm we make inference and learning especially<br>efficient by using the SGVB estimator to optimize a recognition model that allows us to perform very<br>efficient approximate posterior inference using simple ancestral sampling, which in turn allows us<br>to efficiently learn the model parameters, without the need of expensive iterative inference schemes<br>(such as MCMC) per datapoint. The learned approximate posterior inference model can also be used<br>for a host of tasks such as recognition, denoising, representation and visualization purposes. When<br>a neural network is used for the recognition model, we arrive at the variational auto-encoder.</p>",
            "id": 6,
            "page": 1,
            "text": "For the case of an i.i.d. dataset and continuous latent variables per datapoint, we propose the AutoEncoding VB (AEVB) algorithm. In the AEVB algorithm we make inference and learning especially efficient by using the SGVB estimator to optimize a recognition model that allows us to perform very efficient approximate posterior inference using simple ancestral sampling, which in turn allows us to efficiently learn the model parameters, without the need of expensive iterative inference schemes (such as MCMC) per datapoint. The learned approximate posterior inference model can also be used for a host of tasks such as recognition, denoising, representation and visualization purposes. When a neural network is used for the recognition model, we arrive at the variational auto-encoder."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2718
                },
                {
                    "x": 697,
                    "y": 2718
                },
                {
                    "x": 697,
                    "y": 2772
                },
                {
                    "x": 442,
                    "y": 2772
                }
            ],
            "category": "paragraph",
            "html": "<p id='7' style='font-size:18px'>2 Method</p>",
            "id": 7,
            "page": 1,
            "text": "2 Method"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 2819
                },
                {
                    "x": 2110,
                    "y": 2819
                },
                {
                    "x": 2110,
                    "y": 3056
                },
                {
                    "x": 440,
                    "y": 3056
                }
            ],
            "category": "paragraph",
            "html": "<p id='8' style='font-size:16px'>The strategy in this section can be used to derive a lower bound estimator (a stochastic objective<br>function) for a variety of directed graphical models with continuous latent variables. We will restrict<br>ourselves here to the common case where we have an i.i.d. dataset with latent variables per datapoint,<br>and where we like to perform maximum likelihood (ML) or maximum a posteriori (MAP) inference<br>on the (global) parameters, and variational inference on the latent variables. It is, for example,</p>",
            "id": 8,
            "page": 1,
            "text": "The strategy in this section can be used to derive a lower bound estimator (a stochastic objective function) for a variety of directed graphical models with continuous latent variables. We will restrict ourselves here to the common case where we have an i.i.d. dataset with latent variables per datapoint, and where we like to perform maximum likelihood (ML) or maximum a posteriori (MAP) inference on the (global) parameters, and variational inference on the latent variables. It is, for example,"
        },
        {
            "bounding_box": [
                {
                    "x": 59,
                    "y": 860
                },
                {
                    "x": 147,
                    "y": 860
                },
                {
                    "x": 147,
                    "y": 2373
                },
                {
                    "x": 59,
                    "y": 2373
                }
            ],
            "category": "footer",
            "html": "<br><footer id='9' style='font-size:14px'>2022<br>Dec<br>10<br>[stat.ML]<br>arXiv:1312.6114v11</footer>",
            "id": 9,
            "page": 1,
            "text": "2022 Dec 10 [stat.ML] arXiv:1312.6114v11"
        },
        {
            "bounding_box": [
                {
                    "x": 1261,
                    "y": 3132
                },
                {
                    "x": 1290,
                    "y": 3132
                },
                {
                    "x": 1290,
                    "y": 3173
                },
                {
                    "x": 1261,
                    "y": 3173
                }
            ],
            "category": "footer",
            "html": "<footer id='10' style='font-size:14px'>1</footer>",
            "id": 10,
            "page": 1,
            "text": "1"
        },
        {
            "bounding_box": [
                {
                    "x": 1082,
                    "y": 329
                },
                {
                    "x": 1468,
                    "y": 329
                },
                {
                    "x": 1468,
                    "y": 715
                },
                {
                    "x": 1082,
                    "y": 715
                }
            ],
            "category": "figure",
            "html": "<figure><img id='11' style='font-size:14px' alt=\"Z\nx\nN\" data-coord=\"top-left:(1082,329); bottom-right:(1468,715)\" /></figure>",
            "id": 11,
            "page": 2,
            "text": "Z x N"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 763
                },
                {
                    "x": 2109,
                    "y": 763
                },
                {
                    "x": 2109,
                    "y": 953
                },
                {
                    "x": 440,
                    "y": 953
                }
            ],
            "category": "caption",
            "html": "<caption id='12' style='font-size:20px'>Figure 1: The type of directed graphical model under consideration. Solid lines denote the generative<br>model po (z)pe(x|z), dashed lines denote the variational approximation 9⌀ (zx) to the intractable<br>posterior pe(z|x). The variational parameters 0 are learned jointly with the generative model pa-<br>rameters 0.</caption>",
            "id": 12,
            "page": 2,
            "text": "Figure 1: The type of directed graphical model under consideration. Solid lines denote the generative model po (z)pe(x|z), dashed lines denote the variational approximation 9⌀ (zx) to the intractable posterior pe(z|x). The variational parameters 0 are learned jointly with the generative model parameters 0."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1050
                },
                {
                    "x": 2109,
                    "y": 1050
                },
                {
                    "x": 2109,
                    "y": 1236
                },
                {
                    "x": 441,
                    "y": 1236
                }
            ],
            "category": "paragraph",
            "html": "<p id='13' style='font-size:16px'>straightforward to extend this scenario to the case where we also perform variational inference on<br>the global parameters; that algorithm is put in the appendix, but experiments with that case are left to<br>future work. Note that our method can be applied to online, non-stationary settings, e.g. streaming<br>data, but here we assume a fixed dataset for simplicity.</p>",
            "id": 13,
            "page": 2,
            "text": "straightforward to extend this scenario to the case where we also perform variational inference on the global parameters; that algorithm is put in the appendix, but experiments with that case are left to future work. Note that our method can be applied to online, non-stationary settings, e.g. streaming data, but here we assume a fixed dataset for simplicity."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1298
                },
                {
                    "x": 859,
                    "y": 1298
                },
                {
                    "x": 859,
                    "y": 1347
                },
                {
                    "x": 442,
                    "y": 1347
                }
            ],
            "category": "paragraph",
            "html": "<p id='14' style='font-size:18px'>2.1 Problem scenario</p>",
            "id": 14,
            "page": 2,
            "text": "2.1 Problem scenario"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1384
                },
                {
                    "x": 2107,
                    "y": 1384
                },
                {
                    "x": 2107,
                    "y": 1767
                },
                {
                    "x": 442,
                    "y": 1767
                }
            ],
            "category": "paragraph",
            "html": "<p id='15' style='font-size:16px'>Let us consider some dataset X = {x(i) }N=1 consisting of N i.i.d. samples of some continuous<br>or discrete variable x. We assume that the data are generated by some random process, involving<br>an unobserved continuous random variable Z. The process consists of two steps: (1) a value z(i)<br>is generated from some prior distribution po* (z); (2) a value x(i) is generated from some condi-<br>tional distribution po* (xz). We assume that the prior po* (z) and likelihood po* (xz) come from<br>parametric families of distributions pe(z) and po (x|z), and that their PDFs are differentiable almost<br>everywhere w.r.t. both 0 and z. Unfortunately, a lot of this process is hidden from our view: the true<br>parameters 0* as well as the values of the latent variables z(i) are unknown to us.</p>",
            "id": 15,
            "page": 2,
            "text": "Let us consider some dataset X = {x(i) }N=1 consisting of N i.i.d. samples of some continuous or discrete variable x. We assume that the data are generated by some random process, involving an unobserved continuous random variable Z. The process consists of two steps: (1) a value z(i) is generated from some prior distribution po* (z); (2) a value x(i) is generated from some conditional distribution po* (xz). We assume that the prior po* (z) and likelihood po* (xz) come from parametric families of distributions pe(z) and po (x|z), and that their PDFs are differentiable almost everywhere w.r.t. both 0 and z. Unfortunately, a lot of this process is hidden from our view: the true parameters 0* as well as the values of the latent variables z(i) are unknown to us."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1792
                },
                {
                    "x": 2105,
                    "y": 1792
                },
                {
                    "x": 2105,
                    "y": 1930
                },
                {
                    "x": 442,
                    "y": 1930
                }
            ],
            "category": "paragraph",
            "html": "<p id='16' style='font-size:16px'>Very importantly, we do not make the common simplifying assumptions about the marginal or pos-<br>terior probabilities. Conversely, we are here interested in a general algorithm that even works effi-<br>ciently in the case of:</p>",
            "id": 16,
            "page": 2,
            "text": "Very importantly, we do not make the common simplifying assumptions about the marginal or posterior probabilities. Conversely, we are here interested in a general algorithm that even works efficiently in the case of:"
        },
        {
            "bounding_box": [
                {
                    "x": 538,
                    "y": 1970
                },
                {
                    "x": 2109,
                    "y": 1970
                },
                {
                    "x": 2109,
                    "y": 2509
                },
                {
                    "x": 538,
                    "y": 2509
                }
            ],
            "category": "paragraph",
            "html": "<p id='17' style='font-size:20px'>1. Intractability: the case where the integral of the marginal likelihood pe(x) =<br>S pe(z)pe (xz) dz is intractable (so we cannot evaluate or differentiate the marginal like-<br>lihood), where the true posterior density po (zx) = pe (x|z)pe(z)/pe (x) is intractable<br>(so the EM algorithm cannot be used), and where the required integrals for any reason-<br>able mean-field VB algorithm are also intractable. These intractabilities are quite common<br>and appear in cases of moderately complicated likelihood functions pe (xz), e.g. a neural<br>network with a nonlinear hidden layer.<br>2. A large dataset: we have SO much data that batch optimization is too costly; we would like<br>to make parameter updates using small minibatches or even single datapoints. Sampling-<br>based solutions, e.g. Monte Carlo EM, would in general be too slow, since it involves a<br>typically expensive sampling loop per datapoint.</p>",
            "id": 17,
            "page": 2,
            "text": "1. Intractability: the case where the integral of the marginal likelihood pe(x) = S pe(z)pe (xz) dz is intractable (so we cannot evaluate or differentiate the marginal likelihood), where the true posterior density po (zx) = pe (x|z)pe(z)/pe (x) is intractable (so the EM algorithm cannot be used), and where the required integrals for any reasonable mean-field VB algorithm are also intractable. These intractabilities are quite common and appear in cases of moderately complicated likelihood functions pe (xz), e.g. a neural network with a nonlinear hidden layer. 2. A large dataset: we have SO much data that batch optimization is too costly; we would like to make parameter updates using small minibatches or even single datapoints. Samplingbased solutions, e.g. Monte Carlo EM, would in general be too slow, since it involves a typically expensive sampling loop per datapoint."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 2547
                },
                {
                    "x": 1985,
                    "y": 2547
                },
                {
                    "x": 1985,
                    "y": 2594
                },
                {
                    "x": 445,
                    "y": 2594
                }
            ],
            "category": "paragraph",
            "html": "<p id='18' style='font-size:16px'>We are interested in, and propose a solution to, three related problems in the above scenario:</p>",
            "id": 18,
            "page": 2,
            "text": "We are interested in, and propose a solution to, three related problems in the above scenario:"
        },
        {
            "bounding_box": [
                {
                    "x": 538,
                    "y": 2631
                },
                {
                    "x": 2110,
                    "y": 2631
                },
                {
                    "x": 2110,
                    "y": 3055
                },
                {
                    "x": 538,
                    "y": 3055
                }
            ],
            "category": "paragraph",
            "html": "<p id='19' style='font-size:16px'>1. Efficient approximate ML or MAP estimation for the parameters 0. The parameters can be<br>of interest themselves, e.g. if we are analyzing some natural process. They also allow us to<br>mimic the hidden random process and generate artificial data that resembles the real data.<br>2. Efficient approximate posterior inference of the latent variable Z given an observed value X<br>for a choice of parameters 0. This is useful for coding or data representation tasks.<br>3. Efficient approximate marginal inference of the variable x. This allows us to perform all<br>kinds ofinference tasks where a prior over x is required. Common applications in computer<br>vision include image denoising, inpainting and super-resolution.</p>",
            "id": 19,
            "page": 2,
            "text": "1. Efficient approximate ML or MAP estimation for the parameters 0. The parameters can be of interest themselves, e.g. if we are analyzing some natural process. They also allow us to mimic the hidden random process and generate artificial data that resembles the real data. 2. Efficient approximate posterior inference of the latent variable Z given an observed value X for a choice of parameters 0. This is useful for coding or data representation tasks. 3. Efficient approximate marginal inference of the variable x. This allows us to perform all kinds ofinference tasks where a prior over x is required. Common applications in computer vision include image denoising, inpainting and super-resolution."
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3134
                },
                {
                    "x": 1289,
                    "y": 3134
                },
                {
                    "x": 1289,
                    "y": 3171
                },
                {
                    "x": 1260,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='20' style='font-size:18px'>2</footer>",
            "id": 20,
            "page": 2,
            "text": "2"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 346
                },
                {
                    "x": 2107,
                    "y": 346
                },
                {
                    "x": 2107,
                    "y": 578
                },
                {
                    "x": 441,
                    "y": 578
                }
            ],
            "category": "paragraph",
            "html": "<p id='21' style='font-size:14px'>For the purpose of solving the above problems, let us introduce a recognition model 9⌀(Z|X): an<br>approximation to the intractable true posterior pe(z|x). Note that in contrast with the approximate<br>posterior in mean-field variational inference, it is not necessarily factorial and its parameters 0 are<br>not computed from some closed-form expectation. Instead, we'll introduce a method for learning<br>the recognition model parameters 0 jointly with the generative model parameters 0.</p>",
            "id": 21,
            "page": 3,
            "text": "For the purpose of solving the above problems, let us introduce a recognition model 9⌀(Z|X): an approximation to the intractable true posterior pe(z|x). Note that in contrast with the approximate posterior in mean-field variational inference, it is not necessarily factorial and its parameters 0 are not computed from some closed-form expectation. Instead, we'll introduce a method for learning the recognition model parameters 0 jointly with the generative model parameters 0."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 601
                },
                {
                    "x": 2108,
                    "y": 601
                },
                {
                    "x": 2108,
                    "y": 877
                },
                {
                    "x": 441,
                    "y": 877
                }
            ],
            "category": "paragraph",
            "html": "<p id='22' style='font-size:14px'>From a coding theory perspective, the unobserved variables Z have an interpretation as a latent<br>representation or code. In this paper we will therefore also refer to the recognition model 1⌀ (zx)<br>as a probabilistic encoder, since given a datapoint x it produces a distribution (e.g. a Gaussian)<br>over the possible values of the code Z from which the datapoint x could have been generated. In a<br>similar vein we will refer to pe(x|z) as a probabilistic decoder, since given a code Z it produces a<br>distribution over the possible corresponding values of x.</p>",
            "id": 22,
            "page": 3,
            "text": "From a coding theory perspective, the unobserved variables Z have an interpretation as a latent representation or code. In this paper we will therefore also refer to the recognition model 1⌀ (zx) as a probabilistic encoder, since given a datapoint x it produces a distribution (e.g. a Gaussian) over the possible values of the code Z from which the datapoint x could have been generated. In a similar vein we will refer to pe(x|z) as a probabilistic decoder, since given a code Z it produces a distribution over the possible corresponding values of x."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 928
                },
                {
                    "x": 944,
                    "y": 928
                },
                {
                    "x": 944,
                    "y": 978
                },
                {
                    "x": 444,
                    "y": 978
                }
            ],
            "category": "paragraph",
            "html": "<p id='23' style='font-size:14px'>2.2 The variational bound</p>",
            "id": 23,
            "page": 3,
            "text": "2.2 The variational bound"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1014
                },
                {
                    "x": 2104,
                    "y": 1014
                },
                {
                    "x": 2104,
                    "y": 1117
                },
                {
                    "x": 443,
                    "y": 1117
                }
            ],
            "category": "paragraph",
            "html": "<p id='24' style='font-size:14px'>The marginal likelihood is composed of a sum over the marginal likelihoods of individual datapoints<br>log pe (x(1) , , x(N)) = �i=1 logpe(x(i)), which can each be rewritten as:<br>· · ·</p>",
            "id": 24,
            "page": 3,
            "text": "The marginal likelihood is composed of a sum over the marginal likelihoods of individual datapoints log pe (x(1) , , x(N)) = �i=1 logpe(x(i)), which can each be rewritten as: · · ·"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 1199
                },
                {
                    "x": 2105,
                    "y": 1199
                },
                {
                    "x": 2105,
                    "y": 1342
                },
                {
                    "x": 440,
                    "y": 1342
                }
            ],
            "category": "paragraph",
            "html": "<p id='25' style='font-size:14px'>The first RHS term is the KL divergence of the approximate from the true posterior. Since this<br>KL-divergence is non-negative, the second RHS term L(0,⌀;x(i) is called the (variational) lower<br>bound on the marginal likelihood of datapoint i, and can be written as:</p>",
            "id": 25,
            "page": 3,
            "text": "The first RHS term is the KL divergence of the approximate from the true posterior. Since this KL-divergence is non-negative, the second RHS term L(0,⌀;x(i) is called the (variational) lower bound on the marginal likelihood of datapoint i, and can be written as:"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1423
                },
                {
                    "x": 934,
                    "y": 1423
                },
                {
                    "x": 934,
                    "y": 1467
                },
                {
                    "x": 443,
                    "y": 1467
                }
            ],
            "category": "paragraph",
            "html": "<p id='26' style='font-size:14px'>which can also be written as:</p>",
            "id": 26,
            "page": 3,
            "text": "which can also be written as:"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1569
                },
                {
                    "x": 2107,
                    "y": 1569
                },
                {
                    "x": 2107,
                    "y": 1870
                },
                {
                    "x": 441,
                    "y": 1870
                }
            ],
            "category": "paragraph",
            "html": "<p id='27' style='font-size:16px'>We want to differentiate and optimize the lower bound L(日, ⌀;x(i) ) w.r.t. both the variational<br>parameters O and generative parameters 0. However, the gradient of the lower bound w.r.t. O<br>is a bit problematic. The usual (naive) Monte Carlo gradient estimator for this type of problem<br>is: 5xX9⌀ (z) [f(z)] = Eq⌀ (z) [.f(z)▽q⌀(z) log q⌀(z)] 1 I I �I=1 f(z)▽ 9⌀ (z(l)) log 9⌀(z(l) ) where<br>z(l) ~ 9⌀(z|x(i)). This gradient estimator exhibits exhibits very high variance (see e.g. [BJP12])<br>and is impractical for our purposes.</p>",
            "id": 27,
            "page": 3,
            "text": "We want to differentiate and optimize the lower bound L(日, ⌀;x(i) ) w.r.t. both the variational parameters O and generative parameters 0. However, the gradient of the lower bound w.r.t. O is a bit problematic. The usual (naive) Monte Carlo gradient estimator for this type of problem is: 5xX9⌀ (z) [f(z)] = Eq⌀ (z) [.f(z)▽q⌀(z) log q⌀(z)] 1 I I �I=1 f(z)▽ 9⌀ (z(l)) log 9⌀(z(l) ) where z(l) ~ 9⌀(z|x(i)). This gradient estimator exhibits exhibits very high variance (see e.g. [BJP12]) and is impractical for our purposes."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1920
                },
                {
                    "x": 1307,
                    "y": 1920
                },
                {
                    "x": 1307,
                    "y": 1970
                },
                {
                    "x": 443,
                    "y": 1970
                }
            ],
            "category": "paragraph",
            "html": "<p id='28' style='font-size:16px'>2.3 The SGVB estimator and AEVB algorithm</p>",
            "id": 28,
            "page": 3,
            "text": "2.3 The SGVB estimator and AEVB algorithm"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2006
                },
                {
                    "x": 2107,
                    "y": 2006
                },
                {
                    "x": 2107,
                    "y": 2191
                },
                {
                    "x": 442,
                    "y": 2191
                }
            ],
            "category": "paragraph",
            "html": "<p id='29' style='font-size:14px'>In this section we introduce a practical estimator of the lower bound and its derivatives w.r.t. the<br>parameters. We assume an approximate posterior in the form 9⌀(Z|X), but please note that the<br>technique can be applied to the case 9⌀(z), i.e. where we do not condition on x, as well. The fully<br>variational Bayesian method for inferring a posterior over the parameters is given in the appendix.</p>",
            "id": 29,
            "page": 3,
            "text": "In this section we introduce a practical estimator of the lower bound and its derivatives w.r.t. the parameters. We assume an approximate posterior in the form 9⌀(Z|X), but please note that the technique can be applied to the case 9⌀(z), i.e. where we do not condition on x, as well. The fully variational Bayesian method for inferring a posterior over the parameters is given in the appendix."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2216
                },
                {
                    "x": 2105,
                    "y": 2216
                },
                {
                    "x": 2105,
                    "y": 2351
                },
                {
                    "x": 442,
                    "y": 2351
                }
            ],
            "category": "paragraph",
            "html": "<p id='30' style='font-size:14px'>Under certain mild conditions outlined in section 2.4 for a chosen approximate posterior 9⌀(Z|X) we<br>can reparameterize the random variable ~ ~ 9⌀(zx) using a differentiable transformation 9⌀(E,X)<br>of an (auxiliary) noise variable e:</p>",
            "id": 30,
            "page": 3,
            "text": "Under certain mild conditions outlined in section 2.4 for a chosen approximate posterior 9⌀(Z|X) we can reparameterize the random variable ~ ~ 9⌀(zx) using a differentiable transformation 9⌀(E,X) of an (auxiliary) noise variable e:"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2428
                },
                {
                    "x": 2105,
                    "y": 2428
                },
                {
                    "x": 2105,
                    "y": 2564
                },
                {
                    "x": 441,
                    "y": 2564
                }
            ],
            "category": "paragraph",
            "html": "<p id='31' style='font-size:16px'>See section 2.4 for general strategies for chosing such an approriate distribution p(E) and function<br>9⌀(E,X). We can now form Monte Carlo estimates of expectations of some function f(z) w.r.t.<br>9⌀ (zx) as follows:</p>",
            "id": 31,
            "page": 3,
            "text": "See section 2.4 for general strategies for chosing such an approriate distribution p(E) and function 9⌀(E,X). We can now form Monte Carlo estimates of expectations of some function f(z) w.r.t. 9⌀ (zx) as follows:"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2722
                },
                {
                    "x": 2105,
                    "y": 2722
                },
                {
                    "x": 2105,
                    "y": 2826
                },
                {
                    "x": 441,
                    "y": 2826
                }
            ],
            "category": "paragraph",
            "html": "<p id='32' style='font-size:18px'>We apply this technique to the variational lower bound (eq. (2)), yielding our generic Stochastic<br>Gradient Variational Bayes (SGVB) estimator �A(0,⌀;x(i)) 1 L(0,⌀;x(i)):</p>",
            "id": 32,
            "page": 3,
            "text": "We apply this technique to the variational lower bound (eq. (2)), yielding our generic Stochastic Gradient Variational Bayes (SGVB) estimator �A(0,⌀;x(i)) 1 L(0,⌀;x(i)):"
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3134
                },
                {
                    "x": 1288,
                    "y": 3134
                },
                {
                    "x": 1288,
                    "y": 3171
                },
                {
                    "x": 1260,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='33' style='font-size:14px'>3</footer>",
            "id": 33,
            "page": 3,
            "text": "3"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 346
                },
                {
                    "x": 2107,
                    "y": 346
                },
                {
                    "x": 2107,
                    "y": 438
                },
                {
                    "x": 443,
                    "y": 438
                }
            ],
            "category": "paragraph",
            "html": "<p id='34' style='font-size:16px'>Algorithm 1 Minibatch version of the Auto-Encoding VB (AEVB) algorithm. Either of the two<br>SGVB estimators in section 2.3 can be used. We use settings M = 100 and L = 1 in experiments.</p>",
            "id": 34,
            "page": 4,
            "text": "Algorithm 1 Minibatch version of the Auto-Encoding VB (AEVB) algorithm. Either of the two SGVB estimators in section 2.3 can be used. We use settings M = 100 and L = 1 in experiments."
        },
        {
            "bounding_box": [
                {
                    "x": 485,
                    "y": 456
                },
                {
                    "x": 972,
                    "y": 456
                },
                {
                    "x": 972,
                    "y": 501
                },
                {
                    "x": 485,
                    "y": 501
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='35' style='font-size:18px'>0, ⌀ ← Initialize parameters</p>",
            "id": 35,
            "page": 4,
            "text": "0, ⌀ ← Initialize parameters"
        },
        {
            "bounding_box": [
                {
                    "x": 486,
                    "y": 508
                },
                {
                    "x": 613,
                    "y": 508
                },
                {
                    "x": 613,
                    "y": 546
                },
                {
                    "x": 486,
                    "y": 546
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='36' style='font-size:14px'>repeat</p>",
            "id": 36,
            "page": 4,
            "text": "repeat"
        },
        {
            "bounding_box": [
                {
                    "x": 543,
                    "y": 546
                },
                {
                    "x": 1857,
                    "y": 546
                },
                {
                    "x": 1857,
                    "y": 732
                },
                {
                    "x": 543,
                    "y": 732
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='37' style='font-size:18px'>XM ← Random minibatch of M datapoints (drawn from full dataset)<br>E ← Random samples from noise distribution p(E)<br>g ← ▽ 80⌀ CM (日, ⌀; XM , E) (Gradients of minibatch estimator (8))<br>0, 0 ← Update parameters using gradients g (e.g. SGD or Adagrad [DHS10])</p>",
            "id": 37,
            "page": 4,
            "text": "XM ← Random minibatch of M datapoints (drawn from full dataset) E ← Random samples from noise distribution p(E) g ← ▽ 80⌀ CM (日, ⌀; XM , E) (Gradients of minibatch estimator (8)) 0, 0 ← Update parameters using gradients g (e.g. SGD or Adagrad [DHS10])"
        },
        {
            "bounding_box": [
                {
                    "x": 486,
                    "y": 744
                },
                {
                    "x": 1148,
                    "y": 744
                },
                {
                    "x": 1148,
                    "y": 830
                },
                {
                    "x": 486,
                    "y": 830
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='38' style='font-size:20px'>until convergence of parameters (0, ⌀)<br>return 0, ⌀</p>",
            "id": 38,
            "page": 4,
            "text": "until convergence of parameters (0, ⌀) return 0, ⌀"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 921
                },
                {
                    "x": 2108,
                    "y": 921
                },
                {
                    "x": 2108,
                    "y": 1219
                },
                {
                    "x": 441,
                    "y": 1219
                }
            ],
            "category": "paragraph",
            "html": "<p id='39' style='font-size:18px'>Often, the KL-divergence DKL (��(z|x(i))|pe (z)) of eq. (3) can be integrated analytically (see<br>appendix B), such that only the expected reconstruction error Eq⌀ (z|x(i)) [logpe (x(i)|z)] requires<br>estimation by sampling. The KL-divergence term can then be interpreted as regularizing 中, encour-<br>aging the approximate posterior to be close to the prior pe(z). This yields a second version of the<br>SGVB estimator LB (0, ⌀;x(i) ) 12 L(0, ⌀; x(i)), corresponding to eq. (3), which typically has less<br>variance than the generic estimator:</p>",
            "id": 39,
            "page": 4,
            "text": "Often, the KL-divergence DKL (��(z|x(i))|pe (z)) of eq. (3) can be integrated analytically (see appendix B), such that only the expected reconstruction error Eq⌀ (z|x(i)) [logpe (x(i)|z)] requires estimation by sampling. The KL-divergence term can then be interpreted as regularizing 中, encouraging the approximate posterior to be close to the prior pe(z). This yields a second version of the SGVB estimator LB (0, ⌀;x(i) ) 12 L(0, ⌀; x(i)), corresponding to eq. (3), which typically has less variance than the generic estimator:"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1443
                },
                {
                    "x": 2104,
                    "y": 1443
                },
                {
                    "x": 2104,
                    "y": 1534
                },
                {
                    "x": 441,
                    "y": 1534
                }
            ],
            "category": "paragraph",
            "html": "<p id='40' style='font-size:14px'>Given multiple datapoints from a dataset X with N datapoints, we can construct an estimator of the<br>marginal likelihood lower bound of the full dataset, based on minibatches:</p>",
            "id": 40,
            "page": 4,
            "text": "Given multiple datapoints from a dataset X with N datapoints, we can construct an estimator of the marginal likelihood lower bound of the full dataset, based on minibatches:"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1692
                },
                {
                    "x": 2107,
                    "y": 1692
                },
                {
                    "x": 2107,
                    "y": 1980
                },
                {
                    "x": 441,
                    "y": 1980
                }
            ],
            "category": "paragraph",
            "html": "<p id='41' style='font-size:16px'>where the minibatch XM = {x(i)}N=1 is a randomly drawn sample of M datapoints from the<br>full dataset X with N datapoints. In our experiments we found that the number of samples L<br>per datapoint can be set to 1 as long as the minibatch size M was large enough, e.g. M = 100.<br>Derivatives ▽�,�L(0;XM) can be taken, and the resulting gradients can be used in conjunction<br>with stochastic optimization methods such as SGD or Adagrad [DHS10]. See algorithm 1 for a<br>basic approach to compute the stochastic gradients.</p>",
            "id": 41,
            "page": 4,
            "text": "where the minibatch XM = {x(i)}N=1 is a randomly drawn sample of M datapoints from the full dataset X with N datapoints. In our experiments we found that the number of samples L per datapoint can be set to 1 as long as the minibatch size M was large enough, e.g. M = 100. Derivatives ▽�,�L(0;XM) can be taken, and the resulting gradients can be used in conjunction with stochastic optimization methods such as SGD or Adagrad [DHS10]. See algorithm 1 for a basic approach to compute the stochastic gradients."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2003
                },
                {
                    "x": 2108,
                    "y": 2003
                },
                {
                    "x": 2108,
                    "y": 2395
                },
                {
                    "x": 441,
                    "y": 2395
                }
            ],
            "category": "paragraph",
            "html": "<p id='42' style='font-size:16px'>A connection with auto-encoders becomes clear when looking at the objective function given at<br>eq. (7). The first term is (the KL divergence of the approximate posterior from the prior) acts as a<br>regularizer, while the second term is a an expected negative reconstruction error. The function 9⌀(.)<br>is chosen such that it maps a datapoint x(i) and a random noise vector E(l) to a sample from the<br>approximate posterior for that datapoint: z(i,l) = 9⌀(€(l) x(i)) where Z(i,l) ~ 9⌀ (z|x(i). Subse-<br>quently, the sample z(i,l) is then input to function log pe(x@iz(iz(i)), which equals the probability<br>density (or mass) of datapoint x(i) under the generative model, given z(i,l). This term is a negative<br>reconstruction error in auto-encoder parlance.</p>",
            "id": 42,
            "page": 4,
            "text": "A connection with auto-encoders becomes clear when looking at the objective function given at eq. (7). The first term is (the KL divergence of the approximate posterior from the prior) acts as a regularizer, while the second term is a an expected negative reconstruction error. The function 9⌀(.) is chosen such that it maps a datapoint x(i) and a random noise vector E(l) to a sample from the approximate posterior for that datapoint: z(i,l) = 9⌀(€(l) x(i)) where Z(i,l) ~ 9⌀ (z|x(i). Subsequently, the sample z(i,l) is then input to function log pe(x@iz(iz(i)), which equals the probability density (or mass) of datapoint x(i) under the generative model, given z(i,l). This term is a negative reconstruction error in auto-encoder parlance."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2446
                },
                {
                    "x": 1065,
                    "y": 2446
                },
                {
                    "x": 1065,
                    "y": 2495
                },
                {
                    "x": 443,
                    "y": 2495
                }
            ],
            "category": "paragraph",
            "html": "<p id='43' style='font-size:14px'>2.4 The reparameterization trick</p>",
            "id": 43,
            "page": 4,
            "text": "2.4 The reparameterization trick"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2533
                },
                {
                    "x": 2107,
                    "y": 2533
                },
                {
                    "x": 2107,
                    "y": 2764
                },
                {
                    "x": 442,
                    "y": 2764
                }
            ],
            "category": "paragraph",
            "html": "<p id='44' style='font-size:16px'>In order to solve our problem we invoked an alternative method for generating samples from<br>9⌀ (z|x). The essential parameterization trick is quite simple. Let Z be a continuous random vari-<br>able, and Z ~ 9⌀(Z|X) be some conditional distribution. It is then often possible to express the<br>random variable Z as a deterministic variable Z = g⌀(e, x), where E is an auxiliary variable with<br>independent marginal p(E), and 9⌀ (.) is some vector-valued function parameterized by ⌀.</p>",
            "id": 44,
            "page": 4,
            "text": "In order to solve our problem we invoked an alternative method for generating samples from 9⌀ (z|x). The essential parameterization trick is quite simple. Let Z be a continuous random variable, and Z ~ 9⌀(Z|X) be some conditional distribution. It is then often possible to express the random variable Z as a deterministic variable Z = g⌀(e, x), where E is an auxiliary variable with independent marginal p(E), and 9⌀ (.) is some vector-valued function parameterized by ⌀."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2786
                },
                {
                    "x": 2108,
                    "y": 2786
                },
                {
                    "x": 2108,
                    "y": 2986
                },
                {
                    "x": 442,
                    "y": 2986
                }
            ],
            "category": "paragraph",
            "html": "<p id='45' style='font-size:16px'>This reparameterization is useful for our case since it can be used to rewrite an expectation w.r.t<br>1⌀ (zx) such that the Monte Carlo estimate of the expectation is differentiable w.r.t. ⌀. A proof<br>is as follows. Given the deterministic mapping Z = 9⌀(E,X) we know that 9⌀(zx) IIi dzi =<br>p(E) IIi dei. Therefore1 , S q⌀(z|x)f(z) dz = S p(E)f(z) de = S p(E)f(g�(E, x)) de. It follows</p>",
            "id": 45,
            "page": 4,
            "text": "This reparameterization is useful for our case since it can be used to rewrite an expectation w.r.t 1⌀ (zx) such that the Monte Carlo estimate of the expectation is differentiable w.r.t. ⌀. A proof is as follows. Given the deterministic mapping Z = 9⌀(E,X) we know that 9⌀(zx) IIi dzi = p(E) IIi dei. Therefore1 , S q⌀(z|x)f(z) dz = S p(E)f(z) de = S p(E)f(g�(E, x)) de. It follows"
        },
        {
            "bounding_box": [
                {
                    "x": 499,
                    "y": 3004
                },
                {
                    "x": 1630,
                    "y": 3004
                },
                {
                    "x": 1630,
                    "y": 3058
                },
                {
                    "x": 499,
                    "y": 3058
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='46' style='font-size:14px'>1Note that for infinitesimals we use the notational convention dz = IIi dzi</p>",
            "id": 46,
            "page": 4,
            "text": "1Note that for infinitesimals we use the notational convention dz = IIi dzi"
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3135
                },
                {
                    "x": 1288,
                    "y": 3135
                },
                {
                    "x": 1288,
                    "y": 3168
                },
                {
                    "x": 1260,
                    "y": 3168
                }
            ],
            "category": "footer",
            "html": "<footer id='47' style='font-size:14px'>4</footer>",
            "id": 47,
            "page": 4,
            "text": "4"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 340
                },
                {
                    "x": 2108,
                    "y": 340
                },
                {
                    "x": 2108,
                    "y": 494
                },
                {
                    "x": 442,
                    "y": 494
                }
            ],
            "category": "paragraph",
            "html": "<p id='48' style='font-size:16px'>that a differentiable estimator can be constructed: S q⌀(z|x)f(z) dz 1 1�i=1 f(g�(x, E(l)))<br>where €(l) ~ p(E). In section 2.3 we applied this trick to obtain a differentiable estimator of the<br>variational lower bound.</p>",
            "id": 48,
            "page": 5,
            "text": "that a differentiable estimator can be constructed: S q⌀(z|x)f(z) dz 1 1�i=1 f(g�(x, E(l))) where €(l) ~ p(E). In section 2.3 we applied this trick to obtain a differentiable estimator of the variational lower bound."
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 518
                },
                {
                    "x": 2106,
                    "y": 518
                },
                {
                    "x": 2106,
                    "y": 618
                },
                {
                    "x": 440,
                    "y": 618
                }
            ],
            "category": "paragraph",
            "html": "<p id='49' style='font-size:14px'>Take, for example, the univariate Gaussian case: let 2 ~ p(z|x) = N(�, o2). In this case, a valid<br>reparameterization is 2 = H 十 O�, where E is an auxiliary noise variable E ~ N(0, 1). Therefore,</p>",
            "id": 49,
            "page": 5,
            "text": "Take, for example, the univariate Gaussian case: let 2 ~ p(z|x) = N(�, o2). In this case, a valid reparameterization is 2 = H 十 O�, where E is an auxiliary noise variable E ~ N(0, 1). Therefore,"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 694
                },
                {
                    "x": 2106,
                    "y": 694
                },
                {
                    "x": 2106,
                    "y": 789
                },
                {
                    "x": 441,
                    "y": 789
                }
            ],
            "category": "paragraph",
            "html": "<p id='50' style='font-size:14px'>For which 9⌀(zx) can we choose such a differentiable transformation 9⌀(.) and auxiliary variable<br>E ~ p(E)? Three basic approaches are:</p>",
            "id": 50,
            "page": 5,
            "text": "For which 9⌀(zx) can we choose such a differentiable transformation 9⌀(.) and auxiliary variable E ~ p(E)? Three basic approaches are:"
        },
        {
            "bounding_box": [
                {
                    "x": 537,
                    "y": 815
                },
                {
                    "x": 2110,
                    "y": 815
                },
                {
                    "x": 2110,
                    "y": 1356
                },
                {
                    "x": 537,
                    "y": 1356
                }
            ],
            "category": "paragraph",
            "html": "<p id='51' style='font-size:16px'>1. Tractable inverse CDF. In this case, let E ~ U(0,I), and let 9⌀ (E, x) be the inverse CDF of<br>9⌀(Z|x). Examples: Exponential, Cauchy, Logistic, Rayleigh, Pareto, Weibull, Reciprocal,<br>Gompertz, Gumbel and Erlang distributions.<br>2. Analogous to the Gaussian example, for any \"location-scale\" family of distributions we can<br>choose the standard distribution (with location = 0, scale = 1) as the auxiliary variable<br>E, and let g(.) = location + scale · E. Examples: Laplace, Elliptical, Student's t, Logistic,<br>Uniform, Triangular and Gaussian distributions.<br>3. Composition: It is often possible to express random variables as different transformations<br>of auxiliary variables. Examples: Log-Normal (exponentiation of normally distributed<br>variable), Gamma (a sum over exponentially distributed variables), Dirichlet (weighted<br>sum of Gamma variates), Beta, Chi-Squared, and F distributions.</p>",
            "id": 51,
            "page": 5,
            "text": "1. Tractable inverse CDF. In this case, let E ~ U(0,I), and let 9⌀ (E, x) be the inverse CDF of 9⌀(Z|x). Examples: Exponential, Cauchy, Logistic, Rayleigh, Pareto, Weibull, Reciprocal, Gompertz, Gumbel and Erlang distributions. 2. Analogous to the Gaussian example, for any \"location-scale\" family of distributions we can choose the standard distribution (with location = 0, scale = 1) as the auxiliary variable E, and let g(.) = location + scale · E. Examples: Laplace, Elliptical, Student's t, Logistic, Uniform, Triangular and Gaussian distributions. 3. Composition: It is often possible to express random variables as different transformations of auxiliary variables. Examples: Log-Normal (exponentiation of normally distributed variable), Gamma (a sum over exponentially distributed variables), Dirichlet (weighted sum of Gamma variates), Beta, Chi-Squared, and F distributions."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1390
                },
                {
                    "x": 2104,
                    "y": 1390
                },
                {
                    "x": 2104,
                    "y": 1485
                },
                {
                    "x": 442,
                    "y": 1485
                }
            ],
            "category": "paragraph",
            "html": "<p id='52' style='font-size:16px'>When all three approaches fail, good approximations to the inverse CDF exist requiring computa-<br>tions with time complexity comparable to the PDF (see e.g. [Dev86] for some methods).</p>",
            "id": 52,
            "page": 5,
            "text": "When all three approaches fail, good approximations to the inverse CDF exist requiring computations with time complexity comparable to the PDF (see e.g. [Dev86] for some methods)."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1547
                },
                {
                    "x": 1301,
                    "y": 1547
                },
                {
                    "x": 1301,
                    "y": 1603
                },
                {
                    "x": 442,
                    "y": 1603
                }
            ],
            "category": "paragraph",
            "html": "<p id='53' style='font-size:20px'>3 Example: Variational Auto-Encoder</p>",
            "id": 53,
            "page": 5,
            "text": "3 Example: Variational Auto-Encoder"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1650
                },
                {
                    "x": 2106,
                    "y": 1650
                },
                {
                    "x": 2106,
                    "y": 1790
                },
                {
                    "x": 442,
                    "y": 1790
                }
            ],
            "category": "paragraph",
            "html": "<p id='54' style='font-size:16px'>In this section we'll give an example where we use a neural network for the probabilistic encoder<br>1⌀ (z|x) (the approximation to the posterior of the generative model pe (x, z)) and where the param-<br>eters ⌀ and 0 are optimized jointly with the AEVB algorithm.</p>",
            "id": 54,
            "page": 5,
            "text": "In this section we'll give an example where we use a neural network for the probabilistic encoder 1⌀ (z|x) (the approximation to the posterior of the generative model pe (x, z)) and where the parameters ⌀ and 0 are optimized jointly with the AEVB algorithm."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1812
                },
                {
                    "x": 2106,
                    "y": 1812
                },
                {
                    "x": 2106,
                    "y": 2183
                },
                {
                    "x": 442,
                    "y": 2183
                }
            ],
            "category": "paragraph",
            "html": "<p id='55' style='font-size:16px'>Let the prior over the latent variables be the centered isotropic multivariate Gaussian pe(z) =<br>N(z; 0, I). Note that in this case, the prior lacks parameters. We let po (x|z) be a multivariate<br>Gaussian (in case of real-valued data) or Bernoulli (in case of binary data) whose distribution pa-<br>rameters are computed from Z with a MLP (a fully-connected neural network with a single hidden<br>layer, see appendix C). Note the true posterior pe (zx) is in this case intractable. While there is<br>much freedom in the form 1⌀ (z|x), we'll assume the true (but intractable) posterior takes on a ap-<br>proximate Gaussian form with an approximately diagonal covariance. In this case, we can let the<br>variational approximate posterior be a multivariate Gaussian with a diagonal covariance structure2:</p>",
            "id": 55,
            "page": 5,
            "text": "Let the prior over the latent variables be the centered isotropic multivariate Gaussian pe(z) = N(z; 0, I). Note that in this case, the prior lacks parameters. We let po (x|z) be a multivariate Gaussian (in case of real-valued data) or Bernoulli (in case of binary data) whose distribution parameters are computed from Z with a MLP (a fully-connected neural network with a single hidden layer, see appendix C). Note the true posterior pe (zx) is in this case intractable. While there is much freedom in the form 1⌀ (z|x), we'll assume the true (but intractable) posterior takes on a approximate Gaussian form with an approximately diagonal covariance. In this case, we can let the variational approximate posterior be a multivariate Gaussian with a diagonal covariance structure2:"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2273
                },
                {
                    "x": 2105,
                    "y": 2273
                },
                {
                    "x": 2105,
                    "y": 2372
                },
                {
                    "x": 442,
                    "y": 2372
                }
            ],
            "category": "paragraph",
            "html": "<p id='56' style='font-size:16px'>where the mean and s.d. of the approximate posterior, �(i) and �(i) , are outputs of the encoding<br>MLP, i.e. nonlinear functions of datapoint x(i) and the variational parameters 0 (see appendix C).</p>",
            "id": 56,
            "page": 5,
            "text": "where the mean and s.d. of the approximate posterior, �(i) and �(i) , are outputs of the encoding MLP, i.e. nonlinear functions of datapoint x(i) and the variational parameters 0 (see appendix C)."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2398
                },
                {
                    "x": 2107,
                    "y": 2398
                },
                {
                    "x": 2107,
                    "y": 2639
                },
                {
                    "x": 442,
                    "y": 2639
                }
            ],
            "category": "paragraph",
            "html": "<p id='57' style='font-size:16px'>As explained in section 2.4, we sample from the posterior z(i,l) ~ 9⌀(Z|x(i) using z(i,l) =<br>9⌀(x(i) , E(l)) = M(i) + �(i) ⊙ e(l) where E(l) ~ N(0, I). With ⊙ we signify an element-wise<br>product. In this model both pe(z) (the prior) and 1⌀ (z|x) are Gaussian; in this case, we can use the<br>estimator of eq. (7) where the KL divergence can be computed and differentiated without estimation<br>(see appendix B). The resulting estimator for this model and datapoint x(i) is:</p>",
            "id": 57,
            "page": 5,
            "text": "As explained in section 2.4, we sample from the posterior z(i,l) ~ 9⌀(Z|x(i) using z(i,l) = 9⌀(x(i) , E(l)) = M(i) + �(i) ⊙ e(l) where E(l) ~ N(0, I). With ⊙ we signify an element-wise product. In this model both pe(z) (the prior) and 1⌀ (z|x) are Gaussian; in this case, we can use the estimator of eq. (7) where the KL divergence can be computed and differentiated without estimation (see appendix B). The resulting estimator for this model and datapoint x(i) is:"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2883
                },
                {
                    "x": 2103,
                    "y": 2883
                },
                {
                    "x": 2103,
                    "y": 2980
                },
                {
                    "x": 442,
                    "y": 2980
                }
            ],
            "category": "paragraph",
            "html": "<p id='58' style='font-size:16px'>As explained above and in appendix C, the decoding term log pe(x(i)|z(i,l)) is a Bernoulli or Gaus-<br>sian MLP, depending on the type of data we are modelling.</p>",
            "id": 58,
            "page": 5,
            "text": "As explained above and in appendix C, the decoding term log pe(x(i)|z(i,l)) is a Bernoulli or Gaussian MLP, depending on the type of data we are modelling."
        },
        {
            "bounding_box": [
                {
                    "x": 498,
                    "y": 3008
                },
                {
                    "x": 1700,
                    "y": 3008
                },
                {
                    "x": 1700,
                    "y": 3053
                },
                {
                    "x": 498,
                    "y": 3053
                }
            ],
            "category": "paragraph",
            "html": "<p id='59' style='font-size:14px'>2Note that this is just a (simplifying) choice, and not a limitation of our method.</p>",
            "id": 59,
            "page": 5,
            "text": "2Note that this is just a (simplifying) choice, and not a limitation of our method."
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3132
                },
                {
                    "x": 1289,
                    "y": 3132
                },
                {
                    "x": 1289,
                    "y": 3171
                },
                {
                    "x": 1260,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='60' style='font-size:16px'>5</footer>",
            "id": 60,
            "page": 5,
            "text": "5"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 340
                },
                {
                    "x": 815,
                    "y": 340
                },
                {
                    "x": 815,
                    "y": 393
                },
                {
                    "x": 443,
                    "y": 393
                }
            ],
            "category": "paragraph",
            "html": "<p id='61' style='font-size:20px'>4 Related work</p>",
            "id": 61,
            "page": 6,
            "text": "4 Related work"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 441
                },
                {
                    "x": 2107,
                    "y": 441
                },
                {
                    "x": 2107,
                    "y": 766
                },
                {
                    "x": 442,
                    "y": 766
                }
            ],
            "category": "paragraph",
            "html": "<p id='62' style='font-size:18px'>The wake-sleep algorithm [HDFN95] is, to the best of our knowledge, the only other on-line learn-<br>ing method in the literature that is applicable to the same general class of continuous latent variable<br>models. Like our method, the wake-sleep algorithm employs a recognition model that approximates<br>the true posterior. A drawback of the wake-sleep algorithm is that it requires a concurrent optimiza-<br>tion of two objective functions, which together do not correspond to optimization of (a bound of)<br>the marginal likelihood. An advantage of wake-sleep is that it also applies to models with discrete<br>latent variables. Wake-Sleep has the same computational complexity as AEVB per datapoint.</p>",
            "id": 62,
            "page": 6,
            "text": "The wake-sleep algorithm [HDFN95] is, to the best of our knowledge, the only other on-line learning method in the literature that is applicable to the same general class of continuous latent variable models. Like our method, the wake-sleep algorithm employs a recognition model that approximates the true posterior. A drawback of the wake-sleep algorithm is that it requires a concurrent optimization of two objective functions, which together do not correspond to optimization of (a bound of) the marginal likelihood. An advantage of wake-sleep is that it also applies to models with discrete latent variables. Wake-Sleep has the same computational complexity as AEVB per datapoint."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 788
                },
                {
                    "x": 2108,
                    "y": 788
                },
                {
                    "x": 2108,
                    "y": 1112
                },
                {
                    "x": 441,
                    "y": 1112
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='63' style='font-size:16px'>Stochastic variational inference [HB WP13] has recently received increasing interest. Recently,<br>[BJP12] introduced a control variate schemes to reduce the high variance of the naive gradient<br>estimator discussed in section 2.1, and applied to exponential family approximations of the poste-<br>rior. In [RGB13] some general methods, i.e. a control variate scheme, were introduced for reducing<br>the variance of the original gradient estimator. In [SK13], a similar reparameterization as in this<br>paper was used in an efficient version of a stochastic variational inference algorithm for learning the<br>natural parameters of exponential-family approximating distributions.</p>",
            "id": 63,
            "page": 6,
            "text": "Stochastic variational inference [HB WP13] has recently received increasing interest. Recently, [BJP12] introduced a control variate schemes to reduce the high variance of the naive gradient estimator discussed in section 2.1, and applied to exponential family approximations of the posterior. In [RGB13] some general methods, i.e. a control variate scheme, were introduced for reducing the variance of the original gradient estimator. In [SK13], a similar reparameterization as in this paper was used in an efficient version of a stochastic variational inference algorithm for learning the natural parameters of exponential-family approximating distributions."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1131
                },
                {
                    "x": 2108,
                    "y": 1131
                },
                {
                    "x": 2108,
                    "y": 1408
                },
                {
                    "x": 442,
                    "y": 1408
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='64' style='font-size:18px'>The AEVB algorithm exposes a connection between directed probabilistic models (trained with a<br>variational objective) and auto-encoders. A connection between linear auto-encoders and a certain<br>class of generative linear-Gaussian models has long been known. In [Row98] it was shown that PCA<br>corresponds to the maximum-likelihood (ML) solution of a special case of the linear-Gaussian model<br>with a prior p(z) = N(0, I) and a conditional distribution p(x|z) = N(x; Wz, €I), specifically the<br>case with infinitesimally small e.</p>",
            "id": 64,
            "page": 6,
            "text": "The AEVB algorithm exposes a connection between directed probabilistic models (trained with a variational objective) and auto-encoders. A connection between linear auto-encoders and a certain class of generative linear-Gaussian models has long been known. In [Row98] it was shown that PCA corresponds to the maximum-likelihood (ML) solution of a special case of the linear-Gaussian model with a prior p(z) = N(0, I) and a conditional distribution p(x|z) = N(x; Wz, €I), specifically the case with infinitesimally small e."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1432
                },
                {
                    "x": 2109,
                    "y": 1432
                },
                {
                    "x": 2109,
                    "y": 2255
                },
                {
                    "x": 441,
                    "y": 2255
                }
            ],
            "category": "paragraph",
            "html": "<p id='65' style='font-size:18px'>In relevant recent work on autoencoders [VLL+10] it was shown that the training criterion of un-<br>regularized autoencoders corresponds to maximization of a lower bound (see the infomax princi-<br>ple [Lin89]) of the mutual information between input X and latent representation Z. Maximiz-<br>ing (w.r.t. parameters) of the mutual information is equivalent to maximizing the conditional en-<br>tropy, which is lower bounded by the expected loglikelihood of the data under the autoencoding<br>model [VLL + 10], i.e. the negative reconstrution error. However, it is well known that this recon-<br>struction criterion is in itself not sufficient for learning useful representations [BCV13]. Regular-<br>ization techniques have been proposed to make autoencoders learn useful representations, such as<br>denoising, contractive and sparse autoencoder variants [BCV13]. The SGVB objective contains a<br>regularization term dictated by the variational bound (e.g. eq. (10)), lacking the usual nuisance regu-<br>larization hyperparameter required to learn useful representations. Related are also encoder-decoder<br>architectures such as the predictive sparse decomposition (PSD) [KRL08], from which we drew<br>some inspiration. Also relevant are the recently introduced Generative Stochastic Networks [BTL13]<br>where noisy auto-encoders learn the transition operator of a Markov chain that samples from the data<br>distribution. In [SL10] a recognition model was employed for efficient learning with Deep Boltz-<br>mann Machines. These methods are targeted at either unnormalized models (i.e. undirected models<br>like Boltzmann machines) or limited to sparse coding models, in contrast to our proposed algorithm<br>for learning a general class of directed probabilistic models.</p>",
            "id": 65,
            "page": 6,
            "text": "In relevant recent work on autoencoders [VLL+10] it was shown that the training criterion of unregularized autoencoders corresponds to maximization of a lower bound (see the infomax principle [Lin89]) of the mutual information between input X and latent representation Z. Maximizing (w.r.t. parameters) of the mutual information is equivalent to maximizing the conditional entropy, which is lower bounded by the expected loglikelihood of the data under the autoencoding model [VLL + 10], i.e. the negative reconstrution error. However, it is well known that this reconstruction criterion is in itself not sufficient for learning useful representations [BCV13]. Regularization techniques have been proposed to make autoencoders learn useful representations, such as denoising, contractive and sparse autoencoder variants [BCV13]. The SGVB objective contains a regularization term dictated by the variational bound (e.g. eq. (10)), lacking the usual nuisance regularization hyperparameter required to learn useful representations. Related are also encoder-decoder architectures such as the predictive sparse decomposition (PSD) [KRL08], from which we drew some inspiration. Also relevant are the recently introduced Generative Stochastic Networks [BTL13] where noisy auto-encoders learn the transition operator of a Markov chain that samples from the data distribution. In [SL10] a recognition model was employed for efficient learning with Deep Boltzmann Machines. These methods are targeted at either unnormalized models (i.e. undirected models like Boltzmann machines) or limited to sparse coding models, in contrast to our proposed algorithm for learning a general class of directed probabilistic models."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2277
                },
                {
                    "x": 2106,
                    "y": 2277
                },
                {
                    "x": 2106,
                    "y": 2512
                },
                {
                    "x": 442,
                    "y": 2512
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='66' style='font-size:16px'>The recently proposed DARN method [GMW13], also learns a directed probabilistic model using<br>an auto-encoding structure, however their method applies to binary latent variables. Even more<br>recently, [RMW 14] also make the connection between auto-encoders, directed proabilistic models<br>and stochastic variational inference using the reparameterization trick we describe in this paper.<br>Their work was developed independently of ours and provides an additional perspective on AEVB.</p>",
            "id": 66,
            "page": 6,
            "text": "The recently proposed DARN method [GMW13], also learns a directed probabilistic model using an auto-encoding structure, however their method applies to binary latent variables. Even more recently, [RMW 14] also make the connection between auto-encoders, directed proabilistic models and stochastic variational inference using the reparameterization trick we describe in this paper. Their work was developed independently of ours and provides an additional perspective on AEVB."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2573
                },
                {
                    "x": 800,
                    "y": 2573
                },
                {
                    "x": 800,
                    "y": 2627
                },
                {
                    "x": 444,
                    "y": 2627
                }
            ],
            "category": "paragraph",
            "html": "<p id='67' style='font-size:22px'>5 Experiments</p>",
            "id": 67,
            "page": 6,
            "text": "5 Experiments"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2675
                },
                {
                    "x": 2105,
                    "y": 2675
                },
                {
                    "x": 2105,
                    "y": 2770
                },
                {
                    "x": 444,
                    "y": 2770
                }
            ],
            "category": "paragraph",
            "html": "<p id='68' style='font-size:18px'>We trained generative models of images from the MNIST and Frey Face datasets3 and compared<br>learning algorithms in terms of the variational lower bound, and the estimated marginal likelihood.</p>",
            "id": 68,
            "page": 6,
            "text": "We trained generative models of images from the MNIST and Frey Face datasets3 and compared learning algorithms in terms of the variational lower bound, and the estimated marginal likelihood."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2792
                },
                {
                    "x": 2107,
                    "y": 2792
                },
                {
                    "x": 2107,
                    "y": 2977
                },
                {
                    "x": 443,
                    "y": 2977
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='69' style='font-size:16px'>The generative model (encoder) and variational approximation (decoder) from section 3 were used,<br>where the described encoder and decoder have an equal number of hidden units. Since the Frey<br>Face data are continuous, we used a decoder with Gaussian outputs, identical to the encoder, except<br>that the means were constrained to the interval (0, 1) using a sigmoidal activation function at the</p>",
            "id": 69,
            "page": 6,
            "text": "The generative model (encoder) and variational approximation (decoder) from section 3 were used, where the described encoder and decoder have an equal number of hidden units. Since the Frey Face data are continuous, we used a decoder with Gaussian outputs, identical to the encoder, except that the means were constrained to the interval (0, 1) using a sigmoidal activation function at the"
        },
        {
            "bounding_box": [
                {
                    "x": 499,
                    "y": 3007
                },
                {
                    "x": 1580,
                    "y": 3007
                },
                {
                    "x": 1580,
                    "y": 3052
                },
                {
                    "x": 499,
                    "y": 3052
                }
            ],
            "category": "paragraph",
            "html": "<p id='70' style='font-size:14px'>3 Available at http : / / www · CS · nyu · edu/ ~ roweis/ data . html</p>",
            "id": 70,
            "page": 6,
            "text": "3 Available at http : / / www · CS · nyu · edu/ ~ roweis/ data . html"
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3136
                },
                {
                    "x": 1289,
                    "y": 3136
                },
                {
                    "x": 1289,
                    "y": 3172
                },
                {
                    "x": 1260,
                    "y": 3172
                }
            ],
            "category": "footer",
            "html": "<footer id='71' style='font-size:14px'>6</footer>",
            "id": 71,
            "page": 6,
            "text": "6"
        },
        {
            "bounding_box": [
                {
                    "x": 460,
                    "y": 330
                },
                {
                    "x": 2013,
                    "y": 330
                },
                {
                    "x": 2013,
                    "y": 955
                },
                {
                    "x": 460,
                    "y": 955
                }
            ],
            "category": "figure",
            "html": "<figure><img id='72' style='font-size:14px' alt=\"MNIST, N. =3 MNIST, N =5 MNIST, N =10 MNIST, N. =20 MNIST, N =200\n-100 -100 -100 -100 -100\n-110 -110 -110 -110 -110\n-120 -120 -120 -120 -120\nL\n-130 -130 -130 130 -130\n-140 -140 -140 -140 -140\n-150 -150 - 150 -150 -150\n105 106 10 108 105 106 107 108 105 106 107 108 105 106 107 108 105 106 107 108\n# Training samples evaluated\nFrey Face, Nz=2 Frey Face, Nz=5 Frey Face, N2=10 Frey Face, Nz =20\n1600 1600 1600 1600\n1400 1400 1400 1400\n1200 1200 1200 1200\n- · Wake-Sleep (test)\n1000 1000 1000 1000\nWake-Sleep (train)\nL 800 800 800 800\nAEVB (test)\n600 600 600 600\nAEVB (train)\n400 400 400 400\n200 200 200 200\n0 0 0 0\n105 106 10 108 105 106 10 108 105 106 10 108 105 106 10 108\" data-coord=\"top-left:(460,330); bottom-right:(2013,955)\" /></figure>",
            "id": 72,
            "page": 7,
            "text": "MNIST, N. =3 MNIST, N =5 MNIST, N =10 MNIST, N. =20 MNIST, N =200 -100 -100 -100 -100 -100 -110 -110 -110 -110 -110 -120 -120 -120 -120 -120 L -130 -130 -130 130 -130 -140 -140 -140 -140 -140 -150 -150 - 150 -150 -150 105 106 10 108 105 106 107 108 105 106 107 108 105 106 107 108 105 106 107 108 # Training samples evaluated Frey Face, Nz=2 Frey Face, Nz=5 Frey Face, N2=10 Frey Face, Nz =20 1600 1600 1600 1600 1400 1400 1400 1400 1200 1200 1200 1200 - · Wake-Sleep (test) 1000 1000 1000 1000 Wake-Sleep (train) L 800 800 800 800 AEVB (test) 600 600 600 600 AEVB (train) 400 400 400 400 200 200 200 200 0 0 0 0 105 106 10 108 105 106 10 108 105 106 10 108 105 106 10 108"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 991
                },
                {
                    "x": 2108,
                    "y": 991
                },
                {
                    "x": 2108,
                    "y": 1362
                },
                {
                    "x": 442,
                    "y": 1362
                }
            ],
            "category": "caption",
            "html": "<caption id='73' style='font-size:18px'>Figure 2: Comparison of our AEVB method to the wake-sleep algorithm, in terms of optimizing the<br>lower bound, for different dimensionality of latent space (Nz). Our method converged considerably<br>faster and reached a better solution in all experiments. Interestingly enough, more latent variables<br>does not result in more overfitting, which is explained by the regularizing effect of the lower bound.<br>Vertical axis: the estimated average variational lower bound per datapoint. The estimator variance<br>was small (< 1) and omitted. Horizontal axis: amount of training points evaluated. Computa-<br>tion took around 20-40 minutes per million training samples with a Intel Xeon CPU running at an<br>effective 40 GFLOPS.</caption>",
            "id": 73,
            "page": 7,
            "text": "Figure 2: Comparison of our AEVB method to the wake-sleep algorithm, in terms of optimizing the lower bound, for different dimensionality of latent space (Nz). Our method converged considerably faster and reached a better solution in all experiments. Interestingly enough, more latent variables does not result in more overfitting, which is explained by the regularizing effect of the lower bound. Vertical axis: the estimated average variational lower bound per datapoint. The estimator variance was small (< 1) and omitted. Horizontal axis: amount of training points evaluated. Computation took around 20-40 minutes per million training samples with a Intel Xeon CPU running at an effective 40 GFLOPS."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1489
                },
                {
                    "x": 2105,
                    "y": 1489
                },
                {
                    "x": 2105,
                    "y": 1581
                },
                {
                    "x": 442,
                    "y": 1581
                }
            ],
            "category": "paragraph",
            "html": "<p id='74' style='font-size:16px'>decoder output. Note that with hidden units we refer to the hidden layer of the neural networks of<br>the encoder and decoder.</p>",
            "id": 74,
            "page": 7,
            "text": "decoder output. Note that with hidden units we refer to the hidden layer of the neural networks of the encoder and decoder."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1606
                },
                {
                    "x": 2106,
                    "y": 1606
                },
                {
                    "x": 2106,
                    "y": 1834
                },
                {
                    "x": 441,
                    "y": 1834
                }
            ],
            "category": "paragraph",
            "html": "<p id='75' style='font-size:22px'>Parameters are updated using stochastic gradient ascent where gradients are computed by differenti-<br>ating the lower bound estimator ▽�,⌀L(�, ⌀; X) (see algorithm 1), plus a small weight decay term<br>corresponding to a prior p(0) = N(0,I). Optimization of this objective is equivalent to approxi-<br>mate MAP estimation, where the likelihood gradient is approximated by the gradient of the lower<br>bound.</p>",
            "id": 75,
            "page": 7,
            "text": "Parameters are updated using stochastic gradient ascent where gradients are computed by differentiating the lower bound estimator ▽�,⌀L(�, ⌀; X) (see algorithm 1), plus a small weight decay term corresponding to a prior p(0) = N(0,I). Optimization of this objective is equivalent to approximate MAP estimation, where the likelihood gradient is approximated by the gradient of the lower bound."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1857
                },
                {
                    "x": 2107,
                    "y": 1857
                },
                {
                    "x": 2107,
                    "y": 2180
                },
                {
                    "x": 441,
                    "y": 2180
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='76' style='font-size:20px'>We compared performance of AEVB to the wake-sleep algorithm [HDFN95]. We employed the<br>same encoder (also called recognition model) for the wake-sleep algorithm and the variational auto-<br>encoder. All parameters, both variational and generative, were initialized by random sampling from<br>N(0, 0.01), and were jointly stochastically optimized using the MAP criterion. Stepsizes were<br>adapted with Adagrad [DHS10]; the Adagrad global stepsize parameters were chosen from {0.01,<br>0.02, 0.1} based on performance on the training set in the first few iterations. Minibatches of size<br>M = 100 were used, with L = 1 samples per datapoint.</p>",
            "id": 76,
            "page": 7,
            "text": "We compared performance of AEVB to the wake-sleep algorithm [HDFN95]. We employed the same encoder (also called recognition model) for the wake-sleep algorithm and the variational autoencoder. All parameters, both variational and generative, were initialized by random sampling from N(0, 0.01), and were jointly stochastically optimized using the MAP criterion. Stepsizes were adapted with Adagrad [DHS10]; the Adagrad global stepsize parameters were chosen from {0.01, 0.02, 0.1} based on performance on the training set in the first few iterations. Minibatches of size M = 100 were used, with L = 1 samples per datapoint."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2269
                },
                {
                    "x": 2108,
                    "y": 2269
                },
                {
                    "x": 2108,
                    "y": 2596
                },
                {
                    "x": 442,
                    "y": 2596
                }
            ],
            "category": "paragraph",
            "html": "<p id='77' style='font-size:18px'>Likelihood lower bound We trained generative models (decoders) and corresponding encoders<br>(a.k.a. recognition models) having 500 hidden units in case of MNIST, and 200 hidden units in case<br>of the Frey Face dataset (to prevent overfitting, since itis a considerably smaller dataset). The chosen<br>number of hidden units is based on prior literature on auto-encoders, and the relative performance<br>of different algorithms was not very sensitive to these choices. Figure 2 shows the results when<br>comparing the lower bounds. Interestingly, superfluous latent variables did not result in overfitting,<br>which is explained by the regularizing nature of the variational bound.</p>",
            "id": 77,
            "page": 7,
            "text": "Likelihood lower bound We trained generative models (decoders) and corresponding encoders (a.k.a. recognition models) having 500 hidden units in case of MNIST, and 200 hidden units in case of the Frey Face dataset (to prevent overfitting, since itis a considerably smaller dataset). The chosen number of hidden units is based on prior literature on auto-encoders, and the relative performance of different algorithms was not very sensitive to these choices. Figure 2 shows the results when comparing the lower bounds. Interestingly, superfluous latent variables did not result in overfitting, which is explained by the regularizing nature of the variational bound."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2681
                },
                {
                    "x": 2108,
                    "y": 2681
                },
                {
                    "x": 2108,
                    "y": 3056
                },
                {
                    "x": 442,
                    "y": 3056
                }
            ],
            "category": "paragraph",
            "html": "<p id='78' style='font-size:18px'>Marginal likelihood For very low-dimensional latent space it is possible to estimate the marginal<br>likelihood of the learned generative models using an MCMC estimator. More information about the<br>marginal likelihood estimator is available in the appendix. For the encoder and decoder we again<br>used neural networks, this time with 100 hidden units, and 3 latent variables; for higher dimensional<br>latent space the estimates became unreliable. Again, the MNIST dataset was used. The AEVB<br>and Wake-Sleep methods were compared to Monte Carlo EM (MCEM) with a Hybrid Monte Carlo<br>(HMC) [DKPR87] sampler; details are in the appendix. We compared the convergence speed for<br>the three algorithms, for a small and large training set size. Results are in figure 3.</p>",
            "id": 78,
            "page": 7,
            "text": "Marginal likelihood For very low-dimensional latent space it is possible to estimate the marginal likelihood of the learned generative models using an MCMC estimator. More information about the marginal likelihood estimator is available in the appendix. For the encoder and decoder we again used neural networks, this time with 100 hidden units, and 3 latent variables; for higher dimensional latent space the estimates became unreliable. Again, the MNIST dataset was used. The AEVB and Wake-Sleep methods were compared to Monte Carlo EM (MCEM) with a Hybrid Monte Carlo (HMC) [DKPR87] sampler; details are in the appendix. We compared the convergence speed for the three algorithms, for a small and large training set size. Results are in figure 3."
        },
        {
            "bounding_box": [
                {
                    "x": 1261,
                    "y": 3135
                },
                {
                    "x": 1288,
                    "y": 3135
                },
                {
                    "x": 1288,
                    "y": 3170
                },
                {
                    "x": 1261,
                    "y": 3170
                }
            ],
            "category": "footer",
            "html": "<footer id='79' style='font-size:18px'>7</footer>",
            "id": 79,
            "page": 7,
            "text": "7"
        },
        {
            "bounding_box": [
                {
                    "x": 451,
                    "y": 336
                },
                {
                    "x": 2098,
                    "y": 336
                },
                {
                    "x": 2098,
                    "y": 966
                },
                {
                    "x": 451,
                    "y": 966
                }
            ],
            "category": "figure",
            "html": "<figure><img id='80' style='font-size:14px' alt=\"N = 1000 Ntrain\n= 50000\ntrain\n-100 -125\n-130\n-110\nlog-likelihood\nWake-Sleep (train)\n-135\n-120 Wake-Sleep (test)\n-140 MCEM (train)\n-130\nMCEM (test)\n-145\nMarginal\nAEVB (train)\n-140\n-150 AEVB (test)\n-150\n-155\n-160 -160\n0 10 20 30 40 50 60 0 10 20 30 40 50 60\n# Training samples evaluated (millions)\" data-coord=\"top-left:(451,336); bottom-right:(2098,966)\" /></figure>",
            "id": 80,
            "page": 8,
            "text": "N = 1000 Ntrain = 50000 train -100 -125 -130 -110 log-likelihood Wake-Sleep (train) -135 -120 Wake-Sleep (test) -140 MCEM (train) -130 MCEM (test) -145 Marginal AEVB (train) -140 -150 AEVB (test) -150 -155 -160 -160 0 10 20 30 40 50 60 0 10 20 30 40 50 60 # Training samples evaluated (millions)"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 1008
                },
                {
                    "x": 2109,
                    "y": 1008
                },
                {
                    "x": 2109,
                    "y": 1195
                },
                {
                    "x": 440,
                    "y": 1195
                }
            ],
            "category": "caption",
            "html": "<caption id='81' style='font-size:16px'>Figure 3: Comparison of AEVB to the wake-sleep algorithm and Monte Carlo EM, in terms of the<br>estimated marginal likelihood, for a different number of training points. Monte Carlo EM is not an<br>on-line algorithm, and (unlike AEVB and the wake-sleep method) can't be applied efficiently for<br>the full MNIST dataset.</caption>",
            "id": 81,
            "page": 8,
            "text": "Figure 3: Comparison of AEVB to the wake-sleep algorithm and Monte Carlo EM, in terms of the estimated marginal likelihood, for a different number of training points. Monte Carlo EM is not an on-line algorithm, and (unlike AEVB and the wake-sleep method) can't be applied efficiently for the full MNIST dataset."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1281
                },
                {
                    "x": 2107,
                    "y": 1281
                },
                {
                    "x": 2107,
                    "y": 1470
                },
                {
                    "x": 441,
                    "y": 1470
                }
            ],
            "category": "paragraph",
            "html": "<p id='82' style='font-size:16px'>Visualisation of high-dimensional data If we choose a low-dimensional latent space (e.g. 2D),<br>we can use the learned encoders (recognition model) to project high-dimensional data to a low-<br>dimensional manifold. See appendix A for visualisations of the 2D latent manifolds for the MNIST<br>and Frey Face datasets.</p>",
            "id": 82,
            "page": 8,
            "text": "Visualisation of high-dimensional data If we choose a low-dimensional latent space (e.g. 2D), we can use the learned encoders (recognition model) to project high-dimensional data to a lowdimensional manifold. See appendix A for visualisations of the 2D latent manifolds for the MNIST and Frey Face datasets."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1535
                },
                {
                    "x": 769,
                    "y": 1535
                },
                {
                    "x": 769,
                    "y": 1590
                },
                {
                    "x": 443,
                    "y": 1590
                }
            ],
            "category": "paragraph",
            "html": "<p id='83' style='font-size:22px'>6 Conclusion</p>",
            "id": 83,
            "page": 8,
            "text": "6 Conclusion"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1637
                },
                {
                    "x": 2109,
                    "y": 1637
                },
                {
                    "x": 2109,
                    "y": 1964
                },
                {
                    "x": 441,
                    "y": 1964
                }
            ],
            "category": "paragraph",
            "html": "<p id='84' style='font-size:16px'>We have introduced a novel estimator of the variational lower bound, Stochastic Gradient VB<br>(SGVB), for efficient approximate inference with continuous latent variables. The proposed estima-<br>tor can be straightforwardly differentiated and optimized using standard stochastic gradient meth-<br>ods. For the case of i.i.d. datasets and continuous latent variables per datapoint we introduce an<br>efficient algorithm for efficient inference and learning, Auto-Encoding VB (AEVB), that learns an<br>approximate inference model using the SGVB estimator. The theoretical advantages are reflected in<br>experimental results.</p>",
            "id": 84,
            "page": 8,
            "text": "We have introduced a novel estimator of the variational lower bound, Stochastic Gradient VB (SGVB), for efficient approximate inference with continuous latent variables. The proposed estimator can be straightforwardly differentiated and optimized using standard stochastic gradient methods. For the case of i.i.d. datasets and continuous latent variables per datapoint we introduce an efficient algorithm for efficient inference and learning, Auto-Encoding VB (AEVB), that learns an approximate inference model using the SGVB estimator. The theoretical advantages are reflected in experimental results."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2026
                },
                {
                    "x": 798,
                    "y": 2026
                },
                {
                    "x": 798,
                    "y": 2082
                },
                {
                    "x": 444,
                    "y": 2082
                }
            ],
            "category": "paragraph",
            "html": "<p id='85' style='font-size:20px'>7 Future work</p>",
            "id": 85,
            "page": 8,
            "text": "7 Future work"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2131
                },
                {
                    "x": 2109,
                    "y": 2131
                },
                {
                    "x": 2109,
                    "y": 2411
                },
                {
                    "x": 442,
                    "y": 2411
                }
            ],
            "category": "paragraph",
            "html": "<p id='86' style='font-size:18px'>Since the SGVB estimator and the AEVB algorithm can be applied to almost any inference and<br>learning problem with continuous latent variables, there are plenty of future directions: (i) learning<br>hierarchical generative architectures with deep neural networks (e.g. convolutional networks) used<br>for the encoders and decoders, trained jointly with AEVB; (ii) time-series models (i.e. dynamic<br>Bayesian networks); (iii) application of SGVB to the global parameters; (iv) supervised models<br>with latent variables, useful for learning complicated noise distributions.</p>",
            "id": 86,
            "page": 8,
            "text": "Since the SGVB estimator and the AEVB algorithm can be applied to almost any inference and learning problem with continuous latent variables, there are plenty of future directions: (i) learning hierarchical generative architectures with deep neural networks (e.g. convolutional networks) used for the encoders and decoders, trained jointly with AEVB; (ii) time-series models (i.e. dynamic Bayesian networks); (iii) application of SGVB to the global parameters; (iv) supervised models with latent variables, useful for learning complicated noise distributions."
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3133
                },
                {
                    "x": 1289,
                    "y": 3133
                },
                {
                    "x": 1289,
                    "y": 3170
                },
                {
                    "x": 1260,
                    "y": 3170
                }
            ],
            "category": "footer",
            "html": "<footer id='87' style='font-size:16px'>8</footer>",
            "id": 87,
            "page": 8,
            "text": "8"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 340
                },
                {
                    "x": 688,
                    "y": 340
                },
                {
                    "x": 688,
                    "y": 393
                },
                {
                    "x": 443,
                    "y": 393
                }
            ],
            "category": "paragraph",
            "html": "<p id='88' style='font-size:18px'>References</p>",
            "id": 88,
            "page": 9,
            "text": "References"
        },
        {
            "bounding_box": [
                {
                    "x": 437,
                    "y": 420
                },
                {
                    "x": 2118,
                    "y": 420
                },
                {
                    "x": 2118,
                    "y": 2763
                },
                {
                    "x": 437,
                    "y": 2763
                }
            ],
            "category": "table",
            "html": "<table id='89' style='font-size:16px'><tr><td>[BCV13]</td><td>Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A re- view and new perspectives. 2013.</td></tr><tr><td>[BJP12]</td><td>David M Blei, Michael I Jordan, and John W Paisley. Variational Bayesian inference with Stochastic Search. In Proceedings of the 29th International Conference on Ma- chine Learning (ICML-12), pages 1367-1374, 2012.</td></tr><tr><td>[BTL13]</td><td>Yoshua Bengio and Eric Thibodeau-Laufer. Deep generative stochastic networks train- able by backprop. arXiv preprint arXiv:1306.1091, 2013.</td></tr><tr><td>[Dev86]</td><td>Luc Devroye. Sample-based non-uniform random variate generation. In Proceedings of the 18th conference on Winter simulation, pages 260-265. ACM, 1986.</td></tr><tr><td>[DHS10]</td><td>John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:2121- 2159, 2010.</td></tr><tr><td>[DKPR87]</td><td>Simon Duane, Anthony D Kennedy, Brian J Pendleton, and Duncan Roweth. Hybrid monte carlo. Physics letters B, 195(2):216-222, 1987.</td></tr><tr><td>[GMW13]</td><td>Karol Gregor, Andriy Mnih, and Daan Wierstra. Deep autoregressive networks. arXiv preprint arXiv:1310.8499, 2013.</td></tr><tr><td>[HBWP13]</td><td>Matthew D Hoffman, David M Blei, Chong Wang, and John Paisley. Stochastic varia- tional inference. The Journal of Machine Learning Research, 14(1):1303-1347, 2013.</td></tr><tr><td>[HDFN95]</td><td>Geoffrey E Hinton, Peter Dayan, Brendan J Frey, and Radford M Neal. The\" wake- sleep\" algorithm for unsupervised neural networks. SCIENCE, pages 1158-1158, 1995.</td></tr><tr><td>[KRL08]</td><td>Koray Kavukcuoglu, Marc' Aurelio Ranzato, and Yann LeCun. Fast inference in sparse coding algorithms with applications to object recognition. Technical Report CBLL- TR-2008-12-01, Computational and Biological Learning Lab, Courant Institute, NYU, 2008.</td></tr><tr><td>[Lin89]</td><td>Ralph Linsker. An application of the principle of maximum information preservation to linear systems. Morgan Kaufmann Publishers Inc., 1989.</td></tr><tr><td>[RGB13]</td><td>Rajesh Ranganath, Sean Gerrish, and David M Blei. Black Box Variational Inference. arXiv preprint arXiv:1401.0118, 2013.</td></tr><tr><td>[RMW14]</td><td>Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic back- propagation and variational inference in deep latent gaussian models. arXiv preprint arXiv:1401.4082, 2014.</td></tr><tr><td>[Row98]</td><td>Sam Roweis. EM algorithms for PCA and SPCA. Advances in neural information processing systems, pages 626-632, 1998.</td></tr><tr><td>[SK13]</td><td>Tim Salimans and David A Knowles. Fixed-form variational posterior approximation through stochastic linear regression. Bayesian Analysis, 8(4), 2013.</td></tr><tr><td>[SL10]</td><td>Ruslan Salakhutdinov and Hugo Larochelle. Efficient learning of deep boltzmann ma- chines. In International Conference on Artificial Intelligence and Statistics, pages 693- 700, 2010.</td></tr><tr><td>[VLL+10]</td><td>Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. The Journal of Machine Learning Research, 9999:3371-3408, 2010.</td></tr></table>",
            "id": 89,
            "page": 9,
            "text": "[BCV13] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A re- view and new perspectives. 2013.  [BJP12] David M Blei, Michael I Jordan, and John W Paisley. Variational Bayesian inference with Stochastic Search. In Proceedings of the 29th International Conference on Ma- chine Learning (ICML-12), pages 1367-1374, 2012.  [BTL13] Yoshua Bengio and Eric Thibodeau-Laufer. Deep generative stochastic networks train- able by backprop. arXiv preprint arXiv:1306.1091, 2013.  [Dev86] Luc Devroye. Sample-based non-uniform random variate generation. In Proceedings of the 18th conference on Winter simulation, pages 260-265. ACM, 1986.  [DHS10] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12:2121- 2159, 2010.  [DKPR87] Simon Duane, Anthony D Kennedy, Brian J Pendleton, and Duncan Roweth. Hybrid monte carlo. Physics letters B, 195(2):216-222, 1987.  [GMW13] Karol Gregor, Andriy Mnih, and Daan Wierstra. Deep autoregressive networks. arXiv preprint arXiv:1310.8499, 2013.  [HBWP13] Matthew D Hoffman, David M Blei, Chong Wang, and John Paisley. Stochastic varia- tional inference. The Journal of Machine Learning Research, 14(1):1303-1347, 2013.  [HDFN95] Geoffrey E Hinton, Peter Dayan, Brendan J Frey, and Radford M Neal. The\" wake- sleep\" algorithm for unsupervised neural networks. SCIENCE, pages 1158-1158, 1995.  [KRL08] Koray Kavukcuoglu, Marc' Aurelio Ranzato, and Yann LeCun. Fast inference in sparse coding algorithms with applications to object recognition. Technical Report CBLL- TR-2008-12-01, Computational and Biological Learning Lab, Courant Institute, NYU, 2008.  [Lin89] Ralph Linsker. An application of the principle of maximum information preservation to linear systems. Morgan Kaufmann Publishers Inc., 1989.  [RGB13] Rajesh Ranganath, Sean Gerrish, and David M Blei. Black Box Variational Inference. arXiv preprint arXiv:1401.0118, 2013.  [RMW14] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic back- propagation and variational inference in deep latent gaussian models. arXiv preprint arXiv:1401.4082, 2014.  [Row98] Sam Roweis. EM algorithms for PCA and SPCA. Advances in neural information processing systems, pages 626-632, 1998.  [SK13] Tim Salimans and David A Knowles. Fixed-form variational posterior approximation through stochastic linear regression. Bayesian Analysis, 8(4), 2013.  [SL10] Ruslan Salakhutdinov and Hugo Larochelle. Efficient learning of deep boltzmann ma- chines. In International Conference on Artificial Intelligence and Statistics, pages 693- 700, 2010.  [VLL+10]"
        },
        {
            "bounding_box": [
                {
                    "x": 446,
                    "y": 2851
                },
                {
                    "x": 837,
                    "y": 2851
                },
                {
                    "x": 837,
                    "y": 2905
                },
                {
                    "x": 446,
                    "y": 2905
                }
            ],
            "category": "paragraph",
            "html": "<p id='90' style='font-size:20px'>A Visualisations</p>",
            "id": 90,
            "page": 9,
            "text": "A Visualisations"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2958
                },
                {
                    "x": 2107,
                    "y": 2958
                },
                {
                    "x": 2107,
                    "y": 3052
                },
                {
                    "x": 443,
                    "y": 3052
                }
            ],
            "category": "paragraph",
            "html": "<p id='91' style='font-size:14px'>See figures 4 and 5 for visualisations of latent space and corresponding observed space of models<br>learned with SGVB.</p>",
            "id": 91,
            "page": 9,
            "text": "See figures 4 and 5 for visualisations of latent space and corresponding observed space of models learned with SGVB."
        },
        {
            "bounding_box": [
                {
                    "x": 1261,
                    "y": 3134
                },
                {
                    "x": 1288,
                    "y": 3134
                },
                {
                    "x": 1288,
                    "y": 3169
                },
                {
                    "x": 1261,
                    "y": 3169
                }
            ],
            "category": "footer",
            "html": "<footer id='92' style='font-size:14px'>9</footer>",
            "id": 92,
            "page": 9,
            "text": "9"
        },
        {
            "bounding_box": [
                {
                    "x": 537,
                    "y": 345
                },
                {
                    "x": 2032,
                    "y": 345
                },
                {
                    "x": 2032,
                    "y": 1301
                },
                {
                    "x": 537,
                    "y": 1301
                }
            ],
            "category": "figure",
            "html": "<figure><img id='93' style='font-size:14px' alt=\"3\" data-coord=\"top-left:(537,345); bottom-right:(2032,1301)\" /></figure>",
            "id": 93,
            "page": 10,
            "text": "3"
        },
        {
            "bounding_box": [
                {
                    "x": 550,
                    "y": 1298
                },
                {
                    "x": 1038,
                    "y": 1298
                },
                {
                    "x": 1038,
                    "y": 1342
                },
                {
                    "x": 550,
                    "y": 1342
                }
            ],
            "category": "caption",
            "html": "<br><caption id='94' style='font-size:18px'>(a) Learned Frey Face manifold</caption>",
            "id": 94,
            "page": 10,
            "text": "(a) Learned Frey Face manifold"
        },
        {
            "bounding_box": [
                {
                    "x": 1329,
                    "y": 1299
                },
                {
                    "x": 1786,
                    "y": 1299
                },
                {
                    "x": 1786,
                    "y": 1342
                },
                {
                    "x": 1329,
                    "y": 1342
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='95' style='font-size:16px'>(b) Learned MNIST manifold</p>",
            "id": 95,
            "page": 10,
            "text": "(b) Learned MNIST manifold"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 1380
                },
                {
                    "x": 2108,
                    "y": 1380
                },
                {
                    "x": 2108,
                    "y": 1613
                },
                {
                    "x": 440,
                    "y": 1613
                }
            ],
            "category": "paragraph",
            "html": "<p id='96' style='font-size:20px'>Figure 4: Visualisations of learned data manifold for generative models with two-dimensional latent<br>space, learned with AEVB. Since the prior of the latent space is Gaussian, linearly spaced coor-<br>dinates on the unit square were transformed through the inverse CDF of the Gaussian to produce<br>values of the latent variables Z. For each of these values Z, we plotted the corresponding generative<br>pe (x|z) with the learned parameters 0.</p>",
            "id": 96,
            "page": 10,
            "text": "Figure 4: Visualisations of learned data manifold for generative models with two-dimensional latent space, learned with AEVB. Since the prior of the latent space is Gaussian, linearly spaced coordinates on the unit square were transformed through the inverse CDF of the Gaussian to produce values of the latent variables Z. For each of these values Z, we plotted the corresponding generative pe (x|z) with the learned parameters 0."
        },
        {
            "bounding_box": [
                {
                    "x": 512,
                    "y": 1692
                },
                {
                    "x": 2058,
                    "y": 1692
                },
                {
                    "x": 2058,
                    "y": 2127
                },
                {
                    "x": 512,
                    "y": 2127
                }
            ],
            "category": "figure",
            "html": "<figure><img id='97' style='font-size:14px' alt=\"1 6 5 7 6 7 6 7 2 2 8 3 8 3 8 5 7 3 8 8 2 8 フ 2 그 9 0\n8 6 8 B 9 6 3 I 9 8 5 9 4 6 3 2 1 6 2 8 3 8 2 7 9 ? 5 3 8 7 5 イ 9 9 / 7 , 9 4\n8 ] 9 / 6 9 6 9 ② 3 2 8 8 ! 3 3 8 5 8 9 ↙ 0 8 5 / 6 8 9 6 2 8 8 8 8 2 9\n8 9 9 / 9 6 3 2 8 6 8 9 1 O 0 4 1 1 9 2 8 3 3 3 V 9 2 2 8 8 G 3 8 7 0 6 /\n8 2. 3 / 3 8 6 5 / 9 2 ○ 1 5 3 5 9 � 7 3 6 4 3 0 2 0 3 ら 4 7 ? 8 9 0 9 [ 0\nI 6 6 8 6 5 6 ) 4 9 1 7 5 8 5 9 7 ● る 8 3 3 0 5 6 8 8 C 0 � 8 2 8 /\n5 1 8 9 9 ( 3 4 3 9 8 3 7 7 ○ 6 9 4 3 6 2 8 5 , 2 2 5 9 。 5 6 1 3 4 3\n9 9 1 3 1 2 8 2 3 4 5 8 2 9 7 0 7 5 0 8 4 9 0 S 0 7 0 6 6 9 9 3 9 2 7 9 3 - 0\n4 6 1 2 3 2 0 8 8 6 1 9 4 8 7 2 8 タ ら 7 4 ク 6 3 0 3 ! 0 1 4 5 2 5 ろ 8 0 ィ 8 8\n9 9 5 4 9 3 9 8 3 I ? b 4 5 6 0 9 9 9 8 2 , 8 0 4 9 1 0 5 ○ 8 8 7 2 る 1 G 그 3 2\n(a) 2-D latent space (b) 5-D latent space (c) 10-D latent space (d) 20-D latent space\" data-coord=\"top-left:(512,1692); bottom-right:(2058,2127)\" /></figure>",
            "id": 97,
            "page": 10,
            "text": "1 6 5 7 6 7 6 7 2 2 8 3 8 3 8 5 7 3 8 8 2 8 フ 2 그 9 0 8 6 8 B 9 6 3 I 9 8 5 9 4 6 3 2 1 6 2 8 3 8 2 7 9 ? 5 3 8 7 5 イ 9 9 / 7 , 9 4 8 ] 9 / 6 9 6 9 ② 3 2 8 8 ! 3 3 8 5 8 9 ↙ 0 8 5 / 6 8 9 6 2 8 8 8 8 2 9 8 9 9 / 9 6 3 2 8 6 8 9 1 O 0 4 1 1 9 2 8 3 3 3 V 9 2 2 8 8 G 3 8 7 0 6 / 8 2. 3 / 3 8 6 5 / 9 2 ○ 1 5 3 5 9 � 7 3 6 4 3 0 2 0 3 ら 4 7 ? 8 9 0 9 [ 0 I 6 6 8 6 5 6 ) 4 9 1 7 5 8 5 9 7 ● る 8 3 3 0 5 6 8 8 C 0 � 8 2 8 / 5 1 8 9 9 ( 3 4 3 9 8 3 7 7 ○ 6 9 4 3 6 2 8 5 , 2 2 5 9 。 5 6 1 3 4 3 9 9 1 3 1 2 8 2 3 4 5 8 2 9 7 0 7 5 0 8 4 9 0 S 0 7 0 6 6 9 9 3 9 2 7 9 3 - 0 4 6 1 2 3 2 0 8 8 6 1 9 4 8 7 2 8 タ ら 7 4 ク 6 3 0 3 ! 0 1 4 5 2 5 ろ 8 0 ィ 8 8 9 9 5 4 9 3 9 8 3 I ? b 4 5 6 0 9 9 9 8 2 , 8 0 4 9 1 0 5 ○ 8 8 7 2 る 1 G 그 3 2 (a) 2-D latent space (b) 5-D latent space (c) 10-D latent space (d) 20-D latent space"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 2171
                },
                {
                    "x": 2105,
                    "y": 2171
                },
                {
                    "x": 2105,
                    "y": 2268
                },
                {
                    "x": 440,
                    "y": 2268
                }
            ],
            "category": "paragraph",
            "html": "<p id='98' style='font-size:20px'>Figure 5: Random samples from learned generative models of MNIST for different dimensionalities<br>of latent space.</p>",
            "id": 98,
            "page": 10,
            "text": "Figure 5: Random samples from learned generative models of MNIST for different dimensionalities of latent space."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2376
                },
                {
                    "x": 1531,
                    "y": 2376
                },
                {
                    "x": 1531,
                    "y": 2438
                },
                {
                    "x": 444,
                    "y": 2438
                }
            ],
            "category": "paragraph",
            "html": "<p id='99' style='font-size:22px'>B Solution of -D KL (q⌀ (z)||pe(z)), Gaussian case</p>",
            "id": 99,
            "page": 10,
            "text": "B Solution of -D KL (q⌀ (z)||pe(z)), Gaussian case"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 2499
                },
                {
                    "x": 2108,
                    "y": 2499
                },
                {
                    "x": 2108,
                    "y": 2742
                },
                {
                    "x": 440,
                    "y": 2742
                }
            ],
            "category": "paragraph",
            "html": "<p id='100' style='font-size:20px'>The variational lower bound (the objective to be maximized) contains a KL term that can often be<br>integrated analytically. Here we give the solution when both the prior pe(z) = N(0,I) and the<br>posterior approximation 1⌀ (z|x(i)) are Gaussian. Let J be the dimensionality of Z. Let H and �<br>denote the variational mean and s.d. evaluated at datapoint i, and let Mj and �j simply denote the<br>j-th element of these vectors. Then:</p>",
            "id": 100,
            "page": 10,
            "text": "The variational lower bound (the objective to be maximized) contains a KL term that can often be integrated analytically. Here we give the solution when both the prior pe(z) = N(0,I) and the posterior approximation 1⌀ (z|x(i)) are Gaussian. Let J be the dimensionality of Z. Let H and � denote the variational mean and s.d. evaluated at datapoint i, and let Mj and �j simply denote the j-th element of these vectors. Then:"
        },
        {
            "bounding_box": [
                {
                    "x": 1253,
                    "y": 3133
                },
                {
                    "x": 1300,
                    "y": 3133
                },
                {
                    "x": 1300,
                    "y": 3172
                },
                {
                    "x": 1253,
                    "y": 3172
                }
            ],
            "category": "footer",
            "html": "<footer id='101' style='font-size:18px'>10</footer>",
            "id": 101,
            "page": 10,
            "text": "10"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 348
                },
                {
                    "x": 538,
                    "y": 348
                },
                {
                    "x": 538,
                    "y": 392
                },
                {
                    "x": 443,
                    "y": 392
                }
            ],
            "category": "paragraph",
            "html": "<p id='102' style='font-size:16px'>And:</p>",
            "id": 102,
            "page": 11,
            "text": "And:"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 667
                },
                {
                    "x": 627,
                    "y": 667
                },
                {
                    "x": 627,
                    "y": 711
                },
                {
                    "x": 443,
                    "y": 711
                }
            ],
            "category": "paragraph",
            "html": "<p id='103' style='font-size:16px'>Therefore:</p>",
            "id": 103,
            "page": 11,
            "text": "Therefore:"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 988
                },
                {
                    "x": 2108,
                    "y": 988
                },
                {
                    "x": 2108,
                    "y": 1082
                },
                {
                    "x": 442,
                    "y": 1082
                }
            ],
            "category": "paragraph",
            "html": "<p id='104' style='font-size:16px'>When using a recognition model 9⌀ (z|x) then H and s.d. � are simply functions of x and the<br>variational parameters 中, as exemplified in the text.</p>",
            "id": 104,
            "page": 11,
            "text": "When using a recognition model 9⌀ (z|x) then H and s.d. � are simply functions of x and the variational parameters 中, as exemplified in the text."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1145
                },
                {
                    "x": 1517,
                    "y": 1145
                },
                {
                    "x": 1517,
                    "y": 1201
                },
                {
                    "x": 442,
                    "y": 1201
                }
            ],
            "category": "paragraph",
            "html": "<p id='105' style='font-size:20px'>C MLP's as probabilistic encoders and decoders</p>",
            "id": 105,
            "page": 11,
            "text": "C MLP's as probabilistic encoders and decoders"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1247
                },
                {
                    "x": 2108,
                    "y": 1247
                },
                {
                    "x": 2108,
                    "y": 1481
                },
                {
                    "x": 441,
                    "y": 1481
                }
            ],
            "category": "paragraph",
            "html": "<p id='106' style='font-size:14px'>In variational auto-encoders, neural networks are used as probabilistic encoders and decoders. There<br>are many possible choices of encoders and decoders, depending on the type of data and model. In<br>our example we used relatively simple neural networks, namely multi-layered perceptrons (MLPs).<br>For the encoder we used a MLP with Gaussian output, while for the decoder we used MLPs with<br>either Gaussian or Bernoulli outputs, depending on the type of data.</p>",
            "id": 106,
            "page": 11,
            "text": "In variational auto-encoders, neural networks are used as probabilistic encoders and decoders. There are many possible choices of encoders and decoders, depending on the type of data and model. In our example we used relatively simple neural networks, namely multi-layered perceptrons (MLPs). For the encoder we used a MLP with Gaussian output, while for the decoder we used MLPs with either Gaussian or Bernoulli outputs, depending on the type of data."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1531
                },
                {
                    "x": 1022,
                    "y": 1531
                },
                {
                    "x": 1022,
                    "y": 1581
                },
                {
                    "x": 443,
                    "y": 1581
                }
            ],
            "category": "paragraph",
            "html": "<p id='107' style='font-size:14px'>C.1 Bernoulli MLP as decoder</p>",
            "id": 107,
            "page": 11,
            "text": "C.1 Bernoulli MLP as decoder"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1619
                },
                {
                    "x": 2104,
                    "y": 1619
                },
                {
                    "x": 2104,
                    "y": 1711
                },
                {
                    "x": 442,
                    "y": 1711
                }
            ],
            "category": "paragraph",
            "html": "<p id='108' style='font-size:16px'>In this case let po (x|z) be a multivariate Bernoulli whose probabilities are computed from Z with a<br>fully-connected neural network with a single hidden layer:</p>",
            "id": 108,
            "page": 11,
            "text": "In this case let po (x|z) be a multivariate Bernoulli whose probabilities are computed from Z with a fully-connected neural network with a single hidden layer:"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 1940
                },
                {
                    "x": 2105,
                    "y": 1940
                },
                {
                    "x": 2105,
                    "y": 2034
                },
                {
                    "x": 440,
                    "y": 2034
                }
            ],
            "category": "paragraph",
            "html": "<p id='109' style='font-size:18px'>where fo(.) is the elementwise sigmoid activation function, and where 0 = {W1, W2, b1, b2} are<br>the weights and biases of the MLP.</p>",
            "id": 109,
            "page": 11,
            "text": "where fo(.) is the elementwise sigmoid activation function, and where 0 = {W1, W2, b1, b2} are the weights and biases of the MLP."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2087
                },
                {
                    "x": 1220,
                    "y": 2087
                },
                {
                    "x": 1220,
                    "y": 2134
                },
                {
                    "x": 444,
                    "y": 2134
                }
            ],
            "category": "paragraph",
            "html": "<p id='110' style='font-size:14px'>C.2 Gaussian MLP as encoder or decoder</p>",
            "id": 110,
            "page": 11,
            "text": "C.2 Gaussian MLP as encoder or decoder"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2173
                },
                {
                    "x": 2092,
                    "y": 2173
                },
                {
                    "x": 2092,
                    "y": 2221
                },
                {
                    "x": 442,
                    "y": 2221
                }
            ],
            "category": "paragraph",
            "html": "<p id='111' style='font-size:14px'>In this case let encoder or decoder be a multivariate Gaussian with a diagonal covariance structure:</p>",
            "id": 111,
            "page": 11,
            "text": "In this case let encoder or decoder be a multivariate Gaussian with a diagonal covariance structure:"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2485
                },
                {
                    "x": 2105,
                    "y": 2485
                },
                {
                    "x": 2105,
                    "y": 2624
                },
                {
                    "x": 441,
                    "y": 2624
                }
            ],
            "category": "paragraph",
            "html": "<p id='112' style='font-size:16px'>where {W3, W 4, W5, b3, b4, b5} are the weights and biases of the MLP and part of 0 when used<br>as decoder. Note that when this network is used as an encoder 9⌀ (z|x), then Z and x are swapped,<br>and the weights and biases are variational parameters ⌀.</p>",
            "id": 112,
            "page": 11,
            "text": "where {W3, W 4, W5, b3, b4, b5} are the weights and biases of the MLP and part of 0 when used as decoder. Note that when this network is used as an encoder 9⌀ (z|x), then Z and x are swapped, and the weights and biases are variational parameters ⌀."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 2688
                },
                {
                    "x": 1173,
                    "y": 2688
                },
                {
                    "x": 1173,
                    "y": 2745
                },
                {
                    "x": 445,
                    "y": 2745
                }
            ],
            "category": "paragraph",
            "html": "<p id='113' style='font-size:20px'>D Marginal likelihood estimator</p>",
            "id": 113,
            "page": 11,
            "text": "D Marginal likelihood estimator"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2790
                },
                {
                    "x": 2107,
                    "y": 2790
                },
                {
                    "x": 2107,
                    "y": 2987
                },
                {
                    "x": 442,
                    "y": 2987
                }
            ],
            "category": "paragraph",
            "html": "<p id='114' style='font-size:16px'>We derived the following marginal likelihood estimator that produces good estimates of the marginal<br>likelihood as long as the dimensionality of the sampled space is low (less then 5 dimensions), and<br>sufficient samples are taken. Let pe(x, z) = pe(z)pe(x|z) be the generative model we are sampling<br>from, and for a given datapoint x(i) we would like to estimate the marginal likelihood po (x(i)).</p>",
            "id": 114,
            "page": 11,
            "text": "We derived the following marginal likelihood estimator that produces good estimates of the marginal likelihood as long as the dimensionality of the sampled space is low (less then 5 dimensions), and sufficient samples are taken. Let pe(x, z) = pe(z)pe(x|z) be the generative model we are sampling from, and for a given datapoint x(i) we would like to estimate the marginal likelihood po (x(i))."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 3006
                },
                {
                    "x": 1243,
                    "y": 3006
                },
                {
                    "x": 1243,
                    "y": 3053
                },
                {
                    "x": 443,
                    "y": 3053
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='115' style='font-size:14px'>The estimation process consists of three stages:</p>",
            "id": 115,
            "page": 11,
            "text": "The estimation process consists of three stages:"
        },
        {
            "bounding_box": [
                {
                    "x": 1253,
                    "y": 3134
                },
                {
                    "x": 1297,
                    "y": 3134
                },
                {
                    "x": 1297,
                    "y": 3171
                },
                {
                    "x": 1253,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='116' style='font-size:14px'>11</footer>",
            "id": 116,
            "page": 11,
            "text": "11"
        },
        {
            "bounding_box": [
                {
                    "x": 533,
                    "y": 340
                },
                {
                    "x": 2112,
                    "y": 340
                },
                {
                    "x": 2112,
                    "y": 618
                },
                {
                    "x": 533,
                    "y": 618
                }
            ],
            "category": "paragraph",
            "html": "<p id='117' style='font-size:18px'>1. Sample L values {z(l)} from the posterior using gradient-based MCMC, e.g. Hybrid Monte<br>Carlo, using ▽ z logpe(z|x) = Vz logpe(z) + Vz logpe(x|z).<br>2. Fit a density estimator q(z) to these samples {z(1)}.<br>3. Again, sample L new values from the posterior. Plug these samples, as well as the fitted<br>q(z), into the following estimator:</p>",
            "id": 117,
            "page": 12,
            "text": "1. Sample L values {z(l)} from the posterior using gradient-based MCMC, e.g. Hybrid Monte Carlo, using ▽ z logpe(z|x) = Vz logpe(z) + Vz logpe(x|z). 2. Fit a density estimator q(z) to these samples {z(1)}. 3. Again, sample L new values from the posterior. Plug these samples, as well as the fitted q(z), into the following estimator:"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 824
                },
                {
                    "x": 913,
                    "y": 824
                },
                {
                    "x": 913,
                    "y": 869
                },
                {
                    "x": 443,
                    "y": 869
                }
            ],
            "category": "caption",
            "html": "<caption id='118' style='font-size:14px'>Derivation of the estimator:</caption>",
            "id": 118,
            "page": 12,
            "text": "Derivation of the estimator:"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 1460
                },
                {
                    "x": 902,
                    "y": 1460
                },
                {
                    "x": 902,
                    "y": 1515
                },
                {
                    "x": 444,
                    "y": 1515
                }
            ],
            "category": "paragraph",
            "html": "<p id='119' style='font-size:18px'>E Monte Carlo EM</p>",
            "id": 119,
            "page": 12,
            "text": "E Monte Carlo EM"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1565
                },
                {
                    "x": 2107,
                    "y": 1565
                },
                {
                    "x": 2107,
                    "y": 1841
                },
                {
                    "x": 441,
                    "y": 1841
                }
            ],
            "category": "paragraph",
            "html": "<p id='120' style='font-size:16px'>The Monte Carlo EM algorithm does not employ an encoder, instead it samples from the pos-<br>terior of the latent variables using gradients of the posterior computed with Vz log pe(z|x) =<br>Vz logpe(z) + Vz log pe(x|z). The Monte Carlo EM procedure consists of 10 HMC leapfrog<br>steps with an automatically tuned stepsize such that the acceptance rate was 90%, followed by 5<br>weight updates steps using the acquired sample. For all algorithms the parameters were updated<br>using the Adagrad stepsizes (with accompanying annealing schedule).</p>",
            "id": 120,
            "page": 12,
            "text": "The Monte Carlo EM algorithm does not employ an encoder, instead it samples from the posterior of the latent variables using gradients of the posterior computed with Vz log pe(z|x) = Vz logpe(z) + Vz log pe(x|z). The Monte Carlo EM procedure consists of 10 HMC leapfrog steps with an automatically tuned stepsize such that the acceptance rate was 90%, followed by 5 weight updates steps using the acquired sample. For all algorithms the parameters were updated using the Adagrad stepsizes (with accompanying annealing schedule)."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1865
                },
                {
                    "x": 2106,
                    "y": 1865
                },
                {
                    "x": 2106,
                    "y": 2003
                },
                {
                    "x": 442,
                    "y": 2003
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='121' style='font-size:16px'>The marginal likelihood was estimated with the first 1000 datapoints from the train and test sets,<br>for each datapoint sampling 50 values from the posterior of the latent variables using Hybrid Monte<br>Carlo with 4 leapfrog steps.</p>",
            "id": 121,
            "page": 12,
            "text": "The marginal likelihood was estimated with the first 1000 datapoints from the train and test sets, for each datapoint sampling 50 values from the posterior of the latent variables using Hybrid Monte Carlo with 4 leapfrog steps."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2070
                },
                {
                    "x": 701,
                    "y": 2070
                },
                {
                    "x": 701,
                    "y": 2122
                },
                {
                    "x": 443,
                    "y": 2122
                }
            ],
            "category": "paragraph",
            "html": "<p id='122' style='font-size:18px'>F Full VB</p>",
            "id": 122,
            "page": 12,
            "text": "F Full VB"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2175
                },
                {
                    "x": 2105,
                    "y": 2175
                },
                {
                    "x": 2105,
                    "y": 2310
                },
                {
                    "x": 442,
                    "y": 2310
                }
            ],
            "category": "paragraph",
            "html": "<p id='123' style='font-size:14px'>As written in the paper, it is possible to perform variational inference on both the parameters 0 and<br>the latent variables Z, as opposed to just the latent variables as we did in the paper. Here, we'll derive<br>our estimator for that case.</p>",
            "id": 123,
            "page": 12,
            "text": "As written in the paper, it is possible to perform variational inference on both the parameters 0 and the latent variables Z, as opposed to just the latent variables as we did in the paper. Here, we'll derive our estimator for that case."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2338
                },
                {
                    "x": 2103,
                    "y": 2338
                },
                {
                    "x": 2103,
                    "y": 2428
                },
                {
                    "x": 442,
                    "y": 2428
                }
            ],
            "category": "paragraph",
            "html": "<p id='124' style='font-size:14px'>Let Pa (日) be some hyperprior for the parameters introduced above, parameterized by a. The<br>marginal likelihood can be written as:</p>",
            "id": 124,
            "page": 12,
            "text": "Let Pa (日) be some hyperprior for the parameters introduced above, parameterized by a. The marginal likelihood can be written as:"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2531
                },
                {
                    "x": 2103,
                    "y": 2531
                },
                {
                    "x": 2103,
                    "y": 2621
                },
                {
                    "x": 444,
                    "y": 2621
                }
            ],
            "category": "paragraph",
            "html": "<p id='125' style='font-size:14px'>where the first RHS term denotes a KL divergence of the approximate from the true posterior, and<br>where L(⌀;X) denotes the variational lower bound to the marginal likelihood:</p>",
            "id": 125,
            "page": 12,
            "text": "where the first RHS term denotes a KL divergence of the approximate from the true posterior, and where L(⌀;X) denotes the variational lower bound to the marginal likelihood:"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2770
                },
                {
                    "x": 2108,
                    "y": 2770
                },
                {
                    "x": 2108,
                    "y": 2965
                },
                {
                    "x": 441,
                    "y": 2965
                }
            ],
            "category": "paragraph",
            "html": "<p id='126' style='font-size:16px'>Note that this is a lower bound since the KL divergence is non-negative; the bound equals the true<br>marginal when the approximate and true posteriors match exactly. The term logpe(X) is composed<br>of a sum over the marginal likelihoods of individual datapoints logpe(X) = �i=1 logpe(x(i)),<br>which can each be rewritten as:</p>",
            "id": 126,
            "page": 12,
            "text": "Note that this is a lower bound since the KL divergence is non-negative; the bound equals the true marginal when the approximate and true posteriors match exactly. The term logpe(X) is composed of a sum over the marginal likelihoods of individual datapoints logpe(X) = �i=1 logpe(x(i)), which can each be rewritten as:"
        },
        {
            "bounding_box": [
                {
                    "x": 1253,
                    "y": 3134
                },
                {
                    "x": 1300,
                    "y": 3134
                },
                {
                    "x": 1300,
                    "y": 3171
                },
                {
                    "x": 1253,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='127' style='font-size:14px'>12</footer>",
            "id": 127,
            "page": 12,
            "text": "12"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 350
                },
                {
                    "x": 2106,
                    "y": 350
                },
                {
                    "x": 2106,
                    "y": 438
                },
                {
                    "x": 443,
                    "y": 438
                }
            ],
            "category": "paragraph",
            "html": "<p id='128' style='font-size:14px'>where again the first RHS term is the KL divergence of the approximate from the true posterior, and<br>L(日, ⌀;x) is the variational lower bound of the marginal likelihood of datapoint i:</p>",
            "id": 128,
            "page": 13,
            "text": "where again the first RHS term is the KL divergence of the approximate from the true posterior, and L(日, ⌀;x) is the variational lower bound of the marginal likelihood of datapoint i:"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 607
                },
                {
                    "x": 2106,
                    "y": 607
                },
                {
                    "x": 2106,
                    "y": 792
                },
                {
                    "x": 442,
                    "y": 792
                }
            ],
            "category": "paragraph",
            "html": "<p id='129' style='font-size:14px'>The expectations on the RHS of eqs (14) and (16) can obviously be written as a sum of three separate<br>expectations, of which the second and third component can sometimes be analytically solved, e.g.<br>when both pe(x) and 9⌀ (zx) are Gaussian. For generality we will here assume that each of these<br>expectations is intractable.</p>",
            "id": 129,
            "page": 13,
            "text": "The expectations on the RHS of eqs (14) and (16) can obviously be written as a sum of three separate expectations, of which the second and third component can sometimes be analytically solved, e.g. when both pe(x) and 9⌀ (zx) are Gaussian. For generality we will here assume that each of these expectations is intractable."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 820
                },
                {
                    "x": 2106,
                    "y": 820
                },
                {
                    "x": 2106,
                    "y": 908
                },
                {
                    "x": 442,
                    "y": 908
                }
            ],
            "category": "paragraph",
            "html": "<p id='130' style='font-size:14px'>Under certain mild conditions outlined in section (see paper) for chosen approximate posteriors<br>9⌀(0) and 9⌀(zx) we can reparameterize conditional samples 2 ~ 9⌀(zx) as</p>",
            "id": 130,
            "page": 13,
            "text": "Under certain mild conditions outlined in section (see paper) for chosen approximate posteriors 9⌀(0) and 9⌀(zx) we can reparameterize conditional samples 2 ~ 9⌀(zx) as"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 1008
                },
                {
                    "x": 1845,
                    "y": 1008
                },
                {
                    "x": 1845,
                    "y": 1055
                },
                {
                    "x": 444,
                    "y": 1055
                }
            ],
            "category": "paragraph",
            "html": "<p id='131' style='font-size:14px'>where we choose a prior p(E) and a function 9⌀(E,X) such that the following holds:</p>",
            "id": 131,
            "page": 13,
            "text": "where we choose a prior p(E) and a function 9⌀(E,X) such that the following holds:"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 1331
                },
                {
                    "x": 1441,
                    "y": 1331
                },
                {
                    "x": 1441,
                    "y": 1378
                },
                {
                    "x": 444,
                    "y": 1378
                }
            ],
            "category": "paragraph",
            "html": "<p id='132' style='font-size:14px'>The same can be done for the approximate posterior 9⌀ (日):</p>",
            "id": 132,
            "page": 13,
            "text": "The same can be done for the approximate posterior 9⌀ (日):"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 1488
                },
                {
                    "x": 2104,
                    "y": 1488
                },
                {
                    "x": 2104,
                    "y": 1575
                },
                {
                    "x": 440,
                    "y": 1575
                }
            ],
            "category": "paragraph",
            "html": "<p id='133' style='font-size:14px'>where we, similarly as above, choose a prior p(5) and a function h⌀(5) such that the following<br>holds:</p>",
            "id": 133,
            "page": 13,
            "text": "where we, similarly as above, choose a prior p(5) and a function h⌀(5) such that the following holds:"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1869
                },
                {
                    "x": 1675,
                    "y": 1869
                },
                {
                    "x": 1675,
                    "y": 1915
                },
                {
                    "x": 443,
                    "y": 1915
                }
            ],
            "category": "paragraph",
            "html": "<p id='134' style='font-size:14px'>For notational conciseness we introduce a shorthand notation f⌀(x,z,0):</p>",
            "id": 134,
            "page": 13,
            "text": "For notational conciseness we introduce a shorthand notation f⌀(x,z,0):"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2014
                },
                {
                    "x": 2102,
                    "y": 2014
                },
                {
                    "x": 2102,
                    "y": 2107
                },
                {
                    "x": 441,
                    "y": 2107
                }
            ],
            "category": "paragraph",
            "html": "<p id='135' style='font-size:16px'>Using equations (20) and (18), the Monte Carlo estimate of the variational lower bound, given<br>datapoint x(i), is:</p>",
            "id": 135,
            "page": 13,
            "text": "Using equations (20) and (18), the Monte Carlo estimate of the variational lower bound, given datapoint x(i), is:"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2293
                },
                {
                    "x": 2105,
                    "y": 2293
                },
                {
                    "x": 2105,
                    "y": 2528
                },
                {
                    "x": 441,
                    "y": 2528
                }
            ],
            "category": "paragraph",
            "html": "<p id='136' style='font-size:14px'>where e(l) ~ p(E) and 5(1) ~ p(5). The estimator only depends on samples from p(E) and p(5)<br>which are obviously not influenced by 中, therefore the estimator can be differentiated w.r.t. ⌀.<br>The resulting stochastic gradients can be used in conjunction with stochastic optimization methods<br>such as SGD or Adagrad [DHS10]. See algorithm 1 for a basic approach to computing stochastic<br>gradients.</p>",
            "id": 136,
            "page": 13,
            "text": "where e(l) ~ p(E) and 5(1) ~ p(5). The estimator only depends on samples from p(E) and p(5) which are obviously not influenced by 中, therefore the estimator can be differentiated w.r.t. ⌀. The resulting stochastic gradients can be used in conjunction with stochastic optimization methods such as SGD or Adagrad [DHS10]. See algorithm 1 for a basic approach to computing stochastic gradients."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2585
                },
                {
                    "x": 701,
                    "y": 2585
                },
                {
                    "x": 701,
                    "y": 2632
                },
                {
                    "x": 444,
                    "y": 2632
                }
            ],
            "category": "paragraph",
            "html": "<p id='137' style='font-size:18px'>F.1 Example</p>",
            "id": 137,
            "page": 13,
            "text": "F.1 Example"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2670
                },
                {
                    "x": 2107,
                    "y": 2670
                },
                {
                    "x": 2107,
                    "y": 2900
                },
                {
                    "x": 441,
                    "y": 2900
                }
            ],
            "category": "paragraph",
            "html": "<p id='138' style='font-size:14px'>Let the prior over the parameters and latent variables be the centered isotropic Gaussian pa(0) =<br>N(z; 0,I) and pe(z) = N(z;0,I). Note that in this case, the prior lacks parameters. Let's also<br>assume that the true posteriors are approximatily Gaussian with an approximately diagonal covari-<br>ance. In this case, we can let the variational approximate posteriors be multivariate Gaussians with<br>a diagonal covariance structure:</p>",
            "id": 138,
            "page": 13,
            "text": "Let the prior over the parameters and latent variables be the centered isotropic Gaussian pa(0) = N(z; 0,I) and pe(z) = N(z;0,I). Note that in this case, the prior lacks parameters. Let's also assume that the true posteriors are approximatily Gaussian with an approximately diagonal covariance. In this case, we can let the variational approximate posteriors be multivariate Gaussians with a diagonal covariance structure:"
        },
        {
            "bounding_box": [
                {
                    "x": 1252,
                    "y": 3133
                },
                {
                    "x": 1300,
                    "y": 3133
                },
                {
                    "x": 1300,
                    "y": 3171
                },
                {
                    "x": 1252,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='139' style='font-size:14px'>13</footer>",
            "id": 139,
            "page": 13,
            "text": "13"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 347
                },
                {
                    "x": 2103,
                    "y": 347
                },
                {
                    "x": 2103,
                    "y": 438
                },
                {
                    "x": 444,
                    "y": 438
                }
            ],
            "category": "paragraph",
            "html": "<p id='140' style='font-size:18px'>Algorithm 2 Pseudocode for computing a stochastic gradient using our estimator. See text for<br>meaning of the functions f⌀, 9⌀ and h⌀.</p>",
            "id": 140,
            "page": 14,
            "text": "Algorithm 2 Pseudocode for computing a stochastic gradient using our estimator. See text for meaning of the functions f⌀, 9⌀ and h⌀."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 462
                },
                {
                    "x": 1348,
                    "y": 462
                },
                {
                    "x": 1348,
                    "y": 504
                },
                {
                    "x": 445,
                    "y": 504
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='141' style='font-size:20px'>Require: ⌀ (Current value of variational parameters)</p>",
            "id": 141,
            "page": 14,
            "text": "Require: ⌀ (Current value of variational parameters)"
        },
        {
            "bounding_box": [
                {
                    "x": 486,
                    "y": 509
                },
                {
                    "x": 777,
                    "y": 509
                },
                {
                    "x": 777,
                    "y": 594
                },
                {
                    "x": 486,
                    "y": 594
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='142' style='font-size:14px'>g ← 0<br>for l is 1 to L do</p>",
            "id": 142,
            "page": 14,
            "text": "g ← 0 for l is 1 to L do"
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 599
                },
                {
                    "x": 1201,
                    "y": 599
                },
                {
                    "x": 1201,
                    "y": 787
                },
                {
                    "x": 545,
                    "y": 787
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='143' style='font-size:18px'>x ← Random draw from dataset X<br>E ← Random draw from prior p(E)<br>5 ← Random draw from prior p(5)<br>g ← g + �▽�f�(x,g�(e,x), h⌀(5))</p>",
            "id": 143,
            "page": 14,
            "text": "x ← Random draw from dataset X E ← Random draw from prior p(E) 5 ← Random draw from prior p(5) g ← g + �▽�f�(x,g�(e,x), h⌀(5))"
        },
        {
            "bounding_box": [
                {
                    "x": 487,
                    "y": 784
                },
                {
                    "x": 622,
                    "y": 784
                },
                {
                    "x": 622,
                    "y": 821
                },
                {
                    "x": 487,
                    "y": 821
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='144' style='font-size:16px'>end for</p>",
            "id": 144,
            "page": 14,
            "text": "end for"
        },
        {
            "bounding_box": [
                {
                    "x": 487,
                    "y": 832
                },
                {
                    "x": 644,
                    "y": 832
                },
                {
                    "x": 644,
                    "y": 870
                },
                {
                    "x": 487,
                    "y": 870
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='145' style='font-size:18px'>return g</p>",
            "id": 145,
            "page": 14,
            "text": "return g"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 978
                },
                {
                    "x": 2104,
                    "y": 978
                },
                {
                    "x": 2104,
                    "y": 1066
                },
                {
                    "x": 443,
                    "y": 1066
                }
            ],
            "category": "paragraph",
            "html": "<p id='146' style='font-size:14px'>where Hz and Oz are yet unspecified functions of x. Since they are Gaussian, we can parameterize<br>the variational approximate posteriors:</p>",
            "id": 146,
            "page": 14,
            "text": "where Hz and Oz are yet unspecified functions of x. Since they are Gaussian, we can parameterize the variational approximate posteriors:"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1239
                },
                {
                    "x": 2105,
                    "y": 1239
                },
                {
                    "x": 2105,
                    "y": 1328
                },
                {
                    "x": 443,
                    "y": 1328
                }
            ],
            "category": "paragraph",
            "html": "<p id='147' style='font-size:18px'>With ⊙ we signify an element-wise product. These can be plugged into the lower bound defined<br>above (eqs (21) and (22)).</p>",
            "id": 147,
            "page": 14,
            "text": "With ⊙ we signify an element-wise product. These can be plugged into the lower bound defined above (eqs (21) and (22))."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1355
                },
                {
                    "x": 2106,
                    "y": 1355
                },
                {
                    "x": 2106,
                    "y": 1491
                },
                {
                    "x": 442,
                    "y": 1491
                }
            ],
            "category": "paragraph",
            "html": "<p id='148' style='font-size:18px'>In this case it is possible to construct an alternative estimator with a lower variance, since in this<br>model pa (日), pe(z), 9⌀(0) and 9⌀(ZX) are Gaussian, and therefore four terms of f⌀ can be solved<br>analytically. The resulting estimator is:</p>",
            "id": 148,
            "page": 14,
            "text": "In this case it is possible to construct an alternative estimator with a lower variance, since in this model pa (日), pe(z), 9⌀(0) and 9⌀(ZX) are Gaussian, and therefore four terms of f⌀ can be solved analytically. The resulting estimator is:"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 1847
                },
                {
                    "x": 1603,
                    "y": 1847
                },
                {
                    "x": 1603,
                    "y": 1925
                },
                {
                    "x": 440,
                    "y": 1925
                }
            ],
            "category": "paragraph",
            "html": "<p id='149' style='font-size:20px'>OM and 08 simply denote the j-th element of vectors M(i) and �(i)</p>",
            "id": 149,
            "page": 14,
            "text": "OM and 08 simply denote the j-th element of vectors M(i) and �(i)"
        },
        {
            "bounding_box": [
                {
                    "x": 1253,
                    "y": 3133
                },
                {
                    "x": 1300,
                    "y": 3133
                },
                {
                    "x": 1300,
                    "y": 3171
                },
                {
                    "x": 1253,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='150' style='font-size:16px'>14</footer>",
            "id": 150,
            "page": 14,
            "text": "14"
        }
    ]
}