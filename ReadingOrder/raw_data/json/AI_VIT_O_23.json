{
  "id": "6299fd0a-0f92-11ef-8230-426932df3dcf",
  "pdf_path": "./pdf/AI_VIT_O/2012.15840v3.pdf",
  "elements": [
    {
      "bounding_box": [
        {
          "x": 252,
          "y": 437
        },
        {
          "x": 2234,
          "y": 437
        },
        {
          "x": 2234,
          "y": 574
        },
        {
          "x": 252,
          "y": 574
        }
      ],
      "category": "paragraph",
      "html": "<p id='0' style='font-size:22px'>Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective<br>with Transformers</p>",
      "id": 0,
      "page": 1,
      "text": "Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective\nwith Transformers"
    },
    {
      "bounding_box": [
        {
          "x": 230,
          "y": 672
        },
        {
          "x": 2239,
          "y": 672
        },
        {
          "x": 2239,
          "y": 925
        },
        {
          "x": 230,
          "y": 925
        }
      ],
      "category": "paragraph",
      "html": "<p id='1' style='font-size:20px'>Sixiao Zheng1* Jiachen Lu1 Hengshuang Zhao2 Xiatian Zhu3 Zekun Luo4 Yabiao Wang4<br>Yanwei Fu1 Jianfeng Feng1 Tao Xiang3,5 Philip H.S. Torr2 Li Zhang1†<br>1Fudan University 2University of Oxford 3University of Surrey<br>4Tencent Youtu Lab 5Facebook AI</p>",
      "id": 1,
      "page": 1,
      "text": "Sixiao Zheng1* Jiachen Lu1 Hengshuang Zhao2 Xiatian Zhu3 Zekun Luo4 Yabiao Wang4\nYanwei Fu1 Jianfeng Feng1 Tao Xiang3,5 Philip H.S. Torr2 Li Zhang1†\n1Fudan University 2University of Oxford 3University of Surrey\n4Tencent Youtu Lab 5Facebook AI"
    },
    {
      "bounding_box": [
        {
          "x": 747,
          "y": 917
        },
        {
          "x": 1717,
          "y": 917
        },
        {
          "x": 1717,
          "y": 965
        },
        {
          "x": 747,
          "y": 965
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='2' style='font-size:16px'>https : / / fudan - zvg · github · io / SETR</p>",
      "id": 2,
      "page": 1,
      "text": "https : / / fudan - zvg · github · io / SETR"
    },
    {
      "bounding_box": [
        {
          "x": 602,
          "y": 1084
        },
        {
          "x": 798,
          "y": 1084
        },
        {
          "x": 798,
          "y": 1134
        },
        {
          "x": 602,
          "y": 1134
        }
      ],
      "category": "paragraph",
      "html": "<p id='3' style='font-size:20px'>Abstract</p>",
      "id": 3,
      "page": 1,
      "text": "Abstract"
    },
    {
      "bounding_box": [
        {
          "x": 199,
          "y": 1187
        },
        {
          "x": 1201,
          "y": 1187
        },
        {
          "x": 1201,
          "y": 2336
        },
        {
          "x": 199,
          "y": 2336
        }
      ],
      "category": "paragraph",
      "html": "<p id='4' style='font-size:16px'>Most recent semantic segmentation methods adopt<br>a fully-convolutional network (FCN) with an encoder-<br>decoder architecture. The encoder progressively reduces<br>the spatial resolution and learns more abstract/semantic<br>visual concepts with larger receptive fields. Since context<br>modeling is critical for segmentation, the latest efforts have<br>been focused on increasing the receptive field, through ei-<br>ther dilated/atrous convolutions or inserting attention mod-<br>ules. However, the encoder-decoder based FCN architec-<br>ture remains unchanged. In this paper, we aim to provide<br>an alternative perspective by treating semantic segmenta-<br>tion as a sequence-to-sequence prediction task. Specifically,<br>we deploy a pure transformer (i.e., without convolution and<br>resolution reduction) to encode an image as a sequence of<br>patches. With the global context modeled in every layer of<br>the transformer, this encoder can be combined with a simple<br>decoder to provide a powerful segmentation model, termed<br>SEgmentation TRansformer (SETR). Extensive experiments<br>show that SETR achieves new state of the art on ADE20K<br>(50.28% mIoU), Pascal Context (55.83% mIoU) and com-<br>petitive results on Cityscapes. Particularly, we achieve the<br>first position in the highly competitive ADE20K test server<br>leaderboard on the day of submission.</p>",
      "id": 4,
      "page": 1,
      "text": "Most recent semantic segmentation methods adopt\na fully-convolutional network (FCN) with an encoder-\ndecoder architecture. The encoder progressively reduces\nthe spatial resolution and learns more abstract/semantic\nvisual concepts with larger receptive fields. Since context\nmodeling is critical for segmentation, the latest efforts have\nbeen focused on increasing the receptive field, through ei-\nther dilated/atrous convolutions or inserting attention mod-\nules. However, the encoder-decoder based FCN architec-\nture remains unchanged. In this paper, we aim to provide\nan alternative perspective by treating semantic segmenta-\ntion as a sequence-to-sequence prediction task. Specifically,\nwe deploy a pure transformer (i.e., without convolution and\nresolution reduction) to encode an image as a sequence of\npatches. With the global context modeled in every layer of\nthe transformer, this encoder can be combined with a simple\ndecoder to provide a powerful segmentation model, termed\nSEgmentation TRansformer (SETR). Extensive experiments\nshow that SETR achieves new state of the art on ADE20K\n(50.28% mIoU), Pascal Context (55.83% mIoU) and com-\npetitive results on Cityscapes. Particularly, we achieve the\nfirst position in the highly competitive ADE20K test server\nleaderboard on the day of submission."
    },
    {
      "bounding_box": [
        {
          "x": 205,
          "y": 2433
        },
        {
          "x": 532,
          "y": 2433
        },
        {
          "x": 532,
          "y": 2484
        },
        {
          "x": 205,
          "y": 2484
        }
      ],
      "category": "paragraph",
      "html": "<p id='5' style='font-size:18px'>1. Introduction</p>",
      "id": 5,
      "page": 1,
      "text": "1. Introduction"
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 2518
        },
        {
          "x": 1200,
          "y": 2518
        },
        {
          "x": 1200,
          "y": 2817
        },
        {
          "x": 202,
          "y": 2817
        }
      ],
      "category": "paragraph",
      "html": "<p id='6' style='font-size:16px'>Since the seminal work of [36], existing semantic seg-<br>mentation models have been dominated by those based on<br>fully convolutional network (FCN). A standard FCN seg-<br>mentation model has an encoder-decoder architecture: the<br>encoder is for feature representation learning, while the de-<br>coder for pixel-level classification of the feature representa-</p>",
      "id": 6,
      "page": 1,
      "text": "Since the seminal work of [36], existing semantic seg-\nmentation models have been dominated by those based on\nfully convolutional network (FCN). A standard FCN seg-\nmentation model has an encoder-decoder architecture: the\nencoder is for feature representation learning, while the de-\ncoder for pixel-level classification of the feature representa-"
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 2855
        },
        {
          "x": 1197,
          "y": 2855
        },
        {
          "x": 1197,
          "y": 2973
        },
        {
          "x": 201,
          "y": 2973
        }
      ],
      "category": "paragraph",
      "html": "<p id='7' style='font-size:14px'>*Work done while Sixiao Zheng was interning at Tencent Youtu Lab.<br>†Li Zhang (lizhangfd@fudan.edu.cn) is the corresponding author with<br>School of Data Science, Fudan University.</p>",
      "id": 7,
      "page": 1,
      "text": "*Work done while Sixiao Zheng was interning at Tencent Youtu Lab.\n†Li Zhang (lizhangfd@fudan.edu.cn) is the corresponding author with\nSchool of Data Science, Fudan University."
    },
    {
      "bounding_box": [
        {
          "x": 1277,
          "y": 1087
        },
        {
          "x": 2278,
          "y": 1087
        },
        {
          "x": 2278,
          "y": 1985
        },
        {
          "x": 1277,
          "y": 1985
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='8' style='font-size:18px'>tions yielded by the encoder. Among the two, feature rep-<br>resentation learning (i.e., the encoder) is arguably the most<br>important model component [8, 28, 57, 60]. The encoder,<br>like most other CNNs designed for image understanding,<br>consists of stacked convolution layers. Due to concerns<br>on computational cost, the resolution of feature maps is re-<br>duced progressively, and the encoder is hence able to learn<br>more abstract/semantic visual concepts with a gradually in-<br>creased receptive field. Such a design is popular due to two<br>favorable merits, namely translation equivariance and local-<br>ity. The former respects well the nature of imaging pro-<br>cess [58] which underpins the model generalization ability<br>to unseen image data. Whereas the latter controls the model<br>complexity by sharing parameters across space. However, it<br>also raises a fundamental limitation that learning long-range<br>dependency information, critical for semantic segmentation<br>in unconstrained scene images [2,50], becomes challenging<br>due to still limited receptive fields.</p>",
      "id": 8,
      "page": 1,
      "text": "tions yielded by the encoder. Among the two, feature rep-\nresentation learning (i.e., the encoder) is arguably the most\nimportant model component [8, 28, 57, 60]. The encoder,\nlike most other CNNs designed for image understanding,\nconsists of stacked convolution layers. Due to concerns\non computational cost, the resolution of feature maps is re-\nduced progressively, and the encoder is hence able to learn\nmore abstract/semantic visual concepts with a gradually in-\ncreased receptive field. Such a design is popular due to two\nfavorable merits, namely translation equivariance and local-\nity. The former respects well the nature of imaging pro-\ncess [58] which underpins the model generalization ability\nto unseen image data. Whereas the latter controls the model\ncomplexity by sharing parameters across space. However, it\nalso raises a fundamental limitation that learning long-range\ndependency information, critical for semantic segmentation\nin unconstrained scene images [2,50], becomes challenging\ndue to still limited receptive fields."
    },
    {
      "bounding_box": [
        {
          "x": 1277,
          "y": 2029
        },
        {
          "x": 2277,
          "y": 2029
        },
        {
          "x": 2277,
          "y": 2978
        },
        {
          "x": 1277,
          "y": 2978
        }
      ],
      "category": "paragraph",
      "html": "<p id='9' style='font-size:18px'>To overcome this aforementioned limitation, a number<br>of approaches have been introduced recently. One approach<br>is to directly manipulate the convolution operation. This in-<br>cludes large kernel sizes [40], atrous convolutions [8, 22],<br>and image/feature pyramids [60]. The other approach is to<br>integrate attention modules into the FCN architecture. Such<br>a module aims to model global interactions of all pixels in<br>the feature map [48]. When applied to semantic segmenta-<br>tion [25, 29], a common design is to combine the attention<br>module to the FCN architecture with attention layers sitting<br>on the top. Taking either approach, the standard encoder-<br>decoder FCN model architecture remains unchanged. More<br>recently, attempts have been made to get rid of convolutions<br>altogether and deploy attention-alone models [47] instead.<br>However, even without convolution, they do not change the<br>nature of the FCN model structure: an encoder downsam-<br>ples the spatial resolution of the input, developing lower-<br>resolution feature mappings useful for discriminating se-<br>mantic classes, and the decoder upsamples the feature rep-</p>",
      "id": 9,
      "page": 1,
      "text": "To overcome this aforementioned limitation, a number\nof approaches have been introduced recently. One approach\nis to directly manipulate the convolution operation. This in-\ncludes large kernel sizes [40], atrous convolutions [8, 22],\nand image/feature pyramids [60]. The other approach is to\nintegrate attention modules into the FCN architecture. Such\na module aims to model global interactions of all pixels in\nthe feature map [48]. When applied to semantic segmenta-\ntion [25, 29], a common design is to combine the attention\nmodule to the FCN architecture with attention layers sitting\non the top. Taking either approach, the standard encoder-\ndecoder FCN model architecture remains unchanged. More\nrecently, attempts have been made to get rid of convolutions\naltogether and deploy attention-alone models [47] instead.\nHowever, even without convolution, they do not change the\nnature of the FCN model structure: an encoder downsam-\nples the spatial resolution of the input, developing lower-\nresolution feature mappings useful for discriminating se-\nmantic classes, and the decoder upsamples the feature rep-"
    },
    {
      "bounding_box": [
        {
          "x": 58,
          "y": 914
        },
        {
          "x": 149,
          "y": 914
        },
        {
          "x": 149,
          "y": 2331
        },
        {
          "x": 58,
          "y": 2331
        }
      ],
      "category": "footer",
      "html": "<br><footer id='10' style='font-size:14px'>2021<br>Jul<br>25<br>[cs.CV]<br>arXiv:2012.15840v3</footer>",
      "id": 10,
      "page": 1,
      "text": "2021\nJul\n25\n[cs.CV]\narXiv:2012.15840v3"
    },
    {
      "bounding_box": [
        {
          "x": 1226,
          "y": 3056
        },
        {
          "x": 1249,
          "y": 3056
        },
        {
          "x": 1249,
          "y": 3090
        },
        {
          "x": 1226,
          "y": 3090
        }
      ],
      "category": "footer",
      "html": "<footer id='11' style='font-size:16px'>1</footer>",
      "id": 11,
      "page": 1,
      "text": "1"
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 312
        },
        {
          "x": 1084,
          "y": 312
        },
        {
          "x": 1084,
          "y": 353
        },
        {
          "x": 203,
          "y": 353
        }
      ],
      "category": "paragraph",
      "html": "<p id='12' style='font-size:14px'>resentations into a full-resolution segmentation map.</p>",
      "id": 12,
      "page": 2,
      "text": "resentations into a full-resolution segmentation map."
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 400
        },
        {
          "x": 1199,
          "y": 400
        },
        {
          "x": 1199,
          "y": 1445
        },
        {
          "x": 200,
          "y": 1445
        }
      ],
      "category": "paragraph",
      "html": "<p id='13' style='font-size:18px'>In this paper, we aim to provide a rethinking to the se-<br>mantic segmentation model design and contribute an alter-<br>native. In particular, we propose to replace the stacked con-<br>volution layers based encoder with gradually reduced spa-<br>tial resolution with a pure transformer [45], resulting in<br>a new segmentation model termed SEgmentation TRans-<br>former (SETR). This transformer-alone encoder treats an<br>input image as a sequence of image patches represented<br>by learned patch embedding, and transforms the sequence<br>with global self-attention modeling for discriminative fea-<br>ture representation learning. Concretely, we first decom-<br>pose an image into a grid of fixed-sized patches, forming a<br>sequence of patches. With a linear embedding layer applied<br>to the flattened pixel vectors of every patch, we then obtain<br>a sequence of feature embedding vectors as the input to a<br>transformer. Given the learned features from the encoder<br>transformer, a decoder is then used to recover the original<br>image resolution. Crucially there is no downsampling in<br>spatial resolution but global context modeling at every layer<br>of the encoder transformer, thus offering a completely new<br>perspective to the semantic segmentation problem.</p>",
      "id": 13,
      "page": 2,
      "text": "In this paper, we aim to provide a rethinking to the se-\nmantic segmentation model design and contribute an alter-\nnative. In particular, we propose to replace the stacked con-\nvolution layers based encoder with gradually reduced spa-\ntial resolution with a pure transformer [45], resulting in\na new segmentation model termed SEgmentation TRans-\nformer (SETR). This transformer-alone encoder treats an\ninput image as a sequence of image patches represented\nby learned patch embedding, and transforms the sequence\nwith global self-attention modeling for discriminative fea-\nture representation learning. Concretely, we first decom-\npose an image into a grid of fixed-sized patches, forming a\nsequence of patches. With a linear embedding layer applied\nto the flattened pixel vectors of every patch, we then obtain\na sequence of feature embedding vectors as the input to a\ntransformer. Given the learned features from the encoder\ntransformer, a decoder is then used to recover the original\nimage resolution. Crucially there is no downsampling in\nspatial resolution but global context modeling at every layer\nof the encoder transformer, thus offering a completely new\nperspective to the semantic segmentation problem."
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 1488
        },
        {
          "x": 1198,
          "y": 1488
        },
        {
          "x": 1198,
          "y": 2136
        },
        {
          "x": 200,
          "y": 2136
        }
      ],
      "category": "paragraph",
      "html": "<p id='14' style='font-size:16px'>This pure transformer design is inspired by its tremen-<br>dous success in natural language processing (NLP) [15,45].<br>More recently, a pure vision transformer or ViT [17] has<br>shown to be effective for image classification tasks. It thus<br>provides direct evidence that the traditional stacked convo-<br>lution layer (i.e., CNN) design can be challenged and image<br>features do not necessarily need to be learned progressively<br>from local to global context by reducing spatial resolution.<br>However, extending a pure transformer from image classi-<br>fication to a spatial location sensitive task of semantic seg-<br>mentation is non-trivial. We show empirically that SETR<br>not only offers a new perspective in model design, but also<br>achieves new state of the art on a number of benchmarks.</p>",
      "id": 14,
      "page": 2,
      "text": "This pure transformer design is inspired by its tremen-\ndous success in natural language processing (NLP) [15,45].\nMore recently, a pure vision transformer or ViT [17] has\nshown to be effective for image classification tasks. It thus\nprovides direct evidence that the traditional stacked convo-\nlution layer (i.e., CNN) design can be challenged and image\nfeatures do not necessarily need to be learned progressively\nfrom local to global context by reducing spatial resolution.\nHowever, extending a pure transformer from image classi-\nfication to a spatial location sensitive task of semantic seg-\nmentation is non-trivial. We show empirically that SETR\nnot only offers a new perspective in model design, but also\nachieves new state of the art on a number of benchmarks."
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 2177
        },
        {
          "x": 1199,
          "y": 2177
        },
        {
          "x": 1199,
          "y": 2979
        },
        {
          "x": 200,
          "y": 2979
        }
      ],
      "category": "paragraph",
      "html": "<p id='15' style='font-size:18px'>The following contributions are made in this paper: (1)<br>We reformulate the image semantic segmentation problem<br>from a sequence-to-sequence learning perspective, offer-<br>ing an alternative to the dominating encoder-decoder FCN<br>model design. (2) As an instantiation, we exploit the trans-<br>former framework to implement our fully attentive feature<br>representation encoder by sequentializing images. (3) To<br>extensively examine the self-attentive feature presentations,<br>we further introduce three different decoder designs with<br>varying complexities. Extensive experiments show that<br>our SETR models can learn superior feature representa-<br>tions as compared to different FCNs with and without at-<br>tention modules, yielding new state of the art on ADE20K<br>(50.28%), Pascal Context (55.83%) and competitive results<br>on Cityscapes. Particularly, our entry is ranked the 1st place<br>in the highly competitive ADE20K test server leaderboard.</p>",
      "id": 15,
      "page": 2,
      "text": "The following contributions are made in this paper: (1)\nWe reformulate the image semantic segmentation problem\nfrom a sequence-to-sequence learning perspective, offer-\ning an alternative to the dominating encoder-decoder FCN\nmodel design. (2) As an instantiation, we exploit the trans-\nformer framework to implement our fully attentive feature\nrepresentation encoder by sequentializing images. (3) To\nextensively examine the self-attentive feature presentations,\nwe further introduce three different decoder designs with\nvarying complexities. Extensive experiments show that\nour SETR models can learn superior feature representa-\ntions as compared to different FCNs with and without at-\ntention modules, yielding new state of the art on ADE20K\n(50.28%), Pascal Context (55.83%) and competitive results\non Cityscapes. Particularly, our entry is ranked the 1st place\nin the highly competitive ADE20K test server leaderboard."
    },
    {
      "bounding_box": [
        {
          "x": 1282,
          "y": 302
        },
        {
          "x": 1626,
          "y": 302
        },
        {
          "x": 1626,
          "y": 351
        },
        {
          "x": 1282,
          "y": 351
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='16' style='font-size:20px'>2. Related work</p>",
      "id": 16,
      "page": 2,
      "text": "2. Related work"
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 388
        },
        {
          "x": 2276,
          "y": 388
        },
        {
          "x": 2276,
          "y": 983
        },
        {
          "x": 1278,
          "y": 983
        }
      ],
      "category": "paragraph",
      "html": "<p id='17' style='font-size:18px'>Semantic segmentation Semantic image segmentation has<br>been significantly boosted with the development of deep<br>neural networks. By removing fully connected layers, the<br>fully convolutional network (FCN) [36] is able to achieve<br>pixel-wise predictions. While the predictions of FCN are<br>relatively coarse, several CRF/MRF [6, 35, 62] based ap-<br>proaches are developed to help refine the coarse predictions.<br>To address the inherent tension between semantics and lo-<br>cation [36], coarse and fine layers need to be aggregated for<br>both the encoder and decoder. This leads to different vari-<br>ants of the encoder-decoder structures [2, 38, 42] for multi-<br>level feature fusion.</p>",
      "id": 17,
      "page": 2,
      "text": "Semantic segmentation Semantic image segmentation has\nbeen significantly boosted with the development of deep\nneural networks. By removing fully connected layers, the\nfully convolutional network (FCN) [36] is able to achieve\npixel-wise predictions. While the predictions of FCN are\nrelatively coarse, several CRF/MRF [6, 35, 62] based ap-\nproaches are developed to help refine the coarse predictions.\nTo address the inherent tension between semantics and lo-\ncation [36], coarse and fine layers need to be aggregated for\nboth the encoder and decoder. This leads to different vari-\nants of the encoder-decoder structures [2, 38, 42] for multi-\nlevel feature fusion."
    },
    {
      "bounding_box": [
        {
          "x": 1277,
          "y": 991
        },
        {
          "x": 2277,
          "y": 991
        },
        {
          "x": 2277,
          "y": 2186
        },
        {
          "x": 1277,
          "y": 2186
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='18' style='font-size:18px'>Many recent efforts have been focused on addressing<br>the limited receptive field/context modeling problem in<br>FCN. To enlarge the receptive field, DeepLab [7] and Di-<br>lation [53] introduce the dilated convolution. Alterna-<br>tively, context modeling is the focus of PSPNet [60] and<br>DeepLabV2 [9]. The former proposes the PPM module to<br>obtain different region's contextual information while the<br>latter develops ASPP module that adopts pyramid dilated<br>convolutions with different dilation rates. Decomposed<br>large kernels [40] are also utilized for context capturing.<br>Recently, attention based models are popular for capturing<br>long range context information. PSANet [61] develops the<br>pointwise spatial attention module for dynamically captur-<br>ing the long range context. DANet [18] embeds both spatial<br>attention and channel attention. CCNet [26] alternatively<br>focuses on economizing the heavy computation budget in-<br>troduced by full spatial attention. DGMN [57] builds a dy-<br>namic graph message passing network for scene modeling<br>and it can significantly reduce the computational complex-<br>ity. Note that all these approaches are still based on FCNs<br>where the feature encoding and extraction part are based on<br>classical ConvNets like VGG [43] and ResNet [20]. In this<br>work, we alternatively rethink the semantic segmentation<br>task from a different perspective.</p>",
      "id": 18,
      "page": 2,
      "text": "Many recent efforts have been focused on addressing\nthe limited receptive field/context modeling problem in\nFCN. To enlarge the receptive field, DeepLab [7] and Di-\nlation [53] introduce the dilated convolution. Alterna-\ntively, context modeling is the focus of PSPNet [60] and\nDeepLabV2 [9]. The former proposes the PPM module to\nobtain different region's contextual information while the\nlatter develops ASPP module that adopts pyramid dilated\nconvolutions with different dilation rates. Decomposed\nlarge kernels [40] are also utilized for context capturing.\nRecently, attention based models are popular for capturing\nlong range context information. PSANet [61] develops the\npointwise spatial attention module for dynamically captur-\ning the long range context. DANet [18] embeds both spatial\nattention and channel attention. CCNet [26] alternatively\nfocuses on economizing the heavy computation budget in-\ntroduced by full spatial attention. DGMN [57] builds a dy-\nnamic graph message passing network for scene modeling\nand it can significantly reduce the computational complex-\nity. Note that all these approaches are still based on FCNs\nwhere the feature encoding and extraction part are based on\nclassical ConvNets like VGG [43] and ResNet [20]. In this\nwork, we alternatively rethink the semantic segmentation\ntask from a different perspective."
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 2226
        },
        {
          "x": 2277,
          "y": 2226
        },
        {
          "x": 2277,
          "y": 2978
        },
        {
          "x": 1278,
          "y": 2978
        }
      ],
      "category": "paragraph",
      "html": "<p id='19' style='font-size:18px'>Transformer Transformer and self-attention models have<br>revolutionized machine translation and NLP [14, 15,45,51].<br>Recently, there are also some explorations for the usage<br>of transformer structures in image recognition. Non-local<br>network [48] appends transformer style attention onto the<br>convolutional backbone. AANet [3] mixes convolution and<br>self-attention for backbone training. LRNet [24] and stand-<br>alone networks [41] explore local self-attention to avoid<br>the heavy computation brought by global self-attention.<br>SAN [59] explores two types of self-attention modules.<br>Axial-Attention [47] decomposes the global spatial atten-<br>tion into two separate axial attentions such that the com-<br>putation is largely reduced. Apart from these pure trans-<br>former based models, there are also CNN-transformer hy-<br>brid ones. DETR [5] and the following deformable version</p>",
      "id": 19,
      "page": 2,
      "text": "Transformer Transformer and self-attention models have\nrevolutionized machine translation and NLP [14, 15,45,51].\nRecently, there are also some explorations for the usage\nof transformer structures in image recognition. Non-local\nnetwork [48] appends transformer style attention onto the\nconvolutional backbone. AANet [3] mixes convolution and\nself-attention for backbone training. LRNet [24] and stand-\nalone networks [41] explore local self-attention to avoid\nthe heavy computation brought by global self-attention.\nSAN [59] explores two types of self-attention modules.\nAxial-Attention [47] decomposes the global spatial atten-\ntion into two separate axial attentions such that the com-\nputation is largely reduced. Apart from these pure trans-\nformer based models, there are also CNN-transformer hy-\nbrid ones. DETR [5] and the following deformable version"
    },
    {
      "bounding_box": [
        {
          "x": 1223,
          "y": 3054
        },
        {
          "x": 1252,
          "y": 3054
        },
        {
          "x": 1252,
          "y": 3092
        },
        {
          "x": 1223,
          "y": 3092
        }
      ],
      "category": "footer",
      "html": "<footer id='20' style='font-size:16px'>2</footer>",
      "id": 20,
      "page": 2,
      "text": "2"
    },
    {
      "bounding_box": [
        {
          "x": 247,
          "y": 299
        },
        {
          "x": 2262,
          "y": 299
        },
        {
          "x": 2262,
          "y": 1182
        },
        {
          "x": 247,
          "y": 1182
        }
      ],
      "category": "figure",
      "html": "<figure><img id='21' style='font-size:14px' alt=\"↑\nMLP\nDecoder\nLayer Norm\nTransformer Layer\n24x\nTransformer Layer reshape conv→2x conv→2x conv→2x conv→2x\nMulti-Head\nAttention HW x 1024 品 x 可能 x 1024 용 x � x 256 부 x 4 x 256 블 x 受 x 256 H x W x 19\n(b)\nLayer Norm\nreshape-conv\nLinear Projection Z24 conv-conv-4x\nZ18\nZ12\nconv-4x\nPatch\nZ6\nEmbedding\nPosition\nEmbedding\n(a) (c)\" data-coord=\"top-left:(247,299); bottom-right:(2262,1182)\" /></figure>",
      "id": 21,
      "page": 3,
      "text": "↑\nMLP\nDecoder\nLayer Norm\nTransformer Layer\n24x\nTransformer Layer reshape conv→2x conv→2x conv→2x conv→2x\nMulti-Head\nAttention HW x 1024 品 x 可能 x 1024 용 x � x 256 부 x 4 x 256 블 x 受 x 256 H x W x 19\n(b)\nLayer Norm\nreshape-conv\nLinear Projection Z24 conv-conv-4x\nZ18\nZ12\nconv-4x\nPatch\nZ6\nEmbedding\nPosition\nEmbedding\n(a) (c)"
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 1203
        },
        {
          "x": 2278,
          "y": 1203
        },
        {
          "x": 2278,
          "y": 1389
        },
        {
          "x": 200,
          "y": 1389
        }
      ],
      "category": "caption",
      "html": "<caption id='22' style='font-size:14px'>Figure 1. Schematic illustration of the proposed SEgmentation TRansformer (SETR) (a). We first split an image into fixed-size patches,<br>linearly embed each of them, add position embeddings, and feed the resulting sequence of vectors to a standard Transformer encoder. To<br>perform pixel-wise segmentation, we introduce different decoder designs: (b) progressive upsampling (resulting in a variant called SETR-<br>PUP); and (c) multi-level feature aggregation (a variant called SETR-MLA).</caption>",
      "id": 22,
      "page": 3,
      "text": "Figure 1. Schematic illustration of the proposed SEgmentation TRansformer (SETR) (a). We first split an image into fixed-size patches,\nlinearly embed each of them, add position embeddings, and feed the resulting sequence of vectors to a standard Transformer encoder. To\nperform pixel-wise segmentation, we introduce different decoder designs: (b) progressive upsampling (resulting in a variant called SETR-\nPUP); and (c) multi-level feature aggregation (a variant called SETR-MLA)."
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 1438
        },
        {
          "x": 1198,
          "y": 1438
        },
        {
          "x": 1198,
          "y": 1832
        },
        {
          "x": 200,
          "y": 1832
        }
      ],
      "category": "paragraph",
      "html": "<p id='23' style='font-size:18px'>utilize transformer for object detection where transformer<br>is appended inside the detection head. STTR [32] and<br>LSTR [34] adopt transformer for disparity estimation and<br>lane shape prediction respectively. Most recently, ViT [17]<br>is the first work to show that a pure transformer based image<br>classification model can achieve the state-of-the-art. It pro-<br>vides direct inspiration to exploit a pure transformer based<br>encoder design in a semantic segmentation model.</p>",
      "id": 23,
      "page": 3,
      "text": "utilize transformer for object detection where transformer\nis appended inside the detection head. STTR [32] and\nLSTR [34] adopt transformer for disparity estimation and\nlane shape prediction respectively. Most recently, ViT [17]\nis the first work to show that a pure transformer based image\nclassification model can achieve the state-of-the-art. It pro-\nvides direct inspiration to exploit a pure transformer based\nencoder design in a semantic segmentation model."
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 1839
        },
        {
          "x": 1199,
          "y": 1839
        },
        {
          "x": 1199,
          "y": 2534
        },
        {
          "x": 200,
          "y": 2534
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='24' style='font-size:18px'>The most related work is [47] which also leverages at-<br>tention for image segmentation. However, there are several<br>key differences. First, though convolution is completely re-<br>moved in [47] as in our SETR, their model still follows the<br>conventional FCN design in that spatial resolution of feature<br>maps is reduced progressively. In contrast, our sequence-to-<br>sequence prediction model keeps the same spatial resolution<br>throughout and thus represents a step-change in model de-<br>sign. Second, to maximize the scalability on modern hard-<br>ware accelerators and facilitate easy-to-use, we stick to the<br>standard self-attention design. Instead, [47] adopts a spe-<br>cially designed axial-attention [21] which is less scalable to<br>standard computing facilities. Our model is also superior in<br>segmentation accuracy (see Section 4).</p>",
      "id": 24,
      "page": 3,
      "text": "The most related work is [47] which also leverages at-\ntention for image segmentation. However, there are several\nkey differences. First, though convolution is completely re-\nmoved in [47] as in our SETR, their model still follows the\nconventional FCN design in that spatial resolution of feature\nmaps is reduced progressively. In contrast, our sequence-to-\nsequence prediction model keeps the same spatial resolution\nthroughout and thus represents a step-change in model de-\nsign. Second, to maximize the scalability on modern hard-\nware accelerators and facilitate easy-to-use, we stick to the\nstandard self-attention design. Instead, [47] adopts a spe-\ncially designed axial-attention [21] which is less scalable to\nstandard computing facilities. Our model is also superior in\nsegmentation accuracy (see Section 4)."
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 2570
        },
        {
          "x": 427,
          "y": 2570
        },
        {
          "x": 427,
          "y": 2618
        },
        {
          "x": 204,
          "y": 2618
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='25' style='font-size:22px'>3. Method</p>",
      "id": 25,
      "page": 3,
      "text": "3. Method"
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 2652
        },
        {
          "x": 964,
          "y": 2652
        },
        {
          "x": 964,
          "y": 2699
        },
        {
          "x": 203,
          "y": 2699
        }
      ],
      "category": "paragraph",
      "html": "<p id='26' style='font-size:20px'>3.1. FCN-based semantic segmentation</p>",
      "id": 26,
      "page": 3,
      "text": "3.1. FCN-based semantic segmentation"
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 2730
        },
        {
          "x": 1199,
          "y": 2730
        },
        {
          "x": 1199,
          "y": 2975
        },
        {
          "x": 202,
          "y": 2975
        }
      ],
      "category": "paragraph",
      "html": "<p id='27' style='font-size:16px'>In order to contrast with our new model design, let us<br>first revisit the conventional FCN [36] for image semantic<br>segmentation. An FCN encoder consists of a stack of se-<br>quentially connected convolutional layers. The first layer<br>takes as input the image, denoted as H x W x3 with H x W</p>",
      "id": 27,
      "page": 3,
      "text": "In order to contrast with our new model design, let us\nfirst revisit the conventional FCN [36] for image semantic\nsegmentation. An FCN encoder consists of a stack of se-\nquentially connected convolutional layers. The first layer\ntakes as input the image, denoted as H x W x3 with H x W"
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 1440
        },
        {
          "x": 2277,
          "y": 1440
        },
        {
          "x": 2277,
          "y": 2232
        },
        {
          "x": 1278,
          "y": 2232
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='28' style='font-size:18px'>specifying the image size in pixels. The input of subse-<br>quent layer i is a three-dimensional tensor sized h x w x d,<br>where h and w are spatial dimensions of feature maps, and<br>d is the feature/channel dimension. Locations of the ten-<br>sor in a higher layer are computed based on the locations of<br>tensors of all lower layers they are connected to via layer-<br>by-layer convolutions, which are defined as their receptive<br>fields. Due to the locality nature of convolution operation,<br>the receptive field increases linearly along the depth of lay-<br>ers, conditional on the kernel sizes (typically 3 x 3). As<br>a result, only higher layers with big receptive fields can<br>model long-range dependencies in this FCN architecture.<br>However, it is shown that the benefits of adding more layers<br>would diminish rapidly once reaching certain depths [20].<br>Having limited receptive fields for context modeling is thus<br>an intrinsic limitation of the vanilla FCN architecture.</p>",
      "id": 28,
      "page": 3,
      "text": "specifying the image size in pixels. The input of subse-\nquent layer i is a three-dimensional tensor sized h x w x d,\nwhere h and w are spatial dimensions of feature maps, and\nd is the feature/channel dimension. Locations of the ten-\nsor in a higher layer are computed based on the locations of\ntensors of all lower layers they are connected to via layer-\nby-layer convolutions, which are defined as their receptive\nfields. Due to the locality nature of convolution operation,\nthe receptive field increases linearly along the depth of lay-\ners, conditional on the kernel sizes (typically 3 x 3). As\na result, only higher layers with big receptive fields can\nmodel long-range dependencies in this FCN architecture.\nHowever, it is shown that the benefits of adding more layers\nwould diminish rapidly once reaching certain depths [20].\nHaving limited receptive fields for context modeling is thus\nan intrinsic limitation of the vanilla FCN architecture."
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 2426
        },
        {
          "x": 2277,
          "y": 2426
        },
        {
          "x": 2277,
          "y": 2975
        },
        {
          "x": 1279,
          "y": 2975
        }
      ],
      "category": "paragraph",
      "html": "<p id='29' style='font-size:18px'>Recently, a number of state-of-the-art methods [25, 56,<br>57] suggest that combing FCN with attention mechanism<br>is a more effective strategy for learning long-range contex-<br>tual information. These methods limit the attention learn-<br>ing to higher layers with smaller input sizes alone due to its<br>quadratic complexity w.r.t. the pixel number of feature ten-<br>sors. This means that dependency learning on lower-level<br>feature tensors is lacking, leading to sub-optimal represen-<br>tation learning. To overcome this limitation, we propose<br>a pure self-attention based encoder, named SEgmentation<br>TRansformers (SETR).</p>",
      "id": 29,
      "page": 3,
      "text": "Recently, a number of state-of-the-art methods [25, 56,\n57] suggest that combing FCN with attention mechanism\nis a more effective strategy for learning long-range contex-\ntual information. These methods limit the attention learn-\ning to higher layers with smaller input sizes alone due to its\nquadratic complexity w.r.t. the pixel number of feature ten-\nsors. This means that dependency learning on lower-level\nfeature tensors is lacking, leading to sub-optimal represen-\ntation learning. To overcome this limitation, we propose\na pure self-attention based encoder, named SEgmentation\nTRansformers (SETR)."
    },
    {
      "bounding_box": [
        {
          "x": 1224,
          "y": 3054
        },
        {
          "x": 1252,
          "y": 3054
        },
        {
          "x": 1252,
          "y": 3092
        },
        {
          "x": 1224,
          "y": 3092
        }
      ],
      "category": "footer",
      "html": "<footer id='30' style='font-size:14px'>3</footer>",
      "id": 30,
      "page": 3,
      "text": "3"
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 304
        },
        {
          "x": 985,
          "y": 304
        },
        {
          "x": 985,
          "y": 353
        },
        {
          "x": 202,
          "y": 353
        }
      ],
      "category": "paragraph",
      "html": "<p id='31' style='font-size:22px'>3.2. Segmentation transformers (SETR)</p>",
      "id": 31,
      "page": 4,
      "text": "3.2. Segmentation transformers (SETR)"
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 388
        },
        {
          "x": 1199,
          "y": 388
        },
        {
          "x": 1199,
          "y": 784
        },
        {
          "x": 202,
          "y": 784
        }
      ],
      "category": "paragraph",
      "html": "<p id='32' style='font-size:16px'>Image to sequence SETR follows the same input-output<br>structure as in NLP for transformation between 1D se-<br>quences. There thus exists a mismatch between 2D image<br>and 1D sequence. Concretely, the Transformer, as depicted<br>in Figure 1(a), accepts a 1D sequence of feature embeddings<br>Z E RLxC as input, L is the length of sequence, C is the<br>hidden channel size. Image sequentialization is thus needed<br>into Z.<br>to convert an input image x E RHxWx3</p>",
      "id": 32,
      "page": 4,
      "text": "Image to sequence SETR follows the same input-output\nstructure as in NLP for transformation between 1D se-\nquences. There thus exists a mismatch between 2D image\nand 1D sequence. Concretely, the Transformer, as depicted\nin Figure 1(a), accepts a 1D sequence of feature embeddings\nZ E RLxC as input, L is the length of sequence, C is the\nhidden channel size. Image sequentialization is thus needed\ninto Z.\nto convert an input image x E RHxWx3"
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 795
        },
        {
          "x": 1200,
          "y": 795
        },
        {
          "x": 1200,
          "y": 1190
        },
        {
          "x": 202,
          "y": 1190
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='33' style='font-size:18px'>A straightforward way for image sequentialization is to<br>flatten the image pixel values into a 1D vector with size of<br>3HW. For a typical image sized at 480(H) x 480(W) x 3,<br>the resulting vector will have a length of 691,200. Given<br>the quadratic model complexity of Transformer, it is not<br>possible that such high-dimensional vectors can be handled<br>in both space and time. Therefore tokenizing every single<br>pixel as input to our transformer is out of the question.</p>",
      "id": 33,
      "page": 4,
      "text": "A straightforward way for image sequentialization is to\nflatten the image pixel values into a 1D vector with size of\n3HW. For a typical image sized at 480(H) x 480(W) x 3,\nthe resulting vector will have a length of 691,200. Given\nthe quadratic model complexity of Transformer, it is not\npossible that such high-dimensional vectors can be handled\nin both space and time. Therefore tokenizing every single\npixel as input to our transformer is out of the question."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 1200
        },
        {
          "x": 1199,
          "y": 1200
        },
        {
          "x": 1199,
          "y": 1497
        },
        {
          "x": 202,
          "y": 1497
        }
      ],
      "category": "paragraph",
      "html": "<p id='34' style='font-size:14px'>In view of the fact that a typical encoder designed for<br>semantic segmentation would downsample a 2D image x E<br>RHxW x3 feature map Xf E R品 x ※ xC<br>into a<br>, we thus<br>decide to set the transformer input sequence length L as<br>H W HW<br>x = This way, the output sequence of the trans-<br>16 16<br>256 ·<br>former can be simply reshaped to the target feature map xf.</p>",
      "id": 34,
      "page": 4,
      "text": "In view of the fact that a typical encoder designed for\nsemantic segmentation would downsample a 2D image x E\nRHxW x3 feature map Xf E R品 x ※ xC\ninto a\n, we thus\ndecide to set the transformer input sequence length L as\nH W HW\nx = This way, the output sequence of the trans-\n16 16\n256 ·\nformer can be simply reshaped to the target feature map xf."
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 1504
        },
        {
          "x": 1199,
          "y": 1504
        },
        {
          "x": 1199,
          "y": 2104
        },
        {
          "x": 200,
          "y": 2104
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='35' style='font-size:16px'>HW -long input sequence, we divide an<br>To obtain the<br>256<br>image x E RHxWx3 H W<br>into a grid of x patches uni-<br>16 16<br>formly, and then flatten this grid into a sequence. By<br>further mapping each vectorized patch p into a latent C-<br>dimensional embedding space using a linear projection<br>function f: p → e E RC we obtain a 1D sequence of<br>,<br>patch embeddings for an image x. To encode the patch spa-<br>cial information, we learn a specific embedding Pi for every<br>location i which is added to ei to form the final sequence in-<br>put E = {e1 + P1, e2 + p2, · · · eL + PL}. This way, spa-<br>,<br>tial information is kept despite the orderless self-attention<br>nature of transformers.</p>",
      "id": 35,
      "page": 4,
      "text": "HW -long input sequence, we divide an\nTo obtain the\n256\nimage x E RHxWx3 H W\ninto a grid of x patches uni-\n16 16\nformly, and then flatten this grid into a sequence. By\nfurther mapping each vectorized patch p into a latent C-\ndimensional embedding space using a linear projection\nfunction f: p → e E RC we obtain a 1D sequence of\n,\npatch embeddings for an image x. To encode the patch spa-\ncial information, we learn a specific embedding Pi for every\nlocation i which is added to ei to form the final sequence in-\nput E = {e1 + P1, e2 + p2, · · · eL + PL}. This way, spa-\n,\ntial information is kept despite the orderless self-attention\nnature of transformers."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 2165
        },
        {
          "x": 1200,
          "y": 2165
        },
        {
          "x": 1200,
          "y": 2666
        },
        {
          "x": 202,
          "y": 2666
        }
      ],
      "category": "paragraph",
      "html": "<p id='36' style='font-size:18px'>Transformer Given the 1D embedding sequence E as<br>input, a pure transformer based encoder is employed to<br>learn feature representations. This means each transformer<br>layer has a global receptive field, solving the limited re-<br>ceptive field problem of existing FCN encoder once and<br>for all. The transformer encoder consists of Le layers of<br>multi-head self-attention (MSA) and Multilayer Perceptron<br>(MLP) blocks [46] (Figure 1(a)). At each layer l, the in-<br>put to self-attention is in a triplet of (query, key, value)<br>computed from the input Zl-1 E RLxC<br>as:</p>",
      "id": 36,
      "page": 4,
      "text": "Transformer Given the 1D embedding sequence E as\ninput, a pure transformer based encoder is employed to\nlearn feature representations. This means each transformer\nlayer has a global receptive field, solving the limited re-\nceptive field problem of existing FCN encoder once and\nfor all. The transformer encoder consists of Le layers of\nmulti-head self-attention (MSA) and Multilayer Perceptron\n(MLP) blocks [46] (Figure 1(a)). At each layer l, the in-\nput to self-attention is in a triplet of (query, key, value)\ncomputed from the input Zl-1 E RLxC\nas:"
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 2822
        },
        {
          "x": 1199,
          "y": 2822
        },
        {
          "x": 1199,
          "y": 2977
        },
        {
          "x": 201,
          "y": 2977
        }
      ],
      "category": "paragraph",
      "html": "<p id='37' style='font-size:18px'>where WQ/W K/WV E RCxd are the learnable parameters<br>of three linear projection layers and d is the dimension of<br>(query, key, value). Self-attention (SA) is then formu-</p>",
      "id": 37,
      "page": 4,
      "text": "where WQ/W K/WV E RCxd are the learnable parameters\nof three linear projection layers and d is the dimension of\n(query, key, value). Self-attention (SA) is then formu-"
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 307
        },
        {
          "x": 1427,
          "y": 307
        },
        {
          "x": 1427,
          "y": 350
        },
        {
          "x": 1281,
          "y": 350
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='38' style='font-size:14px'>lated as:</p>",
      "id": 38,
      "page": 4,
      "text": "lated as:"
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 489
        },
        {
          "x": 2277,
          "y": 489
        },
        {
          "x": 2277,
          "y": 784
        },
        {
          "x": 1280,
          "y": 784
        }
      ],
      "category": "paragraph",
      "html": "<p id='39' style='font-size:16px'>MSA is an extension with m independent SA operations<br>and project their concatenated outputs: MSA(Zi-1) =<br>[SA1(Zi-1); SA2(Zi-1); · · · ; SAm (Zl-1)]Wo, where<br>Wo E RmdxC C/m. The output of<br>· d is typically set to<br>MSA is then transformed by an MLP block with residual<br>skip as the layer output as:</p>",
      "id": 39,
      "page": 4,
      "text": "MSA is an extension with m independent SA operations\nand project their concatenated outputs: MSA(Zi-1) =\n[SA1(Zi-1); SA2(Zi-1); · · · ; SAm (Zl-1)]Wo, where\nWo E RmdxC C/m. The output of\n· d is typically set to\nMSA is then transformed by an MLP block with residual\nskip as the layer output as:"
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 864
        },
        {
          "x": 2276,
          "y": 864
        },
        {
          "x": 2276,
          "y": 1013
        },
        {
          "x": 1281,
          "y": 1013
        }
      ],
      "category": "paragraph",
      "html": "<p id='40' style='font-size:16px'>Note, layer norm is applied before MSA and MLP<br>blocks which is omitted for simplicity. We denote<br>{Z1, Z2, · · · ZLe} as the features of transformer layers.<br>,</p>",
      "id": 40,
      "page": 4,
      "text": "Note, layer norm is applied before MSA and MLP\nblocks which is omitted for simplicity. We denote\n{Z1, Z2, · · · ZLe} as the features of transformer layers.\n,"
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 1042
        },
        {
          "x": 1685,
          "y": 1042
        },
        {
          "x": 1685,
          "y": 1089
        },
        {
          "x": 1281,
          "y": 1089
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='41' style='font-size:22px'>3.3. Decoder designs</p>",
      "id": 41,
      "page": 4,
      "text": "3.3. Decoder designs"
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 1115
        },
        {
          "x": 2278,
          "y": 1115
        },
        {
          "x": 2278,
          "y": 1522
        },
        {
          "x": 1280,
          "y": 1522
        }
      ],
      "category": "paragraph",
      "html": "<p id='42' style='font-size:16px'>To evaluate the effectiveness of SETR's encoder feature<br>representations Z, we introduce three different decoder de-<br>signs to perform pixel-level segmentation. As the goal of<br>the decoder is to generate the segmentation results in the<br>original 2D image space (H x W), we need to reshape<br>the encoder's features (that are used in the decoder), Z,<br>from a 2D shape of HW C to a standard 3D feature map<br>x<br>256<br>H W briefly describe the three decoders.<br>x x C. Next, we<br>16 16</p>",
      "id": 42,
      "page": 4,
      "text": "To evaluate the effectiveness of SETR's encoder feature\nrepresentations Z, we introduce three different decoder de-\nsigns to perform pixel-level segmentation. As the goal of\nthe decoder is to generate the segmentation results in the\noriginal 2D image space (H x W), we need to reshape\nthe encoder's features (that are used in the decoder), Z,\nfrom a 2D shape of HW C to a standard 3D feature map\nx\n256\nH W briefly describe the three decoders.\nx x C. Next, we\n16 16"
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 1537
        },
        {
          "x": 2277,
          "y": 1537
        },
        {
          "x": 2277,
          "y": 1984
        },
        {
          "x": 1280,
          "y": 1984
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='43' style='font-size:16px'>(1) Naive upsampling (Naive) This naive decoder first<br>projects the transformer feature ZLe to the dimension of<br>category number (e.g., 19 for experiments on Cityscapes).<br>For this we adopt a simple 2-layer network with architec-<br>ture: 1 x 1 conv + sync batch norm (w/ ReLU) + 1 x 1<br>conv. After that, we simply bilinearly upsample the out-<br>put to the full image resolution, followed by a classification<br>layer with pixel-wise cross-entropy loss. When this decoder<br>is used, we denote our model as SETR-Naive.</p>",
      "id": 43,
      "page": 4,
      "text": "(1) Naive upsampling (Naive) This naive decoder first\nprojects the transformer feature ZLe to the dimension of\ncategory number (e.g., 19 for experiments on Cityscapes).\nFor this we adopt a simple 2-layer network with architec-\nture: 1 x 1 conv + sync batch norm (w/ ReLU) + 1 x 1\nconv. After that, we simply bilinearly upsample the out-\nput to the full image resolution, followed by a classification\nlayer with pixel-wise cross-entropy loss. When this decoder\nis used, we denote our model as SETR-Naive."
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 2005
        },
        {
          "x": 2277,
          "y": 2005
        },
        {
          "x": 2277,
          "y": 2452
        },
        {
          "x": 1279,
          "y": 2452
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='44' style='font-size:16px'>(2) Progressive UPsampling (PUP) Instead of one-step<br>upscaling which may introduce noisy predictions, we con-<br>sider a progressive upsampling strategy that alternates conv<br>layers and upsampling operations. To maximally mitigate<br>the adversarial effect, we restrict upsampling to 2x. Hence,<br>a total of 4 operations are needed for reaching the full res-<br>H W<br>olution from ZLe with size x More details of this<br>16 16・<br>process are given in Figure 1(b). When using this decoder,<br>we denote our model as SETR-PUP.</p>",
      "id": 44,
      "page": 4,
      "text": "(2) Progressive UPsampling (PUP) Instead of one-step\nupscaling which may introduce noisy predictions, we con-\nsider a progressive upsampling strategy that alternates conv\nlayers and upsampling operations. To maximally mitigate\nthe adversarial effect, we restrict upsampling to 2x. Hence,\na total of 4 operations are needed for reaching the full res-\nH W\nolution from ZLe with size x More details of this\n16 16・\nprocess are given in Figure 1(b). When using this decoder,\nwe denote our model as SETR-PUP."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 2473
        },
        {
          "x": 2277,
          "y": 2473
        },
        {
          "x": 2277,
          "y": 2769
        },
        {
          "x": 1280,
          "y": 2769
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='45' style='font-size:20px'>(3) Multi-Level feature Aggregation (MLA) The third<br>design is characterized by multi-level feature aggregation<br>(Figure 1(c)) in similar spirit of feature pyramid network<br>[27, 33]. However, our decoder is fundamentally differ-<br>ent because the feature representations Zl of every SETR's<br>layer share the same resolution without a pyramid shape.</p>",
      "id": 45,
      "page": 4,
      "text": "(3) Multi-Level feature Aggregation (MLA) The third\ndesign is characterized by multi-level feature aggregation\n(Figure 1(c)) in similar spirit of feature pyramid network\n[27, 33]. However, our decoder is fundamentally differ-\nent because the feature representations Zl of every SETR's\nlayer share the same resolution without a pyramid shape."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 2774
        },
        {
          "x": 2278,
          "y": 2774
        },
        {
          "x": 2278,
          "y": 2977
        },
        {
          "x": 1280,
          "y": 2977
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='46' style='font-size:16px'>Specifically, we take as input the feature representations<br>{Zm} (m E { LIM , LIM , , [음}) from M layers uni-<br>2<br>· · · M<br>formly distributed across the layers with step LIM to the de-<br>coder. M streams are then deployed, with each focusing on</p>",
      "id": 46,
      "page": 4,
      "text": "Specifically, we take as input the feature representations\n{Zm} (m E { LIM , LIM , , [음}) from M layers uni-\n2\n· · · M\nformly distributed across the layers with step LIM to the de-\ncoder. M streams are then deployed, with each focusing on"
    },
    {
      "bounding_box": [
        {
          "x": 1226,
          "y": 3056
        },
        {
          "x": 1249,
          "y": 3056
        },
        {
          "x": 1249,
          "y": 3089
        },
        {
          "x": 1226,
          "y": 3089
        }
      ],
      "category": "footer",
      "html": "<footer id='47' style='font-size:16px'>4</footer>",
      "id": 47,
      "page": 4,
      "text": "4"
    },
    {
      "bounding_box": [
        {
          "x": 306,
          "y": 296
        },
        {
          "x": 1099,
          "y": 296
        },
        {
          "x": 1099,
          "y": 461
        },
        {
          "x": 306,
          "y": 461
        }
      ],
      "category": "table",
      "html": "<table id='48' style='font-size:18px'><tr><td>Model</td><td>T-layers</td><td>Hidden size</td><td>Att head</td></tr><tr><td>T-Base</td><td>12</td><td>768</td><td>12</td></tr><tr><td>T-Large</td><td>24</td><td>1024</td><td>16</td></tr></table>",
      "id": 48,
      "page": 5,
      "text": "Model T-layers Hidden size Att head\n T-Base 12 768 12\n T-Large 24 1024"
    },
    {
      "bounding_box": [
        {
          "x": 266,
          "y": 453
        },
        {
          "x": 1131,
          "y": 453
        },
        {
          "x": 1131,
          "y": 491
        },
        {
          "x": 266,
          "y": 491
        }
      ],
      "category": "caption",
      "html": "<br><caption id='49' style='font-size:14px'>Table 1. Configuration of Transformer backbone variants.</caption>",
      "id": 49,
      "page": 5,
      "text": "Table 1. Configuration of Transformer backbone variants."
    },
    {
      "bounding_box": [
        {
          "x": 205,
          "y": 543
        },
        {
          "x": 1199,
          "y": 543
        },
        {
          "x": 1199,
          "y": 1291
        },
        {
          "x": 205,
          "y": 1291
        }
      ],
      "category": "table",
      "html": "<table id='50' style='font-size:14px'><tr><td>Method</td><td>Pre</td><td>Backbone</td><td>#Params</td><td>40k</td><td>80k</td></tr><tr><td>FCN [39]</td><td>1K</td><td>R-101</td><td>68.59M</td><td>73.93</td><td>75.52</td></tr><tr><td>Semantic FPN [39]</td><td>1K</td><td>R-101</td><td>47.51M</td><td>-</td><td>75.80</td></tr><tr><td>Hybrid-Base</td><td>R</td><td>T-Base</td><td>112.59M</td><td>74.48</td><td>77.36</td></tr><tr><td>Hybrid-Base</td><td>21K</td><td>T-Base</td><td>112.59M</td><td>76.76</td><td>76.57</td></tr><tr><td>Hybrid-DeiT</td><td>21K</td><td>T-Base</td><td>112.59M</td><td>77.42</td><td>78.28</td></tr><tr><td>SETR-Naive</td><td>21K</td><td>T-Large</td><td>305.67M</td><td>77.37</td><td>77.90</td></tr><tr><td>SETR-MLA</td><td>21K</td><td>T-Large</td><td>310.57M</td><td>76.65</td><td>77.24</td></tr><tr><td>SETR-PUP</td><td>21K</td><td>T-Large</td><td>318.31M</td><td>78.39</td><td>79.34</td></tr><tr><td>SETR-PUP</td><td>R</td><td>T-Large</td><td>318.31M</td><td>42.27</td><td>-</td></tr><tr><td>SETR-Naive-Base</td><td>21K</td><td>T-Base</td><td>87.69M</td><td>75.54</td><td>76.25</td></tr><tr><td>SETR-MLA-Base</td><td>21K</td><td>T-Base</td><td>92.59M</td><td>75.60</td><td>76.87</td></tr><tr><td>SETR-PUP-Base</td><td>21K</td><td>T-Base</td><td>97.64M</td><td>76.71</td><td>78.02</td></tr><tr><td>SETR-Naive-DeiT</td><td>1K</td><td>T-Base</td><td>87.69M</td><td>77.85</td><td>78.66</td></tr><tr><td>SETR-MLA-DeiT</td><td>1K</td><td>T-Base</td><td>92.59M</td><td>78.04</td><td>78.98</td></tr><tr><td>SETR-PUP-DeiT</td><td>1K</td><td>T-Base</td><td>97.64M</td><td>78.79</td><td>79.45</td></tr></table>",
      "id": 50,
      "page": 5,
      "text": "Method Pre Backbone #Params 40k 80k\n FCN [39] 1K R-101 68.59M 73.93 75.52\n Semantic FPN [39] 1K R-101 47.51M - 75.80\n Hybrid-Base R T-Base 112.59M 74.48 77.36\n Hybrid-Base 21K T-Base 112.59M 76.76 76.57\n Hybrid-DeiT 21K T-Base 112.59M 77.42 78.28\n SETR-Naive 21K T-Large 305.67M 77.37 77.90\n SETR-MLA 21K T-Large 310.57M 76.65 77.24\n SETR-PUP 21K T-Large 318.31M 78.39 79.34\n SETR-PUP R T-Large 318.31M 42.27 -\n SETR-Naive-Base 21K T-Base 87.69M 75.54 76.25\n SETR-MLA-Base 21K T-Base 92.59M 75.60 76.87\n SETR-PUP-Base 21K T-Base 97.64M 76.71 78.02\n SETR-Naive-DeiT 1K T-Base 87.69M 77.85 78.66\n SETR-MLA-DeiT 1K T-Base 92.59M 78.04 78.98\n SETR-PUP-DeiT 1K T-Base 97.64M 78.79"
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 1263
        },
        {
          "x": 1200,
          "y": 1263
        },
        {
          "x": 1200,
          "y": 1555
        },
        {
          "x": 203,
          "y": 1555
        }
      ],
      "category": "caption",
      "html": "<br><caption id='51' style='font-size:14px'>Table 2. Comparing SETR variants on different pre-training<br>strategies and backbones. All experiments are trained on<br>Cityscapes train fine set with batch size 8, and evaluated using the<br>single scale test protocol on the Cityscapes validation set in mean<br>IoU (%) rate. \"Pre\" denotes the pre-training of transformer part.<br>\"R\" means the transformer part is randomly initialized.</caption>",
      "id": 51,
      "page": 5,
      "text": "Table 2. Comparing SETR variants on different pre-training\nstrategies and backbones. All experiments are trained on\nCityscapes train fine set with batch size 8, and evaluated using the\nsingle scale test protocol on the Cityscapes validation set in mean\nIoU (%) rate. \"Pre\" denotes the pre-training of transformer part.\n\"R\" means the transformer part is randomly initialized."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 1645
        },
        {
          "x": 1200,
          "y": 1645
        },
        {
          "x": 1200,
          "y": 2346
        },
        {
          "x": 201,
          "y": 2346
        }
      ],
      "category": "paragraph",
      "html": "<p id='52' style='font-size:16px'>one specific selected layer. In each stream, we first reshape<br>the encoder's feature Zl from a 2D shape of HW C to a<br>x<br>256<br>H W 3-layer (kernel size 1 x 1,<br>3D feature map 16 16<br>x x C. A<br>3 x 3, and 3 x 3) network is applied with the feature chan-<br>nels halved at the first and third layers respectively, and the<br>spatial resolution upscaled 4x by bilinear operation after<br>the third layer. To enhance the interactions across differ-<br>ent streams, we introduce a top-down aggregation design<br>via element-wise addition after the first layer. An additional<br>3 x 3 conv is applied after the element-wise additioned fea-<br>ture. After the third layer, we obtain the fused feature from<br>all the streams via channel-wise concatenation which is then<br>bilinearly upsampled 4x to the full resolution. When using<br>this decoder, we denote our model as SETR-MLA.</p>",
      "id": 52,
      "page": 5,
      "text": "one specific selected layer. In each stream, we first reshape\nthe encoder's feature Zl from a 2D shape of HW C to a\nx\n256\nH W 3-layer (kernel size 1 x 1,\n3D feature map 16 16\nx x C. A\n3 x 3, and 3 x 3) network is applied with the feature chan-\nnels halved at the first and third layers respectively, and the\nspatial resolution upscaled 4x by bilinear operation after\nthe third layer. To enhance the interactions across differ-\nent streams, we introduce a top-down aggregation design\nvia element-wise addition after the first layer. An additional\n3 x 3 conv is applied after the element-wise additioned fea-\nture. After the third layer, we obtain the fused feature from\nall the streams via channel-wise concatenation which is then\nbilinearly upsampled 4x to the full resolution. When using\nthis decoder, we denote our model as SETR-MLA."
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 2393
        },
        {
          "x": 533,
          "y": 2393
        },
        {
          "x": 533,
          "y": 2442
        },
        {
          "x": 204,
          "y": 2442
        }
      ],
      "category": "paragraph",
      "html": "<p id='53' style='font-size:22px'>4. Experiments</p>",
      "id": 53,
      "page": 5,
      "text": "4. Experiments"
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 2475
        },
        {
          "x": 674,
          "y": 2475
        },
        {
          "x": 674,
          "y": 2525
        },
        {
          "x": 204,
          "y": 2525
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='54' style='font-size:20px'>4.1. Experimental setup</p>",
      "id": 54,
      "page": 5,
      "text": "4.1. Experimental setup"
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 2554
        },
        {
          "x": 1198,
          "y": 2554
        },
        {
          "x": 1198,
          "y": 2648
        },
        {
          "x": 203,
          "y": 2648
        }
      ],
      "category": "paragraph",
      "html": "<p id='55' style='font-size:18px'>We conduct experiments on three widely-used semantic<br>segmentation benchmark datasets.</p>",
      "id": 55,
      "page": 5,
      "text": "We conduct experiments on three widely-used semantic\nsegmentation benchmark datasets."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 2676
        },
        {
          "x": 1199,
          "y": 2676
        },
        {
          "x": 1199,
          "y": 2976
        },
        {
          "x": 202,
          "y": 2976
        }
      ],
      "category": "paragraph",
      "html": "<p id='56' style='font-size:18px'>Cityscapes [13] densely annotates 19 object categories in<br>images with urban scenes. It contains 5000 finely annotated<br>images, split into 2975, 500 and 1525 for training, valida-<br>tion and testing respectively. The images are all captured at<br>a high resolution of 2048 x 1024. In addition, it provides<br>19,998 coarse annotated images for model training.</p>",
      "id": 56,
      "page": 5,
      "text": "Cityscapes [13] densely annotates 19 object categories in\nimages with urban scenes. It contains 5000 finely annotated\nimages, split into 2975, 500 and 1525 for training, valida-\ntion and testing respectively. The images are all captured at\na high resolution of 2048 x 1024. In addition, it provides\n19,998 coarse annotated images for model training."
    },
    {
      "bounding_box": [
        {
          "x": 1289,
          "y": 308
        },
        {
          "x": 2267,
          "y": 308
        },
        {
          "x": 2267,
          "y": 662
        },
        {
          "x": 1289,
          "y": 662
        }
      ],
      "category": "table",
      "html": "<br><table id='57' style='font-size:18px'><tr><td>Method</td><td>Pre</td><td>Backbone</td><td>ADE20K</td><td>Cityscapes</td></tr><tr><td>FCN [39]</td><td>1K</td><td>R-101</td><td>39.91</td><td>73.93</td></tr><tr><td>FCN</td><td>21K</td><td>R-101</td><td>42.17</td><td>76.38</td></tr><tr><td>SETR-MLA</td><td>21K</td><td>T-Large</td><td>48.64</td><td>76.65</td></tr><tr><td>SETR-PUP</td><td>21K</td><td>T-Large</td><td>48.58</td><td>78.39</td></tr><tr><td>SETR-MLA-DeiT</td><td>1K</td><td>T-Large</td><td>46.15</td><td>78.98</td></tr><tr><td>SETR-PUP-DeiT</td><td>1K</td><td>T-Large</td><td>46.24</td><td>79.45</td></tr></table>",
      "id": 57,
      "page": 5,
      "text": "Method Pre Backbone ADE20K Cityscapes\n FCN [39] 1K R-101 39.91 73.93\n FCN 21K R-101 42.17 76.38\n SETR-MLA 21K T-Large 48.64 76.65\n SETR-PUP 21K T-Large 48.58 78.39\n SETR-MLA-DeiT 1K T-Large 46.15 78.98\n SETR-PUP-DeiT 1K T-Large 46.24"
    },
    {
      "bounding_box": [
        {
          "x": 1283,
          "y": 657
        },
        {
          "x": 2273,
          "y": 657
        },
        {
          "x": 2273,
          "y": 741
        },
        {
          "x": 1283,
          "y": 741
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='58' style='font-size:14px'>Table 3. Comparison to FCN with different pre-training with<br>single-scale inference on the ADE20K val and Cityscapes val set.</p>",
      "id": 58,
      "page": 5,
      "text": "Table 3. Comparison to FCN with different pre-training with\nsingle-scale inference on the ADE20K val and Cityscapes val set."
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 835
        },
        {
          "x": 2274,
          "y": 835
        },
        {
          "x": 2274,
          "y": 983
        },
        {
          "x": 1281,
          "y": 983
        }
      ],
      "category": "paragraph",
      "html": "<p id='59' style='font-size:18px'>ADE20K [63] is a challenging scene parsing benchmark<br>with 150 fine-grained semantic concepts. It contains 20210,<br>2000 and 3352 images for training, validation and testing.</p>",
      "id": 59,
      "page": 5,
      "text": "ADE20K [63] is a challenging scene parsing benchmark\nwith 150 fine-grained semantic concepts. It contains 20210,\n2000 and 3352 images for training, validation and testing."
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 1022
        },
        {
          "x": 2276,
          "y": 1022
        },
        {
          "x": 2276,
          "y": 1317
        },
        {
          "x": 1279,
          "y": 1317
        }
      ],
      "category": "paragraph",
      "html": "<p id='60' style='font-size:18px'>PASCAL Context [37] provides pixel-wise semantic la-<br>bels for the whole scene (both \"thing\" and \"stuff\" classes),<br>and contains 4998 and 5105 images for training and valida-<br>tion respectively. Following previous works, we evaluate on<br>the most frequent 59 classes and the background class (60<br>classes in total).</p>",
      "id": 60,
      "page": 5,
      "text": "PASCAL Context [37] provides pixel-wise semantic la-\nbels for the whole scene (both \"thing\" and \"stuff\" classes),\nand contains 4998 and 5105 images for training and valida-\ntion respectively. Following previous works, we evaluate on\nthe most frequent 59 classes and the background class (60\nclasses in total)."
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 1355
        },
        {
          "x": 2277,
          "y": 1355
        },
        {
          "x": 2277,
          "y": 2154
        },
        {
          "x": 1278,
          "y": 2154
        }
      ],
      "category": "paragraph",
      "html": "<p id='61' style='font-size:18px'>Implementation details Following the default setting<br>(e.g., data augmentation and training schedule) of public<br>codebase mmsegmentation [39], (i) we apply random resize<br>with ratio between 0.5 and 2, random cropping (768, 512<br>and 480 for Cityscapes, ADE20K and Pascal Context re-<br>spectively) and random horizontal flipping during training<br>for all the experiments; (ii) We set batch size 16 and the to-<br>tal iteration to 160,000 and 80,000 for the experiments on<br>ADE20K and Pascal Context. For Cityscapes, we set batch<br>size to 8 with a number of training schedules reported in Ta-<br>ble 2, 6 and 7 for fair comparison. We adopt a polynomial<br>learning rate decay schedule [60] and employ SGD as the<br>optimizer. Momentum and weight decay are set to 0.9 and<br>0 respectively for all the experiments on the three datasets.<br>We set initial learning rate 0.001 on ADE20K and Pascal<br>Context, and 0.01 on Cityscapes.</p>",
      "id": 61,
      "page": 5,
      "text": "Implementation details Following the default setting\n(e.g., data augmentation and training schedule) of public\ncodebase mmsegmentation [39], (i) we apply random resize\nwith ratio between 0.5 and 2, random cropping (768, 512\nand 480 for Cityscapes, ADE20K and Pascal Context re-\nspectively) and random horizontal flipping during training\nfor all the experiments; (ii) We set batch size 16 and the to-\ntal iteration to 160,000 and 80,000 for the experiments on\nADE20K and Pascal Context. For Cityscapes, we set batch\nsize to 8 with a number of training schedules reported in Ta-\nble 2, 6 and 7 for fair comparison. We adopt a polynomial\nlearning rate decay schedule [60] and employ SGD as the\noptimizer. Momentum and weight decay are set to 0.9 and\n0 respectively for all the experiments on the three datasets.\nWe set initial learning rate 0.001 on ADE20K and Pascal\nContext, and 0.01 on Cityscapes."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 2192
        },
        {
          "x": 2277,
          "y": 2192
        },
        {
          "x": 2277,
          "y": 2541
        },
        {
          "x": 1280,
          "y": 2541
        }
      ],
      "category": "paragraph",
      "html": "<p id='62' style='font-size:16px'>Auxiliary loss As [60] we also find the auxiliary seg-<br>mentation loss helps the model training. Each aux-<br>iliary loss head follows a 2-layer network. We add<br>auxiliary losses at different Transformer layers: SETR-<br>Naive (Z10 , Z15 , Z20), SETR-PUP (Z10 , Z15 Z20, Z24),<br>,<br>SETR-MLA (Z6 , Z12 Z18 Z24). Both auxiliary loss and<br>,<br>,<br>main loss heads are applied concurrently.</p>",
      "id": 62,
      "page": 5,
      "text": "Auxiliary loss As [60] we also find the auxiliary seg-\nmentation loss helps the model training. Each aux-\niliary loss head follows a 2-layer network. We add\nauxiliary losses at different Transformer layers: SETR-\nNaive (Z10 , Z15 , Z20), SETR-PUP (Z10 , Z15 Z20, Z24),\n,\nSETR-MLA (Z6 , Z12 Z18 Z24). Both auxiliary loss and\n,\n,\nmain loss heads are applied concurrently."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 2578
        },
        {
          "x": 2277,
          "y": 2578
        },
        {
          "x": 2277,
          "y": 2976
        },
        {
          "x": 1280,
          "y": 2976
        }
      ],
      "category": "paragraph",
      "html": "<p id='63' style='font-size:18px'>Multi-scale test We use the default settings of mmsegmen-<br>tation [39]. Specifically, the input image is first scaled to<br>a uniform size. Multi-scale scaling and random horizontal<br>flip are then performed on the image with a scaling factor<br>(0.5, 0.75, 1.0, 1.25, 1.5, 1.75). Sliding window is adopted<br>for test (e.g., 480 x 480 for Pascal Context). If the shorter<br>side is smaller than the size of the sliding window, the im-<br>age is scaled with its shorter side to the size of the sliding</p>",
      "id": 63,
      "page": 5,
      "text": "Multi-scale test We use the default settings of mmsegmen-\ntation [39]. Specifically, the input image is first scaled to\na uniform size. Multi-scale scaling and random horizontal\nflip are then performed on the image with a scaling factor\n(0.5, 0.75, 1.0, 1.25, 1.5, 1.75). Sliding window is adopted\nfor test (e.g., 480 x 480 for Pascal Context). If the shorter\nside is smaller than the size of the sliding window, the im-\nage is scaled with its shorter side to the size of the sliding"
    },
    {
      "bounding_box": [
        {
          "x": 1226,
          "y": 3055
        },
        {
          "x": 1251,
          "y": 3055
        },
        {
          "x": 1251,
          "y": 3091
        },
        {
          "x": 1226,
          "y": 3091
        }
      ],
      "category": "footer",
      "html": "<footer id='64' style='font-size:14px'>5</footer>",
      "id": 64,
      "page": 5,
      "text": "5"
    },
    {
      "bounding_box": [
        {
          "x": 207,
          "y": 297
        },
        {
          "x": 1196,
          "y": 297
        },
        {
          "x": 1196,
          "y": 671
        },
        {
          "x": 207,
          "y": 671
        }
      ],
      "category": "figure",
      "html": "<figure><img id='65' alt=\"\" data-coord=\"top-left:(207,297); bottom-right:(1196,671)\" /></figure>",
      "id": 65,
      "page": 6,
      "text": ""
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 688
        },
        {
          "x": 1197,
          "y": 688
        },
        {
          "x": 1197,
          "y": 819
        },
        {
          "x": 204,
          "y": 819
        }
      ],
      "category": "caption",
      "html": "<br><caption id='66' style='font-size:16px'>Figure 2. Qualitative results on ADE20K: SETR (right column)<br>VS. dilated FCN baseline (left column) in each pair. Best viewed<br>in color and zoom in.</caption>",
      "id": 66,
      "page": 6,
      "text": "Figure 2. Qualitative results on ADE20K: SETR (right column)\nVS. dilated FCN baseline (left column) in each pair. Best viewed\nin color and zoom in."
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 852
        },
        {
          "x": 1219,
          "y": 852
        },
        {
          "x": 1219,
          "y": 1656
        },
        {
          "x": 204,
          "y": 1656
        }
      ],
      "category": "table",
      "html": "<table id='67' style='font-size:14px'><tr><td>Method</td><td>Pre</td><td>Backbone</td><td>#Params</td><td>mIoU</td></tr><tr><td>FCN (160k, SS) [39]</td><td>1K</td><td>ResNet-101</td><td>68.59M</td><td>39.91</td></tr><tr><td>FCN (160k, MS) [39]</td><td>1K</td><td>ResNet-101</td><td>68.59M</td><td>41.40</td></tr><tr><td>CCNet [25]</td><td>1K</td><td>ResNet-101</td><td>-</td><td>45.22</td></tr><tr><td>Strip pooling [23]</td><td>1K</td><td>ResNet-101</td><td>-</td><td>45.60</td></tr><tr><td>DANet [18]</td><td>1K</td><td>ResNet-101</td><td>69.0M</td><td>45.30</td></tr><tr><td>OCRNet [54]</td><td>1K</td><td>ResNet-101</td><td>71.0M</td><td>45.70</td></tr><tr><td>UperNet [49]</td><td>1K</td><td>ResNet-101</td><td>86.0M</td><td>44.90</td></tr><tr><td>Deeplab V3+ [11]</td><td>1K</td><td>ResNet-101</td><td>63.0M</td><td>46.40</td></tr><tr><td>SETR-Naive (160k, SS)</td><td>21K</td><td>T-Large</td><td>305.67M</td><td>48.06</td></tr><tr><td>SETR-Naive (160k, MS)</td><td>21K</td><td>T-Large</td><td>305.67M</td><td>48.80</td></tr><tr><td>SETR-PUP (160k, SS)</td><td>21K</td><td>T-Large</td><td>318.31M</td><td>48.58</td></tr><tr><td>SETR-PUP (160k, MS)</td><td>21K</td><td>T-Large</td><td>318.31M</td><td>50.09</td></tr><tr><td>SETR-MLA (160k, SS)</td><td>21K</td><td>T-Large</td><td>310.57M</td><td>48.64</td></tr><tr><td>SETR-MLA (160k, MS)</td><td>21K</td><td>T-Large</td><td>310.57M</td><td>50.28</td></tr><tr><td>SETR-PUP-DeiT (160k, SS)</td><td>1K</td><td>T-Base</td><td>97.64M</td><td>46.34</td></tr><tr><td>SETR-PUP-DeiT (160k, MS)</td><td>1K</td><td>T-Base</td><td>97.64M</td><td>47.30</td></tr><tr><td>SETR-MLA-DeiT (160k, SS)</td><td>1K</td><td>T-Base</td><td>92.59M</td><td>46.15</td></tr><tr><td>SETR-MLA-DeiT (160k, MS)</td><td>1K</td><td>T-Base</td><td>92.59M</td><td>47.71</td></tr></table>",
      "id": 67,
      "page": 6,
      "text": "Method Pre Backbone #Params mIoU\n FCN (160k, SS) [39] 1K ResNet-101 68.59M 39.91\n FCN (160k, MS) [39] 1K ResNet-101 68.59M 41.40\n CCNet [25] 1K ResNet-101 - 45.22\n Strip pooling [23] 1K ResNet-101 - 45.60\n DANet [18] 1K ResNet-101 69.0M 45.30\n OCRNet [54] 1K ResNet-101 71.0M 45.70\n UperNet [49] 1K ResNet-101 86.0M 44.90\n Deeplab V3+ [11] 1K ResNet-101 63.0M 46.40\n SETR-Naive (160k, SS) 21K T-Large 305.67M 48.06\n SETR-Naive (160k, MS) 21K T-Large 305.67M 48.80\n SETR-PUP (160k, SS) 21K T-Large 318.31M 48.58\n SETR-PUP (160k, MS) 21K T-Large 318.31M 50.09\n SETR-MLA (160k, SS) 21K T-Large 310.57M 48.64\n SETR-MLA (160k, MS) 21K T-Large 310.57M 50.28\n SETR-PUP-DeiT (160k, SS) 1K T-Base 97.64M 46.34\n SETR-PUP-DeiT (160k, MS) 1K T-Base 97.64M 47.30\n SETR-MLA-DeiT (160k, SS) 1K T-Base 92.59M 46.15\n SETR-MLA-DeiT (160k, MS) 1K T-Base 92.59M"
    },
    {
      "bounding_box": [
        {
          "x": 206,
          "y": 1650
        },
        {
          "x": 1196,
          "y": 1650
        },
        {
          "x": 1196,
          "y": 1778
        },
        {
          "x": 206,
          "y": 1778
        }
      ],
      "category": "caption",
      "html": "<br><caption id='68' style='font-size:16px'>Table 4. State-of-the-art comparison on the ADE20K dataset.<br>Performances of different model variants are reported. SS: Single-<br>scale inference. MS: Multi-scale inference.</caption>",
      "id": 68,
      "page": 6,
      "text": "Table 4. State-of-the-art comparison on the ADE20K dataset.\nPerformances of different model variants are reported. SS: Single-\nscale inference. MS: Multi-scale inference."
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 1866
        },
        {
          "x": 1197,
          "y": 1866
        },
        {
          "x": 1197,
          "y": 2060
        },
        {
          "x": 203,
          "y": 2060
        }
      ],
      "category": "paragraph",
      "html": "<p id='69' style='font-size:22px'>window (e.g., 480) while keeping the aspect ratio. Synchro-<br>nized BN is used in decoder and auxiliary loss heads. For<br>training simplicity, we do not adopt the widely-used tricks<br>such as OHEM [55] loss in model training.</p>",
      "id": 69,
      "page": 6,
      "text": "window (e.g., 480) while keeping the aspect ratio. Synchro-\nnized BN is used in decoder and auxiliary loss heads. For\ntraining simplicity, we do not adopt the widely-used tricks\nsuch as OHEM [55] loss in model training."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 2095
        },
        {
          "x": 1198,
          "y": 2095
        },
        {
          "x": 1198,
          "y": 2441
        },
        {
          "x": 201,
          "y": 2441
        }
      ],
      "category": "paragraph",
      "html": "<p id='70' style='font-size:18px'>Baselines We adopt dilated FCN [36] and Semantic<br>FPN [27] as baselines with their results taken from [39].<br>Our models and the baselines are trained and tested in the<br>same settings for fair comparison. In addition, state-of-the-<br>art models are also compared. Note that the dilated FCN is<br>with output stride 8 and we use output stride 16 in all our<br>models due to GPU memory constrain.</p>",
      "id": 70,
      "page": 6,
      "text": "Baselines We adopt dilated FCN [36] and Semantic\nFPN [27] as baselines with their results taken from [39].\nOur models and the baselines are trained and tested in the\nsame settings for fair comparison. In addition, state-of-the-\nart models are also compared. Note that the dilated FCN is\nwith output stride 8 and we use output stride 16 in all our\nmodels due to GPU memory constrain."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 2477
        },
        {
          "x": 1198,
          "y": 2477
        },
        {
          "x": 1198,
          "y": 2870
        },
        {
          "x": 202,
          "y": 2870
        }
      ],
      "category": "paragraph",
      "html": "<p id='71' style='font-size:18px'>SETR variants Three variants of our model with differ-<br>ent decoder designs (see Sec. 3.3), namely SETR-Naive,<br>SETR-PUP and SETR-MLA. Besides, we use two vari-<br>ants of the encoder \"T-Base\" and \"T-Large\" with 12 and 24<br>layers respectively (Table 1). Unless otherwise specified,<br>we use \"T-Large\" as the encoder for SETR-Naive, SETR-<br>PUP and SETR-MLA. We denote SETR-Naive-Base as the<br>model utilizing \"T-Base\" in SETR-Naive.</p>",
      "id": 71,
      "page": 6,
      "text": "SETR variants Three variants of our model with differ-\nent decoder designs (see Sec. 3.3), namely SETR-Naive,\nSETR-PUP and SETR-MLA. Besides, we use two vari-\nants of the encoder \"T-Base\" and \"T-Large\" with 12 and 24\nlayers respectively (Table 1). Unless otherwise specified,\nwe use \"T-Large\" as the encoder for SETR-Naive, SETR-\nPUP and SETR-MLA. We denote SETR-Naive-Base as the\nmodel utilizing \"T-Base\" in SETR-Naive."
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 2880
        },
        {
          "x": 1197,
          "y": 2880
        },
        {
          "x": 1197,
          "y": 2976
        },
        {
          "x": 203,
          "y": 2976
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='72' style='font-size:18px'>Though designed as a model with a pure transformer<br>encoder, we also set a hybrid baseline Hybrid by using a</p>",
      "id": 72,
      "page": 6,
      "text": "Though designed as a model with a pure transformer\nencoder, we also set a hybrid baseline Hybrid by using a"
    },
    {
      "bounding_box": [
        {
          "x": 1285,
          "y": 294
        },
        {
          "x": 2272,
          "y": 294
        },
        {
          "x": 2272,
          "y": 672
        },
        {
          "x": 1285,
          "y": 672
        }
      ],
      "category": "figure",
      "html": "<br><figure><img id='73' alt=\"\" data-coord=\"top-left:(1285,294); bottom-right:(2272,672)\" /></figure>",
      "id": 73,
      "page": 6,
      "text": ""
    },
    {
      "bounding_box": [
        {
          "x": 1282,
          "y": 687
        },
        {
          "x": 2276,
          "y": 687
        },
        {
          "x": 2276,
          "y": 819
        },
        {
          "x": 1282,
          "y": 819
        }
      ],
      "category": "caption",
      "html": "<br><caption id='74' style='font-size:16px'>Figure 3. Qualitative results on Pascal Context: SETR (right<br>column) VS. dilated FCN baseline (left column) in each pair. Best<br>viewed in color and zoom in.</caption>",
      "id": 74,
      "page": 6,
      "text": "Figure 3. Qualitative results on Pascal Context: SETR (right\ncolumn) VS. dilated FCN baseline (left column) in each pair. Best\nviewed in color and zoom in."
    },
    {
      "bounding_box": [
        {
          "x": 1284,
          "y": 846
        },
        {
          "x": 2291,
          "y": 846
        },
        {
          "x": 2291,
          "y": 1656
        },
        {
          "x": 1284,
          "y": 1656
        }
      ],
      "category": "table",
      "html": "<table id='75' style='font-size:14px'><tr><td>Method</td><td>Pre</td><td>Backbone</td><td>mIoU</td></tr><tr><td>FCN (80k, SS) [39]</td><td>1K</td><td>ResNet-101</td><td>44.47</td></tr><tr><td>FCN (80k, MS) [39]</td><td>1K</td><td>ResNet-101</td><td>45.74</td></tr><tr><td>DANet [18]</td><td>1K</td><td>ResNet-101</td><td>52.60</td></tr><tr><td>EMANet [31]</td><td>1K</td><td>ResNet-101</td><td>53.10</td></tr><tr><td>SVCNet [16]</td><td>1K</td><td>ResNet-101</td><td>53.20</td></tr><tr><td>Strip pooling [23]</td><td>1K</td><td>ResNet-101</td><td>54.50</td></tr><tr><td>GFFNet [30]</td><td>1K</td><td>ResNet-101</td><td>54.20</td></tr><tr><td>APCNet [19]</td><td>1K</td><td>ResNet-101</td><td>54.70</td></tr><tr><td>SETR-Naive (80k, SS)</td><td>21K</td><td>T-Large</td><td>52.89</td></tr><tr><td>SETR-Naive (80k, MS)</td><td>21K</td><td>T-Large</td><td>53.61</td></tr><tr><td>SETR-PUP (80k, SS)</td><td>21K</td><td>T-Large</td><td>54.40</td></tr><tr><td>SETR-PUP (80k, MS)</td><td>21K</td><td>T-Large</td><td>55.27</td></tr><tr><td>SETR-MLA (80k, SS)</td><td>21K</td><td>T-Large</td><td>54.87</td></tr><tr><td>SETR-MLA (80k, MS)</td><td>21K</td><td>T-Large</td><td>55.83</td></tr><tr><td>SETR-PUP-DeiT (80k, SS)</td><td>1K</td><td>T-Base</td><td>52.71</td></tr><tr><td>SETR-PUP-DeiT (80k, MS)</td><td>1K</td><td>T-Base</td><td>53.71</td></tr><tr><td>SETR-MLA-DeiT (80k, SS)</td><td>1K</td><td>T-Base</td><td>52.91</td></tr><tr><td>SETR-MLA-DeiT (80k, MS)</td><td>1K</td><td>T-Base</td><td>53.74</td></tr></table>",
      "id": 75,
      "page": 6,
      "text": "Method Pre Backbone mIoU\n FCN (80k, SS) [39] 1K ResNet-101 44.47\n FCN (80k, MS) [39] 1K ResNet-101 45.74\n DANet [18] 1K ResNet-101 52.60\n EMANet [31] 1K ResNet-101 53.10\n SVCNet [16] 1K ResNet-101 53.20\n Strip pooling [23] 1K ResNet-101 54.50\n GFFNet [30] 1K ResNet-101 54.20\n APCNet [19] 1K ResNet-101 54.70\n SETR-Naive (80k, SS) 21K T-Large 52.89\n SETR-Naive (80k, MS) 21K T-Large 53.61\n SETR-PUP (80k, SS) 21K T-Large 54.40\n SETR-PUP (80k, MS) 21K T-Large 55.27\n SETR-MLA (80k, SS) 21K T-Large 54.87\n SETR-MLA (80k, MS) 21K T-Large 55.83\n SETR-PUP-DeiT (80k, SS) 1K T-Base 52.71\n SETR-PUP-DeiT (80k, MS) 1K T-Base 53.71\n SETR-MLA-DeiT (80k, SS) 1K T-Base 52.91\n SETR-MLA-DeiT (80k, MS) 1K T-Base"
    },
    {
      "bounding_box": [
        {
          "x": 1282,
          "y": 1649
        },
        {
          "x": 2277,
          "y": 1649
        },
        {
          "x": 2277,
          "y": 1777
        },
        {
          "x": 1282,
          "y": 1777
        }
      ],
      "category": "caption",
      "html": "<br><caption id='76' style='font-size:16px'>Table 5. State-of-the-art comparison on the Pascal Context<br>dataset. Performances of different model variants are reported.<br>SS: Single-scale inference. MS: Multi-scale inference.</caption>",
      "id": 76,
      "page": 6,
      "text": "Table 5. State-of-the-art comparison on the Pascal Context\ndataset. Performances of different model variants are reported.\nSS: Single-scale inference. MS: Multi-scale inference."
    },
    {
      "bounding_box": [
        {
          "x": 1282,
          "y": 1867
        },
        {
          "x": 2276,
          "y": 1867
        },
        {
          "x": 2276,
          "y": 2114
        },
        {
          "x": 1282,
          "y": 2114
        }
      ],
      "category": "paragraph",
      "html": "<p id='77' style='font-size:20px'>ResNet-50 based FCN encoder and feeding its output fea-<br>ture into SETR. To cope with the GPU memory constraint<br>and for fair comparison, we only consider 'T-Base\" in Hy-<br>brid and set the output stride of FCN to 1/16. That is, Hy-<br>brid is a combination of ResNet-50 and SETR-Naive-Base.</p>",
      "id": 77,
      "page": 6,
      "text": "ResNet-50 based FCN encoder and feeding its output fea-\nture into SETR. To cope with the GPU memory constraint\nand for fair comparison, we only consider 'T-Base\" in Hy-\nbrid and set the output stride of FCN to 1/16. That is, Hy-\nbrid is a combination of ResNet-50 and SETR-Naive-Base."
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 2146
        },
        {
          "x": 2276,
          "y": 2146
        },
        {
          "x": 2276,
          "y": 2596
        },
        {
          "x": 1279,
          "y": 2596
        }
      ],
      "category": "paragraph",
      "html": "<p id='78' style='font-size:20px'>Pre-training We use the pre-trained weights provided by<br>ViT [17] or DeiT [44] to initialize all the transformer lay-<br>ers and the input linear projection layer in our model. We<br>denote SETR-Naive-DeiT as the model utilizing DeiT [44]<br>pre-training in SETR-Naive-Base. All the layers without<br>pre-training are randomly initialized. For the FCN en-<br>coder of Hybrid, we use the initial weights pre-trained on<br>ImageNet-1k. For the transformer part, we use the weights<br>pre-trained by ViT [17], DeiT [44] or randomly initialized.</p>",
      "id": 78,
      "page": 6,
      "text": "Pre-training We use the pre-trained weights provided by\nViT [17] or DeiT [44] to initialize all the transformer lay-\ners and the input linear projection layer in our model. We\ndenote SETR-Naive-DeiT as the model utilizing DeiT [44]\npre-training in SETR-Naive-Base. All the layers without\npre-training are randomly initialized. For the FCN en-\ncoder of Hybrid, we use the initial weights pre-trained on\nImageNet-1k. For the transformer part, we use the weights\npre-trained by ViT [17], DeiT [44] or randomly initialized."
    },
    {
      "bounding_box": [
        {
          "x": 1282,
          "y": 2599
        },
        {
          "x": 2277,
          "y": 2599
        },
        {
          "x": 2277,
          "y": 2795
        },
        {
          "x": 1282,
          "y": 2795
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='79' style='font-size:20px'>We use patch size 16 x 16 for all the experiments. We<br>perform 2D interpolation on the pre-trained position em-<br>beddings, according to their location in the original image<br>for different input size fine-tuning.</p>",
      "id": 79,
      "page": 6,
      "text": "We use patch size 16 x 16 for all the experiments. We\nperform 2D interpolation on the pre-trained position em-\nbeddings, according to their location in the original image\nfor different input size fine-tuning."
    },
    {
      "bounding_box": [
        {
          "x": 1282,
          "y": 2828
        },
        {
          "x": 2275,
          "y": 2828
        },
        {
          "x": 2275,
          "y": 2976
        },
        {
          "x": 1282,
          "y": 2976
        }
      ],
      "category": "paragraph",
      "html": "<p id='80' style='font-size:18px'>Evaluation metric Following the standard evaluation pro-<br>tocol [13], the metric of mean Intersection over Union<br>(mIoU) averaged over all classes is reported. For ADE20K,</p>",
      "id": 80,
      "page": 6,
      "text": "Evaluation metric Following the standard evaluation pro-\ntocol [13], the metric of mean Intersection over Union\n(mIoU) averaged over all classes is reported. For ADE20K,"
    },
    {
      "bounding_box": [
        {
          "x": 1226,
          "y": 3057
        },
        {
          "x": 1253,
          "y": 3057
        },
        {
          "x": 1253,
          "y": 3092
        },
        {
          "x": 1226,
          "y": 3092
        }
      ],
      "category": "footer",
      "html": "<footer id='81' style='font-size:16px'>6</footer>",
      "id": 81,
      "page": 6,
      "text": "6"
    },
    {
      "bounding_box": [
        {
          "x": 210,
          "y": 297
        },
        {
          "x": 2268,
          "y": 297
        },
        {
          "x": 2268,
          "y": 1087
        },
        {
          "x": 210,
          "y": 1087
        }
      ],
      "category": "figure",
      "html": "<figure><img id='82' alt=\"\" data-coord=\"top-left:(210,297); bottom-right:(2268,1087)\" /></figure>",
      "id": 82,
      "page": 7,
      "text": ""
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 1104
        },
        {
          "x": 2275,
          "y": 1104
        },
        {
          "x": 2275,
          "y": 1189
        },
        {
          "x": 201,
          "y": 1189
        }
      ],
      "category": "caption",
      "html": "<br><caption id='83' style='font-size:16px'>Figure 4. Qualitative results on Cityscapes: SETR (right column) VS. dilated FCN baseline (left column) in each pair. Best viewed in<br>color and zoom in.</caption>",
      "id": 83,
      "page": 7,
      "text": "Figure 4. Qualitative results on Cityscapes: SETR (right column) VS. dilated FCN baseline (left column) in each pair. Best viewed in\ncolor and zoom in."
    },
    {
      "bounding_box": [
        {
          "x": 246,
          "y": 1224
        },
        {
          "x": 1176,
          "y": 1224
        },
        {
          "x": 1176,
          "y": 1898
        },
        {
          "x": 246,
          "y": 1898
        }
      ],
      "category": "table",
      "html": "<table id='84' style='font-size:14px'><tr><td>Method</td><td>Backbone</td><td>mIoU</td></tr><tr><td>FCN (40k, SS) [39]</td><td>ResNet-101</td><td>73.93</td></tr><tr><td>FCN (40k, MS) [39]</td><td>ResNet-101</td><td>75.14</td></tr><tr><td>FCN (80k, SS) [39]</td><td>ResNet-101</td><td>75.52</td></tr><tr><td>FCN (80k, MS) [39]</td><td>ResNet-101</td><td>76.61</td></tr><tr><td>PSPNet [60]</td><td>ResNet-101</td><td>78.50</td></tr><tr><td>DeepLab-v3 [10] (MS)</td><td>ResNet-101</td><td>79.30</td></tr><tr><td>NonLocal [48]</td><td>ResNet-101</td><td>79.10</td></tr><tr><td>CCNet [25]</td><td>ResNet-101</td><td>80.20</td></tr><tr><td>GCNet [4]</td><td>ResNet-101</td><td>78.10</td></tr><tr><td>Axial-DeepLab-XL [47] (MS)</td><td>Axial-ResNet-XL</td><td>81.10</td></tr><tr><td>Axial-DeepLab-L [47] (MS)</td><td>Axial-ResNet-L</td><td>81.50</td></tr><tr><td>SETR-PUP (40k, SS)</td><td>T-Large</td><td>78.39</td></tr><tr><td>SETR-PUP (40k, MS)</td><td>T-Large</td><td>81.57</td></tr><tr><td>SETR-PUP (80k, SS)</td><td>T-Large</td><td>79.34</td></tr><tr><td>SETR-PUP (80k, MS)</td><td>T-Large</td><td>82.15</td></tr></table>",
      "id": 84,
      "page": 7,
      "text": "Method Backbone mIoU\n FCN (40k, SS) [39] ResNet-101 73.93\n FCN (40k, MS) [39] ResNet-101 75.14\n FCN (80k, SS) [39] ResNet-101 75.52\n FCN (80k, MS) [39] ResNet-101 76.61\n PSPNet [60] ResNet-101 78.50\n DeepLab-v3 [10] (MS) ResNet-101 79.30\n NonLocal [48] ResNet-101 79.10\n CCNet [25] ResNet-101 80.20\n GCNet [4] ResNet-101 78.10\n Axial-DeepLab-XL [47] (MS) Axial-ResNet-XL 81.10\n Axial-DeepLab-L [47] (MS) Axial-ResNet-L 81.50\n SETR-PUP (40k, SS) T-Large 78.39\n SETR-PUP (40k, MS) T-Large 81.57\n SETR-PUP (80k, SS) T-Large 79.34\n SETR-PUP (80k, MS) T-Large"
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 1901
        },
        {
          "x": 1197,
          "y": 1901
        },
        {
          "x": 1197,
          "y": 2075
        },
        {
          "x": 202,
          "y": 2075
        }
      ],
      "category": "caption",
      "html": "<br><caption id='85' style='font-size:16px'>Table 6. State-of-the-art comparison on the Cityscapes valida-<br>tion set. Performances of different training schedules (e.g., 40k<br>and 80k) are reported. SS: Single-scale inference. MS: Multi-<br>scale inference.</caption>",
      "id": 85,
      "page": 7,
      "text": "Table 6. State-of-the-art comparison on the Cityscapes valida-\ntion set. Performances of different training schedules (e.g., 40k\nand 80k) are reported. SS: Single-scale inference. MS: Multi-\nscale inference."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 2163
        },
        {
          "x": 1196,
          "y": 2163
        },
        {
          "x": 1196,
          "y": 2256
        },
        {
          "x": 202,
          "y": 2256
        }
      ],
      "category": "paragraph",
      "html": "<p id='86' style='font-size:20px'>additionally pixel-wise accuracy is reported following the<br>existing practice.</p>",
      "id": 86,
      "page": 7,
      "text": "additionally pixel-wise accuracy is reported following the\nexisting practice."
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 2299
        },
        {
          "x": 607,
          "y": 2299
        },
        {
          "x": 607,
          "y": 2344
        },
        {
          "x": 203,
          "y": 2344
        }
      ],
      "category": "paragraph",
      "html": "<p id='87' style='font-size:22px'>4.2. Ablation studies</p>",
      "id": 87,
      "page": 7,
      "text": "4.2. Ablation studies"
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 2377
        },
        {
          "x": 1199,
          "y": 2377
        },
        {
          "x": 1199,
          "y": 2871
        },
        {
          "x": 201,
          "y": 2871
        }
      ],
      "category": "paragraph",
      "html": "<p id='88' style='font-size:18px'>Table 2 and 3 show ablation studies on (a) different vari-<br>ants of SETR on various training schedules, (b) compari-<br>son to FCN [39] and Semantic FPN [39], (c) pre-training<br>on different data, (d) comparison with Hybrid, (e) com-<br>pare to FCN with different pre-training. Unless otherwise<br>specified, all experiments on Table 2 and 3 are trained on<br>Cityscapes train fine set with batch size 8, and evaluated<br>using the single scale test protocol on the Cityscapes vali-<br>dation set in mean IoU (%) rate. Experiments on ADE20K<br>also follow the single scale test protocol.</p>",
      "id": 88,
      "page": 7,
      "text": "Table 2 and 3 show ablation studies on (a) different vari-\nants of SETR on various training schedules, (b) compari-\nson to FCN [39] and Semantic FPN [39], (c) pre-training\non different data, (d) comparison with Hybrid, (e) com-\npare to FCN with different pre-training. Unless otherwise\nspecified, all experiments on Table 2 and 3 are trained on\nCityscapes train fine set with batch size 8, and evaluated\nusing the single scale test protocol on the Cityscapes vali-\ndation set in mean IoU (%) rate. Experiments on ADE20K\nalso follow the single scale test protocol."
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 2881
        },
        {
          "x": 1196,
          "y": 2881
        },
        {
          "x": 1196,
          "y": 2975
        },
        {
          "x": 203,
          "y": 2975
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='89' style='font-size:18px'>From Table 2, we can make the following observations:<br>(i) Progressively upsampling the feature maps, SETR-</p>",
      "id": 89,
      "page": 7,
      "text": "From Table 2, we can make the following observations:\n(i) Progressively upsampling the feature maps, SETR-"
    },
    {
      "bounding_box": [
        {
          "x": 1363,
          "y": 1221
        },
        {
          "x": 2203,
          "y": 1221
        },
        {
          "x": 2203,
          "y": 1738
        },
        {
          "x": 1363,
          "y": 1738
        }
      ],
      "category": "table",
      "html": "<br><table id='90' style='font-size:14px'><tr><td>Method</td><td>Backbone</td><td>mIoU</td></tr><tr><td>PSPNet [60]</td><td>ResNet-101</td><td>78.40</td></tr><tr><td>DenseASPP [50]</td><td>DenseNet-161</td><td>80.60</td></tr><tr><td>BiSeNet [52]</td><td>ResNet-101</td><td>78.90</td></tr><tr><td>PSANet [61]</td><td>ResNet-101</td><td>80.10</td></tr><tr><td>DANet [18]</td><td>ResNet-101</td><td>81.50</td></tr><tr><td>OCNet [55]</td><td>ResNet-101</td><td>80.10</td></tr><tr><td>CCNet [25]</td><td>ResNet-101</td><td>81.90</td></tr><tr><td>Axial-DeepLab-L [47]</td><td>Axial-ResNet-L</td><td>79.50</td></tr><tr><td>Axial-DeepLab-XL [47]</td><td>Axial-ResNet-XL</td><td>79.90</td></tr><tr><td>SETR-PUP (100k)</td><td>T-Large</td><td>81.08</td></tr><tr><td>SETR-PUP</td><td>T-Large</td><td>81.64</td></tr></table>",
      "id": 90,
      "page": 7,
      "text": "Method Backbone mIoU\n PSPNet [60] ResNet-101 78.40\n DenseASPP [50] DenseNet-161 80.60\n BiSeNet [52] ResNet-101 78.90\n PSANet [61] ResNet-101 80.10\n DANet [18] ResNet-101 81.50\n OCNet [55] ResNet-101 80.10\n CCNet [25] ResNet-101 81.90\n Axial-DeepLab-L [47] Axial-ResNet-L 79.50\n Axial-DeepLab-XL [47] Axial-ResNet-XL 79.90\n SETR-PUP (100k) T-Large 81.08\n SETR-PUP T-Large"
    },
    {
      "bounding_box": [
        {
          "x": 1286,
          "y": 1734
        },
        {
          "x": 2273,
          "y": 1734
        },
        {
          "x": 2273,
          "y": 1816
        },
        {
          "x": 1286,
          "y": 1816
        }
      ],
      "category": "caption",
      "html": "<br><caption id='91' style='font-size:14px'>Table 7. Comparison on the Cityscapes test set. 1: trained on<br>fine and coarse annotated data.</caption>",
      "id": 91,
      "page": 7,
      "text": "Table 7. Comparison on the Cityscapes test set. 1: trained on\nfine and coarse annotated data."
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 1929
        },
        {
          "x": 2277,
          "y": 1929
        },
        {
          "x": 2277,
          "y": 2978
        },
        {
          "x": 1278,
          "y": 2978
        }
      ],
      "category": "paragraph",
      "html": "<p id='92' style='font-size:18px'>PUP achieves the best performance among all the vari-<br>ants on Cityscapes. One possible reason for inferior per-<br>formance of SETR-MLA is that the feature outputs of dif-<br>ferent transformer layers do not have the benefits of reso-<br>lution pyramid as in feature pyramid network (FPN) (see<br>Figure 5). However, SETR-MLA performs slightly better<br>than SETR-PUP, and much superior to the variant SETR-<br>Naive that upsamples the transformers output feature by<br>16x in one-shot, on ADE20K val set (Table 3 and 4). (ii)<br>The variants using \"T-Large\" (e.g., SETR-MLA and SETR-<br>Naive) are superior to their \"T-Base\" counterparts, i.e.,<br>SETR-MLA-Base and SETR-Naive-Base, as expected. (iii)<br>While our SETR-PUP-Base (76.71) performs worse than<br>Hybrid-Base (76.76), it shines (78.02) when training with<br>more iterations (80k). It suggests that FCN encoder design<br>can be replaced in semantic segmentation, and further con-<br>firms the effectiveness of our model. (iv) Pre-training is crit-<br>ical for our model. Randomly initialized SETR-PUP only<br>gives 42.27% mIoU on Cityscapes. Model pre-trained with<br>DeiT [44] on ImageNet-1K gives the best performance on<br>Cityscapes, slightly better than the counterpart pre-trained</p>",
      "id": 92,
      "page": 7,
      "text": "PUP achieves the best performance among all the vari-\nants on Cityscapes. One possible reason for inferior per-\nformance of SETR-MLA is that the feature outputs of dif-\nferent transformer layers do not have the benefits of reso-\nlution pyramid as in feature pyramid network (FPN) (see\nFigure 5). However, SETR-MLA performs slightly better\nthan SETR-PUP, and much superior to the variant SETR-\nNaive that upsamples the transformers output feature by\n16x in one-shot, on ADE20K val set (Table 3 and 4). (ii)\nThe variants using \"T-Large\" (e.g., SETR-MLA and SETR-\nNaive) are superior to their \"T-Base\" counterparts, i.e.,\nSETR-MLA-Base and SETR-Naive-Base, as expected. (iii)\nWhile our SETR-PUP-Base (76.71) performs worse than\nHybrid-Base (76.76), it shines (78.02) when training with\nmore iterations (80k). It suggests that FCN encoder design\ncan be replaced in semantic segmentation, and further con-\nfirms the effectiveness of our model. (iv) Pre-training is crit-\nical for our model. Randomly initialized SETR-PUP only\ngives 42.27% mIoU on Cityscapes. Model pre-trained with\nDeiT [44] on ImageNet-1K gives the best performance on\nCityscapes, slightly better than the counterpart pre-trained"
    },
    {
      "bounding_box": [
        {
          "x": 1225,
          "y": 3056
        },
        {
          "x": 1252,
          "y": 3056
        },
        {
          "x": 1252,
          "y": 3089
        },
        {
          "x": 1225,
          "y": 3089
        }
      ],
      "category": "footer",
      "html": "<footer id='93' style='font-size:14px'>7</footer>",
      "id": 93,
      "page": 7,
      "text": "7"
    },
    {
      "bounding_box": [
        {
          "x": 226,
          "y": 314
        },
        {
          "x": 1196,
          "y": 314
        },
        {
          "x": 1196,
          "y": 702
        },
        {
          "x": 226,
          "y": 702
        }
      ],
      "category": "figure",
      "html": "<figure><img id='94' alt=\"\" data-coord=\"top-left:(226,314); bottom-right:(1196,702)\" /></figure>",
      "id": 94,
      "page": 8,
      "text": ""
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 746
        },
        {
          "x": 1190,
          "y": 746
        },
        {
          "x": 1190,
          "y": 835
        },
        {
          "x": 202,
          "y": 835
        }
      ],
      "category": "caption",
      "html": "<caption id='95' style='font-size:14px'>Figure 5. Visualization of output feature of layer Z1, Z9, Z17, Z24<br>of SETR trained on Pascal Context. Best viewed in color.</caption>",
      "id": 95,
      "page": 8,
      "text": "Figure 5. Visualization of output feature of layer Z1, Z9, Z17, Z24\nof SETR trained on Pascal Context. Best viewed in color."
    },
    {
      "bounding_box": [
        {
          "x": 228,
          "y": 895
        },
        {
          "x": 1195,
          "y": 895
        },
        {
          "x": 1195,
          "y": 1284
        },
        {
          "x": 228,
          "y": 1284
        }
      ],
      "category": "figure",
      "html": "<figure><img id='96' alt=\"\" data-coord=\"top-left:(228,895); bottom-right:(1195,1284)\" /></figure>",
      "id": 96,
      "page": 8,
      "text": ""
    },
    {
      "bounding_box": [
        {
          "x": 205,
          "y": 1328
        },
        {
          "x": 1192,
          "y": 1328
        },
        {
          "x": 1192,
          "y": 1413
        },
        {
          "x": 205,
          "y": 1413
        }
      ],
      "category": "caption",
      "html": "<caption id='97' style='font-size:14px'>Figure 6. Examples of attention maps from SETR trained on Pas-<br>cal Context.</caption>",
      "id": 97,
      "page": 8,
      "text": "Figure 6. Examples of attention maps from SETR trained on Pas-\ncal Context."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 1497
        },
        {
          "x": 1199,
          "y": 1497
        },
        {
          "x": 1199,
          "y": 2244
        },
        {
          "x": 201,
          "y": 2244
        }
      ],
      "category": "paragraph",
      "html": "<p id='98' style='font-size:18px'>with ViT [17] on ImageNet-21K. (v) To study the power<br>of pre-training and further verify the effectiveness of our<br>proposed approach, we conduct the ablation study on the<br>pre-training strategy in Table 3. For fair comparison with<br>the FCN baseline, we first pre-train a ResNet-101 on the<br>Imagenet-21k dataset with a classification task and then<br>adopt the pre-trained weights for a dilated FCN training for<br>the semantic segmentation task on ADE20K or Cityscapes.<br>Table 3 shows that with ImageNet-21k pre-training FCN<br>baseline experienced a clear improvement over the variant<br>pre-trained on ImageNet-1k. However, our method out-<br>performs the FCN counterparts by a large margin, veri-<br>fying that the advantage of our approach largely comes<br>from the proposed sequence-to-sequence modeling strategy<br>rather than bigger pre-training data.</p>",
      "id": 98,
      "page": 8,
      "text": "with ViT [17] on ImageNet-21K. (v) To study the power\nof pre-training and further verify the effectiveness of our\nproposed approach, we conduct the ablation study on the\npre-training strategy in Table 3. For fair comparison with\nthe FCN baseline, we first pre-train a ResNet-101 on the\nImagenet-21k dataset with a classification task and then\nadopt the pre-trained weights for a dilated FCN training for\nthe semantic segmentation task on ADE20K or Cityscapes.\nTable 3 shows that with ImageNet-21k pre-training FCN\nbaseline experienced a clear improvement over the variant\npre-trained on ImageNet-1k. However, our method out-\nperforms the FCN counterparts by a large margin, veri-\nfying that the advantage of our approach largely comes\nfrom the proposed sequence-to-sequence modeling strategy\nrather than bigger pre-training data."
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 2277
        },
        {
          "x": 887,
          "y": 2277
        },
        {
          "x": 887,
          "y": 2325
        },
        {
          "x": 203,
          "y": 2325
        }
      ],
      "category": "paragraph",
      "html": "<p id='99' style='font-size:20px'>4.3. Comparison to state-of-the-art</p>",
      "id": 99,
      "page": 8,
      "text": "4.3. Comparison to state-of-the-art"
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 2353
        },
        {
          "x": 1199,
          "y": 2353
        },
        {
          "x": 1199,
          "y": 2849
        },
        {
          "x": 201,
          "y": 2849
        }
      ],
      "category": "paragraph",
      "html": "<p id='100' style='font-size:16px'>Results on ADE20K Table 4 presents our results on<br>the more challenging ADE20K dataset. Our SETR-<br>MLA achieves superior mIoU of 48.64% with single-scale<br>(SS) inference. When multi-scale inference is adopted, our<br>method achieves a new state of the art with mIoU hitting<br>50.28%. Figure 2 shows the qualitative results of our model<br>and dilated FCN on ADE20K. When training a single model<br>on the train+validation set with the default 160,000 itera-<br>tions, our method ranks 1st place in the highly competitive<br>ADE20K test server leaderboard.</p>",
      "id": 100,
      "page": 8,
      "text": "Results on ADE20K Table 4 presents our results on\nthe more challenging ADE20K dataset. Our SETR-\nMLA achieves superior mIoU of 48.64% with single-scale\n(SS) inference. When multi-scale inference is adopted, our\nmethod achieves a new state of the art with mIoU hitting\n50.28%. Figure 2 shows the qualitative results of our model\nand dilated FCN on ADE20K. When training a single model\non the train+validation set with the default 160,000 itera-\ntions, our method ranks 1st place in the highly competitive\nADE20K test server leaderboard."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 2879
        },
        {
          "x": 1196,
          "y": 2879
        },
        {
          "x": 1196,
          "y": 2974
        },
        {
          "x": 202,
          "y": 2974
        }
      ],
      "category": "paragraph",
      "html": "<p id='101' style='font-size:14px'>Results on Pascal Context Table 5 compares the segmen-<br>tation results on Pascal Context. Dilated FCN with the</p>",
      "id": 101,
      "page": 8,
      "text": "Results on Pascal Context Table 5 compares the segmen-\ntation results on Pascal Context. Dilated FCN with the"
    },
    {
      "bounding_box": [
        {
          "x": 1276,
          "y": 307
        },
        {
          "x": 2276,
          "y": 307
        },
        {
          "x": 2276,
          "y": 903
        },
        {
          "x": 1276,
          "y": 903
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='102' style='font-size:18px'>ResNet-101 backbone achieves a mloU of 45.74%. Us-<br>ing the same training schedule, our proposed SETR sig-<br>nificantly outperforms this baseline, achieving mIoU of<br>54.40% (SETR-PUP) and 54.87% (SETR-MLA). SETR-<br>MLA further improves the performance to 55.83% when<br>multi-scale (MS) inference is adopted, outperforming the<br>nearest rival APCNet with a clear margin. Figure 3 gives<br>some qualitative results of SETR and dilated FCN. Fur-<br>ther visualization of the learned attention maps in Figure 6<br>shows that SETR can attend to semantically meaningful<br>foreground regions, demonstrating its ability to learn dis-<br>criminative feature representations useful for segmentation.</p>",
      "id": 102,
      "page": 8,
      "text": "ResNet-101 backbone achieves a mloU of 45.74%. Us-\ning the same training schedule, our proposed SETR sig-\nnificantly outperforms this baseline, achieving mIoU of\n54.40% (SETR-PUP) and 54.87% (SETR-MLA). SETR-\nMLA further improves the performance to 55.83% when\nmulti-scale (MS) inference is adopted, outperforming the\nnearest rival APCNet with a clear margin. Figure 3 gives\nsome qualitative results of SETR and dilated FCN. Fur-\nther visualization of the learned attention maps in Figure 6\nshows that SETR can attend to semantically meaningful\nforeground regions, demonstrating its ability to learn dis-\ncriminative feature representations useful for segmentation."
    },
    {
      "bounding_box": [
        {
          "x": 1277,
          "y": 941
        },
        {
          "x": 2278,
          "y": 941
        },
        {
          "x": 2278,
          "y": 1940
        },
        {
          "x": 1277,
          "y": 1940
        }
      ],
      "category": "paragraph",
      "html": "<p id='103' style='font-size:16px'>Results on Cityscapes Tables 6 and 7 show the compara-<br>tive results on the validation and test set of Cityscapes re-<br>spectively. We can see that our model SETR-PUP is su-<br>perior to FCN baselines, and FCN plus attention based ap-<br>proaches, such as Non-local [48] and CCNet [25]; and its<br>performance is on par with the best results reported SO far.<br>On this dataset we can now compare with the closely related<br>Axial-DeepLab [12, 47] which aims to use an attention-<br>alone model but still follows the basic structure of FCN.<br>Note that Axial-DeepLab sets the same output stride 16 as<br>ours. However, its full input resolution (1024 x 2048) is<br>much larger than our crop size 768 x 768, and it runs more<br>epochs (60k iteration with batch size 32) than our setting<br>(80k iterations with batch size 8). Nevertheless, our model<br>is still superior to Axial-DeepLab when multi-scale infer-<br>ence is adopted on Cityscapes validation set. Using the fine<br>set only, our model (trained with 100k iterations) outper-<br>forms Axial-DeepLab-XL with a clear margin on the test<br>set. Figure 4 shows the qualitative results of our model and<br>dilated FCN on Cityscapes.</p>",
      "id": 103,
      "page": 8,
      "text": "Results on Cityscapes Tables 6 and 7 show the compara-\ntive results on the validation and test set of Cityscapes re-\nspectively. We can see that our model SETR-PUP is su-\nperior to FCN baselines, and FCN plus attention based ap-\nproaches, such as Non-local [48] and CCNet [25]; and its\nperformance is on par with the best results reported SO far.\nOn this dataset we can now compare with the closely related\nAxial-DeepLab [12, 47] which aims to use an attention-\nalone model but still follows the basic structure of FCN.\nNote that Axial-DeepLab sets the same output stride 16 as\nours. However, its full input resolution (1024 x 2048) is\nmuch larger than our crop size 768 x 768, and it runs more\nepochs (60k iteration with batch size 32) than our setting\n(80k iterations with batch size 8). Nevertheless, our model\nis still superior to Axial-DeepLab when multi-scale infer-\nence is adopted on Cityscapes validation set. Using the fine\nset only, our model (trained with 100k iterations) outper-\nforms Axial-DeepLab-XL with a clear margin on the test\nset. Figure 4 shows the qualitative results of our model and\ndilated FCN on Cityscapes."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 1994
        },
        {
          "x": 1578,
          "y": 1994
        },
        {
          "x": 1578,
          "y": 2044
        },
        {
          "x": 1280,
          "y": 2044
        }
      ],
      "category": "paragraph",
      "html": "<p id='104' style='font-size:22px'>5. Conclusion</p>",
      "id": 104,
      "page": 8,
      "text": "5. Conclusion"
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 2079
        },
        {
          "x": 2277,
          "y": 2079
        },
        {
          "x": 2277,
          "y": 2976
        },
        {
          "x": 1278,
          "y": 2976
        }
      ],
      "category": "paragraph",
      "html": "<p id='105' style='font-size:16px'>In this work, we have presented an alternative perspec-<br>tive for semantic segmentation by introducing a sequence-<br>to-sequence prediction framework. In contrast to existing<br>FCN based methods that enlarge the receptive field typi-<br>cally with dilated convolutions and attention modules at the<br>component level, we made a step change at the architectural<br>level to completely eliminate the reliance on FCN and ele-<br>gantly solve the limited receptive field challenge. We imple-<br>mented the proposed idea with Transformers that can model<br>global context at every stage of feature learning. Along<br>with a set of decoder designs in different complexity, strong<br>segmentation models are established with none of the bells<br>and whistles deployed by recent methods. Extensive ex-<br>periments demonstrate that our models set new state of the<br>art on ADE20, Pascal Context and competitive results on<br>Cityscapes. Encouragingly, our method is ranked the 1st<br>place in the highly competitive ADE20K test server leader-<br>board on the day of submission.</p>",
      "id": 105,
      "page": 8,
      "text": "In this work, we have presented an alternative perspec-\ntive for semantic segmentation by introducing a sequence-\nto-sequence prediction framework. In contrast to existing\nFCN based methods that enlarge the receptive field typi-\ncally with dilated convolutions and attention modules at the\ncomponent level, we made a step change at the architectural\nlevel to completely eliminate the reliance on FCN and ele-\ngantly solve the limited receptive field challenge. We imple-\nmented the proposed idea with Transformers that can model\nglobal context at every stage of feature learning. Along\nwith a set of decoder designs in different complexity, strong\nsegmentation models are established with none of the bells\nand whistles deployed by recent methods. Extensive ex-\nperiments demonstrate that our models set new state of the\nart on ADE20, Pascal Context and competitive results on\nCityscapes. Encouragingly, our method is ranked the 1st\nplace in the highly competitive ADE20K test server leader-\nboard on the day of submission."
    },
    {
      "bounding_box": [
        {
          "x": 1226,
          "y": 3056
        },
        {
          "x": 1251,
          "y": 3056
        },
        {
          "x": 1251,
          "y": 3091
        },
        {
          "x": 1226,
          "y": 3091
        }
      ],
      "category": "footer",
      "html": "<footer id='106' style='font-size:16px'>8</footer>",
      "id": 106,
      "page": 8,
      "text": "8"
    },
    {
      "bounding_box": [
        {
          "x": 205,
          "y": 304
        },
        {
          "x": 602,
          "y": 304
        },
        {
          "x": 602,
          "y": 353
        },
        {
          "x": 205,
          "y": 353
        }
      ],
      "category": "paragraph",
      "html": "<p id='107' style='font-size:22px'>Acknowledgments</p>",
      "id": 107,
      "page": 9,
      "text": "Acknowledgments"
    },
    {
      "bounding_box": [
        {
          "x": 1365,
          "y": 310
        },
        {
          "x": 2278,
          "y": 310
        },
        {
          "x": 2278,
          "y": 355
        },
        {
          "x": 1365,
          "y": 355
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='108' style='font-size:14px'>Attentive language models beyond a fixed-length context. In</p>",
      "id": 108,
      "page": 9,
      "text": "Attentive language models beyond a fixed-length context. In"
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 385
        },
        {
          "x": 1200,
          "y": 385
        },
        {
          "x": 1200,
          "y": 584
        },
        {
          "x": 202,
          "y": 584
        }
      ],
      "category": "paragraph",
      "html": "<p id='109' style='font-size:18px'>This work was supported by Shanghai Municipal Sci-<br>ence and Technology Major Project (No.2018SHZDZX01),<br>ZJLab, and Shanghai Center for Brain Science and Brain-<br>Inspired Technology.</p>",
      "id": 109,
      "page": 9,
      "text": "This work was supported by Shanghai Municipal Sci-\nence and Technology Major Project (No.2018SHZDZX01),\nZJLab, and Shanghai Center for Brain Science and Brain-\nInspired Technology."
    },
    {
      "bounding_box": [
        {
          "x": 205,
          "y": 636
        },
        {
          "x": 445,
          "y": 636
        },
        {
          "x": 445,
          "y": 684
        },
        {
          "x": 205,
          "y": 684
        }
      ],
      "category": "paragraph",
      "html": "<p id='110' style='font-size:20px'>References</p>",
      "id": 110,
      "page": 9,
      "text": "References"
    },
    {
      "bounding_box": [
        {
          "x": 217,
          "y": 708
        },
        {
          "x": 1199,
          "y": 708
        },
        {
          "x": 1199,
          "y": 2972
        },
        {
          "x": 217,
          "y": 2972
        }
      ],
      "category": "paragraph",
      "html": "<p id='111' style='font-size:14px'>[1] Samira Abnar and Willem Zuidema. Quantifying attention<br>flow in transformers. arXiv preprint, 2020. 11<br>[2] Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla.<br>Segnet: A deep convolutional encoder-decoder architecture<br>for image segmentation. TPAMI, 2017. 1, 2<br>[3] Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens,<br>and Quoc V Le. Attention augmented convolutional net-<br>works. In ICCV, 2019. 2<br>[4] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han<br>Hu. Gcnet: Non-local networks meet squeeze-excitation net-<br>works and beyond. In ICCV workshops, 2019. 7<br>[5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas<br>Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-<br>end object detection with transformers. In ECCV, 2020. 2<br>[6] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,<br>Kevin Murphy, and Alan L. Yuille. Semantic image seg-<br>mentation with deep convolutional nets and fully connected<br>CRFs. In ICLR, 2015. 2<br>[7] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,<br>Kevin Murphy, and Alan L. Yuille. Semantic image seg-<br>mentation with deep convolutional nets and fully connected<br>CRFs. In ICLR, 2015. 2<br>[8] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,<br>Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image<br>segmentation with deep convolutional nets, atrous convolu-<br>tion, and fully connected crfs. TPAMI, 2018. 1<br>[9] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,<br>Kevin Murphy, and Alan L. Yuille. Deeplab: Semantic im-<br>age segmentation with deep convolutional nets, atrous con-<br>volution, and fully connected crfs. TPAMI, 2018. 2<br>[10] Liang-Chieh Chen, George Papandreou, Florian Schroff, and<br>Hartwig Adam. Rethinking atrous convolution for semantic<br>image segmentation. arXiv preprint, 2017. 7<br>[11] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian<br>Schroff, and Hartwig Adam. Encoder-decoder with atrous<br>separable convolution for semantic image segmentation. In<br>ECCV, 2018. 6<br>[12] Bowen Cheng, Maxwell D Collins, Yukun Zhu, Ting Liu,<br>Thomas S Huang, Hartwig Adam, and Liang-Chieh Chen.<br>Panoptic-deeplab: A simple, strong, and fast baseline for<br>bottom-up panoptic segmentation. In CVPR, 2020. 8<br>[13] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo<br>Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe<br>Franke, Stefan Roth, and Bernt Schiele. The cityscapes<br>dataset for semantic urban scene understanding. In CVPR,<br>2016. 5, 6<br>[14] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell,<br>Quoc V. Le, and Ruslan Salakhutdinov. Transformer-XL:</p>",
      "id": 111,
      "page": 9,
      "text": "[1] Samira Abnar and Willem Zuidema. Quantifying attention\nflow in transformers. arXiv preprint, 2020. 11\n[2] Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla.\nSegnet: A deep convolutional encoder-decoder architecture\nfor image segmentation. TPAMI, 2017. 1, 2\n[3] Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens,\nand Quoc V Le. Attention augmented convolutional net-\nworks. In ICCV, 2019. 2\n[4] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han\nHu. Gcnet: Non-local networks meet squeeze-excitation net-\nworks and beyond. In ICCV workshops, 2019. 7\n[5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In ECCV, 2020. 2\n[6] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,\nKevin Murphy, and Alan L. Yuille. Semantic image seg-\nmentation with deep convolutional nets and fully connected\nCRFs. In ICLR, 2015. 2\n[7] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,\nKevin Murphy, and Alan L. Yuille. Semantic image seg-\nmentation with deep convolutional nets and fully connected\nCRFs. In ICLR, 2015. 2\n[8] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,\nKevin Murphy, and Alan L Yuille. Deeplab: Semantic image\nsegmentation with deep convolutional nets, atrous convolu-\ntion, and fully connected crfs. TPAMI, 2018. 1\n[9] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,\nKevin Murphy, and Alan L. Yuille. Deeplab: Semantic im-\nage segmentation with deep convolutional nets, atrous con-\nvolution, and fully connected crfs. TPAMI, 2018. 2\n[10] Liang-Chieh Chen, George Papandreou, Florian Schroff, and\nHartwig Adam. Rethinking atrous convolution for semantic\nimage segmentation. arXiv preprint, 2017. 7\n[11] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian\nSchroff, and Hartwig Adam. Encoder-decoder with atrous\nseparable convolution for semantic image segmentation. In\nECCV, 2018. 6\n[12] Bowen Cheng, Maxwell D Collins, Yukun Zhu, Ting Liu,\nThomas S Huang, Hartwig Adam, and Liang-Chieh Chen.\nPanoptic-deeplab: A simple, strong, and fast baseline for\nbottom-up panoptic segmentation. In CVPR, 2020. 8\n[13] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo\nRehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe\nFranke, Stefan Roth, and Bernt Schiele. The cityscapes\ndataset for semantic urban scene understanding. In CVPR,\n2016. 5, 6\n[14] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell,\nQuoc V. Le, and Ruslan Salakhutdinov. Transformer-XL:"
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 337
        },
        {
          "x": 2288,
          "y": 337
        },
        {
          "x": 2288,
          "y": 2971
        },
        {
          "x": 1278,
          "y": 2971
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='112' style='font-size:16px'>ACL, 2019. 2<br>[15] Jacob Devlin, Ming- Wei Chang, Kenton Lee, and Kristina<br>Toutanova. BERT: Pre-training of deep bidirectional trans-<br>formers for language understanding. In NAACL-HLT, 2019.<br>2<br>[16] Henghui Ding, Xudong Jiang, Bing Shuai, Ai Qun Liu, and<br>Gang Wang. Semantic correlation promoted shape-variant<br>context for segmentation. In CVPR, 2019. 6<br>[17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,<br>Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,<br>Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-<br>vain Gelly, et al. An image is worth 16x16 words: Trans-<br>formers for image recognition at scale. In ICLR, 2021. 2, 3,<br>6, 8<br>[18] Jun Fu, Jing Liu, Haijie Tian, Zhiwei Fang, and Hanqing Lu.<br>Dual attention network for scene segmentation. In CVPR,<br>2019. 2, 6, 7<br>[19] Junjun He, Zhongying Deng, Lei Zhou, Yali Wang, and Yu<br>Qiao. Adaptive pyramid context network for semantic seg-<br>mentation. In CVPR, 2019. 6<br>[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.<br>Deep residual learning for image recognition. In CVPR,<br>2016. 2, 3<br>[21] Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim<br>Salimans. Axial attention in multidimensional transformers.<br>arXiv preprint, 2019. 3<br>[22] Matthias Holschneider, Richard Kronland-Martinet, Jean<br>Morlet, and Ph Tchamitchian. A real-time algorithm for<br>signal analysis with the help of the wavelet transform. In<br>Wavelets, 1990. 1<br>[23] Qibin Hou, Li Zhang, Ming-Ming Cheng, and Jiashi Feng.<br>Strip pooling: Rethinking spatial pooling for scene parsing.<br>In CVPR, 2020. 6<br>[24] Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local<br>relation networks for image recognition. In ICCV, 2019. 2<br>[25] Zilong Huang, Xinggang Wang, Lichao Huang, Chang<br>Huang, Yunchao Wei, and Wenyu Liu. Ccnet: Criss-cross<br>attention for semantic segmentation. In ICCV, 2019. 1, 3, 6,<br>7, 8<br>[26] Zilong Huang, Xinggang Wang, Lichao Huang, Chang<br>Huang, Yunchao Wei, and Wenyu Liu. Ccnet: Criss-cross<br>attention for semantic segmentation. In ICCV, 2019. 2<br>[27] Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr<br>Dollar. Panoptic feature pyramid networks. In CVPR, 2019.<br>4, 6<br>[28] Xiangtai Li, Xia Li, Li Zhang, Guangliang Cheng, Jianping<br>Shi, Zhouchen Lin, Shaohua Tan, and Yunhai Tong. Improv-<br>ing semantic segmentation via decoupled body and edge su-<br>pervision. In ECCV, 2020. 1<br>[29] Xiangtai Li, Li Zhang, Ansheng You, Maoke Yang, Kuiyuan<br>Yang, and Yunhai Tong. Global aggregation then local dis-<br>tribution in fully convolutional networks. In BMVC, 2019.<br>1<br>[30] Xiangtai Li, Houlong Zhao, Lei Han, Yunhai Tong, and<br>Kuiyuan Yang. Gff: Gated fully fusion for semantic seg-<br>mentation. In AAAI, 2020. 6</p>",
      "id": 112,
      "page": 9,
      "text": "ACL, 2019. 2\n[15] Jacob Devlin, Ming- Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: Pre-training of deep bidirectional trans-\nformers for language understanding. In NAACL-HLT, 2019.\n2\n[16] Henghui Ding, Xudong Jiang, Bing Shuai, Ai Qun Liu, and\nGang Wang. Semantic correlation promoted shape-variant\ncontext for segmentation. In CVPR, 2019. 6\n[17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. In ICLR, 2021. 2, 3,\n6, 8\n[18] Jun Fu, Jing Liu, Haijie Tian, Zhiwei Fang, and Hanqing Lu.\nDual attention network for scene segmentation. In CVPR,\n2019. 2, 6, 7\n[19] Junjun He, Zhongying Deng, Lei Zhou, Yali Wang, and Yu\nQiao. Adaptive pyramid context network for semantic seg-\nmentation. In CVPR, 2019. 6\n[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR,\n2016. 2, 3\n[21] Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim\nSalimans. Axial attention in multidimensional transformers.\narXiv preprint, 2019. 3\n[22] Matthias Holschneider, Richard Kronland-Martinet, Jean\nMorlet, and Ph Tchamitchian. A real-time algorithm for\nsignal analysis with the help of the wavelet transform. In\nWavelets, 1990. 1\n[23] Qibin Hou, Li Zhang, Ming-Ming Cheng, and Jiashi Feng.\nStrip pooling: Rethinking spatial pooling for scene parsing.\nIn CVPR, 2020. 6\n[24] Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local\nrelation networks for image recognition. In ICCV, 2019. 2\n[25] Zilong Huang, Xinggang Wang, Lichao Huang, Chang\nHuang, Yunchao Wei, and Wenyu Liu. Ccnet: Criss-cross\nattention for semantic segmentation. In ICCV, 2019. 1, 3, 6,\n7, 8\n[26] Zilong Huang, Xinggang Wang, Lichao Huang, Chang\nHuang, Yunchao Wei, and Wenyu Liu. Ccnet: Criss-cross\nattention for semantic segmentation. In ICCV, 2019. 2\n[27] Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr\nDollar. Panoptic feature pyramid networks. In CVPR, 2019.\n4, 6\n[28] Xiangtai Li, Xia Li, Li Zhang, Guangliang Cheng, Jianping\nShi, Zhouchen Lin, Shaohua Tan, and Yunhai Tong. Improv-\ning semantic segmentation via decoupled body and edge su-\npervision. In ECCV, 2020. 1\n[29] Xiangtai Li, Li Zhang, Ansheng You, Maoke Yang, Kuiyuan\nYang, and Yunhai Tong. Global aggregation then local dis-\ntribution in fully convolutional networks. In BMVC, 2019.\n1\n[30] Xiangtai Li, Houlong Zhao, Lei Han, Yunhai Tong, and\nKuiyuan Yang. Gff: Gated fully fusion for semantic seg-\nmentation. In AAAI, 2020. 6"
    },
    {
      "bounding_box": [
        {
          "x": 1226,
          "y": 3055
        },
        {
          "x": 1251,
          "y": 3055
        },
        {
          "x": 1251,
          "y": 3090
        },
        {
          "x": 1226,
          "y": 3090
        }
      ],
      "category": "footer",
      "html": "<footer id='113' style='font-size:16px'>9</footer>",
      "id": 113,
      "page": 9,
      "text": "9"
    },
    {
      "bounding_box": [
        {
          "x": 207,
          "y": 291
        },
        {
          "x": 1203,
          "y": 291
        },
        {
          "x": 1203,
          "y": 2968
        },
        {
          "x": 207,
          "y": 2968
        }
      ],
      "category": "paragraph",
      "html": "<p id='114' style='font-size:14px'>[31] Xia Li, Zhisheng Zhong, Jianlong Wu, Yibo Yang, Zhouchen<br>Lin, and Hong Liu. Expectation-maximization attention net-<br>works for semantic segmentation. In CVPR, 2019. 6<br>[32] Zhaoshuo Li, Xingtong Liu, Francis X Creighton, Russell H<br>Taylor, and Mathias Unberath. Revisiting stereo depth esti-<br>mation from a sequence-to-sequence perspective with trans-<br>formers. arXiv preprint, 2020. 3<br>[33] Tsung-Yi Lin, Piotr Dollar, Ross B. Girshick, Kaiming He,<br>Bharath Hariharan, and Serge J. Belongie. Feature pyramid<br>networks for object detection. In CVPR, 2017. 4<br>[34] Ruijin Liu, Zejian Yuan, Tie Liu, and Zhiliang Xiong. End-<br>to-end lane shape prediction with transformers. In WACV,<br>2020. 3<br>[35] Ziwei Liu, Xiaoxiao Li, Ping Luo, Chen Change Loy, and<br>Xiaoou Tang. Semantic image segmentation via deep parsing<br>network. In ICCV, 2015. 2<br>[36] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully<br>convolutional networks for semantic segmentation. In<br>CVPR, 2015. 1 2, 3, 6<br>[37] Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu<br>Cho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, and<br>Alan Yuille. The role of context for object detection and se-<br>mantic segmentation in the wild. In CVPR, 2014. 5<br>[38] Hyeonwoo Noh, Seunghoon Hong, and Bohyung Han.<br>Learning deconvolution network for semantic segmentation.<br>In ICCV, 2015. 2<br>[39] OpenMMLab. mmsegmentation. https : / / github ·<br>com/ open-mml ab /mmsegmentation, 2020. 5, 6, 7<br>[40] Chao Peng, Xiangyu Zhang, Gang Yu, Guiming Luo, and<br>Jian Sun. Large kernel matters improve semantic seg-<br>mentation by global convolutional network. In CVPR, 2017.<br>1, 2<br>[41] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan<br>Bello, Anselm Levskaya, and Jonathon Shlens. Stand-alone<br>self-attention in vision models. In NeurIPS, 2019. 2<br>[42] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:<br>Convolutional networks for biomedical image segmentation.<br>MICCAI, 2015. 2<br>[43] Karen Simonyan and Andrew Zisserman. Very deep convo-<br>lutional networks for large-scale image recognition. In ICLR,<br>2015. 2<br>[44] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco<br>Massa, Alexandre Sablayrolles, and Herve Jegou. Training<br>data-efficient image transformers & distillation through at-<br>tention. arXiv preprint, 2020. 6, 7<br>[45] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-<br>reit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia<br>Polosukhin. Attention is all you need. In NeurIPS, 2017. 2<br>[46] Petar Velickovic, Guillem Cucurull, Arantxa Casanova,<br>Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph at-<br>tention networks. In ICLR, 2018. 4<br>[47] Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam,<br>Alan Yuille, and Liang-Chieh Chen. Axial-deeplab: Stand-<br>alone axial-attention for panoptic segmentation. In ECCV,<br>2020. 1, 2, 3, 7, 8<br>[48] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-<br>ing He. Non-local neural networks. In CVPR, 2018. 1, 2, 7,<br>8</p>",
      "id": 114,
      "page": 10,
      "text": "[31] Xia Li, Zhisheng Zhong, Jianlong Wu, Yibo Yang, Zhouchen\nLin, and Hong Liu. Expectation-maximization attention net-\nworks for semantic segmentation. In CVPR, 2019. 6\n[32] Zhaoshuo Li, Xingtong Liu, Francis X Creighton, Russell H\nTaylor, and Mathias Unberath. Revisiting stereo depth esti-\nmation from a sequence-to-sequence perspective with trans-\nformers. arXiv preprint, 2020. 3\n[33] Tsung-Yi Lin, Piotr Dollar, Ross B. Girshick, Kaiming He,\nBharath Hariharan, and Serge J. Belongie. Feature pyramid\nnetworks for object detection. In CVPR, 2017. 4\n[34] Ruijin Liu, Zejian Yuan, Tie Liu, and Zhiliang Xiong. End-\nto-end lane shape prediction with transformers. In WACV,\n2020. 3\n[35] Ziwei Liu, Xiaoxiao Li, Ping Luo, Chen Change Loy, and\nXiaoou Tang. Semantic image segmentation via deep parsing\nnetwork. In ICCV, 2015. 2\n[36] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully\nconvolutional networks for semantic segmentation. In\nCVPR, 2015. 1 2, 3, 6\n[37] Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu\nCho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, and\nAlan Yuille. The role of context for object detection and se-\nmantic segmentation in the wild. In CVPR, 2014. 5\n[38] Hyeonwoo Noh, Seunghoon Hong, and Bohyung Han.\nLearning deconvolution network for semantic segmentation.\nIn ICCV, 2015. 2\n[39] OpenMMLab. mmsegmentation. https : / / github ·\ncom/ open-mml ab /mmsegmentation, 2020. 5, 6, 7\n[40] Chao Peng, Xiangyu Zhang, Gang Yu, Guiming Luo, and\nJian Sun. Large kernel matters improve semantic seg-\nmentation by global convolutional network. In CVPR, 2017.\n1, 2\n[41] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan\nBello, Anselm Levskaya, and Jonathon Shlens. Stand-alone\nself-attention in vision models. In NeurIPS, 2019. 2\n[42] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:\nConvolutional networks for biomedical image segmentation.\nMICCAI, 2015. 2\n[43] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. In ICLR,\n2015. 2\n[44] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herve Jegou. Training\ndata-efficient image transformers & distillation through at-\ntention. arXiv preprint, 2020. 6, 7\n[45] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NeurIPS, 2017. 2\n[46] Petar Velickovic, Guillem Cucurull, Arantxa Casanova,\nAdriana Romero, Pietro Lio, and Yoshua Bengio. Graph at-\ntention networks. In ICLR, 2018. 4\n[47] Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam,\nAlan Yuille, and Liang-Chieh Chen. Axial-deeplab: Stand-\nalone axial-attention for panoptic segmentation. In ECCV,\n2020. 1, 2, 3, 7, 8\n[48] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-\ning He. Non-local neural networks. In CVPR, 2018. 1, 2, 7,\n8"
    },
    {
      "bounding_box": [
        {
          "x": 1277,
          "y": 300
        },
        {
          "x": 2287,
          "y": 300
        },
        {
          "x": 2287,
          "y": 2424
        },
        {
          "x": 1277,
          "y": 2424
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='115' style='font-size:14px'>[49] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and<br>Jian Sun. Unified perceptual parsing for scene understand-<br>ing. In ECCV, 2018. 6<br>[50] Maoke Yang, Kun Yu, Chi Zhang, Zhiwei Li, and Kuiyuan<br>Yang. Denseaspp for semantic segmentation in street scenes.<br>In CVPR, 2018. 1, 7<br>[51] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell,<br>Ruslan Salakhutdinov, and Quoc V. Le. XLNet: General-<br>ized autoregressive pretraining for language understanding.<br>In NeurIPS, 2019. 2<br>[52] Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao,<br>Gang Yu, and Nong Sang. Bisenet: Bilateral segmenta-<br>tion network for real-time semantic segmentation. In ECCV,<br>2018. 7<br>[53] Fisher Yu and Vladlen Koltun. Multi-scale context aggrega-<br>tion by dilated convolutions. ICLR, 2016. 2<br>[54] Yuhui Yuan, Xilin Chen, and Jingdong Wang. Object-<br>contextual representations for semantic segmentation. In<br>ECCV, 2020. 6<br>[55] Yuhui Yuan and Jingdong Wang. Ocnet: Object context net-<br>work for scene parsing. arXiv preprint, 2018. 6, 7<br>[56] Li Zhang, Xiangtai Li, Anurag Arnab, Kuiyuan Yang, Yun-<br>hai Tong, and Philip HS Torr. Dual graph convolutional net-<br>work for semantic segmentation. In BMVC, 2019. 3<br>[57] Li Zhang, Dan Xu, Anurag Arnab, and Philip HS Torr. Dy-<br>namic graph message passing networks. In CVPR, 2020. 1,<br>2, 3<br>[58] Richard Zhang. Making convolutional networks shift-<br>invariant again. In ICML, 2019. 1<br>[59] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring<br>self-attention for image recognition. In CVPR, 2020. 2<br>[60] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang<br>Wang, and Jiaya Jia. Pyramid scene parsing network. In<br>CVPR, 2017. 1, 2, 5, 7<br>[61] Hengshuang Zhao, Yi Zhang, Shu Liu, Jianping Shi, Chen<br>Change Loy, Dahua Lin, and Jiaya Jia. Psanet: Point-wise<br>spatial attention network for scene parsing. In ECCV, 2018.<br>2, 7<br>[62] Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-<br>Paredes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang<br>Huang, and Philip H. S. Torr. Conditional random fields as<br>recurrent neural networks. In ICCV, 2015. 2<br>[63] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela<br>Barriuso, and Antonio Torralba. Semantic understanding of<br>scenes through the ade20k dataset. arXiv preprint, 2016. 5</p>",
      "id": 115,
      "page": 10,
      "text": "[49] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and\nJian Sun. Unified perceptual parsing for scene understand-\ning. In ECCV, 2018. 6\n[50] Maoke Yang, Kun Yu, Chi Zhang, Zhiwei Li, and Kuiyuan\nYang. Denseaspp for semantic segmentation in street scenes.\nIn CVPR, 2018. 1, 7\n[51] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell,\nRuslan Salakhutdinov, and Quoc V. Le. XLNet: General-\nized autoregressive pretraining for language understanding.\nIn NeurIPS, 2019. 2\n[52] Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao,\nGang Yu, and Nong Sang. Bisenet: Bilateral segmenta-\ntion network for real-time semantic segmentation. In ECCV,\n2018. 7\n[53] Fisher Yu and Vladlen Koltun. Multi-scale context aggrega-\ntion by dilated convolutions. ICLR, 2016. 2\n[54] Yuhui Yuan, Xilin Chen, and Jingdong Wang. Object-\ncontextual representations for semantic segmentation. In\nECCV, 2020. 6\n[55] Yuhui Yuan and Jingdong Wang. Ocnet: Object context net-\nwork for scene parsing. arXiv preprint, 2018. 6, 7\n[56] Li Zhang, Xiangtai Li, Anurag Arnab, Kuiyuan Yang, Yun-\nhai Tong, and Philip HS Torr. Dual graph convolutional net-\nwork for semantic segmentation. In BMVC, 2019. 3\n[57] Li Zhang, Dan Xu, Anurag Arnab, and Philip HS Torr. Dy-\nnamic graph message passing networks. In CVPR, 2020. 1,\n2, 3\n[58] Richard Zhang. Making convolutional networks shift-\ninvariant again. In ICML, 2019. 1\n[59] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring\nself-attention for image recognition. In CVPR, 2020. 2\n[60] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang\nWang, and Jiaya Jia. Pyramid scene parsing network. In\nCVPR, 2017. 1, 2, 5, 7\n[61] Hengshuang Zhao, Yi Zhang, Shu Liu, Jianping Shi, Chen\nChange Loy, Dahua Lin, and Jiaya Jia. Psanet: Point-wise\nspatial attention network for scene parsing. In ECCV, 2018.\n2, 7\n[62] Shuai Zheng, Sadeep Jayasumana, Bernardino Romera-\nParedes, Vibhav Vineet, Zhizhong Su, Dalong Du, Chang\nHuang, and Philip H. S. Torr. Conditional random fields as\nrecurrent neural networks. In ICCV, 2015. 2\n[63] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela\nBarriuso, and Antonio Torralba. Semantic understanding of\nscenes through the ade20k dataset. arXiv preprint, 2016. 5"
    },
    {
      "bounding_box": [
        {
          "x": 1220,
          "y": 3054
        },
        {
          "x": 1263,
          "y": 3054
        },
        {
          "x": 1263,
          "y": 3091
        },
        {
          "x": 1220,
          "y": 3091
        }
      ],
      "category": "footer",
      "html": "<footer id='116' style='font-size:18px'>10</footer>",
      "id": 116,
      "page": 10,
      "text": "10"
    },
    {
      "bounding_box": [
        {
          "x": 206,
          "y": 303
        },
        {
          "x": 418,
          "y": 303
        },
        {
          "x": 418,
          "y": 356
        },
        {
          "x": 206,
          "y": 356
        }
      ],
      "category": "paragraph",
      "html": "<p id='117' style='font-size:22px'>Appendix</p>",
      "id": 117,
      "page": 11,
      "text": "Appendix"
    },
    {
      "bounding_box": [
        {
          "x": 206,
          "y": 391
        },
        {
          "x": 570,
          "y": 391
        },
        {
          "x": 570,
          "y": 438
        },
        {
          "x": 206,
          "y": 438
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='118' style='font-size:20px'>A. Visualizations</p>",
      "id": 118,
      "page": 11,
      "text": "A. Visualizations"
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 472
        },
        {
          "x": 1198,
          "y": 472
        },
        {
          "x": 1198,
          "y": 668
        },
        {
          "x": 202,
          "y": 668
        }
      ],
      "category": "paragraph",
      "html": "<p id='119' style='font-size:18px'>Position embedding Visualization of the learned position<br>embedding in Figure 7 shows that the model learns to en-<br>code distance within the image in the similarity of position<br>embeddings.</p>",
      "id": 119,
      "page": 11,
      "text": "Position embedding Visualization of the learned position\nembedding in Figure 7 shows that the model learns to en-\ncode distance within the image in the similarity of position\nembeddings."
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 692
        },
        {
          "x": 1197,
          "y": 692
        },
        {
          "x": 1197,
          "y": 938
        },
        {
          "x": 203,
          "y": 938
        }
      ],
      "category": "paragraph",
      "html": "<p id='120' style='font-size:14px'>Features Figure 9 shows the feature visualization of our<br>SETR-PUP. For the encoder, 24 output features from the 24<br>transformer layers namely Z1 - Z24 are collected. Mean-<br>while, 5 features (U1 - U5) right after each bilinear inter-<br>polation in the decoder head are visited.</p>",
      "id": 120,
      "page": 11,
      "text": "Features Figure 9 shows the feature visualization of our\nSETR-PUP. For the encoder, 24 output features from the 24\ntransformer layers namely Z1 - Z24 are collected. Mean-\nwhile, 5 features (U1 - U5) right after each bilinear inter-\npolation in the decoder head are visited."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 964
        },
        {
          "x": 1198,
          "y": 964
        },
        {
          "x": 1198,
          "y": 1210
        },
        {
          "x": 202,
          "y": 1210
        }
      ],
      "category": "paragraph",
      "html": "<p id='121' style='font-size:16px'>Attention maps Attention maps (Figure 10) in each trans-<br>former layer catch our interest. There are 16 heads and 24<br>layers in T-large. Similar to [1], a recursion perspective into<br>this problem is applied. Figure 8 shows the attention maps<br>of different selected spatial points (red).</p>",
      "id": 121,
      "page": 11,
      "text": "Attention maps Attention maps (Figure 10) in each trans-\nformer layer catch our interest. There are 16 heads and 24\nlayers in T-large. Similar to [1], a recursion perspective into\nthis problem is applied. Figure 8 shows the attention maps\nof different selected spatial points (red)."
    },
    {
      "bounding_box": [
        {
          "x": 266,
          "y": 1269
        },
        {
          "x": 1128,
          "y": 1269
        },
        {
          "x": 1128,
          "y": 2054
        },
        {
          "x": 266,
          "y": 2054
        }
      ],
      "category": "figure",
      "html": "<figure><img id='122' alt=\"Position embedding similarity\n1 1\nSimilarity\nrow\npatch\n15\nCosine\nInput\n30 -1\n1 15 30\nInput patch column\" data-coord=\"top-left:(266,1269); bottom-right:(1128,2054)\" /></figure>",
      "id": 122,
      "page": 11,
      "text": "Position embedding similarity\n1 1\nSimilarity\nrow\npatch\n15\nCosine\nInput\n30 -1\n1 15 30\nInput patch column"
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 2071
        },
        {
          "x": 1198,
          "y": 2071
        },
        {
          "x": 1198,
          "y": 2252
        },
        {
          "x": 202,
          "y": 2252
        }
      ],
      "category": "caption",
      "html": "<br><caption id='123' style='font-size:14px'>Figure 7. Similarity of position embeddings of SETR-PUP trained<br>on Pascal Context. Tiles show the cosine similarity between the<br>position embedding of the patch with the indicated row and col-<br>umn and the position embeddings of all other patches.</caption>",
      "id": 123,
      "page": 11,
      "text": "Figure 7. Similarity of position embeddings of SETR-PUP trained\non Pascal Context. Tiles show the cosine similarity between the\nposition embedding of the patch with the indicated row and col-\numn and the position embeddings of all other patches."
    },
    {
      "bounding_box": [
        {
          "x": 1401,
          "y": 1198
        },
        {
          "x": 2155,
          "y": 1198
        },
        {
          "x": 2155,
          "y": 1912
        },
        {
          "x": 1401,
          "y": 1912
        }
      ],
      "category": "figure",
      "html": "<br><figure><img id='124' alt=\"\" data-coord=\"top-left:(1401,1198); bottom-right:(2155,1912)\" /></figure>",
      "id": 124,
      "page": 11,
      "text": ""
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 1927
        },
        {
          "x": 2276,
          "y": 1927
        },
        {
          "x": 2276,
          "y": 2061
        },
        {
          "x": 1281,
          "y": 2061
        }
      ],
      "category": "caption",
      "html": "<br><caption id='125' style='font-size:14px'>Figure 8. The first and third columns show images from Pascal<br>Context. The second and fourth columns illustrate the attention<br>map of the picked points (red).</caption>",
      "id": 125,
      "page": 11,
      "text": "Figure 8. The first and third columns show images from Pascal\nContext. The second and fourth columns illustrate the attention\nmap of the picked points (red)."
    },
    {
      "bounding_box": [
        {
          "x": 1219,
          "y": 3055
        },
        {
          "x": 1259,
          "y": 3055
        },
        {
          "x": 1259,
          "y": 3092
        },
        {
          "x": 1219,
          "y": 3092
        }
      ],
      "category": "footer",
      "html": "<footer id='126' style='font-size:14px'>11</footer>",
      "id": 126,
      "page": 11,
      "text": "11"
    },
    {
      "bounding_box": [
        {
          "x": 350,
          "y": 553
        },
        {
          "x": 2124,
          "y": 553
        },
        {
          "x": 2124,
          "y": 1145
        },
        {
          "x": 350,
          "y": 1145
        }
      ],
      "category": "figure",
      "html": "<figure><img id='127' alt=\"\" data-coord=\"top-left:(350,553); bottom-right:(2124,1145)\" /></figure>",
      "id": 127,
      "page": 12,
      "text": ""
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 1140
        },
        {
          "x": 2276,
          "y": 1140
        },
        {
          "x": 2276,
          "y": 1240
        },
        {
          "x": 201,
          "y": 1240
        }
      ],
      "category": "caption",
      "html": "<br><caption id='128' style='font-size:16px'>Figure 9. Visualization of output feature of layer Z1 - Z24 and U1 - U5 of SETR-PUP trained on Pascal Context. Best view in color.<br>First row: The input image. Second row: Layer Z1-Z12. Third row: Layer Z13-Z24. Fourth row: Layer U1 - U5.</caption>",
      "id": 128,
      "page": 12,
      "text": "Figure 9. Visualization of output feature of layer Z1 - Z24 and U1 - U5 of SETR-PUP trained on Pascal Context. Best view in color.\nFirst row: The input image. Second row: Layer Z1-Z12. Third row: Layer Z13-Z24. Fourth row: Layer U1 - U5."
    },
    {
      "bounding_box": [
        {
          "x": 351,
          "y": 1777
        },
        {
          "x": 2120,
          "y": 1777
        },
        {
          "x": 2120,
          "y": 2640
        },
        {
          "x": 351,
          "y": 2640
        }
      ],
      "category": "figure",
      "html": "<figure><img id='129' style='font-size:14px' alt=\"법불\nEXI\n1ONGBOTTOM\nENS\" data-coord=\"top-left:(351,1777); bottom-right:(2120,2640)\" /></figure>",
      "id": 129,
      "page": 12,
      "text": "법불\nEXI\n1ONGBOTTOM\nENS"
    },
    {
      "bounding_box": [
        {
          "x": 567,
          "y": 2661
        },
        {
          "x": 1909,
          "y": 2661
        },
        {
          "x": 1909,
          "y": 2705
        },
        {
          "x": 567,
          "y": 2705
        }
      ],
      "category": "caption",
      "html": "<br><caption id='130' style='font-size:18px'>Figure 10. More examples of attention maps from SETR-PUP trained on Pascal Context.</caption>",
      "id": 130,
      "page": 12,
      "text": "Figure 10. More examples of attention maps from SETR-PUP trained on Pascal Context."
    },
    {
      "bounding_box": [
        {
          "x": 1216,
          "y": 3051
        },
        {
          "x": 1264,
          "y": 3051
        },
        {
          "x": 1264,
          "y": 3095
        },
        {
          "x": 1216,
          "y": 3095
        }
      ],
      "category": "footer",
      "html": "<footer id='131' style='font-size:20px'>12</footer>",
      "id": 131,
      "page": 12,
      "text": "12"
    }
  ]
}