{
    "id": "329e4140-0f94-11ef-8828-426932df3dcf",
    "pdf_path": "./pdf/AI_VIT_X/1502.05477v5.pdf",
    "elements": [
        {
            "bounding_box": [
                {
                    "x": 811,
                    "y": 369
                },
                {
                    "x": 1677,
                    "y": 369
                },
                {
                    "x": 1677,
                    "y": 441
                },
                {
                    "x": 811,
                    "y": 441
                }
            ],
            "category": "paragraph",
            "html": "<p id='0' style='font-size:22px'>Trust Region Policy Optimization</p>",
            "id": 0,
            "page": 1,
            "text": "Trust Region Policy Optimization"
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 583
                },
                {
                    "x": 510,
                    "y": 583
                },
                {
                    "x": 510,
                    "y": 823
                },
                {
                    "x": 225,
                    "y": 823
                }
            ],
            "category": "paragraph",
            "html": "<p id='1' style='font-size:18px'>John Schulman<br>Sergey Levine<br>Philipp Moritz<br>Michael Jordan<br>Pieter Abbeel</p>",
            "id": 1,
            "page": 1,
            "text": "John Schulman Sergey Levine Philipp Moritz Michael Jordan Pieter Abbeel"
        },
        {
            "bounding_box": [
                {
                    "x": 1621,
                    "y": 587
                },
                {
                    "x": 2233,
                    "y": 587
                },
                {
                    "x": 2233,
                    "y": 827
                },
                {
                    "x": 1621,
                    "y": 827
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='2' style='font-size:16px'>JOSCHU@EECS.BERKELEY.EDU<br>SLEVINE@EECS.BERKELEY.EDU<br>PCMORITZ@EECS.BERKELEY.EDU<br>JORDAN@CS.BERKELEY.EDU<br>PABBEEL @CS.BERKELEY.EDU</p>",
            "id": 2,
            "page": 1,
            "text": "JOSCHU@EECS.BERKELEY.EDU SLEVINE@EECS.BERKELEY.EDU PCMORITZ@EECS.BERKELEY.EDU JORDAN@CS.BERKELEY.EDU PABBEEL @CS.BERKELEY.EDU"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 831
                },
                {
                    "x": 1849,
                    "y": 831
                },
                {
                    "x": 1849,
                    "y": 883
                },
                {
                    "x": 224,
                    "y": 883
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='3' style='font-size:20px'>University of California, Berkeley, Department of Electrical Engineering and Computer Sciences</p>",
            "id": 3,
            "page": 1,
            "text": "University of California, Berkeley, Department of Electrical Engineering and Computer Sciences"
        },
        {
            "bounding_box": [
                {
                    "x": 620,
                    "y": 953
                },
                {
                    "x": 817,
                    "y": 953
                },
                {
                    "x": 817,
                    "y": 1007
                },
                {
                    "x": 620,
                    "y": 1007
                }
            ],
            "category": "paragraph",
            "html": "<p id='4' style='font-size:22px'>Abstract</p>",
            "id": 4,
            "page": 1,
            "text": "Abstract"
        },
        {
            "bounding_box": [
                {
                    "x": 306,
                    "y": 1021
                },
                {
                    "x": 1130,
                    "y": 1021
                },
                {
                    "x": 1130,
                    "y": 1800
                },
                {
                    "x": 306,
                    "y": 1800
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='5' style='font-size:16px'>We describe an iterative procedure for optimizing<br>policies, with guaranteed monotonic improve-<br>ment. By making several approximations to the<br>theoretically-justified procedure, we develop a<br>practical algorithm, called Trust Region Policy<br>Optimization (TRPO). This algorithm is similar<br>to natural policy gradient methods and is effec-<br>tive for optimizing large nonlinear policies such<br>as neural networks. Our experiments demon-<br>strate its robust performance on a wide variety<br>of tasks: learning simulated robotic swimming,<br>hopping, and walking gaits; and playing Atari<br>games using images of the screen as input. De-<br>spite its approximations that deviate from the<br>theory, TRPO tends to give monotonic improve-<br>ment, with little tuning of hyperparameters.</p>",
            "id": 5,
            "page": 1,
            "text": "We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters."
        },
        {
            "bounding_box": [
                {
                    "x": 227,
                    "y": 1888
                },
                {
                    "x": 579,
                    "y": 1888
                },
                {
                    "x": 579,
                    "y": 1941
                },
                {
                    "x": 227,
                    "y": 1941
                }
            ],
            "category": "paragraph",
            "html": "<p id='6' style='font-size:20px'>1 Introduction</p>",
            "id": 6,
            "page": 1,
            "text": "1 Introduction"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1965
                },
                {
                    "x": 1214,
                    "y": 1965
                },
                {
                    "x": 1214,
                    "y": 2591
                },
                {
                    "x": 224,
                    "y": 2591
                }
            ],
            "category": "paragraph",
            "html": "<p id='7' style='font-size:16px'>Most algorithms for policy optimization can be classified<br>into three broad categories: (1) policy iteration methods,<br>which alternate between estimating the value function un-<br>der the current policy and improving the policy (Bertsekas,<br>2005); (2) policy gradient methods, which use an estima-<br>tor of the gradient of the expected return (total reward) ob-<br>tained from sample trajectories (Peters & Schaal, 2008a)<br>(and which, as we later discuss, have a close connection to<br>policy iteration); and (3) derivative-free optimization meth-<br>ods, such as the cross-entropy method (CEM) and covari-<br>ance matrix adaptation (CMA), which treat the return as a<br>black box function to be optimized in terms of the policy<br>parameters (Szita & L�rincz, 2006).</p>",
            "id": 7,
            "page": 1,
            "text": "Most algorithms for policy optimization can be classified into three broad categories: (1) policy iteration methods, which alternate between estimating the value function under the current policy and improving the policy (Bertsekas, 2005); (2) policy gradient methods, which use an estimator of the gradient of the expected return (total reward) obtained from sample trajectories (Peters & Schaal, 2008a) (and which, as we later discuss, have a close connection to policy iteration); and (3) derivative-free optimization methods, such as the cross-entropy method (CEM) and covariance matrix adaptation (CMA), which treat the return as a black box function to be optimized in terms of the policy parameters (Szita & L�rincz, 2006)."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2612
                },
                {
                    "x": 1214,
                    "y": 2612
                },
                {
                    "x": 1214,
                    "y": 2809
                },
                {
                    "x": 224,
                    "y": 2809
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='8' style='font-size:16px'>General derivative-free stochastic optimization methods<br>such as CEM and CMA are preferred on many prob-<br>lems, because they achieve good results while being sim-<br>ple to understand and implement. For example, while</p>",
            "id": 8,
            "page": 1,
            "text": "General derivative-free stochastic optimization methods such as CEM and CMA are preferred on many problems, because they achieve good results while being simple to understand and implement. For example, while"
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 2846
                },
                {
                    "x": 1218,
                    "y": 2846
                },
                {
                    "x": 1218,
                    "y": 2972
                },
                {
                    "x": 225,
                    "y": 2972
                }
            ],
            "category": "paragraph",
            "html": "<p id='9' style='font-size:14px'>Proceedings of the 31st International Conference on Machine<br>Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copy-<br>right 2015 by the author(s).</p>",
            "id": 9,
            "page": 1,
            "text": "Proceedings of the 31st International Conference on Machine Learning, Lille, France, 2015. JMLR: W&CP volume 37. Copyright 2015 by the author(s)."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 957
                },
                {
                    "x": 2264,
                    "y": 957
                },
                {
                    "x": 2264,
                    "y": 1822
                },
                {
                    "x": 1272,
                    "y": 1822
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='10' style='font-size:18px'>Tetris is a classic benchmark problem for approximate dy-<br>namic programming (ADP) methods, stochastic optimiza-<br>tion methods are difficult to beat on this task (Gabillon<br>et al., 2013). For continuous control problems, methods<br>like CMA have been successful at learning control poli-<br>cies for challenging tasks like locomotion when provided<br>with hand-engineered policy classes with low-dimensional<br>parameterizations (Wampler & Popovic, 2009). The in-<br>ability of ADP and gradient-based methods to consistently<br>beat gradient-free random search is unsatisfying, since<br>gradient-based optimization algorithms enjoy much better<br>sample complexity guarantees than gradient-free methods<br>(Nemirovski, 2005). Continuous gradient-based optimiza-<br>tion has been very successful at learning function approxi-<br>mators for supervised learning tasks with huge numbers of<br>parameters, and extending their success to reinforcement<br>learning would allow for efficient training of complex and<br>powerful policies.</p>",
            "id": 10,
            "page": 1,
            "text": "Tetris is a classic benchmark problem for approximate dynamic programming (ADP) methods, stochastic optimization methods are difficult to beat on this task (Gabillon , 2013). For continuous control problems, methods like CMA have been successful at learning control policies for challenging tasks like locomotion when provided with hand-engineered policy classes with low-dimensional parameterizations (Wampler & Popovic, 2009). The inability of ADP and gradient-based methods to consistently beat gradient-free random search is unsatisfying, since gradient-based optimization algorithms enjoy much better sample complexity guarantees than gradient-free methods (Nemirovski, 2005). Continuous gradient-based optimization has been very successful at learning function approximators for supervised learning tasks with huge numbers of parameters, and extending their success to reinforcement learning would allow for efficient training of complex and powerful policies."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1845
                },
                {
                    "x": 2264,
                    "y": 1845
                },
                {
                    "x": 2264,
                    "y": 2660
                },
                {
                    "x": 1273,
                    "y": 2660
                }
            ],
            "category": "paragraph",
            "html": "<p id='11' style='font-size:16px'>In this article, we first prove that minimizing a certain sur-<br>rogate objective function guarantees policy improvement<br>with non-trivial step sizes. Then we make a series of ap-<br>proximations to the theoretically-justified algorithm, yield-<br>ing a practical algorithm, which we call trust region pol-<br>icy optimization (TRPO). We describe two variants of this<br>algorithm: first, the single-path method, which can be ap-<br>plied in the model-free setting; second, the vine method,<br>which requires the system to be restored to particular states,<br>which is typically only possible in simulation. These al-<br>gorithms are scalable and can optimize nonlinear policies<br>with tens of thousands of parameters, which have previ-<br>ously posed a major challenge for model-free policy search<br>(Deisenroth et al., 2013). In our experiments, we show that<br>the same TRPO methods can learn complex policies for<br>swimming, hopping, and walking, as well as playing Atari<br>games directly from raw images.</p>",
            "id": 11,
            "page": 1,
            "text": "In this article, we first prove that minimizing a certain surrogate objective function guarantees policy improvement with non-trivial step sizes. Then we make a series of approximations to the theoretically-justified algorithm, yielding a practical algorithm, which we call trust region policy optimization (TRPO). We describe two variants of this algorithm: first, the single-path method, which can be applied in the model-free setting; second, the vine method, which requires the system to be restored to particular states, which is typically only possible in simulation. These algorithms are scalable and can optimize nonlinear policies with tens of thousands of parameters, which have previously posed a major challenge for model-free policy search (Deisenroth , 2013). In our experiments, we show that the same TRPO methods can learn complex policies for swimming, hopping, and walking, as well as playing Atari games directly from raw images."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2683
                },
                {
                    "x": 1649,
                    "y": 2683
                },
                {
                    "x": 1649,
                    "y": 2741
                },
                {
                    "x": 1273,
                    "y": 2741
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='12' style='font-size:20px'>2 Preliminaries</p>",
            "id": 12,
            "page": 1,
            "text": "2 Preliminaries"
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 2763
                },
                {
                    "x": 2264,
                    "y": 2763
                },
                {
                    "x": 2264,
                    "y": 2957
                },
                {
                    "x": 1274,
                    "y": 2957
                }
            ],
            "category": "paragraph",
            "html": "<p id='13' style='font-size:16px'>Consider an infinite-horizon discounted Markov decision<br>process (MDP), defined by the tuple (S, A, P, r, Po, 2),<br>where S is a finite set of states, A is a finite set of actions,<br>P : S x A x S → R is the transition probability distri-</p>",
            "id": 13,
            "page": 1,
            "text": "Consider an infinite-horizon discounted Markov decision process (MDP), defined by the tuple (S, A, P, r, Po, 2), where S is a finite set of states, A is a finite set of actions, P : S x A x S → R is the transition probability distri-"
        },
        {
            "bounding_box": [
                {
                    "x": 64,
                    "y": 875
                },
                {
                    "x": 150,
                    "y": 875
                },
                {
                    "x": 150,
                    "y": 2345
                },
                {
                    "x": 64,
                    "y": 2345
                }
            ],
            "category": "footer",
            "html": "<br><footer id='14' style='font-size:14px'>2017<br>Apr<br>20<br>[cs.LG]<br>arXiv:1502.05477v5</footer>",
            "id": 14,
            "page": 1,
            "text": "2017 Apr 20 [cs.LG] arXiv:1502.05477v5"
        },
        {
            "bounding_box": [
                {
                    "x": 970,
                    "y": 191
                },
                {
                    "x": 1516,
                    "y": 191
                },
                {
                    "x": 1516,
                    "y": 235
                },
                {
                    "x": 970,
                    "y": 235
                }
            ],
            "category": "header",
            "html": "<header id='15' style='font-size:18px'>Trust Region Policy Optimization</header>",
            "id": 15,
            "page": 2,
            "text": "Trust Region Policy Optimization"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 285
                },
                {
                    "x": 1213,
                    "y": 285
                },
                {
                    "x": 1213,
                    "y": 427
                },
                {
                    "x": 223,
                    "y": 427
                }
            ],
            "category": "paragraph",
            "html": "<p id='16' style='font-size:14px'>bution, r : S → R is the reward function, Po : S → R is<br>the distribution of the initial state SO, and 2 E (0,1) is the<br>discount factor.</p>",
            "id": 16,
            "page": 2,
            "text": "bution, r : S → R is the reward function, Po : S → R is the distribution of the initial state SO, and 2 E (0,1) is the discount factor."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 453
                },
                {
                    "x": 1211,
                    "y": 453
                },
                {
                    "x": 1211,
                    "y": 549
                },
                {
                    "x": 223,
                    "y": 549
                }
            ],
            "category": "paragraph",
            "html": "<p id='17' style='font-size:14px'>Let � denote a stochastic policy � : S x A → [0, 1], and<br>let 7(�) denote its expected discounted reward:</p>",
            "id": 17,
            "page": 2,
            "text": "Let � denote a stochastic policy � : S x A → , and let 7(�) denote its expected discounted reward:"
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 811
                },
                {
                    "x": 1212,
                    "y": 811
                },
                {
                    "x": 1212,
                    "y": 955
                },
                {
                    "x": 222,
                    "y": 955
                }
            ],
            "category": "paragraph",
            "html": "<p id='18' style='font-size:16px'>We will use the following standard definitions of the state-<br>action value function Q�, the value function V�, and the<br>advantage function A�:</p>",
            "id": 18,
            "page": 2,
            "text": "We will use the following standard definitions of the stateaction value function Q�, the value function V�, and the advantage function A�:"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1420
                },
                {
                    "x": 1211,
                    "y": 1420
                },
                {
                    "x": 1211,
                    "y": 1612
                },
                {
                    "x": 223,
                    "y": 1612
                }
            ],
            "category": "paragraph",
            "html": "<p id='19' style='font-size:16px'>The following useful identity expresses the expected return<br>of another policy 元 in terms of the advantage over �, accu-<br>mulated over timesteps (see Kakade & Langford (2002) or<br>Appendix A for proof):</p>",
            "id": 19,
            "page": 2,
            "text": "The following useful identity expresses the expected return of another policy 元 in terms of the advantage over �, accumulated over timesteps (see Kakade & Langford (2002) or Appendix A for proof):"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1807
                },
                {
                    "x": 1214,
                    "y": 1807
                },
                {
                    "x": 1214,
                    "y": 1953
                },
                {
                    "x": 223,
                    "y": 1953
                }
            ],
            "category": "paragraph",
            "html": "<p id='20' style='font-size:16px'>where the notation Eso,ao,···~元 [· . ] indicates that actions<br>be the (unnormalized)<br>are sampled at ~ 뉴(·|st). Let ��<br>discounted visitation frequencies</p>",
            "id": 20,
            "page": 2,
            "text": "where the notation Eso,ao,···~元 [· . ] indicates that actions be the (unnormalized) are sampled at ~ 뉴(·|st). Let �� discounted visitation frequencies"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2072
                },
                {
                    "x": 1212,
                    "y": 2072
                },
                {
                    "x": 1212,
                    "y": 2216
                },
                {
                    "x": 223,
                    "y": 2216
                }
            ],
            "category": "paragraph",
            "html": "<p id='21' style='font-size:14px'>where SO ~ Po and the actions are chosen according to �.<br>We can rewrite Equation (1) with a sum over states instead<br>of timesteps:</p>",
            "id": 21,
            "page": 2,
            "text": "where SO ~ Po and the actions are chosen according to �. We can rewrite Equation (1) with a sum over states instead of timesteps:"
        },
        {
            "bounding_box": [
                {
                    "x": 575,
                    "y": 2390
                },
                {
                    "x": 611,
                    "y": 2390
                },
                {
                    "x": 611,
                    "y": 2410
                },
                {
                    "x": 575,
                    "y": 2410
                }
            ],
            "category": "paragraph",
            "html": "<p id='22' style='font-size:14px'>8</p>",
            "id": 22,
            "page": 2,
            "text": "8"
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 2655
                },
                {
                    "x": 1214,
                    "y": 2655
                },
                {
                    "x": 1214,
                    "y": 2994
                },
                {
                    "x": 222,
                    "y": 2994
                }
            ],
            "category": "paragraph",
            "html": "<p id='23' style='font-size:16px'>This equation implies that any policy update � → 元 that<br>has a nonnegative expected advantage at every state s,<br>i.e., Ea �(a|s)A�(s, a) ≥ 0, is guaranteed to increase<br>the policy performance 7, or leave it constant in the case<br>that the expected advantage is zero everywhere. This im-<br>plies the classic result that the update performed by ex-<br>act policy iteration, which uses the deterministic policy</p>",
            "id": 23,
            "page": 2,
            "text": "This equation implies that any policy update � → 元 that has a nonnegative expected advantage at every state s, i.e., Ea �(a|s)A�(s, a) ≥ 0, is guaranteed to increase the policy performance 7, or leave it constant in the case that the expected advantage is zero everywhere. This implies the classic result that the update performed by exact policy iteration, which uses the deterministic policy"
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 284
                },
                {
                    "x": 2264,
                    "y": 284
                },
                {
                    "x": 2264,
                    "y": 810
                },
                {
                    "x": 1270,
                    "y": 810
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='24' style='font-size:16px'>뉴(s) = arg maxa A�(s, a), improves the policy if there is<br>at least one state-action pair with a positive advantage value<br>and nonzero state visitation probability, otherwise the algo-<br>rithm has converged to the optimal policy. However, in the<br>approximate setting, it will typically be unavoidable, due<br>to estimation and approximation error, that there will be<br>some states s for which the expected advantage is negative,<br>that is, �a �(a|s)A�(s, a) < 0. The complex dependency<br>of P元(S) on 元 makes Equation (2) difficult to optimize di-<br>rectly. Instead, we introduce the following local approxi-<br>mation to 7:</p>",
            "id": 24,
            "page": 2,
            "text": "뉴(s) = arg maxa A�(s, a), improves the policy if there is at least one state-action pair with a positive advantage value and nonzero state visitation probability, otherwise the algorithm has converged to the optimal policy. However, in the approximate setting, it will typically be unavoidable, due to estimation and approximation error, that there will be some states s for which the expected advantage is negative, that is, �a �(a|s)A�(s, a) < 0. The complex dependency of P元(S) on 元 makes Equation (2) difficult to optimize directly. Instead, we introduce the following local approximation to 7:"
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 982
                },
                {
                    "x": 2263,
                    "y": 982
                },
                {
                    "x": 2263,
                    "y": 1318
                },
                {
                    "x": 1271,
                    "y": 1318
                }
            ],
            "category": "paragraph",
            "html": "<p id='25' style='font-size:16px'>Note that L� uses the visitation frequency �� rather than<br>P元, ignoring changes in state visitation density due to<br>changes in the policy. However, if we have a parameter-<br>ized policy ��, where ��(a|s) is a differentiable function<br>of the parameter vector 0, then L� matches 7 to first order<br>(see Kakade & Langford (2002)). That is, for any parame-<br>ter value 00,</p>",
            "id": 25,
            "page": 2,
            "text": "Note that L� uses the visitation frequency �� rather than P元, ignoring changes in state visitation density due to changes in the policy. However, if we have a parameterized policy ��, where ��(a|s) is a differentiable function of the parameter vector 0, then L� matches 7 to first order (see Kakade & Langford (2002)). That is, for any parameter value 00,"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1519
                },
                {
                    "x": 2262,
                    "y": 1519
                },
                {
                    "x": 2262,
                    "y": 1662
                },
                {
                    "x": 1272,
                    "y": 1662
                }
            ],
            "category": "paragraph",
            "html": "<p id='26' style='font-size:16px'>Equation (4) implies that a sufficiently small step ��o → 元<br>will also improve 7, but does not give<br>that improves L��old<br>us any guidance on how big of a step to take.</p>",
            "id": 26,
            "page": 2,
            "text": "Equation (4) implies that a sufficiently small step ��o → 元 will also improve 7, but does not give that improves L��old us any guidance on how big of a step to take."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1686
                },
                {
                    "x": 2263,
                    "y": 1686
                },
                {
                    "x": 2263,
                    "y": 2022
                },
                {
                    "x": 1272,
                    "y": 2022
                }
            ],
            "category": "paragraph",
            "html": "<p id='27' style='font-size:16px'>To address this issue, Kakade & Langford (2002) proposed<br>a policy updating scheme called conservative policy iter-<br>ation, for which they could provide explicit lower bounds<br>on the improvement of 7. To define the conservative pol-<br>icy iteration update, let �old denote the current policy, and<br>let �' = arg max�, L�old (�'). The new policy �new was<br>defined to be the following mixture:</p>",
            "id": 27,
            "page": 2,
            "text": "To address this issue, Kakade & Langford (2002) proposed a policy updating scheme called conservative policy iteration, for which they could provide explicit lower bounds on the improvement of 7. To define the conservative policy iteration update, let �old denote the current policy, and let �' = arg max�, L�old (�'). The new policy �new was defined to be the following mixture:"
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 2148
                },
                {
                    "x": 2244,
                    "y": 2148
                },
                {
                    "x": 2244,
                    "y": 2196
                },
                {
                    "x": 1275,
                    "y": 2196
                }
            ],
            "category": "paragraph",
            "html": "<p id='28' style='font-size:16px'>Kakade and Langford derived the following lower bound:</p>",
            "id": 28,
            "page": 2,
            "text": "Kakade and Langford derived the following lower bound:"
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 2446
                },
                {
                    "x": 2262,
                    "y": 2446
                },
                {
                    "x": 2262,
                    "y": 2735
                },
                {
                    "x": 1271,
                    "y": 2735
                }
            ],
            "category": "paragraph",
            "html": "<p id='29' style='font-size:16px'>(We have modified it to make it slightly weaker but sim-<br>pler.) Note, however, that SO far this bound only applies<br>to mixture policies generated by Equation (5). This policy<br>class is unwieldy and restrictive in practice, and it is desir-<br>able for a practical policy update scheme to be applicable<br>to all general stochastic policy classes.</p>",
            "id": 29,
            "page": 2,
            "text": "(We have modified it to make it slightly weaker but simpler.) Note, however, that SO far this bound only applies to mixture policies generated by Equation (5). This policy class is unwieldy and restrictive in practice, and it is desirable for a practical policy update scheme to be applicable to all general stochastic policy classes."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 2760
                },
                {
                    "x": 2208,
                    "y": 2760
                },
                {
                    "x": 2208,
                    "y": 2871
                },
                {
                    "x": 1274,
                    "y": 2871
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='30' style='font-size:20px'>3 Monotonic Improvement Guarantee for<br>General Stochastic Policies</p>",
            "id": 30,
            "page": 2,
            "text": "3 Monotonic Improvement Guarantee for General Stochastic Policies"
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 2896
                },
                {
                    "x": 2262,
                    "y": 2896
                },
                {
                    "x": 2262,
                    "y": 2994
                },
                {
                    "x": 1274,
                    "y": 2994
                }
            ],
            "category": "paragraph",
            "html": "<p id='31' style='font-size:18px'>Equation (6), which applies to conservative policy iteration,<br>implies that a policy update that improves the right-hand</p>",
            "id": 31,
            "page": 2,
            "text": "Equation (6), which applies to conservative policy iteration, implies that a policy update that improves the right-hand"
        },
        {
            "bounding_box": [
                {
                    "x": 970,
                    "y": 192
                },
                {
                    "x": 1516,
                    "y": 192
                },
                {
                    "x": 1516,
                    "y": 235
                },
                {
                    "x": 970,
                    "y": 235
                }
            ],
            "category": "header",
            "html": "<header id='32' style='font-size:20px'>Trust Region Policy Optimization</header>",
            "id": 32,
            "page": 3,
            "text": "Trust Region Policy Optimization"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 284
                },
                {
                    "x": 1214,
                    "y": 284
                },
                {
                    "x": 1214,
                    "y": 821
                },
                {
                    "x": 224,
                    "y": 821
                }
            ],
            "category": "paragraph",
            "html": "<p id='33' style='font-size:16px'>side is guaranteed to improve the true performance 7. Our<br>principal theoretical result is that the policy improvement<br>bound in Equation (6) can be extended to general stochas-<br>tic policies, rather than just mixture polices, by replacing a<br>with a distance measure between � and 元, and changing the<br>constant E appropriately. Since mixture policies are rarely<br>used in practice, this result is crucial for extending the im-<br>provement guarantee to practical problems. The particular<br>distance measure we use is the total variation divergence,<br>which is defined by DTv(p II q) = 12 Eilpi - qi| for dis-<br>crete probability distributions p,q. 1 Define Dmax (�, 元) as</p>",
            "id": 33,
            "page": 3,
            "text": "side is guaranteed to improve the true performance 7. Our principal theoretical result is that the policy improvement bound in Equation (6) can be extended to general stochastic policies, rather than just mixture polices, by replacing a with a distance measure between � and 元, and changing the constant E appropriately. Since mixture policies are rarely used in practice, this result is crucial for extending the improvement guarantee to practical problems. The particular distance measure we use is the total variation divergence, which is defined by DTv(p II q) = 12 Eilpi - qi| for discrete probability distributions p,q. 1 Define Dmax (�, 元) as"
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 957
                },
                {
                    "x": 1210,
                    "y": 957
                },
                {
                    "x": 1210,
                    "y": 1057
                },
                {
                    "x": 222,
                    "y": 1057
                }
            ],
            "category": "paragraph",
            "html": "<p id='34' style='font-size:16px'>Theorem 1. Let a = Dmax (�old, �new). Then the follow-<br>ing bound holds:</p>",
            "id": 34,
            "page": 3,
            "text": "Theorem 1. Let a = Dmax (�old, �new). Then the following bound holds:"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1333
                },
                {
                    "x": 1212,
                    "y": 1333
                },
                {
                    "x": 1212,
                    "y": 1623
                },
                {
                    "x": 223,
                    "y": 1623
                }
            ],
            "category": "paragraph",
            "html": "<p id='35' style='font-size:14px'>We provide two proofs in the appendix. The first proof ex-<br>tends Kakade and Langford's result using the fact that the<br>random variables from two distributions with total varia-<br>tion divergence less than a can be coupled, SO that they are<br>equal with probability 1 - a. The second proof uses per-<br>turbation theory.</p>",
            "id": 35,
            "page": 3,
            "text": "We provide two proofs in the appendix. The first proof extends Kakade and Langford's result using the fact that the random variables from two distributions with total variation divergence less than a can be coupled, SO that they are equal with probability 1 - a. The second proof uses perturbation theory."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1647
                },
                {
                    "x": 1213,
                    "y": 1647
                },
                {
                    "x": 1213,
                    "y": 1887
                },
                {
                    "x": 224,
                    "y": 1887
                }
            ],
            "category": "paragraph",
            "html": "<p id='36' style='font-size:20px'>Next, we note the following relationship between the to-<br>tal variation divergence and the KL divergence (Pollard<br>(2000), Ch. 3): DTv(p II q)2 ≤ DKL(p II q). Let<br>Dmax (�, 元) = max, DKL (�(·|s) II 元(·|s)). The follow-<br>ing bound then follows directly from Theorem 1:</p>",
            "id": 36,
            "page": 3,
            "text": "Next, we note the following relationship between the total variation divergence and the KL divergence (Pollard (2000), Ch. 3): DTv(p II q)2 ≤ DKL(p II q). Let Dmax (�, 元) = max, DKL (�(·|s) II 元(·|s)). The following bound then follows directly from Theorem 1:"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2121
                },
                {
                    "x": 1212,
                    "y": 2121
                },
                {
                    "x": 1212,
                    "y": 2312
                },
                {
                    "x": 223,
                    "y": 2312
                }
            ],
            "category": "paragraph",
            "html": "<p id='37' style='font-size:16px'>Algorithm 1 describes an approximate policy iteration<br>scheme based on the policy improvement bound in Equa-<br>tion (9). Note that for now, we assume exact evaluation of<br>the advantage values A�.</p>",
            "id": 37,
            "page": 3,
            "text": "Algorithm 1 describes an approximate policy iteration scheme based on the policy improvement bound in Equation (9). Note that for now, we assume exact evaluation of the advantage values A�."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2337
                },
                {
                    "x": 1213,
                    "y": 2337
                },
                {
                    "x": 1213,
                    "y": 2532
                },
                {
                    "x": 224,
                    "y": 2532
                }
            ],
            "category": "paragraph",
            "html": "<p id='38' style='font-size:16px'>It follows from Equation (9) that Algorithm 1 is guaranteed<br>to generate a monotonically improving sequence of policies<br>7(�o) ≤ 7(�1) ≤ 7(�2) ≤ · · · · To see this, let Mi(�) =<br>L�i (�) - CDmax (�i, �). Then</p>",
            "id": 38,
            "page": 3,
            "text": "It follows from Equation (9) that Algorithm 1 is guaranteed to generate a monotonically improving sequence of policies 7(�o) ≤ 7(�1) ≤ 7(�2) ≤ · · · · To see this, let Mi(�) = L�i (�) - CDmax (�i, �). Then"
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 2776
                },
                {
                    "x": 1211,
                    "y": 2776
                },
                {
                    "x": 1211,
                    "y": 2876
                },
                {
                    "x": 225,
                    "y": 2876
                }
            ],
            "category": "paragraph",
            "html": "<p id='39' style='font-size:18px'>Thus, by maximizing Mi at each iteration, we guarantee<br>that the true objective 7 is non-decreasing. This algorithm</p>",
            "id": 39,
            "page": 3,
            "text": "Thus, by maximizing Mi at each iteration, we guarantee that the true objective 7 is non-decreasing. This algorithm"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2907
                },
                {
                    "x": 1213,
                    "y": 2907
                },
                {
                    "x": 1213,
                    "y": 2993
                },
                {
                    "x": 223,
                    "y": 2993
                }
            ],
            "category": "paragraph",
            "html": "<p id='40' style='font-size:14px'>1 Our result is straightforward to extend to continuous states<br>and actions by replacing the sums with integrals.</p>",
            "id": 40,
            "page": 3,
            "text": "1 Our result is straightforward to extend to continuous states and actions by replacing the sums with integrals."
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 285
                },
                {
                    "x": 2260,
                    "y": 285
                },
                {
                    "x": 2260,
                    "y": 377
                },
                {
                    "x": 1276,
                    "y": 377
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='41' style='font-size:18px'>Algorithm 1 Policy iteration algorithm guaranteeing non-<br>decreasing expected return 7</p>",
            "id": 41,
            "page": 3,
            "text": "Algorithm 1 Policy iteration algorithm guaranteeing nondecreasing expected return 7"
        },
        {
            "bounding_box": [
                {
                    "x": 1315,
                    "y": 387
                },
                {
                    "x": 1538,
                    "y": 387
                },
                {
                    "x": 1538,
                    "y": 435
                },
                {
                    "x": 1315,
                    "y": 435
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='42' style='font-size:16px'>Initialize �0.</p>",
            "id": 42,
            "page": 3,
            "text": "Initialize �0."
        },
        {
            "bounding_box": [
                {
                    "x": 1314,
                    "y": 420
                },
                {
                    "x": 2123,
                    "y": 420
                },
                {
                    "x": 2123,
                    "y": 581
                },
                {
                    "x": 1314,
                    "y": 581
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='43' style='font-size:16px'>for i = 0, 1, 2, · · · until convergence do<br>Compute all advantage values A�i (s, a).<br>Solve the constrained optimization problem</p>",
            "id": 43,
            "page": 3,
            "text": "for i = 0, 1, 2, · · · until convergence do Compute all advantage values A�i (s, a). Solve the constrained optimization problem"
        },
        {
            "bounding_box": [
                {
                    "x": 1315,
                    "y": 915
                },
                {
                    "x": 1451,
                    "y": 915
                },
                {
                    "x": 1451,
                    "y": 953
                },
                {
                    "x": 1315,
                    "y": 953
                }
            ],
            "category": "paragraph",
            "html": "<p id='44' style='font-size:16px'>end for</p>",
            "id": 44,
            "page": 3,
            "text": "end for"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1041
                },
                {
                    "x": 2264,
                    "y": 1041
                },
                {
                    "x": 2264,
                    "y": 1328
                },
                {
                    "x": 1272,
                    "y": 1328
                }
            ],
            "category": "paragraph",
            "html": "<p id='45' style='font-size:16px'>is a type of minorization-maximization (MM) algorithm<br>(Hunter & Lange, 2004), which is a class of methods that<br>also includes expectation maximization. In the terminol-<br>ogy of MM algorithms, Mi is the surrogate function that<br>minorizes 7 with equality at �i. This algorithm is also rem-<br>iniscent of proximal gradient methods and mirror descent.</p>",
            "id": 45,
            "page": 3,
            "text": "is a type of minorization-maximization (MM) algorithm (Hunter & Lange, 2004), which is a class of methods that also includes expectation maximization. In the terminology of MM algorithms, Mi is the surrogate function that minorizes 7 with equality at �i. This algorithm is also reminiscent of proximal gradient methods and mirror descent."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1351
                },
                {
                    "x": 2263,
                    "y": 1351
                },
                {
                    "x": 2263,
                    "y": 1546
                },
                {
                    "x": 1273,
                    "y": 1546
                }
            ],
            "category": "paragraph",
            "html": "<p id='46' style='font-size:16px'>Trust region policy optimization, which we propose in the<br>following section, is an approximation to Algorithm 1,<br>which uses a constraint on the KL divergence rather than<br>a penalty to robustly allow large updates.</p>",
            "id": 46,
            "page": 3,
            "text": "Trust region policy optimization, which we propose in the following section, is an approximation to Algorithm 1, which uses a constraint on the KL divergence rather than a penalty to robustly allow large updates."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1569
                },
                {
                    "x": 2194,
                    "y": 1569
                },
                {
                    "x": 2194,
                    "y": 1625
                },
                {
                    "x": 1272,
                    "y": 1625
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='47' style='font-size:22px'>4 Optimization of Parameterized Policies</p>",
            "id": 47,
            "page": 3,
            "text": "4 Optimization of Parameterized Policies"
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1650
                },
                {
                    "x": 2262,
                    "y": 1650
                },
                {
                    "x": 2262,
                    "y": 1937
                },
                {
                    "x": 1273,
                    "y": 1937
                }
            ],
            "category": "paragraph",
            "html": "<p id='48' style='font-size:16px'>In the previous section, we considered the policy optimiza-<br>tion problem independently of the parameterization of �<br>and under the assumption that the policy can be evaluated<br>at all states. We now describe how to derive a practical<br>algorithm from these theoretical foundations, under finite<br>sample counts and arbitrary parameterizations.</p>",
            "id": 48,
            "page": 3,
            "text": "In the previous section, we considered the policy optimization problem independently of the parameterization of � and under the assumption that the policy can be evaluated at all states. We now describe how to derive a practical algorithm from these theoretical foundations, under finite sample counts and arbitrary parameterizations."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1960
                },
                {
                    "x": 2264,
                    "y": 1960
                },
                {
                    "x": 2264,
                    "y": 2253
                },
                {
                    "x": 1273,
                    "y": 2253
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='49' style='font-size:18px'>Since we consider parameterized policies ��(a|s) with pa-<br>rameter vector 0, we will overload our previous notation<br>to use functions of 0 rather than �, e.g. 7(0) := 7(��),<br>Lo(u) := L�� (��), and DKL (0 II 0) := DKL(�� II ��). We<br>will use Oold to denote the previous policy parameters that<br>we want to improve upon.</p>",
            "id": 49,
            "page": 3,
            "text": "Since we consider parameterized policies ��(a|s) with parameter vector 0, we will overload our previous notation to use functions of 0 rather than �, e.g. 7(0) := 7(��), Lo(u) := L�� (��), and DKL (0 II 0) := DKL(�� II ��). We will use Oold to denote the previous policy parameters that we want to improve upon."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2277
                },
                {
                    "x": 2262,
                    "y": 2277
                },
                {
                    "x": 2262,
                    "y": 2471
                },
                {
                    "x": 1273,
                    "y": 2471
                }
            ],
            "category": "paragraph",
            "html": "<p id='50' style='font-size:18px'>The preceding section showed that 7(0) ≥ Loold (0) -<br>CDkLx (Hold, 0), with equality at 0 = Oold. Thus, by per-<br>forming the following maximization, we are guaranteed to<br>improve the true objective 7:</p>",
            "id": 50,
            "page": 3,
            "text": "The preceding section showed that 7(0) ≥ Loold (0) CDkLx (Hold, 0), with equality at 0 = Oold. Thus, by performing the following maximization, we are guaranteed to improve the true objective 7:"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2582
                },
                {
                    "x": 2262,
                    "y": 2582
                },
                {
                    "x": 2262,
                    "y": 2824
                },
                {
                    "x": 1272,
                    "y": 2824
                }
            ],
            "category": "paragraph",
            "html": "<p id='51' style='font-size:16px'>In practice, if we used the penalty coefficient C recom-<br>mended by the theory above, the step sizes would be very<br>small. One way to take larger steps in a robust way is to use<br>a constraint on the KL divergence between the new policy<br>and the old policy, i.e., a trust region constraint:</p>",
            "id": 51,
            "page": 3,
            "text": "In practice, if we used the penalty coefficient C recommended by the theory above, the step sizes would be very small. One way to take larger steps in a robust way is to use a constraint on the KL divergence between the new policy and the old policy, i.e., a trust region constraint:"
        },
        {
            "bounding_box": [
                {
                    "x": 970,
                    "y": 191
                },
                {
                    "x": 1516,
                    "y": 191
                },
                {
                    "x": 1516,
                    "y": 235
                },
                {
                    "x": 970,
                    "y": 235
                }
            ],
            "category": "header",
            "html": "<header id='52' style='font-size:18px'>Trust Region Policy Optimization</header>",
            "id": 52,
            "page": 4,
            "text": "Trust Region Policy Optimization"
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 283
                },
                {
                    "x": 1214,
                    "y": 283
                },
                {
                    "x": 1214,
                    "y": 572
                },
                {
                    "x": 222,
                    "y": 572
                }
            ],
            "category": "paragraph",
            "html": "<p id='53' style='font-size:14px'>This problem imposes a constraint that the KL divergence<br>is bounded at every point in the state space. While it is<br>motivated by the theory, this problem is impractical to solve<br>due to the large number of constraints. Instead, we can use<br>a heuristic approximation which considers the average KL<br>divergence:</p>",
            "id": 53,
            "page": 4,
            "text": "This problem imposes a constraint that the KL divergence is bounded at every point in the state space. While it is motivated by the theory, this problem is impractical to solve due to the large number of constraints. Instead, we can use a heuristic approximation which considers the average KL divergence:"
        },
        {
            "bounding_box": [
                {
                    "x": 226,
                    "y": 669
                },
                {
                    "x": 1212,
                    "y": 669
                },
                {
                    "x": 1212,
                    "y": 765
                },
                {
                    "x": 226,
                    "y": 765
                }
            ],
            "category": "paragraph",
            "html": "<p id='54' style='font-size:18px'>We therefore propose solving the following optimization<br>problem to generate a policy update:</p>",
            "id": 54,
            "page": 4,
            "text": "We therefore propose solving the following optimization problem to generate a policy update:"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 944
                },
                {
                    "x": 1213,
                    "y": 944
                },
                {
                    "x": 1213,
                    "y": 1281
                },
                {
                    "x": 223,
                    "y": 1281
                }
            ],
            "category": "paragraph",
            "html": "<p id='55' style='font-size:16px'>Similar policy updates have been proposed in prior work<br>(Bagnell & Schneider, 2003; Peters & Schaal, 2008b; Pe-<br>ters et al., 2010), and we compare our approach to prior<br>methods in Section 7 and in the experiments in Section 8.<br>Our experiments also show that this type of constrained<br>update has similar empirical performance to the maximum<br>KL divergence constraint in Equation (11).</p>",
            "id": 55,
            "page": 4,
            "text": "Similar policy updates have been proposed in prior work (Bagnell & Schneider, 2003; Peters & Schaal, 2008b; Peters , 2010), and we compare our approach to prior methods in Section 7 and in the experiments in Section 8. Our experiments also show that this type of constrained update has similar empirical performance to the maximum KL divergence constraint in Equation (11)."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1307
                },
                {
                    "x": 1207,
                    "y": 1307
                },
                {
                    "x": 1207,
                    "y": 1417
                },
                {
                    "x": 223,
                    "y": 1417
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='56' style='font-size:20px'>5 Sample-Based Estimation of the Objective<br>and Constraint</p>",
            "id": 56,
            "page": 4,
            "text": "5 Sample-Based Estimation of the Objective and Constraint"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1441
                },
                {
                    "x": 1212,
                    "y": 1441
                },
                {
                    "x": 1212,
                    "y": 1776
                },
                {
                    "x": 223,
                    "y": 1776
                }
            ],
            "category": "paragraph",
            "html": "<p id='57' style='font-size:16px'>The previous section proposed a constrained optimization<br>problem on the policy parameters (Equation (12)), which<br>optimizes an estimate of the expected total reward 7 sub-<br>ject to a constraint on the change in the policy at each up-<br>date. This section describes how the objective and con-<br>straint functions can be approximated using Monte Carlo<br>simulation.</p>",
            "id": 57,
            "page": 4,
            "text": "The previous section proposed a constrained optimization problem on the policy parameters (Equation (12)), which optimizes an estimate of the expected total reward 7 subject to a constraint on the change in the policy at each update. This section describes how the objective and constraint functions can be approximated using Monte Carlo simulation."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 1801
                },
                {
                    "x": 1208,
                    "y": 1801
                },
                {
                    "x": 1208,
                    "y": 1899
                },
                {
                    "x": 225,
                    "y": 1899
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='58' style='font-size:18px'>We seek to solve the following optimization problem, ob-<br>tained by expanding Loold in Equation (12):</p>",
            "id": 58,
            "page": 4,
            "text": "We seek to solve the following optimization problem, obtained by expanding Loold in Equation (12):"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2128
                },
                {
                    "x": 1213,
                    "y": 2128
                },
                {
                    "x": 1213,
                    "y": 2475
                },
                {
                    "x": 223,
                    "y": 2475
                }
            ],
            "category": "paragraph",
            "html": "<p id='59' style='font-size:18px'>We first replace Es Poold (s)[. ]in the objective by the ex-<br>pectation E s~Poold<br>[ · ・ ]. Next, we replace the advan-<br>1- 2<br>tage values Aoold by the Q-values Quold in Equation (13),<br>which only changes the objective by a constant. Last, we<br>replace the sum over the actions by an importance sampling<br>estimator. Using q to denote the sampling distribution, the<br>contribution of a single Sn to the loss function is</p>",
            "id": 59,
            "page": 4,
            "text": "We first replace Es Poold (s)[. ]in the objective by the expectation E s~Poold [ · ・ ]. Next, we replace the advan1- 2 tage values Aoold by the Q-values Quold in Equation (13), which only changes the objective by a constant. Last, we replace the sum over the actions by an importance sampling estimator. Using q to denote the sampling distribution, the contribution of a single Sn to the loss function is"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2653
                },
                {
                    "x": 1210,
                    "y": 2653
                },
                {
                    "x": 1210,
                    "y": 2796
                },
                {
                    "x": 224,
                    "y": 2796
                }
            ],
            "category": "paragraph",
            "html": "<p id='60' style='font-size:16px'>Our optimization problem in Equation (13) is exactly<br>equivalent to the following one, written in terms of expec-<br>tations:</p>",
            "id": 60,
            "page": 4,
            "text": "Our optimization problem in Equation (13) is exactly equivalent to the following one, written in terms of expectations:"
        },
        {
            "bounding_box": [
                {
                    "x": 1325,
                    "y": 263
                },
                {
                    "x": 2215,
                    "y": 263
                },
                {
                    "x": 2215,
                    "y": 586
                },
                {
                    "x": 1325,
                    "y": 586
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='61' style='font-size:14px' alt=\"sampling\ntrajectories\ntrajectories\na1 two rollouts\nSn Sn using CRN\nall state-action a2\npairs used in\nobjective\nPo Po rollout set\" data-coord=\"top-left:(1325,263); bottom-right:(2215,586)\" /></figure>",
            "id": 61,
            "page": 4,
            "text": "sampling trajectories trajectories a1 two rollouts Sn Sn using CRN all state-action a2 pairs used in objective Po Po rollout set"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 626
                },
                {
                    "x": 2264,
                    "y": 626
                },
                {
                    "x": 2264,
                    "y": 996
                },
                {
                    "x": 1272,
                    "y": 996
                }
            ],
            "category": "caption",
            "html": "<caption id='62' style='font-size:14px'>Figure 1. Left: illustration of single path procedure. Here, we<br>generate a set of trajectories via simulation of the policy and in-<br>corporate all state-action pairs (Sn, an) into the objective. Right:<br>illustration of vine procedure. We generate a set of \"trunk\" tra-<br>jectories, and then generate \"branch\" rollouts from a subset of the<br>reached states. For each of these states Sn, we perform multiple<br>actions (a1 and a2 here) and perform a rollout after each action,<br>using common random numbers (CRN) to reduce the variance.</caption>",
            "id": 62,
            "page": 4,
            "text": "Figure 1. Left: illustration of single path procedure. Here, we generate a set of trajectories via simulation of the policy and incorporate all state-action pairs (Sn, an) into the objective. Right: illustration of vine procedure. We generate a set of \"trunk\" trajectories, and then generate \"branch\" rollouts from a subset of the reached states. For each of these states Sn, we perform multiple actions (a1 and a2 here) and perform a rollout after each action, using common random numbers (CRN) to reduce the variance."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1085
                },
                {
                    "x": 2264,
                    "y": 1085
                },
                {
                    "x": 2264,
                    "y": 1276
                },
                {
                    "x": 1273,
                    "y": 1276
                }
            ],
            "category": "paragraph",
            "html": "<p id='63' style='font-size:16px'>All that remains is to replace the expectations by sample<br>averages and replace the Q value by an empirical estimate.<br>The following sections describe two different schemes for<br>performing this estimation.</p>",
            "id": 63,
            "page": 4,
            "text": "All that remains is to replace the expectations by sample averages and replace the Q value by an empirical estimate. The following sections describe two different schemes for performing this estimation."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1301
                },
                {
                    "x": 2264,
                    "y": 1301
                },
                {
                    "x": 2264,
                    "y": 1730
                },
                {
                    "x": 1273,
                    "y": 1730
                }
            ],
            "category": "paragraph",
            "html": "<p id='64' style='font-size:16px'>The first sampling scheme, which we call single path, is<br>the one that is typically used for policy gradient estima-<br>tion (Bartlett & Baxter, 2011), and is based on sampling<br>individual trajectories. The second scheme, which we call<br>vine, involves constructing a rollout set and then perform-<br>ing multiple actions from each state in the rollout set. This<br>method has mostly been explored in the context of policy it-<br>eration methods (Lagoudakis & Parr, 2003; Gabillon et al.,<br>2013).</p>",
            "id": 64,
            "page": 4,
            "text": "The first sampling scheme, which we call single path, is the one that is typically used for policy gradient estimation (Bartlett & Baxter, 2011), and is based on sampling individual trajectories. The second scheme, which we call vine, involves constructing a rollout set and then performing multiple actions from each state in the rollout set. This method has mostly been explored in the context of policy iteration methods (Lagoudakis & Parr, 2003; Gabillon , 2013)."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 1778
                },
                {
                    "x": 1580,
                    "y": 1778
                },
                {
                    "x": 1580,
                    "y": 1827
                },
                {
                    "x": 1275,
                    "y": 1827
                }
            ],
            "category": "paragraph",
            "html": "<p id='65' style='font-size:18px'>5.1 Single Path</p>",
            "id": 65,
            "page": 4,
            "text": "5.1 Single Path"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1870
                },
                {
                    "x": 2264,
                    "y": 1870
                },
                {
                    "x": 2264,
                    "y": 2209
                },
                {
                    "x": 1272,
                    "y": 2209
                }
            ],
            "category": "paragraph",
            "html": "<p id='66' style='font-size:16px'>In this estimation procedure, we collect a sequence of<br>states by sampling SO ~ Po and then simulating the pol-<br>for some number of timesteps to generate a trajec-<br>icy �Oold<br>tory SO, ao, S1, a1, · · · , ST-1, aT-1, ST. Hence, q(a|s) =<br>�Oold (a|s). Quold (s, a) is computed at each state-action<br>pair (St, at) by taking the discounted sum of future rewards<br>along the trajectory.</p>",
            "id": 66,
            "page": 4,
            "text": "In this estimation procedure, we collect a sequence of states by sampling SO ~ Po and then simulating the polfor some number of timesteps to generate a trajecicy �Oold tory SO, ao, S1, a1, · · · , ST-1, aT-1, ST. Hence, q(a|s) = �Oold (a|s). Quold (s, a) is computed at each state-action pair (St, at) by taking the discounted sum of future rewards along the trajectory."
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 2252
                },
                {
                    "x": 1460,
                    "y": 2252
                },
                {
                    "x": 1460,
                    "y": 2298
                },
                {
                    "x": 1276,
                    "y": 2298
                }
            ],
            "category": "paragraph",
            "html": "<p id='67' style='font-size:16px'>5.2 Vine</p>",
            "id": 67,
            "page": 4,
            "text": "5.2 Vine"
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 2345
                },
                {
                    "x": 2263,
                    "y": 2345
                },
                {
                    "x": 2263,
                    "y": 2944
                },
                {
                    "x": 1270,
                    "y": 2944
                }
            ],
            "category": "paragraph",
            "html": "<p id='68' style='font-size:14px'>In this estimation procedure, we first sample SO ~ Po and<br>simulate the policy �Oi to generate a number of trajecto-<br>ries. We then choose a subset of N states along these tra-<br>jectories, denoted S1, S2, · · · , SN, which we call the \"roll-<br>out set\". For each state Sn in the rollout set, we sample<br>K actions according to an,k ~ q(·|sn). Any choice of<br>q(·|sn) with a support that includes the support of �Oi (·|sn)<br>will produce a consistent estimator. In practice, we found<br>that q(·|sn) = �Oi (·|sn) works well on continuous prob-<br>lems, such as robotic locomotion, while the uniform dis-<br>tribution works well on discrete tasks, such as the Atari<br>games, where it can sometimes achieve better exploration.</p>",
            "id": 68,
            "page": 4,
            "text": "In this estimation procedure, we first sample SO ~ Po and simulate the policy �Oi to generate a number of trajectories. We then choose a subset of N states along these trajectories, denoted S1, S2, · · · , SN, which we call the \"rollout set\". For each state Sn in the rollout set, we sample K actions according to an,k ~ q(·|sn). Any choice of q(·|sn) with a support that includes the support of �Oi (·|sn) will produce a consistent estimator. In practice, we found that q(·|sn) = �Oi (·|sn) works well on continuous problems, such as robotic locomotion, while the uniform distribution works well on discrete tasks, such as the Atari games, where it can sometimes achieve better exploration."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 2945
                },
                {
                    "x": 2258,
                    "y": 2945
                },
                {
                    "x": 2258,
                    "y": 2995
                },
                {
                    "x": 1275,
                    "y": 2995
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='69' style='font-size:14px'>For each action an,k sampled at each state Sn, we esti-</p>",
            "id": 69,
            "page": 4,
            "text": "For each action an,k sampled at each state Sn, we esti-"
        },
        {
            "bounding_box": [
                {
                    "x": 970,
                    "y": 191
                },
                {
                    "x": 1517,
                    "y": 191
                },
                {
                    "x": 1517,
                    "y": 235
                },
                {
                    "x": 970,
                    "y": 235
                }
            ],
            "category": "header",
            "html": "<header id='70' style='font-size:18px'>Trust Region Policy Optimization</header>",
            "id": 70,
            "page": 5,
            "text": "Trust Region Policy Optimization"
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 279
                },
                {
                    "x": 1214,
                    "y": 279
                },
                {
                    "x": 1214,
                    "y": 715
                },
                {
                    "x": 222,
                    "y": 715
                }
            ],
            "category": "paragraph",
            "html": "<p id='71' style='font-size:16px'>mate QUi (Sn, an,k) by performing a rollout (i.e., a short<br>trajectory) starting with state Sn and action an,k. can<br>We<br>greatly reduce the variance of the Q-value differences be-<br>tween rollouts by using the same random number sequence<br>for the noise in each of the K rollouts, i.e., common random<br>numbers. See (Bertsekas, 2005) for additional discussion<br>on Monte Carlo estimation of Q-values and (Ng & Jordan,<br>2000) for a discussion of common random numbers in re-<br>inforcement learning.</p>",
            "id": 71,
            "page": 5,
            "text": "mate QUi (Sn, an,k) by performing a rollout (i.e., a short trajectory) starting with state Sn and action an,k. can We greatly reduce the variance of the Q-value differences between rollouts by using the same random number sequence for the noise in each of the K rollouts, i.e., common random numbers. See (Bertsekas, 2005) for additional discussion on Monte Carlo estimation of Q-values and (Ng & Jordan, 2000) for a discussion of common random numbers in reinforcement learning."
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 739
                },
                {
                    "x": 1214,
                    "y": 739
                },
                {
                    "x": 1214,
                    "y": 886
                },
                {
                    "x": 222,
                    "y": 886
                }
            ],
            "category": "paragraph",
            "html": "<p id='72' style='font-size:14px'>In small, finite action spaces, we can generate a rollout for<br>every possible action from a given state. The contribution<br>to Loold from a single state Sn is as follows:</p>",
            "id": 72,
            "page": 5,
            "text": "In small, finite action spaces, we can generate a rollout for every possible action from a given state. The contribution to Loold from a single state Sn is as follows:"
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 1062
                },
                {
                    "x": 1213,
                    "y": 1062
                },
                {
                    "x": 1213,
                    "y": 1302
                },
                {
                    "x": 222,
                    "y": 1302
                }
            ],
            "category": "paragraph",
            "html": "<p id='73' style='font-size:14px'>where the action space is A = {a1, a2, · · · , ak}. In large<br>or continuous state spaces, we can construct an estima-<br>tor of the surrogate objective using importance sampling.<br>The self-normalized estimator (Owen (2013), Chapter 9)<br>of Loold obtained at a single state Sn is</p>",
            "id": 73,
            "page": 5,
            "text": "where the action space is A = {a1, a2, · · · , ak}. In large or continuous state spaces, we can construct an estimator of the surrogate objective using importance sampling. The self-normalized estimator (Owen (2013), Chapter 9) of Loold obtained at a single state Sn is"
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 1500
                },
                {
                    "x": 1214,
                    "y": 1500
                },
                {
                    "x": 1214,
                    "y": 1784
                },
                {
                    "x": 222,
                    "y": 1784
                }
            ],
            "category": "paragraph",
            "html": "<p id='74' style='font-size:14px'>assuming that we performed K actions<br>an,1, an,2, · · · , an,K from state Sn. This self-normalized<br>estimator removes the need to use a baseline for the<br>Q-values (note that the gradient is unchanged by adding a<br>constant to the Q-values). Averaging over Sn ~ p(�), we<br>obtain an estimator for Loold, as well as its gradient.</p>",
            "id": 74,
            "page": 5,
            "text": "assuming that we performed K actions an,1, an,2, · · · , an,K from state Sn. This self-normalized estimator removes the need to use a baseline for the Q-values (note that the gradient is unchanged by adding a constant to the Q-values). Averaging over Sn ~ p(�), we obtain an estimator for Loold, as well as its gradient."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1808
                },
                {
                    "x": 1212,
                    "y": 1808
                },
                {
                    "x": 1212,
                    "y": 2047
                },
                {
                    "x": 223,
                    "y": 2047
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='75' style='font-size:16px'>The vine and single path methods are illustrated in Figure 1.<br>We use the term vine, since the trajectories used for sam-<br>pling can be likened to the stems of vines, which branch at<br>various points (the rollout set) into several short offshoots<br>(the rollout trajectories).</p>",
            "id": 75,
            "page": 5,
            "text": "The vine and single path methods are illustrated in Figure 1. We use the term vine, since the trajectories used for sampling can be likened to the stems of vines, which branch at various points (the rollout set) into several short offshoots (the rollout trajectories)."
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 2072
                },
                {
                    "x": 1214,
                    "y": 2072
                },
                {
                    "x": 1214,
                    "y": 2700
                },
                {
                    "x": 222,
                    "y": 2700
                }
            ],
            "category": "paragraph",
            "html": "<p id='76' style='font-size:16px'>The benefit of the vine method over the single path method<br>that is our local estimate of the objective has much lower<br>variance given the same number of Q-value samples in the<br>surrogate objective. That is, the vine method gives much<br>better estimates of the advantage values. The downside of<br>the vine method is that we must perform far more calls to<br>the simulator for each of these advantage estimates. Fur-<br>thermore, the vine method requires us to generate multiple<br>trajectories from each state in the rollout set, which limits<br>this algorithm to settings where the system can be reset to<br>an arbitrary state. In contrast, the single path algorithm re-<br>quires no state resets and can be directly implemented on a<br>physical system (Peters & Schaal, 2008b).</p>",
            "id": 76,
            "page": 5,
            "text": "The benefit of the vine method over the single path method that is our local estimate of the objective has much lower variance given the same number of Q-value samples in the surrogate objective. That is, the vine method gives much better estimates of the advantage values. The downside of the vine method is that we must perform far more calls to the simulator for each of these advantage estimates. Furthermore, the vine method requires us to generate multiple trajectories from each state in the rollout set, which limits this algorithm to settings where the system can be reset to an arbitrary state. In contrast, the single path algorithm requires no state resets and can be directly implemented on a physical system (Peters & Schaal, 2008b)."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2721
                },
                {
                    "x": 733,
                    "y": 2721
                },
                {
                    "x": 733,
                    "y": 2777
                },
                {
                    "x": 224,
                    "y": 2777
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='77' style='font-size:22px'>6 Practical Algorithm</p>",
            "id": 77,
            "page": 5,
            "text": "6 Practical Algorithm"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2800
                },
                {
                    "x": 1212,
                    "y": 2800
                },
                {
                    "x": 1212,
                    "y": 2995
                },
                {
                    "x": 224,
                    "y": 2995
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='78' style='font-size:16px'>Here we present two practical policy optimization algo-<br>rithm based on the ideas above, which use either the single<br>path or vine sampling scheme from the preceding section.<br>The algorithms repeatedly perform the following steps:</p>",
            "id": 78,
            "page": 5,
            "text": "Here we present two practical policy optimization algorithm based on the ideas above, which use either the single path or vine sampling scheme from the preceding section. The algorithms repeatedly perform the following steps:"
        },
        {
            "bounding_box": [
                {
                    "x": 1305,
                    "y": 277
                },
                {
                    "x": 2264,
                    "y": 277
                },
                {
                    "x": 2264,
                    "y": 884
                },
                {
                    "x": 1305,
                    "y": 884
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='79' style='font-size:16px'>1. Use the single path or vine procedures to collect a set<br>of state-action pairs along with Monte Carlo estimates<br>of their Q-values.<br>2. By averaging over samples, construct the estimated<br>objective and constraint in Equation (14).<br>3. Approximately solve this constrained optimization<br>problem to update the policy's parameter vector 0.<br>We use the conjugate gradient algorithm followed by<br>a line search, which is altogether only slightly more<br>expensive than computing the gradient itself. See Ap-<br>pendix C for details.</p>",
            "id": 79,
            "page": 5,
            "text": "1. Use the single path or vine procedures to collect a set of state-action pairs along with Monte Carlo estimates of their Q-values. 2. By averaging over samples, construct the estimated objective and constraint in Equation (14). 3. Approximately solve this constrained optimization problem to update the policy's parameter vector 0. We use the conjugate gradient algorithm followed by a line search, which is altogether only slightly more expensive than computing the gradient itself. See Appendix C for details."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 936
                },
                {
                    "x": 2265,
                    "y": 936
                },
                {
                    "x": 2265,
                    "y": 1647
                },
                {
                    "x": 1272,
                    "y": 1647
                }
            ],
            "category": "paragraph",
            "html": "<p id='80' style='font-size:18px'>With regard to (3), we construct the Fisher informa-<br>tion matrix (FIM) by analytically computing the Hessian<br>of the KL divergence, rather than using the covariance<br>matrix of the gradients. That is, we estimate Aij as<br>02<br>1V En=1 ������<br>DKL (��old (·|sn) II ��(·|Sn)), rather than<br>a (an |sn) % log ��(an|sn). The ana-<br>1 V En=1 dui log ��<br>lytic estimator integrates over the action at each state Sn,<br>and does not depend on the action an that was sampled.<br>As described in Appendix C, this analytic estimator has<br>computational benefits in the large-scale setting, since it<br>removes the need to store a dense Hessian or all policy gra-<br>dients from a batch of trajectories. The rate of improvement<br>in the policy is similar to the empirical FIM, as shown in<br>the experiments.</p>",
            "id": 80,
            "page": 5,
            "text": "With regard to (3), we construct the Fisher information matrix (FIM) by analytically computing the Hessian of the KL divergence, rather than using the covariance matrix of the gradients. That is, we estimate Aij as 02 1V En=1 ������ DKL (��old (·|sn) II ��(·|Sn)), rather than a (an |sn) % log ��(an|sn). The ana1 V En=1 dui log �� lytic estimator integrates over the action at each state Sn, and does not depend on the action an that was sampled. As described in Appendix C, this analytic estimator has computational benefits in the large-scale setting, since it removes the need to store a dense Hessian or all policy gradients from a batch of trajectories. The rate of improvement in the policy is similar to the empirical FIM, as shown in the experiments."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1670
                },
                {
                    "x": 2263,
                    "y": 1670
                },
                {
                    "x": 2263,
                    "y": 1814
                },
                {
                    "x": 1273,
                    "y": 1814
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='81' style='font-size:16px'>Let us briefly summarize the relationship between the the-<br>ory from Section 3 and the practical algorithm we have de-<br>scribed:</p>",
            "id": 81,
            "page": 5,
            "text": "Let us briefly summarize the relationship between the theory from Section 3 and the practical algorithm we have described:"
        },
        {
            "bounding_box": [
                {
                    "x": 1314,
                    "y": 1866
                },
                {
                    "x": 2264,
                    "y": 1866
                },
                {
                    "x": 2264,
                    "y": 2714
                },
                {
                    "x": 1314,
                    "y": 2714
                }
            ],
            "category": "paragraph",
            "html": "<p id='82' style='font-size:16px'>● The theory justifies optimizing a surrogate objective<br>with a penalty on KL divergence. However, the<br>large penalty coefficient C leads to prohibitively small<br>steps, so we would like to decrease this coefficient.<br>Empirically, it is hard to robustly choose the penalty<br>coefficient, SO we use a hard constraint instead of a<br>penalty, with parameter 8 (the bound on KL diver-<br>gence).<br>● The constraint on Dmax (Hold, 0) is hard for numerical<br>optimization and estimation, SO instead we constrain<br>DKL (Hold, 0).<br>● Our theory ignores estimation error for the advantage<br>function. Kakade & Langford (2002) consider this er-<br>ror in their derivation, and the same arguments would<br>hold in the setting of this paper, but we omit them for<br>simplicity.</p>",
            "id": 82,
            "page": 5,
            "text": "● The theory justifies optimizing a surrogate objective with a penalty on KL divergence. However, the large penalty coefficient C leads to prohibitively small steps, so we would like to decrease this coefficient. Empirically, it is hard to robustly choose the penalty coefficient, SO we use a hard constraint instead of a penalty, with parameter 8 (the bound on KL divergence). ● The constraint on Dmax (Hold, 0) is hard for numerical optimization and estimation, SO instead we constrain DKL (Hold, 0). ● Our theory ignores estimation error for the advantage function. Kakade & Langford (2002) consider this error in their derivation, and the same arguments would hold in the setting of this paper, but we omit them for simplicity."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2768
                },
                {
                    "x": 1990,
                    "y": 2768
                },
                {
                    "x": 1990,
                    "y": 2824
                },
                {
                    "x": 1273,
                    "y": 2824
                }
            ],
            "category": "paragraph",
            "html": "<p id='83' style='font-size:20px'>7 Connections with Prior Work</p>",
            "id": 83,
            "page": 5,
            "text": "7 Connections with Prior Work"
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 2848
                },
                {
                    "x": 2263,
                    "y": 2848
                },
                {
                    "x": 2263,
                    "y": 2994
                },
                {
                    "x": 1274,
                    "y": 2994
                }
            ],
            "category": "paragraph",
            "html": "<p id='84' style='font-size:16px'>As mentioned in Section 4, our derivation results in a pol-<br>icy update that is related to several prior methods, provid-<br>ing a unifying perspective on a number of policy update</p>",
            "id": 84,
            "page": 5,
            "text": "As mentioned in Section 4, our derivation results in a policy update that is related to several prior methods, providing a unifying perspective on a number of policy update"
        },
        {
            "bounding_box": [
                {
                    "x": 971,
                    "y": 191
                },
                {
                    "x": 1516,
                    "y": 191
                },
                {
                    "x": 1516,
                    "y": 236
                },
                {
                    "x": 971,
                    "y": 236
                }
            ],
            "category": "header",
            "html": "<header id='85' style='font-size:18px'>Trust Region Policy Optimization</header>",
            "id": 85,
            "page": 6,
            "text": "Trust Region Policy Optimization"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 285
                },
                {
                    "x": 1212,
                    "y": 285
                },
                {
                    "x": 1212,
                    "y": 525
                },
                {
                    "x": 223,
                    "y": 525
                }
            ],
            "category": "paragraph",
            "html": "<p id='86' style='font-size:16px'>schemes. The natural policy gradient (Kakade, 2002) can<br>be obtained as a special case of the update in Equation (12)<br>by using a linear approximation to L and a quadratic ap-<br>proximation to the D constraint, resulting in the follow-<br>KL<br>ing problem:</p>",
            "id": 86,
            "page": 6,
            "text": "schemes. The natural policy gradient (Kakade, 2002) can be obtained as a special case of the update in Equation (12) by using a linear approximation to L and a quadratic approximation to the D constraint, resulting in the followKL ing problem:"
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 954
                },
                {
                    "x": 1212,
                    "y": 954
                },
                {
                    "x": 1212,
                    "y": 1306
                },
                {
                    "x": 222,
                    "y": 1306
                }
            ],
            "category": "paragraph",
            "html": "<p id='87' style='font-size:18px'>The update is Onew = Oold + *A(0old)-1▽0L(0)| 0=Oold ,<br>where the stepsize 1 1 is typically treated as an algorithm<br>parameter. This differs from our approach, which en-<br>forces the constraint at each update. Though this difference<br>might seem subtle, our experiments demonstrate that it sig-<br>nificantly improves the algorithm's performance on larger<br>problems.</p>",
            "id": 87,
            "page": 6,
            "text": "The update is Onew = Oold + *A(0old)-1▽0L(0)| 0=Oold , where the stepsize 1 1 is typically treated as an algorithm parameter. This differs from our approach, which enforces the constraint at each update. Though this difference might seem subtle, our experiments demonstrate that it significantly improves the algorithm's performance on larger problems."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1329
                },
                {
                    "x": 1210,
                    "y": 1329
                },
                {
                    "x": 1210,
                    "y": 1425
                },
                {
                    "x": 224,
                    "y": 1425
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='88' style='font-size:18px'>We can also obtain the standard policy gradient update by<br>using an l2 constraint or penalty:</p>",
            "id": 88,
            "page": 6,
            "text": "We can also obtain the standard policy gradient update by using an l2 constraint or penalty:"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1673
                },
                {
                    "x": 1212,
                    "y": 1673
                },
                {
                    "x": 1212,
                    "y": 1817
                },
                {
                    "x": 223,
                    "y": 1817
                }
            ],
            "category": "paragraph",
            "html": "<p id='89' style='font-size:18px'>The policy iteration update can also be obtained by solving<br>the unconstrained problem maximize� L �old (�), using L<br>as defined in Equation (3).</p>",
            "id": 89,
            "page": 6,
            "text": "The policy iteration update can also be obtained by solving the unconstrained problem maximize� L �old (�), using L as defined in Equation (3)."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1843
                },
                {
                    "x": 1213,
                    "y": 1843
                },
                {
                    "x": 1213,
                    "y": 2419
                },
                {
                    "x": 224,
                    "y": 2419
                }
            ],
            "category": "paragraph",
            "html": "<p id='90' style='font-size:16px'>Several other methods employ an update similar to Equa-<br>tion (12). Relative entropy policy search (REPS) (Peters<br>et al., 2010) constrains the state-action marginals p(s, a),<br>while TRPO constrains the conditionals p(a|s). Unlike<br>REPS, our approach does not require a costly nonlinear op-<br>timization in the inner loop. Levine and Abbeel (2014) also<br>use a KL divergence constraint, but its purpose is to encour-<br>age the policy not to stray from regions where the estimated<br>dynamics model is valid, while we do not attempt to esti-<br>mate the system dynamics explicitly. Pirotta et al. (2013)<br>also build on and generalize Kakade and Langford's results,<br>and they derive different algorithms from the ones here.</p>",
            "id": 90,
            "page": 6,
            "text": "Several other methods employ an update similar to Equation (12). Relative entropy policy search (REPS) (Peters , 2010) constrains the state-action marginals p(s, a), while TRPO constrains the conditionals p(a|s). Unlike REPS, our approach does not require a costly nonlinear optimization in the inner loop. Levine and Abbeel (2014) also use a KL divergence constraint, but its purpose is to encourage the policy not to stray from regions where the estimated dynamics model is valid, while we do not attempt to estimate the system dynamics explicitly. Pirotta  (2013) also build on and generalize Kakade and Langford's results, and they derive different algorithms from the ones here."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2443
                },
                {
                    "x": 582,
                    "y": 2443
                },
                {
                    "x": 582,
                    "y": 2497
                },
                {
                    "x": 224,
                    "y": 2497
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='91' style='font-size:22px'>8 Experiments</p>",
            "id": 91,
            "page": 6,
            "text": "8 Experiments"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2521
                },
                {
                    "x": 1212,
                    "y": 2521
                },
                {
                    "x": 1212,
                    "y": 2616
                },
                {
                    "x": 224,
                    "y": 2616
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='92' style='font-size:18px'>We designed our experiments to investigate the following<br>questions:</p>",
            "id": 92,
            "page": 6,
            "text": "We designed our experiments to investigate the following questions:"
        },
        {
            "bounding_box": [
                {
                    "x": 251,
                    "y": 2621
                },
                {
                    "x": 1216,
                    "y": 2621
                },
                {
                    "x": 1216,
                    "y": 2993
                },
                {
                    "x": 251,
                    "y": 2993
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='93' style='font-size:16px'>1. What are the performance characteristics of the single<br>path and vine sampling procedures?<br>2. TRPO is related to prior methods (e.g. natural policy<br>gradient) but makes several changes, most notably by<br>using a fixed KL divergence rather than a fixed penalty<br>coefficient. How does this affect the performance of<br>the algorithm?</p>",
            "id": 93,
            "page": 6,
            "text": "1. What are the performance characteristics of the single path and vine sampling procedures? 2. TRPO is related to prior methods (e.g. natural policy gradient) but makes several changes, most notably by using a fixed KL divergence rather than a fixed penalty coefficient. How does this affect the performance of the algorithm?"
        },
        {
            "bounding_box": [
                {
                    "x": 1360,
                    "y": 276
                },
                {
                    "x": 2172,
                    "y": 276
                },
                {
                    "x": 2172,
                    "y": 616
                },
                {
                    "x": 1360,
                    "y": 616
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='94' alt=\"\" data-coord=\"top-left:(1360,276); bottom-right:(2172,616)\" /></figure>",
            "id": 94,
            "page": 6,
            "text": ""
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 671
                },
                {
                    "x": 2264,
                    "y": 671
                },
                {
                    "x": 2264,
                    "y": 854
                },
                {
                    "x": 1273,
                    "y": 854
                }
            ],
            "category": "caption",
            "html": "<caption id='95' style='font-size:14px'>Figure 2. 2D robot models used for locomotion experiments.<br>From left to right: swimmer, hopper, walker. The hopper and<br>walker present a particular challenge, due to underactuation and<br>contact discontinuities.</caption>",
            "id": 95,
            "page": 6,
            "text": "Figure 2. 2D robot models used for locomotion experiments. From left to right: swimmer, hopper, walker. The hopper and walker present a particular challenge, due to underactuation and contact discontinuities."
        },
        {
            "bounding_box": [
                {
                    "x": 1319,
                    "y": 847
                },
                {
                    "x": 2197,
                    "y": 847
                },
                {
                    "x": 2197,
                    "y": 1689
                },
                {
                    "x": 1319,
                    "y": 1689
                }
            ],
            "category": "figure",
            "html": "<figure><img id='96' style='font-size:14px' alt=\"Fully\nInput Mean\nconnected Sampling\nlayer parameters\nlayer\nkinematics\nand\nControl\nangles\nJoint\nStandard\n30 units deviations\nInput Conv. Conv. Hidden Action\nlayer layer layer layer probabilities Sampling\n4x4 4x4\ninput\n4x4 4x4\nScreen Control\n4x4 4x4\n4x4 4x4\n16 filters 16 filters 20 units\" data-coord=\"top-left:(1319,847); bottom-right:(2197,1689)\" /></figure>",
            "id": 96,
            "page": 6,
            "text": "Fully Input Mean connected Sampling layer parameters layer kinematics and Control angles Joint Standard 30 units deviations Input Conv. Conv. Hidden Action layer layer layer layer probabilities Sampling 4x4 4x4 input 4x4 4x4 Screen Control 4x4 4x4 4x4 4x4 16 filters 16 filters 20 units"
        },
        {
            "bounding_box": [
                {
                    "x": 1277,
                    "y": 1706
                },
                {
                    "x": 2261,
                    "y": 1706
                },
                {
                    "x": 2261,
                    "y": 1797
                },
                {
                    "x": 1277,
                    "y": 1797
                }
            ],
            "category": "caption",
            "html": "<br><caption id='97' style='font-size:14px'>Figure 3. Neural networks used for the locomotion task (top) and<br>for playing Atari games (bottom).</caption>",
            "id": 97,
            "page": 6,
            "text": "Figure 3. Neural networks used for the locomotion task (top) and for playing Atari games (bottom)."
        },
        {
            "bounding_box": [
                {
                    "x": 1304,
                    "y": 1835
                },
                {
                    "x": 2265,
                    "y": 1835
                },
                {
                    "x": 2265,
                    "y": 2077
                },
                {
                    "x": 1304,
                    "y": 2077
                }
            ],
            "category": "paragraph",
            "html": "<p id='98' style='font-size:18px'>3. Can TRPO be used to solve challenging large-scale<br>problems? How does TRPO compare with other<br>methods when applied to large-scale problems, with<br>regard to final performance, computation time, and<br>sample complexity?</p>",
            "id": 98,
            "page": 6,
            "text": "3. Can TRPO be used to solve challenging large-scale problems? How does TRPO compare with other methods when applied to large-scale problems, with regard to final performance, computation time, and sample complexity?"
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2091
                },
                {
                    "x": 2263,
                    "y": 2091
                },
                {
                    "x": 2263,
                    "y": 2572
                },
                {
                    "x": 1273,
                    "y": 2572
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='99' style='font-size:16px'>To answer (1) and (2), we compare the performance of<br>the single path and vine variants of TRPO, several ablated<br>variants, and a number of prior policy optimization algo-<br>rithms. With regard to (3), we show that both the single<br>path and vine algorithm can obtain high-quality locomo-<br>tion controllers from scratch, which is considered to be a<br>hard problem. We also show that these algorithms produce<br>competitive results when learning policies for playing Atari<br>games from images using convolutional neural networks<br>with tens of thousands of parameters.</p>",
            "id": 99,
            "page": 6,
            "text": "To answer (1) and (2), we compare the performance of the single path and vine variants of TRPO, several ablated variants, and a number of prior policy optimization algorithms. With regard to (3), we show that both the single path and vine algorithm can obtain high-quality locomotion controllers from scratch, which is considered to be a hard problem. We also show that these algorithms produce competitive results when learning policies for playing Atari games from images using convolutional neural networks with tens of thousands of parameters."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2613
                },
                {
                    "x": 1930,
                    "y": 2613
                },
                {
                    "x": 1930,
                    "y": 2661
                },
                {
                    "x": 1273,
                    "y": 2661
                }
            ],
            "category": "paragraph",
            "html": "<p id='100' style='font-size:16px'>8.1 Simulated Robotic Locomotion</p>",
            "id": 100,
            "page": 6,
            "text": "8.1 Simulated Robotic Locomotion"
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 2704
                },
                {
                    "x": 2264,
                    "y": 2704
                },
                {
                    "x": 2264,
                    "y": 2993
                },
                {
                    "x": 1274,
                    "y": 2993
                }
            ],
            "category": "paragraph",
            "html": "<p id='101' style='font-size:16px'>We conducted the robotic locomotion experiments using<br>the MuJoCo simulator (Todorov et al., 2012). The three<br>simulated robots are shown in Figure 2. The states of the<br>robots are their generalized positions and velocities, and the<br>controls are joint torques. Underactuation, high dimension-<br>ality, and non-smooth dynamics due to contacts make these</p>",
            "id": 101,
            "page": 6,
            "text": "We conducted the robotic locomotion experiments using the MuJoCo simulator (Todorov , 2012). The three simulated robots are shown in Figure 2. The states of the robots are their generalized positions and velocities, and the controls are joint torques. Underactuation, high dimensionality, and non-smooth dynamics due to contacts make these"
        },
        {
            "bounding_box": [
                {
                    "x": 970,
                    "y": 191
                },
                {
                    "x": 1516,
                    "y": 191
                },
                {
                    "x": 1516,
                    "y": 235
                },
                {
                    "x": 970,
                    "y": 235
                }
            ],
            "category": "header",
            "html": "<header id='102' style='font-size:20px'>Trust Region Policy Optimization</header>",
            "id": 102,
            "page": 7,
            "text": "Trust Region Policy Optimization"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 285
                },
                {
                    "x": 1212,
                    "y": 285
                },
                {
                    "x": 1212,
                    "y": 381
                },
                {
                    "x": 223,
                    "y": 381
                }
            ],
            "category": "paragraph",
            "html": "<p id='103' style='font-size:16px'>tasks very challenging. The following models are included<br>in our evaluation:</p>",
            "id": 103,
            "page": 7,
            "text": "tasks very challenging. The following models are included in our evaluation:"
        },
        {
            "bounding_box": [
                {
                    "x": 249,
                    "y": 400
                },
                {
                    "x": 1215,
                    "y": 400
                },
                {
                    "x": 1215,
                    "y": 1152
                },
                {
                    "x": 249,
                    "y": 1152
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='104' style='font-size:16px'>1. Swimmer. 10-dimensional state space, linear reward<br>for forward progress and a quadratic penalty on joint<br>effort to produce the reward r(x, u) = Vx - 10-5 ||u||2.<br>The swimmer can propel itself forward by making an<br>undulating motion.<br>2. Hopper. 12-dimensional state space, same reward as<br>the swimmer, with a bonus of +1 for being in a non-<br>terminal state. We ended the episodes when the hop-<br>per fell over, which was defined by thresholds on the<br>torso height and angle.<br>3. Walker. 18-dimensional state space. For the walker,<br>we added a penalty for strong impacts of the feet<br>against the ground to encourage a smooth walk rather<br>than a hopping gait.</p>",
            "id": 104,
            "page": 7,
            "text": "1. Swimmer. 10-dimensional state space, linear reward for forward progress and a quadratic penalty on joint effort to produce the reward r(x, u) = Vx - 10-5 ||u||2. The swimmer can propel itself forward by making an undulating motion. 2. Hopper. 12-dimensional state space, same reward as the swimmer, with a bonus of +1 for being in a nonterminal state. We ended the episodes when the hopper fell over, which was defined by thresholds on the torso height and angle. 3. Walker. 18-dimensional state space. For the walker, we added a penalty for strong impacts of the feet against the ground to encourage a smooth walk rather than a hopping gait."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1171
                },
                {
                    "x": 1213,
                    "y": 1171
                },
                {
                    "x": 1213,
                    "y": 1603
                },
                {
                    "x": 223,
                    "y": 1603
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='105' style='font-size:16px'>We used 8 = 0.01 for all experiments. See Table 2 in the<br>Appendix for more details on the experimental setup and<br>parameters used. We used neural networks to represent the<br>policy, with the architecture shown in Figure 3, and further<br>details provided in Appendix D. To establish a standard<br>baseline, we also included the classic cart-pole balancing<br>problem, based on the formulation from Barto et al. (1983),<br>using a linear policy with six parameters that is easy to opti-<br>mize with derivative-free black-box optimization methods.</p>",
            "id": 105,
            "page": 7,
            "text": "We used 8 = 0.01 for all experiments. See Table 2 in the Appendix for more details on the experimental setup and parameters used. We used neural networks to represent the policy, with the architecture shown in Figure 3, and further details provided in Appendix D. To establish a standard baseline, we also included the classic cart-pole balancing problem, based on the formulation from Barto  (1983), using a linear policy with six parameters that is easy to optimize with derivative-free black-box optimization methods."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1627
                },
                {
                    "x": 1215,
                    "y": 1627
                },
                {
                    "x": 1215,
                    "y": 2539
                },
                {
                    "x": 223,
                    "y": 2539
                }
            ],
            "category": "paragraph",
            "html": "<p id='106' style='font-size:18px'>The following algorithms were considered in the compari-<br>son: single path TRPO; vine TRPO; cross-entropy method<br>(CEM), a gradient-free method (Szita & L�rincz, 2006);<br>covariance matrix adaption (CMA), another gradient-free<br>method (Hansen & Ostermeier, 1996); natural gradi-<br>ent, the classic natural policy gradient algorithm (Kakade,<br>2002), which differs from single path by the use of a fixed<br>penalty coefficient (Lagrange multiplier) instead of the KL<br>divergence constraint; empirical FIM, identical to single<br>path, except that the FIM is estimated using the covariance<br>matrix of the gradients rather than the analytic estimate;<br>max KL, which was only tractable on the cart-pole problem,<br>and uses the maximum KL divergence in Equation (11),<br>rather than the average divergence, allowing us to evaluate<br>the quality of this approximation. The parameters used in<br>the experiments are provided in Appendix E. For the natu-<br>ral gradient method, we swept through the possible values<br>of the stepsize in factors of three, and took the best value<br>according to the final performance.</p>",
            "id": 106,
            "page": 7,
            "text": "The following algorithms were considered in the comparison: single path TRPO; vine TRPO; cross-entropy method (CEM), a gradient-free method (Szita & L�rincz, 2006); covariance matrix adaption (CMA), another gradient-free method (Hansen & Ostermeier, 1996); natural gradient, the classic natural policy gradient algorithm (Kakade, 2002), which differs from single path by the use of a fixed penalty coefficient (Lagrange multiplier) instead of the KL divergence constraint; empirical FIM, identical to single path, except that the FIM is estimated using the covariance matrix of the gradients rather than the analytic estimate; max KL, which was only tractable on the cart-pole problem, and uses the maximum KL divergence in Equation (11), rather than the average divergence, allowing us to evaluate the quality of this approximation. The parameters used in the experiments are provided in Appendix E. For the natural gradient method, we swept through the possible values of the stepsize in factors of three, and took the best value according to the final performance."
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 2559
                },
                {
                    "x": 1213,
                    "y": 2559
                },
                {
                    "x": 1213,
                    "y": 2996
                },
                {
                    "x": 222,
                    "y": 2996
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='107' style='font-size:16px'>Learning curves showing the total reward averaged across<br>five runs of each algorithm are shown in Figure 4. Single<br>path and vine TRPO solved all of the problems, yielding<br>the best solutions. Natural gradient performed well on the<br>two easier problems, but was unable to generate hopping<br>and walking gaits that made forward progress. These re-<br>sults provide empirical evidence that constraining the KL<br>divergence is a more robust way to choose step sizes and<br>make fast, consistent progress, compared to using a fixed</p>",
            "id": 107,
            "page": 7,
            "text": "Learning curves showing the total reward averaged across five runs of each algorithm are shown in Figure 4. Single path and vine TRPO solved all of the problems, yielding the best solutions. Natural gradient performed well on the two easier problems, but was unable to generate hopping and walking gaits that made forward progress. These results provide empirical evidence that constraining the KL divergence is a more robust way to choose step sizes and make fast, consistent progress, compared to using a fixed"
        },
        {
            "bounding_box": [
                {
                    "x": 1285,
                    "y": 271
                },
                {
                    "x": 2245,
                    "y": 271
                },
                {
                    "x": 2245,
                    "y": 970
                },
                {
                    "x": 1285,
                    "y": 970
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='108' style='font-size:14px' alt=\"Cartpole\n10 Swimmer\n0.15\n8\nctrl)\n0.10\n6 +\nreward\n0.05\n4\nVine 0.00\nSingle Path\nVine\nNatural Gradient (-velocity\nSingle Path\nMax KL\ncost\nNatural Gradient\n2 Empirical FIM -0.05 Empirical FIM\nCEM\nCEM\nCMA CMA\nRWR RWR\n0 -0.10\n0 10 20 30 40 50 0 10 20 30 40 50\nnumber of policy iterations number of policy iterations\nHopper Walker\n2.5 3.5\nVine\n3.0 Single Path\n2.0 Natural Gradient\nCEM\n2.5\nRWR\n1.5\n2.0\nreward\n1.0\nreward\n1.5\n0.5 1.0\n0.5\n0.0\nVine\n0.0\nSingle Path\n-0.5 Natural Gradient\n-0.5\nCEM\nRWR\n-1.0 -1.0\n0 50 100 150 200 0 50 100 150 200\nnumber of policy iterations number of policy iterations\" data-coord=\"top-left:(1285,271); bottom-right:(2245,970)\" /></figure>",
            "id": 108,
            "page": 7,
            "text": "Cartpole 10 Swimmer 0.15 8 ctrl) 0.10 6 + reward 0.05 4 Vine 0.00 Single Path Vine Natural Gradient (-velocity Single Path Max KL cost Natural Gradient 2 Empirical FIM -0.05 Empirical FIM CEM CEM CMA CMA RWR RWR 0 -0.10 0 10 20 30 40 50 0 10 20 30 40 50 number of policy iterations number of policy iterations Hopper Walker 2.5 3.5 Vine 3.0 Single Path 2.0 Natural Gradient CEM 2.5 RWR 1.5 2.0 reward 1.0 reward 1.5 0.5 1.0 0.5 0.0 Vine 0.0 Single Path -0.5 Natural Gradient -0.5 CEM RWR -1.0 -1.0 0 50 100 150 200 0 50 100 150 200 number of policy iterations number of policy iterations"
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1011
                },
                {
                    "x": 2263,
                    "y": 1011
                },
                {
                    "x": 2263,
                    "y": 1245
                },
                {
                    "x": 1273,
                    "y": 1245
                }
            ],
            "category": "caption",
            "html": "<caption id='109' style='font-size:14px'>Figure 4. Learning curves for locomotion tasks, averaged across<br>five runs of each algorithm with random initializations. Note that<br>for the hopper and walker, a score of -1 is achievable without any<br>forward velocity, indicating a policy that simply learned balanced<br>standing, but not walking.</caption>",
            "id": 109,
            "page": 7,
            "text": "Figure 4. Learning curves for locomotion tasks, averaged across five runs of each algorithm with random initializations. Note that for the hopper and walker, a score of -1 is achievable without any forward velocity, indicating a policy that simply learned balanced standing, but not walking."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1304
                },
                {
                    "x": 2263,
                    "y": 1304
                },
                {
                    "x": 2263,
                    "y": 1834
                },
                {
                    "x": 1272,
                    "y": 1834
                }
            ],
            "category": "paragraph",
            "html": "<p id='110' style='font-size:16px'>penalty. CEM and CMA are derivative-free algorithms,<br>hence their sample complexity scales unfavorably with the<br>number of parameters, and they performed poorly on the<br>larger problems. The max KL method learned somewhat<br>more slowly than our final method, due to the more restric-<br>tive form of the constraint, but overall the result suggests<br>that the average KL divergence constraint has a similar ef-<br>fect as the theorecally justified maximum KL divergence.<br>Videos of the policies learned by TRPO may be viewed on<br>the project website: http : / / sites · google · com/<br>site / trpopaper/.</p>",
            "id": 110,
            "page": 7,
            "text": "penalty. CEM and CMA are derivative-free algorithms, hence their sample complexity scales unfavorably with the number of parameters, and they performed poorly on the larger problems. The max KL method learned somewhat more slowly than our final method, due to the more restrictive form of the constraint, but overall the result suggests that the average KL divergence constraint has a similar effect as the theorecally justified maximum KL divergence. Videos of the policies learned by TRPO may be viewed on the project website: http : / / sites · google · com/ site / trpopaper/."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1856
                },
                {
                    "x": 2264,
                    "y": 1856
                },
                {
                    "x": 2264,
                    "y": 2194
                },
                {
                    "x": 1273,
                    "y": 2194
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='111' style='font-size:18px'>Note that TRPO learned all of the gaits with general-<br>purpose policies and simple reward functions, using min-<br>imal prior knowledge. This is in contrast with most prior<br>methods for learning locomotion, which typically rely on<br>hand-architected policy classes that explicitly encode no-<br>tions of balance and stepping (Tedrake et al., 2004; Geng<br>et al., 2006; Wampler & Popovic, 2009).</p>",
            "id": 111,
            "page": 7,
            "text": "Note that TRPO learned all of the gaits with generalpurpose policies and simple reward functions, using minimal prior knowledge. This is in contrast with most prior methods for learning locomotion, which typically rely on hand-architected policy classes that explicitly encode notions of balance and stepping (Tedrake , 2004; Geng , 2006; Wampler & Popovic, 2009)."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 2249
                },
                {
                    "x": 1880,
                    "y": 2249
                },
                {
                    "x": 1880,
                    "y": 2298
                },
                {
                    "x": 1275,
                    "y": 2298
                }
            ],
            "category": "paragraph",
            "html": "<p id='112' style='font-size:22px'>8.2 Playing Games from Images</p>",
            "id": 112,
            "page": 7,
            "text": "8.2 Playing Games from Images"
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 2343
                },
                {
                    "x": 2262,
                    "y": 2343
                },
                {
                    "x": 2262,
                    "y": 2874
                },
                {
                    "x": 1270,
                    "y": 2874
                }
            ],
            "category": "paragraph",
            "html": "<p id='113' style='font-size:18px'>To evaluate TRPO on a partially observed task with com-<br>plex observations, we trained policies for playing Atari<br>games, using raw images as input. The games require<br>learning a variety of behaviors, such as dodging bullets and<br>hitting balls with paddles. Aside from the high dimension-<br>ality, challenging elements of these games include delayed<br>rewards (no immediate penalty is incurred when a life is<br>lost in Breakout or Space Invaders); complex sequences of<br>behavior (Q*bert requires a character to hop on 21 differ-<br>ent platforms); and non-stationary image statistics (Enduro<br>involves a changing and flickering background).</p>",
            "id": 113,
            "page": 7,
            "text": "To evaluate TRPO on a partially observed task with complex observations, we trained policies for playing Atari games, using raw images as input. The games require learning a variety of behaviors, such as dodging bullets and hitting balls with paddles. Aside from the high dimensionality, challenging elements of these games include delayed rewards (no immediate penalty is incurred when a life is lost in Breakout or Space Invaders); complex sequences of behavior (Q*bert requires a character to hop on 21 different platforms); and non-stationary image statistics (Enduro involves a changing and flickering background)."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 2895
                },
                {
                    "x": 2263,
                    "y": 2895
                },
                {
                    "x": 2263,
                    "y": 2994
                },
                {
                    "x": 1275,
                    "y": 2994
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='114' style='font-size:14px'>We tested our algorithms on the same seven games reported<br>on in (Mnih et al., 2013) and (Guo et al., 2014), which are</p>",
            "id": 114,
            "page": 7,
            "text": "We tested our algorithms on the same seven games reported on in (Mnih , 2013) and (Guo , 2014), which are"
        },
        {
            "bounding_box": [
                {
                    "x": 970,
                    "y": 191
                },
                {
                    "x": 1517,
                    "y": 191
                },
                {
                    "x": 1517,
                    "y": 236
                },
                {
                    "x": 970,
                    "y": 236
                }
            ],
            "category": "header",
            "html": "<header id='115' style='font-size:20px'>Trust Region Policy Optimization</header>",
            "id": 115,
            "page": 8,
            "text": "Trust Region Policy Optimization"
        },
        {
            "bounding_box": [
                {
                    "x": 563,
                    "y": 275
                },
                {
                    "x": 1928,
                    "y": 275
                },
                {
                    "x": 1928,
                    "y": 588
                },
                {
                    "x": 563,
                    "y": 588
                }
            ],
            "category": "table",
            "html": "<table id='116' style='font-size:14px'><tr><td></td><td>B. Rider</td><td>Breakout</td><td>Enduro</td><td>Pong</td><td>Q*bert</td><td>Seaquest</td><td>S. Invaders</td></tr><tr><td>Random</td><td>354</td><td>1.2</td><td>0</td><td>- 20.4</td><td>157</td><td>110</td><td>179</td></tr><tr><td>Human (Mnih et al., 2013)</td><td>7456</td><td>31.0</td><td>368</td><td>-3.0</td><td>18900</td><td>28010</td><td>3690</td></tr><tr><td>Deep Q Learning (Mnih et al., 2013)</td><td>4092</td><td>168.0</td><td>470</td><td>20.0</td><td>1952</td><td>1705</td><td>581</td></tr><tr><td>UCC-I (Guo et al., 2014)</td><td>5702</td><td>380</td><td>741</td><td>21</td><td>20025</td><td>2995</td><td>692</td></tr><tr><td>TRPO - single path</td><td>1425.2</td><td>10.8</td><td>534.6</td><td>20.9</td><td>1973.5</td><td>1908.6</td><td>568.4</td></tr><tr><td>TRPO - vine</td><td>859.5</td><td>34.2</td><td>430.8</td><td>20.9</td><td>7732.5</td><td>788.4</td><td>450.2</td></tr></table>",
            "id": 116,
            "page": 8,
            "text": "B. Rider Breakout Enduro Pong Q*bert Seaquest S. Invaders  Random 354 1.2 0 - 20.4 157 110 179  Human (Mnih , 2013) 7456 31.0 368 -3.0 18900 28010 3690  Deep Q Learning (Mnih , 2013) 4092 168.0 470 20.0 1952 1705 581  UCC-I (Guo , 2014) 5702 380 741 21 20025 2995 692  TRPO - single path 1425.2 10.8 534.6 20.9 1973.5 1908.6 568.4  TRPO - vine 859.5 34.2 430.8 20.9 7732.5 788.4"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 619
                },
                {
                    "x": 2265,
                    "y": 619
                },
                {
                    "x": 2265,
                    "y": 760
                },
                {
                    "x": 224,
                    "y": 760
                }
            ],
            "category": "caption",
            "html": "<caption id='117' style='font-size:14px'>Table 1. Performance comparison for vision-based RL algorithms on the Atari domain. Our algorithms (bottom rows) were run once<br>on each task, with the same architecture and parameters. Performance varies substantially from run to run (with different random<br>initializations of the policy), but we could not obtain error statistics due to time constraints.</caption>",
            "id": 117,
            "page": 8,
            "text": "Table 1. Performance comparison for vision-based RL algorithms on the Atari domain. Our algorithms (bottom rows) were run once on each task, with the same architecture and parameters. Performance varies substantially from run to run (with different random initializations of the policy), but we could not obtain error statistics due to time constraints."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 844
                },
                {
                    "x": 1214,
                    "y": 844
                },
                {
                    "x": 1214,
                    "y": 1179
                },
                {
                    "x": 224,
                    "y": 1179
                }
            ],
            "category": "paragraph",
            "html": "<p id='118' style='font-size:18px'>made available through the Arcade Learning Environment<br>(Bellemare et al., 2013) The images were preprocessed fol-<br>lowing the protocol in Mnih et al (2013), and the policy was<br>represented by the convolutional neural network shown in<br>Figure 3, with two convolutional layers with 16 channels<br>and stride 2, followed by one fully-connected layer with 20<br>units, yielding 33,500 parameters.</p>",
            "id": 118,
            "page": 8,
            "text": "made available through the Arcade Learning Environment (Bellemare , 2013) The images were preprocessed following the protocol in Mnih et al (2013), and the policy was represented by the convolutional neural network shown in Figure 3, with two convolutional layers with 16 channels and stride 2, followed by one fully-connected layer with 20 units, yielding 33,500 parameters."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1200
                },
                {
                    "x": 1216,
                    "y": 1200
                },
                {
                    "x": 1216,
                    "y": 1876
                },
                {
                    "x": 224,
                    "y": 1876
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='119' style='font-size:18px'>The results of the vine and single path algorithms are sum-<br>marized in Table 1, which also includes an expert human<br>performance and two recent methods: deep Q-learning<br>(Mnih et al., 2013), and a combination of Monte-Carlo Tree<br>Search with supervised training (Guo et al., 2014), called<br>UCC-I. The 500 iterations of our algorithm took about 30<br>hours (with slight variation between games) on a 16-core<br>computer. While our method only outperformed the prior<br>methods on some of the games, it consistently achieved rea-<br>sonable scores. Unlike the prior methods, our approach<br>was not designed specifically for this task. The ability to<br>apply the same policy search method to methods as di-<br>verse as robotic locomotion and image-based game playing<br>demonstrates the generality of TRPO.</p>",
            "id": 119,
            "page": 8,
            "text": "The results of the vine and single path algorithms are summarized in Table 1, which also includes an expert human performance and two recent methods: deep Q-learning (Mnih , 2013), and a combination of Monte-Carlo Tree Search with supervised training (Guo , 2014), called UCC-I. The 500 iterations of our algorithm took about 30 hours (with slight variation between games) on a 16-core computer. While our method only outperformed the prior methods on some of the games, it consistently achieved reasonable scores. Unlike the prior methods, our approach was not designed specifically for this task. The ability to apply the same policy search method to methods as diverse as robotic locomotion and image-based game playing demonstrates the generality of TRPO."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 1899
                },
                {
                    "x": 532,
                    "y": 1899
                },
                {
                    "x": 532,
                    "y": 1952
                },
                {
                    "x": 225,
                    "y": 1952
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='120' style='font-size:20px'>9 Discussion</p>",
            "id": 120,
            "page": 8,
            "text": "9 Discussion"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1976
                },
                {
                    "x": 1214,
                    "y": 1976
                },
                {
                    "x": 1214,
                    "y": 2601
                },
                {
                    "x": 224,
                    "y": 2601
                }
            ],
            "category": "paragraph",
            "html": "<p id='121' style='font-size:18px'>We proposed and analyzed trust region methods for opti-<br>mizing stochastic control policies. We proved monotonic<br>improvement for an algorithm that repeatedly optimizes<br>a local approximation to the expected return of the pol-<br>icy with a KL divergence penalty, and we showed that an<br>approximation to this method that incorporates a KL di-<br>vergence constraint achieves good empirical results on a<br>range of challenging policy learning tasks, outperforming<br>prior methods. Our analysis also provides a perspective<br>that unifies policy gradient and policy iteration methods,<br>and shows them to be special limiting cases of an algo-<br>rithm that optimizes a certain objective subject to a trust<br>region constraint.</p>",
            "id": 121,
            "page": 8,
            "text": "We proposed and analyzed trust region methods for optimizing stochastic control policies. We proved monotonic improvement for an algorithm that repeatedly optimizes a local approximation to the expected return of the policy with a KL divergence penalty, and we showed that an approximation to this method that incorporates a KL divergence constraint achieves good empirical results on a range of challenging policy learning tasks, outperforming prior methods. Our analysis also provides a perspective that unifies policy gradient and policy iteration methods, and shows them to be special limiting cases of an algorithm that optimizes a certain objective subject to a trust region constraint."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2623
                },
                {
                    "x": 1213,
                    "y": 2623
                },
                {
                    "x": 1213,
                    "y": 2964
                },
                {
                    "x": 223,
                    "y": 2964
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='122' style='font-size:18px'>In the domain of robotic locomotion, we successfully<br>learned controllers for swimming, walking and hopping in<br>a physics simulator, using general purpose neural networks<br>and minimally informative rewards. To our knowledge,<br>no prior work has learned controllers from scratch for all<br>of these tasks, using a generic policy search method and<br>non-engineered, general-purpose policy representations. In</p>",
            "id": 122,
            "page": 8,
            "text": "In the domain of robotic locomotion, we successfully learned controllers for swimming, walking and hopping in a physics simulator, using general purpose neural networks and minimally informative rewards. To our knowledge, no prior work has learned controllers from scratch for all of these tasks, using a generic policy search method and non-engineered, general-purpose policy representations. In"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 842
                },
                {
                    "x": 2262,
                    "y": 842
                },
                {
                    "x": 2262,
                    "y": 1080
                },
                {
                    "x": 1272,
                    "y": 1080
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='123' style='font-size:18px'>the game-playing domain, we learned convolutional neu-<br>ral network policies that used raw images as inputs. This<br>requires optimizing extremely high-dimensional policies,<br>and only two prior methods report successful results on this<br>task.</p>",
            "id": 123,
            "page": 8,
            "text": "the game-playing domain, we learned convolutional neural network policies that used raw images as inputs. This requires optimizing extremely high-dimensional policies, and only two prior methods report successful results on this task."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 1105
                },
                {
                    "x": 2265,
                    "y": 1105
                },
                {
                    "x": 2265,
                    "y": 1874
                },
                {
                    "x": 1271,
                    "y": 1874
                }
            ],
            "category": "paragraph",
            "html": "<p id='124' style='font-size:18px'>Since the method we proposed is scalable and has strong<br>theoretical foundations, we hope that it will serve as a<br>jumping-off point for future work on training large, rich<br>function approximators for a range of challenging prob-<br>lems. At the intersection of the two experimental domains<br>we explored, there is the possibility of learning robotic con-<br>trol policies that use vision and raw sensory data as in-<br>put, providing a unified scheme for training robotic con-<br>trollers that perform both perception and control. The use<br>of more sophisticated policies, including recurrent policies<br>with hidden state, could further make it possible to roll state<br>estimation and control into the same policy in the partially-<br>observed setting. By combining our method with model<br>learning, it would also be possible to substantially reduce<br>its sample complexity, making it applicable to real-world<br>settings where samples are expensive.</p>",
            "id": 124,
            "page": 8,
            "text": "Since the method we proposed is scalable and has strong theoretical foundations, we hope that it will serve as a jumping-off point for future work on training large, rich function approximators for a range of challenging problems. At the intersection of the two experimental domains we explored, there is the possibility of learning robotic control policies that use vision and raw sensory data as input, providing a unified scheme for training robotic controllers that perform both perception and control. The use of more sophisticated policies, including recurrent policies with hidden state, could further make it possible to roll state estimation and control into the same policy in the partiallyobserved setting. By combining our method with model learning, it would also be possible to substantially reduce its sample complexity, making it applicable to real-world settings where samples are expensive."
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 1899
                },
                {
                    "x": 1698,
                    "y": 1899
                },
                {
                    "x": 1698,
                    "y": 1955
                },
                {
                    "x": 1276,
                    "y": 1955
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='125' style='font-size:22px'>Acknowledgements</p>",
            "id": 125,
            "page": 8,
            "text": "Acknowledgements"
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 1976
                },
                {
                    "x": 2263,
                    "y": 1976
                },
                {
                    "x": 2263,
                    "y": 2456
                },
                {
                    "x": 1274,
                    "y": 2456
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='126' style='font-size:18px'>We thank Emo Todorov and Yuval Tassa for providing<br>the MuJoCo simulator; Bruno Scherrer, Tom Erez, Greg<br>Wayne, and the anonymous ICML reviewers for insightful<br>comments, and Vitchyr Pong and Shane Gu for pointing<br>our errors in a previous version of the manuscript. This re-<br>search was funded in part by the Office of Naval Research<br>through a Young Investigator Award and under grant num-<br>ber N00014-11-1-0688, DARPA through a Young Faculty<br>Award, by the Army Research Office through the MAST<br>program.</p>",
            "id": 126,
            "page": 8,
            "text": "We thank Emo Todorov and Yuval Tassa for providing the MuJoCo simulator; Bruno Scherrer, Tom Erez, Greg Wayne, and the anonymous ICML reviewers for insightful comments, and Vitchyr Pong and Shane Gu for pointing our errors in a previous version of the manuscript. This research was funded in part by the Office of Naval Research through a Young Investigator Award and under grant number N00014-11-1-0688, DARPA through a Young Faculty Award, by the Army Research Office through the MAST program."
        },
        {
            "bounding_box": [
                {
                    "x": 1277,
                    "y": 2482
                },
                {
                    "x": 1517,
                    "y": 2482
                },
                {
                    "x": 1517,
                    "y": 2534
                },
                {
                    "x": 1277,
                    "y": 2534
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='127' style='font-size:22px'>References</p>",
            "id": 127,
            "page": 8,
            "text": "References"
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 2562
                },
                {
                    "x": 2259,
                    "y": 2562
                },
                {
                    "x": 2259,
                    "y": 2644
                },
                {
                    "x": 1276,
                    "y": 2644
                }
            ],
            "category": "paragraph",
            "html": "<p id='128' style='font-size:16px'>Bagnell, J. A. and Schneider, J. Covariant policy search. IJCAI,<br>2003.</p>",
            "id": 128,
            "page": 8,
            "text": "Bagnell, J. A. and Schneider, J. Covariant policy search. IJCAI, 2003."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 2667
                },
                {
                    "x": 2262,
                    "y": 2667
                },
                {
                    "x": 2262,
                    "y": 2750
                },
                {
                    "x": 1275,
                    "y": 2750
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='129' style='font-size:16px'>Bartlett, P. L. and Baxter, J. Infinite-horizon policy-gradient esti-<br>mation. arXiv preprint arXiv:1106.0665, 2011.</p>",
            "id": 129,
            "page": 8,
            "text": "Bartlett, P. L. and Baxter, J. Infinite-horizon policy-gradient estimation. arXiv preprint arXiv:1106.0665, 2011."
        },
        {
            "bounding_box": [
                {
                    "x": 1277,
                    "y": 2772
                },
                {
                    "x": 2262,
                    "y": 2772
                },
                {
                    "x": 2262,
                    "y": 2932
                },
                {
                    "x": 1277,
                    "y": 2932
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='130' style='font-size:16px'>Barto, A., Sutton, R., and Anderson, C. Neuronlike adaptive ele-<br>ments that can solve difficult learning control problems. IEEE<br>Transactions on Systems, Man and Cybernetics, (5):834-846,<br>1983.</p>",
            "id": 130,
            "page": 8,
            "text": "Barto, A., Sutton, R., and Anderson, C. Neuronlike adaptive elements that can solve difficult learning control problems. IEEE Transactions on Systems, Man and Cybernetics, (5):834-846, 1983."
        },
        {
            "bounding_box": [
                {
                    "x": 970,
                    "y": 191
                },
                {
                    "x": 1516,
                    "y": 191
                },
                {
                    "x": 1516,
                    "y": 236
                },
                {
                    "x": 970,
                    "y": 236
                }
            ],
            "category": "header",
            "html": "<header id='131' style='font-size:22px'>Trust Region Policy Optimization</header>",
            "id": 131,
            "page": 9,
            "text": "Trust Region Policy Optimization"
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 286
                },
                {
                    "x": 1212,
                    "y": 286
                },
                {
                    "x": 1212,
                    "y": 449
                },
                {
                    "x": 225,
                    "y": 449
                }
            ],
            "category": "paragraph",
            "html": "<p id='132' style='font-size:16px'>Bellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M. The ar-<br>cade learning environment: An evaluation platform for general<br>agents. Journal of Artificial Intelligence Research, 47:253-<br>279, jun 2013.</p>",
            "id": 132,
            "page": 9,
            "text": "Bellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253279, jun 2013."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 470
                },
                {
                    "x": 1210,
                    "y": 470
                },
                {
                    "x": 1210,
                    "y": 554
                },
                {
                    "x": 223,
                    "y": 554
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='133' style='font-size:14px'>Bertsekas, D. Dynamic programming and optimal control, vol-<br>ume 1. 2005.</p>",
            "id": 133,
            "page": 9,
            "text": "Bertsekas, D. Dynamic programming and optimal control, volume 1. 2005."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 576
                },
                {
                    "x": 1211,
                    "y": 576
                },
                {
                    "x": 1211,
                    "y": 699
                },
                {
                    "x": 225,
                    "y": 699
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='134' style='font-size:14px'>Deisenroth, M., Neumann, G., and Peters, J. A survey on policy<br>search for robotics. Foundations and Trends in Robotics, 2(1-<br>2):1-142, 2013.</p>",
            "id": 134,
            "page": 9,
            "text": "Deisenroth, M., Neumann, G., and Peters, J. A survey on policy search for robotics. Foundations and Trends in Robotics, 2(12):1-142, 2013."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 720
                },
                {
                    "x": 1213,
                    "y": 720
                },
                {
                    "x": 1213,
                    "y": 883
                },
                {
                    "x": 225,
                    "y": 883
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='135' style='font-size:16px'>Gabillon, Victor, Ghavamzadeh, Mohammad, and Scherrer,<br>Bruno. Approximate dynamic programming finally performs<br>well in the game of Tetris. In Advances in Neural Information<br>Processing Systems, 2013.</p>",
            "id": 135,
            "page": 9,
            "text": "Gabillon, Victor, Ghavamzadeh, Mohammad, and Scherrer, Bruno. Approximate dynamic programming finally performs well in the game of Tetris. In Advances in Neural Information Processing Systems, 2013."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 906
                },
                {
                    "x": 1212,
                    "y": 906
                },
                {
                    "x": 1212,
                    "y": 1029
                },
                {
                    "x": 224,
                    "y": 1029
                }
            ],
            "category": "paragraph",
            "html": "<p id='136' style='font-size:18px'>Geng, T., Porr, B., and Worgotter, F. Fast biped walking with a<br>reflexive controller and realtime policy searching. In Advances<br>in Neural Information Processing Systems (NIPS), 2006.</p>",
            "id": 136,
            "page": 9,
            "text": "Geng, T., Porr, B., and Worgotter, F. Fast biped walking with a reflexive controller and realtime policy searching. In Advances in Neural Information Processing Systems (NIPS), 2006."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1050
                },
                {
                    "x": 1211,
                    "y": 1050
                },
                {
                    "x": 1211,
                    "y": 1213
                },
                {
                    "x": 224,
                    "y": 1213
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='137' style='font-size:16px'>Guo, X., Singh, S., Lee, H., Lewis, R. L., and Wang, X. Deep<br>learning for real-time atari game play using offline Monte-<br>Carlo tree search planning. In Advances in Neural Information<br>Processing Systems, pp. 3338-3346, 2014.</p>",
            "id": 137,
            "page": 9,
            "text": "Guo, X., Singh, S., Lee, H., Lewis, R. L., and Wang, X. Deep learning for real-time atari game play using offline MonteCarlo tree search planning. In Advances in Neural Information Processing Systems, pp. 3338-3346, 2014."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 1234
                },
                {
                    "x": 1210,
                    "y": 1234
                },
                {
                    "x": 1210,
                    "y": 1435
                },
                {
                    "x": 225,
                    "y": 1435
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='138' style='font-size:16px'>Hansen, Nikolaus and Ostermeier, Andreas. Adapting arbitrary<br>normal mutation distributions in evolution strategies: The CO-<br>variance matrix adaptation. In Evolutionary Computation,<br>1996., Proceedings of IEEE International Conference on, pp.<br>312-317. IEEE, 1996.</p>",
            "id": 138,
            "page": 9,
            "text": "Hansen, Nikolaus and Ostermeier, Andreas. Adapting arbitrary normal mutation distributions in evolution strategies: The COvariance matrix adaptation. In Evolutionary Computation, 1996., Proceedings of IEEE International Conference on, pp. 312-317. IEEE, 1996."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1457
                },
                {
                    "x": 1208,
                    "y": 1457
                },
                {
                    "x": 1208,
                    "y": 1541
                },
                {
                    "x": 224,
                    "y": 1541
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='139' style='font-size:14px'>Hunter, David R and Lange, Kenneth. A tutorial on MM algo-<br>rithms. The American Statistician, 58(1):30-37, 2004.</p>",
            "id": 139,
            "page": 9,
            "text": "Hunter, David R and Lange, Kenneth. A tutorial on MM algorithms. The American Statistician, 58(1):30-37, 2004."
        },
        {
            "bounding_box": [
                {
                    "x": 226,
                    "y": 1562
                },
                {
                    "x": 1212,
                    "y": 1562
                },
                {
                    "x": 1212,
                    "y": 1685
                },
                {
                    "x": 226,
                    "y": 1685
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='140' style='font-size:14px'>Kakade, Sham. A natural policy gradient. In Advances in Neural<br>Information Processing Systems, pp. 1057-1063. MIT Press,<br>2002.</p>",
            "id": 140,
            "page": 9,
            "text": "Kakade, Sham. A natural policy gradient. In Advances in Neural Information Processing Systems, pp. 1057-1063. MIT Press, 2002."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 1707
                },
                {
                    "x": 1211,
                    "y": 1707
                },
                {
                    "x": 1211,
                    "y": 1830
                },
                {
                    "x": 225,
                    "y": 1830
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='141' style='font-size:16px'>Kakade, Sham and Langford, John. Approximately optimal ap-<br>proximate reinforcement learning. In ICML, volume 2, pp.<br>267-274, 2002.</p>",
            "id": 141,
            "page": 9,
            "text": "Kakade, Sham and Langford, John. Approximately optimal approximate reinforcement learning. In ICML, volume 2, pp. 267-274, 2002."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1851
                },
                {
                    "x": 1212,
                    "y": 1851
                },
                {
                    "x": 1212,
                    "y": 1975
                },
                {
                    "x": 224,
                    "y": 1975
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='142' style='font-size:14px'>Lagoudakis, Michail G and Parr, Ronald. Reinforcement learn-<br>ing as classification: Leveraging modern classifiers. In ICML,<br>volume 3, pp. 424-431, 2003.</p>",
            "id": 142,
            "page": 9,
            "text": "Lagoudakis, Michail G and Parr, Ronald. Reinforcement learning as classification: Leveraging modern classifiers. In ICML, volume 3, pp. 424-431, 2003."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1996
                },
                {
                    "x": 1213,
                    "y": 1996
                },
                {
                    "x": 1213,
                    "y": 2079
                },
                {
                    "x": 224,
                    "y": 2079
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='143' style='font-size:14px'>Levin, D. A., Peres, Y., and Wilmer, E. L. Markov chains and<br>mixing times. American Mathematical Society, 2009.</p>",
            "id": 143,
            "page": 9,
            "text": "Levin, D. A., Peres, Y., and Wilmer, E. L. Markov chains and mixing times. American Mathematical Society, 2009."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 2100
                },
                {
                    "x": 1210,
                    "y": 2100
                },
                {
                    "x": 1210,
                    "y": 2262
                },
                {
                    "x": 225,
                    "y": 2262
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='144' style='font-size:16px'>Levine, Sergey and Abbeel, Pieter. Learning neural network<br>policies with guided policy search under unknown dynamics.<br>In Advances in Neural Information Processing Systems, pp.<br>1071-1079, 2014.</p>",
            "id": 144,
            "page": 9,
            "text": "Levine, Sergey and Abbeel, Pieter. Learning neural network policies with guided policy search under unknown dynamics. In Advances in Neural Information Processing Systems, pp. 1071-1079, 2014."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 2283
                },
                {
                    "x": 1214,
                    "y": 2283
                },
                {
                    "x": 1214,
                    "y": 2410
                },
                {
                    "x": 225,
                    "y": 2410
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='145' style='font-size:16px'>Martens, J. and Sutskever, I. Training deep and recurrent networks<br>with hessian-free optimization. In Neural Networks: Tricks of<br>the Trade, pp. 479-535. Springer, 2012.</p>",
            "id": 145,
            "page": 9,
            "text": "Martens, J. and Sutskever, I. Training deep and recurrent networks with hessian-free optimization. In Neural Networks: Tricks of the Trade, pp. 479-535. Springer, 2012."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2430
                },
                {
                    "x": 1211,
                    "y": 2430
                },
                {
                    "x": 1211,
                    "y": 2553
                },
                {
                    "x": 224,
                    "y": 2553
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='146' style='font-size:20px'>Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou,<br>I., Wierstra, D., and Riedmiller, M. Playing Atari with deep<br>reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.</p>",
            "id": 146,
            "page": 9,
            "text": "Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M. Playing Atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2575
                },
                {
                    "x": 1207,
                    "y": 2575
                },
                {
                    "x": 1207,
                    "y": 2655
                },
                {
                    "x": 223,
                    "y": 2655
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='147' style='font-size:14px'>Nemirovski, Arkadi. Efficient methods in convex programming.<br>2005.</p>",
            "id": 147,
            "page": 9,
            "text": "Nemirovski, Arkadi. Efficient methods in convex programming. 2005."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2678
                },
                {
                    "x": 1212,
                    "y": 2678
                },
                {
                    "x": 1212,
                    "y": 2801
                },
                {
                    "x": 223,
                    "y": 2801
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='148' style='font-size:16px'>Ng, A. Y. and Jordan, M. PEGASUS: A policy search method<br>for large mdps and pomdps. In Uncertainty in artificial intelli-<br>gence (UAI), 2000.</p>",
            "id": 148,
            "page": 9,
            "text": "Ng, A. Y. and Jordan, M. PEGASUS: A policy search method for large mdps and pomdps. In Uncertainty in artificial intelligence (UAI), 2000."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2824
                },
                {
                    "x": 1208,
                    "y": 2824
                },
                {
                    "x": 1208,
                    "y": 2867
                },
                {
                    "x": 224,
                    "y": 2867
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='149' style='font-size:14px'>Owen, Art B. Monte Carlo theory, methods and examples. 2013.</p>",
            "id": 149,
            "page": 9,
            "text": "Owen, Art B. Monte Carlo theory, methods and examples. 2013."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2888
                },
                {
                    "x": 1211,
                    "y": 2888
                },
                {
                    "x": 1211,
                    "y": 2972
                },
                {
                    "x": 223,
                    "y": 2972
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='150' style='font-size:16px'>Pascanu, Razvan and Bengio, Yoshua. Revisiting natural gradient<br>for deep networks. arXiv preprint arXiv:1301.3584, 2013.</p>",
            "id": 150,
            "page": 9,
            "text": "Pascanu, Razvan and Bengio, Yoshua. Revisiting natural gradient for deep networks. arXiv preprint arXiv:1301.3584, 2013."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 286
                },
                {
                    "x": 2265,
                    "y": 286
                },
                {
                    "x": 2265,
                    "y": 412
                },
                {
                    "x": 1274,
                    "y": 412
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='151' style='font-size:16px'>Peters, J. and Schaal, S. Reinforcement learning of motor<br>skills with policy gradients. Neural Networks, 21(4):682-697,<br>2008a.</p>",
            "id": 151,
            "page": 9,
            "text": "Peters, J. and Schaal, S. Reinforcement learning of motor skills with policy gradients. Neural Networks, 21(4):682-697, 2008a."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 432
                },
                {
                    "x": 2261,
                    "y": 432
                },
                {
                    "x": 2261,
                    "y": 517
                },
                {
                    "x": 1274,
                    "y": 517
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='152' style='font-size:18px'>Peters, J., M�lling, K., and Altun, Y. Relative entropy policy<br>search. In AAAI Conference on Artificial Intelligence, 2010.</p>",
            "id": 152,
            "page": 9,
            "text": "Peters, J., M�lling, K., and Altun, Y. Relative entropy policy search. In AAAI Conference on Artificial Intelligence, 2010."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 537
                },
                {
                    "x": 2262,
                    "y": 537
                },
                {
                    "x": 2262,
                    "y": 622
                },
                {
                    "x": 1273,
                    "y": 622
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='153' style='font-size:16px'>Peters, Jan and Schaal, Stefan. Natural actor-critic. Neurocom-<br>puting, 71(7):1180-1190, 2008b.</p>",
            "id": 153,
            "page": 9,
            "text": "Peters, Jan and Schaal, Stefan. Natural actor-critic. Neurocomputing, 71(7):1180-1190, 2008b."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 642
                },
                {
                    "x": 2263,
                    "y": 642
                },
                {
                    "x": 2263,
                    "y": 804
                },
                {
                    "x": 1274,
                    "y": 804
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='154' style='font-size:16px'>Pirotta, Matteo, Restelli, Marcello, Pecorino, Alessio, and Calan-<br>driello, Daniele. Safe policy iteration. In Proceedings of The<br>30th International Conference on Machine Learning, pp. 307-<br>315, 2013.</p>",
            "id": 154,
            "page": 9,
            "text": "Pirotta, Matteo, Restelli, Marcello, Pecorino, Alessio, and Calandriello, Daniele. Safe policy iteration. In Proceedings of The 30th International Conference on Machine Learning, pp. 307315, 2013."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 825
                },
                {
                    "x": 2259,
                    "y": 825
                },
                {
                    "x": 2259,
                    "y": 952
                },
                {
                    "x": 1275,
                    "y": 952
                }
            ],
            "category": "paragraph",
            "html": "<p id='155' style='font-size:14px'>Pollard, David. Asymptopia: an exposition of statistical asymp-<br>totic theory. 2000. URL http : / / www · stat · yale · edu/<br>~pollard/ Books / Asymptopia.</p>",
            "id": 155,
            "page": 9,
            "text": "Pollard, David. Asymptopia: an exposition of statistical asymptotic theory. 2000. URL http : / / www · stat · yale · edu/ ~pollard/ Books / Asymptopia."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 971
                },
                {
                    "x": 2262,
                    "y": 971
                },
                {
                    "x": 2262,
                    "y": 1092
                },
                {
                    "x": 1274,
                    "y": 1092
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='156' style='font-size:16px'>Szita, Istvan and L�rincz, Andras. Learning tetris using the<br>noisy cross-entropy method. Neural computation, 18(12):<br>2936-2941, 2006.</p>",
            "id": 156,
            "page": 9,
            "text": "Szita, Istvan and L�rincz, Andras. Learning tetris using the noisy cross-entropy method. Neural computation, 18(12): 2936-2941, 2006."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 1114
                },
                {
                    "x": 2262,
                    "y": 1114
                },
                {
                    "x": 2262,
                    "y": 1276
                },
                {
                    "x": 1274,
                    "y": 1276
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='157' style='font-size:18px'>Tedrake, R., Zhang, T., and Seung, H. Stochastic policy gradi-<br>ent reinforcement learning on a simple 3d biped. In IEEE/RSJ<br>International Conference on Intelligent Robots and Systems,<br>2004.</p>",
            "id": 157,
            "page": 9,
            "text": "Tedrake, R., Zhang, T., and Seung, H. Stochastic policy gradient reinforcement learning on a simple 3d biped. In IEEE/RSJ International Conference on Intelligent Robots and Systems, 2004."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 1299
                },
                {
                    "x": 2262,
                    "y": 1299
                },
                {
                    "x": 2262,
                    "y": 1462
                },
                {
                    "x": 1274,
                    "y": 1462
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='158' style='font-size:16px'>Todorov, Emanuel, Erez, Tom, and Tassa, Yuval. MuJoCo: A<br>physics engine for model-based control. In Intelligent Robots<br>and Systems (IROS), 2012 IEEE/RSJ International Conference<br>on, pp. 5026-5033. IEEE, 2012.</p>",
            "id": 158,
            "page": 9,
            "text": "Todorov, Emanuel, Erez, Tom, and Tassa, Yuval. MuJoCo: A physics engine for model-based control. In Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on, pp. 5026-5033. IEEE, 2012."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 1482
                },
                {
                    "x": 2262,
                    "y": 1482
                },
                {
                    "x": 2262,
                    "y": 1608
                },
                {
                    "x": 1275,
                    "y": 1608
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='159' style='font-size:14px'>Wampler, Kevin and Popovic, Zoran. Optimal gait and form for<br>animal locomotion. In ACM Transactions on Graphics (TOG),<br>volume 28, pp. 60. ACM, 2009.</p>",
            "id": 159,
            "page": 9,
            "text": "Wampler, Kevin and Popovic, Zoran. Optimal gait and form for animal locomotion. In ACM Transactions on Graphics (TOG), volume 28, pp. 60. ACM, 2009."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 1627
                },
                {
                    "x": 2263,
                    "y": 1627
                },
                {
                    "x": 2263,
                    "y": 1713
                },
                {
                    "x": 1275,
                    "y": 1713
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='160' style='font-size:20px'>Wright, Stephen J and Nocedal, Jorge. Numerical optimization,<br>volume 2. Springer New York, 1999.</p>",
            "id": 160,
            "page": 9,
            "text": "Wright, Stephen J and Nocedal, Jorge. Numerical optimization, volume 2. Springer New York, 1999."
        },
        {
            "bounding_box": [
                {
                    "x": 971,
                    "y": 190
                },
                {
                    "x": 1516,
                    "y": 190
                },
                {
                    "x": 1516,
                    "y": 236
                },
                {
                    "x": 971,
                    "y": 236
                }
            ],
            "category": "header",
            "html": "<header id='161' style='font-size:18px'>Trust Region Policy Optimization</header>",
            "id": 161,
            "page": 10,
            "text": "Trust Region Policy Optimization"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 278
                },
                {
                    "x": 1092,
                    "y": 278
                },
                {
                    "x": 1092,
                    "y": 335
                },
                {
                    "x": 224,
                    "y": 335
                }
            ],
            "category": "paragraph",
            "html": "<p id='162' style='font-size:22px'>A Proof of Policy Improvement Bound</p>",
            "id": 162,
            "page": 10,
            "text": "A Proof of Policy Improvement Bound"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 356
                },
                {
                    "x": 2264,
                    "y": 356
                },
                {
                    "x": 2264,
                    "y": 647
                },
                {
                    "x": 224,
                    "y": 647
                }
            ],
            "category": "paragraph",
            "html": "<p id='163' style='font-size:16px'>This proof (of Theorem 1) uses techniques from the proof of Theorem 4.1 in (Kakade & Langford, 2002), adapting them<br>to the more general setting considered in this paper. An informal overview is as follows. Our proof relies on the notion<br>of coupling, where we jointly define the policies � and �' so that they choose the same action with high probability<br>= (1 - �). Surrogate loss L�(�) accounts for the the advantage of 元 the first time that it disagrees with �, but not<br>subsequent disagreements. Hence, the error in L� is due to two or more disagreements between � and 元, hence, we get an<br>O(�2) correction term, where a is the probability of disagreement.</p>",
            "id": 163,
            "page": 10,
            "text": "This proof (of Theorem 1) uses techniques from the proof of Theorem 4.1 in (Kakade & Langford, 2002), adapting them to the more general setting considered in this paper. An informal overview is as follows. Our proof relies on the notion of coupling, where we jointly define the policies � and �' so that they choose the same action with high probability = (1 - �). Surrogate loss L�(�) accounts for the the advantage of 元 the first time that it disagrees with �, but not subsequent disagreements. Hence, the error in L� is due to two or more disagreements between � and 元, hence, we get an O(�2) correction term, where a is the probability of disagreement."
        },
        {
            "bounding_box": [
                {
                    "x": 221,
                    "y": 668
                },
                {
                    "x": 2263,
                    "y": 668
                },
                {
                    "x": 2263,
                    "y": 766
                },
                {
                    "x": 221,
                    "y": 766
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='164' style='font-size:16px'>We start out with a lemma from Kakade & Langford (2002) that shows that the difference in policy performance ク(�)ーク(�)<br>can be decomposed as a sum of per-timestep advantages.</p>",
            "id": 164,
            "page": 10,
            "text": "We start out with a lemma from Kakade & Langford (2002) that shows that the difference in policy performance ク(�)ーク(�) can be decomposed as a sum of per-timestep advantages."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 779
                },
                {
                    "x": 830,
                    "y": 779
                },
                {
                    "x": 830,
                    "y": 829
                },
                {
                    "x": 225,
                    "y": 829
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='165' style='font-size:16px'>Lemma 1. Given two policies �, 元,</p>",
            "id": 165,
            "page": 10,
            "text": "Lemma 1. Given two policies �, 元,"
        },
        {
            "bounding_box": [
                {
                    "x": 219,
                    "y": 1082
                },
                {
                    "x": 2261,
                    "y": 1082
                },
                {
                    "x": 2261,
                    "y": 1184
                },
                {
                    "x": 219,
                    "y": 1184
                }
            ],
            "category": "paragraph",
            "html": "<p id='166' style='font-size:14px'>This expectation is taken over trajectories T := (so, ao, S1, ao, · · · ), and the notation ET~� [ . . ] indicates that actions are<br>sampled from 元 to generate T.</p>",
            "id": 166,
            "page": 10,
            "text": "This expectation is taken over trajectories T := (so, ao, S1, ao, · · · ), and the notation ET~� [ . . ] indicates that actions are sampled from 元 to generate T."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 1231
                },
                {
                    "x": 1650,
                    "y": 1231
                },
                {
                    "x": 1650,
                    "y": 1288
                },
                {
                    "x": 225,
                    "y": 1288
                }
            ],
            "category": "paragraph",
            "html": "<p id='167' style='font-size:20px'>Proof. First note that A�(s,a) = Es'~P(s'|s,a) [r(s) +7V�(s') -V�(s)]. Therefore,</p>",
            "id": 167,
            "page": 10,
            "text": "Proof. First note that A�(s,a) = Es'~P(s'|s,a) [r(s) +7V�(s') -V�(s)]. Therefore,"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1969
                },
                {
                    "x": 759,
                    "y": 1969
                },
                {
                    "x": 759,
                    "y": 2022
                },
                {
                    "x": 223,
                    "y": 2022
                }
            ],
            "category": "paragraph",
            "html": "<p id='168' style='font-size:16px'>Rearranging, the result follows.</p>",
            "id": 168,
            "page": 10,
            "text": "Rearranging, the result follows."
        },
        {
            "bounding_box": [
                {
                    "x": 2222,
                    "y": 1978
                },
                {
                    "x": 2256,
                    "y": 1978
                },
                {
                    "x": 2256,
                    "y": 2010
                },
                {
                    "x": 2222,
                    "y": 2010
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='169' style='font-size:14px'>□</p>",
            "id": 169,
            "page": 10,
            "text": "□"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2068
                },
                {
                    "x": 1296,
                    "y": 2068
                },
                {
                    "x": 1296,
                    "y": 2123
                },
                {
                    "x": 223,
                    "y": 2123
                }
            ],
            "category": "paragraph",
            "html": "<p id='170' style='font-size:14px'>Define A(s) to be the expected advantage of 元 over � at state s:</p>",
            "id": 170,
            "page": 10,
            "text": "Define A(s) to be the expected advantage of 元 over � at state s:"
        },
        {
            "bounding_box": [
                {
                    "x": 221,
                    "y": 2262
                },
                {
                    "x": 922,
                    "y": 2262
                },
                {
                    "x": 922,
                    "y": 2313
                },
                {
                    "x": 221,
                    "y": 2313
                }
            ],
            "category": "paragraph",
            "html": "<p id='171' style='font-size:14px'>Now Lemma 1 can be written as follows:</p>",
            "id": 171,
            "page": 10,
            "text": "Now Lemma 1 can be written as follows:"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2495
                },
                {
                    "x": 743,
                    "y": 2495
                },
                {
                    "x": 743,
                    "y": 2546
                },
                {
                    "x": 223,
                    "y": 2546
                }
            ],
            "category": "paragraph",
            "html": "<p id='172' style='font-size:14px'>Note that L� can be written as</p>",
            "id": 172,
            "page": 10,
            "text": "Note that L� can be written as"
        },
        {
            "bounding_box": [
                {
                    "x": 221,
                    "y": 2736
                },
                {
                    "x": 2263,
                    "y": 2736
                },
                {
                    "x": 2263,
                    "y": 2885
                },
                {
                    "x": 221,
                    "y": 2885
                }
            ],
            "category": "paragraph",
            "html": "<p id='173' style='font-size:16px'>The difference in these equations is whether the states are sampled using � or 元. To bound the difference between 7(元) and<br>L� (元), we will bound the difference arising from each timestep. To do this, we first need to introduce a measure of how<br>much � and 元 agree. Specifically, we'll couple the policies, so that they define a joint distribution over pairs of actions.</p>",
            "id": 173,
            "page": 10,
            "text": "The difference in these equations is whether the states are sampled using � or 元. To bound the difference between 7(元) and L� (元), we will bound the difference arising from each timestep. To do this, we first need to introduce a measure of how much � and 元 agree. Specifically, we'll couple the policies, so that they define a joint distribution over pairs of actions."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2895
                },
                {
                    "x": 2263,
                    "y": 2895
                },
                {
                    "x": 2263,
                    "y": 2993
                },
                {
                    "x": 223,
                    "y": 2993
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='174' style='font-size:18px'>Definition 1. (�, 元) is an �-coupled policy pair ifit defines a joint distribution (a, �)|s, such that P(a ≠ as) ≤ afor all<br>s. � and 元 will denote the marginal distributions of a and a, respectively.</p>",
            "id": 174,
            "page": 10,
            "text": "Definition 1. (�, 元) is an �-coupled policy pair ifit defines a joint distribution (a, �)|s, such that P(a ≠ as) ≤ afor all s. � and 元 will denote the marginal distributions of a and a, respectively."
        },
        {
            "bounding_box": [
                {
                    "x": 972,
                    "y": 192
                },
                {
                    "x": 1515,
                    "y": 192
                },
                {
                    "x": 1515,
                    "y": 234
                },
                {
                    "x": 972,
                    "y": 234
                }
            ],
            "category": "header",
            "html": "<header id='175' style='font-size:18px'>Trust Region Policy Optimization</header>",
            "id": 175,
            "page": 11,
            "text": "Trust Region Policy Optimization"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 285
                },
                {
                    "x": 2263,
                    "y": 285
                },
                {
                    "x": 2263,
                    "y": 381
                },
                {
                    "x": 223,
                    "y": 381
                }
            ],
            "category": "paragraph",
            "html": "<p id='176' style='font-size:14px'>Computationally, �-coupling means that if we randomly choose a seed for our random number generator, and then we<br>sample from each of � and 元 after setting that seed, the results will agree for at least fraction 1 - a of seeds.</p>",
            "id": 176,
            "page": 11,
            "text": "Computationally, �-coupling means that if we randomly choose a seed for our random number generator, and then we sample from each of � and 元 after setting that seed, the results will agree for at least fraction 1 - a of seeds."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 398
                },
                {
                    "x": 1234,
                    "y": 398
                },
                {
                    "x": 1234,
                    "y": 448
                },
                {
                    "x": 223,
                    "y": 448
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='177' style='font-size:16px'>Lemma 2. Given that �, 元 are �-coupled policies, for all S,</p>",
            "id": 177,
            "page": 11,
            "text": "Lemma 2. Given that �, 元 are �-coupled policies, for all S,"
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 614
                },
                {
                    "x": 335,
                    "y": 614
                },
                {
                    "x": 335,
                    "y": 660
                },
                {
                    "x": 225,
                    "y": 660
                }
            ],
            "category": "paragraph",
            "html": "<p id='178' style='font-size:20px'>Proof.</p>",
            "id": 178,
            "page": 11,
            "text": "Proof."
        },
        {
            "bounding_box": [
                {
                    "x": 2216,
                    "y": 947
                },
                {
                    "x": 2259,
                    "y": 947
                },
                {
                    "x": 2259,
                    "y": 984
                },
                {
                    "x": 2216,
                    "y": 984
                }
            ],
            "category": "paragraph",
            "html": "<p id='179' style='font-size:16px'>□</p>",
            "id": 179,
            "page": 11,
            "text": "□"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1023
                },
                {
                    "x": 1176,
                    "y": 1023
                },
                {
                    "x": 1176,
                    "y": 1073
                },
                {
                    "x": 224,
                    "y": 1073
                }
            ],
            "category": "paragraph",
            "html": "<p id='180' style='font-size:18px'>Lemma 3. Let (�,元) be an �-coupled policy pair. Then</p>",
            "id": 180,
            "page": 11,
            "text": "Lemma 3. Let (�,元) be an �-coupled policy pair. Then"
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 1234
                },
                {
                    "x": 2262,
                    "y": 1234
                },
                {
                    "x": 2262,
                    "y": 1475
                },
                {
                    "x": 222,
                    "y": 1475
                }
            ],
            "category": "paragraph",
            "html": "<p id='181' style='font-size:16px'>Proof. Given the coupled policy pair (�, 元), we can also obtain a coupling over the trajectory distributions produced by<br>� and 元, respectively. Namely, we have pairs of trajectories T, 7, where T is obtained by taking actions from �, and 수 is<br>obtained by taking actions from 元, where the same random seed is used to generate both trajectories. We will consider<br>the advantage of 元 over � at timestep t, and decompose this expectation based on whether � agrees with 元 at all timesteps<br>i < t.</p>",
            "id": 181,
            "page": 11,
            "text": "Proof. Given the coupled policy pair (�, 元), we can also obtain a coupling over the trajectory distributions produced by � and 元, respectively. Namely, we have pairs of trajectories T, 7, where T is obtained by taking actions from �, and 수 is obtained by taking actions from 元, where the same random seed is used to generate both trajectories. We will consider the advantage of 元 over � at timestep t, and decompose this expectation based on whether � agrees with 元 at all timesteps i < t."
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 1499
                },
                {
                    "x": 2256,
                    "y": 1499
                },
                {
                    "x": 2256,
                    "y": 1553
                },
                {
                    "x": 222,
                    "y": 1553
                }
            ],
            "category": "paragraph",
            "html": "<p id='182' style='font-size:16px'>Let nt denote the number of times that ai ≠ ai for i < t, i.e., the number of times that � and 元 disagree before timestep t.</p>",
            "id": 182,
            "page": 11,
            "text": "Let nt denote the number of times that ai ≠ ai for i < t, i.e., the number of times that � and 元 disagree before timestep t."
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 1680
                },
                {
                    "x": 1418,
                    "y": 1680
                },
                {
                    "x": 1418,
                    "y": 1732
                },
                {
                    "x": 222,
                    "y": 1732
                }
            ],
            "category": "paragraph",
            "html": "<p id='183' style='font-size:16px'>The expectation decomposes similarly for actions are sampled using �:</p>",
            "id": 183,
            "page": 11,
            "text": "The expectation decomposes similarly for actions are sampled using �:"
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 1861
                },
                {
                    "x": 853,
                    "y": 1861
                },
                {
                    "x": 853,
                    "y": 1912
                },
                {
                    "x": 222,
                    "y": 1912
                }
            ],
            "category": "paragraph",
            "html": "<p id='184' style='font-size:14px'>Note that the nt = 0 terms are equal:</p>",
            "id": 184,
            "page": 11,
            "text": "Note that the nt = 0 terms are equal:"
        },
        {
            "bounding_box": [
                {
                    "x": 221,
                    "y": 2040
                },
                {
                    "x": 2191,
                    "y": 2040
                },
                {
                    "x": 2191,
                    "y": 2096
                },
                {
                    "x": 221,
                    "y": 2096
                }
            ],
            "category": "paragraph",
            "html": "<p id='185' style='font-size:16px'>because nt = 0 indicates that � and 元 agreed on all timesteps less than t. Subtracting Equations (33) and (34), we get</p>",
            "id": 185,
            "page": 11,
            "text": "because nt = 0 indicates that � and 元 agreed on all timesteps less than t. Subtracting Equations (33) and (34), we get"
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 2219
                },
                {
                    "x": 1690,
                    "y": 2219
                },
                {
                    "x": 1690,
                    "y": 2279
                },
                {
                    "x": 222,
                    "y": 2279
                }
            ],
            "category": "paragraph",
            "html": "<p id='186' style='font-size:16px'>By definition of a, P(�, 규 agree at timestep i) ≥ 1 - a, so P(nt =0) ≥ (1 - a)t, and</p>",
            "id": 186,
            "page": 11,
            "text": "By definition of a, P(�, 규 agree at timestep i) ≥ 1 - a, so P(nt =0) ≥ (1 - a)t, and"
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2406
                },
                {
                    "x": 480,
                    "y": 2406
                },
                {
                    "x": 480,
                    "y": 2453
                },
                {
                    "x": 224,
                    "y": 2453
                }
            ],
            "category": "paragraph",
            "html": "<p id='187' style='font-size:16px'>Next, note that</p>",
            "id": 187,
            "page": 11,
            "text": "Next, note that"
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 2669
                },
                {
                    "x": 1108,
                    "y": 2669
                },
                {
                    "x": 1108,
                    "y": 2718
                },
                {
                    "x": 225,
                    "y": 2718
                }
            ],
            "category": "paragraph",
            "html": "<p id='188' style='font-size:16px'>Where the second inequality follows from Lemma 3.</p>",
            "id": 188,
            "page": 11,
            "text": "Where the second inequality follows from Lemma 3."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 2742
                },
                {
                    "x": 1380,
                    "y": 2742
                },
                {
                    "x": 1380,
                    "y": 2793
                },
                {
                    "x": 224,
                    "y": 2793
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='189' style='font-size:18px'>Plugging Equation (37) and Equation (39) into Equation (36), we get</p>",
            "id": 189,
            "page": 11,
            "text": "Plugging Equation (37) and Equation (39) into Equation (36), we get"
        },
        {
            "bounding_box": [
                {
                    "x": 2217,
                    "y": 2946
                },
                {
                    "x": 2260,
                    "y": 2946
                },
                {
                    "x": 2260,
                    "y": 2985
                },
                {
                    "x": 2217,
                    "y": 2985
                }
            ],
            "category": "footer",
            "html": "<footer id='190' style='font-size:14px'>□</footer>",
            "id": 190,
            "page": 11,
            "text": "□"
        },
        {
            "bounding_box": [
                {
                    "x": 971,
                    "y": 191
                },
                {
                    "x": 1515,
                    "y": 191
                },
                {
                    "x": 1515,
                    "y": 235
                },
                {
                    "x": 971,
                    "y": 235
                }
            ],
            "category": "header",
            "html": "<header id='191' style='font-size:18px'>Trust Region Policy Optimization</header>",
            "id": 191,
            "page": 12,
            "text": "Trust Region Policy Optimization"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 284
                },
                {
                    "x": 2263,
                    "y": 284
                },
                {
                    "x": 2263,
                    "y": 384
                },
                {
                    "x": 223,
                    "y": 384
                }
            ],
            "category": "paragraph",
            "html": "<p id='192' style='font-size:14px'>The preceding Lemma bounds the difference in expected advantage at each timestep t. We can sum over time to bound the<br>difference between 7(元) and L元(元). Subtracting Equation (26) and Equation (27), and defining 6 = maxs,a |A�(s,a)|,</p>",
            "id": 192,
            "page": 12,
            "text": "The preceding Lemma bounds the difference in expected advantage at each timestep t. We can sum over time to bound the difference between 7(元) and L元(元). Subtracting Equation (26) and Equation (27), and defining 6 = maxs,a |A�(s,a)|,"
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 1096
                },
                {
                    "x": 2259,
                    "y": 1096
                },
                {
                    "x": 2259,
                    "y": 1193
                },
                {
                    "x": 222,
                    "y": 1193
                }
            ],
            "category": "paragraph",
            "html": "<p id='193' style='font-size:14px'>Last, to replace a by the total variation divergence, we need to use the correspondence between TV divergence and coupled<br>random variables:</p>",
            "id": 193,
            "page": 12,
            "text": "Last, to replace a by the total variation divergence, we need to use the correspondence between TV divergence and coupled random variables:"
        },
        {
            "bounding_box": [
                {
                    "x": 301,
                    "y": 1248
                },
                {
                    "x": 2179,
                    "y": 1248
                },
                {
                    "x": 2179,
                    "y": 1348
                },
                {
                    "x": 301,
                    "y": 1348
                }
            ],
            "category": "paragraph",
            "html": "<p id='194' style='font-size:14px'>Suppose px and py are distributions with DTv(px II py) = a. Then there exists a joint distribution (X,Y)<br>whose marginals are px,py, for which X = Y with probability 1 - a.</p>",
            "id": 194,
            "page": 12,
            "text": "Suppose px and py are distributions with DTv(px II py) = a. Then there exists a joint distribution (X,Y) whose marginals are px,py, for which X = Y with probability 1 - a."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1401
                },
                {
                    "x": 909,
                    "y": 1401
                },
                {
                    "x": 909,
                    "y": 1451
                },
                {
                    "x": 223,
                    "y": 1451
                }
            ],
            "category": "paragraph",
            "html": "<p id='195' style='font-size:16px'>See (Levin et al., 2009), Proposition 4.7.</p>",
            "id": 195,
            "page": 12,
            "text": "See (Levin , 2009), Proposition 4.7."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1474
                },
                {
                    "x": 2263,
                    "y": 1474
                },
                {
                    "x": 2263,
                    "y": 1619
                },
                {
                    "x": 223,
                    "y": 1619
                }
            ],
            "category": "paragraph",
            "html": "<p id='196' style='font-size:16px'>It follows that if we have two policies � and 元 such that maxs DTV(�(·|s) ||규(·|s)) ≤ a, then we can define an �-coupled<br>policy pair (�, 元) with appropriate marginals. Taking a = maxs DTV(�(·|s) = �(·|s)) ≤ a in Equation (45), Theorem 1<br>follows.</p>",
            "id": 196,
            "page": 12,
            "text": "It follows that if we have two policies � and 元 such that maxs DTV(�(·|s) ||규(·|s)) ≤ a, then we can define an �-coupled policy pair (�, 元) with appropriate marginals. Taking a = maxs DTV(�(·|s) = �(·|s)) ≤ a in Equation (45), Theorem 1 follows."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1645
                },
                {
                    "x": 1543,
                    "y": 1645
                },
                {
                    "x": 1543,
                    "y": 1703
                },
                {
                    "x": 223,
                    "y": 1703
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='197' style='font-size:22px'>B Perturbation Theory Proof of Policy Improvement Bound</p>",
            "id": 197,
            "page": 12,
            "text": "B Perturbation Theory Proof of Policy Improvement Bound"
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 1722
                },
                {
                    "x": 1511,
                    "y": 1722
                },
                {
                    "x": 1511,
                    "y": 1775
                },
                {
                    "x": 225,
                    "y": 1775
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='198' style='font-size:16px'>We also provide an alternative proof of Theorem 1 using perturbation theory.</p>",
            "id": 198,
            "page": 12,
            "text": "We also provide an alternative proof of Theorem 1 using perturbation theory."
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 1818
                },
                {
                    "x": 2264,
                    "y": 1818
                },
                {
                    "x": 2264,
                    "y": 2066
                },
                {
                    "x": 222,
                    "y": 2066
                }
            ],
            "category": "paragraph",
            "html": "<p id='199' style='font-size:14px'>Proof. LetG = (1+7Pm+(yPx)2+...) = (1-7P�)-1, and similarly Let G = (1+7P�+(ツP�)2+...) = (1-7P元)-1.<br>We will use the convention that P (a density on state space) is a vector and r (a reward function on state space) is a dual<br>vector (i.e., linear functional on vectors), thus rp is a scalar meaning the expected reward under density p. Note that<br>7(�) = rGpo, and 7(元) = cGpo. Let △ = P元 - P�. We want to bound 7(元) -7(�) =r(G - G)po. We start with some<br>standard perturbation theory manipulations.</p>",
            "id": 199,
            "page": 12,
            "text": "Proof. LetG = (1+7Pm+(yPx)2+...) = (1-7P�)-1, and similarly Let G = (1+7P�+(ツP�)2+...) = (1-7P元)-1. We will use the convention that P (a density on state space) is a vector and r (a reward function on state space) is a dual vector (i.e., linear functional on vectors), thus rp is a scalar meaning the expected reward under density p. Note that 7(�) = rGpo, and 7(元) = cGpo. Let △ = P元 - P�. We want to bound 7(元) -7(�) =r(G - G)po. We start with some standard perturbation theory manipulations."
        },
        {
            "bounding_box": [
                {
                    "x": 220,
                    "y": 2256
                },
                {
                    "x": 969,
                    "y": 2256
                },
                {
                    "x": 969,
                    "y": 2321
                },
                {
                    "x": 220,
                    "y": 2321
                }
            ],
            "category": "paragraph",
            "html": "<p id='200' style='font-size:20px'>Left multiply by G and right multiply by G.</p>",
            "id": 200,
            "page": 12,
            "text": "Left multiply by G and right multiply by G."
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 2514
                },
                {
                    "x": 974,
                    "y": 2514
                },
                {
                    "x": 974,
                    "y": 2569
                },
                {
                    "x": 222,
                    "y": 2569
                }
            ],
            "category": "paragraph",
            "html": "<p id='201' style='font-size:18px'>Substituting the right-hand side into G gives</p>",
            "id": 201,
            "page": 12,
            "text": "Substituting the right-hand side into G gives"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2693
                },
                {
                    "x": 427,
                    "y": 2693
                },
                {
                    "x": 427,
                    "y": 2741
                },
                {
                    "x": 223,
                    "y": 2741
                }
            ],
            "category": "paragraph",
            "html": "<p id='202' style='font-size:14px'>So we have</p>",
            "id": 202,
            "page": 12,
            "text": "So we have"
        },
        {
            "bounding_box": [
                {
                    "x": 221,
                    "y": 2895
                },
                {
                    "x": 2263,
                    "y": 2895
                },
                {
                    "x": 2263,
                    "y": 2995
                },
                {
                    "x": 221,
                    "y": 2995
                }
            ],
            "category": "paragraph",
            "html": "<p id='203' style='font-size:14px'>Let us first consider the leading term yrG△Gpo. Note that rG = v, i.e., the infinite-horizon state-value function. Also<br>note that Gpo = ��. Thus we can write ycG△Gpo = �����. We will show that this expression equals the expected</p>",
            "id": 203,
            "page": 12,
            "text": "Let us first consider the leading term yrG△Gpo. Note that rG = v, i.e., the infinite-horizon state-value function. Also note that Gpo = ��. Thus we can write ycG△Gpo = �����. We will show that this expression equals the expected"
        },
        {
            "bounding_box": [
                {
                    "x": 971,
                    "y": 191
                },
                {
                    "x": 1516,
                    "y": 191
                },
                {
                    "x": 1516,
                    "y": 235
                },
                {
                    "x": 971,
                    "y": 235
                }
            ],
            "category": "header",
            "html": "<header id='204' style='font-size:16px'>Trust Region Policy Optimization</header>",
            "id": 204,
            "page": 13,
            "text": "Trust Region Policy Optimization"
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 283
                },
                {
                    "x": 691,
                    "y": 283
                },
                {
                    "x": 691,
                    "y": 338
                },
                {
                    "x": 222,
                    "y": 338
                }
            ],
            "category": "paragraph",
            "html": "<p id='205' style='font-size:20px'>advantage L�(�) - L�(�).</p>",
            "id": 205,
            "page": 13,
            "text": "advantage L�(�) - L�(�)."
        },
        {
            "bounding_box": [
                {
                    "x": 220,
                    "y": 957
                },
                {
                    "x": 2264,
                    "y": 957
                },
                {
                    "x": 2264,
                    "y": 1064
                },
                {
                    "x": 220,
                    "y": 1064
                }
            ],
            "category": "paragraph",
            "html": "<p id='206' style='font-size:14px'>Next let us bound the O(△2) term �rG�G△Gp. First we consider the product YrGA = yv△. Consider the component<br>s of this dual vector.</p>",
            "id": 206,
            "page": 13,
            "text": "Next let us bound the O(△2) term �rG�G△Gp. First we consider the product YrGA = yv△. Consider the component s of this dual vector."
        },
        {
            "bounding_box": [
                {
                    "x": 220,
                    "y": 1563
                },
                {
                    "x": 2263,
                    "y": 1563
                },
                {
                    "x": 2263,
                    "y": 1667
                },
                {
                    "x": 220,
                    "y": 1667
                }
            ],
            "category": "paragraph",
            "html": "<p id='207' style='font-size:14px'>where the last line used the definition of the total-variation divergence, and the definition of E = maxs,a |A�(s,a)|. We<br>bound the other portion G△Gp using the l1 operator norm</p>",
            "id": 207,
            "page": 13,
            "text": "where the last line used the definition of the total-variation divergence, and the definition of E = maxs,a |A�(s,a)|. We bound the other portion G△Gp using the l1 operator norm"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1840
                },
                {
                    "x": 1510,
                    "y": 1840
                },
                {
                    "x": 1510,
                    "y": 1901
                },
                {
                    "x": 223,
                    "y": 1901
                }
            ],
            "category": "paragraph",
            "html": "<p id='208' style='font-size:16px'>where we have that G1 = |G||| =1/(1 - 2) and ||△||1 = 2a. That gives</p>",
            "id": 208,
            "page": 13,
            "text": "where we have that G1 = |G||| =1/(1 - 2) and ||△||1 = 2a. That gives"
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 2121
                },
                {
                    "x": 499,
                    "y": 2121
                },
                {
                    "x": 499,
                    "y": 2169
                },
                {
                    "x": 222,
                    "y": 2169
                }
            ],
            "category": "paragraph",
            "html": "<p id='209' style='font-size:14px'>So we have that</p>",
            "id": 209,
            "page": 13,
            "text": "So we have that"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 2718
                },
                {
                    "x": 1848,
                    "y": 2718
                },
                {
                    "x": 1848,
                    "y": 2776
                },
                {
                    "x": 223,
                    "y": 2776
                }
            ],
            "category": "paragraph",
            "html": "<p id='210' style='font-size:22px'>C Efficiently Solving the Trust-Region Constrained Optimization Problem</p>",
            "id": 210,
            "page": 13,
            "text": "C Efficiently Solving the Trust-Region Constrained Optimization Problem"
        },
        {
            "bounding_box": [
                {
                    "x": 221,
                    "y": 2796
                },
                {
                    "x": 2267,
                    "y": 2796
                },
                {
                    "x": 2267,
                    "y": 2896
                },
                {
                    "x": 221,
                    "y": 2896
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='211' style='font-size:14px'>This section describes how to efficiently approximately solve the following constrained optimization problem, which we<br>must solve at each iteration of TRPO:</p>",
            "id": 211,
            "page": 13,
            "text": "This section describes how to efficiently approximately solve the following constrained optimization problem, which we must solve at each iteration of TRPO:"
        },
        {
            "bounding_box": [
                {
                    "x": 970,
                    "y": 190
                },
                {
                    "x": 1516,
                    "y": 190
                },
                {
                    "x": 1516,
                    "y": 236
                },
                {
                    "x": 970,
                    "y": 236
                }
            ],
            "category": "header",
            "html": "<header id='212' style='font-size:18px'>Trust Region Policy Optimization</header>",
            "id": 212,
            "page": 14,
            "text": "Trust Region Policy Optimization"
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 283
                },
                {
                    "x": 2264,
                    "y": 283
                },
                {
                    "x": 2264,
                    "y": 430
                },
                {
                    "x": 222,
                    "y": 430
                }
            ],
            "category": "paragraph",
            "html": "<p id='213' style='font-size:16px'>The method we will describe involves two steps: (1) compute a search direction, using a linear approximation to objective<br>and quadratic approximation to the constraint; and (2) perform a line search in that direction, ensuring that we improve the<br>nonlinear objective while satisfying the nonlinear constraint.</p>",
            "id": 213,
            "page": 14,
            "text": "The method we will describe involves two steps: (1) compute a search direction, using a linear approximation to objective and quadratic approximation to the constraint; and (2) perform a line search in that direction, ensuring that we improve the nonlinear objective while satisfying the nonlinear constraint."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 450
                },
                {
                    "x": 2264,
                    "y": 450
                },
                {
                    "x": 2264,
                    "y": 859
                },
                {
                    "x": 223,
                    "y": 859
                }
            ],
            "category": "paragraph",
            "html": "<p id='214' style='font-size:18px'>The search direction is computed by approximately solving the equation Ax = g, where A is the Fisher information<br>matrix, i.e., the quadratic approximation to the KL divergence constraint: DKL (Hold, 0) 2 1 (0 -Oold)T A(0 - Hold), where<br>a a D KL (Hold, 0). In large-scale problems, itis prohibitively costly (with respect to computation and memory) to<br>Aij =<br>dui doj<br>form the full matrix A (or A-1). However, the conjugate gradient algorithm allows us to approximately solve the equation<br>Ax = 6 without forming this full matrix, when we merely have access to a function that computes matrix-vector products<br>y → Ay. Appendix C.1 describes the most efficient way to compute matrix-vector products with the Fisher information<br>matrix. For additional exposition on the use of Hessian-vector products for optimizing neural network objectives, see<br>(Martens & Sutskever, 2012) and (Pascanu & Bengio, 2013).</p>",
            "id": 214,
            "page": 14,
            "text": "The search direction is computed by approximately solving the equation Ax = g, where A is the Fisher information matrix, i.e., the quadratic approximation to the KL divergence constraint: DKL (Hold, 0) 2 1 (0 -Oold)T A(0 - Hold), where a a D KL (Hold, 0). In large-scale problems, itis prohibitively costly (with respect to computation and memory) to Aij = dui doj form the full matrix A (or A-1). However, the conjugate gradient algorithm allows us to approximately solve the equation Ax = 6 without forming this full matrix, when we merely have access to a function that computes matrix-vector products y → Ay. Appendix C.1 describes the most efficient way to compute matrix-vector products with the Fisher information matrix. For additional exposition on the use of Hessian-vector products for optimizing neural network objectives, see (Martens & Sutskever, 2012) and (Pascanu & Bengio, 2013)."
        },
        {
            "bounding_box": [
                {
                    "x": 220,
                    "y": 877
                },
                {
                    "x": 2265,
                    "y": 877
                },
                {
                    "x": 2265,
                    "y": 1086
                },
                {
                    "x": 220,
                    "y": 1086
                }
            ],
            "category": "paragraph",
            "html": "<p id='215' style='font-size:18px'>Having computed the search direction s 2 A-1g, we next need to compute the maximal step length B such that 0 + Bs<br>will satisfy the KL divergence constraint. To do this, let 8 = DKL U 늘(Bs)T A(Bs) = 12B2ST As. From this, we obtain<br>B = V28/sT As, where 8 is the desired KL divergence. The term sT As can be computed through a single Hessian vector<br>product, and it is also an intermediate result produced by the conjugate gradient algorithm.</p>",
            "id": 215,
            "page": 14,
            "text": "Having computed the search direction s 2 A-1g, we next need to compute the maximal step length B such that 0 + Bs will satisfy the KL divergence constraint. To do this, let 8 = DKL U 늘(Bs)T A(Bs) = 12B2ST As. From this, we obtain B = V28/sT As, where 8 is the desired KL divergence. The term sT As can be computed through a single Hessian vector product, and it is also an intermediate result produced by the conjugate gradient algorithm."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1104
                },
                {
                    "x": 2264,
                    "y": 1104
                },
                {
                    "x": 2264,
                    "y": 1398
                },
                {
                    "x": 223,
                    "y": 1398
                }
            ],
            "category": "paragraph",
            "html": "<p id='216' style='font-size:16px'>Last, we use a line search to ensure improvement of the surrogate objective and satisfaction of the KL divergence constraint,<br>both of which are nonlinear in the parameter vector 0 (and thus depart from the linear and quadratic approximations used<br>to compute the step). We perform the line search on the objective Loold (0) - X[DKL (Hold, 0) ≤ 8], where X[. ] equals<br>zero when its argument is true and +00 when it is false. Starting with the maximal value of the step length B computed<br>in the previous paragraph, we shrink B exponentially until the objective improves. Without this line search, the algorithm<br>occasionally computes large steps that cause a catastrophic degradation of performance.</p>",
            "id": 216,
            "page": 14,
            "text": "Last, we use a line search to ensure improvement of the surrogate objective and satisfaction of the KL divergence constraint, both of which are nonlinear in the parameter vector 0 (and thus depart from the linear and quadratic approximations used to compute the step). We perform the line search on the objective Loold (0) - X[DKL (Hold, 0) ≤ 8], where X[. ] equals zero when its argument is true and +00 when it is false. Starting with the maximal value of the step length B computed in the previous paragraph, we shrink B exponentially until the objective improves. Without this line search, the algorithm occasionally computes large steps that cause a catastrophic degradation of performance."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 1433
                },
                {
                    "x": 1009,
                    "y": 1433
                },
                {
                    "x": 1009,
                    "y": 1488
                },
                {
                    "x": 224,
                    "y": 1488
                }
            ],
            "category": "paragraph",
            "html": "<p id='217' style='font-size:18px'>C.1 Computing the Fisher- Vector Product</p>",
            "id": 217,
            "page": 14,
            "text": "C.1 Computing the Fisher- Vector Product"
        },
        {
            "bounding_box": [
                {
                    "x": 221,
                    "y": 1523
                },
                {
                    "x": 2265,
                    "y": 1523
                },
                {
                    "x": 2265,
                    "y": 1721
                },
                {
                    "x": 221,
                    "y": 1721
                }
            ],
            "category": "paragraph",
            "html": "<p id='218' style='font-size:14px'>Here we will describe how to compute the matrix-vector product between the averaged Fisher information matrix and<br>arbitrary vectors. This matrix-vector product enables us to perform the conjugate gradient algorithm. Suppose that the<br>parameterized policy maps from the input x to \"distribution parameter\" vector en (x), which parameterizes the distribution<br>�(u|x). Now the KL divergence for a given input x can be written as follows:</p>",
            "id": 218,
            "page": 14,
            "text": "Here we will describe how to compute the matrix-vector product between the averaged Fisher information matrix and arbitrary vectors. This matrix-vector product enables us to perform the conjugate gradient algorithm. Suppose that the parameterized policy maps from the input x to \"distribution parameter\" vector en (x), which parameterizes the distribution �(u|x). Now the KL divergence for a given input x can be written as follows:"
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 1850
                },
                {
                    "x": 2261,
                    "y": 1850
                },
                {
                    "x": 2261,
                    "y": 1948
                },
                {
                    "x": 222,
                    "y": 1948
                }
            ],
            "category": "paragraph",
            "html": "<p id='219' style='font-size:14px'>where kl is the KL divergence between the distributions corresponding to the two mean parameter vectors. Differentiating<br>kl twice with respect to 0, we obtain</p>",
            "id": 219,
            "page": 14,
            "text": "where kl is the KL divergence between the distributions corresponding to the two mean parameter vectors. Differentiating kl twice with respect to 0, we obtain"
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 2131
                },
                {
                    "x": 2262,
                    "y": 2131
                },
                {
                    "x": 2262,
                    "y": 2395
                },
                {
                    "x": 222,
                    "y": 2395
                }
            ],
            "category": "paragraph",
            "html": "<p id='220' style='font-size:14px'>where the primes () indicate differentiation with respect to the first argument, and there is an implied summation over<br>dma(x)<br>indices a, 6. The second term vanishes, leaving just the first term. Let J := (the Jacobian), then the Fisher<br>dui<br>information matrix can be written in matrix form as JT MJ, where M = kl\"ab (��(x), Mold) is the Fisher information<br>matrix of the distribution in terms of the mean parameter m (as opposed to the parameter 0). This has a simple form for<br>most parameterized distributions of interest.</p>",
            "id": 220,
            "page": 14,
            "text": "where the primes () indicate differentiation with respect to the first argument, and there is an implied summation over dma(x) indices a, 6. The second term vanishes, leaving just the first term. Let J := (the Jacobian), then the Fisher dui information matrix can be written in matrix form as JT MJ, where M = kl\"ab (��(x), Mold) is the Fisher information matrix of the distribution in terms of the mean parameter m (as opposed to the parameter 0). This has a simple form for most parameterized distributions of interest."
        },
        {
            "bounding_box": [
                {
                    "x": 221,
                    "y": 2411
                },
                {
                    "x": 2263,
                    "y": 2411
                },
                {
                    "x": 2263,
                    "y": 2610
                },
                {
                    "x": 221,
                    "y": 2610
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='221' style='font-size:16px'>The Fisher-vector product can now be written as a function y → JT M Jy. Multiplication by JT and J can be performed by<br>most automatic differentiation and neural network packages (multiplication by JT is the well-known backprop operation),<br>and the operation for multiplication by M can be derived for the distribution of interest. Note that this Fisher-vector product<br>is straightforward to average over a set of datapoints, i.e., inputs x to u.</p>",
            "id": 221,
            "page": 14,
            "text": "The Fisher-vector product can now be written as a function y → JT M Jy. Multiplication by JT and J can be performed by most automatic differentiation and neural network packages (multiplication by JT is the well-known backprop operation), and the operation for multiplication by M can be derived for the distribution of interest. Note that this Fisher-vector product is straightforward to average over a set of datapoints, i.e., inputs x to u."
        },
        {
            "bounding_box": [
                {
                    "x": 221,
                    "y": 2631
                },
                {
                    "x": 2264,
                    "y": 2631
                },
                {
                    "x": 2264,
                    "y": 2825
                },
                {
                    "x": 221,
                    "y": 2825
                }
            ],
            "category": "paragraph",
            "html": "<p id='222' style='font-size:18px'>One could alternatively use a generic method for calculating Hessian-vector products using reverse mode automatic differ-<br>entiation ((Wright & Nocedal, 1999), chapter 8), computing the Hessian of DKL with respect to 0. This method would be<br>slightly less efficient as it does not exploit the fact that the second derivatives of �(x) (i.e., the second term in Equation (57))<br>can be ignored, but may be substantially easier to implement.</p>",
            "id": 222,
            "page": 14,
            "text": "One could alternatively use a generic method for calculating Hessian-vector products using reverse mode automatic differentiation ((Wright & Nocedal, 1999), chapter 8), computing the Hessian of DKL with respect to 0. This method would be slightly less efficient as it does not exploit the fact that the second derivatives of �(x) (i.e., the second term in Equation (57)) can be ignored, but may be substantially easier to implement."
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 2846
                },
                {
                    "x": 2265,
                    "y": 2846
                },
                {
                    "x": 2265,
                    "y": 2995
                },
                {
                    "x": 222,
                    "y": 2995
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='223' style='font-size:16px'>We have described a procedure for computing the Fisher-vector product y → Ay, where the Fisher information matrix is<br>averaged over a set of inputs to the function H. Computing the Fisher-vector product is typically about as expensive as<br>computing the gradient of an objective that depends on �(x) (Wright & Nocedal, 1999). Furthermore, we need to compute</p>",
            "id": 223,
            "page": 14,
            "text": "We have described a procedure for computing the Fisher-vector product y → Ay, where the Fisher information matrix is averaged over a set of inputs to the function H. Computing the Fisher-vector product is typically about as expensive as computing the gradient of an objective that depends on �(x) (Wright & Nocedal, 1999). Furthermore, we need to compute"
        },
        {
            "bounding_box": [
                {
                    "x": 970,
                    "y": 190
                },
                {
                    "x": 1516,
                    "y": 190
                },
                {
                    "x": 1516,
                    "y": 236
                },
                {
                    "x": 970,
                    "y": 236
                }
            ],
            "category": "header",
            "html": "<header id='224' style='font-size:18px'>Trust Region Policy Optimization</header>",
            "id": 224,
            "page": 15,
            "text": "Trust Region Policy Optimization"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 281
                },
                {
                    "x": 2265,
                    "y": 281
                },
                {
                    "x": 2265,
                    "y": 671
                },
                {
                    "x": 223,
                    "y": 671
                }
            ],
            "category": "paragraph",
            "html": "<p id='225' style='font-size:16px'>k of these Fisher-vector products per gradient, where k is the number of iterations of the conjugate gradient algorithm we<br>perform. We found k = 10 to be quite effective, and using higher k did not result in faster policy improvement. Hence, a<br>naive implementation would spend more than 90% of the computational effort on these Fisher-vector products. However,<br>we can greatly reduce this burden by subsampling the data for the computation of Fisher-vector product. Since the Fisher<br>information matrix merely acts as a metric, it can be computed on a subset of the data without severely degrading the<br>quality of the final step. Hence, we can compute it on 10% of the data, and the total cost of Hessian-vector products will<br>be about the same as computing the gradient. With this optimization, the computation of a natural gradient step A-1g does<br>not incur a significant extra computational cost beyond computing the gradient g.</p>",
            "id": 225,
            "page": 15,
            "text": "k of these Fisher-vector products per gradient, where k is the number of iterations of the conjugate gradient algorithm we perform. We found k = 10 to be quite effective, and using higher k did not result in faster policy improvement. Hence, a naive implementation would spend more than 90% of the computational effort on these Fisher-vector products. However, we can greatly reduce this burden by subsampling the data for the computation of Fisher-vector product. Since the Fisher information matrix merely acts as a metric, it can be computed on a subset of the data without severely degrading the quality of the final step. Hence, we can compute it on 10% of the data, and the total cost of Hessian-vector products will be about the same as computing the gradient. With this optimization, the computation of a natural gradient step A-1g does not incur a significant extra computational cost beyond computing the gradient g."
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 691
                },
                {
                    "x": 1504,
                    "y": 691
                },
                {
                    "x": 1504,
                    "y": 752
                },
                {
                    "x": 222,
                    "y": 752
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='226' style='font-size:20px'>D Approximating Factored Policies with Neural Networks</p>",
            "id": 226,
            "page": 15,
            "text": "D Approximating Factored Policies with Neural Networks"
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 774
                },
                {
                    "x": 2264,
                    "y": 774
                },
                {
                    "x": 2264,
                    "y": 923
                },
                {
                    "x": 222,
                    "y": 923
                }
            ],
            "category": "paragraph",
            "html": "<p id='227' style='font-size:14px'>The policy, which is a conditional probability distribution ��(a|s), can be parameterized with a neural network. This<br>neural network maps (deterministically) from the state vector s to a vector H, which specifies a distribution over action<br>space. Then we can compute the likelihood p(a|�) and sample a ~ p(a|�).</p>",
            "id": 227,
            "page": 15,
            "text": "The policy, which is a conditional probability distribution ��(a|s), can be parameterized with a neural network. This neural network maps (deterministically) from the state vector s to a vector H, which specifies a distribution over action space. Then we can compute the likelihood p(a|�) and sample a ~ p(a|�)."
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 940
                },
                {
                    "x": 2263,
                    "y": 940
                },
                {
                    "x": 2263,
                    "y": 1275
                },
                {
                    "x": 222,
                    "y": 1275
                }
            ],
            "category": "paragraph",
            "html": "<p id='228' style='font-size:18px'>For our experiments with continuous state and action spaces, we used a Gaussian distribution, where the covariance matrix<br>was diagonal and independent of the state. A neural network with several fully-connected (dense) layers maps from the<br>input features to the mean of a Gaussian distribution. A separate set of parameters specifies the log standard deviation of<br>each element. More concretely, the parameters include a set of weights and biases for the neural network computing the<br>mean, {Wi, bi}L=1, and a vector r (log standard deviation) with the same dimension as a. Then, the policy is defined by<br>the normal distribution N (mean = NeuralNet (s; {Wi, bi}i=1 ) , stdev = exp(r)). Here, m = [mean, stdev].</p>",
            "id": 228,
            "page": 15,
            "text": "For our experiments with continuous state and action spaces, we used a Gaussian distribution, where the covariance matrix was diagonal and independent of the state. A neural network with several fully-connected (dense) layers maps from the input features to the mean of a Gaussian distribution. A separate set of parameters specifies the log standard deviation of each element. More concretely, the parameters include a set of weights and biases for the neural network computing the mean, {Wi, bi}L=1, and a vector r (log standard deviation) with the same dimension as a. Then, the policy is defined by the normal distribution N (mean = NeuralNet (s; {Wi, bi}i=1 ) , stdev = exp(r)). Here, m = [mean, stdev]."
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 1287
                },
                {
                    "x": 2266,
                    "y": 1287
                },
                {
                    "x": 2266,
                    "y": 1592
                },
                {
                    "x": 222,
                    "y": 1592
                }
            ],
            "category": "paragraph",
            "html": "<p id='229' style='font-size:14px'>For the experiments with discrete actions (Atari), we use a factored discrete action space, where each factor is parameter-<br>ized as a categorical distribution. That is, the action consists of a tuple (a1, a2, · · · , ak) of integers ak E {1, 2, · · · , Nk},<br>and each of these components is assumed to have a categorical distribution, which is specified by a vector �k =<br>[P1, p2, · · · , pNk]. Hence, H is defined to be the concatenation of the factors' parameters: H = [从1, H2, · · · , �K] and<br>has dimension dim H = EK=1 Nk. The components of H are computed by taking applying a neural network to the input s<br>and then applying the softmax operator to each slice, yielding normalized probabilities for each factor.</p>",
            "id": 229,
            "page": 15,
            "text": "For the experiments with discrete actions (Atari), we use a factored discrete action space, where each factor is parameterized as a categorical distribution. That is, the action consists of a tuple (a1, a2, · · · , ak) of integers ak E {1, 2, · · · , Nk}, and each of these components is assumed to have a categorical distribution, which is specified by a vector �k = [P1, p2, · · · , pNk]. Hence, H is defined to be the concatenation of the factors' parameters: H = [从1, H2, · · · , �K] and has dimension dim H = EK=1 Nk. The components of H are computed by taking applying a neural network to the input s and then applying the softmax operator to each slice, yielding normalized probabilities for each factor."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 1610
                },
                {
                    "x": 832,
                    "y": 1610
                },
                {
                    "x": 832,
                    "y": 1672
                },
                {
                    "x": 223,
                    "y": 1672
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='230' style='font-size:22px'>E Experiment Parameters</p>",
            "id": 230,
            "page": 15,
            "text": "E Experiment Parameters"
        },
        {
            "bounding_box": [
                {
                    "x": 648,
                    "y": 2019
                },
                {
                    "x": 1840,
                    "y": 2019
                },
                {
                    "x": 1840,
                    "y": 2907
                },
                {
                    "x": 648,
                    "y": 2907
                }
            ],
            "category": "table",
            "html": "<table id='231' style='font-size:18px'><tr><td></td><td>Swimmer</td><td>Hopper</td><td>Walker</td></tr><tr><td>State space dim.</td><td>10</td><td>12</td><td>20</td></tr><tr><td>Control space dim.</td><td>2</td><td>3</td><td>6</td></tr><tr><td>Total num. policy params</td><td>364</td><td>4806</td><td>8206</td></tr><tr><td>Sim. steps per iter.</td><td>50K</td><td>1M</td><td>1M</td></tr><tr><td>Policy iter.</td><td>200</td><td>200</td><td>200</td></tr><tr><td>Stepsize (DKL)</td><td>0.01</td><td>0.01</td><td>0.01</td></tr><tr><td>Hidden layer size</td><td>30</td><td>50</td><td>50</td></tr><tr><td>Discount ()</td><td>0.99</td><td>0.99</td><td>0.99</td></tr><tr><td>Vine: rollout length</td><td>50</td><td>100</td><td>100</td></tr><tr><td>Vine: rollouts per state</td><td>4</td><td>4</td><td>4</td></tr><tr><td>Vine: Q-values per batch</td><td>500</td><td>2500</td><td>2500</td></tr><tr><td>Vine: num. rollouts for sampling</td><td>16</td><td>16</td><td>16</td></tr><tr><td>Vine: len. rollouts for sampling</td><td>1000</td><td>1000</td><td>1000</td></tr><tr><td>Vine: computation time (minutes)</td><td>2</td><td>14</td><td>40</td></tr><tr><td>SP: num. path</td><td>50</td><td>1000</td><td>10000</td></tr><tr><td>SP: path len.</td><td>1000</td><td>1000</td><td>1000</td></tr><tr><td>SP: computation time</td><td>5</td><td>35</td><td>100</td></tr></table>",
            "id": 231,
            "page": 15,
            "text": "Swimmer Hopper Walker  State space dim. 10 12 20  Control space dim. 2 3 6  Total num. policy params 364 4806 8206  Sim. steps per iter. 50K 1M 1M  Policy iter. 200 200 200  Stepsize (DKL) 0.01 0.01 0.01  Hidden layer size 30 50 50  Discount () 0.99 0.99 0.99  Vine: rollout length 50 100 100  Vine: rollouts per state 4 4 4  Vine: Q-values per batch 500 2500 2500  Vine: num. rollouts for sampling 16 16 16  Vine: len. rollouts for sampling 1000 1000 1000  Vine: computation time (minutes) 2 14 40  SP: num. path 50 1000 10000  SP: path len. 1000 1000 1000  SP: computation time 5 35"
        },
        {
            "bounding_box": [
                {
                    "x": 592,
                    "y": 2936
                },
                {
                    "x": 1895,
                    "y": 2936
                },
                {
                    "x": 1895,
                    "y": 2988
                },
                {
                    "x": 592,
                    "y": 2988
                }
            ],
            "category": "caption",
            "html": "<caption id='232' style='font-size:14px'>Table 2. Parameters for continuous control tasks, vine and single path (SP) algorithms.</caption>",
            "id": 232,
            "page": 15,
            "text": "Table 2. Parameters for continuous control tasks, vine and single path (SP) algorithms."
        },
        {
            "bounding_box": [
                {
                    "x": 969,
                    "y": 191
                },
                {
                    "x": 1517,
                    "y": 191
                },
                {
                    "x": 1517,
                    "y": 236
                },
                {
                    "x": 969,
                    "y": 236
                }
            ],
            "category": "header",
            "html": "<header id='233' style='font-size:20px'>Trust Region Policy Optimization</header>",
            "id": 233,
            "page": 16,
            "text": "Trust Region Policy Optimization"
        },
        {
            "bounding_box": [
                {
                    "x": 887,
                    "y": 266
                },
                {
                    "x": 1602,
                    "y": 266
                },
                {
                    "x": 1602,
                    "y": 771
                },
                {
                    "x": 887,
                    "y": 771
                }
            ],
            "category": "table",
            "html": "<table id='234' style='font-size:18px'><tr><td></td><td>All games</td></tr><tr><td>Total num. policy params</td><td>33500</td></tr><tr><td>Vine: Sim. steps per iter.</td><td>400K</td></tr><tr><td>SP: Sim. steps per iter.</td><td>100K</td></tr><tr><td>Policy iter.</td><td>500</td></tr><tr><td>Stepsize (DKL )</td><td>0.01</td></tr><tr><td>Discount ()</td><td>0.99</td></tr><tr><td>Vine: rollouts per state</td><td>21 4</td></tr><tr><td>Vine: computation time</td><td>2 30 hrs</td></tr><tr><td>SP: computation time</td><td>21 30 hrs</td></tr></table>",
            "id": 234,
            "page": 16,
            "text": "All games  Total num. policy params 33500  Vine: Sim. steps per iter. 400K  SP: Sim. steps per iter. 100K  Policy iter. 500  Stepsize (DKL ) 0.01  Discount () 0.99  Vine: rollouts per state 21 4  Vine: computation time 2 30 hrs  SP: computation time"
        },
        {
            "bounding_box": [
                {
                    "x": 917,
                    "y": 803
                },
                {
                    "x": 1568,
                    "y": 803
                },
                {
                    "x": 1568,
                    "y": 846
                },
                {
                    "x": 917,
                    "y": 846
                }
            ],
            "category": "caption",
            "html": "<caption id='235' style='font-size:16px'>Table 3. Parameters used for Atari domain.</caption>",
            "id": 235,
            "page": 16,
            "text": "Table 3. Parameters used for Atari domain."
        },
        {
            "bounding_box": [
                {
                    "x": 224,
                    "y": 920
                },
                {
                    "x": 1139,
                    "y": 920
                },
                {
                    "x": 1139,
                    "y": 980
                },
                {
                    "x": 224,
                    "y": 980
                }
            ],
            "category": "caption",
            "html": "<caption id='236' style='font-size:22px'>F Learning Curves for the Atari Domain</caption>",
            "id": 236,
            "page": 16,
            "text": "F Learning Curves for the Atari Domain"
        },
        {
            "bounding_box": [
                {
                    "x": 235,
                    "y": 1016
                },
                {
                    "x": 2256,
                    "y": 1016
                },
                {
                    "x": 2256,
                    "y": 2454
                },
                {
                    "x": 235,
                    "y": 2454
                }
            ],
            "category": "figure",
            "html": "<figure><img id='237' style='font-size:14px' alt=\"breakout endur o\nbeam rider 0\n100\n-400\nsingle path\nsingle path -5 single path\nvine 0 vine\nvine\n-600\n-10\n-100\n-15\n-800\n-20 -200\ncost\ncost\ncost\n-1000\n-25\n-300\n-30\n-1200\n-400\n-35\n-1400\n-500\n-40\n-1600 -45 -600\n0 100 200 300 400 500 0 100 200 300 400 500 0 100 200 300 400 500\nnumber of policy iterati ons number of policy iterati ons number of policy iterations\npong\n30 qbert seaquest\n0 500\nsingle path\nsingle path single path\nvine -1000\n20\nvine vine\n0\n-2000\n10\n-3000\n-500\ncost\ncost\n0 cost -4000\n-1000\n-5000\n-10\n-6000\n-1500\n-20\n-7000\n-30 -8000 -2000\n0 100 200 300 400 500 0 100 200 300 400 500 0 100 200 300 400 500\nnumber of policy iterations number of policy iterations number of policy iterations\nspace invaders\n-100\nsingle path\nvine\n-200\n-300\ncost\n-400\n-500\n-600\n0 100 200 300 400 500\nnumber of policy iterations\" data-coord=\"top-left:(235,1016); bottom-right:(2256,2454)\" /></figure>",
            "id": 237,
            "page": 16,
            "text": "breakout endur o beam rider 0 100 -400 single path single path -5 single path vine 0 vine vine -600 -10 -100 -15 -800 -20 -200 cost cost cost -1000 -25 -300 -30 -1200 -400 -35 -1400 -500 -40 -1600 -45 -600 0 100 200 300 400 500 0 100 200 300 400 500 0 100 200 300 400 500 number of policy iterati ons number of policy iterati ons number of policy iterations pong 30 qbert seaquest 0 500 single path single path single path vine -1000 20 vine vine 0 -2000 10 -3000 -500 cost cost 0 cost -4000 -1000 -5000 -10 -6000 -1500 -20 -7000 -30 -8000 -2000 0 100 200 300 400 500 0 100 200 300 400 500 0 100 200 300 400 500 number of policy iterations number of policy iterations number of policy iterations space invaders -100 single path vine -200 -300 cost -400 -500 -600 0 100 200 300 400 500 number of policy iterations"
        },
        {
            "bounding_box": [
                {
                    "x": 422,
                    "y": 2488
                },
                {
                    "x": 2064,
                    "y": 2488
                },
                {
                    "x": 2064,
                    "y": 2537
                },
                {
                    "x": 422,
                    "y": 2537
                }
            ],
            "category": "caption",
            "html": "<caption id='238' style='font-size:16px'>Figure 5. Learning curves for the Atari domain. For historical reasons, the plots show cost = negative reward.</caption>",
            "id": 238,
            "page": 16,
            "text": "Figure 5. Learning curves for the Atari domain. For historical reasons, the plots show cost = negative reward."
        }
    ]
}