{
  "id": "62a63c00-0f92-11ef-8230-426932df3dcf",
  "pdf_path": "./pdf/AI_VIT_O/2204.02557v2.pdf",
  "elements": [
    {
      "bounding_box": [
        {
          "x": 432,
          "y": 437
        },
        {
          "x": 2044,
          "y": 437
        },
        {
          "x": 2044,
          "y": 505
        },
        {
          "x": 432,
          "y": 505
        }
      ],
      "category": "paragraph",
      "html": "<p id='0' style='font-size:22px'>MixFormer: Mixing Features across Windows and Dimensions</p>",
      "id": 0,
      "page": 1,
      "text": "MixFormer: Mixing Features across Windows and Dimensions"
    },
    {
      "bounding_box": [
        {
          "x": 575,
          "y": 599
        },
        {
          "x": 1900,
          "y": 599
        },
        {
          "x": 1900,
          "y": 783
        },
        {
          "x": 575,
          "y": 783
        }
      ],
      "category": "paragraph",
      "html": "<p id='1' style='font-size:22px'>Qiang Chen1*, Qiman Wu1* Jian Wang1* , Qinghao Hu2+, Tao Hu1<br>Errui Ding1, Jian Cheng2, Jingdong Wang1<br>1Baidu VIS</p>",
      "id": 1,
      "page": 1,
      "text": "Qiang Chen1*, Qiman Wu1* Jian Wang1* , Qinghao Hu2+, Tao Hu1\nErrui Ding1, Jian Cheng2, Jingdong Wang1\n1Baidu VIS"
    },
    {
      "bounding_box": [
        {
          "x": 597,
          "y": 775
        },
        {
          "x": 1881,
          "y": 775
        },
        {
          "x": 1881,
          "y": 831
        },
        {
          "x": 597,
          "y": 831
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='2' style='font-size:22px'>2NLPR, Institute of Automation, Chinese Academy of Sciences</p>",
      "id": 2,
      "page": 1,
      "text": "2NLPR, Institute of Automation, Chinese Academy of Sciences"
    },
    {
      "bounding_box": [
        {
          "x": 420,
          "y": 844
        },
        {
          "x": 2063,
          "y": 844
        },
        {
          "x": 2063,
          "y": 946
        },
        {
          "x": 420,
          "y": 946
        }
      ],
      "category": "paragraph",
      "html": "<p id='3' style='font-size:14px'>{chenqiang1 3, wuqiman, wangjian33 , hutao06, dingerrui, wangjingdong} @baidu . com<br>huqinghao2014@ia · ac · cn, jcheng@nlpr · ia · ac · cn</p>",
      "id": 3,
      "page": 1,
      "text": "{chenqiang1 3, wuqiman, wangjian33 , hutao06, dingerrui, wangjingdong} @baidu . com\nhuqinghao2014@ia · ac · cn, jcheng@nlpr · ia · ac · cn"
    },
    {
      "bounding_box": [
        {
          "x": 602,
          "y": 1068
        },
        {
          "x": 799,
          "y": 1068
        },
        {
          "x": 799,
          "y": 1118
        },
        {
          "x": 602,
          "y": 1118
        }
      ],
      "category": "paragraph",
      "html": "<p id='4' style='font-size:20px'>Abstract</p>",
      "id": 4,
      "page": 1,
      "text": "Abstract"
    },
    {
      "bounding_box": [
        {
          "x": 199,
          "y": 1171
        },
        {
          "x": 1199,
          "y": 1171
        },
        {
          "x": 1199,
          "y": 2168
        },
        {
          "x": 199,
          "y": 2168
        }
      ],
      "category": "paragraph",
      "html": "<p id='5' style='font-size:18px'>While local-window self-attention performs notably in<br>vision tasks, it suffers from limited receptive field and weak<br>modeling capability issues. This is mainly because it per-<br>forms self-attention within non-overlapped windows and<br>shares weights on the channel dimension. We propose Mix-<br>Former to find a solution. First, we combine local-window<br>self-attention with depth-wise convolution in a parallel de-<br>sign, modeling cross-window connections to enlarge the re-<br>ceptive fields. Second, we propose bi-directional interac-<br>tions across branches to provide complementary clues in<br>the channel and spatial dimensions. These two designs are<br>integrated to achieve efficient feature mixing among win-<br>dows and dimensions. Our MixFormer provides compet-<br>itive results on image classification with EfficientNet and<br>shows better results than RegNet and Swin Transformer.<br>Performance in downstream tasks outperforms its alterna-<br>tives by significant margins with less computational costs<br>in 5 dense prediction tasks on MS COCO, ADE20k, and<br>LVIS. Code is available at https : / / github · com /<br>Paddl ePaddl e/Paddl eClas.</p>",
      "id": 5,
      "page": 1,
      "text": "While local-window self-attention performs notably in\nvision tasks, it suffers from limited receptive field and weak\nmodeling capability issues. This is mainly because it per-\nforms self-attention within non-overlapped windows and\nshares weights on the channel dimension. We propose Mix-\nFormer to find a solution. First, we combine local-window\nself-attention with depth-wise convolution in a parallel de-\nsign, modeling cross-window connections to enlarge the re-\nceptive fields. Second, we propose bi-directional interac-\ntions across branches to provide complementary clues in\nthe channel and spatial dimensions. These two designs are\nintegrated to achieve efficient feature mixing among win-\ndows and dimensions. Our MixFormer provides compet-\nitive results on image classification with EfficientNet and\nshows better results than RegNet and Swin Transformer.\nPerformance in downstream tasks outperforms its alterna-\ntives by significant margins with less computational costs\nin 5 dense prediction tasks on MS COCO, ADE20k, and\nLVIS. Code is available at https : / / github · com /\nPaddl ePaddl e/Paddl eClas."
    },
    {
      "bounding_box": [
        {
          "x": 205,
          "y": 2270
        },
        {
          "x": 532,
          "y": 2270
        },
        {
          "x": 532,
          "y": 2322
        },
        {
          "x": 205,
          "y": 2322
        }
      ],
      "category": "paragraph",
      "html": "<p id='6' style='font-size:20px'>1. Introduction</p>",
      "id": 6,
      "page": 1,
      "text": "1. Introduction"
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 2357
        },
        {
          "x": 1199,
          "y": 2357
        },
        {
          "x": 1199,
          "y": 2855
        },
        {
          "x": 203,
          "y": 2855
        }
      ],
      "category": "paragraph",
      "html": "<p id='7' style='font-size:18px'>The success of Vision Transformer (ViT) [11, 44] in im-<br>age classification [9] validates the potential to apply Trans-<br>former [46] to vision tasks. Challenges remain for down-<br>stream tasks, especially the inefficiency in high-resolution<br>vision tasks and the ineffectiveness in capturing local rela-<br>tions. One possible solution is to use local-window self-<br>attention. It performs self-attention within non-overlapped<br>windows and shares weights on the channel dimension. Al-<br>though this process improves efficiency, it poses the issues<br>of limited receptive field and weak modeling capability.</p>",
      "id": 7,
      "page": 1,
      "text": "The success of Vision Transformer (ViT) [11, 44] in im-\nage classification [9] validates the potential to apply Trans-\nformer [46] to vision tasks. Challenges remain for down-\nstream tasks, especially the inefficiency in high-resolution\nvision tasks and the ineffectiveness in capturing local rela-\ntions. One possible solution is to use local-window self-\nattention. It performs self-attention within non-overlapped\nwindows and shares weights on the channel dimension. Al-\nthough this process improves efficiency, it poses the issues\nof limited receptive field and weak modeling capability."
    },
    {
      "bounding_box": [
        {
          "x": 257,
          "y": 2894
        },
        {
          "x": 567,
          "y": 2894
        },
        {
          "x": 567,
          "y": 2970
        },
        {
          "x": 257,
          "y": 2970
        }
      ],
      "category": "paragraph",
      "html": "<p id='8' style='font-size:16px'>*Equal Contribution.<br>1 Corresponding author.</p>",
      "id": 8,
      "page": 1,
      "text": "*Equal Contribution.\n1 Corresponding author."
    },
    {
      "bounding_box": [
        {
          "x": 1672,
          "y": 1087
        },
        {
          "x": 1884,
          "y": 1087
        },
        {
          "x": 1884,
          "y": 1128
        },
        {
          "x": 1672,
          "y": 1128
        }
      ],
      "category": "caption",
      "html": "<br><caption id='9' style='font-size:18px'>Input Features</caption>",
      "id": 9,
      "page": 1,
      "text": "Input Features"
    },
    {
      "bounding_box": [
        {
          "x": 1442,
          "y": 1146
        },
        {
          "x": 2113,
          "y": 1146
        },
        {
          "x": 2113,
          "y": 1851
        },
        {
          "x": 1442,
          "y": 1851
        }
      ],
      "category": "figure",
      "html": "<br><figure><img id='10' style='font-size:16px' alt=\"Channel\nInteraction\nLocal-window Depth-wise\nSelf-attention Convolution\nSpatial\nInteraction\nConcat\nFFN\n↓\nOutput Features\" data-coord=\"top-left:(1442,1146); bottom-right:(2113,1851)\" /></figure>",
      "id": 10,
      "page": 1,
      "text": "Channel\nInteraction\nLocal-window Depth-wise\nSelf-attention Convolution\nSpatial\nInteraction\nConcat\nFFN\n↓\nOutput Features"
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 1902
        },
        {
          "x": 2276,
          "y": 1902
        },
        {
          "x": 2276,
          "y": 2360
        },
        {
          "x": 1279,
          "y": 2360
        }
      ],
      "category": "caption",
      "html": "<caption id='11' style='font-size:16px'>Figure 1. The Mixing Block. We combine local-window self-<br>attention with depth-wise convolution in a parallel design. The<br>captured relations within and across windows in parallel branches<br>are concatenated and sent to the Feed-Forward Network (FFN)<br>for output features. In the figure, the blue arrows marked with<br>Channel Interaction and Spatial Interaction are the proposed bi-<br>directional interactions, which provide complementary clues for<br>better representation learning in both branches. Other details in<br>the block, such as module design, normalization layers, and short-<br>cuts, are omitted for a neat presentation.</caption>",
      "id": 11,
      "page": 1,
      "text": "Figure 1. The Mixing Block. We combine local-window self-\nattention with depth-wise convolution in a parallel design. The\ncaptured relations within and across windows in parallel branches\nare concatenated and sent to the Feed-Forward Network (FFN)\nfor output features. In the figure, the blue arrows marked with\nChannel Interaction and Spatial Interaction are the proposed bi-\ndirectional interactions, which provide complementary clues for\nbetter representation learning in both branches. Other details in\nthe block, such as module design, normalization layers, and short-\ncuts, are omitted for a neat presentation."
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 2429
        },
        {
          "x": 2275,
          "y": 2429
        },
        {
          "x": 2275,
          "y": 2977
        },
        {
          "x": 1278,
          "y": 2977
        }
      ],
      "category": "paragraph",
      "html": "<p id='12' style='font-size:18px'>A common approach to expand receptive field is to cre-<br>ate cross-window connections. Windows are connected by<br>shifting [34], expanding [45, 57], or shuffling [26] opera-<br>tions. Convolution layers are also employed as they capture<br>natural local relations. Researches [26, 61] combine local-<br>window self-attention with depth-wise convolution base on<br>this and provide promising results. Still, the operations cap-<br>ture intra-window and cross-window relations in successive<br>steps, leaving these two types of relations less interweaved.<br>Besides, neglect of modeling weakness in these attempts<br>hinders further advances in feature representation learning.</p>",
      "id": 12,
      "page": 1,
      "text": "A common approach to expand receptive field is to cre-\nate cross-window connections. Windows are connected by\nshifting [34], expanding [45, 57], or shuffling [26] opera-\ntions. Convolution layers are also employed as they capture\nnatural local relations. Researches [26, 61] combine local-\nwindow self-attention with depth-wise convolution base on\nthis and provide promising results. Still, the operations cap-\nture intra-window and cross-window relations in successive\nsteps, leaving these two types of relations less interweaved.\nBesides, neglect of modeling weakness in these attempts\nhinders further advances in feature representation learning."
    },
    {
      "bounding_box": [
        {
          "x": 58,
          "y": 870
        },
        {
          "x": 149,
          "y": 870
        },
        {
          "x": 149,
          "y": 2327
        },
        {
          "x": 58,
          "y": 2327
        }
      ],
      "category": "footer",
      "html": "<br><footer id='13' style='font-size:14px'>2022<br>Apr<br>12<br>[cs.CV]<br>arXiv:2204.02557v2</footer>",
      "id": 13,
      "page": 1,
      "text": "2022\nApr\n12\n[cs.CV]\narXiv:2204.02557v2"
    },
    {
      "bounding_box": [
        {
          "x": 1226,
          "y": 3055
        },
        {
          "x": 1250,
          "y": 3055
        },
        {
          "x": 1250,
          "y": 3092
        },
        {
          "x": 1226,
          "y": 3092
        }
      ],
      "category": "footer",
      "html": "<footer id='14' style='font-size:16px'>1</footer>",
      "id": 14,
      "page": 1,
      "text": "1"
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 306
        },
        {
          "x": 1199,
          "y": 306
        },
        {
          "x": 1199,
          "y": 999
        },
        {
          "x": 200,
          "y": 999
        }
      ],
      "category": "paragraph",
      "html": "<p id='15' style='font-size:16px'>We propose Mixing Block to address both these is-<br>sues (Figure 1). First, we combine local-window self-<br>attention with depth-wise convolution, but in a parallel<br>way. The parallel design enlarges the receptive fields by<br>modeling intra-window and cross-window relations simul-<br>taneously. Second, we introduce bi-directional interactions<br>across branches(illustrated as blue arrows in Figure 1). The<br>interactions offset the limits caused by the weight sharing<br>mechanism 1 and enhance the modeling ability in channel<br>,<br>and spatial dimensions by providing complementary clues<br>for local-window self-attention and depth-wise convolution<br>respectively. The above designs are integrated to achieve<br>complementary feature mixing across windows and dimen-<br>sions.</p>",
      "id": 15,
      "page": 2,
      "text": "We propose Mixing Block to address both these is-\nsues (Figure 1). First, we combine local-window self-\nattention with depth-wise convolution, but in a parallel\nway. The parallel design enlarges the receptive fields by\nmodeling intra-window and cross-window relations simul-\ntaneously. Second, we introduce bi-directional interactions\nacross branches(illustrated as blue arrows in Figure 1). The\ninteractions offset the limits caused by the weight sharing\nmechanism 1 and enhance the modeling ability in channel\n,\nand spatial dimensions by providing complementary clues\nfor local-window self-attention and depth-wise convolution\nrespectively. The above designs are integrated to achieve\ncomplementary feature mixing across windows and dimen-\nsions."
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 1006
        },
        {
          "x": 1199,
          "y": 1006
        },
        {
          "x": 1199,
          "y": 2000
        },
        {
          "x": 200,
          "y": 2000
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='16' style='font-size:16px'>We present MixFormer to verify the block's efficiency<br>and effectiveness. A series of MixFormers with com-<br>putational complexity ranging from 0.7G (B1) to 3.6G<br>(B4) are built to perform distinguished in multiple vi-<br>sion tasks, including image classification, object detection,<br>instance segmentation, semantic segmentation, etc. On<br>ImageNet-1K [9], we achieve competitive results with Ef-<br>ficientNet [43], surpassing RegNet [40] and Swin Trans-<br>former [34] by a large margin. MixFormer markedly out-<br>performs its alternatives in 5 dense prediction tasks with<br>lower computational costs. With Mask R-CNN [18](1x)<br>on MS COCO [33], MixFormer-B4 shows a boost of 2.9<br>box mAP and 2.1 mask mAP on Swin-T [34] while requir-<br>ing less computational cost. Substituting the backbone in<br>UperNet [54], MixFormer-B4 delivers a 2.2 mIoU gain over<br>Swin-T [34] on ADE20k [66]. Plus, MixFormer is effective<br>in keypoint detection [33] and long-tail instance segmenta-<br>tion [15]. In brief, our MixFormer achieves state-of-the-art<br>performance on multiple vision tasks as an efficient general-<br>purpose vision transformer.</p>",
      "id": 16,
      "page": 2,
      "text": "We present MixFormer to verify the block's efficiency\nand effectiveness. A series of MixFormers with com-\nputational complexity ranging from 0.7G (B1) to 3.6G\n(B4) are built to perform distinguished in multiple vi-\nsion tasks, including image classification, object detection,\ninstance segmentation, semantic segmentation, etc. On\nImageNet-1K [9], we achieve competitive results with Ef-\nficientNet [43], surpassing RegNet [40] and Swin Trans-\nformer [34] by a large margin. MixFormer markedly out-\nperforms its alternatives in 5 dense prediction tasks with\nlower computational costs. With Mask R-CNN [18](1x)\non MS COCO [33], MixFormer-B4 shows a boost of 2.9\nbox mAP and 2.1 mask mAP on Swin-T [34] while requir-\ning less computational cost. Substituting the backbone in\nUperNet [54], MixFormer-B4 delivers a 2.2 mIoU gain over\nSwin-T [34] on ADE20k [66]. Plus, MixFormer is effective\nin keypoint detection [33] and long-tail instance segmenta-\ntion [15]. In brief, our MixFormer achieves state-of-the-art\nperformance on multiple vision tasks as an efficient general-\npurpose vision transformer."
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 2047
        },
        {
          "x": 579,
          "y": 2047
        },
        {
          "x": 579,
          "y": 2098
        },
        {
          "x": 203,
          "y": 2098
        }
      ],
      "category": "paragraph",
      "html": "<p id='17' style='font-size:22px'>2. Related Works</p>",
      "id": 17,
      "page": 2,
      "text": "2. Related Works"
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 2131
        },
        {
          "x": 1199,
          "y": 2131
        },
        {
          "x": 1199,
          "y": 2781
        },
        {
          "x": 201,
          "y": 2781
        }
      ],
      "category": "paragraph",
      "html": "<p id='18' style='font-size:16px'>Vision Transformers. The success of the pioneering work,<br>ViT [11, 44], shows great potentials to apply transformer to<br>the computer vision community. After that, various meth-<br>ods [2, 16, 29, 44, 59, 67] are proposed to improve the per-<br>formance of vision transformers, demonstrating competi-<br>tive results on the image classification task. As the self-<br>attention [46] is different from the convolution in nature:<br>self-attention models long-range dependencies while con-<br>volution captures relations in local windows, there are also<br>works aiming for integrating convolution and vision trans-<br>former. Works like PVT [49] and CvT [53] insert spatial<br>reduction or convolution before global self-attention, yield-<br>ing the merits of self-attention and convolution.</p>",
      "id": 18,
      "page": 2,
      "text": "Vision Transformers. The success of the pioneering work,\nViT [11, 44], shows great potentials to apply transformer to\nthe computer vision community. After that, various meth-\nods [2, 16, 29, 44, 59, 67] are proposed to improve the per-\nformance of vision transformers, demonstrating competi-\ntive results on the image classification task. As the self-\nattention [46] is different from the convolution in nature:\nself-attention models long-range dependencies while con-\nvolution captures relations in local windows, there are also\nworks aiming for integrating convolution and vision trans-\nformer. Works like PVT [49] and CvT [53] insert spatial\nreduction or convolution before global self-attention, yield-\ning the merits of self-attention and convolution."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 2811
        },
        {
          "x": 1199,
          "y": 2811
        },
        {
          "x": 1199,
          "y": 2973
        },
        {
          "x": 201,
          "y": 2973
        }
      ],
      "category": "paragraph",
      "html": "<p id='19' style='font-size:14px'>1Local-window self-attention shares weights on the channel dimension<br>while depth-wise convolution shares weights on the spatial one [17]. From<br>the weight sharing perspective, sharing weights results in limited modeling<br>ability in the correspond dimension.</p>",
      "id": 19,
      "page": 2,
      "text": "1Local-window self-attention shares weights on the channel dimension\nwhile depth-wise convolution shares weights on the spatial one [17]. From\nthe weight sharing perspective, sharing weights results in limited modeling\nability in the correspond dimension."
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 305
        },
        {
          "x": 2277,
          "y": 305
        },
        {
          "x": 2277,
          "y": 903
        },
        {
          "x": 1278,
          "y": 903
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='20' style='font-size:18px'>Window-based Vision Transformers. Although global<br>Vision Transformer shows its success on image classifica-<br>tion; challenges remain for downstream tasks. For high-<br>resolution vision tasks, the computation cost of the Vision<br>Transformer is quadratic to image size, making it unafford-<br>able for real-world applications. Recently, researchers have<br>proposed plenty of methods [6, 10, 34, 49, 53, 56] to make<br>vision transformers become general-purpose backbones as<br>ConvNets [19, 22, 55]. Among them, Window-based Vision<br>Transformer [6, 26, 34] adopts the local window attention<br>mechanism, making its computational complexity increase<br>linearly to image size.</p>",
      "id": 20,
      "page": 2,
      "text": "Window-based Vision Transformers. Although global\nVision Transformer shows its success on image classifica-\ntion; challenges remain for downstream tasks. For high-\nresolution vision tasks, the computation cost of the Vision\nTransformer is quadratic to image size, making it unafford-\nable for real-world applications. Recently, researchers have\nproposed plenty of methods [6, 10, 34, 49, 53, 56] to make\nvision transformers become general-purpose backbones as\nConvNets [19, 22, 55]. Among them, Window-based Vision\nTransformer [6, 26, 34] adopts the local window attention\nmechanism, making its computational complexity increase\nlinearly to image size."
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 933
        },
        {
          "x": 2277,
          "y": 933
        },
        {
          "x": 2277,
          "y": 1534
        },
        {
          "x": 1278,
          "y": 1534
        }
      ],
      "category": "paragraph",
      "html": "<p id='21' style='font-size:16px'>Receptive Fields. Receptive fields are important for the<br>downstream vision tasks. However, Window-based Vi-<br>sion Transformer computes self-attention within non-<br>overlapping local windows, which limits the receptive fields<br>in local windows. To solve the problem, researchers pro-<br>pose to use shifting [34], expanding [45, 57], or shuf-<br>fling [26] operations to connect nearby windows. There are<br>also works [26,61] using convolutions to enlarge the recep-<br>tive fields efficiently. Convolution layers are used to cre-<br>ate connections because they capture local relations in na-<br>ture. We combine local-window self-attention and depth-<br>wise convolution in our block design.</p>",
      "id": 21,
      "page": 2,
      "text": "Receptive Fields. Receptive fields are important for the\ndownstream vision tasks. However, Window-based Vi-\nsion Transformer computes self-attention within non-\noverlapping local windows, which limits the receptive fields\nin local windows. To solve the problem, researchers pro-\npose to use shifting [34], expanding [45, 57], or shuf-\nfling [26] operations to connect nearby windows. There are\nalso works [26,61] using convolutions to enlarge the recep-\ntive fields efficiently. Convolution layers are used to cre-\nate connections because they capture local relations in na-\nture. We combine local-window self-attention and depth-\nwise convolution in our block design."
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 1565
        },
        {
          "x": 2278,
          "y": 1565
        },
        {
          "x": 2278,
          "y": 2516
        },
        {
          "x": 1278,
          "y": 2516
        }
      ],
      "category": "paragraph",
      "html": "<p id='22' style='font-size:16px'>Dynamic Mechanism. Dynamic networks here [8, 24, 28,<br>32, 46,52] refer to networks whose parts of weights or paths<br>are data-dependent. Generally speaking, the dynamic net-<br>work achieves higher performance than its static alternative<br>as itis more flexible in modeling relations. In ConvNets, the<br>dynamic mechanism is widely used to better extract cus-<br>tomized features given different inputs. There are various<br>types of dynamic networks that focus on the channel [24,32]<br>and the spatial dimension [8, 28, 52]. These works promote<br>many tasks to new state-of-the-art. For Transformer [46],<br>the self-attention module is a dynamic component, which<br>generates attention maps based on the inputs. In this pa-<br>per, we also adopt the dynamic mechanism in the network<br>design, while our application is based on the finding that<br>the two efficient components share their weights on differ-<br>ent dimensions [17]. To construct a powerful block while<br>maintaining efficiency, we introduce dynamic interactions<br>across two branches, which are light-weighted and improve<br>the modeling ability in both channel and spatial dimensions.</p>",
      "id": 22,
      "page": 2,
      "text": "Dynamic Mechanism. Dynamic networks here [8, 24, 28,\n32, 46,52] refer to networks whose parts of weights or paths\nare data-dependent. Generally speaking, the dynamic net-\nwork achieves higher performance than its static alternative\nas itis more flexible in modeling relations. In ConvNets, the\ndynamic mechanism is widely used to better extract cus-\ntomized features given different inputs. There are various\ntypes of dynamic networks that focus on the channel [24,32]\nand the spatial dimension [8, 28, 52]. These works promote\nmany tasks to new state-of-the-art. For Transformer [46],\nthe self-attention module is a dynamic component, which\ngenerates attention maps based on the inputs. In this pa-\nper, we also adopt the dynamic mechanism in the network\ndesign, while our application is based on the finding that\nthe two efficient components share their weights on differ-\nent dimensions [17]. To construct a powerful block while\nmaintaining efficiency, we introduce dynamic interactions\nacross two branches, which are light-weighted and improve\nthe modeling ability in both channel and spatial dimensions."
    },
    {
      "bounding_box": [
        {
          "x": 1282,
          "y": 2564
        },
        {
          "x": 1505,
          "y": 2564
        },
        {
          "x": 1505,
          "y": 2613
        },
        {
          "x": 1282,
          "y": 2613
        }
      ],
      "category": "paragraph",
      "html": "<p id='23' style='font-size:20px'>3. Method</p>",
      "id": 23,
      "page": 2,
      "text": "3. Method"
    },
    {
      "bounding_box": [
        {
          "x": 1282,
          "y": 2647
        },
        {
          "x": 1722,
          "y": 2647
        },
        {
          "x": 1722,
          "y": 2697
        },
        {
          "x": 1282,
          "y": 2697
        }
      ],
      "category": "paragraph",
      "html": "<p id='24' style='font-size:18px'>3.1. The Mixing Block</p>",
      "id": 24,
      "page": 2,
      "text": "3.1. The Mixing Block"
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 2727
        },
        {
          "x": 2278,
          "y": 2727
        },
        {
          "x": 2278,
          "y": 2977
        },
        {
          "x": 1281,
          "y": 2977
        }
      ],
      "category": "paragraph",
      "html": "<p id='25' style='font-size:16px'>Our Mixing Block (Figure 1) adds two key designs upon<br>the standard window-based attention block: (1) adopt a<br>parallel design to combine local-window self-attention and<br>depth-wise convolution, (2) introduce bi-directional inter-<br>actions across branches. They are proposed to address the</p>",
      "id": 25,
      "page": 2,
      "text": "Our Mixing Block (Figure 1) adds two key designs upon\nthe standard window-based attention block: (1) adopt a\nparallel design to combine local-window self-attention and\ndepth-wise convolution, (2) introduce bi-directional inter-\nactions across branches. They are proposed to address the"
    },
    {
      "bounding_box": [
        {
          "x": 1225,
          "y": 3053
        },
        {
          "x": 1251,
          "y": 3053
        },
        {
          "x": 1251,
          "y": 3091
        },
        {
          "x": 1225,
          "y": 3091
        }
      ],
      "category": "footer",
      "html": "<footer id='26' style='font-size:14px'>2</footer>",
      "id": 26,
      "page": 2,
      "text": "2"
    },
    {
      "bounding_box": [
        {
          "x": 527,
          "y": 294
        },
        {
          "x": 1953,
          "y": 294
        },
        {
          "x": 1953,
          "y": 456
        },
        {
          "x": 527,
          "y": 456
        }
      ],
      "category": "table",
      "html": "<table id='27' style='font-size:16px'><tr><td></td><td>Attention</td><td>W-Attention</td><td>Conv</td><td>DwConv</td></tr><tr><td>Sharing Weights</td><td>Channel Dim</td><td>Channel Dim</td><td>Spatial Dim</td><td>Spatial Dim</td></tr><tr><td>FLOPs</td><td>2NCH2W2</td><td>2NCHW K2</td><td>NC2 HW K2</td><td>NCHW K2</td></tr></table>",
      "id": 27,
      "page": 3,
      "text": "Attention W-Attention Conv DwConv\n Sharing Weights Channel Dim Channel Dim Spatial Dim Spatial Dim\n FLOPs 2NCH2W2 2NCHW K2 NC2 HW K2"
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 489
        },
        {
          "x": 2280,
          "y": 489
        },
        {
          "x": 2280,
          "y": 722
        },
        {
          "x": 200,
          "y": 722
        }
      ],
      "category": "caption",
      "html": "<caption id='28' style='font-size:16px'>Table 1. Sharing Weights Dimensions and FLOPs. We provide comparison among four operations: global self-attention(Attention), local<br>window self-attention(W-Attention), convolution(Conv) and depth-wise convolution(DwConv). In the table, we provide the dimension of<br>weight sharing for all components in the first row. Besides, the FLOPs is calculated with a N x C x H x W input and a output with the<br>same shape. The K in the table represents the window size in local-window self-attention or convolution. Note that the Attention operator<br>adopts a window size of H x W as it models global dependencies in the spatial dimension.</caption>",
      "id": 28,
      "page": 3,
      "text": "Table 1. Sharing Weights Dimensions and FLOPs. We provide comparison among four operations: global self-attention(Attention), local\nwindow self-attention(W-Attention), convolution(Conv) and depth-wise convolution(DwConv). In the table, we provide the dimension of\nweight sharing for all components in the first row. Besides, the FLOPs is calculated with a N x C x H x W input and a output with the\nsame shape. The K in the table represents the window size in local-window self-attention or convolution. Note that the Attention operator\nadopts a window size of H x W as it models global dependencies in the spatial dimension."
    },
    {
      "bounding_box": [
        {
          "x": 616,
          "y": 740
        },
        {
          "x": 1863,
          "y": 740
        },
        {
          "x": 1863,
          "y": 1153
        },
        {
          "x": 616,
          "y": 1153
        }
      ],
      "category": "figure",
      "html": "<figure><img id='29' style='font-size:14px' alt=\"Q K v\nsigmoid\n* convlxl GELU BN convlxl GAP DwConv 3x3\nChannel Interaction projection\nW-Attention 7x7\n▼\nconvixi\nconvixi\nsigmoid\nBN GELU *\nSpatial Interaction\" data-coord=\"top-left:(616,740); bottom-right:(1863,1153)\" /></figure>",
      "id": 29,
      "page": 3,
      "text": "Q K v\nsigmoid\n* convlxl GELU BN convlxl GAP DwConv 3x3\nChannel Interaction projection\nW-Attention 7x7\n▼\nconvixi\nconvixi\nsigmoid\nBN GELU *\nSpatial Interaction"
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 1209
        },
        {
          "x": 2275,
          "y": 1209
        },
        {
          "x": 2275,
          "y": 1302
        },
        {
          "x": 201,
          "y": 1302
        }
      ],
      "category": "caption",
      "html": "<caption id='30' style='font-size:18px'>Figure 2. Detailed design of the Bi-directional Interactions. The channel/spatial interaction provides channel/spatial context extracted<br>by depth-wise convolution/local-window self-attention to the other path.</caption>",
      "id": 30,
      "page": 3,
      "text": "Figure 2. Detailed design of the Bi-directional Interactions. The channel/spatial interaction provides channel/spatial context extracted\nby depth-wise convolution/local-window self-attention to the other path."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 1356
        },
        {
          "x": 1199,
          "y": 1356
        },
        {
          "x": 1199,
          "y": 1552
        },
        {
          "x": 202,
          "y": 1552
        }
      ],
      "category": "paragraph",
      "html": "<p id='31' style='font-size:20px'>limited receptive fields and weak modeling ability issues in<br>local-window self-attention. We first present these two de-<br>signs then integrate them to build the Mixing Block. Details<br>are described next.</p>",
      "id": 31,
      "page": 3,
      "text": "limited receptive fields and weak modeling ability issues in\nlocal-window self-attention. We first present these two de-\nsigns then integrate them to build the Mixing Block. Details\nare described next."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 1581
        },
        {
          "x": 1198,
          "y": 1581
        },
        {
          "x": 1198,
          "y": 2028
        },
        {
          "x": 202,
          "y": 2028
        }
      ],
      "category": "paragraph",
      "html": "<p id='32' style='font-size:20px'>The Parallel Design. Although performing self-attention<br>inside non-overlapped windows brings computational effi-<br>ciency2, itresults in a limited receptive field due to no cross-<br>window connections being extracted. Several methods re-<br>sort to shift [34], expand [45, 57], shuffle [26], or convolu-<br>tion [26, 61] to model connections across windows. Con-<br>sidering that convolution layers are designed to model lo-<br>cal relations, we choose the efficient alternative (depth-wise<br>convolution) as a promising way to connect windows.</p>",
      "id": 32,
      "page": 3,
      "text": "The Parallel Design. Although performing self-attention\ninside non-overlapped windows brings computational effi-\nciency2, itresults in a limited receptive field due to no cross-\nwindow connections being extracted. Several methods re-\nsort to shift [34], expand [45, 57], shuffle [26], or convolu-\ntion [26, 61] to model connections across windows. Con-\nsidering that convolution layers are designed to model lo-\ncal relations, we choose the efficient alternative (depth-wise\nconvolution) as a promising way to connect windows."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 2032
        },
        {
          "x": 1198,
          "y": 2032
        },
        {
          "x": 1198,
          "y": 2325
        },
        {
          "x": 201,
          "y": 2325
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='33' style='font-size:20px'>Attention then moves to adopt a proper way to com-<br>bine local-window self-attention and depth-wise convolu-<br>tion. Previous methods [26, 34, 45, 57, 61] fill the goal by<br>stacking these two operators successively. However, captur-<br>ing intra-window and cross-window relations in successive<br>steps make these two types of relations less interweaved.</p>",
      "id": 33,
      "page": 3,
      "text": "Attention then moves to adopt a proper way to com-\nbine local-window self-attention and depth-wise convolu-\ntion. Previous methods [26, 34, 45, 57, 61] fill the goal by\nstacking these two operators successively. However, captur-\ning intra-window and cross-window relations in successive\nsteps make these two types of relations less interweaved."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 2332
        },
        {
          "x": 1199,
          "y": 2332
        },
        {
          "x": 1199,
          "y": 2780
        },
        {
          "x": 201,
          "y": 2780
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='34' style='font-size:20px'>In this paper, we propose a parallel design that enlarges<br>the receptive fields by simultaneously modeling intra-<br>window and cross-window relations. As illustrated in Fig-<br>ure 1, local-window self-attention and depth-wise convolu-<br>tion lie in two parallel paths. In detail, they use different<br>window sizes. A 7 x 7 window is adopted in local-window<br>self-attention, following previous works [23, 34, 45, 64].<br>While in depth-wise convolution, a smaller kernel size 3 x 3<br>is applied considering the efficiency3. Moreover, as their</p>",
      "id": 34,
      "page": 3,
      "text": "In this paper, we propose a parallel design that enlarges\nthe receptive fields by simultaneously modeling intra-\nwindow and cross-window relations. As illustrated in Fig-\nure 1, local-window self-attention and depth-wise convolu-\ntion lie in two parallel paths. In detail, they use different\nwindow sizes. A 7 x 7 window is adopted in local-window\nself-attention, following previous works [23, 34, 45, 64].\nWhile in depth-wise convolution, a smaller kernel size 3 x 3\nis applied considering the efficiency3. Moreover, as their"
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 2811
        },
        {
          "x": 1198,
          "y": 2811
        },
        {
          "x": 1198,
          "y": 2891
        },
        {
          "x": 200,
          "y": 2891
        }
      ],
      "category": "paragraph",
      "html": "<p id='35' style='font-size:16px'>2It has linear computational complexity concerning image size, as<br>shown in Table 1.</p>",
      "id": 35,
      "page": 3,
      "text": "2It has linear computational complexity concerning image size, as\nshown in Table 1."
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 2896
        },
        {
          "x": 1197,
          "y": 2896
        },
        {
          "x": 1197,
          "y": 2970
        },
        {
          "x": 203,
          "y": 2970
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='36' style='font-size:14px'>3The results in Table 8 show that 3 x 3 is a good choice to achieve<br>balance in accuracy and efficiency.</p>",
      "id": 36,
      "page": 3,
      "text": "3The results in Table 8 show that 3 x 3 is a good choice to achieve\nbalance in accuracy and efficiency."
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 1356
        },
        {
          "x": 2277,
          "y": 1356
        },
        {
          "x": 2277,
          "y": 1703
        },
        {
          "x": 1279,
          "y": 1703
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='37' style='font-size:20px'>FLOPs are different, we adjust the number of channels ac-<br>cording to the FLOPs proportion in Table 1. Then, their out-<br>puts are normalized by different normalization layers [1,27]<br>and merged by concatenation. The merged feature is sent<br>to the successive Feed-Forward Network (FFN) to mix the<br>learned relations across channels, generating the final out-<br>put feature.</p>",
      "id": 37,
      "page": 3,
      "text": "FLOPs are different, we adjust the number of channels ac-\ncording to the FLOPs proportion in Table 1. Then, their out-\nputs are normalized by different normalization layers [1,27]\nand merged by concatenation. The merged feature is sent\nto the successive Feed-Forward Network (FFN) to mix the\nlearned relations across channels, generating the final out-\nput feature."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 1708
        },
        {
          "x": 2277,
          "y": 1708
        },
        {
          "x": 2277,
          "y": 2102
        },
        {
          "x": 1280,
          "y": 2102
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='38' style='font-size:22px'>The parallel design benefits two-folds: First, combin-<br>ing local-window self-attention with depth-wise convolu-<br>tion across branches models connections across windows,<br>addressing the limited receptive fields issue. Second, paral-<br>lel design models intra-window and cross-window relations<br>simultaneously, providing opportunities for feature inter-<br>weaving across branches and achieving better feature rep-<br>resentation learning.</p>",
      "id": 38,
      "page": 3,
      "text": "The parallel design benefits two-folds: First, combin-\ning local-window self-attention with depth-wise convolu-\ntion across branches models connections across windows,\naddressing the limited receptive fields issue. Second, paral-\nlel design models intra-window and cross-window relations\nsimultaneously, providing opportunities for feature inter-\nweaving across branches and achieving better feature rep-\nresentation learning."
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 2128
        },
        {
          "x": 2277,
          "y": 2128
        },
        {
          "x": 2277,
          "y": 2523
        },
        {
          "x": 1279,
          "y": 2523
        }
      ],
      "category": "paragraph",
      "html": "<p id='39' style='font-size:20px'>Bi-directional Interactions. In general, sharing weights<br>limits the modeling ability in the shared dimension. A com-<br>mon way to solve the dilemma is to generate data-dependent<br>weights as done in dynamic networks [4, 24, 30, 52]. Local-<br>window self-attention computes weights on the fly on the<br>spatial dimension while sharing weights across channels,<br>resulting in the weak modeling ability issue on the channel<br>dimension. We focus on this issue in this subsection.</p>",
      "id": 39,
      "page": 3,
      "text": "Bi-directional Interactions. In general, sharing weights\nlimits the modeling ability in the shared dimension. A com-\nmon way to solve the dilemma is to generate data-dependent\nweights as done in dynamic networks [4, 24, 30, 52]. Local-\nwindow self-attention computes weights on the fly on the\nspatial dimension while sharing weights across channels,\nresulting in the weak modeling ability issue on the channel\ndimension. We focus on this issue in this subsection."
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 2530
        },
        {
          "x": 2277,
          "y": 2530
        },
        {
          "x": 2277,
          "y": 2977
        },
        {
          "x": 1279,
          "y": 2977
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='40' style='font-size:20px'>To enhance the modeling capacity of local-window self-<br>attention on the channel dimension, we try to generate<br>channel-wise dynamic weights [24]. Given that depth-wise<br>convolution shares weights on the spatial dimension while<br>focusing on the channel. It can provide complementary<br>clues for local-window self-attention and vice versa. Thus,<br>we propose bi-directional interactions (in Figure 1 and Fig-<br>ure 2) to enhance modeling ability in the channel and spa-<br>tial dimension for local-window self-attention and depth-</p>",
      "id": 40,
      "page": 3,
      "text": "To enhance the modeling capacity of local-window self-\nattention on the channel dimension, we try to generate\nchannel-wise dynamic weights [24]. Given that depth-wise\nconvolution shares weights on the spatial dimension while\nfocusing on the channel. It can provide complementary\nclues for local-window self-attention and vice versa. Thus,\nwe propose bi-directional interactions (in Figure 1 and Fig-\nure 2) to enhance modeling ability in the channel and spa-\ntial dimension for local-window self-attention and depth-"
    },
    {
      "bounding_box": [
        {
          "x": 1224,
          "y": 3054
        },
        {
          "x": 1252,
          "y": 3054
        },
        {
          "x": 1252,
          "y": 3093
        },
        {
          "x": 1224,
          "y": 3093
        }
      ],
      "category": "footer",
      "html": "<footer id='41' style='font-size:18px'>3</footer>",
      "id": 41,
      "page": 3,
      "text": "3"
    },
    {
      "bounding_box": [
        {
          "x": 210,
          "y": 311
        },
        {
          "x": 2254,
          "y": 311
        },
        {
          "x": 2254,
          "y": 709
        },
        {
          "x": 210,
          "y": 709
        }
      ],
      "category": "figure",
      "html": "<figure><img id='42' style='font-size:14px' alt=\"Stage 1 Stage 2 Stage 3 Stage 4\nHead\nStem\nLayer\nImages\nClassification\nConvolution\nMixing Mixing Mixing Mixing\nBlock Conv-2x2\nBlock Conv-2x2\nBlock class\nBlock Conv-2x2\nxN1 Stride\nxN2 Stride\nXN4 Projection\nxN3 Stride\nHxWx3 HxWxC BxWx2C W H W H W H W\n16x 16 x4C x 32 x8C x ㉜ x1280\n32\n㉜\" data-coord=\"top-left:(210,311); bottom-right:(2254,709)\" /></figure>",
      "id": 42,
      "page": 4,
      "text": "Stage 1 Stage 2 Stage 3 Stage 4\nHead\nStem\nLayer\nImages\nClassification\nConvolution\nMixing Mixing Mixing Mixing\nBlock Conv-2x2\nBlock Conv-2x2\nBlock class\nBlock Conv-2x2\nxN1 Stride\nxN2 Stride\nXN4 Projection\nxN3 Stride\nHxWx3 HxWxC BxWx2C W H W H W H W\n16x 16 x4C x 32 x8C x ㉜ x1280\n32\n㉜"
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 747
        },
        {
          "x": 2276,
          "y": 747
        },
        {
          "x": 2276,
          "y": 932
        },
        {
          "x": 201,
          "y": 932
        }
      ],
      "category": "caption",
      "html": "<caption id='43' style='font-size:14px'>Figure 3. Overall Architecture of MixFormer. There are four parts in MixFormer: Convolution Stem, Stages, Projection Layer, and<br>Classification Head. In Convolution Stem, we apply three successive convolutions to increase the channel from 3 to C. In Stages, we stack<br>our Mixing Block in each stage and use stride convolution (stride = 2) to downsample the feature map. For Projection Layer, we use a<br>linear layer with activation to increase the channels to 1280. The Classification Head is for the classification task.</caption>",
      "id": 43,
      "page": 4,
      "text": "Figure 3. Overall Architecture of MixFormer. There are four parts in MixFormer: Convolution Stem, Stages, Projection Layer, and\nClassification Head. In Convolution Stem, we apply three successive convolutions to increase the channel from 3 to C. In Stages, we stack\nour Mixing Block in each stage and use stride convolution (stride = 2) to downsample the feature map. For Projection Layer, we use a\nlinear layer with activation to increase the channels to 1280. The Classification Head is for the classification task."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 987
        },
        {
          "x": 1199,
          "y": 987
        },
        {
          "x": 1199,
          "y": 1532
        },
        {
          "x": 201,
          "y": 1532
        }
      ],
      "category": "paragraph",
      "html": "<p id='44' style='font-size:18px'>wise convolution respectively. The bi-directional interac-<br>tions consist of the channel and spatial interaction among<br>the parallel branches. The information in the depth-wise<br>convolution branch flows to the other branch through the<br>channel interaction, which strengthens the modeling ability<br>in the channel dimension. Meanwhile, the spatial interac-<br>tion enables spatial relations to flow from the local-window<br>self-attention branch to the other. As a result, the proposed<br>bi-directional interactions provide complementary clues for<br>each other. Next, we present the designs of the channel and<br>spatial interactions in detail.</p>",
      "id": 44,
      "page": 4,
      "text": "wise convolution respectively. The bi-directional interac-\ntions consist of the channel and spatial interaction among\nthe parallel branches. The information in the depth-wise\nconvolution branch flows to the other branch through the\nchannel interaction, which strengthens the modeling ability\nin the channel dimension. Meanwhile, the spatial interac-\ntion enables spatial relations to flow from the local-window\nself-attention branch to the other. As a result, the proposed\nbi-directional interactions provide complementary clues for\neach other. Next, we present the designs of the channel and\nspatial interactions in detail."
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 1540
        },
        {
          "x": 1200,
          "y": 1540
        },
        {
          "x": 1200,
          "y": 2233
        },
        {
          "x": 200,
          "y": 2233
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='45' style='font-size:18px'>For the channel interaction, we follow the design of the<br>SE layer [24], as shown in Figure 2. The channel interac-<br>tion contains one global average pooling layer, followed by<br>two successive 1 x 1 convolution layers with normalization<br>(BN [27]) and activation (GELU [20]) between them. At<br>last, we use sigmoid to generate attention in the channel di-<br>mension. Although our channel interaction shares the same<br>design with the SE layer [24], they differ in two aspects: (1)<br>The input of the attention module is different. The input of<br>our channel interaction comes from another parallel branch,<br>while the SE layer is performed in the same branch. (2) We<br>only apply the channel interaction to the value in the local-<br>window self-attention instead of applying it to the module's<br>output as the SE layer does.</p>",
      "id": 45,
      "page": 4,
      "text": "For the channel interaction, we follow the design of the\nSE layer [24], as shown in Figure 2. The channel interac-\ntion contains one global average pooling layer, followed by\ntwo successive 1 x 1 convolution layers with normalization\n(BN [27]) and activation (GELU [20]) between them. At\nlast, we use sigmoid to generate attention in the channel di-\nmension. Although our channel interaction shares the same\ndesign with the SE layer [24], they differ in two aspects: (1)\nThe input of the attention module is different. The input of\nour channel interaction comes from another parallel branch,\nwhile the SE layer is performed in the same branch. (2) We\nonly apply the channel interaction to the value in the local-\nwindow self-attention instead of applying it to the module's\noutput as the SE layer does."
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 2242
        },
        {
          "x": 1199,
          "y": 2242
        },
        {
          "x": 1199,
          "y": 2834
        },
        {
          "x": 200,
          "y": 2834
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='46' style='font-size:18px'>For the spatial interaction, we also adopt a simple de-<br>sign, which consists of two 1 x 1 convolution layers with<br>followed BN [27] and GELU [20]. The detailed design is<br>presented in Figure 2. These two layers reduce the num-<br>ber of channels to one. At last, a sigmoid layer is used to<br>generate the spatial attention map. Same as we did in the<br>channel interaction, the spatial attention is generated by an-<br>other branch, where the local-window self-attention module<br>is applied. It has a larger kernel size (7 x 7) than the depth-<br>wise 3 x 3 convolution and focuses on the spatial dimension,<br>which provides strong spatial clues for the depth-wise con-<br>volution branch.</p>",
      "id": 46,
      "page": 4,
      "text": "For the spatial interaction, we also adopt a simple de-\nsign, which consists of two 1 x 1 convolution layers with\nfollowed BN [27] and GELU [20]. The detailed design is\npresented in Figure 2. These two layers reduce the num-\nber of channels to one. At last, a sigmoid layer is used to\ngenerate the spatial attention map. Same as we did in the\nchannel interaction, the spatial attention is generated by an-\nother branch, where the local-window self-attention module\nis applied. It has a larger kernel size (7 x 7) than the depth-\nwise 3 x 3 convolution and focuses on the spatial dimension,\nwhich provides strong spatial clues for the depth-wise con-\nvolution branch."
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 2879
        },
        {
          "x": 1197,
          "y": 2879
        },
        {
          "x": 1197,
          "y": 2973
        },
        {
          "x": 204,
          "y": 2973
        }
      ],
      "category": "paragraph",
      "html": "<p id='47' style='font-size:18px'>The Mixing Block. Thanks to the above two designs, we<br>mitigate two core issues in local-window self-attention. We</p>",
      "id": 47,
      "page": 4,
      "text": "The Mixing Block. Thanks to the above two designs, we\nmitigate two core issues in local-window self-attention. We"
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 989
        },
        {
          "x": 2276,
          "y": 989
        },
        {
          "x": 2276,
          "y": 1282
        },
        {
          "x": 1279,
          "y": 1282
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='48' style='font-size:18px'>integrate them to build a new transformer block, Mixing<br>Block, upon the standard window attention block. As<br>shown in Figure 1, the Mixing Block consists of two ef-<br>ficient operations in a parallel design, bi-directional in-<br>teractions (Figure 2), and an FFN (Feed-Forward Net-<br>works) [46]. It can be formulated as follow:</p>",
      "id": 48,
      "page": 4,
      "text": "integrate them to build a new transformer block, Mixing\nBlock, upon the standard window attention block. As\nshown in Figure 1, the Mixing Block consists of two ef-\nficient operations in a parallel design, bi-directional in-\nteractions (Figure 2), and an FFN (Feed-Forward Net-\nworks) [46]. It can be formulated as follow:"
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 1476
        },
        {
          "x": 2274,
          "y": 1476
        },
        {
          "x": 2274,
          "y": 2125
        },
        {
          "x": 1278,
          "y": 2125
        }
      ],
      "category": "paragraph",
      "html": "<p id='49' style='font-size:18px'>Where MIX represents a function that achieves feature mix-<br>ing between the W-MSA (Window-based Multi-Head Self-<br>Attention) branch and the CONV (Depth-wise Convolution)<br>branch. The MIX function first projects the input feature to<br>parallel branches by two linear projection layers and two<br>norm layers. Then it mixes the features by following the<br>steps shown in Figure 1 and Figure 2. For FFN, we keep it<br>simple and follow previous works [34,44], which is an MLP<br>that consists of two linear layers with one GELU [20] be-<br>tween them. Moreover, we also try to add depth-wise con-<br>volution as done in PVTv2 [48] and HRFormer [61], which<br>does not give many improvements over the MLP design (Ta-<br>ble 9). Thus, to keep the block simple, we use MLP in FFN.</p>",
      "id": 49,
      "page": 4,
      "text": "Where MIX represents a function that achieves feature mix-\ning between the W-MSA (Window-based Multi-Head Self-\nAttention) branch and the CONV (Depth-wise Convolution)\nbranch. The MIX function first projects the input feature to\nparallel branches by two linear projection layers and two\nnorm layers. Then it mixes the features by following the\nsteps shown in Figure 1 and Figure 2. For FFN, we keep it\nsimple and follow previous works [34,44], which is an MLP\nthat consists of two linear layers with one GELU [20] be-\ntween them. Moreover, we also try to add depth-wise con-\nvolution as done in PVTv2 [48] and HRFormer [61], which\ndoes not give many improvements over the MLP design (Ta-\nble 9). Thus, to keep the block simple, we use MLP in FFN."
    },
    {
      "bounding_box": [
        {
          "x": 1282,
          "y": 2153
        },
        {
          "x": 1597,
          "y": 2153
        },
        {
          "x": 1597,
          "y": 2199
        },
        {
          "x": 1282,
          "y": 2199
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='50' style='font-size:22px'>3.2. MixFormer</p>",
      "id": 50,
      "page": 4,
      "text": "3.2. MixFormer"
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 2229
        },
        {
          "x": 2276,
          "y": 2229
        },
        {
          "x": 2276,
          "y": 2976
        },
        {
          "x": 1278,
          "y": 2976
        }
      ],
      "category": "paragraph",
      "html": "<p id='51' style='font-size:18px'>Overall Architecture. Based on the obtained block, we<br>design an efficient and general-purpose vision transformer,<br>MixFormer, with pyramid feature maps. There are four<br>stages with downsampling rates of {4, 8, 16, 32} respec-<br>tively. MixFormer is a hybrid vision transformer, which<br>uses convolution layers in both stem layers and downsam-<br>pling layers. Besides, we introduce a projection layer in the<br>tail of the stages. The projection layer increases the fea-<br>ture's channels to 1280 with a linear layer followed by an<br>activation layer, aiming to preserve more details in the chan-<br>nel before the classification head. It gives a higher perfor-<br>mance in classification, especially works well with smaller<br>models. Same design can be found in previous efficient net-<br>works, such as MobileNets [22, 41] and EfficeintNets [43].<br>The sketch of our MixFormer is given in Figure 3.</p>",
      "id": 51,
      "page": 4,
      "text": "Overall Architecture. Based on the obtained block, we\ndesign an efficient and general-purpose vision transformer,\nMixFormer, with pyramid feature maps. There are four\nstages with downsampling rates of {4, 8, 16, 32} respec-\ntively. MixFormer is a hybrid vision transformer, which\nuses convolution layers in both stem layers and downsam-\npling layers. Besides, we introduce a projection layer in the\ntail of the stages. The projection layer increases the fea-\nture's channels to 1280 with a linear layer followed by an\nactivation layer, aiming to preserve more details in the chan-\nnel before the classification head. It gives a higher perfor-\nmance in classification, especially works well with smaller\nmodels. Same design can be found in previous efficient net-\nworks, such as MobileNets [22, 41] and EfficeintNets [43].\nThe sketch of our MixFormer is given in Figure 3."
    },
    {
      "bounding_box": [
        {
          "x": 1226,
          "y": 3057
        },
        {
          "x": 1249,
          "y": 3057
        },
        {
          "x": 1249,
          "y": 3087
        },
        {
          "x": 1226,
          "y": 3087
        }
      ],
      "category": "footer",
      "html": "<footer id='52' style='font-size:16px'>4</footer>",
      "id": 52,
      "page": 4,
      "text": "4"
    },
    {
      "bounding_box": [
        {
          "x": 259,
          "y": 295
        },
        {
          "x": 1146,
          "y": 295
        },
        {
          "x": 1146,
          "y": 529
        },
        {
          "x": 259,
          "y": 529
        }
      ],
      "category": "table",
      "html": "<table id='53' style='font-size:14px'><tr><td>Models</td><td>#Channels</td><td>#Blocks</td><td>#Heads</td></tr><tr><td>MixFormer-B1</td><td>C = 32</td><td>[1,2, 6,6]</td><td>[2,4, 8,16]</td></tr><tr><td>MixFormer-B2</td><td>C = 32</td><td>[2,2,8,8]</td><td>[2,4,8,16]</td></tr><tr><td>MixFormer-B3</td><td>C = 48</td><td>[2,2,8,6]</td><td>[3,6,12,24]</td></tr><tr><td>MixFormer-B4</td><td>C = 64</td><td>[2,2,8,8]</td><td>[4,8,16,32]</td></tr></table>",
      "id": 53,
      "page": 5,
      "text": "Models #Channels #Blocks #Heads\n MixFormer-B1 C = 32 [1,2, 6,6] [2,4, 8,16]\n MixFormer-B2 C = 32 [2,2,8,8] [2,4,8,16]\n MixFormer-B3 C = 48 [2,2,8,6] [3,6,12,24]\n MixFormer-B4 C = 64 [2,2,8,8]"
    },
    {
      "bounding_box": [
        {
          "x": 206,
          "y": 568
        },
        {
          "x": 1192,
          "y": 568
        },
        {
          "x": 1192,
          "y": 654
        },
        {
          "x": 206,
          "y": 654
        }
      ],
      "category": "caption",
      "html": "<caption id='54' style='font-size:16px'>Table 2. Architecture Variants. Detailed configurations of archi-<br>tecture variants of MixFormer.</caption>",
      "id": 54,
      "page": 5,
      "text": "Table 2. Architecture Variants. Detailed configurations of archi-\ntecture variants of MixFormer."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 705
        },
        {
          "x": 1198,
          "y": 705
        },
        {
          "x": 1198,
          "y": 1052
        },
        {
          "x": 202,
          "y": 1052
        }
      ],
      "category": "paragraph",
      "html": "<p id='55' style='font-size:18px'>Architecture Variants. We stack the blocks in each stage<br>manually and format several models in different sizes,<br>whose computational complexities ranges from 0.7G (B1)<br>to 3.6G (B4). The number of blocks in different stages is<br>set by following a recipe: putting more blocks in the last<br>two stages, which is roughly verified in Table 10. As shown<br>in Table 2, we present the detailed settings of the models.</p>",
      "id": 55,
      "page": 5,
      "text": "Architecture Variants. We stack the blocks in each stage\nmanually and format several models in different sizes,\nwhose computational complexities ranges from 0.7G (B1)\nto 3.6G (B4). The number of blocks in different stages is\nset by following a recipe: putting more blocks in the last\ntwo stages, which is roughly verified in Table 10. As shown\nin Table 2, we present the detailed settings of the models."
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 1103
        },
        {
          "x": 534,
          "y": 1103
        },
        {
          "x": 534,
          "y": 1155
        },
        {
          "x": 204,
          "y": 1155
        }
      ],
      "category": "paragraph",
      "html": "<p id='56' style='font-size:22px'>4. Experiments</p>",
      "id": 56,
      "page": 5,
      "text": "4. Experiments"
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 1190
        },
        {
          "x": 1199,
          "y": 1190
        },
        {
          "x": 1199,
          "y": 1537
        },
        {
          "x": 202,
          "y": 1537
        }
      ],
      "category": "paragraph",
      "html": "<p id='57' style='font-size:16px'>We validate MixFormer on ImageNet-1K [9], MS<br>COCO [33], and ADE20k [66]. We first present the accu-<br>racy on image classification. Then we do transfer learning<br>to evaluate the models on three main tasks: object detection,<br>instance segmentation, and semantic segmentation. Be-<br>sides, ablations of different design modules in MixFormer<br>and results with more vision tasks are provided.</p>",
      "id": 57,
      "page": 5,
      "text": "We validate MixFormer on ImageNet-1K [9], MS\nCOCO [33], and ADE20k [66]. We first present the accu-\nracy on image classification. Then we do transfer learning\nto evaluate the models on three main tasks: object detection,\ninstance segmentation, and semantic segmentation. Be-\nsides, ablations of different design modules in MixFormer\nand results with more vision tasks are provided."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 1574
        },
        {
          "x": 686,
          "y": 1574
        },
        {
          "x": 686,
          "y": 1622
        },
        {
          "x": 202,
          "y": 1622
        }
      ],
      "category": "paragraph",
      "html": "<p id='58' style='font-size:20px'>4.1. Image Classification</p>",
      "id": 58,
      "page": 5,
      "text": "4.1. Image Classification"
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 1652
        },
        {
          "x": 1200,
          "y": 1652
        },
        {
          "x": 1200,
          "y": 2100
        },
        {
          "x": 202,
          "y": 2100
        }
      ],
      "category": "paragraph",
      "html": "<p id='59' style='font-size:18px'>Setup. We first verify our method by classification on<br>ImageNet-1K [9]. To make a fair comparison with previ-<br>ous works [34, 44, 49], we train all models for 300 epochs<br>with an image size of 224 x 224 and report Top-1 validation<br>accuracy. We apply an AdamW optimizer using a cosine<br>decay schedule. By following the rule that smaller models<br>need less regularization, we adjust the training settings gen-<br>tly when training models in different sizes. Details are in<br>Appendix C.</p>",
      "id": 59,
      "page": 5,
      "text": "Setup. We first verify our method by classification on\nImageNet-1K [9]. To make a fair comparison with previ-\nous works [34, 44, 49], we train all models for 300 epochs\nwith an image size of 224 x 224 and report Top-1 validation\naccuracy. We apply an AdamW optimizer using a cosine\ndecay schedule. By following the rule that smaller models\nneed less regularization, we adjust the training settings gen-\ntly when training models in different sizes. Details are in\nAppendix C."
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 2128
        },
        {
          "x": 1200,
          "y": 2128
        },
        {
          "x": 1200,
          "y": 2980
        },
        {
          "x": 200,
          "y": 2980
        }
      ],
      "category": "paragraph",
      "html": "<p id='60' style='font-size:20px'>Results. Table 3 compares our MixFormer with efficient<br>ConvNets [40, 43] and various Vision Transformers [26,<br>31, 34, 44, 49, 53, 57]. MixFormer performs on par with<br>EfficientNet [43] and outperforms RegNet [40] by signif-<br>icant margins under various computational budgets (from<br>B1 to B4). We note that it is nontrivial to achieve such<br>results for vision transformer-based models, especially on<br>small models (FLOPs < 1.0G ). Previous works such as<br>DeiT [44] and PVT [49] show dramatic performance drops<br>when reducing model complexities (-7.7% from DeiT-S to<br>DeiT-T and -4. 7% from PVT-S to PVT-T). Compared with<br>Swin Transformer [34] and its variants [26, 31, 57], Mix-<br>Former shows better performance with less computational<br>costs. In detail, MixFormer-B4 achieves 83.0% Top-1 accu-<br>racy with only 3.6G FLOPs. It outperforms Swin-T [34] by<br>1.7% while saving 20% computational costs and gives com-<br>parable results with Swin-S [34] but being 2.4x efficient.</p>",
      "id": 60,
      "page": 5,
      "text": "Results. Table 3 compares our MixFormer with efficient\nConvNets [40, 43] and various Vision Transformers [26,\n31, 34, 44, 49, 53, 57]. MixFormer performs on par with\nEfficientNet [43] and outperforms RegNet [40] by signif-\nicant margins under various computational budgets (from\nB1 to B4). We note that it is nontrivial to achieve such\nresults for vision transformer-based models, especially on\nsmall models (FLOPs < 1.0G ). Previous works such as\nDeiT [44] and PVT [49] show dramatic performance drops\nwhen reducing model complexities (-7.7% from DeiT-S to\nDeiT-T and -4. 7% from PVT-S to PVT-T). Compared with\nSwin Transformer [34] and its variants [26, 31, 57], Mix-\nFormer shows better performance with less computational\ncosts. In detail, MixFormer-B4 achieves 83.0% Top-1 accu-\nracy with only 3.6G FLOPs. It outperforms Swin-T [34] by\n1.7% while saving 20% computational costs and gives com-\nparable results with Swin-S [34] but being 2.4x efficient."
    },
    {
      "bounding_box": [
        {
          "x": 1389,
          "y": 287
        },
        {
          "x": 2167,
          "y": 287
        },
        {
          "x": 2167,
          "y": 1750
        },
        {
          "x": 1389,
          "y": 1750
        }
      ],
      "category": "table",
      "html": "<br><table id='61' style='font-size:14px'><tr><td>Method</td><td>#Params</td><td>FLOPs</td><td>Top-1</td></tr><tr><td colspan=\"4\">ConvNets</td></tr><tr><td>RegNetY-0.8G [40]</td><td>6M</td><td>0.8G</td><td>76.3</td></tr><tr><td>RegNetY-1.6G [40]</td><td>11M</td><td>1.6G</td><td>78.0</td></tr><tr><td>RegNetY-4G [40]</td><td>21M</td><td>4.0G</td><td>80.0</td></tr><tr><td>RegNetY-8G [40]</td><td>39M</td><td>8.0G</td><td>81.7</td></tr><tr><td>EffNet-B1 [43]</td><td>8M</td><td>0.7G</td><td>79.1</td></tr><tr><td>EffNet-B2 [43]</td><td>9M</td><td>1.0G</td><td>80.1</td></tr><tr><td>EffNet-B3 [43]</td><td>12M</td><td>1.8G</td><td>81.6</td></tr><tr><td>EffNet-B4 [43]</td><td>19M</td><td>4.2G</td><td>82.9</td></tr><tr><td colspan=\"4\">Vision Transformers</td></tr><tr><td>DeiT-T [44]</td><td>6M</td><td>1.3G</td><td>72.2</td></tr><tr><td>DeiT-S [44]</td><td>22M</td><td>4.6G</td><td>79.9</td></tr><tr><td>DeiT-B [44]</td><td>87M</td><td>17.5G</td><td>81.8</td></tr><tr><td>PVT-T [49]</td><td>13M</td><td>1.8G</td><td>75.1</td></tr><tr><td>PVT-S [49]</td><td>25M</td><td>3.8G</td><td>79.8</td></tr><tr><td>PVT-M [49]</td><td>44M</td><td>6.7G</td><td>81.2</td></tr><tr><td>PVT-L [49]</td><td>61M</td><td>9.8G</td><td>81.7</td></tr><tr><td>CvT-13 [53]</td><td>20M</td><td>4.5G</td><td>81.6</td></tr><tr><td>CvT-21 [53]</td><td>32M</td><td>7.1G</td><td>82.5</td></tr><tr><td>TwinsP-S [6]</td><td>24M</td><td>3.8G</td><td>81.2</td></tr><tr><td>DS-Net-S [38]</td><td>23M</td><td>3.5G</td><td>82.3</td></tr><tr><td>Swin-T [34]</td><td>29M</td><td>4.5G</td><td>81.3</td></tr><tr><td>Swin-S [34]</td><td>50M</td><td>8.7G</td><td>83.0</td></tr><tr><td>Twins-S [6]</td><td>24M</td><td>2.9G</td><td>81.7</td></tr><tr><td>LG-T [31]</td><td>33M</td><td>4.8G</td><td>82.1</td></tr><tr><td>Focal-T [57]</td><td>29M</td><td>4.9G</td><td>82.2</td></tr><tr><td>Shuffle-T [26]</td><td>29M</td><td>4.6G</td><td>82.5</td></tr><tr><td>MixFormer-B1 (Ours)</td><td>8M</td><td>0.7G</td><td>78.9</td></tr><tr><td>MixFormer-B2 (Ours)</td><td>10M</td><td>0.9G</td><td>80.0</td></tr><tr><td>MixFormer-B3 (Ours)</td><td>17M</td><td>1.9G</td><td>81.7</td></tr><tr><td>MixFormer-B4 (Ours)</td><td>35M</td><td>3.6G</td><td>83.0</td></tr></table>",
      "id": 61,
      "page": 5,
      "text": "Method #Params FLOPs Top-1\n ConvNets\n RegNetY-0.8G [40] 6M 0.8G 76.3\n RegNetY-1.6G [40] 11M 1.6G 78.0\n RegNetY-4G [40] 21M 4.0G 80.0\n RegNetY-8G [40] 39M 8.0G 81.7\n EffNet-B1 [43] 8M 0.7G 79.1\n EffNet-B2 [43] 9M 1.0G 80.1\n EffNet-B3 [43] 12M 1.8G 81.6\n EffNet-B4 [43] 19M 4.2G 82.9\n Vision Transformers\n DeiT-T [44] 6M 1.3G 72.2\n DeiT-S [44] 22M 4.6G 79.9\n DeiT-B [44] 87M 17.5G 81.8\n PVT-T [49] 13M 1.8G 75.1\n PVT-S [49] 25M 3.8G 79.8\n PVT-M [49] 44M 6.7G 81.2\n PVT-L [49] 61M 9.8G 81.7\n CvT-13 [53] 20M 4.5G 81.6\n CvT-21 [53] 32M 7.1G 82.5\n TwinsP-S [6] 24M 3.8G 81.2\n DS-Net-S [38] 23M 3.5G 82.3\n Swin-T [34] 29M 4.5G 81.3\n Swin-S [34] 50M 8.7G 83.0\n Twins-S [6] 24M 2.9G 81.7\n LG-T [31] 33M 4.8G 82.1\n Focal-T [57] 29M 4.9G 82.2\n Shuffle-T [26] 29M 4.6G 82.5\n MixFormer-B1 (Ours) 8M 0.7G 78.9\n MixFormer-B2 (Ours) 10M 0.9G 80.0\n MixFormer-B3 (Ours) 17M 1.9G 81.7\n MixFormer-B4 (Ours) 35M 3.6G"
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 1784
        },
        {
          "x": 2277,
          "y": 1784
        },
        {
          "x": 2277,
          "y": 1965
        },
        {
          "x": 1281,
          "y": 1965
        }
      ],
      "category": "caption",
      "html": "<caption id='62' style='font-size:14px'>Table 3. Classification accuracy on the ImageNet validation<br>set. Performances are measured with a single 224 x 224 crop.<br>\"Params\" refers to the number of parameters. \"FLOPs\" is calcu-<br>lated under the input scale of 224 x 224.</caption>",
      "id": 62,
      "page": 5,
      "text": "Table 3. Classification accuracy on the ImageNet validation\nset. Performances are measured with a single 224 x 224 crop.\n\"Params\" refers to the number of parameters. \"FLOPs\" is calcu-\nlated under the input scale of 224 x 224."
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 2006
        },
        {
          "x": 2278,
          "y": 2006
        },
        {
          "x": 2278,
          "y": 2254
        },
        {
          "x": 1279,
          "y": 2254
        }
      ],
      "category": "paragraph",
      "html": "<p id='63' style='font-size:18px'>The competitive advantage of MixFormer maintains when<br>it comes to LG-Transformer [31], Focal Transformer [57]<br>and Shuffle Transformer [26]. Moreover, our MixFormer<br>also scales well to smaller and larger models. More results<br>are provided in Appendix A.</p>",
      "id": 63,
      "page": 5,
      "text": "The competitive advantage of MixFormer maintains when\nit comes to LG-Transformer [31], Focal Transformer [57]\nand Shuffle Transformer [26]. Moreover, our MixFormer\nalso scales well to smaller and larger models. More results\nare provided in Appendix A."
    },
    {
      "bounding_box": [
        {
          "x": 1283,
          "y": 2299
        },
        {
          "x": 2235,
          "y": 2299
        },
        {
          "x": 2235,
          "y": 2348
        },
        {
          "x": 1283,
          "y": 2348
        }
      ],
      "category": "paragraph",
      "html": "<p id='64' style='font-size:20px'>4.2. Object Detection and Instance Segmentation</p>",
      "id": 64,
      "page": 5,
      "text": "4.2. Object Detection and Instance Segmentation"
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 2377
        },
        {
          "x": 2276,
          "y": 2377
        },
        {
          "x": 2276,
          "y": 2978
        },
        {
          "x": 1279,
          "y": 2978
        }
      ],
      "category": "paragraph",
      "html": "<p id='65' style='font-size:18px'>Setup. We validate the effectiveness of MixFormer on<br>downstream tasks. We train Mask R-CNN [18] on the<br>COCO2017 train split and evaluate the models on the val<br>split. Two training schedules (1x and 3x) are adopted to<br>show a consistent comparison with previous methods [19,<br>31, 34, 44, 57]. For the 1x schedule, we train for 12<br>epochs with a single size (resizing the shorter side to 800<br>while keeping its longer side no more than 1333) [18].<br>While in 3x schedule (36 epochs), we use multi-scale train-<br>ing by randomly resizing the shorter side to the range of<br>[480, 800] (See Appendix C for more details). Expect for<br>Mask R-CNN [18], we also provide comparisons with pre-</p>",
      "id": 65,
      "page": 5,
      "text": "Setup. We validate the effectiveness of MixFormer on\ndownstream tasks. We train Mask R-CNN [18] on the\nCOCO2017 train split and evaluate the models on the val\nsplit. Two training schedules (1x and 3x) are adopted to\nshow a consistent comparison with previous methods [19,\n31, 34, 44, 57]. For the 1x schedule, we train for 12\nepochs with a single size (resizing the shorter side to 800\nwhile keeping its longer side no more than 1333) [18].\nWhile in 3x schedule (36 epochs), we use multi-scale train-\ning by randomly resizing the shorter side to the range of\n[480, 800] (See Appendix C for more details). Expect for\nMask R-CNN [18], we also provide comparisons with pre-"
    },
    {
      "bounding_box": [
        {
          "x": 1225,
          "y": 3056
        },
        {
          "x": 1252,
          "y": 3056
        },
        {
          "x": 1252,
          "y": 3091
        },
        {
          "x": 1225,
          "y": 3091
        }
      ],
      "category": "footer",
      "html": "<footer id='66' style='font-size:16px'>5</footer>",
      "id": 66,
      "page": 5,
      "text": "5"
    },
    {
      "bounding_box": [
        {
          "x": 319,
          "y": 290
        },
        {
          "x": 2165,
          "y": 290
        },
        {
          "x": 2165,
          "y": 1113
        },
        {
          "x": 319,
          "y": 1113
        }
      ],
      "category": "table",
      "html": "<table id='67' style='font-size:14px'><tr><td rowspan=\"2\">Backbones</td><td rowspan=\"2\">#Params</td><td rowspan=\"2\">FLOPs</td><td colspan=\"6\">Mask R-CNN 1x schedule</td><td colspan=\"6\">Mask R-CNN 3x + MS schedule</td></tr><tr><td>APb</td><td>AP50</td><td>AP75</td><td>APm</td><td>APm</td><td>APm</td><td>APb</td><td>AP50</td><td>AP75</td><td>APm</td><td>APm</td><td>APm</td></tr><tr><td>ResNet18 [19]</td><td>31M</td><td>-</td><td>34.0</td><td>54.0</td><td>36.7</td><td>31.2</td><td>51.0</td><td>32.7</td><td>36.9</td><td>57.1</td><td>40.0</td><td>33.6</td><td>53.9</td><td>35.7</td></tr><tr><td>ResNet50 [19]</td><td>44M</td><td>260G</td><td>38.0</td><td>58.6</td><td>41.4</td><td>34.4</td><td>55.1</td><td>36.7</td><td>41.0</td><td>61.7</td><td>44.9</td><td>37.1</td><td>58.4</td><td>40.1</td></tr><tr><td>ResNet101 [19]</td><td>63M</td><td>336G</td><td>40.4</td><td>61.1</td><td>44.2</td><td>36.4</td><td>57.7</td><td>38.8</td><td>42.8</td><td>63.2</td><td>47.1</td><td>38.5</td><td>60.1</td><td>41.3</td></tr><tr><td>ResNeXt101-64x4d [55]</td><td>101M</td><td>493G</td><td>42.8</td><td>63.8</td><td>47.3</td><td>38.4</td><td>60.6</td><td>41.3</td><td>44.4</td><td>64.9</td><td>48.8</td><td>39.7</td><td>61.9</td><td>42.6</td></tr><tr><td>PVT-T [49]</td><td>33M</td><td>-</td><td>36.7</td><td>59.2</td><td>39.3</td><td>35.1</td><td>56.7</td><td>37.3</td><td>39.8</td><td>62.2</td><td>43.0</td><td>37.4</td><td>59.3</td><td>39.9</td></tr><tr><td>PVT-S [49]</td><td>44M</td><td>245G</td><td>40.4</td><td>62.9</td><td>43.8</td><td>37.8</td><td>60.1</td><td>40.3</td><td>43.0</td><td>65.3</td><td>46.9</td><td>39.9</td><td>62.5</td><td>42.8</td></tr><tr><td>PVT-M [49]</td><td>64M</td><td>302G</td><td>42.0</td><td>64.4</td><td>45.6</td><td>39.0</td><td>61.6</td><td>42.1</td><td>44.2</td><td>66.0</td><td>48.2</td><td>40.5</td><td>63.1</td><td>43.5</td></tr><tr><td>PVT-L [49]</td><td>81M</td><td>364G</td><td>42.9</td><td>65.0</td><td>46.6</td><td>39.5</td><td>61.9</td><td>42.5</td><td>44.5</td><td>66.0</td><td>48.3</td><td>40.7</td><td>63.4</td><td>43.7</td></tr><tr><td>TwinsP-S [6]</td><td>44M</td><td>245G</td><td>42.9</td><td>65.8</td><td>47.1</td><td>40.0</td><td>62.7</td><td>42.9</td><td>46.8</td><td>69.3</td><td>51.8</td><td>42.6</td><td>66.3</td><td>46.0</td></tr><tr><td>DS-Net-S [38]</td><td>43M</td><td>-</td><td>44.3</td><td>-</td><td>-</td><td>40.2</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Swin-T [34]</td><td>48M</td><td>264G</td><td>42.2</td><td>64.6</td><td>46.2</td><td>39.1</td><td>61.6</td><td>42.0</td><td>46.0</td><td>68.2</td><td>50.2</td><td>41.6</td><td>65.1</td><td>44.8</td></tr><tr><td>Twins-S [6]</td><td>44M</td><td>228G</td><td>43.4</td><td>66.0</td><td>47.3</td><td>40.3</td><td>63.2</td><td>43.4</td><td>46.8</td><td>69.2</td><td>51.2</td><td>42.6</td><td>66.3</td><td>45.8</td></tr><tr><td>Focal-T [57]</td><td>49M</td><td>291G</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>47.2</td><td>69.4</td><td>51.9</td><td>42.7</td><td>66.5</td><td>45.9</td></tr><tr><td>Shuffle-T [26]</td><td>48M</td><td>268G</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td><td>46.8</td><td>68.9</td><td>51.5</td><td>42.3</td><td>66.0</td><td>45.6</td></tr><tr><td>MixFormer-B1(Ours)</td><td>26M</td><td>183G</td><td>40.6</td><td>62.6</td><td>44.1</td><td>37.5</td><td>59.7</td><td>40.0</td><td>43.9</td><td>65.6</td><td>48.1</td><td>40.0</td><td>62.9</td><td>42.9</td></tr><tr><td>MixFormer-B2(Ours)</td><td>28M</td><td>187G</td><td>41.5</td><td>63.3</td><td>45.2</td><td>38.3</td><td>60.6</td><td>41.2</td><td>45.1</td><td>66.9</td><td>49.2</td><td>40.8</td><td>64.1</td><td>43.6</td></tr><tr><td>MixFormer-B3(Ours)</td><td>35M</td><td>207G</td><td>42.8</td><td>64.5</td><td>46.7</td><td>39.3</td><td>61.8</td><td>42.2</td><td>46.2</td><td>68.1</td><td>50.5</td><td>41.9</td><td>65.6</td><td>45.0</td></tr><tr><td>MixFormer-B4(Ours)</td><td>53M</td><td>243G</td><td>45.1</td><td>67.1</td><td>49.2</td><td>41.2</td><td>64.3</td><td>44.1</td><td>47.6</td><td>69.5</td><td>52.2</td><td>43.0</td><td>66.7</td><td>46.4</td></tr></table>",
      "id": 67,
      "page": 6,
      "text": "Backbones #Params FLOPs Mask R-CNN 1x schedule Mask R-CNN 3x + MS schedule\n APb AP50 AP75 APm APm APm APb AP50 AP75 APm APm APm\n ResNet18 [19] 31M - 34.0 54.0 36.7 31.2 51.0 32.7 36.9 57.1 40.0 33.6 53.9 35.7\n ResNet50 [19] 44M 260G 38.0 58.6 41.4 34.4 55.1 36.7 41.0 61.7 44.9 37.1 58.4 40.1\n ResNet101 [19] 63M 336G 40.4 61.1 44.2 36.4 57.7 38.8 42.8 63.2 47.1 38.5 60.1 41.3\n ResNeXt101-64x4d [55] 101M 493G 42.8 63.8 47.3 38.4 60.6 41.3 44.4 64.9 48.8 39.7 61.9 42.6\n PVT-T [49] 33M - 36.7 59.2 39.3 35.1 56.7 37.3 39.8 62.2 43.0 37.4 59.3 39.9\n PVT-S [49] 44M 245G 40.4 62.9 43.8 37.8 60.1 40.3 43.0 65.3 46.9 39.9 62.5 42.8\n PVT-M [49] 64M 302G 42.0 64.4 45.6 39.0 61.6 42.1 44.2 66.0 48.2 40.5 63.1 43.5\n PVT-L [49] 81M 364G 42.9 65.0 46.6 39.5 61.9 42.5 44.5 66.0 48.3 40.7 63.4 43.7\n TwinsP-S [6] 44M 245G 42.9 65.8 47.1 40.0 62.7 42.9 46.8 69.3 51.8 42.6 66.3 46.0\n DS-Net-S [38] 43M - 44.3 - - 40.2 - - - - - - - -\n Swin-T [34] 48M 264G 42.2 64.6 46.2 39.1 61.6 42.0 46.0 68.2 50.2 41.6 65.1 44.8\n Twins-S [6] 44M 228G 43.4 66.0 47.3 40.3 63.2 43.4 46.8 69.2 51.2 42.6 66.3 45.8\n Focal-T [57] 49M 291G - - - - - - 47.2 69.4 51.9 42.7 66.5 45.9\n Shuffle-T [26] 48M 268G - - - - - - 46.8 68.9 51.5 42.3 66.0 45.6\n MixFormer-B1(Ours) 26M 183G 40.6 62.6 44.1 37.5 59.7 40.0 43.9 65.6 48.1 40.0 62.9 42.9\n MixFormer-B2(Ours) 28M 187G 41.5 63.3 45.2 38.3 60.6 41.2 45.1 66.9 49.2 40.8 64.1 43.6\n MixFormer-B3(Ours) 35M 207G 42.8 64.5 46.7 39.3 61.8 42.2 46.2 68.1 50.5 41.9 65.6 45.0\n MixFormer-B4(Ours) 53M 243G 45.1 67.1 49.2 41.2 64.3 44.1 47.6 69.5 52.2 43.0 66.7"
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 1166
        },
        {
          "x": 2274,
          "y": 1166
        },
        {
          "x": 2274,
          "y": 1304
        },
        {
          "x": 201,
          "y": 1304
        }
      ],
      "category": "caption",
      "html": "<caption id='68' style='font-size:16px'>Table 4. COCO detection and segmentation with the Mask R-CNN. The performances are reported on the COCO val split under 1x<br>and 3x schedules. The FLOPs (G) are measured at resolution 800 x 1280, and all models are pre-trained on the ImageNet-1K [9]. In the<br>table, '-' means that the result is not reported by the original paper.</caption>",
      "id": 68,
      "page": 6,
      "text": "Table 4. COCO detection and segmentation with the Mask R-CNN. The performances are reported on the COCO val split under 1x\nand 3x schedules. The FLOPs (G) are measured at resolution 800 x 1280, and all models are pre-trained on the ImageNet-1K [9]. In the\ntable, '-' means that the result is not reported by the original paper."
    },
    {
      "bounding_box": [
        {
          "x": 225,
          "y": 1364
        },
        {
          "x": 1189,
          "y": 1364
        },
        {
          "x": 1189,
          "y": 1556
        },
        {
          "x": 225,
          "y": 1556
        }
      ],
      "category": "table",
      "html": "<table id='69' style='font-size:14px'><tr><td>Backbones</td><td>#Params</td><td>FLOPs</td><td>APb</td><td>AP50</td><td>AP75</td><td>APm</td><td>APm</td><td>APm</td></tr><tr><td>ResNet50 [19]</td><td>82M</td><td>739G</td><td>46.3</td><td>64.3</td><td>50.5</td><td>40.1</td><td>61.7</td><td>43.4</td></tr><tr><td>Swin-T [34]</td><td>86M</td><td>745G</td><td>50.5</td><td>69.3</td><td>54.9</td><td>43.7</td><td>66.6</td><td>47.1</td></tr><tr><td>Shuffle-T [26]</td><td>86M</td><td>746G</td><td>50.8</td><td>69.6</td><td>55.1</td><td>44.1</td><td>66.9</td><td>48.0</td></tr><tr><td>MixFormer-B4(Ours)</td><td>91M</td><td>721G</td><td>51.6</td><td>70.5</td><td>56.1</td><td>44.9</td><td>67.9</td><td>48.7</td></tr></table>",
      "id": 69,
      "page": 6,
      "text": "Backbones #Params FLOPs APb AP50 AP75 APm APm APm\n ResNet50 [19] 82M 739G 46.3 64.3 50.5 40.1 61.7 43.4\n Swin-T [34] 86M 745G 50.5 69.3 54.9 43.7 66.6 47.1\n Shuffle-T [26] 86M 746G 50.8 69.6 55.1 44.1 66.9 48.0\n MixFormer-B4(Ours) 91M 721G 51.6 70.5 56.1 44.9 67.9"
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 1590
        },
        {
          "x": 1198,
          "y": 1590
        },
        {
          "x": 1198,
          "y": 1772
        },
        {
          "x": 203,
          "y": 1772
        }
      ],
      "category": "caption",
      "html": "<caption id='70' style='font-size:16px'>Table 5. COCO detection and segmentation with the Cascade<br>Mask R-CNN. The performances are reported on the COCO val<br>split under a 3x schedule. Results show consistent improvements<br>of MixFormer over Swin Transformer.</caption>",
      "id": 70,
      "page": 6,
      "text": "Table 5. COCO detection and segmentation with the Cascade\nMask R-CNN. The performances are reported on the COCO val\nsplit under a 3x schedule. Results show consistent improvements\nof MixFormer over Swin Transformer."
    },
    {
      "bounding_box": [
        {
          "x": 227,
          "y": 1829
        },
        {
          "x": 1189,
          "y": 1829
        },
        {
          "x": 1189,
          "y": 2588
        },
        {
          "x": 227,
          "y": 2588
        }
      ],
      "category": "table",
      "html": "<table id='71' style='font-size:14px'><tr><td>Backbone</td><td>Method</td><td>#Params</td><td>FLOPs</td><td>mIoUss</td><td>mloU ms</td></tr><tr><td>ResNet-101 [19]</td><td>DANet [13]</td><td>69M</td><td>1119G</td><td>43.6</td><td>45.2</td></tr><tr><td>ResNet-101 [19]</td><td>DLab.v3+ [5]</td><td>63M</td><td>1021G</td><td>45.1</td><td>46.7</td></tr><tr><td>ResNet-101 [19]</td><td>ACNet [14]</td><td>-</td><td>-</td><td>45.9</td><td>-</td></tr><tr><td>ResNet-101 [19]</td><td>DNL [58]</td><td>69M</td><td>1249G</td><td>46.0</td><td>-</td></tr><tr><td>ResNet-101 [19]</td><td>OCRNet [60]</td><td>56M</td><td>923G</td><td>-</td><td>45.3</td></tr><tr><td>ResNet-101 [19]</td><td>UperNet [54]</td><td>86M</td><td>1029G</td><td>43.8</td><td>44.9</td></tr><tr><td>HRNet-w48 [47]</td><td>OCRNet [60]</td><td>71M</td><td>664G</td><td>-</td><td>45.7</td></tr><tr><td>DeiT-S [44]†</td><td>UperNet [54]</td><td>52M</td><td>1099G</td><td>44.0</td><td>-</td></tr><tr><td>TwinsP-S [6]</td><td>UperNet [54]</td><td>55M</td><td>919G</td><td>46.2</td><td>47.5</td></tr><tr><td>Swin-T [34]</td><td>UperNet [54]</td><td>60M</td><td>945G</td><td>44.5</td><td>45.8</td></tr><tr><td>Twins-S [6]</td><td>UperNet [54]</td><td>54M</td><td>901G</td><td>46.2</td><td>47.1</td></tr><tr><td>LG-T [31]</td><td>UperNet [54]</td><td>64M</td><td>957G</td><td>-</td><td>45.3</td></tr><tr><td>Focal-T [57]</td><td>UperNet [54]</td><td>62M</td><td>998G</td><td>45.8</td><td>47.0</td></tr><tr><td>Shuffle-T [26]</td><td>UperNet [54]</td><td>60M</td><td>949G</td><td>46.6</td><td>47.6</td></tr><tr><td>MixFormer-B1(Ours)</td><td>UperNet [54]</td><td>35M</td><td>854G</td><td>42.0</td><td>43.5</td></tr><tr><td>MixFormer-B2(Ours)</td><td>UperNet [54]</td><td>37M</td><td>859G</td><td>43.1</td><td>43.9</td></tr><tr><td>MixFormer-B3(Ours)</td><td>UperNet [54]</td><td>44M</td><td>880G</td><td>44.5</td><td>45.5</td></tr><tr><td>MixFormer-B4(Ours)</td><td>UperNet [54]</td><td>63M</td><td>918G</td><td>46.8</td><td>48.0</td></tr></table>",
      "id": 71,
      "page": 6,
      "text": "Backbone Method #Params FLOPs mIoUss mloU ms\n ResNet-101 [19] DANet [13] 69M 1119G 43.6 45.2\n ResNet-101 [19] DLab.v3+ [5] 63M 1021G 45.1 46.7\n ResNet-101 [19] ACNet [14] - - 45.9 -\n ResNet-101 [19] DNL [58] 69M 1249G 46.0 -\n ResNet-101 [19] OCRNet [60] 56M 923G - 45.3\n ResNet-101 [19] UperNet [54] 86M 1029G 43.8 44.9\n HRNet-w48 [47] OCRNet [60] 71M 664G - 45.7\n DeiT-S [44]† UperNet [54] 52M 1099G 44.0 -\n TwinsP-S [6] UperNet [54] 55M 919G 46.2 47.5\n Swin-T [34] UperNet [54] 60M 945G 44.5 45.8\n Twins-S [6] UperNet [54] 54M 901G 46.2 47.1\n LG-T [31] UperNet [54] 64M 957G - 45.3\n Focal-T [57] UperNet [54] 62M 998G 45.8 47.0\n Shuffle-T [26] UperNet [54] 60M 949G 46.6 47.6\n MixFormer-B1(Ours) UperNet [54] 35M 854G 42.0 43.5\n MixFormer-B2(Ours) UperNet [54] 37M 859G 43.1 43.9\n MixFormer-B3(Ours) UperNet [54] 44M 880G 44.5 45.5\n MixFormer-B4(Ours) UperNet [54] 63M 918G 46.8"
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 2644
        },
        {
          "x": 1199,
          "y": 2644
        },
        {
          "x": 1199,
          "y": 2823
        },
        {
          "x": 202,
          "y": 2823
        }
      ],
      "category": "caption",
      "html": "<caption id='72' style='font-size:16px'>Table 6. ADE20K semantic segmentation. We report mIoU on<br>the ADE20K [66] val split with single scale (ss) testing and multi-<br>scale (ms) testing · A resolution 512 x 2048 is used to measure<br>the FLOPs (G) in various models.</caption>",
      "id": 72,
      "page": 6,
      "text": "Table 6. ADE20K semantic segmentation. We report mIoU on\nthe ADE20K [66] val split with single scale (ss) testing and multi-\nscale (ms) testing · A resolution 512 x 2048 is used to measure\nthe FLOPs (G) in various models."
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 2882
        },
        {
          "x": 1193,
          "y": 2882
        },
        {
          "x": 1193,
          "y": 2971
        },
        {
          "x": 204,
          "y": 2971
        }
      ],
      "category": "paragraph",
      "html": "<p id='73' style='font-size:18px'>vious works based on a stronger model, Cascade Mask R-<br>CNN [3, 18], where a 3x schedule is conducted.</p>",
      "id": 73,
      "page": 6,
      "text": "vious works based on a stronger model, Cascade Mask R-\nCNN [3, 18], where a 3x schedule is conducted."
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 1371
        },
        {
          "x": 2277,
          "y": 1371
        },
        {
          "x": 2277,
          "y": 2116
        },
        {
          "x": 1279,
          "y": 2116
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='74' style='font-size:20px'>Comparison on Mask R-CNN. Table 4 shows that Mix-<br>Former consistently outperforms other competitors [19,<br>31, 34, 49, 53, 57] under various model sizes with Mask<br>R-CNN [18]. In particular, MixFormer-B4 achieves<br>+2.9(+1.6) higher box mAP and +2.1(+1.4) higher mask<br>mAP than the Swin-T [34] baseline with 1x (3x) schedule.<br>Moreover, MixFormer keeps its efficiency in detection and<br>instance segmentation, enabling higher performance with<br>less computational costs than other networks [19,34]. It is a<br>surprise that our MixFormer-B1 (only with 0.7G) performs<br>strongly with Mask R-CNN (1x), which exceeds ResNet-<br>50 (with 4.1G) [19] by 2.3 box mAP and 2.9 mask mAP.<br>The results suggest that implications for designing high-<br>performance small models on detection are highlighted in<br>MixFormer.</p>",
      "id": 74,
      "page": 6,
      "text": "Comparison on Mask R-CNN. Table 4 shows that Mix-\nFormer consistently outperforms other competitors [19,\n31, 34, 49, 53, 57] under various model sizes with Mask\nR-CNN [18]. In particular, MixFormer-B4 achieves\n+2.9(+1.6) higher box mAP and +2.1(+1.4) higher mask\nmAP than the Swin-T [34] baseline with 1x (3x) schedule.\nMoreover, MixFormer keeps its efficiency in detection and\ninstance segmentation, enabling higher performance with\nless computational costs than other networks [19,34]. It is a\nsurprise that our MixFormer-B1 (only with 0.7G) performs\nstrongly with Mask R-CNN (1x), which exceeds ResNet-\n50 (with 4.1G) [19] by 2.3 box mAP and 2.9 mask mAP.\nThe results suggest that implications for designing high-\nperformance small models on detection are highlighted in\nMixFormer."
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 2144
        },
        {
          "x": 2276,
          "y": 2144
        },
        {
          "x": 2276,
          "y": 2490
        },
        {
          "x": 1279,
          "y": 2490
        }
      ],
      "category": "paragraph",
      "html": "<p id='75' style='font-size:18px'>Comparison on Cascade Mask R-CNN. We also evaluate<br>MixFormer with Cascade Mask R-CNN [3, 18], which is<br>a stronger variant of Mask R-CNN [18]. MixFormer-B4<br>provides robust improvements over Swin-T [34] (Table 5)<br>regardless of different detectors, as it shows similar gains<br>(+1.1/1.2 box/mask mAP V.S. +1.6/1.4 box/mask mAP) with<br>the ones on Mask R-CNN (3x) (Table 4).</p>",
      "id": 75,
      "page": 6,
      "text": "Comparison on Cascade Mask R-CNN. We also evaluate\nMixFormer with Cascade Mask R-CNN [3, 18], which is\na stronger variant of Mask R-CNN [18]. MixFormer-B4\nprovides robust improvements over Swin-T [34] (Table 5)\nregardless of different detectors, as it shows similar gains\n(+1.1/1.2 box/mask mAP V.S. +1.6/1.4 box/mask mAP) with\nthe ones on Mask R-CNN (3x) (Table 4)."
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 2527
        },
        {
          "x": 1826,
          "y": 2527
        },
        {
          "x": 1826,
          "y": 2573
        },
        {
          "x": 1281,
          "y": 2573
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='76' style='font-size:22px'>4.3. Semantic Segmentation</p>",
      "id": 76,
      "page": 6,
      "text": "4.3. Semantic Segmentation"
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 2605
        },
        {
          "x": 2276,
          "y": 2605
        },
        {
          "x": 2276,
          "y": 2849
        },
        {
          "x": 1280,
          "y": 2849
        }
      ],
      "category": "paragraph",
      "html": "<p id='77' style='font-size:20px'>Setup. Our experiments are conducted on ADE20K [66]<br>using UperNet [54]. For training recipes, we mainly follow<br>the settings in [34]. We report mIoU of our models in single<br>scale testing (ss) and multi-scale testing (ms). Details are<br>provided in Appendix C.</p>",
      "id": 77,
      "page": 6,
      "text": "Setup. Our experiments are conducted on ADE20K [66]\nusing UperNet [54]. For training recipes, we mainly follow\nthe settings in [34]. We report mIoU of our models in single\nscale testing (ss) and multi-scale testing (ms). Details are\nprovided in Appendix C."
    },
    {
      "bounding_box": [
        {
          "x": 1283,
          "y": 2879
        },
        {
          "x": 2275,
          "y": 2879
        },
        {
          "x": 2275,
          "y": 2974
        },
        {
          "x": 1283,
          "y": 2974
        }
      ],
      "category": "paragraph",
      "html": "<p id='78' style='font-size:18px'>Results. In Table 6, MixFormer-B4 consistently achieves<br>better mIoU performance than previous networks. It seems</p>",
      "id": 78,
      "page": 6,
      "text": "Results. In Table 6, MixFormer-B4 consistently achieves\nbetter mIoU performance than previous networks. It seems"
    },
    {
      "bounding_box": [
        {
          "x": 1226,
          "y": 3058
        },
        {
          "x": 1253,
          "y": 3058
        },
        {
          "x": 1253,
          "y": 3091
        },
        {
          "x": 1226,
          "y": 3091
        }
      ],
      "category": "footer",
      "html": "<footer id='79' style='font-size:16px'>6</footer>",
      "id": 79,
      "page": 6,
      "text": "6"
    },
    {
      "bounding_box": [
        {
          "x": 218,
          "y": 295
        },
        {
          "x": 1182,
          "y": 295
        },
        {
          "x": 1182,
          "y": 679
        },
        {
          "x": 218,
          "y": 679
        }
      ],
      "category": "table",
      "html": "<table id='80' style='font-size:14px'><tr><td>Parallel</td><td>Interactions Channel Spatial</td><td colspan=\"2\">ImageNet Top-1 Top-5</td><td colspan=\"2\">COCO APbox APmask</td><td>ADE20k mIoU</td></tr><tr><td rowspan=\"5\"></td><td rowspan=\"5\">V V</td><td>77.4</td><td>93.8</td><td>38.2</td><td>35.7</td><td>38.9</td></tr><tr><td>78.1</td><td>94.1</td><td>39.4</td><td>36.6</td><td>39.8</td></tr><tr><td>78.3</td><td>94.1</td><td>40.1</td><td>37.1</td><td>40.6</td></tr><tr><td>78.3</td><td>94.1</td><td>39.7</td><td>36.6</td><td>40.5</td></tr><tr><td>78.4</td><td>94.3</td><td>40.3</td><td>37.3</td><td>40.9</td></tr><tr><td colspan=\"2\">△</td><td>+1.0</td><td>+0.5</td><td>+2.1</td><td>+1.6</td><td>+2.0</td></tr></table>",
      "id": 80,
      "page": 7,
      "text": "Parallel Interactions Channel Spatial ImageNet Top-1 Top-5 COCO APbox APmask ADE20k mIoU\n  V V 77.4 93.8 38.2 35.7 38.9\n 78.1 94.1 39.4 36.6 39.8\n 78.3 94.1 40.1 37.1 40.6\n 78.3 94.1 39.7 36.6 40.5\n 78.4 94.3 40.3 37.3 40.9\n △ +1.0 +0.5 +2.1 +1.6"
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 718
        },
        {
          "x": 1198,
          "y": 718
        },
        {
          "x": 1198,
          "y": 853
        },
        {
          "x": 204,
          "y": 853
        }
      ],
      "category": "caption",
      "html": "<caption id='81' style='font-size:14px'>Table 7. Parallel Design with Bi-directional Interactions. The<br>baseline model in this table adopts a successive design and has no<br>interactions in the block.</caption>",
      "id": 81,
      "page": 7,
      "text": "Table 7. Parallel Design with Bi-directional Interactions. The\nbaseline model in this table adopts a successive design and has no\ninteractions in the block."
    },
    {
      "bounding_box": [
        {
          "x": 251,
          "y": 876
        },
        {
          "x": 1153,
          "y": 876
        },
        {
          "x": 1153,
          "y": 1122
        },
        {
          "x": 251,
          "y": 1122
        }
      ],
      "category": "table",
      "html": "<table id='82' style='font-size:14px'><tr><td rowspan=\"2\">Window Sizes</td><td colspan=\"2\">ImageNet</td><td colspan=\"2\">COCO</td><td rowspan=\"2\">ADE20k mIoU</td></tr><tr><td>Top-1</td><td>Top-5</td><td>APbox</td><td>APmask</td></tr><tr><td>1 x 1</td><td>77.1</td><td>93.6</td><td>36.3</td><td>34.3</td><td>37.6</td></tr><tr><td>3 x 3</td><td>78.4</td><td>94.3</td><td>40.3</td><td>37.3</td><td>40.9</td></tr><tr><td>5 x 5</td><td>78.4</td><td>94.3</td><td>40.3</td><td>37.2</td><td>40.8</td></tr></table>",
      "id": 82,
      "page": 7,
      "text": "Window Sizes ImageNet COCO ADE20k mIoU\n Top-1 Top-5 APbox APmask\n 1 x 1 77.1 93.6 36.3 34.3 37.6\n 3 x 3 78.4 94.3 40.3 37.3 40.9\n 5 x 5 78.4 94.3 40.3 37.2"
    },
    {
      "bounding_box": [
        {
          "x": 205,
          "y": 1161
        },
        {
          "x": 1199,
          "y": 1161
        },
        {
          "x": 1199,
          "y": 1294
        },
        {
          "x": 205,
          "y": 1294
        }
      ],
      "category": "caption",
      "html": "<caption id='83' style='font-size:14px'>Table 8. Window Sizes in DwConv. We investigate various win-<br>dow sizes for DwConv. MixFormer uses the 3 x 3 window size<br>for DwConv by default.</caption>",
      "id": 83,
      "page": 7,
      "text": "Table 8. Window Sizes in DwConv. We investigate various win-\ndow sizes for DwConv. MixFormer uses the 3 x 3 window size\nfor DwConv by default."
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 1361
        },
        {
          "x": 1198,
          "y": 1361
        },
        {
          "x": 1198,
          "y": 1600
        },
        {
          "x": 204,
          "y": 1600
        }
      ],
      "category": "paragraph",
      "html": "<p id='84' style='font-size:16px'>that the connections across windows and dimensions in the<br>Mixing Block provide more benefits on semantic segmenta-<br>tion as the gains are larger than the ones on detection tasks<br>(Table 4,Table 5) with the same backbones. In particular,<br>MixFormer-B4 outperforms Swin-T [34] by 2.2 mIoU.</p>",
      "id": 84,
      "page": 7,
      "text": "that the connections across windows and dimensions in the\nMixing Block provide more benefits on semantic segmenta-\ntion as the gains are larger than the ones on detection tasks\n(Table 4,Table 5) with the same backbones. In particular,\nMixFormer-B4 outperforms Swin-T [34] by 2.2 mIoU."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 1609
        },
        {
          "x": 1199,
          "y": 1609
        },
        {
          "x": 1199,
          "y": 2101
        },
        {
          "x": 202,
          "y": 2101
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='85' style='font-size:20px'>Moreover, other variants of MixFormer (from B1 to B3)<br>also achieve higher performance while being more effi-<br>cient than previous networks. Notably, MixFormer-B3 ob-<br>tains 45.5 mIoU (comparable with Swin-T [34] but less<br>FLOPs), which achieves on par results with OCRNet [60]<br>with HRNet- W48 [47] (45.7 mloU). Note that HRNet [47]<br>is carefully designed to aggregate the features in different<br>stages, while MixFormer simply constructs pyramid feature<br>maps, indicating the strong potential for further improve-<br>ments on dense prediction tasks.</p>",
      "id": 85,
      "page": 7,
      "text": "Moreover, other variants of MixFormer (from B1 to B3)\nalso achieve higher performance while being more effi-\ncient than previous networks. Notably, MixFormer-B3 ob-\ntains 45.5 mIoU (comparable with Swin-T [34] but less\nFLOPs), which achieves on par results with OCRNet [60]\nwith HRNet- W48 [47] (45.7 mloU). Note that HRNet [47]\nis carefully designed to aggregate the features in different\nstages, while MixFormer simply constructs pyramid feature\nmaps, indicating the strong potential for further improve-\nments on dense prediction tasks."
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 2134
        },
        {
          "x": 583,
          "y": 2134
        },
        {
          "x": 583,
          "y": 2181
        },
        {
          "x": 203,
          "y": 2181
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='86' style='font-size:22px'>4.4. Ablation Study</p>",
      "id": 86,
      "page": 7,
      "text": "4.4. Ablation Study"
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 2211
        },
        {
          "x": 1198,
          "y": 2211
        },
        {
          "x": 1198,
          "y": 2754
        },
        {
          "x": 201,
          "y": 2754
        }
      ],
      "category": "paragraph",
      "html": "<p id='87' style='font-size:18px'>Setup. We provide ablations with respect to our designs<br>on MixFormer-B1. We report all variations of different<br>designs on ImageNet-1K [9] classification, COCO [33]<br>detection and segmentation, and ADE20K [66] semantic<br>segmentation. To make quick evaluations, we only train<br>MixFormer-B1 for 200 epochs on ImageNet-1K [9]. Then,<br>the pre-trained models are adopted by Mask R-CNN [18]<br>(1x) on MS COCO [33] and UperNet [54] (160k) on<br>ADE20K [66]. Note that, the differences in pre-train mod-<br>els provide slightly different results with the ones in Table 4<br>and Table 6.</p>",
      "id": 87,
      "page": 7,
      "text": "Setup. We provide ablations with respect to our designs\non MixFormer-B1. We report all variations of different\ndesigns on ImageNet-1K [9] classification, COCO [33]\ndetection and segmentation, and ADE20K [66] semantic\nsegmentation. To make quick evaluations, we only train\nMixFormer-B1 for 200 epochs on ImageNet-1K [9]. Then,\nthe pre-trained models are adopted by Mask R-CNN [18]\n(1x) on MS COCO [33] and UperNet [54] (160k) on\nADE20K [66]. Note that, the differences in pre-train mod-\nels provide slightly different results with the ones in Table 4\nand Table 6."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 2779
        },
        {
          "x": 1199,
          "y": 2779
        },
        {
          "x": 1199,
          "y": 2976
        },
        {
          "x": 202,
          "y": 2976
        }
      ],
      "category": "paragraph",
      "html": "<p id='88' style='font-size:18px'>Ablation: Parallel or Not. Table 7 provides the compar-<br>ison of the ways (successive design or parallel design) to<br>combine local-window self-attention and depth-wise con-<br>volution. Our parallel design consistently outperforms the</p>",
      "id": 88,
      "page": 7,
      "text": "Ablation: Parallel or Not. Table 7 provides the compar-\nison of the ways (successive design or parallel design) to\ncombine local-window self-attention and depth-wise con-\nvolution. Our parallel design consistently outperforms the"
    },
    {
      "bounding_box": [
        {
          "x": 1293,
          "y": 293
        },
        {
          "x": 2267,
          "y": 293
        },
        {
          "x": 2267,
          "y": 543
        },
        {
          "x": 1293,
          "y": 543
        }
      ],
      "category": "table",
      "html": "<br><table id='89' style='font-size:14px'><tr><td rowspan=\"2\">Techniques</td><td colspan=\"2\">ImageNet</td><td colspan=\"2\">COCO</td><td rowspan=\"2\">ADE20k mIoU</td></tr><tr><td>Top-1</td><td>Top-5</td><td>APbox</td><td>APmask</td></tr><tr><td>MixFormer-B1(Ours)</td><td>78.4</td><td>94.3</td><td>40.3</td><td>37.3</td><td>40.9</td></tr><tr><td>+shifted windows</td><td>78.3</td><td>94.1</td><td>40.5</td><td>37.3</td><td>40.7</td></tr><tr><td>+DwConv in FFN</td><td>78.6</td><td>94.4</td><td>40.5</td><td>37.4</td><td>40.9</td></tr></table>",
      "id": 89,
      "page": 7,
      "text": "Techniques ImageNet COCO ADE20k mIoU\n Top-1 Top-5 APbox APmask\n MixFormer-B1(Ours) 78.4 94.3 40.3 37.3 40.9\n +shifted windows 78.3 94.1 40.5 37.3 40.7\n +DwConv in FFN 78.6 94.4 40.5 37.4"
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 581
        },
        {
          "x": 2276,
          "y": 581
        },
        {
          "x": 2276,
          "y": 713
        },
        {
          "x": 1280,
          "y": 713
        }
      ],
      "category": "caption",
      "html": "<caption id='90' style='font-size:14px'>Table 9. Other Techniques. We combine two techniques with our<br>MixFormer. When inserting DwConv in FFN, we only consider<br>3 x 3 DwConv.</caption>",
      "id": 90,
      "page": 7,
      "text": "Table 9. Other Techniques. We combine two techniques with our\nMixFormer. When inserting DwConv in FFN, we only consider\n3 x 3 DwConv."
    },
    {
      "bounding_box": [
        {
          "x": 1318,
          "y": 741
        },
        {
          "x": 2239,
          "y": 741
        },
        {
          "x": 2239,
          "y": 1224
        },
        {
          "x": 1318,
          "y": 1224
        }
      ],
      "category": "table",
      "html": "<table id='91' style='font-size:16px'><tr><td>#Blocks #Channels</td><td>FLOPs</td><td>ImageNet Top-1 Top-5</td><td>COCO APbox APmask</td><td>ADE20k mIoU</td></tr><tr><td>[2, 2,8, 2] 32, 64, 160, 256]</td><td>0.9G</td><td>77.7 93.9</td><td>40.1 37.3</td><td>40.6</td></tr><tr><td>[2, 2,6,4] [32, 64, 128, 256]</td><td>0.9G</td><td>77.5 93.7</td><td>39.6 36.7</td><td>39.8</td></tr><tr><td>[1,2,6,2] 32, 64, 160, 320]</td><td>0.8G</td><td>77.2 93.5</td><td>39.3 36.6</td><td>40.4</td></tr><tr><td>[1,2,6,6] [32, 64, 128, 256]</td><td>0.7G</td><td>78.4 94.3</td><td>40.3 37.3</td><td>40.9</td></tr></table>",
      "id": 91,
      "page": 7,
      "text": "#Blocks #Channels FLOPs ImageNet Top-1 Top-5 COCO APbox APmask ADE20k mIoU\n [2, 2,8, 2] 32, 64, 160, 256] 0.9G 77.7 93.9 40.1 37.3 40.6\n [2, 2,6,4] [32, 64, 128, 256] 0.9G 77.5 93.7 39.6 36.7 39.8\n [1,2,6,2] 32, 64, 160, 320] 0.8G 77.2 93.5 39.3 36.6 40.4\n [1,2,6,6] [32, 64, 128, 256] 0.7G 78.4 94.3 40.3 37.3"
    },
    {
      "bounding_box": [
        {
          "x": 1282,
          "y": 1259
        },
        {
          "x": 2275,
          "y": 1259
        },
        {
          "x": 2275,
          "y": 1393
        },
        {
          "x": 1282,
          "y": 1393
        }
      ],
      "category": "caption",
      "html": "<caption id='92' style='font-size:14px'>Table 10. Number of Blocks in Stages. In the table, the first<br>two models and the last two models share similar computational<br>complexities with each other.</caption>",
      "id": 92,
      "page": 7,
      "text": "Table 10. Number of Blocks in Stages. In the table, the first\ntwo models and the last two models share similar computational\ncomplexities with each other."
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 1457
        },
        {
          "x": 2278,
          "y": 1457
        },
        {
          "x": 2278,
          "y": 1650
        },
        {
          "x": 1281,
          "y": 1650
        }
      ],
      "category": "paragraph",
      "html": "<p id='93' style='font-size:18px'>successive design across various vision tasks, which verifies<br>the hypothesis that the parallel design enables better feature<br>representation learning in Section 1 The models below use<br>parallel design by default.</p>",
      "id": 93,
      "page": 7,
      "text": "successive design across various vision tasks, which verifies\nthe hypothesis that the parallel design enables better feature\nrepresentation learning in Section 1 The models below use\nparallel design by default."
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 1677
        },
        {
          "x": 2276,
          "y": 1677
        },
        {
          "x": 2276,
          "y": 2274
        },
        {
          "x": 1278,
          "y": 2274
        }
      ],
      "category": "paragraph",
      "html": "<p id='94' style='font-size:18px'>Ablation: Bi-directional Interactions. Table 7 shows the<br>results of the proposed interactions. According to the re-<br>sults, we see that both channel and spatial interactions out-<br>perform the model without interactions across all different<br>vision tasks. Combining two interactions promotes better<br>performance, resulting in consistent improvements by 0.3%<br>Top-1 accuracy on ImageNet-1K, 0.9/0.7 box/mask mAP<br>on COCO, and 1.1 mIoU on ADE20K. Given that we only<br>use simple and light-weighted designs for bi-directional<br>interactions, the gains are nontrivial, which indicates the<br>effectiveness of providing complementary clues for local-<br>window self-attention and depth-wise convolution.</p>",
      "id": 94,
      "page": 7,
      "text": "Ablation: Bi-directional Interactions. Table 7 shows the\nresults of the proposed interactions. According to the re-\nsults, we see that both channel and spatial interactions out-\nperform the model without interactions across all different\nvision tasks. Combining two interactions promotes better\nperformance, resulting in consistent improvements by 0.3%\nTop-1 accuracy on ImageNet-1K, 0.9/0.7 box/mask mAP\non COCO, and 1.1 mIoU on ADE20K. Given that we only\nuse simple and light-weighted designs for bi-directional\ninteractions, the gains are nontrivial, which indicates the\neffectiveness of providing complementary clues for local-\nwindow self-attention and depth-wise convolution."
    },
    {
      "bounding_box": [
        {
          "x": 1277,
          "y": 2303
        },
        {
          "x": 2276,
          "y": 2303
        },
        {
          "x": 2276,
          "y": 2848
        },
        {
          "x": 1277,
          "y": 2848
        }
      ],
      "category": "paragraph",
      "html": "<p id='95' style='font-size:16px'>Ablation: Window Sizes in DwConv. Table 8 shows that<br>the performance will drop significantly on various vision<br>tasks (-1.3 Top-1 accuracy on ImageNet-1K, -4.0/ - 3.0<br>box/mask mAP on COCO, and -3.3 mIoU on ADE20K) if<br>we reduce the window size of the depth-wise convolution<br>from 3 x 3 to 1 x 1. This phenomenon means that it's nec-<br>essary for depth-wise convolution to use a window size (at<br>least 3 x 3) with the ability to connect across-window. Be-<br>sides, when we increase the window size to 5 x 5, no clear<br>further gains are observed. Thus, we use a window size of<br>3 x 3 regarding the efficiency.</p>",
      "id": 95,
      "page": 7,
      "text": "Ablation: Window Sizes in DwConv. Table 8 shows that\nthe performance will drop significantly on various vision\ntasks (-1.3 Top-1 accuracy on ImageNet-1K, -4.0/ - 3.0\nbox/mask mAP on COCO, and -3.3 mIoU on ADE20K) if\nwe reduce the window size of the depth-wise convolution\nfrom 3 x 3 to 1 x 1. This phenomenon means that it's nec-\nessary for depth-wise convolution to use a window size (at\nleast 3 x 3) with the ability to connect across-window. Be-\nsides, when we increase the window size to 5 x 5, no clear\nfurther gains are observed. Thus, we use a window size of\n3 x 3 regarding the efficiency."
    },
    {
      "bounding_box": [
        {
          "x": 1282,
          "y": 2879
        },
        {
          "x": 2275,
          "y": 2879
        },
        {
          "x": 2275,
          "y": 2977
        },
        {
          "x": 1282,
          "y": 2977
        }
      ],
      "category": "paragraph",
      "html": "<p id='96' style='font-size:20px'>Ablation: Other Techniques. We also investigate other<br>designs in MixFormer, including applying shifted windows</p>",
      "id": 96,
      "page": 7,
      "text": "Ablation: Other Techniques. We also investigate other\ndesigns in MixFormer, including applying shifted windows"
    },
    {
      "bounding_box": [
        {
          "x": 1226,
          "y": 3057
        },
        {
          "x": 1250,
          "y": 3057
        },
        {
          "x": 1250,
          "y": 3089
        },
        {
          "x": 1226,
          "y": 3089
        }
      ],
      "category": "footer",
      "html": "<footer id='97' style='font-size:14px'>7</footer>",
      "id": 97,
      "page": 7,
      "text": "7"
    },
    {
      "bounding_box": [
        {
          "x": 273,
          "y": 292
        },
        {
          "x": 1131,
          "y": 292
        },
        {
          "x": 1131,
          "y": 860
        },
        {
          "x": 273,
          "y": 860
        }
      ],
      "category": "table",
      "html": "<table id='98' style='font-size:16px'><tr><td rowspan=\"2\">Backbones</td><td colspan=\"3\">COCO keypoint detection</td></tr><tr><td>APkp</td><td>APKP</td><td>APKP</td></tr><tr><td>ResNet50 [19]</td><td>71.8</td><td>89.8</td><td>79.5</td></tr><tr><td>Swin-T [34]</td><td>74.2</td><td>92.5</td><td>82.5</td></tr><tr><td>HRFormer-S [34]</td><td>74.5</td><td>92.3</td><td>82.1</td></tr><tr><td>MixFormer-B4(Ours)</td><td>75.3 (+1.1)</td><td>93.5 (+1.0)</td><td>83.5 (+1.0)</td></tr><tr><td rowspan=\"2\">Backbones</td><td colspan=\"3\">LVIS Instance Segmentation</td></tr><tr><td>APmask</td><td>APmask</td><td>APmask</td></tr><tr><td>ResNet50 [19]</td><td>21.7</td><td>34.3</td><td>23.0</td></tr><tr><td>Swin-T [34]</td><td>27.6</td><td>43.0</td><td>29.3</td></tr><tr><td>MixFormer-B4(Ours)</td><td>28.6 (+1.0)</td><td>43.4 (+0.4)</td><td>30.5 (+1.2)</td></tr></table>",
      "id": 98,
      "page": 8,
      "text": "Backbones COCO keypoint detection\n APkp APKP APKP\n ResNet50 [19] 71.8 89.8 79.5\n Swin-T [34] 74.2 92.5 82.5\n HRFormer-S [34] 74.5 92.3 82.1\n MixFormer-B4(Ours) 75.3 (+1.1) 93.5 (+1.0) 83.5 (+1.0)\n Backbones LVIS Instance Segmentation\n APmask APmask APmask\n ResNet50 [19] 21.7 34.3 23.0\n Swin-T [34] 27.6 43.0 29.3\n MixFormer-B4(Ours) 28.6 (+1.0) 43.4 (+0.4)"
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 896
        },
        {
          "x": 1197,
          "y": 896
        },
        {
          "x": 1197,
          "y": 1032
        },
        {
          "x": 203,
          "y": 1032
        }
      ],
      "category": "caption",
      "html": "<caption id='99' style='font-size:14px'>Table 11. More Downstream Tasks. We compare our MixFormer<br>with ResNet50 [19] and Swin Transformer [34] on keypoint detec-<br>tion and long-tail instance segmentation.</caption>",
      "id": 99,
      "page": 8,
      "text": "Table 11. More Downstream Tasks. We compare our MixFormer\nwith ResNet50 [19] and Swin Transformer [34] on keypoint detec-\ntion and long-tail instance segmentation."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 1101
        },
        {
          "x": 1198,
          "y": 1101
        },
        {
          "x": 1198,
          "y": 1545
        },
        {
          "x": 202,
          "y": 1545
        }
      ],
      "category": "paragraph",
      "html": "<p id='100' style='font-size:18px'>and inserting 3 x 3 depth-wise convolution in FFN, which<br>play significant roles in previous works [34, 61]. As pre-<br>sented in Table 9, shifted window fails to provide gains over<br>MixFormer. We hypothesize that the depth-wise convolu-<br>tion builds connections among windows, removing the need<br>for shift operation. Besides, although inserting 3 x 3 depth-<br>wise convolution in FFN can provide further gains, the<br>room for improvements is limited with MixFormer. Thus,<br>we use MLP in FFN by default.</p>",
      "id": 100,
      "page": 8,
      "text": "and inserting 3 x 3 depth-wise convolution in FFN, which\nplay significant roles in previous works [34, 61]. As pre-\nsented in Table 9, shifted window fails to provide gains over\nMixFormer. We hypothesize that the depth-wise convolu-\ntion builds connections among windows, removing the need\nfor shift operation. Besides, although inserting 3 x 3 depth-\nwise convolution in FFN can provide further gains, the\nroom for improvements is limited with MixFormer. Thus,\nwe use MLP in FFN by default."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 1571
        },
        {
          "x": 1198,
          "y": 1571
        },
        {
          "x": 1198,
          "y": 2018
        },
        {
          "x": 201,
          "y": 2018
        }
      ],
      "category": "paragraph",
      "html": "<p id='101' style='font-size:18px'>Ablation: Number of Blocks in Stages. Previous works<br>usually put more blocks in the third stage and greatly in-<br>crease the number of blocks in that stage when scaling mod-<br>els [19,34,49]. We show an alternative way that can achieve<br>the goal. We roughly conduct experiments on the way of<br>stacking blocks. In Table 10, we achieve slightly higher<br>performance on various vision tasks under less computa-<br>tional complexities by putting more blocks in both the last<br>two stages. We follow this recipe to build our MixFormer.</p>",
      "id": 101,
      "page": 8,
      "text": "Ablation: Number of Blocks in Stages. Previous works\nusually put more blocks in the third stage and greatly in-\ncrease the number of blocks in that stage when scaling mod-\nels [19,34,49]. We show an alternative way that can achieve\nthe goal. We roughly conduct experiments on the way of\nstacking blocks. In Table 10, we achieve slightly higher\nperformance on various vision tasks under less computa-\ntional complexities by putting more blocks in both the last\ntwo stages. We follow this recipe to build our MixFormer."
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 2052
        },
        {
          "x": 580,
          "y": 2052
        },
        {
          "x": 580,
          "y": 2099
        },
        {
          "x": 203,
          "y": 2099
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='102' style='font-size:20px'>4.5. Generalization</p>",
      "id": 102,
      "page": 8,
      "text": "4.5. Generalization"
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 2130
        },
        {
          "x": 1197,
          "y": 2130
        },
        {
          "x": 1197,
          "y": 2324
        },
        {
          "x": 202,
          "y": 2324
        }
      ],
      "category": "paragraph",
      "html": "<p id='103' style='font-size:18px'>More Downstream Tasks. In Table 11, we conduct exper-<br>iments on two more downstream tasks: keypoint detection<br>and long-tail instance segmentation. Detailed experimental<br>settings are provided in Appendix C.</p>",
      "id": 103,
      "page": 8,
      "text": "More Downstream Tasks. In Table 11, we conduct exper-\niments on two more downstream tasks: keypoint detection\nand long-tail instance segmentation. Detailed experimental\nsettings are provided in Appendix C."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 2331
        },
        {
          "x": 1198,
          "y": 2331
        },
        {
          "x": 1198,
          "y": 2572
        },
        {
          "x": 201,
          "y": 2572
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='104' style='font-size:18px'>COCO keypoint Detection: In Table 11, MixFormer-B4<br>outperforms baseline models [19,34] by significant margins<br>in all metrics. Moreover, MixFormer also shows clear ad-<br>vantages compared with HRFormer [61], which is specifi-<br>cally designed for dense prediction tasks.</p>",
      "id": 104,
      "page": 8,
      "text": "COCO keypoint Detection: In Table 11, MixFormer-B4\noutperforms baseline models [19,34] by significant margins\nin all metrics. Moreover, MixFormer also shows clear ad-\nvantages compared with HRFormer [61], which is specifi-\ncally designed for dense prediction tasks."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 2579
        },
        {
          "x": 1199,
          "y": 2579
        },
        {
          "x": 1199,
          "y": 2872
        },
        {
          "x": 201,
          "y": 2872
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='105' style='font-size:16px'>LVIS 1.0 Instance Segmentation: This task has ~ 1000<br>long-tailed distribution categories, which relies on the dis-<br>criminative feature learned by the backbone. Results in Ta-<br>ble 11 show that MixFormer outperforms the Swin-T [34]<br>by 1.0 APmask which demonstrates the robustness of the<br>,<br>learned representation in MixFormer.</p>",
      "id": 105,
      "page": 8,
      "text": "LVIS 1.0 Instance Segmentation: This task has ~ 1000\nlong-tailed distribution categories, which relies on the dis-\ncriminative feature learned by the backbone. Results in Ta-\nble 11 show that MixFormer outperforms the Swin-T [34]\nby 1.0 APmask which demonstrates the robustness of the\n,\nlearned representation in MixFormer."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 2880
        },
        {
          "x": 1198,
          "y": 2880
        },
        {
          "x": 1198,
          "y": 2975
        },
        {
          "x": 202,
          "y": 2975
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='106' style='font-size:20px'>Summary: Considering the promising results given by<br>MixFormer in previous tasks: object detection, instance</p>",
      "id": 106,
      "page": 8,
      "text": "Summary: Considering the promising results given by\nMixFormer in previous tasks: object detection, instance"
    },
    {
      "bounding_box": [
        {
          "x": 1287,
          "y": 294
        },
        {
          "x": 2265,
          "y": 294
        },
        {
          "x": 2265,
          "y": 727
        },
        {
          "x": 1287,
          "y": 727
        }
      ],
      "category": "table",
      "html": "<br><table id='107' style='font-size:14px'><tr><td>Models</td><td>FLOPs</td><td>Top-1</td><td>Top-5</td></tr><tr><td>ResNet50 [44]</td><td>4.1G</td><td>78.4</td><td>-</td></tr><tr><td>ResNet50 [51]</td><td>4.1G</td><td>79.8</td><td>-</td></tr><tr><td>ResNet50*</td><td>4.1G</td><td>79.0</td><td>94.3</td></tr><tr><td>ResNet50 + Mixing Block</td><td>3.9G</td><td>80.6 (+1.6)</td><td>95.1 (+0.8)</td></tr><tr><td>MobileNetV2 [41]</td><td>0.3G</td><td>72.0</td><td>-</td></tr><tr><td>MobileNetV2*</td><td>0.3G</td><td>71.7</td><td>90.3</td></tr><tr><td>MobileNetV2+SE+Non-Local\"</td><td>0.3G</td><td>72.5</td><td>91.0</td></tr><tr><td>MobileNetV2 + Mixing Block</td><td>0.3G</td><td>73.6 (+1.9)</td><td>91.6 (+1.3)</td></tr></table>",
      "id": 107,
      "page": 8,
      "text": "Models FLOPs Top-1 Top-5\n ResNet50 [44] 4.1G 78.4 -\n ResNet50 [51] 4.1G 79.8 -\n ResNet50* 4.1G 79.0 94.3\n ResNet50 + Mixing Block 3.9G 80.6 (+1.6) 95.1 (+0.8)\n MobileNetV2 [41] 0.3G 72.0 -\n MobileNetV2* 0.3G 71.7 90.3\n MobileNetV2+SE+Non-Local\" 0.3G 72.5 91.0\n MobileNetV2 + Mixing Block 0.3G 73.6 (+1.9)"
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 763
        },
        {
          "x": 2276,
          "y": 763
        },
        {
          "x": 2276,
          "y": 989
        },
        {
          "x": 1281,
          "y": 989
        }
      ],
      "category": "caption",
      "html": "<caption id='108' style='font-size:14px'>Table 12. Apply Mixing Block to ConvNets on ImageNet-1K.<br>We introduce our Mixing Block to typical ConvNets, ResNet [19]<br>and MobileNetV2 [41]. As different training recipes give variant<br>accuracy [51], we also train ResNet50 [19] and MobileNetV2 [41]<br>with the same setting as ours, denoted with * in the Table.</caption>",
      "id": 108,
      "page": 8,
      "text": "Table 12. Apply Mixing Block to ConvNets on ImageNet-1K.\nWe introduce our Mixing Block to typical ConvNets, ResNet [19]\nand MobileNetV2 [41]. As different training recipes give variant\naccuracy [51], we also train ResNet50 [19] and MobileNetV2 [41]\nwith the same setting as ours, denoted with * in the Table."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 1041
        },
        {
          "x": 2276,
          "y": 1041
        },
        {
          "x": 2276,
          "y": 1183
        },
        {
          "x": 1280,
          "y": 1183
        }
      ],
      "category": "paragraph",
      "html": "<p id='109' style='font-size:16px'>segmentation, and semantic segmentation, MixFomer can<br>serve as a general-purpose backbone and outperform its al-<br>ternatives in 5 dense prediction tasks.</p>",
      "id": 109,
      "page": 8,
      "text": "segmentation, and semantic segmentation, MixFomer can\nserve as a general-purpose backbone and outperform its al-\nternatives in 5 dense prediction tasks."
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 1216
        },
        {
          "x": 2277,
          "y": 1216
        },
        {
          "x": 2277,
          "y": 1861
        },
        {
          "x": 1279,
          "y": 1861
        }
      ],
      "category": "paragraph",
      "html": "<p id='110' style='font-size:18px'>Apply Mixing Block to ConvNets. We apply our Mix-<br>ing Block to typical ConvNets, ResNet50 [19] and Mo-<br>bileNetV2 [41]. Following [42], we replace all the blocks in<br>the last stage with our Mixing Block in ConvNets. To make<br>a fair comparison, we adjust the number of blocks to main-<br>tain the overall computational cost. Table 12 shows that<br>the Mixing Block can provide gains on ConvNets [19, 41]<br>as a alternative to ConvNet blocks. Specifically, Mixing<br>Block brings 1.9% and 1.6% Top-1 accuracy on ImageNet-<br>1K [9] over MobileNetV2 [41] and ResNet50 [19]. More-<br>over, we also provide the result of MobileNetV2 [41] with<br>SE layer [24] and Non-Local [50] in Table 12. It gives in-<br>feriror performance than our mixing block.</p>",
      "id": 110,
      "page": 8,
      "text": "Apply Mixing Block to ConvNets. We apply our Mix-\ning Block to typical ConvNets, ResNet50 [19] and Mo-\nbileNetV2 [41]. Following [42], we replace all the blocks in\nthe last stage with our Mixing Block in ConvNets. To make\na fair comparison, we adjust the number of blocks to main-\ntain the overall computational cost. Table 12 shows that\nthe Mixing Block can provide gains on ConvNets [19, 41]\nas a alternative to ConvNet blocks. Specifically, Mixing\nBlock brings 1.9% and 1.6% Top-1 accuracy on ImageNet-\n1K [9] over MobileNetV2 [41] and ResNet50 [19]. More-\nover, we also provide the result of MobileNetV2 [41] with\nSE layer [24] and Non-Local [50] in Table 12. It gives in-\nferiror performance than our mixing block."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 1911
        },
        {
          "x": 1587,
          "y": 1911
        },
        {
          "x": 1587,
          "y": 1961
        },
        {
          "x": 1280,
          "y": 1961
        }
      ],
      "category": "paragraph",
      "html": "<p id='111' style='font-size:22px'>5. Limitations</p>",
      "id": 111,
      "page": 8,
      "text": "5. Limitations"
    },
    {
      "bounding_box": [
        {
          "x": 1277,
          "y": 1995
        },
        {
          "x": 2278,
          "y": 1995
        },
        {
          "x": 2278,
          "y": 2692
        },
        {
          "x": 1277,
          "y": 2692
        }
      ],
      "category": "paragraph",
      "html": "<p id='112' style='font-size:18px'>Our MixFormer is proposed to mitigate the issues in<br>local-window self-attention [34, 45]. Thus it may be lim-<br>ited to window-based vision transformers in this paper. Al-<br>though the parallel design and the bi-directional interactions<br>can be applied to the global self-attention [11, 44], it is not<br>clear that how many gains can the above designs bring. We<br>conduct a simple experiment on DeiT-Tiny [44]. But the<br>result becomes slightly worse, as shown in Table 16. More<br>efforts are needed to apply our mixing block to global at-<br>tention. We leave this for future work. Moreover, we build<br>the MixFormer series manually, limiting MixFormer in ex-<br>isting instances. Other methods such as NAS (Network Ar-<br>chitecture Search) [43] can be applied to further improve<br>the results.</p>",
      "id": 112,
      "page": 8,
      "text": "Our MixFormer is proposed to mitigate the issues in\nlocal-window self-attention [34, 45]. Thus it may be lim-\nited to window-based vision transformers in this paper. Al-\nthough the parallel design and the bi-directional interactions\ncan be applied to the global self-attention [11, 44], it is not\nclear that how many gains can the above designs bring. We\nconduct a simple experiment on DeiT-Tiny [44]. But the\nresult becomes slightly worse, as shown in Table 16. More\nefforts are needed to apply our mixing block to global at-\ntention. We leave this for future work. Moreover, we build\nthe MixFormer series manually, limiting MixFormer in ex-\nisting instances. Other methods such as NAS (Network Ar-\nchitecture Search) [43] can be applied to further improve\nthe results."
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 2743
        },
        {
          "x": 1578,
          "y": 2743
        },
        {
          "x": 1578,
          "y": 2793
        },
        {
          "x": 1281,
          "y": 2793
        }
      ],
      "category": "paragraph",
      "html": "<p id='113' style='font-size:22px'>6. Conclusion</p>",
      "id": 113,
      "page": 8,
      "text": "6. Conclusion"
    },
    {
      "bounding_box": [
        {
          "x": 1282,
          "y": 2828
        },
        {
          "x": 2276,
          "y": 2828
        },
        {
          "x": 2276,
          "y": 2974
        },
        {
          "x": 1282,
          "y": 2974
        }
      ],
      "category": "paragraph",
      "html": "<p id='114' style='font-size:16px'>In this paper, we propose MixFormer as an efficient<br>general-purpose vision transformer. Addressing issues in<br>Window-based Vision Transformer, we seek to alleviate</p>",
      "id": 114,
      "page": 8,
      "text": "In this paper, we propose MixFormer as an efficient\ngeneral-purpose vision transformer. Addressing issues in\nWindow-based Vision Transformer, we seek to alleviate"
    },
    {
      "bounding_box": [
        {
          "x": 1226,
          "y": 3057
        },
        {
          "x": 1252,
          "y": 3057
        },
        {
          "x": 1252,
          "y": 3090
        },
        {
          "x": 1226,
          "y": 3090
        }
      ],
      "category": "footer",
      "html": "<footer id='115' style='font-size:16px'>8</footer>",
      "id": 115,
      "page": 8,
      "text": "8"
    },
    {
      "bounding_box": [
        {
          "x": 256,
          "y": 294
        },
        {
          "x": 1146,
          "y": 294
        },
        {
          "x": 1146,
          "y": 662
        },
        {
          "x": 256,
          "y": 662
        }
      ],
      "category": "table",
      "html": "<table id='116' style='font-size:16px'><tr><td>Models</td><td>#Channels</td><td>#Blocks</td><td>#Heads</td></tr><tr><td>MixFormer-B0</td><td>C = 24</td><td>[1,2, 6,6]</td><td>[3, 6,12, 24]</td></tr><tr><td>MixFormer-B1</td><td>C = 32</td><td>[1, 2, 6, 6]</td><td>[2, 4, 8,16]</td></tr><tr><td>MixFormer-B2</td><td>C = 32</td><td>[2, 2, 8, 8]</td><td>[2, 4, 8,16]</td></tr><tr><td>MixFormer-B3</td><td>C = 48</td><td>[2, 2, 8, 6]</td><td>[3, 6, 12, 24]</td></tr><tr><td>MixFormer-B4</td><td>C = 64</td><td>[2, 2, 8, 8]</td><td>[4, 8, 16, 32]</td></tr><tr><td>MixFormer-B5</td><td>C = 96</td><td>[1,2, 8, 6]</td><td>[6, 12, 24, 48]</td></tr><tr><td>MixFormer-B6</td><td>C = 96</td><td>[2, 4, 16, 12]</td><td>[6, 12, 24, 48]</td></tr></table>",
      "id": 116,
      "page": 9,
      "text": "Models #Channels #Blocks #Heads\n MixFormer-B0 C = 24 [1,2, 6,6] [3, 6,12, 24]\n MixFormer-B1 C = 32 [1, 2, 6, 6] [2, 4, 8,16]\n MixFormer-B2 C = 32 [2, 2, 8, 8] [2, 4, 8,16]\n MixFormer-B3 C = 48 [2, 2, 8, 6] [3, 6, 12, 24]\n MixFormer-B4 C = 64 [2, 2, 8, 8] [4, 8, 16, 32]\n MixFormer-B5 C = 96 [1,2, 8, 6] [6, 12, 24, 48]\n MixFormer-B6 C = 96 [2, 4, 16, 12]"
    },
    {
      "bounding_box": [
        {
          "x": 205,
          "y": 699
        },
        {
          "x": 1192,
          "y": 699
        },
        {
          "x": 1192,
          "y": 785
        },
        {
          "x": 205,
          "y": 785
        }
      ],
      "category": "caption",
      "html": "<caption id='117' style='font-size:14px'>Table 13. Architecture Variants. Detailed configurations of ar-<br>chitecture variants of MixFormer.</caption>",
      "id": 117,
      "page": 9,
      "text": "Table 13. Architecture Variants. Detailed configurations of ar-\nchitecture variants of MixFormer."
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 824
        },
        {
          "x": 1199,
          "y": 824
        },
        {
          "x": 1199,
          "y": 1420
        },
        {
          "x": 200,
          "y": 1420
        }
      ],
      "category": "paragraph",
      "html": "<p id='118' style='font-size:18px'>limited receptive fields and weak modeling capability on<br>the channel dimension. Our MixFormer enlarges recep-<br>tive fields efficiently without shifting or shuffling win-<br>dows, thanks to a parallel design coupling local window<br>and depth-wise convolution. The bi-directional interactions<br>boost modeling ability in the channel and spatial dimension<br>for local-window self-attention and depth-wise convolution,<br>respectively. Extensive experiments show that MixFormer<br>outperforms its alternatives on image classification and var-<br>ious downstream vision tasks. We expect the designs in<br>MixFormer to serve as a base setup for designing efficient<br>networks.</p>",
      "id": 118,
      "page": 9,
      "text": "limited receptive fields and weak modeling capability on\nthe channel dimension. Our MixFormer enlarges recep-\ntive fields efficiently without shifting or shuffling win-\ndows, thanks to a parallel design coupling local window\nand depth-wise convolution. The bi-directional interactions\nboost modeling ability in the channel and spatial dimension\nfor local-window self-attention and depth-wise convolution,\nrespectively. Extensive experiments show that MixFormer\noutperforms its alternatives on image classification and var-\nious downstream vision tasks. We expect the designs in\nMixFormer to serve as a base setup for designing efficient\nnetworks."
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 1471
        },
        {
          "x": 624,
          "y": 1471
        },
        {
          "x": 624,
          "y": 1523
        },
        {
          "x": 204,
          "y": 1523
        }
      ],
      "category": "paragraph",
      "html": "<p id='119' style='font-size:22px'>Acknowledgements</p>",
      "id": 119,
      "page": 9,
      "text": "Acknowledgements"
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 1557
        },
        {
          "x": 1197,
          "y": 1557
        },
        {
          "x": 1197,
          "y": 1753
        },
        {
          "x": 203,
          "y": 1753
        }
      ],
      "category": "paragraph",
      "html": "<p id='120' style='font-size:18px'>The authors would like to thank Jiaying Guo, Zhe Li,<br>and Fanrong Li for their helpful discussions and feedback.<br>This work was supported in part by National Natural Sci-<br>ence Foundation of China (No.62106267)</p>",
      "id": 120,
      "page": 9,
      "text": "The authors would like to thank Jiaying Guo, Zhe Li,\nand Fanrong Li for their helpful discussions and feedback.\nThis work was supported in part by National Natural Sci-\nence Foundation of China (No.62106267)"
    },
    {
      "bounding_box": [
        {
          "x": 205,
          "y": 1808
        },
        {
          "x": 417,
          "y": 1808
        },
        {
          "x": 417,
          "y": 1858
        },
        {
          "x": 205,
          "y": 1858
        }
      ],
      "category": "paragraph",
      "html": "<p id='121' style='font-size:22px'>Appendix</p>",
      "id": 121,
      "page": 9,
      "text": "Appendix"
    },
    {
      "bounding_box": [
        {
          "x": 205,
          "y": 1894
        },
        {
          "x": 896,
          "y": 1894
        },
        {
          "x": 896,
          "y": 1945
        },
        {
          "x": 205,
          "y": 1945
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='122' style='font-size:20px'>A. More Variants of MixFormer</p>",
      "id": 122,
      "page": 9,
      "text": "A. More Variants of MixFormer"
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 1978
        },
        {
          "x": 1200,
          "y": 1978
        },
        {
          "x": 1200,
          "y": 2423
        },
        {
          "x": 201,
          "y": 2423
        }
      ],
      "category": "paragraph",
      "html": "<p id='123' style='font-size:18px'>We scale our MixFormer to smaller and larger mod-<br>els. In this section, we provide two instantiated models<br>(MixFormer-B0 and MixFormer-B5). Their detailed set-<br>tings are provided in Table 13, along with previous methods<br>(from B1 to B4). Note that MixFormer-B0 and MixFormer-<br>B5 are two examples. More variants can be obtained with<br>further attempts following the design of MixFormer. Then,<br>we validate their effectiveness on ImageNet-1K [9]. The<br>results are illustrated in Table 14.</p>",
      "id": 123,
      "page": 9,
      "text": "We scale our MixFormer to smaller and larger mod-\nels. In this section, we provide two instantiated models\n(MixFormer-B0 and MixFormer-B5). Their detailed set-\ntings are provided in Table 13, along with previous methods\n(from B1 to B4). Note that MixFormer-B0 and MixFormer-\nB5 are two examples. More variants can be obtained with\nfurther attempts following the design of MixFormer. Then,\nwe validate their effectiveness on ImageNet-1K [9]. The\nresults are illustrated in Table 14."
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 2430
        },
        {
          "x": 1199,
          "y": 2430
        },
        {
          "x": 1199,
          "y": 2977
        },
        {
          "x": 200,
          "y": 2977
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='124' style='font-size:18px'>On one side, MixFormer-B0 achieves competitive re-<br>sult (76.5% Top-1 accuracy on ImageNet-1K [9]) even with<br>0.4G FLOPs, which lies in the mobile level [37,41]. While<br>other vision transformer variants [34, 48, 49, 53, 57] did not<br>provide a range of model sizes like our MixFormer, espe-<br>cially in mobile level. We believe that further efforts can<br>be made to give higher performance to achieve state-of-the-<br>art results [21, 43] in mobile level models. On the other<br>side, MixFormer-B5 shows an example to scale our Mix-<br>Former to larger models. It has 6.8G FLOPs, while it can<br>achieve on par results with Swin-B (15.4G) [34], Focal-</p>",
      "id": 124,
      "page": 9,
      "text": "On one side, MixFormer-B0 achieves competitive re-\nsult (76.5% Top-1 accuracy on ImageNet-1K [9]) even with\n0.4G FLOPs, which lies in the mobile level [37,41]. While\nother vision transformer variants [34, 48, 49, 53, 57] did not\nprovide a range of model sizes like our MixFormer, espe-\ncially in mobile level. We believe that further efforts can\nbe made to give higher performance to achieve state-of-the-\nart results [21, 43] in mobile level models. On the other\nside, MixFormer-B5 shows an example to scale our Mix-\nFormer to larger models. It has 6.8G FLOPs, while it can\nachieve on par results with Swin-B (15.4G) [34], Focal-"
    },
    {
      "bounding_box": [
        {
          "x": 1388,
          "y": 287
        },
        {
          "x": 2168,
          "y": 287
        },
        {
          "x": 2168,
          "y": 2061
        },
        {
          "x": 1388,
          "y": 2061
        }
      ],
      "category": "table",
      "html": "<br><table id='125' style='font-size:14px'><tr><td>Method</td><td>#Params</td><td>FLOPs</td><td>Top-1</td></tr><tr><td colspan=\"4\">ConvNets</td></tr><tr><td>RegNetY-4G [40]</td><td>21M</td><td>4.0G</td><td>80.0</td></tr><tr><td>RegNetY-8G [40]</td><td>39M</td><td>8.0G</td><td>81.7</td></tr><tr><td>RegNetY-16G [40]</td><td>84M</td><td>16.0G</td><td>82.9</td></tr><tr><td>EffNet-B0 [43]</td><td>5M</td><td>0.4G</td><td>77.1</td></tr><tr><td>EffNet-B1 [43]</td><td>8M</td><td>0.7G</td><td>79.1</td></tr><tr><td>EffNet-B2 [43]</td><td>9M</td><td>1.0G</td><td>80.1</td></tr><tr><td>EffNet-B3 [43]</td><td>12M</td><td>1.8G</td><td>81.6</td></tr><tr><td>EffNet-B4 [43]</td><td>19M</td><td>4.2G</td><td>82.9</td></tr><tr><td>EffNet-B5 [43]</td><td>30M</td><td>9.9G</td><td>83.6</td></tr><tr><td colspan=\"4\">Vision Transformers</td></tr><tr><td>DeiT-T [44]</td><td>6M</td><td>1.3G</td><td>72.2</td></tr><tr><td>DeiT-S [44]</td><td>22M</td><td>4.6G</td><td>79.9</td></tr><tr><td>DeiT-B [44]</td><td>87M</td><td>17.5G</td><td>81.8</td></tr><tr><td>PVT-T [49]</td><td>13M</td><td>1.8G</td><td>75.1</td></tr><tr><td>PVT-S [49]</td><td>25M</td><td>3.8G</td><td>79.8</td></tr><tr><td>PVT-M [49]</td><td>44M</td><td>6.7G</td><td>81.2</td></tr><tr><td>PVT-L [49]</td><td>61M</td><td>9.8G</td><td>81.7</td></tr><tr><td>CvT-13 [53]</td><td>20M</td><td>4.5G</td><td>81.6</td></tr><tr><td>CvT-21 [53]</td><td>32M</td><td>7.1G</td><td>82.5</td></tr><tr><td>TwinsP-S [6]</td><td>24M</td><td>3.8G</td><td>81.2</td></tr><tr><td>DS-Net-S [38]</td><td>23M</td><td>3.5G</td><td>82.3</td></tr><tr><td>Swin-T [34]</td><td>29M</td><td>4.5G</td><td>81.3</td></tr><tr><td>Swin-S [34]</td><td>50M</td><td>8.7G</td><td>83.0</td></tr><tr><td>Swin-B [34]</td><td>88M</td><td>15.4G</td><td>83.5</td></tr><tr><td>Twins-S [6]</td><td>24M</td><td>2.9G</td><td>81.7</td></tr><tr><td>Twins-B [6]</td><td>56M</td><td>8.6G</td><td>83.2</td></tr><tr><td>LG-T [31]</td><td>33M</td><td>4.8G</td><td>82.1</td></tr><tr><td>LG-S [31]</td><td>61M</td><td>9.4G</td><td>83.3</td></tr><tr><td>Focal-T [57]</td><td>29M</td><td>4.9G</td><td>82.2</td></tr><tr><td>Focal-S [57]</td><td>51M</td><td>9.1G</td><td>83.5</td></tr><tr><td>Shuffle-T [26]</td><td>29M</td><td>4.6G</td><td>82.5</td></tr><tr><td>Shuffle-S [26]</td><td>50M</td><td>8.9G</td><td>83.5</td></tr><tr><td>MixFormer-B0 (Ours)</td><td>5M</td><td>0.4G</td><td>76.5</td></tr><tr><td>MixFormer-B1 (Ours)</td><td>8M</td><td>0.7G</td><td>78.9</td></tr><tr><td>MixFormer-B2 (Ours)</td><td>10M</td><td>0.9G</td><td>80.0</td></tr><tr><td>MixFormer-B3 (Ours)</td><td>17M</td><td>1.9G</td><td>81.7</td></tr><tr><td>MixFormer-B4 (Ours)</td><td>35M</td><td>3.6G</td><td>83.0</td></tr><tr><td>MixFormer-B5 (Ours)</td><td>62M</td><td>6.8G</td><td>83.5</td></tr><tr><td>MixFormer-B6 (Ours)</td><td>119M</td><td>12.7G</td><td>83.8</td></tr></table>",
      "id": 125,
      "page": 9,
      "text": "Method #Params FLOPs Top-1\n ConvNets\n RegNetY-4G [40] 21M 4.0G 80.0\n RegNetY-8G [40] 39M 8.0G 81.7\n RegNetY-16G [40] 84M 16.0G 82.9\n EffNet-B0 [43] 5M 0.4G 77.1\n EffNet-B1 [43] 8M 0.7G 79.1\n EffNet-B2 [43] 9M 1.0G 80.1\n EffNet-B3 [43] 12M 1.8G 81.6\n EffNet-B4 [43] 19M 4.2G 82.9\n EffNet-B5 [43] 30M 9.9G 83.6\n Vision Transformers\n DeiT-T [44] 6M 1.3G 72.2\n DeiT-S [44] 22M 4.6G 79.9\n DeiT-B [44] 87M 17.5G 81.8\n PVT-T [49] 13M 1.8G 75.1\n PVT-S [49] 25M 3.8G 79.8\n PVT-M [49] 44M 6.7G 81.2\n PVT-L [49] 61M 9.8G 81.7\n CvT-13 [53] 20M 4.5G 81.6\n CvT-21 [53] 32M 7.1G 82.5\n TwinsP-S [6] 24M 3.8G 81.2\n DS-Net-S [38] 23M 3.5G 82.3\n Swin-T [34] 29M 4.5G 81.3\n Swin-S [34] 50M 8.7G 83.0\n Swin-B [34] 88M 15.4G 83.5\n Twins-S [6] 24M 2.9G 81.7\n Twins-B [6] 56M 8.6G 83.2\n LG-T [31] 33M 4.8G 82.1\n LG-S [31] 61M 9.4G 83.3\n Focal-T [57] 29M 4.9G 82.2\n Focal-S [57] 51M 9.1G 83.5\n Shuffle-T [26] 29M 4.6G 82.5\n Shuffle-S [26] 50M 8.9G 83.5\n MixFormer-B0 (Ours) 5M 0.4G 76.5\n MixFormer-B1 (Ours) 8M 0.7G 78.9\n MixFormer-B2 (Ours) 10M 0.9G 80.0\n MixFormer-B3 (Ours) 17M 1.9G 81.7\n MixFormer-B4 (Ours) 35M 3.6G 83.0\n MixFormer-B5 (Ours) 62M 6.8G 83.5\n MixFormer-B6 (Ours) 119M 12.7G"
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 2094
        },
        {
          "x": 2276,
          "y": 2094
        },
        {
          "x": 2276,
          "y": 2276
        },
        {
          "x": 1280,
          "y": 2276
        }
      ],
      "category": "caption",
      "html": "<caption id='126' style='font-size:14px'>Table 14. Classification accuracy on the ImageNet validation<br>set. Performances are measured with a single 224 x 224 crop.<br>\"Params\" refers to the number of parameters. \"FLOPs\" is calcu-<br>lated under the input scale of 224 x 224.</caption>",
      "id": 126,
      "page": 9,
      "text": "Table 14. Classification accuracy on the ImageNet validation\nset. Performances are measured with a single 224 x 224 crop.\n\"Params\" refers to the number of parameters. \"FLOPs\" is calcu-\nlated under the input scale of 224 x 224."
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 2302
        },
        {
          "x": 2277,
          "y": 2302
        },
        {
          "x": 2277,
          "y": 2597
        },
        {
          "x": 1278,
          "y": 2597
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='127' style='font-size:18px'>S (9.1G) [57], Shuffle-S (8.9G) [26], and EfficientNet-B5<br>(9.9G) [43], which demonstrates the computational effi-<br>ciency of MixFormer. MixFormer-B6 achieves 83.8% top-<br>1 accuracy on ImageNet-1K [9]. It maintains the superior<br>performance to Swin-B(15.4G) [34] and is comparable to<br>other models with less flops.</p>",
      "id": 127,
      "page": 9,
      "text": "S (9.1G) [57], Shuffle-S (8.9G) [26], and EfficientNet-B5\n(9.9G) [43], which demonstrates the computational effi-\nciency of MixFormer. MixFormer-B6 achieves 83.8% top-\n1 accuracy on ImageNet-1K [9]. It maintains the superior\nperformance to Swin-B(15.4G) [34] and is comparable to\nother models with less flops."
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 2604
        },
        {
          "x": 2277,
          "y": 2604
        },
        {
          "x": 2277,
          "y": 2750
        },
        {
          "x": 1281,
          "y": 2750
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='128' style='font-size:18px'>The above results verify the scalability of MixFormer to<br>smaller and larger models. Moreover, it has the potential for<br>further improvements.</p>",
      "id": 128,
      "page": 9,
      "text": "The above results verify the scalability of MixFormer to\nsmaller and larger models. Moreover, it has the potential for\nfurther improvements."
    },
    {
      "bounding_box": [
        {
          "x": 1282,
          "y": 2792
        },
        {
          "x": 1860,
          "y": 2792
        },
        {
          "x": 1860,
          "y": 2845
        },
        {
          "x": 1282,
          "y": 2845
        }
      ],
      "category": "paragraph",
      "html": "<p id='129' style='font-size:20px'>B. Additional Experiments</p>",
      "id": 129,
      "page": 9,
      "text": "B. Additional Experiments"
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 2876
        },
        {
          "x": 2277,
          "y": 2876
        },
        {
          "x": 2277,
          "y": 2974
        },
        {
          "x": 1281,
          "y": 2974
        }
      ],
      "category": "paragraph",
      "html": "<p id='130' style='font-size:16px'>Window Sizes in Local- Window Self-Attention. We con-<br>duct ablation study on the window size in local-window</p>",
      "id": 130,
      "page": 9,
      "text": "Window Sizes in Local- Window Self-Attention. We con-\nduct ablation study on the window size in local-window"
    },
    {
      "bounding_box": [
        {
          "x": 1225,
          "y": 3056
        },
        {
          "x": 1253,
          "y": 3056
        },
        {
          "x": 1253,
          "y": 3089
        },
        {
          "x": 1225,
          "y": 3089
        }
      ],
      "category": "footer",
      "html": "<footer id='131' style='font-size:16px'>9</footer>",
      "id": 131,
      "page": 9,
      "text": "9"
    },
    {
      "bounding_box": [
        {
          "x": 456,
          "y": 296
        },
        {
          "x": 944,
          "y": 296
        },
        {
          "x": 944,
          "y": 490
        },
        {
          "x": 456,
          "y": 490
        }
      ],
      "category": "table",
      "html": "<table id='132' style='font-size:16px'><tr><td rowspan=\"2\">Window Sizes</td><td colspan=\"2\">ImageNet</td></tr><tr><td>Top-1</td><td>Top-5</td></tr><tr><td>7 x 7</td><td>78.4</td><td>94.3</td></tr><tr><td>12 x 12</td><td>78.4</td><td>94.5</td></tr></table>",
      "id": 132,
      "page": 10,
      "text": "Window Sizes ImageNet\n Top-1 Top-5\n 7 x 7 78.4 94.3\n 12 x 12 78.4"
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 534
        },
        {
          "x": 1197,
          "y": 534
        },
        {
          "x": 1197,
          "y": 667
        },
        {
          "x": 203,
          "y": 667
        }
      ],
      "category": "caption",
      "html": "<caption id='133' style='font-size:14px'>Table 15. Window Sizes in Local-window Self-attention. We<br>investigate various window sizes for Local-window Self-attention<br>in MixFormer.</caption>",
      "id": 133,
      "page": 10,
      "text": "Table 15. Window Sizes in Local-window Self-attention. We\ninvestigate various window sizes for Local-window Self-attention\nin MixFormer."
    },
    {
      "bounding_box": [
        {
          "x": 384,
          "y": 694
        },
        {
          "x": 1011,
          "y": 694
        },
        {
          "x": 1011,
          "y": 898
        },
        {
          "x": 384,
          "y": 898
        }
      ],
      "category": "table",
      "html": "<br><table id='134' style='font-size:20px'><tr><td rowspan=\"2\">DeiT-Tiny [44]</td><td colspan=\"2\">ImageNet</td></tr><tr><td>Top-1</td><td>Top-5</td></tr><tr><td>Baseline</td><td>72.2</td><td>91.1</td></tr><tr><td>Baseline+Mixing Block</td><td>71.3</td><td>90.5</td></tr></table>",
      "id": 134,
      "page": 10,
      "text": "DeiT-Tiny [44] ImageNet\n Top-1 Top-5\n Baseline 72.2 91.1\n Baseline+Mixing Block 71.3"
    },
    {
      "bounding_box": [
        {
          "x": 206,
          "y": 937
        },
        {
          "x": 1195,
          "y": 937
        },
        {
          "x": 1195,
          "y": 1023
        },
        {
          "x": 206,
          "y": 1023
        }
      ],
      "category": "caption",
      "html": "<caption id='135' style='font-size:16px'>Table 16. Apply Mixing Block to DeiT-Tiny. We apply our mix-<br>ing block to global attention.</caption>",
      "id": 135,
      "page": 10,
      "text": "Table 16. Apply Mixing Block to DeiT-Tiny. We apply our mix-\ning block to global attention."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 1099
        },
        {
          "x": 1197,
          "y": 1099
        },
        {
          "x": 1197,
          "y": 1441
        },
        {
          "x": 202,
          "y": 1441
        }
      ],
      "category": "paragraph",
      "html": "<p id='136' style='font-size:18px'>self-attention with MixFormer-B1. The experimental set-<br>tings are follow the ones in the ablation studies. The results<br>in Table 15 show that larger window size (ws=12) achieves<br>on par performance with ws=7 (78.4%) on ImageNet-<br>1K [9]. Based on the above result, We follow the con- ven-<br>tional design of Swin Transformer (ws=7) [34] in all vari-<br>ants of MixFormer.</p>",
      "id": 136,
      "page": 10,
      "text": "self-attention with MixFormer-B1. The experimental set-\ntings are follow the ones in the ablation studies. The results\nin Table 15 show that larger window size (ws=12) achieves\non par performance with ws=7 (78.4%) on ImageNet-\n1K [9]. Based on the above result, We follow the con- ven-\ntional design of Swin Transformer (ws=7) [34] in all vari-\nants of MixFormer."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 1479
        },
        {
          "x": 1198,
          "y": 1479
        },
        {
          "x": 1198,
          "y": 1973
        },
        {
          "x": 202,
          "y": 1973
        }
      ],
      "category": "paragraph",
      "html": "<p id='137' style='font-size:20px'>Apply Mixing Block to DeiT. Although our mixing block<br>is proposed to solve the window connection problem in<br>local-window self-attention [34]. It can also be applied<br>to global attentions [11, 44]. We simply apply our mix-<br>ing block to Deit-Tiny [44]. But the result is slightly lower<br>than baseline (71.3% VS. 72.2%) on ImageNet-1K [9]. We<br>conjecture that global attention (ViT-based model) may not<br>share the same problem and detailed design for global atten-<br>tion may need further investigating. We leave this for future<br>work.</p>",
      "id": 137,
      "page": 10,
      "text": "Apply Mixing Block to DeiT. Although our mixing block\nis proposed to solve the window connection problem in\nlocal-window self-attention [34]. It can also be applied\nto global attentions [11, 44]. We simply apply our mix-\ning block to Deit-Tiny [44]. But the result is slightly lower\nthan baseline (71.3% VS. 72.2%) on ImageNet-1K [9]. We\nconjecture that global attention (ViT-based model) may not\nshare the same problem and detailed design for global atten-\ntion may need further investigating. We leave this for future\nwork."
    },
    {
      "bounding_box": [
        {
          "x": 205,
          "y": 2029
        },
        {
          "x": 935,
          "y": 2029
        },
        {
          "x": 935,
          "y": 2080
        },
        {
          "x": 205,
          "y": 2080
        }
      ],
      "category": "paragraph",
      "html": "<p id='138' style='font-size:22px'>C. Detailed Experimental Settings</p>",
      "id": 138,
      "page": 10,
      "text": "C. Detailed Experimental Settings"
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 2114
        },
        {
          "x": 1198,
          "y": 2114
        },
        {
          "x": 1198,
          "y": 2412
        },
        {
          "x": 202,
          "y": 2412
        }
      ],
      "category": "paragraph",
      "html": "<p id='139' style='font-size:20px'>Successive Design and Parallel Design. In Figure 4, we<br>give the details on how to combine local-window self-<br>attention and depth-wise convolution in the successive de-<br>sign and the parallel design. To make a fair comparison, we<br>adjust the channels in the blocks to keep the computational<br>complexity the same in the two designs.</p>",
      "id": 139,
      "page": 10,
      "text": "Successive Design and Parallel Design. In Figure 4, we\ngive the details on how to combine local-window self-\nattention and depth-wise convolution in the successive de-\nsign and the parallel design. To make a fair comparison, we\nadjust the channels in the blocks to keep the computational\ncomplexity the same in the two designs."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 2446
        },
        {
          "x": 1198,
          "y": 2446
        },
        {
          "x": 1198,
          "y": 2643
        },
        {
          "x": 201,
          "y": 2643
        }
      ],
      "category": "paragraph",
      "html": "<p id='140' style='font-size:20px'>Image Classification. We train all models for 300 epochs<br>with an image size of 224 x 224 on ImageNet-1K [9]. We<br>adjust the training settings gently when training models in<br>different sizes. The detailed setting is in Table 17.</p>",
      "id": 140,
      "page": 10,
      "text": "Image Classification. We train all models for 300 epochs\nwith an image size of 224 x 224 on ImageNet-1K [9]. We\nadjust the training settings gently when training models in\ndifferent sizes. The detailed setting is in Table 17."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 2678
        },
        {
          "x": 1199,
          "y": 2678
        },
        {
          "x": 1199,
          "y": 2975
        },
        {
          "x": 202,
          "y": 2975
        }
      ],
      "category": "paragraph",
      "html": "<p id='141' style='font-size:20px'>Object Detection and Instance Segmentation. When<br>transferring MixFormer to object detection and instance<br>segmentation on MS COCO [33], we consider two typi-<br>cal frameworks: Mask R-CNN [18] and Cascade Mask R-<br>CNN [3, 18]. We adopt Adam W [36] optimizer with an ini-<br>tial learning rate of 0.0002 and a batch size of 16. To make</p>",
      "id": 141,
      "page": 10,
      "text": "Object Detection and Instance Segmentation. When\ntransferring MixFormer to object detection and instance\nsegmentation on MS COCO [33], we consider two typi-\ncal frameworks: Mask R-CNN [18] and Cascade Mask R-\nCNN [3, 18]. We adopt Adam W [36] optimizer with an ini-\ntial learning rate of 0.0002 and a batch size of 16. To make"
    },
    {
      "bounding_box": [
        {
          "x": 1362,
          "y": 299
        },
        {
          "x": 2205,
          "y": 299
        },
        {
          "x": 2205,
          "y": 1017
        },
        {
          "x": 1362,
          "y": 1017
        }
      ],
      "category": "figure",
      "html": "<br><figure><img id='142' style='font-size:14px' alt=\"Input Features Input Features\n\nDepth-wise\nConvolution Local-window Depth-wise\nSelf-attention Convolution\nLocal-window\nConcat\nSelf-attention\nFFN FFN\nOutput Features Output Features\n(a) Successive Design (b) Parallel Design\" data-coord=\"top-left:(1362,299); bottom-right:(2205,1017)\" /></figure>",
      "id": 142,
      "page": 10,
      "text": "Input Features Input Features\n\nDepth-wise\nConvolution Local-window Depth-wise\nSelf-attention Convolution\nLocal-window\nConcat\nSelf-attention\nFFN FFN\nOutput Features Output Features\n(a) Successive Design (b) Parallel Design"
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 1068
        },
        {
          "x": 2277,
          "y": 1068
        },
        {
          "x": 2277,
          "y": 1293
        },
        {
          "x": 1279,
          "y": 1293
        }
      ],
      "category": "caption",
      "html": "<caption id='143' style='font-size:14px'>Figure 4. Successive Design and Parallel Design. We combine<br>local-window self-attention with depth-wise convolution in two<br>different ways. Other details in the block, such as module de-<br>sign, normalization layers, and shortcuts, are omitted for a neat<br>presentation.</caption>",
      "id": 143,
      "page": 10,
      "text": "Figure 4. Successive Design and Parallel Design. We combine\nlocal-window self-attention with depth-wise convolution in two\ndifferent ways. Other details in the block, such as module de-\nsign, normalization layers, and shortcuts, are omitted for a neat\npresentation."
    },
    {
      "bounding_box": [
        {
          "x": 1378,
          "y": 1336
        },
        {
          "x": 2187,
          "y": 1336
        },
        {
          "x": 2187,
          "y": 1929
        },
        {
          "x": 1378,
          "y": 1929
        }
      ],
      "category": "table",
      "html": "<table id='144' style='font-size:14px'><tr><td>config</td><td>value</td></tr><tr><td>optimizer</td><td>AdamW [36]</td></tr><tr><td>base learning rate</td><td>8e-4 (B0-B3), 1e-3 (B4, B5, B6)</td></tr><tr><td>weight decay</td><td>0.04 (B0-B3), 0.05 (B4, B5, B6)</td></tr><tr><td>optimizer momentum</td><td>B1, B2=0.9, 0.999</td></tr><tr><td>batch size</td><td>1024</td></tr><tr><td>learning rate schedule</td><td>cosine decay [35]</td></tr><tr><td>minimum learning rate</td><td>1e-6</td></tr><tr><td>warmup epochs</td><td>20 (B0-B4), 40 (B5, B6)</td></tr><tr><td>warmup learning rate</td><td>1e-7</td></tr><tr><td>training epochs</td><td>300</td></tr><tr><td>augmentation</td><td>RandAug(9, 0.5) [7]</td></tr><tr><td>color jitter</td><td>0.4</td></tr><tr><td>mixup [63]</td><td>0.2</td></tr><tr><td>cutmix [62]</td><td>1.0</td></tr><tr><td>random erasing [65]</td><td>0.25</td></tr><tr><td>drop path [25]</td><td>[0.0, 0.05,0.1,0.2,0.3,0.5] (B0-B6)</td></tr></table>",
      "id": 144,
      "page": 10,
      "text": "config value\n optimizer AdamW [36]\n base learning rate 8e-4 (B0-B3), 1e-3 (B4, B5, B6)\n weight decay 0.04 (B0-B3), 0.05 (B4, B5, B6)\n optimizer momentum B1, B2=0.9, 0.999\n batch size 1024\n learning rate schedule cosine decay [35]\n minimum learning rate 1e-6\n warmup epochs 20 (B0-B4), 40 (B5, B6)\n warmup learning rate 1e-7\n training epochs 300\n augmentation RandAug(9, 0.5) [7]\n color jitter 0.4\n mixup [63] 0.2\n cutmix [62] 1.0\n random erasing [65] 0.25\n drop path [25]"
    },
    {
      "bounding_box": [
        {
          "x": 1397,
          "y": 1962
        },
        {
          "x": 2161,
          "y": 1962
        },
        {
          "x": 2161,
          "y": 2001
        },
        {
          "x": 1397,
          "y": 2001
        }
      ],
      "category": "caption",
      "html": "<caption id='145' style='font-size:20px'>Table 17. Image Classification Training Settings.</caption>",
      "id": 145,
      "page": 10,
      "text": "Table 17. Image Classification Training Settings."
    },
    {
      "bounding_box": [
        {
          "x": 1333,
          "y": 2033
        },
        {
          "x": 2224,
          "y": 2033
        },
        {
          "x": 2224,
          "y": 2412
        },
        {
          "x": 1333,
          "y": 2412
        }
      ],
      "category": "table",
      "html": "<table id='146' style='font-size:14px'><tr><td>config</td><td>value</td></tr><tr><td>optimizer</td><td>AdamW</td></tr><tr><td>base learning rate</td><td>0.0002</td></tr><tr><td>weight decay</td><td>0.04 (B0-B3), 0.05 (B4, B5)</td></tr><tr><td>optimizer momentum</td><td>B1, B2=0.9, 0.999</td></tr><tr><td>batch size</td><td>16</td></tr><tr><td>learning rate schedule</td><td>steps:[8, 11] (1x), [27,33] (3x)</td></tr><tr><td>warmup iterations (ratio)</td><td>500 (0.001)</td></tr><tr><td>training epochs</td><td>12 (1x), 36 (3x)</td></tr><tr><td>scales</td><td>(800, 1333) (1x), Multi-scales [34] (3x)</td></tr><tr><td>drop path</td><td>0.0 (B0-B3), 0.1 (B4, B5)</td></tr></table>",
      "id": 146,
      "page": 10,
      "text": "config value\n optimizer AdamW\n base learning rate 0.0002\n weight decay 0.04 (B0-B3), 0.05 (B4, B5)\n optimizer momentum B1, B2=0.9, 0.999\n batch size 16\n learning rate schedule steps:[8, 11] (1x), [27,33] (3x)\n warmup iterations (ratio) 500 (0.001)\n training epochs 12 (1x), 36 (3x)\n scales (800, 1333) (1x), Multi-scales [34] (3x)\n drop path"
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 2455
        },
        {
          "x": 2272,
          "y": 2455
        },
        {
          "x": 2272,
          "y": 2541
        },
        {
          "x": 1281,
          "y": 2541
        }
      ],
      "category": "caption",
      "html": "<caption id='147' style='font-size:20px'>Table 18. Object Detection and Instance Segmentation Train-<br>ing Settings.</caption>",
      "id": 147,
      "page": 10,
      "text": "Table 18. Object Detection and Instance Segmentation Train-\ning Settings."
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 2586
        },
        {
          "x": 2275,
          "y": 2586
        },
        {
          "x": 2275,
          "y": 2883
        },
        {
          "x": 1279,
          "y": 2883
        }
      ],
      "category": "paragraph",
      "html": "<p id='148' style='font-size:20px'>a fair comparison with other works, we make all normaliza-<br>tion layers trainable in MixFormer. When training differ-<br>ent sizes of models, we adjust the training settings gently<br>according to their settings used in image classification. Ta-<br>ble 18 shows the detailed hyper-parameters used in training<br>models on MS COCO [33].</p>",
      "id": 148,
      "page": 10,
      "text": "a fair comparison with other works, we make all normaliza-\ntion layers trainable in MixFormer. When training differ-\nent sizes of models, we adjust the training settings gently\naccording to their settings used in image classification. Ta-\nble 18 shows the detailed hyper-parameters used in training\nmodels on MS COCO [33]."
    },
    {
      "bounding_box": [
        {
          "x": 1327,
          "y": 2931
        },
        {
          "x": 2231,
          "y": 2931
        },
        {
          "x": 2231,
          "y": 2971
        },
        {
          "x": 1327,
          "y": 2971
        }
      ],
      "category": "paragraph",
      "html": "<p id='149' style='font-size:14px'>4 Wherever BN is applied, we use synchronous BN across all GPUs.</p>",
      "id": 149,
      "page": 10,
      "text": "4 Wherever BN is applied, we use synchronous BN across all GPUs."
    },
    {
      "bounding_box": [
        {
          "x": 1219,
          "y": 3052
        },
        {
          "x": 1264,
          "y": 3052
        },
        {
          "x": 1264,
          "y": 3093
        },
        {
          "x": 1219,
          "y": 3093
        }
      ],
      "category": "footer",
      "html": "<footer id='150' style='font-size:18px'>10</footer>",
      "id": 150,
      "page": 10,
      "text": "10"
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 307
        },
        {
          "x": 1199,
          "y": 307
        },
        {
          "x": 1199,
          "y": 901
        },
        {
          "x": 200,
          "y": 901
        }
      ],
      "category": "paragraph",
      "html": "<p id='151' style='font-size:18px'>Semantic Segmentation. On ADE20K [66], we use the<br>AdamW optimizer [36] with an initial learning rate<br>0.00006, a weight decay 0.01, and a batch size of<br>16. We train all models for 160K on ADE20K. For<br>testing, we report the results with single-scale testing<br>and multi-scale testing on main comparisons, while we<br>only give single-scale testing results on ablation stud-<br>ies. In multi-scale testing, the resolutions used are the<br>[0.5, 0.75, 1.0, 1.25, 1.5, 1.75]x of that in training. The set-<br>tings mainly follow [34]. For the path drop rates in differ-<br>ent models, we adopt the same hyper-parameters as in MS<br>COCO [33].</p>",
      "id": 151,
      "page": 11,
      "text": "Semantic Segmentation. On ADE20K [66], we use the\nAdamW optimizer [36] with an initial learning rate\n0.00006, a weight decay 0.01, and a batch size of\n16. We train all models for 160K on ADE20K. For\ntesting, we report the results with single-scale testing\nand multi-scale testing on main comparisons, while we\nonly give single-scale testing results on ablation stud-\nies. In multi-scale testing, the resolutions used are the\n[0.5, 0.75, 1.0, 1.25, 1.5, 1.75]x of that in training. The set-\ntings mainly follow [34]. For the path drop rates in differ-\nent models, we adopt the same hyper-parameters as in MS\nCOCO [33]."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 924
        },
        {
          "x": 1199,
          "y": 924
        },
        {
          "x": 1199,
          "y": 1220
        },
        {
          "x": 201,
          "y": 1220
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='152' style='font-size:16px'>Keypoint Detection. We conduct experiments on the MS<br>COCO human pose estimation benchmark. We train the<br>models for 210 epochs with an AdamW optimizer, an image<br>size of 256 x 192, and a batch size of 256. The training and<br>evaluation hyper-parameters are mostly following the ones<br>in HRFormer [61].</p>",
      "id": 152,
      "page": 11,
      "text": "Keypoint Detection. We conduct experiments on the MS\nCOCO human pose estimation benchmark. We train the\nmodels for 210 epochs with an AdamW optimizer, an image\nsize of 256 x 192, and a batch size of 256. The training and\nevaluation hyper-parameters are mostly following the ones\nin HRFormer [61]."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 1242
        },
        {
          "x": 1200,
          "y": 1242
        },
        {
          "x": 1200,
          "y": 1590
        },
        {
          "x": 201,
          "y": 1590
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='153' style='font-size:18px'>Long-tail Instance Segmentation. We use the hyper-<br>parameters of Mask R-CNN [18] on MS COCO [33]<br>when training models for long-tail instance segmentation<br>on LVIS [15]. We report the results with a 1x schedule.<br>The training augmentations and sampling methods are the<br>same for all models, which adopt a multi-scale training and<br>use balanced sampling by following [15].</p>",
      "id": 153,
      "page": 11,
      "text": "Long-tail Instance Segmentation. We use the hyper-\nparameters of Mask R-CNN [18] on MS COCO [33]\nwhen training models for long-tail instance segmentation\non LVIS [15]. We report the results with a 1x schedule.\nThe training augmentations and sampling methods are the\nsame for all models, which adopt a multi-scale training and\nuse balanced sampling by following [15]."
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 1634
        },
        {
          "x": 931,
          "y": 1634
        },
        {
          "x": 931,
          "y": 1684
        },
        {
          "x": 204,
          "y": 1684
        }
      ],
      "category": "paragraph",
      "html": "<p id='154' style='font-size:22px'>D. Discussion with Related Works</p>",
      "id": 154,
      "page": 11,
      "text": "D. Discussion with Related Works"
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 1719
        },
        {
          "x": 1197,
          "y": 1719
        },
        {
          "x": 1197,
          "y": 1812
        },
        {
          "x": 202,
          "y": 1812
        }
      ],
      "category": "paragraph",
      "html": "<p id='155' style='font-size:16px'>In MixFormer, we consider two types of information ex-<br>changes: (1) across dimensions, (2) across windows.</p>",
      "id": 155,
      "page": 11,
      "text": "In MixFormer, we consider two types of information ex-\nchanges: (1) across dimensions, (2) across windows."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 1816
        },
        {
          "x": 1199,
          "y": 1816
        },
        {
          "x": 1199,
          "y": 2262
        },
        {
          "x": 201,
          "y": 2262
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='156' style='font-size:18px'>For the first type, Conformer [39] also performs infor-<br>mation exchange between a transformer branch and a con-<br>volution branch. While its motivation is different from ours.<br>Conformer aims to couple local and global features across<br>convolution and transformer branches. MixFormer uses<br>channel and spatial interactions to address the weak mod-<br>eling ability issues caused by weight sharing on the channel<br>(local-window self-attention) and the spatial (depth-wise<br>convolution) dimensions [17].</p>",
      "id": 156,
      "page": 11,
      "text": "For the first type, Conformer [39] also performs infor-\nmation exchange between a transformer branch and a con-\nvolution branch. While its motivation is different from ours.\nConformer aims to couple local and global features across\nconvolution and transformer branches. MixFormer uses\nchannel and spatial interactions to address the weak mod-\neling ability issues caused by weight sharing on the channel\n(local-window self-attention) and the spatial (depth-wise\nconvolution) dimensions [17]."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 2268
        },
        {
          "x": 1200,
          "y": 2268
        },
        {
          "x": 1200,
          "y": 2713
        },
        {
          "x": 201,
          "y": 2713
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='157' style='font-size:18px'>For the second type, Twins (strided convolution +<br>global sub-sampled attention) [6] and Shuffle Transformer<br>(neighbor-window connection (NWC) + random spatial<br>shuffle) [26] construct local and global connections to<br>achieve information exchanges, MSG Transformer (channel<br>shuffle on extra MSG tokens) [12] applies global connec-<br>tion. Our MixFormer achieves this goal by concatenating<br>the parallel features: the non-overlapped window feature<br>and the local-connected feature (output of the dwconv3x3).</p>",
      "id": 157,
      "page": 11,
      "text": "For the second type, Twins (strided convolution +\nglobal sub-sampled attention) [6] and Shuffle Transformer\n(neighbor-window connection (NWC) + random spatial\nshuffle) [26] construct local and global connections to\nachieve information exchanges, MSG Transformer (channel\nshuffle on extra MSG tokens) [12] applies global connec-\ntion. Our MixFormer achieves this goal by concatenating\nthe parallel features: the non-overlapped window feature\nand the local-connected feature (output of the dwconv3x3)."
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 2756
        },
        {
          "x": 444,
          "y": 2756
        },
        {
          "x": 444,
          "y": 2805
        },
        {
          "x": 204,
          "y": 2805
        }
      ],
      "category": "paragraph",
      "html": "<p id='158' style='font-size:20px'>References</p>",
      "id": 158,
      "page": 11,
      "text": "References"
    },
    {
      "bounding_box": [
        {
          "x": 218,
          "y": 2837
        },
        {
          "x": 1198,
          "y": 2837
        },
        {
          "x": 1198,
          "y": 2971
        },
        {
          "x": 218,
          "y": 2971
        }
      ],
      "category": "paragraph",
      "html": "<p id='159' style='font-size:14px'>[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-<br>ton. Layer normalization. arXiv preprint arXiv:1607.06450,<br>2016. 3</p>",
      "id": 159,
      "page": 11,
      "text": "[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. Layer normalization. arXiv preprint arXiv:1607.06450,\n2016. 3"
    },
    {
      "bounding_box": [
        {
          "x": 1290,
          "y": 299
        },
        {
          "x": 2286,
          "y": 299
        },
        {
          "x": 2286,
          "y": 2973
        },
        {
          "x": 1290,
          "y": 2973
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='160' style='font-size:14px'>[2] Song Bai, Philip Torr, et al. Visual parser: Representing<br>part-whole hierarchies with transformers. arXiv preprint<br>arXiv:2107.05790, 2021. 2<br>[3] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delv-<br>ing into high quality object detection. In Proceedings of the<br>IEEE conference on computer vision and pattern recogni-<br>tion, pages 6154-6162, 2018. 6, 10<br>[4] Jin Chen, Xijun Wang, Zichao Guo, Xiangyu Zhang, and<br>Jian Sun. Dynamic region-aware convolution. In Proceed-<br>ings of the IEEE/CVF Conference on Computer Vision and<br>Pattern Recognition, pages 8064-8073, 2021. 3<br>[5] Liang-Chieh CHEN, Yukun ZHU, George PAPANDREOU,<br>F Schroff, CV Aug, and H Adam. Deeplabv3+: Encoder-<br>decoder with atrous separable convolution for semantic im-<br>age segmentation [m]. FERRARI V, HEBERT M, SMIN-<br>CHISESCU C, et al. ECCV (7). Springer, pages 833-851,<br>2018. 6<br>[6] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haib-<br>ing Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen.<br>Twins: Revisiting the design of spatial attention in vi-<br>sion transformers. arXiv preprint arXiv:2104.13840, 1(2):3,<br>2021. 2, 5, 6, 9, 11<br>[7] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V<br>Le. Randaugment: Practical automated data augmenta-<br>tion with a reduced search space. In Proceedings of the<br>IEEE/CVF Conference on Computer Vision and Pattern<br>Recognition Workshops, pages 702-703, 2020. 10<br>[8] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong<br>Zhang, Han Hu, and Yichen Wei. Deformable convolutional<br>networks. In Proceedings of the IEEE international confer-<br>ence on computer vision, pages 764-773, 2017. 2<br>[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,<br>and Li Fei-Fei. Imagenet: A large-scale hierarchical image<br>database. In 2009 IEEE conference on computer vision and<br>pattern recognition, pages 248-255. Ieee, 2009. 1, 2, 5, 6, 7,<br>8, 9, 10<br>[10] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming<br>Zhang, Nenghai Yu, Lu Yuan, Dong Chen, and Bain-<br>ing Guo. Cswin transformer: A general vision trans-<br>former backbone with cross-shaped windows. arXiv preprint<br>arXiv:2107.00652, 2021. 2<br>[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,<br>Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,<br>Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-<br>vain Gelly, et al. An image is worth 16x16 words: Trans-<br>formers for image recognition at scale. arXiv preprint<br>arXiv:2010.11929, 2020. 1, 2, 8, 10<br>[12] Jiemin Fang, Lingxi Xie, Xinggang Wang, Xiaopeng Zhang,<br>Wenyu Liu, and Qi Tian. Msg-transformer: Exchanging lo-<br>cal spatial information by manipulating messenger tokens.<br>arXiv preprint arXiv:2105.15168, 2021. 11<br>[13] Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei<br>Fang, and Hanqing Lu. Dual attention network for scene<br>segmentation. In Proceedings of the IEEE/CVF Conference<br>on Computer Vision and Pattern Recognition, pages 3146-<br>3154, 2019. 6<br>[14] Jun Fu, Jing Liu, Yuhang Wang, Yong Li, Yongjun Bao, Jin-<br>hui Tang, and Hanqing Lu. Adaptive context network for</p>",
      "id": 160,
      "page": 11,
      "text": "[2] Song Bai, Philip Torr, et al. Visual parser: Representing\npart-whole hierarchies with transformers. arXiv preprint\narXiv:2107.05790, 2021. 2\n[3] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delv-\ning into high quality object detection. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 6154-6162, 2018. 6, 10\n[4] Jin Chen, Xijun Wang, Zichao Guo, Xiangyu Zhang, and\nJian Sun. Dynamic region-aware convolution. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 8064-8073, 2021. 3\n[5] Liang-Chieh CHEN, Yukun ZHU, George PAPANDREOU,\nF Schroff, CV Aug, and H Adam. Deeplabv3+: Encoder-\ndecoder with atrous separable convolution for semantic im-\nage segmentation [m]. FERRARI V, HEBERT M, SMIN-\nCHISESCU C, et al. ECCV (7). Springer, pages 833-851,\n2018. 6\n[6] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haib-\ning Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen.\nTwins: Revisiting the design of spatial attention in vi-\nsion transformers. arXiv preprint arXiv:2104.13840, 1(2):3,\n2021. 2, 5, 6, 9, 11\n[7] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V\nLe. Randaugment: Practical automated data augmenta-\ntion with a reduced search space. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition Workshops, pages 702-703, 2020. 10\n[8] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong\nZhang, Han Hu, and Yichen Wei. Deformable convolutional\nnetworks. In Proceedings of the IEEE international confer-\nence on computer vision, pages 764-773, 2017. 2\n[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, pages 248-255. Ieee, 2009. 1, 2, 5, 6, 7,\n8, 9, 10\n[10] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming\nZhang, Nenghai Yu, Lu Yuan, Dong Chen, and Bain-\ning Guo. Cswin transformer: A general vision trans-\nformer backbone with cross-shaped windows. arXiv preprint\narXiv:2107.00652, 2021. 2\n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 1, 2, 8, 10\n[12] Jiemin Fang, Lingxi Xie, Xinggang Wang, Xiaopeng Zhang,\nWenyu Liu, and Qi Tian. Msg-transformer: Exchanging lo-\ncal spatial information by manipulating messenger tokens.\narXiv preprint arXiv:2105.15168, 2021. 11\n[13] Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei\nFang, and Hanqing Lu. Dual attention network for scene\nsegmentation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 3146-\n3154, 2019. 6\n[14] Jun Fu, Jing Liu, Yuhang Wang, Yong Li, Yongjun Bao, Jin-\nhui Tang, and Hanqing Lu. Adaptive context network for"
    },
    {
      "bounding_box": [
        {
          "x": 1218,
          "y": 3053
        },
        {
          "x": 1260,
          "y": 3053
        },
        {
          "x": 1260,
          "y": 3094
        },
        {
          "x": 1218,
          "y": 3094
        }
      ],
      "category": "footer",
      "html": "<footer id='161' style='font-size:16px'>11</footer>",
      "id": 161,
      "page": 11,
      "text": "11"
    },
    {
      "bounding_box": [
        {
          "x": 286,
          "y": 312
        },
        {
          "x": 1198,
          "y": 312
        },
        {
          "x": 1198,
          "y": 440
        },
        {
          "x": 286,
          "y": 440
        }
      ],
      "category": "paragraph",
      "html": "<p id='162' style='font-size:14px'>scene parsing. In Proceedings of the IEEE/CVF Interna-<br>tional Conference on Computer Vision, pages 6748-6757,<br>2019. 6</p>",
      "id": 162,
      "page": 12,
      "text": "scene parsing. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pages 6748-6757,\n2019. 6"
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 439
        },
        {
          "x": 1200,
          "y": 439
        },
        {
          "x": 1200,
          "y": 2970
        },
        {
          "x": 203,
          "y": 2970
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='163' style='font-size:14px'>[15] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A<br>dataset for large vocabulary instance segmentation. In Pro-<br>ceedings of the IEEE/CVF Conference on Computer Vision<br>and Pattern Recognition, pages 5356-5364, 2019. 2, 11<br>[16] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu,<br>and Yunhe Wang. Transformer in transformer. arXiv preprint<br>arXiv:2103.00112, 2021. 2<br>[17] Qi Han, Zejia Fan, Qi Dai, Lei Sun, Ming-Ming Cheng, Ji-<br>aying Liu, and Jingdong Wang. Demystifying local vision<br>transformer: Sparse connectivity, weight sharing, and dy-<br>namic weight. arXiv preprint arXiv:2106.04263, 2021. 2,<br>11<br>[18] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Gir-<br>shick. Mask r-cnn. In Proceedings of the IEEE international<br>conference on computer vision, pages 2961-2969, 2017. 2,<br>5, 6, 7 , 10, 11<br>[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.<br>Deep residual learning for image recognition. In Proceed-<br>ings of the IEEE conference on computer vision and pattern<br>recognition, pages 770-778, 2016. 2, 5, 6, 8<br>[20] Dan Hendrycks and Kevin Gimpel. Gaussian error linear<br>units (gelus). arXiv preprint arXiv: 1606.08415, 2016. 4<br>[21] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh<br>Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu,<br>Ruoming Pang, Vijay Vasudevan, et al. Searching for mo-<br>bilenetv3. In Proceedings of the IEEE/CVF International<br>Conference on Computer Vision, pages 1314-1324, 2019. 9<br>[22] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry<br>Kalenichenko, Weijun Wang, Tobias Weyand, Marco An-<br>dreetto, and Hartwig Adam. Mobilenets: Efficient convolu-<br>tional neural networks for mobile vision applications. arXiv<br>preprint arXiv: 1704. 04861, 2017. 2, 4<br>[23] Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Lo-<br>cal relation networks for image recognition. In Proceedings<br>of the IEEE/CVF International Conference on Computer Vi-<br>sion, pages 3464-3473, 2019. 3<br>[24] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-<br>works. In Proceedings of the IEEE conference on computer<br>vision and pattern recognition, pages 7132-7141, 2018. 2,<br>3, 4, 8<br>[25] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kil-<br>ian Q Weinberger. Deep networks with stochastic depth. In<br>European conference on computer vision, pages 646-661.<br>Springer, 2016. 10<br>[26] Zilong Huang, Youcheng Ben, Guozhong Luo, Pei Cheng,<br>Gang Yu, and Bin Fu. Shuffle transformer: Rethink-<br>ing spatial shuffle for vision transformer. arXiv preprint<br>arXiv:2106.03650, 2021. 1, 2, 3, 5, 6, 9, 11<br>[27] Sergey Ioffe and Christian Szegedy. Batch normalization:<br>Accelerating deep network training by reducing internal co-<br>variate shift. In International conference on machine learn-<br>ing, pages 448-456. PMLR, 2015. 3, 4<br>[28] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al.<br>Spatial transformer networks. Advances in neural informa-<br>tion processing systems, 28:2017-2025, 2015. 2</p>",
      "id": 163,
      "page": 12,
      "text": "[15] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A\ndataset for large vocabulary instance segmentation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 5356-5364, 2019. 2, 11\n[16] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu,\nand Yunhe Wang. Transformer in transformer. arXiv preprint\narXiv:2103.00112, 2021. 2\n[17] Qi Han, Zejia Fan, Qi Dai, Lei Sun, Ming-Ming Cheng, Ji-\naying Liu, and Jingdong Wang. Demystifying local vision\ntransformer: Sparse connectivity, weight sharing, and dy-\nnamic weight. arXiv preprint arXiv:2106.04263, 2021. 2,\n11\n[18] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Gir-\nshick. Mask r-cnn. In Proceedings of the IEEE international\nconference on computer vision, pages 2961-2969, 2017. 2,\n5, 6, 7 , 10, 11\n[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770-778, 2016. 2, 5, 6, 8\n[20] Dan Hendrycks and Kevin Gimpel. Gaussian error linear\nunits (gelus). arXiv preprint arXiv: 1606.08415, 2016. 4\n[21] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh\nChen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu,\nRuoming Pang, Vijay Vasudevan, et al. Searching for mo-\nbilenetv3. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 1314-1324, 2019. 9\n[22] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry\nKalenichenko, Weijun Wang, Tobias Weyand, Marco An-\ndreetto, and Hartwig Adam. Mobilenets: Efficient convolu-\ntional neural networks for mobile vision applications. arXiv\npreprint arXiv: 1704. 04861, 2017. 2, 4\n[23] Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Lo-\ncal relation networks for image recognition. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, pages 3464-3473, 2019. 3\n[24] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-\nworks. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 7132-7141, 2018. 2,\n3, 4, 8\n[25] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kil-\nian Q Weinberger. Deep networks with stochastic depth. In\nEuropean conference on computer vision, pages 646-661.\nSpringer, 2016. 10\n[26] Zilong Huang, Youcheng Ben, Guozhong Luo, Pei Cheng,\nGang Yu, and Bin Fu. Shuffle transformer: Rethink-\ning spatial shuffle for vision transformer. arXiv preprint\narXiv:2106.03650, 2021. 1, 2, 3, 5, 6, 9, 11\n[27] Sergey Ioffe and Christian Szegedy. Batch normalization:\nAccelerating deep network training by reducing internal co-\nvariate shift. In International conference on machine learn-\ning, pages 448-456. PMLR, 2015. 3, 4\n[28] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al.\nSpatial transformer networks. Advances in neural informa-\ntion processing systems, 28:2017-2025, 2015. 2"
    },
    {
      "bounding_box": [
        {
          "x": 1275,
          "y": 305
        },
        {
          "x": 2287,
          "y": 305
        },
        {
          "x": 2287,
          "y": 2971
        },
        {
          "x": 1275,
          "y": 2971
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='164' style='font-size:14px'>[29] Zihang Jiang, Qibin Hou, Li Yuan, Daquan Zhou, Xiaojie<br>Jin, Anran Wang, and Jiashi Feng. Token labeling: Training<br>a 85.5% top-1 accuracy vision transformer with 56m param-<br>eters on imagenet. arXiv preprint arXiv:2104.10858, 2021.<br>2<br>[30] Duo Li, Jie Hu, Changhu Wang, Xiangtai Li, Qi She, Lei<br>Zhu, Tong Zhang, and Qifeng Chen. Involution: Inverting<br>the inherence of convolution for visual recognition. In Pro-<br>ceedings of the IEEE/CVF Conference on Computer Vision<br>and Pattern Recognition, pages 12321-12330, 2021. 3<br>[31] Jinpeng Li, Yichao Yan, Shengcai Liao, Xiaokang Yang, and<br>Ling Shao. Local-to-global self-attention in vision trans-<br>formers. arXiv preprint arXiv:2107.04735, 2021. 5, 6, 9<br>[32] Xiang Li, Wenhai Wang, Xiaolin Hu, and Jian Yang. Selec-<br>tive kernel networks. In Proceedings of the IEEE/CVF Con-<br>ference on Computer Vision and Pattern Recognition, pages<br>510-519, 2019. 2<br>[33] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,<br>Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence<br>Zitnick. Microsoft COCO: Common objects in context. In<br>European conference on computer vision, pages 740-755.<br>Springer, 2014. 2, 5, 7, 10, 11<br>[34] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,<br>Zheng Zhang, Stephen Lin, and Baining Guo. Swin trans-<br>former: Hierarchical vision transformer using shifted win-<br>dows. arXiv preprint arXiv:2103.14030, 2021. 1, 2, 3, 4, 5,<br>6, 7, 8, 9, 10, 11<br>[35] Ilya Loshchilov and Frank Hutter. Sgdr: Stochas-<br>tic gradient descent with warm restarts. arXiv preprint<br>arXiv:1608.03983, 2016. 10<br>[36] I. Loshchilov and F. Hutter. Fixing weight decay regulariza-<br>tion in adam. 2017. 10, 11<br>[37] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun.<br>Shufflenet v2: Practical guidelines for efficient cnn architec-<br>ture design. In Proceedings of the European conference on<br>computer vision (ECCV), pages 116-131, 2018. 9<br>[38] Mingyuan Mao, Renrui Zhang, Honghui Zheng, Peng Gao,<br>Teli Ma, Yan Peng, Errui Ding, and Shumin Han. Dual-<br>stream network for visual recognition. arXiv preprint<br>arXiv:2105.14734, 2021. 5, 6, 9<br>[39] Zhiliang Peng, Wei Huang, Shanzhi Gu, Lingxi Xie, Yaowei<br>Wang, Jianbin Jiao, and Qixiang Ye. Conformer: Local fea-<br>tures coupling global representations for visual recognition.<br>In Proceedings of the IEEE/CVF International Conference<br>on Computer Vision, pages 367-376, 2021. 11<br>[40] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick,<br>Kaiming He, and Piotr Dollar. Designing network design<br>spaces. In Proceedings of the IEEE/CVF Conference on<br>Computer Vision and Pattern Recognition, pages 10428-<br>10436, 2020. 2, 5, 9<br>[41] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-<br>moginov, and Liang-Chieh Chen. Mobilenetv2: Inverted<br>residuals and linear bottlenecks. In Proceedings of the<br>IEEE conference on computer vision and pattern recogni-<br>tion, pages 4510-4520, 2018. 4, 8, 9<br>[42] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon<br>Shlens, Pieter Abbeel, and Ashish Vaswani. Bottleneck</p>",
      "id": 164,
      "page": 12,
      "text": "[29] Zihang Jiang, Qibin Hou, Li Yuan, Daquan Zhou, Xiaojie\nJin, Anran Wang, and Jiashi Feng. Token labeling: Training\na 85.5% top-1 accuracy vision transformer with 56m param-\neters on imagenet. arXiv preprint arXiv:2104.10858, 2021.\n2\n[30] Duo Li, Jie Hu, Changhu Wang, Xiangtai Li, Qi She, Lei\nZhu, Tong Zhang, and Qifeng Chen. Involution: Inverting\nthe inherence of convolution for visual recognition. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 12321-12330, 2021. 3\n[31] Jinpeng Li, Yichao Yan, Shengcai Liao, Xiaokang Yang, and\nLing Shao. Local-to-global self-attention in vision trans-\nformers. arXiv preprint arXiv:2107.04735, 2021. 5, 6, 9\n[32] Xiang Li, Wenhai Wang, Xiaolin Hu, and Jian Yang. Selec-\ntive kernel networks. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n510-519, 2019. 2\n[33] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence\nZitnick. Microsoft COCO: Common objects in context. In\nEuropean conference on computer vision, pages 740-755.\nSpringer, 2014. 2, 5, 7, 10, 11\n[34] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,\nZheng Zhang, Stephen Lin, and Baining Guo. Swin trans-\nformer: Hierarchical vision transformer using shifted win-\ndows. arXiv preprint arXiv:2103.14030, 2021. 1, 2, 3, 4, 5,\n6, 7, 8, 9, 10, 11\n[35] Ilya Loshchilov and Frank Hutter. Sgdr: Stochas-\ntic gradient descent with warm restarts. arXiv preprint\narXiv:1608.03983, 2016. 10\n[36] I. Loshchilov and F. Hutter. Fixing weight decay regulariza-\ntion in adam. 2017. 10, 11\n[37] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun.\nShufflenet v2: Practical guidelines for efficient cnn architec-\nture design. In Proceedings of the European conference on\ncomputer vision (ECCV), pages 116-131, 2018. 9\n[38] Mingyuan Mao, Renrui Zhang, Honghui Zheng, Peng Gao,\nTeli Ma, Yan Peng, Errui Ding, and Shumin Han. Dual-\nstream network for visual recognition. arXiv preprint\narXiv:2105.14734, 2021. 5, 6, 9\n[39] Zhiliang Peng, Wei Huang, Shanzhi Gu, Lingxi Xie, Yaowei\nWang, Jianbin Jiao, and Qixiang Ye. Conformer: Local fea-\ntures coupling global representations for visual recognition.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 367-376, 2021. 11\n[40] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick,\nKaiming He, and Piotr Dollar. Designing network design\nspaces. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 10428-\n10436, 2020. 2, 5, 9\n[41] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-\nmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted\nresiduals and linear bottlenecks. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 4510-4520, 2018. 4, 8, 9\n[42] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon\nShlens, Pieter Abbeel, and Ashish Vaswani. Bottleneck"
    },
    {
      "bounding_box": [
        {
          "x": 1218,
          "y": 3052
        },
        {
          "x": 1263,
          "y": 3052
        },
        {
          "x": 1263,
          "y": 3094
        },
        {
          "x": 1218,
          "y": 3094
        }
      ],
      "category": "footer",
      "html": "<footer id='165' style='font-size:18px'>12</footer>",
      "id": 165,
      "page": 12,
      "text": "12"
    },
    {
      "bounding_box": [
        {
          "x": 287,
          "y": 313
        },
        {
          "x": 1200,
          "y": 313
        },
        {
          "x": 1200,
          "y": 441
        },
        {
          "x": 287,
          "y": 441
        }
      ],
      "category": "paragraph",
      "html": "<p id='166' style='font-size:16px'>transformers for visual recognition. In Proceedings of the<br>IEEE/CVF Conference on Computer Vision and Pattern<br>Recognition, pages 16519-16529, 2021. 8</p>",
      "id": 166,
      "page": 13,
      "text": "transformers for visual recognition. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 16519-16529, 2021. 8"
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 436
        },
        {
          "x": 1200,
          "y": 436
        },
        {
          "x": 1200,
          "y": 2972
        },
        {
          "x": 202,
          "y": 2972
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='167' style='font-size:16px'>[43] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model<br>scaling for convolutional neural networks. In International<br>Conference on Machine Learning, pages 6105-6114. PMLR,<br>2019. 2, 4, 5, 8, 9<br>[44] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco<br>Massa, Alexandre Sablayrolles, and Herve Jegou. Training<br>data-efficient image transformers & distillation through at-<br>tention. In International Conference on Machine Learning,<br>pages 10347-10357. PMLR, 2021. 1, 2, 4, 5, 6, 8, 9, 10<br>[45] Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas,<br>Niki Parmar, Blake Hechtman, and Jonathon Shlens. Scaling<br>local self-attention for parameter efficient visual backbones.<br>In Proceedings of the IEEE/CVF Conference on Computer<br>Vision and Pattern Recognition, pages 12894-12904, 2021.<br>1, 2, 3, 8<br>[46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-<br>reit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia<br>Polosukhin. Attention is all you need. In Advances in neural<br>information processing systems, pages 5998-6008, 2017. 1,<br>2, 4<br>[47] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang,<br>Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui<br>Tan, Xinggang Wang, et al. Deep high-resolution represen-<br>tation learning for visual recognition. IEEE transactions on<br>pattern analysis and machine intelligence, 2020. 6, 7<br>[48] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao<br>Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.<br>Pvtv2: Improved baselines with pyramid vision transformer.<br>arXiv preprint arXiv:2106. 13797, 2021. 4, 9<br>[49] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao<br>Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.<br>Pyramid vision transformer: A versatile backbone for<br>dense prediction without convolutions. arXiv preprint<br>arXiv:2102.12122, 2021. 2, 5, 6, 8, 9<br>[50] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-<br>ing He. Non-local neural networks. In Proceedings of the<br>IEEE conference on computer vision and pattern recogni-<br>tion, pages 7794-7803, 2018. 8<br>[51] Ross Wightman, Hugo Touvron, and Herve Jegou. Resnet<br>strikes back: An improved training procedure in timm. arXiv<br>preprint arXiv:2110.00476, 2021. 8<br>[52] Sanghyun Woo, Jongchan Park, Joon- Young Lee, and In So<br>Kweon. Cbam: Convolutional block attention module. In<br>Proceedings of the European conference on computer vision<br>(ECCV), pages 3-19, 2018. 2, 3<br>[53] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu,<br>Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introduc-<br>ing convolutions to vision transformers. arXiv preprint<br>arXiv:2103.15808, 2021. 2, 5, 6, 9<br>[54] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and<br>Jian Sun. Unified perceptual parsing for scene understand-<br>ing. In Proceedings of the European Conference on Com-<br>puter Vision (ECCV), pages 418-434, 2018. 2, 6, 7<br>[55] Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and<br>Kaiming He. Aggregated residual transformations for deep</p>",
      "id": 167,
      "page": 13,
      "text": "[43] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model\nscaling for convolutional neural networks. In International\nConference on Machine Learning, pages 6105-6114. PMLR,\n2019. 2, 4, 5, 8, 9\n[44] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herve Jegou. Training\ndata-efficient image transformers & distillation through at-\ntention. In International Conference on Machine Learning,\npages 10347-10357. PMLR, 2021. 1, 2, 4, 5, 6, 8, 9, 10\n[45] Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas,\nNiki Parmar, Blake Hechtman, and Jonathon Shlens. Scaling\nlocal self-attention for parameter efficient visual backbones.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 12894-12904, 2021.\n1, 2, 3, 8\n[46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in neural\ninformation processing systems, pages 5998-6008, 2017. 1,\n2, 4\n[47] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang,\nChaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui\nTan, Xinggang Wang, et al. Deep high-resolution represen-\ntation learning for visual recognition. IEEE transactions on\npattern analysis and machine intelligence, 2020. 6, 7\n[48] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.\nPvtv2: Improved baselines with pyramid vision transformer.\narXiv preprint arXiv:2106. 13797, 2021. 4, 9\n[49] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.\nPyramid vision transformer: A versatile backbone for\ndense prediction without convolutions. arXiv preprint\narXiv:2102.12122, 2021. 2, 5, 6, 8, 9\n[50] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-\ning He. Non-local neural networks. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 7794-7803, 2018. 8\n[51] Ross Wightman, Hugo Touvron, and Herve Jegou. Resnet\nstrikes back: An improved training procedure in timm. arXiv\npreprint arXiv:2110.00476, 2021. 8\n[52] Sanghyun Woo, Jongchan Park, Joon- Young Lee, and In So\nKweon. Cbam: Convolutional block attention module. In\nProceedings of the European conference on computer vision\n(ECCV), pages 3-19, 2018. 2, 3\n[53] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu,\nXiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introduc-\ning convolutions to vision transformers. arXiv preprint\narXiv:2103.15808, 2021. 2, 5, 6, 9\n[54] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and\nJian Sun. Unified perceptual parsing for scene understand-\ning. In Proceedings of the European Conference on Com-\nputer Vision (ECCV), pages 418-434, 2018. 2, 6, 7\n[55] Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and\nKaiming He. Aggregated residual transformations for deep"
    },
    {
      "bounding_box": [
        {
          "x": 1363,
          "y": 309
        },
        {
          "x": 2277,
          "y": 309
        },
        {
          "x": 2277,
          "y": 354
        },
        {
          "x": 1363,
          "y": 354
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='168' style='font-size:14px'>neural networks. In Proceedings of the IEEE conference on</p>",
      "id": 168,
      "page": 13,
      "text": "neural networks. In Proceedings of the IEEE conference on"
    },
    {
      "bounding_box": [
        {
          "x": 1274,
          "y": 337
        },
        {
          "x": 2288,
          "y": 337
        },
        {
          "x": 2288,
          "y": 2874
        },
        {
          "x": 1274,
          "y": 2874
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='169' style='font-size:20px'>computer vision and pattern recognition, pages 1492-1500,<br>2017. 2, 6<br>[56] Haotian Yan, Zhe Li, Weijian Li, Changhu Wang, Ming<br>Wu, and Chuang Zhang. Contnet: Why not use convo-<br>lution and transformer at the same time? arXiv preprint<br>arXiv:2104.13497, 2021. 2<br>[57] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai,<br>Bin Xiao, Lu Yuan, and Jianfeng Gao. Focal self-attention<br>for local-global interactions in vision transformers. arXiv<br>preprint arXiv:2107.00641, 2021. 1, 2, 3, 5, 6, 9<br>[58] Minghao Yin, Zhuliang Yao, Yue Cao, Xiu Li, Zheng Zhang,<br>Stephen Lin, and Han Hu. Disentangled non-local neural net-<br>works. In European Conference on Computer Vision, pages<br>191-207. Springer, 2020. 6<br>[59] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,<br>Zihang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng<br>Yan. Tokens-to-token vit: Training vision transformers<br>from scratch on imagenet. arXiv preprint arXiv:2101.11986,<br>2021. 2<br>[60] Yuhui Yuan, Xilin Chen, and Jingdong Wang. Object-<br>contextual representations for semantic segmentation. In<br>Computer Vision-ECCV 2020: 16th European Conference,<br>Glasgow, UK, August 23-28, 2020, Proceedings, Part VI 16,<br>pages 173-190. Springer, 2020. 6, 7<br>[61] Yuhui Yuan, Rao Fu, Lang Huang, Weihong Lin, Chao<br>Zhang, Xilin Chen, and Jingdong Wang. Hrformer: High-<br>resolution transformer for dense prediction. Advances in<br>Neural Information Processing Systems, 2021. 1, 2, 3, 4,<br>8, 11<br>[62] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk<br>Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regular-<br>ization strategy to train strong classifiers with localizable fea-<br>tures. In Proceedings of the IEEE/CVF International Con-<br>ference on Computer Vision, pages 6023-6032, 2019. 10<br>[63] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and<br>David Lopez-Paz. mixup: Beyond empirical risk minimiza-<br>tion. arXiv preprint arXiv:1710.09412, 2017. 10<br>[64] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Explor-<br>ing self-attention for image recognition. In Proceedings of<br>the IEEE/CVF Conference on Computer Vision and Pattern<br>Recognition, pages 10076-10085, 2020. 3<br>[65] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and<br>Yi Yang. Random erasing data augmentation. In Proceedings<br>ofthe AAAI Conference on Artificial Intelligence, number 07,<br>pages 13001-13008, 2020. 10<br>[66] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fi-<br>dler, Adela Barriuso, and Antonio Torralba. Semantic under-<br>standing of scenes through the ade20k dataset. International<br>Journal of Computer Vision, 127(3):302-321, 2019. 2, 5, 6,<br>7, 11<br>[67] Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xi-<br>aochen Lian, Zihang Jiang, Qibin Hou, and Jiashi Feng.<br>Deepvit: Towards deeper vision transformer. arXiv preprint<br>arXiv:2103.11886, 2021. 2</p>",
      "id": 169,
      "page": 13,
      "text": "computer vision and pattern recognition, pages 1492-1500,\n2017. 2, 6\n[56] Haotian Yan, Zhe Li, Weijian Li, Changhu Wang, Ming\nWu, and Chuang Zhang. Contnet: Why not use convo-\nlution and transformer at the same time? arXiv preprint\narXiv:2104.13497, 2021. 2\n[57] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai,\nBin Xiao, Lu Yuan, and Jianfeng Gao. Focal self-attention\nfor local-global interactions in vision transformers. arXiv\npreprint arXiv:2107.00641, 2021. 1, 2, 3, 5, 6, 9\n[58] Minghao Yin, Zhuliang Yao, Yue Cao, Xiu Li, Zheng Zhang,\nStephen Lin, and Han Hu. Disentangled non-local neural net-\nworks. In European Conference on Computer Vision, pages\n191-207. Springer, 2020. 6\n[59] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\nZihang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng\nYan. Tokens-to-token vit: Training vision transformers\nfrom scratch on imagenet. arXiv preprint arXiv:2101.11986,\n2021. 2\n[60] Yuhui Yuan, Xilin Chen, and Jingdong Wang. Object-\ncontextual representations for semantic segmentation. In\nComputer Vision-ECCV 2020: 16th European Conference,\nGlasgow, UK, August 23-28, 2020, Proceedings, Part VI 16,\npages 173-190. Springer, 2020. 6, 7\n[61] Yuhui Yuan, Rao Fu, Lang Huang, Weihong Lin, Chao\nZhang, Xilin Chen, and Jingdong Wang. Hrformer: High-\nresolution transformer for dense prediction. Advances in\nNeural Information Processing Systems, 2021. 1, 2, 3, 4,\n8, 11\n[62] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk\nChun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regular-\nization strategy to train strong classifiers with localizable fea-\ntures. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, pages 6023-6032, 2019. 10\n[63] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and\nDavid Lopez-Paz. mixup: Beyond empirical risk minimiza-\ntion. arXiv preprint arXiv:1710.09412, 2017. 10\n[64] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Explor-\ning self-attention for image recognition. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10076-10085, 2020. 3\n[65] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and\nYi Yang. Random erasing data augmentation. In Proceedings\nofthe AAAI Conference on Artificial Intelligence, number 07,\npages 13001-13008, 2020. 10\n[66] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fi-\ndler, Adela Barriuso, and Antonio Torralba. Semantic under-\nstanding of scenes through the ade20k dataset. International\nJournal of Computer Vision, 127(3):302-321, 2019. 2, 5, 6,\n7, 11\n[67] Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xi-\naochen Lian, Zihang Jiang, Qibin Hou, and Jiashi Feng.\nDeepvit: Towards deeper vision transformer. arXiv preprint\narXiv:2103.11886, 2021. 2"
    },
    {
      "bounding_box": [
        {
          "x": 1218,
          "y": 3052
        },
        {
          "x": 1263,
          "y": 3052
        },
        {
          "x": 1263,
          "y": 3093
        },
        {
          "x": 1218,
          "y": 3093
        }
      ],
      "category": "footer",
      "html": "<footer id='170' style='font-size:20px'>13</footer>",
      "id": 170,
      "page": 13,
      "text": "13"
    }
  ]
}