{
  "id": "629feb66-0f92-11ef-8230-426932df3dcf",
  "pdf_path": "./pdf/AI_VIT_O/2102.12122v2.pdf",
  "elements": [
    {
      "bounding_box": [
        {
          "x": 304,
          "y": 436
        },
        {
          "x": 2176,
          "y": 436
        },
        {
          "x": 2176,
          "y": 574
        },
        {
          "x": 304,
          "y": 574
        }
      ],
      "category": "paragraph",
      "html": "<p id='0' style='font-size:22px'>Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction<br>without Convolutions</p>",
      "id": 0,
      "page": 1,
      "text": "Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction\nwithout Convolutions"
    },
    {
      "bounding_box": [
        {
          "x": 423,
          "y": 673
        },
        {
          "x": 2049,
          "y": 673
        },
        {
          "x": 2049,
          "y": 912
        },
        {
          "x": 423,
          "y": 912
        }
      ],
      "category": "paragraph",
      "html": "<p id='1' style='font-size:22px'>Wenhai Wang1 , Enze Xie2, Xiang Li3, Deng-Ping Fan4X<br>,<br>Kaitao Song3, Ding Liang5, Tong Lu1⌀ Ping Luo2 Ling Shao4<br>,<br>1Nanjing University 2The University of Hong Kong<br>3Nanjing University of Science and Technology 4IIAI 5SenseTime Research</p>",
      "id": 1,
      "page": 1,
      "text": "Wenhai Wang1 , Enze Xie2, Xiang Li3, Deng-Ping Fan4X\n,\nKaitao Song3, Ding Liang5, Tong Lu1⌀ Ping Luo2 Ling Shao4\n,\n1Nanjing University 2The University of Hong Kong\n3Nanjing University of Science and Technology 4IIAI 5SenseTime Research"
    },
    {
      "bounding_box": [
        {
          "x": 899,
          "y": 923
        },
        {
          "x": 1581,
          "y": 923
        },
        {
          "x": 1581,
          "y": 961
        },
        {
          "x": 899,
          "y": 961
        }
      ],
      "category": "paragraph",
      "html": "<p id='2' style='font-size:16px'>https : / / github · com/ whai362 /PVT</p>",
      "id": 2,
      "page": 1,
      "text": "https : / / github · com/ whai362 /PVT"
    },
    {
      "bounding_box": [
        {
          "x": 211,
          "y": 1005
        },
        {
          "x": 808,
          "y": 1005
        },
        {
          "x": 808,
          "y": 1420
        },
        {
          "x": 211,
          "y": 1420
        }
      ],
      "category": "figure",
      "html": "<figure><img id='3' style='font-size:14px' alt=\"TASK\nConv 4\n1. CLS\n↑Conv 3 2. DET\n3. SEG\n↑Conv 2\n↑Conv 1·\" data-coord=\"top-left:(211,1005); bottom-right:(808,1420)\" /></figure>",
      "id": 3,
      "page": 1,
      "text": "TASK\nConv 4\n1. CLS\n↑Conv 3 2. DET\n3. SEG\n↑Conv 2\n↑Conv 1·"
    },
    {
      "bounding_box": [
        {
          "x": 208,
          "y": 1440
        },
        {
          "x": 808,
          "y": 1440
        },
        {
          "x": 808,
          "y": 1484
        },
        {
          "x": 208,
          "y": 1484
        }
      ],
      "category": "caption",
      "html": "<br><caption id='4' style='font-size:18px'>(a) CNNs: VGG [54], ResNet [22], etc.</caption>",
      "id": 4,
      "page": 1,
      "text": "(a) CNNs: VGG [54], ResNet [22], etc."
    },
    {
      "bounding_box": [
        {
          "x": 921,
          "y": 1005
        },
        {
          "x": 1508,
          "y": 1005
        },
        {
          "x": 1508,
          "y": 1415
        },
        {
          "x": 921,
          "y": 1415
        }
      ],
      "category": "figure",
      "html": "<br><figure><img id='5' style='font-size:18px' alt=\"TASK\n1. CLS\n브\nLx\nTF-E·: Transformer\nBlock\" data-coord=\"top-left:(921,1005); bottom-right:(1508,1415)\" /></figure>",
      "id": 5,
      "page": 1,
      "text": "TASK\n1. CLS\n브\nLx\nTF-E·: Transformer\nBlock"
    },
    {
      "bounding_box": [
        {
          "x": 995,
          "y": 1438
        },
        {
          "x": 1419,
          "y": 1438
        },
        {
          "x": 1419,
          "y": 1483
        },
        {
          "x": 995,
          "y": 1483
        }
      ],
      "category": "caption",
      "html": "<br><caption id='6' style='font-size:18px'>(b) Vision Transformer [13]</caption>",
      "id": 6,
      "page": 1,
      "text": "(b) Vision Transformer [13]"
    },
    {
      "bounding_box": [
        {
          "x": 1620,
          "y": 1008
        },
        {
          "x": 2205,
          "y": 1008
        },
        {
          "x": 2205,
          "y": 1414
        },
        {
          "x": 1620,
          "y": 1414
        }
      ],
      "category": "figure",
      "html": "<br><figure><img id='7' style='font-size:14px' alt=\"TASK\nTF-E 4\n1. CLS\nTF-E 3 2. DET\n3. SEG\nTF-E 2\nLix\nTF-E 1·: Transformer\nBlock\nShrink\" data-coord=\"top-left:(1620,1008); bottom-right:(2205,1414)\" /></figure>",
      "id": 7,
      "page": 1,
      "text": "TASK\nTF-E 4\n1. CLS\nTF-E 3 2. DET\n3. SEG\nTF-E 2\nLix\nTF-E 1·: Transformer\nBlock\nShrink"
    },
    {
      "bounding_box": [
        {
          "x": 1614,
          "y": 1438
        },
        {
          "x": 2203,
          "y": 1438
        },
        {
          "x": 2203,
          "y": 1485
        },
        {
          "x": 1614,
          "y": 1485
        }
      ],
      "category": "caption",
      "html": "<br><caption id='8' style='font-size:18px'>(c) Pyramid Vision Transformer (ours)</caption>",
      "id": 8,
      "page": 1,
      "text": "(c) Pyramid Vision Transformer (ours)"
    },
    {
      "bounding_box": [
        {
          "x": 197,
          "y": 1493
        },
        {
          "x": 2278,
          "y": 1493
        },
        {
          "x": 2278,
          "y": 1841
        },
        {
          "x": 197,
          "y": 1841
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='9' style='font-size:20px'>Figure 1: Comparisons of different architectures, where \"Conv\" and \"TF-E\" stand for \"convolution\" and \"Transformer<br>encoder\", respectively. (a) Many CNN backbones use a pyramid structure for dense prediction tasks such as object detection<br>(DET), instance and semantic segmentation (SEG). (b) The recently proposed Vision Transformer (ViT) [13] is a \"columnar\"<br>structure specifically designed for image classification (CLS). (c) By incorporating the pyramid structure from CNNs, we<br>present the Pyramid Vision Transformer (PVT), which can be used as a versatile backbone for many computer vision tasks,<br>broadening the scope and impact of ViT. Moreover, our experiments also show that PVT can easily be combined with<br>DETR [6] to build an end-to-end object detection system without convolutions.</p>",
      "id": 9,
      "page": 1,
      "text": "Figure 1: Comparisons of different architectures, where \"Conv\" and \"TF-E\" stand for \"convolution\" and \"Transformer\nencoder\", respectively. (a) Many CNN backbones use a pyramid structure for dense prediction tasks such as object detection\n(DET), instance and semantic segmentation (SEG). (b) The recently proposed Vision Transformer (ViT) [13] is a \"columnar\"\nstructure specifically designed for image classification (CLS). (c) By incorporating the pyramid structure from CNNs, we\npresent the Pyramid Vision Transformer (PVT), which can be used as a versatile backbone for many computer vision tasks,\nbroadening the scope and impact of ViT. Moreover, our experiments also show that PVT can easily be combined with\nDETR [6] to build an end-to-end object detection system without convolutions."
    },
    {
      "bounding_box": [
        {
          "x": 603,
          "y": 1915
        },
        {
          "x": 797,
          "y": 1915
        },
        {
          "x": 797,
          "y": 1967
        },
        {
          "x": 603,
          "y": 1967
        }
      ],
      "category": "paragraph",
      "html": "<p id='10' style='font-size:20px'>Abstract</p>",
      "id": 10,
      "page": 1,
      "text": "Abstract"
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 2020
        },
        {
          "x": 1199,
          "y": 2020
        },
        {
          "x": 1199,
          "y": 2869
        },
        {
          "x": 200,
          "y": 2869
        }
      ],
      "category": "paragraph",
      "html": "<p id='11' style='font-size:20px'>Although convolutional neural networks (CNNs) have<br>achieved great success in computer vision, this work inves-<br>tigates a simpler, convolution-free backbone network use-<br>ful for many dense prediction tasks. Unlike the recently-<br>proposed Vision Transformer (ViT) that was designed for<br>image classification specifically, we introduce the Pyra-<br>mid Vision Transformer (PVT), which overcomes the diffi-<br>culties of porting Transformer to various dense prediction<br>tasks. PVT has several merits compared to current state<br>of the arts. (1) Different from ViT that typically yields low-<br>resolution outputs and incurs high computational and mem-<br>ory costs, PVT not only can be trained on dense partitions<br>of an image to achieve high output resolution, which is im-<br>portant for dense prediction, but also uses a progressive<br>shrinking pyramid to reduce the computations of large fea-<br>ture maps. (2) PVT inherits the advantages of both CNN<br>and Transformer, making it a unified backbone for vari-</p>",
      "id": 11,
      "page": 1,
      "text": "Although convolutional neural networks (CNNs) have\nachieved great success in computer vision, this work inves-\ntigates a simpler, convolution-free backbone network use-\nful for many dense prediction tasks. Unlike the recently-\nproposed Vision Transformer (ViT) that was designed for\nimage classification specifically, we introduce the Pyra-\nmid Vision Transformer (PVT), which overcomes the diffi-\nculties of porting Transformer to various dense prediction\ntasks. PVT has several merits compared to current state\nof the arts. (1) Different from ViT that typically yields low-\nresolution outputs and incurs high computational and mem-\nory costs, PVT not only can be trained on dense partitions\nof an image to achieve high output resolution, which is im-\nportant for dense prediction, but also uses a progressive\nshrinking pyramid to reduce the computations of large fea-\nture maps. (2) PVT inherits the advantages of both CNN\nand Transformer, making it a unified backbone for vari-"
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 2895
        },
        {
          "x": 1196,
          "y": 2895
        },
        {
          "x": 1196,
          "y": 2972
        },
        {
          "x": 204,
          "y": 2972
        }
      ],
      "category": "paragraph",
      "html": "<p id='12' style='font-size:16px'>X Corresponding authors: Deng-Ping Fan (dengpfan@gmail.com);<br>Tong Lu (lutong@nju.edu.cn).</p>",
      "id": 12,
      "page": 1,
      "text": "X Corresponding authors: Deng-Ping Fan (dengpfan@gmail.com);\nTong Lu (lutong@nju.edu.cn)."
    },
    {
      "bounding_box": [
        {
          "x": 1277,
          "y": 1921
        },
        {
          "x": 2275,
          "y": 1921
        },
        {
          "x": 2275,
          "y": 2468
        },
        {
          "x": 1277,
          "y": 2468
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='13' style='font-size:20px'>ous vision tasks without convolutions, where it can be used<br>as a direct replacement for CNN backbones. (3) We val-<br>idate PVT through extensive experiments, showing that it<br>boosts the performance of many downstream tasks, includ-<br>ing object detection, instance and semantic segmentation.<br>For example, with a comparable number of parameters,<br>PVT+RetinaNet achieves 40.4 AP on the COCO dataset,<br>surpassing ResNet50+RetinNet (36.3 AP) by 4.1 absolute<br>AP (see Figure 2). We hope that PVT could serve as an<br>alternative and useful backbone for pixel-level predictions<br>and facilitate future research.</p>",
      "id": 13,
      "page": 1,
      "text": "ous vision tasks without convolutions, where it can be used\nas a direct replacement for CNN backbones. (3) We val-\nidate PVT through extensive experiments, showing that it\nboosts the performance of many downstream tasks, includ-\ning object detection, instance and semantic segmentation.\nFor example, with a comparable number of parameters,\nPVT+RetinaNet achieves 40.4 AP on the COCO dataset,\nsurpassing ResNet50+RetinNet (36.3 AP) by 4.1 absolute\nAP (see Figure 2). We hope that PVT could serve as an\nalternative and useful backbone for pixel-level predictions\nand facilitate future research."
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 2585
        },
        {
          "x": 1610,
          "y": 2585
        },
        {
          "x": 1610,
          "y": 2637
        },
        {
          "x": 1281,
          "y": 2637
        }
      ],
      "category": "paragraph",
      "html": "<p id='14' style='font-size:20px'>1. Introduction</p>",
      "id": 14,
      "page": 1,
      "text": "1. Introduction"
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 2677
        },
        {
          "x": 2277,
          "y": 2677
        },
        {
          "x": 2277,
          "y": 2976
        },
        {
          "x": 1280,
          "y": 2976
        }
      ],
      "category": "paragraph",
      "html": "<p id='15' style='font-size:18px'>Convolutional neural network (CNNs) have achieved re-<br>markable success in computer vision, making them a ver-<br>satile and dominant approach for almost all tasks [54, 22,<br>73, 49, 21, 39, 9, 32]. Nevertheless, this work aims to ex-<br>plore an alternative backbone network beyond CNN, which<br>can be used for dense prediction tasks such as object detec-</p>",
      "id": 15,
      "page": 1,
      "text": "Convolutional neural network (CNNs) have achieved re-\nmarkable success in computer vision, making them a ver-\nsatile and dominant approach for almost all tasks [54, 22,\n73, 49, 21, 39, 9, 32]. Nevertheless, this work aims to ex-\nplore an alternative backbone network beyond CNN, which\ncan be used for dense prediction tasks such as object detec-"
    },
    {
      "bounding_box": [
        {
          "x": 60,
          "y": 866
        },
        {
          "x": 152,
          "y": 866
        },
        {
          "x": 152,
          "y": 2332
        },
        {
          "x": 60,
          "y": 2332
        }
      ],
      "category": "footer",
      "html": "<br><footer id='16' style='font-size:14px'>2021<br>Aug<br>11<br>[cs.CV]<br>arXiv:2102.12122v2</footer>",
      "id": 16,
      "page": 1,
      "text": "2021\nAug\n11\n[cs.CV]\narXiv:2102.12122v2"
    },
    {
      "bounding_box": [
        {
          "x": 1224,
          "y": 3058
        },
        {
          "x": 1249,
          "y": 3058
        },
        {
          "x": 1249,
          "y": 3090
        },
        {
          "x": 1224,
          "y": 3090
        }
      ],
      "category": "footer",
      "html": "<footer id='17' style='font-size:16px'>1</footer>",
      "id": 17,
      "page": 1,
      "text": "1"
    },
    {
      "bounding_box": [
        {
          "x": 215,
          "y": 287
        },
        {
          "x": 1203,
          "y": 287
        },
        {
          "x": 1203,
          "y": 1028
        },
        {
          "x": 215,
          "y": 1028
        }
      ],
      "category": "figure",
      "html": "<figure><img id='18' style='font-size:14px' alt=\"44\nPVT-L\nPVT-M\n42\nPVT-\n40 X101-64x4d\n(%) X101-32x4d\nBackbone #Param (M) AP\nAP 38\nPVT-T R101 R18 [22] 21.3 31.8\nBBOX 36\nPVT-T (ours) 23.0 36.7\nR50 R50 [22] 37.7 36.3\nPVT-S (ours) 34.2 40.4\nCOCO 34\nR101[22] 56.7 38.5\nX101-32x4d [73] 56.4 39.9\n32\nViT-S/32 [13] 60.8 31.7\nR18 ViT-S/32\nPVT-M (ours) 53.9 41.9\n30\nX101-64x4d [73] 95.5 41.0\nPVT-L (ours) 71.1 42.6\n28\n20 30 40 50 60 70 80 90 100 110 120 130\n#Parameter (M)\" data-coord=\"top-left:(215,287); bottom-right:(1203,1028)\" /></figure>",
      "id": 18,
      "page": 2,
      "text": "44\nPVT-L\nPVT-M\n42\nPVT-\n40 X101-64x4d\n(%) X101-32x4d\nBackbone #Param (M) AP\nAP 38\nPVT-T R101 R18 [22] 21.3 31.8\nBBOX 36\nPVT-T (ours) 23.0 36.7\nR50 R50 [22] 37.7 36.3\nPVT-S (ours) 34.2 40.4\nCOCO 34\nR101[22] 56.7 38.5\nX101-32x4d [73] 56.4 39.9\n32\nViT-S/32 [13] 60.8 31.7\nR18 ViT-S/32\nPVT-M (ours) 53.9 41.9\n30\nX101-64x4d [73] 95.5 41.0\nPVT-L (ours) 71.1 42.6\n28\n20 30 40 50 60 70 80 90 100 110 120 130\n#Parameter (M)"
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 1060
        },
        {
          "x": 1200,
          "y": 1060
        },
        {
          "x": 1200,
          "y": 1465
        },
        {
          "x": 200,
          "y": 1465
        }
      ],
      "category": "caption",
      "html": "<caption id='19' style='font-size:18px'>Figure 2: Performance comparison on COCO val2017<br>of different backbones using RetinaNet for object detec-<br>tion, where \"T\" \"S\" , \"M\" and \"L\" denote our PVT models<br>,<br>with tiny, small, medium and large size. We see that when<br>the number of parameters among different models are com-<br>parable, PVT variants significantly outperform their corre-<br>sponding counterparts such as ResNets (R) [22], ResNeXts<br>(X) [73], and ViT [13].</caption>",
      "id": 19,
      "page": 2,
      "text": "Figure 2: Performance comparison on COCO val2017\nof different backbones using RetinaNet for object detec-\ntion, where \"T\" \"S\" , \"M\" and \"L\" denote our PVT models\n,\nwith tiny, small, medium and large size. We see that when\nthe number of parameters among different models are com-\nparable, PVT variants significantly outperform their corre-\nsponding counterparts such as ResNets (R) [22], ResNeXts\n(X) [73], and ViT [13]."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 1561
        },
        {
          "x": 1195,
          "y": 1561
        },
        {
          "x": 1195,
          "y": 1656
        },
        {
          "x": 202,
          "y": 1656
        }
      ],
      "category": "paragraph",
      "html": "<p id='20' style='font-size:22px'>tion [40, 14], semantic [83] and instance segmentation [40],<br>in addition to image classification [12].</p>",
      "id": 20,
      "page": 2,
      "text": "tion [40, 14], semantic [83] and instance segmentation [40],\nin addition to image classification [12]."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 1665
        },
        {
          "x": 1199,
          "y": 1665
        },
        {
          "x": 1199,
          "y": 2211
        },
        {
          "x": 201,
          "y": 2211
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='21' style='font-size:18px'>Inspired by the success of Transformer [64] in natu-<br>ral language processing, many researchers have explored<br>its application in computer vision. For example, some<br>works [6, 85, 72, 56, 24, 42] model the vision task as a dic-<br>tionary lookup problem with learnable queries, and use the<br>Transformer decoder as a task-specific head on top of the<br>CNN backbone. Although some prior arts have also incor-<br>porated attention modules [70, 48, 80] into CNNs, as far<br>as we know, exploring a clean and convolution-free Trans-<br>former backbone to address dense prediction tasks in com-<br>puter vision is rarely studied.</p>",
      "id": 21,
      "page": 2,
      "text": "Inspired by the success of Transformer [64] in natu-\nral language processing, many researchers have explored\nits application in computer vision. For example, some\nworks [6, 85, 72, 56, 24, 42] model the vision task as a dic-\ntionary lookup problem with learnable queries, and use the\nTransformer decoder as a task-specific head on top of the\nCNN backbone. Although some prior arts have also incor-\nporated attention modules [70, 48, 80] into CNNs, as far\nas we know, exploring a clean and convolution-free Trans-\nformer backbone to address dense prediction tasks in com-\nputer vision is rarely studied."
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 2216
        },
        {
          "x": 1200,
          "y": 2216
        },
        {
          "x": 1200,
          "y": 2768
        },
        {
          "x": 200,
          "y": 2768
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='22' style='font-size:20px'>Recently, Dosovitskiy et al. [13] introduced the Vision<br>Transformer (ViT) for image classification. This is an in-<br>teresting and meaningful attempt to replace the CNN back-<br>bone with a convolution-free model. As shown in Figure 1<br>(b), ViT has a columnar structure with coarse image patches<br>as input. 1 Although ViT is applicable to image classifi-<br>cation, it is challenging to directly adapt it to pixel-level<br>dense predictions such as object detection and segmenta-<br>tion, because (1) its output feature map is single-scale and<br>low-resolution, and (2) its computational and memory costs<br>are relatively high even for common input image sizes (e.g.,</p>",
      "id": 22,
      "page": 2,
      "text": "Recently, Dosovitskiy et al. [13] introduced the Vision\nTransformer (ViT) for image classification. This is an in-\nteresting and meaningful attempt to replace the CNN back-\nbone with a convolution-free model. As shown in Figure 1\n(b), ViT has a columnar structure with coarse image patches\nas input. 1 Although ViT is applicable to image classifi-\ncation, it is challenging to directly adapt it to pixel-level\ndense predictions such as object detection and segmenta-\ntion, because (1) its output feature map is single-scale and\nlow-resolution, and (2) its computational and memory costs\nare relatively high even for common input image sizes (e.g.,"
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 2812
        },
        {
          "x": 1200,
          "y": 2812
        },
        {
          "x": 1200,
          "y": 2974
        },
        {
          "x": 200,
          "y": 2974
        }
      ],
      "category": "paragraph",
      "html": "<p id='23' style='font-size:14px'>1Due to resource constraints, ViT cannot use fine-grained image<br>patches (e.g., 4x4 pixels per patch) as input, instead only receive coarse<br>patches (e.g., 32x32 pixels per patch) as input, which leads to its low out-<br>put resolution (e.g., 32-stride).</p>",
      "id": 23,
      "page": 2,
      "text": "1Due to resource constraints, ViT cannot use fine-grained image\npatches (e.g., 4x4 pixels per patch) as input, instead only receive coarse\npatches (e.g., 32x32 pixels per patch) as input, which leads to its low out-\nput resolution (e.g., 32-stride)."
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 307
        },
        {
          "x": 2247,
          "y": 307
        },
        {
          "x": 2247,
          "y": 353
        },
        {
          "x": 1281,
          "y": 353
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='24' style='font-size:22px'>shorter edge of 800 pixels in the COCO benchmark [40]).</p>",
      "id": 24,
      "page": 2,
      "text": "shorter edge of 800 pixels in the COCO benchmark [40])."
    },
    {
      "bounding_box": [
        {
          "x": 1277,
          "y": 366
        },
        {
          "x": 2277,
          "y": 366
        },
        {
          "x": 2277,
          "y": 1104
        },
        {
          "x": 1277,
          "y": 1104
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='25' style='font-size:20px'>To address the above limitations, this work proposes a<br>pure Transformer backbone, termed Pyramid Vision Trans-<br>former (PVT), which can serve as an alternative to the CNN<br>backbone in many downstream tasks, including image-level<br>prediction as well as pixel-level dense predictions. Specifi-<br>cally, as illustrated in Figure 1 (c), our PVT overcomes the<br>difficulties of the conventional Transformer by (1) taking<br>fine-grained image patches (i.e., 4x4 pixels per patch) as in-<br>put to learn high-resolution representation, which is essen-<br>tial for dense prediction tasks; (2) introducing a progressive<br>shrinking pyramid to reduce the sequence length of Trans-<br>former as the network deepens, significantly reducing the<br>computational cost, and (3) adopting a spatial-reduction at-<br>tention (SRA) layer to further reduce the resource consump-<br>tion when learning high-resolution features.</p>",
      "id": 25,
      "page": 2,
      "text": "To address the above limitations, this work proposes a\npure Transformer backbone, termed Pyramid Vision Trans-\nformer (PVT), which can serve as an alternative to the CNN\nbackbone in many downstream tasks, including image-level\nprediction as well as pixel-level dense predictions. Specifi-\ncally, as illustrated in Figure 1 (c), our PVT overcomes the\ndifficulties of the conventional Transformer by (1) taking\nfine-grained image patches (i.e., 4x4 pixels per patch) as in-\nput to learn high-resolution representation, which is essen-\ntial for dense prediction tasks; (2) introducing a progressive\nshrinking pyramid to reduce the sequence length of Trans-\nformer as the network deepens, significantly reducing the\ncomputational cost, and (3) adopting a spatial-reduction at-\ntention (SRA) layer to further reduce the resource consump-\ntion when learning high-resolution features."
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 1113
        },
        {
          "x": 2278,
          "y": 1113
        },
        {
          "x": 2278,
          "y": 1809
        },
        {
          "x": 1278,
          "y": 1809
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='26' style='font-size:20px'>Overall, the proposed PVT possesses the following mer-<br>its. Firstly, compared to the traditional CNN backbones<br>(see Figure 1 (a)), which have local receptive fields that in-<br>crease with the network depth, our PVT always produces a<br>global receptive field, which is more suitable for detection<br>and segmentation. Secondly, compared to ViT (see Fig-<br>ure 1 (b)), thanks to its advanced pyramid structure, our<br>method can more easily be plugged into many represen-<br>tative dense prediction pipelines, e.g., RetinaNet [39] and<br>Mask R-CNN [21]. Thirdly, we can build a convolution-<br>free pipeline by combining our PVT with other task-specific<br>Transformer decoders, such as PVT+DETR [6] for ob-<br>ject detection. To our knowledge, this is the first entirely<br>convolution-free object detection pipeline.</p>",
      "id": 26,
      "page": 2,
      "text": "Overall, the proposed PVT possesses the following mer-\nits. Firstly, compared to the traditional CNN backbones\n(see Figure 1 (a)), which have local receptive fields that in-\ncrease with the network depth, our PVT always produces a\nglobal receptive field, which is more suitable for detection\nand segmentation. Secondly, compared to ViT (see Fig-\nure 1 (b)), thanks to its advanced pyramid structure, our\nmethod can more easily be plugged into many represen-\ntative dense prediction pipelines, e.g., RetinaNet [39] and\nMask R-CNN [21]. Thirdly, we can build a convolution-\nfree pipeline by combining our PVT with other task-specific\nTransformer decoders, such as PVT+DETR [6] for ob-\nject detection. To our knowledge, this is the first entirely\nconvolution-free object detection pipeline."
    },
    {
      "bounding_box": [
        {
          "x": 1331,
          "y": 1816
        },
        {
          "x": 1977,
          "y": 1816
        },
        {
          "x": 1977,
          "y": 1862
        },
        {
          "x": 1331,
          "y": 1862
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='27' style='font-size:14px'>Our main contributions are as follows:</p>",
      "id": 27,
      "page": 2,
      "text": "Our main contributions are as follows:"
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 1872
        },
        {
          "x": 2277,
          "y": 1872
        },
        {
          "x": 2277,
          "y": 2216
        },
        {
          "x": 1280,
          "y": 2216
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='28' style='font-size:16px'>(1) We propose Pyramid Vision Transformer (PVT),<br>which is the first pure Transformer backbone designed for<br>various pixel-level dense prediction tasks. Combining our<br>PVT and DETR, we can construct an end-to-end object de-<br>tection system without convolutions and handcrafted com-<br>ponents such as dense anchors and non-maximum suppres-<br>sion (NMS).</p>",
      "id": 28,
      "page": 2,
      "text": "(1) We propose Pyramid Vision Transformer (PVT),\nwhich is the first pure Transformer backbone designed for\nvarious pixel-level dense prediction tasks. Combining our\nPVT and DETR, we can construct an end-to-end object de-\ntection system without convolutions and handcrafted com-\nponents such as dense anchors and non-maximum suppres-\nsion (NMS)."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 2223
        },
        {
          "x": 2277,
          "y": 2223
        },
        {
          "x": 2277,
          "y": 2520
        },
        {
          "x": 1280,
          "y": 2520
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='29' style='font-size:20px'>(2) We overcome many difficulties when porting Trans-<br>former to dense predictions, by designing a progressive<br>shrinking pyramid and a spatial-reduction attention (SRA).<br>These are able to reduce the resource consumption of Trans-<br>former, making PVT flexible to learning multi-scale and<br>high-resolution features.</p>",
      "id": 29,
      "page": 2,
      "text": "(2) We overcome many difficulties when porting Trans-\nformer to dense predictions, by designing a progressive\nshrinking pyramid and a spatial-reduction attention (SRA).\nThese are able to reduce the resource consumption of Trans-\nformer, making PVT flexible to learning multi-scale and\nhigh-resolution features."
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 2527
        },
        {
          "x": 2278,
          "y": 2527
        },
        {
          "x": 2278,
          "y": 2977
        },
        {
          "x": 1279,
          "y": 2977
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='30' style='font-size:20px'>(3) We evaluate the proposed PVT on several differ-<br>ent tasks, including image classification, object detection,<br>instance and semantic segmentation, and compare it with<br>popular ResNets [22] and ResNeXts [73]. As presented<br>in Figure 2, our PVT with different parameter scales can<br>consistently archived improved performance compared to<br>the prior arts. For example, under a comparable number<br>of parameters, using RetinaNet [39] for object detection,<br>PVT-Small achieves 40.4 AP on COCO val201 7, outper-</p>",
      "id": 30,
      "page": 2,
      "text": "(3) We evaluate the proposed PVT on several differ-\nent tasks, including image classification, object detection,\ninstance and semantic segmentation, and compare it with\npopular ResNets [22] and ResNeXts [73]. As presented\nin Figure 2, our PVT with different parameter scales can\nconsistently archived improved performance compared to\nthe prior arts. For example, under a comparable number\nof parameters, using RetinaNet [39] for object detection,\nPVT-Small achieves 40.4 AP on COCO val201 7, outper-"
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 305
        },
        {
          "x": 1200,
          "y": 305
        },
        {
          "x": 1200,
          "y": 454
        },
        {
          "x": 201,
          "y": 454
        }
      ],
      "category": "paragraph",
      "html": "<p id='31' style='font-size:16px'>forming ResNet50 by 4.1 points (40.4 vs. 36.3). Moreover,<br>PVT-Large achieves 42.6 AP, which is 1.6 points better than<br>ResNeXt101-64x4d, with 30% less parameters.</p>",
      "id": 31,
      "page": 3,
      "text": "forming ResNet50 by 4.1 points (40.4 vs. 36.3). Moreover,\nPVT-Large achieves 42.6 AP, which is 1.6 points better than\nResNeXt101-64x4d, with 30% less parameters."
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 505
        },
        {
          "x": 559,
          "y": 505
        },
        {
          "x": 559,
          "y": 556
        },
        {
          "x": 203,
          "y": 556
        }
      ],
      "category": "paragraph",
      "html": "<p id='32' style='font-size:22px'>2. Related Work</p>",
      "id": 32,
      "page": 3,
      "text": "2. Related Work"
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 587
        },
        {
          "x": 613,
          "y": 587
        },
        {
          "x": 613,
          "y": 635
        },
        {
          "x": 204,
          "y": 635
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='33' style='font-size:18px'>2.1. CNN Backbones</p>",
      "id": 33,
      "page": 3,
      "text": "2.1. CNN Backbones"
    },
    {
      "bounding_box": [
        {
          "x": 199,
          "y": 664
        },
        {
          "x": 1200,
          "y": 664
        },
        {
          "x": 1200,
          "y": 1813
        },
        {
          "x": 199,
          "y": 1813
        }
      ],
      "category": "paragraph",
      "html": "<p id='34' style='font-size:14px'>CNNs are the work-horses of deep neural networks in vi-<br>sual recognition. The standard CNN was first introduced in<br>[34] to distinguish handwritten numbers. The model con-<br>tains convolutional kernels with a certain receptive field<br>that captures favorable visual context. To provide trans-<br>lation equivariance, the weights of convolutional kernels<br>are shared over the entire image space. More recently,<br>with the rapid development of the computational resources<br>(e.g., GPU), the successful training of stacked convolutional<br>blocks [33, 54] on large-scale image classification datasets<br>(e.g., ImageNet [51]) has become possible. For instance,<br>GoogLeNet [59] demonstrated that a convolutional opera-<br>tor containing multiple kernel paths can achieve very com-<br>petitive performance. The effectiveness of a multi-path<br>convolutional block was further validated in Inception se-<br>ries [60, 58], ResNeXt [73], DPN [10], MixNet [65] and<br>SKNet [36]. Further, ResNet [22] introduced skip connec-<br>tions into the convolutional block, making it possible to cre-<br>ate/train very deep networks and obtaining impressive re-<br>sults in the field of computer vision. DenseNet [25] intro-<br>duced a densely connected topology, which connects each<br>convolutional block to all previous blocks. More recent ad-<br>vances can be found in recent survey/review papers [31, 53].</p>",
      "id": 34,
      "page": 3,
      "text": "CNNs are the work-horses of deep neural networks in vi-\nsual recognition. The standard CNN was first introduced in\n[34] to distinguish handwritten numbers. The model con-\ntains convolutional kernels with a certain receptive field\nthat captures favorable visual context. To provide trans-\nlation equivariance, the weights of convolutional kernels\nare shared over the entire image space. More recently,\nwith the rapid development of the computational resources\n(e.g., GPU), the successful training of stacked convolutional\nblocks [33, 54] on large-scale image classification datasets\n(e.g., ImageNet [51]) has become possible. For instance,\nGoogLeNet [59] demonstrated that a convolutional opera-\ntor containing multiple kernel paths can achieve very com-\npetitive performance. The effectiveness of a multi-path\nconvolutional block was further validated in Inception se-\nries [60, 58], ResNeXt [73], DPN [10], MixNet [65] and\nSKNet [36]. Further, ResNet [22] introduced skip connec-\ntions into the convolutional block, making it possible to cre-\nate/train very deep networks and obtaining impressive re-\nsults in the field of computer vision. DenseNet [25] intro-\nduced a densely connected topology, which connects each\nconvolutional block to all previous blocks. More recent ad-\nvances can be found in recent survey/review papers [31, 53]."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 1814
        },
        {
          "x": 1200,
          "y": 1814
        },
        {
          "x": 1200,
          "y": 2060
        },
        {
          "x": 202,
          "y": 2060
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='35' style='font-size:14px'>Unlike the full-blown CNNs, the vision Transformer<br>backbone is still in its early stage of development. In this<br>work, we try to extend the scope of Vision Transformer by<br>designing a new versatile Transformer backbone suitable<br>for most vision tasks.</p>",
      "id": 35,
      "page": 3,
      "text": "Unlike the full-blown CNNs, the vision Transformer\nbackbone is still in its early stage of development. In this\nwork, we try to extend the scope of Vision Transformer by\ndesigning a new versatile Transformer backbone suitable\nfor most vision tasks."
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 2097
        },
        {
          "x": 745,
          "y": 2097
        },
        {
          "x": 745,
          "y": 2148
        },
        {
          "x": 203,
          "y": 2148
        }
      ],
      "category": "paragraph",
      "html": "<p id='36' style='font-size:16px'>2.2. Dense Prediction Tasks</p>",
      "id": 36,
      "page": 3,
      "text": "2.2. Dense Prediction Tasks"
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 2177
        },
        {
          "x": 1198,
          "y": 2177
        },
        {
          "x": 1198,
          "y": 2374
        },
        {
          "x": 201,
          "y": 2374
        }
      ],
      "category": "paragraph",
      "html": "<p id='37' style='font-size:14px'>Preliminary. The dense prediction task aims to perform<br>pixel-level classification or regression on a feature map.<br>Object detection and semantic segmentation are two rep-<br>resentative dense prediction tasks.</p>",
      "id": 37,
      "page": 3,
      "text": "Preliminary. The dense prediction task aims to perform\npixel-level classification or regression on a feature map.\nObject detection and semantic segmentation are two rep-\nresentative dense prediction tasks."
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 2378
        },
        {
          "x": 1199,
          "y": 2378
        },
        {
          "x": 1199,
          "y": 2978
        },
        {
          "x": 200,
          "y": 2978
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='38' style='font-size:16px'>Object Detection. In the era of deep learning,<br>CNNs [34] have become the dominant framework for ob-<br>ject detection, which includes single-stage detectors (e.g.,<br>SSD [43], RetinaNet [39], FCOS [62], GFL [37, 35], Po-<br>larMask [71] and OneNet [55]) and multi-stage detectors<br>(Faster R-CNN [49], Mask R-CNN [21], Cascade R-CNN<br>[4] and Sparse R-CNN [57]). Most of these popular ob-<br>ject detectors are built on high-resolution or multi-scale fea-<br>ture maps to obtain good detection performance. Recently,<br>DETR [6] and deformable DETR [85] combined the CNN<br>backbone and the Transformer decoder to build an end-<br>to-end object detector. Likewise, they also require high-</p>",
      "id": 38,
      "page": 3,
      "text": "Object Detection. In the era of deep learning,\nCNNs [34] have become the dominant framework for ob-\nject detection, which includes single-stage detectors (e.g.,\nSSD [43], RetinaNet [39], FCOS [62], GFL [37, 35], Po-\nlarMask [71] and OneNet [55]) and multi-stage detectors\n(Faster R-CNN [49], Mask R-CNN [21], Cascade R-CNN\n[4] and Sparse R-CNN [57]). Most of these popular ob-\nject detectors are built on high-resolution or multi-scale fea-\nture maps to obtain good detection performance. Recently,\nDETR [6] and deformable DETR [85] combined the CNN\nbackbone and the Transformer decoder to build an end-\nto-end object detector. Likewise, they also require high-"
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 308
        },
        {
          "x": 2276,
          "y": 308
        },
        {
          "x": 2276,
          "y": 401
        },
        {
          "x": 1278,
          "y": 401
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='39' style='font-size:14px'>resolution or multi-scale feature maps for accurate object<br>detection.</p>",
      "id": 39,
      "page": 3,
      "text": "resolution or multi-scale feature maps for accurate object\ndetection."
    },
    {
      "bounding_box": [
        {
          "x": 1276,
          "y": 411
        },
        {
          "x": 2279,
          "y": 411
        },
        {
          "x": 2279,
          "y": 1358
        },
        {
          "x": 1276,
          "y": 1358
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='40' style='font-size:14px'>Semantic Segmentation. CNNs also play an important<br>role in semantic segmentation. In the early stages, FCN<br>[44] introduced a fully convolutional architecture to gen-<br>erate a spatial segmentation map for a given image of any<br>size. After that, the deconvolution operation was introduced<br>by Noh et al. [47] and achieved impressive performance on<br>the PASCAL VOC 2012 dataset [52]. Inspired by FCN, U-<br>Net [50] was proposed for the medical image segmentation<br>domain specifically, bridging the information flow between<br>corresponding low-level and high-level feature maps of the<br>same spatial sizes. To explore richer global context rep-<br>resentation, Zhao et al. [81] designed a pyramid pooling<br>module over various pooling scales, and Kirillov et al. [32]<br>developed a lightweight segmentation head termed Seman-<br>tic FPN, based on FPN [38]. Finally, the DeepLab family<br>[8, 41] applies dilated convolutions to enlarge the receptive<br>field while maintaining the feature map resolution. Similar<br>to object detection methods, semantic segmentation models<br>also rely on high-resolution or multi-scale feature maps.</p>",
      "id": 40,
      "page": 3,
      "text": "Semantic Segmentation. CNNs also play an important\nrole in semantic segmentation. In the early stages, FCN\n[44] introduced a fully convolutional architecture to gen-\nerate a spatial segmentation map for a given image of any\nsize. After that, the deconvolution operation was introduced\nby Noh et al. [47] and achieved impressive performance on\nthe PASCAL VOC 2012 dataset [52]. Inspired by FCN, U-\nNet [50] was proposed for the medical image segmentation\ndomain specifically, bridging the information flow between\ncorresponding low-level and high-level feature maps of the\nsame spatial sizes. To explore richer global context rep-\nresentation, Zhao et al. [81] designed a pyramid pooling\nmodule over various pooling scales, and Kirillov et al. [32]\ndeveloped a lightweight segmentation head termed Seman-\ntic FPN, based on FPN [38]. Finally, the DeepLab family\n[8, 41] applies dilated convolutions to enlarge the receptive\nfield while maintaining the feature map resolution. Similar\nto object detection methods, semantic segmentation models\nalso rely on high-resolution or multi-scale feature maps."
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 1400
        },
        {
          "x": 2174,
          "y": 1400
        },
        {
          "x": 2174,
          "y": 1450
        },
        {
          "x": 1279,
          "y": 1450
        }
      ],
      "category": "paragraph",
      "html": "<p id='41' style='font-size:20px'>2.3. Self-Attention and Transformer in Vision</p>",
      "id": 41,
      "page": 3,
      "text": "2.3. Self-Attention and Transformer in Vision"
    },
    {
      "bounding_box": [
        {
          "x": 1276,
          "y": 1483
        },
        {
          "x": 2278,
          "y": 1483
        },
        {
          "x": 2278,
          "y": 2979
        },
        {
          "x": 1276,
          "y": 2979
        }
      ],
      "category": "paragraph",
      "html": "<p id='42' style='font-size:14px'>As convolutional filter weights are usually fixed after<br>training, they cannot be dynamically adapted to different<br>inputs. Many methods have been proposed to alleviate this<br>problem using dynamic filters [30] or self-attention oper-<br>ations [64]. The non-local block [70] attempts to model<br>long-range dependencies in both space and time, which<br>has been shown beneficial for accurate video classifica-<br>tion. However, despite its success, the non-local opera-<br>tor suffers from the high computational and memory costs.<br>Criss-cross [26] further reduces the complexity by gen-<br>erating sparse attention maps through a criss-cross path.<br>Ramachandran et al. [48] proposed the stand-alone self-<br>attention to replace convolutional layers with local self-<br>attention units. AANet [3] achieves competitive results<br>when combining the self-attention and convolutional oper-<br>ations. LambdaNetworks [2] uses the lambda layer, an ef-<br>ficient self-attention to replace the convolution in the CNN.<br>DETR [6] utilizes the Transformer decoder to model ob-<br>ject detection as an end-to-end dictionary lookup problem<br>with learnable queries, successfully removing the need for<br>handcrafted processes such as NMS. Based on DETR, de-<br>formable DETR [85] further adopts a deformable atten-<br>tion layer to focus on a sparse set of contextual elements,<br>obtaining faster convergence and better performance. Re-<br>cently, Vision Transformer (ViT) [13] employs a pure<br>Transformer [64] model for image classification by treat-<br>ing an image as a sequence of patches. DeiT [63] further<br>extends ViT using a novel distillation approach. Different<br>from previous models, this work introduces the pyramid<br>structure into Transformer to present a pure Transformer</p>",
      "id": 42,
      "page": 3,
      "text": "As convolutional filter weights are usually fixed after\ntraining, they cannot be dynamically adapted to different\ninputs. Many methods have been proposed to alleviate this\nproblem using dynamic filters [30] or self-attention oper-\nations [64]. The non-local block [70] attempts to model\nlong-range dependencies in both space and time, which\nhas been shown beneficial for accurate video classifica-\ntion. However, despite its success, the non-local opera-\ntor suffers from the high computational and memory costs.\nCriss-cross [26] further reduces the complexity by gen-\nerating sparse attention maps through a criss-cross path.\nRamachandran et al. [48] proposed the stand-alone self-\nattention to replace convolutional layers with local self-\nattention units. AANet [3] achieves competitive results\nwhen combining the self-attention and convolutional oper-\nations. LambdaNetworks [2] uses the lambda layer, an ef-\nficient self-attention to replace the convolution in the CNN.\nDETR [6] utilizes the Transformer decoder to model ob-\nject detection as an end-to-end dictionary lookup problem\nwith learnable queries, successfully removing the need for\nhandcrafted processes such as NMS. Based on DETR, de-\nformable DETR [85] further adopts a deformable atten-\ntion layer to focus on a sparse set of contextual elements,\nobtaining faster convergence and better performance. Re-\ncently, Vision Transformer (ViT) [13] employs a pure\nTransformer [64] model for image classification by treat-\ning an image as a sequence of patches. DeiT [63] further\nextends ViT using a novel distillation approach. Different\nfrom previous models, this work introduces the pyramid\nstructure into Transformer to present a pure Transformer"
    },
    {
      "bounding_box": [
        {
          "x": 254,
          "y": 308
        },
        {
          "x": 2233,
          "y": 308
        },
        {
          "x": 2233,
          "y": 1230
        },
        {
          "x": 254,
          "y": 1230
        }
      ],
      "category": "figure",
      "html": "<figure><img id='43' style='font-size:22px' alt=\"HxW x3 H W H W H W H W\nF1: 4x4 xC1 F2: 8 x 8 xC2 F3: 16 x16 XC3 F4: x xC4\n32 32\nStage 1 Stage 2 Stage 3 Stage 4\nPatch Encoder Patch Enco\nPatch\nEncoder Patch Encoder\nEmb\nEmb Emb\nEmb\nStage i\nSRA\nMulti-Head\nAttention\nForward\nNorm Feed Reshape\nNorm Reshape Norm Reduction\nSpacial Wi-1\nPosition Embedding Linear\nxCi\nP2\nElement-wise Add Patch li-1\nW Embedding\nxCi\nFeature Map X(Pi2Ci-1) Transformer Encoder (Lix)\" data-coord=\"top-left:(254,308); bottom-right:(2233,1230)\" /></figure>",
      "id": 43,
      "page": 4,
      "text": "HxW x3 H W H W H W H W\nF1: 4x4 xC1 F2: 8 x 8 xC2 F3: 16 x16 XC3 F4: x xC4\n32 32\nStage 1 Stage 2 Stage 3 Stage 4\nPatch Encoder Patch Enco\nPatch\nEncoder Patch Encoder\nEmb\nEmb Emb\nEmb\nStage i\nSRA\nMulti-Head\nAttention\nForward\nNorm Feed Reshape\nNorm Reshape Norm Reduction\nSpacial Wi-1\nPosition Embedding Linear\nxCi\nP2\nElement-wise Add Patch li-1\nW Embedding\nxCi\nFeature Map X(Pi2Ci-1) Transformer Encoder (Lix)"
    },
    {
      "bounding_box": [
        {
          "x": 198,
          "y": 1266
        },
        {
          "x": 2279,
          "y": 1266
        },
        {
          "x": 2279,
          "y": 1426
        },
        {
          "x": 198,
          "y": 1426
        }
      ],
      "category": "caption",
      "html": "<caption id='44' style='font-size:18px'>Figure 3: Overall architecture of Pyramid Vision Transformer (PVT). The entire model is divided into four stages, each<br>of which is comprised of a patch embedding layer and a Li-layer Transformer encoder. Following a pyramid structure, the<br>output resolution of the four stages progressively shrinks from high (4-stride) to low (32-stride).</caption>",
      "id": 44,
      "page": 4,
      "text": "Figure 3: Overall architecture of Pyramid Vision Transformer (PVT). The entire model is divided into four stages, each\nof which is comprised of a patch embedding layer and a Li-layer Transformer encoder. Following a pyramid structure, the\noutput resolution of the four stages progressively shrinks from high (4-stride) to low (32-stride)."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 1511
        },
        {
          "x": 1199,
          "y": 1511
        },
        {
          "x": 1199,
          "y": 1611
        },
        {
          "x": 201,
          "y": 1611
        }
      ],
      "category": "paragraph",
      "html": "<p id='45' style='font-size:16px'>backbone for dense prediction tasks, rather than a task-<br>specific head or an image classification model.</p>",
      "id": 45,
      "page": 4,
      "text": "backbone for dense prediction tasks, rather than a task-\nspecific head or an image classification model."
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 1653
        },
        {
          "x": 1024,
          "y": 1653
        },
        {
          "x": 1024,
          "y": 1710
        },
        {
          "x": 200,
          "y": 1710
        }
      ],
      "category": "paragraph",
      "html": "<p id='46' style='font-size:22px'>3. Pyramid Vision Transformer (PVT)</p>",
      "id": 46,
      "page": 4,
      "text": "3. Pyramid Vision Transformer (PVT)"
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 1738
        },
        {
          "x": 698,
          "y": 1738
        },
        {
          "x": 698,
          "y": 1786
        },
        {
          "x": 202,
          "y": 1786
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='47' style='font-size:20px'>3.1. Overall Architecture</p>",
      "id": 47,
      "page": 4,
      "text": "3.1. Overall Architecture"
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 1814
        },
        {
          "x": 1200,
          "y": 1814
        },
        {
          "x": 1200,
          "y": 2260
        },
        {
          "x": 201,
          "y": 2260
        }
      ],
      "category": "paragraph",
      "html": "<p id='48' style='font-size:16px'>Our goal is to introduce the pyramid structure into the<br>Transformer framework, SO that it can generate multi-scale<br>feature maps for dense prediction tasks (e.g., object detec-<br>tion and semantic segmentation). An overview of PVT is<br>depicted in Figure 3. Similar to CNN backbones [22], our<br>method has four stages that generate feature maps of dif-<br>ferent scales. All stages share a similar architecture, which<br>consists of a patch embedding layer and Li Transformer en-<br>coder layers.</p>",
      "id": 48,
      "page": 4,
      "text": "Our goal is to introduce the pyramid structure into the\nTransformer framework, SO that it can generate multi-scale\nfeature maps for dense prediction tasks (e.g., object detec-\ntion and semantic segmentation). An overview of PVT is\ndepicted in Figure 3. Similar to CNN backbones [22], our\nmethod has four stages that generate feature maps of dif-\nferent scales. All stages share a similar architecture, which\nconsists of a patch embedding layer and Li Transformer en-\ncoder layers."
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 2261
        },
        {
          "x": 1199,
          "y": 2261
        },
        {
          "x": 1199,
          "y": 2865
        },
        {
          "x": 200,
          "y": 2865
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='49' style='font-size:16px'>In the first stage, given an input image of size HxW x3,<br>HW<br>we first divide it into patches,2 each of size 4x4x 3.<br>42<br>Then, we feed the flattened patches to a linear projection<br>and obtain embedded patches of size HW xC1. After that,<br>42<br>the embedded patches along with a position embedding are<br>passed through a Transformer encoder with L1 layers, and<br>the output is reshaped to a feature map F1 of size H|4 x W4 xC1.<br>In the same way, using the feature map from the previ-<br>ous stage as input, we obtain the following feature maps:<br>F2, F3, and F4, whose strides are 8, 16, and 32 pixels<br>with respect to the input image. With the feature pyramid<br>{F1, F2, F3, F4}, our method can be easily applied to most</p>",
      "id": 49,
      "page": 4,
      "text": "In the first stage, given an input image of size HxW x3,\nHW\nwe first divide it into patches,2 each of size 4x4x 3.\n42\nThen, we feed the flattened patches to a linear projection\nand obtain embedded patches of size HW xC1. After that,\n42\nthe embedded patches along with a position embedding are\npassed through a Transformer encoder with L1 layers, and\nthe output is reshaped to a feature map F1 of size H|4 x W4 xC1.\nIn the same way, using the feature map from the previ-\nous stage as input, we obtain the following feature maps:\nF2, F3, and F4, whose strides are 8, 16, and 32 pixels\nwith respect to the input image. With the feature pyramid\n{F1, F2, F3, F4}, our method can be easily applied to most"
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 2890
        },
        {
          "x": 1197,
          "y": 2890
        },
        {
          "x": 1197,
          "y": 2973
        },
        {
          "x": 201,
          "y": 2973
        }
      ],
      "category": "paragraph",
      "html": "<p id='50' style='font-size:14px'>2As done for ResNet, we keep the highest resolution of our output fea-<br>ture map at 4-stride.</p>",
      "id": 50,
      "page": 4,
      "text": "2As done for ResNet, we keep the highest resolution of our output fea-\nture map at 4-stride."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 1512
        },
        {
          "x": 2275,
          "y": 1512
        },
        {
          "x": 2275,
          "y": 1609
        },
        {
          "x": 1280,
          "y": 1609
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='51' style='font-size:18px'>downstream tasks, including image classification, object de-<br>tection, and semantic segmentation.</p>",
      "id": 51,
      "page": 4,
      "text": "downstream tasks, including image classification, object de-\ntection, and semantic segmentation."
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 1638
        },
        {
          "x": 2031,
          "y": 1638
        },
        {
          "x": 2031,
          "y": 1690
        },
        {
          "x": 1279,
          "y": 1690
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='52' style='font-size:20px'>3.2. Feature Pyramid for Transformer</p>",
      "id": 52,
      "page": 4,
      "text": "3.2. Feature Pyramid for Transformer"
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 1715
        },
        {
          "x": 2278,
          "y": 1715
        },
        {
          "x": 2278,
          "y": 1914
        },
        {
          "x": 1279,
          "y": 1914
        }
      ],
      "category": "paragraph",
      "html": "<p id='53' style='font-size:16px'>Unlike CNN backbone networks [54, 22], which use<br>different convolutional strides to obtain multi-scale feature<br>maps, our PVT uses a progressive shrinking strategy to con-<br>trol the scale of feature maps by patch embedding layers.</p>",
      "id": 53,
      "page": 4,
      "text": "Unlike CNN backbone networks [54, 22], which use\ndifferent convolutional strides to obtain multi-scale feature\nmaps, our PVT uses a progressive shrinking strategy to con-\ntrol the scale of feature maps by patch embedding layers."
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 1918
        },
        {
          "x": 2278,
          "y": 1918
        },
        {
          "x": 2278,
          "y": 2272
        },
        {
          "x": 1278,
          "y": 2272
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='54' style='font-size:16px'>Here, we denote the patch size of the i-th stage as Pi. At<br>the beginning of stage 2, we first evenly divide the input fea-<br>Hi-1 Wi-1<br>ture map Fi-1 E RHi-1XWi-1XCi-1 into patches, and<br>P2<br>then each patch is flatten and projected to a Ci-dimensional<br>embedding. After the linear projection, the shape of the em-<br>Hi-1 Wi-1 Ci, where<br>bedded patches can be viewed as x x<br>Pi P<br>the height and width are Pi times smaller than the input.</p>",
      "id": 54,
      "page": 4,
      "text": "Here, we denote the patch size of the i-th stage as Pi. At\nthe beginning of stage 2, we first evenly divide the input fea-\nHi-1 Wi-1\nture map Fi-1 E RHi-1XWi-1XCi-1 into patches, and\nP2\nthen each patch is flatten and projected to a Ci-dimensional\nembedding. After the linear projection, the shape of the em-\nHi-1 Wi-1 Ci, where\nbedded patches can be viewed as x x\nPi P\nthe height and width are Pi times smaller than the input."
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 2273
        },
        {
          "x": 2277,
          "y": 2273
        },
        {
          "x": 2277,
          "y": 2423
        },
        {
          "x": 1278,
          "y": 2423
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='55' style='font-size:16px'>In this way, we can flexibly adjust the scale of the feature<br>map in each stage, making it possible to construct a feature<br>pyramid for Transformer.</p>",
      "id": 55,
      "page": 4,
      "text": "In this way, we can flexibly adjust the scale of the feature\nmap in each stage, making it possible to construct a feature\npyramid for Transformer."
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 2449
        },
        {
          "x": 1797,
          "y": 2449
        },
        {
          "x": 1797,
          "y": 2501
        },
        {
          "x": 1279,
          "y": 2501
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='56' style='font-size:18px'>3.3. Transformer Encoder</p>",
      "id": 56,
      "page": 4,
      "text": "3.3. Transformer Encoder"
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 2528
        },
        {
          "x": 2278,
          "y": 2528
        },
        {
          "x": 2278,
          "y": 2824
        },
        {
          "x": 1280,
          "y": 2824
        }
      ],
      "category": "paragraph",
      "html": "<p id='57' style='font-size:16px'>The Transformer encoder in the stage i has Li encoder<br>layers, each of which is composed of an attention layer<br>and a feed-forward layer [64]. Since PVT needs to process<br>high-resolution (e.g., 4-stride) feature maps, we propose a<br>spatial-reduction attention (SRA) layer to replace the tradi-<br>tional multi-head attention (MHA) layer [64] in the encoder.</p>",
      "id": 57,
      "page": 4,
      "text": "The Transformer encoder in the stage i has Li encoder\nlayers, each of which is composed of an attention layer\nand a feed-forward layer [64]. Since PVT needs to process\nhigh-resolution (e.g., 4-stride) feature maps, we propose a\nspatial-reduction attention (SRA) layer to replace the tradi-\ntional multi-head attention (MHA) layer [64] in the encoder."
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 2827
        },
        {
          "x": 2278,
          "y": 2827
        },
        {
          "x": 2278,
          "y": 2976
        },
        {
          "x": 1278,
          "y": 2976
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='58' style='font-size:14px'>Similar to MHA, our SRA receives a query Q, a key K,<br>and a value V as input, and outputs a refined feature. The<br>difference is that our SRA reduces the spatial scale of K</p>",
      "id": 58,
      "page": 4,
      "text": "Similar to MHA, our SRA receives a query Q, a key K,\nand a value V as input, and outputs a refined feature. The\ndifference is that our SRA reduces the spatial scale of K"
    },
    {
      "bounding_box": [
        {
          "x": 301,
          "y": 317
        },
        {
          "x": 1098,
          "y": 317
        },
        {
          "x": 1098,
          "y": 684
        },
        {
          "x": 301,
          "y": 684
        }
      ],
      "category": "figure",
      "html": "<figure><img id='59' style='font-size:14px' alt=\"Multi-Head Multi-Head\nAttention Attention\nHiWi\nxCi\nR2\nSpatial\nReduction\nQ K V Q K V (HiWi)xCi\nMulti-Head Attention Spatial-Reduction Attention (ours)\" data-coord=\"top-left:(301,317); bottom-right:(1098,684)\" /></figure>",
      "id": 59,
      "page": 5,
      "text": "Multi-Head Multi-Head\nAttention Attention\nHiWi\nxCi\nR2\nSpatial\nReduction\nQ K V Q K V (HiWi)xCi\nMulti-Head Attention Spatial-Reduction Attention (ours)"
    },
    {
      "bounding_box": [
        {
          "x": 199,
          "y": 755
        },
        {
          "x": 1200,
          "y": 755
        },
        {
          "x": 1200,
          "y": 957
        },
        {
          "x": 199,
          "y": 957
        }
      ],
      "category": "caption",
      "html": "<caption id='60' style='font-size:16px'>Figure 4: Multi-head attention (MHA) vs. spatial-<br>reduction attention (SRA). With the spatial-reduction op-<br>eration, the computational/memory cost of our SRA is<br>much lower than that of MHA.</caption>",
      "id": 60,
      "page": 5,
      "text": "Figure 4: Multi-head attention (MHA) vs. spatial-\nreduction attention (SRA). With the spatial-reduction op-\neration, the computational/memory cost of our SRA is\nmuch lower than that of MHA."
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 1046
        },
        {
          "x": 1201,
          "y": 1046
        },
        {
          "x": 1201,
          "y": 1196
        },
        {
          "x": 200,
          "y": 1196
        }
      ],
      "category": "paragraph",
      "html": "<p id='61' style='font-size:16px'>and V before the attention operation (see Figure 4), which<br>largely reduces the computational/memory overhead. De-<br>tails of the SRA in the stage i can be formulated as follows:</p>",
      "id": 61,
      "page": 5,
      "text": "and V before the attention operation (see Figure 4), which\nlargely reduces the computational/memory overhead. De-\ntails of the SRA in the stage i can be formulated as follows:"
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 1411
        },
        {
          "x": 1199,
          "y": 1411
        },
        {
          "x": 1199,
          "y": 1770
        },
        {
          "x": 201,
          "y": 1770
        }
      ],
      "category": "paragraph",
      "html": "<p id='62' style='font-size:18px'>where Concat(·) is the concatenation operation as in [64].<br>W3 E RCixdhead, WK E RCixdhead, W,V ERCixdhead and<br>,<br>WO E RCixCi are linear projection parameters. Ni is the<br>head number of the attention layer in Stage i. Therefore, the<br>dimension of each head (i.e., dhead) is equal to CIN SR(·) is<br>the operation for reducing the spatial dimension of the input<br>sequence (i.e., K or V), which is written as:</p>",
      "id": 62,
      "page": 5,
      "text": "where Concat(·) is the concatenation operation as in [64].\nW3 E RCixdhead, WK E RCixdhead, W,V ERCixdhead and\n,\nWO E RCixCi are linear projection parameters. Ni is the\nhead number of the attention layer in Stage i. Therefore, the\ndimension of each head (i.e., dhead) is equal to CIN SR(·) is\nthe operation for reducing the spatial dimension of the input\nsequence (i.e., K or V), which is written as:"
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 1892
        },
        {
          "x": 1201,
          "y": 1892
        },
        {
          "x": 1201,
          "y": 2315
        },
        {
          "x": 200,
          "y": 2315
        }
      ],
      "category": "paragraph",
      "html": "<p id='63' style='font-size:16px'>Here, x E R(HiWi)xCi and<br>represents a input sequence,<br>Ri denotes the reduction ratio of the attention layers in<br>Stage i. Reshape(x, Ri) is an operation of reshaping the<br>HiWi (R2 Ci).<br>input sequence x to a sequence of size x<br>R2<br>Ws E R(R2Ci)XCi is a linear projection that reduces the di-<br>mension of the input sequence to Ci. Norm(·) refers to<br>layer normalization [1]. As in the original Transformer [64],<br>our attention operation Attention(·) is calculated as:</p>",
      "id": 63,
      "page": 5,
      "text": "Here, x E R(HiWi)xCi and\nrepresents a input sequence,\nRi denotes the reduction ratio of the attention layers in\nStage i. Reshape(x, Ri) is an operation of reshaping the\nHiWi (R2 Ci).\ninput sequence x to a sequence of size x\nR2\nWs E R(R2Ci)XCi is a linear projection that reduces the di-\nmension of the input sequence to Ci. Norm(·) refers to\nlayer normalization [1]. As in the original Transformer [64],\nour attention operation Attention(·) is calculated as:"
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 2495
        },
        {
          "x": 1199,
          "y": 2495
        },
        {
          "x": 1199,
          "y": 2699
        },
        {
          "x": 200,
          "y": 2699
        }
      ],
      "category": "paragraph",
      "html": "<p id='64' style='font-size:16px'>Through these formulas, we can find that the computa-<br>tional/memory costs of our attention operation are R2 times<br>lower than those of MHA, SO our SRA can handle larger<br>input feature maps/sequences with limited resources.</p>",
      "id": 64,
      "page": 5,
      "text": "Through these formulas, we can find that the computa-\ntional/memory costs of our attention operation are R2 times\nlower than those of MHA, SO our SRA can handle larger\ninput feature maps/sequences with limited resources."
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 2728
        },
        {
          "x": 562,
          "y": 2728
        },
        {
          "x": 562,
          "y": 2777
        },
        {
          "x": 203,
          "y": 2777
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='65' style='font-size:20px'>3.4. Model Details</p>",
      "id": 65,
      "page": 5,
      "text": "3.4. Model Details"
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 2805
        },
        {
          "x": 1200,
          "y": 2805
        },
        {
          "x": 1200,
          "y": 2902
        },
        {
          "x": 201,
          "y": 2902
        }
      ],
      "category": "paragraph",
      "html": "<p id='66' style='font-size:14px'>In summary, the hyper parameters of our method are<br>listed as follows:</p>",
      "id": 66,
      "page": 5,
      "text": "In summary, the hyper parameters of our method are\nlisted as follows:"
    },
    {
      "bounding_box": [
        {
          "x": 250,
          "y": 2926
        },
        {
          "x": 769,
          "y": 2926
        },
        {
          "x": 769,
          "y": 2978
        },
        {
          "x": 250,
          "y": 2978
        }
      ],
      "category": "paragraph",
      "html": "<p id='67' style='font-size:16px'>· Pi: the patch size of Stage i;</p>",
      "id": 67,
      "page": 5,
      "text": "· Pi: the patch size of Stage i;"
    },
    {
      "bounding_box": [
        {
          "x": 1327,
          "y": 304
        },
        {
          "x": 2274,
          "y": 304
        },
        {
          "x": 2274,
          "y": 615
        },
        {
          "x": 1327,
          "y": 615
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='68' style='font-size:16px'>· Ci: the channel number of the output of Stage i;<br>· Li: the number of encoder layers in Stage 2;<br>● Ri: the reduction ratio of the SRA in Stage 2;<br>· Ni: the head number of the SRA in Stage 2;<br>· Ei: the expansion ratio of the feed-forward layer [64]<br>in Stage 2;</p>",
      "id": 68,
      "page": 5,
      "text": "· Ci: the channel number of the output of Stage i;\n· Li: the number of encoder layers in Stage 2;\n● Ri: the reduction ratio of the SRA in Stage 2;\n· Ni: the head number of the SRA in Stage 2;\n· Ei: the expansion ratio of the feed-forward layer [64]\nin Stage 2;"
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 640
        },
        {
          "x": 2275,
          "y": 640
        },
        {
          "x": 2275,
          "y": 787
        },
        {
          "x": 1279,
          "y": 787
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='69' style='font-size:16px'>Following the design rules of ResNet [22], we (1) use small<br>output channel numbers in shallow stages; and (2) concen-<br>trate the major computation resource in intermediate stages.</p>",
      "id": 69,
      "page": 5,
      "text": "Following the design rules of ResNet [22], we (1) use small\noutput channel numbers in shallow stages; and (2) concen-\ntrate the major computation resource in intermediate stages."
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 794
        },
        {
          "x": 2277,
          "y": 794
        },
        {
          "x": 2277,
          "y": 1091
        },
        {
          "x": 1278,
          "y": 1091
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='70' style='font-size:16px'>To provide instances for discussion, we describe a series<br>of PVT models with different scales, namely PVT-Tiny, -<br>Small, -Medium, and -Large, in Table 1, whose parameter<br>numbers are comparable to ResNet18, 50, 101, and 152 re-<br>spectively. More details of employing these models in spe-<br>cific downstream tasks will be introduced in Section 4.</p>",
      "id": 70,
      "page": 5,
      "text": "To provide instances for discussion, we describe a series\nof PVT models with different scales, namely PVT-Tiny, -\nSmall, -Medium, and -Large, in Table 1, whose parameter\nnumbers are comparable to ResNet18, 50, 101, and 152 re-\nspectively. More details of employing these models in spe-\ncific downstream tasks will be introduced in Section 4."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 1129
        },
        {
          "x": 1576,
          "y": 1129
        },
        {
          "x": 1576,
          "y": 1179
        },
        {
          "x": 1280,
          "y": 1179
        }
      ],
      "category": "paragraph",
      "html": "<p id='71' style='font-size:20px'>3.5. Discussion</p>",
      "id": 71,
      "page": 5,
      "text": "3.5. Discussion"
    },
    {
      "bounding_box": [
        {
          "x": 1277,
          "y": 1210
        },
        {
          "x": 2278,
          "y": 1210
        },
        {
          "x": 2278,
          "y": 1857
        },
        {
          "x": 1277,
          "y": 1857
        }
      ],
      "category": "paragraph",
      "html": "<p id='72' style='font-size:16px'>The most related work to our model is ViT [13]. Here,<br>we discuss the relationship and differences between them.<br>First, both PVT and ViT are pure Transformer models with-<br>out convolutions. The primary difference between them<br>is the pyramid structure. Similar to the traditional Trans-<br>former [64], the length of ViT's output sequence is the same<br>as the input, which means that the output of ViT is single-<br>scale (see Figure 1 (b)). Moreover, due to the limited re-<br>source, the input of ViT is coarse-grained (e.g., the patch<br>size is 16 or 32 pixels), and thus its output resolution is rel-<br>atively low (e.g., 16-stride or 32-stride). As a result, it is<br>difficult to directly apply ViT to dense prediction tasks that<br>require high-resolution or multi-scale feature maps.</p>",
      "id": 72,
      "page": 5,
      "text": "The most related work to our model is ViT [13]. Here,\nwe discuss the relationship and differences between them.\nFirst, both PVT and ViT are pure Transformer models with-\nout convolutions. The primary difference between them\nis the pyramid structure. Similar to the traditional Trans-\nformer [64], the length of ViT's output sequence is the same\nas the input, which means that the output of ViT is single-\nscale (see Figure 1 (b)). Moreover, due to the limited re-\nsource, the input of ViT is coarse-grained (e.g., the patch\nsize is 16 or 32 pixels), and thus its output resolution is rel-\natively low (e.g., 16-stride or 32-stride). As a result, it is\ndifficult to directly apply ViT to dense prediction tasks that\nrequire high-resolution or multi-scale feature maps."
    },
    {
      "bounding_box": [
        {
          "x": 1276,
          "y": 1862
        },
        {
          "x": 2277,
          "y": 1862
        },
        {
          "x": 2277,
          "y": 2510
        },
        {
          "x": 1276,
          "y": 2510
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='73' style='font-size:16px'>Our PVT breaks the routine of Transformer by intro-<br>ducing a progressive shrinking pyramid. It can gener-<br>ate multi-scale feature maps like a traditional CNN back-<br>bone. In addition, we also designed a simple but effec-<br>tive attention layer-SRA, to process high-resolution fea-<br>ture maps and reduce computational/memory costs. Ben-<br>efiting from the above designs, our method has the fol-<br>lowing advantages over ViT: 1) more flexible-can gen-<br>erate feature maps of different scales/channels in differ-<br>ent stages; 2) more versatile-can be easily plugged and<br>played in most downstream task models; 3) more friendly<br>to computation/memory- -can handle higher resolution fea-<br>ture maps or longer sequences.</p>",
      "id": 73,
      "page": 5,
      "text": "Our PVT breaks the routine of Transformer by intro-\nducing a progressive shrinking pyramid. It can gener-\nate multi-scale feature maps like a traditional CNN back-\nbone. In addition, we also designed a simple but effec-\ntive attention layer-SRA, to process high-resolution fea-\nture maps and reduce computational/memory costs. Ben-\nefiting from the above designs, our method has the fol-\nlowing advantages over ViT: 1) more flexible-can gen-\nerate feature maps of different scales/channels in differ-\nent stages; 2) more versatile-can be easily plugged and\nplayed in most downstream task models; 3) more friendly\nto computation/memory- -can handle higher resolution fea-\nture maps or longer sequences."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 2562
        },
        {
          "x": 2059,
          "y": 2562
        },
        {
          "x": 2059,
          "y": 2617
        },
        {
          "x": 1280,
          "y": 2617
        }
      ],
      "category": "paragraph",
      "html": "<p id='74' style='font-size:22px'>4. Application to Downstream Tasks</p>",
      "id": 74,
      "page": 5,
      "text": "4. Application to Downstream Tasks"
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 2646
        },
        {
          "x": 1826,
          "y": 2646
        },
        {
          "x": 1826,
          "y": 2698
        },
        {
          "x": 1280,
          "y": 2698
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='75' style='font-size:20px'>4.1. Image-Level Prediction</p>",
      "id": 75,
      "page": 5,
      "text": "4.1. Image-Level Prediction"
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 2727
        },
        {
          "x": 2278,
          "y": 2727
        },
        {
          "x": 2278,
          "y": 2977
        },
        {
          "x": 1278,
          "y": 2977
        }
      ],
      "category": "paragraph",
      "html": "<p id='76' style='font-size:16px'>Image classification is the most classical task of image-<br>level prediction. To provide instances for discussion, we<br>design a series of PVT models with different scales, namely<br>PVT-Tiny, -Small, -Medium, and -Large, whose parameter<br>numbers are similar to ResNet18, 50, 101, and 152, respec-</p>",
      "id": 76,
      "page": 5,
      "text": "Image classification is the most classical task of image-\nlevel prediction. To provide instances for discussion, we\ndesign a series of PVT models with different scales, namely\nPVT-Tiny, -Small, -Medium, and -Large, whose parameter\nnumbers are similar to ResNet18, 50, 101, and 152, respec-"
    },
    {
      "bounding_box": [
        {
          "x": 209,
          "y": 282
        },
        {
          "x": 2257,
          "y": 282
        },
        {
          "x": 2257,
          "y": 1168
        },
        {
          "x": 209,
          "y": 1168
        }
      ],
      "category": "table",
      "html": "<table id='77' style='font-size:14px'><tr><td></td><td>Output Size</td><td>Layer Name</td><td colspan=\"2\">PVT-Tiny</td><td colspan=\"2\">PVT-Small</td><td colspan=\"2\">PVT-Medium</td><td colspan=\"2\">PVT-Large</td></tr><tr><td rowspan=\"2\">Stage 1</td><td rowspan=\"2\">H W x 4 4</td><td>Patch Embedding</td><td colspan=\"8\">P1 = 4; C1 = 64</td></tr><tr><td>Transformer Encoder</td><td>R1 = 8 N1 = 1 E1 = 8</td><td>x 2</td><td>R1 = 8 N1 = 1 E1 = 8</td><td>x 3</td><td>R1 = 8 N1 = 1 E1 = 8</td><td>x 3</td><td>R1 = 8 N1 = 1 E1 = 8</td><td>x 3</td></tr><tr><td rowspan=\"2\">Stage 2</td><td rowspan=\"2\">W H8 x 8</td><td>Patch Embedding</td><td colspan=\"8\">P2 = 2; C2 = 128</td></tr><tr><td>Transformer Encoder</td><td>R2 = 4 N2 = 2 E2 = 8</td><td>x 2</td><td>R2 = 4 N2 = 2 E2 = 8</td><td>x 3</td><td>R2 = 4 N2 = 2 E2 = 8</td><td>x 3</td><td>R2 = 4 N2 2 E2 = 8</td><td>x 8</td></tr><tr><td rowspan=\"2\">Stage 3</td><td rowspan=\"2\">H W x 16 16</td><td>Patch Embedding</td><td colspan=\"8\">P3 = 2; C3 = 320</td></tr><tr><td>Transformer Encoder</td><td>R3 = 2 N3 = 5 E3 = 4</td><td>x 2</td><td>R3 = 2 N3 = 5 E3 = 4</td><td>x 6</td><td>R3 = 2 N3 = 5 E3 = 4</td><td>x 18</td><td>R3 = 2 N3 = 5 E3 = 4</td><td>x 27</td></tr><tr><td rowspan=\"2\">Stage 4</td><td rowspan=\"2\">H W x 32 32</td><td>Patch Embedding</td><td colspan=\"8\">P4 = 2; C4=512</td></tr><tr><td>Transformer Encoder</td><td>R4 = 1 N4 = 8 E4 = 4</td><td>x 2</td><td>R4 = 1 N4 = 8 E4 = 4</td><td>x 3</td><td>R4 = 1 N4 = 8 E4 = 4</td><td>x 3</td><td>R4 = 1 N4 = 8 E4 = 4</td><td>x 3</td></tr></table>",
      "id": 77,
      "page": 6,
      "text": "Output Size Layer Name PVT-Tiny PVT-Small PVT-Medium PVT-Large\n Stage 1 H W x 4 4 Patch Embedding P1 = 4; C1 = 64\n Transformer Encoder R1 = 8 N1 = 1 E1 = 8 x 2 R1 = 8 N1 = 1 E1 = 8 x 3 R1 = 8 N1 = 1 E1 = 8 x 3 R1 = 8 N1 = 1 E1 = 8 x 3\n Stage 2 W H8 x 8 Patch Embedding P2 = 2; C2 = 128\n Transformer Encoder R2 = 4 N2 = 2 E2 = 8 x 2 R2 = 4 N2 = 2 E2 = 8 x 3 R2 = 4 N2 = 2 E2 = 8 x 3 R2 = 4 N2 2 E2 = 8 x 8\n Stage 3 H W x 16 16 Patch Embedding P3 = 2; C3 = 320\n Transformer Encoder R3 = 2 N3 = 5 E3 = 4 x 2 R3 = 2 N3 = 5 E3 = 4 x 6 R3 = 2 N3 = 5 E3 = 4 x 18 R3 = 2 N3 = 5 E3 = 4 x 27\n Stage 4 H W x 32 32 Patch Embedding P4 = 2; C4=512\n Transformer Encoder R4 = 1 N4 = 8 E4 = 4 x 2 R4 = 1 N4 = 8 E4 = 4 x 3 R4 = 1 N4 = 8 E4 = 4 x 3 R4 = 1 N4 = 8 E4 = 4"
    },
    {
      "bounding_box": [
        {
          "x": 199,
          "y": 1193
        },
        {
          "x": 2280,
          "y": 1193
        },
        {
          "x": 2280,
          "y": 1347
        },
        {
          "x": 199,
          "y": 1347
        }
      ],
      "category": "caption",
      "html": "<br><caption id='78' style='font-size:16px'>Table 1: Detailed settings of PVT series. The design follows the two rules of ResNet [22]: (1) with the growth of network<br>depth, the hidden dimension gradually increases, and the output resolution progressively shrinks; (2) the major computation<br>resource is concentrated in Stage 3.</caption>",
      "id": 78,
      "page": 6,
      "text": "Table 1: Detailed settings of PVT series. The design follows the two rules of ResNet [22]: (1) with the growth of network\ndepth, the hidden dimension gradually increases, and the output resolution progressively shrinks; (2) the major computation\nresource is concentrated in Stage 3."
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 1435
        },
        {
          "x": 1200,
          "y": 1435
        },
        {
          "x": 1200,
          "y": 1533
        },
        {
          "x": 200,
          "y": 1533
        }
      ],
      "category": "paragraph",
      "html": "<p id='79' style='font-size:18px'>tively. Detailed hyper-parameter settings of the PVT series<br>are provided in the supplementary material (SM).</p>",
      "id": 79,
      "page": 6,
      "text": "tively. Detailed hyper-parameter settings of the PVT series\nare provided in the supplementary material (SM)."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 1541
        },
        {
          "x": 1201,
          "y": 1541
        },
        {
          "x": 1201,
          "y": 1740
        },
        {
          "x": 202,
          "y": 1740
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='80' style='font-size:16px'>For image classification, we follow ViT [13] and<br>DeiT [63] to append a learnable classification token to the<br>input of the last stage, and then employ a fully connected<br>(FC) layer to conduct classification on top of the token.</p>",
      "id": 80,
      "page": 6,
      "text": "For image classification, we follow ViT [13] and\nDeiT [63] to append a learnable classification token to the\ninput of the last stage, and then employ a fully connected\n(FC) layer to conduct classification on top of the token."
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 1784
        },
        {
          "x": 851,
          "y": 1784
        },
        {
          "x": 851,
          "y": 1837
        },
        {
          "x": 203,
          "y": 1837
        }
      ],
      "category": "paragraph",
      "html": "<p id='81' style='font-size:18px'>4.2. Pixel-Level Dense Prediction</p>",
      "id": 81,
      "page": 6,
      "text": "4.2. Pixel-Level Dense Prediction"
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 1868
        },
        {
          "x": 1200,
          "y": 1868
        },
        {
          "x": 1200,
          "y": 2118
        },
        {
          "x": 201,
          "y": 2118
        }
      ],
      "category": "paragraph",
      "html": "<p id='82' style='font-size:16px'>In addition to image-level prediction, dense prediction<br>that requires pixel-level classification or regression to be<br>performed on the feature map, is also often seen in down-<br>stream tasks. Here, we discuss two typical tasks, namely<br>object detection, and semantic segmentation.</p>",
      "id": 82,
      "page": 6,
      "text": "In addition to image-level prediction, dense prediction\nthat requires pixel-level classification or regression to be\nperformed on the feature map, is also often seen in down-\nstream tasks. Here, we discuss two typical tasks, namely\nobject detection, and semantic segmentation."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 2124
        },
        {
          "x": 1200,
          "y": 2124
        },
        {
          "x": 1200,
          "y": 2570
        },
        {
          "x": 201,
          "y": 2570
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='83' style='font-size:16px'>We apply our PVT models to three representative dense<br>prediction methods, namely RetinaNet [39], Mask R-<br>CNN [21], and Semantic FPN [32]. RetinaNet is a widely<br>used single-stage detector, Mask R-CNN is the most pop-<br>ular two-stage instance segmentation framework, and Se-<br>mantic FPN is a vanilla semantic segmentation method<br>without special operations (e.g., dilated convolution). Us-<br>ing these methods as baselines enables us to adequately ex-<br>amine the effectiveness of different backbones.</p>",
      "id": 83,
      "page": 6,
      "text": "We apply our PVT models to three representative dense\nprediction methods, namely RetinaNet [39], Mask R-\nCNN [21], and Semantic FPN [32]. RetinaNet is a widely\nused single-stage detector, Mask R-CNN is the most pop-\nular two-stage instance segmentation framework, and Se-\nmantic FPN is a vanilla semantic segmentation method\nwithout special operations (e.g., dilated convolution). Us-\ning these methods as baselines enables us to adequately ex-\namine the effectiveness of different backbones."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 2580
        },
        {
          "x": 1201,
          "y": 2580
        },
        {
          "x": 1201,
          "y": 2977
        },
        {
          "x": 201,
          "y": 2977
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='84' style='font-size:16px'>The implementation details are as follows: (1) Like<br>ResNet, we initialize the PVT backbone with the weights<br>pre-trained on ImageNet; (2) We use the output feature<br>pyramid {F1, F2, F3, F4} as the input of FPN [38], and<br>then the refined feature maps are fed to the follow-up de-<br>tection/segmentation head; (3) When training the detec-<br>tion/segmentation model, none of the layers in PVT are<br>frozen; (4) Since the input for detection/segmentation can</p>",
      "id": 84,
      "page": 6,
      "text": "The implementation details are as follows: (1) Like\nResNet, we initialize the PVT backbone with the weights\npre-trained on ImageNet; (2) We use the output feature\npyramid {F1, F2, F3, F4} as the input of FPN [38], and\nthen the refined feature maps are fed to the follow-up de-\ntection/segmentation head; (3) When training the detec-\ntion/segmentation model, none of the layers in PVT are\nfrozen; (4) Since the input for detection/segmentation can"
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 1435
        },
        {
          "x": 2279,
          "y": 1435
        },
        {
          "x": 2279,
          "y": 1635
        },
        {
          "x": 1279,
          "y": 1635
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='85' style='font-size:18px'>be an arbitrary shape, the position embeddings pre-trained<br>on ImageNet may no longer be meaningful. Therefore, we<br>perform bilinear interpolation on the pre-trained position<br>embeddings according to the input resolution.</p>",
      "id": 85,
      "page": 6,
      "text": "be an arbitrary shape, the position embeddings pre-trained\non ImageNet may no longer be meaningful. Therefore, we\nperform bilinear interpolation on the pre-trained position\nembeddings according to the input resolution."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 1681
        },
        {
          "x": 1612,
          "y": 1681
        },
        {
          "x": 1612,
          "y": 1734
        },
        {
          "x": 1280,
          "y": 1734
        }
      ],
      "category": "paragraph",
      "html": "<p id='86' style='font-size:22px'>5. Experiments</p>",
      "id": 86,
      "page": 6,
      "text": "5. Experiments"
    },
    {
      "bounding_box": [
        {
          "x": 1282,
          "y": 1767
        },
        {
          "x": 2278,
          "y": 1767
        },
        {
          "x": 2278,
          "y": 1916
        },
        {
          "x": 1282,
          "y": 1916
        }
      ],
      "category": "paragraph",
      "html": "<p id='87' style='font-size:16px'>We compare PVT with the two most representative CNN<br>backbones, i.e., ResNet [22] and ResNeXt [73], which are<br>widely used in the benchmarks of many downstream tasks.</p>",
      "id": 87,
      "page": 6,
      "text": "We compare PVT with the two most representative CNN\nbackbones, i.e., ResNet [22] and ResNeXt [73], which are\nwidely used in the benchmarks of many downstream tasks."
    },
    {
      "bounding_box": [
        {
          "x": 1277,
          "y": 1949
        },
        {
          "x": 1765,
          "y": 1949
        },
        {
          "x": 1765,
          "y": 2001
        },
        {
          "x": 1277,
          "y": 2001
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='88' style='font-size:20px'>5.1. Image Classification</p>",
      "id": 88,
      "page": 6,
      "text": "5.1. Image Classification"
    },
    {
      "bounding_box": [
        {
          "x": 1277,
          "y": 2028
        },
        {
          "x": 2278,
          "y": 2028
        },
        {
          "x": 2278,
          "y": 2977
        },
        {
          "x": 1277,
          "y": 2977
        }
      ],
      "category": "paragraph",
      "html": "<p id='89' style='font-size:14px'>Settings. Image classification experiments are performed<br>on the ImageNet 2012 dataset [51], which comprises 1.28<br>million training images and 50K validation images from<br>1,000 categories. For fair comparison, all models are<br>trained on the training set, and report the top-1 error on the<br>validation set. We follow DeiT [63] and apply random crop-<br>ping, random horizontal flipping [59], label-smoothing reg-<br>ularization [60], mixup [78], CutMix [76], and random eras-<br>ing [82] as data augmentations. During training, we employ<br>AdamW [46] with a momentum of 0.9, a mini-batch size of<br>128, and a weight decay of 5 x 10-2 to optimize models.<br>The initial learning rate is set to 1 x 10-3 and decreases fol-<br>lowing the cosine schedule [45]. All models are trained for<br>300 epochs from scratch on 8 V100 GPUs. To benchmark,<br>we apply a center crop on the validation set, where a 224x<br>224 patch is cropped to evaluate the classification accuracy.<br>Results. In Table 2, we see that our PVT models are supe-<br>rior to conventional CNN backbones under similar parame-<br>ter numbers and computational budgets. For example, when</p>",
      "id": 89,
      "page": 6,
      "text": "Settings. Image classification experiments are performed\non the ImageNet 2012 dataset [51], which comprises 1.28\nmillion training images and 50K validation images from\n1,000 categories. For fair comparison, all models are\ntrained on the training set, and report the top-1 error on the\nvalidation set. We follow DeiT [63] and apply random crop-\nping, random horizontal flipping [59], label-smoothing reg-\nularization [60], mixup [78], CutMix [76], and random eras-\ning [82] as data augmentations. During training, we employ\nAdamW [46] with a momentum of 0.9, a mini-batch size of\n128, and a weight decay of 5 x 10-2 to optimize models.\nThe initial learning rate is set to 1 x 10-3 and decreases fol-\nlowing the cosine schedule [45]. All models are trained for\n300 epochs from scratch on 8 V100 GPUs. To benchmark,\nwe apply a center crop on the validation set, where a 224x\n224 patch is cropped to evaluate the classification accuracy.\nResults. In Table 2, we see that our PVT models are supe-\nrior to conventional CNN backbones under similar parame-\nter numbers and computational budgets. For example, when"
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 287
        },
        {
          "x": 1195,
          "y": 287
        },
        {
          "x": 1195,
          "y": 1378
        },
        {
          "x": 203,
          "y": 1378
        }
      ],
      "category": "table",
      "html": "<table id='90' style='font-size:14px'><tr><td>Method</td><td>#Param (M)</td><td>GFLOPs</td><td>Top-1 Err (%)</td></tr><tr><td>ResNet18* [22]</td><td>11.7</td><td>1.8</td><td>30.2</td></tr><tr><td>ResNet18 [22]</td><td>11.7</td><td>1.8</td><td>31.5</td></tr><tr><td>DeiT-Tiny/16 [63]</td><td>5.7</td><td>1.3</td><td>27.8</td></tr><tr><td>PVT-Tiny (ours)</td><td>13.2</td><td>1.9</td><td>24.9</td></tr><tr><td>ResNet50* [22]</td><td>25.6</td><td>4.1</td><td>23.9</td></tr><tr><td>ResNet50 [22]</td><td>25.6</td><td>4.1</td><td>21.5</td></tr><tr><td>ResNeXt50-32x4d* [73]</td><td>25.0</td><td>4.3</td><td>22.4</td></tr><tr><td>ResNeXt50-32x4d [73]</td><td>25.0</td><td>4.3</td><td>20.5</td></tr><tr><td>T2T-ViTt-14 [75]</td><td>22.0</td><td>6.1</td><td>19.3</td></tr><tr><td>TNT-S [19]</td><td>23.8</td><td>5.2</td><td>18.7</td></tr><tr><td>DeiT-Small/16 [63]</td><td>22.1</td><td>4.6</td><td>20.1</td></tr><tr><td>PVT-Small (ours)</td><td>24.5</td><td>3.8</td><td>20.2</td></tr><tr><td>ResNet101* [22]</td><td>44.7</td><td>7.9</td><td>22.6</td></tr><tr><td>ResNet101 [22]</td><td>44.7</td><td>7.9</td><td>20.2</td></tr><tr><td>ResNeXt101-32x4d* [73]</td><td>44.2</td><td>8.0</td><td>21.2</td></tr><tr><td>ResNeXt101-32x4d [73]</td><td>44.2</td><td>8.0</td><td>19.4</td></tr><tr><td>T2T-ViTt-19 [75]</td><td>39.0</td><td>9.8</td><td>18.6</td></tr><tr><td>ViT-Small/16 [13]</td><td>48.8</td><td>9.9</td><td>19.2</td></tr><tr><td>PVT-Medium (ours)</td><td>44.2</td><td>6.7</td><td>18.8</td></tr><tr><td>ResNeXt101-64x4d* [73]</td><td>83.5</td><td>15.6</td><td>20.4</td></tr><tr><td>ResNeXt101-64x4d [73]</td><td>83.5</td><td>15.6</td><td>18.5</td></tr><tr><td>ViT-Base/16 [13]</td><td>86.6</td><td>17.6</td><td>18.2</td></tr><tr><td>T2T-ViTt-24 [75]</td><td>64.0</td><td>15.0</td><td>17.8</td></tr><tr><td>TNT-B [19]</td><td>66.0</td><td>14.1</td><td>17.2</td></tr><tr><td>DeiT-Base/16 [63]</td><td>86.6</td><td>17.6</td><td>18.2</td></tr><tr><td>PVT-Large (ours)</td><td>61.4</td><td>9.8</td><td>18.3</td></tr></table>",
      "id": 90,
      "page": 7,
      "text": "Method #Param (M) GFLOPs Top-1 Err (%)\n ResNet18* [22] 11.7 1.8 30.2\n ResNet18 [22] 11.7 1.8 31.5\n DeiT-Tiny/16 [63] 5.7 1.3 27.8\n PVT-Tiny (ours) 13.2 1.9 24.9\n ResNet50* [22] 25.6 4.1 23.9\n ResNet50 [22] 25.6 4.1 21.5\n ResNeXt50-32x4d* [73] 25.0 4.3 22.4\n ResNeXt50-32x4d [73] 25.0 4.3 20.5\n T2T-ViTt-14 [75] 22.0 6.1 19.3\n TNT-S [19] 23.8 5.2 18.7\n DeiT-Small/16 [63] 22.1 4.6 20.1\n PVT-Small (ours) 24.5 3.8 20.2\n ResNet101* [22] 44.7 7.9 22.6\n ResNet101 [22] 44.7 7.9 20.2\n ResNeXt101-32x4d* [73] 44.2 8.0 21.2\n ResNeXt101-32x4d [73] 44.2 8.0 19.4\n T2T-ViTt-19 [75] 39.0 9.8 18.6\n ViT-Small/16 [13] 48.8 9.9 19.2\n PVT-Medium (ours) 44.2 6.7 18.8\n ResNeXt101-64x4d* [73] 83.5 15.6 20.4\n ResNeXt101-64x4d [73] 83.5 15.6 18.5\n ViT-Base/16 [13] 86.6 17.6 18.2\n T2T-ViTt-24 [75] 64.0 15.0 17.8\n TNT-B [19] 66.0 14.1 17.2\n DeiT-Base/16 [63] 86.6 17.6 18.2\n PVT-Large (ours) 61.4 9.8"
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 1416
        },
        {
          "x": 1200,
          "y": 1416
        },
        {
          "x": 1200,
          "y": 1669
        },
        {
          "x": 201,
          "y": 1669
        }
      ],
      "category": "paragraph",
      "html": "<p id='91' style='font-size:16px'>Table 2: Image classification performance on the Ima-<br>geNet validation set. \"#Param\" refers to the number of<br>parameters. \"GFLOPs\" is calculated under the input scale<br>of 224 x 224. \"*\" indicates the performance of the method<br>trained under the strategy of its original paper.</p>",
      "id": 91,
      "page": 7,
      "text": "Table 2: Image classification performance on the Ima-\ngeNet validation set. \"#Param\" refers to the number of\nparameters. \"GFLOPs\" is calculated under the input scale\nof 224 x 224. \"*\" indicates the performance of the method\ntrained under the strategy of its original paper."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 1760
        },
        {
          "x": 1200,
          "y": 1760
        },
        {
          "x": 1200,
          "y": 2258
        },
        {
          "x": 202,
          "y": 2258
        }
      ],
      "category": "paragraph",
      "html": "<p id='92' style='font-size:18px'>the GFLOPs are roughly similar, the top-1 error of PVT-<br>Small reaches 20.2, which is 1.3 points higher than that of<br>ResNet50 [22] (20.2 vs. 21.5). Meanwhile, under similar or<br>lower complexity, PVT models archive performances com-<br>parable to the recently proposed Transformer-based mod-<br>els, such as ViT [13] and DeiT [63] (PVT-Large: 18.3 vs.<br>ViT(DeiT)-Base/16: 18.3). Here, we clarify that these re-<br>sults are within our expectations, because the pyramid struc-<br>ture is beneficial to dense prediction tasks, but brings little<br>improvements to image classification.</p>",
      "id": 92,
      "page": 7,
      "text": "the GFLOPs are roughly similar, the top-1 error of PVT-\nSmall reaches 20.2, which is 1.3 points higher than that of\nResNet50 [22] (20.2 vs. 21.5). Meanwhile, under similar or\nlower complexity, PVT models archive performances com-\nparable to the recently proposed Transformer-based mod-\nels, such as ViT [13] and DeiT [63] (PVT-Large: 18.3 vs.\nViT(DeiT)-Base/16: 18.3). Here, we clarify that these re-\nsults are within our expectations, because the pyramid struc-\nture is beneficial to dense prediction tasks, but brings little\nimprovements to image classification."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 2261
        },
        {
          "x": 1199,
          "y": 2261
        },
        {
          "x": 1199,
          "y": 2461
        },
        {
          "x": 202,
          "y": 2461
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='93' style='font-size:16px'>Note that ViT and DeiT have limitations as they are<br>specifically designed for classification tasks, and thus are<br>not suitable for dense prediction tasks, which usually re-<br>quire effective feature pyramids.</p>",
      "id": 93,
      "page": 7,
      "text": "Note that ViT and DeiT have limitations as they are\nspecifically designed for classification tasks, and thus are\nnot suitable for dense prediction tasks, which usually re-\nquire effective feature pyramids."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 2498
        },
        {
          "x": 620,
          "y": 2498
        },
        {
          "x": 620,
          "y": 2548
        },
        {
          "x": 202,
          "y": 2548
        }
      ],
      "category": "paragraph",
      "html": "<p id='94' style='font-size:20px'>5.2. Object Detection</p>",
      "id": 94,
      "page": 7,
      "text": "5.2. Object Detection"
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 2577
        },
        {
          "x": 1201,
          "y": 2577
        },
        {
          "x": 1201,
          "y": 2978
        },
        {
          "x": 200,
          "y": 2978
        }
      ],
      "category": "paragraph",
      "html": "<p id='95' style='font-size:16px'>Settings. Object detection experiments are conducted on<br>the challenging COCO benchmark [40]. All models are<br>trained on COCO train2017 (118k images) and evalu-<br>ated on val201 7 (5k images). We verify the effectiveness<br>of PVT backbones on top of two standard detectors, namely<br>RetinaNet [39] and Mask R-CNN [21]. Before training, we<br>use the weights pre-trained on ImageNet to initialize the<br>backbone and Xavier [18] to initialize the newly added lay-</p>",
      "id": 95,
      "page": 7,
      "text": "Settings. Object detection experiments are conducted on\nthe challenging COCO benchmark [40]. All models are\ntrained on COCO train2017 (118k images) and evalu-\nated on val201 7 (5k images). We verify the effectiveness\nof PVT backbones on top of two standard detectors, namely\nRetinaNet [39] and Mask R-CNN [21]. Before training, we\nuse the weights pre-trained on ImageNet to initialize the\nbackbone and Xavier [18] to initialize the newly added lay-"
    },
    {
      "bounding_box": [
        {
          "x": 1277,
          "y": 306
        },
        {
          "x": 2278,
          "y": 306
        },
        {
          "x": 2278,
          "y": 852
        },
        {
          "x": 1277,
          "y": 852
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='96' style='font-size:18px'>ers. Our models are trained with a batch size of 16 on 8<br>V100 GPUs and optimized by AdamW [46] with an ini-<br>tial learning rate of 1 x 10-4. Following common prac-<br>tices [39, 21, 7], we adopt 1x or 3x training schedule (i.e.,<br>12 or 36 epochs) to train all detection models. The training<br>image is resized to have a shorter side of 800 pixels, while<br>the longer side does not exceed 1,333 pixels. When using<br>the 3x training schedule, we randomly resize the shorter<br>side of the input image within the range of [640, 800]. In<br>the testing phase, the shorter side of the input image is fixed<br>to 800 pixels.</p>",
      "id": 96,
      "page": 7,
      "text": "ers. Our models are trained with a batch size of 16 on 8\nV100 GPUs and optimized by AdamW [46] with an ini-\ntial learning rate of 1 x 10-4. Following common prac-\ntices [39, 21, 7], we adopt 1x or 3x training schedule (i.e.,\n12 or 36 epochs) to train all detection models. The training\nimage is resized to have a shorter side of 800 pixels, while\nthe longer side does not exceed 1,333 pixels. When using\nthe 3x training schedule, we randomly resize the shorter\nside of the input image within the range of [640, 800]. In\nthe testing phase, the shorter side of the input image is fixed\nto 800 pixels."
    },
    {
      "bounding_box": [
        {
          "x": 1277,
          "y": 853
        },
        {
          "x": 2277,
          "y": 853
        },
        {
          "x": 2277,
          "y": 1404
        },
        {
          "x": 1277,
          "y": 1404
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='97' style='font-size:16px'>Results. As shown in Table 3, when using RetinaNet for<br>object detection, we find that under comparable number of<br>parameters, the PVT-based models significantly surpasses<br>their counterparts. For example, with the 1 x training sched-<br>ule, the AP of PVT-Tiny is 4.9 points better than that of<br>ResNet18 (36.7 vs. 31.8). Moreover, with the 3x training<br>schedule and multi-scale training, PVT-Large archive the<br>best AP of 43.4, surpassing ResNeXt101-64x4d (43.4 vs.<br>41.8), while our parameter number is 30% fewer. These re-<br>sults indicate that our PVT can be a good alternative to the<br>CNN backbone for object detection.</p>",
      "id": 97,
      "page": 7,
      "text": "Results. As shown in Table 3, when using RetinaNet for\nobject detection, we find that under comparable number of\nparameters, the PVT-based models significantly surpasses\ntheir counterparts. For example, with the 1 x training sched-\nule, the AP of PVT-Tiny is 4.9 points better than that of\nResNet18 (36.7 vs. 31.8). Moreover, with the 3x training\nschedule and multi-scale training, PVT-Large archive the\nbest AP of 43.4, surpassing ResNeXt101-64x4d (43.4 vs.\n41.8), while our parameter number is 30% fewer. These re-\nsults indicate that our PVT can be a good alternative to the\nCNN backbone for object detection."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 1410
        },
        {
          "x": 2278,
          "y": 1410
        },
        {
          "x": 2278,
          "y": 1805
        },
        {
          "x": 1280,
          "y": 1805
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='98' style='font-size:18px'>Similar results are found in instance segmentation exper-<br>iments based on Mask R-CNN, as shown in Table 4. With<br>the 1 x training schedule, PVT-Tiny achieves 35.1 mask AP<br>(APm), which is 3.9 points better than ResNet18 (35.1 vs.<br>31.2) and even 0.7 points higher than ResNet50 (35.1 vs.<br>34.4). The best APm obtained by PVT-Large is 40.7, which<br>is 1.0 points higher than ResNeXt101-64x4d (40.7 vs. 39.7),<br>with 20% fewer parameters.</p>",
      "id": 98,
      "page": 7,
      "text": "Similar results are found in instance segmentation exper-\niments based on Mask R-CNN, as shown in Table 4. With\nthe 1 x training schedule, PVT-Tiny achieves 35.1 mask AP\n(APm), which is 3.9 points better than ResNet18 (35.1 vs.\n31.2) and even 0.7 points higher than ResNet50 (35.1 vs.\n34.4). The best APm obtained by PVT-Large is 40.7, which\nis 1.0 points higher than ResNeXt101-64x4d (40.7 vs. 39.7),\nwith 20% fewer parameters."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 1845
        },
        {
          "x": 1830,
          "y": 1845
        },
        {
          "x": 1830,
          "y": 1898
        },
        {
          "x": 1280,
          "y": 1898
        }
      ],
      "category": "paragraph",
      "html": "<p id='99' style='font-size:22px'>5.3. Semantic Segmentation</p>",
      "id": 99,
      "page": 7,
      "text": "5.3. Semantic Segmentation"
    },
    {
      "bounding_box": [
        {
          "x": 1277,
          "y": 1925
        },
        {
          "x": 2278,
          "y": 1925
        },
        {
          "x": 2278,
          "y": 2773
        },
        {
          "x": 1277,
          "y": 2773
        }
      ],
      "category": "paragraph",
      "html": "<p id='100' style='font-size:18px'>Settings. We choose ADE20K [83], a challenging scene<br>parsing dataset, to benchmark the performance of semantic<br>segmentation. ADE20K contains 150 fine-grained semantic<br>categories, with 20,210, 2,000, and 3,352 images for train-<br>ing, validation, and testing, respectively. We evaluate our<br>PVT backbones on the basis of Semantic FPN [32], a sim-<br>ple segmentation method without dilated convolutions [74].<br>In the training phase, the backbone is initialized with the<br>weights pre-trained on ImageNet [12], and other newly<br>added layers are initialized with Xavier [18]. We optimize<br>our models using AdamW [46] with an initial learning rate<br>of 1e-4. Following common practices [32, 8], we train our<br>models for 80k iterations with a batch size of 16 on 4 V100<br>GPUs. The learning rate is decayed following the polyno-<br>mial decay schedule with a power of 0.9. We randomly<br>resize and crop the image to 512 x 512 for training, and<br>rescale to have a shorter side of 512 pixels during testing.</p>",
      "id": 100,
      "page": 7,
      "text": "Settings. We choose ADE20K [83], a challenging scene\nparsing dataset, to benchmark the performance of semantic\nsegmentation. ADE20K contains 150 fine-grained semantic\ncategories, with 20,210, 2,000, and 3,352 images for train-\ning, validation, and testing, respectively. We evaluate our\nPVT backbones on the basis of Semantic FPN [32], a sim-\nple segmentation method without dilated convolutions [74].\nIn the training phase, the backbone is initialized with the\nweights pre-trained on ImageNet [12], and other newly\nadded layers are initialized with Xavier [18]. We optimize\nour models using AdamW [46] with an initial learning rate\nof 1e-4. Following common practices [32, 8], we train our\nmodels for 80k iterations with a batch size of 16 on 4 V100\nGPUs. The learning rate is decayed following the polyno-\nmial decay schedule with a power of 0.9. We randomly\nresize and crop the image to 512 x 512 for training, and\nrescale to have a shorter side of 512 pixels during testing."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 2777
        },
        {
          "x": 2278,
          "y": 2777
        },
        {
          "x": 2278,
          "y": 2976
        },
        {
          "x": 1280,
          "y": 2976
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='101' style='font-size:18px'>Results. As shown in Table 5, when using Seman-<br>tic FPN [32] for semantic segmentation, PVT-based<br>models consistently outperforms the models based on<br>ResNet [22] or ResNeXt [73]. For example, with al-</p>",
      "id": 101,
      "page": 7,
      "text": "Results. As shown in Table 5, when using Seman-\ntic FPN [32] for semantic segmentation, PVT-based\nmodels consistently outperforms the models based on\nResNet [22] or ResNeXt [73]. For example, with al-"
    },
    {
      "bounding_box": [
        {
          "x": 207,
          "y": 290
        },
        {
          "x": 2273,
          "y": 290
        },
        {
          "x": 2273,
          "y": 748
        },
        {
          "x": 207,
          "y": 748
        }
      ],
      "category": "table",
      "html": "<table id='102' style='font-size:14px'><tr><td rowspan=\"2\">Backbone</td><td rowspan=\"2\">#Param (M)</td><td colspan=\"6\">RetinaNet 1x</td><td colspan=\"6\">RetinaNet 3x + MS</td></tr><tr><td>AP</td><td>AP50</td><td>AP75</td><td>APs</td><td>APM</td><td>APL</td><td>AP</td><td>AP50</td><td>AP75</td><td>APs</td><td>APM</td><td>APL</td></tr><tr><td>ResNet18 [22]</td><td>21.3</td><td>31.8</td><td>49.6</td><td>33.6</td><td>16.3</td><td>34.3</td><td>43.2</td><td>35.4</td><td>53.9</td><td>37.6</td><td>19.5</td><td>38.2</td><td>46.8</td></tr><tr><td>PVT-Tiny (ours)</td><td>23.0</td><td>36.7(+4.9)</td><td>56.9</td><td>38.9</td><td>22.6</td><td>38.8</td><td>50.0</td><td>39.4(+4.0)</td><td>59.8</td><td>42.0</td><td>25.5</td><td>42.0</td><td>52.1</td></tr><tr><td>ResNet50 [22]</td><td>37.7</td><td>36.3</td><td>55.3</td><td>38.6</td><td>19.3</td><td>40.0</td><td>48.8</td><td>39.0</td><td>58.4</td><td>41.8</td><td>22.4</td><td>42.8</td><td>51.6</td></tr><tr><td>PVT-Small (ours)</td><td>34.2</td><td>40.4(+4.1)</td><td>61.3</td><td>43.0</td><td>25.0</td><td>42.9</td><td>55.7</td><td>42.2(+3.2)</td><td>62.7</td><td>45.0</td><td>26.2</td><td>45.2</td><td>57.2</td></tr><tr><td>ResNet101 [22]</td><td>56.7</td><td>38.5</td><td>57.8</td><td>41.2</td><td>21.4</td><td>42.6</td><td>51.1</td><td>40.9</td><td>60.1</td><td>44.0</td><td>23.7</td><td>45.0</td><td>53.8</td></tr><tr><td>ResNeXt101-32x4d [73]</td><td>56.4</td><td>39.9(+1.4)</td><td>59.6</td><td>42.7</td><td>22.3</td><td>44.2</td><td>52.5</td><td>41.4(+0.5)</td><td>61.0</td><td>44.3</td><td>23.9</td><td>45.5</td><td>53.7</td></tr><tr><td>PVT-Medium (ours)</td><td>53.9</td><td>41.9(+3.4)</td><td>63.1</td><td>44.3</td><td>25.0</td><td>44.9</td><td>57.6</td><td>43.2(+2.3)</td><td>63.8</td><td>46.1</td><td>27.3</td><td>46.3</td><td>58.9</td></tr><tr><td>ResNeXt101-64x4d [73]</td><td>95.5</td><td>41.0</td><td>60.9</td><td>44.0</td><td>23.9</td><td>45.2</td><td>54.0</td><td>41.8</td><td>61.5</td><td>44.4</td><td>25.2</td><td>45.4</td><td>54.6</td></tr><tr><td>PVT-Large (ours)</td><td>71.1</td><td>42.6(+1.6)</td><td>63.7</td><td>45.4</td><td>25.8</td><td>46.0</td><td>58.4</td><td>43.4(+1.6)</td><td>63.6</td><td>46.1</td><td>26.1</td><td>46.0</td><td>59.5</td></tr></table>",
      "id": 102,
      "page": 8,
      "text": "Backbone #Param (M) RetinaNet 1x RetinaNet 3x + MS\n AP AP50 AP75 APs APM APL AP AP50 AP75 APs APM APL\n ResNet18 [22] 21.3 31.8 49.6 33.6 16.3 34.3 43.2 35.4 53.9 37.6 19.5 38.2 46.8\n PVT-Tiny (ours) 23.0 36.7(+4.9) 56.9 38.9 22.6 38.8 50.0 39.4(+4.0) 59.8 42.0 25.5 42.0 52.1\n ResNet50 [22] 37.7 36.3 55.3 38.6 19.3 40.0 48.8 39.0 58.4 41.8 22.4 42.8 51.6\n PVT-Small (ours) 34.2 40.4(+4.1) 61.3 43.0 25.0 42.9 55.7 42.2(+3.2) 62.7 45.0 26.2 45.2 57.2\n ResNet101 [22] 56.7 38.5 57.8 41.2 21.4 42.6 51.1 40.9 60.1 44.0 23.7 45.0 53.8\n ResNeXt101-32x4d [73] 56.4 39.9(+1.4) 59.6 42.7 22.3 44.2 52.5 41.4(+0.5) 61.0 44.3 23.9 45.5 53.7\n PVT-Medium (ours) 53.9 41.9(+3.4) 63.1 44.3 25.0 44.9 57.6 43.2(+2.3) 63.8 46.1 27.3 46.3 58.9\n ResNeXt101-64x4d [73] 95.5 41.0 60.9 44.0 23.9 45.2 54.0 41.8 61.5 44.4 25.2 45.4 54.6\n PVT-Large (ours) 71.1 42.6(+1.6) 63.7 45.4 25.8 46.0 58.4 43.4(+1.6) 63.6 46.1 26.1 46.0"
    },
    {
      "bounding_box": [
        {
          "x": 246,
          "y": 786
        },
        {
          "x": 2226,
          "y": 786
        },
        {
          "x": 2226,
          "y": 836
        },
        {
          "x": 246,
          "y": 836
        }
      ],
      "category": "caption",
      "html": "<caption id='103' style='font-size:18px'>Table 3: Object detection performance on COCO val2017. \"MS\" means that multi-scale training [39, 21] is used.</caption>",
      "id": 103,
      "page": 8,
      "text": "Table 3: Object detection performance on COCO val2017. \"MS\" means that multi-scale training [39, 21] is used."
    },
    {
      "bounding_box": [
        {
          "x": 208,
          "y": 877
        },
        {
          "x": 2274,
          "y": 877
        },
        {
          "x": 2274,
          "y": 1296
        },
        {
          "x": 208,
          "y": 1296
        }
      ],
      "category": "table",
      "html": "<table id='104' style='font-size:14px'><tr><td rowspan=\"2\">Backbone</td><td rowspan=\"2\">#Param (M)</td><td colspan=\"6\">Mask R-CNN 1x</td><td colspan=\"6\">Mask R-CNN 3x + MS</td></tr><tr><td>APb</td><td>AP50</td><td>AP75</td><td>APm</td><td>APm</td><td>APm</td><td>APb</td><td>AP50</td><td>AP75</td><td>APm</td><td>APm</td><td>APm</td></tr><tr><td>ResNet18 [22]</td><td>31.2</td><td>34.0</td><td>54.0</td><td>36.7</td><td>31.2</td><td>51.0</td><td>32.7</td><td>36.9</td><td>57.1</td><td>40.0</td><td>33.6</td><td>53.9</td><td>35.7</td></tr><tr><td>PVT-Tiny (ours)</td><td>32.9</td><td>36.7(+2.7)</td><td>59.2</td><td>39.3</td><td>35.1(+3.9)</td><td>56.7</td><td>37.3</td><td>39.8(+2.9)</td><td>62.2</td><td>43.0</td><td>37.4(+3.8)</td><td>59.3</td><td>39.9</td></tr><tr><td>ResNet50 [22]</td><td>44.2</td><td>38.0</td><td>58.6</td><td>41.4</td><td>34.4</td><td>55.1</td><td>36.7</td><td>41.0</td><td>61.7</td><td>44.9</td><td>37.1</td><td>58.4</td><td>40.1</td></tr><tr><td>PVT-Small (ours)</td><td>44.1</td><td>40.4(+2.4)</td><td>62.9</td><td>43.8</td><td>37.8(+3.4)</td><td>60.1</td><td>40.3</td><td>43.0(+2.0)</td><td>65.3</td><td>46.9</td><td>39.9(+2.8)</td><td>62.5</td><td>42.8</td></tr><tr><td>ResNet101 [22]</td><td>63.2</td><td>40.4</td><td>61.1</td><td>44.2</td><td>36.4</td><td>57.7</td><td>38.8</td><td>42.8</td><td>63.2</td><td>47.1</td><td>38.5</td><td>60.1</td><td>41.3</td></tr><tr><td>ResNeXt101-32x4d [73]</td><td>62.8</td><td>41.9(+1.5)</td><td>62.5</td><td>45.9</td><td>37.5(+1.1)</td><td>59.4</td><td>40.2</td><td>44.0(+1.2)</td><td>64.4</td><td>48.0</td><td>39.2(+0.7)</td><td>61.4</td><td>41.9</td></tr><tr><td>PVT-Medium (ours)</td><td>63.9</td><td>42.0(+1.6)</td><td>64.4</td><td>45.6</td><td>39.0(+2.6)</td><td>61.6</td><td>42.1</td><td>44.2(+1.4)</td><td>66.0</td><td>48.2</td><td>40.5(+2.0)</td><td>63.1</td><td>43.5</td></tr><tr><td>ResNeXt101-64x4d [73]</td><td>101.9</td><td>42.8</td><td>63.8</td><td>47.3</td><td>38.4</td><td>60.6</td><td>41.3</td><td>44.4</td><td>64.9</td><td>48.8</td><td>39.7</td><td>61.9</td><td>42.6</td></tr><tr><td>PVT-Large (ours)</td><td>81.0</td><td>42.9(+0.1)</td><td>65.0</td><td>46.6</td><td>39.5(+1.1)</td><td>61.9</td><td>42.5</td><td>44.5(+0.1)</td><td>66.0</td><td>48.3</td><td>40.7(+1.0)</td><td>63.4</td><td>43.7</td></tr></table>",
      "id": 104,
      "page": 8,
      "text": "Backbone #Param (M) Mask R-CNN 1x Mask R-CNN 3x + MS\n APb AP50 AP75 APm APm APm APb AP50 AP75 APm APm APm\n ResNet18 [22] 31.2 34.0 54.0 36.7 31.2 51.0 32.7 36.9 57.1 40.0 33.6 53.9 35.7\n PVT-Tiny (ours) 32.9 36.7(+2.7) 59.2 39.3 35.1(+3.9) 56.7 37.3 39.8(+2.9) 62.2 43.0 37.4(+3.8) 59.3 39.9\n ResNet50 [22] 44.2 38.0 58.6 41.4 34.4 55.1 36.7 41.0 61.7 44.9 37.1 58.4 40.1\n PVT-Small (ours) 44.1 40.4(+2.4) 62.9 43.8 37.8(+3.4) 60.1 40.3 43.0(+2.0) 65.3 46.9 39.9(+2.8) 62.5 42.8\n ResNet101 [22] 63.2 40.4 61.1 44.2 36.4 57.7 38.8 42.8 63.2 47.1 38.5 60.1 41.3\n ResNeXt101-32x4d [73] 62.8 41.9(+1.5) 62.5 45.9 37.5(+1.1) 59.4 40.2 44.0(+1.2) 64.4 48.0 39.2(+0.7) 61.4 41.9\n PVT-Medium (ours) 63.9 42.0(+1.6) 64.4 45.6 39.0(+2.6) 61.6 42.1 44.2(+1.4) 66.0 48.2 40.5(+2.0) 63.1 43.5\n ResNeXt101-64x4d [73] 101.9 42.8 63.8 47.3 38.4 60.6 41.3 44.4 64.9 48.8 39.7 61.9 42.6\n PVT-Large (ours) 81.0 42.9(+0.1) 65.0 46.6 39.5(+1.1) 61.9 42.5 44.5(+0.1) 66.0 48.3 40.7(+1.0) 63.4"
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 1331
        },
        {
          "x": 2278,
          "y": 1331
        },
        {
          "x": 2278,
          "y": 1436
        },
        {
          "x": 203,
          "y": 1436
        }
      ],
      "category": "caption",
      "html": "<caption id='105' style='font-size:18px'>Table 4: Object detection and instance segmentation performance on COCO val2017. APb and APm denote bounding<br>box AP and mask AP, respectively.</caption>",
      "id": 105,
      "page": 8,
      "text": "Table 4: Object detection and instance segmentation performance on COCO val2017. APb and APm denote bounding\nbox AP and mask AP, respectively."
    },
    {
      "bounding_box": [
        {
          "x": 210,
          "y": 1540
        },
        {
          "x": 1196,
          "y": 1540
        },
        {
          "x": 1196,
          "y": 2033
        },
        {
          "x": 210,
          "y": 2033
        }
      ],
      "category": "table",
      "html": "<table id='106' style='font-size:14px'><tr><td rowspan=\"2\">Backbone</td><td colspan=\"3\">Semantic FPN</td></tr><tr><td>#Param (M)</td><td>GFLOPs</td><td>mIoU (%)</td></tr><tr><td>ResNet18 [22]</td><td>15.5</td><td>32.2</td><td>32.9</td></tr><tr><td>PVT-Tiny (ours)</td><td>17.0</td><td>33.2</td><td>35.7(+2.8)</td></tr><tr><td>ResNet50 [22]</td><td>28.5</td><td>45.6</td><td>36.7</td></tr><tr><td>PVT-Small (ours)</td><td>28.2</td><td>44.5</td><td>39.8(+3.1)</td></tr><tr><td>ResNet101 [22]</td><td>47.5</td><td>65.1</td><td>38.8</td></tr><tr><td>ResNeXt101-32x4d [73]</td><td>47.1</td><td>64.7</td><td>39.7(+0.9)</td></tr><tr><td>PVT-Medium (ours)</td><td>48.0</td><td>61.0</td><td>41.6(+2.8)</td></tr><tr><td>ResNeXt101-64x4d [73]</td><td>86.4</td><td>103.9</td><td>40.2</td></tr><tr><td>PVT-Large (ours)</td><td>65.1</td><td>79.6</td><td>42.1(+1.9)</td></tr><tr><td>PVT-Large* (ours)</td><td>65.1</td><td>79.6</td><td>44.8</td></tr></table>",
      "id": 106,
      "page": 8,
      "text": "Backbone Semantic FPN\n #Param (M) GFLOPs mIoU (%)\n ResNet18 [22] 15.5 32.2 32.9\n PVT-Tiny (ours) 17.0 33.2 35.7(+2.8)\n ResNet50 [22] 28.5 45.6 36.7\n PVT-Small (ours) 28.2 44.5 39.8(+3.1)\n ResNet101 [22] 47.5 65.1 38.8\n ResNeXt101-32x4d [73] 47.1 64.7 39.7(+0.9)\n PVT-Medium (ours) 48.0 61.0 41.6(+2.8)\n ResNeXt101-64x4d [73] 86.4 103.9 40.2\n PVT-Large (ours) 65.1 79.6 42.1(+1.9)\n PVT-Large* (ours) 65.1 79.6"
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 2070
        },
        {
          "x": 1197,
          "y": 2070
        },
        {
          "x": 1197,
          "y": 2272
        },
        {
          "x": 200,
          "y": 2272
        }
      ],
      "category": "caption",
      "html": "<caption id='107' style='font-size:16px'>Table 5: Semantic segmentation performance of differ-<br>ent backbones on the ADE20K validation set. \"GFLOPs\"<br>is calculated under the input scale of 512 x 512. \"*\" indi-<br>cates 320K iterations training and multi-scale flip testing.</caption>",
      "id": 107,
      "page": 8,
      "text": "Table 5: Semantic segmentation performance of differ-\nent backbones on the ADE20K validation set. \"GFLOPs\"\nis calculated under the input scale of 512 x 512. \"*\" indi-\ncates 320K iterations training and multi-scale flip testing."
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 2378
        },
        {
          "x": 1199,
          "y": 2378
        },
        {
          "x": 1199,
          "y": 2977
        },
        {
          "x": 200,
          "y": 2977
        }
      ],
      "category": "paragraph",
      "html": "<p id='108' style='font-size:16px'>most the same number of parameters and GFLOPs, our<br>PVT-Tiny/Small/Medium are at least 2.8 points higher than<br>ResNet-18/50/101. In addition, although the parameter<br>number and GFLOPs of our PVT-Large are 20% lower than<br>those of ResNeXt101-64x4d, the mIoU is still 1.9 points<br>higher (42.1 vs. 40.2). With a longer training schedule and<br>multi-scale testing, PVT-Large+Semantic FPN archives the<br>best mIoU of 44.8, which is very close to the state-of-the-art<br>performance of the ADE20K benchmark. Note that Seman-<br>tic FPN is just a simple segmentation head. These results<br>demonstrate that our PVT backbones can extract better fea-<br>tures for semantic segmentation than the CNN backbone,</p>",
      "id": 108,
      "page": 8,
      "text": "most the same number of parameters and GFLOPs, our\nPVT-Tiny/Small/Medium are at least 2.8 points higher than\nResNet-18/50/101. In addition, although the parameter\nnumber and GFLOPs of our PVT-Large are 20% lower than\nthose of ResNeXt101-64x4d, the mIoU is still 1.9 points\nhigher (42.1 vs. 40.2). With a longer training schedule and\nmulti-scale testing, PVT-Large+Semantic FPN archives the\nbest mIoU of 44.8, which is very close to the state-of-the-art\nperformance of the ADE20K benchmark. Note that Seman-\ntic FPN is just a simple segmentation head. These results\ndemonstrate that our PVT backbones can extract better fea-\ntures for semantic segmentation than the CNN backbone,"
    },
    {
      "bounding_box": [
        {
          "x": 1285,
          "y": 1541
        },
        {
          "x": 2266,
          "y": 1541
        },
        {
          "x": 2266,
          "y": 1708
        },
        {
          "x": 1285,
          "y": 1708
        }
      ],
      "category": "table",
      "html": "<br><table id='109' style='font-size:14px'><tr><td rowspan=\"2\">Method</td><td colspan=\"6\">DETR (50 Epochs)</td></tr><tr><td>AP</td><td>AP50</td><td>AP75</td><td>APs</td><td>APM</td><td>APL</td></tr><tr><td>ResNet50 [22]</td><td>32.3</td><td>53.9</td><td>32.3</td><td>10.7</td><td>33.8</td><td>53.0</td></tr><tr><td>PVT-Small (ours)</td><td>34.7(+2.4)</td><td>55.7</td><td>35.4</td><td>12.0</td><td>36.4</td><td>56.7</td></tr></table>",
      "id": 109,
      "page": 8,
      "text": "Method DETR (50 Epochs)\n AP AP50 AP75 APs APM APL\n ResNet50 [22] 32.3 53.9 32.3 10.7 33.8 53.0\n PVT-Small (ours) 34.7(+2.4) 55.7 35.4 12.0 36.4"
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 1741
        },
        {
          "x": 2279,
          "y": 1741
        },
        {
          "x": 2279,
          "y": 1947
        },
        {
          "x": 1280,
          "y": 1947
        }
      ],
      "category": "paragraph",
      "html": "<p id='110' style='font-size:18px'>Table 6: Performance of the pure Transformer object<br>detection pipeline. We build a pure Transformer detector<br>by combining PVT and DETR [6], whose AP is 2.4 points<br>higher than the original DETR based on ResNet50 [22].</p>",
      "id": 110,
      "page": 8,
      "text": "Table 6: Performance of the pure Transformer object\ndetection pipeline. We build a pure Transformer detector\nby combining PVT and DETR [6], whose AP is 2.4 points\nhigher than the original DETR based on ResNet50 [22]."
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 2045
        },
        {
          "x": 2087,
          "y": 2045
        },
        {
          "x": 2087,
          "y": 2093
        },
        {
          "x": 1281,
          "y": 2093
        }
      ],
      "category": "paragraph",
      "html": "<p id='111' style='font-size:20px'>benefiting from the global attention mechanism.</p>",
      "id": 111,
      "page": 8,
      "text": "benefiting from the global attention mechanism."
    },
    {
      "bounding_box": [
        {
          "x": 1282,
          "y": 2141
        },
        {
          "x": 2248,
          "y": 2141
        },
        {
          "x": 2248,
          "y": 2192
        },
        {
          "x": 1282,
          "y": 2192
        }
      ],
      "category": "paragraph",
      "html": "<p id='112' style='font-size:22px'>5.4. Pure Transformer Detection & Segmentation</p>",
      "id": 112,
      "page": 8,
      "text": "5.4. Pure Transformer Detection & Segmentation"
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 2222
        },
        {
          "x": 2279,
          "y": 2222
        },
        {
          "x": 2279,
          "y": 2871
        },
        {
          "x": 1278,
          "y": 2871
        }
      ],
      "category": "paragraph",
      "html": "<p id='113' style='font-size:16px'>PVT+DETR. To reach the limit of no convolution, we build<br>a pure Transformer pipeline for object detection by sim-<br>ply combining our PVT with a Transformer-based detection<br>head-DETR [6]. We train models on COCO train2017<br>for 50 epochs with an initial learning rate of 1 x 10-4.<br>The learning rate is divided by 10 at the 33rd epoch. We<br>use random flipping and multi-scale training as data aug-<br>mentation. All other experimental settings is the same as<br>those in Sec. 5.2. As reported in Table 6, PVT-based DETR<br>archieves 34.7 AP on COCO val2017, outperforming<br>the original ResNet50-based DETR by 2.4 points (34.7 vs.<br>32.3). These results prove that a pure Transformer detector<br>can also works well in the object detection task.</p>",
      "id": 113,
      "page": 8,
      "text": "PVT+DETR. To reach the limit of no convolution, we build\na pure Transformer pipeline for object detection by sim-\nply combining our PVT with a Transformer-based detection\nhead-DETR [6]. We train models on COCO train2017\nfor 50 epochs with an initial learning rate of 1 x 10-4.\nThe learning rate is divided by 10 at the 33rd epoch. We\nuse random flipping and multi-scale training as data aug-\nmentation. All other experimental settings is the same as\nthose in Sec. 5.2. As reported in Table 6, PVT-based DETR\narchieves 34.7 AP on COCO val2017, outperforming\nthe original ResNet50-based DETR by 2.4 points (34.7 vs.\n32.3). These results prove that a pure Transformer detector\ncan also works well in the object detection task."
    },
    {
      "bounding_box": [
        {
          "x": 1282,
          "y": 2874
        },
        {
          "x": 2278,
          "y": 2874
        },
        {
          "x": 2278,
          "y": 2976
        },
        {
          "x": 1282,
          "y": 2976
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='114' style='font-size:18px'>PVT+Trans2Seg.We build a pure Transformer model<br>for semantic segmentation by combining our PVT with</p>",
      "id": 114,
      "page": 8,
      "text": "PVT+Trans2Seg.We build a pure Transformer model\nfor semantic segmentation by combining our PVT with"
    },
    {
      "bounding_box": [
        {
          "x": 205,
          "y": 299
        },
        {
          "x": 1194,
          "y": 299
        },
        {
          "x": 1194,
          "y": 505
        },
        {
          "x": 205,
          "y": 505
        }
      ],
      "category": "table",
      "html": "<table id='115' style='font-size:16px'><tr><td>Method</td><td>#Param (M)</td><td>GFLOPs</td><td>mloU (%)</td></tr><tr><td>ResNet50-d8+DeeplabV3+ [9]</td><td>26.8</td><td>120.5</td><td>41.5</td></tr><tr><td>ResNet50-d16+DeeplabV3+ [9]</td><td>26.8</td><td>45.5</td><td>40.6</td></tr><tr><td>ResNet50-d16+Trans2Seg [72]</td><td>56.1</td><td>79.3</td><td>39.7</td></tr><tr><td>PVT-Small+Trans2Seg</td><td>32.1</td><td>31.6</td><td>42.6(+2.9)</td></tr></table>",
      "id": 115,
      "page": 9,
      "text": "Method #Param (M) GFLOPs mloU (%)\n ResNet50-d8+DeeplabV3+ [9] 26.8 120.5 41.5\n ResNet50-d16+DeeplabV3+ [9] 26.8 45.5 40.6\n ResNet50-d16+Trans2Seg [72] 56.1 79.3 39.7\n PVT-Small+Trans2Seg 32.1 31.6"
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 542
        },
        {
          "x": 1199,
          "y": 542
        },
        {
          "x": 1199,
          "y": 841
        },
        {
          "x": 201,
          "y": 841
        }
      ],
      "category": "paragraph",
      "html": "<p id='116' style='font-size:20px'>Table 7: Performance of the pure Transformer semantic<br>segmentation pipeline. We build a pure Transformer de-<br>tector by combining PVT and Trans2Seg [72]. It is 2.9%<br>higher than ResNet50-d16+ Trans2Seg and 1.1% higher<br>than ResNet50-d8+DeeplabV3+ with lower GFlops. \"d8\"<br>and \"d16\" means dilation 8 and 16, respectively.</p>",
      "id": 116,
      "page": 9,
      "text": "Table 7: Performance of the pure Transformer semantic\nsegmentation pipeline. We build a pure Transformer de-\ntector by combining PVT and Trans2Seg [72]. It is 2.9%\nhigher than ResNet50-d16+ Trans2Seg and 1.1% higher\nthan ResNet50-d8+DeeplabV3+ with lower GFlops. \"d8\"\nand \"d16\" means dilation 8 and 16, respectively."
    },
    {
      "bounding_box": [
        {
          "x": 206,
          "y": 899
        },
        {
          "x": 1189,
          "y": 899
        },
        {
          "x": 1189,
          "y": 1108
        },
        {
          "x": 206,
          "y": 1108
        }
      ],
      "category": "table",
      "html": "<table id='117' style='font-size:16px'><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">#Param (M)</td><td colspan=\"6\">RetinaNet 1x</td></tr><tr><td>AP</td><td>AP50</td><td>AP75</td><td>APs</td><td>APM</td><td>APL</td></tr><tr><td>ViT-Small/4 [13]</td><td>60.9</td><td colspan=\"6\">Out of Memory</td></tr><tr><td>ViT-Small/32 [13]</td><td>60.8</td><td>31.7</td><td>51.3</td><td>32.3</td><td>14.8</td><td>33.7</td><td>47.9</td></tr><tr><td>PVT-Small (ours)</td><td>34.2</td><td>40.4</td><td>61.3</td><td>43.0</td><td>25.0</td><td>42.9</td><td>55.7</td></tr></table>",
      "id": 117,
      "page": 9,
      "text": "Method #Param (M) RetinaNet 1x\n AP AP50 AP75 APs APM APL\n ViT-Small/4 [13] 60.9 Out of Memory\n ViT-Small/32 [13] 60.8 31.7 51.3 32.3 14.8 33.7 47.9\n PVT-Small (ours) 34.2 40.4 61.3 43.0 25.0 42.9"
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 1145
        },
        {
          "x": 1199,
          "y": 1145
        },
        {
          "x": 1199,
          "y": 1393
        },
        {
          "x": 201,
          "y": 1393
        }
      ],
      "category": "paragraph",
      "html": "<p id='118' style='font-size:18px'>Table 8: Performance comparison between ViT and our<br>PVT using RetinaNet for object detection. ViT-Small/4<br>runs out of GPU memory due to small patch size (i.e.,<br>4x 4 per patch). ViT-Small/32 obtains 31.7 AP on COCO<br>val2017, which is 8.7 points lower than our PVT-Small.</p>",
      "id": 118,
      "page": 9,
      "text": "Table 8: Performance comparison between ViT and our\nPVT using RetinaNet for object detection. ViT-Small/4\nruns out of GPU memory due to small patch size (i.e.,\n4x 4 per patch). ViT-Small/32 obtains 31.7 AP on COCO\nval2017, which is 8.7 points lower than our PVT-Small."
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 1494
        },
        {
          "x": 1201,
          "y": 1494
        },
        {
          "x": 1201,
          "y": 2241
        },
        {
          "x": 200,
          "y": 2241
        }
      ],
      "category": "paragraph",
      "html": "<p id='119' style='font-size:20px'>Trans2Seg [72], a Transformer-based segmentation head.<br>According to the experimental settings in Sec. 5.3, we<br>perform experiments on ADE20K [83] with 40k iter-<br>ations training, single scale testing, and compare it<br>with ResNet50+Trans2Seg [72] and DeeplabV3+ [9] with<br>ResNet50-d8 (dilation 8) and -d16(dilation 8) in Table<br>7. We find that our PVT-Small+Trans2Seg achieves 42.6<br>mIoU, outperforming ResNet50-d8+DeeplabV3+ (41.5).<br>Note that, ResNet50-d8+DeeplabV3+ has 120.5 GFLOPs<br>due to the high computation cost of dilated convolution, and<br>our method has only 31.6 GFLOPs, which is 4 times fewer.<br>In addition, our PVT-Small+Trans2Seg performs better than<br>ResNet50-d16+ Trans2Seg (mIoU: 42.6 vs. 39.7, GFlops:<br>31.6 vs. 79.3). These results prove that a pure Transformer<br>segmentation network is workable.</p>",
      "id": 119,
      "page": 9,
      "text": "Trans2Seg [72], a Transformer-based segmentation head.\nAccording to the experimental settings in Sec. 5.3, we\nperform experiments on ADE20K [83] with 40k iter-\nations training, single scale testing, and compare it\nwith ResNet50+Trans2Seg [72] and DeeplabV3+ [9] with\nResNet50-d8 (dilation 8) and -d16(dilation 8) in Table\n7. We find that our PVT-Small+Trans2Seg achieves 42.6\nmIoU, outperforming ResNet50-d8+DeeplabV3+ (41.5).\nNote that, ResNet50-d8+DeeplabV3+ has 120.5 GFLOPs\ndue to the high computation cost of dilated convolution, and\nour method has only 31.6 GFLOPs, which is 4 times fewer.\nIn addition, our PVT-Small+Trans2Seg performs better than\nResNet50-d16+ Trans2Seg (mIoU: 42.6 vs. 39.7, GFlops:\n31.6 vs. 79.3). These results prove that a pure Transformer\nsegmentation network is workable."
    },
    {
      "bounding_box": [
        {
          "x": 203,
          "y": 2288
        },
        {
          "x": 585,
          "y": 2288
        },
        {
          "x": 585,
          "y": 2340
        },
        {
          "x": 203,
          "y": 2340
        }
      ],
      "category": "paragraph",
      "html": "<p id='120' style='font-size:22px'>5.5. Ablation Study</p>",
      "id": 120,
      "page": 9,
      "text": "5.5. Ablation Study"
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 2372
        },
        {
          "x": 1199,
          "y": 2372
        },
        {
          "x": 1199,
          "y": 2668
        },
        {
          "x": 201,
          "y": 2668
        }
      ],
      "category": "paragraph",
      "html": "<p id='121' style='font-size:18px'>Settings. We conduct ablation studies on ImageNet [12]<br>and COCO [40] datasets. The experimental settings on Im-<br>ageNet are the same as the settings in Sec. 5.1. For COCO,<br>all models are trained with a 1 x training schedule (i.e., 12<br>epochs) and without multi-scale training, and other settings<br>follow those in Sec. 5.2.</p>",
      "id": 121,
      "page": 9,
      "text": "Settings. We conduct ablation studies on ImageNet [12]\nand COCO [40] datasets. The experimental settings on Im-\nageNet are the same as the settings in Sec. 5.1. For COCO,\nall models are trained with a 1 x training schedule (i.e., 12\nepochs) and without multi-scale training, and other settings\nfollow those in Sec. 5.2."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 2675
        },
        {
          "x": 1200,
          "y": 2675
        },
        {
          "x": 1200,
          "y": 2979
        },
        {
          "x": 202,
          "y": 2979
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='122' style='font-size:20px'>Pyramid Structure. A Pyramid structure is crucial when<br>applying Transformer to dense prediction tasks. ViT (see<br>Figure 1 (b)) is a columnar framework, whose output is<br>single-scale. This results in a low-resolution output fea-<br>ture map when using coarse image patches (e.g., 32x32<br>pixels per patch) as input, leading to poor detection perfor-</p>",
      "id": 122,
      "page": 9,
      "text": "Pyramid Structure. A Pyramid structure is crucial when\napplying Transformer to dense prediction tasks. ViT (see\nFigure 1 (b)) is a columnar framework, whose output is\nsingle-scale. This results in a low-resolution output fea-\nture map when using coarse image patches (e.g., 32x32\npixels per patch) as input, leading to poor detection perfor-"
    },
    {
      "bounding_box": [
        {
          "x": 1303,
          "y": 307
        },
        {
          "x": 2245,
          "y": 307
        },
        {
          "x": 2245,
          "y": 1350
        },
        {
          "x": 1303,
          "y": 1350
        }
      ],
      "category": "figure",
      "html": "<br><figure><img id='123' style='font-size:16px' alt=\"45\n42.2\n90 40.4\nAP 35 33.8\nBBox\n26.6\n25\nPVT-Small W pre-train 1x\nCOCO 15 w/ pre-train 3x\nPVT-Small\nPVT-Small w/o pre-train 1x\nPVT-Small w/o pre-train 3x\n5\n0 5 10 15 20 25 30 35\n45\n(%) 42.2\n40.4\nAP 40 39.0\nBBox\n35 36.3\nPVT-Small 1x\nCOCO 30 PVT-Small 3x\nResNet50 1x\nResNet50 3x\n25\n0 5 10 15 20 25 30 35\nEpoch\" data-coord=\"top-left:(1303,307); bottom-right:(2245,1350)\" /></figure>",
      "id": 123,
      "page": 9,
      "text": "45\n42.2\n90 40.4\nAP 35 33.8\nBBox\n26.6\n25\nPVT-Small W pre-train 1x\nCOCO 15 w/ pre-train 3x\nPVT-Small\nPVT-Small w/o pre-train 1x\nPVT-Small w/o pre-train 3x\n5\n0 5 10 15 20 25 30 35\n45\n(%) 42.2\n40.4\nAP 40 39.0\nBBox\n35 36.3\nPVT-Small 1x\nCOCO 30 PVT-Small 3x\nResNet50 1x\nResNet50 3x\n25\n0 5 10 15 20 25 30 35\nEpoch"
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 1399
        },
        {
          "x": 2278,
          "y": 1399
        },
        {
          "x": 2278,
          "y": 1601
        },
        {
          "x": 1278,
          "y": 1601
        }
      ],
      "category": "caption",
      "html": "<caption id='124' style='font-size:20px'>Figure 5: AP curves of RetinaNet on COCO val2017<br>under different backbone settings. Top: using weights<br>pre-trained on ImageNet vs. random initialization. Bottom:<br>PVT-S vs. R50 [22].</caption>",
      "id": 124,
      "page": 9,
      "text": "Figure 5: AP curves of RetinaNet on COCO val2017\nunder different backbone settings. Top: using weights\npre-trained on ImageNet vs. random initialization. Bottom:\nPVT-S vs. R50 [22]."
    },
    {
      "bounding_box": [
        {
          "x": 1295,
          "y": 1652
        },
        {
          "x": 2255,
          "y": 1652
        },
        {
          "x": 2255,
          "y": 1822
        },
        {
          "x": 1295,
          "y": 1822
        }
      ],
      "category": "table",
      "html": "<table id='125' style='font-size:14px'><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">#Param (M)</td><td rowspan=\"2\">Top-1</td><td colspan=\"3\">RetinaNet 1x</td></tr><tr><td>AP</td><td>AP50</td><td>AP75</td></tr><tr><td>Wider PVT-Small</td><td>46.8</td><td>19.3</td><td>40.8</td><td>61.8</td><td>43.3</td></tr><tr><td>Deeper PVT-Small</td><td>44.2</td><td>18.8</td><td>41.9</td><td>63.1</td><td>44.3</td></tr></table>",
      "id": 125,
      "page": 9,
      "text": "Method #Param (M) Top-1 RetinaNet 1x\n AP AP50 AP75\n Wider PVT-Small 46.8 19.3 40.8 61.8 43.3\n Deeper PVT-Small 44.2 18.8 41.9 63.1"
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 1857
        },
        {
          "x": 2278,
          "y": 1857
        },
        {
          "x": 2278,
          "y": 2153
        },
        {
          "x": 1279,
          "y": 2153
        }
      ],
      "category": "paragraph",
      "html": "<p id='126' style='font-size:18px'>Table 9: Deeper vs. Wider. \"Top-1\" denotes the top-1 error<br>on the ImageNet validation set. \"AP\" denotes the bounding<br>box AP on COCO val2017. The deep model (i.e., PVT-<br>Medium) obtains better performance than the wide model<br>(i.e., PVT-Small-Wide ) under comparable parameter num-<br>ber.</p>",
      "id": 126,
      "page": 9,
      "text": "Table 9: Deeper vs. Wider. \"Top-1\" denotes the top-1 error\non the ImageNet validation set. \"AP\" denotes the bounding\nbox AP on COCO val2017. The deep model (i.e., PVT-\nMedium) obtains better performance than the wide model\n(i.e., PVT-Small-Wide ) under comparable parameter num-\nber."
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 2253
        },
        {
          "x": 2278,
          "y": 2253
        },
        {
          "x": 2278,
          "y": 2697
        },
        {
          "x": 1278,
          "y": 2697
        }
      ],
      "category": "paragraph",
      "html": "<p id='127' style='font-size:20px'>mance (31.7 AP on COCO val2017), as shown in Table<br>8. When using fine-grained image patches (e.g., 4x4 pixels<br>per patch) as input like our PVT, ViT will exhaust the GPU<br>memory (32G). Our method avoids this problem through a<br>progressive shrinking pyramid. Specifically, our model can<br>process high-resolution feature maps in shallow stages and<br>low-resolution feature maps in deep stages. Thus, it obtains<br>a promising AP of 40.4 on COCO val2017, 8.7 points<br>higher than ViT-Small/32 (40.4 vs. 31.7).</p>",
      "id": 127,
      "page": 9,
      "text": "mance (31.7 AP on COCO val2017), as shown in Table\n8. When using fine-grained image patches (e.g., 4x4 pixels\nper patch) as input like our PVT, ViT will exhaust the GPU\nmemory (32G). Our method avoids this problem through a\nprogressive shrinking pyramid. Specifically, our model can\nprocess high-resolution feature maps in shallow stages and\nlow-resolution feature maps in deep stages. Thus, it obtains\na promising AP of 40.4 on COCO val2017, 8.7 points\nhigher than ViT-Small/32 (40.4 vs. 31.7)."
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 2702
        },
        {
          "x": 2278,
          "y": 2702
        },
        {
          "x": 2278,
          "y": 2852
        },
        {
          "x": 1278,
          "y": 2852
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='128' style='font-size:20px'>Deeper vs. Wider. The problem of whether the CNN back-<br>bone should go deeper or wider has been extensively dis-<br>cussed in previous work [22, 77]. Here, we explore this</p>",
      "id": 128,
      "page": 9,
      "text": "Deeper vs. Wider. The problem of whether the CNN back-\nbone should go deeper or wider has been extensively dis-\ncussed in previous work [22, 77]. Here, we explore this"
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 2893
        },
        {
          "x": 2276,
          "y": 2893
        },
        {
          "x": 2276,
          "y": 2971
        },
        {
          "x": 1279,
          "y": 2971
        }
      ],
      "category": "paragraph",
      "html": "<p id='129' style='font-size:14px'>3For adapting ViT to RetinaNet, we extract the features from the layer<br>2, 4, 6, and 8 of ViT-Small/32, and interpolate them to different scales.</p>",
      "id": 129,
      "page": 9,
      "text": "3For adapting ViT to RetinaNet, we extract the features from the layer\n2, 4, 6, and 8 of ViT-Small/32, and interpolate them to different scales."
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 293
        },
        {
          "x": 1199,
          "y": 293
        },
        {
          "x": 1199,
          "y": 467
        },
        {
          "x": 202,
          "y": 467
        }
      ],
      "category": "table",
      "html": "<table id='130' style='font-size:14px'><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">#Param (M)</td><td rowspan=\"2\">GFLOPs</td><td colspan=\"3\">Mask R-CNN 1x</td></tr><tr><td>APm</td><td>APm</td><td>APm</td></tr><tr><td>ResNet50+GC r4 [5]</td><td>54.2</td><td>279.6</td><td>36.2</td><td>58.7</td><td>38.3</td></tr><tr><td>PVT-Small (ours)</td><td>44.1</td><td>304.4</td><td>37.8</td><td>60.1</td><td>40.3</td></tr></table>",
      "id": 130,
      "page": 10,
      "text": "Method #Param (M) GFLOPs Mask R-CNN 1x\n APm APm APm\n ResNet50+GC r4 [5] 54.2 279.6 36.2 58.7 38.3\n PVT-Small (ours) 44.1 304.4 37.8 60.1"
    },
    {
      "bounding_box": [
        {
          "x": 199,
          "y": 503
        },
        {
          "x": 1201,
          "y": 503
        },
        {
          "x": 1201,
          "y": 705
        },
        {
          "x": 199,
          "y": 705
        }
      ],
      "category": "caption",
      "html": "<caption id='131' style='font-size:16px'>Table 10: PVT vs. CNN w/ non-local. APm denotes<br>mask AP. Under similar parameter nubmer and GFLOPs,<br>our PVT outperform the CNN backbone w/ Non-Local<br>(ResNet50+GC r4) by 1.6 APm (37.8 vs. 36.2).</caption>",
      "id": 131,
      "page": 10,
      "text": "Table 10: PVT vs. CNN w/ non-local. APm denotes\nmask AP. Under similar parameter nubmer and GFLOPs,\nour PVT outperform the CNN backbone w/ Non-Local\n(ResNet50+GC r4) by 1.6 APm (37.8 vs. 36.2)."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 809
        },
        {
          "x": 1199,
          "y": 809
        },
        {
          "x": 1199,
          "y": 1308
        },
        {
          "x": 201,
          "y": 1308
        }
      ],
      "category": "paragraph",
      "html": "<p id='132' style='font-size:18px'>problem in our PVT. For fair comparisons, we multiply<br>the hidden dimensions {C1, C2, C3, C4} of PVT-Small by<br>a scale factor 1.4 to make it have an equivalent parameter<br>number to the deep model (i.e., PVT-Medium). As shown<br>in Table 9, the deep model (i.e., PVT-Medium) consistently<br>works better than the wide model (i.e., PVT-Small- Wide) on<br>both ImageNet and COCO. Therefore, going deeper is more<br>effective than going wider in the design of PVT. Based on<br>this observation, in Table 1, we develop PVT models with<br>different scales by increasing the model depth.</p>",
      "id": 132,
      "page": 10,
      "text": "problem in our PVT. For fair comparisons, we multiply\nthe hidden dimensions {C1, C2, C3, C4} of PVT-Small by\na scale factor 1.4 to make it have an equivalent parameter\nnumber to the deep model (i.e., PVT-Medium). As shown\nin Table 9, the deep model (i.e., PVT-Medium) consistently\nworks better than the wide model (i.e., PVT-Small- Wide) on\nboth ImageNet and COCO. Therefore, going deeper is more\neffective than going wider in the design of PVT. Based on\nthis observation, in Table 1, we develop PVT models with\ndifferent scales by increasing the model depth."
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 1315
        },
        {
          "x": 1199,
          "y": 1315
        },
        {
          "x": 1199,
          "y": 2062
        },
        {
          "x": 200,
          "y": 2062
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='133' style='font-size:18px'>Pre-trained Weights. Most dense prediction models (e.g.,<br>RetinaNet [39]) rely on the backbone whose weights are<br>pre-trained on ImageNet. We also discuss this problem in<br>our PVT. In the top of Figure 5, we plot the validation AP<br>curves of RetinaNet-PVT-Small w/ (red curves) and w/o<br>(blue curves) pre-trained weights. We find that the model<br>w/ pre-trained weights converges better than the one w/o<br>pre-trained weights, and the gap between their final AP<br>reaches 13.8 under the 1 x training schedule and 8.4 under<br>the 3x training schedule and multi-scale training. There-<br>fore, like CNN-based models, pre-training weights can also<br>help PVT-based models converge faster and better. More-<br>over, in the bottom of Figure 5, we also see that the con-<br>vergence speed of PVT-based models (red curves) is faster<br>than that of ResNet-based models (green curves).</p>",
      "id": 133,
      "page": 10,
      "text": "Pre-trained Weights. Most dense prediction models (e.g.,\nRetinaNet [39]) rely on the backbone whose weights are\npre-trained on ImageNet. We also discuss this problem in\nour PVT. In the top of Figure 5, we plot the validation AP\ncurves of RetinaNet-PVT-Small w/ (red curves) and w/o\n(blue curves) pre-trained weights. We find that the model\nw/ pre-trained weights converges better than the one w/o\npre-trained weights, and the gap between their final AP\nreaches 13.8 under the 1 x training schedule and 8.4 under\nthe 3x training schedule and multi-scale training. There-\nfore, like CNN-based models, pre-training weights can also\nhelp PVT-based models converge faster and better. More-\nover, in the bottom of Figure 5, we also see that the con-\nvergence speed of PVT-based models (red curves) is faster\nthan that of ResNet-based models (green curves)."
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 2067
        },
        {
          "x": 1200,
          "y": 2067
        },
        {
          "x": 1200,
          "y": 2572
        },
        {
          "x": 201,
          "y": 2572
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='134' style='font-size:18px'>PVT vs. \"CNN w/ Non-Local\" To obtain a global recep-<br>tive field, some well-engineered CNN backbones, such as<br>GCNet [5], integrate the non-local block in the CNN frame-<br>work. Here, we compare the performance of our PVT (pure<br>Transformer) and GCNet (CNN w/ non-local), using Mask<br>R-CNN for instance segmentation. As reported in Table<br>10, we find that our PVT-Small outperforms ResNet50+GC<br>r4 [5] by 1.6 points in APm (37.8 vs. 36.2), and 2.0 points in<br>APm (38.3 vs. 40.3), under comparable parameter number<br>and GFLOPs. There are two possible reasons for this result:</p>",
      "id": 134,
      "page": 10,
      "text": "PVT vs. \"CNN w/ Non-Local\" To obtain a global recep-\ntive field, some well-engineered CNN backbones, such as\nGCNet [5], integrate the non-local block in the CNN frame-\nwork. Here, we compare the performance of our PVT (pure\nTransformer) and GCNet (CNN w/ non-local), using Mask\nR-CNN for instance segmentation. As reported in Table\n10, we find that our PVT-Small outperforms ResNet50+GC\nr4 [5] by 1.6 points in APm (37.8 vs. 36.2), and 2.0 points in\nAPm (38.3 vs. 40.3), under comparable parameter number\nand GFLOPs. There are two possible reasons for this result:"
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 2579
        },
        {
          "x": 1200,
          "y": 2579
        },
        {
          "x": 1200,
          "y": 2976
        },
        {
          "x": 200,
          "y": 2976
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='135' style='font-size:18px'>(1) Although a single global attention layer (e.g., non-<br>local [70] or multi-head attention (MHA) [64]) can ac-<br>quire global-receptive-field features, the model perfor-<br>mance keeps improving as the model deepens. This indi-<br>cates that stacking multiple MHAs can further enhance the<br>representation capabilities offeatures. Therefore, as a pure<br>Transformer backbone with more global attention layers,<br>our PVT tends to perform better than the CNN backbone</p>",
      "id": 135,
      "page": 10,
      "text": "(1) Although a single global attention layer (e.g., non-\nlocal [70] or multi-head attention (MHA) [64]) can ac-\nquire global-receptive-field features, the model perfor-\nmance keeps improving as the model deepens. This indi-\ncates that stacking multiple MHAs can further enhance the\nrepresentation capabilities offeatures. Therefore, as a pure\nTransformer backbone with more global attention layers,\nour PVT tends to perform better than the CNN backbone"
    },
    {
      "bounding_box": [
        {
          "x": 1282,
          "y": 291
        },
        {
          "x": 2268,
          "y": 291
        },
        {
          "x": 2268,
          "y": 506
        },
        {
          "x": 1282,
          "y": 506
        }
      ],
      "category": "table",
      "html": "<br><table id='136' style='font-size:14px'><tr><td rowspan=\"2\">Method</td><td rowspan=\"2\">Scale</td><td rowspan=\"2\">GFLOPs</td><td rowspan=\"2\">Time (ms)</td><td colspan=\"3\">RetinaNet 1x</td></tr><tr><td>AP</td><td>AP50</td><td>AP75</td></tr><tr><td>ResNet50 [22]</td><td>800</td><td>239.3</td><td>55.9</td><td>36.3</td><td>55.3</td><td>38.6</td></tr><tr><td rowspan=\"2\">PVT-Small (ours)</td><td>640</td><td>157.2</td><td>51.7</td><td>38.7</td><td>59.3</td><td>40.8</td></tr><tr><td>800</td><td>285.8</td><td>76.9</td><td>40.4</td><td>61.3</td><td>43.0</td></tr></table>",
      "id": 136,
      "page": 10,
      "text": "Method Scale GFLOPs Time (ms) RetinaNet 1x\n AP AP50 AP75\n ResNet50 [22] 800 239.3 55.9 36.3 55.3 38.6\n PVT-Small (ours) 640 157.2 51.7 38.7 59.3 40.8\n 800 285.8 76.9 40.4 61.3"
    },
    {
      "bounding_box": [
        {
          "x": 1281,
          "y": 540
        },
        {
          "x": 2277,
          "y": 540
        },
        {
          "x": 2277,
          "y": 842
        },
        {
          "x": 1281,
          "y": 842
        }
      ],
      "category": "paragraph",
      "html": "<p id='137' style='font-size:16px'>Table 11: Latency and AP under different input scales.<br>\"Scale\" and \"Time\" denote the input scale and time cost<br>per image. When the shorter side is 640 pixels, the PVT-<br>Small+RetinaNet has a lower GFLOPs and time cost (on a<br>V100 GPU) than ResNet50+RetinaNet, while obtaining 2.4<br>points better AP (38.7 vs. 36.3).</p>",
      "id": 137,
      "page": 10,
      "text": "Table 11: Latency and AP under different input scales.\n\"Scale\" and \"Time\" denote the input scale and time cost\nper image. When the shorter side is 640 pixels, the PVT-\nSmall+RetinaNet has a lower GFLOPs and time cost (on a\nV100 GPU) than ResNet50+RetinaNet, while obtaining 2.4\npoints better AP (38.7 vs. 36.3)."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 939
        },
        {
          "x": 2053,
          "y": 939
        },
        {
          "x": 2053,
          "y": 983
        },
        {
          "x": 1280,
          "y": 983
        }
      ],
      "category": "paragraph",
      "html": "<p id='138' style='font-size:20px'>equipped with non-local blocks (e.g., GCNet).</p>",
      "id": 138,
      "page": 10,
      "text": "equipped with non-local blocks (e.g., GCNet)."
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 990
        },
        {
          "x": 2278,
          "y": 990
        },
        {
          "x": 2278,
          "y": 1384
        },
        {
          "x": 1278,
          "y": 1384
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='139' style='font-size:18px'>(2) Regular convolutions can be deemed as special in-<br>stantiations of spatial attention mechanisms [84]. In other<br>words, the format of MHA is more flexible than the regular<br>convolution. For example, for different inputs, the weights<br>of the convolution are fixed, but the attention weights of<br>MHA change dynamically with the input. Thus, the features<br>learned by the pure Transformer backbone full of MHA lay-<br>ers, could be more flexible and expressive.</p>",
      "id": 139,
      "page": 10,
      "text": "(2) Regular convolutions can be deemed as special in-\nstantiations of spatial attention mechanisms [84]. In other\nwords, the format of MHA is more flexible than the regular\nconvolution. For example, for different inputs, the weights\nof the convolution are fixed, but the attention weights of\nMHA change dynamically with the input. Thus, the features\nlearned by the pure Transformer backbone full of MHA lay-\ners, could be more flexible and expressive."
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 1388
        },
        {
          "x": 2278,
          "y": 1388
        },
        {
          "x": 2278,
          "y": 1735
        },
        {
          "x": 1279,
          "y": 1735
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='140' style='font-size:16px'>Computation Overhead. With increasing input scale, the<br>growth rate of the GFLOPs of our PVT is greater than<br>ResNet [22], but lower than ViT [13], as shown in Figure<br>6. However, when the input scale does not exceed 640x 640<br>pixels, the GFLOPs of PVT-Small and ResNet50 are simi-<br>lar. This means that our PVT is more suitable for tasks with<br>medium-resolution input.</p>",
      "id": 140,
      "page": 10,
      "text": "Computation Overhead. With increasing input scale, the\ngrowth rate of the GFLOPs of our PVT is greater than\nResNet [22], but lower than ViT [13], as shown in Figure\n6. However, when the input scale does not exceed 640x 640\npixels, the GFLOPs of PVT-Small and ResNet50 are simi-\nlar. This means that our PVT is more suitable for tasks with\nmedium-resolution input."
    },
    {
      "bounding_box": [
        {
          "x": 1277,
          "y": 1741
        },
        {
          "x": 2278,
          "y": 1741
        },
        {
          "x": 2278,
          "y": 2334
        },
        {
          "x": 1277,
          "y": 2334
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='141' style='font-size:18px'>On COCO, the shorter side of the input image is 800<br>pixels. Under this condition, the inference speed of Reti-<br>naNet based on PVT-Small is slower than the ResNet50-<br>based model, as reported in Table 11. (1) A direct solution<br>for this problem is to reduce the input scale. When reduc-<br>ing the shorter side of the input image to 640 pixels, the<br>model based on PVT-Small runs faster than the ResNet50-<br>based model (51.7ms VS., 55.9ms), with 2.4 higher AP<br>(38.7 vs. 36.3). 2) Another solution is to develop a self-<br>attention layer with lower computational complexity. This<br>is a worth exploring direction, we recently propose a solu-<br>tion PVTv2 [67].</p>",
      "id": 141,
      "page": 10,
      "text": "On COCO, the shorter side of the input image is 800\npixels. Under this condition, the inference speed of Reti-\nnaNet based on PVT-Small is slower than the ResNet50-\nbased model, as reported in Table 11. (1) A direct solution\nfor this problem is to reduce the input scale. When reduc-\ning the shorter side of the input image to 640 pixels, the\nmodel based on PVT-Small runs faster than the ResNet50-\nbased model (51.7ms VS., 55.9ms), with 2.4 higher AP\n(38.7 vs. 36.3). 2) Another solution is to develop a self-\nattention layer with lower computational complexity. This\nis a worth exploring direction, we recently propose a solu-\ntion PVTv2 [67]."
    },
    {
      "bounding_box": [
        {
          "x": 1278,
          "y": 2338
        },
        {
          "x": 2278,
          "y": 2338
        },
        {
          "x": 2278,
          "y": 2740
        },
        {
          "x": 1278,
          "y": 2740
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='142' style='font-size:18px'>Detection & Segmentation Results. In Figure 7, we also<br>present some qualitative object detection and instance seg-<br>mentation results on COCO val201 7 [40], and semantic<br>segmentation results on ADE20K [83]. These results indi-<br>cate that a pure Transformer backbone (i.e., PVT) without<br>convolutions can also be easily plugged in dense prediction<br>models (e.g., RetinaNet [39], Mask R-CNN [21], and Se-<br>mantic FPN [32]), and obtain high-quality results.</p>",
      "id": 142,
      "page": 10,
      "text": "Detection & Segmentation Results. In Figure 7, we also\npresent some qualitative object detection and instance seg-\nmentation results on COCO val201 7 [40], and semantic\nsegmentation results on ADE20K [83]. These results indi-\ncate that a pure Transformer backbone (i.e., PVT) without\nconvolutions can also be easily plugged in dense prediction\nmodels (e.g., RetinaNet [39], Mask R-CNN [21], and Se-\nmantic FPN [32]), and obtain high-quality results."
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 2790
        },
        {
          "x": 1982,
          "y": 2790
        },
        {
          "x": 1982,
          "y": 2844
        },
        {
          "x": 1280,
          "y": 2844
        }
      ],
      "category": "paragraph",
      "html": "<p id='143' style='font-size:22px'>6. Conclusions and Future Work</p>",
      "id": 143,
      "page": 10,
      "text": "6. Conclusions and Future Work"
    },
    {
      "bounding_box": [
        {
          "x": 1279,
          "y": 2875
        },
        {
          "x": 2277,
          "y": 2875
        },
        {
          "x": 2277,
          "y": 2976
        },
        {
          "x": 1279,
          "y": 2976
        }
      ],
      "category": "paragraph",
      "html": "<p id='144' style='font-size:16px'>We introduce PVT, a pure Transformer backbone for<br>dense prediction tasks, such as object detection and seman-</p>",
      "id": 144,
      "page": 10,
      "text": "We introduce PVT, a pure Transformer backbone for\ndense prediction tasks, such as object detection and seman-"
    },
    {
      "bounding_box": [
        {
          "x": 221,
          "y": 300
        },
        {
          "x": 1188,
          "y": 300
        },
        {
          "x": 1188,
          "y": 970
        },
        {
          "x": 221,
          "y": 970
        }
      ],
      "category": "figure",
      "html": "<figure><img id='145' style='font-size:16px' alt=\"300\nViT-Small/16\n250 ViT-Small/32\nPVT-Small (ours)\n200\nResNet50\nGFLOPS 150\n100\n50\n0 160 320 480 640 800 960 1120 1280\nInput Scale\" data-coord=\"top-left:(221,300); bottom-right:(1188,970)\" /></figure>",
      "id": 145,
      "page": 11,
      "text": "300\nViT-Small/16\n250 ViT-Small/32\nPVT-Small (ours)\n200\nResNet50\nGFLOPS 150\n100\n50\n0 160 320 480 640 800 960 1120 1280\nInput Scale"
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 1020
        },
        {
          "x": 1201,
          "y": 1020
        },
        {
          "x": 1201,
          "y": 1271
        },
        {
          "x": 200,
          "y": 1271
        }
      ],
      "category": "caption",
      "html": "<caption id='146' style='font-size:16px'>Figure 6: Models' GFLOPs under different input scales.<br>The growth rate of GFLOPs: ViT-Small/16 [13]> ViT-<br>Small/32 [13]>PVT-Small (ours)>ResNet50 [22]. When<br>the input scale is less than 640 x 640, the GFLOPs of PVT-<br>Small and ResNet50 [22] are similar.</caption>",
      "id": 146,
      "page": 11,
      "text": "Figure 6: Models' GFLOPs under different input scales.\nThe growth rate of GFLOPs: ViT-Small/16 [13]> ViT-\nSmall/32 [13]>PVT-Small (ours)>ResNet50 [22]. When\nthe input scale is less than 640 x 640, the GFLOPs of PVT-\nSmall and ResNet50 [22] are similar."
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 1388
        },
        {
          "x": 1199,
          "y": 1388
        },
        {
          "x": 1199,
          "y": 1735
        },
        {
          "x": 200,
          "y": 1735
        }
      ],
      "category": "paragraph",
      "html": "<p id='147' style='font-size:14px'>tic segmentation. We develop a progressive shrinking pyra-<br>mid and a spatial-reduction attention layer to obtain high-<br>resolution and multi-scale feature maps under limited com-<br>putation/memory resources. Extensive experiments on ob-<br>ject detection and semantic segmentation benchmarks ver-<br>ify that our PVT is stronger than well-designed CNN back-<br>bones under comparable numbers of parameters.</p>",
      "id": 147,
      "page": 11,
      "text": "tic segmentation. We develop a progressive shrinking pyra-\nmid and a spatial-reduction attention layer to obtain high-\nresolution and multi-scale feature maps under limited com-\nputation/memory resources. Extensive experiments on ob-\nject detection and semantic segmentation benchmarks ver-\nify that our PVT is stronger than well-designed CNN back-\nbones under comparable numbers of parameters."
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 1750
        },
        {
          "x": 1200,
          "y": 1750
        },
        {
          "x": 1200,
          "y": 2448
        },
        {
          "x": 200,
          "y": 2448
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='148' style='font-size:16px'>Although PVT can serve as an alternative to CNN back-<br>bones (e.g., ResNet, ResNeXt), there are still some specific<br>modules and operations designed for CNNs and not consid-<br>ered in this work, such as SE [23], SK [36], dilated convo-<br>lution [74], model pruning [20], and NAS [61]. Moreover,<br>with years of rapid developments, there have been many<br>well-engineered CNN backbones such as Res2Net [17],<br>EfficientNet [61], and ResNeSt [79]. In contrast, the<br>Transformer-based model in computer vision is still in<br>its early stage of development. Therefore, we believe<br>there are many potential technologies and applications (e.g.,<br>OCR [68, 66, 69], 3D [28, 11, 27] and medical [15, 16, 29]<br>image analysis) to be explored in the future, and hope that<br>PVT could serve as a good starting point.</p>",
      "id": 148,
      "page": 11,
      "text": "Although PVT can serve as an alternative to CNN back-\nbones (e.g., ResNet, ResNeXt), there are still some specific\nmodules and operations designed for CNNs and not consid-\nered in this work, such as SE [23], SK [36], dilated convo-\nlution [74], model pruning [20], and NAS [61]. Moreover,\nwith years of rapid developments, there have been many\nwell-engineered CNN backbones such as Res2Net [17],\nEfficientNet [61], and ResNeSt [79]. In contrast, the\nTransformer-based model in computer vision is still in\nits early stage of development. Therefore, we believe\nthere are many potential technologies and applications (e.g.,\nOCR [68, 66, 69], 3D [28, 11, 27] and medical [15, 16, 29]\nimage analysis) to be explored in the future, and hope that\nPVT could serve as a good starting point."
    },
    {
      "bounding_box": [
        {
          "x": 204,
          "y": 2532
        },
        {
          "x": 602,
          "y": 2532
        },
        {
          "x": 602,
          "y": 2584
        },
        {
          "x": 204,
          "y": 2584
        }
      ],
      "category": "paragraph",
      "html": "<p id='149' style='font-size:22px'>Acknowledgments</p>",
      "id": 149,
      "page": 11,
      "text": "Acknowledgments"
    },
    {
      "bounding_box": [
        {
          "x": 200,
          "y": 2627
        },
        {
          "x": 1200,
          "y": 2627
        },
        {
          "x": 1200,
          "y": 2976
        },
        {
          "x": 200,
          "y": 2976
        }
      ],
      "category": "paragraph",
      "html": "<p id='150' style='font-size:18px'>This work was supported by the Natural Science Founda-<br>tion of China under Grant 61672273 and Grant 61832008,<br>the Science Foundation for Distinguished Young Schol-<br>ars of Jiangsu under Grant BK20160021, Postdoctoral In-<br>novative Talent Support Program of China under Grant<br>BX20200168, 2020M681 608, the General Research Fund<br>of Hong Kong No. 27208720.</p>",
      "id": 150,
      "page": 11,
      "text": "This work was supported by the Natural Science Founda-\ntion of China under Grant 61672273 and Grant 61832008,\nthe Science Foundation for Distinguished Young Schol-\nars of Jiangsu under Grant BK20160021, Postdoctoral In-\nnovative Talent Support Program of China under Grant\nBX20200168, 2020M681 608, the General Research Fund\nof Hong Kong No. 27208720."
    },
    {
      "bounding_box": [
        {
          "x": 1283,
          "y": 301
        },
        {
          "x": 1523,
          "y": 301
        },
        {
          "x": 1523,
          "y": 352
        },
        {
          "x": 1283,
          "y": 352
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='151' style='font-size:20px'>References</p>",
      "id": 151,
      "page": 11,
      "text": "References"
    },
    {
      "bounding_box": [
        {
          "x": 1292,
          "y": 376
        },
        {
          "x": 2285,
          "y": 376
        },
        {
          "x": 2285,
          "y": 2982
        },
        {
          "x": 1292,
          "y": 2982
        }
      ],
      "category": "paragraph",
      "html": "<p id='152' style='font-size:14px'>[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-<br>ton. Layer normalization. arXiv preprint arXiv:1607.06450,<br>2016. 5<br>[2] Irwan Bello. Lambdanetworks: Modeling long-range inter-<br>actions without attention. In Proc. Int. Conf. Learn. Repre-<br>sentations, 2021. 3<br>[3] Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens,<br>and Quoc V Le. Attention augmented convolutional net-<br>works. In Proc. IEEE Int. Conf. Comp. Vis., 2019. 3<br>[4] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delv-<br>ing into high quality object detection. In Proc. IEEE Conf.<br>Comp. Vis. Patt. Recogn., 2018. 3<br>[5] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han<br>Hu. Gcnet: Non-local networks meet squeeze-excitation net-<br>works and beyond. In Proc. IEEE Int. Conf. Comp. Vis.,<br>2019. 10<br>[6] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas<br>Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-<br>end object detection with transformers. In Proc. Eur. Conf.<br>Comp. Vis., 2020. 1, 2, 3, 8<br>[7] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu<br>Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu,<br>Jiarui Xu, et al. Mmdetection: Open mmlab detection tool-<br>box and benchmark. arXiv preprint arXiv:1906.07155, 2019.<br>7<br>[8] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,<br>Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image<br>segmentation with deep convolutional nets, atrous convolu-<br>tion, and fully connected crfs. IEEE Trans. Pattern Anal.<br>Mach. Intell., 2017. 3, 7<br>[9] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian<br>Schroff, and Hartwig Adam. Encoder-decoder with atrous<br>separable convolution for semantic image segmentation. In<br>Proc. Eur. Conf. Comp. Vis., 2018. 1, 9<br>[10] Yunpeng Chen, Jianan Li, Huaxin Xiao, Xiaojie Jin,<br>Shuicheng Yan, and Jiashi Feng. Dual path networks. Proc.<br>Advances in Neural Inf. Process. Syst., 2017. 3<br>[11] Mingmei Cheng, Le Hui, Jin Xie, and Jian Yang. SSPC-<br>Net: Semi-supervised semantic 3D point cloud segmentation<br>network. In Proc. AAAI Conf. Artificial Intell., 2021. 11<br>[12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,<br>and Li Fei-Fei. Imagenet: A large-scale hierarchical im-<br>age database. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn.,<br>2009. 2, 7, 9<br>[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,<br>Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,<br>Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-<br>vain Gelly, et al. An image is worth 16x16 words: Trans-<br>formers for image recognition at scale. Proc. Int. Conf.<br>Learn. Representations, 2021. 1, 2, 3, 5, 6, 7, 9, 10, 11<br>[14] Mark Everingham, Luc Van Gool, Christopher KI Williams,<br>John Winn, and Andrew Zisserman. The pascal visual object<br>classes (voc) challenge. Int. J. Comput. Vision, 88(2):303-<br>338, 2010. 2</p>",
      "id": 152,
      "page": 11,
      "text": "[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. Layer normalization. arXiv preprint arXiv:1607.06450,\n2016. 5\n[2] Irwan Bello. Lambdanetworks: Modeling long-range inter-\nactions without attention. In Proc. Int. Conf. Learn. Repre-\nsentations, 2021. 3\n[3] Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens,\nand Quoc V Le. Attention augmented convolutional net-\nworks. In Proc. IEEE Int. Conf. Comp. Vis., 2019. 3\n[4] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delv-\ning into high quality object detection. In Proc. IEEE Conf.\nComp. Vis. Patt. Recogn., 2018. 3\n[5] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han\nHu. Gcnet: Non-local networks meet squeeze-excitation net-\nworks and beyond. In Proc. IEEE Int. Conf. Comp. Vis.,\n2019. 10\n[6] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In Proc. Eur. Conf.\nComp. Vis., 2020. 1, 2, 3, 8\n[7] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu\nXiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu,\nJiarui Xu, et al. Mmdetection: Open mmlab detection tool-\nbox and benchmark. arXiv preprint arXiv:1906.07155, 2019.\n7\n[8] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos,\nKevin Murphy, and Alan L Yuille. Deeplab: Semantic image\nsegmentation with deep convolutional nets, atrous convolu-\ntion, and fully connected crfs. IEEE Trans. Pattern Anal.\nMach. Intell., 2017. 3, 7\n[9] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian\nSchroff, and Hartwig Adam. Encoder-decoder with atrous\nseparable convolution for semantic image segmentation. In\nProc. Eur. Conf. Comp. Vis., 2018. 1, 9\n[10] Yunpeng Chen, Jianan Li, Huaxin Xiao, Xiaojie Jin,\nShuicheng Yan, and Jiashi Feng. Dual path networks. Proc.\nAdvances in Neural Inf. Process. Syst., 2017. 3\n[11] Mingmei Cheng, Le Hui, Jin Xie, and Jian Yang. SSPC-\nNet: Semi-supervised semantic 3D point cloud segmentation\nnetwork. In Proc. AAAI Conf. Artificial Intell., 2021. 11\n[12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical im-\nage database. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn.,\n2009. 2, 7, 9\n[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. Proc. Int. Conf.\nLearn. Representations, 2021. 1, 2, 3, 5, 6, 7, 9, 10, 11\n[14] Mark Everingham, Luc Van Gool, Christopher KI Williams,\nJohn Winn, and Andrew Zisserman. The pascal visual object\nclasses (voc) challenge. Int. J. Comput. Vision, 88(2):303-\n338, 2010. 2"
    },
    {
      "bounding_box": [
        {
          "x": 207,
          "y": 291
        },
        {
          "x": 2268,
          "y": 291
        },
        {
          "x": 2268,
          "y": 1870
        },
        {
          "x": 207,
          "y": 1870
        }
      ],
      "category": "figure",
      "html": "<figure><img id='153' style='font-size:14px' alt=\"pe rson :0 . 88\npe rson :0 .97\nze bra:1 .00\nze bra:1 · 00\nze bra:1 .00\npe rson :0 .99\npe rson :1 00\npe rson :1 .00\npe rson :0 .85\nCOGNAO\nJACQUET\nsk ateb oard:0 .83\nca ke:0 99\npe rson :0 · 99\nperson :0 · 94\npe rson :1 . 00\ntr uck:0 · 84\nca r:1 .00\npe rson :0 94 pe rson :0 . 99\nbe nch:1 .00\nTROU 72169\n22G\npe rson:0 .99\ncar:0 .89\nObject Detection on COCO Instance Segmentation on COCO Semantic Segmentation on ADE20K\" data-coord=\"top-left:(207,291); bottom-right:(2268,1870)\" /></figure>",
      "id": 153,
      "page": 12,
      "text": "pe rson :0 . 88\npe rson :0 .97\nze bra:1 .00\nze bra:1 · 00\nze bra:1 .00\npe rson :0 .99\npe rson :1 00\npe rson :1 .00\npe rson :0 .85\nCOGNAO\nJACQUET\nsk ateb oard:0 .83\nca ke:0 99\npe rson :0 · 99\nperson :0 · 94\npe rson :1 . 00\ntr uck:0 · 84\nca r:1 .00\npe rson :0 94 pe rson :0 . 99\nbe nch:1 .00\nTROU 72169\n22G\npe rson:0 .99\ncar:0 .89\nObject Detection on COCO Instance Segmentation on COCO Semantic Segmentation on ADE20K"
    },
    {
      "bounding_box": [
        {
          "x": 199,
          "y": 1945
        },
        {
          "x": 2279,
          "y": 1945
        },
        {
          "x": 2279,
          "y": 2099
        },
        {
          "x": 199,
          "y": 2099
        }
      ],
      "category": "paragraph",
      "html": "<p id='154' style='font-size:20px'>Figure 7: Qualitative results of object detection and instance segmentation on COCO val2017 [40], and semantic<br>segmentation on ADE20K [83]. The results (from left to right) are generated by PVT-Small-based RetinaNet [39], Mask<br>R-CNN [21], and Semantic FPN [32], respectively.</p>",
      "id": 154,
      "page": 12,
      "text": "Figure 7: Qualitative results of object detection and instance segmentation on COCO val2017 [40], and semantic\nsegmentation on ADE20K [83]. The results (from left to right) are generated by PVT-Small-based RetinaNet [39], Mask\nR-CNN [21], and Semantic FPN [32], respectively."
    },
    {
      "bounding_box": [
        {
          "x": 199,
          "y": 2180
        },
        {
          "x": 1201,
          "y": 2180
        },
        {
          "x": 1201,
          "y": 2979
        },
        {
          "x": 199,
          "y": 2979
        }
      ],
      "category": "paragraph",
      "html": "<p id='155' style='font-size:18px'>[15] Deng-Ping Fan, Ge-Peng Ji, Ming-Ming Cheng, and Ling<br>Shao. Concealed object detection. IEEE Trans. Pattern Anal.<br>Mach. Intell., 2021. 11<br>[16] Deng-Ping Fan, Ge-Peng Ji, Tao Zhou, Geng Chen, Huazhu<br>Fu, Jianbing Shen, and Ling Shao. Pranet: Parallel reverse<br>attention network for polyp segmentation. In International<br>Conference on Medical Image Computing and Computer-<br>Assisted Intervention, 2020. 11<br>[17] Shanghua Gao, Ming-Ming Cheng, Kai Zhao, Xin-Yu<br>Zhang, Ming-Hsuan Yang, and Philip HS Torr. Res2net: A<br>new multi-scale backbone architecture. IEEE Trans. Pattern<br>Anal. Mach. Intell., 2019. 11<br>[18] Xavier Glorot and Yoshua Bengio. Understanding the diffi-<br>culty of training deep feedforward neural networks. In Proc.<br>Int. Conf. Artificial Intell. & Stat., 2010. 7<br>[19] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu,<br>and Yunhe Wang. Transformer in transformer. arXiv preprint</p>",
      "id": 155,
      "page": 12,
      "text": "[15] Deng-Ping Fan, Ge-Peng Ji, Ming-Ming Cheng, and Ling\nShao. Concealed object detection. IEEE Trans. Pattern Anal.\nMach. Intell., 2021. 11\n[16] Deng-Ping Fan, Ge-Peng Ji, Tao Zhou, Geng Chen, Huazhu\nFu, Jianbing Shen, and Ling Shao. Pranet: Parallel reverse\nattention network for polyp segmentation. In International\nConference on Medical Image Computing and Computer-\nAssisted Intervention, 2020. 11\n[17] Shanghua Gao, Ming-Ming Cheng, Kai Zhao, Xin-Yu\nZhang, Ming-Hsuan Yang, and Philip HS Torr. Res2net: A\nnew multi-scale backbone architecture. IEEE Trans. Pattern\nAnal. Mach. Intell., 2019. 11\n[18] Xavier Glorot and Yoshua Bengio. Understanding the diffi-\nculty of training deep feedforward neural networks. In Proc.\nInt. Conf. Artificial Intell. & Stat., 2010. 7\n[19] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu,\nand Yunhe Wang. Transformer in transformer. arXiv preprint"
    },
    {
      "bounding_box": [
        {
          "x": 1363,
          "y": 2190
        },
        {
          "x": 1780,
          "y": 2190
        },
        {
          "x": 1780,
          "y": 2232
        },
        {
          "x": 1363,
          "y": 2232
        }
      ],
      "category": "caption",
      "html": "<br><caption id='156' style='font-size:16px'>arXiv:2103.00112, 2021. 7</caption>",
      "id": 156,
      "page": 12,
      "text": "arXiv:2103.00112, 2021. 7"
    },
    {
      "bounding_box": [
        {
          "x": 1280,
          "y": 2235
        },
        {
          "x": 2279,
          "y": 2235
        },
        {
          "x": 2279,
          "y": 2974
        },
        {
          "x": 1280,
          "y": 2974
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='157' style='font-size:16px'>[20] Song Han, Huizi Mao, and William J Dally. Deep com-<br>pression: Compressing deep neural networks with pruning,<br>trained quantization and huffman coding. arXiv preprint<br>arXiv:1510.00149, 2015. 11<br>[21] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Gir-<br>shick. Mask r-cnn. In Proc. IEEE Int. Conf. Comp. Vis.,<br>2017. 1, 2, 3, 6, 7, 8, 10, 12<br>[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.<br>Deep residual learning for image recognition. In Proc. IEEE<br>Conf. Comp. Vis. Patt. Recogn., 2016. 1, 2, 3, 4, 5, 6, 7, 8, 9,<br>10, 11<br>[23] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-<br>works. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2018.<br>11<br>[24] Ronghang Hu and Amanpreet Singh. Transformer is all you<br>need: Multimodal multitask learning with a unified trans-</p>",
      "id": 157,
      "page": 12,
      "text": "[20] Song Han, Huizi Mao, and William J Dally. Deep com-\npression: Compressing deep neural networks with pruning,\ntrained quantization and huffman coding. arXiv preprint\narXiv:1510.00149, 2015. 11\n[21] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Gir-\nshick. Mask r-cnn. In Proc. IEEE Int. Conf. Comp. Vis.,\n2017. 1, 2, 3, 6, 7, 8, 10, 12\n[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proc. IEEE\nConf. Comp. Vis. Patt. Recogn., 2016. 1, 2, 3, 4, 5, 6, 7, 8, 9,\n10, 11\n[23] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-\nworks. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2018.\n11\n[24] Ronghang Hu and Amanpreet Singh. Transformer is all you\nneed: Multimodal multitask learning with a unified trans-"
    },
    {
      "bounding_box": [
        {
          "x": 286,
          "y": 311
        },
        {
          "x": 1049,
          "y": 311
        },
        {
          "x": 1049,
          "y": 350
        },
        {
          "x": 286,
          "y": 350
        }
      ],
      "category": "paragraph",
      "html": "<p id='158' style='font-size:14px'>former. arXiv preprint arXiv:2102.10772, 2211. 2</p>",
      "id": 158,
      "page": 13,
      "text": "former. arXiv preprint arXiv:2102.10772, 2211. 2"
    },
    {
      "bounding_box": [
        {
          "x": 201,
          "y": 357
        },
        {
          "x": 1200,
          "y": 357
        },
        {
          "x": 1200,
          "y": 2974
        },
        {
          "x": 201,
          "y": 2974
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='159' style='font-size:14px'>[25] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-<br>ian Q Weinberger. Densely connected convolutional net-<br>works. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2017.<br>3<br>[26] Zilong Huang, Xinggang Wang, Lichao Huang, Chang<br>Huang, Yunchao Wei, and Wenyu Liu. Ccnet: Criss-cross at-<br>tention for semantic segmentation. In Proc. IEEE Int. Conf.<br>Comp. Vis., 2019. 3<br>[27] Le Hui, Mingmei Cheng, Jin Xie, and Jian Yang. Efficient<br>3D point cloud feature learning for large-scale place recog-<br>nition. arXiv preprint arXiv:2101.02374, 2021. 11<br>[28] Le Hui, Rui Xu, Jin Xie, Jianjun Qian, and Jian Yang. Pro-<br>gressive point cloud deconvolution generation network. In<br>Proc. Eur. Conf. Comp. Vis., 2020. 11<br>[29] Ge-Peng Ji, Yu-Cheng Chou, Deng-Ping Fan, Geng Chen,<br>Huazhu Fu, Debesh Jha, and Ling Shao. Progressively nor-<br>malized self-attention network for video polyp segmentation.<br>In International Conference on Medical Image Computing<br>and Computer-Assisted Intervention, 2021. 11<br>[30] Xu Jia, Bert De Brabandere, Tinne Tuytelaars, and Luc V<br>Gool. Dynamic filter networks. In Proc. Advances in Neural<br>Inf. Process. Syst., 2016. 3<br>[31] Asifullah Khan, Anabia Sohail, Umme Zahoora, and<br>Aqsa Saeed Qureshi. A survey of the recent architectures of<br>deep convolutional neural networks. Artificial Intelligence<br>Review, 53(8):5455-5516, 2020. 3<br>[32] Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr<br>Dollar. Panoptic feature pyramid networks. In Proc. IEEE<br>Conf. Comp. Vis. Patt. Recogn., 2019. 1, 3, 6, 7, 10, 12<br>[33] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.<br>Imagenet classification with deep convolutional neural net-<br>works. Proc. Advances in Neural Inf. Process. Syst., 2012.<br>3<br>[34] Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick<br>Haffner. Gradient-based learning applied to document recog-<br>nition. 1998. 3<br>[35] Xiang Li, Wenhai Wang, Xiaolin Hu, Jun Li, Jinhui Tang,<br>and Jian Yang. Generalized focal loss v2: Learning reliable<br>localization quality estimation for dense object detection. In<br>Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2021. 3<br>[36] Xiang Li, Wenhai Wang, Xiaolin Hu, and Jian Yang. Selec-<br>tive kernel networks. In Proc. IEEE Conf. Comp. Vis. Patt.<br>Recogn., 2019. 3, 11<br>[37] Xiang Li, Wenhai Wang, Lijun Wu, Shuo Chen, Xiaolin Hu,<br>Jun Li, Jinhui Tang, and Jian Yang. Generalized focal loss:<br>Learning qualified and distributed bounding boxes for dense<br>object detection. In Proc. Advances in Neural Inf. Process.<br>Syst., 2020. 3<br>[38] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He,<br>Bharath Hariharan, and Serge Belongie. Feature pyramid<br>networks for object detection. In Proc. IEEE Conf. Comp.<br>Vis. Patt. Recogn., 2017. 3, 6<br>[39] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and<br>Piotr Dollar. Focal loss for dense object detection. In Proc.<br>IEEE Int. Conf. Comp. Vis., 2017. 1, 2, 3, 6, 7, 8, 10, 12</p>",
      "id": 159,
      "page": 13,
      "text": "[25] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-\nian Q Weinberger. Densely connected convolutional net-\nworks. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2017.\n3\n[26] Zilong Huang, Xinggang Wang, Lichao Huang, Chang\nHuang, Yunchao Wei, and Wenyu Liu. Ccnet: Criss-cross at-\ntention for semantic segmentation. In Proc. IEEE Int. Conf.\nComp. Vis., 2019. 3\n[27] Le Hui, Mingmei Cheng, Jin Xie, and Jian Yang. Efficient\n3D point cloud feature learning for large-scale place recog-\nnition. arXiv preprint arXiv:2101.02374, 2021. 11\n[28] Le Hui, Rui Xu, Jin Xie, Jianjun Qian, and Jian Yang. Pro-\ngressive point cloud deconvolution generation network. In\nProc. Eur. Conf. Comp. Vis., 2020. 11\n[29] Ge-Peng Ji, Yu-Cheng Chou, Deng-Ping Fan, Geng Chen,\nHuazhu Fu, Debesh Jha, and Ling Shao. Progressively nor-\nmalized self-attention network for video polyp segmentation.\nIn International Conference on Medical Image Computing\nand Computer-Assisted Intervention, 2021. 11\n[30] Xu Jia, Bert De Brabandere, Tinne Tuytelaars, and Luc V\nGool. Dynamic filter networks. In Proc. Advances in Neural\nInf. Process. Syst., 2016. 3\n[31] Asifullah Khan, Anabia Sohail, Umme Zahoora, and\nAqsa Saeed Qureshi. A survey of the recent architectures of\ndeep convolutional neural networks. Artificial Intelligence\nReview, 53(8):5455-5516, 2020. 3\n[32] Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr\nDollar. Panoptic feature pyramid networks. In Proc. IEEE\nConf. Comp. Vis. Patt. Recogn., 2019. 1, 3, 6, 7, 10, 12\n[33] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classification with deep convolutional neural net-\nworks. Proc. Advances in Neural Inf. Process. Syst., 2012.\n3\n[34] Yann LeCun, Leon Bottou, Yoshua Bengio, and Patrick\nHaffner. Gradient-based learning applied to document recog-\nnition. 1998. 3\n[35] Xiang Li, Wenhai Wang, Xiaolin Hu, Jun Li, Jinhui Tang,\nand Jian Yang. Generalized focal loss v2: Learning reliable\nlocalization quality estimation for dense object detection. In\nProc. IEEE Conf. Comp. Vis. Patt. Recogn., 2021. 3\n[36] Xiang Li, Wenhai Wang, Xiaolin Hu, and Jian Yang. Selec-\ntive kernel networks. In Proc. IEEE Conf. Comp. Vis. Patt.\nRecogn., 2019. 3, 11\n[37] Xiang Li, Wenhai Wang, Lijun Wu, Shuo Chen, Xiaolin Hu,\nJun Li, Jinhui Tang, and Jian Yang. Generalized focal loss:\nLearning qualified and distributed bounding boxes for dense\nobject detection. In Proc. Advances in Neural Inf. Process.\nSyst., 2020. 3\n[38] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He,\nBharath Hariharan, and Serge Belongie. Feature pyramid\nnetworks for object detection. In Proc. IEEE Conf. Comp.\nVis. Patt. Recogn., 2017. 3, 6\n[39] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and\nPiotr Dollar. Focal loss for dense object detection. In Proc.\nIEEE Int. Conf. Comp. Vis., 2017. 1, 2, 3, 6, 7, 8, 10, 12"
    },
    {
      "bounding_box": [
        {
          "x": 1275,
          "y": 302
        },
        {
          "x": 2288,
          "y": 302
        },
        {
          "x": 2288,
          "y": 2977
        },
        {
          "x": 1275,
          "y": 2977
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='160' style='font-size:14px'>[40] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,<br>Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence<br>Zitnick. Microsoft COCO: Common objects in context. In<br>Proc. Eur. Conf. Comp. Vis., 2014. 2, 7, 9, 10, 12<br>[41] Chenxi Liu, Liang-Chieh Chen, Florian Schroff, Hartwig<br>Adam, Wei Hua, Alan L Yuille, and Li Fei-Fei. Auto-<br>deeplab: Hierarchical neural architecture search for semantic<br>image segmentation. In Proc. IEEE Conf. Comp. Vis. Patt.<br>Recogn., 2019. 3<br>[42] Nian Liu, Ni Zhang, Kaiyuan Wan, Ling Shao, and Junwei<br>Han. Visual saliency transformer. In Proc. IEEE Int. Conf.<br>Comp. Vis., 2021. 2<br>[43] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian<br>Szegedy, Scott Reed, Cheng- Yang Fu, and Alexander C<br>Berg. Ssd: Single shot multibox detector. In Proc. Eur. Conf.<br>Comp. Vis., 2016. 3<br>[44] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully<br>convolutional networks for semantic segmentation. In Proc.<br>IEEE Conf. Comp. Vis. Patt. Recogn., 2015. 3<br>[45] Ilya Loshchilov and Frank Hutter. SGDR: stochastic gradi-<br>ent descent with warm restarts. In Proc. Int. Conf. Learn.<br>Representations, 2017. 6<br>[46] Ilya Loshchilov and Frank Hutter. Decoupled weight decay<br>regularization. In Proc. Int. Conf. Learn. Representations,<br>2019. 6, 7<br>[47] Hyeonwoo Noh, Seunghoon Hong, and Bohyung Han.<br>Learning deconvolution network for semantic segmentation.<br>In Proc. IEEE Int. Conf. Comp. Vis., 2015. 3<br>[48] Niki Parmar, Prajit Ramachandran, Ashish Vaswani, Irwan<br>Bello, Anselm Levskaya, and Jon Shlens. Stand-alone<br>self-attention in vision models. In Hanna M. Wallach,<br>Hugo Larochelle, Alina Beygelzimer, Florence d'Alche-<br>Buc, Emily B. Fox, and Roman Garnett, editors, Proc. Ad-<br>vances in Neural Inf. Process. Syst., 2019. 2, 3<br>[49] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.<br>Faster r-cnn: Towards real-time object detection with region<br>proposal networks. In Proc. Advances in Neural Inf. Process.<br>Syst., 2015. 1, 3<br>[50] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-<br>net: Convolutional networks for biomedical image segmen-<br>tation. In International Conference on Medical image com-<br>puting and computer-assisted intervention, 2015. 3<br>[51] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-<br>jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,<br>Aditya Khosla, Michael Bernstein, et al. Imagenet large<br>scale visual recognition challenge. Int. J. Comput. Vision,<br>2015. 3, 6<br>[52] Suyash Shetty. Application of convolutional neural net-<br>work for image classification on pascal VOC challenge 2012<br>dataset. arXiv preprint arXiv:1607.03785, 2016. 3<br>[53] Connor Shorten and Taghi M Khoshgoftaar. A survey on<br>image data augmentation for deep learning. Journal of Big<br>Data, 6(1):1-48, 2019. 3<br>[54] Karen Simonyan and Andrew Zisserman. Very deep con-<br>volutional networks for large-scale image recognition. In<br>Yoshua Bengio and Yann LeCun, editors, Proc. Int. Conf.<br>Learn. Representations, 2015. 1, 3, 4</p>",
      "id": 160,
      "page": 13,
      "text": "[40] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence\nZitnick. Microsoft COCO: Common objects in context. In\nProc. Eur. Conf. Comp. Vis., 2014. 2, 7, 9, 10, 12\n[41] Chenxi Liu, Liang-Chieh Chen, Florian Schroff, Hartwig\nAdam, Wei Hua, Alan L Yuille, and Li Fei-Fei. Auto-\ndeeplab: Hierarchical neural architecture search for semantic\nimage segmentation. In Proc. IEEE Conf. Comp. Vis. Patt.\nRecogn., 2019. 3\n[42] Nian Liu, Ni Zhang, Kaiyuan Wan, Ling Shao, and Junwei\nHan. Visual saliency transformer. In Proc. IEEE Int. Conf.\nComp. Vis., 2021. 2\n[43] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian\nSzegedy, Scott Reed, Cheng- Yang Fu, and Alexander C\nBerg. Ssd: Single shot multibox detector. In Proc. Eur. Conf.\nComp. Vis., 2016. 3\n[44] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully\nconvolutional networks for semantic segmentation. In Proc.\nIEEE Conf. Comp. Vis. Patt. Recogn., 2015. 3\n[45] Ilya Loshchilov and Frank Hutter. SGDR: stochastic gradi-\nent descent with warm restarts. In Proc. Int. Conf. Learn.\nRepresentations, 2017. 6\n[46] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. In Proc. Int. Conf. Learn. Representations,\n2019. 6, 7\n[47] Hyeonwoo Noh, Seunghoon Hong, and Bohyung Han.\nLearning deconvolution network for semantic segmentation.\nIn Proc. IEEE Int. Conf. Comp. Vis., 2015. 3\n[48] Niki Parmar, Prajit Ramachandran, Ashish Vaswani, Irwan\nBello, Anselm Levskaya, and Jon Shlens. Stand-alone\nself-attention in vision models. In Hanna M. Wallach,\nHugo Larochelle, Alina Beygelzimer, Florence d'Alche-\nBuc, Emily B. Fox, and Roman Garnett, editors, Proc. Ad-\nvances in Neural Inf. Process. Syst., 2019. 2, 3\n[49] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster r-cnn: Towards real-time object detection with region\nproposal networks. In Proc. Advances in Neural Inf. Process.\nSyst., 2015. 1, 3\n[50] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nnet: Convolutional networks for biomedical image segmen-\ntation. In International Conference on Medical image com-\nputing and computer-assisted intervention, 2015. 3\n[51] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, et al. Imagenet large\nscale visual recognition challenge. Int. J. Comput. Vision,\n2015. 3, 6\n[52] Suyash Shetty. Application of convolutional neural net-\nwork for image classification on pascal VOC challenge 2012\ndataset. arXiv preprint arXiv:1607.03785, 2016. 3\n[53] Connor Shorten and Taghi M Khoshgoftaar. A survey on\nimage data augmentation for deep learning. Journal of Big\nData, 6(1):1-48, 2019. 3\n[54] Karen Simonyan and Andrew Zisserman. Very deep con-\nvolutional networks for large-scale image recognition. In\nYoshua Bengio and Yann LeCun, editors, Proc. Int. Conf.\nLearn. Representations, 2015. 1, 3, 4"
    },
    {
      "bounding_box": [
        {
          "x": 199,
          "y": 298
        },
        {
          "x": 1201,
          "y": 298
        },
        {
          "x": 1201,
          "y": 2975
        },
        {
          "x": 199,
          "y": 2975
        }
      ],
      "category": "paragraph",
      "html": "<p id='161' style='font-size:18px'>[55] Peize Sun, Yi Jiang, Enze Xie, Zehuan Yuan, Changhu<br>Wang, and Ping Luo. Onenet: Towards end-to-end one-stage<br>object detection. arXiv preprint arXiv:2012.05780, 2020. 3<br>[56] Peize Sun, Yi Jiang, Rufeng Zhang, Enze Xie, Jinkun Cao,<br>Xinting Hu, Tao Kong, Zehuan Yuan, Changhu Wang, and<br>Ping Luo. Transtrack: Multiple-object tracking with trans-<br>former. arXiv preprint arXiv:2012.15460, 2020. 2<br>[57] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng<br>Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan Yuan,<br>Changhu Wang, et al. Sparse r-cnn: End-to-end object de-<br>tection with learnable proposals. In Proc. IEEE Conf. Comp.<br>Vis. Patt. Recogn., 2021. 3<br>[58] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and<br>Alexander Alemi. Inception-v4, inception-resnet and the im-<br>pact of residual connections on learning. In Proc. AAAI Conf.<br>Artificial Intell., 2017. 3<br>[59] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,<br>Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent<br>Vanhoucke, and Andrew Rabinovich. Going deeper with<br>convolutions. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn.,<br>2015. 3, 6<br>[60] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon<br>Shlens, and Zbigniew Wojna. Rethinking the inception ar-<br>chitecture for computer vision. In Proc. IEEE Conf. Comp.<br>Vis. Patt. Recogn., 2016. 3, 6<br>[61] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model<br>scaling for convolutional neural networks. In Proc. Int. Conf.<br>Mach. Learn., 2019. 11<br>[62] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos:<br>Fully convolutional one-stage object detection. In Proc.<br>IEEE Int. Conf. Comp. Vis., 2019. 3<br>[63] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco<br>Massa, Alexandre Sablayrolles, and Herve Jegou. Training<br>data-efficient image transformers & distillation through at-<br>tention. In Proc. Int. Conf. Mach. Learn., 2021. 3, 6, 7<br>[64] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-<br>reit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia<br>Polosukhin. Attention is all you need. In Proc. Advances in<br>Neural Inf. Process. Syst., 2017. 2, 3, 4, 5, 10<br>[65] Wenhai Wang, Xiang Li, Jian Yang, and Tong Lu. Mixed<br>link networks. Proc. Int. Joint Conf. Artificial Intell., 2018.<br>3<br>[66] Wenhai Wang, Xuebo Liu, Xiaozhong Ji, Enze Xie, Ding<br>Liang, ZhiBo Yang, Tong Lu, Chunhua Shen, and Ping Luo.<br>Ae textspotter: Learning visual and linguistic representation<br>for ambiguous text spotting. In Proc. Eur. Conf. Comp. Vis.,<br>2020. 11<br>[67] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao<br>Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.<br>Pvtv2: Improved baselines with pyramid vision transformer.<br>arXiv preprint arXiv:2106. 13797, 2021. 10<br>[68] Wenhai Wang, Enze Xie, Xiang Li, Wenbo Hou, Tong Lu,<br>Gang Yu, and Shuai Shao. Shape robust text detection with<br>progressive scale expansion network. In Proc. IEEE Conf.<br>Comp. Vis. Patt. Recogn., 2019. 11<br>[69] Wenhai Wang, Enze Xie, Xiang Li, Xuebo Liu, Ding Liang,<br>Yang Zhibo, Tong Lu, and Chunhua Shen. Pan++: Towards</p>",
      "id": 161,
      "page": 14,
      "text": "[55] Peize Sun, Yi Jiang, Enze Xie, Zehuan Yuan, Changhu\nWang, and Ping Luo. Onenet: Towards end-to-end one-stage\nobject detection. arXiv preprint arXiv:2012.05780, 2020. 3\n[56] Peize Sun, Yi Jiang, Rufeng Zhang, Enze Xie, Jinkun Cao,\nXinting Hu, Tao Kong, Zehuan Yuan, Changhu Wang, and\nPing Luo. Transtrack: Multiple-object tracking with trans-\nformer. arXiv preprint arXiv:2012.15460, 2020. 2\n[57] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng\nXu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan Yuan,\nChanghu Wang, et al. Sparse r-cnn: End-to-end object de-\ntection with learnable proposals. In Proc. IEEE Conf. Comp.\nVis. Patt. Recogn., 2021. 3\n[58] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and\nAlexander Alemi. Inception-v4, inception-resnet and the im-\npact of residual connections on learning. In Proc. AAAI Conf.\nArtificial Intell., 2017. 3\n[59] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\nVanhoucke, and Andrew Rabinovich. Going deeper with\nconvolutions. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn.,\n2015. 3, 6\n[60] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon\nShlens, and Zbigniew Wojna. Rethinking the inception ar-\nchitecture for computer vision. In Proc. IEEE Conf. Comp.\nVis. Patt. Recogn., 2016. 3, 6\n[61] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model\nscaling for convolutional neural networks. In Proc. Int. Conf.\nMach. Learn., 2019. 11\n[62] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos:\nFully convolutional one-stage object detection. In Proc.\nIEEE Int. Conf. Comp. Vis., 2019. 3\n[63] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herve Jegou. Training\ndata-efficient image transformers & distillation through at-\ntention. In Proc. Int. Conf. Mach. Learn., 2021. 3, 6, 7\n[64] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Proc. Advances in\nNeural Inf. Process. Syst., 2017. 2, 3, 4, 5, 10\n[65] Wenhai Wang, Xiang Li, Jian Yang, and Tong Lu. Mixed\nlink networks. Proc. Int. Joint Conf. Artificial Intell., 2018.\n3\n[66] Wenhai Wang, Xuebo Liu, Xiaozhong Ji, Enze Xie, Ding\nLiang, ZhiBo Yang, Tong Lu, Chunhua Shen, and Ping Luo.\nAe textspotter: Learning visual and linguistic representation\nfor ambiguous text spotting. In Proc. Eur. Conf. Comp. Vis.,\n2020. 11\n[67] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.\nPvtv2: Improved baselines with pyramid vision transformer.\narXiv preprint arXiv:2106. 13797, 2021. 10\n[68] Wenhai Wang, Enze Xie, Xiang Li, Wenbo Hou, Tong Lu,\nGang Yu, and Shuai Shao. Shape robust text detection with\nprogressive scale expansion network. In Proc. IEEE Conf.\nComp. Vis. Patt. Recogn., 2019. 11\n[69] Wenhai Wang, Enze Xie, Xiang Li, Xuebo Liu, Ding Liang,\nYang Zhibo, Tong Lu, and Chunhua Shen. Pan++: Towards"
    },
    {
      "bounding_box": [
        {
          "x": 1364,
          "y": 311
        },
        {
          "x": 2276,
          "y": 311
        },
        {
          "x": 2276,
          "y": 441
        },
        {
          "x": 1364,
          "y": 441
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='162' style='font-size:14px'>efficient and accurate end-to-end spotting of arbitrarily-<br>shaped text. IEEE Trans. Pattern Anal. Mach. Intell., 2021.<br>11</p>",
      "id": 162,
      "page": 14,
      "text": "efficient and accurate end-to-end spotting of arbitrarily-\nshaped text. IEEE Trans. Pattern Anal. Mach. Intell., 2021.\n11"
    },
    {
      "bounding_box": [
        {
          "x": 1274,
          "y": 404
        },
        {
          "x": 2287,
          "y": 404
        },
        {
          "x": 2287,
          "y": 2978
        },
        {
          "x": 1274,
          "y": 2978
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='163' style='font-size:18px'>[70] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-<br>ing He. Non-local neural networks. In Proc. IEEE Conf.<br>Comp. Vis. Patt. Recogn., 2018. 2, 3, 10<br>[71] Enze Xie, Peize Sun, Xiaoge Song, Wenhai Wang, Xuebo<br>Liu, Ding Liang, Chunhua Shen, and Ping Luo. Polarmask:<br>Single shot instance segmentation with polar representation.<br>In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2020. 3<br>[72] Enze Xie, Wenjia Wang, Wenhai Wang, Peize Sun, Hang Xu,<br>Ding Liang, and Ping Luo. Segmenting transparent object in<br>the wild with transformer. In Proc. Int. Joint Conf. Artificial<br>Intell., 2021. 2, 9<br>[73] Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and<br>Kaiming He. Aggregated residual transformations for deep<br>neural networks. In Proc. IEEE Conf. Comp. Vis. Patt.<br>Recogn., 2017. 1, 2, 3, 6, 7, 8<br>[74] Fisher Yu and Vladlen Koltun. Multi-scale context aggrega-<br>tion by dilated convolutions. In Yoshua Bengio and Yann Le-<br>Cun, editors, Proc. Int. Conf. Learn. Representations, 2016.<br>7, 11<br>[75] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,<br>Zihang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng<br>Yan. Tokens-to-token vit: Training vision transformers<br>from scratch on imagenet. arXiv preprint arXiv:2101.11986,<br>2021. 7<br>[76] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk<br>Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regular-<br>ization strategy to train strong classifiers with localizable fea-<br>tures. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages<br>6023-6032, 2019. 6<br>[77] Erwan Zerhouni, David Lanyi, Matheus Viana, and Maria<br>Gabrani. Wide residual networks for mitosis detection.<br>In IEEE International Symposium on Biomedical Imaging,<br>2017. 9<br>[78] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and<br>David Lopez-Paz. mixup: Beyond empirical risk minimiza-<br>tion. In Proc. Int. Conf. Learn. Representations, 2018. 6<br>[79] Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Zhi<br>Zhang, Haibin Lin, Yue Sun, Tong He, Jonas Mueller, R<br>Manmatha, et al. Resnest: Split-attention networks. arXiv<br>preprint arXiv:2004. 08955, 2020. 11<br>[80] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring<br>self-attention for image recognition. In Proc. IEEE Conf.<br>Comp. Vis. Patt. Recogn., 2020. 2<br>[81] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang<br>Wang, and Jiaya Jia. Pyramid scene parsing network. In<br>Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2017. 3<br>[82] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and<br>Yi Yang. Random erasing data augmentation. In Proc. AAAI<br>Conf. Artificial Intell., 2020. 6<br>[83] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela<br>Barriuso, and Antonio Torralba. Scene parsing through<br>ade20k dataset. In Proc. IEEE Conf. Comp. Vis. Patt.<br>Recogn., 2017. 2, 7, 9, 10, 12</p>",
      "id": 163,
      "page": 14,
      "text": "[70] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-\ning He. Non-local neural networks. In Proc. IEEE Conf.\nComp. Vis. Patt. Recogn., 2018. 2, 3, 10\n[71] Enze Xie, Peize Sun, Xiaoge Song, Wenhai Wang, Xuebo\nLiu, Ding Liang, Chunhua Shen, and Ping Luo. Polarmask:\nSingle shot instance segmentation with polar representation.\nIn Proc. IEEE Conf. Comp. Vis. Patt. Recogn., 2020. 3\n[72] Enze Xie, Wenjia Wang, Wenhai Wang, Peize Sun, Hang Xu,\nDing Liang, and Ping Luo. Segmenting transparent object in\nthe wild with transformer. In Proc. Int. Joint Conf. Artificial\nIntell., 2021. 2, 9\n[73] Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and\nKaiming He. Aggregated residual transformations for deep\nneural networks. In Proc. IEEE Conf. Comp. Vis. Patt.\nRecogn., 2017. 1, 2, 3, 6, 7, 8\n[74] Fisher Yu and Vladlen Koltun. Multi-scale context aggrega-\ntion by dilated convolutions. In Yoshua Bengio and Yann Le-\nCun, editors, Proc. Int. Conf. Learn. Representations, 2016.\n7, 11\n[75] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\nZihang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng\nYan. Tokens-to-token vit: Training vision transformers\nfrom scratch on imagenet. arXiv preprint arXiv:2101.11986,\n2021. 7\n[76] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk\nChun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regular-\nization strategy to train strong classifiers with localizable fea-\ntures. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn., pages\n6023-6032, 2019. 6\n[77] Erwan Zerhouni, David Lanyi, Matheus Viana, and Maria\nGabrani. Wide residual networks for mitosis detection.\nIn IEEE International Symposium on Biomedical Imaging,\n2017. 9\n[78] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and\nDavid Lopez-Paz. mixup: Beyond empirical risk minimiza-\ntion. In Proc. Int. Conf. Learn. Representations, 2018. 6\n[79] Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Zhi\nZhang, Haibin Lin, Yue Sun, Tong He, Jonas Mueller, R\nManmatha, et al. Resnest: Split-attention networks. arXiv\npreprint arXiv:2004. 08955, 2020. 11\n[80] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring\nself-attention for image recognition. In Proc. IEEE Conf.\nComp. Vis. Patt. Recogn., 2020. 2\n[81] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang\nWang, and Jiaya Jia. Pyramid scene parsing network. In\nProc. IEEE Conf. Comp. Vis. Patt. Recogn., 2017. 3\n[82] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and\nYi Yang. Random erasing data augmentation. In Proc. AAAI\nConf. Artificial Intell., 2020. 6\n[83] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela\nBarriuso, and Antonio Torralba. Scene parsing through\nade20k dataset. In Proc. IEEE Conf. Comp. Vis. Patt.\nRecogn., 2017. 2, 7, 9, 10, 12"
    },
    {
      "bounding_box": [
        {
          "x": 202,
          "y": 307
        },
        {
          "x": 1203,
          "y": 307
        },
        {
          "x": 1203,
          "y": 679
        },
        {
          "x": 202,
          "y": 679
        }
      ],
      "category": "paragraph",
      "html": "<p id='164' style='font-size:14px'>[84] Xizhou Zhu, Dazhi Cheng, Zheng Zhang, Stephen Lin, and<br>Jifeng Dai. An empirical study of spatial attention mech-<br>anisms in deep networks. In Proc. IEEE Int. Conf. Comp.<br>Vis., 2019. 10<br>[85] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,<br>and Jifeng Dai. Deformable DETR: deformable transformers<br>for end-to-end object detection. In Proc. Int. Conf. Learn.<br>Representations, 2021. 2, 3</p>",
      "id": 164,
      "page": 15,
      "text": "[84] Xizhou Zhu, Dazhi Cheng, Zheng Zhang, Stephen Lin, and\nJifeng Dai. An empirical study of spatial attention mech-\nanisms in deep networks. In Proc. IEEE Int. Conf. Comp.\nVis., 2019. 10\n[85] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang,\nand Jifeng Dai. Deformable DETR: deformable transformers\nfor end-to-end object detection. In Proc. Int. Conf. Learn.\nRepresentations, 2021. 2, 3"
    }
  ]
}