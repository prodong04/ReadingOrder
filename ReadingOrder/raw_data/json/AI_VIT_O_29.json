{
  "id": "629efef4-0f92-11ef-8230-426932df3dcf",
  "pdf_path": "./pdf/AI_VIT_O/2107.10224v4.pdf",
  "elements": [
    {
      "bounding_box": [
        {
          "x": 444,
          "y": 109
        },
        {
          "x": 1225,
          "y": 109
        },
        {
          "x": 1225,
          "y": 158
        },
        {
          "x": 444,
          "y": 158
        }
      ],
      "category": "header",
      "html": "<header id='0' style='font-size:16px'>Published as a conference paper at ICLR 2022</header>",
      "id": 0,
      "page": 1,
      "text": "Published as a conference paper at ICLR 2022"
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 330
        },
        {
          "x": 2105,
          "y": 330
        },
        {
          "x": 2105,
          "y": 483
        },
        {
          "x": 442,
          "y": 483
        }
      ],
      "category": "paragraph",
      "html": "<p id='1' style='font-size:22px'>CYCLEMLP: A MLP-LIKE ARCHITECTURE FOR<br>DENSE PREDICTION</p>",
      "id": 1,
      "page": 1,
      "text": "CYCLEMLP: A MLP-LIKE ARCHITECTURE FOR\nDENSE PREDICTION"
    },
    {
      "bounding_box": [
        {
          "x": 475,
          "y": 561
        },
        {
          "x": 2128,
          "y": 561
        },
        {
          "x": 2128,
          "y": 614
        },
        {
          "x": 475,
          "y": 614
        }
      ],
      "category": "paragraph",
      "html": "<p id='2' style='font-size:20px'>Shoufa Chen1 Enze Xie1 Chongjian Ge1 Runjian Chen1 Ding Liang2 Ping Lu01,3</p>",
      "id": 2,
      "page": 1,
      "text": "Shoufa Chen1 Enze Xie1 Chongjian Ge1 Runjian Chen1 Ding Liang2 Ping Lu01,3"
    },
    {
      "bounding_box": [
        {
          "x": 475,
          "y": 614
        },
        {
          "x": 1431,
          "y": 614
        },
        {
          "x": 1431,
          "y": 708
        },
        {
          "x": 475,
          "y": 708
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='3' style='font-size:18px'>1 The University of Hong Kong 2 SenseTime Research<br>3 Shanghai AI Laboratory, Shanghai, China</p>",
      "id": 3,
      "page": 1,
      "text": "1 The University of Hong Kong 2 SenseTime Research\n3 Shanghai AI Laboratory, Shanghai, China"
    },
    {
      "bounding_box": [
        {
          "x": 474,
          "y": 716
        },
        {
          "x": 1636,
          "y": 716
        },
        {
          "x": 1636,
          "y": 803
        },
        {
          "x": 474,
          "y": 803
        }
      ],
      "category": "paragraph",
      "html": "<p id='4' style='font-size:14px'>{shoufach, xieenze, rhettgee, rjchen}@connect · hku hk<br>liangding@sensetime · com pluo@cs · hku · hk</p>",
      "id": 4,
      "page": 1,
      "text": "{shoufach, xieenze, rhettgee, rjchen}@connect · hku hk\nliangding@sensetime · com pluo@cs · hku · hk"
    },
    {
      "bounding_box": [
        {
          "x": 1153,
          "y": 923
        },
        {
          "x": 1396,
          "y": 923
        },
        {
          "x": 1396,
          "y": 974
        },
        {
          "x": 1153,
          "y": 974
        }
      ],
      "category": "paragraph",
      "html": "<p id='5' style='font-size:22px'>ABSTRACT</p>",
      "id": 5,
      "page": 1,
      "text": "ABSTRACT"
    },
    {
      "bounding_box": [
        {
          "x": 591,
          "y": 1023
        },
        {
          "x": 1963,
          "y": 1023
        },
        {
          "x": 1963,
          "y": 1808
        },
        {
          "x": 591,
          "y": 1808
        }
      ],
      "category": "paragraph",
      "html": "<p id='6' style='font-size:18px'>This paper presents a simple MLP-like architecture, CycleMLP, which is a versa-<br>tile backbone for visual recognition and dense predictions. As compared to mod-<br>ern MLP architectures, e.g. , MLP-Mixer (Tolstikhin et al., 2021), ResMLP (Tou-<br>vron et al., 2021a), and gMLP (Liu et al., 2021a), whose architectures are cor-<br>related to image size and thus are infeasible in object detection and segmen-<br>tation, CycleMLP has two advantages compared to modern approaches. (1) It<br>can cope with various image sizes. (2) It achieves linear computational com-<br>plexity to image size by using local windows. In contrast, previous MLPs have<br>O(N2) computations due to fully spatial connections. We build a family of<br>models which surpass existing MLPs and even state-of-the-art Transformer-based<br>models, e.g. Swin Transformer (Liu et al., 2021b), while using fewer parame-<br>ters and FLOPs. We expand the MLP-like models' applicability, making them<br>a versatile backbone for dense prediction tasks. CycleMLP achieves competi-<br>tive results on object detection, instance segmentation, and semantic segmenta-<br>tion. In particular, CycleMLP-Tiny outperforms Swin-Tiny by 1.3% mloU on<br>ADE20K dataset with fewer FLOPs. Moreover, CycleMLP also shows excel-<br>lent zero-shot robustness on ImageNet-C dataset. Code is available at https :</p>",
      "id": 6,
      "page": 1,
      "text": "This paper presents a simple MLP-like architecture, CycleMLP, which is a versa-\ntile backbone for visual recognition and dense predictions. As compared to mod-\nern MLP architectures, e.g. , MLP-Mixer (Tolstikhin et al., 2021), ResMLP (Tou-\nvron et al., 2021a), and gMLP (Liu et al., 2021a), whose architectures are cor-\nrelated to image size and thus are infeasible in object detection and segmen-\ntation, CycleMLP has two advantages compared to modern approaches. (1) It\ncan cope with various image sizes. (2) It achieves linear computational com-\nplexity to image size by using local windows. In contrast, previous MLPs have\nO(N2) computations due to fully spatial connections. We build a family of\nmodels which surpass existing MLPs and even state-of-the-art Transformer-based\nmodels, e.g. Swin Transformer (Liu et al., 2021b), while using fewer parame-\nters and FLOPs. We expand the MLP-like models' applicability, making them\na versatile backbone for dense prediction tasks. CycleMLP achieves competi-\ntive results on object detection, instance segmentation, and semantic segmenta-\ntion. In particular, CycleMLP-Tiny outperforms Swin-Tiny by 1.3% mloU on\nADE20K dataset with fewer FLOPs. Moreover, CycleMLP also shows excel-\nlent zero-shot robustness on ImageNet-C dataset. Code is available at https :"
    },
    {
      "bounding_box": [
        {
          "x": 596,
          "y": 1805
        },
        {
          "x": 1410,
          "y": 1805
        },
        {
          "x": 1410,
          "y": 1847
        },
        {
          "x": 596,
          "y": 1847
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='7' style='font-size:18px'>/ / github · com/ ShoufaChen/CycleMLP.</p>",
      "id": 7,
      "page": 1,
      "text": "/ / github · com/ ShoufaChen/CycleMLP."
    },
    {
      "bounding_box": [
        {
          "x": 449,
          "y": 1934
        },
        {
          "x": 862,
          "y": 1934
        },
        {
          "x": 862,
          "y": 1984
        },
        {
          "x": 449,
          "y": 1984
        }
      ],
      "category": "paragraph",
      "html": "<p id='8' style='font-size:20px'>1 INTRODUCTION</p>",
      "id": 8,
      "page": 1,
      "text": "1 INTRODUCTION"
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 2034
        },
        {
          "x": 2109,
          "y": 2034
        },
        {
          "x": 2109,
          "y": 2406
        },
        {
          "x": 443,
          "y": 2406
        }
      ],
      "category": "paragraph",
      "html": "<p id='9' style='font-size:20px'>Vision models in computer vision have been long dominated by convolutional neural net-<br>works (CNNs) (Krizhevsky et al., 2012; He et al., 2016). Recently, inspired by the successes in<br>Natural Language Processing (NLP) field, Transformers (Vaswani et al., 2017) are adopted into the<br>computer vision community. Built with self-attention layers, multi-layer perceptrons (MLPs), and<br>skip connections, Transformers make numerous breakthroughs on visual tasks (Dosovitskiy et al.,<br>2020; Liu et al., 2021b). More recently, (Tolstikhin et al., 2021; Liu et al., 2021a) have validated that<br>building models solely on MLPs and skip connections without the self-attention layers can achieve<br>surprisingly promising results on ImageNet (Deng et al., 2009) classification.</p>",
      "id": 9,
      "page": 1,
      "text": "Vision models in computer vision have been long dominated by convolutional neural net-\nworks (CNNs) (Krizhevsky et al., 2012; He et al., 2016). Recently, inspired by the successes in\nNatural Language Processing (NLP) field, Transformers (Vaswani et al., 2017) are adopted into the\ncomputer vision community. Built with self-attention layers, multi-layer perceptrons (MLPs), and\nskip connections, Transformers make numerous breakthroughs on visual tasks (Dosovitskiy et al.,\n2020; Liu et al., 2021b). More recently, (Tolstikhin et al., 2021; Liu et al., 2021a) have validated that\nbuilding models solely on MLPs and skip connections without the self-attention layers can achieve\nsurprisingly promising results on ImageNet (Deng et al., 2009) classification."
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 2425
        },
        {
          "x": 1084,
          "y": 2425
        },
        {
          "x": 1084,
          "y": 2954
        },
        {
          "x": 443,
          "y": 2954
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='10' style='font-size:18px'>Despite promising results on visual<br>recognition tasks, these MLP-like<br>models can not be used in dense pre-<br>diction tasks (e.g., object detection<br>and semantic segmentation) due to<br>the three challenges: (1) Current<br>models are composed of blocks with<br>non-hierarchical architectures, which<br>make the model infeasible to provide<br>pyramid and high-resolution feature<br>representations. (2) Current models<br>shown in 1b. The FC</p>",
      "id": 10,
      "page": 1,
      "text": "Despite promising results on visual\nrecognition tasks, these MLP-like\nmodels can not be used in dense pre-\ndiction tasks (e.g., object detection\nand semantic segmentation) due to\nthe three challenges: (1) Current\nmodels are composed of blocks with\nnon-hierarchical architectures, which\nmake the model infeasible to provide\npyramid and high-resolution feature\nrepresentations. (2) Current models\nshown in 1b. The FC"
    },
    {
      "bounding_box": [
        {
          "x": 1105,
          "y": 2467
        },
        {
          "x": 2097,
          "y": 2467
        },
        {
          "x": 2097,
          "y": 2767
        },
        {
          "x": 1105,
          "y": 2767
        }
      ],
      "category": "table",
      "html": "<br><table id='11' style='font-size:18px'><tr><td>FC</td><td>Stepsize</td><td>O(HW)</td><td>Scale Variable</td><td>ImgNet Top-1</td><td>COCO AP</td><td>ADE20K mIoU</td></tr><tr><td>Channel</td><td>1</td><td>HW</td><td>V</td><td>79.4</td><td>35.0</td><td>36.3</td></tr><tr><td>Spatial</td><td>-</td><td>H2W2</td><td>X</td><td>80.9</td><td>X</td><td>X</td></tr><tr><td>Cycle</td><td>7</td><td>HW</td><td>V</td><td>81.6</td><td>41.7</td><td>42.4</td></tr></table>",
      "id": 11,
      "page": 1,
      "text": "FC Stepsize O(HW) Scale Variable ImgNet Top-1 COCO AP ADE20K mIoU\n Channel 1 HW V 79.4 35.0 36.3\n Spatial - H2W2 X 80.9 X X\n Cycle 7 HW V 81.6 41.7"
    },
    {
      "bounding_box": [
        {
          "x": 1165,
          "y": 2803
        },
        {
          "x": 2039,
          "y": 2803
        },
        {
          "x": 2039,
          "y": 2851
        },
        {
          "x": 1165,
          "y": 2851
        }
      ],
      "category": "caption",
      "html": "<caption id='12' style='font-size:18px'>Table 1: Comparison of three types of FC operators.</caption>",
      "id": 12,
      "page": 1,
      "text": "Table 1: Comparison of three types of FC operators."
    },
    {
      "bounding_box": [
        {
          "x": 444,
          "y": 2884
        },
        {
          "x": 2108,
          "y": 2884
        },
        {
          "x": 2108,
          "y": 2977
        },
        {
          "x": 444,
          "y": 2977
        }
      ],
      "category": "paragraph",
      "html": "<p id='13' style='font-size:18px'>cannot deal with flexible input scales due to the Spatial FC<br>as Figure spatial is configured by an image-size related weight1 . Thus, this</p>",
      "id": 13,
      "page": 1,
      "text": "cannot deal with flexible input scales due to the Spatial FC\nas Figure spatial is configured by an image-size related weight1 . Thus, this"
    },
    {
      "bounding_box": [
        {
          "x": 502,
          "y": 3008
        },
        {
          "x": 1213,
          "y": 3008
        },
        {
          "x": 1213,
          "y": 3051
        },
        {
          "x": 502,
          "y": 3051
        }
      ],
      "category": "paragraph",
      "html": "<p id='14' style='font-size:16px'>1We omit bias here for discussion convenience.</p>",
      "id": 14,
      "page": 1,
      "text": "1We omit bias here for discussion convenience."
    },
    {
      "bounding_box": [
        {
          "x": 60,
          "y": 865
        },
        {
          "x": 146,
          "y": 865
        },
        {
          "x": 146,
          "y": 2330
        },
        {
          "x": 60,
          "y": 2330
        }
      ],
      "category": "footer",
      "html": "<br><footer id='15' style='font-size:14px'>2022<br>Mar<br>18<br>[cs.CV]<br>arXiv:2107.10224y4</footer>",
      "id": 15,
      "page": 1,
      "text": "2022\nMar\n18\n[cs.CV]\narXiv:2107.10224y4"
    },
    {
      "bounding_box": [
        {
          "x": 1261,
          "y": 3133
        },
        {
          "x": 1289,
          "y": 3133
        },
        {
          "x": 1289,
          "y": 3172
        },
        {
          "x": 1261,
          "y": 3172
        }
      ],
      "category": "footer",
      "html": "<footer id='16' style='font-size:14px'>1</footer>",
      "id": 16,
      "page": 1,
      "text": "1"
    },
    {
      "bounding_box": [
        {
          "x": 445,
          "y": 111
        },
        {
          "x": 1224,
          "y": 111
        },
        {
          "x": 1224,
          "y": 157
        },
        {
          "x": 445,
          "y": 157
        }
      ],
      "category": "header",
      "html": "<header id='17' style='font-size:16px'>Published as a conference paper at ICLR 2022</header>",
      "id": 17,
      "page": 2,
      "text": "Published as a conference paper at ICLR 2022"
    },
    {
      "bounding_box": [
        {
          "x": 449,
          "y": 326
        },
        {
          "x": 2036,
          "y": 326
        },
        {
          "x": 2036,
          "y": 937
        },
        {
          "x": 449,
          "y": 937
        }
      ],
      "category": "figure",
      "html": "<figure><img id='18' style='font-size:14px' alt=\"Stepsize\n★ 8i(0)=-1\n8i(1)=0\nHxW 8i(2)=1\n(a) Channel FC\nC ●\n·\n·\n8i(7)=-1\nHxW\n(b) Spatial FC 8i(8)=0\nC 8i(9)=1\n(d) SH = 3,Sw =1 (e) SH = H,Sw = 1 (f) SH = 1,Sw = 1\nHxW (c) Cycle FC\" data-coord=\"top-left:(449,326); bottom-right:(2036,937)\" /></figure>",
      "id": 18,
      "page": 2,
      "text": "Stepsize\n★ 8i(0)=-1\n8i(1)=0\nHxW 8i(2)=1\n(a) Channel FC\nC ●\n·\n·\n8i(7)=-1\nHxW\n(b) Spatial FC 8i(8)=0\nC 8i(9)=1\n(d) SH = 3,Sw =1 (e) SH = H,Sw = 1 (f) SH = 1,Sw = 1\nHxW (c) Cycle FC"
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 965
        },
        {
          "x": 2108,
          "y": 965
        },
        {
          "x": 2108,
          "y": 1431
        },
        {
          "x": 442,
          "y": 1431
        }
      ],
      "category": "paragraph",
      "html": "<p id='19' style='font-size:20px'>Figure 1: (a)-(c): motivation of Cycle Fully-Connected Layer (Cycle FC) compared to Channel<br>FC and Spatial FC. (a) Channel FC aggregates features in the channel dimension with spatial size<br>'1' It can handle various input scales but cannot learn spatial context. (b) Spatial FC (Tolstikhin<br>et al., 2021 ; Touvron et al., 2021a; Liu et al., 2021a) has a global receptive field in the spatial<br>dimension. However, its parameter size is fixed and it has quadratic computational complexity to<br>image scale. (c) Our proposed Cycle Fully-Connected Layer (Cycle FC) has linear complexity<br>the same as channel FC and a larger receptive field than Channel FC. (d)-(f): Three examples of<br>different stepsizes. Orange blocks denote the sampled positions. ★ denotes the output position.<br>For simplicity, we omit batch dimension and set the feature's width to 1 here for example. Several<br>more general cases can be found in Figure 7 (Appendix G). Best viewed in color.</p>",
      "id": 19,
      "page": 2,
      "text": "Figure 1: (a)-(c): motivation of Cycle Fully-Connected Layer (Cycle FC) compared to Channel\nFC and Spatial FC. (a) Channel FC aggregates features in the channel dimension with spatial size\n'1' It can handle various input scales but cannot learn spatial context. (b) Spatial FC (Tolstikhin\net al., 2021 ; Touvron et al., 2021a; Liu et al., 2021a) has a global receptive field in the spatial\ndimension. However, its parameter size is fixed and it has quadratic computational complexity to\nimage scale. (c) Our proposed Cycle Fully-Connected Layer (Cycle FC) has linear complexity\nthe same as channel FC and a larger receptive field than Channel FC. (d)-(f): Three examples of\ndifferent stepsizes. Orange blocks denote the sampled positions. ★ denotes the output position.\nFor simplicity, we omit batch dimension and set the feature's width to 1 here for example. Several\nmore general cases can be found in Figure 7 (Appendix G). Best viewed in color."
    },
    {
      "bounding_box": [
        {
          "x": 441,
          "y": 1627
        },
        {
          "x": 2106,
          "y": 1627
        },
        {
          "x": 2106,
          "y": 1904
        },
        {
          "x": 441,
          "y": 1904
        }
      ],
      "category": "paragraph",
      "html": "<p id='20' style='font-size:20px'>structure typically requires the input image with a fixed scale during both the training and infer-<br>ence procedure. It contradicts the requirements of dense prediction tasks, which usually adopt a<br>multi-scale training strategy (Carion et al., 2020) and different input resolutions in training and in-<br>ference stages (Lin et al., 2014; Cordts et al., 2016). (3) The computational and memory costs of<br>the current MLP models are quadratic to input image sizes for dense prediction tasks (e.g., COCO<br>benchmark (Lin et al., 2014)).</p>",
      "id": 20,
      "page": 2,
      "text": "structure typically requires the input image with a fixed scale during both the training and infer-\nence procedure. It contradicts the requirements of dense prediction tasks, which usually adopt a\nmulti-scale training strategy (Carion et al., 2020) and different input resolutions in training and in-\nference stages (Lin et al., 2014; Cordts et al., 2016). (3) The computational and memory costs of\nthe current MLP models are quadratic to input image sizes for dense prediction tasks (e.g., COCO\nbenchmark (Lin et al., 2014))."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 1925
        },
        {
          "x": 2106,
          "y": 1925
        },
        {
          "x": 2106,
          "y": 2112
        },
        {
          "x": 442,
          "y": 2112
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='21' style='font-size:18px'>To address the first challenge, we construct a hierarchical architecture to generate pyramid features.<br>For the second and third issues, we propose a novel variant of fully connected layer, named as Cycle<br>Fully-Connected Layer (Cycle FC), as illustrated in Figure 1c. The Cycle FC is capable of dealing<br>with various image scales and has linear computational complexity to image size.</p>",
      "id": 21,
      "page": 2,
      "text": "To address the first challenge, we construct a hierarchical architecture to generate pyramid features.\nFor the second and third issues, we propose a novel variant of fully connected layer, named as Cycle\nFully-Connected Layer (Cycle FC), as illustrated in Figure 1c. The Cycle FC is capable of dealing\nwith various image scales and has linear computational complexity to image size."
    },
    {
      "bounding_box": [
        {
          "x": 441,
          "y": 2132
        },
        {
          "x": 2108,
          "y": 2132
        },
        {
          "x": 2108,
          "y": 2365
        },
        {
          "x": 441,
          "y": 2365
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='22' style='font-size:20px'>Our Cycle FC is inspired by Channel FC layer illustrated in Figure 1a, which is designed for channel<br>information communication (Lin et al., 2013; Szegedy et al., 2015; He et al., 2016; Howard et al.,<br>2017). The main merit of Channel FC lies in that it can deal with flexible image sizes since it is<br>configured by image-size agnostic weight of Cin and Cout· However, the Channel FC is infeasible to<br>aggregate spatial context information due to its limited receptive field.</p>",
      "id": 22,
      "page": 2,
      "text": "Our Cycle FC is inspired by Channel FC layer illustrated in Figure 1a, which is designed for channel\ninformation communication (Lin et al., 2013; Szegedy et al., 2015; He et al., 2016; Howard et al.,\n2017). The main merit of Channel FC lies in that it can deal with flexible image sizes since it is\nconfigured by image-size agnostic weight of Cin and Cout· However, the Channel FC is infeasible to\naggregate spatial context information due to its limited receptive field."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 2384
        },
        {
          "x": 2106,
          "y": 2384
        },
        {
          "x": 2106,
          "y": 2710
        },
        {
          "x": 442,
          "y": 2710
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='23' style='font-size:22px'>Our Cycle FC is designed to enjoy Channel FC's merit of taking input with arbitrary resolution and<br>linear computational complexity while enlarging its receptive field for context aggregation. Specif-<br>ically, Cycle FC samples points in a cyclical style along the channel dimension (Figure 1c). In this<br>way, Cycle FC has the same complexity (both the number of parameters and FLOPs) as channel<br>FC while increasing the receptive field simultaneously. To this end, we adopt Cycle FC to replace<br>the Spatial FC for spatial context aggregation (i.e., token mixing) and build a family of MLP-like<br>models for both recognition and dense prediction tasks.</p>",
      "id": 23,
      "page": 2,
      "text": "Our Cycle FC is designed to enjoy Channel FC's merit of taking input with arbitrary resolution and\nlinear computational complexity while enlarging its receptive field for context aggregation. Specif-\nically, Cycle FC samples points in a cyclical style along the channel dimension (Figure 1c). In this\nway, Cycle FC has the same complexity (both the number of parameters and FLOPs) as channel\nFC while increasing the receptive field simultaneously. To this end, we adopt Cycle FC to replace\nthe Spatial FC for spatial context aggregation (i.e., token mixing) and build a family of MLP-like\nmodels for both recognition and dense prediction tasks."
    },
    {
      "bounding_box": [
        {
          "x": 441,
          "y": 2730
        },
        {
          "x": 2107,
          "y": 2730
        },
        {
          "x": 2107,
          "y": 3054
        },
        {
          "x": 441,
          "y": 3054
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='24' style='font-size:18px'>The contributions of this paper are as follows: (1) We propose a new MLP-like operator, Cycle FC,<br>which is computational friendly to cope with flexible input resolutions. (2) We take the first attempt<br>to build a family of hierarchical MLP-like architectures (CycleMLP) based on Cycle FC operator for<br>dense prediction tasks. (3) Extensive experiments on various tasks (e.g., ImageNet classification,<br>COCO object instance detection, and segmentation, and ADE20K semantic segmentation) demon-<br>strate that CycleMLP outperforms existing MLP-like models and is comparable to and sometimes<br>better than CNNs and Transformers on dense predictions.</p>",
      "id": 24,
      "page": 2,
      "text": "The contributions of this paper are as follows: (1) We propose a new MLP-like operator, Cycle FC,\nwhich is computational friendly to cope with flexible input resolutions. (2) We take the first attempt\nto build a family of hierarchical MLP-like architectures (CycleMLP) based on Cycle FC operator for\ndense prediction tasks. (3) Extensive experiments on various tasks (e.g., ImageNet classification,\nCOCO object instance detection, and segmentation, and ADE20K semantic segmentation) demon-\nstrate that CycleMLP outperforms existing MLP-like models and is comparable to and sometimes\nbetter than CNNs and Transformers on dense predictions."
    },
    {
      "bounding_box": [
        {
          "x": 1259,
          "y": 3133
        },
        {
          "x": 1289,
          "y": 3133
        },
        {
          "x": 1289,
          "y": 3172
        },
        {
          "x": 1259,
          "y": 3172
        }
      ],
      "category": "footer",
      "html": "<footer id='25' style='font-size:14px'>2</footer>",
      "id": 25,
      "page": 2,
      "text": "2"
    },
    {
      "bounding_box": [
        {
          "x": 444,
          "y": 111
        },
        {
          "x": 1224,
          "y": 111
        },
        {
          "x": 1224,
          "y": 157
        },
        {
          "x": 444,
          "y": 157
        }
      ],
      "category": "header",
      "html": "<header id='26' style='font-size:14px'>Published as a conference paper at ICLR 2022</header>",
      "id": 26,
      "page": 3,
      "text": "Published as a conference paper at ICLR 2022"
    },
    {
      "bounding_box": [
        {
          "x": 497,
          "y": 341
        },
        {
          "x": 1321,
          "y": 341
        },
        {
          "x": 1321,
          "y": 859
        },
        {
          "x": 497,
          "y": 859
        }
      ],
      "category": "figure",
      "html": "<figure><img id='27' style='font-size:14px' alt=\"82\n(%)\nAcc\n80\nTop-1\n78\nDeiT\nMixer\nImageNet\n76 S2MLP\nResMLP\n74 gMLP\nViP\nCycleMLP (ours)\n72\n5 10 15 20 25\nModel capacity (MAdds: G)\" data-coord=\"top-left:(497,341); bottom-right:(1321,859)\" /></figure>",
      "id": 27,
      "page": 3,
      "text": "82\n(%)\nAcc\n80\nTop-1\n78\nDeiT\nMixer\nImageNet\n76 S2MLP\nResMLP\n74 gMLP\nViP\nCycleMLP (ours)\n72\n5 10 15 20 25\nModel capacity (MAdds: G)"
    },
    {
      "bounding_box": [
        {
          "x": 1381,
          "y": 345
        },
        {
          "x": 2061,
          "y": 345
        },
        {
          "x": 2061,
          "y": 758
        },
        {
          "x": 1381,
          "y": 758
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='28' style='font-size:18px'>Figure 2: ImageNet accuracy v.s.<br>model capacity. All models are<br>trained on ImageNet-1K (Deng et al.,<br>2009) without extra data. CycleMLP<br>surpasses existing MLP-like models<br>such as MLP-Mixer (Tolstikhin et al.,<br>2021), ResMLP (Touvron et al., 2021a),<br>gMLP (Liu et al., 2021a), S2-MLP (Yu<br>et al., 2021) and ViP (Hou et al., 2021).</p>",
      "id": 28,
      "page": 3,
      "text": "Figure 2: ImageNet accuracy v.s.\nmodel capacity. All models are\ntrained on ImageNet-1K (Deng et al.,\n2009) without extra data. CycleMLP\nsurpasses existing MLP-like models\nsuch as MLP-Mixer (Tolstikhin et al.,\n2021), ResMLP (Touvron et al., 2021a),\ngMLP (Liu et al., 2021a), S2-MLP (Yu\net al., 2021) and ViP (Hou et al., 2021)."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 892
        },
        {
          "x": 2109,
          "y": 892
        },
        {
          "x": 2109,
          "y": 1220
        },
        {
          "x": 442,
          "y": 1220
        }
      ],
      "category": "caption",
      "html": "<caption id='29' style='font-size:16px'>Related Work. Convolution Neural Networks (CNNs) has dominated the visual backbones for<br>several years (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014; He et al., 2016). (Dosovitskiy<br>et al., 2020) introduced the first pure Transformer-based (Vaswani et al., 2017) model into computer<br>vision and achieved promising performance, especially pre-trained on the large scale JFT dataset.<br>Recently, some works (Tolstikhin et al., 2021; Touvron et al., 2021a; Liu et al., 2021a) removed<br>the attention in Transformer and proposed pure MLP-based models. Please see Appendix A for a<br>comprehensive review of the literature on the visual backbones.</caption>",
      "id": 29,
      "page": 3,
      "text": "Related Work. Convolution Neural Networks (CNNs) has dominated the visual backbones for\nseveral years (Krizhevsky et al., 2012; Simonyan & Zisserman, 2014; He et al., 2016). (Dosovitskiy\net al., 2020) introduced the first pure Transformer-based (Vaswani et al., 2017) model into computer\nvision and achieved promising performance, especially pre-trained on the large scale JFT dataset.\nRecently, some works (Tolstikhin et al., 2021; Touvron et al., 2021a; Liu et al., 2021a) removed\nthe attention in Transformer and proposed pure MLP-based models. Please see Appendix A for a\ncomprehensive review of the literature on the visual backbones."
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 1286
        },
        {
          "x": 726,
          "y": 1286
        },
        {
          "x": 726,
          "y": 1339
        },
        {
          "x": 443,
          "y": 1339
        }
      ],
      "category": "paragraph",
      "html": "<p id='30' style='font-size:22px'>2 METHOD</p>",
      "id": 30,
      "page": 3,
      "text": "2 METHOD"
    },
    {
      "bounding_box": [
        {
          "x": 440,
          "y": 1390
        },
        {
          "x": 2107,
          "y": 1390
        },
        {
          "x": 2107,
          "y": 1669
        },
        {
          "x": 440,
          "y": 1669
        }
      ],
      "category": "paragraph",
      "html": "<p id='31' style='font-size:18px'>In this section, we introduce CycleMLP models for vision tasks including recognition and dense<br>predictions. To begin with, in Sec. 2.1 we formulate our proposed novel operator, Cycle FC, which<br>serves as a basic component for building CycleMLP models. Then we compare Cycle FC with<br>Channel FC and multi-head attention adopted in recent Transformer-based models (Dosovitskiy<br>et al., 2020; Touvron et al., 2020; Liu et al., 2021b) in Sec. 2.2. Finally, we present the detailed<br>configurations of CycleMLP models in Sec. 2.3.</p>",
      "id": 31,
      "page": 3,
      "text": "In this section, we introduce CycleMLP models for vision tasks including recognition and dense\npredictions. To begin with, in Sec. 2.1 we formulate our proposed novel operator, Cycle FC, which\nserves as a basic component for building CycleMLP models. Then we compare Cycle FC with\nChannel FC and multi-head attention adopted in recent Transformer-based models (Dosovitskiy\net al., 2020; Touvron et al., 2020; Liu et al., 2021b) in Sec. 2.2. Finally, we present the detailed\nconfigurations of CycleMLP models in Sec. 2.3."
    },
    {
      "bounding_box": [
        {
          "x": 445,
          "y": 1726
        },
        {
          "x": 1175,
          "y": 1726
        },
        {
          "x": 1175,
          "y": 1773
        },
        {
          "x": 445,
          "y": 1773
        }
      ],
      "category": "paragraph",
      "html": "<p id='32' style='font-size:18px'>2.1 CYCLE FULLY-CONNECTED LAYER</p>",
      "id": 32,
      "page": 3,
      "text": "2.1 CYCLE FULLY-CONNECTED LAYER"
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 1810
        },
        {
          "x": 2107,
          "y": 1810
        },
        {
          "x": 2107,
          "y": 1998
        },
        {
          "x": 442,
          "y": 1998
        }
      ],
      "category": "paragraph",
      "html": "<p id='33' style='font-size:18px'>Notation. We denote an input feature map as X E RHxWxCin, where H, W denote the height<br>and width of the image and Cin is the number of feature channels. We use subscripts to index the<br>feature map. For example, Xi,j,c is the value of cth channel at the spatial position (i,j) and Xi,j,:<br>are values of all channels at the spatial (i,j).</p>",
      "id": 33,
      "page": 3,
      "text": "Notation. We denote an input feature map as X E RHxWxCin, where H, W denote the height\nand width of the image and Cin is the number of feature channels. We use subscripts to index the\nfeature map. For example, Xi,j,c is the value of cth channel at the spatial position (i,j) and Xi,j,:\nare values of all channels at the spatial (i,j)."
    },
    {
      "bounding_box": [
        {
          "x": 441,
          "y": 2018
        },
        {
          "x": 2108,
          "y": 2018
        },
        {
          "x": 2108,
          "y": 2296
        },
        {
          "x": 441,
          "y": 2296
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='34' style='font-size:20px'>The motivation behind Cycle FC is to enlarge receptive field of MLP-like models to cope with<br>downstream dense prediction tasks while maintaining the computational efficiency. As illustrated<br>in Figure 1a, Channel FC applies weighting matrix on X along the channel dimension on fixed<br>position (i,j). However, Cycle FC introduces a receptive field of (SH, Sw), where SH and Sw are<br>stepsize along with the height and width dimension respectively (illustrated in Figure 1 (d)). The<br>basic Cycle FC operator can be formulated as below:</p>",
      "id": 34,
      "page": 3,
      "text": "The motivation behind Cycle FC is to enlarge receptive field of MLP-like models to cope with\ndownstream dense prediction tasks while maintaining the computational efficiency. As illustrated\nin Figure 1a, Channel FC applies weighting matrix on X along the channel dimension on fixed\nposition (i,j). However, Cycle FC introduces a receptive field of (SH, Sw), where SH and Sw are\nstepsize along with the height and width dimension respectively (illustrated in Figure 1 (d)). The\nbasic Cycle FC operator can be formulated as below:"
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 2532
        },
        {
          "x": 2106,
          "y": 2532
        },
        {
          "x": 2106,
          "y": 2636
        },
        {
          "x": 442,
          "y": 2636
        }
      ],
      "category": "paragraph",
      "html": "<p id='35' style='font-size:16px'>where Wmlp E RCinxCout and b E RCout are parameters of Cycle FC. di(c) and dj(c) are the spatial<br>offset of the two axis on the cth channel, which are defined as below:</p>",
      "id": 35,
      "page": 3,
      "text": "where Wmlp E RCinxCout and b E RCout are parameters of Cycle FC. di(c) and dj(c) are the spatial\noffset of the two axis on the cth channel, which are defined as below:"
    },
    {
      "bounding_box": [
        {
          "x": 441,
          "y": 2820
        },
        {
          "x": 2108,
          "y": 2820
        },
        {
          "x": 2108,
          "y": 3055
        },
        {
          "x": 441,
          "y": 3055
        }
      ],
      "category": "paragraph",
      "html": "<p id='36' style='font-size:16px'>Examples. We provide several examples (Figure 1 (d)-(f)) to illustrate the stepsize. For the sake of<br>visualization convenience, we set the tensor's W = 1. Thus, these three examples naturally all have<br>Sw = 1. Figure 1 (d) illustrates the offsets along two axis when SH = 3, that is dj(c) ≡ 0 and<br>di(c) = {-1, 0, 1, -1, 0, 1, · · · } when c = 0, 1, 2, · · · 8. Figure 1 (e) shows that when SH = H,<br>,<br>Cycle FC has a global receptive field. Figure 1 (f) shows that when SH = 1, there will be no offset</p>",
      "id": 36,
      "page": 3,
      "text": "Examples. We provide several examples (Figure 1 (d)-(f)) to illustrate the stepsize. For the sake of\nvisualization convenience, we set the tensor's W = 1. Thus, these three examples naturally all have\nSw = 1. Figure 1 (d) illustrates the offsets along two axis when SH = 3, that is dj(c) ≡ 0 and\ndi(c) = {-1, 0, 1, -1, 0, 1, · · · } when c = 0, 1, 2, · · · 8. Figure 1 (e) shows that when SH = H,\n,\nCycle FC has a global receptive field. Figure 1 (f) shows that when SH = 1, there will be no offset"
    },
    {
      "bounding_box": [
        {
          "x": 1261,
          "y": 3134
        },
        {
          "x": 1288,
          "y": 3134
        },
        {
          "x": 1288,
          "y": 3170
        },
        {
          "x": 1261,
          "y": 3170
        }
      ],
      "category": "footer",
      "html": "<footer id='37' style='font-size:16px'>3</footer>",
      "id": 37,
      "page": 3,
      "text": "3"
    },
    {
      "bounding_box": [
        {
          "x": 444,
          "y": 110
        },
        {
          "x": 1224,
          "y": 110
        },
        {
          "x": 1224,
          "y": 157
        },
        {
          "x": 444,
          "y": 157
        }
      ],
      "category": "header",
      "html": "<header id='38' style='font-size:14px'>Published as a conference paper at ICLR 2022</header>",
      "id": 38,
      "page": 4,
      "text": "Published as a conference paper at ICLR 2022"
    },
    {
      "bounding_box": [
        {
          "x": 441,
          "y": 347
        },
        {
          "x": 2105,
          "y": 347
        },
        {
          "x": 2105,
          "y": 440
        },
        {
          "x": 441,
          "y": 440
        }
      ],
      "category": "paragraph",
      "html": "<p id='39' style='font-size:20px'>along either axis and thus Cycle FC degrades to Channel FC (Figure 1 (a)). We also provide a more<br>general case where W ≠ 1 and SH = 3, Sw = 3 in Figure 7 (Appendix).</p>",
      "id": 39,
      "page": 4,
      "text": "along either axis and thus Cycle FC degrades to Channel FC (Figure 1 (a)). We also provide a more\ngeneral case where W ≠ 1 and SH = 3, Sw = 3 in Figure 7 (Appendix)."
    },
    {
      "bounding_box": [
        {
          "x": 441,
          "y": 461
        },
        {
          "x": 2107,
          "y": 461
        },
        {
          "x": 2107,
          "y": 831
        },
        {
          "x": 441,
          "y": 831
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='40' style='font-size:20px'>The offsets di(c) and dj(c) enlarge the receptive field of Cycle FC as compared to Channel FC<br>(Figure 1a), which applies weights solely on the same spatial position for all channels. The larger<br>receptive field in return brings improvements on dense prediction tasks like semantic segmentation<br>and object detection as shown in Table 1. Meanwhile, Cycle FC still maintains computational effi-<br>ciency and flexibility on input resolution. Both the FLOPs and the number of parameters are linear<br>to the spatial scale which are exactly the same as those of Channel FC. In contrast, although Spatial<br>FC has a global receptive field over the whole spatial space, its computational cost is quadratic to<br>the image scale. Besides, it fails to handle inputs with different resolutions.</p>",
      "id": 40,
      "page": 4,
      "text": "The offsets di(c) and dj(c) enlarge the receptive field of Cycle FC as compared to Channel FC\n(Figure 1a), which applies weights solely on the same spatial position for all channels. The larger\nreceptive field in return brings improvements on dense prediction tasks like semantic segmentation\nand object detection as shown in Table 1. Meanwhile, Cycle FC still maintains computational effi-\nciency and flexibility on input resolution. Both the FLOPs and the number of parameters are linear\nto the spatial scale which are exactly the same as those of Channel FC. In contrast, although Spatial\nFC has a global receptive field over the whole spatial space, its computational cost is quadratic to\nthe image scale. Besides, it fails to handle inputs with different resolutions."
    },
    {
      "bounding_box": [
        {
          "x": 444,
          "y": 890
        },
        {
          "x": 1991,
          "y": 890
        },
        {
          "x": 1991,
          "y": 939
        },
        {
          "x": 444,
          "y": 939
        }
      ],
      "category": "paragraph",
      "html": "<p id='41' style='font-size:16px'>2.2 COMPARISON BETWEEN MULTI-HEAD SELF-ATTENTION (MHSA) AND CYCLE FC</p>",
      "id": 41,
      "page": 4,
      "text": "2.2 COMPARISON BETWEEN MULTI-HEAD SELF-ATTENTION (MHSA) AND CYCLE FC"
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 980
        },
        {
          "x": 2106,
          "y": 980
        },
        {
          "x": 2106,
          "y": 1120
        },
        {
          "x": 442,
          "y": 1120
        }
      ],
      "category": "paragraph",
      "html": "<p id='42' style='font-size:20px'>Inspired by Cordonnier et al. (2020), when re-parametried properly, a multi-head self-attention layer<br>with Nh heads can be formulated as below, which is similar to a convolution with kernel size VNh x<br>VNh. (Please refer to Appendix C for detailed derivation)</p>",
      "id": 42,
      "page": 4,
      "text": "Inspired by Cordonnier et al. (2020), when re-parametried properly, a multi-head self-attention layer\nwith Nh heads can be formulated as below, which is similar to a convolution with kernel size VNh x\nVNh. (Please refer to Appendix C for detailed derivation)"
    },
    {
      "bounding_box": [
        {
          "x": 441,
          "y": 1343
        },
        {
          "x": 2109,
          "y": 1343
        },
        {
          "x": 2109,
          "y": 1585
        },
        {
          "x": 441,
          "y": 1585
        }
      ],
      "category": "paragraph",
      "html": "<p id='43' style='font-size:16px'>where Wmhsa,h RCin xCout is the parameter matrix for hth head in MHSA. b E RCout is the<br>E<br>bias vector. {△i(h),△j(h)} = {(0, 0), (1,0), (-1, 0), · · · } contains all possible positional shift in<br>convolution with kernel size VNh x VNh. Further, we stack all Wmhsa,h<br>together and reshape it<br>into Wmhsa RKxKxCin xCout Then a relationship between Wmlp and Wmhsa<br>E<br>can be formulated<br>as follow.</p>",
      "id": 43,
      "page": 4,
      "text": "where Wmhsa,h RCin xCout is the parameter matrix for hth head in MHSA. b E RCout is the\nE\nbias vector. {△i(h),△j(h)} = {(0, 0), (1,0), (-1, 0), · · · } contains all possible positional shift in\nconvolution with kernel size VNh x VNh. Further, we stack all Wmhsa,h\ntogether and reshape it\ninto Wmhsa RKxKxCin xCout Then a relationship between Wmlp and Wmhsa\nE\ncan be formulated\nas follow."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 1770
        },
        {
          "x": 2106,
          "y": 1770
        },
        {
          "x": 2106,
          "y": 2102
        },
        {
          "x": 442,
          "y": 2102
        }
      ],
      "category": "paragraph",
      "html": "<p id='44' style='font-size:20px'>equation 4 shows that only the weights of Wmhsa on spatial shift (ji(c) +1,8j(c) + 1) are taken into<br>account in Wmlp This indicates that Cycle FC introduce an inductive bias that the weighting matrix<br>in MHSA should be sparse. Thus Cycle FC inherits the large receptive field introduced in MHSA.<br>The receptive field in Cycle FC is enlarged to (SH, Sw), which enables Cycle FC to tackle with<br>downstream dense prediction tasks better. Meanwhile, with the sparsity inductive bias, Cycle FC<br>maintains computational efficiency in MLP-based methods as compared to convolution and multi-<br>head self-attention. The parameter size in Cycle FCis Cin x Cout while Wmhsa RKxKxCin xCout<br>E</p>",
      "id": 44,
      "page": 4,
      "text": "equation 4 shows that only the weights of Wmhsa on spatial shift (ji(c) +1,8j(c) + 1) are taken into\naccount in Wmlp This indicates that Cycle FC introduce an inductive bias that the weighting matrix\nin MHSA should be sparse. Thus Cycle FC inherits the large receptive field introduced in MHSA.\nThe receptive field in Cycle FC is enlarged to (SH, Sw), which enables Cycle FC to tackle with\ndownstream dense prediction tasks better. Meanwhile, with the sparsity inductive bias, Cycle FC\nmaintains computational efficiency in MLP-based methods as compared to convolution and multi-\nhead self-attention. The parameter size in Cycle FCis Cin x Cout while Wmhsa RKxKxCin xCout\nE"
    },
    {
      "bounding_box": [
        {
          "x": 444,
          "y": 2160
        },
        {
          "x": 1022,
          "y": 2160
        },
        {
          "x": 1022,
          "y": 2210
        },
        {
          "x": 444,
          "y": 2210
        }
      ],
      "category": "paragraph",
      "html": "<p id='45' style='font-size:16px'>2.3 OVERALL ARCHITECTURE</p>",
      "id": 45,
      "page": 4,
      "text": "2.3 OVERALL ARCHITECTURE"
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 2247
        },
        {
          "x": 2108,
          "y": 2247
        },
        {
          "x": 2108,
          "y": 2529
        },
        {
          "x": 442,
          "y": 2529
        }
      ],
      "category": "paragraph",
      "html": "<p id='46' style='font-size:18px'>Patch Embedding. Given the raw input image with the size of H x W x 3, our model first splits it<br>into patches by a patch embedding module (Dosovitskiy et al., 2020). Each patch is then treated as a<br>\"token\" · Specifically, we follow (Fan et al., 2021; Wang et al., 2021a) to adopt an overlapping patch<br>embedding module with the window size 7 and stride 4. These raw patches are further projected<br>to a higher dimension (denoted as C) by a linear embedding layer. Therefore, the overall patch<br>embedding module generates the features with the shape of H W<br>x x C.<br>4 4</p>",
      "id": 46,
      "page": 4,
      "text": "Patch Embedding. Given the raw input image with the size of H x W x 3, our model first splits it\ninto patches by a patch embedding module (Dosovitskiy et al., 2020). Each patch is then treated as a\n\"token\" · Specifically, we follow (Fan et al., 2021; Wang et al., 2021a) to adopt an overlapping patch\nembedding module with the window size 7 and stride 4. These raw patches are further projected\nto a higher dimension (denoted as C) by a linear embedding layer. Therefore, the overall patch\nembedding module generates the features with the shape of H W\nx x C.\n4 4"
    },
    {
      "bounding_box": [
        {
          "x": 441,
          "y": 2547
        },
        {
          "x": 2107,
          "y": 2547
        },
        {
          "x": 2107,
          "y": 3054
        },
        {
          "x": 441,
          "y": 3054
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='47' style='font-size:20px'>CycleMLP Block. Then, we sequentially apply several Cycle FC Bloc blocks. Comparing with the<br>previous MLP blocks (Tolstikhin et al., 2021; Touvron et al., 2021a; Liu et al., 2021a) visualized<br>in Figure 5 (Appendix), the key difference of Cycle FC block is that it utilizes our proposed Cycle<br>Fully-Connected Layer (Cycle FC) for spatial projection and advances the models in context aggre-<br>gation and information communication. Specifically, the Cycle FC block consists of three parallel<br>Cycle FCs, which have stepsizes SH x Sw of 1 x 7, 7 x 1, and 1 x 1. This design is inspired by<br>the factorization of convolution (Szegedy et al., 2016) and criss-cross attention (Huang et al., 2019).<br>Then, there is a channel-MLP with two linear layers and a GELU (Hendrycks & Gimpel, 2016)<br>non-linearity in between. A LayerNorm (LN) (Ba et al., 2016) layer is applied before both parallel<br>Cycle FC layers and channel-MLP modules. A residual connection (He et al., 2016) is applied after<br>each module.</p>",
      "id": 47,
      "page": 4,
      "text": "CycleMLP Block. Then, we sequentially apply several Cycle FC Bloc blocks. Comparing with the\nprevious MLP blocks (Tolstikhin et al., 2021; Touvron et al., 2021a; Liu et al., 2021a) visualized\nin Figure 5 (Appendix), the key difference of Cycle FC block is that it utilizes our proposed Cycle\nFully-Connected Layer (Cycle FC) for spatial projection and advances the models in context aggre-\ngation and information communication. Specifically, the Cycle FC block consists of three parallel\nCycle FCs, which have stepsizes SH x Sw of 1 x 7, 7 x 1, and 1 x 1. This design is inspired by\nthe factorization of convolution (Szegedy et al., 2016) and criss-cross attention (Huang et al., 2019).\nThen, there is a channel-MLP with two linear layers and a GELU (Hendrycks & Gimpel, 2016)\nnon-linearity in between. A LayerNorm (LN) (Ba et al., 2016) layer is applied before both parallel\nCycle FC layers and channel-MLP modules. A residual connection (He et al., 2016) is applied after\neach module."
    },
    {
      "bounding_box": [
        {
          "x": 1258,
          "y": 3134
        },
        {
          "x": 1289,
          "y": 3134
        },
        {
          "x": 1289,
          "y": 3170
        },
        {
          "x": 1258,
          "y": 3170
        }
      ],
      "category": "footer",
      "html": "<footer id='48' style='font-size:16px'>4</footer>",
      "id": 48,
      "page": 4,
      "text": "4"
    },
    {
      "bounding_box": [
        {
          "x": 445,
          "y": 111
        },
        {
          "x": 1224,
          "y": 111
        },
        {
          "x": 1224,
          "y": 157
        },
        {
          "x": 445,
          "y": 157
        }
      ],
      "category": "header",
      "html": "<header id='49' style='font-size:16px'>Published as a conference paper at ICLR 2022</header>",
      "id": 49,
      "page": 5,
      "text": "Published as a conference paper at ICLR 2022"
    },
    {
      "bounding_box": [
        {
          "x": 461,
          "y": 334
        },
        {
          "x": 1125,
          "y": 334
        },
        {
          "x": 1125,
          "y": 1528
        },
        {
          "x": 461,
          "y": 1528
        }
      ],
      "category": "table",
      "html": "<table id='50' style='font-size:16px'><tr><td>Model</td><td>Param</td><td>FLOPs</td><td>Top-1</td></tr><tr><td>EAMLP-14</td><td>30M</td><td>-</td><td>78.9</td></tr><tr><td>EAMLP-19</td><td>55M</td><td>-</td><td>79.4</td></tr><tr><td>Mixer-B/16</td><td>59M</td><td>12.7G</td><td>76.4</td></tr><tr><td>Mixer-B/16+</td><td>59M</td><td>12.7G</td><td>77.3</td></tr><tr><td>ResMLP-S12</td><td>15M</td><td>3.0G</td><td>76.6</td></tr><tr><td>ResMLP-S24</td><td>30M</td><td>6.0G</td><td>79.4</td></tr><tr><td>ResMLP-B24</td><td>116M</td><td>23.0G</td><td>81.0</td></tr><tr><td>gMLP-Ti</td><td>6M</td><td>1.4G</td><td>72.3</td></tr><tr><td>gMLP-S</td><td>20M</td><td>4.5G</td><td>79.6</td></tr><tr><td>gMLP-B</td><td>73M</td><td>15.8G</td><td>81.6</td></tr><tr><td>S2-MLP-wide</td><td>71M</td><td>14.0G</td><td>80.0</td></tr><tr><td>S2-MLP-deep</td><td>51M</td><td>10.5G</td><td>80.7</td></tr><tr><td>ViP-Small/7</td><td>25M</td><td>6.9G</td><td>81.5</td></tr><tr><td>ViP-Medium/7</td><td>55M</td><td>16.3G</td><td>82.7</td></tr><tr><td>ViP-Large/7</td><td>88M</td><td>24.4G</td><td>83.2</td></tr><tr><td>AS-MLP-T</td><td>28M</td><td>4.4G</td><td>81.3</td></tr><tr><td>AS-MLP-S</td><td>50M</td><td>8.5G</td><td>83.1</td></tr><tr><td>AS-MLP-B</td><td>88M</td><td>15.2G</td><td>83.3</td></tr><tr><td>CycleMLP-B1</td><td>15M</td><td>2.1G</td><td>79.1</td></tr><tr><td>CycleMLP-B2</td><td>27M</td><td>3.9G</td><td>81.6</td></tr><tr><td>CycleMLP-B3</td><td>38M</td><td>6.9G</td><td>82.6</td></tr><tr><td>CycleMLP-B4</td><td>52M</td><td>10.1G</td><td>83.0</td></tr><tr><td>CycleMLP-B5</td><td>76M</td><td>12.3G</td><td>83.1</td></tr><tr><td>CycleMLP-T</td><td>28M</td><td>4.4G</td><td>81.3</td></tr><tr><td>CycleMLP-S</td><td>50M</td><td>8.5G</td><td>82.9</td></tr><tr><td>CycleMLP-B</td><td>88M</td><td>15.2G</td><td>83.4</td></tr></table>",
      "id": 50,
      "page": 5,
      "text": "Model Param FLOPs Top-1\n EAMLP-14 30M - 78.9\n EAMLP-19 55M - 79.4\n Mixer-B/16 59M 12.7G 76.4\n Mixer-B/16+ 59M 12.7G 77.3\n ResMLP-S12 15M 3.0G 76.6\n ResMLP-S24 30M 6.0G 79.4\n ResMLP-B24 116M 23.0G 81.0\n gMLP-Ti 6M 1.4G 72.3\n gMLP-S 20M 4.5G 79.6\n gMLP-B 73M 15.8G 81.6\n S2-MLP-wide 71M 14.0G 80.0\n S2-MLP-deep 51M 10.5G 80.7\n ViP-Small/7 25M 6.9G 81.5\n ViP-Medium/7 55M 16.3G 82.7\n ViP-Large/7 88M 24.4G 83.2\n AS-MLP-T 28M 4.4G 81.3\n AS-MLP-S 50M 8.5G 83.1\n AS-MLP-B 88M 15.2G 83.3\n CycleMLP-B1 15M 2.1G 79.1\n CycleMLP-B2 27M 3.9G 81.6\n CycleMLP-B3 38M 6.9G 82.6\n CycleMLP-B4 52M 10.1G 83.0\n CycleMLP-B5 76M 12.3G 83.1\n CycleMLP-T 28M 4.4G 81.3\n CycleMLP-S 50M 8.5G 82.9\n CycleMLP-B 88M 15.2G"
    },
    {
      "bounding_box": [
        {
          "x": 1200,
          "y": 333
        },
        {
          "x": 2066,
          "y": 333
        },
        {
          "x": 2066,
          "y": 1527
        },
        {
          "x": 1200,
          "y": 1527
        }
      ],
      "category": "table",
      "html": "<br><table id='51' style='font-size:14px'><tr><td>Model</td><td>Family</td><td>Scale</td><td>Param</td><td>FLOPs</td><td>Top-1</td></tr><tr><td>ResNet18</td><td>CNN</td><td>2242</td><td>12M</td><td>1.8G</td><td>69.8</td></tr><tr><td>EffNet-B3</td><td>CNN</td><td>3002</td><td>12M</td><td>1.8G</td><td>81.6</td></tr><tr><td>GFNet-H-Ti</td><td>FFT</td><td>2242</td><td>15M</td><td>2.0G</td><td>80.1</td></tr><tr><td>CycleMLP-B1</td><td>MLP</td><td>2242</td><td>15M</td><td>2.1G</td><td>78.9</td></tr><tr><td>ResNet50</td><td>CNN</td><td>2242</td><td>26M</td><td>4.1G</td><td>78.5</td></tr><tr><td>DeiT-S</td><td>Trans</td><td>2242</td><td>22M</td><td>4.6G</td><td>79.8</td></tr><tr><td>BoT-S1-50</td><td>Hybrid</td><td>2242</td><td>21M</td><td>4.3G</td><td>79.1</td></tr><tr><td>PVT-S</td><td>Trans</td><td>2242</td><td>25M</td><td>3.8G</td><td>79.8</td></tr><tr><td>Swin-T</td><td>Trans</td><td>2242</td><td>29M</td><td>4.5G</td><td>81.3</td></tr><tr><td>GFNet-H-S</td><td>FFT</td><td>2242</td><td>32M</td><td>4.5G</td><td>81.5</td></tr><tr><td>CycleMLP-B2</td><td>MLP</td><td>2242</td><td>27M</td><td>3.9G</td><td>81.6</td></tr><tr><td>ResNet101</td><td>CNN</td><td>2242</td><td>45M</td><td>7.9G</td><td>79.8</td></tr><tr><td>RegNetY-8G</td><td>CNN</td><td>2242</td><td>39M</td><td>8.0G</td><td>81.7</td></tr><tr><td>BoT-S1-59</td><td>Hybrid</td><td>2242</td><td>34M</td><td>7.3G</td><td>81.7</td></tr><tr><td>PVT-M</td><td>Trans</td><td>2242</td><td>44M</td><td>6.7G</td><td>81.2</td></tr><tr><td>CycleMLP-B3</td><td>MLP</td><td>2242</td><td>38M</td><td>6.9G</td><td>82.4</td></tr><tr><td>GFNet-H-B</td><td>FFT</td><td>2242</td><td>54M</td><td>8.4G</td><td>82.9</td></tr><tr><td>Swin-S</td><td>Trans</td><td>2242</td><td>50M</td><td>8.7G</td><td>83.0</td></tr><tr><td>PVT-L</td><td>Trans</td><td>2242</td><td>61M</td><td>9.8G</td><td>81.7</td></tr><tr><td>CycleMLP-S</td><td>MLP</td><td>2242</td><td>50M</td><td>8.5G</td><td>82.9</td></tr><tr><td>ViT-B/16</td><td>Trans</td><td>3842</td><td>86M</td><td>55.4G</td><td>77.9</td></tr><tr><td>DeiT-B</td><td>Trans</td><td>2242</td><td>86M</td><td>17.5G</td><td>81.8</td></tr><tr><td>DeiT-B</td><td>Trans</td><td>3842</td><td>86M</td><td>55.4G</td><td>83.1</td></tr><tr><td>Swin-B</td><td>Trans</td><td>2242</td><td>88M</td><td>15.4G</td><td>83.3</td></tr><tr><td>CycleMLP-B</td><td>MLP</td><td>2242</td><td>88M</td><td>15.2G</td><td>83.4</td></tr></table>",
      "id": 51,
      "page": 5,
      "text": "Model Family Scale Param FLOPs Top-1\n ResNet18 CNN 2242 12M 1.8G 69.8\n EffNet-B3 CNN 3002 12M 1.8G 81.6\n GFNet-H-Ti FFT 2242 15M 2.0G 80.1\n CycleMLP-B1 MLP 2242 15M 2.1G 78.9\n ResNet50 CNN 2242 26M 4.1G 78.5\n DeiT-S Trans 2242 22M 4.6G 79.8\n BoT-S1-50 Hybrid 2242 21M 4.3G 79.1\n PVT-S Trans 2242 25M 3.8G 79.8\n Swin-T Trans 2242 29M 4.5G 81.3\n GFNet-H-S FFT 2242 32M 4.5G 81.5\n CycleMLP-B2 MLP 2242 27M 3.9G 81.6\n ResNet101 CNN 2242 45M 7.9G 79.8\n RegNetY-8G CNN 2242 39M 8.0G 81.7\n BoT-S1-59 Hybrid 2242 34M 7.3G 81.7\n PVT-M Trans 2242 44M 6.7G 81.2\n CycleMLP-B3 MLP 2242 38M 6.9G 82.4\n GFNet-H-B FFT 2242 54M 8.4G 82.9\n Swin-S Trans 2242 50M 8.7G 83.0\n PVT-L Trans 2242 61M 9.8G 81.7\n CycleMLP-S MLP 2242 50M 8.5G 82.9\n ViT-B/16 Trans 3842 86M 55.4G 77.9\n DeiT-B Trans 2242 86M 17.5G 81.8\n DeiT-B Trans 3842 86M 55.4G 83.1\n Swin-B Trans 2242 88M 15.4G 83.3\n CycleMLP-B MLP 2242 88M 15.2G"
    },
    {
      "bounding_box": [
        {
          "x": 1207,
          "y": 1566
        },
        {
          "x": 2090,
          "y": 1566
        },
        {
          "x": 2090,
          "y": 1658
        },
        {
          "x": 1207,
          "y": 1658
        }
      ],
      "category": "caption",
      "html": "<caption id='52' style='font-size:18px'>Table 3: Comparison with SOTA models on<br>ImageNet-1K without extra data.</caption>",
      "id": 52,
      "page": 5,
      "text": "Table 3: Comparison with SOTA models on\nImageNet-1K without extra data."
    },
    {
      "bounding_box": [
        {
          "x": 478,
          "y": 1565
        },
        {
          "x": 1143,
          "y": 1565
        },
        {
          "x": 1143,
          "y": 1659
        },
        {
          "x": 478,
          "y": 1659
        }
      ],
      "category": "caption",
      "html": "<br><caption id='53' style='font-size:18px'>Table 2: ImageNet-1K classification<br>for MLP-like models.</caption>",
      "id": 53,
      "page": 5,
      "text": "Table 2: ImageNet-1K classification\nfor MLP-like models."
    },
    {
      "bounding_box": [
        {
          "x": 441,
          "y": 1708
        },
        {
          "x": 2107,
          "y": 1708
        },
        {
          "x": 2107,
          "y": 2077
        },
        {
          "x": 441,
          "y": 2077
        }
      ],
      "category": "paragraph",
      "html": "<p id='54' style='font-size:18px'>Stage. The blocks with the same architecture are stacked to form one Stage (He et al., 2016). The<br>number of tokens (feature scale) is maintained within each stage. At each stage transition, the<br>channel capacity of the processed tokens is expanded while the number of tokens is reduced. This<br>strategy effectively reduces the spatial resolution complexity. Overall, each of our model variants<br>W C4. These stage<br>has four stages, and the output feature at the last stage has a shape of H32 x x<br>32<br>settings are widely utilized in both CNN (Simonyan & Zisserman, 2014; He et al., 2016) and Trans-<br>former (Wang et al., 2021b; Liu et al., 2021b) models. Therefore, CycleMLP can conveniently serve<br>as a general-purpose visual backbone and a generic replacement for existing backbones.</p>",
      "id": 54,
      "page": 5,
      "text": "Stage. The blocks with the same architecture are stacked to form one Stage (He et al., 2016). The\nnumber of tokens (feature scale) is maintained within each stage. At each stage transition, the\nchannel capacity of the processed tokens is expanded while the number of tokens is reduced. This\nstrategy effectively reduces the spatial resolution complexity. Overall, each of our model variants\nW C4. These stage\nhas four stages, and the output feature at the last stage has a shape of H32 x x\n32\nsettings are widely utilized in both CNN (Simonyan & Zisserman, 2014; He et al., 2016) and Trans-\nformer (Wang et al., 2021b; Liu et al., 2021b) models. Therefore, CycleMLP can conveniently serve\nas a general-purpose visual backbone and a generic replacement for existing backbones."
    },
    {
      "bounding_box": [
        {
          "x": 441,
          "y": 2097
        },
        {
          "x": 2107,
          "y": 2097
        },
        {
          "x": 2107,
          "y": 2651
        },
        {
          "x": 441,
          "y": 2651
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='55' style='font-size:18px'>Model Variants. The design principle of the model's macro structure is mainly inspired by the phi-<br>losophy of hierarchical Transformer (Wang et al., 2021b; Liu et al., 2021b) models, which reduce<br>the number of tokens at the transition layers as the network goes deeper and increase the channel<br>dimension. In this way, we can build a hierarchical architecture that is critical for dense predic-<br>tion tasks (Lin et al., 2014; Zhou et al., 2017). Specifically, we build two model ZOOS following<br>two widely used Transformer architectures, PVT (Wang et al., 2021b) and Swin (Liu et al., 2021b).<br>Models in PVT-style are named from CycleMLP-B1 to CycleMLP-B5 and in Swin-Style are named<br>as CycleMLP-T, -S, and -B, which represent models in tiny, small, and base sizes. These models are<br>built by adapting several architecture-related hyper-parameters, including Si, Ci, Ei, and Li which<br>represent the stride of the transition, the token channel dimension, the number of blocks, and the<br>expansion ratio respectively at Stage i. Detailed configurations of these models are in Table 11 (Ap-<br>pendix).</p>",
      "id": 55,
      "page": 5,
      "text": "Model Variants. The design principle of the model's macro structure is mainly inspired by the phi-\nlosophy of hierarchical Transformer (Wang et al., 2021b; Liu et al., 2021b) models, which reduce\nthe number of tokens at the transition layers as the network goes deeper and increase the channel\ndimension. In this way, we can build a hierarchical architecture that is critical for dense predic-\ntion tasks (Lin et al., 2014; Zhou et al., 2017). Specifically, we build two model ZOOS following\ntwo widely used Transformer architectures, PVT (Wang et al., 2021b) and Swin (Liu et al., 2021b).\nModels in PVT-style are named from CycleMLP-B1 to CycleMLP-B5 and in Swin-Style are named\nas CycleMLP-T, -S, and -B, which represent models in tiny, small, and base sizes. These models are\nbuilt by adapting several architecture-related hyper-parameters, including Si, Ci, Ei, and Li which\nrepresent the stride of the transition, the token channel dimension, the number of blocks, and the\nexpansion ratio respectively at Stage i. Detailed configurations of these models are in Table 11 (Ap-\npendix)."
    },
    {
      "bounding_box": [
        {
          "x": 447,
          "y": 2748
        },
        {
          "x": 838,
          "y": 2748
        },
        {
          "x": 838,
          "y": 2798
        },
        {
          "x": 447,
          "y": 2798
        }
      ],
      "category": "paragraph",
      "html": "<p id='56' style='font-size:22px'>3 EXPERIMENTS</p>",
      "id": 56,
      "page": 5,
      "text": "3 EXPERIMENTS"
    },
    {
      "bounding_box": [
        {
          "x": 440,
          "y": 2868
        },
        {
          "x": 2106,
          "y": 2868
        },
        {
          "x": 2106,
          "y": 3053
        },
        {
          "x": 440,
          "y": 3053
        }
      ],
      "category": "paragraph",
      "html": "<p id='57' style='font-size:20px'>In this section, we first examine CycleMLP by conducting experiments on ImageNet-1K (Deng<br>et al., 2009) image classification. Then, we present a bunch of baseline models achieved by Cy-<br>cleMLP in dense prediction tasks, i.e., COCO (Lin et al., 2014) object detection, instance segmen-<br>tation, and ADE20K (Zhou et al., 2017) semantic segmentation.</p>",
      "id": 57,
      "page": 5,
      "text": "In this section, we first examine CycleMLP by conducting experiments on ImageNet-1K (Deng\net al., 2009) image classification. Then, we present a bunch of baseline models achieved by Cy-\ncleMLP in dense prediction tasks, i.e., COCO (Lin et al., 2014) object detection, instance segmen-\ntation, and ADE20K (Zhou et al., 2017) semantic segmentation."
    },
    {
      "bounding_box": [
        {
          "x": 1260,
          "y": 3134
        },
        {
          "x": 1289,
          "y": 3134
        },
        {
          "x": 1289,
          "y": 3171
        },
        {
          "x": 1260,
          "y": 3171
        }
      ],
      "category": "footer",
      "html": "<footer id='58' style='font-size:16px'>5</footer>",
      "id": 58,
      "page": 5,
      "text": "5"
    },
    {
      "bounding_box": [
        {
          "x": 445,
          "y": 111
        },
        {
          "x": 1224,
          "y": 111
        },
        {
          "x": 1224,
          "y": 156
        },
        {
          "x": 445,
          "y": 156
        }
      ],
      "category": "header",
      "html": "<header id='59' style='font-size:16px'>Published as a conference paper at ICLR 2022</header>",
      "id": 59,
      "page": 6,
      "text": "Published as a conference paper at ICLR 2022"
    },
    {
      "bounding_box": [
        {
          "x": 477,
          "y": 335
        },
        {
          "x": 1246,
          "y": 335
        },
        {
          "x": 1246,
          "y": 682
        },
        {
          "x": 477,
          "y": 682
        }
      ],
      "category": "table",
      "html": "<table id='60' style='font-size:14px'><tr><td colspan=\"2\">1 x 7 7 x 1 1 x 1</td><td>Params</td><td>FLOPs</td><td>Top-1 Acc</td></tr><tr><td rowspan=\"3\">V V</td><td rowspan=\"3\"></td><td rowspan=\"3\">24.5M</td><td rowspan=\"3\">3.6G</td><td>80.5</td></tr><tr><td>80.4</td></tr><tr><td>81.3</td></tr><tr><td rowspan=\"2\">VV</td><td rowspan=\"2\">VV</td><td rowspan=\"2\">26.8M</td><td rowspan=\"2\">3.9G</td><td>80.6</td></tr><tr><td>80.5</td></tr><tr><td>V</td><td></td><td>26.8M</td><td>3.9G</td><td>81.6</td></tr></table>",
      "id": 60,
      "page": 6,
      "text": "1 x 7 7 x 1 1 x 1 Params FLOPs Top-1 Acc\n V V  24.5M 3.6G 80.5\n 80.4\n 81.3\n VV VV 26.8M 3.9G 80.6\n 80.5\n V  26.8M 3.9G"
    },
    {
      "bounding_box": [
        {
          "x": 477,
          "y": 715
        },
        {
          "x": 1283,
          "y": 715
        },
        {
          "x": 1283,
          "y": 902
        },
        {
          "x": 477,
          "y": 902
        }
      ],
      "category": "caption",
      "html": "<caption id='61' style='font-size:18px'>Table 4: Ablation on three parallel branches.<br>We adopt CycleMLP-B2 variant for this abla-<br>tion study. Double check marks (VV) denote<br>two same branches.</caption>",
      "id": 61,
      "page": 6,
      "text": "Table 4: Ablation on three parallel branches.\nWe adopt CycleMLP-B2 variant for this abla-\ntion study. Double check marks (VV) denote\ntwo same branches."
    },
    {
      "bounding_box": [
        {
          "x": 1325,
          "y": 335
        },
        {
          "x": 2010,
          "y": 335
        },
        {
          "x": 2010,
          "y": 631
        },
        {
          "x": 1325,
          "y": 631
        }
      ],
      "category": "table",
      "html": "<br><table id='62' style='font-size:20px'><tr><td>Stepsize</td><td>ImgNet Top-1</td><td>ADE20K mIoU</td></tr><tr><td>3</td><td>81.6</td><td>42.4</td></tr><tr><td>5</td><td>81.6 (+0.0)</td><td>43.2 (+0.8)</td></tr><tr><td>7</td><td>81.6 (+0.0)</td><td>43.9 (+1.5)</td></tr><tr><td>9</td><td>81.5 (-0.1)</td><td>43.2 (+0.8)</td></tr></table>",
      "id": 62,
      "page": 6,
      "text": "Stepsize ImgNet Top-1 ADE20K mIoU\n 3 81.6 42.4\n 5 81.6 (+0.0) 43.2 (+0.8)\n 7 81.6 (+0.0) 43.9 (+1.5)\n 9 81.5 (-0.1)"
    },
    {
      "bounding_box": [
        {
          "x": 1321,
          "y": 664
        },
        {
          "x": 2062,
          "y": 664
        },
        {
          "x": 2062,
          "y": 895
        },
        {
          "x": 1321,
          "y": 895
        }
      ],
      "category": "caption",
      "html": "<caption id='63' style='font-size:22px'>Table 5: Stepsize ablation: CycleMLP<br>achieves the highest mIoU on ADE20K<br>when stepsize is 7. However, the stepsize<br>has negligible influence on the ImageNet<br>classification.</caption>",
      "id": 63,
      "page": 6,
      "text": "Table 5: Stepsize ablation: CycleMLP\nachieves the highest mIoU on ADE20K\nwhen stepsize is 7. However, the stepsize\nhas negligible influence on the ImageNet\nclassification."
    },
    {
      "bounding_box": [
        {
          "x": 447,
          "y": 980
        },
        {
          "x": 1124,
          "y": 980
        },
        {
          "x": 1124,
          "y": 1025
        },
        {
          "x": 447,
          "y": 1025
        }
      ],
      "category": "paragraph",
      "html": "<p id='64' style='font-size:18px'>3.1 IMAGENET-1K CLASSIFICATION</p>",
      "id": 64,
      "page": 6,
      "text": "3.1 IMAGENET-1K CLASSIFICATION"
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 1076
        },
        {
          "x": 2106,
          "y": 1076
        },
        {
          "x": 2106,
          "y": 1214
        },
        {
          "x": 442,
          "y": 1214
        }
      ],
      "category": "paragraph",
      "html": "<p id='65' style='font-size:20px'>The experimental settings for ImageNet classification are mostly from DeiT (Touvron et al., 2020),<br>Swin (Liu et al., 2021b). The detailed experimental settings for ImageNet classification can be found<br>in Appendix E.1.</p>",
      "id": 65,
      "page": 6,
      "text": "The experimental settings for ImageNet classification are mostly from DeiT (Touvron et al., 2020),\nSwin (Liu et al., 2021b). The detailed experimental settings for ImageNet classification can be found\nin Appendix E.1."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 1237
        },
        {
          "x": 2108,
          "y": 1237
        },
        {
          "x": 2108,
          "y": 1698
        },
        {
          "x": 442,
          "y": 1698
        }
      ],
      "category": "paragraph",
      "html": "<p id='66' style='font-size:20px'>Comparison with MLP-like Models. We first compare CycleMLP with existing MLP-like models<br>and the results are summarized in Table 2 and Figure 2. The accuracy-FLOPs tradeoff of CycleMLP<br>consistently outperforms existing MLP-like models (Tolstikhin et al., 2021; Touvron et al., 2021a;<br>Liu et al., 2021a; Guo et al., 2021 ; Yu et al., 2021; Hou et al., 2021) under a wide range of FLOPs,<br>which we attribute to the effectiveness of our Cycle FC. Specifically, compared with one of the<br>pioneering MLP work, i.e., gMLP (Liu et al., 2021a), CycleMLP-B2 achieves the same top-1 accu-<br>racy (81.6%) as gMLP-B while reducing more than 3x FLOPs (3.9G for CycleMLP-B2 and 15.8G<br>for gMLP-B). Furthermore, compared with existing SOTA MLP-like model, i.e., ViP (Hou et al.,<br>2021), our model CycleMLP-B utilizes less FLOPs (15.2G) than ViP-Large/7 (24.4G, the largest<br>one of ViP family) while achiving higher top-1 accuracy.</p>",
      "id": 66,
      "page": 6,
      "text": "Comparison with MLP-like Models. We first compare CycleMLP with existing MLP-like models\nand the results are summarized in Table 2 and Figure 2. The accuracy-FLOPs tradeoff of CycleMLP\nconsistently outperforms existing MLP-like models (Tolstikhin et al., 2021; Touvron et al., 2021a;\nLiu et al., 2021a; Guo et al., 2021 ; Yu et al., 2021; Hou et al., 2021) under a wide range of FLOPs,\nwhich we attribute to the effectiveness of our Cycle FC. Specifically, compared with one of the\npioneering MLP work, i.e., gMLP (Liu et al., 2021a), CycleMLP-B2 achieves the same top-1 accu-\nracy (81.6%) as gMLP-B while reducing more than 3x FLOPs (3.9G for CycleMLP-B2 and 15.8G\nfor gMLP-B). Furthermore, compared with existing SOTA MLP-like model, i.e., ViP (Hou et al.,\n2021), our model CycleMLP-B utilizes less FLOPs (15.2G) than ViP-Large/7 (24.4G, the largest\none of ViP family) while achiving higher top-1 accuracy."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 1720
        },
        {
          "x": 2107,
          "y": 1720
        },
        {
          "x": 2107,
          "y": 1905
        },
        {
          "x": 442,
          "y": 1905
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='67' style='font-size:20px'>It is noted that all previous MLP-like models listed in Table 2 do not conduct experiments on dense<br>prediction tasks due to the incapability of dealing with variable input scales, which is discussed in<br>Sec. 1. However, CycleMLP solved this issue by adopting Cycle FC. The experimental results on<br>dense prediction tasks are presented in Sec. 3.3 and Sec. 3.4.</p>",
      "id": 67,
      "page": 6,
      "text": "It is noted that all previous MLP-like models listed in Table 2 do not conduct experiments on dense\nprediction tasks due to the incapability of dealing with variable input scales, which is discussed in\nSec. 1. However, CycleMLP solved this issue by adopting Cycle FC. The experimental results on\ndense prediction tasks are presented in Sec. 3.3 and Sec. 3.4."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 1928
        },
        {
          "x": 2108,
          "y": 1928
        },
        {
          "x": 2108,
          "y": 2388
        },
        {
          "x": 442,
          "y": 2388
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='68' style='font-size:20px'>Comparison with SOTA Models. Table 3 further compares CycleMLP with previous state-of-<br>the-art CNN, Transformer and Hybrid architectures. It is interesting to see that CycleMLP models<br>achieve comparable performance to Swin Transformer (Liu et al., 2021b), which is the state-of-<br>the-art Transformer-based model. Specifically, CycleMLP-B achieves slightly better top-1 accu-<br>racy (83.4%) than Swin-B (83.3%) with similar parameters and FLOPs. GFNet (Rao et al., 2021)<br>utilizes the fast Fourier transform (FFT) (Cooley & Tukey, 1965) to learn spatial information and<br>achieves similar performance as CycleMLP on ImageNet-1K classification. However, the architec-<br>ture of GFNet is correlated with the input resolution, and extra operation (parameter interpolation)<br>is required when input scale changes, which may hurt the performance of dense predictions. We<br>will thoroughly compare CycleMLP with GFNet in Sec. 3.4 on ADE20K.</p>",
      "id": 68,
      "page": 6,
      "text": "Comparison with SOTA Models. Table 3 further compares CycleMLP with previous state-of-\nthe-art CNN, Transformer and Hybrid architectures. It is interesting to see that CycleMLP models\nachieve comparable performance to Swin Transformer (Liu et al., 2021b), which is the state-of-\nthe-art Transformer-based model. Specifically, CycleMLP-B achieves slightly better top-1 accu-\nracy (83.4%) than Swin-B (83.3%) with similar parameters and FLOPs. GFNet (Rao et al., 2021)\nutilizes the fast Fourier transform (FFT) (Cooley & Tukey, 1965) to learn spatial information and\nachieves similar performance as CycleMLP on ImageNet-1K classification. However, the architec-\nture of GFNet is correlated with the input resolution, and extra operation (parameter interpolation)\nis required when input scale changes, which may hurt the performance of dense predictions. We\nwill thoroughly compare CycleMLP with GFNet in Sec. 3.4 on ADE20K."
    },
    {
      "bounding_box": [
        {
          "x": 446,
          "y": 2471
        },
        {
          "x": 875,
          "y": 2471
        },
        {
          "x": 875,
          "y": 2518
        },
        {
          "x": 446,
          "y": 2518
        }
      ],
      "category": "paragraph",
      "html": "<p id='69' style='font-size:18px'>3.2 ABLATION STUDY</p>",
      "id": 69,
      "page": 6,
      "text": "3.2 ABLATION STUDY"
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 2570
        },
        {
          "x": 2103,
          "y": 2570
        },
        {
          "x": 2103,
          "y": 2663
        },
        {
          "x": 443,
          "y": 2663
        }
      ],
      "category": "paragraph",
      "html": "<p id='70' style='font-size:18px'>In this subsection, we conduct extensive ablation studies to analyze each component of our design.<br>Unless otherwise stated, We adopt CycleMLP-B2 instantiation in this subsection.</p>",
      "id": 70,
      "page": 6,
      "text": "In this subsection, we conduct extensive ablation studies to analyze each component of our design.\nUnless otherwise stated, We adopt CycleMLP-B2 instantiation in this subsection."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 2684
        },
        {
          "x": 2107,
          "y": 2684
        },
        {
          "x": 2107,
          "y": 3055
        },
        {
          "x": 442,
          "y": 3055
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='71' style='font-size:20px'>Cycle Fully-Connected Layer. To demonstrate the advantage of the Cycle FC, we compare<br>CycleMLP-B2 with two other baseline models equipped with channel FC and Spatial FC as spatial<br>context aggregation operators, respectively. The differences of these operators are visualized in Fig-<br>ure 1, and the comparison results are shown in Table 1. CycleMLP-B2 outperforms the counterparts<br>built on both Spatial and Channel FC for ImageNet classification, COCO object detection, instance<br>segmentation, and ADE20K semantic segmentation. The results validate that Cycle FC is capable<br>of serving as a general-purpose, plug-and-play operator for spatial information communication and<br>context aggregation.</p>",
      "id": 71,
      "page": 6,
      "text": "Cycle Fully-Connected Layer. To demonstrate the advantage of the Cycle FC, we compare\nCycleMLP-B2 with two other baseline models equipped with channel FC and Spatial FC as spatial\ncontext aggregation operators, respectively. The differences of these operators are visualized in Fig-\nure 1, and the comparison results are shown in Table 1. CycleMLP-B2 outperforms the counterparts\nbuilt on both Spatial and Channel FC for ImageNet classification, COCO object detection, instance\nsegmentation, and ADE20K semantic segmentation. The results validate that Cycle FC is capable\nof serving as a general-purpose, plug-and-play operator for spatial information communication and\ncontext aggregation."
    },
    {
      "bounding_box": [
        {
          "x": 1260,
          "y": 3136
        },
        {
          "x": 1289,
          "y": 3136
        },
        {
          "x": 1289,
          "y": 3170
        },
        {
          "x": 1260,
          "y": 3170
        }
      ],
      "category": "footer",
      "html": "<footer id='72' style='font-size:14px'>6</footer>",
      "id": 72,
      "page": 6,
      "text": "6"
    },
    {
      "bounding_box": [
        {
          "x": 444,
          "y": 110
        },
        {
          "x": 1225,
          "y": 110
        },
        {
          "x": 1225,
          "y": 157
        },
        {
          "x": 444,
          "y": 157
        }
      ],
      "category": "header",
      "html": "<header id='73' style='font-size:18px'>Published as a conference paper at ICLR 2022</header>",
      "id": 73,
      "page": 7,
      "text": "Published as a conference paper at ICLR 2022"
    },
    {
      "bounding_box": [
        {
          "x": 1470,
          "y": 338
        },
        {
          "x": 2079,
          "y": 338
        },
        {
          "x": 2079,
          "y": 388
        },
        {
          "x": 1470,
          "y": 388
        }
      ],
      "category": "paragraph",
      "html": "<p id='74' style='font-size:22px'>Figure 3: Resolution adaptability.</p>",
      "id": 74,
      "page": 7,
      "text": "Figure 3: Resolution adaptability."
    },
    {
      "bounding_box": [
        {
          "x": 1474,
          "y": 385
        },
        {
          "x": 2077,
          "y": 385
        },
        {
          "x": 2077,
          "y": 420
        },
        {
          "x": 1474,
          "y": 420
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='75' style='font-size:16px'>All models are trained on 224x224 and</p>",
      "id": 75,
      "page": 7,
      "text": "All models are trained on 224x224 and"
    },
    {
      "bounding_box": [
        {
          "x": 1472,
          "y": 424
        },
        {
          "x": 2077,
          "y": 424
        },
        {
          "x": 2077,
          "y": 463
        },
        {
          "x": 1472,
          "y": 463
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='76' style='font-size:16px'>evaluated on various resolutions with-</p>",
      "id": 76,
      "page": 7,
      "text": "evaluated on various resolutions with-"
    },
    {
      "bounding_box": [
        {
          "x": 1472,
          "y": 466
        },
        {
          "x": 2076,
          "y": 466
        },
        {
          "x": 2076,
          "y": 511
        },
        {
          "x": 1472,
          "y": 511
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='77' style='font-size:18px'>out fine-tuning. Left: Absolute top-1</p>",
      "id": 77,
      "page": 7,
      "text": "out fine-tuning. Left: Absolute top-1"
    },
    {
      "bounding_box": [
        {
          "x": 1472,
          "y": 506
        },
        {
          "x": 2078,
          "y": 506
        },
        {
          "x": 2078,
          "y": 553
        },
        {
          "x": 1472,
          "y": 553
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='78' style='font-size:22px'>accuracy; Right: Accuracy difference</p>",
      "id": 78,
      "page": 7,
      "text": "accuracy; Right: Accuracy difference"
    },
    {
      "bounding_box": [
        {
          "x": 494,
          "y": 305
        },
        {
          "x": 1449,
          "y": 305
        },
        {
          "x": 1449,
          "y": 738
        },
        {
          "x": 494,
          "y": 738
        }
      ],
      "category": "figure",
      "html": "<br><figure><img id='79' style='font-size:14px' alt=\"82.5\nImageNet\n(%) 0.0\n80.0\nAcc\n-2.5\n77.5\nTop-1\n-5.0\n75.0 on\nAcc\n-7.5\nImageNet\n72.5\nDeiT-S -10.0 DeiT-S\n70.0\nGFNet-S GFNet-S\n67.5 CycleMLP-B2 Top-1\n◁ -12.5 CycleMLP-B2\n128 224 320 416 128 224 320 416\nTesting Resolution Testing Resolution\" data-coord=\"top-left:(494,305); bottom-right:(1449,738)\" /></figure>",
      "id": 79,
      "page": 7,
      "text": "82.5\nImageNet\n(%) 0.0\n80.0\nAcc\n-2.5\n77.5\nTop-1\n-5.0\n75.0 on\nAcc\n-7.5\nImageNet\n72.5\nDeiT-S -10.0 DeiT-S\n70.0\nGFNet-S GFNet-S\n67.5 CycleMLP-B2 Top-1\n◁ -12.5 CycleMLP-B2\n128 224 320 416 128 224 320 416\nTesting Resolution Testing Resolution"
    },
    {
      "bounding_box": [
        {
          "x": 1470,
          "y": 548
        },
        {
          "x": 2077,
          "y": 548
        },
        {
          "x": 2077,
          "y": 590
        },
        {
          "x": 1470,
          "y": 590
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='80' style='font-size:16px'>relative to that tested on 224x224. The</p>",
      "id": 80,
      "page": 7,
      "text": "relative to that tested on 224x224. The"
    },
    {
      "bounding_box": [
        {
          "x": 1471,
          "y": 586
        },
        {
          "x": 2079,
          "y": 586
        },
        {
          "x": 2079,
          "y": 636
        },
        {
          "x": 1471,
          "y": 636
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='81' style='font-size:22px'>superiority of CycleMLP's robustness</p>",
      "id": 81,
      "page": 7,
      "text": "superiority of CycleMLP's robustness"
    },
    {
      "bounding_box": [
        {
          "x": 1472,
          "y": 633
        },
        {
          "x": 2079,
          "y": 633
        },
        {
          "x": 2079,
          "y": 676
        },
        {
          "x": 1472,
          "y": 676
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='82' style='font-size:18px'>becomes more significant when scale</p>",
      "id": 82,
      "page": 7,
      "text": "becomes more significant when scale"
    },
    {
      "bounding_box": [
        {
          "x": 1472,
          "y": 675
        },
        {
          "x": 1858,
          "y": 675
        },
        {
          "x": 1858,
          "y": 718
        },
        {
          "x": 1472,
          "y": 718
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='83' style='font-size:14px'>varies to a greater extent.</p>",
      "id": 83,
      "page": 7,
      "text": "varies to a greater extent."
    },
    {
      "bounding_box": [
        {
          "x": 440,
          "y": 844
        },
        {
          "x": 2109,
          "y": 844
        },
        {
          "x": 2109,
          "y": 1127
        },
        {
          "x": 440,
          "y": 1127
        }
      ],
      "category": "caption",
      "html": "<caption id='84' style='font-size:20px'>Table 4 further details the ablation study on the structure of CycleMLP block. It is observed that<br>the top-1 accuracy drops significantly after removing one of the three parallel branches, especially<br>when discarding the 1 x7 or 7x1 branch. To eliminate the probability that the fewer parameters<br>and FLOPs cause the performance drop, we further use two same branches (denoted as \"V\" in<br>Table 4) and one 1 x 1 branch to align the parameters and FLOPs. The accuracy still drops relative<br>to CycleMLP, which further demonstrates the necessity of these three unique branches.</caption>",
      "id": 84,
      "page": 7,
      "text": "Table 4 further details the ablation study on the structure of CycleMLP block. It is observed that\nthe top-1 accuracy drops significantly after removing one of the three parallel branches, especially\nwhen discarding the 1 x7 or 7x1 branch. To eliminate the probability that the fewer parameters\nand FLOPs cause the performance drop, we further use two same branches (denoted as \"V\" in\nTable 4) and one 1 x 1 branch to align the parameters and FLOPs. The accuracy still drops relative\nto CycleMLP, which further demonstrates the necessity of these three unique branches."
    },
    {
      "bounding_box": [
        {
          "x": 441,
          "y": 1143
        },
        {
          "x": 2110,
          "y": 1143
        },
        {
          "x": 2110,
          "y": 1655
        },
        {
          "x": 441,
          "y": 1655
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='85' style='font-size:22px'>Resolution adaptability. One remarkable advantage of CycleMLP is that it can take arbitrary-<br>resolution images as input without any modification. On the contrary, GFNet (Rao et al., 2021)<br>needs to interpolate the learnable parameters on the fly when the input scale is different from the one<br>for training. We compare the resolution adaptability by directly evaluating models at a broad spec-<br>trum of resolutions using the weight pre-trained on 224x224, without fine-tuning. Figure 3 (left)<br>shows that the absolute Top-1 accuracy on ImagNet and Figure 3 (right) shows the accuracy dif-<br>ferences between one specific resolution and the resolution of 224x224. Compared with DeiT and<br>GFNet, CycleMLP is more robust when resolution varies. In particular, at the 128x 128, CycleMLP<br>saves more than 2 points drop compared to GFNet. Furthermore, at higher resolution, the perfor-<br>mance drop of CycleMLP is less than GFNet. Note that the superiority of CycleMLP becomes more<br>significant when the resolution changes to a greater extent.</p>",
      "id": 85,
      "page": 7,
      "text": "Resolution adaptability. One remarkable advantage of CycleMLP is that it can take arbitrary-\nresolution images as input without any modification. On the contrary, GFNet (Rao et al., 2021)\nneeds to interpolate the learnable parameters on the fly when the input scale is different from the one\nfor training. We compare the resolution adaptability by directly evaluating models at a broad spec-\ntrum of resolutions using the weight pre-trained on 224x224, without fine-tuning. Figure 3 (left)\nshows that the absolute Top-1 accuracy on ImagNet and Figure 3 (right) shows the accuracy dif-\nferences between one specific resolution and the resolution of 224x224. Compared with DeiT and\nGFNet, CycleMLP is more robust when resolution varies. In particular, at the 128x 128, CycleMLP\nsaves more than 2 points drop compared to GFNet. Furthermore, at higher resolution, the perfor-\nmance drop of CycleMLP is less than GFNet. Note that the superiority of CycleMLP becomes more\nsignificant when the resolution changes to a greater extent."
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 1707
        },
        {
          "x": 1487,
          "y": 1707
        },
        {
          "x": 1487,
          "y": 1756
        },
        {
          "x": 443,
          "y": 1756
        }
      ],
      "category": "paragraph",
      "html": "<p id='86' style='font-size:20px'>3.3 OBJECT DETECTION AND INSTANCE SEGMENTATION</p>",
      "id": 86,
      "page": 7,
      "text": "3.3 OBJECT DETECTION AND INSTANCE SEGMENTATION"
    },
    {
      "bounding_box": [
        {
          "x": 441,
          "y": 1792
        },
        {
          "x": 2110,
          "y": 1792
        },
        {
          "x": 2110,
          "y": 2071
        },
        {
          "x": 441,
          "y": 2071
        }
      ],
      "category": "paragraph",
      "html": "<p id='87' style='font-size:22px'>Settings. We conduct object detection and instance segmentation experiments on COCO (Lin et al.,<br>2014) dataset. We first follow the experimental settings of PVT (Wang et al., 2021b), which are<br>introduced in Appendix. E.2. The corresponding results are presented in Table 6. Then, in order<br>to compare fairly with Swin Transformer, which adopts a different experimental recipe with PVT,<br>we further follow the experimental settings of Swin with our CycleMLP-S model and the results are<br>presented in Table 7.</p>",
      "id": 87,
      "page": 7,
      "text": "Settings. We conduct object detection and instance segmentation experiments on COCO (Lin et al.,\n2014) dataset. We first follow the experimental settings of PVT (Wang et al., 2021b), which are\nintroduced in Appendix. E.2. The corresponding results are presented in Table 6. Then, in order\nto compare fairly with Swin Transformer, which adopts a different experimental recipe with PVT,\nwe further follow the experimental settings of Swin with our CycleMLP-S model and the results are\npresented in Table 7."
    },
    {
      "bounding_box": [
        {
          "x": 454,
          "y": 2111
        },
        {
          "x": 2091,
          "y": 2111
        },
        {
          "x": 2091,
          "y": 2838
        },
        {
          "x": 454,
          "y": 2838
        }
      ],
      "category": "table",
      "html": "<table id='88' style='font-size:18px'><tr><td rowspan=\"2\">Backbone</td><td colspan=\"7\">RetinaNet 1 x</td><td colspan=\"7\">Mask R-CNN 1x</td></tr><tr><td>Param</td><td>AP</td><td>AP50</td><td>AP75</td><td>APs</td><td>APM</td><td>APL</td><td>Param</td><td>APb</td><td>AP50</td><td>AP75</td><td>APm</td><td>AP50</td><td>APm</td></tr><tr><td>ResNet18</td><td>21.3M</td><td>31.8</td><td>49.6</td><td>33.6</td><td>16.3</td><td>34.3</td><td>43.2</td><td>31.2M</td><td>34.0</td><td>54.0</td><td>36.7</td><td>31.2</td><td>51.0</td><td>32.7</td></tr><tr><td>PVT-Tiny</td><td>23.0M</td><td>36.7</td><td>56.9</td><td>38.9</td><td>22.6</td><td>38.8</td><td>50.0</td><td>32.9M</td><td>36.7</td><td>59.2</td><td>39.3</td><td>35.1</td><td>56.7</td><td>37.3</td></tr><tr><td>CycleMLP-B1</td><td>24.9M</td><td>38.1</td><td>58.7</td><td>40.1</td><td>21.9</td><td>41.9</td><td>50.4</td><td>34.8M</td><td>39.8</td><td>61.7</td><td>43.3</td><td>37.0</td><td>58.8</td><td>39.7</td></tr><tr><td>ResNet50</td><td>37.7M</td><td>36.3</td><td>55.3</td><td>38.6</td><td>19.3</td><td>40.0</td><td>48.8</td><td>44.2M</td><td>38.0</td><td>58.6</td><td>41.4</td><td>34.4</td><td>55.1</td><td>36.7</td></tr><tr><td>PVT-Small</td><td>34.2M</td><td>40.4</td><td>61.3</td><td>43.0</td><td>25.0</td><td>42.9</td><td>55.7</td><td>44.1M</td><td>40.4</td><td>62.9</td><td>43.8</td><td>37.8</td><td>60.1</td><td>40.3</td></tr><tr><td>CycleMLP-B2</td><td>36.5M</td><td>40.6</td><td>61.4</td><td>43.2</td><td>22.9</td><td>44.4</td><td>54.5</td><td>46.5M</td><td>42.1</td><td>64.0</td><td>45.7</td><td>38.9</td><td>61.2</td><td>41.8</td></tr><tr><td>ResNet101</td><td>56.7M</td><td>38.5</td><td>57.8</td><td>41.2</td><td>21.4</td><td>42.6</td><td>51.1</td><td>63.2M</td><td>40.4</td><td>61.1</td><td>44.2</td><td>36.4</td><td>57.7</td><td>38.8</td></tr><tr><td>ResNeXt101-32x4d</td><td>56.4M</td><td>39.9</td><td>59.6</td><td>42.7</td><td>22.3</td><td>44.2</td><td>52.5</td><td>62.8M</td><td>41.9</td><td>62.5</td><td>45.9</td><td>37.5</td><td>59.4</td><td>40.2</td></tr><tr><td>PVT-Medium</td><td>53.9M</td><td>41.9</td><td>63.1</td><td>44.3</td><td>25.0</td><td>44.9</td><td>57.6</td><td>63.9M</td><td>42.0</td><td>64.4</td><td>45.6</td><td>39.0</td><td>61.6</td><td>42.1</td></tr><tr><td>CycleMLP-B3</td><td>48.1M</td><td>42.5</td><td>63.2</td><td>45.3</td><td>25.2</td><td>45.5</td><td>56.2</td><td>58.0M</td><td>43.4</td><td>65.0</td><td>47.7</td><td>39.5</td><td>62.0</td><td>42.4</td></tr><tr><td>PVT-Large</td><td>71.1M</td><td>42.6</td><td>63.7</td><td>45.4</td><td>25.8</td><td>46.0</td><td>58.4</td><td>81.0M</td><td>42.9</td><td>65.0</td><td>46.6</td><td>39.5</td><td>61.9</td><td>42.5</td></tr><tr><td>CycleMLP-B4</td><td>61.5M</td><td>43.2</td><td>63.9</td><td>46.2</td><td>26.6</td><td>46.5</td><td>57.4</td><td>71.5M</td><td>44.1</td><td>65.7</td><td>48.1</td><td>40.2</td><td>62.7</td><td>43.5</td></tr><tr><td>ResNeXt101-64x4d</td><td>95.5M</td><td>41.0</td><td>60.9</td><td>44.0</td><td>23.9</td><td>45.2</td><td>54.0</td><td>101.9M</td><td>42.8</td><td>63.8</td><td>47.3</td><td>38.4</td><td>60.6</td><td>41.3</td></tr><tr><td>CycleMLP-B5</td><td>85.9M</td><td>42.7</td><td>63.3</td><td>45.3</td><td>24.1</td><td>46.3</td><td>57.4</td><td>95.3M</td><td>44.1</td><td>65.5</td><td>48.4</td><td>40.1</td><td>62.8</td><td>43.0</td></tr></table>",
      "id": 88,
      "page": 7,
      "text": "Backbone RetinaNet 1 x Mask R-CNN 1x\n Param AP AP50 AP75 APs APM APL Param APb AP50 AP75 APm AP50 APm\n ResNet18 21.3M 31.8 49.6 33.6 16.3 34.3 43.2 31.2M 34.0 54.0 36.7 31.2 51.0 32.7\n PVT-Tiny 23.0M 36.7 56.9 38.9 22.6 38.8 50.0 32.9M 36.7 59.2 39.3 35.1 56.7 37.3\n CycleMLP-B1 24.9M 38.1 58.7 40.1 21.9 41.9 50.4 34.8M 39.8 61.7 43.3 37.0 58.8 39.7\n ResNet50 37.7M 36.3 55.3 38.6 19.3 40.0 48.8 44.2M 38.0 58.6 41.4 34.4 55.1 36.7\n PVT-Small 34.2M 40.4 61.3 43.0 25.0 42.9 55.7 44.1M 40.4 62.9 43.8 37.8 60.1 40.3\n CycleMLP-B2 36.5M 40.6 61.4 43.2 22.9 44.4 54.5 46.5M 42.1 64.0 45.7 38.9 61.2 41.8\n ResNet101 56.7M 38.5 57.8 41.2 21.4 42.6 51.1 63.2M 40.4 61.1 44.2 36.4 57.7 38.8\n ResNeXt101-32x4d 56.4M 39.9 59.6 42.7 22.3 44.2 52.5 62.8M 41.9 62.5 45.9 37.5 59.4 40.2\n PVT-Medium 53.9M 41.9 63.1 44.3 25.0 44.9 57.6 63.9M 42.0 64.4 45.6 39.0 61.6 42.1\n CycleMLP-B3 48.1M 42.5 63.2 45.3 25.2 45.5 56.2 58.0M 43.4 65.0 47.7 39.5 62.0 42.4\n PVT-Large 71.1M 42.6 63.7 45.4 25.8 46.0 58.4 81.0M 42.9 65.0 46.6 39.5 61.9 42.5\n CycleMLP-B4 61.5M 43.2 63.9 46.2 26.6 46.5 57.4 71.5M 44.1 65.7 48.1 40.2 62.7 43.5\n ResNeXt101-64x4d 95.5M 41.0 60.9 44.0 23.9 45.2 54.0 101.9M 42.8 63.8 47.3 38.4 60.6 41.3\n CycleMLP-B5 85.9M 42.7 63.3 45.3 24.1 46.3 57.4 95.3M 44.1 65.5 48.4 40.1 62.8"
    },
    {
      "bounding_box": [
        {
          "x": 441,
          "y": 2872
        },
        {
          "x": 2109,
          "y": 2872
        },
        {
          "x": 2109,
          "y": 3014
        },
        {
          "x": 441,
          "y": 3014
        }
      ],
      "category": "caption",
      "html": "<caption id='89' style='font-size:22px'>Table 6: Object detection and instance segmentation on COCO val2017 (Lin et al., 2014).<br>We compare CycleMLP with various backbones including ResNet (He et al., 2016), ResNeXt (Xie<br>et al., 2017) and PVT (Wang et al., 2021b).</caption>",
      "id": 89,
      "page": 7,
      "text": "Table 6: Object detection and instance segmentation on COCO val2017 (Lin et al., 2014).\nWe compare CycleMLP with various backbones including ResNet (He et al., 2016), ResNeXt (Xie\net al., 2017) and PVT (Wang et al., 2021b)."
    },
    {
      "bounding_box": [
        {
          "x": 1260,
          "y": 3134
        },
        {
          "x": 1289,
          "y": 3134
        },
        {
          "x": 1289,
          "y": 3170
        },
        {
          "x": 1260,
          "y": 3170
        }
      ],
      "category": "footer",
      "html": "<footer id='90' style='font-size:18px'>7</footer>",
      "id": 90,
      "page": 7,
      "text": "7"
    },
    {
      "bounding_box": [
        {
          "x": 445,
          "y": 111
        },
        {
          "x": 1224,
          "y": 111
        },
        {
          "x": 1224,
          "y": 157
        },
        {
          "x": 445,
          "y": 157
        }
      ],
      "category": "header",
      "html": "<header id='91' style='font-size:14px'>Published as a conference paper at ICLR 2022</header>",
      "id": 91,
      "page": 8,
      "text": "Published as a conference paper at ICLR 2022"
    },
    {
      "bounding_box": [
        {
          "x": 454,
          "y": 339
        },
        {
          "x": 2081,
          "y": 339
        },
        {
          "x": 2081,
          "y": 599
        },
        {
          "x": 454,
          "y": 599
        }
      ],
      "category": "table",
      "html": "<table id='92' style='font-size:16px'><tr><td>Backbone</td><td>APb</td><td>AP50</td><td>AP75</td><td>APm</td><td>APm</td><td>APm</td><td>Params</td><td>FLOPs</td></tr><tr><td>ResNet50 (He et al., 2016)</td><td>41.0</td><td>61.7</td><td>44.9</td><td>37.1</td><td>58.4</td><td>40.1</td><td>44M</td><td>260G</td></tr><tr><td>PVT-Small (Wang et al., 2021b)</td><td>43.0</td><td>65.3</td><td>46.9</td><td>39.9</td><td>62.5</td><td>42.8</td><td>44M</td><td>245G</td></tr><tr><td>Swin-T (Liu et al., 2021b)</td><td>46.0</td><td>68.2</td><td>50.2</td><td>41.6</td><td>65.1</td><td>44.8</td><td>48M</td><td>264G</td></tr><tr><td>CycleMLP-T (ours)</td><td>46.4</td><td>68.1</td><td>51.1</td><td>41.8</td><td>64.9</td><td>45.1</td><td>48M</td><td>260G</td></tr></table>",
      "id": 92,
      "page": 8,
      "text": "Backbone APb AP50 AP75 APm APm APm Params FLOPs\n ResNet50 (He et al., 2016) 41.0 61.7 44.9 37.1 58.4 40.1 44M 260G\n PVT-Small (Wang et al., 2021b) 43.0 65.3 46.9 39.9 62.5 42.8 44M 245G\n Swin-T (Liu et al., 2021b) 46.0 68.2 50.2 41.6 65.1 44.8 48M 264G\n CycleMLP-T (ours) 46.4 68.1 51.1 41.8 64.9 45.1 48M"
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 637
        },
        {
          "x": 2102,
          "y": 637
        },
        {
          "x": 2102,
          "y": 730
        },
        {
          "x": 442,
          "y": 730
        }
      ],
      "category": "paragraph",
      "html": "<p id='93' style='font-size:18px'>Table 7: The instance segmentation results of different backbones on the COCO val2017 dataset.<br>Mask R-CNN frameworks are employed.</p>",
      "id": 93,
      "page": 8,
      "text": "Table 7: The instance segmentation results of different backbones on the COCO val2017 dataset.\nMask R-CNN frameworks are employed."
    },
    {
      "bounding_box": [
        {
          "x": 441,
          "y": 824
        },
        {
          "x": 2108,
          "y": 824
        },
        {
          "x": 2108,
          "y": 1100
        },
        {
          "x": 441,
          "y": 1100
        }
      ],
      "category": "paragraph",
      "html": "<p id='94' style='font-size:20px'>Results. Firstly, as shown in Table 6, CycleMLP-based RetinaNet consistently surpasses the CNN-<br>based ResNet (He et al., 2016), ResNeXt (Xie et al., 2017) and Transformer-based PVT (Wang<br>et al., 2021b) under similar parameter constraints, indicating that CycleMLP can serve as an ex-<br>cellent general-purpose backbone. Furthermore, using Mask R-CNN (He et al., 2017) for instance<br>segmentation also demonstrates similar comparison results. Furthermore, from Table 7, the Cy-<br>cleMLP can achieve a slightly better performance than Swin Transformer.</p>",
      "id": 94,
      "page": 8,
      "text": "Results. Firstly, as shown in Table 6, CycleMLP-based RetinaNet consistently surpasses the CNN-\nbased ResNet (He et al., 2016), ResNeXt (Xie et al., 2017) and Transformer-based PVT (Wang\net al., 2021b) under similar parameter constraints, indicating that CycleMLP can serve as an ex-\ncellent general-purpose backbone. Furthermore, using Mask R-CNN (He et al., 2017) for instance\nsegmentation also demonstrates similar comparison results. Furthermore, from Table 7, the Cy-\ncleMLP can achieve a slightly better performance than Swin Transformer."
    },
    {
      "bounding_box": [
        {
          "x": 445,
          "y": 1161
        },
        {
          "x": 1037,
          "y": 1161
        },
        {
          "x": 1037,
          "y": 1208
        },
        {
          "x": 445,
          "y": 1208
        }
      ],
      "category": "paragraph",
      "html": "<p id='95' style='font-size:18px'>3.4 SEMANTIC SEGMENTATION</p>",
      "id": 95,
      "page": 8,
      "text": "3.4 SEMANTIC SEGMENTATION"
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 1248
        },
        {
          "x": 2107,
          "y": 1248
        },
        {
          "x": 2107,
          "y": 1387
        },
        {
          "x": 442,
          "y": 1387
        }
      ],
      "category": "paragraph",
      "html": "<p id='96' style='font-size:22px'>Settings. We conduct semantic segmentation experiments on ADE20K (Zhou et al., 2017) dataset<br>and present the detailed settings in Appendix. E.3. Table 8 and Table 9 show the experimental results<br>using training recipes from PVT and Swin respectively.</p>",
      "id": 96,
      "page": 8,
      "text": "Settings. We conduct semantic segmentation experiments on ADE20K (Zhou et al., 2017) dataset\nand present the detailed settings in Appendix. E.3. Table 8 and Table 9 show the experimental results\nusing training recipes from PVT and Swin respectively."
    },
    {
      "bounding_box": [
        {
          "x": 451,
          "y": 1434
        },
        {
          "x": 1406,
          "y": 1434
        },
        {
          "x": 1406,
          "y": 2338
        },
        {
          "x": 451,
          "y": 2338
        }
      ],
      "category": "table",
      "html": "<table id='97' style='font-size:16px'><tr><td rowspan=\"2\">Backbone</td><td colspan=\"2\">Semantic FPN</td></tr><tr><td>Param</td><td>mIoU (%)</td></tr><tr><td>ResNet18 (He et al., 2016)</td><td>15.5M</td><td>32.9</td></tr><tr><td>PVT-Tiny (Wang et al., 2021b)</td><td>17.0M</td><td>35.7</td></tr><tr><td>CycleMLP-B1 (ours)</td><td>18.9M</td><td>40.8</td></tr><tr><td>ResNet50 (He et al., 2016)</td><td>28.5M</td><td>36.7</td></tr><tr><td>PVT-Small (Wang et al., 2021b)</td><td>28.2M</td><td>39.8</td></tr><tr><td>Swin-T† (Liu et al., 2021b)</td><td>31.9M</td><td>41.5</td></tr><tr><td>GFNet-Tiny (Rao et al., 2021)</td><td>26.6M</td><td>41.0</td></tr><tr><td>CycleMLP-B2 (ours)</td><td>30.6M</td><td>43.4</td></tr><tr><td>ResNet101 (He et al., 2016)</td><td>47.5M</td><td>38.8</td></tr><tr><td>ResNeXt101-32x4d (Xie et al., 2017)</td><td>47.1M</td><td>39.7</td></tr><tr><td>PVT-Medium (Wang et al., 2021b)</td><td>48.0M</td><td>41.6</td></tr><tr><td>GFNet-Small (Rao et al., 2021)</td><td>47.5M</td><td>42.5</td></tr><tr><td>CycleMLP-B3 (ours)</td><td>42.1M</td><td>44.3</td></tr><tr><td>PVT-Large (Wang et al., 2021b)</td><td>65.1M</td><td>42.1</td></tr><tr><td>Swin-St (Liu et al., 2021b)</td><td>53.2M</td><td>45.2</td></tr><tr><td>CycleMLP-B4 (ours)</td><td>55.6M</td><td>45.1</td></tr><tr><td>GFNet-Base (Rao et al., 2021)</td><td>74.7M</td><td>44.8</td></tr><tr><td>ResNeXt101-64x4d (Xie et al., 2017)</td><td>86.4M</td><td>40.2</td></tr><tr><td>CycleMLP-B5 (ours)</td><td>79.4M</td><td>45.5</td></tr></table>",
      "id": 97,
      "page": 8,
      "text": "Backbone Semantic FPN\n Param mIoU (%)\n ResNet18 (He et al., 2016) 15.5M 32.9\n PVT-Tiny (Wang et al., 2021b) 17.0M 35.7\n CycleMLP-B1 (ours) 18.9M 40.8\n ResNet50 (He et al., 2016) 28.5M 36.7\n PVT-Small (Wang et al., 2021b) 28.2M 39.8\n Swin-T† (Liu et al., 2021b) 31.9M 41.5\n GFNet-Tiny (Rao et al., 2021) 26.6M 41.0\n CycleMLP-B2 (ours) 30.6M 43.4\n ResNet101 (He et al., 2016) 47.5M 38.8\n ResNeXt101-32x4d (Xie et al., 2017) 47.1M 39.7\n PVT-Medium (Wang et al., 2021b) 48.0M 41.6\n GFNet-Small (Rao et al., 2021) 47.5M 42.5\n CycleMLP-B3 (ours) 42.1M 44.3\n PVT-Large (Wang et al., 2021b) 65.1M 42.1\n Swin-St (Liu et al., 2021b) 53.2M 45.2\n CycleMLP-B4 (ours) 55.6M 45.1\n GFNet-Base (Rao et al., 2021) 74.7M 44.8\n ResNeXt101-64x4d (Xie et al., 2017) 86.4M 40.2\n CycleMLP-B5 (ours) 79.4M"
    },
    {
      "bounding_box": [
        {
          "x": 1451,
          "y": 1431
        },
        {
          "x": 2003,
          "y": 1431
        },
        {
          "x": 2003,
          "y": 2283
        },
        {
          "x": 1451,
          "y": 2283
        }
      ],
      "category": "figure",
      "html": "<br><figure><img id='98' style='font-size:20px' alt=\"1.0\n0.8\n0.6\n(a) Swin\n0.4\n0.2\n0.0\n(b) CycleMLP\" data-coord=\"top-left:(1451,1431); bottom-right:(2003,2283)\" /></figure>",
      "id": 98,
      "page": 8,
      "text": "1.0\n0.8\n0.6\n(a) Swin\n0.4\n0.2\n0.0\n(b) CycleMLP"
    },
    {
      "bounding_box": [
        {
          "x": 489,
          "y": 2371
        },
        {
          "x": 1414,
          "y": 2371
        },
        {
          "x": 1414,
          "y": 2557
        },
        {
          "x": 489,
          "y": 2557
        }
      ],
      "category": "paragraph",
      "html": "<p id='99' style='font-size:18px'>Table 8: Semantic segmentation on ADE20K (Zhou<br>et al., 2017) val. All models are equipped with Se-<br>mantic FPN (Kirillov et al., 2019). 1 Results are from<br>GFNet (Rao et al., 2021).</p>",
      "id": 99,
      "page": 8,
      "text": "Table 8: Semantic segmentation on ADE20K (Zhou\net al., 2017) val. All models are equipped with Se-\nmantic FPN (Kirillov et al., 2019). 1 Results are from\nGFNet (Rao et al., 2021)."
    },
    {
      "bounding_box": [
        {
          "x": 1452,
          "y": 2325
        },
        {
          "x": 2077,
          "y": 2325
        },
        {
          "x": 2077,
          "y": 2554
        },
        {
          "x": 1452,
          "y": 2554
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='100' style='font-size:20px'>Figure 4: Effective Receptive Field<br>(ERF). We visualize the ERFs of the<br>last stage for both Swin (Liu et al.,<br>2021b) and CycleMLP. Best viewed<br>with zoom in.</p>",
      "id": 100,
      "page": 8,
      "text": "Figure 4: Effective Receptive Field\n(ERF). We visualize the ERFs of the\nlast stage for both Swin (Liu et al.,\n2021b) and CycleMLP. Best viewed\nwith zoom in."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 2613
        },
        {
          "x": 2107,
          "y": 2613
        },
        {
          "x": 2107,
          "y": 2846
        },
        {
          "x": 442,
          "y": 2846
        }
      ],
      "category": "paragraph",
      "html": "<p id='101' style='font-size:20px'>Results. As shown in Table 8, CycleMLP outperforms ResNet (He et al., 2016) and PVT (Wang<br>et al., 2021b) significantly with similar parameters. Moreover, compared to the state-of-the-art<br>Transformer-based backbone, Swin Transformer (Liu et al., 2021b), CycleMLP can obtain com-<br>parable or even better performance. Specifically, CycleMLP-B2 surpasses Swin-T by 0.9 mIoU<br>with slightly less parameters (30.6M V.S. 31.9M).</p>",
      "id": 101,
      "page": 8,
      "text": "Results. As shown in Table 8, CycleMLP outperforms ResNet (He et al., 2016) and PVT (Wang\net al., 2021b) significantly with similar parameters. Moreover, compared to the state-of-the-art\nTransformer-based backbone, Swin Transformer (Liu et al., 2021b), CycleMLP can obtain com-\nparable or even better performance. Specifically, CycleMLP-B2 surpasses Swin-T by 0.9 mIoU\nwith slightly less parameters (30.6M V.S. 31.9M)."
    },
    {
      "bounding_box": [
        {
          "x": 441,
          "y": 2869
        },
        {
          "x": 2107,
          "y": 2869
        },
        {
          "x": 2107,
          "y": 3054
        },
        {
          "x": 441,
          "y": 3054
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='102' style='font-size:22px'>Although GFNet (Rao et al., 2021) achieves similar performance as CycleMLP on ImageNet clas-<br>sification, CycleMLP notably outperforms GFNet on ADE20K semantic segmentation where input<br>scale varies. We attribute the superiority of CycleMLP under a scale-variable scenario to the capa-<br>bility of dealing with arbitrary scales. On the contrary, GFNet (Rao et al., 2021) requires additional</p>",
      "id": 102,
      "page": 8,
      "text": "Although GFNet (Rao et al., 2021) achieves similar performance as CycleMLP on ImageNet clas-\nsification, CycleMLP notably outperforms GFNet on ADE20K semantic segmentation where input\nscale varies. We attribute the superiority of CycleMLP under a scale-variable scenario to the capa-\nbility of dealing with arbitrary scales. On the contrary, GFNet (Rao et al., 2021) requires additional"
    },
    {
      "bounding_box": [
        {
          "x": 1261,
          "y": 3134
        },
        {
          "x": 1289,
          "y": 3134
        },
        {
          "x": 1289,
          "y": 3170
        },
        {
          "x": 1261,
          "y": 3170
        }
      ],
      "category": "footer",
      "html": "<footer id='103' style='font-size:18px'>8</footer>",
      "id": 103,
      "page": 8,
      "text": "8"
    },
    {
      "bounding_box": [
        {
          "x": 445,
          "y": 112
        },
        {
          "x": 1224,
          "y": 112
        },
        {
          "x": 1224,
          "y": 156
        },
        {
          "x": 445,
          "y": 156
        }
      ],
      "category": "header",
      "html": "<header id='104' style='font-size:14px'>Published as a conference paper at ICLR 2022</header>",
      "id": 104,
      "page": 9,
      "text": "Published as a conference paper at ICLR 2022"
    },
    {
      "bounding_box": [
        {
          "x": 457,
          "y": 337
        },
        {
          "x": 2088,
          "y": 337
        },
        {
          "x": 2088,
          "y": 932
        },
        {
          "x": 457,
          "y": 932
        }
      ],
      "category": "table",
      "html": "<table id='105' style='font-size:20px'><tr><td>Method</td><td>Backbone</td><td>val MS mIoU</td><td>Params</td><td>FLOPs</td></tr><tr><td rowspan=\"3\">UperNet (Xiao et al., 2018)</td><td>Swin-T (Liu et al., 2021b)</td><td>45.8</td><td>60M</td><td>945G</td></tr><tr><td>AS-MLP-T (Lian et al., 2021)</td><td>46.5</td><td>60M</td><td>937G</td></tr><tr><td>CycleMLP-T (ours)</td><td>47.1</td><td>60M</td><td>937G</td></tr><tr><td rowspan=\"3\">UperNet (Xiao et al., 2018)</td><td>Swin-S (Liu et al., 2021b)</td><td>49.5</td><td>81M</td><td>1038G</td></tr><tr><td>AS-MLP-S (Lian et al., 2021)</td><td>49.2</td><td>81M</td><td>1024G</td></tr><tr><td>CycleMLP-S (ours)</td><td>49.6</td><td>81M</td><td>1024G</td></tr><tr><td rowspan=\"3\">UperNet (Xiao et al., 2018)</td><td>Swin-B (Liu et al., 2021b)</td><td>49.7</td><td>121M</td><td>1188G</td></tr><tr><td>AS-MLP-B(Lian et al., 2021)</td><td>49.5</td><td>121M</td><td>1166G</td></tr><tr><td>CycleMLP-B (ours)</td><td>49.7</td><td>121M</td><td>1166G</td></tr></table>",
      "id": 105,
      "page": 9,
      "text": "Method Backbone val MS mIoU Params FLOPs\n UperNet (Xiao et al., 2018) Swin-T (Liu et al., 2021b) 45.8 60M 945G\n AS-MLP-T (Lian et al., 2021) 46.5 60M 937G\n CycleMLP-T (ours) 47.1 60M 937G\n UperNet (Xiao et al., 2018) Swin-S (Liu et al., 2021b) 49.5 81M 1038G\n AS-MLP-S (Lian et al., 2021) 49.2 81M 1024G\n CycleMLP-S (ours) 49.6 81M 1024G\n UperNet (Xiao et al., 2018) Swin-B (Liu et al., 2021b) 49.7 121M 1188G\n AS-MLP-B(Lian et al., 2021) 49.5 121M 1166G\n CycleMLP-B (ours) 49.7 121M"
    },
    {
      "bounding_box": [
        {
          "x": 460,
          "y": 941
        },
        {
          "x": 2087,
          "y": 941
        },
        {
          "x": 2087,
          "y": 986
        },
        {
          "x": 460,
          "y": 986
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='106' style='font-size:16px'>Table 9: The semantic segmentation results of different backbones on the ADE20K validation set.</p>",
      "id": 106,
      "page": 9,
      "text": "Table 9: The semantic segmentation results of different backbones on the ADE20K validation set."
    },
    {
      "bounding_box": [
        {
          "x": 441,
          "y": 1102
        },
        {
          "x": 2102,
          "y": 1102
        },
        {
          "x": 2102,
          "y": 1190
        },
        {
          "x": 441,
          "y": 1190
        }
      ],
      "category": "paragraph",
      "html": "<p id='107' style='font-size:20px'>heuristic operation (weight interpolation) when the input scale varies, which may hurt the perfor-<br>mance.</p>",
      "id": 107,
      "page": 9,
      "text": "heuristic operation (weight interpolation) when the input scale varies, which may hurt the perfor-\nmance."
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 1217
        },
        {
          "x": 2105,
          "y": 1217
        },
        {
          "x": 2105,
          "y": 1353
        },
        {
          "x": 443,
          "y": 1353
        }
      ],
      "category": "paragraph",
      "html": "<p id='108' style='font-size:18px'>Moreover, we also visualized the receptive field following (Xie et al., 2021), and the results are<br>visualized in Figure 4, which demonstrate that our CycleMLP has a larger effective receptive field<br>than Swin.</p>",
      "id": 108,
      "page": 9,
      "text": "Moreover, we also visualized the receptive field following (Xie et al., 2021), and the results are\nvisualized in Figure 4, which demonstrate that our CycleMLP has a larger effective receptive field\nthan Swin."
    },
    {
      "bounding_box": [
        {
          "x": 445,
          "y": 1437
        },
        {
          "x": 790,
          "y": 1437
        },
        {
          "x": 790,
          "y": 1482
        },
        {
          "x": 445,
          "y": 1482
        }
      ],
      "category": "paragraph",
      "html": "<p id='109' style='font-size:20px'>3.5 ROBUSTNESS</p>",
      "id": 109,
      "page": 9,
      "text": "3.5 ROBUSTNESS"
    },
    {
      "bounding_box": [
        {
          "x": 450,
          "y": 1565
        },
        {
          "x": 2094,
          "y": 1565
        },
        {
          "x": 2094,
          "y": 1962
        },
        {
          "x": 450,
          "y": 1962
        }
      ],
      "category": "table",
      "html": "<table id='110' style='font-size:14px'><tr><td rowspan=\"2\">Network</td><td rowspan=\"2\">mCE↓</td><td colspan=\"3\">Noise</td><td colspan=\"4\">Blur</td><td colspan=\"3\">Weather</td><td colspan=\"3\">Digital</td></tr><tr><td>Gauss Shot</td><td></td><td>Impulse</td><td>Defocus</td><td>Glass</td><td>Motion</td><td>Zoom</td><td>Snow</td><td>Frost Fog</td><td>Bright</td><td>Contrast</td><td>Elastic Pixel</td><td>JPEG</td></tr><tr><td>ResNet-50</td><td>76.7</td><td>79.8</td><td>81.6</td><td>82.6</td><td>74.7</td><td>88.6</td><td>78.0</td><td>79.9</td><td>77.8 74.8</td><td>66.1</td><td>56.6</td><td>71.4</td><td>84.7 76.9</td><td>76.8</td></tr><tr><td>DeiT-S</td><td>54.6</td><td>46.3</td><td>47.7</td><td>46.4</td><td>61.6</td><td>71.9</td><td>57.9</td><td>71.9</td><td>49.9 46.2</td><td>46.0</td><td>44.9</td><td>42.3</td><td>66.6 59.1</td><td>60.4</td></tr><tr><td>Swin-S</td><td>62.0</td><td>52.2</td><td>53.7</td><td>53.6</td><td>67.9</td><td>78.6</td><td>64.1</td><td>75.3</td><td>55.8 52.8</td><td>51.3</td><td>48.1</td><td>45.1</td><td>75.7 76.3</td><td>79.1</td></tr><tr><td>MLP-Mixer</td><td>78.8</td><td>80.9</td><td>82.6</td><td>84.2</td><td>86.9</td><td>92.1</td><td>79.1</td><td>93.6</td><td>78.3 67.4</td><td>64.6</td><td>59.5</td><td>57.1</td><td>90.5 72.7</td><td>92.2</td></tr><tr><td>ResMLP-12</td><td>66.0</td><td>57.6</td><td>58.2</td><td>57.8</td><td>72.6</td><td>83.2</td><td>67.9</td><td>76.5</td><td>61.4 57.8</td><td>63.8</td><td>53.9</td><td>52.1</td><td>78.3 72.9</td><td>75.3</td></tr><tr><td>gMLP-S</td><td>64.0</td><td>52.1</td><td>53.2</td><td>52.5</td><td>73.1</td><td>77.6</td><td>64.6</td><td>79.9</td><td>77.7 78.8</td><td>54.3</td><td>55.3</td><td>43.6</td><td>70.6 58.6</td><td>67.5</td></tr><tr><td>CycleMLP-S</td><td>53.7</td><td>42.1</td><td>43.4</td><td>43.2</td><td>61.5</td><td>76.7</td><td>56.0</td><td>66.4</td><td>51.5 47.2</td><td>50.8</td><td>41.2</td><td>39.5</td><td>72.3 57.5</td><td>56.1</td></tr></table>",
      "id": 110,
      "page": 9,
      "text": "Network mCE↓ Noise Blur Weather Digital\n Gauss Shot  Impulse Defocus Glass Motion Zoom Snow Frost Fog Bright Contrast Elastic Pixel JPEG\n ResNet-50 76.7 79.8 81.6 82.6 74.7 88.6 78.0 79.9 77.8 74.8 66.1 56.6 71.4 84.7 76.9 76.8\n DeiT-S 54.6 46.3 47.7 46.4 61.6 71.9 57.9 71.9 49.9 46.2 46.0 44.9 42.3 66.6 59.1 60.4\n Swin-S 62.0 52.2 53.7 53.6 67.9 78.6 64.1 75.3 55.8 52.8 51.3 48.1 45.1 75.7 76.3 79.1\n MLP-Mixer 78.8 80.9 82.6 84.2 86.9 92.1 79.1 93.6 78.3 67.4 64.6 59.5 57.1 90.5 72.7 92.2\n ResMLP-12 66.0 57.6 58.2 57.8 72.6 83.2 67.9 76.5 61.4 57.8 63.8 53.9 52.1 78.3 72.9 75.3\n gMLP-S 64.0 52.1 53.2 52.5 73.1 77.6 64.6 79.9 77.7 78.8 54.3 55.3 43.6 70.6 58.6 67.5\n CycleMLP-S 53.7 42.1 43.4 43.2 61.5 76.7 56.0 66.4 51.5 47.2 50.8 41.2 39.5 72.3 57.5"
    },
    {
      "bounding_box": [
        {
          "x": 444,
          "y": 1997
        },
        {
          "x": 2105,
          "y": 1997
        },
        {
          "x": 2105,
          "y": 2136
        },
        {
          "x": 444,
          "y": 2136
        }
      ],
      "category": "caption",
      "html": "<caption id='111' style='font-size:18px'>Table 10: Robustness on ImageNet-C (Hendrycks & Dietterich, 2019). The mean corruption<br>error (mCE) normalized by AlexNet (Krizhevsky et al., 2012) errors is used as the robustness metric.<br>The lower, the better.</caption>",
      "id": 111,
      "page": 9,
      "text": "Table 10: Robustness on ImageNet-C (Hendrycks & Dietterich, 2019). The mean corruption\nerror (mCE) normalized by AlexNet (Krizhevsky et al., 2012) errors is used as the robustness metric.\nThe lower, the better."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 2218
        },
        {
          "x": 2107,
          "y": 2218
        },
        {
          "x": 2107,
          "y": 2404
        },
        {
          "x": 442,
          "y": 2404
        }
      ],
      "category": "paragraph",
      "html": "<p id='112' style='font-size:20px'>We further conduct experiments on ImageNet-C (Hendrycks & Gimpel, 2016) to analyze the ro-<br>bustness ability of the CycleMLP, following (Mao et al., 2021) and results are presented in Table 10.<br>Compared with both Transformers (e.g. DeiT and Swin) and existing MLP models (e.g. MLP-Mixer,<br>ResMLP, gMLP), CycleMLP achieves a stronger robustness ability.</p>",
      "id": 112,
      "page": 9,
      "text": "We further conduct experiments on ImageNet-C (Hendrycks & Gimpel, 2016) to analyze the ro-\nbustness ability of the CycleMLP, following (Mao et al., 2021) and results are presented in Table 10.\nCompared with both Transformers (e.g. DeiT and Swin) and existing MLP models (e.g. MLP-Mixer,\nResMLP, gMLP), CycleMLP achieves a stronger robustness ability."
    },
    {
      "bounding_box": [
        {
          "x": 444,
          "y": 2498
        },
        {
          "x": 817,
          "y": 2498
        },
        {
          "x": 817,
          "y": 2547
        },
        {
          "x": 444,
          "y": 2547
        }
      ],
      "category": "paragraph",
      "html": "<p id='113' style='font-size:22px'>4 CONCLUSION</p>",
      "id": 113,
      "page": 9,
      "text": "4 CONCLUSION"
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 2614
        },
        {
          "x": 2107,
          "y": 2614
        },
        {
          "x": 2107,
          "y": 2937
        },
        {
          "x": 443,
          "y": 2937
        }
      ],
      "category": "paragraph",
      "html": "<p id='114' style='font-size:18px'>We present a versatile MLP-like architecture, CycleMLP, in this work. CycleMLP is built upon the<br>Cycle Fully-Connected Layer (Cycle FC), which is capable of dealing with variable input scales<br>and can serve as a generic, plug-and-play replacement of vanilla FC layers. Experimental results<br>demonstrate that CycleMLP outperforms existing MLP-like models on ImageNet classification and<br>achieves promising performance on dense prediction tasks, i.e., object detection, instance segmen-<br>tation and semantic segmentation. This work indicates that an attention-free architecture can also<br>serve as a general vision backbone.</p>",
      "id": 114,
      "page": 9,
      "text": "We present a versatile MLP-like architecture, CycleMLP, in this work. CycleMLP is built upon the\nCycle Fully-Connected Layer (Cycle FC), which is capable of dealing with variable input scales\nand can serve as a generic, plug-and-play replacement of vanilla FC layers. Experimental results\ndemonstrate that CycleMLP outperforms existing MLP-like models on ImageNet classification and\nachieves promising performance on dense prediction tasks, i.e., object detection, instance segmen-\ntation and semantic segmentation. This work indicates that an attention-free architecture can also\nserve as a general vision backbone."
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 2961
        },
        {
          "x": 2104,
          "y": 2961
        },
        {
          "x": 2104,
          "y": 3052
        },
        {
          "x": 443,
          "y": 3052
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='115' style='font-size:20px'>Acknowledgment. Ping Luo is supported by the General Research Fund of HK No.27208720,<br>No.17212120, and the HKU-TCL Joint Research Center for Artificial Intelligence.</p>",
      "id": 115,
      "page": 9,
      "text": "Acknowledgment. Ping Luo is supported by the General Research Fund of HK No.27208720,\nNo.17212120, and the HKU-TCL Joint Research Center for Artificial Intelligence."
    },
    {
      "bounding_box": [
        {
          "x": 1259,
          "y": 3134
        },
        {
          "x": 1288,
          "y": 3134
        },
        {
          "x": 1288,
          "y": 3169
        },
        {
          "x": 1259,
          "y": 3169
        }
      ],
      "category": "footer",
      "html": "<footer id='116' style='font-size:18px'>9</footer>",
      "id": 116,
      "page": 9,
      "text": "9"
    },
    {
      "bounding_box": [
        {
          "x": 444,
          "y": 112
        },
        {
          "x": 1224,
          "y": 112
        },
        {
          "x": 1224,
          "y": 156
        },
        {
          "x": 444,
          "y": 156
        }
      ],
      "category": "header",
      "html": "<header id='117' style='font-size:14px'>Published as a conference paper at ICLR 2022</header>",
      "id": 117,
      "page": 10,
      "text": "Published as a conference paper at ICLR 2022"
    },
    {
      "bounding_box": [
        {
          "x": 446,
          "y": 342
        },
        {
          "x": 734,
          "y": 342
        },
        {
          "x": 734,
          "y": 391
        },
        {
          "x": 446,
          "y": 391
        }
      ],
      "category": "paragraph",
      "html": "<p id='118' style='font-size:20px'>REFERENCES</p>",
      "id": 118,
      "page": 10,
      "text": "REFERENCES"
    },
    {
      "bounding_box": [
        {
          "x": 445,
          "y": 422
        },
        {
          "x": 2104,
          "y": 422
        },
        {
          "x": 2104,
          "y": 513
        },
        {
          "x": 445,
          "y": 513
        }
      ],
      "category": "paragraph",
      "html": "<p id='119' style='font-size:18px'>Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lucic, and Cordelia Schmid.<br>Vivit: A video vision transformer. arXiv preprint arXiv:2103.15691, 2021.</p>",
      "id": 119,
      "page": 10,
      "text": "Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Lucic, and Cordelia Schmid.\nVivit: A video vision transformer. arXiv preprint arXiv:2103.15691, 2021."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 549
        },
        {
          "x": 2106,
          "y": 549
        },
        {
          "x": 2106,
          "y": 638
        },
        {
          "x": 442,
          "y": 638
        }
      ],
      "category": "paragraph",
      "html": "<p id='120' style='font-size:18px'>Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint<br>arXiv:1607.06450, 2016.</p>",
      "id": 120,
      "page": 10,
      "text": "Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016."
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 675
        },
        {
          "x": 2108,
          "y": 675
        },
        {
          "x": 2108,
          "y": 767
        },
        {
          "x": 443,
          "y": 767
        }
      ],
      "category": "paragraph",
      "html": "<p id='121' style='font-size:18px'>Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video<br>understanding? arXiv preprint arXiv:2102.05095, 2021.</p>",
      "id": 121,
      "page": 10,
      "text": "Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video\nunderstanding? arXiv preprint arXiv:2102.05095, 2021."
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 801
        },
        {
          "x": 2107,
          "y": 801
        },
        {
          "x": 2107,
          "y": 938
        },
        {
          "x": 443,
          "y": 938
        }
      ],
      "category": "paragraph",
      "html": "<p id='122' style='font-size:18px'>Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,<br>Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are<br>few-shot learners. arXiv preprint arXiv:2005.14165, 2020.</p>",
      "id": 122,
      "page": 10,
      "text": "Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. arXiv preprint arXiv:2005.14165, 2020."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 974
        },
        {
          "x": 2108,
          "y": 974
        },
        {
          "x": 2108,
          "y": 1111
        },
        {
          "x": 442,
          "y": 1111
        }
      ],
      "category": "paragraph",
      "html": "<p id='123' style='font-size:18px'>Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and<br>Sergey Zagoruyko. End-to-end object detection with transformers. In European Conference<br>on Computer Vision, pp. 213-229. Springer, 2020.</p>",
      "id": 123,
      "page": 10,
      "text": "Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\nSergey Zagoruyko. End-to-end object detection with transformers. In European Conference\non Computer Vision, pp. 213-229. Springer, 2020."
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 1145
        },
        {
          "x": 2109,
          "y": 1145
        },
        {
          "x": 2109,
          "y": 1375
        },
        {
          "x": 443,
          "y": 1375
        }
      ],
      "category": "paragraph",
      "html": "<p id='124' style='font-size:22px'>Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen<br>Feng, Ziwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tianheng Cheng, Qijie<br>Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli<br>Ouyang, Chen Change Loy, and Dahua Lin. MMDetection: Open mmlab detection toolbox and<br>benchmark. arXiv preprint arXiv: 1906.07155, 2019.</p>",
      "id": 124,
      "page": 10,
      "text": "Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen\nFeng, Ziwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tianheng Cheng, Qijie\nZhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli\nOuyang, Chen Change Loy, and Dahua Lin. MMDetection: Open mmlab detection toolbox and\nbenchmark. arXiv preprint arXiv: 1906.07155, 2019."
    },
    {
      "bounding_box": [
        {
          "x": 441,
          "y": 1408
        },
        {
          "x": 2108,
          "y": 1408
        },
        {
          "x": 2108,
          "y": 1548
        },
        {
          "x": 441,
          "y": 1548
        }
      ],
      "category": "paragraph",
      "html": "<p id='125' style='font-size:18px'>Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-<br>decoder with atrous separable convolution for semantic image segmentation. In Proceedings of<br>the European conference on computer vision (ECCV), pp. 801-818, 2018.</p>",
      "id": 125,
      "page": 10,
      "text": "Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-\ndecoder with atrous separable convolution for semantic image segmentation. In Proceedings of\nthe European conference on computer vision (ECCV), pp. 801-818, 2018."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 1581
        },
        {
          "x": 2106,
          "y": 1581
        },
        {
          "x": 2106,
          "y": 1673
        },
        {
          "x": 442,
          "y": 1673
        }
      ],
      "category": "paragraph",
      "html": "<p id='126' style='font-size:14px'>MMSegmentation Contributors. MMSegmentation: Openmmlab semantic segmentation toolbox<br>and benchmark. https : / / github · com/ open-mml ab / mmsegmentation, 2020.</p>",
      "id": 126,
      "page": 10,
      "text": "MMSegmentation Contributors. MMSegmentation: Openmmlab semantic segmentation toolbox\nand benchmark. https : / / github · com/ open-mml ab / mmsegmentation, 2020."
    },
    {
      "bounding_box": [
        {
          "x": 444,
          "y": 1710
        },
        {
          "x": 2105,
          "y": 1710
        },
        {
          "x": 2105,
          "y": 1801
        },
        {
          "x": 444,
          "y": 1801
        }
      ],
      "category": "paragraph",
      "html": "<p id='127' style='font-size:18px'>James W Cooley and John W Tukey. An algorithm for the machine calculation of complex fourier<br>series. Mathematics of computation, 19(90):297-301, 1965.</p>",
      "id": 127,
      "page": 10,
      "text": "James W Cooley and John W Tukey. An algorithm for the machine calculation of complex fourier\nseries. Mathematics of computation, 19(90):297-301, 1965."
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 1835
        },
        {
          "x": 2106,
          "y": 1835
        },
        {
          "x": 2106,
          "y": 1973
        },
        {
          "x": 443,
          "y": 1973
        }
      ],
      "category": "paragraph",
      "html": "<p id='128' style='font-size:16px'>Jean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. On the relationship between self-<br>attention and convolutional layers. In International Conference on Learning Representations,<br>2020. URL https : / / openreview · net / forum? id=HJlnC1 rKPB.</p>",
      "id": 128,
      "page": 10,
      "text": "Jean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. On the relationship between self-\nattention and convolutional layers. In International Conference on Learning Representations,\n2020. URL https : / / openreview · net / forum? id=HJlnC1 rKPB."
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 2006
        },
        {
          "x": 2109,
          "y": 2006
        },
        {
          "x": 2109,
          "y": 2191
        },
        {
          "x": 443,
          "y": 2191
        }
      ],
      "category": "paragraph",
      "html": "<p id='129' style='font-size:18px'>Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo<br>Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban<br>scene understanding. In Proceedings of the IEEE conference on computer vision and pattern<br>recognition, pp. 3213-3223, 2016.</p>",
      "id": 129,
      "page": 10,
      "text": "Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo\nBenenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban\nscene understanding. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pp. 3213-3223, 2016."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 2223
        },
        {
          "x": 2108,
          "y": 2223
        },
        {
          "x": 2108,
          "y": 2363
        },
        {
          "x": 442,
          "y": 2363
        }
      ],
      "category": "paragraph",
      "html": "<p id='130' style='font-size:16px'>Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated<br>data augmentation with a reduced search space. In Proceedings of the IEEE/CVF Conference on<br>Computer Vision and Pattern Recognition Workshops, pp. 702-703, 2020.</p>",
      "id": 130,
      "page": 10,
      "text": "Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated\ndata augmentation with a reduced search space. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition Workshops, pp. 702-703, 2020."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 2397
        },
        {
          "x": 2107,
          "y": 2397
        },
        {
          "x": 2107,
          "y": 2533
        },
        {
          "x": 442,
          "y": 2533
        }
      ],
      "category": "paragraph",
      "html": "<p id='131' style='font-size:18px'>Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdi-<br>nov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint<br>arXiv:1901.02860, 2019.</p>",
      "id": 131,
      "page": 10,
      "text": "Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdi-\nnov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint\narXiv:1901.02860, 2019."
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 2569
        },
        {
          "x": 2106,
          "y": 2569
        },
        {
          "x": 2106,
          "y": 2708
        },
        {
          "x": 443,
          "y": 2708
        }
      ],
      "category": "paragraph",
      "html": "<p id='132' style='font-size:18px'>Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-<br>erarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,<br>pp. 248-255. Ieee, 2009.</p>",
      "id": 132,
      "page": 10,
      "text": "Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-\nerarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,\npp. 248-255. Ieee, 2009."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 2741
        },
        {
          "x": 2106,
          "y": 2741
        },
        {
          "x": 2106,
          "y": 2833
        },
        {
          "x": 442,
          "y": 2833
        }
      ],
      "category": "paragraph",
      "html": "<p id='133' style='font-size:20px'>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep<br>bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.</p>",
      "id": 133,
      "page": 10,
      "text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018."
    },
    {
      "bounding_box": [
        {
          "x": 445,
          "y": 2869
        },
        {
          "x": 2109,
          "y": 2869
        },
        {
          "x": 2109,
          "y": 3053
        },
        {
          "x": 445,
          "y": 3053
        }
      ],
      "category": "paragraph",
      "html": "<p id='134' style='font-size:18px'>Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas<br>Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An<br>image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint<br>arXiv:2010.11929, 2020.</p>",
      "id": 134,
      "page": 10,
      "text": "Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An\nimage is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020."
    },
    {
      "bounding_box": [
        {
          "x": 1253,
          "y": 3133
        },
        {
          "x": 1301,
          "y": 3133
        },
        {
          "x": 1301,
          "y": 3171
        },
        {
          "x": 1253,
          "y": 3171
        }
      ],
      "category": "footer",
      "html": "<footer id='135' style='font-size:14px'>10</footer>",
      "id": 135,
      "page": 10,
      "text": "10"
    },
    {
      "bounding_box": [
        {
          "x": 444,
          "y": 111
        },
        {
          "x": 1223,
          "y": 111
        },
        {
          "x": 1223,
          "y": 157
        },
        {
          "x": 444,
          "y": 157
        }
      ],
      "category": "header",
      "html": "<header id='136' style='font-size:14px'>Published as a conference paper at ICLR 2022</header>",
      "id": 136,
      "page": 11,
      "text": "Published as a conference paper at ICLR 2022"
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 345
        },
        {
          "x": 2107,
          "y": 345
        },
        {
          "x": 2107,
          "y": 485
        },
        {
          "x": 442,
          "y": 485
        }
      ],
      "category": "paragraph",
      "html": "<p id='137' style='font-size:16px'>Maksim Dzabraev, Maksim Kalashnikov, Stepan Komkov, and Aleksandr Petiushko. Mdmmt: Mul-<br>tidomain multimodal transformer for video retrieval. In Proceedings of the IEEE/CVF Conference<br>on Computer Vision and Pattern Recognition, pp. 3354-3363, 2021.</p>",
      "id": 137,
      "page": 11,
      "text": "Maksim Dzabraev, Maksim Kalashnikov, Stepan Komkov, and Aleksandr Petiushko. Mdmmt: Mul-\ntidomain multimodal transformer for video retrieval. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 3354-3363, 2021."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 521
        },
        {
          "x": 2108,
          "y": 521
        },
        {
          "x": 2108,
          "y": 616
        },
        {
          "x": 442,
          "y": 616
        }
      ],
      "category": "paragraph",
      "html": "<p id='138' style='font-size:22px'>Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and<br>Christoph Feichtenhofer. Multiscale vision transformers. arXiv preprint arXiv:2104. 11227, 2021.</p>",
      "id": 138,
      "page": 11,
      "text": "Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and\nChristoph Feichtenhofer. Multiscale vision transformers. arXiv preprint arXiv:2104. 11227, 2021."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 650
        },
        {
          "x": 2107,
          "y": 650
        },
        {
          "x": 2107,
          "y": 789
        },
        {
          "x": 442,
          "y": 789
        }
      ],
      "category": "paragraph",
      "html": "<p id='139' style='font-size:20px'>Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang, and Hanqing Lu. Dual attention<br>network for scene segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision<br>and Pattern Recognition, pp. 3146-3154, 2019a.</p>",
      "id": 139,
      "page": 11,
      "text": "Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang, and Hanqing Lu. Dual attention\nnetwork for scene segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pp. 3146-3154, 2019a."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 826
        },
        {
          "x": 2107,
          "y": 826
        },
        {
          "x": 2107,
          "y": 963
        },
        {
          "x": 442,
          "y": 963
        }
      ],
      "category": "paragraph",
      "html": "<p id='140' style='font-size:22px'>Jun Fu, Jing Liu, Yuhang Wang, Yong Li, Yongjun Bao, Jinhui Tang, and Hanqing Lu. Adaptive<br>context network for scene parsing. In Proceedings of the IEEE/CVF International Conference on<br>Computer Vision, pp. 6748-6757, 2019b.</p>",
      "id": 140,
      "page": 11,
      "text": "Jun Fu, Jing Liu, Yuhang Wang, Yong Li, Yongjun Bao, Jinhui Tang, and Hanqing Lu. Adaptive\ncontext network for scene parsing. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision, pp. 6748-6757, 2019b."
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 1000
        },
        {
          "x": 2106,
          "y": 1000
        },
        {
          "x": 2106,
          "y": 1138
        },
        {
          "x": 443,
          "y": 1138
        }
      ],
      "category": "paragraph",
      "html": "<p id='141' style='font-size:18px'>Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid. Multi-modal transformer for<br>video retrieval. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK,<br>August 23-28, 2020, Proceedings, Part IV 16, pp. 214-229. Springer, 2020.</p>",
      "id": 141,
      "page": 11,
      "text": "Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid. Multi-modal transformer for\nvideo retrieval. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK,\nAugust 23-28, 2020, Proceedings, Part IV 16, pp. 214-229. Springer, 2020."
    },
    {
      "bounding_box": [
        {
          "x": 441,
          "y": 1174
        },
        {
          "x": 2108,
          "y": 1174
        },
        {
          "x": 2108,
          "y": 1313
        },
        {
          "x": 441,
          "y": 1313
        }
      ],
      "category": "paragraph",
      "html": "<p id='142' style='font-size:20px'>Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural<br>networks. In Proceedings of the thirteenth international conference on artificial intelligence and<br>statistics, pp. 249-256. JMLR Workshop and Conference Proceedings, 2010.</p>",
      "id": 142,
      "page": 11,
      "text": "Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural\nnetworks. In Proceedings of the thirteenth international conference on artificial intelligence and\nstatistics, pp. 249-256. JMLR Workshop and Conference Proceedings, 2010."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 1349
        },
        {
          "x": 2106,
          "y": 1349
        },
        {
          "x": 2106,
          "y": 1444
        },
        {
          "x": 442,
          "y": 1444
        }
      ],
      "category": "paragraph",
      "html": "<p id='143' style='font-size:20px'>Meng-Hao Guo, Zheng-Ning Liu, Tai-Jiang Mu, and Shi-Min Hu. Beyond self-attention: External<br>attention using two linear layers for visual tasks. arXiv preprint arXiv:2105.02358, 2021.</p>",
      "id": 143,
      "page": 11,
      "text": "Meng-Hao Guo, Zheng-Ning Liu, Tai-Jiang Mu, and Shi-Min Hu. Beyond self-attention: External\nattention using two linear layers for visual tasks. arXiv preprint arXiv:2105.02358, 2021."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 1479
        },
        {
          "x": 2106,
          "y": 1479
        },
        {
          "x": 2106,
          "y": 1616
        },
        {
          "x": 442,
          "y": 1616
        }
      ],
      "category": "paragraph",
      "html": "<p id='144' style='font-size:20px'>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-<br>nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.<br>770-778, 2016.</p>",
      "id": 144,
      "page": 11,
      "text": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.\n770-778, 2016."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 1654
        },
        {
          "x": 2105,
          "y": 1654
        },
        {
          "x": 2105,
          "y": 1747
        },
        {
          "x": 442,
          "y": 1747
        }
      ],
      "category": "paragraph",
      "html": "<p id='145' style='font-size:18px'>Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn. In Proceedings of the<br>IEEE international conference on computer vision, pp. 2961-2969, 2017.</p>",
      "id": 145,
      "page": 11,
      "text": "Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn. In Proceedings of the\nIEEE international conference on computer vision, pp. 2961-2969, 2017."
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 1783
        },
        {
          "x": 2106,
          "y": 1783
        },
        {
          "x": 2106,
          "y": 1920
        },
        {
          "x": 443,
          "y": 1920
        }
      ],
      "category": "paragraph",
      "html": "<p id='146' style='font-size:16px'>Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common cor-<br>ruptions and perturbations. Proceedings of the International Conference on Learning Represen-<br>tations, 2019.</p>",
      "id": 146,
      "page": 11,
      "text": "Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common cor-\nruptions and perturbations. Proceedings of the International Conference on Learning Represen-\ntations, 2019."
    },
    {
      "bounding_box": [
        {
          "x": 441,
          "y": 1957
        },
        {
          "x": 2106,
          "y": 1957
        },
        {
          "x": 2106,
          "y": 2049
        },
        {
          "x": 441,
          "y": 2049
        }
      ],
      "category": "paragraph",
      "html": "<p id='147' style='font-size:16px'>Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint<br>arXiv:1606.08415, 2016.</p>",
      "id": 147,
      "page": 11,
      "text": "Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint\narXiv:1606.08415, 2016."
    },
    {
      "bounding_box": [
        {
          "x": 441,
          "y": 2086
        },
        {
          "x": 2106,
          "y": 2086
        },
        {
          "x": 2106,
          "y": 2223
        },
        {
          "x": 441,
          "y": 2223
        }
      ],
      "category": "paragraph",
      "html": "<p id='148' style='font-size:22px'>Qibin Hou, Zihang Jiang, Li Yuan, Ming-Ming Cheng, Shuicheng Yan, and Jiashi Feng. Vi-<br>sion permutator: A permutable mlp-like architecture for visual recognition. arXiv preprint<br>arXiv:2106.12368, 2021.</p>",
      "id": 148,
      "page": 11,
      "text": "Qibin Hou, Zihang Jiang, Li Yuan, Ming-Ming Cheng, Shuicheng Yan, and Jiashi Feng. Vi-\nsion permutator: A permutable mlp-like architecture for visual recognition. arXiv preprint\narXiv:2106.12368, 2021."
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 2263
        },
        {
          "x": 2106,
          "y": 2263
        },
        {
          "x": 2106,
          "y": 2399
        },
        {
          "x": 443,
          "y": 2399
        }
      ],
      "category": "paragraph",
      "html": "<p id='149' style='font-size:20px'>Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand,<br>Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for<br>mobile vision applications. arXiv preprint arXiv:1704.04861, 2017.</p>",
      "id": 149,
      "page": 11,
      "text": "Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand,\nMarco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for\nmobile vision applications. arXiv preprint arXiv:1704.04861, 2017."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 2435
        },
        {
          "x": 2107,
          "y": 2435
        },
        {
          "x": 2107,
          "y": 2529
        },
        {
          "x": 442,
          "y": 2529
        }
      ],
      "category": "paragraph",
      "html": "<p id='150' style='font-size:20px'>Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with<br>stochastic depth. In European conference on computer vision, pp. 646-661. Springer, 2016.</p>",
      "id": 150,
      "page": 11,
      "text": "Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with\nstochastic depth. In European conference on computer vision, pp. 646-661. Springer, 2016."
    },
    {
      "bounding_box": [
        {
          "x": 444,
          "y": 2565
        },
        {
          "x": 2107,
          "y": 2565
        },
        {
          "x": 2107,
          "y": 2702
        },
        {
          "x": 444,
          "y": 2702
        }
      ],
      "category": "paragraph",
      "html": "<p id='151' style='font-size:18px'>Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q Weinberger. Densely connected<br>convolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern<br>Recognition, 2017.</p>",
      "id": 151,
      "page": 11,
      "text": "Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q Weinberger. Densely connected\nconvolutional networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2017."
    },
    {
      "bounding_box": [
        {
          "x": 441,
          "y": 2738
        },
        {
          "x": 2105,
          "y": 2738
        },
        {
          "x": 2105,
          "y": 2877
        },
        {
          "x": 441,
          "y": 2877
        }
      ],
      "category": "paragraph",
      "html": "<p id='152' style='font-size:20px'>Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, and Wenyu Liu. Ccnet:<br>Criss-cross attention for semantic segmentation. In Proceedings of the IEEE/CVF International<br>Conference on Computer Vision, pp. 603-612, 2019.</p>",
      "id": 152,
      "page": 11,
      "text": "Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, and Wenyu Liu. Ccnet:\nCriss-cross attention for semantic segmentation. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pp. 603-612, 2019."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 2914
        },
        {
          "x": 2106,
          "y": 2914
        },
        {
          "x": 2106,
          "y": 3052
        },
        {
          "x": 442,
          "y": 3052
        }
      ],
      "category": "paragraph",
      "html": "<p id='153' style='font-size:20px'>Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by<br>reducing internal covariate shift. In International conference on machine learning, pp. 448-456.<br>PMLR, 2015.</p>",
      "id": 153,
      "page": 11,
      "text": "Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. In International conference on machine learning, pp. 448-456.\nPMLR, 2015."
    },
    {
      "bounding_box": [
        {
          "x": 1252,
          "y": 3133
        },
        {
          "x": 1297,
          "y": 3133
        },
        {
          "x": 1297,
          "y": 3172
        },
        {
          "x": 1252,
          "y": 3172
        }
      ],
      "category": "footer",
      "html": "<footer id='154' style='font-size:14px'>11</footer>",
      "id": 154,
      "page": 11,
      "text": "11"
    },
    {
      "bounding_box": [
        {
          "x": 444,
          "y": 112
        },
        {
          "x": 1223,
          "y": 112
        },
        {
          "x": 1223,
          "y": 156
        },
        {
          "x": 444,
          "y": 156
        }
      ],
      "category": "header",
      "html": "<header id='155' style='font-size:14px'>Published as a conference paper at ICLR 2022</header>",
      "id": 155,
      "page": 12,
      "text": "Published as a conference paper at ICLR 2022"
    },
    {
      "bounding_box": [
        {
          "x": 444,
          "y": 347
        },
        {
          "x": 2107,
          "y": 347
        },
        {
          "x": 2107,
          "y": 483
        },
        {
          "x": 444,
          "y": 483
        }
      ],
      "category": "paragraph",
      "html": "<p id='156' style='font-size:16px'>Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr Dollar. Panoptic feature pyramid net-<br>works. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,<br>pp. 6399-6408, 2019.</p>",
      "id": 156,
      "page": 12,
      "text": "Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr Dollar. Panoptic feature pyramid net-\nworks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npp. 6399-6408, 2019."
    },
    {
      "bounding_box": [
        {
          "x": 444,
          "y": 517
        },
        {
          "x": 2106,
          "y": 517
        },
        {
          "x": 2106,
          "y": 651
        },
        {
          "x": 444,
          "y": 651
        }
      ],
      "category": "paragraph",
      "html": "<p id='157' style='font-size:16px'>Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep con-<br>volutional neural networks. Advances in neural information processing systems, 25:1097-1105,<br>2012.</p>",
      "id": 157,
      "page": 12,
      "text": "Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep con-\nvolutional neural networks. Advances in neural information processing systems, 25:1097-1105,\n2012."
    },
    {
      "bounding_box": [
        {
          "x": 444,
          "y": 686
        },
        {
          "x": 2106,
          "y": 686
        },
        {
          "x": 2106,
          "y": 822
        },
        {
          "x": 444,
          "y": 822
        }
      ],
      "category": "paragraph",
      "html": "<p id='158' style='font-size:16px'>Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hub-<br>bard, and Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition.<br>Neural computation, 1(4):541-551, 1989.</p>",
      "id": 158,
      "page": 12,
      "text": "Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne Hub-\nbard, and Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition.\nNeural computation, 1(4):541-551, 1989."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 854
        },
        {
          "x": 2106,
          "y": 854
        },
        {
          "x": 2106,
          "y": 991
        },
        {
          "x": 442,
          "y": 991
        }
      ],
      "category": "paragraph",
      "html": "<p id='159' style='font-size:22px'>Changlin Li, Tao Tang, Guangrun Wang, Jiefeng Peng, Bing Wang, Xiaodan Liang, and Xiaojun<br>Chang. Bossnas: Exploring hybrid cnn-transformers with block-wisely self-supervised neural<br>architecture search. arXiv preprint arXiv:2103.12424, 2021.</p>",
      "id": 159,
      "page": 12,
      "text": "Changlin Li, Tao Tang, Guangrun Wang, Jiefeng Peng, Bing Wang, Xiaodan Liang, and Xiaojun\nChang. Bossnas: Exploring hybrid cnn-transformers with block-wisely self-supervised neural\narchitecture search. arXiv preprint arXiv:2103.12424, 2021."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 1024
        },
        {
          "x": 2108,
          "y": 1024
        },
        {
          "x": 2108,
          "y": 1113
        },
        {
          "x": 442,
          "y": 1113
        }
      ],
      "category": "paragraph",
      "html": "<p id='160' style='font-size:18px'>Dongze Lian, Zehao Yu, Xing Sun, and Shenghua Gao. As-mlp: An axial shifted mlp architecture<br>for vision. arXiv preprint arXiv:2107.08391, 2021.</p>",
      "id": 160,
      "page": 12,
      "text": "Dongze Lian, Zehao Yu, Xing Sun, and Shenghua Gao. As-mlp: An axial shifted mlp architecture\nfor vision. arXiv preprint arXiv:2107.08391, 2021."
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 1146
        },
        {
          "x": 2104,
          "y": 1146
        },
        {
          "x": 2104,
          "y": 1236
        },
        {
          "x": 443,
          "y": 1236
        }
      ],
      "category": "paragraph",
      "html": "<p id='161' style='font-size:16px'>Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv preprint arXiv:1312.4400,<br>2013.</p>",
      "id": 161,
      "page": 12,
      "text": "Min Lin, Qiang Chen, and Shuicheng Yan. Network in network. arXiv preprint arXiv:1312.4400,\n2013."
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 1268
        },
        {
          "x": 2106,
          "y": 1268
        },
        {
          "x": 2106,
          "y": 1408
        },
        {
          "x": 443,
          "y": 1408
        }
      ],
      "category": "paragraph",
      "html": "<p id='162' style='font-size:16px'>Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr<br>Dollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European<br>conference on computer vision, pp. 740-755. Springer, 2014.</p>",
      "id": 162,
      "page": 12,
      "text": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDollar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European\nconference on computer vision, pp. 740-755. Springer, 2014."
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 1438
        },
        {
          "x": 2107,
          "y": 1438
        },
        {
          "x": 2107,
          "y": 1574
        },
        {
          "x": 443,
          "y": 1574
        }
      ],
      "category": "paragraph",
      "html": "<p id='163' style='font-size:16px'>Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal loss for dense<br>object detection. In Proceedings of the IEEE international conference on computer vision, pp.<br>2980-2988, 2017.</p>",
      "id": 163,
      "page": 12,
      "text": "Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal loss for dense\nobject detection. In Proceedings of the IEEE international conference on computer vision, pp.\n2980-2988, 2017."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 1608
        },
        {
          "x": 2104,
          "y": 1608
        },
        {
          "x": 2104,
          "y": 1698
        },
        {
          "x": 442,
          "y": 1698
        }
      ],
      "category": "paragraph",
      "html": "<p id='164' style='font-size:18px'>Hanxiao Liu, Zihang Dai, David R So, and Quoc V Le. Pay attention to mlps. arXiv preprint<br>arXiv:2105.08050, 2021a.</p>",
      "id": 164,
      "page": 12,
      "text": "Hanxiao Liu, Zihang Dai, David R So, and Quoc V Le. Pay attention to mlps. arXiv preprint\narXiv:2105.08050, 2021a."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 1731
        },
        {
          "x": 2106,
          "y": 1731
        },
        {
          "x": 2106,
          "y": 1868
        },
        {
          "x": 442,
          "y": 1868
        }
      ],
      "category": "paragraph",
      "html": "<p id='165' style='font-size:18px'>Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining<br>Guo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint<br>arXiv:2103.14030, 2021b.</p>",
      "id": 165,
      "page": 12,
      "text": "Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining\nGuo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint\narXiv:2103.14030, 2021b."
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 1900
        },
        {
          "x": 2107,
          "y": 1900
        },
        {
          "x": 2107,
          "y": 1991
        },
        {
          "x": 443,
          "y": 1991
        }
      ],
      "category": "paragraph",
      "html": "<p id='166' style='font-size:20px'>Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint<br>arXiv:1711.05101, 2017.</p>",
      "id": 166,
      "page": 12,
      "text": "Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101, 2017."
    },
    {
      "bounding_box": [
        {
          "x": 444,
          "y": 2023
        },
        {
          "x": 2106,
          "y": 2023
        },
        {
          "x": 2106,
          "y": 2116
        },
        {
          "x": 444,
          "y": 2116
        }
      ],
      "category": "paragraph",
      "html": "<p id='167' style='font-size:18px'>Xiaofeng Mao, Gege Qi, Yuefeng Chen, Xiaodan Li, Shaokai Ye, Yuan He, and Hui Xue. Rethinking<br>the design principles of robust vision transformer. arXiv preprint arXiv:2105.07926, 2021.</p>",
      "id": 167,
      "page": 12,
      "text": "Xiaofeng Mao, Gege Qi, Yuefeng Chen, Xiaodan Li, Shaokai Ye, Yuan He, and Hui Xue. Rethinking\nthe design principles of robust vision transformer. arXiv preprint arXiv:2105.07926, 2021."
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 2147
        },
        {
          "x": 2106,
          "y": 2147
        },
        {
          "x": 2106,
          "y": 2466
        },
        {
          "x": 443,
          "y": 2466
        }
      ],
      "category": "paragraph",
      "html": "<p id='168' style='font-size:20px'>Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor<br>Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward<br>Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,<br>Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance<br>deep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and<br>R. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp. 8024-8035. Curran<br>Associates, Inc., 2019.</p>",
      "id": 168,
      "page": 12,
      "text": "Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,\nLu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance\ndeep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and\nR. Garnett (eds.), Advances in Neural Information Processing Systems 32, pp. 8024-8035. Curran\nAssociates, Inc., 2019."
    },
    {
      "bounding_box": [
        {
          "x": 444,
          "y": 2499
        },
        {
          "x": 2106,
          "y": 2499
        },
        {
          "x": 2106,
          "y": 2635
        },
        {
          "x": 444,
          "y": 2635
        }
      ],
      "category": "paragraph",
      "html": "<p id='169' style='font-size:18px'>Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,<br>Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual<br>models from natural language supervision. arXiv preprint arXiv:2103.00020, 2021.</p>",
      "id": 169,
      "page": 12,
      "text": "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. arXiv preprint arXiv:2103.00020, 2021."
    },
    {
      "bounding_box": [
        {
          "x": 444,
          "y": 2667
        },
        {
          "x": 2106,
          "y": 2667
        },
        {
          "x": 2106,
          "y": 2760
        },
        {
          "x": 444,
          "y": 2760
        }
      ],
      "category": "paragraph",
      "html": "<p id='170' style='font-size:18px'>Yongming Rao, Wenliang Zhao, Zheng Zhu, Jiwen Lu, and Jie Zhou. Global filter networks for<br>image classification. arXiv preprint arXiv:2107.00645, 2021.</p>",
      "id": 170,
      "page": 12,
      "text": "Yongming Rao, Wenliang Zhao, Zheng Zhu, Jiwen Lu, and Jie Zhou. Global filter networks for\nimage classification. arXiv preprint arXiv:2107.00645, 2021."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 2791
        },
        {
          "x": 2105,
          "y": 2791
        },
        {
          "x": 2105,
          "y": 2881
        },
        {
          "x": 442,
          "y": 2881
        }
      ],
      "category": "paragraph",
      "html": "<p id='171' style='font-size:18px'>Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image<br>recognition. arXiv preprint arXiv:1409.1556, 2014.</p>",
      "id": 171,
      "page": 12,
      "text": "Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image\nrecognition. arXiv preprint arXiv:1409.1556, 2014."
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 2917
        },
        {
          "x": 2107,
          "y": 2917
        },
        {
          "x": 2107,
          "y": 3052
        },
        {
          "x": 443,
          "y": 3052
        }
      ],
      "category": "paragraph",
      "html": "<p id='172' style='font-size:16px'>Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon Shlens, Pieter Abbeel, and Ashish Vaswani.<br>Bottleneck transformers for visual recognition. In Proceedings of the IEEE/CVF Conference on<br>Computer Vision and Pattern Recognition, pp. 16519-16529, 2021.</p>",
      "id": 172,
      "page": 12,
      "text": "Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon Shlens, Pieter Abbeel, and Ashish Vaswani.\nBottleneck transformers for visual recognition. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 16519-16529, 2021."
    },
    {
      "bounding_box": [
        {
          "x": 1253,
          "y": 3133
        },
        {
          "x": 1299,
          "y": 3133
        },
        {
          "x": 1299,
          "y": 3171
        },
        {
          "x": 1253,
          "y": 3171
        }
      ],
      "category": "footer",
      "html": "<footer id='173' style='font-size:14px'>12</footer>",
      "id": 173,
      "page": 12,
      "text": "12"
    },
    {
      "bounding_box": [
        {
          "x": 444,
          "y": 112
        },
        {
          "x": 1224,
          "y": 112
        },
        {
          "x": 1224,
          "y": 156
        },
        {
          "x": 444,
          "y": 156
        }
      ],
      "category": "header",
      "html": "<header id='174' style='font-size:14px'>Published as a conference paper at ICLR 2022</header>",
      "id": 174,
      "page": 13,
      "text": "Published as a conference paper at ICLR 2022"
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 344
        },
        {
          "x": 2108,
          "y": 344
        },
        {
          "x": 2108,
          "y": 530
        },
        {
          "x": 443,
          "y": 530
        }
      ],
      "category": "paragraph",
      "html": "<p id='175' style='font-size:20px'>Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng Xu, Wei Zhan, Masayoshi Tomizuka,<br>Lei Li, Zehuan Yuan, Changhu Wang, et al. Sparse r-cnn: End-to-end object detection with<br>learnable proposals. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and Pattern<br>Recognition, pp. 14454-14463, 2021.</p>",
      "id": 175,
      "page": 13,
      "text": "Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng Xu, Wei Zhan, Masayoshi Tomizuka,\nLei Li, Zehuan Yuan, Changhu Wang, et al. Sparse r-cnn: End-to-end object detection with\nlearnable proposals. In Proceedings ofthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 14454-14463, 2021."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 566
        },
        {
          "x": 2107,
          "y": 566
        },
        {
          "x": 2107,
          "y": 706
        },
        {
          "x": 442,
          "y": 706
        }
      ],
      "category": "paragraph",
      "html": "<p id='176' style='font-size:18px'>Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-<br>mitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In<br>Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1-9, 2015.</p>",
      "id": 176,
      "page": 13,
      "text": "Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Du-\nmitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pp. 1-9, 2015."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 741
        },
        {
          "x": 2107,
          "y": 741
        },
        {
          "x": 2107,
          "y": 878
        },
        {
          "x": 442,
          "y": 878
        }
      ],
      "category": "paragraph",
      "html": "<p id='177' style='font-size:18px'>Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethink-<br>ing the inception architecture for computer vision. In Proceedings of the IEEE conference on<br>computer vision and pattern recognition, pp. 2818-2826, 2016.</p>",
      "id": 177,
      "page": 13,
      "text": "Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethink-\ning the inception architecture for computer vision. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pp. 2818-2826, 2016."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 914
        },
        {
          "x": 2107,
          "y": 914
        },
        {
          "x": 2107,
          "y": 1053
        },
        {
          "x": 442,
          "y": 1053
        }
      ],
      "category": "paragraph",
      "html": "<p id='178' style='font-size:18px'>Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Un-<br>terthiner, Jessica Yung, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, et al. Mlp-mixer: An<br>all-mlp architecture for vision. arXiv preprint arXiv:2105.01601, 2021.</p>",
      "id": 178,
      "page": 13,
      "text": "Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Un-\nterthiner, Jessica Yung, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, et al. Mlp-mixer: An\nall-mlp architecture for vision. arXiv preprint arXiv:2105.01601, 2021."
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 1089
        },
        {
          "x": 2107,
          "y": 1089
        },
        {
          "x": 2107,
          "y": 1228
        },
        {
          "x": 443,
          "y": 1228
        }
      ],
      "category": "paragraph",
      "html": "<p id='179' style='font-size:20px'>Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and<br>Herve Jegou. Training data-efficient image transformers & distillation through attention. arXiv<br>preprint arXiv:2012.12877, 2020.</p>",
      "id": 179,
      "page": 13,
      "text": "Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and\nHerve Jegou. Training data-efficient image transformers & distillation through attention. arXiv\npreprint arXiv:2012.12877, 2020."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 1261
        },
        {
          "x": 2107,
          "y": 1261
        },
        {
          "x": 2107,
          "y": 1445
        },
        {
          "x": 442,
          "y": 1445
        }
      ],
      "category": "paragraph",
      "html": "<p id='180' style='font-size:18px'>Hugo Touvron, Piotr Bojanowski, Mathilde Caron, Matthieu Cord, Alaaeldin El-Nouby, Edouard<br>Grave, Armand Joulin, Gabriel Synnaeve, Jakob Verbeek, and Herve Jegou. Resmlp: Feedforward<br>networks for image classification with data-efficient training. arXiv preprint arXiv:2105.03404,<br>2021a.</p>",
      "id": 180,
      "page": 13,
      "text": "Hugo Touvron, Piotr Bojanowski, Mathilde Caron, Matthieu Cord, Alaaeldin El-Nouby, Edouard\nGrave, Armand Joulin, Gabriel Synnaeve, Jakob Verbeek, and Herve Jegou. Resmlp: Feedforward\nnetworks for image classification with data-efficient training. arXiv preprint arXiv:2105.03404,\n2021a."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 1483
        },
        {
          "x": 2106,
          "y": 1483
        },
        {
          "x": 2106,
          "y": 1577
        },
        {
          "x": 442,
          "y": 1577
        }
      ],
      "category": "paragraph",
      "html": "<p id='181' style='font-size:22px'>Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Herve Jegou. Going<br>deeper with image transformers. arXiv preprint arXiv:2103.17239, 2021b.</p>",
      "id": 181,
      "page": 13,
      "text": "Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Herve Jegou. Going\ndeeper with image transformers. arXiv preprint arXiv:2103.17239, 2021b."
    },
    {
      "bounding_box": [
        {
          "x": 444,
          "y": 1612
        },
        {
          "x": 2106,
          "y": 1612
        },
        {
          "x": 2106,
          "y": 1751
        },
        {
          "x": 444,
          "y": 1751
        }
      ],
      "category": "paragraph",
      "html": "<p id='182' style='font-size:16px'>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,<br>Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information<br>processing systems, pp. 5998-6008, 2017.</p>",
      "id": 182,
      "page": 13,
      "text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information\nprocessing systems, pp. 5998-6008, 2017."
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 1787
        },
        {
          "x": 2106,
          "y": 1787
        },
        {
          "x": 2106,
          "y": 1924
        },
        {
          "x": 443,
          "y": 1924
        }
      ],
      "category": "paragraph",
      "html": "<p id='183' style='font-size:18px'>Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.<br>Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv<br>preprint arXiv:1804.07461, 2018.</p>",
      "id": 183,
      "page": 13,
      "text": "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman.\nGlue: A multi-task benchmark and analysis platform for natural language understanding. arXiv\npreprint arXiv:1804.07461, 2018."
    },
    {
      "bounding_box": [
        {
          "x": 444,
          "y": 1960
        },
        {
          "x": 2107,
          "y": 1960
        },
        {
          "x": 2107,
          "y": 2098
        },
        {
          "x": 444,
          "y": 2098
        }
      ],
      "category": "paragraph",
      "html": "<p id='184' style='font-size:20px'>Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo,<br>and Ling Shao. Pvtv2: Improved baselines with pyramid vision transformer. arXiv preprint<br>arXiv:2106.13797, 2021a.</p>",
      "id": 184,
      "page": 13,
      "text": "Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo,\nand Ling Shao. Pvtv2: Improved baselines with pyramid vision transformer. arXiv preprint\narXiv:2106.13797, 2021a."
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 2133
        },
        {
          "x": 2107,
          "y": 2133
        },
        {
          "x": 2107,
          "y": 2271
        },
        {
          "x": 443,
          "y": 2271
        }
      ],
      "category": "paragraph",
      "html": "<p id='185' style='font-size:20px'>Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo,<br>and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without<br>convolutions. arXiv preprint arXiv:2102.12122, 2021b.</p>",
      "id": 185,
      "page": 13,
      "text": "Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo,\nand Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without\nconvolutions. arXiv preprint arXiv:2102.12122, 2021b."
    },
    {
      "bounding_box": [
        {
          "x": 444,
          "y": 2308
        },
        {
          "x": 2108,
          "y": 2308
        },
        {
          "x": 2108,
          "y": 2447
        },
        {
          "x": 444,
          "y": 2447
        }
      ],
      "category": "paragraph",
      "html": "<p id='186' style='font-size:18px'>Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan Cheng, Hao Shen, and<br>Huaxia Xia. End-to-end video instance segmentation with transformers. In Proceedings of the<br>IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8741-8750, 2021c.</p>",
      "id": 186,
      "page": 13,
      "text": "Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan Cheng, Hao Shen, and\nHuaxia Xia. End-to-end video instance segmentation with transformers. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8741-8750, 2021c."
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 2481
        },
        {
          "x": 2101,
          "y": 2481
        },
        {
          "x": 2101,
          "y": 2574
        },
        {
          "x": 443,
          "y": 2574
        }
      ],
      "category": "paragraph",
      "html": "<p id='187' style='font-size:18px'>Ross Wightman. Pytorch image models. https : / /github · com/ rwightman/<br>pytorch- image-models, 2019.</p>",
      "id": 187,
      "page": 13,
      "text": "Ross Wightman. Pytorch image models. https : / /github · com/ rwightman/\npytorch- image-models, 2019."
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 2611
        },
        {
          "x": 2106,
          "y": 2611
        },
        {
          "x": 2106,
          "y": 2704
        },
        {
          "x": 443,
          "y": 2704
        }
      ],
      "category": "paragraph",
      "html": "<p id='188' style='font-size:18px'>Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt:<br>Introducing convolutions to vision transformers. arXiv preprint arXiv:2103.15808, 2021.</p>",
      "id": 188,
      "page": 13,
      "text": "Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt:\nIntroducing convolutions to vision transformers. arXiv preprint arXiv:2103.15808, 2021."
    },
    {
      "bounding_box": [
        {
          "x": 444,
          "y": 2739
        },
        {
          "x": 2106,
          "y": 2739
        },
        {
          "x": 2106,
          "y": 2878
        },
        {
          "x": 444,
          "y": 2878
        }
      ],
      "category": "paragraph",
      "html": "<p id='189' style='font-size:18px'>Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for<br>scene understanding. In Proceedings of the European Conference on Computer Vision (ECCV),<br>pp. 418-434, 2018.</p>",
      "id": 189,
      "page": 13,
      "text": "Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for\nscene understanding. In Proceedings of the European Conference on Computer Vision (ECCV),\npp. 418-434, 2018."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 2915
        },
        {
          "x": 2107,
          "y": 2915
        },
        {
          "x": 2107,
          "y": 3052
        },
        {
          "x": 442,
          "y": 3052
        }
      ],
      "category": "paragraph",
      "html": "<p id='190' style='font-size:18px'>Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Seg-<br>former: Simple and efficient design for semantic segmentation with transformers. arXiv preprint<br>arXiv:2105.15203, 2021.</p>",
      "id": 190,
      "page": 13,
      "text": "Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Seg-\nformer: Simple and efficient design for semantic segmentation with transformers. arXiv preprint\narXiv:2105.15203, 2021."
    },
    {
      "bounding_box": [
        {
          "x": 1253,
          "y": 3133
        },
        {
          "x": 1300,
          "y": 3133
        },
        {
          "x": 1300,
          "y": 3171
        },
        {
          "x": 1253,
          "y": 3171
        }
      ],
      "category": "footer",
      "html": "<footer id='191' style='font-size:14px'>13</footer>",
      "id": 191,
      "page": 13,
      "text": "13"
    },
    {
      "bounding_box": [
        {
          "x": 444,
          "y": 111
        },
        {
          "x": 1223,
          "y": 111
        },
        {
          "x": 1223,
          "y": 157
        },
        {
          "x": 444,
          "y": 157
        }
      ],
      "category": "header",
      "html": "<header id='192' style='font-size:14px'>Published as a conference paper at ICLR 2022</header>",
      "id": 192,
      "page": 14,
      "text": "Published as a conference paper at ICLR 2022"
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 345
        },
        {
          "x": 2108,
          "y": 345
        },
        {
          "x": 2108,
          "y": 484
        },
        {
          "x": 442,
          "y": 484
        }
      ],
      "category": "paragraph",
      "html": "<p id='193' style='font-size:18px'>Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and Kaiming He. Aggregated residual trans-<br>formations for deep neural networks. In Proceedings of the IEEE conference on computer vision<br>and pattern recognition, pp. 1492-1500, 2017.</p>",
      "id": 193,
      "page": 14,
      "text": "Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and Kaiming He. Aggregated residual trans-\nformations for deep neural networks. In Proceedings of the IEEE conference on computer vision\nand pattern recognition, pp. 1492-1500, 2017."
    },
    {
      "bounding_box": [
        {
          "x": 441,
          "y": 517
        },
        {
          "x": 2108,
          "y": 517
        },
        {
          "x": 2108,
          "y": 656
        },
        {
          "x": 441,
          "y": 656
        }
      ],
      "category": "paragraph",
      "html": "<p id='194' style='font-size:20px'>Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le.<br>Xlnet: Generalized autoregressive pretraining for language understanding. Advances in neural<br>information processing systems, 32, 2019.</p>",
      "id": 194,
      "page": 14,
      "text": "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le.\nXlnet: Generalized autoregressive pretraining for language understanding. Advances in neural\ninformation processing systems, 32, 2019."
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 686
        },
        {
          "x": 2106,
          "y": 686
        },
        {
          "x": 2106,
          "y": 824
        },
        {
          "x": 443,
          "y": 824
        }
      ],
      "category": "paragraph",
      "html": "<p id='195' style='font-size:22px'>Minghao Yin, Zhuliang Yao, Yue Cao, Xiu Li, Zheng Zhang, Stephen Lin, and Han Hu. Disen-<br>tangled non-local neural networks. In European Conference on Computer Vision, pp. 191-207.<br>Springer, 2020.</p>",
      "id": 195,
      "page": 14,
      "text": "Minghao Yin, Zhuliang Yao, Yue Cao, Xiu Li, Zheng Zhang, Stephen Lin, and Han Hu. Disen-\ntangled non-local neural networks. In European Conference on Computer Vision, pp. 191-207.\nSpringer, 2020."
    },
    {
      "bounding_box": [
        {
          "x": 441,
          "y": 857
        },
        {
          "x": 2105,
          "y": 857
        },
        {
          "x": 2105,
          "y": 947
        },
        {
          "x": 441,
          "y": 947
        }
      ],
      "category": "paragraph",
      "html": "<p id='196' style='font-size:16px'>Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. In ICLR,<br>2016.</p>",
      "id": 196,
      "page": 14,
      "text": "Fisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. In ICLR,\n2016."
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 981
        },
        {
          "x": 2107,
          "y": 981
        },
        {
          "x": 2107,
          "y": 1075
        },
        {
          "x": 443,
          "y": 1075
        }
      ],
      "category": "paragraph",
      "html": "<p id='197' style='font-size:22px'>Tan Yu, Xu Li, Yunfeng Cai, Mingming Sun, and Ping Li. S2-mlp: Spatial-shift mlp architecture for<br>vision. arXiv preprint arXiv:2106.07477, 2021.</p>",
      "id": 197,
      "page": 14,
      "text": "Tan Yu, Xu Li, Yunfeng Cai, Mingming Sun, and Ping Li. S2-mlp: Spatial-shift mlp architecture for\nvision. arXiv preprint arXiv:2106.07477, 2021."
    },
    {
      "bounding_box": [
        {
          "x": 440,
          "y": 1105
        },
        {
          "x": 2107,
          "y": 1105
        },
        {
          "x": 2107,
          "y": 1245
        },
        {
          "x": 440,
          "y": 1245
        }
      ],
      "category": "paragraph",
      "html": "<p id='198' style='font-size:20px'>Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay, Jiashi<br>Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on<br>imagenet. arXiv preprint arXiv:2101.11986, 2021.</p>",
      "id": 198,
      "page": 14,
      "text": "Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay, Jiashi\nFeng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on\nimagenet. arXiv preprint arXiv:2101.11986, 2021."
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 1276
        },
        {
          "x": 2107,
          "y": 1276
        },
        {
          "x": 2107,
          "y": 1415
        },
        {
          "x": 443,
          "y": 1415
        }
      ],
      "category": "paragraph",
      "html": "<p id='199' style='font-size:22px'>Yuhui Yuan, Xilin Chen, and Jingdong Wang. Object-contextual representations for semantic seg-<br>mentation. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August<br>23-28, 2020, Proceedings, Part VI 16, pp. 173-190. Springer, 2020.</p>",
      "id": 199,
      "page": 14,
      "text": "Yuhui Yuan, Xilin Chen, and Jingdong Wang. Object-contextual representations for semantic seg-\nmentation. In Computer Vision-ECCV 2020: 16th European Conference, Glasgow, UK, August\n23-28, 2020, Proceedings, Part VI 16, pp. 173-190. Springer, 2020."
    },
    {
      "bounding_box": [
        {
          "x": 441,
          "y": 1447
        },
        {
          "x": 2106,
          "y": 1447
        },
        {
          "x": 2106,
          "y": 1586
        },
        {
          "x": 441,
          "y": 1586
        }
      ],
      "category": "paragraph",
      "html": "<p id='200' style='font-size:20px'>Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo.<br>Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceed-<br>ings of the IEEE/CVF International Conference on Computer Vision, pp. 6023-6032, 2019.</p>",
      "id": 200,
      "page": 14,
      "text": "Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo.\nCutmix: Regularization strategy to train strong classifiers with localizable features. In Proceed-\nings of the IEEE/CVF International Conference on Computer Vision, pp. 6023-6032, 2019."
    },
    {
      "bounding_box": [
        {
          "x": 441,
          "y": 1618
        },
        {
          "x": 2105,
          "y": 1618
        },
        {
          "x": 2105,
          "y": 1708
        },
        {
          "x": 441,
          "y": 1708
        }
      ],
      "category": "paragraph",
      "html": "<p id='201' style='font-size:22px'>Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical<br>risk minimization. arXiv preprint arXiv:1710.09412, 2017.</p>",
      "id": 201,
      "page": 14,
      "text": "Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical\nrisk minimization. arXiv preprint arXiv:1710.09412, 2017."
    },
    {
      "bounding_box": [
        {
          "x": 441,
          "y": 1741
        },
        {
          "x": 2107,
          "y": 1741
        },
        {
          "x": 2107,
          "y": 1925
        },
        {
          "x": 441,
          "y": 1925
        }
      ],
      "category": "paragraph",
      "html": "<p id='202' style='font-size:20px'>Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei<br>Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from<br>a sequence-to-sequence perspective with transformers. In Proceedings of the IEEE/CVF Confer-<br>ence on Computer Vision and Pattern Recognition, pp. 6881-6890, 2021.</p>",
      "id": 202,
      "page": 14,
      "text": "Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei\nFu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from\na sequence-to-sequence perspective with transformers. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pp. 6881-6890, 2021."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 1957
        },
        {
          "x": 2103,
          "y": 1957
        },
        {
          "x": 2103,
          "y": 2052
        },
        {
          "x": 442,
          "y": 2052
        }
      ],
      "category": "paragraph",
      "html": "<p id='203' style='font-size:22px'>Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmen-<br>tation. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2020.</p>",
      "id": 203,
      "page": 14,
      "text": "Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmen-\ntation. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 2020."
    },
    {
      "bounding_box": [
        {
          "x": 441,
          "y": 2082
        },
        {
          "x": 2108,
          "y": 2082
        },
        {
          "x": 2108,
          "y": 2221
        },
        {
          "x": 441,
          "y": 2221
        }
      ],
      "category": "paragraph",
      "html": "<p id='204' style='font-size:20px'>Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene<br>parsing through ade20k dataset. In Proceedings of the IEEE conference on computer vision and<br>pattern recognition, pp. 633-641, 2017.</p>",
      "id": 204,
      "page": 14,
      "text": "Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene\nparsing through ade20k dataset. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pp. 633-641, 2017."
    },
    {
      "bounding_box": [
        {
          "x": 441,
          "y": 2252
        },
        {
          "x": 2106,
          "y": 2252
        },
        {
          "x": 2106,
          "y": 2347
        },
        {
          "x": 441,
          "y": 2347
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='205' style='font-size:20px'>Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr:<br>Deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020.</p>",
      "id": 205,
      "page": 14,
      "text": "Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr:\nDeformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020."
    },
    {
      "bounding_box": [
        {
          "x": 1253,
          "y": 3132
        },
        {
          "x": 1300,
          "y": 3132
        },
        {
          "x": 1300,
          "y": 3171
        },
        {
          "x": 1253,
          "y": 3171
        }
      ],
      "category": "footer",
      "html": "<footer id='206' style='font-size:16px'>14</footer>",
      "id": 206,
      "page": 14,
      "text": "14"
    },
    {
      "bounding_box": [
        {
          "x": 444,
          "y": 111
        },
        {
          "x": 1224,
          "y": 111
        },
        {
          "x": 1224,
          "y": 156
        },
        {
          "x": 444,
          "y": 156
        }
      ],
      "category": "header",
      "html": "<header id='207' style='font-size:14px'>Published as a conference paper at ICLR 2022</header>",
      "id": 207,
      "page": 15,
      "text": "Published as a conference paper at ICLR 2022"
    },
    {
      "bounding_box": [
        {
          "x": 447,
          "y": 342
        },
        {
          "x": 1233,
          "y": 342
        },
        {
          "x": 1233,
          "y": 393
        },
        {
          "x": 447,
          "y": 393
        }
      ],
      "category": "paragraph",
      "html": "<p id='208' style='font-size:22px'>A LITERATURE ON VISION MODEL</p>",
      "id": 208,
      "page": 15,
      "text": "A LITERATURE ON VISION MODEL"
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 443
        },
        {
          "x": 2108,
          "y": 443
        },
        {
          "x": 2108,
          "y": 995
        },
        {
          "x": 442,
          "y": 995
        }
      ],
      "category": "paragraph",
      "html": "<p id='209' style='font-size:18px'>CNN-based Models. Originally introduced over twenty years ago (LeCun et al., 1989),<br>convolutional neural networks (CNN) have been widely adopted since the success of the<br>AlexNet (Krizhevsky et al., 2012) which outperformed prevailing approaches based on hand-crafted<br>image features. There have been several attempts made to improve the design of CNN-based mod-<br>els. VGG (Simonyan & Zisserman, 2014) demonstrated a state-of-the-art performance on ImageNet<br>via deploying small (3 x 3) convolution kernels to all layers. He et al.introduced skip-connections in<br>ResNets (He et al., 2016), enabling a model variant with more than 1000 layers. DenseNet (Huang<br>et al., 2017) connected each layer to every other layer in a feed-forward fashion, strengthening fea-<br>ture propagation and reducing the number of parameters. In parallel with these architecture design<br>works, some other works also made significant contributions to the popularity of CNNs, including<br>normalization (Ioffe & Szegedy, 2015; Ba et al., 2016), data augmentation (Cubuk et al., 2020; Yun<br>et al., 2019; Zhang et al., 2017), etc.</p>",
      "id": 209,
      "page": 15,
      "text": "CNN-based Models. Originally introduced over twenty years ago (LeCun et al., 1989),\nconvolutional neural networks (CNN) have been widely adopted since the success of the\nAlexNet (Krizhevsky et al., 2012) which outperformed prevailing approaches based on hand-crafted\nimage features. There have been several attempts made to improve the design of CNN-based mod-\nels. VGG (Simonyan & Zisserman, 2014) demonstrated a state-of-the-art performance on ImageNet\nvia deploying small (3 x 3) convolution kernels to all layers. He et al.introduced skip-connections in\nResNets (He et al., 2016), enabling a model variant with more than 1000 layers. DenseNet (Huang\net al., 2017) connected each layer to every other layer in a feed-forward fashion, strengthening fea-\nture propagation and reducing the number of parameters. In parallel with these architecture design\nworks, some other works also made significant contributions to the popularity of CNNs, including\nnormalization (Ioffe & Szegedy, 2015; Ba et al., 2016), data augmentation (Cubuk et al., 2020; Yun\net al., 2019; Zhang et al., 2017), etc."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 1017
        },
        {
          "x": 2107,
          "y": 1017
        },
        {
          "x": 2107,
          "y": 1567
        },
        {
          "x": 442,
          "y": 1567
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='210' style='font-size:16px'>Transformer-based Models. Transformers were first proposed by Vaswani et al.for machine trans-<br>lation and have since become the dominant choice in many NLP tasks (Devlin et al., 2018; Wang<br>et al., 2018; Yang et al., 2019; Brown et al., 2020). Recently, transformer have also led to a series of<br>breakthroughs in computer vision community since the invention of ViT (Dosovitskiy et al., 2020),<br>and have been working as a de facto standard for various tasks, e.g., image classification (Dosovit-<br>skiy et al., 2020; Touvron et al., 2020; Yuan et al., 2021), detection and segmentation (Wang et al.,<br>2021b; Liu et al., 2021b; Zheng et al., 2021 ; Xie et al., 2021), video recognition (Wang et al., 2021c;<br>Bertasius et al., 2021 ; Arnab et al., 2021 ; Fan et al., 2021) and SO on. Moreover, there has also been<br>lots of interest in adopting transformer to cross aggregate multiple modality information (Radford<br>et al., 2021 ; Gabeur et al., 2020; Dzabraev et al., 2021). Furthermore, combining CNNs and trans-<br>formers is also explored in (Srinivas et al., 2021 ; Li et al., 2021; Wu et al., 2021; Touvron et al.,<br>2021b).</p>",
      "id": 210,
      "page": 15,
      "text": "Transformer-based Models. Transformers were first proposed by Vaswani et al.for machine trans-\nlation and have since become the dominant choice in many NLP tasks (Devlin et al., 2018; Wang\net al., 2018; Yang et al., 2019; Brown et al., 2020). Recently, transformer have also led to a series of\nbreakthroughs in computer vision community since the invention of ViT (Dosovitskiy et al., 2020),\nand have been working as a de facto standard for various tasks, e.g., image classification (Dosovit-\nskiy et al., 2020; Touvron et al., 2020; Yuan et al., 2021), detection and segmentation (Wang et al.,\n2021b; Liu et al., 2021b; Zheng et al., 2021 ; Xie et al., 2021), video recognition (Wang et al., 2021c;\nBertasius et al., 2021 ; Arnab et al., 2021 ; Fan et al., 2021) and SO on. Moreover, there has also been\nlots of interest in adopting transformer to cross aggregate multiple modality information (Radford\net al., 2021 ; Gabeur et al., 2020; Dzabraev et al., 2021). Furthermore, combining CNNs and trans-\nformers is also explored in (Srinivas et al., 2021 ; Li et al., 2021; Wu et al., 2021; Touvron et al.,\n2021b)."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 1589
        },
        {
          "x": 2109,
          "y": 1589
        },
        {
          "x": 2109,
          "y": 2508
        },
        {
          "x": 442,
          "y": 2508
        }
      ],
      "category": "paragraph",
      "html": "<p id='211' style='font-size:16px'>MLP-based Models. MLP-based models (Tolstikhin et al., 2021; Touvron et al., 2021a; Liu et al.,<br>2021a) differ from the above discussed CNN- and Transformer-based models because they resort<br>to neither convolution nor self-attention layers. Instead, they use MLP layers over feature patches<br>on spatial dimensions to aggregate the spatial context. These MLP-based models share similar<br>macro structures but differ from each other in the detailed design of the micro block. In addition,<br>MLP-based models provide more efficient computation than transformer-based models since they<br>do not need to calculate affinity matrix using key-query multiplication. Concurrent to our work,<br>S2-MLP (Yu et al., 2021) utilizes a spatial-shift operation for spatial information communication.<br>The similar aspect between our work and S2-MLP lies in that we all conduct MLP operations along<br>the channel dimension. However, our Cycle FC is different from S2-MLP in: (1) S2-MLP achieves<br>communications between patches by splitting feature maps along channel dimension into several<br>groups and shifting different groups in different directions. It introduces extra splitting and shifting<br>operations on the feature map. On the contrary, we propose a novel operator-Cycle Fully-Connected<br>Layer-for spatial context aggregation. It does not modify the feature map and is formulated as a<br>generic, plug-and-play MLP unit that can be used as a direct replacement of vanilla without any<br>adjustments. (2) We design a pyramid structure for and conduct extensive experiments on classi-<br>fication, object detection, instance segmentation, and semantic segmentation. However, the output<br>feature map of S2-MLP has only one single scale in low resolution, which is unsuitable for dense<br>prediction tasks. Only ImageNet classification is evaluated on S2-MLP. We compared Cycle FC<br>with S2-MLP in details in the Section 3.</p>",
      "id": 211,
      "page": 15,
      "text": "MLP-based Models. MLP-based models (Tolstikhin et al., 2021; Touvron et al., 2021a; Liu et al.,\n2021a) differ from the above discussed CNN- and Transformer-based models because they resort\nto neither convolution nor self-attention layers. Instead, they use MLP layers over feature patches\non spatial dimensions to aggregate the spatial context. These MLP-based models share similar\nmacro structures but differ from each other in the detailed design of the micro block. In addition,\nMLP-based models provide more efficient computation than transformer-based models since they\ndo not need to calculate affinity matrix using key-query multiplication. Concurrent to our work,\nS2-MLP (Yu et al., 2021) utilizes a spatial-shift operation for spatial information communication.\nThe similar aspect between our work and S2-MLP lies in that we all conduct MLP operations along\nthe channel dimension. However, our Cycle FC is different from S2-MLP in: (1) S2-MLP achieves\ncommunications between patches by splitting feature maps along channel dimension into several\ngroups and shifting different groups in different directions. It introduces extra splitting and shifting\noperations on the feature map. On the contrary, we propose a novel operator-Cycle Fully-Connected\nLayer-for spatial context aggregation. It does not modify the feature map and is formulated as a\ngeneric, plug-and-play MLP unit that can be used as a direct replacement of vanilla without any\nadjustments. (2) We design a pyramid structure for and conduct extensive experiments on classi-\nfication, object detection, instance segmentation, and semantic segmentation. However, the output\nfeature map of S2-MLP has only one single scale in low resolution, which is unsuitable for dense\nprediction tasks. Only ImageNet classification is evaluated on S2-MLP. We compared Cycle FC\nwith S2-MLP in details in the Section 3."
    },
    {
      "bounding_box": [
        {
          "x": 446,
          "y": 2647
        },
        {
          "x": 1207,
          "y": 2647
        },
        {
          "x": 1207,
          "y": 2698
        },
        {
          "x": 446,
          "y": 2698
        }
      ],
      "category": "paragraph",
      "html": "<p id='212' style='font-size:22px'>B COMPARISON OF MLP BLOCKS</p>",
      "id": 212,
      "page": 15,
      "text": "B COMPARISON OF MLP BLOCKS"
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 2750
        },
        {
          "x": 2107,
          "y": 2750
        },
        {
          "x": 2107,
          "y": 2937
        },
        {
          "x": 443,
          "y": 2937
        }
      ],
      "category": "paragraph",
      "html": "<p id='213' style='font-size:18px'>We summary MLP blocks proposed by recent MLP-related works in Figure 5. We notice that ex-<br>isting MLP blocks, i.e., MLP-Mixer, ResMLP and gMLP share similar method of Spatial Proj:<br>Transpose → Fully-Connected over spatial dimension → Transpose back. These models can not<br>cope with variable image scales as the FC layers in Spatial Proj are configured by the seq_len.</p>",
      "id": 213,
      "page": 15,
      "text": "We summary MLP blocks proposed by recent MLP-related works in Figure 5. We notice that ex-\nisting MLP blocks, i.e., MLP-Mixer, ResMLP and gMLP share similar method of Spatial Proj:\nTranspose → Fully-Connected over spatial dimension → Transpose back. These models can not\ncope with variable image scales as the FC layers in Spatial Proj are configured by the seq_len."
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 2960
        },
        {
          "x": 2106,
          "y": 2960
        },
        {
          "x": 2106,
          "y": 3054
        },
        {
          "x": 443,
          "y": 3054
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='214' style='font-size:20px'>The blocks used for building CycleMLP consist of our proposed novel Cycle FC, whose configura-<br>tion has nothing to do with image scales and can naturally deal with dynamic image scales.</p>",
      "id": 214,
      "page": 15,
      "text": "The blocks used for building CycleMLP consist of our proposed novel Cycle FC, whose configura-\ntion has nothing to do with image scales and can naturally deal with dynamic image scales."
    },
    {
      "bounding_box": [
        {
          "x": 1253,
          "y": 3132
        },
        {
          "x": 1300,
          "y": 3132
        },
        {
          "x": 1300,
          "y": 3171
        },
        {
          "x": 1253,
          "y": 3171
        }
      ],
      "category": "footer",
      "html": "<footer id='215' style='font-size:16px'>15</footer>",
      "id": 215,
      "page": 15,
      "text": "15"
    },
    {
      "bounding_box": [
        {
          "x": 445,
          "y": 113
        },
        {
          "x": 1224,
          "y": 113
        },
        {
          "x": 1224,
          "y": 156
        },
        {
          "x": 445,
          "y": 156
        }
      ],
      "category": "header",
      "html": "<header id='216' style='font-size:16px'>Published as a conference paper at ICLR 2022</header>",
      "id": 216,
      "page": 16,
      "text": "Published as a conference paper at ICLR 2022"
    },
    {
      "bounding_box": [
        {
          "x": 563,
          "y": 345
        },
        {
          "x": 1986,
          "y": 345
        },
        {
          "x": 1986,
          "y": 2035
        },
        {
          "x": 563,
          "y": 2035
        }
      ],
      "category": "figure",
      "html": "<figure><img id='217' style='font-size:14px' alt=\"FC(4*dim, dim) FC(4*dim, dim)\nAffine\nChannel Proj GeLU Channel Proj GeLU\n↑ ↑\nLayerNorm FC(dim, 4*dim) Affine (Bias) FC(dim, 4*dim)\nTranspose(1, 2)\nAffine Transpose(1, 2)\nFC(0.5*seq_ 1en, seq_len)\nSpatial Proj Spatial Proj FC(seq_len, seq_len)\nGeLU\nLayerNorm Affine (Bias) Transpose(1, 2)\nFC(seq_len, 0.5*seq_len)\nTranspose(1, 2)\nInput Embeddings Input Embeddings\n(a) MLP-Mixer (Tolstikhin et al., 2021) (b) ResMLP (Touvron et al., 2021a)\nChannel Proj FC(6*dim, dim) ↑\nFC(r*dim, dim)\nTranspose(1, 2)\nChannel Proj GeLU\n↑\nSpatial Proj FC(seq_ len, seq_len)\nLayerNorm FC(dim, r*dim)\nLayerNorm Transpose(1, 2)\nLinear\n↑\nSplit\nGeLU Sum\nChannel Proj Spatial Proj ↑\nFC(dim, 6*dim)\nLayerNorm LayerNorm 1x1 1x1 1x1\nInput Embeddings Input Embeddings\n(c) gMLP (Liu et al., 2021a) (d) CycleMLP (Ours)\" data-coord=\"top-left:(563,345); bottom-right:(1986,2035)\" /></figure>",
      "id": 217,
      "page": 16,
      "text": "FC(4*dim, dim) FC(4*dim, dim)\nAffine\nChannel Proj GeLU Channel Proj GeLU\n↑ ↑\nLayerNorm FC(dim, 4*dim) Affine (Bias) FC(dim, 4*dim)\nTranspose(1, 2)\nAffine Transpose(1, 2)\nFC(0.5*seq_ 1en, seq_len)\nSpatial Proj Spatial Proj FC(seq_len, seq_len)\nGeLU\nLayerNorm Affine (Bias) Transpose(1, 2)\nFC(seq_len, 0.5*seq_len)\nTranspose(1, 2)\nInput Embeddings Input Embeddings\n(a) MLP-Mixer (Tolstikhin et al., 2021) (b) ResMLP (Touvron et al., 2021a)\nChannel Proj FC(6*dim, dim) ↑\nFC(r*dim, dim)\nTranspose(1, 2)\nChannel Proj GeLU\n↑\nSpatial Proj FC(seq_ len, seq_len)\nLayerNorm FC(dim, r*dim)\nLayerNorm Transpose(1, 2)\nLinear\n↑\nSplit\nGeLU Sum\nChannel Proj Spatial Proj ↑\nFC(dim, 6*dim)\nLayerNorm LayerNorm 1x1 1x1 1x1\nInput Embeddings Input Embeddings\n(c) gMLP (Liu et al., 2021a) (d) CycleMLP (Ours)"
    },
    {
      "bounding_box": [
        {
          "x": 870,
          "y": 2057
        },
        {
          "x": 1677,
          "y": 2057
        },
        {
          "x": 1677,
          "y": 2103
        },
        {
          "x": 870,
          "y": 2103
        }
      ],
      "category": "caption",
      "html": "<br><caption id='218' style='font-size:20px'>Figure 5: Comparison of MLP blocks in details.</caption>",
      "id": 218,
      "page": 16,
      "text": "Figure 5: Comparison of MLP blocks in details."
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 2200
        },
        {
          "x": 1727,
          "y": 2200
        },
        {
          "x": 1727,
          "y": 2253
        },
        {
          "x": 443,
          "y": 2253
        }
      ],
      "category": "paragraph",
      "html": "<p id='219' style='font-size:20px'>C FROM MULTI-HEAD SELF-ATTENTION TO CONVOLUTION</p>",
      "id": 219,
      "page": 16,
      "text": "C FROM MULTI-HEAD SELF-ATTENTION TO CONVOLUTION"
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 2303
        },
        {
          "x": 2106,
          "y": 2303
        },
        {
          "x": 2106,
          "y": 2395
        },
        {
          "x": 442,
          "y": 2395
        }
      ],
      "category": "paragraph",
      "html": "<p id='220' style='font-size:16px'>In this section, we provide details in how MHSA can be transferred into a convolution-like operator<br>in equation 3. To start with, the a MHSA layer can be formulated as below:</p>",
      "id": 220,
      "page": 16,
      "text": "In this section, we provide details in how MHSA can be transferred into a convolution-like operator\nin equation 3. To start with, the a MHSA layer can be formulated as below:"
    },
    {
      "bounding_box": [
        {
          "x": 443,
          "y": 2576
        },
        {
          "x": 2104,
          "y": 2576
        },
        {
          "x": 2104,
          "y": 2723
        },
        {
          "x": 443,
          "y": 2723
        }
      ],
      "category": "paragraph",
      "html": "<p id='221' style='font-size:18px'>where Wout R(NhCout)xC'out and b E RC'out are parameters for the final linear projection. SAh<br>E<br>is the hth self-attention module. Then we reshape X into X E RHW xCin and let T = H x W,<br>which indicates that there are T tokens in X. SAh can be defined as follow:</p>",
      "id": 221,
      "page": 16,
      "text": "where Wout R(NhCout)xC'out and b E RC'out are parameters for the final linear projection. SAh\nE\nis the hth self-attention module. Then we reshape X into X E RHW xCin and let T = H x W,\nwhich indicates that there are T tokens in X. SAh can be defined as follow:"
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 2955
        },
        {
          "x": 2107,
          "y": 2955
        },
        {
          "x": 2107,
          "y": 3051
        },
        {
          "x": 442,
          "y": 3051
        }
      ],
      "category": "paragraph",
      "html": "<p id='222' style='font-size:18px'>where V = XWval Q = XWqry K = XWkey are respectively the value, query and key matrix<br>with learnable matrices Wu E RCin xCout, Wq E R Cin XCk Wk E RCin xCk. P E RT xCin is the</p>",
      "id": 222,
      "page": 16,
      "text": "where V = XWval Q = XWqry K = XWkey are respectively the value, query and key matrix\nwith learnable matrices Wu E RCin xCout, Wq E R Cin XCk Wk E RCin xCk. P E RT xCin is the"
    },
    {
      "bounding_box": [
        {
          "x": 1253,
          "y": 3134
        },
        {
          "x": 1300,
          "y": 3134
        },
        {
          "x": 1300,
          "y": 3172
        },
        {
          "x": 1253,
          "y": 3172
        }
      ],
      "category": "footer",
      "html": "<footer id='223' style='font-size:18px'>16</footer>",
      "id": 223,
      "page": 16,
      "text": "16"
    },
    {
      "bounding_box": [
        {
          "x": 444,
          "y": 111
        },
        {
          "x": 1224,
          "y": 111
        },
        {
          "x": 1224,
          "y": 157
        },
        {
          "x": 444,
          "y": 157
        }
      ],
      "category": "header",
      "html": "<header id='224' style='font-size:14px'>Published as a conference paper at ICLR 2022</header>",
      "id": 224,
      "page": 17,
      "text": "Published as a conference paper at ICLR 2022"
    },
    {
      "bounding_box": [
        {
          "x": 441,
          "y": 347
        },
        {
          "x": 2108,
          "y": 347
        },
        {
          "x": 2108,
          "y": 573
        },
        {
          "x": 441,
          "y": 573
        }
      ],
      "category": "paragraph",
      "html": "<p id='225' style='font-size:16px'>positional embedding matrix containing positional information for every input token, which can be<br>replaced by the output of any function fp that encodes the position of tokens. And A E RTxT is the<br>attention matrix where each element Ai,j is the attention score between the ith and jth token in X.<br>With absolute positional encoding, the second line in equation 6 can be expanded as (Cordonnier<br>et al., 2020):</p>",
      "id": 225,
      "page": 17,
      "text": "positional embedding matrix containing positional information for every input token, which can be\nreplaced by the output of any function fp that encodes the position of tokens. And A E RTxT is the\nattention matrix where each element Ai,j is the attention score between the ith and jth token in X.\nWith absolute positional encoding, the second line in equation 6 can be expanded as (Cordonnier\net al., 2020):"
    },
    {
      "bounding_box": [
        {
          "x": 453,
          "y": 779
        },
        {
          "x": 2101,
          "y": 779
        },
        {
          "x": 2101,
          "y": 831
        },
        {
          "x": 453,
          "y": 831
        }
      ],
      "category": "paragraph",
      "html": "<p id='226' style='font-size:16px'>When we apply relative positional encoding scheme in (Dai et al., 2019), A is re-parametried into:</p>",
      "id": 226,
      "page": 17,
      "text": "When we apply relative positional encoding scheme in (Dai et al., 2019), A is re-parametried into:"
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 1025
        },
        {
          "x": 2107,
          "y": 1025
        },
        {
          "x": 2107,
          "y": 1430
        },
        {
          "x": 442,
          "y": 1430
        }
      ],
      "category": "paragraph",
      "html": "<p id='227' style='font-size:16px'>where r�q,k is a positional encoding for relative distance Sq,k = (81,82) between token q and k in X.<br>Wkey is introduced to only pertain to the positional encoding rq,k. u and v are learnable parameter<br>vectors that replace the original Pq,: Wqry term, which implies that the attention bias remains the<br>same regardless of the absolution positions of the query. If we set Wqry = Wkey = 0 and<br>Wkey = I, the first three terms in equation 8 vanish and Aq,k = vrq,k. We set {△i(h), △j(h)} =<br>{(0,0), (1,0), (-1, 0), · · · } contains all possible positional shift in convolution with kernel size<br>VNh x VNh. For each head h, let rq,k = (I|Sq,k 112, 81, 82) and vh = -ah(1, -2△i(h), -2△j(h)),<br>each softmax attention matrix becomes:</p>",
      "id": 227,
      "page": 17,
      "text": "where r�q,k is a positional encoding for relative distance Sq,k = (81,82) between token q and k in X.\nWkey is introduced to only pertain to the positional encoding rq,k. u and v are learnable parameter\nvectors that replace the original Pq,: Wqry term, which implies that the attention bias remains the\nsame regardless of the absolution positions of the query. If we set Wqry = Wkey = 0 and\nWkey = I, the first three terms in equation 8 vanish and Aq,k = vrq,k. We set {△i(h), △j(h)} =\n{(0,0), (1,0), (-1, 0), · · · } contains all possible positional shift in convolution with kernel size\nVNh x VNh. For each head h, let rq,k = (I|Sq,k 112, 81, 82) and vh = -ah(1, -2△i(h), -2△j(h)),\neach softmax attention matrix becomes:"
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 1636
        },
        {
          "x": 1315,
          "y": 1636
        },
        {
          "x": 1315,
          "y": 1691
        },
        {
          "x": 442,
          "y": 1691
        }
      ],
      "category": "paragraph",
      "html": "<p id='228' style='font-size:16px'>Substitute softmax(. Ah) into equation 5 and we get</p>",
      "id": 228,
      "page": 17,
      "text": "Substitute softmax(. Ah) into equation 5 and we get"
    },
    {
      "bounding_box": [
        {
          "x": 446,
          "y": 1913
        },
        {
          "x": 1117,
          "y": 1913
        },
        {
          "x": 1117,
          "y": 1965
        },
        {
          "x": 446,
          "y": 1965
        }
      ],
      "category": "paragraph",
      "html": "<p id='229' style='font-size:22px'>D ARCHITECTURE VARIANTS</p>",
      "id": 229,
      "page": 17,
      "text": "D ARCHITECTURE VARIANTS"
    },
    {
      "bounding_box": [
        {
          "x": 441,
          "y": 2016
        },
        {
          "x": 2108,
          "y": 2016
        },
        {
          "x": 2108,
          "y": 2293
        },
        {
          "x": 441,
          "y": 2293
        }
      ],
      "category": "paragraph",
      "html": "<p id='230' style='font-size:14px'>In order to conduct fair and convenient comparison, we build two model zoos: the one is in PVT-<br>Style (named as CycleMLP-B1 to -B5) and the other in Swin-Style (named as CycleMLP-T, -S and -<br>B). These models are scaled up by adapting several architecture-related hyper-parameters, including<br>Si, Ci, Ei and Li which represent the stride of the transition, the token channel dimension, the<br>number of blocks and the expansion ratio respectively at Stage i. Detailed configurations of these<br>models are in Table 11.</p>",
      "id": 230,
      "page": 17,
      "text": "In order to conduct fair and convenient comparison, we build two model zoos: the one is in PVT-\nStyle (named as CycleMLP-B1 to -B5) and the other in Swin-Style (named as CycleMLP-T, -S and -\nB). These models are scaled up by adapting several architecture-related hyper-parameters, including\nSi, Ci, Ei and Li which represent the stride of the transition, the token channel dimension, the\nnumber of blocks and the expansion ratio respectively at Stage i. Detailed configurations of these\nmodels are in Table 11."
    },
    {
      "bounding_box": [
        {
          "x": 446,
          "y": 2359
        },
        {
          "x": 1045,
          "y": 2359
        },
        {
          "x": 1045,
          "y": 2412
        },
        {
          "x": 446,
          "y": 2412
        }
      ],
      "category": "paragraph",
      "html": "<p id='231' style='font-size:18px'>E EXPERIMENTAL SETUPS</p>",
      "id": 231,
      "page": 17,
      "text": "E EXPERIMENTAL SETUPS"
    },
    {
      "bounding_box": [
        {
          "x": 444,
          "y": 2461
        },
        {
          "x": 1063,
          "y": 2461
        },
        {
          "x": 1063,
          "y": 2510
        },
        {
          "x": 444,
          "y": 2510
        }
      ],
      "category": "paragraph",
      "html": "<p id='232' style='font-size:14px'>E.1 IMAGENET CLASSIFICATION</p>",
      "id": 232,
      "page": 17,
      "text": "E.1 IMAGENET CLASSIFICATION"
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 2547
        },
        {
          "x": 2106,
          "y": 2547
        },
        {
          "x": 2106,
          "y": 3053
        },
        {
          "x": 442,
          "y": 3053
        }
      ],
      "category": "paragraph",
      "html": "<p id='233' style='font-size:16px'>Settings. We train our models on the ImageNet-1K dataset (Deng et al., 2009), which contains<br>1.2M training images and 50K validation images evenly spreading 1,000 categories. We follow<br>the standard practice in the community by reporting the top-1 accuracy on the validation set. Our<br>code is implemented based on PyTorch (Paszke et al., 2019) framework and heavily relies on the<br>timm (Wightman, 2019) repository. For apple-to-apple comparison, our training strategy is mostly<br>adopted from DeiT (Touvron et al., 2020), which includes RandAugment (Cubuk et al., 2020),<br>Mixup (Zhang et al., 2017), Cutmix (Yun et al., 2019) random erasing (Zhong et al., 2020) and<br>stochastic depth (Huang et al., 2016). The optimizer is AdamW (Loshchilov & Hutter, 2017) with<br>the momentum of 0.9 and weight decay of 5x10-2 by default. The cosine learning rate schedule is<br>adopted with the initial value of 1x10-3 · All models are trained for 300 epochs on 8 Tesla V100<br>GPUs with a total batch size of 1024.</p>",
      "id": 233,
      "page": 17,
      "text": "Settings. We train our models on the ImageNet-1K dataset (Deng et al., 2009), which contains\n1.2M training images and 50K validation images evenly spreading 1,000 categories. We follow\nthe standard practice in the community by reporting the top-1 accuracy on the validation set. Our\ncode is implemented based on PyTorch (Paszke et al., 2019) framework and heavily relies on the\ntimm (Wightman, 2019) repository. For apple-to-apple comparison, our training strategy is mostly\nadopted from DeiT (Touvron et al., 2020), which includes RandAugment (Cubuk et al., 2020),\nMixup (Zhang et al., 2017), Cutmix (Yun et al., 2019) random erasing (Zhong et al., 2020) and\nstochastic depth (Huang et al., 2016). The optimizer is AdamW (Loshchilov & Hutter, 2017) with\nthe momentum of 0.9 and weight decay of 5x10-2 by default. The cosine learning rate schedule is\nadopted with the initial value of 1x10-3 · All models are trained for 300 epochs on 8 Tesla V100\nGPUs with a total batch size of 1024."
    },
    {
      "bounding_box": [
        {
          "x": 1253,
          "y": 3133
        },
        {
          "x": 1299,
          "y": 3133
        },
        {
          "x": 1299,
          "y": 3171
        },
        {
          "x": 1253,
          "y": 3171
        }
      ],
      "category": "footer",
      "html": "<footer id='234' style='font-size:14px'>17</footer>",
      "id": 234,
      "page": 17,
      "text": "17"
    },
    {
      "bounding_box": [
        {
          "x": 444,
          "y": 111
        },
        {
          "x": 1225,
          "y": 111
        },
        {
          "x": 1225,
          "y": 157
        },
        {
          "x": 444,
          "y": 157
        }
      ],
      "category": "header",
      "html": "<header id='235' style='font-size:14px'>Published as a conference paper at ICLR 2022</header>",
      "id": 235,
      "page": 18,
      "text": "Published as a conference paper at ICLR 2022"
    },
    {
      "bounding_box": [
        {
          "x": 461,
          "y": 341
        },
        {
          "x": 2081,
          "y": 341
        },
        {
          "x": 2081,
          "y": 1235
        },
        {
          "x": 461,
          "y": 1235
        }
      ],
      "category": "table",
      "html": "<table id='236' style='font-size:14px'><tr><td rowspan=\"2\"></td><td rowspan=\"2\">Output Size</td><td rowspan=\"2\">Layer Name</td><td colspan=\"5\">PVT-Style (Wang et al., 2021b)</td><td colspan=\"3\">Swin-Style (Liu etal., 2021b)</td></tr><tr><td>B1</td><td>B2</td><td>B3</td><td>B4</td><td>B5</td><td>Tiny</td><td>Small</td><td>Base</td></tr><tr><td rowspan=\"3\">Stage 1</td><td rowspan=\"3\">H W x 4 4</td><td rowspan=\"2\">Overlapping Patch Embedding</td><td colspan=\"5\">S1 = 4</td><td colspan=\"3\">S1 = 4</td></tr><tr><td colspan=\"4\">C1 = 64</td><td>C1 = 96</td><td colspan=\"2\">C1 = 96</td><td>C1 = 128</td></tr><tr><td>CycleMLP Block</td><td>E1 = 4 L1 = 2</td><td>E1 = 4 L1 = 2</td><td>E1 = 8 L1 = 3</td><td>E1 = 8 L1 = 3</td><td>E1 = 4 L1 = 3</td><td>E1 = 4 L1 = 2</td><td>E1 = 4 L1 = 2</td><td>E1 = 4 L1 = 2</td></tr><tr><td rowspan=\"3\">Stage 2</td><td rowspan=\"3\">W H8 x 8</td><td rowspan=\"2\">Overlapping Patch Embedding</td><td colspan=\"5\">S2 = 2</td><td colspan=\"3\">S2 = 2</td></tr><tr><td colspan=\"4\">C2 = 128</td><td>C2 = 192</td><td colspan=\"2\">C2 = 192</td><td>C2 = 256</td></tr><tr><td>CycleMLP Block</td><td>E2 = 4 L2 = 2</td><td>E2 = 4 L2 = 3</td><td>E2 = 8 L2 = 4</td><td>E2 = 8 L2 = 8</td><td>E2 = 4 L2 = 4</td><td>E1 = 4 L1 = 2</td><td>E1 = 4 L1 = 2</td><td>E1 = 4 L1 = 2</td></tr><tr><td rowspan=\"3\">Stage 3</td><td rowspan=\"3\">H W x 16 16</td><td rowspan=\"2\">Overlapping Patch Embedding</td><td colspan=\"5\">S3 = 2</td><td colspan=\"3\">S3 = 2</td></tr><tr><td colspan=\"4\">C3 = 320</td><td>C3 = 384</td><td colspan=\"2\">C3 = 384</td><td>C3 = 512</td></tr><tr><td>CycleMLP Block</td><td>E3 = 4 L3 = 4</td><td>E3 = 4 L3 = 10</td><td>E3 = 4 L3 = 18</td><td>E3 = 4 L3 = 27</td><td>E3 = 4 L3 = 24</td><td>E1 = 4 L1 = 6</td><td>E1 = 4 L1 = 18</td><td>E1 = 4 L1 = 18</td></tr><tr><td rowspan=\"3\">Stage 4</td><td rowspan=\"3\">H W x 32 32</td><td rowspan=\"2\">Overlapping Patch Embedding</td><td colspan=\"5\">S4 = 2</td><td colspan=\"3\">S4 = 2</td></tr><tr><td colspan=\"4\">C4 = 512</td><td>C4 = 768</td><td colspan=\"2\">C4 = 768</td><td>C4 = 1024</td></tr><tr><td>CycleMLP Block</td><td>E4 = 4 L4 = 2</td><td>E4 = 4 L4 = 3</td><td>E4 = 4 L4 = 3</td><td>E4 = 4 L4 = 3</td><td>E4 = 4 L4 =3</td><td>E1 = 4 L1 = 2</td><td>E1 =4 L1 =2</td><td>E1 = 4 L1 = 2</td></tr><tr><td colspan=\"3\">Parameters (M)</td><td>15.2</td><td>26.8</td><td>38.4</td><td>51.8</td><td>75.7</td><td>28.3</td><td>49.6</td><td>87.7</td></tr><tr><td colspan=\"3\">FLOPs (G)</td><td>2.1</td><td>3.9</td><td>6.9</td><td>10.1</td><td>12.3</td><td>4.4</td><td>8.6</td><td>15.2</td></tr></table>",
      "id": 236,
      "page": 18,
      "text": "Output Size Layer Name PVT-Style (Wang et al., 2021b) Swin-Style (Liu etal., 2021b)\n B1 B2 B3 B4 B5 Tiny Small Base\n Stage 1 H W x 4 4 Overlapping Patch Embedding S1 = 4 S1 = 4\n C1 = 64 C1 = 96 C1 = 96 C1 = 128\n CycleMLP Block E1 = 4 L1 = 2 E1 = 4 L1 = 2 E1 = 8 L1 = 3 E1 = 8 L1 = 3 E1 = 4 L1 = 3 E1 = 4 L1 = 2 E1 = 4 L1 = 2 E1 = 4 L1 = 2\n Stage 2 W H8 x 8 Overlapping Patch Embedding S2 = 2 S2 = 2\n C2 = 128 C2 = 192 C2 = 192 C2 = 256\n CycleMLP Block E2 = 4 L2 = 2 E2 = 4 L2 = 3 E2 = 8 L2 = 4 E2 = 8 L2 = 8 E2 = 4 L2 = 4 E1 = 4 L1 = 2 E1 = 4 L1 = 2 E1 = 4 L1 = 2\n Stage 3 H W x 16 16 Overlapping Patch Embedding S3 = 2 S3 = 2\n C3 = 320 C3 = 384 C3 = 384 C3 = 512\n CycleMLP Block E3 = 4 L3 = 4 E3 = 4 L3 = 10 E3 = 4 L3 = 18 E3 = 4 L3 = 27 E3 = 4 L3 = 24 E1 = 4 L1 = 6 E1 = 4 L1 = 18 E1 = 4 L1 = 18\n Stage 4 H W x 32 32 Overlapping Patch Embedding S4 = 2 S4 = 2\n C4 = 512 C4 = 768 C4 = 768 C4 = 1024\n CycleMLP Block E4 = 4 L4 = 2 E4 = 4 L4 = 3 E4 = 4 L4 = 3 E4 = 4 L4 = 3 E4 = 4 L4 =3 E1 = 4 L1 = 2 E1 =4 L1 =2 E1 = 4 L1 = 2\n Parameters (M) 15.2 26.8 38.4 51.8 75.7 28.3 49.6 87.7\n FLOPs (G) 2.1 3.9 6.9 10.1 12.3 4.4 8.6"
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 1275
        },
        {
          "x": 2107,
          "y": 1275
        },
        {
          "x": 2107,
          "y": 1464
        },
        {
          "x": 442,
          "y": 1464
        }
      ],
      "category": "paragraph",
      "html": "<p id='237' style='font-size:22px'>Table 11: Instantiations of the CycleMLP with varying complexity. The Ei and Li denote the<br>expand ratio and number of repeated layers. Our design principle is inspired by the philosophy of<br>ResNet (He et al., 2016), where the channel dimension increases while the spatial resolution shrinks<br>with the layer going deeper.</p>",
      "id": 237,
      "page": 18,
      "text": "Table 11: Instantiations of the CycleMLP with varying complexity. The Ei and Li denote the\nexpand ratio and number of repeated layers. Our design principle is inspired by the philosophy of\nResNet (He et al., 2016), where the channel dimension increases while the spatial resolution shrinks\nwith the layer going deeper."
    },
    {
      "bounding_box": [
        {
          "x": 441,
          "y": 1569
        },
        {
          "x": 2105,
          "y": 1569
        },
        {
          "x": 2105,
          "y": 1661
        },
        {
          "x": 441,
          "y": 1661
        }
      ],
      "category": "paragraph",
      "html": "<p id='238' style='font-size:18px'>Further kernel optimization for Cycle FC may bring a faster speed but is beyond the scope of this<br>work.</p>",
      "id": 238,
      "page": 18,
      "text": "Further kernel optimization for Cycle FC may bring a faster speed but is beyond the scope of this\nwork."
    },
    {
      "bounding_box": [
        {
          "x": 444,
          "y": 1737
        },
        {
          "x": 1164,
          "y": 1737
        },
        {
          "x": 1164,
          "y": 1785
        },
        {
          "x": 444,
          "y": 1785
        }
      ],
      "category": "paragraph",
      "html": "<p id='239' style='font-size:16px'>E.2 COCO INSTANCE SEGMENTATION</p>",
      "id": 239,
      "page": 18,
      "text": "E.2 COCO INSTANCE SEGMENTATION"
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 1829
        },
        {
          "x": 2107,
          "y": 1829
        },
        {
          "x": 2107,
          "y": 2381
        },
        {
          "x": 442,
          "y": 2381
        }
      ],
      "category": "paragraph",
      "html": "<p id='240' style='font-size:16px'>We conduct object detection and instance segmentation experiments on COCO (Lin et al., 2014)<br>dataset, which contains 118K and 5K images for train and validation splits. We adopt<br>the mmdetection (Chen et al., 2019) toolbox for all experiments in this subsection. To evalu-<br>ate the our CycleMLP backbones, we adopt two widely used detectors, i.e., RetinaNet (Lin et al.,<br>2017) and Mask R-CNN (He et al., 2017). All backbones are initialized with ImageNet pre-trained<br>weights and other newly added layers are initialized via Xavier (Glorot & Bengio, 2010). We use<br>the AdamW (Loshchilov & Hutter, 2017) optimizer with the initial learning rate of 1x10-4 · All<br>models are trained on 8 Tesla V100 GPUs with a total batch size of 16 for 12 epochs (i.e., 1x train-<br>ing scheduler). The input images are resized to the shorted side of 800 pixels and the longer side<br>does not exceed 1333 pixels during training. We do not use the multi-scale (Carion et al., 2020; Zhu<br>et al., 2020; Sun et al., 2021) training strategy. In the testing stage, the shorter side of input images<br>is resized to 800 pixels while no constraint on the longer side.</p>",
      "id": 240,
      "page": 18,
      "text": "We conduct object detection and instance segmentation experiments on COCO (Lin et al., 2014)\ndataset, which contains 118K and 5K images for train and validation splits. We adopt\nthe mmdetection (Chen et al., 2019) toolbox for all experiments in this subsection. To evalu-\nate the our CycleMLP backbones, we adopt two widely used detectors, i.e., RetinaNet (Lin et al.,\n2017) and Mask R-CNN (He et al., 2017). All backbones are initialized with ImageNet pre-trained\nweights and other newly added layers are initialized via Xavier (Glorot & Bengio, 2010). We use\nthe AdamW (Loshchilov & Hutter, 2017) optimizer with the initial learning rate of 1x10-4 · All\nmodels are trained on 8 Tesla V100 GPUs with a total batch size of 16 for 12 epochs (i.e., 1x train-\ning scheduler). The input images are resized to the shorted side of 800 pixels and the longer side\ndoes not exceed 1333 pixels during training. We do not use the multi-scale (Carion et al., 2020; Zhu\net al., 2020; Sun et al., 2021) training strategy. In the testing stage, the shorter side of input images\nis resized to 800 pixels while no constraint on the longer side."
    },
    {
      "bounding_box": [
        {
          "x": 445,
          "y": 2457
        },
        {
          "x": 1220,
          "y": 2457
        },
        {
          "x": 1220,
          "y": 2503
        },
        {
          "x": 445,
          "y": 2503
        }
      ],
      "category": "paragraph",
      "html": "<p id='241' style='font-size:20px'>E.3 ADE20K SEMANTIC SEGMENTATION</p>",
      "id": 241,
      "page": 18,
      "text": "E.3 ADE20K SEMANTIC SEGMENTATION"
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 2546
        },
        {
          "x": 2108,
          "y": 2546
        },
        {
          "x": 2108,
          "y": 3054
        },
        {
          "x": 442,
          "y": 3054
        }
      ],
      "category": "paragraph",
      "html": "<p id='242' style='font-size:18px'>We conduct semantic segmentation experiments on ADE20K (Zhou et al., 2017) dataset, which COV-<br>ers a broad range of 150 semantic categories. ADE20K contains 20K training, 2K validation and<br>3K testing images. We adopt the mmsegmenati on (Contributors, 2020) toolbox as our codebase<br>in this subsection. The experimental settings mostly follow PVT (Wang et al., 2021b), which trains<br>models for 40K iterations on 8 Tesla V100 GPUs with 4 samples per GPU. The backbone is initial-<br>ized with the pre-trained weights on ImageNet. All models are optimized by AdamW (Loshchilov &<br>Hutter, 2017). The initial learning rate is configured as 2 x 10-4 with the polynomial decay parame-<br>ter of 0.9. Input images are randomly resized and cropped to 512x512 at the training phase. During<br>testing, we scale the images to the shorted side of 512. We adopt the simple approach Semantic<br>FPN (Kirillov et al., 2019) as the semantic segmentation method following (Wang et al., 2021b) for<br>fair comparison.</p>",
      "id": 242,
      "page": 18,
      "text": "We conduct semantic segmentation experiments on ADE20K (Zhou et al., 2017) dataset, which COV-\ners a broad range of 150 semantic categories. ADE20K contains 20K training, 2K validation and\n3K testing images. We adopt the mmsegmenati on (Contributors, 2020) toolbox as our codebase\nin this subsection. The experimental settings mostly follow PVT (Wang et al., 2021b), which trains\nmodels for 40K iterations on 8 Tesla V100 GPUs with 4 samples per GPU. The backbone is initial-\nized with the pre-trained weights on ImageNet. All models are optimized by AdamW (Loshchilov &\nHutter, 2017). The initial learning rate is configured as 2 x 10-4 with the polynomial decay parame-\nter of 0.9. Input images are randomly resized and cropped to 512x512 at the training phase. During\ntesting, we scale the images to the shorted side of 512. We adopt the simple approach Semantic\nFPN (Kirillov et al., 2019) as the semantic segmentation method following (Wang et al., 2021b) for\nfair comparison."
    },
    {
      "bounding_box": [
        {
          "x": 1253,
          "y": 3133
        },
        {
          "x": 1299,
          "y": 3133
        },
        {
          "x": 1299,
          "y": 3172
        },
        {
          "x": 1253,
          "y": 3172
        }
      ],
      "category": "footer",
      "html": "<footer id='243' style='font-size:16px'>18</footer>",
      "id": 243,
      "page": 18,
      "text": "18"
    },
    {
      "bounding_box": [
        {
          "x": 445,
          "y": 112
        },
        {
          "x": 1224,
          "y": 112
        },
        {
          "x": 1224,
          "y": 156
        },
        {
          "x": 445,
          "y": 156
        }
      ],
      "category": "header",
      "html": "<header id='244' style='font-size:14px'>Published as a conference paper at ICLR 2022</header>",
      "id": 244,
      "page": 19,
      "text": "Published as a conference paper at ICLR 2022"
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 339
        },
        {
          "x": 2155,
          "y": 339
        },
        {
          "x": 2155,
          "y": 1393
        },
        {
          "x": 442,
          "y": 1393
        }
      ],
      "category": "table",
      "html": "<table id='245' style='font-size:18px'><tr><td>Method</td><td>Backbone</td><td>val MS mloU</td><td>Params</td><td>FLOPs</td></tr><tr><td>DANet (Fu et al., 2019a) DeepLabv3+ (Chen et al., 2018) ACNet (Fu et al., 2019b) DNL (Yin et al., 2020) OCRNet (Yuan et al., 2020) UperNet (Xiao et al., 2018)</td><td>ResNet-101 ResNet-101 ResNet-101 ResNet-101 ResNet-101 ResNet-101</td><td>45.2 44.1 45.9 46.0 45.3 44.9</td><td>69M 63M - 69M 56M 86M</td><td>1119G 1021G - 1249G 923G 1029G</td></tr><tr><td>OCRNet (Yuan et al., 2020) DeepLabv3+ (Chen et al., 2018) DeepLabv3+ (Chen et al., 2018)</td><td>HRNet-w48 ResNeSt-101 ResNeSt-200</td><td>45.7 46.9 48.4</td><td>71M 66M 88M</td><td>664G 1051G 1381G</td></tr><tr><td>UperNet (Xiao et al., 2018)</td><td>Swin-T (Liu et al., 2021b) AS-MLP-T (Lian et al., 2021) CycleMLP-T (ours)</td><td>45.8 46.5 47.1</td><td>60M 60M 60M</td><td>945G 937G 937G</td></tr><tr><td>UperNet (Xiao et al., 2018)</td><td>Swin-S (Liu et al., 2021b) AS-MLP-S (Lian et al., 2021) CycleMLP-S (ours)</td><td>49.5 49.2 49.6</td><td>81M 81M 81M</td><td>1038G 1024G 1024G</td></tr><tr><td>UperNet (Xiao et al., 2018)</td><td>Swin-B (Liu et al., 2021b) AS-MLP-B(Lian et al., 2021) CycleMLP-B (ours)</td><td>49.7 49.5 49.7</td><td>121M 121M 121M</td><td>1188G 1166G 1166G</td></tr></table>",
      "id": 245,
      "page": 19,
      "text": "Method Backbone val MS mloU Params FLOPs\n DANet (Fu et al., 2019a) DeepLabv3+ (Chen et al., 2018) ACNet (Fu et al., 2019b) DNL (Yin et al., 2020) OCRNet (Yuan et al., 2020) UperNet (Xiao et al., 2018) ResNet-101 ResNet-101 ResNet-101 ResNet-101 ResNet-101 ResNet-101 45.2 44.1 45.9 46.0 45.3 44.9 69M 63M - 69M 56M 86M 1119G 1021G - 1249G 923G 1029G\n OCRNet (Yuan et al., 2020) DeepLabv3+ (Chen et al., 2018) DeepLabv3+ (Chen et al., 2018) HRNet-w48 ResNeSt-101 ResNeSt-200 45.7 46.9 48.4 71M 66M 88M 664G 1051G 1381G\n UperNet (Xiao et al., 2018) Swin-T (Liu et al., 2021b) AS-MLP-T (Lian et al., 2021) CycleMLP-T (ours) 45.8 46.5 47.1 60M 60M 60M 945G 937G 937G\n UperNet (Xiao et al., 2018) Swin-S (Liu et al., 2021b) AS-MLP-S (Lian et al., 2021) CycleMLP-S (ours) 49.5 49.2 49.6 81M 81M 81M 1038G 1024G 1024G\n UperNet (Xiao et al., 2018) Swin-B (Liu et al., 2021b) AS-MLP-B(Lian et al., 2021) CycleMLP-B (ours) 49.7 49.5 49.7 121M 121M 121M"
    },
    {
      "bounding_box": [
        {
          "x": 448,
          "y": 1446
        },
        {
          "x": 2100,
          "y": 1446
        },
        {
          "x": 2100,
          "y": 1493
        },
        {
          "x": 448,
          "y": 1493
        }
      ],
      "category": "paragraph",
      "html": "<p id='246' style='font-size:14px'>Table 12: The semantic segmentation results of different backbones on the ADE20K validation set.</p>",
      "id": 246,
      "page": 19,
      "text": "Table 12: The semantic segmentation results of different backbones on the ADE20K validation set."
    },
    {
      "bounding_box": [
        {
          "x": 447,
          "y": 1725
        },
        {
          "x": 1039,
          "y": 1725
        },
        {
          "x": 1039,
          "y": 1775
        },
        {
          "x": 447,
          "y": 1775
        }
      ],
      "category": "paragraph",
      "html": "<p id='247' style='font-size:22px'>F S AMPLING STRATEGIES</p>",
      "id": 247,
      "page": 19,
      "text": "F S AMPLING STRATEGIES"
    },
    {
      "bounding_box": [
        {
          "x": 444,
          "y": 1927
        },
        {
          "x": 2104,
          "y": 1927
        },
        {
          "x": 2104,
          "y": 2064
        },
        {
          "x": 444,
          "y": 2064
        }
      ],
      "category": "paragraph",
      "html": "<p id='248' style='font-size:20px'>We explore more sampling strategies in this subsection, including random sampling and dilated sam-<br>pling inspired by dilated convolution (Yu & Koltun, 2016; Chen et al., 2018) (as shown in Figure 6).<br>We also compare the dense sampling method with ours.</p>",
      "id": 248,
      "page": 19,
      "text": "We explore more sampling strategies in this subsection, including random sampling and dilated sam-\npling inspired by dilated convolution (Yu & Koltun, 2016; Chen et al., 2018) (as shown in Figure 6).\nWe also compare the dense sampling method with ours."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 2087
        },
        {
          "x": 2106,
          "y": 2087
        },
        {
          "x": 2106,
          "y": 2364
        },
        {
          "x": 442,
          "y": 2364
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='249' style='font-size:20px'>Random sampling. As shown in Table 13, we conduct experiments with random sampling for<br>three independent trials and observe that the averaged Top-1 accuracy on ImageNet-1K drops by<br>1.3%. We hypothesize that the decreased performance is caused by the fact that random sampling<br>will totally disturb the semantic information of objects, which is essential to image recognition.<br>Compared with the random sampling strategy, our cyclical sampling is able to aggregate the adjacent<br>pixels, which benefits in capturing the semantic information.</p>",
      "id": 249,
      "page": 19,
      "text": "Random sampling. As shown in Table 13, we conduct experiments with random sampling for\nthree independent trials and observe that the averaged Top-1 accuracy on ImageNet-1K drops by\n1.3%. We hypothesize that the decreased performance is caused by the fact that random sampling\nwill totally disturb the semantic information of objects, which is essential to image recognition.\nCompared with the random sampling strategy, our cyclical sampling is able to aggregate the adjacent\npixels, which benefits in capturing the semantic information."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 2386
        },
        {
          "x": 2107,
          "y": 2386
        },
        {
          "x": 2107,
          "y": 2618
        },
        {
          "x": 442,
          "y": 2618
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='250' style='font-size:18px'>Dilated Stepsize (Figure 6). As shown in Table 13, we observe the result of dilated sampling is<br>better than the random one (+1.0% acc) but lower than ours (-0.5% acc). In fact, compared with the<br>random sampling, dilated solutions take their advantages in local information aggregation. However,<br>compared with the cyclical sampling strategy, dilated solutions lose the fine-grained information for<br>recognition. It may hurt the accuracy performance to some extent.</p>",
      "id": 250,
      "page": 19,
      "text": "Dilated Stepsize (Figure 6). As shown in Table 13, we observe the result of dilated sampling is\nbetter than the random one (+1.0% acc) but lower than ours (-0.5% acc). In fact, compared with the\nrandom sampling, dilated solutions take their advantages in local information aggregation. However,\ncompared with the cyclical sampling strategy, dilated solutions lose the fine-grained information for\nrecognition. It may hurt the accuracy performance to some extent."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 2640
        },
        {
          "x": 2105,
          "y": 2640
        },
        {
          "x": 2105,
          "y": 3054
        },
        {
          "x": 442,
          "y": 3054
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='251' style='font-size:18px'>Dense sampling. we conduct ablation studies by using dense sampling strategies (i.e., vanilla con-<br>volution with kernel size 1x3 and 3x 1). Since dense sampling strategies incredibly increase the<br>models' parameters and FLOPs, we do not have enough time to thoroughly optimize the model for<br>300 epochs. Therefore, for fair comparisons, we conducted extra ablation studies on training mod-<br>els for 100 epochs with the strictly same learning configurations. The results shown in Table 14<br>demonstrate that the sparse sampling strategy (ours) outperforms the dense one. The comparison in-<br>dicates that the dense sampling strategies introduce redundant parameters, which makes the model<br>hard to optimize. Our sparse sampling strategy with fewer parameters is proven to be efficient and<br>optimization-friendly.</p>",
      "id": 251,
      "page": 19,
      "text": "Dense sampling. we conduct ablation studies by using dense sampling strategies (i.e., vanilla con-\nvolution with kernel size 1x3 and 3x 1). Since dense sampling strategies incredibly increase the\nmodels' parameters and FLOPs, we do not have enough time to thoroughly optimize the model for\n300 epochs. Therefore, for fair comparisons, we conducted extra ablation studies on training mod-\nels for 100 epochs with the strictly same learning configurations. The results shown in Table 14\ndemonstrate that the sparse sampling strategy (ours) outperforms the dense one. The comparison in-\ndicates that the dense sampling strategies introduce redundant parameters, which makes the model\nhard to optimize. Our sparse sampling strategy with fewer parameters is proven to be efficient and\noptimization-friendly."
    },
    {
      "bounding_box": [
        {
          "x": 1253,
          "y": 3133
        },
        {
          "x": 1299,
          "y": 3133
        },
        {
          "x": 1299,
          "y": 3172
        },
        {
          "x": 1253,
          "y": 3172
        }
      ],
      "category": "footer",
      "html": "<footer id='252' style='font-size:16px'>19</footer>",
      "id": 252,
      "page": 19,
      "text": "19"
    },
    {
      "bounding_box": [
        {
          "x": 445,
          "y": 111
        },
        {
          "x": 1224,
          "y": 111
        },
        {
          "x": 1224,
          "y": 157
        },
        {
          "x": 445,
          "y": 157
        }
      ],
      "category": "header",
      "html": "<header id='253' style='font-size:16px'>Published as a conference paper at ICLR 2022</header>",
      "id": 253,
      "page": 20,
      "text": "Published as a conference paper at ICLR 2022"
    },
    {
      "bounding_box": [
        {
          "x": 554,
          "y": 424
        },
        {
          "x": 1045,
          "y": 424
        },
        {
          "x": 1045,
          "y": 1278
        },
        {
          "x": 554,
          "y": 1278
        }
      ],
      "category": "figure",
      "html": "<figure><img id='254' alt=\"\" data-coord=\"top-left:(554,424); bottom-right:(1045,1278)\" /></figure>",
      "id": 254,
      "page": 20,
      "text": ""
    },
    {
      "bounding_box": [
        {
          "x": 549,
          "y": 1310
        },
        {
          "x": 1057,
          "y": 1310
        },
        {
          "x": 1057,
          "y": 1447
        },
        {
          "x": 549,
          "y": 1447
        }
      ],
      "category": "caption",
      "html": "<caption id='255' style='font-size:18px'>Figure 6: An example of dilated<br>CycleMLP where dilation=2<br>and stepsize=3.</caption>",
      "id": 255,
      "page": 20,
      "text": "Figure 6: An example of dilated\nCycleMLP where dilation=2\nand stepsize=3."
    },
    {
      "bounding_box": [
        {
          "x": 1194,
          "y": 336
        },
        {
          "x": 2012,
          "y": 336
        },
        {
          "x": 2012,
          "y": 632
        },
        {
          "x": 1194,
          "y": 632
        }
      ],
      "category": "table",
      "html": "<br><table id='256' style='font-size:18px'><tr><td>Sampling</td><td>Params</td><td>FLOPs</td><td>Top-1 Acc</td></tr><tr><td>dilation=2</td><td>26.8M</td><td>3.9G</td><td>81.1</td></tr><tr><td>Random, S=1 Random, S=2 Random, S=3</td><td>26.8M</td><td>3.9G</td><td>80.4 80.2 80.4</td></tr><tr><td>CycleMLP</td><td>26.8M</td><td>3.9G</td><td>81.6</td></tr></table>",
      "id": 256,
      "page": 20,
      "text": "Sampling Params FLOPs Top-1 Acc\n dilation=2 26.8M 3.9G 81.1\n Random, S=1 Random, S=2 Random, S=3 26.8M 3.9G 80.4 80.2 80.4\n CycleMLP 26.8M 3.9G"
    },
    {
      "bounding_box": [
        {
          "x": 1195,
          "y": 635
        },
        {
          "x": 2006,
          "y": 635
        },
        {
          "x": 2006,
          "y": 803
        },
        {
          "x": 1195,
          "y": 803
        }
      ],
      "category": "caption",
      "html": "<br><caption id='257' style='font-size:18px'>Table 13: Comparison with dilated and random<br>sampling. For random sampling, we conduct the<br>experiments for three independent trials with three<br>seeds (S=1, 2, 3).</caption>",
      "id": 257,
      "page": 20,
      "text": "Table 13: Comparison with dilated and random\nsampling. For random sampling, we conduct the\nexperiments for three independent trials with three\nseeds (S=1, 2, 3)."
    },
    {
      "bounding_box": [
        {
          "x": 1200,
          "y": 851
        },
        {
          "x": 2002,
          "y": 851
        },
        {
          "x": 2002,
          "y": 997
        },
        {
          "x": 1200,
          "y": 997
        }
      ],
      "category": "table",
      "html": "<table id='258' style='font-size:14px'><tr><td>Operators</td><td>Dense</td><td>Params</td><td>FLOPs</td><td>Top-1 Acc</td></tr><tr><td>Conv: 1x3+3x1</td><td>V</td><td>34.3M</td><td>5.1G</td><td>75.0</td></tr><tr><td>CycleMLP: 1x3+3x1</td><td>X</td><td>26.8M</td><td>3.9G</td><td>76.1</td></tr></table>",
      "id": 258,
      "page": 20,
      "text": "Operators Dense Params FLOPs Top-1 Acc\n Conv: 1x3+3x1 V 34.3M 5.1G 75.0\n CycleMLP: 1x3+3x1 X 26.8M 3.9G"
    },
    {
      "bounding_box": [
        {
          "x": 1199,
          "y": 1007
        },
        {
          "x": 2000,
          "y": 1007
        },
        {
          "x": 2000,
          "y": 1131
        },
        {
          "x": 1199,
          "y": 1131
        }
      ],
      "category": "caption",
      "html": "<br><caption id='259' style='font-size:18px'>Table 14: Comparison with dense sampling: On<br>the consideration of training time, we only train both<br>models for 100 epochs for fair comparison.</caption>",
      "id": 259,
      "page": 20,
      "text": "Table 14: Comparison with dense sampling: On\nthe consideration of training time, we only train both\nmodels for 100 epochs for fair comparison."
    },
    {
      "bounding_box": [
        {
          "x": 1196,
          "y": 1170
        },
        {
          "x": 1980,
          "y": 1170
        },
        {
          "x": 1980,
          "y": 1439
        },
        {
          "x": 1196,
          "y": 1439
        }
      ],
      "category": "table",
      "html": "<table id='260' style='font-size:16px'><tr><td>branch1</td><td>branch2</td><td>ImgNet Top-1</td><td>ADE20K mIoU</td></tr><tr><td>7x1</td><td>1x 7</td><td>81.6</td><td>43.9</td></tr><tr><td>7x2</td><td>2x7</td><td>81.5</td><td>43.4</td></tr><tr><td>7x3</td><td>3x7</td><td>81.4</td><td>42.7</td></tr><tr><td>4x4</td><td>4x4</td><td>81.5</td><td>43.1</td></tr></table>",
      "id": 260,
      "page": 20,
      "text": "branch1 branch2 ImgNet Top-1 ADE20K mIoU\n 7x1 1x 7 81.6 43.9\n 7x2 2x7 81.5 43.4\n 7x3 3x7 81.4 42.7\n 4x4 4x4 81.5"
    },
    {
      "bounding_box": [
        {
          "x": 1198,
          "y": 1442
        },
        {
          "x": 1998,
          "y": 1442
        },
        {
          "x": 1998,
          "y": 1561
        },
        {
          "x": 1198,
          "y": 1561
        }
      ],
      "category": "caption",
      "html": "<br><caption id='261' style='font-size:18px'>Table 15: Comparison on different stepsizes (e.g.,<br>even stepsize and odd stepsize), including 7x2,<br>4x4.</caption>",
      "id": 261,
      "page": 20,
      "text": "Table 15: Comparison on different stepsizes (e.g.,\neven stepsize and odd stepsize), including 7x2,\n4x4."
    },
    {
      "bounding_box": [
        {
          "x": 445,
          "y": 1650
        },
        {
          "x": 1132,
          "y": 1650
        },
        {
          "x": 1132,
          "y": 1702
        },
        {
          "x": 445,
          "y": 1702
        }
      ],
      "category": "paragraph",
      "html": "<p id='262' style='font-size:22px'>G VISUALIZATION EXAMPLES</p>",
      "id": 262,
      "page": 20,
      "text": "G VISUALIZATION EXAMPLES"
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 1754
        },
        {
          "x": 2107,
          "y": 1754
        },
        {
          "x": 2107,
          "y": 1894
        },
        {
          "x": 442,
          "y": 1894
        }
      ],
      "category": "paragraph",
      "html": "<p id='263' style='font-size:20px'>For easier understanding of our proposed CycleMLP, we visualize several instances of CycleMLP<br>in Figure 7, including general case with stepsize 3x3 (7a), even stepsize (7b), and examples where<br>stepsize along height or wi dth equals to 1 (7c, 7d).</p>",
      "id": 263,
      "page": 20,
      "text": "For easier understanding of our proposed CycleMLP, we visualize several instances of CycleMLP\nin Figure 7, including general case with stepsize 3x3 (7a), even stepsize (7b), and examples where\nstepsize along height or wi dth equals to 1 (7c, 7d)."
    },
    {
      "bounding_box": [
        {
          "x": 442,
          "y": 1916
        },
        {
          "x": 2108,
          "y": 1916
        },
        {
          "x": 2108,
          "y": 2101
        },
        {
          "x": 442,
          "y": 2101
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='264' style='font-size:20px'>We note that given specific number ofinput and output channels, no matter how the stepsize changes,<br>the number of parameters of the CycleMLP does not change. Therefore, there is a trade-off of<br>representation abilities between spatial and channel dimensions, which will be discussed in details<br>in following experimental analysis.</p>",
      "id": 264,
      "page": 20,
      "text": "We note that given specific number ofinput and output channels, no matter how the stepsize changes,\nthe number of parameters of the CycleMLP does not change. Therefore, there is a trade-off of\nrepresentation abilities between spatial and channel dimensions, which will be discussed in details\nin following experimental analysis."
    },
    {
      "bounding_box": [
        {
          "x": 441,
          "y": 2121
        },
        {
          "x": 2109,
          "y": 2121
        },
        {
          "x": 2109,
          "y": 2720
        },
        {
          "x": 441,
          "y": 2720
        }
      ],
      "category": "paragraph",
      "html": "<br><p id='265' style='font-size:20px'>Experiments: We further conduct experiments on CycleMLPs with stepsize of 2x7, 7x2, 7x3,<br>3x7, and 4x4, respectively. The results are summarized Table 15. For fair comparisons, all the<br>models in the above table have the same parameters and FLOPs. We observe that the model with<br>stepsize of 1 x7 and 7x1 achieves the best performance, especially for semantic segmentation on<br>ADE20K. To analyze the impact of stepsize on the performance, we take Figure 7 for better illus-<br>tration. One can see that enlarging the stepsize can expand the spatial receptive field. However, at<br>a cost, it will reduce the number of periods (groups) running along the channel dimension, which<br>may hurt the channel-wise representation abilities. Taking a feature map with C = 18 for example,<br>the CycleMLP with stepsize 3x3 (Figure 7(a)) runs through only 2 channel groups (curly brackets<br>in the figure). However, the CycleMLP with stepsize 3x1 (Figure 7(c)) will run through 6 groups<br>in total, making better use of the representation in the channel dimension. That's to say, there is a<br>trade-off between spatial and channel representation. We empirically found that CyCleMLP with<br>stepsize of 1x7 and 7x1 achieves the best performance.</p>",
      "id": 265,
      "page": 20,
      "text": "Experiments: We further conduct experiments on CycleMLPs with stepsize of 2x7, 7x2, 7x3,\n3x7, and 4x4, respectively. The results are summarized Table 15. For fair comparisons, all the\nmodels in the above table have the same parameters and FLOPs. We observe that the model with\nstepsize of 1 x7 and 7x1 achieves the best performance, especially for semantic segmentation on\nADE20K. To analyze the impact of stepsize on the performance, we take Figure 7 for better illus-\ntration. One can see that enlarging the stepsize can expand the spatial receptive field. However, at\na cost, it will reduce the number of periods (groups) running along the channel dimension, which\nmay hurt the channel-wise representation abilities. Taking a feature map with C = 18 for example,\nthe CycleMLP with stepsize 3x3 (Figure 7(a)) runs through only 2 channel groups (curly brackets\nin the figure). However, the CycleMLP with stepsize 3x1 (Figure 7(c)) will run through 6 groups\nin total, making better use of the representation in the channel dimension. That's to say, there is a\ntrade-off between spatial and channel representation. We empirically found that CyCleMLP with\nstepsize of 1x7 and 7x1 achieves the best performance."
    },
    {
      "bounding_box": [
        {
          "x": 1249,
          "y": 3132
        },
        {
          "x": 1299,
          "y": 3132
        },
        {
          "x": 1299,
          "y": 3170
        },
        {
          "x": 1249,
          "y": 3170
        }
      ],
      "category": "footer",
      "html": "<footer id='266' style='font-size:16px'>20</footer>",
      "id": 266,
      "page": 20,
      "text": "20"
    },
    {
      "bounding_box": [
        {
          "x": 445,
          "y": 112
        },
        {
          "x": 1224,
          "y": 112
        },
        {
          "x": 1224,
          "y": 156
        },
        {
          "x": 445,
          "y": 156
        }
      ],
      "category": "header",
      "html": "<header id='267' style='font-size:16px'>Published as a conference paper at ICLR 2022</header>",
      "id": 267,
      "page": 21,
      "text": "Published as a conference paper at ICLR 2022"
    },
    {
      "bounding_box": [
        {
          "x": 445,
          "y": 642
        },
        {
          "x": 2111,
          "y": 642
        },
        {
          "x": 2111,
          "y": 2490
        },
        {
          "x": 445,
          "y": 2490
        }
      ],
      "category": "figure",
      "html": "<figure><img id='268' style='font-size:14px' alt=\"[H, W, C] H W\n[0, 0, 0]\nC\n[H, W, C] H W\n[1, 0, 1]\n[0, 0, 0\n[2, 0, 2]\n[1, 0, 1]\n[0, 1, 3]\n[2, 0, 2]\n[1, 1, 4] [0, 1, 3]\n[1, 1, 4]\n[2, 1, 5]\n[2, 1, 5]\n[0, 2, 6 \n[0, 0, 6\n[1, 2, 7]\n[1, 0, 7\n[2, 2, 8 ]\n[2, 0, 8\n[0, 0, 9] [0, 1, 9]\n[1, 0, 10] [1, 1, 10]\n[2, 1, 11]\n[2, 0, 11]\n0, 0, 12\n[0, 1, 12]\n[1, 0, 13\n[1, 1, 13]\n[2, 0, 14]\n[2, 1, 14] [⌀, 1, 15]\n[⌀, 2, 15] [1, 1, 16]\n[2, 1, 17]\n[1, 2, 16]\nH3 H2\n[2, 2, 17]\nH1\nWo W1 W2\nHo\nH3 H2 H1 Ho\nWo W1 W2 (b) SH x Sw: 3 x 2\n(a) SH x Sw: 3 x 3\n[H, W, C] H W\n[0, 0, 0]\n[H, W, C] H W\nC\n[0, 1, 1]\n[0, 0, 0\n[1, 0, 1 [⌀, 2, 2]\n[2, 0, 2 [0, 0, 3]\n[0, 0, [0, 1, 4]\n3\n[1, 0\n[0, 2, 5]\n[0, 0, 6]\n[0,\n[0, 1, 7]\n[0, 2, 8]\n[0, 0, 9]\n[0, 1, 10]\n2,\n[0, 2, 11\n[0, 12\n[0, 0, 12]\n[1, 0, 13\n[0, 1, 13]\n[2, 0, 14\n[0, 0, 15 [0, 2, 14]\n[1, 0, 16 [0, 0, 15]\n[2, 0, 17 [0, 1, 16]\nH3 H2 H1 Ho W1 W2 H3 H2 H1 Ho\n[0, 2, 17]\nWo\nWo W1 W2\n(c) SH x Sw: 3 x 1\n(d) SH x Sw: 1 x 3\" data-coord=\"top-left:(445,642); bottom-right:(2111,2490)\" /></figure>",
      "id": 268,
      "page": 21,
      "text": "[H, W, C] H W\n[0, 0, 0]\nC\n[H, W, C] H W\n[1, 0, 1]\n[0, 0, 0\n[2, 0, 2]\n[1, 0, 1]\n[0, 1, 3]\n[2, 0, 2]\n[1, 1, 4] [0, 1, 3]\n[1, 1, 4]\n[2, 1, 5]\n[2, 1, 5]\n[0, 2, 6 \n[0, 0, 6\n[1, 2, 7]\n[1, 0, 7\n[2, 2, 8 ]\n[2, 0, 8\n[0, 0, 9] [0, 1, 9]\n[1, 0, 10] [1, 1, 10]\n[2, 1, 11]\n[2, 0, 11]\n0, 0, 12\n[0, 1, 12]\n[1, 0, 13\n[1, 1, 13]\n[2, 0, 14]\n[2, 1, 14] [⌀, 1, 15]\n[⌀, 2, 15] [1, 1, 16]\n[2, 1, 17]\n[1, 2, 16]\nH3 H2\n[2, 2, 17]\nH1\nWo W1 W2\nHo\nH3 H2 H1 Ho\nWo W1 W2 (b) SH x Sw: 3 x 2\n(a) SH x Sw: 3 x 3\n[H, W, C] H W\n[0, 0, 0]\n[H, W, C] H W\nC\n[0, 1, 1]\n[0, 0, 0\n[1, 0, 1 [⌀, 2, 2]\n[2, 0, 2 [0, 0, 3]\n[0, 0, [0, 1, 4]\n3\n[1, 0\n[0, 2, 5]\n[0, 0, 6]\n[0,\n[0, 1, 7]\n[0, 2, 8]\n[0, 0, 9]\n[0, 1, 10]\n2,\n[0, 2, 11\n[0, 12\n[0, 0, 12]\n[1, 0, 13\n[0, 1, 13]\n[2, 0, 14\n[0, 0, 15 [0, 2, 14]\n[1, 0, 16 [0, 0, 15]\n[2, 0, 17 [0, 1, 16]\nH3 H2 H1 Ho W1 W2 H3 H2 H1 Ho\n[0, 2, 17]\nWo\nWo W1 W2\n(c) SH x Sw: 3 x 1\n(d) SH x Sw: 1 x 3"
    },
    {
      "bounding_box": [
        {
          "x": 439,
          "y": 2525
        },
        {
          "x": 2108,
          "y": 2525
        },
        {
          "x": 2108,
          "y": 2718
        },
        {
          "x": 439,
          "y": 2718
        }
      ],
      "category": "caption",
      "html": "<caption id='269' style='font-size:20px'>Figure 7: Examples of Stepsize cases: Here we separate the feature map along the width<br>dimension for convenient visualization. ★ denotes the output position. We place the absolute<br>coordinates (h, W, c) of the sampled points at the left of the feature. Sampled points within a<br>curly bracket () belong to the same period (group). Dash lines link two cyclical periods.</caption>",
      "id": 269,
      "page": 21,
      "text": "Figure 7: Examples of Stepsize cases: Here we separate the feature map along the width\ndimension for convenient visualization. ★ denotes the output position. We place the absolute\ncoordinates (h, W, c) of the sampled points at the left of the feature. Sampled points within a\ncurly bracket () belong to the same period (group). Dash lines link two cyclical periods."
    },
    {
      "bounding_box": [
        {
          "x": 1249,
          "y": 3133
        },
        {
          "x": 1296,
          "y": 3133
        },
        {
          "x": 1296,
          "y": 3171
        },
        {
          "x": 1249,
          "y": 3171
        }
      ],
      "category": "footer",
      "html": "<footer id='270' style='font-size:18px'>21</footer>",
      "id": 270,
      "page": 21,
      "text": "21"
    }
  ]
}