{
    "id": "629d6512-0f92-11ef-8230-426932df3dcf",
    "pdf_path": "/root/data/pdf/2012.12877v2.pdf",
    "elements": [
        {
            "bounding_box": [
                {
                    "x": 597,
                    "y": 673
                },
                {
                    "x": 1950,
                    "y": 673
                },
                {
                    "x": 1950,
                    "y": 851
                },
                {
                    "x": 597,
                    "y": 851
                }
            ],
            "category": "paragraph",
            "html": "<p id='0' style='font-size:22px'>Training data-efficient image transformers<br>& distillation through attention</p>",
            "id": 0,
            "page": 1,
            "text": "Training data-efficient image transformers\n& distillation through attention"
        },
        {
            "bounding_box": [
                {
                    "x": 632,
                    "y": 929
                },
                {
                    "x": 1958,
                    "y": 929
                },
                {
                    "x": 1958,
                    "y": 1077
                },
                {
                    "x": 632,
                    "y": 1077
                }
            ],
            "category": "paragraph",
            "html": "<p id='1' style='font-size:20px'>Hugo Touvron*,† Matthieu Cord† Matthijs Douze*<br>Francisco Massa* Alexandre Sablayrolles* Herve Jegou*</p>",
            "id": 1,
            "page": 1,
            "text": "Hugo Touvron*,† Matthieu Cord† Matthijs Douze*\nFrancisco Massa* Alexandre Sablayrolles* Herve Jegou*"
        },
        {
            "bounding_box": [
                {
                    "x": 863,
                    "y": 1127
                },
                {
                    "x": 1730,
                    "y": 1127
                },
                {
                    "x": 1730,
                    "y": 1193
                },
                {
                    "x": 863,
                    "y": 1193
                }
            ],
            "category": "paragraph",
            "html": "<p id='2' style='font-size:20px'>*Facebook AI †Sorbonne University</p>",
            "id": 2,
            "page": 1,
            "text": "*Facebook AI †Sorbonne University"
        },
        {
            "bounding_box": [
                {
                    "x": 1195,
                    "y": 1350
                },
                {
                    "x": 1350,
                    "y": 1350
                },
                {
                    "x": 1350,
                    "y": 1393
                },
                {
                    "x": 1195,
                    "y": 1393
                }
            ],
            "category": "paragraph",
            "html": "<p id='3' style='font-size:16px'>Abstract</p>",
            "id": 3,
            "page": 1,
            "text": "Abstract"
        },
        {
            "bounding_box": [
                {
                    "x": 654,
                    "y": 1418
                },
                {
                    "x": 1894,
                    "y": 1418
                },
                {
                    "x": 1894,
                    "y": 1599
                },
                {
                    "x": 654,
                    "y": 1599
                }
            ],
            "category": "paragraph",
            "html": "<p id='4' style='font-size:16px'>Recently, neural networks purely based on attention were shown to ad-<br>dress image understanding tasks such as image classification. These high-<br>performing vision transformers are pre-trained with hundreds of millions<br>of images using a large infrastructure, thereby limiting their adoption.</p>",
            "id": 4,
            "page": 1,
            "text": "Recently, neural networks purely based on attention were shown to ad-\ndress image understanding tasks such as image classification. These high-\nperforming vision transformers are pre-trained with hundreds of millions\nof images using a large infrastructure, thereby limiting their adoption."
        },
        {
            "bounding_box": [
                {
                    "x": 655,
                    "y": 1603
                },
                {
                    "x": 1894,
                    "y": 1603
                },
                {
                    "x": 1894,
                    "y": 1784
                },
                {
                    "x": 655,
                    "y": 1784
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='5' style='font-size:16px'>In this work, we produce competitive convolution-free transformers by<br>training on Imagenet only. We train them on a single computer in less than<br>3 days. Our reference vision transformer (86M parameters) achieves top-1<br>accuracy of 83.1% (single-crop) on ImageNet with no external data.</p>",
            "id": 5,
            "page": 1,
            "text": "In this work, we produce competitive convolution-free transformers by\ntraining on Imagenet only. We train them on a single computer in less than\n3 days. Our reference vision transformer (86M parameters) achieves top-1\naccuracy of 83.1% (single-crop) on ImageNet with no external data."
        },
        {
            "bounding_box": [
                {
                    "x": 653,
                    "y": 1787
                },
                {
                    "x": 1896,
                    "y": 1787
                },
                {
                    "x": 1896,
                    "y": 2104
                },
                {
                    "x": 653,
                    "y": 2104
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='6' style='font-size:16px'>More importantly, we introduce a teacher-student strategy specific to<br>transformers. It relies on a distillation token ensuring that the student<br>learns from the teacher through attention. We show the interest of this<br>token-based distillation, especially when using a convnet as a teacher. This<br>leads us to report results competitive with convnets for both Imagenet<br>(where we obtain up to 85.2% accuracy) and when transferring to other<br>tasks. We share our code and models.</p>",
            "id": 6,
            "page": 1,
            "text": "More importantly, we introduce a teacher-student strategy specific to\ntransformers. It relies on a distillation token ensuring that the student\nlearns from the teacher through attention. We show the interest of this\ntoken-based distillation, especially when using a convnet as a teacher. This\nleads us to report results competitive with convnets for both Imagenet\n(where we obtain up to 85.2% accuracy) and when transferring to other\ntasks. We share our code and models."
        },
        {
            "bounding_box": [
                {
                    "x": 550,
                    "y": 2184
                },
                {
                    "x": 996,
                    "y": 2184
                },
                {
                    "x": 996,
                    "y": 2248
                },
                {
                    "x": 550,
                    "y": 2248
                }
            ],
            "category": "paragraph",
            "html": "<p id='7' style='font-size:20px'>1 Introduction</p>",
            "id": 7,
            "page": 1,
            "text": "1 Introduction"
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 2290
                },
                {
                    "x": 1998,
                    "y": 2290
                },
                {
                    "x": 1998,
                    "y": 2691
                },
                {
                    "x": 549,
                    "y": 2691
                }
            ],
            "category": "paragraph",
            "html": "<p id='8' style='font-size:18px'>Convolutional neural networks have been the main design paradigm for image<br>understanding tasks, as initially demonstrated on image classification tasks.<br>One of the ingredient to their success was the availability of a large training set,<br>namely Imagenet [13, 42]. Motivated by the success of attention-based mod-<br>els in Natural Language Processing [14, 52], there has been increasing interest<br>in architectures leveraging attention mechanisms within convnets [2, 34, 61].<br>More recently several researchers have proposed hybrid architecture trans-<br>planting transformer ingredients to convnets to solve vision tasks [6, 43].</p>",
            "id": 8,
            "page": 1,
            "text": "Convolutional neural networks have been the main design paradigm for image\nunderstanding tasks, as initially demonstrated on image classification tasks.\nOne of the ingredient to their success was the availability of a large training set,\nnamely Imagenet [13, 42]. Motivated by the success of attention-based mod-\nels in Natural Language Processing [14, 52], there has been increasing interest\nin architectures leveraging attention mechanisms within convnets [2, 34, 61].\nMore recently several researchers have proposed hybrid architecture trans-\nplanting transformer ingredients to convnets to solve vision tasks [6, 43]."
        },
        {
            "bounding_box": [
                {
                    "x": 550,
                    "y": 2693
                },
                {
                    "x": 1998,
                    "y": 2693
                },
                {
                    "x": 1998,
                    "y": 2793
                },
                {
                    "x": 550,
                    "y": 2793
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='9' style='font-size:18px'>The vision transformer (ViT) introduced by Dosovitskiy et al. [15] is an ar-<br>chitecture directly inherited from Natural Language Processing [52], but<br>ap-</p>",
            "id": 9,
            "page": 1,
            "text": "The vision transformer (ViT) introduced by Dosovitskiy et al. [15] is an ar-\nchitecture directly inherited from Natural Language Processing [52], but\nap-"
        },
        {
            "bounding_box": [
                {
                    "x": 1258,
                    "y": 2894
                },
                {
                    "x": 1287,
                    "y": 2894
                },
                {
                    "x": 1287,
                    "y": 2934
                },
                {
                    "x": 1258,
                    "y": 2934
                }
            ],
            "category": "footer",
            "html": "<footer id='10' style='font-size:14px'>1</footer>",
            "id": 10,
            "page": 1,
            "text": "1"
        },
        {
            "bounding_box": [
                {
                    "x": 64,
                    "y": 894
                },
                {
                    "x": 149,
                    "y": 894
                },
                {
                    "x": 149,
                    "y": 2335
                },
                {
                    "x": 64,
                    "y": 2335
                }
            ],
            "category": "footer",
            "html": "<br><footer id='11' style='font-size:14px'>2021<br>Jan<br>15<br>[cs.CV]<br>arXiv:2012.12877v2</footer>",
            "id": 11,
            "page": 1,
            "text": "2021\nJan\n15\n[cs.CV]\narXiv:2012.12877v2"
        },
        {
            "bounding_box": [
                {
                    "x": 753,
                    "y": 509
                },
                {
                    "x": 1786,
                    "y": 509
                },
                {
                    "x": 1786,
                    "y": 1529
                },
                {
                    "x": 753,
                    "y": 1529
                }
            ],
            "category": "figure",
            "html": "<figure><img id='12' style='font-size:20px' alt=\"86\nDeiT-Bo ↑384\nDeiT-B1\n84 B7\n(%) DeiT-Som\naccuracy ViT\nEfficientNet\nOurs\nOursom\n80\ntop-1 B1\nViT-B\n78\nViT-L B0\n76\n50 100 200 500 1000 2500\nimages/s\" data-coord=\"top-left:(753,509); bottom-right:(1786,1529)\" /></figure>",
            "id": 12,
            "page": 2,
            "text": "86\nDeiT-Bo ↑384\nDeiT-B1\n84 B7\n(%) DeiT-Som\naccuracy ViT\nEfficientNet\nOurs\nOursom\n80\ntop-1 B1\nViT-B\n78\nViT-L B0\n76\n50 100 200 500 1000 2500\nimages/s"
        },
        {
            "bounding_box": [
                {
                    "x": 548,
                    "y": 1574
                },
                {
                    "x": 1998,
                    "y": 1574
                },
                {
                    "x": 1998,
                    "y": 1879
                },
                {
                    "x": 548,
                    "y": 1879
                }
            ],
            "category": "caption",
            "html": "<caption id='13' style='font-size:16px'>Figure 1: Throughput and accuracy on Imagenet of our methods compared to<br>EfficientNets, trained on Imagenet1k only. The throughput is measured as the<br>number of images processed per second on a V100 GPU. DeiT-B is identical to<br>VIT-B, but the training is more adapted to a data-starving regime. It is learned<br>in a few days on one machine. The symbol 유 refers to models trained with our<br>transformer-specific distillation. See Table 5 for details and more models.</caption>",
            "id": 13,
            "page": 2,
            "text": "Figure 1: Throughput and accuracy on Imagenet of our methods compared to\nEfficientNets, trained on Imagenet1k only. The throughput is measured as the\nnumber of images processed per second on a V100 GPU. DeiT-B is identical to\nVIT-B, but the training is more adapted to a data-starving regime. It is learned\nin a few days on one machine. The symbol 유 refers to models trained with our\ntransformer-specific distillation. See Table 5 for details and more models."
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 1963
                },
                {
                    "x": 1996,
                    "y": 1963
                },
                {
                    "x": 1996,
                    "y": 2211
                },
                {
                    "x": 549,
                    "y": 2211
                }
            ],
            "category": "paragraph",
            "html": "<p id='14' style='font-size:18px'>plied to image classification with raw image patches as input. Their paper pre-<br>sented excellent results with transformers trained with a large private labelled<br>image dataset (JFT-300M [46], 300 millions images). The paper concluded that<br>transformers \"do not generalize well when trained on insufficient amounts of data\",<br>and the training of these models involved extensive computing resources.</p>",
            "id": 14,
            "page": 2,
            "text": "plied to image classification with raw image patches as input. Their paper pre-\nsented excellent results with transformers trained with a large private labelled\nimage dataset (JFT-300M [46], 300 millions images). The paper concluded that\ntransformers \"do not generalize well when trained on insufficient amounts of data\",\nand the training of these models involved extensive computing resources."
        },
        {
            "bounding_box": [
                {
                    "x": 548,
                    "y": 2213
                },
                {
                    "x": 1997,
                    "y": 2213
                },
                {
                    "x": 1997,
                    "y": 2659
                },
                {
                    "x": 548,
                    "y": 2659
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='15' style='font-size:18px'>In this paper, we train a vision transformer on a single 8-GPU node in two<br>to three days (53 hours of pre-training, and optionally 20 hours of fine-tuning)<br>that is competitive with convnets having a similar number of parameters and<br>efficiency. It uses Imagenet as the sole training set. We build upon the vi-<br>sual transformer architecture from Dosovitskiy et al. [15] and improvements<br>included in the timm library [55]. With our Data-efficient image Transformers<br>(DeiT), we report large improvements over previous results, see Figure 1. Our<br>ablation study details the hyper-parameters and key ingredients for a success-<br>ful training, such as repeated augmentation.</p>",
            "id": 15,
            "page": 2,
            "text": "In this paper, we train a vision transformer on a single 8-GPU node in two\nto three days (53 hours of pre-training, and optionally 20 hours of fine-tuning)\nthat is competitive with convnets having a similar number of parameters and\nefficiency. It uses Imagenet as the sole training set. We build upon the vi-\nsual transformer architecture from Dosovitskiy et al. [15] and improvements\nincluded in the timm library [55]. With our Data-efficient image Transformers\n(DeiT), we report large improvements over previous results, see Figure 1. Our\nablation study details the hyper-parameters and key ingredients for a success-\nful training, such as repeated augmentation."
        },
        {
            "bounding_box": [
                {
                    "x": 548,
                    "y": 2661
                },
                {
                    "x": 1997,
                    "y": 2661
                },
                {
                    "x": 1997,
                    "y": 2811
                },
                {
                    "x": 548,
                    "y": 2811
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='16' style='font-size:16px'>We address another question: how to distill these models? We introduce<br>a token-based strategy, specific to transformers and denoted by DeiTm, and<br>show that it advantageously replaces the usual distillation.</p>",
            "id": 16,
            "page": 2,
            "text": "We address another question: how to distill these models? We introduce\na token-based strategy, specific to transformers and denoted by DeiTm, and\nshow that it advantageously replaces the usual distillation."
        },
        {
            "bounding_box": [
                {
                    "x": 1258,
                    "y": 2895
                },
                {
                    "x": 1288,
                    "y": 2895
                },
                {
                    "x": 1288,
                    "y": 2934
                },
                {
                    "x": 1258,
                    "y": 2934
                }
            ],
            "category": "footer",
            "html": "<footer id='17' style='font-size:14px'>2</footer>",
            "id": 17,
            "page": 2,
            "text": "2"
        },
        {
            "bounding_box": [
                {
                    "x": 612,
                    "y": 527
                },
                {
                    "x": 1684,
                    "y": 527
                },
                {
                    "x": 1684,
                    "y": 573
                },
                {
                    "x": 612,
                    "y": 573
                }
            ],
            "category": "paragraph",
            "html": "<p id='18' style='font-size:18px'>In summary, our work makes the following contributions:</p>",
            "id": 18,
            "page": 3,
            "text": "In summary, our work makes the following contributions:"
        },
        {
            "bounding_box": [
                {
                    "x": 607,
                    "y": 594
                },
                {
                    "x": 2002,
                    "y": 594
                },
                {
                    "x": 2002,
                    "y": 1487
                },
                {
                    "x": 607,
                    "y": 1487
                }
            ],
            "category": "paragraph",
            "html": "<p id='19' style='font-size:16px'>● We show that our neural networks that contains no convolutional layer<br>can achieve competitive results against the state of the art on ImageNet<br>with no external data. They are learned on a single node with 4 GPUs in<br>three days1 Our two new models DeiT-S and DeiT-Ti have fewer param-<br>eters and can be seen as the counterpart of ResNet-50 and ResNet-18.<br>● We introduce a new distillation procedure based on a distillation token,<br>which plays the same role as the class token, except that it aims at re-<br>producing the label estimated by the teacher. Both tokens interact in the<br>transformer through attention. This transformer-specific strategy outper-<br>forms vanilla distillation by a significant margin.<br>● Interestingly, with our distillation, image transformers learn more from a<br>convnet than from another transformer with comparable performance.<br>● Our models pre-learned on Imagenet are competitive when transferred to<br>different downstream tasks such as fine-grained classification, on several<br>popular public benchmarks: CIFAR-10, CIFAR-100, Oxford-102 flowers,<br>Stanford Cars and iNaturalist-18 /19.</p>",
            "id": 19,
            "page": 3,
            "text": "● We show that our neural networks that contains no convolutional layer\ncan achieve competitive results against the state of the art on ImageNet\nwith no external data. They are learned on a single node with 4 GPUs in\nthree days1 Our two new models DeiT-S and DeiT-Ti have fewer param-\neters and can be seen as the counterpart of ResNet-50 and ResNet-18.\n● We introduce a new distillation procedure based on a distillation token,\nwhich plays the same role as the class token, except that it aims at re-\nproducing the label estimated by the teacher. Both tokens interact in the\ntransformer through attention. This transformer-specific strategy outper-\nforms vanilla distillation by a significant margin.\n● Interestingly, with our distillation, image transformers learn more from a\nconvnet than from another transformer with comparable performance.\n● Our models pre-learned on Imagenet are competitive when transferred to\ndifferent downstream tasks such as fine-grained classification, on several\npopular public benchmarks: CIFAR-10, CIFAR-100, Oxford-102 flowers,\nStanford Cars and iNaturalist-18 /19."
        },
        {
            "bounding_box": [
                {
                    "x": 550,
                    "y": 1511
                },
                {
                    "x": 1996,
                    "y": 1511
                },
                {
                    "x": 1996,
                    "y": 1909
                },
                {
                    "x": 550,
                    "y": 1909
                }
            ],
            "category": "paragraph",
            "html": "<p id='20' style='font-size:16px'>This paper is organized as follows: we review related works in Section 2,<br>and focus on transformers for image classification in Section 3. We introduce<br>our distillation strategy for transformers in Section 4. The experimental sec-<br>tion 5 provides analysis and comparisons against both convnets and recent<br>transformers, as well as a comparative evaluation of our transformer-specific<br>distillation. Section 6 details our training scheme. It includes an extensive ab-<br>lation of our data-efficient training choices, which gives some insight on the<br>key ingredients involved in DeiT. We conclude in Section 7.</p>",
            "id": 20,
            "page": 3,
            "text": "This paper is organized as follows: we review related works in Section 2,\nand focus on transformers for image classification in Section 3. We introduce\nour distillation strategy for transformers in Section 4. The experimental sec-\ntion 5 provides analysis and comparisons against both convnets and recent\ntransformers, as well as a comparative evaluation of our transformer-specific\ndistillation. Section 6 details our training scheme. It includes an extensive ab-\nlation of our data-efficient training choices, which gives some insight on the\nkey ingredients involved in DeiT. We conclude in Section 7."
        },
        {
            "bounding_box": [
                {
                    "x": 551,
                    "y": 1990
                },
                {
                    "x": 1015,
                    "y": 1990
                },
                {
                    "x": 1015,
                    "y": 2049
                },
                {
                    "x": 551,
                    "y": 2049
                }
            ],
            "category": "paragraph",
            "html": "<p id='21' style='font-size:22px'>2 Related work</p>",
            "id": 21,
            "page": 3,
            "text": "2 Related work"
        },
        {
            "bounding_box": [
                {
                    "x": 548,
                    "y": 2095
                },
                {
                    "x": 1997,
                    "y": 2095
                },
                {
                    "x": 1997,
                    "y": 2442
                },
                {
                    "x": 548,
                    "y": 2442
                }
            ],
            "category": "paragraph",
            "html": "<p id='22' style='font-size:18px'>Image Classification is so core to computer vision that it is often used as a<br>benchmark to measure progress in image understanding. Any progress usu-<br>ally translates to improvement in other related tasks such as detection or seg-<br>mentation. Since 2012's AlexNet [32], convnets have dominated this bench-<br>mark and have become the de facto standard. The evolution of the state of the<br>art on the ImageNet dataset [42] reflects the progress with convolutional neural<br>network architectures and learning [32, 44, 48, 50, 51, 57].</p>",
            "id": 22,
            "page": 3,
            "text": "Image Classification is so core to computer vision that it is often used as a\nbenchmark to measure progress in image understanding. Any progress usu-\nally translates to improvement in other related tasks such as detection or seg-\nmentation. Since 2012's AlexNet [32], convnets have dominated this bench-\nmark and have become the de facto standard. The evolution of the state of the\nart on the ImageNet dataset [42] reflects the progress with convolutional neural\nnetwork architectures and learning [32, 44, 48, 50, 51, 57]."
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 2444
                },
                {
                    "x": 1997,
                    "y": 2444
                },
                {
                    "x": 1997,
                    "y": 2743
                },
                {
                    "x": 549,
                    "y": 2743
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='23' style='font-size:20px'>Despite several attempts to use transformers for image classification [7], un-<br>til now their performance has been inferior to that of convnets. Nevertheless<br>hybrid architectures that combine convnets and transformers, including the<br>self-attention mechanism, have recently exhibited competitive results in image<br>classification [56], detection [6, 28], video processing [45, 53], unsupervised ob-<br>ject discovery [35], and unified text-vision tasks [8, 33, 37].</p>",
            "id": 23,
            "page": 3,
            "text": "Despite several attempts to use transformers for image classification [7], un-\ntil now their performance has been inferior to that of convnets. Nevertheless\nhybrid architectures that combine convnets and transformers, including the\nself-attention mechanism, have recently exhibited competitive results in image\nclassification [56], detection [6, 28], video processing [45, 53], unsupervised ob-\nject discovery [35], and unified text-vision tasks [8, 33, 37]."
        },
        {
            "bounding_box": [
                {
                    "x": 600,
                    "y": 2771
                },
                {
                    "x": 1991,
                    "y": 2771
                },
                {
                    "x": 1991,
                    "y": 2815
                },
                {
                    "x": 600,
                    "y": 2815
                }
            ],
            "category": "paragraph",
            "html": "<p id='24' style='font-size:14px'>1We can accelerate the learning of the larger model DeiT-B by training it on 8 GPUs in two days.</p>",
            "id": 24,
            "page": 3,
            "text": "1We can accelerate the learning of the larger model DeiT-B by training it on 8 GPUs in two days."
        },
        {
            "bounding_box": [
                {
                    "x": 1258,
                    "y": 2894
                },
                {
                    "x": 1287,
                    "y": 2894
                },
                {
                    "x": 1287,
                    "y": 2931
                },
                {
                    "x": 1258,
                    "y": 2931
                }
            ],
            "category": "footer",
            "html": "<footer id='25' style='font-size:14px'>3</footer>",
            "id": 25,
            "page": 3,
            "text": "3"
        },
        {
            "bounding_box": [
                {
                    "x": 548,
                    "y": 524
                },
                {
                    "x": 1997,
                    "y": 524
                },
                {
                    "x": 1997,
                    "y": 877
                },
                {
                    "x": 548,
                    "y": 877
                }
            ],
            "category": "paragraph",
            "html": "<p id='26' style='font-size:18px'>Recently Vision transformers (ViT) [15] closed the gap with the state of the<br>art on ImageNet, without using any convolution. This performance is remark-<br>able since convnet methods for image classification have benefited from years<br>of tuning and optimization [22, 55]. Nevertheless, according to this study [15],<br>a pre-training phase on a large volume of curated data is required for the<br>learned transformer to be effective. In our paper we achieve a strong perfor-<br>mance without requiring a large training dataset, i.e., with Imagenet1k only.</p>",
            "id": 26,
            "page": 4,
            "text": "Recently Vision transformers (ViT) [15] closed the gap with the state of the\nart on ImageNet, without using any convolution. This performance is remark-\nable since convnet methods for image classification have benefited from years\nof tuning and optimization [22, 55]. Nevertheless, according to this study [15],\na pre-training phase on a large volume of curated data is required for the\nlearned transformer to be effective. In our paper we achieve a strong perfor-\nmance without requiring a large training dataset, i.e., with Imagenet1k only."
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 935
                },
                {
                    "x": 1998,
                    "y": 935
                },
                {
                    "x": 1998,
                    "y": 1236
                },
                {
                    "x": 549,
                    "y": 1236
                }
            ],
            "category": "paragraph",
            "html": "<p id='27' style='font-size:20px'>The Transformer architecture, introduced by Vaswani et al. [52] for machine<br>translation are currently the reference model for all natural language process-<br>ing (NLP) tasks. Many improvements of convnets for image classification are<br>inspired by transformers. For example, Squeeze and Excitation [2], Selective<br>Kernel [34] and Split-Attention Networks [61] exploit mechanism akin to trans-<br>formers self-attention (SA) mechanism.</p>",
            "id": 27,
            "page": 4,
            "text": "The Transformer architecture, introduced by Vaswani et al. [52] for machine\ntranslation are currently the reference model for all natural language process-\ning (NLP) tasks. Many improvements of convnets for image classification are\ninspired by transformers. For example, Squeeze and Excitation [2], Selective\nKernel [34] and Split-Attention Networks [61] exploit mechanism akin to trans-\nformers self-attention (SA) mechanism."
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 1296
                },
                {
                    "x": 1998,
                    "y": 1296
                },
                {
                    "x": 1998,
                    "y": 2248
                },
                {
                    "x": 549,
                    "y": 2248
                }
            ],
            "category": "paragraph",
            "html": "<p id='28' style='font-size:14px'>Knowledge Distillation (KD), introduced by Hinton et al. [24], refers to the<br>training paradigm in which a student model leverages \"soft\" labels coming<br>from a strong teacher network. This is the output vector of the teacher`s softmax<br>function rather than just the maximum of scores, wich gives a \"hard\" label.<br>Such a training improves the performance of the student model (alternatively,<br>it can be regarded as a form of compression of the teacher model into a smaller<br>one - the student). On the one hand the teacher's soft labels will have a similar<br>effect to labels smoothing [58]. On the other hand as shown by Wei et al. [54]<br>the teacher's supervision takes into account the effects of the data augmenta-<br>tion, which sometimes causes a misalignment between the real label and the<br>image. For example, let us consider image with a \"cat\" label that represents a<br>large landscape and a small cat in a corner. If the cat is no longer on the crop<br>of the data augmentation it implicitly changes the label of the image. KD can<br>transfer inductive biases [1] in a soft way in a student model using a teacher<br>model where they would be incorporated in a hard way. For example, it may<br>be useful to induce biases due to convolutions in a transformer model by using<br>a convolutional model as teacher. In our paper we study the distillation of a<br>transformer student by either a convnet or a transformer teacher. We introduce<br>a new distillation procedure specific to transformers and show its superiority.</p>",
            "id": 28,
            "page": 4,
            "text": "Knowledge Distillation (KD), introduced by Hinton et al. [24], refers to the\ntraining paradigm in which a student model leverages \"soft\" labels coming\nfrom a strong teacher network. This is the output vector of the teacher`s softmax\nfunction rather than just the maximum of scores, wich gives a \"hard\" label.\nSuch a training improves the performance of the student model (alternatively,\nit can be regarded as a form of compression of the teacher model into a smaller\none - the student). On the one hand the teacher's soft labels will have a similar\neffect to labels smoothing [58]. On the other hand as shown by Wei et al. [54]\nthe teacher's supervision takes into account the effects of the data augmenta-\ntion, which sometimes causes a misalignment between the real label and the\nimage. For example, let us consider image with a \"cat\" label that represents a\nlarge landscape and a small cat in a corner. If the cat is no longer on the crop\nof the data augmentation it implicitly changes the label of the image. KD can\ntransfer inductive biases [1] in a soft way in a student model using a teacher\nmodel where they would be incorporated in a hard way. For example, it may\nbe useful to induce biases due to convolutions in a transformer model by using\na convolutional model as teacher. In our paper we study the distillation of a\ntransformer student by either a convnet or a transformer teacher. We introduce\na new distillation procedure specific to transformers and show its superiority."
        },
        {
            "bounding_box": [
                {
                    "x": 551,
                    "y": 2327
                },
                {
                    "x": 1452,
                    "y": 2327
                },
                {
                    "x": 1452,
                    "y": 2388
                },
                {
                    "x": 551,
                    "y": 2388
                }
            ],
            "category": "paragraph",
            "html": "<p id='29' style='font-size:22px'>3 Vision transformer: overview</p>",
            "id": 29,
            "page": 4,
            "text": "3 Vision transformer: overview"
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 2434
                },
                {
                    "x": 1993,
                    "y": 2434
                },
                {
                    "x": 1993,
                    "y": 2533
                },
                {
                    "x": 549,
                    "y": 2533
                }
            ],
            "category": "paragraph",
            "html": "<p id='30' style='font-size:16px'>In this section, we briefly recall preliminaries associated with the vision trans-<br>former [15, 52], and further discuss positional encoding and resolution.</p>",
            "id": 30,
            "page": 4,
            "text": "In this section, we briefly recall preliminaries associated with the vision trans-\nformer [15, 52], and further discuss positional encoding and resolution."
        },
        {
            "bounding_box": [
                {
                    "x": 548,
                    "y": 2596
                },
                {
                    "x": 1996,
                    "y": 2596
                },
                {
                    "x": 1996,
                    "y": 2796
                },
                {
                    "x": 548,
                    "y": 2796
                }
            ],
            "category": "paragraph",
            "html": "<p id='31' style='font-size:14px'>Multi-head Self Attention layers (MSA). The attention mechanism is based<br>on a trainable associative memory with (key, value) vector pairs. A query vector<br>q E Rd is matched against a set of k key vectors (packed together into a matrix<br>K E Rkxd) using inner products. These inner products are then scaled and</p>",
            "id": 31,
            "page": 4,
            "text": "Multi-head Self Attention layers (MSA). The attention mechanism is based\non a trainable associative memory with (key, value) vector pairs. A query vector\nq E Rd is matched against a set of k key vectors (packed together into a matrix\nK E Rkxd) using inner products. These inner products are then scaled and"
        },
        {
            "bounding_box": [
                {
                    "x": 1259,
                    "y": 2896
                },
                {
                    "x": 1286,
                    "y": 2896
                },
                {
                    "x": 1286,
                    "y": 2931
                },
                {
                    "x": 1259,
                    "y": 2931
                }
            ],
            "category": "footer",
            "html": "<footer id='32' style='font-size:14px'>4</footer>",
            "id": 32,
            "page": 4,
            "text": "4"
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 526
                },
                {
                    "x": 1995,
                    "y": 526
                },
                {
                    "x": 1995,
                    "y": 722
                },
                {
                    "x": 549,
                    "y": 722
                }
            ],
            "category": "paragraph",
            "html": "<p id='33' style='font-size:16px'>normalized with a softmax function to obtain k weights. The output of the<br>attention is the weighted sum of a set of k value vectors (packed into V E Rkxd).<br>For a sequence of N query vectors (packed into Q E RNxd), it produces an<br>output matrix (of size N x d):</p>",
            "id": 33,
            "page": 5,
            "text": "normalized with a softmax function to obtain k weights. The output of the\nattention is the weighted sum of a set of k value vectors (packed into V E Rkxd).\nFor a sequence of N query vectors (packed into Q E RNxd), it produces an\noutput matrix (of size N x d):"
        },
        {
            "bounding_box": [
                {
                    "x": 550,
                    "y": 846
                },
                {
                    "x": 1997,
                    "y": 846
                },
                {
                    "x": 1997,
                    "y": 1401
                },
                {
                    "x": 550,
                    "y": 1401
                }
            ],
            "category": "paragraph",
            "html": "<p id='34' style='font-size:18px'>where the Softmax function is applied over each row of the input matrix and<br>the Vd term provides appropriate normalization.<br>In [52], a Self-attention layer is proposed. Query, key and values matrices<br>are themselves computed from a sequence of N input vectors (packed into<br>X E RNxD): Q = XWQ, K = XWK, V = XWv, using linear transformations<br>WQ, WK, Wv with the constraint k = N, meaning that the attention is in be-<br>tween all the input vectors.<br>Finally, Multi-head self-attention layer (MSA) is defined by considering h at-<br>tention \"heads\" , ie h self-attention functions applied to the input. Each head<br>provides a sequence of size N x d. These h sequences are rearranged into a<br>N x dh sequence that is reprojected by a linear layer into N x D.</p>",
            "id": 34,
            "page": 5,
            "text": "where the Softmax function is applied over each row of the input matrix and\nthe Vd term provides appropriate normalization.\nIn [52], a Self-attention layer is proposed. Query, key and values matrices\nare themselves computed from a sequence of N input vectors (packed into\nX E RNxD): Q = XWQ, K = XWK, V = XWv, using linear transformations\nWQ, WK, Wv with the constraint k = N, meaning that the attention is in be-\ntween all the input vectors.\nFinally, Multi-head self-attention layer (MSA) is defined by considering h at-\ntention \"heads\" , ie h self-attention functions applied to the input. Each head\nprovides a sequence of size N x d. These h sequences are rearranged into a\nN x dh sequence that is reprojected by a linear layer into N x D."
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 1459
                },
                {
                    "x": 1996,
                    "y": 1459
                },
                {
                    "x": 1996,
                    "y": 1757
                },
                {
                    "x": 549,
                    "y": 1757
                }
            ],
            "category": "paragraph",
            "html": "<p id='35' style='font-size:18px'>Transformer block for images. To get a full transformer block as in [52], we<br>add a Feed-Forward Network (FFN) on top of the MSA layer. This FFN is<br>composed of two linear layers separated by a GeLu activation [23]. The first<br>linear layer expands the dimension from D to 4D, and the second layer reduces<br>the dimension from 4D back to D. Both MSA and FFN are operating as residual<br>operators thank to skip-connections, and with a layer normalization [3].</p>",
            "id": 35,
            "page": 5,
            "text": "Transformer block for images. To get a full transformer block as in [52], we\nadd a Feed-Forward Network (FFN) on top of the MSA layer. This FFN is\ncomposed of two linear layers separated by a GeLu activation [23]. The first\nlinear layer expands the dimension from D to 4D, and the second layer reduces\nthe dimension from 4D back to D. Both MSA and FFN are operating as residual\noperators thank to skip-connections, and with a layer normalization [3]."
        },
        {
            "bounding_box": [
                {
                    "x": 548,
                    "y": 1762
                },
                {
                    "x": 1999,
                    "y": 1762
                },
                {
                    "x": 1999,
                    "y": 2307
                },
                {
                    "x": 548,
                    "y": 2307
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='36' style='font-size:16px'>In order to get a transformer to process images, our work builds upon the<br>ViT model [15]. It is a simple and elegant architecture that processes input<br>images as if they were a sequence of input tokens. The fixed-size input RGB<br>image is decomposed into a batch of N patches of a fixed size of 16 x 16 pixels<br>(N = 14 x 14). Each patch is projected with a linear layer that conserves its<br>overall dimension 3 x 16 x 16 = 768.<br>The transformer block described above is invariant to the order of the patch<br>embeddings, and thus does not consider their relative position. The positional<br>information is incorporated as fixed [52] or trainable [18] positional embed-<br>dings. They are added before the first transformer block to the patch tokens,<br>which are then fed to the stack of transformer blocks.</p>",
            "id": 36,
            "page": 5,
            "text": "In order to get a transformer to process images, our work builds upon the\nViT model [15]. It is a simple and elegant architecture that processes input\nimages as if they were a sequence of input tokens. The fixed-size input RGB\nimage is decomposed into a batch of N patches of a fixed size of 16 x 16 pixels\n(N = 14 x 14). Each patch is projected with a linear layer that conserves its\noverall dimension 3 x 16 x 16 = 768.\nThe transformer block described above is invariant to the order of the patch\nembeddings, and thus does not consider their relative position. The positional\ninformation is incorporated as fixed [52] or trainable [18] positional embed-\ndings. They are added before the first transformer block to the patch tokens,\nwhich are then fed to the stack of transformer blocks."
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 2366
                },
                {
                    "x": 1994,
                    "y": 2366
                },
                {
                    "x": 1994,
                    "y": 2820
                },
                {
                    "x": 549,
                    "y": 2820
                }
            ],
            "category": "paragraph",
            "html": "<p id='37' style='font-size:20px'>The class token is a trainable vector, appended to the patch tokens before the<br>first layer, that goes through the transformer layers, and is then projected with<br>a linear layer to predict the class. This class token is inherited from NLP [14],<br>and departs from the typical pooling layers used in computer vision to predict<br>the class. The transformer thus process batches of (N + 1) tokens of dimension<br>D, of which only the class vector is used to predict the output. This architecture<br>forces the self-attention to spread information between the patch tokens and<br>the class token: at training time the supervision signal comes only from the<br>class embedding, while the patch tokens are the model's only variable input.</p>",
            "id": 37,
            "page": 5,
            "text": "The class token is a trainable vector, appended to the patch tokens before the\nfirst layer, that goes through the transformer layers, and is then projected with\na linear layer to predict the class. This class token is inherited from NLP [14],\nand departs from the typical pooling layers used in computer vision to predict\nthe class. The transformer thus process batches of (N + 1) tokens of dimension\nD, of which only the class vector is used to predict the output. This architecture\nforces the self-attention to spread information between the patch tokens and\nthe class token: at training time the supervision signal comes only from the\nclass embedding, while the patch tokens are the model's only variable input."
        },
        {
            "bounding_box": [
                {
                    "x": 1258,
                    "y": 2894
                },
                {
                    "x": 1287,
                    "y": 2894
                },
                {
                    "x": 1287,
                    "y": 2931
                },
                {
                    "x": 1258,
                    "y": 2931
                }
            ],
            "category": "footer",
            "html": "<footer id='38' style='font-size:14px'>5</footer>",
            "id": 38,
            "page": 5,
            "text": "5"
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 521
                },
                {
                    "x": 1997,
                    "y": 521
                },
                {
                    "x": 1997,
                    "y": 1076
                },
                {
                    "x": 549,
                    "y": 1076
                }
            ],
            "category": "paragraph",
            "html": "<p id='39' style='font-size:16px'>Fixing the positional encoding across resolutions. Touvron et al. [50] show<br>that it is desirable to use a lower training resolution and fine-tune the network<br>at the larger resolution. This speeds up the full training and improves the accu-<br>racy under prevailing data augmentation schemes. When increasing the reso-<br>lution of an input image, we keep the patch size the same, therefore the number<br>N of input patches does change. Due to the architecture of transformer blocks<br>and the class token, the model and classifier do not need to be modified to pro-<br>cess more tokens. In contrast, one needs to adapt the positional embeddings,<br>because there are N of them, one for each patch. Dosovitskiy et al. [15] inter-<br>polate the positional encoding when changing the resolution and demonstrate<br>that this method works with the subsequent fine-tuning stage.</p>",
            "id": 39,
            "page": 6,
            "text": "Fixing the positional encoding across resolutions. Touvron et al. [50] show\nthat it is desirable to use a lower training resolution and fine-tune the network\nat the larger resolution. This speeds up the full training and improves the accu-\nracy under prevailing data augmentation schemes. When increasing the reso-\nlution of an input image, we keep the patch size the same, therefore the number\nN of input patches does change. Due to the architecture of transformer blocks\nand the class token, the model and classifier do not need to be modified to pro-\ncess more tokens. In contrast, one needs to adapt the positional embeddings,\nbecause there are N of them, one for each patch. Dosovitskiy et al. [15] inter-\npolate the positional encoding when changing the resolution and demonstrate\nthat this method works with the subsequent fine-tuning stage."
        },
        {
            "bounding_box": [
                {
                    "x": 552,
                    "y": 1152
                },
                {
                    "x": 1460,
                    "y": 1152
                },
                {
                    "x": 1460,
                    "y": 1214
                },
                {
                    "x": 552,
                    "y": 1214
                }
            ],
            "category": "paragraph",
            "html": "<p id='40' style='font-size:22px'>4 Distillation through attention</p>",
            "id": 40,
            "page": 6,
            "text": "4 Distillation through attention"
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 1257
                },
                {
                    "x": 1996,
                    "y": 1257
                },
                {
                    "x": 1996,
                    "y": 1608
                },
                {
                    "x": 549,
                    "y": 1608
                }
            ],
            "category": "paragraph",
            "html": "<p id='41' style='font-size:14px'>In this section we assume we have access to a strong image classifier as a<br>teacher model. It could be a convnet, or a mixture of classifiers. We address<br>the question of how to learn a transformer by exploiting this teacher. As we<br>will see in Section 5 by comparing the trade-off between accuracy and image<br>throughput, it can be beneficial to replace a convolutional neural network by<br>a transformer. This section covers two axes of distillation: hard distillation<br>versus soft distillation, and classical distillation versus the distillation token.</p>",
            "id": 41,
            "page": 6,
            "text": "In this section we assume we have access to a strong image classifier as a\nteacher model. It could be a convnet, or a mixture of classifiers. We address\nthe question of how to learn a transformer by exploiting this teacher. As we\nwill see in Section 5 by comparing the trade-off between accuracy and image\nthroughput, it can be beneficial to replace a convolutional neural network by\na transformer. This section covers two axes of distillation: hard distillation\nversus soft distillation, and classical distillation versus the distillation token."
        },
        {
            "bounding_box": [
                {
                    "x": 550,
                    "y": 1668
                },
                {
                    "x": 1994,
                    "y": 1668
                },
                {
                    "x": 1994,
                    "y": 1764
                },
                {
                    "x": 550,
                    "y": 1764
                }
            ],
            "category": "paragraph",
            "html": "<p id='42' style='font-size:16px'>Soft distillation [24, 54] minimizes the Kullback-Leibler divergence between<br>the softmax of the teacher and the softmax of the student model.</p>",
            "id": 42,
            "page": 6,
            "text": "Soft distillation [24, 54] minimizes the Kullback-Leibler divergence between\nthe softmax of the teacher and the softmax of the student model."
        },
        {
            "bounding_box": [
                {
                    "x": 550,
                    "y": 1769
                },
                {
                    "x": 1995,
                    "y": 1769
                },
                {
                    "x": 1995,
                    "y": 1969
                },
                {
                    "x": 550,
                    "y": 1969
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='43' style='font-size:18px'>Let Zt be the logits of the teacher model, Zs the logits of the student model.<br>We denote by T the temperature for the distillation, 入 the coefficient balanc-<br>ing the Kullback-Leibler divergence loss (KL) and the cross-entropy (LCE) on<br>ground truth labels y, and V the softmax function. The distillation objective is</p>",
            "id": 43,
            "page": 6,
            "text": "Let Zt be the logits of the teacher model, Zs the logits of the student model.\nWe denote by T the temperature for the distillation, 入 the coefficient balanc-\ning the Kullback-Leibler divergence loss (KL) and the cross-entropy (LCE) on\nground truth labels y, and V the softmax function. The distillation objective is"
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 2125
                },
                {
                    "x": 1995,
                    "y": 2125
                },
                {
                    "x": 1995,
                    "y": 2322
                },
                {
                    "x": 549,
                    "y": 2322
                }
            ],
            "category": "paragraph",
            "html": "<p id='44' style='font-size:14px'>Hard-label distillation. We introduce a variant of distillation where we take<br>the hard decision of the teacher as a true label. Let yt = argmaxcZt(c) be the<br>hard decision of the teacher, the objective associated with this hard-label distil-<br>lation is:</p>",
            "id": 44,
            "page": 6,
            "text": "Hard-label distillation. We introduce a variant of distillation where we take\nthe hard decision of the teacher as a true label. Let yt = argmaxcZt(c) be the\nhard decision of the teacher, the objective associated with this hard-label distil-\nlation is:"
        },
        {
            "bounding_box": [
                {
                    "x": 548,
                    "y": 2417
                },
                {
                    "x": 1996,
                    "y": 2417
                },
                {
                    "x": 1996,
                    "y": 2615
                },
                {
                    "x": 548,
                    "y": 2615
                }
            ],
            "category": "paragraph",
            "html": "<p id='45' style='font-size:16px'>For a given image, the hard label associated with the teacher may change<br>depending on the specific data augmentation. We will see that this choice is<br>better than the traditional one, while being parameter-free and conceptually<br>simpler: The teacher prediction Yt plays the same role as the true label y.</p>",
            "id": 45,
            "page": 6,
            "text": "For a given image, the hard label associated with the teacher may change\ndepending on the specific data augmentation. We will see that this choice is\nbetter than the traditional one, while being parameter-free and conceptually\nsimpler: The teacher prediction Yt plays the same role as the true label y."
        },
        {
            "bounding_box": [
                {
                    "x": 548,
                    "y": 2617
                },
                {
                    "x": 1997,
                    "y": 2617
                },
                {
                    "x": 1997,
                    "y": 2817
                },
                {
                    "x": 548,
                    "y": 2817
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='46' style='font-size:14px'>Note also that the hard labels can also be converted into soft labels with<br>label smoothing [47], where the true label is considered to have a probability<br>of 1 - E, and the remaining & is shared across the remaining classes. We fix this<br>parameter to E = 0.1 in our all experiments that use true labels.</p>",
            "id": 46,
            "page": 6,
            "text": "Note also that the hard labels can also be converted into soft labels with\nlabel smoothing [47], where the true label is considered to have a probability\nof 1 - E, and the remaining & is shared across the remaining classes. We fix this\nparameter to E = 0.1 in our all experiments that use true labels."
        },
        {
            "bounding_box": [
                {
                    "x": 1258,
                    "y": 2896
                },
                {
                    "x": 1287,
                    "y": 2896
                },
                {
                    "x": 1287,
                    "y": 2932
                },
                {
                    "x": 1258,
                    "y": 2932
                }
            ],
            "category": "footer",
            "html": "<footer id='47' style='font-size:14px'>6</footer>",
            "id": 47,
            "page": 6,
            "text": "6"
        },
        {
            "bounding_box": [
                {
                    "x": 906,
                    "y": 497
                },
                {
                    "x": 1633,
                    "y": 497
                },
                {
                    "x": 1633,
                    "y": 1627
                },
                {
                    "x": 906,
                    "y": 1627
                }
            ],
            "category": "figure",
            "html": "<figure><img id='48' style='font-size:16px' alt=\"LCE Lteacher\n↑ 介\nFFN\nself - attention\n↑\nclass patch distillation\ntoken tokens token\" data-coord=\"top-left:(906,497); bottom-right:(1633,1627)\" /></figure>",
            "id": 48,
            "page": 7,
            "text": "LCE Lteacher\n↑ 介\nFFN\nself - attention\n↑\nclass patch distillation\ntoken tokens token"
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 1668
                },
                {
                    "x": 1997,
                    "y": 1668
                },
                {
                    "x": 1997,
                    "y": 1973
                },
                {
                    "x": 549,
                    "y": 1973
                }
            ],
            "category": "caption",
            "html": "<caption id='49' style='font-size:18px'>Figure 2: Our distillation procedure: we simply include a new distillation token.<br>It interacts with the class and patch tokens through the self-attention layers.<br>This distillation token is employed in a similar fashion as the class token, ex-<br>cept that on output of the network its objective is to reproduce the (hard) label<br>predicted by the teacher, instead of true label. Both the class and distillation<br>tokens input to the transformers are learned by back-propagation.</caption>",
            "id": 49,
            "page": 7,
            "text": "Figure 2: Our distillation procedure: we simply include a new distillation token.\nIt interacts with the class and patch tokens through the self-attention layers.\nThis distillation token is employed in a similar fashion as the class token, ex-\ncept that on output of the network its objective is to reproduce the (hard) label\npredicted by the teacher, instead of true label. Both the class and distillation\ntokens input to the transformers are learned by back-propagation."
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 2055
                },
                {
                    "x": 1997,
                    "y": 2055
                },
                {
                    "x": 1997,
                    "y": 2453
                },
                {
                    "x": 549,
                    "y": 2453
                }
            ],
            "category": "paragraph",
            "html": "<p id='50' style='font-size:20px'>Distillation token. We now focus on our proposal, which is illustrated in<br>Figure 2. We add a new token, the distillation token, to the initial embeddings<br>(patches and class token). Our distillation token is used similarly as the class<br>token: it interacts with other embeddings through self-attention, and is output<br>by the network after the last layer. Its target objective is given by the distillation<br>component of the loss. The distillation embedding allows our model to learn<br>from the output of the teacher, as in a regular distillation, while remaining<br>complementary to the class embedding.</p>",
            "id": 50,
            "page": 7,
            "text": "Distillation token. We now focus on our proposal, which is illustrated in\nFigure 2. We add a new token, the distillation token, to the initial embeddings\n(patches and class token). Our distillation token is used similarly as the class\ntoken: it interacts with other embeddings through self-attention, and is output\nby the network after the last layer. Its target objective is given by the distillation\ncomponent of the loss. The distillation embedding allows our model to learn\nfrom the output of the teacher, as in a regular distillation, while remaining\ncomplementary to the class embedding."
        },
        {
            "bounding_box": [
                {
                    "x": 548,
                    "y": 2455
                },
                {
                    "x": 1999,
                    "y": 2455
                },
                {
                    "x": 1999,
                    "y": 2803
                },
                {
                    "x": 548,
                    "y": 2803
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='51' style='font-size:18px'>Interestingly, we observe that the learned class and distillation tokens con-<br>verge towards different vectors: the average cosine similarity between these<br>tokens equal to 0.06. As the class and distillation embeddings are computed<br>at each layer, they gradually become more similar through the network, all the<br>way through the last layer at which their similarity is high (cos=0.93), but still<br>lower than 1. This is expected since as they aim at producing targets that are<br>similar but not identical.</p>",
            "id": 51,
            "page": 7,
            "text": "Interestingly, we observe that the learned class and distillation tokens con-\nverge towards different vectors: the average cosine similarity between these\ntokens equal to 0.06. As the class and distillation embeddings are computed\nat each layer, they gradually become more similar through the network, all the\nway through the last layer at which their similarity is high (cos=0.93), but still\nlower than 1. This is expected since as they aim at producing targets that are\nsimilar but not identical."
        },
        {
            "bounding_box": [
                {
                    "x": 1259,
                    "y": 2895
                },
                {
                    "x": 1286,
                    "y": 2895
                },
                {
                    "x": 1286,
                    "y": 2928
                },
                {
                    "x": 1259,
                    "y": 2928
                }
            ],
            "category": "footer",
            "html": "<footer id='52' style='font-size:14px'>7</footer>",
            "id": 52,
            "page": 7,
            "text": "7"
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 522
                },
                {
                    "x": 1998,
                    "y": 522
                },
                {
                    "x": 1998,
                    "y": 975
                },
                {
                    "x": 549,
                    "y": 975
                }
            ],
            "category": "paragraph",
            "html": "<p id='53' style='font-size:18px'>We verified that our distillation token adds something to the model, com-<br>pared to simply adding an additional class token associated with the same tar-<br>get label: instead of a teacher pseudo-label, we experimented with a trans-<br>former with two class tokens. Even if we initialize them randomly and inde-<br>pendently, during training they converge towards the same vector (cos=0.999),<br>and the output embedding are also quasi-identical. This additional class token<br>does not bring anything to the classification performance. In contrast, our dis-<br>tillation strategy provides a significant improvement over a vanilla distillation<br>baseline, as validated by our experiments in Section 5.2.</p>",
            "id": 53,
            "page": 8,
            "text": "We verified that our distillation token adds something to the model, com-\npared to simply adding an additional class token associated with the same tar-\nget label: instead of a teacher pseudo-label, we experimented with a trans-\nformer with two class tokens. Even if we initialize them randomly and inde-\npendently, during training they converge towards the same vector (cos=0.999),\nand the output embedding are also quasi-identical. This additional class token\ndoes not bring anything to the classification performance. In contrast, our dis-\ntillation strategy provides a significant improvement over a vanilla distillation\nbaseline, as validated by our experiments in Section 5.2."
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 1035
                },
                {
                    "x": 1997,
                    "y": 1035
                },
                {
                    "x": 1997,
                    "y": 1286
                },
                {
                    "x": 549,
                    "y": 1286
                }
            ],
            "category": "paragraph",
            "html": "<p id='54' style='font-size:18px'>Fine-tuning with distillation. We use both the true label and teacher predic-<br>tion during the fine-tuning stage athigher resolution. We use a teacher with the<br>same target resolution, typically obtained from the lower-resolution teacher by<br>the method of Touvron et al [50]. We have also tested with true labels only but<br>this reduces the benefit of the teacher and leads to a lower performance.</p>",
            "id": 54,
            "page": 8,
            "text": "Fine-tuning with distillation. We use both the true label and teacher predic-\ntion during the fine-tuning stage athigher resolution. We use a teacher with the\nsame target resolution, typically obtained from the lower-resolution teacher by\nthe method of Touvron et al [50]. We have also tested with true labels only but\nthis reduces the benefit of the teacher and leads to a lower performance."
        },
        {
            "bounding_box": [
                {
                    "x": 548,
                    "y": 1346
                },
                {
                    "x": 1998,
                    "y": 1346
                },
                {
                    "x": 1998,
                    "y": 1649
                },
                {
                    "x": 548,
                    "y": 1649
                }
            ],
            "category": "paragraph",
            "html": "<p id='55' style='font-size:16px'>Classification with our approach: joint classifiers. At test time, both the<br>class or the distillation embeddings produced by the transformer are associ-<br>ated with linear classifiers and able to infer the image label. Yet our referent<br>method is the late fusion of these two separate heads, for which we add the<br>softmax output by the two classifiers to make the prediction. We evaluate these<br>three options in Section 5.</p>",
            "id": 55,
            "page": 8,
            "text": "Classification with our approach: joint classifiers. At test time, both the\nclass or the distillation embeddings produced by the transformer are associ-\nated with linear classifiers and able to infer the image label. Yet our referent\nmethod is the late fusion of these two separate heads, for which we add the\nsoftmax output by the two classifiers to make the prediction. We evaluate these\nthree options in Section 5."
        },
        {
            "bounding_box": [
                {
                    "x": 552,
                    "y": 1730
                },
                {
                    "x": 992,
                    "y": 1730
                },
                {
                    "x": 992,
                    "y": 1793
                },
                {
                    "x": 552,
                    "y": 1793
                }
            ],
            "category": "paragraph",
            "html": "<p id='56' style='font-size:22px'>5 Experiments</p>",
            "id": 56,
            "page": 8,
            "text": "5 Experiments"
        },
        {
            "bounding_box": [
                {
                    "x": 548,
                    "y": 1834
                },
                {
                    "x": 1996,
                    "y": 1834
                },
                {
                    "x": 1996,
                    "y": 1985
                },
                {
                    "x": 548,
                    "y": 1985
                }
            ],
            "category": "paragraph",
            "html": "<p id='57' style='font-size:18px'>This section presents a few analytical experiments and results. We first discuss<br>our distillation strategy. Then we comparatively analyze the efficiency and<br>accuracy of convnets and vision transformers.</p>",
            "id": 57,
            "page": 8,
            "text": "This section presents a few analytical experiments and results. We first discuss\nour distillation strategy. Then we comparatively analyze the efficiency and\naccuracy of convnets and vision transformers."
        },
        {
            "bounding_box": [
                {
                    "x": 552,
                    "y": 2051
                },
                {
                    "x": 1131,
                    "y": 2051
                },
                {
                    "x": 1131,
                    "y": 2106
                },
                {
                    "x": 552,
                    "y": 2106
                }
            ],
            "category": "paragraph",
            "html": "<p id='58' style='font-size:20px'>5.1 Transformer models</p>",
            "id": 58,
            "page": 8,
            "text": "5.1 Transformer models"
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 2135
                },
                {
                    "x": 1996,
                    "y": 2135
                },
                {
                    "x": 1996,
                    "y": 2582
                },
                {
                    "x": 549,
                    "y": 2582
                }
            ],
            "category": "paragraph",
            "html": "<p id='59' style='font-size:18px'>As mentioned earlier, our architecture design is identical to the one proposed<br>by Dosovitskiy et al. [15] with no convolutions. Our only differences are the<br>training strategies, and the distillation token. Also we do not use a MLP head<br>for the pre-training but only a linear classifier. To avoid any confusion, we refer<br>to the results obtained in the prior work by ViT, and prefix ours by DeiT. If not<br>specified, DeiT refers to our referent model DeiT-B, which has the same archi-<br>tecture as ViT-B. When we fine-tune DeiT at a larger resolution, we append the<br>resulting operating resolution at the end, e.g, DeiT-B↑384. Last, when using<br>our distillation procedure, we identify it with an alembic sign as DeiTm.</p>",
            "id": 59,
            "page": 8,
            "text": "As mentioned earlier, our architecture design is identical to the one proposed\nby Dosovitskiy et al. [15] with no convolutions. Our only differences are the\ntraining strategies, and the distillation token. Also we do not use a MLP head\nfor the pre-training but only a linear classifier. To avoid any confusion, we refer\nto the results obtained in the prior work by ViT, and prefix ours by DeiT. If not\nspecified, DeiT refers to our referent model DeiT-B, which has the same archi-\ntecture as ViT-B. When we fine-tune DeiT at a larger resolution, we append the\nresulting operating resolution at the end, e.g, DeiT-B↑384. Last, when using\nour distillation procedure, we identify it with an alembic sign as DeiTm."
        },
        {
            "bounding_box": [
                {
                    "x": 548,
                    "y": 2585
                },
                {
                    "x": 1997,
                    "y": 2585
                },
                {
                    "x": 1997,
                    "y": 2785
                },
                {
                    "x": 548,
                    "y": 2785
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='60' style='font-size:16px'>The parameters of ViT-B (and therefore of DeiT-B) are fixed as D = 768,<br>h = 12 and d = D/h = 64. We introduce two smaller models, namely DeiT-S<br>and DeiT-Ti, for which we change the number ofheads, keeping d fixed. Table 1<br>summarizes the models that we consider in our paper.</p>",
            "id": 60,
            "page": 8,
            "text": "The parameters of ViT-B (and therefore of DeiT-B) are fixed as D = 768,\nh = 12 and d = D/h = 64. We introduce two smaller models, namely DeiT-S\nand DeiT-Ti, for which we change the number ofheads, keeping d fixed. Table 1\nsummarizes the models that we consider in our paper."
        },
        {
            "bounding_box": [
                {
                    "x": 1258,
                    "y": 2894
                },
                {
                    "x": 1288,
                    "y": 2894
                },
                {
                    "x": 1288,
                    "y": 2932
                },
                {
                    "x": 1258,
                    "y": 2932
                }
            ],
            "category": "footer",
            "html": "<footer id='61' style='font-size:14px'>8</footer>",
            "id": 61,
            "page": 8,
            "text": "8"
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 554
                },
                {
                    "x": 1996,
                    "y": 554
                },
                {
                    "x": 1996,
                    "y": 851
                },
                {
                    "x": 549,
                    "y": 851
                }
            ],
            "category": "caption",
            "html": "<caption id='62' style='font-size:18px'>Table 1: Variants of our DeiT architecture. The larger model, DeiT-B, has the<br>same architecture as the ViT-B [15]. The only parameters that vary across mod-<br>els are the embedding dimension and the number of heads, and we keep the<br>dimension per head constant (equal to 64). Smaller models have a lower pa-<br>rameter count, and a faster throughput. The throughput is measured for im-<br>ages at resolution 224x 224.</caption>",
            "id": 62,
            "page": 9,
            "text": "Table 1: Variants of our DeiT architecture. The larger model, DeiT-B, has the\nsame architecture as the ViT-B [15]. The only parameters that vary across mod-\nels are the embedding dimension and the number of heads, and we keep the\ndimension per head constant (equal to 64). Smaller models have a lower pa-\nrameter count, and a faster throughput. The throughput is measured for im-\nages at resolution 224x 224."
        },
        {
            "bounding_box": [
                {
                    "x": 570,
                    "y": 866
                },
                {
                    "x": 1987,
                    "y": 866
                },
                {
                    "x": 1987,
                    "y": 1112
                },
                {
                    "x": 570,
                    "y": 1112
                }
            ],
            "category": "table",
            "html": "<br><table id='63' style='font-size:14px'><tr><td>Model</td><td>ViT model</td><td>embedding dimension</td><td>#heads</td><td>#layers</td><td>#params</td><td>training resolution</td><td>throughput (im/sec)</td></tr><tr><td>DeiT-Ti</td><td>N/A</td><td>192</td><td>3</td><td>12</td><td>5M</td><td>224</td><td>2536</td></tr><tr><td>DeiT-S</td><td>N/A</td><td>384</td><td>6</td><td>12</td><td>22M</td><td>224</td><td>940</td></tr><tr><td>DeiT-B</td><td>ViT-B</td><td>768</td><td>12</td><td>12</td><td>86M</td><td>224</td><td>292</td></tr></table>",
            "id": 63,
            "page": 9,
            "text": "Model ViT model embedding dimension #heads #layers #params training resolution throughput (im/sec)\n DeiT-Ti N/A 192 3 12 5M 224 2536\n DeiT-S N/A 384 6 12 22M 224 940\n DeiT-B ViT-B 768 12 12 86M 224"
        },
        {
            "bounding_box": [
                {
                    "x": 550,
                    "y": 1192
                },
                {
                    "x": 1993,
                    "y": 1192
                },
                {
                    "x": 1993,
                    "y": 1287
                },
                {
                    "x": 550,
                    "y": 1287
                }
            ],
            "category": "caption",
            "html": "<caption id='64' style='font-size:18px'>Table 2: We compare on ImageNet [42] the performance (top-1 acc., %) of the<br>student as a function of the teacher model used for distillation.</caption>",
            "id": 64,
            "page": 9,
            "text": "Table 2: We compare on ImageNet [42] the performance (top-1 acc., %) of the\nstudent as a function of the teacher model used for distillation."
        },
        {
            "bounding_box": [
                {
                    "x": 899,
                    "y": 1295
                },
                {
                    "x": 1654,
                    "y": 1295
                },
                {
                    "x": 1654,
                    "y": 1667
                },
                {
                    "x": 899,
                    "y": 1667
                }
            ],
            "category": "table",
            "html": "<br><table id='65' style='font-size:16px'><tr><td>Teacher Models</td><td>acc.</td><td>Student: pretrain</td><td>DeiT-B 1m ↑384</td></tr><tr><td>DeiT-B</td><td>81.8</td><td>81.9</td><td>83.1</td></tr><tr><td>RegNetY-4GF</td><td>80.0</td><td>82.7</td><td>83.6</td></tr><tr><td>RegNetY-8GF</td><td>81.7</td><td>82.7</td><td>83.8</td></tr><tr><td>RegNetY-12GF</td><td>82.4</td><td>83.1</td><td>84.1</td></tr><tr><td>RegNetY-16GF</td><td>82.9</td><td>83.1</td><td>84.2</td></tr></table>",
            "id": 65,
            "page": 9,
            "text": "Teacher Models acc. Student: pretrain DeiT-B 1m ↑384\n DeiT-B 81.8 81.9 83.1\n RegNetY-4GF 80.0 82.7 83.6\n RegNetY-8GF 81.7 82.7 83.8\n RegNetY-12GF 82.4 83.1 84.1\n RegNetY-16GF 82.9 83.1"
        },
        {
            "bounding_box": [
                {
                    "x": 553,
                    "y": 1744
                },
                {
                    "x": 936,
                    "y": 1744
                },
                {
                    "x": 936,
                    "y": 1796
                },
                {
                    "x": 553,
                    "y": 1796
                }
            ],
            "category": "paragraph",
            "html": "<p id='66' style='font-size:22px'>5.2 Distillation</p>",
            "id": 66,
            "page": 9,
            "text": "5.2 Distillation"
        },
        {
            "bounding_box": [
                {
                    "x": 551,
                    "y": 1829
                },
                {
                    "x": 1994,
                    "y": 1829
                },
                {
                    "x": 1994,
                    "y": 2275
                },
                {
                    "x": 551,
                    "y": 2275
                }
            ],
            "category": "paragraph",
            "html": "<p id='67' style='font-size:20px'>Our distillation method produces a vision transformer that becomes on par<br>with the best convnets in terms of the trade-off between accuracy and through-<br>put, see Table 5. Interestingly, the distilled model outperforms its teacher in<br>terms of the trade-off between accuracy and throughput. Our best model on<br>ImageNet-1k is 85.2% top-1 accuracy outperforms the best Vit-B model pre-<br>trained on JFT-300M at resolution 384 (84.15%). For reference, the current state<br>of the art of 88.55% achieved with extra training data was obtained by the ViT-<br>H model (600M parameters) trained on JFT-300M at resolution 512. Hereafter<br>we provide several analysis and observations.</p>",
            "id": 67,
            "page": 9,
            "text": "Our distillation method produces a vision transformer that becomes on par\nwith the best convnets in terms of the trade-off between accuracy and through-\nput, see Table 5. Interestingly, the distilled model outperforms its teacher in\nterms of the trade-off between accuracy and throughput. Our best model on\nImageNet-1k is 85.2% top-1 accuracy outperforms the best Vit-B model pre-\ntrained on JFT-300M at resolution 384 (84.15%). For reference, the current state\nof the art of 88.55% achieved with extra training data was obtained by the ViT-\nH model (600M parameters) trained on JFT-300M at resolution 512. Hereafter\nwe provide several analysis and observations."
        },
        {
            "bounding_box": [
                {
                    "x": 548,
                    "y": 2341
                },
                {
                    "x": 1996,
                    "y": 2341
                },
                {
                    "x": 1996,
                    "y": 2738
                },
                {
                    "x": 548,
                    "y": 2738
                }
            ],
            "category": "paragraph",
            "html": "<p id='68' style='font-size:20px'>Convnets teachers. We have observed that using a convnet teacher gives bet-<br>ter performance than using a transformer. Table 2 compares distillation results<br>with different teacher architectures. The fact that the convnet is a better teacher<br>is probably due to the inductive bias inherited by the transformers through<br>distillation, as explained in Abnar et al. [1]. In all of our subsequent distilla-<br>tion experiments the default teacher is a RegNetY-16GF [40] (84M parameters)<br>that we trained with the same data and same data-augmentation as DeiT. This<br>teacher reaches 82.9% top-1 accuracy on ImageNet.</p>",
            "id": 68,
            "page": 9,
            "text": "Convnets teachers. We have observed that using a convnet teacher gives bet-\nter performance than using a transformer. Table 2 compares distillation results\nwith different teacher architectures. The fact that the convnet is a better teacher\nis probably due to the inductive bias inherited by the transformers through\ndistillation, as explained in Abnar et al. [1]. In all of our subsequent distilla-\ntion experiments the default teacher is a RegNetY-16GF [40] (84M parameters)\nthat we trained with the same data and same data-augmentation as DeiT. This\nteacher reaches 82.9% top-1 accuracy on ImageNet."
        },
        {
            "bounding_box": [
                {
                    "x": 1258,
                    "y": 2895
                },
                {
                    "x": 1287,
                    "y": 2895
                },
                {
                    "x": 1287,
                    "y": 2930
                },
                {
                    "x": 1258,
                    "y": 2930
                }
            ],
            "category": "footer",
            "html": "<footer id='69' style='font-size:14px'>9</footer>",
            "id": 69,
            "page": 9,
            "text": "9"
        },
        {
            "bounding_box": [
                {
                    "x": 550,
                    "y": 553
                },
                {
                    "x": 1996,
                    "y": 553
                },
                {
                    "x": 1996,
                    "y": 851
                },
                {
                    "x": 550,
                    "y": 851
                }
            ],
            "category": "paragraph",
            "html": "<p id='70' style='font-size:20px'>Table 3: Distillation experiments on Imagenet with DeiT, 300 epochs of<br>pre-<br>training. We report the results for our new distillation method in the last three<br>rows. We separately report the performance when classifying with only one of<br>the class or distillation embeddings, and then with a classifier taking both of<br>them as input. In the last row (class+distillation), the result correspond to the<br>late fusion of the class and distillation classifiers.</p>",
            "id": 70,
            "page": 10,
            "text": "Table 3: Distillation experiments on Imagenet with DeiT, 300 epochs of\npre-\ntraining. We report the results for our new distillation method in the last three\nrows. We separately report the performance when classifying with only one of\nthe class or distillation embeddings, and then with a classifier taking both of\nthem as input. In the last row (class+distillation), the result correspond to the\nlate fusion of the class and distillation classifiers."
        },
        {
            "bounding_box": [
                {
                    "x": 633,
                    "y": 859
                },
                {
                    "x": 1920,
                    "y": 859
                },
                {
                    "x": 1920,
                    "y": 1274
                },
                {
                    "x": 633,
                    "y": 1274
                }
            ],
            "category": "table",
            "html": "<br><table id='71' style='font-size:14px'><tr><td></td><td colspan=\"2\">Supervision</td><td colspan=\"4\">ImageNet top-1 (%)</td></tr><tr><td>method ↓</td><td>label</td><td>teacher</td><td>Ti 224</td><td>S 224</td><td>B 224</td><td>B↑384</td></tr><tr><td>DeiT- no distillation</td><td></td><td>X</td><td>72.2</td><td>79.8</td><td>81.8</td><td>83.1</td></tr><tr><td>DeiT- usual distillation</td><td>X</td><td>soft</td><td>72.2</td><td>79.8</td><td>81.8</td><td>83.2</td></tr><tr><td>DeiT- hard distillation</td><td>X</td><td>hard</td><td>74.3</td><td>80.9</td><td>83.0</td><td>84.0</td></tr><tr><td>DeiTm: class embedding</td><td></td><td>hard</td><td>73.9</td><td>80.9</td><td>83.0</td><td>84.2</td></tr><tr><td>DeiTm: distil. embedding</td><td></td><td>hard</td><td>74.6</td><td>81.1</td><td>83.1</td><td>84.4</td></tr><tr><td>DeiTm: class+distillation</td><td></td><td>hard</td><td>74.5</td><td>81.2</td><td>83.4</td><td>84.5</td></tr></table>",
            "id": 71,
            "page": 10,
            "text": "Supervision ImageNet top-1 (%)\n method ↓ label teacher Ti 224 S 224 B 224 B↑384\n DeiT- no distillation  X 72.2 79.8 81.8 83.1\n DeiT- usual distillation X soft 72.2 79.8 81.8 83.2\n DeiT- hard distillation X hard 74.3 80.9 83.0 84.0\n DeiTm: class embedding  hard 73.9 80.9 83.0 84.2\n DeiTm: distil. embedding  hard 74.6 81.1 83.1 84.4\n DeiTm: class+distillation  hard 74.5 81.2 83.4"
        },
        {
            "bounding_box": [
                {
                    "x": 550,
                    "y": 1353
                },
                {
                    "x": 1995,
                    "y": 1353
                },
                {
                    "x": 1995,
                    "y": 1803
                },
                {
                    "x": 550,
                    "y": 1803
                }
            ],
            "category": "paragraph",
            "html": "<p id='72' style='font-size:22px'>Comparison of distillation methods. We compare the performance of differ-<br>ent distillation strategies in Table 3. Hard distillation significantly outperforms<br>soft distillation for transformers, even when using only a class token: hard dis-<br>tillation reaches 83.0% at resolution 224x224, compared to the soft distillation<br>accuracy of 81.8%. Our distillation strategy from Section 4 further improves<br>the performance, showing that the two tokens provide complementary infor-<br>mation useful for classification: the classifier on the two tokens is significantly<br>better than the independent class and distillation classifiers, which by them-<br>selves already outperform the distillation baseline.</p>",
            "id": 72,
            "page": 10,
            "text": "Comparison of distillation methods. We compare the performance of differ-\nent distillation strategies in Table 3. Hard distillation significantly outperforms\nsoft distillation for transformers, even when using only a class token: hard dis-\ntillation reaches 83.0% at resolution 224x224, compared to the soft distillation\naccuracy of 81.8%. Our distillation strategy from Section 4 further improves\nthe performance, showing that the two tokens provide complementary infor-\nmation useful for classification: the classifier on the two tokens is significantly\nbetter than the independent class and distillation classifiers, which by them-\nselves already outperform the distillation baseline."
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 1806
                },
                {
                    "x": 1997,
                    "y": 1806
                },
                {
                    "x": 1997,
                    "y": 2055
                },
                {
                    "x": 549,
                    "y": 2055
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='73' style='font-size:20px'>The distillation token gives slightly better results than the class token. It<br>is also more correlated to the convnets prediction. This difference in perfor-<br>mance is probably due to the fact that it benefits more from the inductive bias<br>of convnets. We give more details and an analysis in the next paragraph. The<br>distillation token has an undeniable advantage for the initial training.</p>",
            "id": 73,
            "page": 10,
            "text": "The distillation token gives slightly better results than the class token. It\nis also more correlated to the convnets prediction. This difference in perfor-\nmance is probably due to the fact that it benefits more from the inductive bias\nof convnets. We give more details and an analysis in the next paragraph. The\ndistillation token has an undeniable advantage for the initial training."
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 2116
                },
                {
                    "x": 1997,
                    "y": 2116
                },
                {
                    "x": 1997,
                    "y": 2414
                },
                {
                    "x": 549,
                    "y": 2414
                }
            ],
            "category": "paragraph",
            "html": "<p id='74' style='font-size:20px'>Agreement with the teacher & inductive bias? As discussed above, the ar-<br>chitecture of the teacher has an important impact. Does it inherit existing in-<br>ductive bias that would facilitate the training? While we believe it difficult to<br>formally answer this question, we analyze in Table 4 the decision agreement<br>between the convnet teacher, our image transformer DeiT learned from labels<br>only, and our transformer DeiTm.</p>",
            "id": 74,
            "page": 10,
            "text": "Agreement with the teacher & inductive bias? As discussed above, the ar-\nchitecture of the teacher has an important impact. Does it inherit existing in-\nductive bias that would facilitate the training? While we believe it difficult to\nformally answer this question, we analyze in Table 4 the decision agreement\nbetween the convnet teacher, our image transformer DeiT learned from labels\nonly, and our transformer DeiTm."
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 2417
                },
                {
                    "x": 1997,
                    "y": 2417
                },
                {
                    "x": 1997,
                    "y": 2715
                },
                {
                    "x": 549,
                    "y": 2715
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='75' style='font-size:18px'>Our distilled model is more correlated to the convnet than with a trans-<br>former learned from scratch. As to be expected, the classifier associated with<br>the distillation embedding is closer to the convnet that the one associated with<br>the class embedding, and conversely the one associated with the class embed-<br>ding is more similar to DeiT learned without distillation. Unsurprisingly, the<br>joint class+distil classifier offers a middle ground.</p>",
            "id": 75,
            "page": 10,
            "text": "Our distilled model is more correlated to the convnet than with a trans-\nformer learned from scratch. As to be expected, the classifier associated with\nthe distillation embedding is closer to the convnet that the one associated with\nthe class embedding, and conversely the one associated with the class embed-\nding is more similar to DeiT learned without distillation. Unsurprisingly, the\njoint class+distil classifier offers a middle ground."
        },
        {
            "bounding_box": [
                {
                    "x": 1250,
                    "y": 2894
                },
                {
                    "x": 1297,
                    "y": 2894
                },
                {
                    "x": 1297,
                    "y": 2933
                },
                {
                    "x": 1250,
                    "y": 2933
                }
            ],
            "category": "footer",
            "html": "<footer id='76' style='font-size:16px'>10</footer>",
            "id": 76,
            "page": 10,
            "text": "10"
        },
        {
            "bounding_box": [
                {
                    "x": 550,
                    "y": 554
                },
                {
                    "x": 1997,
                    "y": 554
                },
                {
                    "x": 1997,
                    "y": 801
                },
                {
                    "x": 550,
                    "y": 801
                }
            ],
            "category": "paragraph",
            "html": "<p id='77' style='font-size:18px'>Table 4: Disagreement analysis between convnet, image transformers and dis-<br>tillated transformers: We report the fraction of sample classified differently for<br>all classifier pairs, i.e., the rate of different decisions. We include two models<br>without distillation (a RegNetY and DeiT-B), so that we can compare how our<br>distilled models and classification heads are correlated to these teachers.</p>",
            "id": 77,
            "page": 11,
            "text": "Table 4: Disagreement analysis between convnet, image transformers and dis-\ntillated transformers: We report the fraction of sample classified differently for\nall classifier pairs, i.e., the rate of different decisions. We include two models\nwithout distillation (a RegNetY and DeiT-B), so that we can compare how our\ndistilled models and classification heads are correlated to these teachers."
        },
        {
            "bounding_box": [
                {
                    "x": 562,
                    "y": 809
                },
                {
                    "x": 1989,
                    "y": 809
                },
                {
                    "x": 1989,
                    "y": 1214
                },
                {
                    "x": 562,
                    "y": 1214
                }
            ],
            "category": "table",
            "html": "<br><table id='78' style='font-size:14px'><tr><td rowspan=\"2\"></td><td rowspan=\"2\">groundtruth</td><td rowspan=\"2\" colspan=\"2\">no distillation convnet DeiT</td><td colspan=\"3\">DeiTm student (of the convnet)</td></tr><tr><td>class</td><td>distillation</td><td>DeiTm</td></tr><tr><td>groundtruth</td><td>0.000</td><td>0.171</td><td>0.182</td><td>0.170</td><td>0.169</td><td>0.166</td></tr><tr><td>convnet (RegNetY)</td><td>0.171</td><td>0.000</td><td>0.133</td><td>0.112</td><td>0.100</td><td>0.102</td></tr><tr><td>DeiT</td><td>0.182</td><td>0.133</td><td>0.000</td><td>0.109</td><td>0.110</td><td>0.107</td></tr><tr><td>DeiTom- class only</td><td>0.170</td><td>0.112</td><td>0.109</td><td>0.000</td><td>0.050</td><td>0.033</td></tr><tr><td>DeiTm- distil. only</td><td>0.169</td><td>0.100</td><td>0.110</td><td>0.050</td><td>0.000</td><td>0.019</td></tr><tr><td>DeiTm- class+distil.</td><td>0.166</td><td>0.102</td><td>0.107</td><td>0.033</td><td>0.019</td><td>0.000</td></tr></table>",
            "id": 78,
            "page": 11,
            "text": "groundtruth no distillation convnet DeiT DeiTm student (of the convnet)\n class distillation DeiTm\n groundtruth 0.000 0.171 0.182 0.170 0.169 0.166\n convnet (RegNetY) 0.171 0.000 0.133 0.112 0.100 0.102\n DeiT 0.182 0.133 0.000 0.109 0.110 0.107\n DeiTom- class only 0.170 0.112 0.109 0.000 0.050 0.033\n DeiTm- distil. only 0.169 0.100 0.110 0.050 0.000 0.019\n DeiTm- class+distil. 0.166 0.102 0.107 0.033 0.019"
        },
        {
            "bounding_box": [
                {
                    "x": 550,
                    "y": 1297
                },
                {
                    "x": 1996,
                    "y": 1297
                },
                {
                    "x": 1996,
                    "y": 1546
                },
                {
                    "x": 550,
                    "y": 1546
                }
            ],
            "category": "paragraph",
            "html": "<p id='79' style='font-size:20px'>Number of epochs. Increasing the number of epochs significantly improves<br>the performance of training with distillation, see Figure 3. With 300 epochs,<br>our distilled network DeiT-Bo is already better than DeiT-B. But while for the<br>latter the performance saturates with longer schedules, our distilled network<br>clearly benefits from a longer training time.</p>",
            "id": 79,
            "page": 11,
            "text": "Number of epochs. Increasing the number of epochs significantly improves\nthe performance of training with distillation, see Figure 3. With 300 epochs,\nour distilled network DeiT-Bo is already better than DeiT-B. But while for the\nlatter the performance saturates with longer schedules, our distilled network\nclearly benefits from a longer training time."
        },
        {
            "bounding_box": [
                {
                    "x": 553,
                    "y": 1613
                },
                {
                    "x": 1982,
                    "y": 1613
                },
                {
                    "x": 1982,
                    "y": 1669
                },
                {
                    "x": 553,
                    "y": 1669
                }
            ],
            "category": "paragraph",
            "html": "<p id='80' style='font-size:22px'>5.3 Efficiency VS accuracy: a comparative study with convnets</p>",
            "id": 80,
            "page": 11,
            "text": "5.3 Efficiency VS accuracy: a comparative study with convnets"
        },
        {
            "bounding_box": [
                {
                    "x": 550,
                    "y": 1697
                },
                {
                    "x": 1996,
                    "y": 1697
                },
                {
                    "x": 1996,
                    "y": 1844
                },
                {
                    "x": 550,
                    "y": 1844
                }
            ],
            "category": "paragraph",
            "html": "<p id='81' style='font-size:16px'>In the literature, the image classificaton methods are often compared as a com-<br>promise between accuracy and another criterion, such as FLOPs, number of<br>parameters, size of the network, etc.</p>",
            "id": 81,
            "page": 11,
            "text": "In the literature, the image classificaton methods are often compared as a com-\npromise between accuracy and another criterion, such as FLOPs, number of\nparameters, size of the network, etc."
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 1848
                },
                {
                    "x": 1997,
                    "y": 1848
                },
                {
                    "x": 1997,
                    "y": 2092
                },
                {
                    "x": 549,
                    "y": 2092
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='82' style='font-size:16px'>We focus in Figure 1 on the tradeoff between the throughput (images pro-<br>cessed per second) and the top-1 classification accuracy on ImageNet. We focus<br>on the popular state-of-the-art EfficientNet convnet, which has benefited from<br>years of research on convnets and was optimized by architecture search on the<br>ImageNet validation set.</p>",
            "id": 82,
            "page": 11,
            "text": "We focus in Figure 1 on the tradeoff between the throughput (images pro-\ncessed per second) and the top-1 classification accuracy on ImageNet. We focus\non the popular state-of-the-art EfficientNet convnet, which has benefited from\nyears of research on convnets and was optimized by architecture search on the\nImageNet validation set."
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 2096
                },
                {
                    "x": 1995,
                    "y": 2096
                },
                {
                    "x": 1995,
                    "y": 2492
                },
                {
                    "x": 549,
                    "y": 2492
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='83' style='font-size:18px'>Our method DeiT is slightly below EfficientNet, which shows that we have<br>almost closed the gap between vision transformers and convnets when training<br>with Imagenet only. These results are a major improvement (+6.3% top-1 in a<br>comparable setting) over previous ViT models trained on Imagenet1k only [15].<br>Furthermore, when DeiT benefits from the distillation from a relatively weaker<br>RegNetY to produce DeiTm, it outperforms EfficientNet. It also outperforms<br>by 1% (top-1 acc.) the Vit-B model pre-trained on JFT300M at resolution 384<br>(85.2% VS 84.15%), while being significantly faster to train.</p>",
            "id": 83,
            "page": 11,
            "text": "Our method DeiT is slightly below EfficientNet, which shows that we have\nalmost closed the gap between vision transformers and convnets when training\nwith Imagenet only. These results are a major improvement (+6.3% top-1 in a\ncomparable setting) over previous ViT models trained on Imagenet1k only [15].\nFurthermore, when DeiT benefits from the distillation from a relatively weaker\nRegNetY to produce DeiTm, it outperforms EfficientNet. It also outperforms\nby 1% (top-1 acc.) the Vit-B model pre-trained on JFT300M at resolution 384\n(85.2% VS 84.15%), while being significantly faster to train."
        },
        {
            "bounding_box": [
                {
                    "x": 548,
                    "y": 2495
                },
                {
                    "x": 1997,
                    "y": 2495
                },
                {
                    "x": 1997,
                    "y": 2744
                },
                {
                    "x": 548,
                    "y": 2744
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='84' style='font-size:16px'>Table 5 reports the numerical results in more details and additional evalu-<br>ations on ImageNet V2 and ImageNet Real, that have a test set distinct from<br>the ImageNet validation, which reduces overfitting on the validation set. Our<br>results show that DeiT-Bo and DeiT-Bo ↑384 outperform, by some margin, the<br>state of the art on the trade-off between accuracy and inference time on GPU.</p>",
            "id": 84,
            "page": 11,
            "text": "Table 5 reports the numerical results in more details and additional evalu-\nations on ImageNet V2 and ImageNet Real, that have a test set distinct from\nthe ImageNet validation, which reduces overfitting on the validation set. Our\nresults show that DeiT-Bo and DeiT-Bo ↑384 outperform, by some margin, the\nstate of the art on the trade-off between accuracy and inference time on GPU."
        },
        {
            "bounding_box": [
                {
                    "x": 1249,
                    "y": 2894
                },
                {
                    "x": 1295,
                    "y": 2894
                },
                {
                    "x": 1295,
                    "y": 2934
                },
                {
                    "x": 1249,
                    "y": 2934
                }
            ],
            "category": "footer",
            "html": "<footer id='85' style='font-size:14px'>11</footer>",
            "id": 85,
            "page": 11,
            "text": "11"
        },
        {
            "bounding_box": [
                {
                    "x": 659,
                    "y": 508
                },
                {
                    "x": 1889,
                    "y": 508
                },
                {
                    "x": 1889,
                    "y": 2191
                },
                {
                    "x": 659,
                    "y": 2191
                }
            ],
            "category": "table",
            "html": "<table id='86' style='font-size:14px'><tr><td>Network</td><td>#param.</td><td colspan=\"2\">image throughput size (image/s)</td><td>ImNet top-1</td><td>Real top-1</td><td>V2 top-1</td></tr><tr><td colspan=\"7\">Convnets</td></tr><tr><td>ResNet-18 [21]</td><td>12M</td><td>2242</td><td>4458.4</td><td>69.8</td><td>77.3</td><td>57.1</td></tr><tr><td>ResNet-50 [21]</td><td>25M</td><td>2242</td><td>1226.1</td><td>76.2</td><td>82.5</td><td>63.3</td></tr><tr><td>ResNet-101 [21]</td><td>45M</td><td>2242</td><td>753.6</td><td>77.4</td><td>83.7</td><td>65.7</td></tr><tr><td>ResNet-152 [21]</td><td>60M</td><td>2242</td><td>526.4</td><td>78.3</td><td>84.1</td><td>67.0</td></tr><tr><td>RegNetY-4GF [40]*</td><td>21M</td><td>2242</td><td>1156.7</td><td>80.0</td><td>86.4</td><td>69.4</td></tr><tr><td>RegNetY-8GF [40]*</td><td>39M</td><td>2242</td><td>591.6</td><td>81.7</td><td>87.4</td><td>70.8</td></tr><tr><td>RegNetY-16GF [40]*</td><td>84M</td><td>2242</td><td>334.7</td><td>82.9</td><td>88.1</td><td>72.4</td></tr><tr><td>EfficientNet-B0 [48]</td><td>5M</td><td>2242</td><td>2694.3</td><td>77.1</td><td>83.5</td><td>64.3</td></tr><tr><td>EfficientNet-B1 [48]</td><td>8M</td><td>2402</td><td>1662.5</td><td>79.1</td><td>84.9</td><td>66.9</td></tr><tr><td>EfficientNet-B2 [48]</td><td>9M</td><td>2602</td><td>1255.7</td><td>80.1</td><td>85.9</td><td>68.8</td></tr><tr><td>EfficientNet-B3 [48]</td><td>12M</td><td>3002</td><td>732.1</td><td>81.6</td><td>86.8</td><td>70.6</td></tr><tr><td>EfficientNet-B4 [48]</td><td>19M</td><td>3802</td><td>349.4</td><td>82.9</td><td>88.0</td><td>72.3</td></tr><tr><td>EfficientNet-B5 [48]</td><td>30M</td><td>4562</td><td>169.1</td><td>83.6</td><td>88.3</td><td>73.6</td></tr><tr><td>EfficientNet-B6 [48]</td><td>43M</td><td>5282</td><td>96.9</td><td>84.0</td><td>88.8</td><td>73.9</td></tr><tr><td>EfficientNet-B7 [48]</td><td>66M</td><td>6002</td><td>55.1</td><td>84.3</td><td>-</td><td>-</td></tr><tr><td>EfficientNet-B5 RA [12]</td><td>30M</td><td>4562</td><td>96.9</td><td>83.7</td><td>-</td><td>-</td></tr><tr><td>EfficientNet-B7 RA [12]</td><td>66M</td><td>6002</td><td>55.1</td><td>84.7</td><td>-</td><td></td></tr><tr><td>KDforAA-B8</td><td>87M</td><td>8002</td><td>25.2</td><td>85.8</td><td>-</td><td>-</td></tr><tr><td colspan=\"7\">Transformers</td></tr><tr><td>ViT-B/16 [15]</td><td>86M</td><td>3842</td><td>85.9</td><td>77.9</td><td>83.6</td><td></td></tr><tr><td>ViT-L/16 [15]</td><td>307M</td><td>3842</td><td>27.3</td><td>76.5</td><td>82.2</td><td></td></tr><tr><td>DeiT-Ti</td><td>5M</td><td>2242</td><td>2536.5</td><td>72.2</td><td>80.1</td><td>60.4</td></tr><tr><td>DeiT-S</td><td>22M</td><td>2242</td><td>940.4</td><td>79.8</td><td>85.7</td><td>68.5</td></tr><tr><td>DeiT-B</td><td>86M</td><td>2242</td><td>292.3</td><td>81.8</td><td>86.7</td><td>71.5</td></tr><tr><td>DeiT-B↑384</td><td>86M</td><td>3842</td><td>85.9</td><td>83.1</td><td>87.7</td><td>72.4</td></tr><tr><td>DeiT-Tim</td><td>6M</td><td>2242</td><td>2529.5</td><td>74.5</td><td>82.1</td><td>62.9</td></tr><tr><td>DeiT-Som</td><td>22M</td><td>2242</td><td>936.2</td><td>81.2</td><td>86.8</td><td>70.0</td></tr><tr><td>DeiT-Bo</td><td>87M</td><td>2242</td><td>290.9</td><td>83.4</td><td>88.3</td><td>73.2</td></tr><tr><td>DeiT-Tim / 1000 epochs</td><td>6M</td><td>2242</td><td>2529.5</td><td>76.6</td><td>83.9</td><td>65.4</td></tr><tr><td>DeiT-Som / 1000 epochs</td><td>22M</td><td>2242</td><td>936.2</td><td>82.6</td><td>87.8</td><td>71.7</td></tr><tr><td>DeiT-Bo / 1000 epochs</td><td>87M</td><td>2242</td><td>290.9</td><td>84.2</td><td>88.7</td><td>73.9</td></tr><tr><td>DeiT-Bo ↑384</td><td>87M</td><td>3842</td><td>85.8</td><td>84.5</td><td>89.0</td><td>74.8</td></tr><tr><td>DeiT-Bo ↑384 / 1000 epochs</td><td>87M</td><td>3842</td><td>85.8</td><td>85.2</td><td>89.3</td><td>75.2</td></tr></table>",
            "id": 86,
            "page": 12,
            "text": "Network #param. image throughput size (image/s) ImNet top-1 Real top-1 V2 top-1\n Convnets\n ResNet-18 [21] 12M 2242 4458.4 69.8 77.3 57.1\n ResNet-50 [21] 25M 2242 1226.1 76.2 82.5 63.3\n ResNet-101 [21] 45M 2242 753.6 77.4 83.7 65.7\n ResNet-152 [21] 60M 2242 526.4 78.3 84.1 67.0\n RegNetY-4GF [40]* 21M 2242 1156.7 80.0 86.4 69.4\n RegNetY-8GF [40]* 39M 2242 591.6 81.7 87.4 70.8\n RegNetY-16GF [40]* 84M 2242 334.7 82.9 88.1 72.4\n EfficientNet-B0 [48] 5M 2242 2694.3 77.1 83.5 64.3\n EfficientNet-B1 [48] 8M 2402 1662.5 79.1 84.9 66.9\n EfficientNet-B2 [48] 9M 2602 1255.7 80.1 85.9 68.8\n EfficientNet-B3 [48] 12M 3002 732.1 81.6 86.8 70.6\n EfficientNet-B4 [48] 19M 3802 349.4 82.9 88.0 72.3\n EfficientNet-B5 [48] 30M 4562 169.1 83.6 88.3 73.6\n EfficientNet-B6 [48] 43M 5282 96.9 84.0 88.8 73.9\n EfficientNet-B7 [48] 66M 6002 55.1 84.3 - -\n EfficientNet-B5 RA [12] 30M 4562 96.9 83.7 - -\n EfficientNet-B7 RA [12] 66M 6002 55.1 84.7 - \n KDforAA-B8 87M 8002 25.2 85.8 - -\n Transformers\n ViT-B/16 [15] 86M 3842 85.9 77.9 83.6 \n ViT-L/16 [15] 307M 3842 27.3 76.5 82.2 \n DeiT-Ti 5M 2242 2536.5 72.2 80.1 60.4\n DeiT-S 22M 2242 940.4 79.8 85.7 68.5\n DeiT-B 86M 2242 292.3 81.8 86.7 71.5\n DeiT-B↑384 86M 3842 85.9 83.1 87.7 72.4\n DeiT-Tim 6M 2242 2529.5 74.5 82.1 62.9\n DeiT-Som 22M 2242 936.2 81.2 86.8 70.0\n DeiT-Bo 87M 2242 290.9 83.4 88.3 73.2\n DeiT-Tim / 1000 epochs 6M 2242 2529.5 76.6 83.9 65.4\n DeiT-Som / 1000 epochs 22M 2242 936.2 82.6 87.8 71.7\n DeiT-Bo / 1000 epochs 87M 2242 290.9 84.2 88.7 73.9\n DeiT-Bo ↑384 87M 3842 85.8 84.5 89.0 74.8\n DeiT-Bo ↑384 / 1000 epochs 87M 3842 85.8 85.2 89.3"
        },
        {
            "bounding_box": [
                {
                    "x": 548,
                    "y": 2215
                },
                {
                    "x": 1997,
                    "y": 2215
                },
                {
                    "x": 1997,
                    "y": 2717
                },
                {
                    "x": 548,
                    "y": 2717
                }
            ],
            "category": "paragraph",
            "html": "<p id='87' style='font-size:20px'>Table 5: Throughput on and accuracy on Imagenet [42], Imagenet Real [5] and<br>Imagenet V2 matched frequency [41] of DeiT and of several state-of-the-art<br>convnets, for models trained with no external data. The throughput is mea-<br>sured as the number of images that we can process per second on one 16GB<br>V100 GPU. For each model we take the largest possible batch size for the usual<br>resolution of the model and calculate the average time over 30 runs to process<br>that batch. With that we calculate the number of images processed per second.<br>Throughput can vary according to the implementation: for a direct comparison<br>and in order to be as fair as possible, we use for each model the definition in<br>the same GitHub [55] repository.</p>",
            "id": 87,
            "page": 12,
            "text": "Table 5: Throughput on and accuracy on Imagenet [42], Imagenet Real [5] and\nImagenet V2 matched frequency [41] of DeiT and of several state-of-the-art\nconvnets, for models trained with no external data. The throughput is mea-\nsured as the number of images that we can process per second on one 16GB\nV100 GPU. For each model we take the largest possible batch size for the usual\nresolution of the model and calculate the average time over 30 runs to process\nthat batch. With that we calculate the number of images processed per second.\nThroughput can vary according to the implementation: for a direct comparison\nand in order to be as fair as possible, we use for each model the definition in\nthe same GitHub [55] repository."
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 2718
                },
                {
                    "x": 1995,
                    "y": 2718
                },
                {
                    "x": 1995,
                    "y": 2814
                },
                {
                    "x": 549,
                    "y": 2814
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='88' style='font-size:16px'>★ : Regnet optimized with a similar optimization procedure as ours, which boosts the<br>results. These networks serve as teachers when we use our distillation strategy.</p>",
            "id": 88,
            "page": 12,
            "text": "★ : Regnet optimized with a similar optimization procedure as ours, which boosts the\nresults. These networks serve as teachers when we use our distillation strategy."
        },
        {
            "bounding_box": [
                {
                    "x": 1249,
                    "y": 2895
                },
                {
                    "x": 1298,
                    "y": 2895
                },
                {
                    "x": 1298,
                    "y": 2932
                },
                {
                    "x": 1249,
                    "y": 2932
                }
            ],
            "category": "footer",
            "html": "<footer id='89' style='font-size:16px'>12</footer>",
            "id": 89,
            "page": 12,
            "text": "12"
        },
        {
            "bounding_box": [
                {
                    "x": 733,
                    "y": 504
                },
                {
                    "x": 1812,
                    "y": 504
                },
                {
                    "x": 1812,
                    "y": 1567
                },
                {
                    "x": 733,
                    "y": 1567
                }
            ],
            "category": "figure",
            "html": "<figure><img id='90' style='font-size:22px' alt=\"86\n85\n(%)\naccuracy No distillation\nUsual distillation\n83 Hard distillation\ntop-1 Distillation token�\nDistillation token 1m ↑ 384\n82\n81\n400 600 800 1000\nepochs\" data-coord=\"top-left:(733,504); bottom-right:(1812,1567)\" /></figure>",
            "id": 90,
            "page": 13,
            "text": "86\n85\n(%)\naccuracy No distillation\nUsual distillation\n83 Hard distillation\ntop-1 Distillation token�\nDistillation token 1m ↑ 384\n82\n81\n400 600 800 1000\nepochs"
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 1622
                },
                {
                    "x": 1997,
                    "y": 1622
                },
                {
                    "x": 1997,
                    "y": 1772
                },
                {
                    "x": 549,
                    "y": 1772
                }
            ],
            "category": "caption",
            "html": "<caption id='91' style='font-size:16px'>Figure 3: Distillation on ImageNet [42] with DeiT-B: performance as a func-<br>tion of the number of training epochs. We provide the performance without<br>distillation (horizontal dotted line) as it saturates after 400 epochs.</caption>",
            "id": 91,
            "page": 13,
            "text": "Figure 3: Distillation on ImageNet [42] with DeiT-B: performance as a func-\ntion of the number of training epochs. We provide the performance without\ndistillation (horizontal dotted line) as it saturates after 400 epochs."
        },
        {
            "bounding_box": [
                {
                    "x": 553,
                    "y": 1850
                },
                {
                    "x": 1873,
                    "y": 1850
                },
                {
                    "x": 1873,
                    "y": 1903
                },
                {
                    "x": 553,
                    "y": 1903
                }
            ],
            "category": "paragraph",
            "html": "<p id='92' style='font-size:20px'>5.4 Transfer learning: Performance on downstream tasks</p>",
            "id": 92,
            "page": 13,
            "text": "5.4 Transfer learning: Performance on downstream tasks"
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 1933
                },
                {
                    "x": 1997,
                    "y": 1933
                },
                {
                    "x": 1997,
                    "y": 2281
                },
                {
                    "x": 549,
                    "y": 2281
                }
            ],
            "category": "paragraph",
            "html": "<p id='93' style='font-size:16px'>Although DeiT perform very well on ImageNetit is important to evaluate them<br>on other datasets with transfer learning in order to measure the power of gen-<br>eralization of DeiT. We evaluated this on transfer learning tasks by fine-tuning<br>on the datasets in Table 6. Table 7 compares DeiT transfer learning results to<br>those of ViT [15] and state of the art convolutional architectures [48]. DeiT is<br>on par with competitive convnet models, which is in line with our previous<br>conclusion on ImageNet.</p>",
            "id": 93,
            "page": 13,
            "text": "Although DeiT perform very well on ImageNetit is important to evaluate them\non other datasets with transfer learning in order to measure the power of gen-\neralization of DeiT. We evaluated this on transfer learning tasks by fine-tuning\non the datasets in Table 6. Table 7 compares DeiT transfer learning results to\nthose of ViT [15] and state of the art convolutional architectures [48]. DeiT is\non par with competitive convnet models, which is in line with our previous\nconclusion on ImageNet."
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 2345
                },
                {
                    "x": 1996,
                    "y": 2345
                },
                {
                    "x": 1996,
                    "y": 2539
                },
                {
                    "x": 549,
                    "y": 2539
                }
            ],
            "category": "paragraph",
            "html": "<p id='94' style='font-size:16px'>Comparison VS training from scratch. We investigate the performance when<br>training from scratch on a small dataset, without Imagenet pre-training. We<br>get the following results on the small CIFAR-10, which is small both w.r.t. the<br>number of images and labels:</p>",
            "id": 94,
            "page": 13,
            "text": "Comparison VS training from scratch. We investigate the performance when\ntraining from scratch on a small dataset, without Imagenet pre-training. We\nget the following results on the small CIFAR-10, which is small both w.r.t. the\nnumber of images and labels:"
        },
        {
            "bounding_box": [
                {
                    "x": 861,
                    "y": 2578
                },
                {
                    "x": 1687,
                    "y": 2578
                },
                {
                    "x": 1687,
                    "y": 2687
                },
                {
                    "x": 861,
                    "y": 2687
                }
            ],
            "category": "table",
            "html": "<table id='95' style='font-size:14px'><tr><td>Method</td><td>RegNetY-16GF</td><td>DeiT-B</td><td>DeiT-Bo</td></tr><tr><td>Top-1</td><td>98.0</td><td>97.5</td><td>98.5</td></tr></table>",
            "id": 95,
            "page": 13,
            "text": "Method RegNetY-16GF DeiT-B DeiT-Bo\n Top-1 98.0 97.5"
        },
        {
            "bounding_box": [
                {
                    "x": 550,
                    "y": 2717
                },
                {
                    "x": 1997,
                    "y": 2717
                },
                {
                    "x": 1997,
                    "y": 2819
                },
                {
                    "x": 550,
                    "y": 2819
                }
            ],
            "category": "paragraph",
            "html": "<p id='96' style='font-size:18px'>For this experiment, we tried we get as close as possible to the Imagenet<br>pre-training counterpart, meaning that (1) we consider longer training sched-</p>",
            "id": 96,
            "page": 13,
            "text": "For this experiment, we tried we get as close as possible to the Imagenet\npre-training counterpart, meaning that (1) we consider longer training sched-"
        },
        {
            "bounding_box": [
                {
                    "x": 1250,
                    "y": 2895
                },
                {
                    "x": 1297,
                    "y": 2895
                },
                {
                    "x": 1297,
                    "y": 2933
                },
                {
                    "x": 1250,
                    "y": 2933
                }
            ],
            "category": "footer",
            "html": "<footer id='97' style='font-size:14px'>13</footer>",
            "id": 97,
            "page": 13,
            "text": "13"
        },
        {
            "bounding_box": [
                {
                    "x": 855,
                    "y": 558
                },
                {
                    "x": 1679,
                    "y": 558
                },
                {
                    "x": 1679,
                    "y": 600
                },
                {
                    "x": 855,
                    "y": 600
                }
            ],
            "category": "caption",
            "html": "<caption id='98' style='font-size:18px'>Table 6: Datasets used for our different tasks.</caption>",
            "id": 98,
            "page": 14,
            "text": "Table 6: Datasets used for our different tasks."
        },
        {
            "bounding_box": [
                {
                    "x": 784,
                    "y": 612
                },
                {
                    "x": 1752,
                    "y": 612
                },
                {
                    "x": 1752,
                    "y": 1024
                },
                {
                    "x": 784,
                    "y": 1024
                }
            ],
            "category": "table",
            "html": "<br><table id='99' style='font-size:18px'><tr><td>Dataset</td><td>Train size</td><td>Test size</td><td>#classes</td></tr><tr><td>ImageNet [42]</td><td>1,281,167</td><td>50,000</td><td>1000</td></tr><tr><td>iNaturalist 2018 [26]</td><td>437,513</td><td>24,426</td><td>8,142</td></tr><tr><td>iNaturalist 2019 [27]</td><td>265,240</td><td>3,003</td><td>1,010</td></tr><tr><td>Flowers-102 [38]</td><td>2,040</td><td>6,149</td><td>102</td></tr><tr><td>Stanford Cars [30]</td><td>8,144</td><td>8,041</td><td>196</td></tr><tr><td>CIFAR-100 [31]</td><td>50,000</td><td>10,000</td><td>100</td></tr><tr><td>CIFAR-10 [31]</td><td>50,000</td><td>10,000</td><td>10</td></tr></table>",
            "id": 99,
            "page": 14,
            "text": "Dataset Train size Test size #classes\n ImageNet [42] 1,281,167 50,000 1000\n iNaturalist 2018 [26] 437,513 24,426 8,142\n iNaturalist 2019 [27] 265,240 3,003 1,010\n Flowers-102 [38] 2,040 6,149 102\n Stanford Cars [30] 8,144 8,041 196\n CIFAR-100 [31] 50,000 10,000 100\n CIFAR-10 [31] 50,000 10,000"
        },
        {
            "bounding_box": [
                {
                    "x": 548,
                    "y": 1069
                },
                {
                    "x": 1997,
                    "y": 1069
                },
                {
                    "x": 1997,
                    "y": 1219
                },
                {
                    "x": 548,
                    "y": 1219
                }
            ],
            "category": "caption",
            "html": "<caption id='100' style='font-size:18px'>Table 7: We compare Transformers based models on different transfer learning<br>task with ImageNet pre-training. We also report results with convolutional<br>architectures for reference.</caption>",
            "id": 100,
            "page": 14,
            "text": "Table 7: We compare Transformers based models on different transfer learning\ntask with ImageNet pre-training. We also report results with convolutional\narchitectures for reference."
        },
        {
            "bounding_box": [
                {
                    "x": 561,
                    "y": 1222
                },
                {
                    "x": 1991,
                    "y": 1222
                },
                {
                    "x": 1991,
                    "y": 1733
                },
                {
                    "x": 561,
                    "y": 1733
                }
            ],
            "category": "table",
            "html": "<br><table id='101' style='font-size:14px'><tr><td>Model</td><td>ImageNet</td><td>CIFAR-10</td><td>CIFAR-100</td><td>Flowers</td><td>Cars</td><td>iNat-18</td><td>iNat-19</td><td>im/ sec</td></tr><tr><td>Grafit ResNet-50 [49]</td><td>79.6</td><td>-</td><td>-</td><td>98.2</td><td>92.5</td><td>69.8</td><td>75.9</td><td>1226.1</td></tr><tr><td>Grafit RegNetY-8GF [49]</td><td>-</td><td>-</td><td>-</td><td>99.0</td><td>94.0</td><td>76.8</td><td>80.0</td><td>591.6</td></tr><tr><td>ResNet-152 [10]</td><td>-</td><td>-</td><td>-</td><td></td><td>-</td><td>69.1</td><td>-</td><td>526.3</td></tr><tr><td>EfficientNet-B7 [48]</td><td>84.3</td><td>98.9</td><td>91.7</td><td>98.8</td><td>94.7</td><td>-</td><td>-</td><td>55.1</td></tr><tr><td>ViT-B/32 [15]</td><td>73.4</td><td>97.8</td><td>86.3</td><td>85.4</td><td></td><td></td><td></td><td>394.5</td></tr><tr><td>ViT-B/16 [15]</td><td>77.9</td><td>98.1</td><td>87.1</td><td>89.5</td><td></td><td></td><td></td><td>85.9</td></tr><tr><td>ViT-L/32 [15]</td><td>71.2</td><td>97.9</td><td>87.1</td><td>86.4</td><td></td><td></td><td></td><td>124.1</td></tr><tr><td>ViT-L/16 [15]</td><td>76.5</td><td>97.9</td><td>86.4</td><td>89.7</td><td></td><td></td><td></td><td>27.3</td></tr><tr><td>DeiT-B</td><td>81.8</td><td>99.1</td><td>90.8</td><td>98.4</td><td>92.1</td><td>73.2</td><td>77.7</td><td>292.3</td></tr><tr><td>DeiT-B↑384</td><td>83.1</td><td>99.1</td><td>90.8</td><td>98.5</td><td>93.3</td><td>79.5</td><td>81.4</td><td>85.9</td></tr><tr><td>DeiT-Bo</td><td>83.4</td><td>99.1</td><td>91.3</td><td>98.8</td><td>92.9</td><td>73.7</td><td>78.4</td><td>290.9</td></tr><tr><td>DeiT-Bo ↑384</td><td>84.4</td><td>99.2</td><td>91.4</td><td>98.9</td><td>93.9</td><td>80.1</td><td>83.0</td><td>85.9</td></tr></table>",
            "id": 101,
            "page": 14,
            "text": "Model ImageNet CIFAR-10 CIFAR-100 Flowers Cars iNat-18 iNat-19 im/ sec\n Grafit ResNet-50 [49] 79.6 - - 98.2 92.5 69.8 75.9 1226.1\n Grafit RegNetY-8GF [49] - - - 99.0 94.0 76.8 80.0 591.6\n ResNet-152 [10] - - -  - 69.1 - 526.3\n EfficientNet-B7 [48] 84.3 98.9 91.7 98.8 94.7 - - 55.1\n ViT-B/32 [15] 73.4 97.8 86.3 85.4    394.5\n ViT-B/16 [15] 77.9 98.1 87.1 89.5    85.9\n ViT-L/32 [15] 71.2 97.9 87.1 86.4    124.1\n ViT-L/16 [15] 76.5 97.9 86.4 89.7    27.3\n DeiT-B 81.8 99.1 90.8 98.4 92.1 73.2 77.7 292.3\n DeiT-B↑384 83.1 99.1 90.8 98.5 93.3 79.5 81.4 85.9\n DeiT-Bo 83.4 99.1 91.3 98.8 92.9 73.7 78.4 290.9\n DeiT-Bo ↑384 84.4 99.2 91.4 98.9 93.9 80.1 83.0"
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 1815
                },
                {
                    "x": 1995,
                    "y": 1815
                },
                {
                    "x": 1995,
                    "y": 2115
                },
                {
                    "x": 549,
                    "y": 2115
                }
            ],
            "category": "paragraph",
            "html": "<p id='102' style='font-size:18px'>ules (up to 7200 epochs, which corresponds to 300 Imagenet epochs) so that<br>the network has been fed a comparable number of images in total; (2) we re-<br>scale images to 224 x 224 to ensure that we have the same augmentation. The<br>results are not as good as with Imagenet pre-training (98.5% VS 99.1%), which<br>is expected since the network has seen a much lower diversity. However they<br>show that it is possible to learn a reasonable transformer on CIFAR-10 only.</p>",
            "id": 102,
            "page": 14,
            "text": "ules (up to 7200 epochs, which corresponds to 300 Imagenet epochs) so that\nthe network has been fed a comparable number of images in total; (2) we re-\nscale images to 224 x 224 to ensure that we have the same augmentation. The\nresults are not as good as with Imagenet pre-training (98.5% VS 99.1%), which\nis expected since the network has seen a much lower diversity. However they\nshow that it is possible to learn a reasonable transformer on CIFAR-10 only."
        },
        {
            "bounding_box": [
                {
                    "x": 552,
                    "y": 2196
                },
                {
                    "x": 1384,
                    "y": 2196
                },
                {
                    "x": 1384,
                    "y": 2257
                },
                {
                    "x": 552,
                    "y": 2257
                }
            ],
            "category": "paragraph",
            "html": "<p id='103' style='font-size:22px'>6 Training details & ablation</p>",
            "id": 103,
            "page": 14,
            "text": "6 Training details & ablation"
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 2302
                },
                {
                    "x": 1996,
                    "y": 2302
                },
                {
                    "x": 1996,
                    "y": 2500
                },
                {
                    "x": 549,
                    "y": 2500
                }
            ],
            "category": "paragraph",
            "html": "<p id='104' style='font-size:18px'>In this section we discuss the DeiT training strategy to learn vision transform-<br>ers in a data-efficient manner. We build upon PyTorch [39] and the timm li-<br>brary [55]2. We provide hyper-parameters as well as an ablation study in which<br>we analyze the impact of each choice.</p>",
            "id": 104,
            "page": 14,
            "text": "In this section we discuss the DeiT training strategy to learn vision transform-\ners in a data-efficient manner. We build upon PyTorch [39] and the timm li-\nbrary [55]2. We provide hyper-parameters as well as an ablation study in which\nwe analyze the impact of each choice."
        },
        {
            "bounding_box": [
                {
                    "x": 552,
                    "y": 2564
                },
                {
                    "x": 1995,
                    "y": 2564
                },
                {
                    "x": 1995,
                    "y": 2664
                },
                {
                    "x": 552,
                    "y": 2664
                }
            ],
            "category": "paragraph",
            "html": "<p id='105' style='font-size:20px'>Initialization and hyper-parameters. Transformers are relatively sensitive to<br>initialization. After testing several options in preliminary experiments, some</p>",
            "id": 105,
            "page": 14,
            "text": "Initialization and hyper-parameters. Transformers are relatively sensitive to\ninitialization. After testing several options in preliminary experiments, some"
        },
        {
            "bounding_box": [
                {
                    "x": 554,
                    "y": 2697
                },
                {
                    "x": 1991,
                    "y": 2697
                },
                {
                    "x": 1991,
                    "y": 2777
                },
                {
                    "x": 554,
                    "y": 2777
                }
            ],
            "category": "paragraph",
            "html": "<p id='106' style='font-size:14px'>2The timm implementation already included a training procedure that improved the accuracy<br>of ViT-B from 77.91% to 79.35% top-1, and trained on Imagenet-1k with a 8xV100 GPU machine.</p>",
            "id": 106,
            "page": 14,
            "text": "2The timm implementation already included a training procedure that improved the accuracy\nof ViT-B from 77.91% to 79.35% top-1, and trained on Imagenet-1k with a 8xV100 GPU machine."
        },
        {
            "bounding_box": [
                {
                    "x": 1249,
                    "y": 2895
                },
                {
                    "x": 1297,
                    "y": 2895
                },
                {
                    "x": 1297,
                    "y": 2933
                },
                {
                    "x": 1249,
                    "y": 2933
                }
            ],
            "category": "footer",
            "html": "<footer id='107' style='font-size:16px'>14</footer>",
            "id": 107,
            "page": 14,
            "text": "14"
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 510
                },
                {
                    "x": 2060,
                    "y": 510
                },
                {
                    "x": 2060,
                    "y": 1433
                },
                {
                    "x": 545,
                    "y": 1433
                }
            ],
            "category": "figure",
            "html": "<figure><img id='108' style='font-size:14px' alt=\"top-1 accuracy\nAvg.\n2242\nRand-Augment\n3842\nAug.\nMoving\nDepth\nFine-tuning\npre-trained\nfine-tuned\nAutoAug\nRepeated\nDropout\nCutMix\nErasing\nMixup\nStoch.\nExp.\nAblation on ↓ Pre-training\nnone: DeiT-B adamw adamw V X V V V V V X X 81.8 ±0.2 83.1 ±0.1\nSGD adamw X 74.5 77.3\noptimizer\nadamw SGD X 81.8 83.1\nadamw adamw X X X X 79.6 80.4\nadamw adamw X V X 81.2 81.9\ndata\nadamw adamw X X 78.7 79.8\naugmentation\nadamw adamw X x X 80.0 80.6\nadamw adamw X X X X 75.8 76.7\nadamw adamw X X X X 4.3* 0.1\nadamw adamw X X X 3.4* 0.1\nregularization adamw adamw X X X 76.5 77.4\nadamw adamw X V X 81.3 83.1\nadamw adamw X X V 81.9 83.1\" data-coord=\"top-left:(545,510); bottom-right:(2060,1433)\" /></figure>",
            "id": 108,
            "page": 15,
            "text": "top-1 accuracy\nAvg.\n2242\nRand-Augment\n3842\nAug.\nMoving\nDepth\nFine-tuning\npre-trained\nfine-tuned\nAutoAug\nRepeated\nDropout\nCutMix\nErasing\nMixup\nStoch.\nExp.\nAblation on ↓ Pre-training\nnone: DeiT-B adamw adamw V X V V V V V X X 81.8 ±0.2 83.1 ±0.1\nSGD adamw X 74.5 77.3\noptimizer\nadamw SGD X 81.8 83.1\nadamw adamw X X X X 79.6 80.4\nadamw adamw X V X 81.2 81.9\ndata\nadamw adamw X X 78.7 79.8\naugmentation\nadamw adamw X x X 80.0 80.6\nadamw adamw X X X X 75.8 76.7\nadamw adamw X X X X 4.3* 0.1\nadamw adamw X X X 3.4* 0.1\nregularization adamw adamw X X X 76.5 77.4\nadamw adamw X V X 81.3 83.1\nadamw adamw X X V 81.9 83.1"
        },
        {
            "bounding_box": [
                {
                    "x": 550,
                    "y": 1459
                },
                {
                    "x": 1998,
                    "y": 1459
                },
                {
                    "x": 1998,
                    "y": 1811
                },
                {
                    "x": 550,
                    "y": 1811
                }
            ],
            "category": "caption",
            "html": "<caption id='109' style='font-size:18px'>Table 8: Ablation study on training methods on ImageNet [42]. The top row<br>(\"none\") corresponds to our default configuration employed for DeiT. The<br>symbols V and X indicates that we use and do not use the corresponding<br>method, respectively. We report the accuracy scores (%) after the initial train-<br>ing at resolution 224x224, and after fine-tuning at resolution 384x384. The<br>hyper-parameters are fixed according to Table 9, and may be suboptimal.<br>* indicates that the model did not train well, possibly because hyper-parameters are not adapted.</caption>",
            "id": 109,
            "page": 15,
            "text": "Table 8: Ablation study on training methods on ImageNet [42]. The top row\n(\"none\") corresponds to our default configuration employed for DeiT. The\nsymbols V and X indicates that we use and do not use the corresponding\nmethod, respectively. We report the accuracy scores (%) after the initial train-\ning at resolution 224x224, and after fine-tuning at resolution 384x384. The\nhyper-parameters are fixed according to Table 9, and may be suboptimal.\n* indicates that the model did not train well, possibly because hyper-parameters are not adapted."
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 1898
                },
                {
                    "x": 1993,
                    "y": 1898
                },
                {
                    "x": 1993,
                    "y": 1993
                },
                {
                    "x": 549,
                    "y": 1993
                }
            ],
            "category": "paragraph",
            "html": "<p id='110' style='font-size:18px'>of them not converging, we follow the recommendation of Hanin and Rol-<br>nick [20] to initialize the weights with a truncated normal distribution.</p>",
            "id": 110,
            "page": 15,
            "text": "of them not converging, we follow the recommendation of Hanin and Rol-\nnick [20] to initialize the weights with a truncated normal distribution."
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 1999
                },
                {
                    "x": 1998,
                    "y": 1999
                },
                {
                    "x": 1998,
                    "y": 2193
                },
                {
                    "x": 549,
                    "y": 2193
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='111' style='font-size:18px'>Table 9 indicates the hyper-parameters that we use by default at training<br>time for all our experiments, unless stated otherwise. For distillation we follow<br>the recommendations from Cho et al. [9] to select the parameters T and 入. We<br>take the typical values T = 3.0 and 入 = 0.1 for the usual (soft) distillation.</p>",
            "id": 111,
            "page": 15,
            "text": "Table 9 indicates the hyper-parameters that we use by default at training\ntime for all our experiments, unless stated otherwise. For distillation we follow\nthe recommendations from Cho et al. [9] to select the parameters T and 入. We\ntake the typical values T = 3.0 and 入 = 0.1 for the usual (soft) distillation."
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 2257
                },
                {
                    "x": 1998,
                    "y": 2257
                },
                {
                    "x": 1998,
                    "y": 2504
                },
                {
                    "x": 549,
                    "y": 2504
                }
            ],
            "category": "paragraph",
            "html": "<p id='112' style='font-size:20px'>Data-Augmentation. Compared to models that integrate more priors (such<br>as convolutions), transformers require a larger amount of data. Thus, in order<br>to train with datasets of the same size, we rely on extensive data augmentation.<br>We evaluate different types of strong data augmentation, with the objective to<br>reach a data-efficient training regime.</p>",
            "id": 112,
            "page": 15,
            "text": "Data-Augmentation. Compared to models that integrate more priors (such\nas convolutions), transformers require a larger amount of data. Thus, in order\nto train with datasets of the same size, we rely on extensive data augmentation.\nWe evaluate different types of strong data augmentation, with the objective to\nreach a data-efficient training regime."
        },
        {
            "bounding_box": [
                {
                    "x": 548,
                    "y": 2508
                },
                {
                    "x": 1998,
                    "y": 2508
                },
                {
                    "x": 1998,
                    "y": 2806
                },
                {
                    "x": 548,
                    "y": 2806
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='113' style='font-size:20px'>Auto-Augment [11], Rand-Augment [12], and random erasing [62] im-<br>prove the results. For the two latter we use the timm [55] customizations, and<br>after ablation we choose Rand-Augment instead of AutoAugment. Overall our<br>experiments confirm that transformers require a strong data augmentation: al-<br>most all the data-augmentation methods that we evaluate prove to be useful.<br>One exception is dropout, which we exclude from our training procedure.</p>",
            "id": 113,
            "page": 15,
            "text": "Auto-Augment [11], Rand-Augment [12], and random erasing [62] im-\nprove the results. For the two latter we use the timm [55] customizations, and\nafter ablation we choose Rand-Augment instead of AutoAugment. Overall our\nexperiments confirm that transformers require a strong data augmentation: al-\nmost all the data-augmentation methods that we evaluate prove to be useful.\nOne exception is dropout, which we exclude from our training procedure."
        },
        {
            "bounding_box": [
                {
                    "x": 1249,
                    "y": 2894
                },
                {
                    "x": 1298,
                    "y": 2894
                },
                {
                    "x": 1298,
                    "y": 2934
                },
                {
                    "x": 1249,
                    "y": 2934
                }
            ],
            "category": "footer",
            "html": "<footer id='114' style='font-size:16px'>15</footer>",
            "id": 114,
            "page": 15,
            "text": "15"
        },
        {
            "bounding_box": [
                {
                    "x": 825,
                    "y": 517
                },
                {
                    "x": 1728,
                    "y": 517
                },
                {
                    "x": 1728,
                    "y": 1380
                },
                {
                    "x": 825,
                    "y": 1380
                }
            ],
            "category": "table",
            "html": "<table id='115' style='font-size:16px'><tr><td>Methods</td><td>ViT-B [15]</td><td>DeiT-B</td></tr><tr><td>Epochs</td><td>300</td><td>300</td></tr><tr><td>Batch size</td><td>4096</td><td>1024</td></tr><tr><td>Optimizer</td><td>AdamW</td><td>AdamW</td></tr><tr><td>learning rate</td><td>0.003</td><td>batchsize 0.0005 x 512</td></tr><tr><td>Learning rate decay</td><td>cosine</td><td>cosine</td></tr><tr><td>Weight decay</td><td>0.3</td><td>0.05</td></tr><tr><td>Warmup epochs</td><td>3.4</td><td>5</td></tr><tr><td>Label smoothing E</td><td>X</td><td>0.1</td></tr><tr><td>Dropout</td><td>0.1</td><td>X</td></tr><tr><td>Stoch. Depth</td><td>X</td><td>0.1</td></tr><tr><td>Repeated Aug</td><td>X</td><td>V</td></tr><tr><td>Gradient Clip.</td><td></td><td>X</td></tr><tr><td>Rand Augment</td><td>X</td><td>9/0.5</td></tr><tr><td>Mixup prob.</td><td>X</td><td>0.8</td></tr><tr><td>Cutmix prob.</td><td>X</td><td>1.0</td></tr><tr><td>Erasing prob.</td><td>X</td><td>0.25</td></tr></table>",
            "id": 115,
            "page": 16,
            "text": "Methods ViT-B [15] DeiT-B\n Epochs 300 300\n Batch size 4096 1024\n Optimizer AdamW AdamW\n learning rate 0.003 batchsize 0.0005 x 512\n Learning rate decay cosine cosine\n Weight decay 0.3 0.05\n Warmup epochs 3.4 5\n Label smoothing E X 0.1\n Dropout 0.1 X\n Stoch. Depth X 0.1\n Repeated Aug X V\n Gradient Clip.  X\n Rand Augment X 9/0.5\n Mixup prob. X 0.8\n Cutmix prob. X 1.0\n Erasing prob. X"
        },
        {
            "bounding_box": [
                {
                    "x": 625,
                    "y": 1388
                },
                {
                    "x": 1908,
                    "y": 1388
                },
                {
                    "x": 1908,
                    "y": 1439
                },
                {
                    "x": 625,
                    "y": 1439
                }
            ],
            "category": "caption",
            "html": "<br><caption id='116' style='font-size:18px'>Table 9: Ingredients and hyper-parameters for our method and Vit-B.</caption>",
            "id": 116,
            "page": 16,
            "text": "Table 9: Ingredients and hyper-parameters for our method and Vit-B."
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 1524
                },
                {
                    "x": 1996,
                    "y": 1524
                },
                {
                    "x": 1996,
                    "y": 1871
                },
                {
                    "x": 549,
                    "y": 1871
                }
            ],
            "category": "paragraph",
            "html": "<p id='117' style='font-size:20px'>Regularization & Optimizers. We have considered different optimizers and<br>cross-validated different learning rates and weight decays. Transformers are<br>sensitive to the setting of optimization hyper-parameters. Therefore, during<br>cross-validation, we tried 3 different learning rates (5.10-4, 3.10-4, 5.10-5) and<br>3 weight decay (0.03, 0.04, 0.05). We scale the learning rate according to the<br>lr batchsize, similarly to Goyal et<br>batch size with the formula: lrscaled = x<br>512<br>al. [19] except that we use 512 instead of 256 as the base value.</p>",
            "id": 117,
            "page": 16,
            "text": "Regularization & Optimizers. We have considered different optimizers and\ncross-validated different learning rates and weight decays. Transformers are\nsensitive to the setting of optimization hyper-parameters. Therefore, during\ncross-validation, we tried 3 different learning rates (5.10-4, 3.10-4, 5.10-5) and\n3 weight decay (0.03, 0.04, 0.05). We scale the learning rate according to the\nlr batchsize, similarly to Goyal et\nbatch size with the formula: lrscaled = x\n512\nal. [19] except that we use 512 instead of 256 as the base value."
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 1876
                },
                {
                    "x": 1995,
                    "y": 1876
                },
                {
                    "x": 1995,
                    "y": 2021
                },
                {
                    "x": 549,
                    "y": 2021
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='118' style='font-size:18px'>The best results use the AdamW optimizer with the same learning rates as<br>ViT [15] but with a much smaller weight decay, as the weight decay reported<br>in the paper hurts the convergence in our setting.</p>",
            "id": 118,
            "page": 16,
            "text": "The best results use the AdamW optimizer with the same learning rates as\nViT [15] but with a much smaller weight decay, as the weight decay reported\nin the paper hurts the convergence in our setting."
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 2024
                },
                {
                    "x": 1997,
                    "y": 2024
                },
                {
                    "x": 1997,
                    "y": 2323
                },
                {
                    "x": 549,
                    "y": 2323
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='119' style='font-size:22px'>We have employed stochastic depth [29], which facilitates the convergence<br>of transformers, especially deep ones [16, 17]. For vision transformers, they<br>were first adopted in the training procedure by Wightman [55]. Regularization<br>like Mixup [60] and Cutmix [59] improve performance. We also use repeated<br>augmentation [4, 25], which provides a significant boost in performance and is<br>one of the key ingredients of our proposed training procedure.</p>",
            "id": 119,
            "page": 16,
            "text": "We have employed stochastic depth [29], which facilitates the convergence\nof transformers, especially deep ones [16, 17]. For vision transformers, they\nwere first adopted in the training procedure by Wightman [55]. Regularization\nlike Mixup [60] and Cutmix [59] improve performance. We also use repeated\naugmentation [4, 25], which provides a significant boost in performance and is\none of the key ingredients of our proposed training procedure."
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 2384
                },
                {
                    "x": 1996,
                    "y": 2384
                },
                {
                    "x": 1996,
                    "y": 2584
                },
                {
                    "x": 549,
                    "y": 2584
                }
            ],
            "category": "paragraph",
            "html": "<p id='120' style='font-size:18px'>Exponential Moving Average (EMA). We evaluate the EMA of our network<br>obtained after training. There are small gains, which vanish after fine-tuning:<br>the EMA model has an edge of is 0.1 accuracy points, but when fine-tuned the<br>two models reach the same (improved) performance.</p>",
            "id": 120,
            "page": 16,
            "text": "Exponential Moving Average (EMA). We evaluate the EMA of our network\nobtained after training. There are small gains, which vanish after fine-tuning:\nthe EMA model has an edge of is 0.1 accuracy points, but when fine-tuned the\ntwo models reach the same (improved) performance."
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 2648
                },
                {
                    "x": 1999,
                    "y": 2648
                },
                {
                    "x": 1999,
                    "y": 2800
                },
                {
                    "x": 549,
                    "y": 2800
                }
            ],
            "category": "paragraph",
            "html": "<p id='121' style='font-size:20px'>Fine-tuning at different resolution. We adopt the fine-tuning procedure from<br>Touvron et al. [51]: our schedule, regularization and optimization procedure<br>are identical to that of FixEfficientNet but we keep the training-time data aug-</p>",
            "id": 121,
            "page": 16,
            "text": "Fine-tuning at different resolution. We adopt the fine-tuning procedure from\nTouvron et al. [51]: our schedule, regularization and optimization procedure\nare identical to that of FixEfficientNet but we keep the training-time data aug-"
        },
        {
            "bounding_box": [
                {
                    "x": 1249,
                    "y": 2895
                },
                {
                    "x": 1299,
                    "y": 2895
                },
                {
                    "x": 1299,
                    "y": 2935
                },
                {
                    "x": 1249,
                    "y": 2935
                }
            ],
            "category": "footer",
            "html": "<footer id='122' style='font-size:14px'>16</footer>",
            "id": 122,
            "page": 16,
            "text": "16"
        },
        {
            "bounding_box": [
                {
                    "x": 783,
                    "y": 519
                },
                {
                    "x": 1761,
                    "y": 519
                },
                {
                    "x": 1761,
                    "y": 824
                },
                {
                    "x": 783,
                    "y": 824
                }
            ],
            "category": "table",
            "html": "<table id='123' style='font-size:14px'><tr><td colspan=\"2\">image throughput</td><td rowspan=\"2\">Imagenet [42] acc. top-1</td><td rowspan=\"2\">Real [5] acc. top-1</td><td rowspan=\"2\">V2 [41] acc. top-1</td></tr><tr><td>size</td><td>(image/s)</td></tr><tr><td>1602</td><td>609.31</td><td>79.9</td><td>84.8</td><td>67.6</td></tr><tr><td>2242</td><td>291.05</td><td>81.8</td><td>86.7</td><td>71.5</td></tr><tr><td>3202</td><td>134.13</td><td>82.7</td><td>87.2</td><td>71.9</td></tr><tr><td>3842</td><td>85.87</td><td>83.1</td><td>87.7</td><td>72.4</td></tr></table>",
            "id": 123,
            "page": 17,
            "text": "image throughput Imagenet [42] acc. top-1 Real [5] acc. top-1 V2 [41] acc. top-1\n size (image/s)\n 1602 609.31 79.9 84.8 67.6\n 2242 291.05 81.8 86.7 71.5\n 3202 134.13 82.7 87.2 71.9\n 3842 85.87 83.1 87.7"
        },
        {
            "bounding_box": [
                {
                    "x": 548,
                    "y": 861
                },
                {
                    "x": 1997,
                    "y": 861
                },
                {
                    "x": 1997,
                    "y": 963
                },
                {
                    "x": 548,
                    "y": 963
                }
            ],
            "category": "caption",
            "html": "<caption id='124' style='font-size:20px'>Table 10: Performance of DeiT trained at size 2242 for varying finetuning sizes<br>on ImageNet-1k, ImageNet-Real and ImageNet-v2 matched frequency.</caption>",
            "id": 124,
            "page": 17,
            "text": "Table 10: Performance of DeiT trained at size 2242 for varying finetuning sizes\non ImageNet-1k, ImageNet-Real and ImageNet-v2 matched frequency."
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 1050
                },
                {
                    "x": 1996,
                    "y": 1050
                },
                {
                    "x": 1996,
                    "y": 1546
                },
                {
                    "x": 549,
                    "y": 1546
                }
            ],
            "category": "paragraph",
            "html": "<p id='125' style='font-size:20px'>mentation (contrary to the dampened data augmentation of Touvron et al. [51]).<br>We also interpolate the positional embeddings: In principle any classical image<br>scaling technique, like bilinear interpolation, could be used. However, a bilin-<br>ear interpolation of a vector from its neighbors reduces its l2-norm compared<br>to its neighbors. These low-norm vectors are not adapted to the pre-trained<br>transformers and we observe a significant drop in accuracy if we employ use<br>directly without any form of fine-tuning. Therefore we adopt a bicubic interpo-<br>lation that approximately preserves the norm of the vectors, before fine-tuning<br>the network with either AdamW [36] or SGD. These optimizers have a similar<br>performance for the fine-tuning stage, see Table 8.</p>",
            "id": 125,
            "page": 17,
            "text": "mentation (contrary to the dampened data augmentation of Touvron et al. [51]).\nWe also interpolate the positional embeddings: In principle any classical image\nscaling technique, like bilinear interpolation, could be used. However, a bilin-\near interpolation of a vector from its neighbors reduces its l2-norm compared\nto its neighbors. These low-norm vectors are not adapted to the pre-trained\ntransformers and we observe a significant drop in accuracy if we employ use\ndirectly without any form of fine-tuning. Therefore we adopt a bicubic interpo-\nlation that approximately preserves the norm of the vectors, before fine-tuning\nthe network with either AdamW [36] or SGD. These optimizers have a similar\nperformance for the fine-tuning stage, see Table 8."
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 1549
                },
                {
                    "x": 1997,
                    "y": 1549
                },
                {
                    "x": 1997,
                    "y": 1747
                },
                {
                    "x": 549,
                    "y": 1747
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='126' style='font-size:18px'>By default and similar to ViT [15] we train DeiT models with at resolution<br>224 and we fine-tune at resolution 384. We detail how to do this interpolation<br>in Section 3. However, in order to measure the influence of the resolution we<br>have finetuned DeiT at different resolutions. We report these results in Table 10.</p>",
            "id": 126,
            "page": 17,
            "text": "By default and similar to ViT [15] we train DeiT models with at resolution\n224 and we fine-tune at resolution 384. We detail how to do this interpolation\nin Section 3. However, in order to measure the influence of the resolution we\nhave finetuned DeiT at different resolutions. We report these results in Table 10."
        },
        {
            "bounding_box": [
                {
                    "x": 548,
                    "y": 1808
                },
                {
                    "x": 1997,
                    "y": 1808
                },
                {
                    "x": 1997,
                    "y": 2311
                },
                {
                    "x": 548,
                    "y": 2311
                }
            ],
            "category": "paragraph",
            "html": "<p id='127' style='font-size:18px'>Training time. A typical training of 300 epochs takes 37 hours with 2 nodes<br>or 53 hours on a single node for the DeiT-B.As a comparison point, a similar<br>training with a RegNetY-16GF [40] (84M parameters) is 20% slower. DeiT-S and<br>DeiT-Ti are trained in less than 3 days on 4 GPU. Then, optionally we fine-tune<br>the model at a larger resolution. This takes 20 hours on a single node (8 GPU)<br>to produce a FixDeiT-B model at resolution 384x384, which corresponds to 25<br>epochs. Not having to rely on batch-norm allows one to reduce the batch size<br>without impacting performance, which makes it easier to train larger models.<br>Note that, since we use repeated augmentation [4, 25] with 3 repetitions, we<br>only see one third of the images during a single epoch3.</p>",
            "id": 127,
            "page": 17,
            "text": "Training time. A typical training of 300 epochs takes 37 hours with 2 nodes\nor 53 hours on a single node for the DeiT-B.As a comparison point, a similar\ntraining with a RegNetY-16GF [40] (84M parameters) is 20% slower. DeiT-S and\nDeiT-Ti are trained in less than 3 days on 4 GPU. Then, optionally we fine-tune\nthe model at a larger resolution. This takes 20 hours on a single node (8 GPU)\nto produce a FixDeiT-B model at resolution 384x384, which corresponds to 25\nepochs. Not having to rely on batch-norm allows one to reduce the batch size\nwithout impacting performance, which makes it easier to train larger models.\nNote that, since we use repeated augmentation [4, 25] with 3 repetitions, we\nonly see one third of the images during a single epoch3."
        },
        {
            "bounding_box": [
                {
                    "x": 553,
                    "y": 2390
                },
                {
                    "x": 961,
                    "y": 2390
                },
                {
                    "x": 961,
                    "y": 2451
                },
                {
                    "x": 553,
                    "y": 2451
                }
            ],
            "category": "paragraph",
            "html": "<p id='128' style='font-size:22px'>7 Conclusion</p>",
            "id": 128,
            "page": 17,
            "text": "7 Conclusion"
        },
        {
            "bounding_box": [
                {
                    "x": 548,
                    "y": 2495
                },
                {
                    "x": 1996,
                    "y": 2495
                },
                {
                    "x": 1996,
                    "y": 2594
                },
                {
                    "x": 548,
                    "y": 2594
                }
            ],
            "category": "paragraph",
            "html": "<p id='129' style='font-size:18px'>In this paper, we have introduced DeiT, which are image transformers that<br>do not require very large amount of data to be trained, thanks to improved</p>",
            "id": 129,
            "page": 17,
            "text": "In this paper, we have introduced DeiT, which are image transformers that\ndo not require very large amount of data to be trained, thanks to improved"
        },
        {
            "bounding_box": [
                {
                    "x": 550,
                    "y": 2629
                },
                {
                    "x": 1996,
                    "y": 2629
                },
                {
                    "x": 1996,
                    "y": 2750
                },
                {
                    "x": 550,
                    "y": 2750
                }
            ],
            "category": "paragraph",
            "html": "<p id='130' style='font-size:14px'>3Formally it means that we have 100 epochs, but each is 3x longer because of the repeated<br>augmentations. We prefer to refer to this as 300 epochs in order to have a direct comparison on the<br>effective training time with and without repeated augmentation.</p>",
            "id": 130,
            "page": 17,
            "text": "3Formally it means that we have 100 epochs, but each is 3x longer because of the repeated\naugmentations. We prefer to refer to this as 300 epochs in order to have a direct comparison on the\neffective training time with and without repeated augmentation."
        },
        {
            "bounding_box": [
                {
                    "x": 1249,
                    "y": 2894
                },
                {
                    "x": 1296,
                    "y": 2894
                },
                {
                    "x": 1296,
                    "y": 2933
                },
                {
                    "x": 1249,
                    "y": 2933
                }
            ],
            "category": "footer",
            "html": "<footer id='131' style='font-size:16px'>17</footer>",
            "id": 131,
            "page": 17,
            "text": "17"
        },
        {
            "bounding_box": [
                {
                    "x": 548,
                    "y": 524
                },
                {
                    "x": 2012,
                    "y": 524
                },
                {
                    "x": 2012,
                    "y": 922
                },
                {
                    "x": 548,
                    "y": 922
                }
            ],
            "category": "paragraph",
            "html": "<p id='132' style='font-size:18px'>training and in particular a novel distillation procedure. Convolutional neu-<br>ral networks have optimized, both in terms of architecture and optimization<br>during almost a decade, including through extensive architecture search that<br>is prone to overfiting, as it is the case for instance for EfficientNets [51]. For<br>DeiT we have started the existing data augmentation and regularization strate-<br>gies pre-existing for convnets, not introducing any significant architectural be-<br>yond our novel distillation token. Therefore it is likely that research on data-<br>augmentation more adapted or learned for transformers will bring further gains.</p>",
            "id": 132,
            "page": 18,
            "text": "training and in particular a novel distillation procedure. Convolutional neu-\nral networks have optimized, both in terms of architecture and optimization\nduring almost a decade, including through extensive architecture search that\nis prone to overfiting, as it is the case for instance for EfficientNets [51]. For\nDeiT we have started the existing data augmentation and regularization strate-\ngies pre-existing for convnets, not introducing any significant architectural be-\nyond our novel distillation token. Therefore it is likely that research on data-\naugmentation more adapted or learned for transformers will bring further gains."
        },
        {
            "bounding_box": [
                {
                    "x": 548,
                    "y": 924
                },
                {
                    "x": 2000,
                    "y": 924
                },
                {
                    "x": 2000,
                    "y": 1073
                },
                {
                    "x": 548,
                    "y": 1073
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='133' style='font-size:20px'>Therefore, considering our results, where image transformers are on par<br>with convnets already, we believe that they will rapidly become a method of<br>choice considering their lower memory footprint for a given accuracy.</p>",
            "id": 133,
            "page": 18,
            "text": "Therefore, considering our results, where image transformers are on par\nwith convnets already, we believe that they will rapidly become a method of\nchoice considering their lower memory footprint for a given accuracy."
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 1075
                },
                {
                    "x": 1997,
                    "y": 1075
                },
                {
                    "x": 1997,
                    "y": 1172
                },
                {
                    "x": 549,
                    "y": 1172
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='134' style='font-size:14px'>We provide an open-source implementation of our method. It is available<br>at https : / / github · com/ facebookresearch / deit.</p>",
            "id": 134,
            "page": 18,
            "text": "We provide an open-source implementation of our method. It is available\nat https : / / github · com/ facebookresearch / deit."
        },
        {
            "bounding_box": [
                {
                    "x": 552,
                    "y": 1240
                },
                {
                    "x": 1015,
                    "y": 1240
                },
                {
                    "x": 1015,
                    "y": 1296
                },
                {
                    "x": 552,
                    "y": 1296
                }
            ],
            "category": "paragraph",
            "html": "<p id='135' style='font-size:22px'>Acknowledgements</p>",
            "id": 135,
            "page": 18,
            "text": "Acknowledgements"
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 1322
                },
                {
                    "x": 2000,
                    "y": 1322
                },
                {
                    "x": 2000,
                    "y": 1623
                },
                {
                    "x": 549,
                    "y": 1623
                }
            ],
            "category": "paragraph",
            "html": "<p id='136' style='font-size:20px'>Many thanks to Ross Wightman for sharing his ViT code and bootstrapping<br>training method with the community, as well as for valuable feedback that<br>helped us to fix different aspects of this paper. Thanks to Vinicius Reis, Mannat<br>Singh, Ari Morcos, Mark Tygert, Gabriel Synnaeve, and other colleagues at<br>Facebook for brainstorming and some exploration on this axis. Thanks to Ross<br>Girshick and Piotr Dollar for constructive comments.</p>",
            "id": 136,
            "page": 18,
            "text": "Many thanks to Ross Wightman for sharing his ViT code and bootstrapping\ntraining method with the community, as well as for valuable feedback that\nhelped us to fix different aspects of this paper. Thanks to Vinicius Reis, Mannat\nSingh, Ari Morcos, Mark Tygert, Gabriel Synnaeve, and other colleagues at\nFacebook for brainstorming and some exploration on this axis. Thanks to Ross\nGirshick and Piotr Dollar for constructive comments."
        },
        {
            "bounding_box": [
                {
                    "x": 1248,
                    "y": 2892
                },
                {
                    "x": 1299,
                    "y": 2892
                },
                {
                    "x": 1299,
                    "y": 2934
                },
                {
                    "x": 1248,
                    "y": 2934
                }
            ],
            "category": "footer",
            "html": "<footer id='137' style='font-size:16px'>18</footer>",
            "id": 137,
            "page": 18,
            "text": "18"
        },
        {
            "bounding_box": [
                {
                    "x": 551,
                    "y": 515
                },
                {
                    "x": 861,
                    "y": 515
                },
                {
                    "x": 861,
                    "y": 577
                },
                {
                    "x": 551,
                    "y": 577
                }
            ],
            "category": "paragraph",
            "html": "<p id='138' style='font-size:20px'>References</p>",
            "id": 138,
            "page": 19,
            "text": "References"
        },
        {
            "bounding_box": [
                {
                    "x": 553,
                    "y": 594
                },
                {
                    "x": 2003,
                    "y": 594
                },
                {
                    "x": 2003,
                    "y": 2794
                },
                {
                    "x": 553,
                    "y": 2794
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='139' style='font-size:16px'>[1] Samira Abnar, Mostafa Dehghani, and Willem Zuidema. Transferring inductive<br>biases through knowledge distillation. arXiv preprint arXiv:2006.00555, 2020.<br>[2] Jie Hu andLi Shen and Gang Sun. Squeeze-and-excitation networks. arXiv preprint<br>arXiv:1709.01507, 2017.<br>[3] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization.<br>arXiv preprint arXiv:1607.06450, 2016.<br>[4] Maxim Berman, Herve Jegou, Andrea Vedaldi, Iasonas Kokkinos, and Matthijs<br>Douze. Multigrain: a unified image embedding for classes and instances. arXiv<br>preprint arXiv:1902.05509, 2019.<br>[5] Lucas Beyer, Olivier J. Henaff, Alexander Kolesnikov, Xiaohua Zhai, and Aaron<br>van den Oord. Are we done with imagenet? arXiv preprint arXiv:2006.07159, 2020.<br>[6] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander<br>Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In<br>European Conference on Computer Vision, 2020.<br>[7] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan,<br>and Ilya Sutskever. Generative pretraining from pixels. In International Conference<br>on Machine Learning, 2020.<br>[8] Yen-Chun Chen, Linjie Li, Licheng Yu, A. E. Kholy, Faisal Ahmed, Zhe Gan, Y.<br>Cheng, and Jing jing Liu. Uniter: Universal image-text representation learning. In<br>European Conference on Computer Vision, 2020.<br>[9] J. H. Cho and B. Hariharan. On the efficacy of knowledge distillation. International<br>Conference on Computer Vision, 2019.<br>[10] P. Chu, Xiao Bian, Shaopeng Liu, and Haibin Ling. Feature space augmentation<br>for long-tailed data. arXiv preprint arXiv:2008.03673, 2020.<br>[11] Ekin Dogus Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V.<br>Le. Autoaugment: Learning augmentation policies from data. arXiv preprint<br>arXiv:1805.09501, 2018.<br>[12] Ekin D. Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V. Le. Randaugment:<br>Practical automated data augmentation with a reduced search space. arXiv preprint<br>arXiv:1909.13719, 2019.<br>[13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet:<br>A large-scale hierarchical image database. In Conference on Computer Vision and<br>Pattern Recognition, pages 248-255, 2009.<br>[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-<br>training of deep bidirectional transformers for language understanding. arXiv<br>preprint arXiv:1810.04805, 2018.<br>[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-<br>aohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg<br>Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for<br>image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.<br>[16] Angela Fan, Edouard Grave, and Armand Joulin. Reducing transformer depth<br>on demand with structured dropout. arXiv preprint arXiv:1909.11556, 2019. ICLR<br>2020.<br>[17] Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, Remi Gribonval,<br>Herve Jegou, and Armand Joulin. Training with quantization noise for extreme<br>model compression. arXiv preprint arXiv:2004.07320, 2020.</p>",
            "id": 139,
            "page": 19,
            "text": "[1] Samira Abnar, Mostafa Dehghani, and Willem Zuidema. Transferring inductive\nbiases through knowledge distillation. arXiv preprint arXiv:2006.00555, 2020.\n[2] Jie Hu andLi Shen and Gang Sun. Squeeze-and-excitation networks. arXiv preprint\narXiv:1709.01507, 2017.\n[3] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization.\narXiv preprint arXiv:1607.06450, 2016.\n[4] Maxim Berman, Herve Jegou, Andrea Vedaldi, Iasonas Kokkinos, and Matthijs\nDouze. Multigrain: a unified image embedding for classes and instances. arXiv\npreprint arXiv:1902.05509, 2019.\n[5] Lucas Beyer, Olivier J. Henaff, Alexander Kolesnikov, Xiaohua Zhai, and Aaron\nvan den Oord. Are we done with imagenet? arXiv preprint arXiv:2006.07159, 2020.\n[6] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander\nKirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In\nEuropean Conference on Computer Vision, 2020.\n[7] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan,\nand Ilya Sutskever. Generative pretraining from pixels. In International Conference\non Machine Learning, 2020.\n[8] Yen-Chun Chen, Linjie Li, Licheng Yu, A. E. Kholy, Faisal Ahmed, Zhe Gan, Y.\nCheng, and Jing jing Liu. Uniter: Universal image-text representation learning. In\nEuropean Conference on Computer Vision, 2020.\n[9] J. H. Cho and B. Hariharan. On the efficacy of knowledge distillation. International\nConference on Computer Vision, 2019.\n[10] P. Chu, Xiao Bian, Shaopeng Liu, and Haibin Ling. Feature space augmentation\nfor long-tailed data. arXiv preprint arXiv:2008.03673, 2020.\n[11] Ekin Dogus Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V.\nLe. Autoaugment: Learning augmentation policies from data. arXiv preprint\narXiv:1805.09501, 2018.\n[12] Ekin D. Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V. Le. Randaugment:\nPractical automated data augmentation with a reduced search space. arXiv preprint\narXiv:1909.13719, 2019.\n[13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet:\nA large-scale hierarchical image database. In Conference on Computer Vision and\nPattern Recognition, pages 248-255, 2009.\n[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-\ntraining of deep bidirectional transformers for language understanding. arXiv\npreprint arXiv:1810.04805, 2018.\n[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi-\naohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg\nHeigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for\nimage recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n[16] Angela Fan, Edouard Grave, and Armand Joulin. Reducing transformer depth\non demand with structured dropout. arXiv preprint arXiv:1909.11556, 2019. ICLR\n2020.\n[17] Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, Remi Gribonval,\nHerve Jegou, and Armand Joulin. Training with quantization noise for extreme\nmodel compression. arXiv preprint arXiv:2004.07320, 2020."
        },
        {
            "bounding_box": [
                {
                    "x": 1249,
                    "y": 2893
                },
                {
                    "x": 1298,
                    "y": 2893
                },
                {
                    "x": 1298,
                    "y": 2934
                },
                {
                    "x": 1249,
                    "y": 2934
                }
            ],
            "category": "footer",
            "html": "<footer id='140' style='font-size:14px'>19</footer>",
            "id": 140,
            "page": 19,
            "text": "19"
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 510
                },
                {
                    "x": 2002,
                    "y": 510
                },
                {
                    "x": 2002,
                    "y": 2805
                },
                {
                    "x": 549,
                    "y": 2805
                }
            ],
            "category": "paragraph",
            "html": "<p id='141' style='font-size:18px'>[18] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin.<br>Convolutional sequence to sequence learning. arXiv preprint arXiv:1705.03122,<br>2017.<br>[19] Priya Goyal, Piotr Dollar, Ross B. Girshick, Pieter Noordhuis, Lukasz Wesolowski,<br>Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large<br>minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.<br>[20] Boris Hanin and David Rolnick. How to start training: The effect of initialization<br>and architecture. NIPS, 31, 2018.<br>[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning<br>for image recognition. In Conference on Computer Vision and Pattern Recognition,<br>June 2016.<br>[22] Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li. Bag<br>of tricks for image classification with convolutional neural networks. In Conference<br>on Computer Vision and Pattern Recognition, 2019.<br>[23] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv<br>preprint arXiv:1606.08415, 2016.<br>[24] Geoffrey E. Hinton, Oriol Vinyals, andJ. Dean. Distilling the knowledge in a neural<br>network. arXiv preprint arXiv:1503.02531, 2015.<br>[25] Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoefler, and Daniel<br>Soudry. Augment your batch: Improving generalization through instance repeti-<br>tion. In Conference on Computer Vision and Pattern Recognition, 2020.<br>[26] Grant Van Horn, Oisin Mac Aodha, Yang Song, Alexander Shepard, Hartwig<br>Adam, Pietro Perona, and Serge J. Belongie. The inaturalist challenge 2018 dataset.<br>arXiv preprint arXiv:1707.06642, 2018.<br>[27] Grant Van Horn, Oisin Mac Aodha, Yang Song, Alexander Shepard, Hartwig<br>Adam, Pietro Perona, and Serge J. Belongie. The inaturalist challenge 2019 dataset.<br>arXiv preprint arXiv:1707.06642, 2019.<br>[28] H. Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Y. Wei. Relation networks for<br>object detection. Conference on Computer Vision and Pattern Recognition, 2018.<br>[29] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q. Weinberger. Deep<br>networks with stochastic depth. In European Conference on Computer Vision, 2016.<br>[30] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations<br>for fine-grained categorization. In 4th International IEEE Workshop on 3D Represen-<br>tation and Recognition (3dRR-13), 2013.<br>[31] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical<br>report, CIFAR, 2009.<br>[32] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification<br>with deep convolutional neural networks. In NIPS, 2012.<br>[33] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visu-<br>alBERT: a simple and performant baseline for vision and language. arXiv preprint<br>arXiv:1908.03557, 2019.<br>[34] Xiang Li, Wenhai Wang, Xiaolin Hu, and Jian Yang. Selective kernel networks.<br>Conference on Computer Vision and Pattern Recognition, 2019.<br>[35] Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahen-<br>dran, Georg Heigold, Jakob Uszkoreit, A. Dosovitskiy, and Thomas Kipf. Object-<br>centric learning with slot attention. arXiv preprint arXiv:2006.15055, 2020.<br>[36] I. Loshchilov and F. Hutter. Fixing weight decay regularization in adam. arXiv<br>preprint arXiv:1711.05101, 2017.</p>",
            "id": 141,
            "page": 20,
            "text": "[18] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin.\nConvolutional sequence to sequence learning. arXiv preprint arXiv:1705.03122,\n2017.\n[19] Priya Goyal, Piotr Dollar, Ross B. Girshick, Pieter Noordhuis, Lukasz Wesolowski,\nAapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large\nminibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.\n[20] Boris Hanin and David Rolnick. How to start training: The effect of initialization\nand architecture. NIPS, 31, 2018.\n[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning\nfor image recognition. In Conference on Computer Vision and Pattern Recognition,\nJune 2016.\n[22] Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Junyuan Xie, and Mu Li. Bag\nof tricks for image classification with convolutional neural networks. In Conference\non Computer Vision and Pattern Recognition, 2019.\n[23] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv\npreprint arXiv:1606.08415, 2016.\n[24] Geoffrey E. Hinton, Oriol Vinyals, andJ. Dean. Distilling the knowledge in a neural\nnetwork. arXiv preprint arXiv:1503.02531, 2015.\n[25] Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoefler, and Daniel\nSoudry. Augment your batch: Improving generalization through instance repeti-\ntion. In Conference on Computer Vision and Pattern Recognition, 2020.\n[26] Grant Van Horn, Oisin Mac Aodha, Yang Song, Alexander Shepard, Hartwig\nAdam, Pietro Perona, and Serge J. Belongie. The inaturalist challenge 2018 dataset.\narXiv preprint arXiv:1707.06642, 2018.\n[27] Grant Van Horn, Oisin Mac Aodha, Yang Song, Alexander Shepard, Hartwig\nAdam, Pietro Perona, and Serge J. Belongie. The inaturalist challenge 2019 dataset.\narXiv preprint arXiv:1707.06642, 2019.\n[28] H. Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Y. Wei. Relation networks for\nobject detection. Conference on Computer Vision and Pattern Recognition, 2018.\n[29] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q. Weinberger. Deep\nnetworks with stochastic depth. In European Conference on Computer Vision, 2016.\n[30] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations\nfor fine-grained categorization. In 4th International IEEE Workshop on 3D Represen-\ntation and Recognition (3dRR-13), 2013.\n[31] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical\nreport, CIFAR, 2009.\n[32] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification\nwith deep convolutional neural networks. In NIPS, 2012.\n[33] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visu-\nalBERT: a simple and performant baseline for vision and language. arXiv preprint\narXiv:1908.03557, 2019.\n[34] Xiang Li, Wenhai Wang, Xiaolin Hu, and Jian Yang. Selective kernel networks.\nConference on Computer Vision and Pattern Recognition, 2019.\n[35] Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahen-\ndran, Georg Heigold, Jakob Uszkoreit, A. Dosovitskiy, and Thomas Kipf. Object-\ncentric learning with slot attention. arXiv preprint arXiv:2006.15055, 2020.\n[36] I. Loshchilov and F. Hutter. Fixing weight decay regularization in adam. arXiv\npreprint arXiv:1711.05101, 2017."
        },
        {
            "bounding_box": [
                {
                    "x": 1247,
                    "y": 2892
                },
                {
                    "x": 1298,
                    "y": 2892
                },
                {
                    "x": 1298,
                    "y": 2934
                },
                {
                    "x": 1247,
                    "y": 2934
                }
            ],
            "category": "footer",
            "html": "<footer id='142' style='font-size:14px'>20</footer>",
            "id": 142,
            "page": 20,
            "text": "20"
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 510
                },
                {
                    "x": 2001,
                    "y": 510
                },
                {
                    "x": 2001,
                    "y": 2795
                },
                {
                    "x": 549,
                    "y": 2795
                }
            ],
            "category": "paragraph",
            "html": "<p id='143' style='font-size:18px'>[37] Jiasen Lu, Dhruv Batra, D. Parikh, and Stefan Lee. Vilbert: Pretraining task-<br>agnostic visiolinguistic representations for vision-and-language tasks. In NIPS,<br>2019.<br>[38] M-E. Nilsback and A. Zisserman. Automated flower classification over a large<br>number of classes. In Proceedings of the Indian Conference on Computer Vision, Graph-<br>ics and Image Processing, 2008.<br>[39] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory<br>Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Py-<br>torch: An imperative style, high-performance deep learning library. In Advances in<br>neural information processing systems, pages 8026-8037, 2019.<br>[40] Ilija Radosavovic, Raj Prateek Kosaraju, Ross B. Girshick, Kaiming He, and Piotr<br>Dollar. Designing network design spaces. Conference on Computer Vision and Pattern<br>Recognition, 2020.<br>[41] B. Recht, Rebecca Roelofs, L. Schmidt, and V. Shankar. Do imagenet classifiers<br>generalize to imagenet? arXiv preprint arXiv:1902.10811, 2019.<br>[42] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean<br>Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexan-<br>der C. Berg, and Li Fei-Fei. Imagenet large scale visual recognition challenge. In-<br>ternational journal of Computer Vision, 2015.<br>[43] Zhuoran Shen, Irwan Bello, Raviteja Vemulapalli, Xuhui Jia, and Ching-Hui<br>Chen. Global self-attention networks for image recognition. arXiv preprint<br>arXiv:2010.03019, 2020.<br>[44] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale<br>image recognition. In International Conference on Learning Representations, 2015.<br>[45] C. Sun, A. Myers, Carl Vondrick, Kevin Murphy, and C. Schmid. Videobert: A<br>joint model for video and language representation learning. Conference on Computer<br>Vision and Pattern Recognition, 2019.<br>[46] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting<br>unreasonable effectiveness of data in deep learning era. In Proceedings of the IEEE<br>international conference on computer vision, pages 843-852, 2017.<br>[47] Christian Szegedy, V. Vanhoucke, S. Ioffe, Jon Shlens, and Z. Wojna. Rethinking<br>the inception architecture for computer vision. Conference on Computer Vision and<br>Pattern Recognition, 2016.<br>[48] Mingxing Tan and Quoc V. Le. Efficientnet: Rethinking model scaling for convo-<br>lutional neural networks. arXiv preprint arXiv:1905.11946, 2019.<br>[49] Hugo Touvron, Alexandre Sablayrolles, M. Douze, M. Cord, and H. Jegou. Grafit:<br>Learning fine-grained image representations with coarse labels. arXiv preprint<br>arXiv:2011.12982, 2020.<br>[50] Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve Jegou. Fixing the<br>train-test resolution discrepancy. NIPS, 2019.<br>[51] Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve Jegou. Fixing the<br>train-test resolution discrepancy: Fixefficientnet. arXiv preprint arXiv:2003.08237,<br>2020.<br>[52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,<br>Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need.<br>In NIPS, 2017.<br>[53] X. Wang, Ross B. Girshick, A. Gupta, and Kaiming He. Non-local neural networks.<br>Conference on Computer Vision and Pattern Recognition, 2018.</p>",
            "id": 143,
            "page": 21,
            "text": "[37] Jiasen Lu, Dhruv Batra, D. Parikh, and Stefan Lee. Vilbert: Pretraining task-\nagnostic visiolinguistic representations for vision-and-language tasks. In NIPS,\n2019.\n[38] M-E. Nilsback and A. Zisserman. Automated flower classification over a large\nnumber of classes. In Proceedings of the Indian Conference on Computer Vision, Graph-\nics and Image Processing, 2008.\n[39] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory\nChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Py-\ntorch: An imperative style, high-performance deep learning library. In Advances in\nneural information processing systems, pages 8026-8037, 2019.\n[40] Ilija Radosavovic, Raj Prateek Kosaraju, Ross B. Girshick, Kaiming He, and Piotr\nDollar. Designing network design spaces. Conference on Computer Vision and Pattern\nRecognition, 2020.\n[41] B. Recht, Rebecca Roelofs, L. Schmidt, and V. Shankar. Do imagenet classifiers\ngeneralize to imagenet? arXiv preprint arXiv:1902.10811, 2019.\n[42] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean\nMa, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexan-\nder C. Berg, and Li Fei-Fei. Imagenet large scale visual recognition challenge. In-\nternational journal of Computer Vision, 2015.\n[43] Zhuoran Shen, Irwan Bello, Raviteja Vemulapalli, Xuhui Jia, and Ching-Hui\nChen. Global self-attention networks for image recognition. arXiv preprint\narXiv:2010.03019, 2020.\n[44] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale\nimage recognition. In International Conference on Learning Representations, 2015.\n[45] C. Sun, A. Myers, Carl Vondrick, Kevin Murphy, and C. Schmid. Videobert: A\njoint model for video and language representation learning. Conference on Computer\nVision and Pattern Recognition, 2019.\n[46] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting\nunreasonable effectiveness of data in deep learning era. In Proceedings of the IEEE\ninternational conference on computer vision, pages 843-852, 2017.\n[47] Christian Szegedy, V. Vanhoucke, S. Ioffe, Jon Shlens, and Z. Wojna. Rethinking\nthe inception architecture for computer vision. Conference on Computer Vision and\nPattern Recognition, 2016.\n[48] Mingxing Tan and Quoc V. Le. Efficientnet: Rethinking model scaling for convo-\nlutional neural networks. arXiv preprint arXiv:1905.11946, 2019.\n[49] Hugo Touvron, Alexandre Sablayrolles, M. Douze, M. Cord, and H. Jegou. Grafit:\nLearning fine-grained image representations with coarse labels. arXiv preprint\narXiv:2011.12982, 2020.\n[50] Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve Jegou. Fixing the\ntrain-test resolution discrepancy. NIPS, 2019.\n[51] Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve Jegou. Fixing the\ntrain-test resolution discrepancy: Fixefficientnet. arXiv preprint arXiv:2003.08237,\n2020.\n[52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need.\nIn NIPS, 2017.\n[53] X. Wang, Ross B. Girshick, A. Gupta, and Kaiming He. Non-local neural networks.\nConference on Computer Vision and Pattern Recognition, 2018."
        },
        {
            "bounding_box": [
                {
                    "x": 1246,
                    "y": 2892
                },
                {
                    "x": 1294,
                    "y": 2892
                },
                {
                    "x": 1294,
                    "y": 2934
                },
                {
                    "x": 1246,
                    "y": 2934
                }
            ],
            "category": "footer",
            "html": "<footer id='144' style='font-size:14px'>21</footer>",
            "id": 144,
            "page": 21,
            "text": "21"
        },
        {
            "bounding_box": [
                {
                    "x": 550,
                    "y": 519
                },
                {
                    "x": 2001,
                    "y": 519
                },
                {
                    "x": 2001,
                    "y": 1707
                },
                {
                    "x": 550,
                    "y": 1707
                }
            ],
            "category": "paragraph",
            "html": "<p id='145' style='font-size:18px'>[54] Longhui Wei, An Xiao, Lingxi Xie, Xin Chen, Xiaopeng Zhang, and Qi Tian. Cir-<br>cumventing outliers of autoaugment with knowledge distillation. European Con-<br>ference on Computer Vision, 2020.<br>[55] Ross Wightman. Pytorch image models. https : / / github · com/ rwi ghtman/<br>pytorch-image-models, 2019.<br>[56] Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Masayoshi<br>Tomizuka, Kurt Keutzer, and Peter Vajda. Visual transformers: Token-based image<br>representation and processing for computer vision. arXiv preprint arXiv:2006.03677,<br>2020.<br>[57] Qizhe Xie, Eduard H. Hovy, Minh-Thang Luong, and Quoc V. Le. Self-<br>training with noisy student improves imagenet classification. arXiv preprint<br>arXiv:1911.04252, 2019.<br>[58] L. Yuan, F. Tay, G. Li, T. Wang, and Jiashi Feng. Revisit knowledge distillation:<br>a teacher-free framework. Conference on Computer Vision and Pattern Recognition,<br>2020.<br>[59] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and<br>Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with<br>localizable features. arXiv preprint arXiv:1905.04899, 2019.<br>[60] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup:<br>Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017.<br>[61] Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Zhi Zhang, Haibin Lin, Yue<br>Sun, Tong He, Jonas Muller, R. Manmatha, Mu Li, and Alexander Smola. Resnest:<br>Split-attention networks. arXiv preprint arXiv:2004.08955, 2020.<br>[62] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random eras-<br>ing data augmentation. In AAAI, 2020.</p>",
            "id": 145,
            "page": 22,
            "text": "[54] Longhui Wei, An Xiao, Lingxi Xie, Xin Chen, Xiaopeng Zhang, and Qi Tian. Cir-\ncumventing outliers of autoaugment with knowledge distillation. European Con-\nference on Computer Vision, 2020.\n[55] Ross Wightman. Pytorch image models. https : / / github · com/ rwi ghtman/\npytorch-image-models, 2019.\n[56] Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Masayoshi\nTomizuka, Kurt Keutzer, and Peter Vajda. Visual transformers: Token-based image\nrepresentation and processing for computer vision. arXiv preprint arXiv:2006.03677,\n2020.\n[57] Qizhe Xie, Eduard H. Hovy, Minh-Thang Luong, and Quoc V. Le. Self-\ntraining with noisy student improves imagenet classification. arXiv preprint\narXiv:1911.04252, 2019.\n[58] L. Yuan, F. Tay, G. Li, T. Wang, and Jiashi Feng. Revisit knowledge distillation:\na teacher-free framework. Conference on Computer Vision and Pattern Recognition,\n2020.\n[59] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and\nYoungjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with\nlocalizable features. arXiv preprint arXiv:1905.04899, 2019.\n[60] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and David Lopez-Paz. mixup:\nBeyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017.\n[61] Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Zhi Zhang, Haibin Lin, Yue\nSun, Tong He, Jonas Muller, R. Manmatha, Mu Li, and Alexander Smola. Resnest:\nSplit-attention networks. arXiv preprint arXiv:2004.08955, 2020.\n[62] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random eras-\ning data augmentation. In AAAI, 2020."
        },
        {
            "bounding_box": [
                {
                    "x": 1246,
                    "y": 2892
                },
                {
                    "x": 1298,
                    "y": 2892
                },
                {
                    "x": 1298,
                    "y": 2934
                },
                {
                    "x": 1246,
                    "y": 2934
                }
            ],
            "category": "footer",
            "html": "<footer id='146' style='font-size:14px'>22</footer>",
            "id": 146,
            "page": 22,
            "text": "22"
        }
    ]
}