{
    "id": "32a877fa-0f94-11ef-8828-426932df3dcf",
    "pdf_path": "/root/data/pdf/1503.00075v3.pdf",
    "elements": [
        {
            "bounding_box": [
                {
                    "x": 552,
                    "y": 286
                },
                {
                    "x": 1939,
                    "y": 286
                },
                {
                    "x": 1939,
                    "y": 427
                },
                {
                    "x": 552,
                    "y": 427
                }
            ],
            "category": "paragraph",
            "html": "<p id='0' style='font-size:22px'>Improved Semantic Representations From<br>Tree-Structured Long Short- Term Memory Networks</p>",
            "id": 0,
            "page": 1,
            "text": "Improved Semantic Representations From Tree-Structured Long Short- Term Memory Networks"
        },
        {
            "bounding_box": [
                {
                    "x": 611,
                    "y": 511
                },
                {
                    "x": 1871,
                    "y": 511
                },
                {
                    "x": 1871,
                    "y": 574
                },
                {
                    "x": 611,
                    "y": 574
                }
            ],
            "category": "paragraph",
            "html": "<p id='1' style='font-size:22px'>Kai Sheng Tai, Richard Socher*, Christopher D. Manning</p>",
            "id": 1,
            "page": 1,
            "text": "Kai Sheng Tai, Richard Socher*, Christopher D. Manning"
        },
        {
            "bounding_box": [
                {
                    "x": 547,
                    "y": 573
                },
                {
                    "x": 1938,
                    "y": 573
                },
                {
                    "x": 1938,
                    "y": 630
                },
                {
                    "x": 547,
                    "y": 630
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='2' style='font-size:20px'>Computer Science Department, Stanford University, *MetaMind Inc.</p>",
            "id": 2,
            "page": 1,
            "text": "Computer Science Department, Stanford University, *MetaMind Inc."
        },
        {
            "bounding_box": [
                {
                    "x": 314,
                    "y": 635
                },
                {
                    "x": 2177,
                    "y": 635
                },
                {
                    "x": 2177,
                    "y": 689
                },
                {
                    "x": 314,
                    "y": 689
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='3' style='font-size:14px'>kst@cs · stanford · edu, ri chard@met amind · io, manning@stanford. edu</p>",
            "id": 3,
            "page": 1,
            "text": "kst@cs · stanford · edu, ri chard@met amind · io, manning@stanford. edu"
        },
        {
            "bounding_box": [
                {
                    "x": 653,
                    "y": 855
                },
                {
                    "x": 854,
                    "y": 855
                },
                {
                    "x": 854,
                    "y": 911
                },
                {
                    "x": 653,
                    "y": 911
                }
            ],
            "category": "paragraph",
            "html": "<p id='4' style='font-size:20px'>Abstract</p>",
            "id": 4,
            "page": 1,
            "text": "Abstract"
        },
        {
            "bounding_box": [
                {
                    "x": 360,
                    "y": 935
                },
                {
                    "x": 1151,
                    "y": 935
                },
                {
                    "x": 1151,
                    "y": 2070
                },
                {
                    "x": 360,
                    "y": 2070
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='5' style='font-size:16px'>Because of their superior ability to pre-<br>serve sequence information over time,<br>Long Short-Term Memory (LSTM) net-<br>works, a type of recurrent neural net-<br>work with a more complex computational<br>unit, have obtained strong results on a va-<br>riety of sequence modeling tasks. The<br>only underlying LSTM structure that has<br>been explored SO far is a linear chain.<br>However, natural language exhibits syn-<br>tactic properties that would naturally com-<br>bine words to phrases. We introduce the<br>Tree-LSTM, a generalization of LSTMs to<br>tree-structured network topologies. Tree-<br>LSTMs outperform all existing systems<br>and strong LSTM baselines on two tasks:<br>predicting the semantic relatedness of two<br>sentences (SemEval 2014, Task 1) and<br>sentiment classification (Stanford Senti-<br>ment Treebank).</p>",
            "id": 5,
            "page": 1,
            "text": "Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored SO far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. TreeLSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank)."
        },
        {
            "bounding_box": [
                {
                    "x": 1334,
                    "y": 849
                },
                {
                    "x": 2128,
                    "y": 849
                },
                {
                    "x": 2128,
                    "y": 1657
                },
                {
                    "x": 1334,
                    "y": 1657
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='6' style='font-size:14px' alt=\"Y1 Y2 Y3 Y4\nX1 X2 X3 X4\nY1\n↑\nY2 Y3\nX1\nY4 Y6\nX2\nX4 X5 X6\" data-coord=\"top-left:(1334,849); bottom-right:(2128,1657)\" /></figure>",
            "id": 6,
            "page": 1,
            "text": "Y1 Y2 Y3 Y4 X1 X2 X3 X4 Y1 ↑ Y2 Y3 X1 Y4 Y6 X2 X4 X5 X6"
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 1731
                },
                {
                    "x": 2199,
                    "y": 1731
                },
                {
                    "x": 2199,
                    "y": 1905
                },
                {
                    "x": 1271,
                    "y": 1905
                }
            ],
            "category": "caption",
            "html": "<caption id='7' style='font-size:18px'>Figure 1: Top: A chain-structured LSTM net-<br>work. Bottom: A tree-structured LSTM network<br>with arbitrary branching factor.</caption>",
            "id": 7,
            "page": 1,
            "text": "Figure 1: Top: A chain-structured LSTM network. Bottom: A tree-structured LSTM network with arbitrary branching factor."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 2091
                },
                {
                    "x": 650,
                    "y": 2091
                },
                {
                    "x": 650,
                    "y": 2149
                },
                {
                    "x": 293,
                    "y": 2149
                }
            ],
            "category": "paragraph",
            "html": "<p id='8' style='font-size:18px'>1 Introduction</p>",
            "id": 8,
            "page": 1,
            "text": "1 Introduction"
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 2174
                },
                {
                    "x": 1219,
                    "y": 2174
                },
                {
                    "x": 1219,
                    "y": 3193
                },
                {
                    "x": 290,
                    "y": 3193
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='9' style='font-size:16px'>Most models for distributed representations of<br>phrases and sentences-thatis, models where real-<br>valued vectors are used to represent meaning-fall<br>into one of three classes: bag-of-words models,<br>sequence models, and tree-structured models. In<br>bag-of-words models, phrase and sentence repre-<br>sentations are independent of word order; for ex-<br>ample, they can be generated by averaging con-<br>stituent word representations (Landauer and Du-<br>mais, 1997; Foltz et al., 1998). In contrast, se-<br>quence models construct sentence representations<br>as an order-sensitive function of the sequence of<br>tokens (Elman, 1990; Mikolov, 2012). Lastly,<br>tree-structured models compose each phrase and<br>sentence representation from its constituent sub-<br>phrases according to a given syntactic structure<br>over the sentence (Goller and Kuchler, 1996;<br>Socher et al., 2011).</p>",
            "id": 9,
            "page": 1,
            "text": "Most models for distributed representations of phrases and sentences-thatis, models where realvalued vectors are used to represent meaning-fall into one of three classes: bag-of-words models, sequence models, and tree-structured models. In bag-of-words models, phrase and sentence representations are independent of word order; for example, they can be generated by averaging constituent word representations (Landauer and Dumais, 1997; Foltz , 1998). In contrast, sequence models construct sentence representations as an order-sensitive function of the sequence of tokens (Elman, 1990; Mikolov, 2012). Lastly, tree-structured models compose each phrase and sentence representation from its constituent subphrases according to a given syntactic structure over the sentence (Goller and Kuchler, 1996; Socher , 2011)."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2001
                },
                {
                    "x": 2198,
                    "y": 2001
                },
                {
                    "x": 2198,
                    "y": 3071
                },
                {
                    "x": 1272,
                    "y": 3071
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='10' style='font-size:16px'>Order-insensitive models are insufficient to<br>fully capture the semantics of natural language<br>due to their inability to account for differences in<br>meaning as a result of differences in word order<br>or syntactic structure (e.g., \"cats climb trees\" vs.<br>\"trees climb cats\"). We therefore turn to order-<br>sensitive sequential or tree-structured models. In<br>particular, tree-structured models are a linguisti-<br>cally attractive option due to their relation to syn-<br>tactic interpretations of sentence structure. A nat-<br>ural question, then, is the following: to what ex-<br>tent (if at all) can we do better with tree-structured<br>models as opposed to sequential models for sen-<br>tence representation? In this paper, we work to-<br>wards addressing this question by directly com-<br>paring a type of sequential model that has recently<br>been used to achieve state-of-the-art results in sev-<br>eral NLP tasks against its tree-structured general-<br>ization.</p>",
            "id": 10,
            "page": 1,
            "text": "Order-insensitive models are insufficient to fully capture the semantics of natural language due to their inability to account for differences in meaning as a result of differences in word order or syntactic structure (e.g., \"cats climb trees\" vs. \"trees climb cats\"). We therefore turn to ordersensitive sequential or tree-structured models. In particular, tree-structured models are a linguistically attractive option due to their relation to syntactic interpretations of sentence structure. A natural question, then, is the following: to what extent (if at all) can we do better with tree-structured models as opposed to sequential models for sentence representation? In this paper, we work towards addressing this question by directly comparing a type of sequential model that has recently been used to achieve state-of-the-art results in several NLP tasks against its tree-structured generalization."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 3083
                },
                {
                    "x": 2196,
                    "y": 3083
                },
                {
                    "x": 2196,
                    "y": 3194
                },
                {
                    "x": 1271,
                    "y": 3194
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='11' style='font-size:18px'>Due to their capability for processing arbitrary-<br>length sequences, recurrent neural networks</p>",
            "id": 11,
            "page": 1,
            "text": "Due to their capability for processing arbitrarylength sequences, recurrent neural networks"
        },
        {
            "bounding_box": [
                {
                    "x": 58,
                    "y": 1071
                },
                {
                    "x": 150,
                    "y": 1071
                },
                {
                    "x": 150,
                    "y": 2533
                },
                {
                    "x": 58,
                    "y": 2533
                }
            ],
            "category": "footer",
            "html": "<br><footer id='12' style='font-size:14px'>2015<br>May<br>30<br>[cs.CL]<br>arXiv:1503.00075v3</footer>",
            "id": 12,
            "page": 1,
            "text": "2015 May 30 [cs.CL] arXiv:1503.00075v3"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 265
                },
                {
                    "x": 1216,
                    "y": 265
                },
                {
                    "x": 1216,
                    "y": 1053
                },
                {
                    "x": 291,
                    "y": 1053
                }
            ],
            "category": "paragraph",
            "html": "<p id='13' style='font-size:18px'>(RNNs) are a natural choice for sequence model-<br>ing tasks. Recently, RNNs with Long Short-Term<br>Memory (LSTM) units (Hochreiter and Schmid-<br>huber, 1997) have re-emerged as a popular archi-<br>tecture due to their representational power and ef-<br>fectiveness at capturing long-term dependencies.<br>LSTM networks, which we review in Sec. 2, have<br>been successfully applied to a variety of sequence<br>modeling and prediction tasks, notably machine<br>translation (Bahdanau et al., 2014; Sutskever et al.,<br>2014), speech recognition (Graves et al., 2013),<br>image caption generation (Vinyals et al., 2014),<br>and program execution (Zaremba and Sutskever,<br>2014).</p>",
            "id": 13,
            "page": 2,
            "text": "(RNNs) are a natural choice for sequence modeling tasks. Recently, RNNs with Long Short-Term Memory (LSTM) units (Hochreiter and Schmidhuber, 1997) have re-emerged as a popular architecture due to their representational power and effectiveness at capturing long-term dependencies. LSTM networks, which we review in Sec. 2, have been successfully applied to a variety of sequence modeling and prediction tasks, notably machine translation (Bahdanau , 2014; Sutskever , 2014), speech recognition (Graves , 2013), image caption generation (Vinyals , 2014), and program execution (Zaremba and Sutskever, 2014)."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 1061
                },
                {
                    "x": 1216,
                    "y": 1061
                },
                {
                    "x": 1216,
                    "y": 1790
                },
                {
                    "x": 291,
                    "y": 1790
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='14' style='font-size:16px'>In this we introduce a generalization of<br>paper,<br>the standard LSTM architecture to tree-structured<br>network topologies and show its superiority for<br>representing sentence meaning over a sequential<br>LSTM. While the standard LSTM composes its<br>hidden state from the input at the current time<br>step and the hidden state of the LSTM unit in the<br>previous time step, the tree-structured LSTM, or<br>Tree-LSTM, composes its state from an input vec-<br>tor and the hidden states of arbitrarily many child<br>units. The standard LSTM can then be considered<br>a special case of the Tree-LSTM where each inter-<br>nal node has exactly one child.</p>",
            "id": 14,
            "page": 2,
            "text": "In this we introduce a generalization of paper, the standard LSTM architecture to tree-structured network topologies and show its superiority for representing sentence meaning over a sequential LSTM. While the standard LSTM composes its hidden state from the input at the current time step and the hidden state of the LSTM unit in the previous time step, the tree-structured LSTM, or Tree-LSTM, composes its state from an input vector and the hidden states of arbitrarily many child units. The standard LSTM can then be considered a special case of the Tree-LSTM where each internal node has exactly one child."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 1797
                },
                {
                    "x": 1214,
                    "y": 1797
                },
                {
                    "x": 1214,
                    "y": 2414
                },
                {
                    "x": 291,
                    "y": 2414
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='15' style='font-size:16px'>In our evaluations, we demonstrate the empiri-<br>cal strength of Tree-LSTMs as models for repre-<br>senting sentences. We evaluate the Tree-LSTM<br>architecture on two tasks: semantic relatedness<br>prediction on sentence pairs and sentiment clas-<br>sification of sentences drawn from movie reviews.<br>Our experiments show that Tree-LSTMs outper-<br>form existing systems and sequential LSTM base-<br>lines on both tasks. Implementations of our mod-<br>els and experiments are available at https : //<br>github · com/ stanfordnlp /treelstm.</p>",
            "id": 15,
            "page": 2,
            "text": "In our evaluations, we demonstrate the empirical strength of Tree-LSTMs as models for representing sentences. We evaluate the Tree-LSTM architecture on two tasks: semantic relatedness prediction on sentence pairs and sentiment classification of sentences drawn from movie reviews. Our experiments show that Tree-LSTMs outperform existing systems and sequential LSTM baselines on both tasks. Implementations of our models and experiments are available at https : // github · com/ stanfordnlp /treelstm."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 2462
                },
                {
                    "x": 1165,
                    "y": 2462
                },
                {
                    "x": 1165,
                    "y": 2519
                },
                {
                    "x": 292,
                    "y": 2519
                }
            ],
            "category": "paragraph",
            "html": "<p id='16' style='font-size:22px'>2 Long Short- Term Memory Networks</p>",
            "id": 16,
            "page": 2,
            "text": "2 Long Short- Term Memory Networks"
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 2555
                },
                {
                    "x": 591,
                    "y": 2555
                },
                {
                    "x": 591,
                    "y": 2606
                },
                {
                    "x": 292,
                    "y": 2606
                }
            ],
            "category": "paragraph",
            "html": "<p id='17' style='font-size:16px'>2.1 Overview</p>",
            "id": 17,
            "page": 2,
            "text": "2.1 Overview"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 2632
                },
                {
                    "x": 1216,
                    "y": 2632
                },
                {
                    "x": 1216,
                    "y": 3196
                },
                {
                    "x": 291,
                    "y": 3196
                }
            ],
            "category": "paragraph",
            "html": "<p id='18' style='font-size:14px'>Recurrent neural networks (RNNs) are able to pro-<br>cess input sequences of arbitrary length via the re-<br>cursive application of a transition function on a<br>hidden state vector ht. At each time step t, the<br>hidden state ht is a function of the input vector Xt<br>that the network receives at time t and its previous<br>hidden state ht-1. For example, the input vector Xt<br>could be a vector representation of the t-th word in<br>body of text (Elman, 1990; Mikolov, 2012). The<br>hidden state ht E Rd can be interpreted as a d-</p>",
            "id": 18,
            "page": 2,
            "text": "Recurrent neural networks (RNNs) are able to process input sequences of arbitrary length via the recursive application of a transition function on a hidden state vector ht. At each time step t, the hidden state ht is a function of the input vector Xt that the network receives at time t and its previous hidden state ht-1. For example, the input vector Xt could be a vector representation of the t-th word in body of text (Elman, 1990; Mikolov, 2012). The hidden state ht E Rd can be interpreted as a d-"
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 269
                },
                {
                    "x": 2193,
                    "y": 269
                },
                {
                    "x": 2193,
                    "y": 375
                },
                {
                    "x": 1273,
                    "y": 375
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='19' style='font-size:14px'>dimensional distributed representation of the se-<br>quence of tokens observed up to time t.</p>",
            "id": 19,
            "page": 2,
            "text": "dimensional distributed representation of the sequence of tokens observed up to time t."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 384
                },
                {
                    "x": 2196,
                    "y": 384
                },
                {
                    "x": 2196,
                    "y": 549
                },
                {
                    "x": 1272,
                    "y": 549
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='20' style='font-size:18px'>Commonly, the RNN transition function is an<br>affine transformation followed by a pointwise non-<br>linearity such as the hyperbolic tangent function:</p>",
            "id": 20,
            "page": 2,
            "text": "Commonly, the RNN transition function is an affine transformation followed by a pointwise nonlinearity such as the hyperbolic tangent function:"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 684
                },
                {
                    "x": 2196,
                    "y": 684
                },
                {
                    "x": 2196,
                    "y": 1131
                },
                {
                    "x": 1272,
                    "y": 1131
                }
            ],
            "category": "paragraph",
            "html": "<p id='21' style='font-size:18px'>Unfortunately, a problem with RNNs with transi-<br>tion functions of this form is that during training,<br>components of the gradient vector can grow or de-<br>cay exponentially over long sequences (Hochre-<br>iter, 1998; Bengio et al., 1994). This problem with<br>exploding or vanishing gradients makes it difficult<br>for the RNN model to learn long-distance correla-<br>tions in a sequence.</p>",
            "id": 21,
            "page": 2,
            "text": "Unfortunately, a problem with RNNs with transition functions of this form is that during training, components of the gradient vector can grow or decay exponentially over long sequences (Hochreiter, 1998; Bengio , 1994). This problem with exploding or vanishing gradients makes it difficult for the RNN model to learn long-distance correlations in a sequence."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 1135
                },
                {
                    "x": 2197,
                    "y": 1135
                },
                {
                    "x": 2197,
                    "y": 1526
                },
                {
                    "x": 1271,
                    "y": 1526
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='22' style='font-size:18px'>The LSTM architecture (Hochreiter and<br>Schmidhuber, 1997) addresses this problem of<br>learning long-term dependencies by introducing a<br>memory cell that is able to preserve state over long<br>periods of time. While numerous LSTM variants<br>have been described, here we describe the version<br>used by Zaremba and Sutskever (2014).</p>",
            "id": 22,
            "page": 2,
            "text": "The LSTM architecture (Hochreiter and Schmidhuber, 1997) addresses this problem of learning long-term dependencies by introducing a memory cell that is able to preserve state over long periods of time. While numerous LSTM variants have been described, here we describe the version used by Zaremba and Sutskever (2014)."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1531
                },
                {
                    "x": 2198,
                    "y": 1531
                },
                {
                    "x": 2198,
                    "y": 1864
                },
                {
                    "x": 1272,
                    "y": 1864
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='23' style='font-size:14px'>We define the LSTM unit at each time step t to<br>be a collection of vectors in Rd: an input gate it, a<br>forget gate ft, an output gate Ot, a memory cell Ct<br>and a hidden state ht. The entries of the gating<br>vectors it, ft and Ot are in [0, 1]. We refer to d as<br>the memory dimension of the LSTM.</p>",
            "id": 23,
            "page": 2,
            "text": "We define the LSTM unit at each time step t to be a collection of vectors in Rd: an input gate it, a forget gate ft, an output gate Ot, a memory cell Ct and a hidden state ht. The entries of the gating vectors it, ft and Ot are in . We refer to d as the memory dimension of the LSTM."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 1871
                },
                {
                    "x": 2194,
                    "y": 1871
                },
                {
                    "x": 2194,
                    "y": 1978
                },
                {
                    "x": 1271,
                    "y": 1978
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='24' style='font-size:18px'>The LSTM transition equations are the follow-<br>ing:</p>",
            "id": 24,
            "page": 2,
            "text": "The LSTM transition equations are the following:"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2575
                },
                {
                    "x": 2197,
                    "y": 2575
                },
                {
                    "x": 2197,
                    "y": 3196
                },
                {
                    "x": 1272,
                    "y": 3196
                }
            ],
            "category": "paragraph",
            "html": "<p id='25' style='font-size:16px'>where Xt is the input at the current time step, 0 de-<br>notes the logistic sigmoid function and ⊙ denotes<br>elementwise multiplication. Intuitively, the for-<br>get gate controls the extent to which the previous<br>memory cell is forgotten, the input gate controls<br>how much each unit is updated, and the output gate<br>controls the exposure of the internal memory state.<br>The hidden state vector in an LSTM unit is there-<br>fore a gated, partial view of the state of the unit's<br>internal memory cell. Since the value of the gating<br>variables vary for each vector element, the model</p>",
            "id": 25,
            "page": 2,
            "text": "where Xt is the input at the current time step, 0 denotes the logistic sigmoid function and ⊙ denotes elementwise multiplication. Intuitively, the forget gate controls the extent to which the previous memory cell is forgotten, the input gate controls how much each unit is updated, and the output gate controls the exposure of the internal memory state. The hidden state vector in an LSTM unit is therefore a gated, partial view of the state of the unit's internal memory cell. Since the value of the gating variables vary for each vector element, the model"
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 270
                },
                {
                    "x": 1215,
                    "y": 270
                },
                {
                    "x": 1215,
                    "y": 375
                },
                {
                    "x": 292,
                    "y": 375
                }
            ],
            "category": "paragraph",
            "html": "<p id='26' style='font-size:14px'>can learn to represent information over multiple<br>time scales.</p>",
            "id": 26,
            "page": 3,
            "text": "can learn to represent information over multiple time scales."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 420
                },
                {
                    "x": 572,
                    "y": 420
                },
                {
                    "x": 572,
                    "y": 471
                },
                {
                    "x": 292,
                    "y": 471
                }
            ],
            "category": "paragraph",
            "html": "<p id='27' style='font-size:14px'>2.2 Variants</p>",
            "id": 27,
            "page": 3,
            "text": "2.2 Variants"
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 495
                },
                {
                    "x": 1215,
                    "y": 495
                },
                {
                    "x": 1215,
                    "y": 716
                },
                {
                    "x": 292,
                    "y": 716
                }
            ],
            "category": "paragraph",
            "html": "<p id='28' style='font-size:14px'>Two commonly-used variants of the basic LSTM<br>architecture are the Bidirectional LSTM and the<br>Multilayer LSTM (also known as the stacked or<br>deep LSTM).</p>",
            "id": 28,
            "page": 3,
            "text": "Two commonly-used variants of the basic LSTM architecture are the Bidirectional LSTM and the Multilayer LSTM (also known as the stacked or deep LSTM)."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 752
                },
                {
                    "x": 1216,
                    "y": 752
                },
                {
                    "x": 1216,
                    "y": 1258
                },
                {
                    "x": 292,
                    "y": 1258
                }
            ],
            "category": "paragraph",
            "html": "<p id='29' style='font-size:14px'>Bidirectional LSTM. A Bidirectional LSTM<br>(Graves et al., 2013) consists of two LSTMs that<br>are run in parallel: one on the input sequence and<br>the other on the reverse of the input sequence. At<br>each time step, the hidden state of the Bidirec-<br>tional LSTM is the concatenation of the forward<br>and backward hidden states. This setup allows the<br>hidden state to capture both past and future infor-<br>mation.</p>",
            "id": 29,
            "page": 3,
            "text": "Bidirectional LSTM. A Bidirectional LSTM (Graves , 2013) consists of two LSTMs that are run in parallel: one on the input sequence and the other on the reverse of the input sequence. At each time step, the hidden state of the Bidirectional LSTM is the concatenation of the forward and backward hidden states. This setup allows the hidden state to capture both past and future information."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 1296
                },
                {
                    "x": 1215,
                    "y": 1296
                },
                {
                    "x": 1215,
                    "y": 1688
                },
                {
                    "x": 292,
                    "y": 1688
                }
            ],
            "category": "paragraph",
            "html": "<p id='30' style='font-size:16px'>Multilayer LSTM. In Multilayer LSTM archi-<br>tectures, the hidden state of an LSTM unit in layer<br>lis used as input to the LSTM unit in layer l+1 in<br>the same time step (Graves et al., 2013; Sutskever<br>et al., 2014; Zaremba and Sutskever, 2014). Here,<br>the idea is to let the higher layers capture longer-<br>term dependencies of the input sequence.</p>",
            "id": 30,
            "page": 3,
            "text": "Multilayer LSTM. In Multilayer LSTM architectures, the hidden state of an LSTM unit in layer lis used as input to the LSTM unit in layer l+1 in the same time step (Graves , 2013; Sutskever , 2014; Zaremba and Sutskever, 2014). Here, the idea is to let the higher layers capture longerterm dependencies of the input sequence."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 1706
                },
                {
                    "x": 1211,
                    "y": 1706
                },
                {
                    "x": 1211,
                    "y": 1815
                },
                {
                    "x": 292,
                    "y": 1815
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='31' style='font-size:14px'>These two variants can be combined as a Multi-<br>layer Bidirectional LSTM (Graves et al., 2013).</p>",
            "id": 31,
            "page": 3,
            "text": "These two variants can be combined as a Multilayer Bidirectional LSTM (Graves , 2013)."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 1860
                },
                {
                    "x": 895,
                    "y": 1860
                },
                {
                    "x": 895,
                    "y": 1916
                },
                {
                    "x": 292,
                    "y": 1916
                }
            ],
            "category": "paragraph",
            "html": "<p id='32' style='font-size:18px'>3 Tree-Structured LSTMs</p>",
            "id": 32,
            "page": 3,
            "text": "3 Tree-Structured LSTMs"
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 1953
                },
                {
                    "x": 1216,
                    "y": 1953
                },
                {
                    "x": 1216,
                    "y": 2454
                },
                {
                    "x": 292,
                    "y": 2454
                }
            ],
            "category": "paragraph",
            "html": "<p id='33' style='font-size:14px'>A limitation of the LSTM architectures described<br>in the previous section is that they only allow for<br>strictly sequential information propagation. Here,<br>we propose two natural extensions to the basic<br>LSTM architecture: the Child-Sum Tree-LSTM<br>and the N-ary Tree-LSTM. Both variants allow for<br>richer network topologies where each LSTM unit<br>is able to incorporate information from multiple<br>child units.</p>",
            "id": 33,
            "page": 3,
            "text": "A limitation of the LSTM architectures described in the previous section is that they only allow for strictly sequential information propagation. Here, we propose two natural extensions to the basic LSTM architecture: the Child-Sum Tree-LSTM and the N-ary Tree-LSTM. Both variants allow for richer network topologies where each LSTM unit is able to incorporate information from multiple child units."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 2464
                },
                {
                    "x": 1216,
                    "y": 2464
                },
                {
                    "x": 1216,
                    "y": 3198
                },
                {
                    "x": 290,
                    "y": 3198
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='34' style='font-size:16px'>As in standard LSTM units, each Tree-LSTM<br>unit (indexed by j) contains input and output<br>gates ij and a memory cell and hidden<br>Cj<br>Oj,<br>state hj. The difference between the standard<br>LSTM unit and Tree-LSTM units is that gating<br>vectors and memory cell updates are dependent<br>on the states of possibly many child units. Ad-<br>ditionally, instead of a single forget gate, the Tree-<br>LSTM unit contains one forget gate fjk for each<br>child k. This allows the Tree-LSTM unit to se-<br>lectively incorporate information from each child.<br>For example, a Tree-LSTM model can learn to em-<br>phasize semantic heads in a semantic relatedness</p>",
            "id": 34,
            "page": 3,
            "text": "As in standard LSTM units, each Tree-LSTM unit (indexed by j) contains input and output gates ij and a memory cell and hidden Cj Oj, state hj. The difference between the standard LSTM unit and Tree-LSTM units is that gating vectors and memory cell updates are dependent on the states of possibly many child units. Additionally, instead of a single forget gate, the TreeLSTM unit contains one forget gate fjk for each child k. This allows the Tree-LSTM unit to selectively incorporate information from each child. For example, a Tree-LSTM model can learn to emphasize semantic heads in a semantic relatedness"
        },
        {
            "bounding_box": [
                {
                    "x": 1354,
                    "y": 245
                },
                {
                    "x": 2111,
                    "y": 245
                },
                {
                    "x": 2111,
                    "y": 725
                },
                {
                    "x": 1354,
                    "y": 725
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='35' style='font-size:14px' alt=\"h2 C2\nf2\n21 01\nX1 u1 C1 h1\nf3\nh3 C3\" data-coord=\"top-left:(1354,245); bottom-right:(2111,725)\" /></figure>",
            "id": 35,
            "page": 3,
            "text": "h2 C2 f2 21 01 X1 u1 C1 h1 f3 h3 C3"
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 791
                },
                {
                    "x": 2196,
                    "y": 791
                },
                {
                    "x": 2196,
                    "y": 1074
                },
                {
                    "x": 1270,
                    "y": 1074
                }
            ],
            "category": "caption",
            "html": "<caption id='36' style='font-size:16px'>Figure 2: Composing the memory cell C1 and hid-<br>den state h1 of a Tree-LSTM unit with two chil-<br>dren (subscripts 2 and 3). Labeled edges cor-<br>respond to gating by the indicated gating vector,<br>with dependencies omitted for compactness.</caption>",
            "id": 36,
            "page": 3,
            "text": "Figure 2: Composing the memory cell C1 and hidden state h1 of a Tree-LSTM unit with two children (subscripts 2 and 3). Labeled edges correspond to gating by the indicated gating vector, with dependencies omitted for compactness."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1157
                },
                {
                    "x": 2196,
                    "y": 1157
                },
                {
                    "x": 2196,
                    "y": 1318
                },
                {
                    "x": 1272,
                    "y": 1318
                }
            ],
            "category": "paragraph",
            "html": "<p id='37' style='font-size:14px'>task, or it can learn to preserve the representation<br>of sentiment-rich children for sentiment classifica-<br>tion.</p>",
            "id": 37,
            "page": 3,
            "text": "task, or it can learn to preserve the representation of sentiment-rich children for sentiment classification."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1324
                },
                {
                    "x": 2197,
                    "y": 1324
                },
                {
                    "x": 2197,
                    "y": 1888
                },
                {
                    "x": 1272,
                    "y": 1888
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='38' style='font-size:14px'>As with the standard LSTM, each Tree-LSTM<br>unit takes an input vector xj. In our applications,<br>each xj is a vector representation of a word in a<br>sentence. The input word at each node depends<br>on the tree structure used for the network. For in-<br>stance, in a Tree-LSTM over a dependency tree,<br>each node in the tree takes the vector correspond-<br>ing to the head word as input, whereas in a Tree-<br>LSTM over a constituency tree, the leaf nodes take<br>the corresponding word vectors as input.</p>",
            "id": 38,
            "page": 3,
            "text": "As with the standard LSTM, each Tree-LSTM unit takes an input vector xj. In our applications, each xj is a vector representation of a word in a sentence. The input word at each node depends on the tree structure used for the network. For instance, in a Tree-LSTM over a dependency tree, each node in the tree takes the vector corresponding to the head word as input, whereas in a TreeLSTM over a constituency tree, the leaf nodes take the corresponding word vectors as input."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1924
                },
                {
                    "x": 1860,
                    "y": 1924
                },
                {
                    "x": 1860,
                    "y": 1976
                },
                {
                    "x": 1273,
                    "y": 1976
                }
            ],
            "category": "paragraph",
            "html": "<p id='39' style='font-size:16px'>3.1 Child-Sum Tree-LSTMs</p>",
            "id": 39,
            "page": 3,
            "text": "3.1 Child-Sum Tree-LSTMs"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1998
                },
                {
                    "x": 2195,
                    "y": 1998
                },
                {
                    "x": 2195,
                    "y": 2163
                },
                {
                    "x": 1272,
                    "y": 2163
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='40' style='font-size:14px'>Given a tree, let C(j) denote the set of children<br>of node j. The Child-Sum Tree-LSTM transition<br>equations are the following:</p>",
            "id": 40,
            "page": 3,
            "text": "Given a tree, let C(j) denote the set of children of node j. The Child-Sum Tree-LSTM transition equations are the following:"
        },
        {
            "bounding_box": [
                {
                    "x": 1637,
                    "y": 2807
                },
                {
                    "x": 1765,
                    "y": 2807
                },
                {
                    "x": 1765,
                    "y": 2857
                },
                {
                    "x": 1637,
                    "y": 2857
                }
            ],
            "category": "paragraph",
            "html": "<p id='41' style='font-size:20px'>kEC(j)</p>",
            "id": 41,
            "page": 3,
            "text": "kEC(j)"
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 2970
                },
                {
                    "x": 1756,
                    "y": 2970
                },
                {
                    "x": 1756,
                    "y": 3026
                },
                {
                    "x": 1274,
                    "y": 3026
                }
            ],
            "category": "paragraph",
            "html": "<p id='42' style='font-size:16px'>where in Eq. 4, k E C(j).</p>",
            "id": 42,
            "page": 3,
            "text": "where in Eq. 4, k E C(j)."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 3030
                },
                {
                    "x": 2195,
                    "y": 3030
                },
                {
                    "x": 2195,
                    "y": 3194
                },
                {
                    "x": 1271,
                    "y": 3194
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='43' style='font-size:14px'>Intuitively, we can interpret each parameter ma-<br>trix in these equations as encoding correlations be-<br>tween the component vectors of the Tree-LSTM</p>",
            "id": 43,
            "page": 3,
            "text": "Intuitively, we can interpret each parameter matrix in these equations as encoding correlations between the component vectors of the Tree-LSTM"
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 268
                },
                {
                    "x": 1216,
                    "y": 268
                },
                {
                    "x": 1216,
                    "y": 773
                },
                {
                    "x": 292,
                    "y": 773
                }
            ],
            "category": "paragraph",
            "html": "<p id='44' style='font-size:16px'>unit, the input xj, and the hidden states hk of the<br>unit's children. For example, in a dependency tree<br>application, the model can learn parameters W(i)<br>such that the components of the input gate ij have<br>values close to 1 (i.e., \"open\") when a semanti-<br>cally important content word (such as a verb) is<br>given as input, and values close to 0 (i.e., \"closed\")<br>when the input is a relatively unimportant word<br>(such as a determiner).</p>",
            "id": 44,
            "page": 4,
            "text": "unit, the input xj, and the hidden states hk of the unit's children. For example, in a dependency tree application, the model can learn parameters W(i) such that the components of the input gate ij have values close to 1 (i.e., \"open\") when a semantically important content word (such as a verb) is given as input, and values close to 0 (i.e., \"closed\") when the input is a relatively unimportant word (such as a determiner)."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 804
                },
                {
                    "x": 1216,
                    "y": 804
                },
                {
                    "x": 1216,
                    "y": 1312
                },
                {
                    "x": 292,
                    "y": 1312
                }
            ],
            "category": "paragraph",
            "html": "<p id='45' style='font-size:14px'>Dependency Tree-LSTMs. Since the Child-<br>Sum Tree-LSTM unit conditions its components<br>on the sum of child hidden states hk, it is well-<br>suited for trees with high branching factor or<br>whose children are unordered. For example, itis a<br>good choice for dependency trees, where the num-<br>ber of dependents of a head can be highly variable.<br>We refer to a Child-Sum Tree-LSTM applied to a<br>dependency tree as a Dependency Tree-LSTM.</p>",
            "id": 45,
            "page": 4,
            "text": "Dependency Tree-LSTMs. Since the ChildSum Tree-LSTM unit conditions its components on the sum of child hidden states hk, it is wellsuited for trees with high branching factor or whose children are unordered. For example, itis a good choice for dependency trees, where the number of dependents of a head can be highly variable. We refer to a Child-Sum Tree-LSTM applied to a dependency tree as a Dependency Tree-LSTM."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 1346
                },
                {
                    "x": 789,
                    "y": 1346
                },
                {
                    "x": 789,
                    "y": 1398
                },
                {
                    "x": 292,
                    "y": 1398
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='46' style='font-size:18px'>3.2 N-ary Tree-LSTMs</p>",
            "id": 46,
            "page": 4,
            "text": "3.2 N-ary Tree-LSTMs"
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 1419
                },
                {
                    "x": 1215,
                    "y": 1419
                },
                {
                    "x": 1215,
                    "y": 1813
                },
                {
                    "x": 292,
                    "y": 1813
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='47' style='font-size:14px'>The N-ary Tree-LSTM can be used on tree struc-<br>tures where the branching factor is at most N and<br>where children are ordered, i.e., they can be in-<br>dexed from 1 to N. For any node j, write the hid-<br>den state and memory cell of its kth child as hjk<br>and Cjk respectively. The N-ary Tree-LSTM tran-<br>sition equations are the following:</p>",
            "id": 47,
            "page": 4,
            "text": "The N-ary Tree-LSTM can be used on tree structures where the branching factor is at most N and where children are ordered, i.e., they can be indexed from 1 to N. For any node j, write the hidden state and memory cell of its kth child as hjk and Cjk respectively. The N-ary Tree-LSTM transition equations are the following:"
        },
        {
            "bounding_box": [
                {
                    "x": 651,
                    "y": 2606
                },
                {
                    "x": 689,
                    "y": 2606
                },
                {
                    "x": 689,
                    "y": 2640
                },
                {
                    "x": 651,
                    "y": 2640
                }
            ],
            "category": "paragraph",
            "html": "<p id='48' style='font-size:14px'>N</p>",
            "id": 48,
            "page": 4,
            "text": "N"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 2857
                },
                {
                    "x": 1214,
                    "y": 2857
                },
                {
                    "x": 1214,
                    "y": 3078
                },
                {
                    "x": 291,
                    "y": 3078
                }
            ],
            "category": "paragraph",
            "html": "<p id='49' style='font-size:14px'>where in Eq. 10, k = 1,2, . · , N. Note that<br>when the tree is simply a chain, both Eqs. 2-8<br>and Eqs. 9-14 reduce to the standard LSTM tran-<br>sitions, Eqs. 1.</p>",
            "id": 49,
            "page": 4,
            "text": "where in Eq. 10, k = 1,2, . · , N. Note that when the tree is simply a chain, both Eqs. 2-8 and Eqs. 9-14 reduce to the standard LSTM transitions, Eqs. 1."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 3085
                },
                {
                    "x": 1215,
                    "y": 3085
                },
                {
                    "x": 1215,
                    "y": 3196
                },
                {
                    "x": 292,
                    "y": 3196
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='50' style='font-size:14px'>The introduction of separate parameter matri-<br>ces for each child k allows the N-ary Tree-LSTM</p>",
            "id": 50,
            "page": 4,
            "text": "The introduction of separate parameter matrices for each child k allows the N-ary Tree-LSTM"
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 268
                },
                {
                    "x": 2198,
                    "y": 268
                },
                {
                    "x": 2198,
                    "y": 896
                },
                {
                    "x": 1271,
                    "y": 896
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='51' style='font-size:14px'>model to learn more fine-grained conditioning on<br>the states of a unit's children than the Child-<br>Sum Tree-LSTM. Consider, for example, a con-<br>stituency tree application where the left child of a<br>node corresponds to a noun phrase, and the right<br>child to a verb phrase. Suppose that in this case<br>it is advantageous to emphasize the verb phrase<br>in the representation. Then the Ukf) parameters<br>can be trained such that the components of fj1 are<br>close to 0 (i.e., \"forget\"), while the components of<br>fj2 are close to 1 (i.e., \"preserve\").</p>",
            "id": 51,
            "page": 4,
            "text": "model to learn more fine-grained conditioning on the states of a unit's children than the ChildSum Tree-LSTM. Consider, for example, a constituency tree application where the left child of a node corresponds to a noun phrase, and the right child to a verb phrase. Suppose that in this case it is advantageous to emphasize the verb phrase in the representation. Then the Ukf) parameters can be trained such that the components of fj1 are close to 0 (i.e., \"forget\"), while the components of fj2 are close to 1 (i.e., \"preserve\")."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 932
                },
                {
                    "x": 2197,
                    "y": 932
                },
                {
                    "x": 2197,
                    "y": 1563
                },
                {
                    "x": 1272,
                    "y": 1563
                }
            ],
            "category": "paragraph",
            "html": "<p id='52' style='font-size:14px'>Forget gate parameterization. In Eq. 10, we<br>define a parameterization of the kth child's for-<br>get gate fjk that contains \"off-diagonal\" param-<br>k ≠ l. This parameteriza-<br>eter matrices U(f) ,<br>tion allows for more flexible control of informa-<br>tion propagation from child to parent. For exam-<br>ple, this allows the left hidden state in a binary tree<br>to have either an excitatory or inhibitory effect on<br>the forget gate of the right child. However, for<br>large values of N, these additional parameters are<br>impractical and may be tied or fixed to zero.</p>",
            "id": 52,
            "page": 4,
            "text": "Forget gate parameterization. In Eq. 10, we define a parameterization of the kth child's forget gate fjk that contains \"off-diagonal\" paramk ≠ l. This parameterizaeter matrices U(f) , tion allows for more flexible control of information propagation from child to parent. For example, this allows the left hidden state in a binary tree to have either an excitatory or inhibitory effect on the forget gate of the right child. However, for large values of N, these additional parameters are impractical and may be tied or fixed to zero."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 1601
                },
                {
                    "x": 2196,
                    "y": 1601
                },
                {
                    "x": 2196,
                    "y": 1999
                },
                {
                    "x": 1271,
                    "y": 1999
                }
            ],
            "category": "paragraph",
            "html": "<p id='53' style='font-size:16px'>Constituency Tree-LSTMs. We can naturally<br>apply Binary Tree-LSTM units to binarized con-<br>stituency trees since left and right child nodes are<br>distinguished. We refer to this application of Bi-<br>nary Tree-LSTMs as a Constituency Tree-LSTM.<br>Note that in Constituency Tree-LSTMs, a node j<br>receives an input vector xj only ifit is a leaf node.</p>",
            "id": 53,
            "page": 4,
            "text": "Constituency Tree-LSTMs. We can naturally apply Binary Tree-LSTM units to binarized constituency trees since left and right child nodes are distinguished. We refer to this application of Binary Tree-LSTMs as a Constituency Tree-LSTM. Note that in Constituency Tree-LSTMs, a node j receives an input vector xj only ifit is a leaf node."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2027
                },
                {
                    "x": 2196,
                    "y": 2027
                },
                {
                    "x": 2196,
                    "y": 2592
                },
                {
                    "x": 1272,
                    "y": 2592
                }
            ],
            "category": "paragraph",
            "html": "<p id='54' style='font-size:16px'>In the remainder of this paper, we focus on<br>the special cases of Dependency Tree-LSTMs and<br>Constituency Tree-LSTMs. These architectures<br>are in fact closely related; since we consider only<br>binarized constituency trees, the parameterizations<br>of the two models are very similar. The key dif-<br>ference is in the application of the compositional<br>parameters: dependent vs. head for Dependency<br>Tree-LSTMs, and left child vs. right child for Con-<br>stituency Tree-LSTMs.</p>",
            "id": 54,
            "page": 4,
            "text": "In the remainder of this paper, we focus on the special cases of Dependency Tree-LSTMs and Constituency Tree-LSTMs. These architectures are in fact closely related; since we consider only binarized constituency trees, the parameterizations of the two models are very similar. The key difference is in the application of the compositional parameters: dependent vs. head for Dependency Tree-LSTMs, and left child vs. right child for Constituency Tree-LSTMs."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2641
                },
                {
                    "x": 1514,
                    "y": 2641
                },
                {
                    "x": 1514,
                    "y": 2694
                },
                {
                    "x": 1273,
                    "y": 2694
                }
            ],
            "category": "paragraph",
            "html": "<p id='55' style='font-size:16px'>4 Models</p>",
            "id": 55,
            "page": 4,
            "text": "4 Models"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2737
                },
                {
                    "x": 2194,
                    "y": 2737
                },
                {
                    "x": 2194,
                    "y": 2900
                },
                {
                    "x": 1272,
                    "y": 2900
                }
            ],
            "category": "paragraph",
            "html": "<p id='56' style='font-size:14px'>We now describe two specific models that apply<br>the Tree-LSTM architectures described in the pre-<br>vious section.</p>",
            "id": 56,
            "page": 4,
            "text": "We now describe two specific models that apply the Tree-LSTM architectures described in the previous section."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2950
                },
                {
                    "x": 1887,
                    "y": 2950
                },
                {
                    "x": 1887,
                    "y": 2999
                },
                {
                    "x": 1273,
                    "y": 2999
                }
            ],
            "category": "paragraph",
            "html": "<p id='57' style='font-size:16px'>4.1 Tree-LSTM Classification</p>",
            "id": 57,
            "page": 4,
            "text": "4.1 Tree-LSTM Classification"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 3027
                },
                {
                    "x": 2197,
                    "y": 3027
                },
                {
                    "x": 2197,
                    "y": 3194
                },
                {
                    "x": 1272,
                    "y": 3194
                }
            ],
            "category": "paragraph",
            "html": "<p id='58' style='font-size:14px'>In this setting, we wish to predict labels y from a<br>discrete set of classes y for some subset of nodes<br>in a tree. For example, the label for a node in a</p>",
            "id": 58,
            "page": 4,
            "text": "In this setting, we wish to predict labels y from a discrete set of classes y for some subset of nodes in a tree. For example, the label for a node in a"
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 271
                },
                {
                    "x": 1214,
                    "y": 271
                },
                {
                    "x": 1214,
                    "y": 375
                },
                {
                    "x": 292,
                    "y": 375
                }
            ],
            "category": "paragraph",
            "html": "<p id='59' style='font-size:16px'>parse tree could correspond to some property of<br>the phrase spanned by that node.</p>",
            "id": 59,
            "page": 5,
            "text": "parse tree could correspond to some property of the phrase spanned by that node."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 384
                },
                {
                    "x": 1215,
                    "y": 384
                },
                {
                    "x": 1215,
                    "y": 605
                },
                {
                    "x": 292,
                    "y": 605
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='60' style='font-size:16px'>At each node j, we use a softmax classifier to<br>predict the label Y j given the inputs {x}j observed<br>at nodes in the subtree rooted at j. The classifier<br>takes the hidden state hj at the node as input:</p>",
            "id": 60,
            "page": 5,
            "text": "At each node j, we use a softmax classifier to predict the label Y j given the inputs {x}j observed at nodes in the subtree rooted at j. The classifier takes the hidden state hj at the node as input:"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 866
                },
                {
                    "x": 1214,
                    "y": 866
                },
                {
                    "x": 1214,
                    "y": 973
                },
                {
                    "x": 291,
                    "y": 973
                }
            ],
            "category": "paragraph",
            "html": "<p id='61' style='font-size:16px'>The cost function is the negative log-likelihood<br>of the true class labels y(k) at each labeled node:</p>",
            "id": 61,
            "page": 5,
            "text": "The cost function is the negative log-likelihood of the true class labels y(k) at each labeled node:"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 1194
                },
                {
                    "x": 1214,
                    "y": 1194
                },
                {
                    "x": 1214,
                    "y": 1412
                },
                {
                    "x": 291,
                    "y": 1412
                }
            ],
            "category": "paragraph",
            "html": "<p id='62' style='font-size:16px'>where m is the number of labeled nodes in the<br>training set, the superscript k indicates the kth la-<br>beled node, and 入 is an L2 regularization hyperpa-<br>rameter.</p>",
            "id": 62,
            "page": 5,
            "text": "where m is the number of labeled nodes in the training set, the superscript k indicates the kth labeled node, and 入 is an L2 regularization hyperparameter."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 1452
                },
                {
                    "x": 1171,
                    "y": 1452
                },
                {
                    "x": 1171,
                    "y": 1506
                },
                {
                    "x": 293,
                    "y": 1506
                }
            ],
            "category": "paragraph",
            "html": "<p id='63' style='font-size:18px'>4.2 Semantic Relatedness of Sentence Pairs</p>",
            "id": 63,
            "page": 5,
            "text": "4.2 Semantic Relatedness of Sentence Pairs"
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 1529
                },
                {
                    "x": 1214,
                    "y": 1529
                },
                {
                    "x": 1214,
                    "y": 1975
                },
                {
                    "x": 292,
                    "y": 1975
                }
            ],
            "category": "paragraph",
            "html": "<p id='64' style='font-size:16px'>Given a sentence pair, we wish to predict a<br>real-valued similarity score in some range [1, K],<br>where K > 1 is an integer. The sequence<br>{1, 2, · · · , K} is some ordinal scale of similarity,<br>where higher scores indicate greater degrees of<br>similarity, and we allow real-valued scores to ac-<br>count for ground-truth ratings that are an average<br>over the evaluations of several human annotators.</p>",
            "id": 64,
            "page": 5,
            "text": "Given a sentence pair, we wish to predict a real-valued similarity score in some range [1, K], where K > 1 is an integer. The sequence {1, 2, · · · , K} is some ordinal scale of similarity, where higher scores indicate greater degrees of similarity, and we allow real-valued scores to account for ground-truth ratings that are an average over the evaluations of several human annotators."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 1982
                },
                {
                    "x": 1214,
                    "y": 1982
                },
                {
                    "x": 1214,
                    "y": 2373
                },
                {
                    "x": 292,
                    "y": 2373
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='65' style='font-size:18px'>We first produce sentence representations hL<br>and hR for each sentence in the pair using a<br>Tree-LSTM model over each sentence's parse tree.<br>Given these sentence representations, we predict<br>the similarity score y using a neural network that<br>considers both the distance and angle between the<br>pair (hL, hR):</p>",
            "id": 65,
            "page": 5,
            "text": "We first produce sentence representations hL and hR for each sentence in the pair using a Tree-LSTM model over each sentence's parse tree. Given these sentence representations, we predict the similarity score y using a neural network that considers both the distance and angle between the pair (hL, hR):"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 2849
                },
                {
                    "x": 1216,
                    "y": 2849
                },
                {
                    "x": 1216,
                    "y": 3194
                },
                {
                    "x": 291,
                    "y": 3194
                }
            ],
            "category": "paragraph",
            "html": "<p id='66' style='font-size:16px'>where r.T = [1 2 · · · K] and the absolute value<br>function is applied elementwise. The use of both<br>distance measures hx and h+ is empirically mo-<br>tivated: we find that the combination outperforms<br>the use of either measure alone. The multiplicative<br>measure hx can be interpreted as an elementwise</p>",
            "id": 66,
            "page": 5,
            "text": "where r.T = [1 2 · · · K] and the absolute value function is applied elementwise. The use of both distance measures hx and h+ is empirically motivated: we find that the combination outperforms the use of either measure alone. The multiplicative measure hx can be interpreted as an elementwise"
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 272
                },
                {
                    "x": 2193,
                    "y": 272
                },
                {
                    "x": 2193,
                    "y": 374
                },
                {
                    "x": 1273,
                    "y": 374
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='67' style='font-size:18px'>comparison of the signs of the input representa-<br>tions.</p>",
            "id": 67,
            "page": 5,
            "text": "comparison of the signs of the input representations."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 384
                },
                {
                    "x": 2197,
                    "y": 384
                },
                {
                    "x": 2197,
                    "y": 661
                },
                {
                    "x": 1272,
                    "y": 661
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='68' style='font-size:18px'>We want the expected rating under the predicted<br>distribution po given model parameters 0 to be<br>close to the gold rating y E [1, K]: y = r.T po 22 y.<br>We therefore define a sparse target distribution1 p<br>that satisfies y = r.T p:</p>",
            "id": 68,
            "page": 5,
            "text": "We want the expected rating under the predicted distribution po given model parameters 0 to be close to the gold rating y E [1, K]: y = r.T po 22 y. We therefore define a sparse target distribution1 p that satisfies y = r.T p:"
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 953
                },
                {
                    "x": 2195,
                    "y": 953
                },
                {
                    "x": 2195,
                    "y": 1063
                },
                {
                    "x": 1273,
                    "y": 1063
                }
            ],
            "category": "paragraph",
            "html": "<p id='69' style='font-size:16px'>for 1 ≤ i ≤ K. The cost function is the regular-<br>ized KL-divergence between p and po:</p>",
            "id": 69,
            "page": 5,
            "text": "for 1 ≤ i ≤ K. The cost function is the regularized KL-divergence between p and po:"
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 1292
                },
                {
                    "x": 2195,
                    "y": 1292
                },
                {
                    "x": 2195,
                    "y": 1401
                },
                {
                    "x": 1274,
                    "y": 1401
                }
            ],
            "category": "paragraph",
            "html": "<p id='70' style='font-size:16px'>where m is the number of training pairs and the<br>superscript k indicates the kth sentence pair.</p>",
            "id": 70,
            "page": 5,
            "text": "where m is the number of training pairs and the superscript k indicates the kth sentence pair."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 1444
                },
                {
                    "x": 1631,
                    "y": 1444
                },
                {
                    "x": 1631,
                    "y": 1499
                },
                {
                    "x": 1274,
                    "y": 1499
                }
            ],
            "category": "paragraph",
            "html": "<p id='71' style='font-size:20px'>5 Experiments</p>",
            "id": 71,
            "page": 5,
            "text": "5 Experiments"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1535
                },
                {
                    "x": 2196,
                    "y": 1535
                },
                {
                    "x": 2196,
                    "y": 1756
                },
                {
                    "x": 1272,
                    "y": 1756
                }
            ],
            "category": "paragraph",
            "html": "<p id='72' style='font-size:16px'>We evaluate our Tree-LSTM architectures on two<br>tasks: (1) sentiment classification of sentences<br>sampled from movie reviews and (2) predicting<br>the semantic relatedness of sentence pairs.</p>",
            "id": 72,
            "page": 5,
            "text": "We evaluate our Tree-LSTM architectures on two tasks: (1) sentiment classification of sentences sampled from movie reviews and (2) predicting the semantic relatedness of sentence pairs."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 1764
                },
                {
                    "x": 2196,
                    "y": 1764
                },
                {
                    "x": 2196,
                    "y": 2041
                },
                {
                    "x": 1271,
                    "y": 2041
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='73' style='font-size:18px'>In comparing our Tree-LSTMs against sequen-<br>tial LSTMs, we control for the number of LSTM<br>parameters by varying the dimensionality of the<br>hidden states2. Details for each model variant are<br>summarized in Table 1.</p>",
            "id": 73,
            "page": 5,
            "text": "In comparing our Tree-LSTMs against sequential LSTMs, we control for the number of LSTM parameters by varying the dimensionality of the hidden states2. Details for each model variant are summarized in Table 1."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2082
                },
                {
                    "x": 1856,
                    "y": 2082
                },
                {
                    "x": 1856,
                    "y": 2135
                },
                {
                    "x": 1273,
                    "y": 2135
                }
            ],
            "category": "paragraph",
            "html": "<p id='74' style='font-size:18px'>5.1 Sentiment Classification</p>",
            "id": 74,
            "page": 5,
            "text": "5.1 Sentiment Classification"
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2157
                },
                {
                    "x": 2197,
                    "y": 2157
                },
                {
                    "x": 2197,
                    "y": 2776
                },
                {
                    "x": 1273,
                    "y": 2776
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='75' style='font-size:18px'>In this task, we predict the sentiment of sen-<br>tences sampled from movie reviews. We use<br>the Stanford Sentiment Treebank (Socher et al.,<br>2013). There are two subtasks: binary classifica-<br>tion of sentences, and fine-grained classification<br>over five classes: very negative, negative, neu-<br>tral, positive, and very positive. We use the stan-<br>dard train/dev/test splits of 6920/872/1821 for the<br>binary classification subtask and 8544/1101/2210<br>for the fine-grained classification subtask (there<br>are fewer examples for the binary subtask since</p>",
            "id": 75,
            "page": 5,
            "text": "In this task, we predict the sentiment of sentences sampled from movie reviews. We use the Stanford Sentiment Treebank (Socher , 2013). There are two subtasks: binary classification of sentences, and fine-grained classification over five classes: very negative, negative, neutral, positive, and very positive. We use the standard train/dev/test splits of 6920/872/1821 for the binary classification subtask and 8544/1101/2210 for the fine-grained classification subtask (there are fewer examples for the binary subtask since"
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 2811
                },
                {
                    "x": 2196,
                    "y": 2811
                },
                {
                    "x": 2196,
                    "y": 2933
                },
                {
                    "x": 1274,
                    "y": 2933
                }
            ],
            "category": "paragraph",
            "html": "<p id='76' style='font-size:14px'>1In the subsequent experiments, we found that optimizing<br>this objective yielded better performance than a mean squared<br>error objective.</p>",
            "id": 76,
            "page": 5,
            "text": "1In the subsequent experiments, we found that optimizing this objective yielded better performance than a mean squared error objective."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2938
                },
                {
                    "x": 2196,
                    "y": 2938
                },
                {
                    "x": 2196,
                    "y": 3189
                },
                {
                    "x": 1272,
                    "y": 3189
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='77' style='font-size:14px'>2For our Bidirectional LSTMs, the parameters of the for-<br>ward and backward transition functions are shared. In our<br>experiments, this achieved superior performance to Bidirec-<br>tional LSTMs with untied weights and the same number of<br>parameters (and therefore smaller hidden vector dimension-<br>ality).</p>",
            "id": 77,
            "page": 5,
            "text": "2For our Bidirectional LSTMs, the parameters of the forward and backward transition functions are shared. In our experiments, this achieved superior performance to Bidirectional LSTMs with untied weights and the same number of parameters (and therefore smaller hidden vector dimensionality)."
        },
        {
            "bounding_box": [
                {
                    "x": 297,
                    "y": 253
                },
                {
                    "x": 1207,
                    "y": 253
                },
                {
                    "x": 1207,
                    "y": 709
                },
                {
                    "x": 297,
                    "y": 709
                }
            ],
            "category": "table",
            "html": "<table id='78' style='font-size:16px'><tr><td></td><td colspan=\"2\">Relatedness</td><td colspan=\"2\">Sentiment</td></tr><tr><td>LSTM Variant</td><td>d</td><td>101</td><td>d</td><td>101</td></tr><tr><td>Standard</td><td>150</td><td>203,400</td><td>168</td><td>315,840</td></tr><tr><td>Bidirectional</td><td>150</td><td>203,400</td><td>168</td><td>315,840</td></tr><tr><td>2-layer</td><td>108</td><td>203,472</td><td>120</td><td>318,720</td></tr><tr><td>Bidirectional 2-layer</td><td>108</td><td>203,472</td><td>120</td><td>318,720</td></tr><tr><td>Constituency Tree</td><td>142</td><td>205,190</td><td>150</td><td>316,800</td></tr><tr><td>Dependency Tree</td><td>150</td><td>203,400</td><td>168</td><td>315,840</td></tr></table>",
            "id": 78,
            "page": 6,
            "text": "Relatedness Sentiment  LSTM Variant d 101 d 101  Standard 150 203,400 168 315,840  Bidirectional 150 203,400 168 315,840  2-layer 108 203,472 120 318,720  Bidirectional 2-layer 108 203,472 120 318,720  Constituency Tree 142 205,190 150 316,800  Dependency Tree 150 203,400 168"
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 739
                },
                {
                    "x": 1216,
                    "y": 739
                },
                {
                    "x": 1216,
                    "y": 905
                },
                {
                    "x": 292,
                    "y": 905
                }
            ],
            "category": "caption",
            "html": "<caption id='79' style='font-size:18px'>Table 1: Memory dimensions d and composition<br>function parameter counts 101 for each LSTM vari-<br>ant that we evaluate.</caption>",
            "id": 79,
            "page": 6,
            "text": "Table 1: Memory dimensions d and composition function parameter counts 101 for each LSTM variant that we evaluate."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 986
                },
                {
                    "x": 1216,
                    "y": 986
                },
                {
                    "x": 1216,
                    "y": 1204
                },
                {
                    "x": 291,
                    "y": 1204
                }
            ],
            "category": "paragraph",
            "html": "<p id='80' style='font-size:18px'>neutral sentences are excluded). Standard bina-<br>rized constituency parse trees are provided for<br>each sentence in the dataset, and each node in<br>these trees is annotated with a sentiment label.</p>",
            "id": 80,
            "page": 6,
            "text": "neutral sentences are excluded). Standard binarized constituency parse trees are provided for each sentence in the dataset, and each node in these trees is annotated with a sentiment label."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 1211
                },
                {
                    "x": 1216,
                    "y": 1211
                },
                {
                    "x": 1216,
                    "y": 1487
                },
                {
                    "x": 292,
                    "y": 1487
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='81' style='font-size:20px'>For the sequential LSTM baselines, we predict<br>the sentiment of a phrase using the representation<br>given by the final LSTM hidden state. The sequen-<br>tial LSTM models are trained on the spans corre-<br>sponding to labeled nodes in the training set.</p>",
            "id": 81,
            "page": 6,
            "text": "For the sequential LSTM baselines, we predict the sentiment of a phrase using the representation given by the final LSTM hidden state. The sequential LSTM models are trained on the spans corresponding to labeled nodes in the training set."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 1495
                },
                {
                    "x": 1217,
                    "y": 1495
                },
                {
                    "x": 1217,
                    "y": 1999
                },
                {
                    "x": 292,
                    "y": 1999
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='82' style='font-size:20px'>We use the classification model described in<br>Sec. 4.1 with both Dependency Tree-LSTMs<br>(Sec. 3.1) and Constituency Tree-LSTMs<br>(Sec. 3.2). The Constituency Tree-LSTMs are<br>structured according to the provided parse trees.<br>For the Dependency Tree-LSTMs, we produce<br>dependency parses3 of each sentence; each node<br>in a tree is given a sentiment label if its span<br>matches a labeled span in the training set.</p>",
            "id": 82,
            "page": 6,
            "text": "We use the classification model described in Sec. 4.1 with both Dependency Tree-LSTMs (Sec. 3.1) and Constituency Tree-LSTMs (Sec. 3.2). The Constituency Tree-LSTMs are structured according to the provided parse trees. For the Dependency Tree-LSTMs, we produce dependency parses3 of each sentence; each node in a tree is given a sentiment label if its span matches a labeled span in the training set."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 2038
                },
                {
                    "x": 827,
                    "y": 2038
                },
                {
                    "x": 827,
                    "y": 2089
                },
                {
                    "x": 293,
                    "y": 2089
                }
            ],
            "category": "paragraph",
            "html": "<p id='83' style='font-size:18px'>5.2 Semantic Relatedness</p>",
            "id": 83,
            "page": 6,
            "text": "5.2 Semantic Relatedness"
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 2112
                },
                {
                    "x": 1213,
                    "y": 2112
                },
                {
                    "x": 1213,
                    "y": 2278
                },
                {
                    "x": 292,
                    "y": 2278
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='84' style='font-size:20px'>For a given pair of sentences, the semantic relat-<br>edness task is to predict a human-generated rating<br>of the similarity of the two sentences in meaning.</p>",
            "id": 84,
            "page": 6,
            "text": "For a given pair of sentences, the semantic relatedness task is to predict a human-generated rating of the similarity of the two sentences in meaning."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 2284
                },
                {
                    "x": 1214,
                    "y": 2284
                },
                {
                    "x": 1214,
                    "y": 2898
                },
                {
                    "x": 290,
                    "y": 2898
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='85' style='font-size:20px'>We use the Sentences Involving Composi-<br>tional Knowledge (SICK) dataset (Marelli et al.,<br>2014), consisting of 9927 sentence pairs in a<br>4500/500/4927 train/dev/test split. The sentences<br>are derived from existing image and video descrip-<br>tion datasets. Each sentence pair is annotated with<br>a relatedness score y E [1, 5], with 1 indicating<br>that the two sentences are completely unrelated,<br>and 5 indicating that the two sentences are very<br>related. Each label is the average of 10 ratings as-<br>signed by different human annotators.</p>",
            "id": 85,
            "page": 6,
            "text": "We use the Sentences Involving Compositional Knowledge (SICK) dataset (Marelli , 2014), consisting of 9927 sentence pairs in a 4500/500/4927 train/dev/test split. The sentences are derived from existing image and video description datasets. Each sentence pair is annotated with a relatedness score y E , with 1 indicating that the two sentences are completely unrelated, and 5 indicating that the two sentences are very related. Each label is the average of 10 ratings assigned by different human annotators."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 2906
                },
                {
                    "x": 1216,
                    "y": 2906
                },
                {
                    "x": 1216,
                    "y": 3069
                },
                {
                    "x": 292,
                    "y": 3069
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='86' style='font-size:18px'>Here, we use the similarity model described in<br>Sec. 4.2. For the similarity prediction network<br>(Eqs. 15) we use a hidden layer of size 50. We</p>",
            "id": 86,
            "page": 6,
            "text": "Here, we use the similarity model described in Sec. 4.2. For the similarity prediction network (Eqs. 15) we use a hidden layer of size 50. We"
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 3103
                },
                {
                    "x": 1215,
                    "y": 3103
                },
                {
                    "x": 1215,
                    "y": 3192
                },
                {
                    "x": 292,
                    "y": 3192
                }
            ],
            "category": "paragraph",
            "html": "<p id='87' style='font-size:16px'>3Dependency parses produced by the Stanford Neural<br>Network Dependency Parser (Chen and Manning, 2014).</p>",
            "id": 87,
            "page": 6,
            "text": "3Dependency parses produced by the Stanford Neural Network Dependency Parser (Chen and Manning, 2014)."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 262
                },
                {
                    "x": 2183,
                    "y": 262
                },
                {
                    "x": 2183,
                    "y": 1033
                },
                {
                    "x": 1281,
                    "y": 1033
                }
            ],
            "category": "table",
            "html": "<br><table id='88' style='font-size:14px'><tr><td>Method</td><td>Fine-grained</td><td>Binary</td></tr><tr><td>RAE (Socher et al., 2013)</td><td>43.2</td><td>82.4</td></tr><tr><td>MV-RNN (Socher et al., 2013)</td><td>44.4</td><td>82.9</td></tr><tr><td>RNTN (Socher et al., 2013)</td><td>45.7</td><td>85.4</td></tr><tr><td>DCNN (Blunsom et al., 2014)</td><td>48.5</td><td>86.8</td></tr><tr><td>Paragraph-Vec (Le and Mikolov, 2014)</td><td>48.7</td><td>87.8</td></tr><tr><td>CNN-non-static (Kim, 2014)</td><td>48.0</td><td>87.2</td></tr><tr><td>CNN-multichannel (Kim, 2014)</td><td>47.4</td><td>88.1</td></tr><tr><td>DRNN (Irsoy and Cardie, 2014)</td><td>49.8</td><td>86.6</td></tr><tr><td>LSTM</td><td>46.4 (1.1)</td><td>84.9 (0.6)</td></tr><tr><td>Bidirectional LSTM</td><td>49.1 (1.0)</td><td>87.5 (0.5)</td></tr><tr><td>2-layer LSTM</td><td>46.0 (1.3)</td><td>86.3 (0.6)</td></tr><tr><td>2-layer Bidirectional LSTM</td><td>48.5 (1.0)</td><td>87.2 (1.0)</td></tr><tr><td>Dependency Tree-LSTM</td><td>48.4 (0.4)</td><td>85.7 (0.4)</td></tr><tr><td>Constituency Tree-LSTM</td><td></td><td></td></tr><tr><td>- randomly initialized vectors</td><td>43.9 (0.6)</td><td>82.0 (0.5)</td></tr><tr><td>Glove vectors, fixed</td><td>49.7 (0.4)</td><td>87.5 (0.8)</td></tr><tr><td>Glove vectors, tuned</td><td>51.0 (0.5)</td><td>88.0 (0.3)</td></tr></table>",
            "id": 88,
            "page": 6,
            "text": "Method Fine-grained Binary  RAE (Socher , 2013) 43.2 82.4  MV-RNN (Socher , 2013) 44.4 82.9  RNTN (Socher , 2013) 45.7 85.4  DCNN (Blunsom , 2014) 48.5 86.8  Paragraph-Vec (Le and Mikolov, 2014) 48.7 87.8  CNN-non-static (Kim, 2014) 48.0 87.2  CNN-multichannel (Kim, 2014) 47.4 88.1  DRNN (Irsoy and Cardie, 2014) 49.8 86.6  LSTM 46.4 (1.1) 84.9 (0.6)  Bidirectional LSTM 49.1 (1.0) 87.5 (0.5)  2-layer LSTM 46.0 (1.3) 86.3 (0.6)  2-layer Bidirectional LSTM 48.5 (1.0) 87.2 (1.0)  Dependency Tree-LSTM 48.4 (0.4) 85.7 (0.4)  Constituency Tree-LSTM    - randomly initialized vectors 43.9 (0.6) 82.0 (0.5)  Glove vectors, fixed 49.7 (0.4) 87.5 (0.8)  Glove vectors, tuned 51.0 (0.5)"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1085
                },
                {
                    "x": 2196,
                    "y": 1085
                },
                {
                    "x": 2196,
                    "y": 1425
                },
                {
                    "x": 1272,
                    "y": 1425
                }
            ],
            "category": "paragraph",
            "html": "<p id='89' style='font-size:18px'>Table 2: Test set accuracies on the Stanford Sen-<br>timent Treebank. For our experiments, we report<br>mean accuracies over 5 runs (standard deviations<br>in parentheses). Fine-grained: 5-class sentiment<br>classification. Binary: positive/negative senti-<br>ment classification.</p>",
            "id": 89,
            "page": 6,
            "text": "Table 2: Test set accuracies on the Stanford Sentiment Treebank. For our experiments, we report mean accuracies over 5 runs (standard deviations in parentheses). Fine-grained: 5-class sentiment classification. Binary: positive/negative sentiment classification."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1501
                },
                {
                    "x": 2196,
                    "y": 1501
                },
                {
                    "x": 2196,
                    "y": 1727
                },
                {
                    "x": 1272,
                    "y": 1727
                }
            ],
            "category": "paragraph",
            "html": "<p id='90' style='font-size:20px'>produce binarized constituency parses4 and depen-<br>dency parses of the sentences in the dataset for our<br>Constituency Tree-LSTM and Dependency Tree-<br>LSTM models.</p>",
            "id": 90,
            "page": 6,
            "text": "produce binarized constituency parses4 and dependency parses of the sentences in the dataset for our Constituency Tree-LSTM and Dependency TreeLSTM models."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 1776
                },
                {
                    "x": 2143,
                    "y": 1776
                },
                {
                    "x": 2143,
                    "y": 1827
                },
                {
                    "x": 1274,
                    "y": 1827
                }
            ],
            "category": "paragraph",
            "html": "<p id='91' style='font-size:22px'>5.3 Hyperparameters and Training Details</p>",
            "id": 91,
            "page": 6,
            "text": "5.3 Hyperparameters and Training Details"
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 1853
                },
                {
                    "x": 2191,
                    "y": 1853
                },
                {
                    "x": 2191,
                    "y": 1960
                },
                {
                    "x": 1274,
                    "y": 1960
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='92' style='font-size:18px'>The hyperparameters for our models were tuned<br>on the development set for each task.</p>",
            "id": 92,
            "page": 6,
            "text": "The hyperparameters for our models were tuned on the development set for each task."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1972
                },
                {
                    "x": 2196,
                    "y": 1972
                },
                {
                    "x": 2196,
                    "y": 2471
                },
                {
                    "x": 1273,
                    "y": 2471
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='93' style='font-size:18px'>We initialized our word representations using<br>publicly available 300-dimensional Glove vec-<br>tors5 (Pennington et al., 2014). For the sentiment<br>classification task, word representations were up-<br>dated during training with a learning rate of 0.1.<br>For the semantic relatedness task, word represen-<br>tations were held fixed as we did not observe any<br>significant improvement when the representations<br>were tuned.</p>",
            "id": 93,
            "page": 6,
            "text": "We initialized our word representations using publicly available 300-dimensional Glove vectors5 (Pennington , 2014). For the sentiment classification task, word representations were updated during training with a learning rate of 0.1. For the semantic relatedness task, word representations were held fixed as we did not observe any significant improvement when the representations were tuned."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2479
                },
                {
                    "x": 2196,
                    "y": 2479
                },
                {
                    "x": 2196,
                    "y": 2984
                },
                {
                    "x": 1273,
                    "y": 2984
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='94' style='font-size:20px'>Our models were trained using AdaGrad (Duchi<br>et al., 2011) with a learning rate of 0.05 and a<br>minibatch size of 25. The model parameters were<br>regularized with a per-minibatch L2 regularization<br>strength of 10-4 The sentiment classifier was ad-<br>ditionally regularized using dropout (Hinton et al.,<br>2012) with a dropout rate of 0.5. We did not ob-<br>serve performance gains using dropout on the se-<br>mantic relatedness task.</p>",
            "id": 94,
            "page": 6,
            "text": "Our models were trained using AdaGrad (Duchi , 2011) with a learning rate of 0.05 and a minibatch size of 25. The model parameters were regularized with a per-minibatch L2 regularization strength of 10-4 The sentiment classifier was additionally regularized using dropout (Hinton , 2012) with a dropout rate of 0.5. We did not observe performance gains using dropout on the semantic relatedness task."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 3013
                },
                {
                    "x": 2193,
                    "y": 3013
                },
                {
                    "x": 2193,
                    "y": 3195
                },
                {
                    "x": 1273,
                    "y": 3195
                }
            ],
            "category": "paragraph",
            "html": "<p id='95' style='font-size:14px'>4Constituency parses produced by the Stanford PCFG<br>Parser (Klein and Manning, 2003).<br>5Trained on 840 billion tokens of Common Crawl data,<br>http : / /nlp · stanford. edu /projects /glove/.</p>",
            "id": 95,
            "page": 6,
            "text": "4Constituency parses produced by the Stanford PCFG Parser (Klein and Manning, 2003). 5Trained on 840 billion tokens of Common Crawl data, http : / /nlp · stanford. edu /projects /glove/."
        },
        {
            "bounding_box": [
                {
                    "x": 450,
                    "y": 257
                },
                {
                    "x": 2045,
                    "y": 257
                },
                {
                    "x": 2045,
                    "y": 1028
                },
                {
                    "x": 450,
                    "y": 1028
                }
            ],
            "category": "table",
            "html": "<table id='96' style='font-size:14px'><tr><td>Method</td><td colspan=\"2\">Pearson's r</td><td colspan=\"2\">Spearman's P</td><td colspan=\"2\">MSE</td></tr><tr><td>Illinois-LH (Lai and Hockenmaier, 2014)</td><td colspan=\"2\">0.7993</td><td colspan=\"2\">0.7538</td><td colspan=\"2\">0.3692</td></tr><tr><td>UNAL-NLP (Jimenez et al., 2014)</td><td colspan=\"2\">0.8070</td><td colspan=\"2\">0.7489</td><td colspan=\"2\">0.3550</td></tr><tr><td>Meaning Factory (Bjerva et al., 2014)</td><td colspan=\"2\">0.8268</td><td colspan=\"2\">0.7721</td><td colspan=\"2\">0.3224</td></tr><tr><td>ECNU (Zhao et al., 2014)</td><td colspan=\"2\">0.8414</td><td colspan=\"2\">-</td><td colspan=\"2\">-</td></tr><tr><td>Mean vectors</td><td colspan=\"2\">0.7577 (0.0013)</td><td colspan=\"2\">0.6738 (0.0027)</td><td colspan=\"2\">0.4557 (0.0090)</td></tr><tr><td>DT-RNN (Socher et al., 2014)</td><td colspan=\"2\">0.7923 (0.0070)</td><td colspan=\"2\">0.7319 (0.0071)</td><td colspan=\"2\">0.3822 (0.0137)</td></tr><tr><td>SDT-RNN (Socher et al., 2014)</td><td colspan=\"2\">0.7900 (0.0042)</td><td colspan=\"2\">0.7304 (0.0076)</td><td colspan=\"2\">0.3848 (0.0074)</td></tr><tr><td>LSTM</td><td>0.8528</td><td>(0.0031)</td><td>0.7911 (0.0059)</td><td></td><td>0.2831</td><td>(0.0092)</td></tr><tr><td>Bidirectional LSTM</td><td>0.8567</td><td>(0.0028)</td><td>0.7966</td><td>(0.0053)</td><td>0.2736</td><td>(0.0063)</td></tr><tr><td>2-layer LSTM</td><td>0.8515</td><td>(0.0066)</td><td>0.7896</td><td>(0.0088)</td><td>0.2838</td><td>(0.0150)</td></tr><tr><td>2-layer Bidirectional LSTM</td><td>0.8558</td><td>(0.0014)</td><td>0.7965</td><td>(0.0018)</td><td>0.2762</td><td>(0.0020)</td></tr><tr><td>Constituency Tree-LSTM</td><td>0.8582</td><td>(0.0038)</td><td>0.7966</td><td>(0.0053)</td><td>0.2734</td><td>(0.0108)</td></tr><tr><td>Dependency Tree-LSTM</td><td>0.8676</td><td>(0.0030)</td><td>0.8083</td><td>(0.0042)</td><td>0.2532</td><td>(0.0052)</td></tr></table>",
            "id": 96,
            "page": 7,
            "text": "Method Pearson's r Spearman's P MSE  Illinois-LH (Lai and Hockenmaier, 2014) 0.7993 0.7538 0.3692  UNAL-NLP (Jimenez , 2014) 0.8070 0.7489 0.3550  Meaning Factory (Bjerva , 2014) 0.8268 0.7721 0.3224  ECNU (Zhao , 2014) 0.8414 -  Mean vectors 0.7577 (0.0013) 0.6738 (0.0027) 0.4557 (0.0090)  DT-RNN (Socher , 2014) 0.7923 (0.0070) 0.7319 (0.0071) 0.3822 (0.0137)  SDT-RNN (Socher , 2014) 0.7900 (0.0042) 0.7304 (0.0076) 0.3848 (0.0074)  LSTM 0.8528 (0.0031) 0.7911 (0.0059)  0.2831 (0.0092)  Bidirectional LSTM 0.8567 (0.0028) 0.7966 (0.0053) 0.2736 (0.0063)  2-layer LSTM 0.8515 (0.0066) 0.7896 (0.0088) 0.2838 (0.0150)  2-layer Bidirectional LSTM 0.8558 (0.0014) 0.7965 (0.0018) 0.2762 (0.0020)  Constituency Tree-LSTM 0.8582 (0.0038) 0.7966 (0.0053) 0.2734 (0.0108)  Dependency Tree-LSTM 0.8676 (0.0030) 0.8083 (0.0042) 0.2532"
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 1079
                },
                {
                    "x": 2197,
                    "y": 1079
                },
                {
                    "x": 2197,
                    "y": 1253
                },
                {
                    "x": 290,
                    "y": 1253
                }
            ],
            "category": "caption",
            "html": "<caption id='97' style='font-size:18px'>Table 3: Test set results on the SICK semantic relatedness subtask. For our experiments, we report mean<br>scores over 5 runs (standard deviations in parentheses). Results are grouped as follows: (1) SemEval<br>2014 submissions; (2) Our own baselines; (3) Sequential LSTMs; (4) Tree-structured LSTMs.</caption>",
            "id": 97,
            "page": 7,
            "text": "Table 3: Test set results on the SICK semantic relatedness subtask. For our experiments, we report mean scores over 5 runs (standard deviations in parentheses). Results are grouped as follows: (1) SemEval 2014 submissions; (2) Our own baselines; (3) Sequential LSTMs; (4) Tree-structured LSTMs."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 1327
                },
                {
                    "x": 535,
                    "y": 1327
                },
                {
                    "x": 535,
                    "y": 1383
                },
                {
                    "x": 291,
                    "y": 1383
                }
            ],
            "category": "paragraph",
            "html": "<p id='98' style='font-size:22px'>6 Results</p>",
            "id": 98,
            "page": 7,
            "text": "6 Results"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 1421
                },
                {
                    "x": 875,
                    "y": 1421
                },
                {
                    "x": 875,
                    "y": 1474
                },
                {
                    "x": 291,
                    "y": 1474
                }
            ],
            "category": "paragraph",
            "html": "<p id='99' style='font-size:18px'>6.1 Sentiment Classification</p>",
            "id": 99,
            "page": 7,
            "text": "6.1 Sentiment Classification"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 1497
                },
                {
                    "x": 1217,
                    "y": 1497
                },
                {
                    "x": 1217,
                    "y": 2343
                },
                {
                    "x": 291,
                    "y": 2343
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='100' style='font-size:20px'>Our results are summarized in Table 2. The Con-<br>stituency Tree-LSTM outperforms existing sys-<br>tems on the fine-grained classification subtask and<br>achieves accuracy comparable to the state-of-the-<br>art on the binary subtask. In particular, we find that<br>it outperforms the Dependency Tree-LSTM. This<br>performance gap is at least partially attributable to<br>the fact that the Dependency Tree-LSTMis trained<br>on less data: about 150K labeled nodes vs. 319K<br>for the Constituency Tree-LSTM. This difference<br>is due to (1) the dependency representations con-<br>taining fewer nodes than the corresponding con-<br>stituency representations, and (2) the inability to<br>match about 9% of the dependency nodes to a cor-<br>responding span in the training data.</p>",
            "id": 100,
            "page": 7,
            "text": "Our results are summarized in Table 2. The Constituency Tree-LSTM outperforms existing systems on the fine-grained classification subtask and achieves accuracy comparable to the state-of-theart on the binary subtask. In particular, we find that it outperforms the Dependency Tree-LSTM. This performance gap is at least partially attributable to the fact that the Dependency Tree-LSTMis trained on less data: about 150K labeled nodes vs. 319K for the Constituency Tree-LSTM. This difference is due to (1) the dependency representations containing fewer nodes than the corresponding constituency representations, and (2) the inability to match about 9% of the dependency nodes to a corresponding span in the training data."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 2347
                },
                {
                    "x": 1216,
                    "y": 2347
                },
                {
                    "x": 1216,
                    "y": 2910
                },
                {
                    "x": 291,
                    "y": 2910
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='101' style='font-size:20px'>We found that updating the word representa-<br>tions during training (\"fine-tuning\" the word em-<br>bedding) yields a significant boost in performance<br>on the fine-grained classification subtask and gives<br>a minor gain on the binary classification subtask<br>(this finding is consistent with previous work on<br>this task by Kim (2014)). These gains are to be<br>expected since the Glove vectors used to initial-<br>ize our word representations were not originally<br>trained to capture sentiment.</p>",
            "id": 101,
            "page": 7,
            "text": "We found that updating the word representations during training (\"fine-tuning\" the word embedding) yields a significant boost in performance on the fine-grained classification subtask and gives a minor gain on the binary classification subtask (this finding is consistent with previous work on this task by Kim (2014)). These gains are to be expected since the Glove vectors used to initialize our word representations were not originally trained to capture sentiment."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 2951
                },
                {
                    "x": 828,
                    "y": 2951
                },
                {
                    "x": 828,
                    "y": 3003
                },
                {
                    "x": 291,
                    "y": 3003
                }
            ],
            "category": "paragraph",
            "html": "<p id='102' style='font-size:20px'>6.2 Semantic Relatedness</p>",
            "id": 102,
            "page": 7,
            "text": "6.2 Semantic Relatedness"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 3027
                },
                {
                    "x": 1216,
                    "y": 3027
                },
                {
                    "x": 1216,
                    "y": 3193
                },
                {
                    "x": 291,
                    "y": 3193
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='103' style='font-size:16px'>Our results are summarized in Table 3. Following<br>Marelli et al. (2014), we use Pearson's r, Spear-<br>man's P and mean squared error (MSE) as evalua-</p>",
            "id": 103,
            "page": 7,
            "text": "Our results are summarized in Table 3. Following Marelli  (2014), we use Pearson's r, Spearman's P and mean squared error (MSE) as evalua-"
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 1333
                },
                {
                    "x": 2197,
                    "y": 1333
                },
                {
                    "x": 2197,
                    "y": 1497
                },
                {
                    "x": 1271,
                    "y": 1497
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='104' style='font-size:16px'>tion metrics. The first two metrics are measures of<br>correlation against human evaluations of semantic<br>relatedness.</p>",
            "id": 104,
            "page": 7,
            "text": "tion metrics. The first two metrics are measures of correlation against human evaluations of semantic relatedness."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1509
                },
                {
                    "x": 2199,
                    "y": 1509
                },
                {
                    "x": 2199,
                    "y": 2237
                },
                {
                    "x": 1272,
                    "y": 2237
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='105' style='font-size:16px'>We compare our models against a number of<br>non-LSTM baselines. The mean vector baseline<br>computes sentence representations as a mean of<br>the representations of the constituent words. The<br>DT-RNN and SDT-RNN models (Socher et al.,<br>2014) both compose vector representations for the<br>nodes in a dependency tree as a sum over affine-<br>transformed child vectors, followed by a nonlin-<br>earity. The SDT-RNN is an extension of the DT-<br>RNN that uses a separate transformation for each<br>dependency relation. For each of our baselines,<br>including the LSTM models, we use the similarity<br>model described in Sec. 4.2.</p>",
            "id": 105,
            "page": 7,
            "text": "We compare our models against a number of non-LSTM baselines. The mean vector baseline computes sentence representations as a mean of the representations of the constituent words. The DT-RNN and SDT-RNN models (Socher , 2014) both compose vector representations for the nodes in a dependency tree as a sum over affinetransformed child vectors, followed by a nonlinearity. The SDT-RNN is an extension of the DTRNN that uses a separate transformation for each dependency relation. For each of our baselines, including the LSTM models, we use the similarity model described in Sec. 4.2."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2247
                },
                {
                    "x": 2197,
                    "y": 2247
                },
                {
                    "x": 2197,
                    "y": 2862
                },
                {
                    "x": 1273,
                    "y": 2862
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='106' style='font-size:20px'>We also compare against four of the top-<br>performing systems6 submitted to the SemEval<br>2014 semantic relatedness shared task: ECNU<br>(Zhao et al., 2014), The Meaning Factory (Bjerva<br>et al., 2014), UNAL-NLP (Jimenez et al., 2014),<br>and Illinois-LH (Lai and Hockenmaier, 2014).<br>These systems are heavily feature engineered,<br>generally using a combination of surface form<br>overlap features and lexical distance features de-<br>rived from WordNet or the Paraphrase Database<br>(Ganitkevitch et al., 2013).</p>",
            "id": 106,
            "page": 7,
            "text": "We also compare against four of the topperforming systems6 submitted to the SemEval 2014 semantic relatedness shared task: ECNU (Zhao , 2014), The Meaning Factory (Bjerva , 2014), UNAL-NLP (Jimenez , 2014), and Illinois-LH (Lai and Hockenmaier, 2014). These systems are heavily feature engineered, generally using a combination of surface form overlap features and lexical distance features derived from WordNet or the Paraphrase Database (Ganitkevitch , 2013)."
        },
        {
            "bounding_box": [
                {
                    "x": 1321,
                    "y": 2873
                },
                {
                    "x": 2192,
                    "y": 2873
                },
                {
                    "x": 2192,
                    "y": 2926
                },
                {
                    "x": 1321,
                    "y": 2926
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='107' style='font-size:18px'>Our LSTM models outperform all these sys-</p>",
            "id": 107,
            "page": 7,
            "text": "Our LSTM models outperform all these sys-"
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2980
                },
                {
                    "x": 2196,
                    "y": 2980
                },
                {
                    "x": 2196,
                    "y": 3190
                },
                {
                    "x": 1273,
                    "y": 3190
                }
            ],
            "category": "paragraph",
            "html": "<p id='108' style='font-size:14px'>6We list the strongest results we were able to find for this<br>task; in some cases, these results are stronger than the official<br>performance by the team on the shared task. For example,<br>the listed result by Zhao et al. (2014) is stronger than their<br>submitted system's Pearson correlation score of 0.8280.</p>",
            "id": 108,
            "page": 7,
            "text": "6We list the strongest results we were able to find for this task; in some cases, these results are stronger than the official performance by the team on the shared task. For example, the listed result by Zhao  (2014) is stronger than their submitted system's Pearson correlation score of 0.8280."
        },
        {
            "bounding_box": [
                {
                    "x": 295,
                    "y": 248
                },
                {
                    "x": 1225,
                    "y": 248
                },
                {
                    "x": 1225,
                    "y": 856
                },
                {
                    "x": 295,
                    "y": 856
                }
            ],
            "category": "figure",
            "html": "<figure><img id='109' style='font-size:14px' alt=\"0.70\n0.65\n0.60\n0.55\naccuracy\n0.50\n0.45\nDT-LSTM\n0.40\nCT-LSTM\nLSTM\n0.35\n↔ Bi-LSTM\n0.30\n0 5 10 15 20 25 30 35 40 45\nsentence length\" data-coord=\"top-left:(295,248); bottom-right:(1225,856)\" /></figure>",
            "id": 109,
            "page": 8,
            "text": "0.70 0.65 0.60 0.55 accuracy 0.50 0.45 DT-LSTM 0.40 CT-LSTM LSTM 0.35 ↔ Bi-LSTM 0.30 0 5 10 15 20 25 30 35 40 45 sentence length"
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 904
                },
                {
                    "x": 1217,
                    "y": 904
                },
                {
                    "x": 1217,
                    "y": 1244
                },
                {
                    "x": 290,
                    "y": 1244
                }
            ],
            "category": "caption",
            "html": "<caption id='110' style='font-size:16px'>Figure 3: Fine-grained sentiment classification ac-<br>curacy vs. sentence length. For each l, we plot<br>accuracy for the test set sentences with length in<br>the window [l - 2, l + 2]. Examples in the tail<br>of the length distribution are batched in the final<br>window (l = 45).</caption>",
            "id": 110,
            "page": 8,
            "text": "Figure 3: Fine-grained sentiment classification accuracy vs. sentence length. For each l, we plot accuracy for the test set sentences with length in the window [l - 2, l + 2]. Examples in the tail of the length distribution are batched in the final window (l = 45)."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 1330
                },
                {
                    "x": 1216,
                    "y": 1330
                },
                {
                    "x": 1216,
                    "y": 2063
                },
                {
                    "x": 290,
                    "y": 2063
                }
            ],
            "category": "paragraph",
            "html": "<p id='111' style='font-size:16px'>tems without any additional feature engineering,<br>with the best results achieved by the Dependency<br>Tree-LSTM. Recall that in this task, both Tree-<br>LSTM models only receive supervision at the root<br>of the tree, in contrast to the sentiment classifi-<br>cation task where supervision was also provided<br>at the intermediate nodes. We conjecture that in<br>this setting, the Dependency Tree-LSTM benefits<br>from its more compact structure relative to the<br>Constituency Tree-LSTM, in the sense that paths<br>from input word vectors to the root of the tree<br>are shorter on aggregate for the Dependency Tree-<br>LSTM.</p>",
            "id": 111,
            "page": 8,
            "text": "tems without any additional feature engineering, with the best results achieved by the Dependency Tree-LSTM. Recall that in this task, both TreeLSTM models only receive supervision at the root of the tree, in contrast to the sentiment classification task where supervision was also provided at the intermediate nodes. We conjecture that in this setting, the Dependency Tree-LSTM benefits from its more compact structure relative to the Constituency Tree-LSTM, in the sense that paths from input word vectors to the root of the tree are shorter on aggregate for the Dependency TreeLSTM."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 2114
                },
                {
                    "x": 1137,
                    "y": 2114
                },
                {
                    "x": 1137,
                    "y": 2172
                },
                {
                    "x": 293,
                    "y": 2172
                }
            ],
            "category": "paragraph",
            "html": "<p id='112' style='font-size:22px'>7 Discussion and Qualitative Analysis</p>",
            "id": 112,
            "page": 8,
            "text": "7 Discussion and Qualitative Analysis"
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 2210
                },
                {
                    "x": 1023,
                    "y": 2210
                },
                {
                    "x": 1023,
                    "y": 2264
                },
                {
                    "x": 293,
                    "y": 2264
                }
            ],
            "category": "paragraph",
            "html": "<p id='113' style='font-size:20px'>7.1 Modeling Semantic Relatedness</p>",
            "id": 113,
            "page": 8,
            "text": "7.1 Modeling Semantic Relatedness"
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 2289
                },
                {
                    "x": 1216,
                    "y": 2289
                },
                {
                    "x": 1216,
                    "y": 2625
                },
                {
                    "x": 292,
                    "y": 2625
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='114' style='font-size:16px'>In Table 4, we list nearest-neighbor sentences re-<br>trieved from a 1000-sentence sample of the SICK<br>test set. We compare the neighbors ranked by the<br>Dependency Tree-LSTM model against a baseline<br>ranking by cosine similarity of the mean word vec-<br>tors for each sentence.</p>",
            "id": 114,
            "page": 8,
            "text": "In Table 4, we list nearest-neighbor sentences retrieved from a 1000-sentence sample of the SICK test set. We compare the neighbors ranked by the Dependency Tree-LSTM model against a baseline ranking by cosine similarity of the mean word vectors for each sentence."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 2631
                },
                {
                    "x": 1215,
                    "y": 2631
                },
                {
                    "x": 1215,
                    "y": 3196
                },
                {
                    "x": 291,
                    "y": 3196
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='115' style='font-size:16px'>The Dependency Tree-LSTM model exhibits<br>several desirable properties. Note that in the de-<br>pendency parse of the second query sentence, the<br>word \"ocean\" is the second-furthest word from the<br>root (\"waving\"), with a depth of 4. Regardless, the<br>retrieved sentences are all semantically related to<br>the word \"ocean\" , which indicates that the Tree-<br>LSTM is able to both preserve and emphasize in-<br>formation from relatively distant nodes. Addi-<br>tionally, the Tree-LSTM model shows greater ro-</p>",
            "id": 115,
            "page": 8,
            "text": "The Dependency Tree-LSTM model exhibits several desirable properties. Note that in the dependency parse of the second query sentence, the word \"ocean\" is the second-furthest word from the root (\"waving\"), with a depth of 4. Regardless, the retrieved sentences are all semantically related to the word \"ocean\" , which indicates that the TreeLSTM is able to both preserve and emphasize information from relatively distant nodes. Additionally, the Tree-LSTM model shows greater ro-"
        },
        {
            "bounding_box": [
                {
                    "x": 1283,
                    "y": 248
                },
                {
                    "x": 2197,
                    "y": 248
                },
                {
                    "x": 2197,
                    "y": 857
                },
                {
                    "x": 1283,
                    "y": 857
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='116' style='font-size:14px' alt=\"0.90\n0.88\n0.86\nr 0.84\n0.82\nDT-LSTM\nCT-LSTM\n0.80 LSTM\n↔ Bi-LSTM\n0.78\n4 6 8 10 12 14 16 18 20\nmean sentence length\" data-coord=\"top-left:(1283,248); bottom-right:(2197,857)\" /></figure>",
            "id": 116,
            "page": 8,
            "text": "0.90 0.88 0.86 r 0.84 0.82 DT-LSTM CT-LSTM 0.80 LSTM ↔ Bi-LSTM 0.78 4 6 8 10 12 14 16 18 20 mean sentence length"
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 904
                },
                {
                    "x": 2198,
                    "y": 904
                },
                {
                    "x": 2198,
                    "y": 1245
                },
                {
                    "x": 1271,
                    "y": 1245
                }
            ],
            "category": "caption",
            "html": "<caption id='117' style='font-size:18px'>Figure 4: Pearson correlations r between pre-<br>dicted similarities and gold ratings vs. sentence<br>length. For each l, we plot r for the pairs with<br>mean length in the window [l. -2, l+2]. Examples<br>in the tail of the length distribution are batched in<br>the final window (l = 18.5).</caption>",
            "id": 117,
            "page": 8,
            "text": "Figure 4: Pearson correlations r between predicted similarities and gold ratings vs. sentence length. For each l, we plot r for the pairs with mean length in the window [l. -2, l+2]. Examples in the tail of the length distribution are batched in the final window (l = 18.5)."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 1323
                },
                {
                    "x": 2198,
                    "y": 1323
                },
                {
                    "x": 2198,
                    "y": 1659
                },
                {
                    "x": 1271,
                    "y": 1659
                }
            ],
            "category": "paragraph",
            "html": "<p id='118' style='font-size:16px'>bustness to differences in sentence length. Given<br>the query \"two men are playing guitar\", the Tree-<br>LSTM associates the phrase \"playing guitar\" with<br>the longer, related phrase \"dancing and singing in<br>front of a crowd\" (note as well that there is zero<br>token overlap between the two phrases).</p>",
            "id": 118,
            "page": 8,
            "text": "bustness to differences in sentence length. Given the query \"two men are playing guitar\", the TreeLSTM associates the phrase \"playing guitar\" with the longer, related phrase \"dancing and singing in front of a crowd\" (note as well that there is zero token overlap between the two phrases)."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1696
                },
                {
                    "x": 1886,
                    "y": 1696
                },
                {
                    "x": 1886,
                    "y": 1750
                },
                {
                    "x": 1273,
                    "y": 1750
                }
            ],
            "category": "paragraph",
            "html": "<p id='119' style='font-size:20px'>7.2 Effect of Sentence Length</p>",
            "id": 119,
            "page": 8,
            "text": "7.2 Effect of Sentence Length"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1771
                },
                {
                    "x": 2197,
                    "y": 1771
                },
                {
                    "x": 2197,
                    "y": 2387
                },
                {
                    "x": 1272,
                    "y": 2387
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='120' style='font-size:16px'>One hypothesis to explain the empirical strength<br>of Tree-LSTMs is that tree structures help miti-<br>gate the problem of preserving state over long se-<br>quences of words. If this were true, we would ex-<br>pect to see the greatest improvement over sequen-<br>tial LSTMs on longer sentences. In Figs. 3 and 4,<br>we show the relationship between sentence length<br>and performance as measured by the relevant task-<br>specific metric. Each data point is a mean score<br>over 5 runs, and error bars have been omitted for<br>clarity.</p>",
            "id": 120,
            "page": 8,
            "text": "One hypothesis to explain the empirical strength of Tree-LSTMs is that tree structures help mitigate the problem of preserving state over long sequences of words. If this were true, we would expect to see the greatest improvement over sequential LSTMs on longer sentences. In Figs. 3 and 4, we show the relationship between sentence length and performance as measured by the relevant taskspecific metric. Each data point is a mean score over 5 runs, and error bars have been omitted for clarity."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2393
                },
                {
                    "x": 2197,
                    "y": 2393
                },
                {
                    "x": 2197,
                    "y": 2898
                },
                {
                    "x": 1272,
                    "y": 2898
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='121' style='font-size:18px'>We observe that while the Dependency Tree-<br>LSTM does significantly outperform its sequen-<br>tial counterparts on the relatedness task for<br>longer sentences of length 13 to 15 (Fig. 4), it<br>also achieves consistently strong performance on<br>shorter sentences. This suggests that unlike se-<br>quential LSTMs, Tree-LSTMs are able to encode<br>semantically-useful structural information in the<br>sentence representations that they compose.</p>",
            "id": 121,
            "page": 8,
            "text": "We observe that while the Dependency TreeLSTM does significantly outperform its sequential counterparts on the relatedness task for longer sentences of length 13 to 15 (Fig. 4), it also achieves consistently strong performance on shorter sentences. This suggests that unlike sequential LSTMs, Tree-LSTMs are able to encode semantically-useful structural information in the sentence representations that they compose."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 2938
                },
                {
                    "x": 1654,
                    "y": 2938
                },
                {
                    "x": 1654,
                    "y": 2990
                },
                {
                    "x": 1274,
                    "y": 2990
                }
            ],
            "category": "paragraph",
            "html": "<p id='122' style='font-size:20px'>8 Related Work</p>",
            "id": 122,
            "page": 8,
            "text": "8 Related Work"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 3025
                },
                {
                    "x": 2197,
                    "y": 3025
                },
                {
                    "x": 2197,
                    "y": 3196
                },
                {
                    "x": 1272,
                    "y": 3196
                }
            ],
            "category": "paragraph",
            "html": "<p id='123' style='font-size:18px'>Distributed representations of words (Rumelhart<br>et al., 1988; Collobert et al., 2011; Turian et al.,<br>2010; Huang et al., 2012; Mikolov et al., 2013;</p>",
            "id": 123,
            "page": 8,
            "text": "Distributed representations of words (Rumelhart , 1988; Collobert , 2011; Turian , 2010; Huang , 2012; Mikolov , 2013;"
        },
        {
            "bounding_box": [
                {
                    "x": 320,
                    "y": 253
                },
                {
                    "x": 2177,
                    "y": 253
                },
                {
                    "x": 2177,
                    "y": 1022
                },
                {
                    "x": 320,
                    "y": 1022
                }
            ],
            "category": "table",
            "html": "<table id='124' style='font-size:14px'><tr><td>Ranking by mean word vector cosine similarity</td><td>Score</td><td>Ranking by Dependency Tree-LSTM model</td><td>Score</td></tr><tr><td>a woman is slicing potatoes</td><td></td><td>a woman is slicing potatoes</td><td></td></tr><tr><td>a woman is cutting potatoes</td><td>0.96</td><td>a woman is cutting potatoes</td><td>4.82</td></tr><tr><td>a woman is slicing herbs</td><td>0.92</td><td>potatoes are being sliced by a woman</td><td>4.70</td></tr><tr><td>a woman is slicing tofu</td><td>0.92</td><td>tofu is being sliced by a woman</td><td>4.39</td></tr><tr><td>a boy is waving at some young runners from the ocean a man and a boy are standing at the bottom of some stairs , which are outdoors</td><td>0.92</td><td>a boy is waving at some young runners from the ocean a group of men is playing with a ball on the beach</td><td>3.79</td></tr><tr><td>a group of children in uniforms is standing at a gate and one is kissing the mother</td><td>0.90</td><td>a young boy wearing a red swimsuit is jumping out of a blue kiddies pool</td><td>3.37</td></tr><tr><td>a group of children in uniforms is standing at a gate and there is no one kissing the mother</td><td>0.90</td><td>the man is tossing a kid into the swimming pool that is near the ocean</td><td>3.19</td></tr><tr><td>two men are playing guitar</td><td></td><td>two men are playing guitar</td><td></td></tr><tr><td>some men are playing rugby</td><td>0.88</td><td>the man is singing and playing the guitar</td><td>4.08</td></tr><tr><td>two men are talking</td><td>0.87</td><td>the man is opening the guitar for donations and plays with the case</td><td>4.01</td></tr><tr><td>two dogs are playing with each other</td><td>0.87</td><td>two men are dancing and singing in front of a crowd</td><td>4.00</td></tr></table>",
            "id": 124,
            "page": 9,
            "text": "Ranking by mean word vector cosine similarity Score Ranking by Dependency Tree-LSTM model Score  a woman is slicing potatoes  a woman is slicing potatoes   a woman is cutting potatoes 0.96 a woman is cutting potatoes 4.82  a woman is slicing herbs 0.92 potatoes are being sliced by a woman 4.70  a woman is slicing tofu 0.92 tofu is being sliced by a woman 4.39  a boy is waving at some young runners from the ocean a man and a boy are standing at the bottom of some stairs , which are outdoors 0.92 a boy is waving at some young runners from the ocean a group of men is playing with a ball on the beach 3.79  a group of children in uniforms is standing at a gate and one is kissing the mother 0.90 a young boy wearing a red swimsuit is jumping out of a blue kiddies pool 3.37  a group of children in uniforms is standing at a gate and there is no one kissing the mother 0.90 the man is tossing a kid into the swimming pool that is near the ocean 3.19  two men are playing guitar  two men are playing guitar   some men are playing rugby 0.88 the man is singing and playing the guitar 4.08  two men are talking 0.87 the man is opening the guitar for donations and plays with the case 4.01  two dogs are playing with each other 0.87 two men are dancing and singing in front of a crowd"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 1046
                },
                {
                    "x": 2196,
                    "y": 1046
                },
                {
                    "x": 2196,
                    "y": 1220
                },
                {
                    "x": 291,
                    "y": 1220
                }
            ],
            "category": "caption",
            "html": "<br><caption id='125' style='font-size:14px'>Table 4: Most similar sentences from a 1000-sentence sample drawn from the SICK test set. The Tree-<br>LSTM model is able to pick up on more subtle relationships, such as that between \"beach\" and \"ocean\"<br>in the second example.</caption>",
            "id": 125,
            "page": 9,
            "text": "Table 4: Most similar sentences from a 1000-sentence sample drawn from the SICK test set. The TreeLSTM model is able to pick up on more subtle relationships, such as that between \"beach\" and \"ocean\" in the second example."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 1319
                },
                {
                    "x": 1217,
                    "y": 1319
                },
                {
                    "x": 1217,
                    "y": 1885
                },
                {
                    "x": 291,
                    "y": 1885
                }
            ],
            "category": "paragraph",
            "html": "<p id='126' style='font-size:16px'>Pennington et al., 2014) have found wide appli-<br>cability in a variety of NLP tasks. Following<br>this success, there has been substantial interest in<br>the area of learning distributed phrase and sen-<br>tence representations (Mitchell and Lapata, 2010;<br>Yessenalina and Cardie, 2011; Grefenstette et al.,<br>2013; Mikolov et al., 2013), as well as distributed<br>representations of longer bodies of text such as<br>paragraphs and documents (Srivastava et al., 2013;<br>Le and Mikolov, 2014).</p>",
            "id": 126,
            "page": 9,
            "text": "Pennington , 2014) have found wide applicability in a variety of NLP tasks. Following this success, there has been substantial interest in the area of learning distributed phrase and sentence representations (Mitchell and Lapata, 2010; Yessenalina and Cardie, 2011; Grefenstette , 2013; Mikolov , 2013), as well as distributed representations of longer bodies of text such as paragraphs and documents (Srivastava , 2013; Le and Mikolov, 2014)."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 1889
                },
                {
                    "x": 1216,
                    "y": 1889
                },
                {
                    "x": 1216,
                    "y": 2735
                },
                {
                    "x": 291,
                    "y": 2735
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='127' style='font-size:16px'>Our approach builds on recursive neural net-<br>works (Goller and Kuchler, 1996; Socher et al.,<br>2011), which we abbreviate as Tree-RNNs in or-<br>der to avoid confusion with recurrent neural net-<br>works. Under the Tree-RNN framework, the vec-<br>tor representation associated with each node of<br>a tree is composed as a function of the vectors<br>corresponding to the children of the node. The<br>choice of composition function gives rise to nu-<br>merous variants of this basic framework. Tree-<br>RNNs have been used to parse images of natu-<br>ral scenes (Socher et al., 2011), compose phrase<br>representations from word vectors (Socher et al.,<br>2012), and classify the sentiment polarity of sen-<br>tences (Socher et al., 2013).</p>",
            "id": 127,
            "page": 9,
            "text": "Our approach builds on recursive neural networks (Goller and Kuchler, 1996; Socher , 2011), which we abbreviate as Tree-RNNs in order to avoid confusion with recurrent neural networks. Under the Tree-RNN framework, the vector representation associated with each node of a tree is composed as a function of the vectors corresponding to the children of the node. The choice of composition function gives rise to numerous variants of this basic framework. TreeRNNs have been used to parse images of natural scenes (Socher , 2011), compose phrase representations from word vectors (Socher , 2012), and classify the sentiment polarity of sentences (Socher , 2013)."
        },
        {
            "bounding_box": [
                {
                    "x": 294,
                    "y": 2771
                },
                {
                    "x": 617,
                    "y": 2771
                },
                {
                    "x": 617,
                    "y": 2823
                },
                {
                    "x": 294,
                    "y": 2823
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='128' style='font-size:20px'>9 Conclusion</p>",
            "id": 128,
            "page": 9,
            "text": "9 Conclusion"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 2857
                },
                {
                    "x": 1217,
                    "y": 2857
                },
                {
                    "x": 1217,
                    "y": 3194
                },
                {
                    "x": 291,
                    "y": 3194
                }
            ],
            "category": "paragraph",
            "html": "<p id='129' style='font-size:16px'>In this paper, we introduced a generalization of<br>LSTMs to tree-structured network topologies. The<br>Tree-LSTM architecture can be applied to trees<br>with arbitrary branching factor. We demonstrated<br>the effectiveness of the Tree-LSTM by applying<br>the architecture in two tasks: semantic relatedness</p>",
            "id": 129,
            "page": 9,
            "text": "In this paper, we introduced a generalization of LSTMs to tree-structured network topologies. The Tree-LSTM architecture can be applied to trees with arbitrary branching factor. We demonstrated the effectiveness of the Tree-LSTM by applying the architecture in two tasks: semantic relatedness"
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1323
                },
                {
                    "x": 2196,
                    "y": 1323
                },
                {
                    "x": 2196,
                    "y": 1717
                },
                {
                    "x": 1272,
                    "y": 1717
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='130' style='font-size:16px'>and sentiment classification, outperforming exist-<br>ing systems on both. Controlling for model di-<br>mensionality, we demonstrated that Tree-LSTM<br>models are able to outperform their sequential<br>counterparts. Our results suggest further lines of<br>work in characterizing the role of structure in pro-<br>ducing distributed representations of sentences.</p>",
            "id": 130,
            "page": 9,
            "text": "and sentiment classification, outperforming existing systems on both. Controlling for model dimensionality, we demonstrated that Tree-LSTM models are able to outperform their sequential counterparts. Our results suggest further lines of work in characterizing the role of structure in producing distributed representations of sentences."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 1759
                },
                {
                    "x": 1698,
                    "y": 1759
                },
                {
                    "x": 1698,
                    "y": 1813
                },
                {
                    "x": 1275,
                    "y": 1813
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='131' style='font-size:22px'>Acknowledgements</p>",
            "id": 131,
            "page": 9,
            "text": "Acknowledgements"
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1845
                },
                {
                    "x": 2196,
                    "y": 1845
                },
                {
                    "x": 2196,
                    "y": 2525
                },
                {
                    "x": 1273,
                    "y": 2525
                }
            ],
            "category": "paragraph",
            "html": "<p id='132' style='font-size:16px'>We thank our anonymous reviewers for their valu-<br>able feedback. Stanford University gratefully ac-<br>knowledges the support of a Natural Language<br>Understanding-focused gift from Google Inc. and<br>the Defense Advanced Research Projects Agency<br>(DARPA) Deep Exploration and Filtering of Text<br>(DEFT) Program under Air Force Research Lab-<br>oratory (AFRL) contract no. FA8750-13-2-0040.<br>Any opinions, findings, and conclusion or recom-<br>mendations expressed in this material are those of<br>the authors and do not necessarily reflect the view<br>of the DARPA, AFRL, or the US government.</p>",
            "id": 132,
            "page": 9,
            "text": "We thank our anonymous reviewers for their valuable feedback. Stanford University gratefully acknowledges the support of a Natural Language Understanding-focused gift from Google Inc. and the Defense Advanced Research Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under Air Force Research Laboratory (AFRL) contract no. FA8750-13-2-0040. Any opinions, findings, and conclusion or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of the DARPA, AFRL, or the US government."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 2564
                },
                {
                    "x": 1517,
                    "y": 2564
                },
                {
                    "x": 1517,
                    "y": 2618
                },
                {
                    "x": 1275,
                    "y": 2618
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='133' style='font-size:20px'>References</p>",
            "id": 133,
            "page": 9,
            "text": "References"
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 2654
                },
                {
                    "x": 2196,
                    "y": 2654
                },
                {
                    "x": 2196,
                    "y": 2876
                },
                {
                    "x": 1274,
                    "y": 2876
                }
            ],
            "category": "paragraph",
            "html": "<p id='134' style='font-size:18px'>Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua<br>Bengio. 2014. Neural machine translation by<br>jointly learning to align and translate. arXiv<br>preprint arXiv:1 409.0473 ·</p>",
            "id": 134,
            "page": 9,
            "text": "Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1 409.0473 ·"
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 2897
                },
                {
                    "x": 2195,
                    "y": 2897
                },
                {
                    "x": 2195,
                    "y": 3121
                },
                {
                    "x": 1275,
                    "y": 3121
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='135' style='font-size:18px'>Bengio, Yoshua, Patrice Simard, and Paolo Fras-<br>coni. 1994. Learning long-term dependencies<br>with gradient descent is difficult. IEEE Trans-<br>actions on Neural Networks 5(2):157-166.</p>",
            "id": 135,
            "page": 9,
            "text": "Bengio, Yoshua, Patrice Simard, and Paolo Frasconi. 1994. Learning long-term dependencies with gradient descent is difficult. IEEE Transactions on Neural Networks 5(2):157-166."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 3140
                },
                {
                    "x": 2191,
                    "y": 3140
                },
                {
                    "x": 2191,
                    "y": 3193
                },
                {
                    "x": 1274,
                    "y": 3193
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='136' style='font-size:18px'>Bjerva, Johannes, Johan Bos, Rob van der Goot,</p>",
            "id": 136,
            "page": 9,
            "text": "Bjerva, Johannes, Johan Bos, Rob van der Goot,"
        },
        {
            "bounding_box": [
                {
                    "x": 335,
                    "y": 268
                },
                {
                    "x": 1215,
                    "y": 268
                },
                {
                    "x": 1215,
                    "y": 489
                },
                {
                    "x": 335,
                    "y": 489
                }
            ],
            "category": "paragraph",
            "html": "<p id='137' style='font-size:18px'>and Malvina Nissim. 2014. The Meaning Fac-<br>tory: Formal semantics for recognizing textual<br>entailment and determining semantic similarity.<br>SemEval 2014</p>",
            "id": 137,
            "page": 10,
            "text": "and Malvina Nissim. 2014. The Meaning Factory: Formal semantics for recognizing textual entailment and determining semantic similarity. SemEval 2014"
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 515
                },
                {
                    "x": 1215,
                    "y": 515
                },
                {
                    "x": 1215,
                    "y": 795
                },
                {
                    "x": 293,
                    "y": 795
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='138' style='font-size:18px'>Blunsom, Phil, Edward Grefenstette, Nal Kalch-<br>brenner, et al. 2014. A convolutional neural net-<br>work for modelling sentences. In Proceedings<br>of the 52nd Annual Meeting of the Association<br>for Computational Linguistics.</p>",
            "id": 138,
            "page": 10,
            "text": "Blunsom, Phil, Edward Grefenstette, Nal Kalchbrenner,  2014. A convolutional neural network for modelling sentences. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 821
                },
                {
                    "x": 1214,
                    "y": 821
                },
                {
                    "x": 1214,
                    "y": 1098
                },
                {
                    "x": 292,
                    "y": 1098
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='139' style='font-size:18px'>Chen, Danqi and Christopher D Manning. 2014. A<br>fast and accurate dependency parser using neu-<br>ral networks. In Proceedings of the 2014 Con-<br>ference on Empirical Methods in Natural Lan-<br>guage Processing (EMNLP). pages 740-750.</p>",
            "id": 139,
            "page": 10,
            "text": "Chen, Danqi and Christopher D Manning. 2014. A fast and accurate dependency parser using neural networks. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). pages 740-750."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 1123
                },
                {
                    "x": 1215,
                    "y": 1123
                },
                {
                    "x": 1215,
                    "y": 1401
                },
                {
                    "x": 292,
                    "y": 1401
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='140' style='font-size:20px'>Collobert, Ronan, Jason Weston, Leon Bottou,<br>Michael Karlen, Koray Kavukcuoglu, and Pavel<br>Kuksa. 2011. Natural language processing (al-<br>most) from scratch. The Journal of Machine<br>Learning Research 12:2493-2537.</p>",
            "id": 140,
            "page": 10,
            "text": "Collobert, Ronan, Jason Weston, Leon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research 12:2493-2537."
        },
        {
            "bounding_box": [
                {
                    "x": 294,
                    "y": 1427
                },
                {
                    "x": 1216,
                    "y": 1427
                },
                {
                    "x": 1216,
                    "y": 1649
                },
                {
                    "x": 294,
                    "y": 1649
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='141' style='font-size:20px'>Duchi, John, Elad Hazan, and Yoram Singer. 2011.<br>Adaptive subgradient methods for online learn-<br>ing and stochastic optimization. The Journal of<br>Machine Learning Research 12:2121-2159.</p>",
            "id": 141,
            "page": 10,
            "text": "Duchi, John, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. The Journal of Machine Learning Research 12:2121-2159."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 1676
                },
                {
                    "x": 1216,
                    "y": 1676
                },
                {
                    "x": 1216,
                    "y": 1782
                },
                {
                    "x": 292,
                    "y": 1782
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='142' style='font-size:20px'>Elman, Jeffrey L. 1990. Finding structure in time.<br>Cognitive science 14(2):179-211.</p>",
            "id": 142,
            "page": 10,
            "text": "Elman, Jeffrey L. 1990. Finding structure in time. Cognitive science 14(2):179-211."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 1810
                },
                {
                    "x": 1214,
                    "y": 1810
                },
                {
                    "x": 1214,
                    "y": 2031
                },
                {
                    "x": 292,
                    "y": 2031
                }
            ],
            "category": "paragraph",
            "html": "<p id='143' style='font-size:16px'>Foltz, Peter W, Walter Kintsch, and Thomas K<br>Landauer. 1998. The measurement of textual<br>coherence with latent semantic analysis. Dis-<br>course processes 25(2-3):285-307.</p>",
            "id": 143,
            "page": 10,
            "text": "Foltz, Peter W, Walter Kintsch, and Thomas K Landauer. 1998. The measurement of textual coherence with latent semantic analysis. Discourse processes 25(2-3):285-307."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 2058
                },
                {
                    "x": 1215,
                    "y": 2058
                },
                {
                    "x": 1215,
                    "y": 2276
                },
                {
                    "x": 293,
                    "y": 2276
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='144' style='font-size:18px'>Ganitkevitch, Juri, Benjamin Van Durme, and<br>Chris Callison-Burch. 2013. PPDB: The Para-<br>phrase Database. In HLT-NAACL. pages 758-<br>764.</p>",
            "id": 144,
            "page": 10,
            "text": "Ganitkevitch, Juri, Benjamin Van Durme, and Chris Callison-Burch. 2013. PPDB: The Paraphrase Database. In HLT-NAACL. pages 758764."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 2306
                },
                {
                    "x": 1215,
                    "y": 2306
                },
                {
                    "x": 1215,
                    "y": 2583
                },
                {
                    "x": 292,
                    "y": 2583
                }
            ],
            "category": "paragraph",
            "html": "<p id='145' style='font-size:18px'>Goller, Christoph and Andreas Kuchler. 1996.<br>Learning task-dependent distributed representa-<br>tions by backpropagation through structure. In<br>IEEE International Conference on Neural Net-<br>works. volume 1, pages 347-352.</p>",
            "id": 145,
            "page": 10,
            "text": "Goller, Christoph and Andreas Kuchler. 1996. Learning task-dependent distributed representations by backpropagation through structure. In IEEE International Conference on Neural Networks. volume 1, pages 347-352."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 2610
                },
                {
                    "x": 1215,
                    "y": 2610
                },
                {
                    "x": 1215,
                    "y": 2889
                },
                {
                    "x": 292,
                    "y": 2889
                }
            ],
            "category": "paragraph",
            "html": "<p id='146' style='font-size:20px'>Graves, Alex, Navdeep Jaitly, and A-R Mohamed.<br>2013. Hybrid speech recognition with deep<br>bidirectional LSTM. In IEEE Workshop on Au-<br>tomatic Speech Recognition and Understanding<br>(ASRU). pages 273-278.</p>",
            "id": 146,
            "page": 10,
            "text": "Graves, Alex, Navdeep Jaitly, and A-R Mohamed. 2013. Hybrid speech recognition with deep bidirectional LSTM. In IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU). pages 273-278."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 2913
                },
                {
                    "x": 1215,
                    "y": 2913
                },
                {
                    "x": 1215,
                    "y": 3193
                },
                {
                    "x": 292,
                    "y": 3193
                }
            ],
            "category": "paragraph",
            "html": "<p id='147' style='font-size:18px'>Grefenstette, Edward, Georgiana Dinu, Yao-<br>Zhong Zhang, Mehrnoosh Sadrzadeh, and<br>Marco Baroni. 2013. Multi-step regression<br>learning for compositional distributional se-<br>mantics. arXiv preprint arXiv:1301.6939 ·</p>",
            "id": 147,
            "page": 10,
            "text": "Grefenstette, Edward, Georgiana Dinu, YaoZhong Zhang, Mehrnoosh Sadrzadeh, and Marco Baroni. 2013. Multi-step regression learning for compositional distributional semantics. arXiv preprint arXiv:1301.6939 ·"
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 267
                },
                {
                    "x": 2198,
                    "y": 267
                },
                {
                    "x": 2198,
                    "y": 545
                },
                {
                    "x": 1275,
                    "y": 545
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='148' style='font-size:20px'>Hinton, Geoffrey E, Nitish Srivastava, Alex<br>Krizhevsky, Ilya Sutskever, and Ruslan R<br>Salakhutdinov. 2012. Improving neural net-<br>works by preventing co-adaptation of feature<br>detectors. arXiv preprint arXiv:1207.0580</p>",
            "id": 148,
            "page": 10,
            "text": "Hinton, Geoffrey E, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R Salakhutdinov. 2012. Improving neural networks by preventing co-adaptation of feature detectors. arXiv preprint arXiv:1207.0580"
        },
        {
            "bounding_box": [
                {
                    "x": 1277,
                    "y": 567
                },
                {
                    "x": 2198,
                    "y": 567
                },
                {
                    "x": 2198,
                    "y": 842
                },
                {
                    "x": 1277,
                    "y": 842
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='149' style='font-size:22px'>Hochreiter, Sepp. 1998. The vanishing gradient<br>problem during learning recurrent neural nets<br>and problem solutions. International Journal of<br>Uncertainty, Fuzziness and Knowledge-Based<br>Systems 6(02):107-116.</p>",
            "id": 149,
            "page": 10,
            "text": "Hochreiter, Sepp. 1998. The vanishing gradient problem during learning recurrent neural nets and problem solutions. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems 6(02):107-116."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 865
                },
                {
                    "x": 2194,
                    "y": 865
                },
                {
                    "x": 2194,
                    "y": 1026
                },
                {
                    "x": 1275,
                    "y": 1026
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='150' style='font-size:22px'>Hochreiter, Sepp and Jurgen Schmidhuber. 1997.<br>Long Short-Term Memory. Neural Computa-<br>tion 9(8):1735-1780.</p>",
            "id": 150,
            "page": 10,
            "text": "Hochreiter, Sepp and Jurgen Schmidhuber. 1997. Long Short-Term Memory. Neural Computation 9(8):1735-1780."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 1051
                },
                {
                    "x": 2197,
                    "y": 1051
                },
                {
                    "x": 2197,
                    "y": 1384
                },
                {
                    "x": 1275,
                    "y": 1384
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='151' style='font-size:22px'>Huang, Eric H., Richard Socher, Christopher D.<br>Manning, and Andrew Y. Ng. 2012. Improv-<br>ing word representations via global context and<br>multiple word prototypes. In Annual Meeting<br>of the Association for Computational Linguis-<br>tics (ACL).</p>",
            "id": 151,
            "page": 10,
            "text": "Huang, Eric H., Richard Socher, Christopher D. Manning, and Andrew Y. Ng. 2012. Improving word representations via global context and multiple word prototypes. In Annual Meeting of the Association for Computational Linguistics (ACL)."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 1406
                },
                {
                    "x": 2196,
                    "y": 1406
                },
                {
                    "x": 2196,
                    "y": 1627
                },
                {
                    "x": 1275,
                    "y": 1627
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='152' style='font-size:18px'>Irsoy, Ozan and Claire Cardie. 2014. Deep re-<br>cursive neural networks for compositionality in<br>language. In Advances in Neural Information<br>Processing Systems. pages 2096-2104.</p>",
            "id": 152,
            "page": 10,
            "text": "Irsoy, Ozan and Claire Cardie. 2014. Deep recursive neural networks for compositionality in language. In Advances in Neural Information Processing Systems. pages 2096-2104."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 1650
                },
                {
                    "x": 2197,
                    "y": 1650
                },
                {
                    "x": 2197,
                    "y": 1977
                },
                {
                    "x": 1275,
                    "y": 1977
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='153' style='font-size:18px'>Jimenez, Sergio, George Duenas, Julia Baquero,<br>Alexander Gelbukh, Av Juan Dios Batiz, and<br>Av Mendizabal. 2014. UNAL-NLP: Combin-<br>ing soft cardinality features for semantic textual<br>similarity, relatedness and entailment. SemEval<br>2014 .</p>",
            "id": 153,
            "page": 10,
            "text": "Jimenez, Sergio, George Duenas, Julia Baquero, Alexander Gelbukh, Av Juan Dios Batiz, and Av Mendizabal. 2014. UNAL-NLP: Combining soft cardinality features for semantic textual similarity, relatedness and entailment. SemEval 2014 ."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 2003
                },
                {
                    "x": 2195,
                    "y": 2003
                },
                {
                    "x": 2195,
                    "y": 2166
                },
                {
                    "x": 1275,
                    "y": 2166
                }
            ],
            "category": "paragraph",
            "html": "<p id='154' style='font-size:14px'>Kim, Yoon. 2014. Convolutional neural net-<br>works for sentence classification. arXiv preprint<br>arXiv:1408.5882 ·</p>",
            "id": 154,
            "page": 10,
            "text": "Kim, Yoon. 2014. Convolutional neural networks for sentence classification. arXiv preprint arXiv:1408.5882 ·"
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 2187
                },
                {
                    "x": 2198,
                    "y": 2187
                },
                {
                    "x": 2198,
                    "y": 2516
                },
                {
                    "x": 1274,
                    "y": 2516
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='155' style='font-size:20px'>Klein, Dan and Christopher D Manning. 2003.<br>Accurate unlexicalized parsing. In Proceedings<br>of the 41st Annual Meeting on Association for<br>Computational Linguistics-Volume 1. Associa-<br>tion for Computational Linguistics, pages 423-<br>430.</p>",
            "id": 155,
            "page": 10,
            "text": "Klein, Dan and Christopher D Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Annual Meeting on Association for Computational Linguistics-Volume 1. Association for Computational Linguistics, pages 423430."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 2544
                },
                {
                    "x": 2194,
                    "y": 2544
                },
                {
                    "x": 2194,
                    "y": 2706
                },
                {
                    "x": 1274,
                    "y": 2706
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='156' style='font-size:16px'>Lai, Alice and Julia Hockenmaier. 2014. Illinois-<br>lh: A denotational and distributional approach<br>to semantics. SemEval 2014</p>",
            "id": 156,
            "page": 10,
            "text": "Lai, Alice and Julia Hockenmaier. 2014. Illinoislh: A denotational and distributional approach to semantics. SemEval 2014"
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 2729
                },
                {
                    "x": 2196,
                    "y": 2729
                },
                {
                    "x": 2196,
                    "y": 3004
                },
                {
                    "x": 1276,
                    "y": 3004
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='157' style='font-size:20px'>Landauer, Thomas K and Susan T Dumais. 1997.<br>A solution to plato`s problem: The latent se-<br>mantic analysis theory of acquisition, induction,<br>and representation of knowledge. Psychological<br>review 104(2):211.</p>",
            "id": 157,
            "page": 10,
            "text": "Landauer, Thomas K and Susan T Dumais. 1997. A solution to plato`s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological review 104(2):211."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 3026
                },
                {
                    "x": 2196,
                    "y": 3026
                },
                {
                    "x": 2196,
                    "y": 3192
                },
                {
                    "x": 1273,
                    "y": 3192
                }
            ],
            "category": "paragraph",
            "html": "<p id='158' style='font-size:14px'>Le, Quoc V and Tomas Mikolov. 2014. Dis-<br>tributed representations of sentences and doc-<br>uments. arXiv preprint arXiv:1405.4053 ·</p>",
            "id": 158,
            "page": 10,
            "text": "Le, Quoc V and Tomas Mikolov. 2014. Distributed representations of sentences and documents. arXiv preprint arXiv:1405.4053 ·"
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 265
                },
                {
                    "x": 1216,
                    "y": 265
                },
                {
                    "x": 1216,
                    "y": 658
                },
                {
                    "x": 292,
                    "y": 658
                }
            ],
            "category": "paragraph",
            "html": "<p id='159' style='font-size:16px'>Marelli, Marco, Luisa Bentivogli, Marco Ba-<br>roni, Raffaella Bernardi, Stefano Menini, and<br>Roberto Zamparelli. 2014. SemEval-2014 Task<br>1: Evaluation of compositional distributional<br>semantic models on full sentences through se-<br>mantic relatedness and textual entailment. In<br>SemEval 2014.</p>",
            "id": 159,
            "page": 11,
            "text": "Marelli, Marco, Luisa Bentivogli, Marco Baroni, Raffaella Bernardi, Stefano Menini, and Roberto Zamparelli. 2014. SemEval-2014 Task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment. In SemEval 2014."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 680
                },
                {
                    "x": 1216,
                    "y": 680
                },
                {
                    "x": 1216,
                    "y": 846
                },
                {
                    "x": 292,
                    "y": 846
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='160' style='font-size:18px'>Mikolov, Tomas. 2012. Statistical Language Mod-<br>els Based on Neural Networks. Ph.D. thesis,<br>Brno University of Technology.</p>",
            "id": 160,
            "page": 11,
            "text": "Mikolov, Tomas. 2012. Statistical Language Models Based on Neural Networks. Ph.D. thesis, Brno University of Technology."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 869
                },
                {
                    "x": 1215,
                    "y": 869
                },
                {
                    "x": 1215,
                    "y": 1146
                },
                {
                    "x": 293,
                    "y": 1146
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='161' style='font-size:18px'>Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S<br>Corrado, and Jeff Dean. 2013. Distributed<br>representations of words and phrases and their<br>compositionality. In Advances in Neural Infor-<br>mation Processing Systems. pages 3111-3119.</p>",
            "id": 161,
            "page": 11,
            "text": "Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems. pages 3111-3119."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 1167
                },
                {
                    "x": 1215,
                    "y": 1167
                },
                {
                    "x": 1215,
                    "y": 1331
                },
                {
                    "x": 292,
                    "y": 1331
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='162' style='font-size:18px'>Mitchell, Jeff and Mirella Lapata. 2010. Composi-<br>tion in distributional models of semantics. Cog-<br>nitive science 34(8):1388-1429.</p>",
            "id": 162,
            "page": 11,
            "text": "Mitchell, Jeff and Mirella Lapata. 2010. Composition in distributional models of semantics. Cognitive science 34(8):1388-1429."
        },
        {
            "bounding_box": [
                {
                    "x": 294,
                    "y": 1357
                },
                {
                    "x": 1215,
                    "y": 1357
                },
                {
                    "x": 1215,
                    "y": 1632
                },
                {
                    "x": 294,
                    "y": 1632
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='163' style='font-size:20px'>Pennington, Jeffrey, Richard Socher, and Christo-<br>pher D Manning. 2014. Glove: Global vectors<br>for word representation. Proceedings of the Em-<br>piricial Methods in Natural Language Process-<br>ing (EMNLP 2014) 12.</p>",
            "id": 163,
            "page": 11,
            "text": "Pennington, Jeffrey, Richard Socher, and Christopher D Manning. 2014. Glove: Global vectors for word representation. Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP 2014) 12."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 1657
                },
                {
                    "x": 1216,
                    "y": 1657
                },
                {
                    "x": 1216,
                    "y": 1877
                },
                {
                    "x": 293,
                    "y": 1877
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='164' style='font-size:20px'>Rumelhart, David E, Geoffrey E Hinton, and<br>Ronald J Williams. 1988. Learning represen-<br>tations by back-propagating errors. Cognitive<br>modeling 5.</p>",
            "id": 164,
            "page": 11,
            "text": "Rumelhart, David E, Geoffrey E Hinton, and Ronald J Williams. 1988. Learning representations by back-propagating errors. Cognitive modeling 5."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 1902
                },
                {
                    "x": 1215,
                    "y": 1902
                },
                {
                    "x": 1215,
                    "y": 2349
                },
                {
                    "x": 293,
                    "y": 2349
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='165' style='font-size:20px'>Socher, Richard, Brody Huval, Christopher D<br>Manning, and Andrew Y Ng. 2012. Seman-<br>tic compositionality through recursive matrix-<br>vector spaces. In Proceedings of the 2012 Joint<br>Conference on Empirical Methods in Natural<br>Language Processing and Computational Nat-<br>ural Language Learning. Association for Com-<br>putational Linguistics, pages 1201-1211.</p>",
            "id": 165,
            "page": 11,
            "text": "Socher, Richard, Brody Huval, Christopher D Manning, and Andrew Y Ng. 2012. Semantic compositionality through recursive matrixvector spaces. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning. Association for Computational Linguistics, pages 1201-1211."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 2370
                },
                {
                    "x": 1215,
                    "y": 2370
                },
                {
                    "x": 1215,
                    "y": 2703
                },
                {
                    "x": 292,
                    "y": 2703
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='166' style='font-size:22px'>Socher, Richard, Andrej Karpathy, Quoc V Le,<br>Christopher D Manning, and Andrew Y Ng.<br>2014. Grounded compositional semantics for<br>finding and describing images with sentences.<br>Transactions of the Association for Computa-<br>tional Linguistics 2:207-218.</p>",
            "id": 166,
            "page": 11,
            "text": "Socher, Richard, Andrej Karpathy, Quoc V Le, Christopher D Manning, and Andrew Y Ng. 2014. Grounded compositional semantics for finding and describing images with sentences. Transactions of the Association for Computational Linguistics 2:207-218."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 2725
                },
                {
                    "x": 1216,
                    "y": 2725
                },
                {
                    "x": 1216,
                    "y": 3061
                },
                {
                    "x": 293,
                    "y": 3061
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='167' style='font-size:18px'>Socher, Richard, Cliff C Lin, Chris Manning, and<br>Andrew Y Ng. 2011. Parsing natural scenes<br>and natural language with recursive neural net-<br>works. In Proceedings of the 28th International<br>Conference on Machine Learning (ICML-11).<br>pages 129-136.</p>",
            "id": 167,
            "page": 11,
            "text": "Socher, Richard, Cliff C Lin, Chris Manning, and Andrew Y Ng. 2011. Parsing natural scenes and natural language with recursive neural networks. In Proceedings of the 28th International Conference on Machine Learning (ICML-11). pages 129-136."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 3083
                },
                {
                    "x": 1215,
                    "y": 3083
                },
                {
                    "x": 1215,
                    "y": 3195
                },
                {
                    "x": 291,
                    "y": 3195
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='168' style='font-size:22px'>Socher, Richard, Alex Perelygin, Jean Y Wu,<br>Jason Chuang, Christopher D Manning, An-</p>",
            "id": 168,
            "page": 11,
            "text": "Socher, Richard, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, An-"
        },
        {
            "bounding_box": [
                {
                    "x": 1318,
                    "y": 267
                },
                {
                    "x": 2197,
                    "y": 267
                },
                {
                    "x": 2197,
                    "y": 546
                },
                {
                    "x": 1318,
                    "y": 546
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='169' style='font-size:18px'>drew Y Ng, and Christopher Potts. 2013. Re-<br>cursive deep models for semantic composition-<br>ality over a sentiment treebank. In Proceedings<br>ofthe Conference on Empirical Methods in Nat-<br>ural Language Processing (EMNLP).</p>",
            "id": 169,
            "page": 11,
            "text": "drew Y Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings ofthe Conference on Empirical Methods in Natural Language Processing (EMNLP)."
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 567
                },
                {
                    "x": 2197,
                    "y": 567
                },
                {
                    "x": 2197,
                    "y": 786
                },
                {
                    "x": 1274,
                    "y": 786
                }
            ],
            "category": "paragraph",
            "html": "<p id='170' style='font-size:18px'>Srivastava, Nitish, Ruslan R Salakhutdinov, and<br>Geoffrey E Hinton. 2013. Modeling documents<br>with deep boltzmann machines. arXiv preprint<br>arXiv:1309.6865 ·</p>",
            "id": 170,
            "page": 11,
            "text": "Srivastava, Nitish, Ruslan R Salakhutdinov, and Geoffrey E Hinton. 2013. Modeling documents with deep boltzmann machines. arXiv preprint arXiv:1309.6865 ·"
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 810
                },
                {
                    "x": 2195,
                    "y": 810
                },
                {
                    "x": 2195,
                    "y": 1030
                },
                {
                    "x": 1275,
                    "y": 1030
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='171' style='font-size:18px'>Sutskever, Ilya, Oriol Vinyals, and Quoc VV Le.<br>2014. Sequence to sequence learning with neu-<br>ral networks. In Advances in Neural Informa-<br>tion Processing Systems. pages 3104-3112.</p>",
            "id": 171,
            "page": 11,
            "text": "Sutskever, Ilya, Oriol Vinyals, and Quoc VV Le. 2014. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems. pages 3104-3112."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 1053
                },
                {
                    "x": 2197,
                    "y": 1053
                },
                {
                    "x": 2197,
                    "y": 1438
                },
                {
                    "x": 1275,
                    "y": 1438
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='172' style='font-size:20px'>Turian, Joseph, Lev Ratinov, and Yoshua Bengio.<br>2010. Word representations: a simple and gen-<br>eral method for semi-supervised learning. In<br>Proceedings of the 48th annual meeting of the<br>association for computational linguistics. As-<br>sociation for Computational Linguistics, pages<br>384-394.</p>",
            "id": 172,
            "page": 11,
            "text": "Turian, Joseph, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th annual meeting of the association for computational linguistics. Association for Computational Linguistics, pages 384-394."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 1463
                },
                {
                    "x": 2197,
                    "y": 1463
                },
                {
                    "x": 2197,
                    "y": 1683
                },
                {
                    "x": 1275,
                    "y": 1683
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='173' style='font-size:18px'>Vinyals, Oriol, Alexander Toshev, Samy Bengio,<br>and Dumitru Erhan. 2014. Show and tell: A<br>neural image caption generator. arXiv preprint<br>arXiv:1411.4555</p>",
            "id": 173,
            "page": 11,
            "text": "Vinyals, Oriol, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2014. Show and tell: A neural image caption generator. arXiv preprint arXiv:1411.4555"
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 1706
                },
                {
                    "x": 2197,
                    "y": 1706
                },
                {
                    "x": 2197,
                    "y": 2042
                },
                {
                    "x": 1275,
                    "y": 2042
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='174' style='font-size:20px'>Yessenalina, Ainur and Claire Cardie. 2011. Com-<br>positional matrix-space models for sentiment<br>analysis. In Proceedings of the Conference<br>on Empirical Methods in Natural Language<br>Processing. Association for Computational Lin-<br>guistics, pages 172-182.</p>",
            "id": 174,
            "page": 11,
            "text": "Yessenalina, Ainur and Claire Cardie. 2011. Compositional matrix-space models for sentiment analysis. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, pages 172-182."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2062
                },
                {
                    "x": 2196,
                    "y": 2062
                },
                {
                    "x": 2196,
                    "y": 2224
                },
                {
                    "x": 1273,
                    "y": 2224
                }
            ],
            "category": "paragraph",
            "html": "<p id='175' style='font-size:14px'>Zaremba, Wojciech and Ilya Sutskever.<br>2014. Learning to execute. arXiv preprint<br>arXiv:1410.4615 .</p>",
            "id": 175,
            "page": 11,
            "text": "Zaremba, Wojciech and Ilya Sutskever. 2014. Learning to execute. arXiv preprint arXiv:1410.4615 ."
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 2248
                },
                {
                    "x": 2195,
                    "y": 2248
                },
                {
                    "x": 2195,
                    "y": 2471
                },
                {
                    "x": 1275,
                    "y": 2471
                }
            ],
            "category": "paragraph",
            "html": "<p id='176' style='font-size:14px'>Zhao, Jiang, Tian Tian Zhu, and Man Lan. 2014.<br>ECNU: One stone two birds: Ensemble of het-<br>erogenous measures for semantic relatedness<br>and textual entailment. SemEval 2014 .</p>",
            "id": 176,
            "page": 11,
            "text": "Zhao, Jiang, Tian Tian Zhu, and Man Lan. 2014. ECNU: One stone two birds: Ensemble of heterogenous measures for semantic relatedness and textual entailment. SemEval 2014 ."
        }
    ]
}