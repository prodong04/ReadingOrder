{
    "id": "32a56ab0-0f94-11ef-8828-426932df3dcf",
    "pdf_path": "/root/data/pdf/1509.06461v3.pdf",
    "elements": [
        {
            "bounding_box": [
                {
                    "x": 572,
                    "y": 401
                },
                {
                    "x": 1977,
                    "y": 401
                },
                {
                    "x": 1977,
                    "y": 479
                },
                {
                    "x": 572,
                    "y": 479
                }
            ],
            "category": "paragraph",
            "html": "<p id='0' style='font-size:22px'>Deep Reinforcement Learning with Double Q-learning</p>",
            "id": 0,
            "page": 1,
            "text": "Deep Reinforcement Learning with Double Q-learning"
        },
        {
            "bounding_box": [
                {
                    "x": 688,
                    "y": 580
                },
                {
                    "x": 1859,
                    "y": 580
                },
                {
                    "x": 1859,
                    "y": 687
                },
                {
                    "x": 688,
                    "y": 687
                }
            ],
            "category": "paragraph",
            "html": "<p id='1' style='font-size:20px'>Hado van Hasselt and Arthur Guez and David Silver<br>Google DeepMind</p>",
            "id": 1,
            "page": 1,
            "text": "Hado van Hasselt and Arthur Guez and David Silver Google DeepMind"
        },
        {
            "bounding_box": [
                {
                    "x": 638,
                    "y": 906
                },
                {
                    "x": 805,
                    "y": 906
                },
                {
                    "x": 805,
                    "y": 953
                },
                {
                    "x": 638,
                    "y": 953
                }
            ],
            "category": "paragraph",
            "html": "<p id='2' style='font-size:18px'>Abstract</p>",
            "id": 2,
            "page": 1,
            "text": "Abstract"
        },
        {
            "bounding_box": [
                {
                    "x": 258,
                    "y": 975
                },
                {
                    "x": 1185,
                    "y": 975
                },
                {
                    "x": 1185,
                    "y": 1647
                },
                {
                    "x": 258,
                    "y": 1647
                }
            ],
            "category": "paragraph",
            "html": "<p id='3' style='font-size:14px'>The popular Q-learning algorithm is known to overestimate<br>action values under certain conditions. It was not previously<br>known whether, in practice, such overestimations are com-<br>mon, whether they harm performance, and whether they can<br>generally be prevented. In this paper, we answer all these<br>questions affirmatively. In particular, we first show that the<br>recent DQN algorithm, which combines Q-learning with a<br>deep neural network, suffers from substantial overestimations<br>in some games in the Atari 2600 domain. We then show that<br>the idea behind the Double Q-learning algorithm, which was<br>introduced in a tabular setting, can be generalized to work<br>with large-scale function approximation. We propose a spe-<br>cific adaptation to the DQN algorithm and show that the re-<br>sulting algorithm not only reduces the observed overestima-<br>tions, as hypothesized, but that this also leads to much better<br>performance on several games.</p>",
            "id": 3,
            "page": 1,
            "text": "The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games."
        },
        {
            "bounding_box": [
                {
                    "x": 218,
                    "y": 1679
                },
                {
                    "x": 1228,
                    "y": 1679
                },
                {
                    "x": 1228,
                    "y": 2050
                },
                {
                    "x": 218,
                    "y": 2050
                }
            ],
            "category": "paragraph",
            "html": "<p id='4' style='font-size:18px'>The goal of reinforcement learning (Sutton and Barto, 1998)<br>is to learn good policies for sequential decision problems,<br>by optimizing a cumulative future reward signal. Q-learning<br>(Watkins, 1989) is one of the most popular reinforcement<br>learning algorithms, but it is known to sometimes learn un-<br>realistically high action values because it includes a maxi-<br>mization step over estimated action values, which tends to<br>prefer overestimated to underestimated values.</p>",
            "id": 4,
            "page": 1,
            "text": "The goal of reinforcement learning (Sutton and Barto, 1998) is to learn good policies for sequential decision problems, by optimizing a cumulative future reward signal. Q-learning (Watkins, 1989) is one of the most popular reinforcement learning algorithms, but it is known to sometimes learn unrealistically high action values because it includes a maximization step over estimated action values, which tends to prefer overestimated to underestimated values."
        },
        {
            "bounding_box": [
                {
                    "x": 217,
                    "y": 2051
                },
                {
                    "x": 1227,
                    "y": 2051
                },
                {
                    "x": 1227,
                    "y": 2459
                },
                {
                    "x": 217,
                    "y": 2459
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='5' style='font-size:16px'>In previous work, overestimations have been attributed<br>to insufficiently flexible function approximation (Thrun and<br>Schwartz, 1993) and noise (van Hasselt, 2010, 2011). In<br>this paper, we unify these views and show overestimations<br>can occur when the action values are inaccurate, irrespective<br>of the source of approximation error. Of course, imprecise<br>value estimates are the norm during learning, which indi-<br>cates that overestimations may be much more common than<br>previously appreciated.</p>",
            "id": 5,
            "page": 1,
            "text": "In previous work, overestimations have been attributed to insufficiently flexible function approximation (Thrun and Schwartz, 1993) and noise (van Hasselt, 2010, 2011). In this paper, we unify these views and show overestimations can occur when the action values are inaccurate, irrespective of the source of approximation error. Of course, imprecise value estimates are the norm during learning, which indicates that overestimations may be much more common than previously appreciated."
        },
        {
            "bounding_box": [
                {
                    "x": 218,
                    "y": 2460
                },
                {
                    "x": 1227,
                    "y": 2460
                },
                {
                    "x": 1227,
                    "y": 2828
                },
                {
                    "x": 218,
                    "y": 2828
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='6' style='font-size:16px'>It is an open question whether, if the overestimations<br>do occur, this negatively affects performance in practice.<br>Overoptimistic value estimates are not necessarily a prob-<br>lem in and of themselves. If all values would be uniformly<br>higher then the relative action preferences are preserved and<br>we would not expect the resulting policy to be any worse.<br>Furthermore, it is known that sometimes it is good to be op-<br>timistic: optimism in the face of uncertainty is a well-known</p>",
            "id": 6,
            "page": 1,
            "text": "It is an open question whether, if the overestimations do occur, this negatively affects performance in practice. Overoptimistic value estimates are not necessarily a problem in and of themselves. If all values would be uniformly higher then the relative action preferences are preserved and we would not expect the resulting policy to be any worse. Furthermore, it is known that sometimes it is good to be optimistic: optimism in the face of uncertainty is a well-known"
        },
        {
            "bounding_box": [
                {
                    "x": 218,
                    "y": 2850
                },
                {
                    "x": 1223,
                    "y": 2850
                },
                {
                    "x": 1223,
                    "y": 2937
                },
                {
                    "x": 218,
                    "y": 2937
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='7' style='font-size:18px'>Copyright ⓒ 2016, Association for the Advancement of Artificial<br>Intelligence (www.aaai.org). All rights reserved.</p>",
            "id": 7,
            "page": 1,
            "text": "Copyright ⓒ 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved."
        },
        {
            "bounding_box": [
                {
                    "x": 1323,
                    "y": 906
                },
                {
                    "x": 2332,
                    "y": 906
                },
                {
                    "x": 2332,
                    "y": 1181
                },
                {
                    "x": 1323,
                    "y": 1181
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='8' style='font-size:18px'>exploration technique (Kaelbling et al., 1996). If, however,<br>the overestimations are not uniform and not concentrated at<br>states about which we wish to learn more, then they might<br>negatively affect the quality of the resulting policy. Thrun<br>and Schwartz (1993) give specific examples in which this<br>leads to suboptimal policies, even asymptotically.</p>",
            "id": 8,
            "page": 1,
            "text": "exploration technique (Kaelbling , 1996). If, however, the overestimations are not uniform and not concentrated at states about which we wish to learn more, then they might negatively affect the quality of the resulting policy. Thrun and Schwartz (1993) give specific examples in which this leads to suboptimal policies, even asymptotically."
        },
        {
            "bounding_box": [
                {
                    "x": 1323,
                    "y": 1181
                },
                {
                    "x": 2332,
                    "y": 1181
                },
                {
                    "x": 2332,
                    "y": 1775
                },
                {
                    "x": 1323,
                    "y": 1775
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='9' style='font-size:16px'>To test whether overestimations occur in practice and at<br>scale, we investigate the performance of the recent DQN al-<br>gorithm (Mnih et al., 2015). DQN combines Q-learning with<br>a flexible deep neural network and was tested on a varied<br>and large set of deterministic Atari 2600 games, reaching<br>human-level performance on many games. In some ways,<br>this setting is a best-case scenario for Q-learning, because<br>the deep neural network provides flexible function approx-<br>imation with the potential for a low asymptotic approxima-<br>tion error, and the determinism of the environments prevents<br>the harmful effects of noise. Perhaps surprisingly, we show<br>that even in this comparatively favorable setting DQN some-<br>times substantially overestimates the values of the actions.</p>",
            "id": 9,
            "page": 1,
            "text": "To test whether overestimations occur in practice and at scale, we investigate the performance of the recent DQN algorithm (Mnih , 2015). DQN combines Q-learning with a flexible deep neural network and was tested on a varied and large set of deterministic Atari 2600 games, reaching human-level performance on many games. In some ways, this setting is a best-case scenario for Q-learning, because the deep neural network provides flexible function approximation with the potential for a low asymptotic approximation error, and the determinism of the environments prevents the harmful effects of noise. Perhaps surprisingly, we show that even in this comparatively favorable setting DQN sometimes substantially overestimates the values of the actions."
        },
        {
            "bounding_box": [
                {
                    "x": 1324,
                    "y": 1775
                },
                {
                    "x": 2331,
                    "y": 1775
                },
                {
                    "x": 2331,
                    "y": 2279
                },
                {
                    "x": 1324,
                    "y": 2279
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='10' style='font-size:16px'>We show that the idea behind the Double Q-learning algo-<br>rithm (van Hasselt, 2010), which was first proposed in a tab-<br>ular setting, can be generalized to work with arbitrary func-<br>tion approximation, including deep neural networks. We use<br>this to construct a new algorithm we call Double DQN. We<br>then show that this algorithm not only yields more accurate<br>value estimates, but leads to much higher scores on several<br>games. This demonstrates that the overestimations of DQN<br>were indeed leading to poorer policies and that it is benefi-<br>cial to reduce them. In addition, by improving upon DQN<br>we obtain state-of-the-art results on the Atari domain.</p>",
            "id": 10,
            "page": 1,
            "text": "We show that the idea behind the Double Q-learning algorithm (van Hasselt, 2010), which was first proposed in a tabular setting, can be generalized to work with arbitrary function approximation, including deep neural networks. We use this to construct a new algorithm we call Double DQN. We then show that this algorithm not only yields more accurate value estimates, but leads to much higher scores on several games. This demonstrates that the overestimations of DQN were indeed leading to poorer policies and that it is beneficial to reduce them. In addition, by improving upon DQN we obtain state-of-the-art results on the Atari domain."
        },
        {
            "bounding_box": [
                {
                    "x": 1689,
                    "y": 2319
                },
                {
                    "x": 1962,
                    "y": 2319
                },
                {
                    "x": 1962,
                    "y": 2374
                },
                {
                    "x": 1689,
                    "y": 2374
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='11' style='font-size:20px'>Background</p>",
            "id": 11,
            "page": 1,
            "text": "Background"
        },
        {
            "bounding_box": [
                {
                    "x": 1324,
                    "y": 2382
                },
                {
                    "x": 2332,
                    "y": 2382
                },
                {
                    "x": 2332,
                    "y": 2614
                },
                {
                    "x": 1324,
                    "y": 2614
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='12' style='font-size:16px'>To solve sequential decision problems we can learn esti-<br>mates for the optimal value of each action, defined as the<br>expected sum of future rewards when taking that action and<br>following the optimal policy thereafter. Under a given policy<br>�, the true value of an action a in a state s is</p>",
            "id": 12,
            "page": 1,
            "text": "To solve sequential decision problems we can learn estimates for the optimal value of each action, defined as the expected sum of future rewards when taking that action and following the optimal policy thereafter. Under a given policy �, the true value of an action a in a state s is"
        },
        {
            "bounding_box": [
                {
                    "x": 1373,
                    "y": 2633
                },
                {
                    "x": 2287,
                    "y": 2633
                },
                {
                    "x": 2287,
                    "y": 2688
                },
                {
                    "x": 1373,
                    "y": 2688
                }
            ],
            "category": "paragraph",
            "html": "<p id='13' style='font-size:14px'>Q�(S, a) ≡ E [R1 + YR2 + · · · I So = s, Ao = a, �] ,</p>",
            "id": 13,
            "page": 1,
            "text": "Q�(S, a) ≡ E [R1 + YR2 + · · · I So = s, Ao = a, �] ,"
        },
        {
            "bounding_box": [
                {
                    "x": 1324,
                    "y": 2708
                },
                {
                    "x": 2331,
                    "y": 2708
                },
                {
                    "x": 2331,
                    "y": 2937
                },
                {
                    "x": 1324,
                    "y": 2937
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='14' style='font-size:18px'>where 2 E [0, 1] is a discount factor that trades off the impor-<br>tance of immediate and later rewards. The optimal value is<br>then Q* (s, a) = max� Q� (s, a). An optimal policy is eas-<br>ily derived from the optimal values by selecting the highest-<br>valued action in each state.</p>",
            "id": 14,
            "page": 1,
            "text": "where 2 E  is a discount factor that trades off the importance of immediate and later rewards. The optimal value is then Q* (s, a) = max� Q� (s, a). An optimal policy is easily derived from the optimal values by selecting the highestvalued action in each state."
        },
        {
            "bounding_box": [
                {
                    "x": 64,
                    "y": 888
                },
                {
                    "x": 150,
                    "y": 888
                },
                {
                    "x": 150,
                    "y": 2324
                },
                {
                    "x": 64,
                    "y": 2324
                }
            ],
            "category": "footer",
            "html": "<br><footer id='15' style='font-size:14px'>2015<br>Dec<br>8<br>[cs.LG]<br>arXiv:1509.06461v3</footer>",
            "id": 15,
            "page": 1,
            "text": "2015 Dec 8 [cs.LG] arXiv:1509.06461v3"
        },
        {
            "bounding_box": [
                {
                    "x": 217,
                    "y": 232
                },
                {
                    "x": 1227,
                    "y": 232
                },
                {
                    "x": 1227,
                    "y": 600
                },
                {
                    "x": 217,
                    "y": 600
                }
            ],
            "category": "paragraph",
            "html": "<p id='16' style='font-size:18px'>Estimates for the optimal action values can be learned<br>using Q-learning (Watkins, 1989), a form of temporal dif-<br>ference learning (Sutton, 1988). Most interesting problems<br>are too large to learn all action values in all states sepa-<br>rately. Instead, we can learn a parameterized value function<br>Q(s, a; 0t). The standard Q-learning update for the param-<br>eters after taking action At in state St and observing the<br>immediate reward Rt+1 and resulting state St+1 is then</p>",
            "id": 16,
            "page": 2,
            "text": "Estimates for the optimal action values can be learned using Q-learning (Watkins, 1989), a form of temporal difference learning (Sutton, 1988). Most interesting problems are too large to learn all action values in all states separately. Instead, we can learn a parameterized value function Q(s, a; 0t). The standard Q-learning update for the parameters after taking action At in state St and observing the immediate reward Rt+1 and resulting state St+1 is then"
        },
        {
            "bounding_box": [
                {
                    "x": 217,
                    "y": 718
                },
                {
                    "x": 1216,
                    "y": 718
                },
                {
                    "x": 1216,
                    "y": 778
                },
                {
                    "x": 217,
                    "y": 778
                }
            ],
            "category": "paragraph",
            "html": "<p id='17' style='font-size:16px'>where a is a scalar step size and the target YO is defined as</p>",
            "id": 17,
            "page": 2,
            "text": "where a is a scalar step size and the target YO is defined as"
        },
        {
            "bounding_box": [
                {
                    "x": 219,
                    "y": 907
                },
                {
                    "x": 1225,
                    "y": 907
                },
                {
                    "x": 1225,
                    "y": 1013
                },
                {
                    "x": 219,
                    "y": 1013
                }
            ],
            "category": "paragraph",
            "html": "<p id='18' style='font-size:18px'>This update resembles stochastic gradient descent, updating<br>the current value Q(St, At; 0t) towards a target value YtQ.</p>",
            "id": 18,
            "page": 2,
            "text": "This update resembles stochastic gradient descent, updating the current value Q(St, At; 0t) towards a target value YtQ."
        },
        {
            "bounding_box": [
                {
                    "x": 219,
                    "y": 1050
                },
                {
                    "x": 574,
                    "y": 1050
                },
                {
                    "x": 574,
                    "y": 1103
                },
                {
                    "x": 219,
                    "y": 1103
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='19' style='font-size:22px'>Deep Q Networks</p>",
            "id": 19,
            "page": 2,
            "text": "Deep Q Networks"
        },
        {
            "bounding_box": [
                {
                    "x": 218,
                    "y": 1120
                },
                {
                    "x": 1227,
                    "y": 1120
                },
                {
                    "x": 1227,
                    "y": 1671
                },
                {
                    "x": 218,
                    "y": 1671
                }
            ],
            "category": "paragraph",
            "html": "<p id='20' style='font-size:16px'>A deep Q network (DQN) is a multi-layered neural network<br>that for a given state s outputs a vector of action values<br>Q(s, · ; 0), where 0 are the parameters of the network. For<br>an n-dimensional state space and an action space contain-<br>ing m actions, the neural network is a function from Rn to<br>Rm. Two important ingredients of the DQN algorithm as<br>proposed by Mnih et al. (2015) are the use of a target net-<br>work, and the use of experience replay. The target network,<br>with parameters 0- is the same as the online network ex-<br>,<br>cept that its parameters are copied every T steps from the<br>online network, SO that then 0t = 0t, and kept fixed on all<br>other steps. The target used by DQN is then</p>",
            "id": 20,
            "page": 2,
            "text": "A deep Q network (DQN) is a multi-layered neural network that for a given state s outputs a vector of action values Q(s, · ; 0), where 0 are the parameters of the network. For an n-dimensional state space and an action space containing m actions, the neural network is a function from Rn to Rm. Two important ingredients of the DQN algorithm as proposed by Mnih  (2015) are the use of a target network, and the use of experience replay. The target network, with parameters 0- is the same as the online network ex, cept that its parameters are copied every T steps from the online network, SO that then 0t = 0t, and kept fixed on all other steps. The target used by DQN is then"
        },
        {
            "bounding_box": [
                {
                    "x": 218,
                    "y": 1804
                },
                {
                    "x": 1226,
                    "y": 1804
                },
                {
                    "x": 1226,
                    "y": 2034
                },
                {
                    "x": 218,
                    "y": 2034
                }
            ],
            "category": "paragraph",
            "html": "<p id='21' style='font-size:18px'>For the experience replay (Lin, 1992), observed transitions<br>are stored for some time and sampled uniformly from this<br>memory bank to update the network. Both the target network<br>and the experience replay dramatically improve the perfor-<br>mance of the algorithm (Mnih et al., 2015).</p>",
            "id": 21,
            "page": 2,
            "text": "For the experience replay (Lin, 1992), observed transitions are stored for some time and sampled uniformly from this memory bank to update the network. Both the target network and the experience replay dramatically improve the performance of the algorithm (Mnih , 2015)."
        },
        {
            "bounding_box": [
                {
                    "x": 219,
                    "y": 2076
                },
                {
                    "x": 591,
                    "y": 2076
                },
                {
                    "x": 591,
                    "y": 2128
                },
                {
                    "x": 219,
                    "y": 2128
                }
            ],
            "category": "paragraph",
            "html": "<p id='22' style='font-size:22px'>Double Q-learning</p>",
            "id": 22,
            "page": 2,
            "text": "Double Q-learning"
        },
        {
            "bounding_box": [
                {
                    "x": 217,
                    "y": 2145
                },
                {
                    "x": 1226,
                    "y": 2145
                },
                {
                    "x": 1226,
                    "y": 2463
                },
                {
                    "x": 217,
                    "y": 2463
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='23' style='font-size:16px'>The max operator in standard Q-learning and DQN, in (2)<br>and (3), uses the same values both to select and to evalu-<br>ate an action. This makes it more likely to select overesti-<br>mated values, resulting in overoptimistic value estimates. To<br>prevent this, we can decouple the selection from the evalua-<br>tion. This is the idea behind Double Q-learning (van Hasselt,<br>2010).</p>",
            "id": 23,
            "page": 2,
            "text": "The max operator in standard Q-learning and DQN, in (2) and (3), uses the same values both to select and to evaluate an action. This makes it more likely to select overestimated values, resulting in overoptimistic value estimates. To prevent this, we can decouple the selection from the evaluation. This is the idea behind Double Q-learning (van Hasselt, 2010)."
        },
        {
            "bounding_box": [
                {
                    "x": 218,
                    "y": 2468
                },
                {
                    "x": 1227,
                    "y": 2468
                },
                {
                    "x": 1227,
                    "y": 2836
                },
                {
                    "x": 218,
                    "y": 2836
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='24' style='font-size:16px'>In the original Double Q-learning algorithm, two value<br>functions are learned by assigning each experience ran-<br>domly to update one of the two value functions, such that<br>there are two sets of weights, 0 and 0'. For each update, one<br>set of weights is used to determine the greedy policy and the<br>other to determine its value. For a clear comparison, we can<br>first untangle the selection and evaluation in Q-learning and<br>rewrite its target (2) as</p>",
            "id": 24,
            "page": 2,
            "text": "In the original Double Q-learning algorithm, two value functions are learned by assigning each experience randomly to update one of the two value functions, such that there are two sets of weights, 0 and 0'. For each update, one set of weights is used to determine the greedy policy and the other to determine its value. For a clear comparison, we can first untangle the selection and evaluation in Q-learning and rewrite its target (2) as"
        },
        {
            "bounding_box": [
                {
                    "x": 752,
                    "y": 2924
                },
                {
                    "x": 774,
                    "y": 2924
                },
                {
                    "x": 774,
                    "y": 2944
                },
                {
                    "x": 752,
                    "y": 2944
                }
            ],
            "category": "paragraph",
            "html": "<p id='25' style='font-size:14px'>a</p>",
            "id": 25,
            "page": 2,
            "text": "a"
        },
        {
            "bounding_box": [
                {
                    "x": 1327,
                    "y": 231
                },
                {
                    "x": 2188,
                    "y": 231
                },
                {
                    "x": 2188,
                    "y": 278
                },
                {
                    "x": 1327,
                    "y": 278
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='26' style='font-size:14px'>The Double Q-learning error can then be written as</p>",
            "id": 26,
            "page": 2,
            "text": "The Double Q-learning error can then be written as"
        },
        {
            "bounding_box": [
                {
                    "x": 1323,
                    "y": 428
                },
                {
                    "x": 2331,
                    "y": 428
                },
                {
                    "x": 2331,
                    "y": 750
                },
                {
                    "x": 1323,
                    "y": 750
                }
            ],
            "category": "paragraph",
            "html": "<p id='27' style='font-size:16px'>Notice that the selection of the action, in the argmax, is<br>still due to the online weights 0t. This means that, as in Q-<br>learning, we are still estimating the value of the greedy pol-<br>icy according to the current values, as defined by 0t. How-<br>ever, we use the second set of weights 0't to fairly evaluate<br>the value of this policy. This second set of weights can be<br>updated symmetrically by switching the roles of 0 and 0'.</p>",
            "id": 27,
            "page": 2,
            "text": "Notice that the selection of the action, in the argmax, is still due to the online weights 0t. This means that, as in Qlearning, we are still estimating the value of the greedy policy according to the current values, as defined by 0t. However, we use the second set of weights 0't to fairly evaluate the value of this policy. This second set of weights can be updated symmetrically by switching the roles of 0 and 0'."
        },
        {
            "bounding_box": [
                {
                    "x": 1408,
                    "y": 790
                },
                {
                    "x": 2244,
                    "y": 790
                },
                {
                    "x": 2244,
                    "y": 843
                },
                {
                    "x": 1408,
                    "y": 843
                }
            ],
            "category": "paragraph",
            "html": "<p id='28' style='font-size:20px'>Overoptimism due to estimation errors</p>",
            "id": 28,
            "page": 2,
            "text": "Overoptimism due to estimation errors"
        },
        {
            "bounding_box": [
                {
                    "x": 1322,
                    "y": 852
                },
                {
                    "x": 2330,
                    "y": 852
                },
                {
                    "x": 2330,
                    "y": 1406
                },
                {
                    "x": 1322,
                    "y": 1406
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='29' style='font-size:16px'>Q-learning's overestimations were first investigated by<br>Thrun and Schwartz (1993), who showed that if the action<br>values contain random errors uniformly distributed in an in-<br>terval [ -E, e] then each target is overestimated up to YE m+1,<br>where m is the number of actions. In addition, Thrun and<br>Schwartz give a concrete example in which these overes-<br>timations even asymptotically lead to sub-optimal policies,<br>and show the overestimations manifest themselves in a small<br>toy problem when using function approximation. Later van<br>Hasselt (2010) argued that noise in the environment can lead<br>to overestimations even when using tabular representation,<br>and proposed Double Q-learning as a solution.</p>",
            "id": 29,
            "page": 2,
            "text": "Q-learning's overestimations were first investigated by Thrun and Schwartz (1993), who showed that if the action values contain random errors uniformly distributed in an interval [ -E, e] then each target is overestimated up to YE m+1, where m is the number of actions. In addition, Thrun and Schwartz give a concrete example in which these overestimations even asymptotically lead to sub-optimal policies, and show the overestimations manifest themselves in a small toy problem when using function approximation. Later van Hasselt (2010) argued that noise in the environment can lead to overestimations even when using tabular representation, and proposed Double Q-learning as a solution."
        },
        {
            "bounding_box": [
                {
                    "x": 1324,
                    "y": 1407
                },
                {
                    "x": 2332,
                    "y": 1407
                },
                {
                    "x": 2332,
                    "y": 1726
                },
                {
                    "x": 1324,
                    "y": 1726
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='30' style='font-size:16px'>In this section we demonstrate more generally that esti-<br>mation errors of any kind can induce an upward bias, re-<br>gardless of whether these errors are due to environmental<br>noise, function approximation, non-stationarity, or any other<br>source. This is important, because in practice any method<br>will incur some inaccuracies during learning, simply due to<br>the fact that the true values are initially unknown.</p>",
            "id": 30,
            "page": 2,
            "text": "In this section we demonstrate more generally that estimation errors of any kind can induce an upward bias, regardless of whether these errors are due to environmental noise, function approximation, non-stationarity, or any other source. This is important, because in practice any method will incur some inaccuracies during learning, simply due to the fact that the true values are initially unknown."
        },
        {
            "bounding_box": [
                {
                    "x": 1324,
                    "y": 1727
                },
                {
                    "x": 2331,
                    "y": 1727
                },
                {
                    "x": 2331,
                    "y": 1909
                },
                {
                    "x": 1324,
                    "y": 1909
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='31' style='font-size:16px'>The result by Thrun and Schwartz (1993) cited above<br>gives an upper bound to the overestimation for a specific<br>setup, but it is also possible, and potentially more interest-<br>ing, to derive a lower bound.</p>",
            "id": 31,
            "page": 2,
            "text": "The result by Thrun and Schwartz (1993) cited above gives an upper bound to the overestimation for a specific setup, but it is also possible, and potentially more interesting, to derive a lower bound."
        },
        {
            "bounding_box": [
                {
                    "x": 1323,
                    "y": 1922
                },
                {
                    "x": 2332,
                    "y": 1922
                },
                {
                    "x": 2332,
                    "y": 2420
                },
                {
                    "x": 1323,
                    "y": 2420
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='32' style='font-size:18px'>Theorem 1. Consider a state s in which all the true optimal<br>action values are equal at Q*(s,a) = V*(s) for some V* (s).<br>Let Qt be arbitrary value estimates that are on the whole un-<br>biased in the sense that �a (Qt(s,a) - V*(s)) = 0, but that<br>are not all correct, such that 1 �a (Qt(s,a) - V*(s))2 = C<br>m<br>for some C > 0, where m ≥ 2 is the number ofactions in s.<br>Under these conditions, maxa Qt(s,a) ≥ V*(s) + V m-1.<br>This lower bound is tight. Under the same conditions, the<br>lower bound on the absolute error of the Double Q-learning<br>estimate is zero. (Proof in appendix. )</p>",
            "id": 32,
            "page": 2,
            "text": "Theorem 1. Consider a state s in which all the true optimal action values are equal at Q*(s,a) = V*(s) for some V* (s). Let Qt be arbitrary value estimates that are on the whole unbiased in the sense that �a (Qt(s,a) - V*(s)) = 0, but that are not all correct, such that 1 �a (Qt(s,a) - V*(s))2 = C m for some C > 0, where m ≥ 2 is the number ofactions in s. Under these conditions, maxa Qt(s,a) ≥ V*(s) + V m-1. This lower bound is tight. Under the same conditions, the lower bound on the absolute error of the Double Q-learning estimate is zero. (Proof in appendix. )"
        },
        {
            "bounding_box": [
                {
                    "x": 1323,
                    "y": 2433
                },
                {
                    "x": 2331,
                    "y": 2433
                },
                {
                    "x": 2331,
                    "y": 2661
                },
                {
                    "x": 1323,
                    "y": 2661
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='33' style='font-size:14px'>Note that we did not need to assume that estimation errors<br>for different actions are independent. This theorem shows<br>that even if the value estimates are on average correct, esti-<br>mation errors of any source can drive the estimates up and<br>away from the true optimal values.</p>",
            "id": 33,
            "page": 2,
            "text": "Note that we did not need to assume that estimation errors for different actions are independent. This theorem shows that even if the value estimates are on average correct, estimation errors of any source can drive the estimates up and away from the true optimal values."
        },
        {
            "bounding_box": [
                {
                    "x": 1324,
                    "y": 2662
                },
                {
                    "x": 2331,
                    "y": 2662
                },
                {
                    "x": 2331,
                    "y": 2937
                },
                {
                    "x": 1324,
                    "y": 2937
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='34' style='font-size:16px'>The lower bound in Theorem 1 decreases with the num-<br>ber of actions. This is an artifact of considering the lower<br>bound, which requires very specific values to be attained.<br>More typically, the overoptimism increases with the num-<br>ber of actions as shown in Figure 1. Q-learning's overesti-<br>mations there indeed increase with the number of actions,</p>",
            "id": 34,
            "page": 2,
            "text": "The lower bound in Theorem 1 decreases with the number of actions. This is an artifact of considering the lower bound, which requires very specific values to be attained. More typically, the overoptimism increases with the number of actions as shown in Figure 1. Q-learning's overestimations there indeed increase with the number of actions,"
        },
        {
            "bounding_box": [
                {
                    "x": 805,
                    "y": 258
                },
                {
                    "x": 1186,
                    "y": 258
                },
                {
                    "x": 1186,
                    "y": 337
                },
                {
                    "x": 805,
                    "y": 337
                }
            ],
            "category": "paragraph",
            "html": "<p id='35' style='font-size:14px'>maxa Q(s,a) - V*(s)<br>Q'(s, argmax�Q(s,a)) - V*(s)</p>",
            "id": 35,
            "page": 3,
            "text": "maxa Q(s,a) - V*(s) Q'(s, argmax�Q(s,a)) - V*(s)"
        },
        {
            "bounding_box": [
                {
                    "x": 247,
                    "y": 234
                },
                {
                    "x": 763,
                    "y": 234
                },
                {
                    "x": 763,
                    "y": 553
                },
                {
                    "x": 247,
                    "y": 553
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='36' style='font-size:14px' alt=\"1.5\n1.0\nerror\n0.5\n0.0\n256\n1024\n128\n512\n8 る 23 업\" data-coord=\"top-left:(247,234); bottom-right:(763,553)\" /></figure>",
            "id": 36,
            "page": 3,
            "text": "1.5 1.0 error 0.5 0.0 256 1024 128 512 8 る 23 업"
        },
        {
            "bounding_box": [
                {
                    "x": 416,
                    "y": 554
                },
                {
                    "x": 669,
                    "y": 554
                },
                {
                    "x": 669,
                    "y": 589
                },
                {
                    "x": 416,
                    "y": 589
                }
            ],
            "category": "caption",
            "html": "<br><caption id='37' style='font-size:14px'>number of actions</caption>",
            "id": 37,
            "page": 3,
            "text": "number of actions"
        },
        {
            "bounding_box": [
                {
                    "x": 217,
                    "y": 587
                },
                {
                    "x": 1227,
                    "y": 587
                },
                {
                    "x": 1227,
                    "y": 863
                },
                {
                    "x": 217,
                    "y": 863
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='38' style='font-size:22px'>Figure 1: The orange bars show the bias in a single Q-<br>learning update when the action values are Q(s, a) =<br>V*(s) + Ea and the errors {Ea }a=1 are independent standard<br>normal random variables. The second set of action values<br>Q', used for the blue bars, was generated identically and in-<br>dependently. All bars are the average of 100 repetitions.</p>",
            "id": 38,
            "page": 3,
            "text": "Figure 1: The orange bars show the bias in a single Qlearning update when the action values are Q(s, a) = V*(s) + Ea and the errors {Ea }a=1 are independent standard normal random variables. The second set of action values Q', used for the blue bars, was generated identically and independently. All bars are the average of 100 repetitions."
        },
        {
            "bounding_box": [
                {
                    "x": 217,
                    "y": 900
                },
                {
                    "x": 1225,
                    "y": 900
                },
                {
                    "x": 1225,
                    "y": 1083
                },
                {
                    "x": 217,
                    "y": 1083
                }
            ],
            "category": "paragraph",
            "html": "<p id='39' style='font-size:22px'>while Double Q-learning is unbiased. As another example,<br>if for all actions Q* (s, a) = V* (s) and the estimation errors<br>Qt(s,a) - V*(s) are uniformly random in - 1, 1], then the<br>overoptimism is m-1 (Proof in appendix.)<br>m+1</p>",
            "id": 39,
            "page": 3,
            "text": "while Double Q-learning is unbiased. As another example, if for all actions Q* (s, a) = V* (s) and the estimation errors Qt(s,a) - V*(s) are uniformly random in - 1, 1], then the overoptimism is m-1 (Proof in appendix.) m+1"
        },
        {
            "bounding_box": [
                {
                    "x": 218,
                    "y": 1090
                },
                {
                    "x": 1228,
                    "y": 1090
                },
                {
                    "x": 1228,
                    "y": 2185
                },
                {
                    "x": 218,
                    "y": 2185
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='40' style='font-size:18px'>We now turn to function approximation and consider a<br>real-valued continuous state space with 10 discrete actions<br>in each state. For simplicity, the true optimal action values<br>in this example depend only on state SO that in each state<br>all actions have the same true value. These true values are<br>shown in the left column of plots in Figure 2 (purple lines)<br>and are defined as either Q* (s, a) = sin(s) (top row) or<br>Q* (s, a) = 2 exp(- - s2) (middle and bottom rows). The left<br>plots also show an approximation for a single action (green<br>lines) as a function of state as well as the samples the es-<br>timate is based on (green dots). The estimate is a d-degree<br>polynomial that is fit to the true values at sampled states,<br>where d = 6 (top and middle rows) or d = 9 (bottom<br>row). The samples match the true function exactly: there is<br>no noise and we assume we have ground truth for the action<br>value on these sampled states. The approximation is inex-<br>act even on the sampled states for the top two rows because<br>the function approximation is insufficiently flexible. In the<br>bottom row, the function is flexible enough to fit the green<br>dots, but this reduces the accuracy in unsampled states. No-<br>tice that the sampled states are spaced further apart near the<br>left side of the left plots, resulting in larger estimation errors.<br>In many ways this is a typical learning setting, where at each<br>point in time we only have limited data.</p>",
            "id": 40,
            "page": 3,
            "text": "We now turn to function approximation and consider a real-valued continuous state space with 10 discrete actions in each state. For simplicity, the true optimal action values in this example depend only on state SO that in each state all actions have the same true value. These true values are shown in the left column of plots in Figure 2 (purple lines) and are defined as either Q* (s, a) = sin(s) (top row) or Q* (s, a) = 2 exp(- - s2) (middle and bottom rows). The left plots also show an approximation for a single action (green lines) as a function of state as well as the samples the estimate is based on (green dots). The estimate is a d-degree polynomial that is fit to the true values at sampled states, where d = 6 (top and middle rows) or d = 9 (bottom row). The samples match the true function exactly: there is no noise and we assume we have ground truth for the action value on these sampled states. The approximation is inexact even on the sampled states for the top two rows because the function approximation is insufficiently flexible. In the bottom row, the function is flexible enough to fit the green dots, but this reduces the accuracy in unsampled states. Notice that the sampled states are spaced further apart near the left side of the left plots, resulting in larger estimation errors. In many ways this is a typical learning setting, where at each point in time we only have limited data."
        },
        {
            "bounding_box": [
                {
                    "x": 218,
                    "y": 2188
                },
                {
                    "x": 1226,
                    "y": 2188
                },
                {
                    "x": 1226,
                    "y": 2647
                },
                {
                    "x": 218,
                    "y": 2647
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='41' style='font-size:20px'>The middle column of plots in Figure 2 shows estimated<br>action value functions for all 10 actions (green lines), as<br>functions of state, along with the maximum action value in<br>each state (black dashed line). Although the true value func-<br>tion is the same for all actions, the approximations differ<br>because we have supplied different sets of sampled states. 1<br>The maximum is often higher than the ground truth shown<br>in purple on the left. This is confirmed in the right plots,<br>which shows the difference between the black and purple<br>curves in orange. The orange line is almost always positive,</p>",
            "id": 41,
            "page": 3,
            "text": "The middle column of plots in Figure 2 shows estimated action value functions for all 10 actions (green lines), as functions of state, along with the maximum action value in each state (black dashed line). Although the true value function is the same for all actions, the approximations differ because we have supplied different sets of sampled states. 1 The maximum is often higher than the ground truth shown in purple on the left. This is confirmed in the right plots, which shows the difference between the black and purple curves in orange. The orange line is almost always positive,"
        },
        {
            "bounding_box": [
                {
                    "x": 219,
                    "y": 2683
                },
                {
                    "x": 1226,
                    "y": 2683
                },
                {
                    "x": 1226,
                    "y": 2933
                },
                {
                    "x": 219,
                    "y": 2933
                }
            ],
            "category": "paragraph",
            "html": "<p id='42' style='font-size:14px'>1 Each action-value function is fit with a different subset of in-<br>teger states. States -6 and 6 are always included to avoid extrap-<br>olations, and for each action two adjacent integers are missing: for<br>action a1 states -5 and -4 are not sampled, for a2 states -4 and<br>-3 are not sampled, and so on. This causes the estimated values to<br>differ.</p>",
            "id": 42,
            "page": 3,
            "text": "1 Each action-value function is fit with a different subset of integer states. States -6 and 6 are always included to avoid extrapolations, and for each action two adjacent integers are missing: for action a1 states -5 and -4 are not sampled, for a2 states -4 and -3 are not sampled, and so on. This causes the estimated values to differ."
        },
        {
            "bounding_box": [
                {
                    "x": 1323,
                    "y": 232
                },
                {
                    "x": 2331,
                    "y": 232
                },
                {
                    "x": 2331,
                    "y": 461
                },
                {
                    "x": 1323,
                    "y": 461
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='43' style='font-size:20px'>indicating an upward bias. The right plots also show the es-<br>timates from Double Q-learning in blue2, which are on aver-<br>age much closer to zero. This demonstrates that Double Q-<br>learning indeed can successfully reduce the overoptimism of<br>Q-learning.</p>",
            "id": 43,
            "page": 3,
            "text": "indicating an upward bias. The right plots also show the estimates from Double Q-learning in blue2, which are on average much closer to zero. This demonstrates that Double Qlearning indeed can successfully reduce the overoptimism of Q-learning."
        },
        {
            "bounding_box": [
                {
                    "x": 1321,
                    "y": 469
                },
                {
                    "x": 2332,
                    "y": 469
                },
                {
                    "x": 2332,
                    "y": 1112
                },
                {
                    "x": 1321,
                    "y": 1112
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='44' style='font-size:18px'>The different rows in Figure 2 show variations of the same<br>experiment. The difference between the top and middle rows<br>is the true value function, demonstrating that overestima-<br>tions are not an artifact of a specific true value function.<br>The difference between the middle and bottom rows is the<br>flexibility of the function approximation. In the left-middle<br>plot, the estimates are even incorrect for some of the sam-<br>pled states because the function is insufficiently flexible.<br>The function in the bottom-left plot is more flexible but this<br>causes higher estimation errors for unseen states, resulting<br>in higher overestimations. This is important because flexi-<br>ble parametric function approximators are often employed<br>in reinforcement learning (see, e.g., Tesauro 1995; Sallans<br>and Hinton 2004; Riedmiller 2005; Mnih et al. 2015).</p>",
            "id": 44,
            "page": 3,
            "text": "The different rows in Figure 2 show variations of the same experiment. The difference between the top and middle rows is the true value function, demonstrating that overestimations are not an artifact of a specific true value function. The difference between the middle and bottom rows is the flexibility of the function approximation. In the left-middle plot, the estimates are even incorrect for some of the sampled states because the function is insufficiently flexible. The function in the bottom-left plot is more flexible but this causes higher estimation errors for unseen states, resulting in higher overestimations. This is important because flexible parametric function approximators are often employed in reinforcement learning (see, e.g., Tesauro 1995; Sallans and Hinton 2004; Riedmiller 2005; Mnih  2015)."
        },
        {
            "bounding_box": [
                {
                    "x": 1323,
                    "y": 1119
                },
                {
                    "x": 2331,
                    "y": 1119
                },
                {
                    "x": 2331,
                    "y": 1486
                },
                {
                    "x": 1323,
                    "y": 1486
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='45' style='font-size:18px'>In contrast to van Hasselt (2010) we did not use a sta-<br>tistical argument to find overestimations, the process to ob-<br>tain Figure 2 is fully deterministic. In contrast to Thrun and<br>Schwartz (1993), we did not rely on inflexible function ap-<br>proximation with irreducible asymptotic errors; the bottom<br>row shows that a function that is flexible enough to cover all<br>samples leads to high overestimations. This indicates that<br>the overestimations can occur quite generally.</p>",
            "id": 45,
            "page": 3,
            "text": "In contrast to van Hasselt (2010) we did not use a statistical argument to find overestimations, the process to obtain Figure 2 is fully deterministic. In contrast to Thrun and Schwartz (1993), we did not rely on inflexible function approximation with irreducible asymptotic errors; the bottom row shows that a function that is flexible enough to cover all samples leads to high overestimations. This indicates that the overestimations can occur quite generally."
        },
        {
            "bounding_box": [
                {
                    "x": 1322,
                    "y": 1493
                },
                {
                    "x": 2331,
                    "y": 1493
                },
                {
                    "x": 2331,
                    "y": 2043
                },
                {
                    "x": 1322,
                    "y": 2043
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='46' style='font-size:20px'>In the examples above, overestimations occur even when<br>assuming we have samples of the true action value at cer-<br>tain states. The value estimates can further deteriorate if we<br>bootstrap off of action values that are already overoptimistic,<br>since this causes overestimations to propagate throughout<br>our estimates. Although uniformly overestimating values<br>might not hurt the resulting policy, in practice overestima-<br>tion errors will differ for different states and actions. Over-<br>estimation combined with bootstrapping then has the perni-<br>cious effect of propagating the wrong relative information<br>about which states are more valuable than others, directly<br>affecting the quality of the learned policies.</p>",
            "id": 46,
            "page": 3,
            "text": "In the examples above, overestimations occur even when assuming we have samples of the true action value at certain states. The value estimates can further deteriorate if we bootstrap off of action values that are already overoptimistic, since this causes overestimations to propagate throughout our estimates. Although uniformly overestimating values might not hurt the resulting policy, in practice overestimation errors will differ for different states and actions. Overestimation combined with bootstrapping then has the pernicious effect of propagating the wrong relative information about which states are more valuable than others, directly affecting the quality of the learned policies."
        },
        {
            "bounding_box": [
                {
                    "x": 1322,
                    "y": 2051
                },
                {
                    "x": 2331,
                    "y": 2051
                },
                {
                    "x": 2331,
                    "y": 2737
                },
                {
                    "x": 1322,
                    "y": 2737
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='47' style='font-size:22px'>The overestimations should not be confused with opti-<br>mism in the face of uncertainty (Sutton, 1990; Agrawal,<br>1995; Kaelbling et al., 1996; Auer et al., 2002; Brafman and<br>Tennenholtz, 2003; Szita and L�rincz, 2008; Strehl et al.,<br>2009), where an exploration bonus is given to states or<br>actions with uncertain values. Conversely, the overestima-<br>tions discussed here occur only after updating, resulting in<br>overoptimism in the face of apparent certainty. This was al-<br>ready observed by Thrun and Schwartz (1993), who noted<br>that, in contrast to optimism in the face of uncertainty, these<br>overestimations actually can impede learning an optimal<br>policy. We will see this negative effect on policy quality con-<br>firmed later in the experiments as well: when we reduce the<br>overestimations using Double Q-learning, the policies im-<br>prove.</p>",
            "id": 47,
            "page": 3,
            "text": "The overestimations should not be confused with optimism in the face of uncertainty (Sutton, 1990; Agrawal, 1995; Kaelbling , 1996; Auer , 2002; Brafman and Tennenholtz, 2003; Szita and L�rincz, 2008; Strehl , 2009), where an exploration bonus is given to states or actions with uncertain values. Conversely, the overestimations discussed here occur only after updating, resulting in overoptimism in the face of apparent certainty. This was already observed by Thrun and Schwartz (1993), who noted that, in contrast to optimism in the face of uncertainty, these overestimations actually can impede learning an optimal policy. We will see this negative effect on policy quality confirmed later in the experiments as well: when we reduce the overestimations using Double Q-learning, the policies improve."
        },
        {
            "bounding_box": [
                {
                    "x": 1322,
                    "y": 2808
                },
                {
                    "x": 2332,
                    "y": 2808
                },
                {
                    "x": 2332,
                    "y": 2935
                },
                {
                    "x": 1322,
                    "y": 2935
                }
            ],
            "category": "paragraph",
            "html": "<p id='48' style='font-size:16px'>2We arbitrarily used the samples of action ai+5 (for i ≤ 5)<br>or ai-5 (for i > 5) as the second set of samples for the double<br>estimator of action ai.</p>",
            "id": 48,
            "page": 3,
            "text": "2We arbitrarily used the samples of action ai+5 (for i ≤ 5) or ai-5 (for i > 5) as the second set of samples for the double estimator of action ai."
        },
        {
            "bounding_box": [
                {
                    "x": 408,
                    "y": 207
                },
                {
                    "x": 2107,
                    "y": 207
                },
                {
                    "x": 2107,
                    "y": 938
                },
                {
                    "x": 408,
                    "y": 938
                }
            ],
            "category": "figure",
            "html": "<figure><img id='49' style='font-size:14px' alt=\"True value and an estimate All estimates and max Bias as function of state Average error\n2 2 Qt(s,a) - maxa Q*(s,a) +0.61\nmaxa\nQ*(s,a) maxa Qt(s, a) 1\n0 0 0 -0.02\nQt(s, a) -1 Double-Q estimate\n-2 -2\nmaxa Qt(s,a) - maxa Q*(s,a) +0.47\nmaxa Qt(s,a) 1\n2 Q*(s,a) 2\nQt(s,a) 0 +0.02\nDouble-Q estimate\n0 0 -1\n4 4 4 maxa Qt(s(s,a)\nQt(s,a) maxa Qt(s,a)I maxa\n2 2 2\n+3.35\n0 0 0 -0.02\nQ*(s,a) Double-Q estimate\n-6 -4 -2 0 2 4 6 -6 -4 -2 0 2 4 6 -6 -4 -2 0 2 4 6\nstate state state\" data-coord=\"top-left:(408,207); bottom-right:(2107,938)\" /></figure>",
            "id": 49,
            "page": 4,
            "text": "True value and an estimate All estimates and max Bias as function of state Average error 2 2 Qt(s,a) - maxa Q*(s,a) +0.61 maxa Q*(s,a) maxa Qt(s, a) 1 0 0 0 -0.02 Qt(s, a) -1 Double-Q estimate -2 -2 maxa Qt(s,a) - maxa Q*(s,a) +0.47 maxa Qt(s,a) 1 2 Q*(s,a) 2 Qt(s,a) 0 +0.02 Double-Q estimate 0 0 -1 4 4 4 maxa Qt(s(s,a) Qt(s,a) maxa Qt(s,a)I maxa 2 2 2 +3.35 0 0 0 -0.02 Q*(s,a) Double-Q estimate -6 -4 -2 0 2 4 6 -6 -4 -2 0 2 4 6 -6 -4 -2 0 2 4 6 state state state"
        },
        {
            "bounding_box": [
                {
                    "x": 221,
                    "y": 944
                },
                {
                    "x": 2336,
                    "y": 944
                },
                {
                    "x": 2336,
                    "y": 1243
                },
                {
                    "x": 221,
                    "y": 1243
                }
            ],
            "category": "caption",
            "html": "<br><caption id='50' style='font-size:16px'>Figure 2: Illustration of overestimations during learning. In each state (x-axis), there are 10 actions. The left column shows the true values<br>V*(s) (purple line). All true action values are defined by Q* (s, a) = V*(s). The green line shows estimated values Q(s, a) for one action<br>as a function of state, fitted to the true value at several sampled states (green dots). The middle column plots show all the estimated values<br>(green), and the maximum of these values (dashed black). The maximum is higher than the true value (purple, left plot) almost everywhere.<br>The right column plots shows the difference in orange. The blue line in the right plots is the estimate used by Double Q-learning with a<br>second set of samples for each state. The blue line is much closer to zero, indicating less bias. The three rows correspond to different true<br>functions (left, purple) or capacities of the fitted function (left, green). (Details in the text)</caption>",
            "id": 50,
            "page": 4,
            "text": "Figure 2: Illustration of overestimations during learning. In each state (x-axis), there are 10 actions. The left column shows the true values V*(s) (purple line). All true action values are defined by Q* (s, a) = V*(s). The green line shows estimated values Q(s, a) for one action as a function of state, fitted to the true value at several sampled states (green dots). The middle column plots show all the estimated values (green), and the maximum of these values (dashed black). The maximum is higher than the true value (purple, left plot) almost everywhere. The right column plots shows the difference in orange. The blue line in the right plots is the estimate used by Double Q-learning with a second set of samples for each state. The blue line is much closer to zero, indicating less bias. The three rows correspond to different true functions (left, purple) or capacities of the fitted function (left, green). (Details in the text)"
        },
        {
            "bounding_box": [
                {
                    "x": 577,
                    "y": 1331
                },
                {
                    "x": 863,
                    "y": 1331
                },
                {
                    "x": 863,
                    "y": 1387
                },
                {
                    "x": 577,
                    "y": 1387
                }
            ],
            "category": "paragraph",
            "html": "<p id='51' style='font-size:22px'>Double DQN</p>",
            "id": 51,
            "page": 4,
            "text": "Double DQN"
        },
        {
            "bounding_box": [
                {
                    "x": 220,
                    "y": 1391
                },
                {
                    "x": 1233,
                    "y": 1391
                },
                {
                    "x": 1233,
                    "y": 1938
                },
                {
                    "x": 220,
                    "y": 1938
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='52' style='font-size:16px'>The idea of Double Q-learning is to reduce overestimations<br>by decomposing the max operation in the target into action<br>selection and action evaluation. Although not fully decou-<br>pled, the target network in the DQN architecture provides<br>a natural candidate for the second value function, without<br>having to introduce additional networks. We therefore pro-<br>pose to evaluate the greedy policy according to the online<br>network, but using the target network to estimate its value.<br>In reference to both Double Q-learning and DQN, we refer<br>to the resulting algorithm as Double DQN. Its update is the<br>same as for DQN, but replacing the target YDQN with<br>yDoubleDQN</p>",
            "id": 52,
            "page": 4,
            "text": "The idea of Double Q-learning is to reduce overestimations by decomposing the max operation in the target into action selection and action evaluation. Although not fully decoupled, the target network in the DQN architecture provides a natural candidate for the second value function, without having to introduce additional networks. We therefore propose to evaluate the greedy policy according to the online network, but using the target network to estimate its value. In reference to both Double Q-learning and DQN, we refer to the resulting algorithm as Double DQN. Its update is the same as for DQN, but replacing the target YDQN with yDoubleDQN"
        },
        {
            "bounding_box": [
                {
                    "x": 815,
                    "y": 1975
                },
                {
                    "x": 837,
                    "y": 1975
                },
                {
                    "x": 837,
                    "y": 1996
                },
                {
                    "x": 815,
                    "y": 1996
                }
            ],
            "category": "paragraph",
            "html": "<p id='53' style='font-size:14px'>a</p>",
            "id": 53,
            "page": 4,
            "text": "a"
        },
        {
            "bounding_box": [
                {
                    "x": 219,
                    "y": 2008
                },
                {
                    "x": 1226,
                    "y": 2008
                },
                {
                    "x": 1226,
                    "y": 2239
                },
                {
                    "x": 219,
                    "y": 2239
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='54' style='font-size:18px'>In comparison to Double Q-learning (4), the weights of the<br>second network 0't are replaced with the weights of the tar-<br>get network 0t for the evaluation of the current greedy pol-<br>icy. The update to the target network stays unchanged from<br>DQN, and remains a periodic copy of the online network.</p>",
            "id": 54,
            "page": 4,
            "text": "In comparison to Double Q-learning (4), the weights of the second network 0't are replaced with the weights of the target network 0t for the evaluation of the current greedy policy. The update to the target network stays unchanged from DQN, and remains a periodic copy of the online network."
        },
        {
            "bounding_box": [
                {
                    "x": 219,
                    "y": 2242
                },
                {
                    "x": 1224,
                    "y": 2242
                },
                {
                    "x": 1224,
                    "y": 2470
                },
                {
                    "x": 219,
                    "y": 2470
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='55' style='font-size:16px'>This version of Double DQN is perhaps the minimal pos-<br>sible change to DQN towards Double Q-learning. The goal<br>is to get most of the benefit of Double Q-learning, while<br>keeping the rest of the DQN algorithm intact for a fair com-<br>parison, and with minimal computational overhead.</p>",
            "id": 55,
            "page": 4,
            "text": "This version of Double DQN is perhaps the minimal possible change to DQN towards Double Q-learning. The goal is to get most of the benefit of Double Q-learning, while keeping the rest of the DQN algorithm intact for a fair comparison, and with minimal computational overhead."
        },
        {
            "bounding_box": [
                {
                    "x": 534,
                    "y": 2509
                },
                {
                    "x": 907,
                    "y": 2509
                },
                {
                    "x": 907,
                    "y": 2562
                },
                {
                    "x": 534,
                    "y": 2562
                }
            ],
            "category": "paragraph",
            "html": "<p id='56' style='font-size:20px'>Empirical results</p>",
            "id": 56,
            "page": 4,
            "text": "Empirical results"
        },
        {
            "bounding_box": [
                {
                    "x": 218,
                    "y": 2570
                },
                {
                    "x": 1226,
                    "y": 2570
                },
                {
                    "x": 1226,
                    "y": 2844
                },
                {
                    "x": 218,
                    "y": 2844
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='57' style='font-size:16px'>In this section, we analyze the overestimations of DQN and<br>show that Double DQN improves over DQN both in terms of<br>value accuracy and in terms of policy quality. To further test<br>the robustness of the approach we additionally evaluate the<br>algorithms with random starts generated from expert human<br>trajectories, as proposed by Nair et al. (2015).</p>",
            "id": 57,
            "page": 4,
            "text": "In this section, we analyze the overestimations of DQN and show that Double DQN improves over DQN both in terms of value accuracy and in terms of policy quality. To further test the robustness of the approach we additionally evaluate the algorithms with random starts generated from expert human trajectories, as proposed by Nair  (2015)."
        },
        {
            "bounding_box": [
                {
                    "x": 219,
                    "y": 2847
                },
                {
                    "x": 1225,
                    "y": 2847
                },
                {
                    "x": 1225,
                    "y": 2938
                },
                {
                    "x": 219,
                    "y": 2938
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='58' style='font-size:18px'>Our testbed consists of Atari 2600 games, using the Ar-<br>cade Learning Environment (Bellemare et al., 2013). The</p>",
            "id": 58,
            "page": 4,
            "text": "Our testbed consists of Atari 2600 games, using the Arcade Learning Environment (Bellemare , 2013). The"
        },
        {
            "bounding_box": [
                {
                    "x": 1322,
                    "y": 1340
                },
                {
                    "x": 2331,
                    "y": 1340
                },
                {
                    "x": 2331,
                    "y": 1705
                },
                {
                    "x": 1322,
                    "y": 1705
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='59' style='font-size:16px'>goal is for a single algorithm, with a fixed set of hyperpa-<br>rameters, to learn to play each of the games separately from<br>interaction given only the screen pixels as input. This is a de-<br>manding testbed: not only are the inputs high-dimensional,<br>the game visuals and game mechanics vary substantially be-<br>tween games. Good solutions must therefore rely heavily<br>on the learning algorithm - it is not practically feasible to<br>overfit the domain by relying only on tuning.</p>",
            "id": 59,
            "page": 4,
            "text": "goal is for a single algorithm, with a fixed set of hyperparameters, to learn to play each of the games separately from interaction given only the screen pixels as input. This is a demanding testbed: not only are the inputs high-dimensional, the game visuals and game mechanics vary substantially between games. Good solutions must therefore rely heavily on the learning algorithm - it is not practically feasible to overfit the domain by relying only on tuning."
        },
        {
            "bounding_box": [
                {
                    "x": 1324,
                    "y": 1712
                },
                {
                    "x": 2330,
                    "y": 1712
                },
                {
                    "x": 2330,
                    "y": 2125
                },
                {
                    "x": 1324,
                    "y": 2125
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='60' style='font-size:16px'>We closely follow the experimental setting and net-<br>work architecture outlined by Mnih et al. (2015). Briefly,<br>the network architecture is a convolutional neural network<br>(Fukushima, 1988; LeCun et al., 1998) with 3 convolution<br>layers and a fully-connected hidden layer (approximately<br>1.5M parameters in total). The network takes the last four<br>frames as input and outputs the action value of each action.<br>On each game, the network is trained on a single GPU for<br>200M frames, or approximately 1 week.</p>",
            "id": 60,
            "page": 4,
            "text": "We closely follow the experimental setting and network architecture outlined by Mnih  (2015). Briefly, the network architecture is a convolutional neural network (Fukushima, 1988; LeCun , 1998) with 3 convolution layers and a fully-connected hidden layer (approximately 1.5M parameters in total). The network takes the last four frames as input and outputs the action value of each action. On each game, the network is trained on a single GPU for 200M frames, or approximately 1 week."
        },
        {
            "bounding_box": [
                {
                    "x": 1324,
                    "y": 2184
                },
                {
                    "x": 1812,
                    "y": 2184
                },
                {
                    "x": 1812,
                    "y": 2235
                },
                {
                    "x": 1324,
                    "y": 2235
                }
            ],
            "category": "paragraph",
            "html": "<p id='61' style='font-size:18px'>Results on overoptimism</p>",
            "id": 61,
            "page": 4,
            "text": "Results on overoptimism"
        },
        {
            "bounding_box": [
                {
                    "x": 1322,
                    "y": 2265
                },
                {
                    "x": 2331,
                    "y": 2265
                },
                {
                    "x": 2331,
                    "y": 2772
                },
                {
                    "x": 1322,
                    "y": 2772
                }
            ],
            "category": "paragraph",
            "html": "<p id='62' style='font-size:18px'>Figure 3 shows examples of DQN's overestimations in six<br>Atari games. DQN and Double DQN were both trained un-<br>der the exact conditions described by Mnih et al. (2015).<br>DQN is consistently and sometimes vastly overoptimistic<br>about the value of the current greedy policy, as can be seen<br>by comparing the orange learning curves in the top row of<br>plots to the straight orange lines, which represent the ac-<br>tual discounted value of the best learned policy. More pre-<br>cisely, the (averaged) value estimates are computed regu-<br>larly during training with full evaluation phases of length<br>T = 125, 000 steps as</p>",
            "id": 62,
            "page": 4,
            "text": "Figure 3 shows examples of DQN's overestimations in six Atari games. DQN and Double DQN were both trained under the exact conditions described by Mnih  (2015). DQN is consistently and sometimes vastly overoptimistic about the value of the current greedy policy, as can be seen by comparing the orange learning curves in the top row of plots to the straight orange lines, which represent the actual discounted value of the best learned policy. More precisely, the (averaged) value estimates are computed regularly during training with full evaluation phases of length T = 125, 000 steps as"
        },
        {
            "bounding_box": [
                {
                    "x": 541,
                    "y": 202
                },
                {
                    "x": 2021,
                    "y": 202
                },
                {
                    "x": 2021,
                    "y": 1253
                },
                {
                    "x": 541,
                    "y": 1253
                }
            ],
            "category": "figure",
            "html": "<figure><img id='63' style='font-size:14px' alt=\"Alien Space Invaders Time Pilot Zaxxon\nestimates\n20 2.5\n8 DQN estimate\n8 2.0\n6\n15\n1.5 4\n6\nDouble DQN estimate\nValue\n10 1.0 2\nDouble DQN true value\n4 DQN true value\n0\n0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200\nTraining steps (in millions)\nWizard of Wor Asterix\nestimates\n100 80\nscale)\n40 DQN\n10\n20\nDQN\n(log\nValue\n1 10\nDouble DQN\n5 Double DQN\n0 50 100 150 200 0 50 100 150 200\nWizard of Wor Asterix\n4000 Double DQN\n6000\nDouble DQN\n3000\nScore 2000\n4000\n2000\n1000\nDQN DQN\n0 0\n0 50 100 150 200 0 50 100 150 200\nTraining steps (in millions) Training steps (in millions)\" data-coord=\"top-left:(541,202); bottom-right:(2021,1253)\" /></figure>",
            "id": 63,
            "page": 5,
            "text": "Alien Space Invaders Time Pilot Zaxxon estimates 20 2.5 8 DQN estimate 8 2.0 6 15 1.5 4 6 Double DQN estimate Value 10 1.0 2 Double DQN true value 4 DQN true value 0 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 Training steps (in millions) Wizard of Wor Asterix estimates 100 80 scale) 40 DQN 10 20 DQN (log Value 1 10 Double DQN 5 Double DQN 0 50 100 150 200 0 50 100 150 200 Wizard of Wor Asterix 4000 Double DQN 6000 Double DQN 3000 Score 2000 4000 2000 1000 DQN DQN 0 0 0 50 100 150 200 0 50 100 150 200 Training steps (in millions) Training steps (in millions)"
        },
        {
            "bounding_box": [
                {
                    "x": 218,
                    "y": 1261
                },
                {
                    "x": 2334,
                    "y": 1261
                },
                {
                    "x": 2334,
                    "y": 1603
                },
                {
                    "x": 218,
                    "y": 1603
                }
            ],
            "category": "caption",
            "html": "<br><caption id='64' style='font-size:14px'>Figure 3: The top and middle rows show value estimates by DQN (orange) and Double DQN (blue) on six Atari games. The results are<br>obtained by running DQN and Double DQN with 6 different random seeds with the hyper-parameters employed by Mnih et al. (2015). The<br>darker line shows the median over seeds and we average the two extreme values to obtain the shaded area (i.e., 10% and 90% quantiles with<br>linear interpolation). The straight horizontal orange (for DQN) and blue (for Double DQN) lines in the top row are computed by running the<br>corresponding agents after learning concluded, and averaging the actual discounted return obtained from each visited state. These straight<br>lines would match the learning curves at the right side of the plots if there is no bias. The middle row shows the value estimates (in log scale)<br>for two games in which DQN's overoptimism is quite extreme. The bottom row shows the detrimental effect of this on the score achieved by<br>the agent as it is evaluated during training: the scores drop when the overestimations begin. Learning with Double DQN is much more stable.</caption>",
            "id": 64,
            "page": 5,
            "text": "Figure 3: The top and middle rows show value estimates by DQN (orange) and Double DQN (blue) on six Atari games. The results are obtained by running DQN and Double DQN with 6 different random seeds with the hyper-parameters employed by Mnih  (2015). The darker line shows the median over seeds and we average the two extreme values to obtain the shaded area (i.e., 10% and 90% quantiles with linear interpolation). The straight horizontal orange (for DQN) and blue (for Double DQN) lines in the top row are computed by running the corresponding agents after learning concluded, and averaging the actual discounted return obtained from each visited state. These straight lines would match the learning curves at the right side of the plots if there is no bias. The middle row shows the value estimates (in log scale) for two games in which DQN's overoptimism is quite extreme. The bottom row shows the detrimental effect of this on the score achieved by the agent as it is evaluated during training: the scores drop when the overestimations begin. Learning with Double DQN is much more stable."
        },
        {
            "bounding_box": [
                {
                    "x": 219,
                    "y": 1708
                },
                {
                    "x": 1228,
                    "y": 1708
                },
                {
                    "x": 1228,
                    "y": 2262
                },
                {
                    "x": 219,
                    "y": 2262
                }
            ],
            "category": "paragraph",
            "html": "<p id='65' style='font-size:18px'>The ground truth averaged values are obtained by running<br>the best learned policies for several episodes and computing<br>the actual cumulative rewards. Without overestimations we<br>would expect these quantities to match up (i.e., the curve to<br>match the straight line at the right of each plot). Instead, the<br>learning curves of DQN consistently end up much higher<br>than the true values. The learning curves for Double DQN,<br>shown in blue, are much closer to the blue straight line rep-<br>resenting the true value of the final policy. Note that the blue<br>straight line is often higher than the orange straight line. This<br>indicates that Double DQN does not just produce more ac-<br>curate value estimates but also better policies.</p>",
            "id": 65,
            "page": 5,
            "text": "The ground truth averaged values are obtained by running the best learned policies for several episodes and computing the actual cumulative rewards. Without overestimations we would expect these quantities to match up (i.e., the curve to match the straight line at the right of each plot). Instead, the learning curves of DQN consistently end up much higher than the true values. The learning curves for Double DQN, shown in blue, are much closer to the blue straight line representing the true value of the final policy. Note that the blue straight line is often higher than the orange straight line. This indicates that Double DQN does not just produce more accurate value estimates but also better policies."
        },
        {
            "bounding_box": [
                {
                    "x": 216,
                    "y": 2296
                },
                {
                    "x": 1229,
                    "y": 2296
                },
                {
                    "x": 1229,
                    "y": 2937
                },
                {
                    "x": 216,
                    "y": 2937
                }
            ],
            "category": "paragraph",
            "html": "<p id='66' style='font-size:18px'>More extreme overestimations are shown in the middle<br>two plots, where DQN is highly unstable on the games As-<br>terix and Wizard of Wor. Notice the log scale for the values<br>on the y-axis. The bottom two plots shows the correspond-<br>ing scores for these two games. Notice that the increases in<br>value estimates for DQN in the middle plots coincide with<br>decreasing scores in bottom plots. Again, this indicates that<br>the overestimations are harming the quality of the resulting<br>policies. If seen in isolation, one might perhaps be tempted<br>to think the observed instability is related to inherent insta-<br>bility problems of off-policy learning with function approx-<br>imation (Baird, 1995; Tsitsiklis and Van Roy, 1997; Sutton<br>et al., 2008; Maei, 2011; Sutton et al., 2015). However, we<br>see that learning is much more stable with Double DQN,</p>",
            "id": 66,
            "page": 5,
            "text": "More extreme overestimations are shown in the middle two plots, where DQN is highly unstable on the games Asterix and Wizard of Wor. Notice the log scale for the values on the y-axis. The bottom two plots shows the corresponding scores for these two games. Notice that the increases in value estimates for DQN in the middle plots coincide with decreasing scores in bottom plots. Again, this indicates that the overestimations are harming the quality of the resulting policies. If seen in isolation, one might perhaps be tempted to think the observed instability is related to inherent instability problems of off-policy learning with function approximation (Baird, 1995; Tsitsiklis and Van Roy, 1997; Sutton , 2008; Maei, 2011; Sutton , 2015). However, we see that learning is much more stable with Double DQN,"
        },
        {
            "bounding_box": [
                {
                    "x": 1528,
                    "y": 1703
                },
                {
                    "x": 2122,
                    "y": 1703
                },
                {
                    "x": 2122,
                    "y": 1861
                },
                {
                    "x": 1528,
                    "y": 1861
                }
            ],
            "category": "table",
            "html": "<br><table id='67' style='font-size:16px'><tr><td></td><td>DQN</td><td>Double DQN</td></tr><tr><td>Median</td><td>93.5%</td><td>114.7%</td></tr><tr><td>Mean</td><td>241.1%</td><td>330.3%</td></tr></table>",
            "id": 67,
            "page": 5,
            "text": "DQN Double DQN  Median 93.5% 114.7%  Mean 241.1%"
        },
        {
            "bounding_box": [
                {
                    "x": 1326,
                    "y": 1870
                },
                {
                    "x": 2331,
                    "y": 1870
                },
                {
                    "x": 2331,
                    "y": 1957
                },
                {
                    "x": 1326,
                    "y": 1957
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='68' style='font-size:14px'>Table 1: Summary of normalized performance up to 5 minutes of<br>play on 49 games. Results for DQN are from Mnih et al. (2015)</p>",
            "id": 68,
            "page": 5,
            "text": "Table 1: Summary of normalized performance up to 5 minutes of play on 49 games. Results for DQN are from Mnih  (2015)"
        },
        {
            "bounding_box": [
                {
                    "x": 1324,
                    "y": 1981
                },
                {
                    "x": 2328,
                    "y": 1981
                },
                {
                    "x": 2328,
                    "y": 2165
                },
                {
                    "x": 1324,
                    "y": 2165
                }
            ],
            "category": "paragraph",
            "html": "<p id='69' style='font-size:18px'>suggesting that the cause for these instabilities is in fact Q-<br>learning's overoptimism. Figure 3 only shows a few exam-<br>ples, but overestimations were observed for DQN in all 49<br>tested Atari games, albeit in varying amounts.</p>",
            "id": 69,
            "page": 5,
            "text": "suggesting that the cause for these instabilities is in fact Qlearning's overoptimism. Figure 3 only shows a few examples, but overestimations were observed for DQN in all 49 tested Atari games, albeit in varying amounts."
        },
        {
            "bounding_box": [
                {
                    "x": 1326,
                    "y": 2216
                },
                {
                    "x": 1914,
                    "y": 2216
                },
                {
                    "x": 1914,
                    "y": 2266
                },
                {
                    "x": 1326,
                    "y": 2266
                }
            ],
            "category": "paragraph",
            "html": "<p id='70' style='font-size:22px'>Quality of the learned policies</p>",
            "id": 70,
            "page": 5,
            "text": "Quality of the learned policies"
        },
        {
            "bounding_box": [
                {
                    "x": 1323,
                    "y": 2292
                },
                {
                    "x": 2331,
                    "y": 2292
                },
                {
                    "x": 2331,
                    "y": 2658
                },
                {
                    "x": 1323,
                    "y": 2658
                }
            ],
            "category": "paragraph",
            "html": "<p id='71' style='font-size:20px'>Overoptimism does not always adversely affect the quality<br>of the learned policy. For example, DQN achieves optimal<br>behavior in Pong despite slightly overestimating the policy<br>value. Nevertheless, reducing overestimations can signifi-<br>cantly benefit the stability of learning; we see clear examples<br>of this in Figure 3. We now assess more generally how much<br>Double DQN helps in terms of policy quality by evaluating<br>on all 49 games that DQN was tested on.</p>",
            "id": 71,
            "page": 5,
            "text": "Overoptimism does not always adversely affect the quality of the learned policy. For example, DQN achieves optimal behavior in Pong despite slightly overestimating the policy value. Nevertheless, reducing overestimations can significantly benefit the stability of learning; we see clear examples of this in Figure 3. We now assess more generally how much Double DQN helps in terms of policy quality by evaluating on all 49 games that DQN was tested on."
        },
        {
            "bounding_box": [
                {
                    "x": 1322,
                    "y": 2662
                },
                {
                    "x": 2332,
                    "y": 2662
                },
                {
                    "x": 2332,
                    "y": 2939
                },
                {
                    "x": 1322,
                    "y": 2939
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='72' style='font-size:18px'>As described by Mnih et al. (2015) each evaluation<br>episode starts by executing a special no-op action that does<br>not affect the environment up to 30 times, to provide differ-<br>ent starting points for the agent. Some exploration during<br>evaluation provides additional randomization. For Double<br>DQN we used the exact same hyper-parameters as for DQN,</p>",
            "id": 72,
            "page": 5,
            "text": "As described by Mnih  (2015) each evaluation episode starts by executing a special no-op action that does not affect the environment up to 30 times, to provide different starting points for the agent. Some exploration during evaluation provides additional randomization. For Double DQN we used the exact same hyper-parameters as for DQN,"
        },
        {
            "bounding_box": [
                {
                    "x": 243,
                    "y": 220
                },
                {
                    "x": 1204,
                    "y": 220
                },
                {
                    "x": 1204,
                    "y": 384
                },
                {
                    "x": 243,
                    "y": 384
                }
            ],
            "category": "table",
            "html": "<table id='73' style='font-size:18px'><tr><td></td><td>DQN</td><td>Double DQN</td><td>Double DQN (tuned)</td></tr><tr><td>Median</td><td>47.5%</td><td>88.4%</td><td>116.7%</td></tr><tr><td>Mean</td><td>122.0%</td><td>273.1%</td><td>475.2%</td></tr></table>",
            "id": 73,
            "page": 6,
            "text": "DQN Double DQN Double DQN (tuned)  Median 47.5% 88.4% 116.7%  Mean 122.0% 273.1%"
        },
        {
            "bounding_box": [
                {
                    "x": 219,
                    "y": 392
                },
                {
                    "x": 1225,
                    "y": 392
                },
                {
                    "x": 1225,
                    "y": 515
                },
                {
                    "x": 219,
                    "y": 515
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='74' style='font-size:14px'>Table 2: Summary of normalized performance up to 30 minutes<br>of play on 49 games with human starts. Results for DQN are from<br>Nair et al. (2015).</p>",
            "id": 74,
            "page": 6,
            "text": "Table 2: Summary of normalized performance up to 30 minutes of play on 49 games with human starts. Results for DQN are from Nair  (2015)."
        },
        {
            "bounding_box": [
                {
                    "x": 219,
                    "y": 542
                },
                {
                    "x": 1224,
                    "y": 542
                },
                {
                    "x": 1224,
                    "y": 912
                },
                {
                    "x": 219,
                    "y": 912
                }
            ],
            "category": "paragraph",
            "html": "<p id='75' style='font-size:20px'>to allow for a controlled experiment focused just on re-<br>ducing overestimations. The learned policies are evaluated<br>for 5 mins of emulator time (18,000 frames) with an 6-<br>greedy policy where E = 0.05. The scores are averaged over<br>100 episodes. The only difference between Double DQN<br>and DQN is the target, using y,DoubleDQN rather than YDQN<br>This evaluation is somewhat adversarial, as the used hyper-<br>parameters were tuned for DQN but not for Double DQN.</p>",
            "id": 75,
            "page": 6,
            "text": "to allow for a controlled experiment focused just on reducing overestimations. The learned policies are evaluated for 5 mins of emulator time (18,000 frames) with an 6greedy policy where E = 0.05. The scores are averaged over 100 episodes. The only difference between Double DQN and DQN is the target, using y,DoubleDQN rather than YDQN This evaluation is somewhat adversarial, as the used hyperparameters were tuned for DQN but not for Double DQN."
        },
        {
            "bounding_box": [
                {
                    "x": 218,
                    "y": 915
                },
                {
                    "x": 1224,
                    "y": 915
                },
                {
                    "x": 1224,
                    "y": 1005
                },
                {
                    "x": 218,
                    "y": 1005
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='76' style='font-size:16px'>To obtain summary statistics across games, we normalize<br>the score for each game as follows:</p>",
            "id": 76,
            "page": 6,
            "text": "To obtain summary statistics across games, we normalize the score for each game as follows:"
        },
        {
            "bounding_box": [
                {
                    "x": 218,
                    "y": 1148
                },
                {
                    "x": 1221,
                    "y": 1148
                },
                {
                    "x": 1221,
                    "y": 1237
                },
                {
                    "x": 218,
                    "y": 1237
                }
            ],
            "category": "paragraph",
            "html": "<p id='77' style='font-size:16px'>The 'random' and 'human' scores are the same as used by<br>Mnih et al. (2015), and are given in the appendix.</p>",
            "id": 77,
            "page": 6,
            "text": "The 'random' and 'human' scores are the same as used by Mnih  (2015), and are given in the appendix."
        },
        {
            "bounding_box": [
                {
                    "x": 218,
                    "y": 1241
                },
                {
                    "x": 1226,
                    "y": 1241
                },
                {
                    "x": 1226,
                    "y": 1555
                },
                {
                    "x": 218,
                    "y": 1555
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='78' style='font-size:22px'>Table 1, under no ops, shows that on the whole Double<br>DQN clearly improves over DQN. A detailed comparison<br>(in appendix) shows that there are several games in which<br>Double DQN greatly improves upon DQN. Noteworthy ex-<br>amples include Road Runner (from 233% to 617%), Asterix<br>(from 70% to 180%), Zaxxon (from 54% to 111%), and<br>Double Dunk (from 17% to 397%).</p>",
            "id": 78,
            "page": 6,
            "text": "Table 1, under no ops, shows that on the whole Double DQN clearly improves over DQN. A detailed comparison (in appendix) shows that there are several games in which Double DQN greatly improves upon DQN. Noteworthy examples include Road Runner (from 233% to 617%), Asterix (from 70% to 180%), Zaxxon (from 54% to 111%), and Double Dunk (from 17% to 397%)."
        },
        {
            "bounding_box": [
                {
                    "x": 219,
                    "y": 1560
                },
                {
                    "x": 1225,
                    "y": 1560
                },
                {
                    "x": 1225,
                    "y": 1833
                },
                {
                    "x": 219,
                    "y": 1833
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='79' style='font-size:20px'>The Gorila algorithm (Nair et al., 2015), which is a mas-<br>sively distributed version of DQN, is not included in the ta-<br>ble because the architecture and infrastructure is sufficiently<br>different to make a direct comparison unclear. For complete-<br>ness, we note that Gorila obtained median and mean normal-<br>ized scores of 96% and 495%, respectively.</p>",
            "id": 79,
            "page": 6,
            "text": "The Gorila algorithm (Nair , 2015), which is a massively distributed version of DQN, is not included in the table because the architecture and infrastructure is sufficiently different to make a direct comparison unclear. For completeness, we note that Gorila obtained median and mean normalized scores of 96% and 495%, respectively."
        },
        {
            "bounding_box": [
                {
                    "x": 221,
                    "y": 1869
                },
                {
                    "x": 775,
                    "y": 1869
                },
                {
                    "x": 775,
                    "y": 1919
                },
                {
                    "x": 221,
                    "y": 1919
                }
            ],
            "category": "paragraph",
            "html": "<p id='80' style='font-size:22px'>Robustness to Human starts</p>",
            "id": 80,
            "page": 6,
            "text": "Robustness to Human starts"
        },
        {
            "bounding_box": [
                {
                    "x": 219,
                    "y": 1932
                },
                {
                    "x": 1226,
                    "y": 1932
                },
                {
                    "x": 1226,
                    "y": 2296
                },
                {
                    "x": 219,
                    "y": 2296
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='81' style='font-size:20px'>One concern with the previous evaluation is that in deter-<br>ministic games with a unique starting point the learner could<br>potentially learn to remember sequences of actions with-<br>out much need to generalize. While successful, the solution<br>would not be particularly robust. By testing the agents from<br>various starting points, we can test whether the found SO-<br>lutions generalize well, and as such provide a challenging<br>testbed for the learned polices (Nair et al., 2015).</p>",
            "id": 81,
            "page": 6,
            "text": "One concern with the previous evaluation is that in deterministic games with a unique starting point the learner could potentially learn to remember sequences of actions without much need to generalize. While successful, the solution would not be particularly robust. By testing the agents from various starting points, we can test whether the found SOlutions generalize well, and as such provide a challenging testbed for the learned polices (Nair , 2015)."
        },
        {
            "bounding_box": [
                {
                    "x": 218,
                    "y": 2298
                },
                {
                    "x": 1226,
                    "y": 2298
                },
                {
                    "x": 1226,
                    "y": 2614
                },
                {
                    "x": 218,
                    "y": 2614
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='82' style='font-size:22px'>We obtained 100 starting points sampled for each game<br>from a human expert's trajectory, as proposed by Nair et al.<br>(2015). We start an evaluation episode from each of these<br>starting points and run the emulator for up to 108,000 frames<br>(30 mins at 60Hz including the trajectory before the starting<br>point). Each agent is only evaluated on the rewards accumu-<br>lated after the starting point.</p>",
            "id": 82,
            "page": 6,
            "text": "We obtained 100 starting points sampled for each game from a human expert's trajectory, as proposed by Nair  (2015). We start an evaluation episode from each of these starting points and run the emulator for up to 108,000 frames (30 mins at 60Hz including the trajectory before the starting point). Each agent is only evaluated on the rewards accumulated after the starting point."
        },
        {
            "bounding_box": [
                {
                    "x": 219,
                    "y": 2618
                },
                {
                    "x": 1226,
                    "y": 2618
                },
                {
                    "x": 1226,
                    "y": 2939
                },
                {
                    "x": 219,
                    "y": 2939
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='83' style='font-size:20px'>For this evaluation we include a tuned version of Double<br>DQN. Some tuning is appropriate because the hyperparame-<br>ters were tuned for DQN, which is a different algorithm. For<br>the tuned version of Double DQN, we increased the num-<br>ber of frames between each two copies of the target network<br>from 10,000 to 30,000, to reduce overestimations further be-<br>cause immediately after each switch DQN and Double DQN</p>",
            "id": 83,
            "page": 6,
            "text": "For this evaluation we include a tuned version of Double DQN. Some tuning is appropriate because the hyperparameters were tuned for DQN, which is a different algorithm. For the tuned version of Double DQN, we increased the number of frames between each two copies of the target network from 10,000 to 30,000, to reduce overestimations further because immediately after each switch DQN and Double DQN"
        },
        {
            "bounding_box": [
                {
                    "x": 1358,
                    "y": 241
                },
                {
                    "x": 2225,
                    "y": 241
                },
                {
                    "x": 2225,
                    "y": 1878
                },
                {
                    "x": 1358,
                    "y": 1878
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='84' style='font-size:14px' alt=\"Video Pinball\nAtlantis\nDemon Attack\nBreakout\nAssault\nDouble Dunk\nRobotank\nGopher\nBoxing\nStar Gunner\nRoad Runner\nKrull\nCrazy Climber\nKangaroo\nAsterix\n**Defender**\n**Phoenix**\nUp and Down\nSpace Invaders\nJames Bond\nEnduro\nKung-Fu Master\nWizard of Wor\nName This Game\nTime Pilot\nBank Heist\nBeam Rider\nFreeway\nPong\nZaxxon\nFishing Derby\nTennis\nQ*Bert\n**Surround**\nRiver Raid\nBattle Zone\nIce Hockey\nTutankham\nH.E.R.O.\n**Berzerk**\nSeaquest\nChopper Command\nFrostbite\n**Skiing**\nBowling\nCentipede\nAlien\n** Yars Revenge**\nAmidar :\nMs. Pacman Human\n**Pitfall**\nAsteroids\nMontezuma's Revenge\nDouble DQN (tuned)\nVenture :\nGravitar Double DQN\nPrivate Eye DQN\n**Solaris**\n300%\n200%\n7500%\n0%\n1000%\n400%\n5000%\n500%\n100%\n2500%\n1500%\n2000\nNormalized score\" data-coord=\"top-left:(1358,241); bottom-right:(2225,1878)\" /></figure>",
            "id": 84,
            "page": 6,
            "text": "Video Pinball Atlantis Demon Attack Breakout Assault Double Dunk Robotank Gopher Boxing Star Gunner Road Runner Krull Crazy Climber Kangaroo Asterix **Defender** **Phoenix** Up and Down Space Invaders James Bond Enduro Kung-Fu Master Wizard of Wor Name This Game Time Pilot Bank Heist Beam Rider Freeway Pong Zaxxon Fishing Derby Tennis Q*Bert **Surround** River Raid Battle Zone Ice Hockey Tutankham H.E.R.O. **Berzerk** Seaquest Chopper Command Frostbite **Skiing** Bowling Centipede Alien ** Yars Revenge** Amidar : Ms. Pacman Human **Pitfall** Asteroids Montezuma's Revenge Double DQN (tuned) Venture : Gravitar Double DQN Private Eye DQN **Solaris** 300% 200% 7500% 0% 1000% 400% 5000% 500% 100% 2500% 1500% 2000 Normalized score"
        },
        {
            "bounding_box": [
                {
                    "x": 1322,
                    "y": 1901
                },
                {
                    "x": 2333,
                    "y": 1901
                },
                {
                    "x": 2333,
                    "y": 2090
                },
                {
                    "x": 1322,
                    "y": 2090
                }
            ],
            "category": "caption",
            "html": "<br><caption id='85' style='font-size:18px'>Figure 4: Normalized scores on 57 Atari games, tested<br>for 100 episodes per game with human starts. Compared<br>to Mnih et al. (2015), eight games additional games were<br>tested. These are indicated with stars and a bold font.</caption>",
            "id": 85,
            "page": 6,
            "text": "Figure 4: Normalized scores on 57 Atari games, tested for 100 episodes per game with human starts. Compared to Mnih  (2015), eight games additional games were tested. These are indicated with stars and a bold font."
        },
        {
            "bounding_box": [
                {
                    "x": 1323,
                    "y": 2124
                },
                {
                    "x": 2332,
                    "y": 2124
                },
                {
                    "x": 2332,
                    "y": 2401
                },
                {
                    "x": 1323,
                    "y": 2401
                }
            ],
            "category": "paragraph",
            "html": "<p id='86' style='font-size:18px'>both revert to Q-learning. In addition, we reduced the explo-<br>ration during learning from E = 0.1 to E = 0.01, and then<br>used E = 0.001 during evaluation. Finally, the tuned ver-<br>sion uses a single shared bias for all action values in the top<br>layer of the network. Each of these changes improved per-<br>formance and together they result in clearly better results. 3</p>",
            "id": 86,
            "page": 6,
            "text": "both revert to Q-learning. In addition, we reduced the exploration during learning from E = 0.1 to E = 0.01, and then used E = 0.001 during evaluation. Finally, the tuned version uses a single shared bias for all action values in the top layer of the network. Each of these changes improved performance and together they result in clearly better results. 3"
        },
        {
            "bounding_box": [
                {
                    "x": 1323,
                    "y": 2402
                },
                {
                    "x": 2332,
                    "y": 2402
                },
                {
                    "x": 2332,
                    "y": 2815
                },
                {
                    "x": 1323,
                    "y": 2815
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='87' style='font-size:20px'>Table 2 reports summary statistics for this evaluation on<br>the 49 games from Mnih et al. (2015). Double DQN ob-<br>tains clearly higher median and mean scores. Again Gorila<br>DQN (Nair et al., 2015) is not included in the table, but for<br>completeness note it obtained a median of 78% and a mean<br>of 259%. Detailed results, plus results for an additional 8<br>games, are available in Figure 4 and in the appendix. On<br>several games the improvements from DQN to Double DQN<br>are striking, in some cases bringing scores much closer to</p>",
            "id": 87,
            "page": 6,
            "text": "Table 2 reports summary statistics for this evaluation on the 49 games from Mnih  (2015). Double DQN obtains clearly higher median and mean scores. Again Gorila DQN (Nair , 2015) is not included in the table, but for completeness note it obtained a median of 78% and a mean of 259%. Detailed results, plus results for an additional 8 games, are available in Figure 4 and in the appendix. On several games the improvements from DQN to Double DQN are striking, in some cases bringing scores much closer to"
        },
        {
            "bounding_box": [
                {
                    "x": 1325,
                    "y": 2849
                },
                {
                    "x": 2330,
                    "y": 2849
                },
                {
                    "x": 2330,
                    "y": 2936
                },
                {
                    "x": 1325,
                    "y": 2936
                }
            ],
            "category": "paragraph",
            "html": "<p id='88' style='font-size:18px'>3Except for Tennis, where the lower E during training seemed<br>to hurt rather than help.</p>",
            "id": 88,
            "page": 6,
            "text": "3Except for Tennis, where the lower E during training seemed to hurt rather than help."
        },
        {
            "bounding_box": [
                {
                    "x": 219,
                    "y": 235
                },
                {
                    "x": 775,
                    "y": 235
                },
                {
                    "x": 775,
                    "y": 276
                },
                {
                    "x": 219,
                    "y": 276
                }
            ],
            "category": "paragraph",
            "html": "<p id='89' style='font-size:14px'>human, or even surpassing these.</p>",
            "id": 89,
            "page": 7,
            "text": "human, or even surpassing these."
        },
        {
            "bounding_box": [
                {
                    "x": 218,
                    "y": 280
                },
                {
                    "x": 1226,
                    "y": 280
                },
                {
                    "x": 1226,
                    "y": 553
                },
                {
                    "x": 218,
                    "y": 553
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='90' style='font-size:14px'>Double DQN appears more robust to this more challeng-<br>ing evaluation, suggesting that appropriate generalizations<br>occur and that the found solutions do not exploit the deter-<br>minism of the environments. This is appealing, as it indi-<br>cates progress towards finding general solutions rather than<br>a deterministic sequence of steps that would be less robust.</p>",
            "id": 90,
            "page": 7,
            "text": "Double DQN appears more robust to this more challenging evaluation, suggesting that appropriate generalizations occur and that the found solutions do not exploit the determinism of the environments. This is appealing, as it indicates progress towards finding general solutions rather than a deterministic sequence of steps that would be less robust."
        },
        {
            "bounding_box": [
                {
                    "x": 605,
                    "y": 593
                },
                {
                    "x": 837,
                    "y": 593
                },
                {
                    "x": 837,
                    "y": 642
                },
                {
                    "x": 605,
                    "y": 642
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='91' style='font-size:18px'>Discussion</p>",
            "id": 91,
            "page": 7,
            "text": "Discussion"
        },
        {
            "bounding_box": [
                {
                    "x": 218,
                    "y": 654
                },
                {
                    "x": 1226,
                    "y": 654
                },
                {
                    "x": 1226,
                    "y": 1339
                },
                {
                    "x": 218,
                    "y": 1339
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='92' style='font-size:14px'>This paper has five contributions. First, we have shown why<br>Q-learning can be overoptimistic in large-scale problems,<br>even if these are deterministic, due to the inherent estima-<br>tion errors of learning. Second, by analyzing the value es-<br>timates on Atari games we have shown that these overesti-<br>mations are more common and severe in practice than pre-<br>viously acknowledged. Third, we have shown that Double<br>Q-learning can be used at scale to successfully reduce this<br>overoptimism, resulting in more stable and reliable learning.<br>Fourth, we have proposed a specific implementation called<br>Double DQN, that uses the existing architecture and deep<br>neural network of the DQN algorithm without requiring ad-<br>ditional networks or parameters. Finally, we have shown that<br>Double DQN finds better policies, obtaining new state-of-<br>the-art results on the Atari 2600 domain.</p>",
            "id": 92,
            "page": 7,
            "text": "This paper has five contributions. First, we have shown why Q-learning can be overoptimistic in large-scale problems, even if these are deterministic, due to the inherent estimation errors of learning. Second, by analyzing the value estimates on Atari games we have shown that these overestimations are more common and severe in practice than previously acknowledged. Third, we have shown that Double Q-learning can be used at scale to successfully reduce this overoptimism, resulting in more stable and reliable learning. Fourth, we have proposed a specific implementation called Double DQN, that uses the existing architecture and deep neural network of the DQN algorithm without requiring additional networks or parameters. Finally, we have shown that Double DQN finds better policies, obtaining new state-ofthe-art results on the Atari 2600 domain."
        },
        {
            "bounding_box": [
                {
                    "x": 525,
                    "y": 1382
                },
                {
                    "x": 918,
                    "y": 1382
                },
                {
                    "x": 918,
                    "y": 1432
                },
                {
                    "x": 525,
                    "y": 1432
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='93' style='font-size:22px'>Acknowledgments</p>",
            "id": 93,
            "page": 7,
            "text": "Acknowledgments"
        },
        {
            "bounding_box": [
                {
                    "x": 219,
                    "y": 1438
                },
                {
                    "x": 1224,
                    "y": 1438
                },
                {
                    "x": 1224,
                    "y": 1608
                },
                {
                    "x": 219,
                    "y": 1608
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='94' style='font-size:16px'>We would like to thank Tom Schaul, Volodymyr Mnih, Marc<br>Bellemare, Thomas Degris, Georg Ostrovski, and Richard<br>Sutton for helpful comments, and everyone at Google Deep-<br>Mind for a constructive research environment.</p>",
            "id": 94,
            "page": 7,
            "text": "We would like to thank Tom Schaul, Volodymyr Mnih, Marc Bellemare, Thomas Degris, Georg Ostrovski, and Richard Sutton for helpful comments, and everyone at Google DeepMind for a constructive research environment."
        },
        {
            "bounding_box": [
                {
                    "x": 602,
                    "y": 1646
                },
                {
                    "x": 840,
                    "y": 1646
                },
                {
                    "x": 840,
                    "y": 1695
                },
                {
                    "x": 602,
                    "y": 1695
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='95' style='font-size:20px'>References</p>",
            "id": 95,
            "page": 7,
            "text": "References"
        },
        {
            "bounding_box": [
                {
                    "x": 216,
                    "y": 1704
                },
                {
                    "x": 1231,
                    "y": 1704
                },
                {
                    "x": 1231,
                    "y": 2938
                },
                {
                    "x": 216,
                    "y": 2938
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='96' style='font-size:14px'>R. Agrawal. Sample mean based index policies with O(log n) re-<br>gret for the multi-armed bandit problem. Advances in Applied<br>Probability, pages 1054-1078, 1995.<br>P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the<br>multiarmed bandit problem. Machine learning, 47(2-3):235-<br>256, 2002.<br>L. Baird. Residual algorithms: Reinforcement learning with func-<br>tion approximation. In Machine Learning: Proceedings of the<br>Twelfth International Conference, pages 30-37, 1995.<br>M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The ar-<br>cade learning environment: An evaluation platform for general<br>agents. J. Artif. Intell. Res. (JAIR), 47:253-279, 2013.<br>R. I. Brafman and M. Tennenholtz. R-max-a general polynomial<br>time algorithm for near-optimal reinforcement learning. The<br>Journal of Machine Learning Research, 3:213-231, 2003.<br>K. Fukushima. Neocognitron: A hierarchical neural network ca-<br>pable of visual pattern recognition. Neural networks, 1(2):119-<br>130, 1988.<br>L. P. Kaelbling, M. L. Littman, and A. W. Moore. Reinforcement<br>learning: A survey. Journal of Artificial Intelligence Research,<br>4:237-285, 1996.<br>Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based<br>learning applied to document recognition. Proceedings of the<br>IEEE, 86(11):2278-2324, 1998.<br>L. Lin. Self-improving reactive agents based on reinforcement<br>learning, planning and teaching. Machine learning, 8(3):293-<br>321, 1992.</p>",
            "id": 96,
            "page": 7,
            "text": "R. Agrawal. Sample mean based index policies with O(log n) regret for the multi-armed bandit problem. Advances in Applied Probability, pages 1054-1078, 1995. P. Auer, N. Cesa-Bianchi, and P. Fischer. Finite-time analysis of the multiarmed bandit problem. Machine learning, 47(2-3):235256, 2002. L. Baird. Residual algorithms: Reinforcement learning with function approximation. In Machine Learning: Proceedings of the Twelfth International Conference, pages 30-37, 1995. M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An evaluation platform for general agents. J. Artif. Intell. Res. (JAIR), 47:253-279, 2013. R. I. Brafman and M. Tennenholtz. R-max-a general polynomial time algorithm for near-optimal reinforcement learning. The Journal of Machine Learning Research, 3:213-231, 2003. K. Fukushima. Neocognitron: A hierarchical neural network capable of visual pattern recognition. Neural networks, 1(2):119130, 1988. L. P. Kaelbling, M. L. Littman, and A. W. Moore. Reinforcement learning: A survey. Journal of Artificial Intelligence Research, 4:237-285, 1996. Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998. L. Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching. Machine learning, 8(3):293321, 1992."
        },
        {
            "bounding_box": [
                {
                    "x": 1327,
                    "y": 227
                },
                {
                    "x": 2330,
                    "y": 227
                },
                {
                    "x": 2330,
                    "y": 2928
                },
                {
                    "x": 1327,
                    "y": 2928
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='97' style='font-size:14px'>H. R. Maei. Gradient temporal-difference learning algorithms.<br>PhD thesis, University of Alberta, 2011.<br>V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.<br>Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostro-<br>vski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King,<br>D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-<br>level control through deep reinforcement learning. Nature, 518<br>(7540):529-533, 2015.<br>A. Nair, P. Srinivasan, S. Blackwell, C. Alcicek, R. Fearon, A. D.<br>Maria, V. Panneershelvam, M. Suleyman, C. Beattie, S. Pe-<br>tersen, S. Legg, V. Mnih, K. Kavukcuoglu, and D. Silver. Mas-<br>sively parallel methods for deep reinforcement learning. In Deep<br>Learning Workshop, ICML, 2015.<br>M. Riedmiller. Neural fitted Q iteration - first experiences with a<br>data efficient neural reinforcement learning method. In J. Gama,<br>R. Camacho, P. Brazdil, A. Jorge, and L. Torgo, editors, Pro-<br>ceedings of the 16th European Conference on Machine Learning<br>(ECML '05), pages 317-328. Springer, 2005.<br>B. Sallans and G. E. Hinton. Reinforcement learning with factored<br>states and actions. The Journal of Machine Learning Research,<br>5:1063-1088, 2004.<br>A. L. Strehl, L. Li, and M. L. Littman. Reinforcement learning in<br>finite MDPs: PAC analysis. The Journal of Machine Learning<br>Research, 10:2413-2444, 2009.<br>R. S. Sutton. Learning to predict by the methods of temporal dif-<br>ferences. Machine learning, 3(1):9-44, 1988.<br>R. S. Sutton. Integrated architectures for learning, planning, and<br>reacting based on approximating dynamic programming. In<br>Proceedings of the seventh international conference on machine<br>learning, pages 216-224, 1990.<br>R. S. Sutton and A. G. Barto. Introduction to reinforcement learn-<br>ing. MIT Press, 1998.<br>R. S. Sutton, C. Szepesv�ri, and H. R. Maei. A convergent O(n)<br>algorithm for off-policy temporal-difference learning with lin-<br>ear function approximation. Advances in Neural Information<br>Processing Systems 21 (NIPS-08), 21:1609-1616, 2008.<br>R. S. Sutton, A. R. Mahmood, and M. White. An emphatic ap-<br>proach to the problem of off-policy temporal-difference learn-<br>ing. arXiv preprint arXiv:1503.04269, 2015.<br>I. Szita and A. L�rincz. The many faces of optimism: a unifying<br>approach. In Proceedings of the 25th international conference<br>on Machine learning, pages 1048-1055. ACM, 2008.<br>G. Tesauro. Temporal difference learning and td-gammon. Com-<br>munications of the ACM, 38(3):58-68, 1995.<br>S. Thrun and A. Schwartz. Issues in using function approxima-<br>tion for reinforcement learning. In M. Mozer, P. Smolensky,<br>D. Touretzky, J. Elman, and A. Weigend, editors, Proceedings<br>ofthe 1993 Connectionist Models Summer School, Hillsdale, NJ,<br>1993. Lawrence Erlbaum.<br>J.N. Tsitsiklis and B. Van Roy. An analysis of temporal-difference<br>learning with function approximation. IEEE Transactions on<br>Automatic Control, 42(5):674-690, 1997.<br>H. van Hasselt. Double Q-learning. Advances in Neural Informa-<br>tion Processing Systems, 23:2613-2621, 2010.<br>H. van Hasselt. Insights in Reinforcement Learning. PhD thesis,<br>Utrecht University, 2011.<br>C. J. C. H. Watkins. Learning from delayed rewards. PhD thesis,<br>University of Cambridge England, 1989.</p>",
            "id": 97,
            "page": 7,
            "text": "H. R. Maei. Gradient temporal-difference learning algorithms. PhD thesis, University of Alberta, 2011. V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Humanlevel control through deep reinforcement learning. Nature, 518 (7540):529-533, 2015. A. Nair, P. Srinivasan, S. Blackwell, C. Alcicek, R. Fearon, A. D. Maria, V. Panneershelvam, M. Suleyman, C. Beattie, S. Petersen, S. Legg, V. Mnih, K. Kavukcuoglu, and D. Silver. Massively parallel methods for deep reinforcement learning. In Deep Learning Workshop, ICML, 2015. M. Riedmiller. Neural fitted Q iteration - first experiences with a data efficient neural reinforcement learning method. In J. Gama, R. Camacho, P. Brazdil, A. Jorge, and L. Torgo, editors, Proceedings of the 16th European Conference on Machine Learning (ECML '05), pages 317-328. Springer, 2005. B. Sallans and G. E. Hinton. Reinforcement learning with factored states and actions. The Journal of Machine Learning Research, 5:1063-1088, 2004. A. L. Strehl, L. Li, and M. L. Littman. Reinforcement learning in finite MDPs: PAC analysis. The Journal of Machine Learning Research, 10:2413-2444, 2009. R. S. Sutton. Learning to predict by the methods of temporal differences. Machine learning, 3(1):9-44, 1988. R. S. Sutton. Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. In Proceedings of the seventh international conference on machine learning, pages 216-224, 1990. R. S. Sutton and A. G. Barto. Introduction to reinforcement learning. MIT Press, 1998. R. S. Sutton, C. Szepesv�ri, and H. R. Maei. A convergent O(n) algorithm for off-policy temporal-difference learning with linear function approximation. Advances in Neural Information Processing Systems 21 (NIPS-08), 21:1609-1616, 2008. R. S. Sutton, A. R. Mahmood, and M. White. An emphatic approach to the problem of off-policy temporal-difference learning. arXiv preprint arXiv:1503.04269, 2015. I. Szita and A. L�rincz. The many faces of optimism: a unifying approach. In Proceedings of the 25th international conference on Machine learning, pages 1048-1055. ACM, 2008. G. Tesauro. Temporal difference learning and td-gammon. Communications of the ACM, 38(3):58-68, 1995. S. Thrun and A. Schwartz. Issues in using function approximation for reinforcement learning. In M. Mozer, P. Smolensky, D. Touretzky, J. Elman, and A. Weigend, editors, Proceedings ofthe 1993 Connectionist Models Summer School, Hillsdale, NJ, 1993. Lawrence Erlbaum. J.N. Tsitsiklis and B. Van Roy. An analysis of temporal-difference learning with function approximation. IEEE Transactions on Automatic Control, 42(5):674-690, 1997. H. van Hasselt. Double Q-learning. Advances in Neural Information Processing Systems, 23:2613-2621, 2010. H. van Hasselt. Insights in Reinforcement Learning. PhD thesis, Utrecht University, 2011. C. J. C. H. Watkins. Learning from delayed rewards. PhD thesis, University of Cambridge England, 1989."
        },
        {
            "bounding_box": [
                {
                    "x": 616,
                    "y": 230
                },
                {
                    "x": 827,
                    "y": 230
                },
                {
                    "x": 827,
                    "y": 280
                },
                {
                    "x": 616,
                    "y": 280
                }
            ],
            "category": "paragraph",
            "html": "<p id='98' style='font-size:22px'>Appendix</p>",
            "id": 98,
            "page": 8,
            "text": "Appendix"
        },
        {
            "bounding_box": [
                {
                    "x": 217,
                    "y": 289
                },
                {
                    "x": 1227,
                    "y": 289
                },
                {
                    "x": 1227,
                    "y": 699
                },
                {
                    "x": 217,
                    "y": 699
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='99' style='font-size:16px'>Theorem 1. Consider a state s in which all the true optimal ac-<br>tion values are equal at Q*(S, a) = V*(s) for some V*(s). Let<br>Qt be arbitrary value estimates that are on the whole unbiased in<br>the sense that �a (Qt(s,a) - V*(s)) = 0, but that are not all<br>zero, such that � �a(Qt(s,a) - V*(s))2 = C for some C > 0,<br>where m ≥ 2isthe numberofactions in s. Under these conditions,<br>C<br>This lower bound is tight. Un-<br>maxa Qt(s,a)≥V*(s)+ V m-1.<br>der the same conditions, the lower bound on the absolute error of<br>the Double Q-learning estimate is zero.</p>",
            "id": 99,
            "page": 8,
            "text": "Theorem 1. Consider a state s in which all the true optimal action values are equal at Q*(S, a) = V*(s) for some V*(s). Let Qt be arbitrary value estimates that are on the whole unbiased in the sense that �a (Qt(s,a) - V*(s)) = 0, but that are not all zero, such that � �a(Qt(s,a) - V*(s))2 = C for some C > 0, where m ≥ 2isthe numberofactions in s. Under these conditions, C This lower bound is tight. Unmaxa Qt(s,a)≥V*(s)+ V m-1. der the same conditions, the lower bound on the absolute error of the Double Q-learning estimate is zero."
        },
        {
            "bounding_box": [
                {
                    "x": 216,
                    "y": 722
                },
                {
                    "x": 1227,
                    "y": 722
                },
                {
                    "x": 1227,
                    "y": 1262
                },
                {
                    "x": 216,
                    "y": 1262
                }
            ],
            "category": "paragraph",
            "html": "<p id='100' style='font-size:16px'>Proof ofTheorem 1. Define the errors for each action a as Ea<br>Qt(s,a) - V*(s). Suppose that there exists a setting of {Ea} such<br>that maxa Ea < V m-1. Let {�} be the set of positive E of size<br>n, and {E,} the set of strictly negative 6 of size m - n (such<br>that {€} = {�} U {E,}). If n = m, then �a Ea = 0<br>Ea = 0 Va, which contradicts �a €2 = mC. Hence, it must be<br>C<br>that n ≤ m - 1. Then, �i=1 et ≤ nmaxiet < nV m-1'<br>and therefore (using the constraint �a Ea = 0) we also have that<br>C implies maxj lej|<nv m-1 By<br>C<br>Em=1 n 10,1 < nV m-1<br>This<br>Holder's inequality, then</p>",
            "id": 100,
            "page": 8,
            "text": "Proof ofTheorem 1. Define the errors for each action a as Ea Qt(s,a) - V*(s). Suppose that there exists a setting of {Ea} such that maxa Ea < V m-1. Let {�} be the set of positive E of size n, and {E,} the set of strictly negative 6 of size m - n (such that {€} = {�} U {E,}). If n = m, then �a Ea = 0 Ea = 0 Va, which contradicts �a €2 = mC. Hence, it must be C that n ≤ m - 1. Then, �i=1 et ≤ nmaxiet < nV m-1' and therefore (using the constraint �a Ea = 0) we also have that C implies maxj lej|<nv m-1 By C Em=1 n 10,1 < nV m-1 This Holder's inequality, then"
        },
        {
            "bounding_box": [
                {
                    "x": 220,
                    "y": 1541
                },
                {
                    "x": 1224,
                    "y": 1541
                },
                {
                    "x": 1224,
                    "y": 1623
                },
                {
                    "x": 220,
                    "y": 1623
                }
            ],
            "category": "caption",
            "html": "<caption id='101' style='font-size:14px'>We can now combine these relations to compute an upper-bound<br>on the sum of squares for all Ea:</caption>",
            "id": 101,
            "page": 8,
            "text": "We can now combine these relations to compute an upper-bound on the sum of squares for all Ea:"
        },
        {
            "bounding_box": [
                {
                    "x": 217,
                    "y": 2059
                },
                {
                    "x": 1226,
                    "y": 2059
                },
                {
                    "x": 1226,
                    "y": 2343
                },
                {
                    "x": 217,
                    "y": 2343
                }
            ],
            "category": "paragraph",
            "html": "<p id='102' style='font-size:14px'>This contradicts the assumption that �a=1 €2 < mC, and there-<br>C settings of E that satisfy the con-<br>fore maxa Ea ≥ for all<br>V m-1<br>straints. We can check that the lower-bound is tight by setting<br>C -V(m - 1)C.<br>for a = 1, . · · , m - 1 and Em =<br>Ea = V m-1<br>This verifies �a €2a = mC and �a Ea = 0.</p>",
            "id": 102,
            "page": 8,
            "text": "This contradicts the assumption that �a=1 €2 < mC, and thereC settings of E that satisfy the confore maxa Ea ≥ for all V m-1 straints. We can check that the lower-bound is tight by setting C -V(m - 1)C. for a = 1, . · · , m - 1 and Em = Ea = V m-1 This verifies �a €2a = mC and �a Ea = 0."
        },
        {
            "bounding_box": [
                {
                    "x": 219,
                    "y": 2340
                },
                {
                    "x": 1223,
                    "y": 2340
                },
                {
                    "x": 1223,
                    "y": 2464
                },
                {
                    "x": 219,
                    "y": 2464
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='103' style='font-size:16px'>The only tight lower bound on the absolute error for Double Q-<br>learning |Q't(s, argmaxa Qt(s,a)) - V*(s)| is zero. This can be<br>seen by because we can have</p>",
            "id": 103,
            "page": 8,
            "text": "The only tight lower bound on the absolute error for Double Qlearning |Q't(s, argmaxa Qt(s,a)) - V*(s)| is zero. This can be seen by because we can have"
        },
        {
            "bounding_box": [
                {
                    "x": 219,
                    "y": 2612
                },
                {
                    "x": 284,
                    "y": 2612
                },
                {
                    "x": 284,
                    "y": 2646
                },
                {
                    "x": 219,
                    "y": 2646
                }
            ],
            "category": "paragraph",
            "html": "<p id='104' style='font-size:14px'>and</p>",
            "id": 104,
            "page": 8,
            "text": "and"
        },
        {
            "bounding_box": [
                {
                    "x": 219,
                    "y": 2810
                },
                {
                    "x": 1226,
                    "y": 2810
                },
                {
                    "x": 1226,
                    "y": 2938
                },
                {
                    "x": 219,
                    "y": 2938
                }
            ],
            "category": "paragraph",
            "html": "<p id='105' style='font-size:16px'>Then the conditions of the theorem hold. If then, furthermore, we<br>have Q't(s, a1) = V*(s) then the error is zero. The remaining ac-<br>tion values Q't(s,ai), fori > 1, are arbitrary. □</p>",
            "id": 105,
            "page": 8,
            "text": "Then the conditions of the theorem hold. If then, furthermore, we have Q't(s, a1) = V*(s) then the error is zero. The remaining action values Q't(s,ai), fori > 1, are arbitrary. □"
        },
        {
            "bounding_box": [
                {
                    "x": 1323,
                    "y": 234
                },
                {
                    "x": 2331,
                    "y": 234
                },
                {
                    "x": 2331,
                    "y": 403
                },
                {
                    "x": 1323,
                    "y": 403
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='106' style='font-size:18px'>Theorem 2. Considerastate s in which all the true optimal action<br>values are equal atQ*(s,a) = V* (s). Suppose that the estimation<br>errors Qt(s,a)-Q* (s,a) are independently distributed uniformly<br>randomly in [-1,1]. Then,</p>",
            "id": 106,
            "page": 8,
            "text": "Theorem 2. Considerastate s in which all the true optimal action values are equal atQ*(s,a) = V* (s). Suppose that the estimation errors Qt(s,a)-Q* (s,a) are independently distributed uniformly randomly in [-1,1]. Then,"
        },
        {
            "bounding_box": [
                {
                    "x": 1323,
                    "y": 564
                },
                {
                    "x": 2331,
                    "y": 564
                },
                {
                    "x": 2331,
                    "y": 773
                },
                {
                    "x": 1323,
                    "y": 773
                }
            ],
            "category": "paragraph",
            "html": "<p id='107' style='font-size:16px'>Proof. Define Ea = Qt(s,a) - Q*(s,a); this is a uniform ran-<br>dom variable in [-1, 1]. The probability that maxa Qt(s,a) ≤ x<br>for some x is equal to the probability that Ea ≤ x for all a simul-<br>taneously. Because the estimation errors are independent, we can<br>derive</p>",
            "id": 107,
            "page": 8,
            "text": "Proof. Define Ea = Qt(s,a) - Q*(s,a); this is a uniform random variable in [-1, 1]. The probability that maxa Qt(s,a) ≤ x for some x is equal to the probability that Ea ≤ x for all a simultaneously. Because the estimation errors are independent, we can derive"
        },
        {
            "bounding_box": [
                {
                    "x": 1327,
                    "y": 1038
                },
                {
                    "x": 2330,
                    "y": 1038
                },
                {
                    "x": 2330,
                    "y": 1120
                },
                {
                    "x": 1327,
                    "y": 1120
                }
            ],
            "category": "paragraph",
            "html": "<p id='108' style='font-size:16px'>The function P(Ea ≤ x) is the cumulative distribution function<br>(CDF) of €a, which here is simply defined as</p>",
            "id": 108,
            "page": 8,
            "text": "The function P(Ea ≤ x) is the cumulative distribution function (CDF) of €a, which here is simply defined as"
        },
        {
            "bounding_box": [
                {
                    "x": 1326,
                    "y": 1335
                },
                {
                    "x": 1585,
                    "y": 1335
                },
                {
                    "x": 1585,
                    "y": 1377
                },
                {
                    "x": 1326,
                    "y": 1377
                }
            ],
            "category": "paragraph",
            "html": "<p id='109' style='font-size:18px'>This implies that</p>",
            "id": 109,
            "page": 8,
            "text": "This implies that"
        },
        {
            "bounding_box": [
                {
                    "x": 1328,
                    "y": 1717
                },
                {
                    "x": 2327,
                    "y": 1717
                },
                {
                    "x": 2327,
                    "y": 1800
                },
                {
                    "x": 1328,
                    "y": 1800
                }
            ],
            "category": "paragraph",
            "html": "<p id='110' style='font-size:14px'>This gives us the CDF of the random variable maxa Ea. Its expec-<br>tation can be written as an integral</p>",
            "id": 110,
            "page": 8,
            "text": "This gives us the CDF of the random variable maxa Ea. Its expectation can be written as an integral"
        },
        {
            "bounding_box": [
                {
                    "x": 1323,
                    "y": 1973
                },
                {
                    "x": 2329,
                    "y": 1973
                },
                {
                    "x": 2329,
                    "y": 2155
                },
                {
                    "x": 1323,
                    "y": 2155
                }
            ],
            "category": "paragraph",
            "html": "<p id='111' style='font-size:16px'>where fmax is the probability density function of this variable, de-<br>fined as the derivative of the CDF: fmax (x) = 음P(maxa Ea ≤<br>m-1<br>x), SO that for x E [-1, 1] we have fmax (x) = 쁠 (1±x) ·<br>Evaluating the integral yields</p>",
            "id": 111,
            "page": 8,
            "text": "where fmax is the probability density function of this variable, defined as the derivative of the CDF: fmax (x) = 음P(maxa Ea ≤ m-1 x), SO that for x E [-1, 1] we have fmax (x) = 쁠 (1±x) · Evaluating the integral yields"
        },
        {
            "bounding_box": [
                {
                    "x": 1402,
                    "y": 2545
                },
                {
                    "x": 2254,
                    "y": 2545
                },
                {
                    "x": 2254,
                    "y": 2659
                },
                {
                    "x": 1402,
                    "y": 2659
                }
            ],
            "category": "paragraph",
            "html": "<p id='112' style='font-size:20px'>Experimental Details for the Atari 2600<br>Domain</p>",
            "id": 112,
            "page": 8,
            "text": "Experimental Details for the Atari 2600 Domain"
        },
        {
            "bounding_box": [
                {
                    "x": 1324,
                    "y": 2686
                },
                {
                    "x": 2330,
                    "y": 2686
                },
                {
                    "x": 2330,
                    "y": 2935
                },
                {
                    "x": 1324,
                    "y": 2935
                }
            ],
            "category": "paragraph",
            "html": "<p id='113' style='font-size:16px'>We selected the 49 games to match the list used by Mnih et al.<br>(2015), see Tables below for the full list. Each agent step is com-<br>posed of four frames (the last selected action is repeated during<br>these frames) and reward values (obtained from the Arcade Learn-<br>ing Environment (Bellemare et al., 2013)) are clipped between -1<br>and 1.</p>",
            "id": 113,
            "page": 8,
            "text": "We selected the 49 games to match the list used by Mnih  (2015), see Tables below for the full list. Each agent step is composed of four frames (the last selected action is repeated during these frames) and reward values (obtained from the Arcade Learning Environment (Bellemare , 2013)) are clipped between -1 and 1."
        },
        {
            "bounding_box": [
                {
                    "x": 219,
                    "y": 229
                },
                {
                    "x": 654,
                    "y": 229
                },
                {
                    "x": 654,
                    "y": 277
                },
                {
                    "x": 219,
                    "y": 277
                }
            ],
            "category": "paragraph",
            "html": "<p id='114' style='font-size:18px'>Network Architecture</p>",
            "id": 114,
            "page": 9,
            "text": "Network Architecture"
        },
        {
            "bounding_box": [
                {
                    "x": 218,
                    "y": 286
                },
                {
                    "x": 1227,
                    "y": 286
                },
                {
                    "x": 1227,
                    "y": 791
                },
                {
                    "x": 218,
                    "y": 791
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='115' style='font-size:16px'>The convolution network used in the experiment is exactly the one<br>proposed by proposed by Mnih et al. (2015), we only provide de-<br>tails here for completeness. The input to the network is a 84x84x4<br>tensor containing a rescaled, and gray-scale, version of the last four<br>frames. The first convolution layer convolves the input with 32 fil-<br>ters of size 8 (stride 4), the second layer has 64 layers of size 4<br>(stride 2), the final convolution layer has 64 filters of size 3 (stride<br>1). This is followed by a fully-connected hidden layer of 512 units.<br>All these layers are separated by Rectifier Linear Units (ReLu). Fi-<br>nally, a fully-connected linear layer projects to the output of the<br>network, i.e., the Q-values. The optimization employed to train the<br>network is RMSProp (with momentum parameter 0.95).</p>",
            "id": 115,
            "page": 9,
            "text": "The convolution network used in the experiment is exactly the one proposed by proposed by Mnih  (2015), we only provide details here for completeness. The input to the network is a 84x84x4 tensor containing a rescaled, and gray-scale, version of the last four frames. The first convolution layer convolves the input with 32 filters of size 8 (stride 4), the second layer has 64 layers of size 4 (stride 2), the final convolution layer has 64 filters of size 3 (stride 1). This is followed by a fully-connected hidden layer of 512 units. All these layers are separated by Rectifier Linear Units (ReLu). Finally, a fully-connected linear layer projects to the output of the network, i.e., the Q-values. The optimization employed to train the network is RMSProp (with momentum parameter 0.95)."
        },
        {
            "bounding_box": [
                {
                    "x": 220,
                    "y": 822
                },
                {
                    "x": 588,
                    "y": 822
                },
                {
                    "x": 588,
                    "y": 874
                },
                {
                    "x": 220,
                    "y": 874
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='116' style='font-size:22px'>Hyper-parameters</p>",
            "id": 116,
            "page": 9,
            "text": "Hyper-parameters"
        },
        {
            "bounding_box": [
                {
                    "x": 218,
                    "y": 879
                },
                {
                    "x": 1226,
                    "y": 879
                },
                {
                    "x": 1226,
                    "y": 1301
                },
                {
                    "x": 218,
                    "y": 1301
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='117' style='font-size:14px'>In all experiments, the discount was set to 2 = 0.99, and the learn-<br>ing rate to a = 0.00025. The number of steps between target net-<br>work updates was T = 10, 000. Training is done over 50M steps<br>(i.e., 200M frames). The agent is evaluated every 1M steps, and<br>the best policy across these evaluations is kept as the output of the<br>learning process. The size of the experience replay memory is 1M<br>tuples. The memory gets sampled to update the network every 4<br>steps with minibatches of size 32. The simple exploration policy<br>used is an e-greedy policy with the 6 decreasing linearly from 1 to<br>0.1 over 1M steps.</p>",
            "id": 117,
            "page": 9,
            "text": "In all experiments, the discount was set to 2 = 0.99, and the learning rate to a = 0.00025. The number of steps between target network updates was T = 10, 000. Training is done over 50M steps (i.e., 200M frames). The agent is evaluated every 1M steps, and the best policy across these evaluations is kept as the output of the learning process. The size of the experience replay memory is 1M tuples. The memory gets sampled to update the network every 4 steps with minibatches of size 32. The simple exploration policy used is an e-greedy policy with the 6 decreasing linearly from 1 to 0.1 over 1M steps."
        },
        {
            "bounding_box": [
                {
                    "x": 282,
                    "y": 1339
                },
                {
                    "x": 1157,
                    "y": 1339
                },
                {
                    "x": 1157,
                    "y": 1448
                },
                {
                    "x": 282,
                    "y": 1448
                }
            ],
            "category": "paragraph",
            "html": "<p id='118' style='font-size:20px'>Supplementary Results in the Atari 2600<br>Domain</p>",
            "id": 118,
            "page": 9,
            "text": "Supplementary Results in the Atari 2600 Domain"
        },
        {
            "bounding_box": [
                {
                    "x": 219,
                    "y": 1460
                },
                {
                    "x": 1221,
                    "y": 1460
                },
                {
                    "x": 1221,
                    "y": 1543
                },
                {
                    "x": 219,
                    "y": 1543
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='119' style='font-size:14px'>The Tables below provide further detailed results for our experi-<br>ments in the Atari domain.</p>",
            "id": 119,
            "page": 9,
            "text": "The Tables below provide further detailed results for our experiments in the Atari domain."
        },
        {
            "bounding_box": [
                {
                    "x": 608,
                    "y": 379
                },
                {
                    "x": 1947,
                    "y": 379
                },
                {
                    "x": 1947,
                    "y": 2680
                },
                {
                    "x": 608,
                    "y": 2680
                }
            ],
            "category": "paragraph",
            "html": "<p id='120' style='font-size:14px'>Game Random Human DQN Double DQN<br>Alien 227.80 6875.40 3069.33 2907.30<br>Amidar 5.80 1675.80 739.50 702.10<br>Assault 222.40 1496.40 3358.63 5022.90<br>Asterix 210.00 8503.30 6011.67 15150.00<br>Asteroids 719.10 13156.70 1629.33 930.60<br>Atlantis 12850.00 29028.10 85950.00 64758.00<br>Bank Heist 14.20 734.40 429.67 728.30<br>Battle Zone 2360.00 37800.00 26300.00 25730.00<br>Beam Rider 363.90 5774.70 6845.93 7654.00<br>Bowling 23.10 154.80 42.40 70.50<br>Boxing 0.10 4.30 71.83 81.70<br>Breakout 1.70 31.80 401.20 375.00<br>Centipede 2090.90 11963.20 8309.40 4139.40<br>Chopper Command 811.00 9881.80 6686.67 4653.00<br>Crazy Climber 10780.50 35410.50 114103.33 101874.00<br>Demon Attack 152.10 3401.30 9711.17 9711.90<br>Double Dunk -18.60 -15.50 -18.07 -6.30<br>Enduro 0.00 309.60 301.77 319.50<br>Fishing Derby -91.70 5.50 -0.80 20.30<br>Freeway 0.00 29.60 30.30 31.80<br>Frostbite 65.20 4334.70 328.33 241.50<br>Gopher 257.60 2321.00 8520.00 8215.40<br>Gravitar 173.00 2672.00 306.67 170.50<br>H.E.R.O. 1027.00 25762.50 19950.33 20357.00<br>Ice Hockey -11.20 0.90 -1.60 -2.40<br>James Bond 29.00 406.70 576.67 438.00<br>Kangaroo 52.00 3035.00 6740.00 13651.00<br>Krull 1598.00 2394.60 3804.67 4396.70<br>Kung-Fu Master 258.50 22736.20 23270.00 29486.00<br>Montezuma's Revenge 0.00 4366.70 0.00 0.00<br>Ms. Pacman 307.30 15693.40 2311.00 3210.00<br>Name This Game 2292.30 4076.20 7256.67 6997.10<br>Pong -20.70 9.30 18.90 21.00<br>Private Eye 24.90 69571.30 1787.57 670.10<br>Q*Bert 163.90 13455.00 10595.83 14875.00<br>River Raid 1338.50 13513.30 8315.67 12015.30<br>Road Runner 11.50 7845.00 18256.67 48377.00<br>Robotank 2.20 11.90 51.57 46.70<br>Seaquest 68.40 20181.80 5286.00 7995.00<br>Space Invaders 148.00 1652.30 1975.50 3154.60<br>Star Gunner 664.00 10250.00 57996.67 65188.00<br>Tennis -23.80 -8.90 -2.47 1.70<br>Time Pilot 3568.00 5925.00 5946.67 7964.00<br>Tutankham 11.40 167.60 186.70 190.60<br>Up and Down 533.40 9082.00 8456.33 16769.90<br>Venture 0.00 1187.50 380.00 93.00<br>Video Pinball 16256.90 17297.60 42684.07 70009.00<br>Wizard of Wor 563.50 4756.50 3393.33 5204.00<br>Zaxxon 32.50 9173.30 4976.67 10182.00</p>",
            "id": 120,
            "page": 10,
            "text": "Game Random Human DQN Double DQN Alien 227.80 6875.40 3069.33 2907.30 Amidar 5.80 1675.80 739.50 702.10 Assault 222.40 1496.40 3358.63 5022.90 Asterix 210.00 8503.30 6011.67 15150.00 Asteroids 719.10 13156.70 1629.33 930.60 Atlantis 12850.00 29028.10 85950.00 64758.00 Bank Heist 14.20 734.40 429.67 728.30 Battle Zone 2360.00 37800.00 26300.00 25730.00 Beam Rider 363.90 5774.70 6845.93 7654.00 Bowling 23.10 154.80 42.40 70.50 Boxing 0.10 4.30 71.83 81.70 Breakout 1.70 31.80 401.20 375.00 Centipede 2090.90 11963.20 8309.40 4139.40 Chopper Command 811.00 9881.80 6686.67 4653.00 Crazy Climber 10780.50 35410.50 114103.33 101874.00 Demon Attack 152.10 3401.30 9711.17 9711.90 Double Dunk -18.60 -15.50 -18.07 -6.30 Enduro 0.00 309.60 301.77 319.50 Fishing Derby -91.70 5.50 -0.80 20.30 Freeway 0.00 29.60 30.30 31.80 Frostbite 65.20 4334.70 328.33 241.50 Gopher 257.60 2321.00 8520.00 8215.40 Gravitar 173.00 2672.00 306.67 170.50 H.E.R.O. 1027.00 25762.50 19950.33 20357.00 Ice Hockey -11.20 0.90 -1.60 -2.40 James Bond 29.00 406.70 576.67 438.00 Kangaroo 52.00 3035.00 6740.00 13651.00 Krull 1598.00 2394.60 3804.67 4396.70 Kung-Fu Master 258.50 22736.20 23270.00 29486.00 Montezuma's Revenge 0.00 4366.70 0.00 0.00 Ms. Pacman 307.30 15693.40 2311.00 3210.00 Name This Game 2292.30 4076.20 7256.67 6997.10 Pong -20.70 9.30 18.90 21.00 Private Eye 24.90 69571.30 1787.57 670.10 Q*Bert 163.90 13455.00 10595.83 14875.00 River Raid 1338.50 13513.30 8315.67 12015.30 Road Runner 11.50 7845.00 18256.67 48377.00 Robotank 2.20 11.90 51.57 46.70 Seaquest 68.40 20181.80 5286.00 7995.00 Space Invaders 148.00 1652.30 1975.50 3154.60 Star Gunner 664.00 10250.00 57996.67 65188.00 Tennis -23.80 -8.90 -2.47 1.70 Time Pilot 3568.00 5925.00 5946.67 7964.00 Tutankham 11.40 167.60 186.70 190.60 Up and Down 533.40 9082.00 8456.33 16769.90 Venture 0.00 1187.50 380.00 93.00 Video Pinball 16256.90 17297.60 42684.07 70009.00 Wizard of Wor 563.50 4756.50 3393.33 5204.00 Zaxxon 32.50 9173.30 4976.67 10182.00"
        },
        {
            "bounding_box": [
                {
                    "x": 276,
                    "y": 2688
                },
                {
                    "x": 2271,
                    "y": 2688
                },
                {
                    "x": 2271,
                    "y": 2741
                },
                {
                    "x": 276,
                    "y": 2741
                }
            ],
            "category": "caption",
            "html": "<br><caption id='121' style='font-size:14px'>Table 3: Raw scores for the no-op evaluation condition (5 minutes emulator time). DQN as given by Mnih et al. (2015).</caption>",
            "id": 121,
            "page": 10,
            "text": "Table 3: Raw scores for the no-op evaluation condition (5 minutes emulator time). DQN as given by Mnih  (2015)."
        },
        {
            "bounding_box": [
                {
                    "x": 546,
                    "y": 2688
                },
                {
                    "x": 1998,
                    "y": 2688
                },
                {
                    "x": 1998,
                    "y": 2739
                },
                {
                    "x": 546,
                    "y": 2739
                }
            ],
            "category": "caption",
            "html": "<caption id='122' style='font-size:14px'>Table 4: Normalized results for no-op evaluation condition (5 minutes emulator time).</caption>",
            "id": 122,
            "page": 11,
            "text": "Table 4: Normalized results for no-op evaluation condition (5 minutes emulator time)."
        },
        {
            "bounding_box": [
                {
                    "x": 815,
                    "y": 387
                },
                {
                    "x": 1737,
                    "y": 387
                },
                {
                    "x": 1737,
                    "y": 2681
                },
                {
                    "x": 815,
                    "y": 2681
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='123' style='font-size:14px'>Game DQN Double DQN<br>Alien 42.75 % 40.31 %<br>Amidar 43.93 % 41.69 %<br>Assault 246.17 % 376.81 %<br>Asterix 69.96 % 180.15 %<br>Asteroids 7.32 % 1.70 %<br>Atlantis 451.85 % 320.85 %<br>Bank Heist 57.69 % 99.15 %<br>Battle Zone 67.55 % 65.94 %<br>Beam Rider 119.80 % 134.73 %<br>Bowling 14.65 % 35.99 %<br>Boxing 1707.86 % 1942.86 %<br>Breakout 1327.24 % 1240.20 %<br>Centipede 62.99 % 20.75 %<br>Chopper Command 64.78 % 42.36 %<br>Crazy Climber 419.50 % 369.85 %<br>Demon Attack 294.20 % 294.22 %<br>Double Dunk 17.10 % 396.77 %<br>Enduro 97.47 % 103.20 %<br>Fishing Derby 93.52 % 115.23 %<br>Freeway 102.36 % 107.43 %<br>Frostbite 6.16 % 4.13 %<br>Gopher 400.43 % 385.66 %<br>Gravitar 5.35 % -0.10 %<br>H.E.R.O. 76.50 % 78.15 %<br>Ice Hockey 79.34 % 72.73 %<br>James Bond 145.00 % 108.29 %<br>Kangaroo 224.20 % 455.88 %<br>Krull 277.01 % 351.33 %<br>Kung-Fu Master 102.37 % 130.03 %<br>Montezuma's Revenge 0.00 % 0.00 %<br>Ms. Pacman 13.02 % 18.87 %<br>Name This Game 278.29 % 263.74 %<br>Pong 132.00 % 139.00 %<br>Private Eye 2.53 % 0.93 %<br>Q*Bert 78.49 % 110.68 %<br>River Raid 57.31 % 87.70 %<br>Road Runner 232.91 % 617.42 %<br>Robotank 508.97 % 458.76 %<br>Seaquest 25.94 % 39.41 %<br>Space Invaders 121.49 % 199.87 %<br>Star Gunner 598.09 % 673.11 %<br>Tennis 143.15 % 171.14 %<br>Time Pilot 100.92 % 186.51 %<br>Tutankham 112.23 % 114.72 %<br>Up and Down 92.68 % 189.93 %<br>Venture 32.00 % 7.83 %<br>Video Pinball 2539.36 % 5164.99 %<br>Wizard of Wor 67.49 % 110.67 %<br>Zaxxon 54.09 % 111.04 %</p>",
            "id": 123,
            "page": 11,
            "text": "Game DQN Double DQN Alien 42.75 % 40.31 % Amidar 43.93 % 41.69 % Assault 246.17 % 376.81 % Asterix 69.96 % 180.15 % Asteroids 7.32 % 1.70 % Atlantis 451.85 % 320.85 % Bank Heist 57.69 % 99.15 % Battle Zone 67.55 % 65.94 % Beam Rider 119.80 % 134.73 % Bowling 14.65 % 35.99 % Boxing 1707.86 % 1942.86 % Breakout 1327.24 % 1240.20 % Centipede 62.99 % 20.75 % Chopper Command 64.78 % 42.36 % Crazy Climber 419.50 % 369.85 % Demon Attack 294.20 % 294.22 % Double Dunk 17.10 % 396.77 % Enduro 97.47 % 103.20 % Fishing Derby 93.52 % 115.23 % Freeway 102.36 % 107.43 % Frostbite 6.16 % 4.13 % Gopher 400.43 % 385.66 % Gravitar 5.35 % -0.10 % H.E.R.O. 76.50 % 78.15 % Ice Hockey 79.34 % 72.73 % James Bond 145.00 % 108.29 % Kangaroo 224.20 % 455.88 % Krull 277.01 % 351.33 % Kung-Fu Master 102.37 % 130.03 % Montezuma's Revenge 0.00 % 0.00 % Ms. Pacman 13.02 % 18.87 % Name This Game 278.29 % 263.74 % Pong 132.00 % 139.00 % Private Eye 2.53 % 0.93 % Q*Bert 78.49 % 110.68 % River Raid 57.31 % 87.70 % Road Runner 232.91 % 617.42 % Robotank 508.97 % 458.76 % Seaquest 25.94 % 39.41 % Space Invaders 121.49 % 199.87 % Star Gunner 598.09 % 673.11 % Tennis 143.15 % 171.14 % Time Pilot 100.92 % 186.51 % Tutankham 112.23 % 114.72 % Up and Down 92.68 % 189.93 % Venture 32.00 % 7.83 % Video Pinball 2539.36 % 5164.99 % Wizard of Wor 67.49 % 110.67 % Zaxxon 54.09 % 111.04 %"
        },
        {
            "bounding_box": [
                {
                    "x": 418,
                    "y": 218
                },
                {
                    "x": 2124,
                    "y": 218
                },
                {
                    "x": 2124,
                    "y": 2866
                },
                {
                    "x": 418,
                    "y": 2866
                }
            ],
            "category": "paragraph",
            "html": "<p id='124' style='font-size:18px'>Game Random Human DQN Double DQN Double DQN (tuned)<br>Alien 128.30 6371.30 570.2 621.6 1033.4<br>Amidar 11.80 1540.40 133.4 188.2 169.1<br>Assault 166.90 628.90 3332.3 2774.3 6060.8<br>Asterix 164.50 7536.00 124.5 5285.0 16837.0<br>Asteroids 871.30 36517.30 697.1 1219.0 1193.2<br>Atlantis 13463.00 26575.00 76108.0 260556.0 319688.0<br>Bank Heist 21.70 644.50 176.3 469.8 886.0<br>Battle Zone 3560.00 33030.00 17560.0 25240.0 24740.0<br>Beam Rider 254.60 14961.00 8672.4 9107.9 17417.2<br>Berzerk 196.10 2237.50 635.8 1011.1<br>Bowling 35.20 146.50 41.2 62.3 69.6<br>Boxing -1.50 9.60 25.8 52.1 73.5<br>Breakout 1.60 27.90 303.9 338.7 368.9<br>Centipede 1925.50 10321.90 3773.1 5166.6 3853.5<br>Chopper Command 644.00 8930.00 3046.0 2483.0 3495.0<br>Crazy Climber 9337.00 32667.00 50992.0 94315.0 113782.0<br>Defender 1965.50 14296.00 8531.0 27510.0<br>Demon Attack 208.30 3442.80 12835.2 13943.5 69803.4<br>Double Dunk -16.00 -14.40 -21.6 -6.4 -0.3<br>Enduro -81.80 740.20 475.6 475.9 1216.6<br>Fishing Derby -77.10 5.10 -2.3 -3.4 3.2<br>Freeway 0.10 25.60 25.8 26.3 28.8<br>Frostbite 66.40 4202.80 157.4 258.3 1448.1<br>Gopher 250.00 2311.00 2731.8 8742.8 15253.0<br>Gravitar 245.50 3116.00 216.5 170.0 200.5<br>H.E.R.O. 1580.30 25839.40 12952.5 15341.4 14892.5<br>Ice Hockey -9.70 0.50 -3.8 -3.6 -2.5<br>James Bond 33.50 368.50 348.5 416.0 573.0<br>Kangaroo 100.00 2739.00 2696.0 6138.0 11204.0<br>Krull 1151.90 2109.10 3864.0 6130.4 6796.1<br>Kung-Fu Master 304.00 20786.80 11875.0 22771.0 30207.0<br>Montezuma's Revenge 25.00 4182.00 50.0 30.0 42.0<br>Ms. Pacman 197.80 15375.00 763.5 1401.8 1241.3<br>Name This Game 1747.80 6796.00 5439.9 7871.5 8960.3<br>Phoenix 1134.40 6686.20 10364.0 12366.5<br>Pit Fall -348.80 5998.90 -432.9 -186.7<br>Pong -18.00 15.50 16.2 17.7 19.1<br>Private Eye 662.80 64169.10 298.2 346.3 -575.5<br>Q*Bert 183.00 12085.00 4589.8 10713.3 11020.8<br>River Raid 588.30 14382.20 4065.3 6579.0 10838.4<br>Road Runner 200.00 6878.00 9264.0 43884.0 43156.0<br>Robotank 2.40 8.90 58.5 52.0 59.1<br>Seaquest 215.50 40425.80 2793.9 4199.4 14498.0<br>Skiing -15287.40 -3686.60 -29404.3 -11490.4<br>Solaris 2047.20 11032.60 2166.8 810.0<br>Space Invaders 182.60 1464.90 1449.7 1495.7 2628.7<br>Star Gunner 697.00 9528.00 34081.0 53052.0 58365.0<br>Surround -9.70 5.40 -7.6 1.9<br>Tennis -21.40 -6.70 -2.3 11.0 -7.8<br>Time Pilot 3273.00 5650.00 5640.0 5375.0 6608.0<br>Tutankham 12.70 138.30 32.4 63.6 92.2<br>Up and Down 707.20 9896.10 3311.3 4721.1 19086.9<br>Venture 18.00 1039.00 54.0 75.0 21.0<br>Video Pinball 20452.0 15641.10 20228.1 148883.6 367823.7<br>Wizard of Wor 804.00 4556.00 246.0 155.0 6201.0<br>Yars Revenge 1476.90 47135.20 5439.5 6270.6<br>Zaxxon 475.00 8443.00 831.0 7874.0 8593.0</p>",
            "id": 124,
            "page": 12,
            "text": "Game Random Human DQN Double DQN Double DQN (tuned) Alien 128.30 6371.30 570.2 621.6 1033.4 Amidar 11.80 1540.40 133.4 188.2 169.1 Assault 166.90 628.90 3332.3 2774.3 6060.8 Asterix 164.50 7536.00 124.5 5285.0 16837.0 Asteroids 871.30 36517.30 697.1 1219.0 1193.2 Atlantis 13463.00 26575.00 76108.0 260556.0 319688.0 Bank Heist 21.70 644.50 176.3 469.8 886.0 Battle Zone 3560.00 33030.00 17560.0 25240.0 24740.0 Beam Rider 254.60 14961.00 8672.4 9107.9 17417.2 Berzerk 196.10 2237.50 635.8 1011.1 Bowling 35.20 146.50 41.2 62.3 69.6 Boxing -1.50 9.60 25.8 52.1 73.5 Breakout 1.60 27.90 303.9 338.7 368.9 Centipede 1925.50 10321.90 3773.1 5166.6 3853.5 Chopper Command 644.00 8930.00 3046.0 2483.0 3495.0 Crazy Climber 9337.00 32667.00 50992.0 94315.0 113782.0 Defender 1965.50 14296.00 8531.0 27510.0 Demon Attack 208.30 3442.80 12835.2 13943.5 69803.4 Double Dunk -16.00 -14.40 -21.6 -6.4 -0.3 Enduro -81.80 740.20 475.6 475.9 1216.6 Fishing Derby -77.10 5.10 -2.3 -3.4 3.2 Freeway 0.10 25.60 25.8 26.3 28.8 Frostbite 66.40 4202.80 157.4 258.3 1448.1 Gopher 250.00 2311.00 2731.8 8742.8 15253.0 Gravitar 245.50 3116.00 216.5 170.0 200.5 H.E.R.O. 1580.30 25839.40 12952.5 15341.4 14892.5 Ice Hockey -9.70 0.50 -3.8 -3.6 -2.5 James Bond 33.50 368.50 348.5 416.0 573.0 Kangaroo 100.00 2739.00 2696.0 6138.0 11204.0 Krull 1151.90 2109.10 3864.0 6130.4 6796.1 Kung-Fu Master 304.00 20786.80 11875.0 22771.0 30207.0 Montezuma's Revenge 25.00 4182.00 50.0 30.0 42.0 Ms. Pacman 197.80 15375.00 763.5 1401.8 1241.3 Name This Game 1747.80 6796.00 5439.9 7871.5 8960.3 Phoenix 1134.40 6686.20 10364.0 12366.5 Pit Fall -348.80 5998.90 -432.9 -186.7 Pong -18.00 15.50 16.2 17.7 19.1 Private Eye 662.80 64169.10 298.2 346.3 -575.5 Q*Bert 183.00 12085.00 4589.8 10713.3 11020.8 River Raid 588.30 14382.20 4065.3 6579.0 10838.4 Road Runner 200.00 6878.00 9264.0 43884.0 43156.0 Robotank 2.40 8.90 58.5 52.0 59.1 Seaquest 215.50 40425.80 2793.9 4199.4 14498.0 Skiing -15287.40 -3686.60 -29404.3 -11490.4 Solaris 2047.20 11032.60 2166.8 810.0 Space Invaders 182.60 1464.90 1449.7 1495.7 2628.7 Star Gunner 697.00 9528.00 34081.0 53052.0 58365.0 Surround -9.70 5.40 -7.6 1.9 Tennis -21.40 -6.70 -2.3 11.0 -7.8 Time Pilot 3273.00 5650.00 5640.0 5375.0 6608.0 Tutankham 12.70 138.30 32.4 63.6 92.2 Up and Down 707.20 9896.10 3311.3 4721.1 19086.9 Venture 18.00 1039.00 54.0 75.0 21.0 Video Pinball 20452.0 15641.10 20228.1 148883.6 367823.7 Wizard of Wor 804.00 4556.00 246.0 155.0 6201.0 Yars Revenge 1476.90 47135.20 5439.5 6270.6 Zaxxon 475.00 8443.00 831.0 7874.0 8593.0"
        },
        {
            "bounding_box": [
                {
                    "x": 314,
                    "y": 2876
                },
                {
                    "x": 2230,
                    "y": 2876
                },
                {
                    "x": 2230,
                    "y": 2929
                },
                {
                    "x": 314,
                    "y": 2929
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='125' style='font-size:14px'>Table 5: Raw scores for the human start condition (30 minutes emulator time). DQN as given by Nair et al. (2015).</p>",
            "id": 125,
            "page": 12,
            "text": "Table 5: Raw scores for the human start condition (30 minutes emulator time). DQN as given by Nair  (2015)."
        },
        {
            "bounding_box": [
                {
                    "x": 601,
                    "y": 206
                },
                {
                    "x": 1944,
                    "y": 206
                },
                {
                    "x": 1944,
                    "y": 2876
                },
                {
                    "x": 601,
                    "y": 2876
                }
            ],
            "category": "paragraph",
            "html": "<p id='126' style='font-size:18px'>Game DQN Double DQN Double DQN (tuned)<br>Alien 7.08% 7.90% 14.50%<br>Amidar 7.95% 11.54% 10.29%<br>Assault 685.15% 564.37% 1275.74%<br>Asterix -0.54% 69.46% 226.18%<br>Asteroids -0.49% 0.98% 0.90%<br>Atlantis 477.77% 1884.48% 2335.46%<br>Bank Heist 24.82% 71.95% 138.78%<br>Battle Zone 47.51% 73.57% 71.87%<br>Beam Rider 57.24% 60.20% 116.70%<br>Berzerk 21.54% 39.92%<br>Bowling 5.39% 24.35% 30.91%<br>Boxing 245.95% 482.88% 675.68%<br>Breakout 1149.43% 1281.75% 1396.58%<br>Centipede 22.00% 38.60% 22.96%<br>Chopper Command 28.99% 22.19% 34.41%<br>Crazy Climber 178.55% 364.24% 447.69%<br>Defender 53.25% 207.17%<br>Demon Attack 390.38% 424.65% 2151.65%<br>Double Dunk -350.00% 600.00% 981.25%<br>Enduro 67.81% 67.85% 157.96%<br>Fishing Derby 91.00% 89.66% 97.69%<br>Freeway 100.78% 102.75% 112.55%<br>Frostbite 2.20% 4.64% 33.40%<br>Gopher 120.42% 412.07% 727.95%<br>Gravitar -1.01% -2.63% -1.57%<br>H.E.R.O. 46.88% 56.73% 54.88%<br>Ice Hockey 57.84% 59.80% 70.59%<br>James Bond 94.03% 114.18% 161.04%<br>Kangaroo 98.37% 228.80% 420.77%<br>Krull 283.34% 520.11% 589.66%<br>Kung-Fu Master 56.49% 109.69% 145.99%<br>Montezuma's Revenge 0.60% 0.12% 0.41%<br>Ms. Pacman 3.73% 7.93% 6.88%<br>Name This Game 73.14% 121.30% 142.87%<br>Phoenix 166.25% 202.31%<br>Pit Fall -1.32% 2.55%<br>Pong 102.09% 106.57% 110.75%<br>Private Eye -0.57% -0.50% -1.95%<br>Q*Bert 37.03% 88.48% 91.06%<br>River Raid 25.21% 43.43% 74.31%<br>Road Runner 135.73% 654.15% 643.25%<br>Robotank 863.08% 763.08% 872.31%<br>Seaquest 6.41% 9.91% 35.52%<br>Skiing -121.69% 32.73%<br>Solaris 1.33% -13.77%<br>Space Invaders 98.81% 102.40% 190.76%<br>Star Gunner 378.03% 592.85% 653.02%<br>Surround 13.91% 76.82%<br>Tennis 129.93% 220.41% 92.52%<br>Time Pilot 99.58% 88.43% 140.30%<br>Tutankham 15.68% 40.53% 63.30%<br>Up and Down 28.34% 43.68% 200.02%<br>Venture 3.53% 5.58% 0.29%<br>Video Pinball -4.65% 2669.60% 7220.51%<br>Wizard of Wor -14.87% -17.30% 143.84%<br>Yars Revenge 8.68% 10.50%<br>Zaxxon 4.47% 92.86% 101.88%</p>",
            "id": 126,
            "page": 13,
            "text": "Game DQN Double DQN Double DQN (tuned) Alien 7.08% 7.90% 14.50% Amidar 7.95% 11.54% 10.29% Assault 685.15% 564.37% 1275.74% Asterix -0.54% 69.46% 226.18% Asteroids -0.49% 0.98% 0.90% Atlantis 477.77% 1884.48% 2335.46% Bank Heist 24.82% 71.95% 138.78% Battle Zone 47.51% 73.57% 71.87% Beam Rider 57.24% 60.20% 116.70% Berzerk 21.54% 39.92% Bowling 5.39% 24.35% 30.91% Boxing 245.95% 482.88% 675.68% Breakout 1149.43% 1281.75% 1396.58% Centipede 22.00% 38.60% 22.96% Chopper Command 28.99% 22.19% 34.41% Crazy Climber 178.55% 364.24% 447.69% Defender 53.25% 207.17% Demon Attack 390.38% 424.65% 2151.65% Double Dunk -350.00% 600.00% 981.25% Enduro 67.81% 67.85% 157.96% Fishing Derby 91.00% 89.66% 97.69% Freeway 100.78% 102.75% 112.55% Frostbite 2.20% 4.64% 33.40% Gopher 120.42% 412.07% 727.95% Gravitar -1.01% -2.63% -1.57% H.E.R.O. 46.88% 56.73% 54.88% Ice Hockey 57.84% 59.80% 70.59% James Bond 94.03% 114.18% 161.04% Kangaroo 98.37% 228.80% 420.77% Krull 283.34% 520.11% 589.66% Kung-Fu Master 56.49% 109.69% 145.99% Montezuma's Revenge 0.60% 0.12% 0.41% Ms. Pacman 3.73% 7.93% 6.88% Name This Game 73.14% 121.30% 142.87% Phoenix 166.25% 202.31% Pit Fall -1.32% 2.55% Pong 102.09% 106.57% 110.75% Private Eye -0.57% -0.50% -1.95% Q*Bert 37.03% 88.48% 91.06% River Raid 25.21% 43.43% 74.31% Road Runner 135.73% 654.15% 643.25% Robotank 863.08% 763.08% 872.31% Seaquest 6.41% 9.91% 35.52% Skiing -121.69% 32.73% Solaris 1.33% -13.77% Space Invaders 98.81% 102.40% 190.76% Star Gunner 378.03% 592.85% 653.02% Surround 13.91% 76.82% Tennis 129.93% 220.41% 92.52% Time Pilot 99.58% 88.43% 140.30% Tutankham 15.68% 40.53% 63.30% Up and Down 28.34% 43.68% 200.02% Venture 3.53% 5.58% 0.29% Video Pinball -4.65% 2669.60% 7220.51% Wizard of Wor -14.87% -17.30% 143.84% Yars Revenge 8.68% 10.50% Zaxxon 4.47% 92.86% 101.88%"
        },
        {
            "bounding_box": [
                {
                    "x": 552,
                    "y": 2878
                },
                {
                    "x": 1993,
                    "y": 2878
                },
                {
                    "x": 1993,
                    "y": 2927
                },
                {
                    "x": 552,
                    "y": 2927
                }
            ],
            "category": "caption",
            "html": "<br><caption id='127' style='font-size:14px'>Table 6: Normalized scores for the human start condition (30 minutes emulator time).</caption>",
            "id": 127,
            "page": 13,
            "text": "Table 6: Normalized scores for the human start condition (30 minutes emulator time)."
        }
    ]
}