{
    "id": "32b078f6-0f94-11ef-8828-426932df3dcf",
    "pdf_path": "/root/data/pdf/2309.03409v3.pdf",
    "elements": [
        {
            "bounding_box": [
                {
                    "x": 518,
                    "y": 102
                },
                {
                    "x": 871,
                    "y": 102
                },
                {
                    "x": 871,
                    "y": 160
                },
                {
                    "x": 518,
                    "y": 160
                }
            ],
            "category": "header",
            "html": "<header id='0' style='font-size:20px'>Google DeepMind</header>",
            "id": 0,
            "page": 1,
            "text": "Google DeepMind"
        },
        {
            "bounding_box": [
                {
                    "x": 449,
                    "y": 97
                },
                {
                    "x": 512,
                    "y": 97
                },
                {
                    "x": 512,
                    "y": 161
                },
                {
                    "x": 449,
                    "y": 161
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='1' style='font-size:22px'>·</p>",
            "id": 1,
            "page": 1,
            "text": "·"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 330
                },
                {
                    "x": 1828,
                    "y": 330
                },
                {
                    "x": 1828,
                    "y": 403
                },
                {
                    "x": 445,
                    "y": 403
                }
            ],
            "category": "paragraph",
            "html": "<p id='2' style='font-size:22px'>LARGE LANGUAGE MODELS AS OPTIMIZERS</p>",
            "id": 2,
            "page": 1,
            "text": "LARGE LANGUAGE MODELS AS OPTIMIZERS"
        },
        {
            "bounding_box": [
                {
                    "x": 477,
                    "y": 480
                },
                {
                    "x": 1555,
                    "y": 480
                },
                {
                    "x": 1555,
                    "y": 580
                },
                {
                    "x": 477,
                    "y": 580
                }
            ],
            "category": "paragraph",
            "html": "<p id='3' style='font-size:18px'>Xuezhi Wang Yifeng Lu Hanxiao Liu<br>Chengrun Yang *<br>*<br>Quoc V. Le Denny Zhou Xinyun Chen</p>",
            "id": 3,
            "page": 1,
            "text": "Xuezhi Wang Yifeng Lu Hanxiao Liu Chengrun Yang * * Quoc V. Le Denny Zhou Xinyun Chen"
        },
        {
            "bounding_box": [
                {
                    "x": 476,
                    "y": 597
                },
                {
                    "x": 1728,
                    "y": 597
                },
                {
                    "x": 1728,
                    "y": 690
                },
                {
                    "x": 476,
                    "y": 690
                }
            ],
            "category": "paragraph",
            "html": "<p id='4' style='font-size:16px'>{ chengrun, xuezhiw, yifenglu, hanxiaol } @google · com<br>{qvl, dennyzhou, xinyunchen} @google · com</p>",
            "id": 4,
            "page": 1,
            "text": "{ chengrun, xuezhiw, yifenglu, hanxiaol } @google · com {qvl, dennyzhou, xinyunchen} @google · com"
        },
        {
            "bounding_box": [
                {
                    "x": 469,
                    "y": 707
                },
                {
                    "x": 1178,
                    "y": 707
                },
                {
                    "x": 1178,
                    "y": 757
                },
                {
                    "x": 469,
                    "y": 757
                }
            ],
            "category": "paragraph",
            "html": "<p id='5' style='font-size:18px'>Google DeepMind *<br>Equal contribution</p>",
            "id": 5,
            "page": 1,
            "text": "Google DeepMind * Equal contribution"
        },
        {
            "bounding_box": [
                {
                    "x": 1155,
                    "y": 836
                },
                {
                    "x": 1395,
                    "y": 836
                },
                {
                    "x": 1395,
                    "y": 885
                },
                {
                    "x": 1155,
                    "y": 885
                }
            ],
            "category": "paragraph",
            "html": "<p id='6' style='font-size:20px'>ABSTRACT</p>",
            "id": 6,
            "page": 1,
            "text": "ABSTRACT"
        },
        {
            "bounding_box": [
                {
                    "x": 590,
                    "y": 926
                },
                {
                    "x": 1962,
                    "y": 926
                },
                {
                    "x": 1962,
                    "y": 1535
                },
                {
                    "x": 590,
                    "y": 1535
                }
            ],
            "category": "paragraph",
            "html": "<p id='7' style='font-size:18px'>Optimization is ubiquitous. While derivative-based algorithms have been powerful<br>tools for various problems, the absence of gradient imposes challenges on many<br>real-world applications. In this work, we propose Optimization by PROmpting<br>(OPRO), a simple and effective approach to leverage large language models (LLMs)<br>as optimizers, where the optimization task is described in natural language. In<br>each optimization step, the LLM generates new solutions from the prompt that<br>contains previously generated solutions with their values, then the new solutions are<br>evaluated and added to the prompt for the next optimization step. We first showcase<br>OPRO on linear regression and traveling salesman problems, then move on to our<br>main application in prompt optimization, where the goal is to find instructions<br>that maximize the task accuracy. With a variety of LLMs, we demonstrate that<br>the best prompts optimized by OPRO outperform human-designed prompts by<br>up to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks. Code at</p>",
            "id": 7,
            "page": 1,
            "text": "Optimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications. In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. We first showcase OPRO on linear regression and traveling salesman problems, then move on to our main application in prompt optimization, where the goal is to find instructions that maximize the task accuracy. With a variety of LLMs, we demonstrate that the best prompts optimized by OPRO outperform human-designed prompts by up to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks. Code at"
        },
        {
            "bounding_box": [
                {
                    "x": 594,
                    "y": 1529
                },
                {
                    "x": 1583,
                    "y": 1529
                },
                {
                    "x": 1583,
                    "y": 1570
                },
                {
                    "x": 594,
                    "y": 1570
                }
            ],
            "category": "caption",
            "html": "<br><caption id='8' style='font-size:16px'>https : / / github · com/ google-deepmind/ opro.</caption>",
            "id": 8,
            "page": 1,
            "text": "https : / / github · com/ google-deepmind/ opro."
        },
        {
            "bounding_box": [
                {
                    "x": 712,
                    "y": 1617
                },
                {
                    "x": 1822,
                    "y": 1617
                },
                {
                    "x": 1822,
                    "y": 2028
                },
                {
                    "x": 712,
                    "y": 2028
                }
            ],
            "category": "figure",
            "html": "<figure><img id='9' style='font-size:14px' alt=\"100.0\n80.0\naccuracy\naccuracy\n70.0\n80.0\ntraining 60.0\nBBH\n50.0 training\nmovie_ recommendation\nGSM8K\n60.0\n0 50 100 150 0 50 100 150 200\n# steps # steps\n(a) GSM8K (b) BBH movie_recommendation\" data-coord=\"top-left:(712,1617); bottom-right:(1822,2028)\" /></figure>",
            "id": 9,
            "page": 1,
            "text": "100.0 80.0 accuracy accuracy 70.0 80.0 training 60.0 BBH 50.0 training movie_ recommendation GSM8K 60.0 0 50 100 150 0 50 100 150 200 # steps # steps (a) GSM8K (b) BBH movie_recommendation"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 2070
                },
                {
                    "x": 2112,
                    "y": 2070
                },
                {
                    "x": 2112,
                    "y": 2355
                },
                {
                    "x": 440,
                    "y": 2355
                }
            ],
            "category": "paragraph",
            "html": "<p id='10' style='font-size:16px'>Figure 1: Prompt optimization on GSM8K (Cobbe et al., 2021) and BBH (Suzgun et al., 2022)<br>movie_recommendation. The optimization on GSM8K has pre-trained PaLM 2-L as the scorer and<br>the instruction-tuned PaLM 2-L (denoted PaLM 2-L-IT) as the optimizer; the optimization on<br>BBH movie_recommendation has text-bison as the scorer and PaLM 2-L-IT as the optimizer.<br>Each dot is the average accuracy across all (up to 8) generated instructions in the single step, and the<br>shaded region represents standard deviation. See Section 5 for more details on experimental setup.</p>",
            "id": 10,
            "page": 1,
            "text": "Figure 1: Prompt optimization on GSM8K (Cobbe , 2021) and BBH (Suzgun , 2022) movie_recommendation. The optimization on GSM8K has pre-trained PaLM 2-L as the scorer and the instruction-tuned PaLM 2-L (denoted PaLM 2-L-IT) as the optimizer; the optimization on BBH movie_recommendation has text-bison as the scorer and PaLM 2-L-IT as the optimizer. Each dot is the average accuracy across all (up to 8) generated instructions in the single step, and the shaded region represents standard deviation. See Section 5 for more details on experimental setup."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2425
                },
                {
                    "x": 2105,
                    "y": 2425
                },
                {
                    "x": 2105,
                    "y": 2522
                },
                {
                    "x": 441,
                    "y": 2522
                }
            ],
            "category": "caption",
            "html": "<caption id='11' style='font-size:16px'>Table 1: Top instructions with the highest GSM8K zero-shot test accuracies from prompt optimization<br>with different optimizer LLMs. All results use the pre-trained PaLM 2-L as the scorer.</caption>",
            "id": 11,
            "page": 1,
            "text": "Table 1: Top instructions with the highest GSM8K zero-shot test accuracies from prompt optimization with different optimizer LLMs. All results use the pre-trained PaLM 2-L as the scorer."
        },
        {
            "bounding_box": [
                {
                    "x": 490,
                    "y": 2525
                },
                {
                    "x": 2062,
                    "y": 2525
                },
                {
                    "x": 2062,
                    "y": 3048
                },
                {
                    "x": 490,
                    "y": 3048
                }
            ],
            "category": "table",
            "html": "<br><table id='12' style='font-size:14px'><tr><td>Source</td><td>Instruction</td><td>Acc</td></tr><tr><td colspan=\"3\">Baselines</td></tr><tr><td>- (Kojima et al., 2022)</td><td>Let's think step by step.</td><td>71.8</td></tr><tr><td>(Zhou et al., 2022b)</td><td>Let's work this out in a step by step way to be sure we have the right answer.</td><td>58.8</td></tr><tr><td></td><td>(empty string)</td><td>34.0</td></tr><tr><td colspan=\"3\">Ours</td></tr><tr><td>PaLM 2-L-IT</td><td>Take a deep breath and work on this problem step-by-step.</td><td>80.2</td></tr><tr><td>PaLM 2-L</td><td>Break this down.</td><td>79.9</td></tr><tr><td>gpt-3 5-turbo</td><td>A little bit of arithmetic and a logical approach will help us quickly arrive at the solution to this problem.</td><td>78.5</td></tr><tr><td>gpt-4</td><td>Let's combine our numerical command and clear thinking to quickly and accurately decipher the answer.</td><td>74.5</td></tr></table>",
            "id": 12,
            "page": 1,
            "text": "Source Instruction Acc  Baselines  - (Kojima , 2022) Let's think step by step. 71.8  (Zhou , 2022b) Let's work this out in a step by step way to be sure we have the right answer. 58.8   (empty string) 34.0  Ours  PaLM 2-L-IT Take a deep breath and work on this problem step-by-step. 80.2  PaLM 2-L Break this down. 79.9  gpt-3 5-turbo A little bit of arithmetic and a logical approach will help us quickly arrive at the solution to this problem. 78.5  gpt-4 Let's combine our numerical command and clear thinking to quickly and accurately decipher the answer."
        },
        {
            "bounding_box": [
                {
                    "x": 61,
                    "y": 881
                },
                {
                    "x": 147,
                    "y": 881
                },
                {
                    "x": 147,
                    "y": 2329
                },
                {
                    "x": 61,
                    "y": 2329
                }
            ],
            "category": "footer",
            "html": "<br><footer id='13' style='font-size:14px'>2024<br>Apr<br>15<br>[cs.LG]<br>arXiv:2309.03409v3</footer>",
            "id": 13,
            "page": 1,
            "text": "2024 Apr 15 [cs.LG] arXiv:2309.03409v3"
        },
        {
            "bounding_box": [
                {
                    "x": 1259,
                    "y": 3132
                },
                {
                    "x": 1288,
                    "y": 3132
                },
                {
                    "x": 1288,
                    "y": 3173
                },
                {
                    "x": 1259,
                    "y": 3173
                }
            ],
            "category": "footer",
            "html": "<footer id='14' style='font-size:14px'>1</footer>",
            "id": 14,
            "page": 1,
            "text": "1"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 112
                },
                {
                    "x": 1100,
                    "y": 112
                },
                {
                    "x": 1100,
                    "y": 158
                },
                {
                    "x": 443,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='15' style='font-size:22px'>Large Language Models as Optimizers</header>",
            "id": 15,
            "page": 2,
            "text": "Large Language Models as Optimizers"
        },
        {
            "bounding_box": [
                {
                    "x": 449,
                    "y": 341
                },
                {
                    "x": 863,
                    "y": 341
                },
                {
                    "x": 863,
                    "y": 393
                },
                {
                    "x": 449,
                    "y": 393
                }
            ],
            "category": "paragraph",
            "html": "<p id='16' style='font-size:20px'>1 INTRODUCTION</p>",
            "id": 16,
            "page": 2,
            "text": "1 INTRODUCTION"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 459
                },
                {
                    "x": 2111,
                    "y": 459
                },
                {
                    "x": 2111,
                    "y": 737
                },
                {
                    "x": 441,
                    "y": 737
                }
            ],
            "category": "paragraph",
            "html": "<p id='17' style='font-size:18px'>Optimization is critical for all areas. Many optimization techniques are iterative: the optimization<br>starts from an initial solution, then iteratively updates the solution to optimize the objective func-<br>tion (Amari, 1993; Qian, 1999; Kingma & Ba, 2015; Back & Schwefel, 1993; Rios & Sahinidis,<br>2013; Reeves, 1993). The optimization algorithm typically needs to be customized for an individual<br>task to deal with the specific challenges posed by the decision space and the performance landscape,<br>especially for derivative-free optimization.</p>",
            "id": 17,
            "page": 2,
            "text": "Optimization is critical for all areas. Many optimization techniques are iterative: the optimization starts from an initial solution, then iteratively updates the solution to optimize the objective function (Amari, 1993; Qian, 1999; Kingma & Ba, 2015; Back & Schwefel, 1993; Rios & Sahinidis, 2013; Reeves, 1993). The optimization algorithm typically needs to be customized for an individual task to deal with the specific challenges posed by the decision space and the performance landscape, especially for derivative-free optimization."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 758
                },
                {
                    "x": 2109,
                    "y": 758
                },
                {
                    "x": 2109,
                    "y": 1264
                },
                {
                    "x": 442,
                    "y": 1264
                }
            ],
            "category": "paragraph",
            "html": "<p id='18' style='font-size:16px'>In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to<br>utilize large language models (LLMs) as optimizers. With the advancement of prompting techniques,<br>LLMs have achieved impressive performance in various domains (Wei et al., 2022; Kojima et al.,<br>2022; Wang et al., 2022; Zhou et al., 2022a; Madaan et al., 2023; Bai et al., 2022; Chen et al., 2023e).<br>Their ability to understand natural language lays out a new possibility for optimization: instead of<br>formally defining the optimization problem and deriving the update step with a programmed solver,<br>we describe the optimization problem in natural language, then instruct the LLM to iteratively generate<br>new solutions based on the problem description and the previously found solutions. Optimization<br>with LLMs enables quick adaptation to different tasks by changing the problem description in the<br>prompt, and the optimization process can be customized by adding instructions to specify the desired<br>properties of the solutions.</p>",
            "id": 18,
            "page": 2,
            "text": "In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to utilize large language models (LLMs) as optimizers. With the advancement of prompting techniques, LLMs have achieved impressive performance in various domains (Wei , 2022; Kojima , 2022; Wang , 2022; Zhou , 2022a; Madaan , 2023; Bai , 2022; Chen , 2023e). Their ability to understand natural language lays out a new possibility for optimization: instead of formally defining the optimization problem and deriving the update step with a programmed solver, we describe the optimization problem in natural language, then instruct the LLM to iteratively generate new solutions based on the problem description and the previously found solutions. Optimization with LLMs enables quick adaptation to different tasks by changing the problem description in the prompt, and the optimization process can be customized by adding instructions to specify the desired properties of the solutions."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1286
                },
                {
                    "x": 2109,
                    "y": 1286
                },
                {
                    "x": 2109,
                    "y": 1518
                },
                {
                    "x": 441,
                    "y": 1518
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='19' style='font-size:14px'>To demonstrate the potential of LLMs for optimization, we first present case studies on linear<br>regression and the traveling salesman problem, which are two classic optimization problems that<br>underpin many others in mathematical optimization, computer science, and operations research. On<br>small-scale optimization problems, we show that LLMs are able to find good-quality solutions simply<br>through prompting, and sometimes match or surpass hand-designed heuristic algorithms.</p>",
            "id": 19,
            "page": 2,
            "text": "To demonstrate the potential of LLMs for optimization, we first present case studies on linear regression and the traveling salesman problem, which are two classic optimization problems that underpin many others in mathematical optimization, computer science, and operations research. On small-scale optimization problems, we show that LLMs are able to find good-quality solutions simply through prompting, and sometimes match or surpass hand-designed heuristic algorithms."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1539
                },
                {
                    "x": 2109,
                    "y": 1539
                },
                {
                    "x": 2109,
                    "y": 2137
                },
                {
                    "x": 441,
                    "y": 2137
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='20' style='font-size:16px'>Next, we demonstrate the ability of LLMs to optimize prompts: the goal is to find a prompt that<br>maximizes the task accuracy. Specifically, we focus on natural language tasks where both the task<br>input and output are texts. LLMs are shown to be sensitive to the prompt format (Zhao et al., 2021;<br>Lu et al., 2021; Wei et al., 2023; Madaan & Yazdanbakhsh, 2022); in particular, semantically similar<br>prompts may have drastically different performance (Kojima et al., 2022; Zhou et al., 2022b; Zhang<br>et al., 2023), and the optimal prompt formats can be model-specific and task-specific (Ma et al., 2023;<br>Chen et al., 2023c). Therefore, prompt engineering is often important for LLMs to achieve good<br>performance (Reynolds & McDonell, 2021). However, the large and discrete prompt space makes it<br>challenging for optimization, especially when only API access to the LLM is available. Following<br>prior work on continuous and discrete prompt optimization (Lester et al., 2021; Li & Liang, 2021;<br>Zhou et al., 2022b; Pryzant et al., 2023), we assume a training set is available to compute the training<br>accuracy as the objective value for optimization, and we show in experiments that optimizing the<br>prompt for accuracy on a small training set is sufficient to reach high performance on the test set.</p>",
            "id": 20,
            "page": 2,
            "text": "Next, we demonstrate the ability of LLMs to optimize prompts: the goal is to find a prompt that maximizes the task accuracy. Specifically, we focus on natural language tasks where both the task input and output are texts. LLMs are shown to be sensitive to the prompt format (Zhao , 2021; Lu , 2021; Wei , 2023; Madaan & Yazdanbakhsh, 2022); in particular, semantically similar prompts may have drastically different performance (Kojima , 2022; Zhou , 2022b; Zhang , 2023), and the optimal prompt formats can be model-specific and task-specific (Ma , 2023; Chen , 2023c). Therefore, prompt engineering is often important for LLMs to achieve good performance (Reynolds & McDonell, 2021). However, the large and discrete prompt space makes it challenging for optimization, especially when only API access to the LLM is available. Following prior work on continuous and discrete prompt optimization (Lester , 2021; Li & Liang, 2021; Zhou , 2022b; Pryzant , 2023), we assume a training set is available to compute the training accuracy as the objective value for optimization, and we show in experiments that optimizing the prompt for accuracy on a small training set is sufficient to reach high performance on the test set."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2157
                },
                {
                    "x": 2109,
                    "y": 2157
                },
                {
                    "x": 2109,
                    "y": 2755
                },
                {
                    "x": 441,
                    "y": 2755
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='21' style='font-size:14px'>The prompt to the LLM serves as a call to the optimizer, and we name it the meta-prompt. Figure 3<br>shows an example. The meta-prompt contains two core pieces of information. The first piece is<br>previously generated prompts with their corresponding training accuracies. The second piece is the<br>optimization problem description, which includes several exemplars randomly selected from the<br>training set to exemplify the task of interest. We also provide instructions for the LLM to understand<br>the relationships among different parts and the desired output format. Different from recent work<br>on using LLMs for automatic prompt generation (Zhou et al., 2022b; Pryzant et al., 2023), each<br>optimization step in our work generates new prompts that aim to increase the test accuracy based on<br>a trajectory of previously generated prompts, instead of editing one input prompt according to natural<br>language feedback (Pryzant et al., 2023) or requiring the new prompt to follow the same semantic<br>meaning (Zhou et al., 2022b). Making use of the full optimization trajectory, OPRO enables the<br>LLM to gradually generate new prompts that improve the task accuracy throughout the optimization<br>process, where the initial prompts have low task accuracies.</p>",
            "id": 21,
            "page": 2,
            "text": "The prompt to the LLM serves as a call to the optimizer, and we name it the meta-prompt. Figure 3 shows an example. The meta-prompt contains two core pieces of information. The first piece is previously generated prompts with their corresponding training accuracies. The second piece is the optimization problem description, which includes several exemplars randomly selected from the training set to exemplify the task of interest. We also provide instructions for the LLM to understand the relationships among different parts and the desired output format. Different from recent work on using LLMs for automatic prompt generation (Zhou , 2022b; Pryzant , 2023), each optimization step in our work generates new prompts that aim to increase the test accuracy based on a trajectory of previously generated prompts, instead of editing one input prompt according to natural language feedback (Pryzant , 2023) or requiring the new prompt to follow the same semantic meaning (Zhou , 2022b). Making use of the full optimization trajectory, OPRO enables the LLM to gradually generate new prompts that improve the task accuracy throughout the optimization process, where the initial prompts have low task accuracies."
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 2777
                },
                {
                    "x": 2110,
                    "y": 2777
                },
                {
                    "x": 2110,
                    "y": 3055
                },
                {
                    "x": 440,
                    "y": 3055
                }
            ],
            "category": "paragraph",
            "html": "<p id='22' style='font-size:14px'>We conduct comprehensive evaluation on several LLMs, including text-bison and Palm 2-L<br>in the PaLM-2 model family (Anil et al., 2023), as well as gpt-3 · 5-turbo and gpt-4 in the GPT<br>model family. We optimize prompts on GSM8K (Cobbe et al., 2021) and Big-Bench Hard (Suzgun<br>et al., 2022), which are reasoning benchmarks where prompting techniques have achieved remarkable<br>performance breakthrough (Wei et al., 2022; Kojima et al., 2022; Suzgun et al., 2022). Starting<br>from initial prompts with low task accuracies, we show that all LLMs in our evaluation are able to</p>",
            "id": 22,
            "page": 2,
            "text": "We conduct comprehensive evaluation on several LLMs, including text-bison and Palm 2-L in the PaLM-2 model family (Anil , 2023), as well as gpt-3 · 5-turbo and gpt-4 in the GPT model family. We optimize prompts on GSM8K (Cobbe , 2021) and Big-Bench Hard (Suzgun , 2022), which are reasoning benchmarks where prompting techniques have achieved remarkable performance breakthrough (Wei , 2022; Kojima , 2022; Suzgun , 2022). Starting from initial prompts with low task accuracies, we show that all LLMs in our evaluation are able to"
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3134
                },
                {
                    "x": 1289,
                    "y": 3134
                },
                {
                    "x": 1289,
                    "y": 3171
                },
                {
                    "x": 1260,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='23' style='font-size:14px'>2</footer>",
            "id": 23,
            "page": 2,
            "text": "2"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 112
                },
                {
                    "x": 1100,
                    "y": 112
                },
                {
                    "x": 1100,
                    "y": 158
                },
                {
                    "x": 443,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='24' style='font-size:22px'>Large Language Models as Optimizers</header>",
            "id": 24,
            "page": 3,
            "text": "Large Language Models as Optimizers"
        },
        {
            "bounding_box": [
                {
                    "x": 817,
                    "y": 324
                },
                {
                    "x": 1717,
                    "y": 324
                },
                {
                    "x": 1717,
                    "y": 821
                },
                {
                    "x": 817,
                    "y": 821
                }
            ],
            "category": "figure",
            "html": "<figure><img id='25' style='font-size:14px' alt=\"objective function\nscores\nevaluator\ngenerated\nreturn top solutions solutions\nwhen finish\nmeta-prompt\nsolution-score pairs\nLLM as\noptimizer task description\" data-coord=\"top-left:(817,324); bottom-right:(1717,821)\" /></figure>",
            "id": 25,
            "page": 3,
            "text": "objective function scores evaluator generated return top solutions solutions when finish meta-prompt solution-score pairs LLM as optimizer task description"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 889
                },
                {
                    "x": 2112,
                    "y": 889
                },
                {
                    "x": 2112,
                    "y": 1129
                },
                {
                    "x": 441,
                    "y": 1129
                }
            ],
            "category": "caption",
            "html": "<caption id='26' style='font-size:16px'>Figure 2: An overview of the OPRO framework. Given the meta-prompt as the input, the LLM<br>generates new solutions to the objective function, then the new solutions and their scores are added<br>into the meta-prompt for the next optimization step. The meta-prompt contains the solution-score<br>pairs obtained throughout optimization, a natural language description of the task, and (in prompt<br>optimization) a few task exemplars. Figure 3 shows a sample meta-prompt for prompt optimization.</caption>",
            "id": 26,
            "page": 3,
            "text": "Figure 2: An overview of the OPRO framework. Given the meta-prompt as the input, the LLM generates new solutions to the objective function, then the new solutions and their scores are added into the meta-prompt for the next optimization step. The meta-prompt contains the solution-score pairs obtained throughout optimization, a natural language description of the task, and (in prompt optimization) a few task exemplars. Figure 3 shows a sample meta-prompt for prompt optimization."
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 1192
                },
                {
                    "x": 2110,
                    "y": 1192
                },
                {
                    "x": 2110,
                    "y": 1517
                },
                {
                    "x": 440,
                    "y": 1517
                }
            ],
            "category": "paragraph",
            "html": "<p id='27' style='font-size:18px'>serve as optimizers, which consistently improve the performance of the generated prompts through<br>iterative optimization until convergence (see Figure 1). In particular, while these LLMs generally<br>produce instructions of different styles (see Table 1), with zero-shot prompting, their best generated<br>instructions match the few-shot chain-of-thought prompting performance when applied to PaLM<br>2-L, outperforming the zero-shot performance with human-designed prompts by up to 8% on<br>GSM8K. Additionally, we observe that the OPRO-optimized prompts transfer to other benchmarks<br>of the same domain and also deliver notable performance gain.</p>",
            "id": 27,
            "page": 3,
            "text": "serve as optimizers, which consistently improve the performance of the generated prompts through iterative optimization until convergence (see Figure 1). In particular, while these LLMs generally produce instructions of different styles (see Table 1), with zero-shot prompting, their best generated instructions match the few-shot chain-of-thought prompting performance when applied to PaLM 2-L, outperforming the zero-shot performance with human-designed prompts by up to 8% on GSM8K. Additionally, we observe that the OPRO-optimized prompts transfer to other benchmarks of the same domain and also deliver notable performance gain."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1585
                },
                {
                    "x": 1233,
                    "y": 1585
                },
                {
                    "x": 1233,
                    "y": 1639
                },
                {
                    "x": 443,
                    "y": 1639
                }
            ],
            "category": "paragraph",
            "html": "<p id='28' style='font-size:20px'>2 OPRO: LLM AS THE OPTIMIZER</p>",
            "id": 28,
            "page": 3,
            "text": "2 OPRO: LLM AS THE OPTIMIZER"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 1690
                },
                {
                    "x": 2110,
                    "y": 1690
                },
                {
                    "x": 2110,
                    "y": 2017
                },
                {
                    "x": 440,
                    "y": 2017
                }
            ],
            "category": "paragraph",
            "html": "<p id='29' style='font-size:16px'>Figure 2 illustrates the overall framework of OPRO. In each optimization step, the LLM generates<br>candidate solutions to the optimization task based on the optimization problem description and<br>previously evaluated solutions in the meta-prompt. Then the new solutions are evaluated and added to<br>the meta-prompt for the subsequent optimization process. The optimization process terminates when<br>the LLM is unable to propose new solutions with better optimization scores, or a maximum number<br>of optimization steps has reached. We first outline the desired features of LLMs for optimization,<br>then describe the key design choices based on these desirables.</p>",
            "id": 29,
            "page": 3,
            "text": "Figure 2 illustrates the overall framework of OPRO. In each optimization step, the LLM generates candidate solutions to the optimization task based on the optimization problem description and previously evaluated solutions in the meta-prompt. Then the new solutions are evaluated and added to the meta-prompt for the subsequent optimization process. The optimization process terminates when the LLM is unable to propose new solutions with better optimization scores, or a maximum number of optimization steps has reached. We first outline the desired features of LLMs for optimization, then describe the key design choices based on these desirables."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2075
                },
                {
                    "x": 1300,
                    "y": 2075
                },
                {
                    "x": 1300,
                    "y": 2125
                },
                {
                    "x": 443,
                    "y": 2125
                }
            ],
            "category": "paragraph",
            "html": "<p id='30' style='font-size:14px'>2.1 DESIRABLES OF OPTIMIZATION BY LLMs</p>",
            "id": 30,
            "page": 3,
            "text": "2.1 DESIRABLES OF OPTIMIZATION BY LLMs"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2163
                },
                {
                    "x": 2110,
                    "y": 2163
                },
                {
                    "x": 2110,
                    "y": 2396
                },
                {
                    "x": 441,
                    "y": 2396
                }
            ],
            "category": "paragraph",
            "html": "<p id='31' style='font-size:18px'>Making use of natural language descriptions. The main advantage of LLMs for optimization is<br>their ability of understanding natural language, which allows people to describe their optimization<br>tasks without formal specifications. For instance, in prompt optimization where the goal is to find a<br>prompt that optimizes the task accuracy, the task can be described with a high-level text summary<br>along with input-output examples.</p>",
            "id": 31,
            "page": 3,
            "text": "Making use of natural language descriptions. The main advantage of LLMs for optimization is their ability of understanding natural language, which allows people to describe their optimization tasks without formal specifications. For instance, in prompt optimization where the goal is to find a prompt that optimizes the task accuracy, the task can be described with a high-level text summary along with input-output examples."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2416
                },
                {
                    "x": 2110,
                    "y": 2416
                },
                {
                    "x": 2110,
                    "y": 2650
                },
                {
                    "x": 441,
                    "y": 2650
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='32' style='font-size:16px'>Trading off exploration and exploitation. The exploration-exploitation trade-off is a fundamental<br>challenge in optimization, and it is important for LLMs serving as optimizers to balance these two<br>competing goals. This means that the LLM should be able to exploit promising areas of the search<br>space where good solutions are already found, while also exploring new regions of the search space<br>SO as to not miss potentially better solutions.</p>",
            "id": 32,
            "page": 3,
            "text": "Trading off exploration and exploitation. The exploration-exploitation trade-off is a fundamental challenge in optimization, and it is important for LLMs serving as optimizers to balance these two competing goals. This means that the LLM should be able to exploit promising areas of the search space where good solutions are already found, while also exploring new regions of the search space SO as to not miss potentially better solutions."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2708
                },
                {
                    "x": 970,
                    "y": 2708
                },
                {
                    "x": 970,
                    "y": 2756
                },
                {
                    "x": 443,
                    "y": 2756
                }
            ],
            "category": "paragraph",
            "html": "<p id='33' style='font-size:16px'>2.2 META-PROMPT DESIGN</p>",
            "id": 33,
            "page": 3,
            "text": "2.2 META-PROMPT DESIGN"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2798
                },
                {
                    "x": 2030,
                    "y": 2798
                },
                {
                    "x": 2030,
                    "y": 2845
                },
                {
                    "x": 444,
                    "y": 2845
                }
            ],
            "category": "paragraph",
            "html": "<p id='34' style='font-size:16px'>As the input to the optimizer LLM, the meta-prompt contains the following two essential parts.</p>",
            "id": 34,
            "page": 3,
            "text": "As the input to the optimizer LLM, the meta-prompt contains the following two essential parts."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2867
                },
                {
                    "x": 2111,
                    "y": 2867
                },
                {
                    "x": 2111,
                    "y": 3055
                },
                {
                    "x": 442,
                    "y": 3055
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='35' style='font-size:16px'>Optimization problem description. The first part is the text description of the optimization problem,<br>including the objective function and solution constraints. For example, for prompt optimization,<br>the LLM can be instructed to \"generate a new instruction that achieves a higher accuracy\", and we<br>denote such instructions in the meta-prompt as meta-instructions. We can also provide customized</p>",
            "id": 35,
            "page": 3,
            "text": "Optimization problem description. The first part is the text description of the optimization problem, including the objective function and solution constraints. For example, for prompt optimization, the LLM can be instructed to \"generate a new instruction that achieves a higher accuracy\", and we denote such instructions in the meta-prompt as meta-instructions. We can also provide customized"
        },
        {
            "bounding_box": [
                {
                    "x": 1261,
                    "y": 3133
                },
                {
                    "x": 1288,
                    "y": 3133
                },
                {
                    "x": 1288,
                    "y": 3171
                },
                {
                    "x": 1261,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='36' style='font-size:16px'>3</footer>",
            "id": 36,
            "page": 3,
            "text": "3"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 112
                },
                {
                    "x": 1100,
                    "y": 112
                },
                {
                    "x": 1100,
                    "y": 158
                },
                {
                    "x": 443,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='37' style='font-size:22px'>Large Language Models as Optimizers</header>",
            "id": 37,
            "page": 4,
            "text": "Large Language Models as Optimizers"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 347
                },
                {
                    "x": 2106,
                    "y": 347
                },
                {
                    "x": 2106,
                    "y": 439
                },
                {
                    "x": 441,
                    "y": 439
                }
            ],
            "category": "paragraph",
            "html": "<p id='38' style='font-size:14px'>meta-instructions as an informal regularization of the generated solutions, such as \"the instruction<br>should be concise and generally applicable\".</p>",
            "id": 38,
            "page": 4,
            "text": "meta-instructions as an informal regularization of the generated solutions, such as \"the instruction should be concise and generally applicable\"."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 461
                },
                {
                    "x": 2110,
                    "y": 461
                },
                {
                    "x": 2110,
                    "y": 835
                },
                {
                    "x": 442,
                    "y": 835
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='39' style='font-size:18px'>Optimization trajectory. Besides understanding natural language instructions, LLMs are also<br>shown to be able to recognize patterns from in-context demonstrations (Wei et al., 2023; Madaan &<br>Yazdanbakhsh, 2022; Mirchandani et al., 2023). Our meta-prompt makes use of this property and in-<br>structs the LLM to leverage the optimization trajectory for generating new solutions. Specifically, the<br>optimization trajectory includes past solutions and their optimization scores, sorted in the ascending<br>order. Including optimization trajectory in the meta-prompt allows the LLM to identify similarities of<br>solutions with high scores, encouraging the LLM to build upon existing good solutions to construct<br>potentially better ones without the need of explicitly defining how the solution should be updated.</p>",
            "id": 39,
            "page": 4,
            "text": "Optimization trajectory. Besides understanding natural language instructions, LLMs are also shown to be able to recognize patterns from in-context demonstrations (Wei , 2023; Madaan & Yazdanbakhsh, 2022; Mirchandani , 2023). Our meta-prompt makes use of this property and instructs the LLM to leverage the optimization trajectory for generating new solutions. Specifically, the optimization trajectory includes past solutions and their optimization scores, sorted in the ascending order. Including optimization trajectory in the meta-prompt allows the LLM to identify similarities of solutions with high scores, encouraging the LLM to build upon existing good solutions to construct potentially better ones without the need of explicitly defining how the solution should be updated."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 885
                },
                {
                    "x": 988,
                    "y": 885
                },
                {
                    "x": 988,
                    "y": 932
                },
                {
                    "x": 445,
                    "y": 932
                }
            ],
            "category": "paragraph",
            "html": "<p id='40' style='font-size:16px'>2.3 SOLUTION GENERATION</p>",
            "id": 40,
            "page": 4,
            "text": "2.3 SOLUTION GENERATION"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 971
                },
                {
                    "x": 2106,
                    "y": 971
                },
                {
                    "x": 2106,
                    "y": 1064
                },
                {
                    "x": 442,
                    "y": 1064
                }
            ],
            "category": "paragraph",
            "html": "<p id='41' style='font-size:16px'>At the solution generation step, the LLM generates new solutions with the meta-prompt as input. The<br>following are the key optimization challenges we address in this stage.</p>",
            "id": 41,
            "page": 4,
            "text": "At the solution generation step, the LLM generates new solutions with the meta-prompt as input. The following are the key optimization challenges we address in this stage."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1086
                },
                {
                    "x": 2109,
                    "y": 1086
                },
                {
                    "x": 2109,
                    "y": 1411
                },
                {
                    "x": 441,
                    "y": 1411
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='42' style='font-size:20px'>Optimization stability. In the optimization process, not all solutions achieve high scores and<br>monotonically improve over prior ones. Due to the sensitivity of in-context learning to the prompt,<br>LLM output can be drastically affected by low-quality solutions in the input optimization trajectory,<br>especially at the beginning when the solution space has not been adequately explored. This sometimes<br>results in optimization instability and large variance. To improve stability, we prompt the LLM to<br>generate multiple solutions at each optimization step, allowing the LLM to simultaneously explore<br>multiple possibilities and quickly discover promising directions to move forward.</p>",
            "id": 42,
            "page": 4,
            "text": "Optimization stability. In the optimization process, not all solutions achieve high scores and monotonically improve over prior ones. Due to the sensitivity of in-context learning to the prompt, LLM output can be drastically affected by low-quality solutions in the input optimization trajectory, especially at the beginning when the solution space has not been adequately explored. This sometimes results in optimization instability and large variance. To improve stability, we prompt the LLM to generate multiple solutions at each optimization step, allowing the LLM to simultaneously explore multiple possibilities and quickly discover promising directions to move forward."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1431
                },
                {
                    "x": 2108,
                    "y": 1431
                },
                {
                    "x": 2108,
                    "y": 1619
                },
                {
                    "x": 441,
                    "y": 1619
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='43' style='font-size:18px'>Exploration-exploitation trade-off. We tune the LLM sampling temperature to balance between<br>exploration and exploitation. A lower temperature encourages the LLM to exploit the solution space<br>around the previously found solutions and make small adaptations, while a high temperature allows<br>the LLM to more aggressively explore solutions that can be notably different.</p>",
            "id": 43,
            "page": 4,
            "text": "Exploration-exploitation trade-off. We tune the LLM sampling temperature to balance between exploration and exploitation. A lower temperature encourages the LLM to exploit the solution space around the previously found solutions and make small adaptations, while a high temperature allows the LLM to more aggressively explore solutions that can be notably different."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1682
                },
                {
                    "x": 1745,
                    "y": 1682
                },
                {
                    "x": 1745,
                    "y": 1737
                },
                {
                    "x": 442,
                    "y": 1737
                }
            ],
            "category": "paragraph",
            "html": "<p id='44' style='font-size:22px'>3 MOTIVATING EXAMPLE: MATHEMATICAL OPTIMIZATION</p>",
            "id": 44,
            "page": 4,
            "text": "3 MOTIVATING EXAMPLE: MATHEMATICAL OPTIMIZATION"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1785
                },
                {
                    "x": 2111,
                    "y": 1785
                },
                {
                    "x": 2111,
                    "y": 2019
                },
                {
                    "x": 441,
                    "y": 2019
                }
            ],
            "category": "paragraph",
            "html": "<p id='45' style='font-size:16px'>We first demonstrate the potential of LLMs in serving as optimizers for mathematical optimization.<br>In particular, we present a case study on linear regression as an example of continuous optimization,<br>and on the Traveling Salesman Problem (TSP) as an example of discrete optimization. On both tasks,<br>we see LLMs properly capture the optimization directions on small-scale problems merely based on<br>the past optimization trajectory provided in the meta-prompt.</p>",
            "id": 45,
            "page": 4,
            "text": "We first demonstrate the potential of LLMs in serving as optimizers for mathematical optimization. In particular, we present a case study on linear regression as an example of continuous optimization, and on the Traveling Salesman Problem (TSP) as an example of discrete optimization. On both tasks, we see LLMs properly capture the optimization directions on small-scale problems merely based on the past optimization trajectory provided in the meta-prompt."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 2072
                },
                {
                    "x": 934,
                    "y": 2072
                },
                {
                    "x": 934,
                    "y": 2121
                },
                {
                    "x": 445,
                    "y": 2121
                }
            ],
            "category": "paragraph",
            "html": "<p id='46' style='font-size:14px'>3.1 LINEAR REGRESSION</p>",
            "id": 46,
            "page": 4,
            "text": "3.1 LINEAR REGRESSION"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2157
                },
                {
                    "x": 2110,
                    "y": 2157
                },
                {
                    "x": 2110,
                    "y": 2801
                },
                {
                    "x": 441,
                    "y": 2801
                }
            ],
            "category": "paragraph",
            "html": "<p id='47' style='font-size:16px'>In linear regression problems, the goal is to find the linear coefficients that probabilistically best<br>explain the response from the input variables. We study the setting in which the independent<br>and dependent variables X and y are both one-dimensional and an intercept 6 is present, SO that<br>there are two one-dimensional variables w, 6 to optimize over. In a synthetic setting, we sample<br>ground truth values for one-dimensional variables Wtrue and btrue, and generate 50 data points by<br>y = WtrueX + btrue + E, in which x ranges from 1 to 50 and E is the standard Gaussian noise. Our<br>optimization starts from 5 randomly sampled (w, 6) pairs. In each step, we prompt an instruction-<br>tuned LLM with a meta-prompt that includes the best 20 (w, 6) pairs in history and their sorted<br>objective values. The meta-prompt then asks for a new (w, 6) pair that further decreases the objective<br>value. A sample meta-prompt is shown in Figure 19 of Appendix C.1. We prompt the meta-prompt 8<br>times to generate at most 8 new (w, 6) pairs in each step to improve optimization stability. Then we<br>evaluate the objective value of the proposed pair and add it to history. We do black-box optimization:<br>the analytic form does not appear in the meta-prompt text. This is because the LLM can often<br>calculate the solution directly from the analytic form.</p>",
            "id": 47,
            "page": 4,
            "text": "In linear regression problems, the goal is to find the linear coefficients that probabilistically best explain the response from the input variables. We study the setting in which the independent and dependent variables X and y are both one-dimensional and an intercept 6 is present, SO that there are two one-dimensional variables w, 6 to optimize over. In a synthetic setting, we sample ground truth values for one-dimensional variables Wtrue and btrue, and generate 50 data points by y = WtrueX + btrue + E, in which x ranges from 1 to 50 and E is the standard Gaussian noise. Our optimization starts from 5 randomly sampled (w, 6) pairs. In each step, we prompt an instructiontuned LLM with a meta-prompt that includes the best 20 (w, 6) pairs in history and their sorted objective values. The meta-prompt then asks for a new (w, 6) pair that further decreases the objective value. A sample meta-prompt is shown in Figure 19 of Appendix C.1. We prompt the meta-prompt 8 times to generate at most 8 new (w, 6) pairs in each step to improve optimization stability. Then we evaluate the objective value of the proposed pair and add it to history. We do black-box optimization: the analytic form does not appear in the meta-prompt text. This is because the LLM can often calculate the solution directly from the analytic form."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2821
                },
                {
                    "x": 2111,
                    "y": 2821
                },
                {
                    "x": 2111,
                    "y": 3053
                },
                {
                    "x": 441,
                    "y": 3053
                }
            ],
            "category": "paragraph",
            "html": "<p id='48' style='font-size:18px'>Table 2 summarizes the results with one of the following optimizer LLMs: text-bi son,<br>gpt-3 · 5-turbo, and gpt -4. We study three settings of Wtrue and btrue: within the starting<br>region [10, 20] x [10, 20], \"near outside\" (each of Wtrue and btrue is outside the starting region but the<br>distance is less than 10), and \"far outside\" (each of Wtrue and btrue is outside the starting region and<br>the distance is greater than 10). We see:</p>",
            "id": 48,
            "page": 4,
            "text": "Table 2 summarizes the results with one of the following optimizer LLMs: text-bi son, gpt-3 · 5-turbo, and gpt -4. We study three settings of Wtrue and btrue: within the starting region  x , \"near outside\" (each of Wtrue and btrue is outside the starting region but the distance is less than 10), and \"far outside\" (each of Wtrue and btrue is outside the starting region and the distance is greater than 10). We see:"
        },
        {
            "bounding_box": [
                {
                    "x": 1258,
                    "y": 3134
                },
                {
                    "x": 1288,
                    "y": 3134
                },
                {
                    "x": 1288,
                    "y": 3170
                },
                {
                    "x": 1258,
                    "y": 3170
                }
            ],
            "category": "footer",
            "html": "<footer id='49' style='font-size:14px'>4</footer>",
            "id": 49,
            "page": 4,
            "text": "4"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 113
                },
                {
                    "x": 1100,
                    "y": 113
                },
                {
                    "x": 1100,
                    "y": 158
                },
                {
                    "x": 444,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='50' style='font-size:22px'>Large Language Models as Optimizers</header>",
            "id": 50,
            "page": 5,
            "text": "Large Language Models as Optimizers"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 372
                },
                {
                    "x": 2107,
                    "y": 372
                },
                {
                    "x": 2107,
                    "y": 648
                },
                {
                    "x": 441,
                    "y": 648
                }
            ],
            "category": "paragraph",
            "html": "<p id='51' style='font-size:20px'>Table 2: Linear regression by optimizer LLMs: the mean 士 standard deviation of the number of steps<br>and the number of unique (w, 6) pairs explored before reaching the global optima. Both w and 6 start<br>from 5 random starting points in [10, 20]. We use temperature 1.0 for all models. We run each setting<br>5 times. The starting points are the same across optimizer LLMs but are different across 5 runs, and<br>are grouped by: within the starting region, outside and close to the starting region, and outside and<br>farther from the starting region. Bold numbers indicate the best among three LLMs in each setting.</p>",
            "id": 51,
            "page": 5,
            "text": "Table 2: Linear regression by optimizer LLMs: the mean 士 standard deviation of the number of steps and the number of unique (w, 6) pairs explored before reaching the global optima. Both w and 6 start from 5 random starting points in . We use temperature 1.0 for all models. We run each setting 5 times. The starting points are the same across optimizer LLMs but are different across 5 runs, and are grouped by: within the starting region, outside and close to the starting region, and outside and farther from the starting region. Bold numbers indicate the best among three LLMs in each setting."
        },
        {
            "bounding_box": [
                {
                    "x": 461,
                    "y": 658
                },
                {
                    "x": 2093,
                    "y": 658
                },
                {
                    "x": 2093,
                    "y": 1057
                },
                {
                    "x": 461,
                    "y": 1057
                }
            ],
            "category": "table",
            "html": "<table id='52' style='font-size:14px'><tr><td rowspan=\"2\">Wtrue</td><td rowspan=\"2\">btrue</td><td colspan=\"3\">number of steps</td><td colspan=\"4\">number of unique (w, 6) pairs explored</td></tr><tr><td>text-bison</td><td>gpt-3 · 5-turbo</td><td>gpt-4</td><td>text-bison</td><td></td><td>gpt-3 · 5-turbo</td><td>gpt-4</td></tr><tr><td>15</td><td>14</td><td>5.8 ±2.6</td><td>7.6 ±4.5</td><td>4.0 ±1.5</td><td></td><td>40.0 ±12.4</td><td>36.0 土 15.2</td><td>17.2 ±5.1</td></tr><tr><td>17</td><td>17</td><td>4.0 ±1.8</td><td>12.6 ±6.0</td><td>6.0 ±3.7</td><td></td><td>33.4 ±11.7</td><td>53.8 土 16.9</td><td>26.0 土 10.6</td></tr><tr><td>16 -</td><td>10 -</td><td>3.8 ±2.2 -</td><td>10.4 ±5.4</td><td>6.2 ±3.1 -</td><td></td><td>30.2 ±13.4 -</td><td>42.8 土 16.3 - - - -</td><td>24.2 ±8.2 - -</td></tr><tr><td>3</td><td>- 5</td><td>- 9.8 ±2.8</td><td>- - - 10.8 ±2.7</td><td>- - - 12.2 ±2.0</td><td></td><td>- - 55.8 土 16.1</td><td>39.6 土 10.1</td><td>- 33.0 ±4.0</td></tr><tr><td>25</td><td>23</td><td>19.6 ±11.4</td><td>26.4 ±18.3</td><td>12.2 ±3.7</td><td></td><td>104.0 ±52.3</td><td>78.6 土 26.2</td><td>44.2 ±8.3</td></tr><tr><td>- - 2</td><td>- - 30</td><td>- - 31.4 ±6.3</td><td>- - - - 42.8 ±9.7</td><td>- - - 38.0 土 15.9</td><td>- 126.4 土 17.7</td><td>- - -</td><td>一 - - - 125.6 ±21.7</td><td>- - - 99.0 ±24.6</td></tr><tr><td>36</td><td>-1</td><td>35.8 ±6.4</td><td>45.4 土 16.9</td><td>50.4 土 18.8</td><td>174.0 土 28.2</td><td></td><td>142.2 ±31.2</td><td>116.4 ±32.7</td></tr></table>",
            "id": 52,
            "page": 5,
            "text": "Wtrue btrue number of steps number of unique (w, 6) pairs explored  text-bison gpt-3 · 5-turbo gpt-4 text-bison  gpt-3 · 5-turbo gpt-4  15 14 5.8 ±2.6 7.6 ±4.5 4.0 ±1.5  40.0 ±12.4 36.0 土 15.2 17.2 ±5.1  17 17 4.0 ±1.8 12.6 ±6.0 6.0 ±3.7  33.4 ±11.7 53.8 土 16.9 26.0 土 10.6  16 - 10 - 3.8 ±2.2 - 10.4 ±5.4 6.2 ±3.1 -  30.2 ±13.4 - 42.8 土 16.3 - - - - 24.2 ±8.2 -  3 - 5 - 9.8 ±2.8 - - - 10.8 ±2.7 - - - 12.2 ±2.0  - - 55.8 土 16.1 39.6 土 10.1 - 33.0 ±4.0  25 23 19.6 ±11.4 26.4 ±18.3 12.2 ±3.7  104.0 ±52.3 78.6 土 26.2 44.2 ±8.3  - - 2 - - 30 - - 31.4 ±6.3 - - - - 42.8 ±9.7 - - - 38.0 土 15.9 - 126.4 土 17.7 - - - 一 - - - 125.6 ±21.7 - - - 99.0 ±24.6  36 -1 35.8 ±6.4 45.4 土 16.9 50.4 土 18.8 174.0 土 28.2  142.2 ±31.2"
        },
        {
            "bounding_box": [
                {
                    "x": 488,
                    "y": 1148
                },
                {
                    "x": 2113,
                    "y": 1148
                },
                {
                    "x": 2113,
                    "y": 1681
                },
                {
                    "x": 488,
                    "y": 1681
                }
            ],
            "category": "paragraph",
            "html": "<p id='53' style='font-size:20px'>● The number of unique (w, 6) pairs explored by each model is fewer than exhaustive search,<br>indicating these models are able to to do black-box optimization: compare the numbers and<br>propose a descent direction.<br>· The text-bi son and gpt-4 models outperform gpt-3 · 5-turbo in convergence speed:<br>they arrive at the optima with fewer steps. The gpt-4 model also outperforms in finding the<br>optima with fewer explored unique points. Taking a closer look at the optimization trajectory, we<br>see gpt -4 is the best at proposing a reasonable next step from the history: for example, when<br>the history shows the objective values of (w, 6) = (8, 7), (w, 6) = (8, 6), and (w, 6) = (8, 5)<br>are decreasing, it has a highest chance to propose (w, 6) = (8, 4) for evaluation.<br>· The problem becomes harder for all models when the ground truth moves farther from the<br>starting region: all models need more explorations and more steps.</p>",
            "id": 53,
            "page": 5,
            "text": "● The number of unique (w, 6) pairs explored by each model is fewer than exhaustive search, indicating these models are able to to do black-box optimization: compare the numbers and propose a descent direction. · The text-bi son and gpt-4 models outperform gpt-3 · 5-turbo in convergence speed: they arrive at the optima with fewer steps. The gpt-4 model also outperforms in finding the optima with fewer explored unique points. Taking a closer look at the optimization trajectory, we see gpt -4 is the best at proposing a reasonable next step from the history: for example, when the history shows the objective values of (w, 6) = (8, 7), (w, 6) = (8, 6), and (w, 6) = (8, 5) are decreasing, it has a highest chance to propose (w, 6) = (8, 4) for evaluation. · The problem becomes harder for all models when the ground truth moves farther from the starting region: all models need more explorations and more steps."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 1739
                },
                {
                    "x": 1278,
                    "y": 1739
                },
                {
                    "x": 1278,
                    "y": 1789
                },
                {
                    "x": 445,
                    "y": 1789
                }
            ],
            "category": "paragraph",
            "html": "<p id='54' style='font-size:20px'>3.2 TRAVELING SALESMAN PROBLEM (TSP)</p>",
            "id": 54,
            "page": 5,
            "text": "3.2 TRAVELING SALESMAN PROBLEM (TSP)"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1829
                },
                {
                    "x": 2109,
                    "y": 1829
                },
                {
                    "x": 2109,
                    "y": 2152
                },
                {
                    "x": 442,
                    "y": 2152
                }
            ],
            "category": "paragraph",
            "html": "<p id='55' style='font-size:20px'>Next, we consider the Traveling Salesman Problem (TSP) (Junger et al., 1995; Gutin & Punnen, 2006),<br>a classical combinatorial optimization problem with numerous algorithms proposed in literature,<br>including heuristic algorithms and solvers (Rosenkrantz et al., 1977; Golden et al., 1980; Optimization<br>et al., 2020; Applegate et al., 2006; Helsgaun, 2017), and approaches based on training deep neural<br>networks (Kool et al., 2019; Deudon et al., 2018; Chen & Tian, 2019; Nazari et al., 2018). Specifically,<br>given a set of n nodes with their coordinates, the TSP task is to find the shortest route that traverses<br>all nodes from the starting node and finally returns to the starting node.</p>",
            "id": 55,
            "page": 5,
            "text": "Next, we consider the Traveling Salesman Problem (TSP) (Junger , 1995; Gutin & Punnen, 2006), a classical combinatorial optimization problem with numerous algorithms proposed in literature, including heuristic algorithms and solvers (Rosenkrantz , 1977; Golden , 1980; Optimization , 2020; Applegate , 2006; Helsgaun, 2017), and approaches based on training deep neural networks (Kool , 2019; Deudon , 2018; Chen & Tian, 2019; Nazari , 2018). Specifically, given a set of n nodes with their coordinates, the TSP task is to find the shortest route that traverses all nodes from the starting node and finally returns to the starting node."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2172
                },
                {
                    "x": 2108,
                    "y": 2172
                },
                {
                    "x": 2108,
                    "y": 2588
                },
                {
                    "x": 442,
                    "y": 2588
                }
            ],
            "category": "paragraph",
            "html": "<p id='56' style='font-size:20px'>Our optimization process with LLMs starts from 5 randomly generated solutions, and each optimiza-<br>tion step produces at most 8 new solutions. We present the meta-prompt in Figure 20 of Appendix C.1.<br>We generate the problem instances by sampling n nodes with both x and y coordinates in [-100, 100].<br>We use the Gurobi solver (Optimization et al., 2020) to construct the oracle solutions and compute the<br>optimality gap for all approaches, where the optimality gap is defined as the difference between the<br>distance in the solution constructed by the evaluated approach and the distance achieved by the oracle<br>solution, divided by the distance of the oracle solution. Besides evaluating OPRO with different<br>LLMs including text-bison, gpt-3 · 5-turbo and gpt-4, we also compare OPRO to the<br>following heuristics:</p>",
            "id": 56,
            "page": 5,
            "text": "Our optimization process with LLMs starts from 5 randomly generated solutions, and each optimization step produces at most 8 new solutions. We present the meta-prompt in Figure 20 of Appendix C.1. We generate the problem instances by sampling n nodes with both x and y coordinates in [-100, 100]. We use the Gurobi solver (Optimization , 2020) to construct the oracle solutions and compute the optimality gap for all approaches, where the optimality gap is defined as the difference between the distance in the solution constructed by the evaluated approach and the distance achieved by the oracle solution, divided by the distance of the oracle solution. Besides evaluating OPRO with different LLMs including text-bison, gpt-3 · 5-turbo and gpt-4, we also compare OPRO to the following heuristics:"
        },
        {
            "bounding_box": [
                {
                    "x": 486,
                    "y": 2627
                },
                {
                    "x": 2112,
                    "y": 2627
                },
                {
                    "x": 2112,
                    "y": 3056
                },
                {
                    "x": 486,
                    "y": 3056
                }
            ],
            "category": "paragraph",
            "html": "<p id='57' style='font-size:16px'>● Nearest Nei ghbor (NN) · Starting from an initial node, the solution is constructed with<br>the nearest neighbor heuristic: At each step, among the remaining nodes that are not included in<br>the current partial solution, NN selects the node with the shortest distance to the end node of the<br>partial solution, and adds it as the new end node. The process finishes when all nodes have been<br>added to the solution.<br>· Farthest Insertion (FI) · One caveat of the nearest neighbor heuristic is that it does<br>not take the distance between the start and end node into consideration when constructing partial<br>solutions. To address this issue, FI aims to optimize the cost of inserting new nodes into the<br>partial solution at each step. Define the minimal insertion cost of adding a new node k as</p>",
            "id": 57,
            "page": 5,
            "text": "● Nearest Nei ghbor (NN) · Starting from an initial node, the solution is constructed with the nearest neighbor heuristic: At each step, among the remaining nodes that are not included in the current partial solution, NN selects the node with the shortest distance to the end node of the partial solution, and adds it as the new end node. The process finishes when all nodes have been added to the solution. · Farthest Insertion (FI) · One caveat of the nearest neighbor heuristic is that it does not take the distance between the start and end node into consideration when constructing partial solutions. To address this issue, FI aims to optimize the cost of inserting new nodes into the partial solution at each step. Define the minimal insertion cost of adding a new node k as"
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3134
                },
                {
                    "x": 1289,
                    "y": 3134
                },
                {
                    "x": 1289,
                    "y": 3171
                },
                {
                    "x": 1260,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='58' style='font-size:18px'>5</footer>",
            "id": 58,
            "page": 5,
            "text": "5"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 113
                },
                {
                    "x": 1099,
                    "y": 113
                },
                {
                    "x": 1099,
                    "y": 158
                },
                {
                    "x": 444,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='59' style='font-size:20px'>Large Language Models as Optimizers</header>",
            "id": 59,
            "page": 6,
            "text": "Large Language Models as Optimizers"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 372
                },
                {
                    "x": 2109,
                    "y": 372
                },
                {
                    "x": 2109,
                    "y": 602
                },
                {
                    "x": 441,
                    "y": 602
                }
            ],
            "category": "paragraph",
            "html": "<p id='60' style='font-size:16px'>Table 3: Results of the Traveling Salesman Problem (TSP) with different number of nodes n, where<br>each n contains 5 problems. \"# steps\" calculates the mean 土 standard error of optimization steps<br>for successful runs that find the optimal solution. \"# successes\" counts the number of problems that<br>OPRO results in the optimal solution. When no optimal solution is found for any evaluated problem,<br>the corresponding number of steps is N/A.</p>",
            "id": 60,
            "page": 6,
            "text": "Table 3: Results of the Traveling Salesman Problem (TSP) with different number of nodes n, where each n contains 5 problems. \"# steps\" calculates the mean 土 standard error of optimization steps for successful runs that find the optimal solution. \"# successes\" counts the number of problems that OPRO results in the optimal solution. When no optimal solution is found for any evaluated problem, the corresponding number of steps is N/A."
        },
        {
            "bounding_box": [
                {
                    "x": 455,
                    "y": 608
                },
                {
                    "x": 2096,
                    "y": 608
                },
                {
                    "x": 2096,
                    "y": 850
                },
                {
                    "x": 455,
                    "y": 850
                }
            ],
            "category": "table",
            "html": "<br><table id='61' style='font-size:14px'><tr><td rowspan=\"2\">n</td><td colspan=\"5\">optimality gap (%)</td><td colspan=\"3\"># steps (# successes)</td></tr><tr><td>NN</td><td>FI</td><td>text-bison</td><td>gpt-3. 5-turbo</td><td>gpt-4</td><td>text-bison</td><td>gpt-3.5-turbo</td><td>gpt-4</td></tr><tr><td>10</td><td>13.0 ±1.3</td><td>3.2 ±1.4</td><td>0.0 ±0.0</td><td>0.0 ±0.0</td><td>0.0 ±0.0</td><td>40.4 ±5.6 (5)</td><td>46.8 ±9.3 (5)</td><td>9.6 ±3.0 (5)</td></tr><tr><td>15</td><td>9.4 ±3.7</td><td>1.2 ±0.6</td><td>4.4±1.3</td><td>1.2 ±1.1</td><td>0.2 ±0.2</td><td>N/A (0)</td><td>202.0 ±41.1 (4)</td><td>58.5 土 29.0 (4)</td></tr><tr><td>20</td><td>16.0±3.9</td><td>0.2±0.1</td><td>30.4 ±10.6</td><td>4.4 ±2.5</td><td>1.4 ±0.6</td><td>N/A (0)</td><td>438.0 土 0.0 (1)</td><td>195.5 土 127.6 (2)</td></tr><tr><td>50</td><td>19.7 ±3.1</td><td>9.8 ±1.5</td><td>219.8 ±13.7</td><td>133.0 ±6.8</td><td>11.0 ±2.6</td><td>N/A (0)</td><td>N/A (0)</td><td>N/A (0)</td></tr></table>",
            "id": 61,
            "page": 6,
            "text": "n optimality gap (%) # steps (# successes)  NN FI text-bison gpt-3. 5-turbo gpt-4 text-bison gpt-3.5-turbo gpt-4  10 13.0 ±1.3 3.2 ±1.4 0.0 ±0.0 0.0 ±0.0 0.0 ±0.0 40.4 ±5.6 (5) 46.8 ±9.3 (5) 9.6 ±3.0 (5)  15 9.4 ±3.7 1.2 ±0.6 4.4±1.3 1.2 ±1.1 0.2 ±0.2 N/A (0) 202.0 ±41.1 (4) 58.5 土 29.0 (4)  20 16.0±3.9 0.2±0.1 30.4 ±10.6 4.4 ±2.5 1.4 ±0.6 N/A (0) 438.0 土 0.0 (1) 195.5 土 127.6 (2)  50 19.7 ±3.1 9.8 ±1.5 219.8 ±13.7 133.0 ±6.8 11.0 ±2.6 N/A (0) N/A (0)"
        },
        {
            "bounding_box": [
                {
                    "x": 522,
                    "y": 943
                },
                {
                    "x": 2110,
                    "y": 943
                },
                {
                    "x": 2110,
                    "y": 1087
                },
                {
                    "x": 522,
                    "y": 1087
                }
            ],
            "category": "paragraph",
            "html": "<p id='62' style='font-size:16px'>c(k) = min(i,j) d(i, k) + d(k,j) - d(i,j), where i and 7 are adjacent nodes in the current tour,<br>and d(·, · ) represents the distance between two nodes. At each step, FI adds a new node that<br>maximizes the minimal insertion cost.</p>",
            "id": 62,
            "page": 6,
            "text": "c(k) = min(i,j) d(i, k) + d(k,j) - d(i,j), where i and 7 are adjacent nodes in the current tour, and d(·, · ) represents the distance between two nodes. At each step, FI adds a new node that maximizes the minimal insertion cost."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1127
                },
                {
                    "x": 2108,
                    "y": 1127
                },
                {
                    "x": 2108,
                    "y": 1497
                },
                {
                    "x": 441,
                    "y": 1497
                }
            ],
            "category": "paragraph",
            "html": "<p id='63' style='font-size:16px'>We present the results in Table 3. We randomly generate 5 problem instances for each number of<br>nodes n. In addition to measuring the optimality gap, on problems where the LLM finds the optimal<br>solutions, we also show the number of optimization steps taken to reach the global optimum. First,<br>we observe that gpt-4 significantly outperforms gpt -3 · 5-turbo and text-bison across all<br>problem sizes. Specifically, on smaller-scale problems, gpt -4 reaches the global optimum about 4x<br>faster than other LLMs. On larger-scale problems, especially with n = 50, gpt-4 still finds solutions<br>with a comparable quality to heuristic algorithms, while both text-bison and gpt-3 · 5-turbo<br>get stuck at local optima with up to 20x worse optimality gaps.</p>",
            "id": 63,
            "page": 6,
            "text": "We present the results in Table 3. We randomly generate 5 problem instances for each number of nodes n. In addition to measuring the optimality gap, on problems where the LLM finds the optimal solutions, we also show the number of optimization steps taken to reach the global optimum. First, we observe that gpt-4 significantly outperforms gpt -3 · 5-turbo and text-bison across all problem sizes. Specifically, on smaller-scale problems, gpt -4 reaches the global optimum about 4x faster than other LLMs. On larger-scale problems, especially with n = 50, gpt-4 still finds solutions with a comparable quality to heuristic algorithms, while both text-bison and gpt-3 · 5-turbo get stuck at local optima with up to 20x worse optimality gaps."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1519
                },
                {
                    "x": 2110,
                    "y": 1519
                },
                {
                    "x": 2110,
                    "y": 1705
                },
                {
                    "x": 441,
                    "y": 1705
                }
            ],
            "category": "paragraph",
            "html": "<p id='64' style='font-size:18px'>On the other hand, the performance of OPRO degrades dramatically on problems with larger sizes.<br>When n = 10, all LLMs find the optimal solutions for every evaluated problem; as the problem size<br>gets larger, the OPRO optimality gaps increase quickly, and the farthest insertion heuristic starts to<br>outperform all LLMs in the optimality gap.</p>",
            "id": 64,
            "page": 6,
            "text": "On the other hand, the performance of OPRO degrades dramatically on problems with larger sizes. When n = 10, all LLMs find the optimal solutions for every evaluated problem; as the problem size gets larger, the OPRO optimality gaps increase quickly, and the farthest insertion heuristic starts to outperform all LLMs in the optimality gap."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1756
                },
                {
                    "x": 2109,
                    "y": 1756
                },
                {
                    "x": 2109,
                    "y": 2267
                },
                {
                    "x": 441,
                    "y": 2267
                }
            ],
            "category": "paragraph",
            "html": "<p id='65' style='font-size:18px'>Limitations. We would like to note that OPRO is designed for neither outperforming the state-<br>of-the-art gradient-based optimization algorithms for continuous mathematical optimization, nor<br>surpassing the performance of specialized solvers for classical combinatorial optimization problems<br>such as TSP. Instead, the goal is to demonstrate that LLMs are able to optimize different kinds<br>of objective functions simply through prompting, and reach the global optimum for some small-<br>scale problems. Our evaluation reveals several limitations of OPRO for mathematical optimization.<br>Specifically, the length limit of the LLM context window makes it hard to fit large-scale optimization<br>problem descriptions in the prompt, e.g., linear regression with high-dimensional data, and traveling<br>salesman problems with a large set of nodes to visit. In addition, the optimization landscape of some<br>objective functions are too bumpy for the LLM to propose a correct descending direction, causing the<br>optimization to get stuck halfway. We further elaborate our observed failure cases in Appendix A.</p>",
            "id": 65,
            "page": 6,
            "text": "Limitations. We would like to note that OPRO is designed for neither outperforming the stateof-the-art gradient-based optimization algorithms for continuous mathematical optimization, nor surpassing the performance of specialized solvers for classical combinatorial optimization problems such as TSP. Instead, the goal is to demonstrate that LLMs are able to optimize different kinds of objective functions simply through prompting, and reach the global optimum for some smallscale problems. Our evaluation reveals several limitations of OPRO for mathematical optimization. Specifically, the length limit of the LLM context window makes it hard to fit large-scale optimization problem descriptions in the prompt, e.g., linear regression with high-dimensional data, and traveling salesman problems with a large set of nodes to visit. In addition, the optimization landscape of some objective functions are too bumpy for the LLM to propose a correct descending direction, causing the optimization to get stuck halfway. We further elaborate our observed failure cases in Appendix A."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2336
                },
                {
                    "x": 1361,
                    "y": 2336
                },
                {
                    "x": 1361,
                    "y": 2389
                },
                {
                    "x": 442,
                    "y": 2389
                }
            ],
            "category": "paragraph",
            "html": "<p id='66' style='font-size:22px'>4 APPLICATION: PROMPT OPTIMIZATION</p>",
            "id": 66,
            "page": 6,
            "text": "4 APPLICATION: PROMPT OPTIMIZATION"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2442
                },
                {
                    "x": 2108,
                    "y": 2442
                },
                {
                    "x": 2108,
                    "y": 2583
                },
                {
                    "x": 442,
                    "y": 2583
                }
            ],
            "category": "paragraph",
            "html": "<p id='67' style='font-size:18px'>Next, we demonstrate the effectiveness of OPRO on prompt optimization, where the objective is to<br>find the prompt that maximizes task accuracy. We first introduce the problem setup, then illustrate<br>the meta-prompt design.</p>",
            "id": 67,
            "page": 6,
            "text": "Next, we demonstrate the effectiveness of OPRO on prompt optimization, where the objective is to find the prompt that maximizes task accuracy. We first introduce the problem setup, then illustrate the meta-prompt design."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 2644
                },
                {
                    "x": 855,
                    "y": 2644
                },
                {
                    "x": 855,
                    "y": 2690
                },
                {
                    "x": 445,
                    "y": 2690
                }
            ],
            "category": "paragraph",
            "html": "<p id='68' style='font-size:16px'>4.1 PROBLEM SETUP</p>",
            "id": 68,
            "page": 6,
            "text": "4.1 PROBLEM SETUP"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2730
                },
                {
                    "x": 2110,
                    "y": 2730
                },
                {
                    "x": 2110,
                    "y": 3054
                },
                {
                    "x": 441,
                    "y": 3054
                }
            ],
            "category": "paragraph",
            "html": "<p id='69' style='font-size:16px'>We focus on prompt optimization for natural language tasks, where both the input and output are in<br>the text format. The task is represented as a dataset with training and test splits, where the training<br>set is used to calculate the training accuracy as the objective value during the optimization process,<br>and we compute the test accuracy on the test set after the optimization finishes. While traditional<br>optimization often requires a decently large training set, our experiment shows that a small number<br>or fraction of training samples (e.g., 3.5% of the training set for GSM8K (Cobbe et al., 2021), 20%<br>for Big-Bench Hard (Suzgun et al., 2022)) is sufficient. The objective function evaluator is an LLM</p>",
            "id": 69,
            "page": 6,
            "text": "We focus on prompt optimization for natural language tasks, where both the input and output are in the text format. The task is represented as a dataset with training and test splits, where the training set is used to calculate the training accuracy as the objective value during the optimization process, and we compute the test accuracy on the test set after the optimization finishes. While traditional optimization often requires a decently large training set, our experiment shows that a small number or fraction of training samples (e.g., 3.5% of the training set for GSM8K (Cobbe , 2021), 20% for Big-Bench Hard (Suzgun , 2022)) is sufficient. The objective function evaluator is an LLM"
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3135
                },
                {
                    "x": 1289,
                    "y": 3135
                },
                {
                    "x": 1289,
                    "y": 3171
                },
                {
                    "x": 1260,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='70' style='font-size:16px'>6</footer>",
            "id": 70,
            "page": 6,
            "text": "6"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 114
                },
                {
                    "x": 1099,
                    "y": 114
                },
                {
                    "x": 1099,
                    "y": 157
                },
                {
                    "x": 444,
                    "y": 157
                }
            ],
            "category": "header",
            "html": "<header id='71' style='font-size:22px'>Large Language Models as Optimizers</header>",
            "id": 71,
            "page": 7,
            "text": "Large Language Models as Optimizers"
        },
        {
            "bounding_box": [
                {
                    "x": 466,
                    "y": 348
                },
                {
                    "x": 2130,
                    "y": 348
                },
                {
                    "x": 2130,
                    "y": 440
                },
                {
                    "x": 466,
                    "y": 440
                }
            ],
            "category": "paragraph",
            "html": "<p id='72' style='font-size:18px'>I have some texts along with their corresponding scores. The texts are arranged in ascending order<br>based on their scores, where higher scores indicate better quality.</p>",
            "id": 72,
            "page": 7,
            "text": "I have some texts along with their corresponding scores. The texts are arranged in ascending order based on their scores, where higher scores indicate better quality."
        },
        {
            "bounding_box": [
                {
                    "x": 468,
                    "y": 485
                },
                {
                    "x": 782,
                    "y": 485
                },
                {
                    "x": 782,
                    "y": 661
                },
                {
                    "x": 468,
                    "y": 661
                }
            ],
            "category": "paragraph",
            "html": "<p id='73' style='font-size:16px'>text:<br>Let's figure it out!<br>score:<br>61</p>",
            "id": 73,
            "page": 7,
            "text": "text: Let's figure it out! score: 61"
        },
        {
            "bounding_box": [
                {
                    "x": 467,
                    "y": 708
                },
                {
                    "x": 879,
                    "y": 708
                },
                {
                    "x": 879,
                    "y": 885
                },
                {
                    "x": 467,
                    "y": 885
                }
            ],
            "category": "paragraph",
            "html": "<p id='74' style='font-size:16px'>text:<br>Let's solve the problem.<br>score:<br>63</p>",
            "id": 74,
            "page": 7,
            "text": "text: Let's solve the problem. score: 63"
        },
        {
            "bounding_box": [
                {
                    "x": 466,
                    "y": 932
                },
                {
                    "x": 1103,
                    "y": 932
                },
                {
                    "x": 1103,
                    "y": 977
                },
                {
                    "x": 466,
                    "y": 977
                }
            ],
            "category": "paragraph",
            "html": "<p id='75' style='font-size:14px'>(. . more instructions and scores · )</p>",
            "id": 75,
            "page": 7,
            "text": "(. . more instructions and scores · )"
        },
        {
            "bounding_box": [
                {
                    "x": 467,
                    "y": 1017
                },
                {
                    "x": 2133,
                    "y": 1017
                },
                {
                    "x": 2133,
                    "y": 1154
                },
                {
                    "x": 467,
                    "y": 1154
                }
            ],
            "category": "paragraph",
            "html": "<p id='76' style='font-size:18px'>The following exemplars show how to apply your text: you replace <INS> in each input with your<br>text, then read the input and give an output. We say your output is wrong if your output is different<br>from the given output, and we say your output is correct if they are the same.</p>",
            "id": 76,
            "page": 7,
            "text": "The following exemplars show how to apply your text: you replace <INS> in each input with your text, then read the input and give an output. We say your output is wrong if your output is different from the given output, and we say your output is correct if they are the same."
        },
        {
            "bounding_box": [
                {
                    "x": 466,
                    "y": 1199
                },
                {
                    "x": 2134,
                    "y": 1199
                },
                {
                    "x": 2134,
                    "y": 1508
                },
                {
                    "x": 466,
                    "y": 1508
                }
            ],
            "category": "paragraph",
            "html": "<p id='77' style='font-size:18px'>input:<br>Q: Alannah, Beatrix, and Queen are preparing for the new school year and have been given books<br>by their parents. Alannah has 20 more books than Beatrix. Queen has 1/5 times more books than<br>Alannah. If Beatrix has 30 books, how many books do the three have together?<br>A: <INS><br>output:<br>140</p>",
            "id": 77,
            "page": 7,
            "text": "input: Q: Alannah, Beatrix, and Queen are preparing for the new school year and have been given books by their parents. Alannah has 20 more books than Beatrix. Queen has 1/5 times more books than Alannah. If Beatrix has 30 books, how many books do the three have together? A: <INS> output: 140"
        },
        {
            "bounding_box": [
                {
                    "x": 465,
                    "y": 1559
                },
                {
                    "x": 895,
                    "y": 1559
                },
                {
                    "x": 895,
                    "y": 1603
                },
                {
                    "x": 465,
                    "y": 1603
                }
            ],
            "category": "paragraph",
            "html": "<p id='78' style='font-size:14px'>(... more exemplars · )</p>",
            "id": 78,
            "page": 7,
            "text": "(... more exemplars · )"
        },
        {
            "bounding_box": [
                {
                    "x": 465,
                    "y": 1643
                },
                {
                    "x": 2130,
                    "y": 1643
                },
                {
                    "x": 2130,
                    "y": 1735
                },
                {
                    "x": 465,
                    "y": 1735
                }
            ],
            "category": "paragraph",
            "html": "<p id='79' style='font-size:16px'>Write your new text that is different from the old ones and has a score as high as possible. Write the<br>text in square brackets.</p>",
            "id": 79,
            "page": 7,
            "text": "Write your new text that is different from the old ones and has a score as high as possible. Write the text in square brackets."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1802
                },
                {
                    "x": 2111,
                    "y": 1802
                },
                {
                    "x": 2111,
                    "y": 2035
                },
                {
                    "x": 442,
                    "y": 2035
                }
            ],
            "category": "paragraph",
            "html": "<p id='80' style='font-size:18px'>Figure 3: An example of the meta-prompt for prompt optimization with instruction-tuned PaLM 2-L<br>(PaLM 2-L-IT) on GSM8K, where the generated instruction will be prepended to the beginning<br>of\"A:\" in the scorer LLM output (A_begin in Section 4.1). <INS> denotes the position where the<br>generated instruction will be added. The blue text contains solution-score pairs; the purple text<br>describes the optimization task and output format; the orange text are meta-instructions.</p>",
            "id": 80,
            "page": 7,
            "text": "Figure 3: An example of the meta-prompt for prompt optimization with instruction-tuned PaLM 2-L (PaLM 2-L-IT) on GSM8K, where the generated instruction will be prepended to the beginning of\"A:\" in the scorer LLM output (A_begin in Section 4.1). <INS> denotes the position where the generated instruction will be added. The blue text contains solution-score pairs; the purple text describes the optimization task and output format; the orange text are meta-instructions."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2137
                },
                {
                    "x": 2108,
                    "y": 2137
                },
                {
                    "x": 2108,
                    "y": 2274
                },
                {
                    "x": 442,
                    "y": 2274
                }
            ],
            "category": "paragraph",
            "html": "<p id='81' style='font-size:18px'>to which the optimized prompt will be applied, and it can be the same or different from the LLM for<br>optimization. We denote the LLM for objective function evaluation as the scorer LLM, and the LLM<br>for optimization as the optimizer LLM.</p>",
            "id": 81,
            "page": 7,
            "text": "to which the optimized prompt will be applied, and it can be the same or different from the LLM for optimization. We denote the LLM for objective function evaluation as the scorer LLM, and the LLM for optimization as the optimizer LLM."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2300
                },
                {
                    "x": 2111,
                    "y": 2300
                },
                {
                    "x": 2111,
                    "y": 2394
                },
                {
                    "x": 442,
                    "y": 2394
                }
            ],
            "category": "paragraph",
            "html": "<p id='82' style='font-size:18px'>The output of the optimizer LLM is an instruction, which is concatenated to the question part of every<br>exemplar and prompts the scorer LLM. We consider the following positions to insert the instruction:</p>",
            "id": 82,
            "page": 7,
            "text": "The output of the optimizer LLM is an instruction, which is concatenated to the question part of every exemplar and prompts the scorer LLM. We consider the following positions to insert the instruction:"
        },
        {
            "bounding_box": [
                {
                    "x": 489,
                    "y": 2432
                },
                {
                    "x": 2109,
                    "y": 2432
                },
                {
                    "x": 2109,
                    "y": 2700
                },
                {
                    "x": 489,
                    "y": 2700
                }
            ],
            "category": "paragraph",
            "html": "<p id='83' style='font-size:18px'>· Q_begin: the instruction is added before the original question.<br>· Q_end: the instruction is added after the original question.<br>· A_begin: the instruction is added to the beginning of the scorer LLM output. This is applicable<br>to pretrained LLMs without instruction tuning, where the prompt is formatted as a sequence of<br>QA pairs.</p>",
            "id": 83,
            "page": 7,
            "text": "· Q_begin: the instruction is added before the original question. · Q_end: the instruction is added after the original question. · A_begin: the instruction is added to the beginning of the scorer LLM output. This is applicable to pretrained LLMs without instruction tuning, where the prompt is formatted as a sequence of QA pairs."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2742
                },
                {
                    "x": 1359,
                    "y": 2742
                },
                {
                    "x": 1359,
                    "y": 2790
                },
                {
                    "x": 444,
                    "y": 2790
                }
            ],
            "category": "paragraph",
            "html": "<p id='84' style='font-size:20px'>We exemplify these prompting formats in Appendix B.</p>",
            "id": 84,
            "page": 7,
            "text": "We exemplify these prompting formats in Appendix B."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2867
                },
                {
                    "x": 972,
                    "y": 2867
                },
                {
                    "x": 972,
                    "y": 2913
                },
                {
                    "x": 444,
                    "y": 2913
                }
            ],
            "category": "paragraph",
            "html": "<p id='85' style='font-size:20px'>4.2 META-PROMPT DESIGN</p>",
            "id": 85,
            "page": 7,
            "text": "4.2 META-PROMPT DESIGN"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2959
                },
                {
                    "x": 2110,
                    "y": 2959
                },
                {
                    "x": 2110,
                    "y": 3052
                },
                {
                    "x": 443,
                    "y": 3052
                }
            ],
            "category": "paragraph",
            "html": "<p id='86' style='font-size:18px'>Figure 3 shows an example of the meta-prompt for prompt optimization on GSM8K (Cobbe et al.,<br>2021). More details are as follows.</p>",
            "id": 86,
            "page": 7,
            "text": "Figure 3 shows an example of the meta-prompt for prompt optimization on GSM8K (Cobbe , 2021). More details are as follows."
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3134
                },
                {
                    "x": 1288,
                    "y": 3134
                },
                {
                    "x": 1288,
                    "y": 3170
                },
                {
                    "x": 1260,
                    "y": 3170
                }
            ],
            "category": "footer",
            "html": "<footer id='87' style='font-size:18px'>7</footer>",
            "id": 87,
            "page": 7,
            "text": "7"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 113
                },
                {
                    "x": 1099,
                    "y": 113
                },
                {
                    "x": 1099,
                    "y": 158
                },
                {
                    "x": 443,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='88' style='font-size:22px'>Large Language Models as Optimizers</header>",
            "id": 88,
            "page": 8,
            "text": "Large Language Models as Optimizers"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 344
                },
                {
                    "x": 2108,
                    "y": 344
                },
                {
                    "x": 2108,
                    "y": 669
                },
                {
                    "x": 442,
                    "y": 669
                }
            ],
            "category": "paragraph",
            "html": "<p id='89' style='font-size:20px'>Optimization problem examples. The problem description includes a few examples taken from the<br>training set to demonstrate the task for the generated instructions. For example, from the input-output<br>pair in Figure 3, we can infer this is a math word problem. The input-output pair also demonstrates<br>the position where the generated instruction will be added to, and this is essential for the optimizer<br>LLM to generate instructions of the same style. In each optimization step, we add several (three for<br>example) training examples to the meta-prompt by random sampling the training set or choose the<br>ones the previous instructions fall short of.</p>",
            "id": 89,
            "page": 8,
            "text": "Optimization problem examples. The problem description includes a few examples taken from the training set to demonstrate the task for the generated instructions. For example, from the input-output pair in Figure 3, we can infer this is a math word problem. The input-output pair also demonstrates the position where the generated instruction will be added to, and this is essential for the optimizer LLM to generate instructions of the same style. In each optimization step, we add several (three for example) training examples to the meta-prompt by random sampling the training set or choose the ones the previous instructions fall short of."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 690
                },
                {
                    "x": 2109,
                    "y": 690
                },
                {
                    "x": 2109,
                    "y": 877
                },
                {
                    "x": 442,
                    "y": 877
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='90' style='font-size:20px'>Optimization trajectory. The optimization trajectory includes instructions generated from the past<br>optimization steps, along with their scores. The old instructions and scores are sorted by the score in<br>ascending order. The score is the training accuracy in prompt optimization. We only keep instructions<br>with the highest scores in the meta-prompt in consideration of the LLM context length limit.</p>",
            "id": 90,
            "page": 8,
            "text": "Optimization trajectory. The optimization trajectory includes instructions generated from the past optimization steps, along with their scores. The old instructions and scores are sorted by the score in ascending order. The score is the training accuracy in prompt optimization. We only keep instructions with the highest scores in the meta-prompt in consideration of the LLM context length limit."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 898
                },
                {
                    "x": 2108,
                    "y": 898
                },
                {
                    "x": 2108,
                    "y": 1039
                },
                {
                    "x": 441,
                    "y": 1039
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='91' style='font-size:18px'>Meta-instructions. We also add meta-instructions: the instructions to the optimizer LLM that explain<br>the optimization goal and instruct the model how to use the above information. The meta-instructions<br>may also specify the desired generated instruction format for easier parsing.</p>",
            "id": 91,
            "page": 8,
            "text": "Meta-instructions. We also add meta-instructions: the instructions to the optimizer LLM that explain the optimization goal and instruct the model how to use the above information. The meta-instructions may also specify the desired generated instruction format for easier parsing."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1103
                },
                {
                    "x": 1359,
                    "y": 1103
                },
                {
                    "x": 1359,
                    "y": 1156
                },
                {
                    "x": 443,
                    "y": 1156
                }
            ],
            "category": "paragraph",
            "html": "<p id='92' style='font-size:22px'>5 PROMPT OPTIMIZATION EXPERIMENTS</p>",
            "id": 92,
            "page": 8,
            "text": "5 PROMPT OPTIMIZATION EXPERIMENTS"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1205
                },
                {
                    "x": 2106,
                    "y": 1205
                },
                {
                    "x": 2106,
                    "y": 1344
                },
                {
                    "x": 441,
                    "y": 1344
                }
            ],
            "category": "paragraph",
            "html": "<p id='93' style='font-size:18px'>We present the evaluation results for prompt optimization in this section. Our experiments demonstrate<br>that OPRO brings a significant performance gain across the board, with different combinations of<br>LLMs as the optimizer and the scorer.</p>",
            "id": 93,
            "page": 8,
            "text": "We present the evaluation results for prompt optimization in this section. Our experiments demonstrate that OPRO brings a significant performance gain across the board, with different combinations of LLMs as the optimizer and the scorer."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1368
                },
                {
                    "x": 2109,
                    "y": 1368
                },
                {
                    "x": 2109,
                    "y": 1554
                },
                {
                    "x": 442,
                    "y": 1554
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='94' style='font-size:20px'>Section 5.1 describes the experiment setup. Section 5.2 shows main results on reasoning tasks like<br>GSM8K and BBH. Section 5.3 shows ablation studies. Section 5.4 analyzes overfitting in prompt<br>optimization. Section 5.5 compares the prompt optimization performance of meta-prompts in OPRO<br>and EvoPrompt (Guo et al., 2023).</p>",
            "id": 94,
            "page": 8,
            "text": "Section 5.1 describes the experiment setup. Section 5.2 shows main results on reasoning tasks like GSM8K and BBH. Section 5.3 shows ablation studies. Section 5.4 analyzes overfitting in prompt optimization. Section 5.5 compares the prompt optimization performance of meta-prompts in OPRO and EvoPrompt (Guo , 2023)."
        },
        {
            "bounding_box": [
                {
                    "x": 446,
                    "y": 1606
                },
                {
                    "x": 912,
                    "y": 1606
                },
                {
                    "x": 912,
                    "y": 1654
                },
                {
                    "x": 446,
                    "y": 1654
                }
            ],
            "category": "paragraph",
            "html": "<p id='95' style='font-size:18px'>5.1 EVALUATION SETUP</p>",
            "id": 95,
            "page": 8,
            "text": "5.1 EVALUATION SETUP"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1693
                },
                {
                    "x": 1509,
                    "y": 1693
                },
                {
                    "x": 1509,
                    "y": 1742
                },
                {
                    "x": 442,
                    "y": 1742
                }
            ],
            "category": "paragraph",
            "html": "<p id='96' style='font-size:14px'>Models. The LLMs we use as the optimizer and the scorer are:</p>",
            "id": 96,
            "page": 8,
            "text": "Models. The LLMs we use as the optimizer and the scorer are:"
        },
        {
            "bounding_box": [
                {
                    "x": 490,
                    "y": 1782
                },
                {
                    "x": 2108,
                    "y": 1782
                },
                {
                    "x": 2108,
                    "y": 1928
                },
                {
                    "x": 490,
                    "y": 1928
                }
            ],
            "category": "paragraph",
            "html": "<p id='97' style='font-size:14px'>· Optimizer LLM: Pre-trained PaLM 2-L (Anil et al., 2023), instruction-tuned PaLM 2-L<br>(denoted PaLM 2-L-IT), text-bison, gpt-3 · 5-turbo, and gpt-4.<br>· Scorer LLM: Pre-trained PaLM 2-L and text-bi son.</p>",
            "id": 97,
            "page": 8,
            "text": "· Optimizer LLM: Pre-trained PaLM 2-L (Anil , 2023), instruction-tuned PaLM 2-L (denoted PaLM 2-L-IT), text-bison, gpt-3 · 5-turbo, and gpt-4. · Scorer LLM: Pre-trained PaLM 2-L and text-bi son."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1969
                },
                {
                    "x": 2107,
                    "y": 1969
                },
                {
                    "x": 2107,
                    "y": 2110
                },
                {
                    "x": 441,
                    "y": 2110
                }
            ],
            "category": "paragraph",
            "html": "<p id='98' style='font-size:16px'>With pre-trained PaLM 2-L as the scorer, the optimizer LLM generates A_begin instructions.<br>Since text-bison has been instruction-tuned, the optimizer LLM generates Q_begin and Q_end<br>instructions when text-bison is used as the scorer.</p>",
            "id": 98,
            "page": 8,
            "text": "With pre-trained PaLM 2-L as the scorer, the optimizer LLM generates A_begin instructions. Since text-bison has been instruction-tuned, the optimizer LLM generates Q_begin and Q_end instructions when text-bison is used as the scorer."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2131
                },
                {
                    "x": 2108,
                    "y": 2131
                },
                {
                    "x": 2108,
                    "y": 2459
                },
                {
                    "x": 443,
                    "y": 2459
                }
            ],
            "category": "paragraph",
            "html": "<p id='99' style='font-size:20px'>Benchmarks. Our primary evaluation benchmarks are GSM8K (Cobbe et al., 2021) and Big-Bench<br>Hard (BBH) (Suzgun et al., 2022). GSM8K is a benchmark of grade school math word problems<br>with 7,473 training samples and 1,319 test samples, where chain-of-thought prompting (Wei et al.,<br>2022) and the zero-shot instruction \"Let's think step by step.' , (Kojima et al., 2022) have drastically<br>improved the performance over the standard prompting. BBH is a suite of 23 challenging BIG-Bench<br>tasks (Srivastava et al., 2022) that covers a wide range of topics beyond arithmetic reasoning, including<br>symbolic manipulation and commonsense reasoning. Each task contains up to 250 examples in total.</p>",
            "id": 99,
            "page": 8,
            "text": "Benchmarks. Our primary evaluation benchmarks are GSM8K (Cobbe , 2021) and Big-Bench Hard (BBH) (Suzgun , 2022). GSM8K is a benchmark of grade school math word problems with 7,473 training samples and 1,319 test samples, where chain-of-thought prompting (Wei , 2022) and the zero-shot instruction \"Let's think step by step.' , (Kojima , 2022) have drastically improved the performance over the standard prompting. BBH is a suite of 23 challenging BIG-Bench tasks (Srivastava , 2022) that covers a wide range of topics beyond arithmetic reasoning, including symbolic manipulation and commonsense reasoning. Each task contains up to 250 examples in total."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2477
                },
                {
                    "x": 2109,
                    "y": 2477
                },
                {
                    "x": 2109,
                    "y": 2615
                },
                {
                    "x": 442,
                    "y": 2615
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='100' style='font-size:20px'>To examine the transferability of the optimized instructions, we also evaluate the instructions op-<br>timized for GSM8K on two other mathematical reasoning datasets, i.e., MultiArith (Roy & Roth,<br>2016) and AQuA (Ling et al., 2017).</p>",
            "id": 100,
            "page": 8,
            "text": "To examine the transferability of the optimized instructions, we also evaluate the instructions optimized for GSM8K on two other mathematical reasoning datasets, i.e., MultiArith (Roy & Roth, 2016) and AQuA (Ling , 2017)."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2639
                },
                {
                    "x": 2109,
                    "y": 2639
                },
                {
                    "x": 2109,
                    "y": 3052
                },
                {
                    "x": 442,
                    "y": 3052
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='101' style='font-size:18px'>Implementation details. We set the temperature to be 0 when evaluating the performance of<br>generated instructions, in which case the scorer LLM greedily decodes. Unless otherwise specified, we<br>set the default temperature to be 1.0 for optimizer LLMs to generate diverse and creative instructions.<br>At each optimization step, we prompt the optimizer LLM with the meta-prompt 8 times to generate 8<br>instructions, then we add these instructions with their training scores to the optimization trajectory<br>in the meta-prompt. Our meta-prompt at each step contains the best 20 instructions so far and 3<br>randomly picked exemplars from the training set. We study the effect of different hyperparameters in<br>ablation studies (Section 5.3). Appendix C.2 presents the full meta-prompts for different optimizer<br>LLMs.</p>",
            "id": 101,
            "page": 8,
            "text": "Implementation details. We set the temperature to be 0 when evaluating the performance of generated instructions, in which case the scorer LLM greedily decodes. Unless otherwise specified, we set the default temperature to be 1.0 for optimizer LLMs to generate diverse and creative instructions. At each optimization step, we prompt the optimizer LLM with the meta-prompt 8 times to generate 8 instructions, then we add these instructions with their training scores to the optimization trajectory in the meta-prompt. Our meta-prompt at each step contains the best 20 instructions so far and 3 randomly picked exemplars from the training set. We study the effect of different hyperparameters in ablation studies (Section 5.3). Appendix C.2 presents the full meta-prompts for different optimizer LLMs."
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3133
                },
                {
                    "x": 1289,
                    "y": 3133
                },
                {
                    "x": 1289,
                    "y": 3170
                },
                {
                    "x": 1260,
                    "y": 3170
                }
            ],
            "category": "footer",
            "html": "<footer id='102' style='font-size:18px'>8</footer>",
            "id": 102,
            "page": 8,
            "text": "8"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 113
                },
                {
                    "x": 1099,
                    "y": 113
                },
                {
                    "x": 1099,
                    "y": 158
                },
                {
                    "x": 444,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='103' style='font-size:22px'>Large Language Models as Optimizers</header>",
            "id": 103,
            "page": 9,
            "text": "Large Language Models as Optimizers"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 374
                },
                {
                    "x": 2105,
                    "y": 374
                },
                {
                    "x": 2105,
                    "y": 469
                },
                {
                    "x": 443,
                    "y": 469
                }
            ],
            "category": "caption",
            "html": "<caption id='104' style='font-size:18px'>Table 4: Test accuracies on GSM8K. We show the instruction with the highest test accuracy for each<br>scorer-optimizer pair.</caption>",
            "id": 104,
            "page": 9,
            "text": "Table 4: Test accuracies on GSM8K. We show the instruction with the highest test accuracy for each scorer-optimizer pair."
        },
        {
            "bounding_box": [
                {
                    "x": 458,
                    "y": 482
                },
                {
                    "x": 2110,
                    "y": 482
                },
                {
                    "x": 2110,
                    "y": 2109
                },
                {
                    "x": 458,
                    "y": 2109
                }
            ],
            "category": "table",
            "html": "<table id='105' style='font-size:14px'><tr><td>Scorer</td><td>Optimizer / Source</td><td>Instruction position</td><td>Top instruction</td><td>Acc</td></tr><tr><td colspan=\"5\">Baselines</td></tr><tr><td>PaLM 2-L</td><td>(Kojima et al., 2022)</td><td>A_begin</td><td>Let's think step by step.</td><td>71.8</td></tr><tr><td>PaLM 2-L</td><td>(Zhou et al., 2022b)</td><td>A_begin</td><td>Let's work this out in a step by step way to be sure we have the right answer.</td><td>58.8</td></tr><tr><td>PaLM 2-L</td><td></td><td>A_begin</td><td>Let's solve the problem.</td><td>60.8</td></tr><tr><td>PaLM 2-L</td><td></td><td>A_begin</td><td>(empty string)</td><td>34.0</td></tr><tr><td>text-bison</td><td>(Kojima et al., 2022)</td><td>Q_begin</td><td>Let's think step by step.</td><td>64.4</td></tr><tr><td>text-bison</td><td>(Zhou et al., 2022b)</td><td>Q_begin</td><td>Let's work this out in a step by step way to be sure we have the right answer.</td><td>65.6</td></tr><tr><td>text-bison</td><td></td><td>Q_begin</td><td>Let's solve the problem.</td><td>59.1</td></tr><tr><td>text-bison</td><td></td><td>Q_begin</td><td>(empty string)</td><td>56.8</td></tr><tr><td colspan=\"5\">Ours</td></tr><tr><td>PaLM 2-L</td><td>PaLM 2-L-IT</td><td>A_begin</td><td>Take a deep breath and work on this problem step-by-step.</td><td>- 80.2</td></tr><tr><td>PaLM 2-L</td><td>PaLM 2-L</td><td>A_begin</td><td>Break this down.</td><td>79.9</td></tr><tr><td>PaLM 2-L</td><td>gpt-3 5-turbo</td><td>A_begin</td><td>A little bit of arithmetic and a logical approach will help us quickly arrive at the solution to this problem.</td><td>78.5</td></tr><tr><td>PaLM 2-L</td><td>gpt-4</td><td>A_begin</td><td>Let's combine our numerical command and clear thinking to quickly and accurately decipher the answer.</td><td>74.5</td></tr><tr><td>text-bison</td><td>PaLM 2-L-IT</td><td>Q_begin</td><td>Let's work together to solve math word problems! First, we will read and discuss the problem together to make sure we understand it. Then, we will work together to find the solution. I will give you hints and help you work through the problem if you get stuck.</td><td>64.4</td></tr><tr><td>text-bison</td><td>text-bison</td><td>Q_end</td><td>Let's work through this problem step-by-step:</td><td>68.5</td></tr><tr><td>text-bison</td><td>gpt-3 5-turbo</td><td>Q_end</td><td>Analyze the given information, break down the problem into manageable steps, apply suitable mathematical operations, and provide a clear, accurate, and concise solution, ensuring precise rounding if necessary. Consider all variables and carefully consider the problem's context for an efficient solution.</td><td>66.5</td></tr><tr><td>text-bison</td><td>gpt-4</td><td>Q_begin</td><td>Start by dissecting the problem to highlight important numbers and their relations. Decide on the necessary mathematical operations like addition, subtraction, multiplication, or division, required for resolution. Implement these operations, keeping in mind any units or conditions. Round off by ensuring your solution fits the context of the problem to ensure accuracy.</td><td>62.7</td></tr></table>",
            "id": 105,
            "page": 9,
            "text": "Scorer Optimizer / Source Instruction position Top instruction Acc  Baselines  PaLM 2-L (Kojima , 2022) A_begin Let's think step by step. 71.8  PaLM 2-L (Zhou , 2022b) A_begin Let's work this out in a step by step way to be sure we have the right answer. 58.8  PaLM 2-L  A_begin Let's solve the problem. 60.8  PaLM 2-L  A_begin (empty string) 34.0  text-bison (Kojima , 2022) Q_begin Let's think step by step. 64.4  text-bison (Zhou , 2022b) Q_begin Let's work this out in a step by step way to be sure we have the right answer. 65.6  text-bison  Q_begin Let's solve the problem. 59.1  text-bison  Q_begin (empty string) 56.8  Ours  PaLM 2-L PaLM 2-L-IT A_begin Take a deep breath and work on this problem step-by-step. - 80.2  PaLM 2-L PaLM 2-L A_begin Break this down. 79.9  PaLM 2-L gpt-3 5-turbo A_begin A little bit of arithmetic and a logical approach will help us quickly arrive at the solution to this problem. 78.5  PaLM 2-L gpt-4 A_begin Let's combine our numerical command and clear thinking to quickly and accurately decipher the answer. 74.5  text-bison PaLM 2-L-IT Q_begin Let's work together to solve math word problems! First, we will read and discuss the problem together to make sure we understand it. Then, we will work together to find the solution. I will give you hints and help you work through the problem if you get stuck. 64.4  text-bison text-bison Q_end Let's work through this problem step-by-step: 68.5  text-bison gpt-3 5-turbo Q_end Analyze the given information, break down the problem into manageable steps, apply suitable mathematical operations, and provide a clear, accurate, and concise solution, ensuring precise rounding if necessary. Consider all variables and carefully consider the problem's context for an efficient solution. 66.5  text-bison gpt-4 Q_begin Start by dissecting the problem to highlight important numbers and their relations. Decide on the necessary mathematical operations like addition, subtraction, multiplication, or division, required for resolution. Implement these operations, keeping in mind any units or conditions. Round off by ensuring your solution fits the context of the problem to ensure accuracy."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 2224
                },
                {
                    "x": 830,
                    "y": 2224
                },
                {
                    "x": 830,
                    "y": 2272
                },
                {
                    "x": 445,
                    "y": 2272
                }
            ],
            "category": "paragraph",
            "html": "<p id='106' style='font-size:18px'>5.2 MAIN RESULTS</p>",
            "id": 106,
            "page": 9,
            "text": "5.2 MAIN RESULTS"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2317
                },
                {
                    "x": 2107,
                    "y": 2317
                },
                {
                    "x": 2107,
                    "y": 2457
                },
                {
                    "x": 442,
                    "y": 2457
                }
            ],
            "category": "paragraph",
            "html": "<p id='107' style='font-size:16px'>We show prompt optimization curves on GSM8K and two BBH tasks in this section. The curves on<br>other BBH tasks are deferred to Appendix D, and the tables containing all accuracy numbers are in<br>Appendix E.</p>",
            "id": 107,
            "page": 9,
            "text": "We show prompt optimization curves on GSM8K and two BBH tasks in this section. The curves on other BBH tasks are deferred to Appendix D, and the tables containing all accuracy numbers are in Appendix E."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2526
                },
                {
                    "x": 739,
                    "y": 2526
                },
                {
                    "x": 739,
                    "y": 2572
                },
                {
                    "x": 444,
                    "y": 2572
                }
            ],
            "category": "paragraph",
            "html": "<p id='108' style='font-size:20px'>5.2.1 GSM8K</p>",
            "id": 108,
            "page": 9,
            "text": "5.2.1 GSM8K"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2614
                },
                {
                    "x": 2107,
                    "y": 2614
                },
                {
                    "x": 2107,
                    "y": 2846
                },
                {
                    "x": 442,
                    "y": 2846
                }
            ],
            "category": "paragraph",
            "html": "<p id='109' style='font-size:18px'>For prompt optimization, we randomly sample 3.5% examples from the GSM8K training set. The<br>same subset is used throughout optimization, SO that the task accuracies computed at intermediate<br>optimization steps are approximations of the training accuracy on all 7,473 training examples. This<br>balances the evaluation cost with the generalization performance. After the optimization procedure<br>finishes, we evaluate the found instructions on the entire GSM8K test set.</p>",
            "id": 109,
            "page": 9,
            "text": "For prompt optimization, we randomly sample 3.5% examples from the GSM8K training set. The same subset is used throughout optimization, SO that the task accuracies computed at intermediate optimization steps are approximations of the training accuracy on all 7,473 training examples. This balances the evaluation cost with the generalization performance. After the optimization procedure finishes, we evaluate the found instructions on the entire GSM8K test set."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2867
                },
                {
                    "x": 2109,
                    "y": 2867
                },
                {
                    "x": 2109,
                    "y": 3054
                },
                {
                    "x": 441,
                    "y": 3054
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='110' style='font-size:18px'>Figure 1(a) in Section 1 shows prompt optimization curves with pre-trained PaLM 2-L as scorer<br>and PaLM 2-L-IT as optimizer, and the initial instruction is \"Let's solve the problem\" with a<br>(approximated, and same below) training accuracy of 60.5. We observe that the optimization curve<br>shows an overall upward trend with several leaps throughout the optimization process, for example:</p>",
            "id": 110,
            "page": 9,
            "text": "Figure 1(a) in Section 1 shows prompt optimization curves with pre-trained PaLM 2-L as scorer and PaLM 2-L-IT as optimizer, and the initial instruction is \"Let's solve the problem\" with a (approximated, and same below) training accuracy of 60.5. We observe that the optimization curve shows an overall upward trend with several leaps throughout the optimization process, for example:"
        },
        {
            "bounding_box": [
                {
                    "x": 1259,
                    "y": 3133
                },
                {
                    "x": 1289,
                    "y": 3133
                },
                {
                    "x": 1289,
                    "y": 3169
                },
                {
                    "x": 1259,
                    "y": 3169
                }
            ],
            "category": "footer",
            "html": "<footer id='111' style='font-size:18px'>9</footer>",
            "id": 111,
            "page": 9,
            "text": "9"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 113
                },
                {
                    "x": 1099,
                    "y": 113
                },
                {
                    "x": 1099,
                    "y": 158
                },
                {
                    "x": 443,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='112' style='font-size:20px'>Large Language Models as Optimizers</header>",
            "id": 112,
            "page": 10,
            "text": "Large Language Models as Optimizers"
        },
        {
            "bounding_box": [
                {
                    "x": 487,
                    "y": 346
                },
                {
                    "x": 2106,
                    "y": 346
                },
                {
                    "x": 2106,
                    "y": 439
                },
                {
                    "x": 487,
                    "y": 439
                }
            ],
            "category": "paragraph",
            "html": "<p id='113' style='font-size:18px'>· \"Let's think carefully about the problem and solve it together.\" at Step 2 with the training<br>accuracy 63.2;</p>",
            "id": 113,
            "page": 10,
            "text": "· \"Let's think carefully about the problem and solve it together.\" at Step 2 with the training accuracy 63.2;"
        },
        {
            "bounding_box": [
                {
                    "x": 489,
                    "y": 472
                },
                {
                    "x": 1538,
                    "y": 472
                },
                {
                    "x": 1538,
                    "y": 520
                },
                {
                    "x": 489,
                    "y": 520
                }
            ],
            "category": "paragraph",
            "html": "<p id='114' style='font-size:16px'>· \"Let's break it down!\" at Step 4 with training accuracy 71.3;</p>",
            "id": 114,
            "page": 10,
            "text": "· \"Let's break it down!\" at Step 4 with training accuracy 71.3;"
        },
        {
            "bounding_box": [
                {
                    "x": 490,
                    "y": 553
                },
                {
                    "x": 1850,
                    "y": 553
                },
                {
                    "x": 1850,
                    "y": 600
                },
                {
                    "x": 490,
                    "y": 600
                }
            ],
            "category": "paragraph",
            "html": "<p id='115' style='font-size:14px'>· \"Let's calculate our way to the solution!\" at Step 5 with training accuracy 73.9;</p>",
            "id": 115,
            "page": 10,
            "text": "· \"Let's calculate our way to the solution!\" at Step 5 with training accuracy 73.9;"
        },
        {
            "bounding_box": [
                {
                    "x": 491,
                    "y": 633
                },
                {
                    "x": 1501,
                    "y": 633
                },
                {
                    "x": 1501,
                    "y": 679
                },
                {
                    "x": 491,
                    "y": 679
                }
            ],
            "category": "paragraph",
            "html": "<p id='116' style='font-size:14px'>· \"Let's do the math!\" at Step 6 with training accuracy 78.2.</p>",
            "id": 116,
            "page": 10,
            "text": "· \"Let's do the math!\" at Step 6 with training accuracy 78.2."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 702
                },
                {
                    "x": 2107,
                    "y": 702
                },
                {
                    "x": 2107,
                    "y": 840
                },
                {
                    "x": 441,
                    "y": 840
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='117' style='font-size:18px'>The optimization curves also generally show a decrease of the variance among the accuracies of<br>instructions generated at each step, indicating that the optimizer LLM generates distributionally<br>better instructions throughout the optimization.</p>",
            "id": 117,
            "page": 10,
            "text": "The optimization curves also generally show a decrease of the variance among the accuracies of instructions generated at each step, indicating that the optimizer LLM generates distributionally better instructions throughout the optimization."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 864
                },
                {
                    "x": 2107,
                    "y": 864
                },
                {
                    "x": 2107,
                    "y": 1049
                },
                {
                    "x": 441,
                    "y": 1049
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='118' style='font-size:18px'>Next, we present the results of generating Q_begin instructions with the text-bi son scorer and<br>the PaLM 2-L-IT optimizer, starting from an empty instruction with a 57.1 training accuracy. The<br>optimization curve in Figure 4(a) shows a similar upward trend, during which a few leaps in the<br>training accuracy include:</p>",
            "id": 118,
            "page": 10,
            "text": "Next, we present the results of generating Q_begin instructions with the text-bi son scorer and the PaLM 2-L-IT optimizer, starting from an empty instruction with a 57.1 training accuracy. The optimization curve in Figure 4(a) shows a similar upward trend, during which a few leaps in the training accuracy include:"
        },
        {
            "bounding_box": [
                {
                    "x": 482,
                    "y": 1067
                },
                {
                    "x": 2110,
                    "y": 1067
                },
                {
                    "x": 2110,
                    "y": 1685
                },
                {
                    "x": 482,
                    "y": 1685
                }
            ],
            "category": "paragraph",
            "html": "<p id='119' style='font-size:16px'>· \"Solve the following problems using the given information.\" at Step 2 with training accuracy<br>59.8;<br>● \"Solve the following problems by applying the given information and using the appropriate<br>mathematical operations.' 7 at Step 3 with training accuracy 64.0;<br>· \"Let's read the problem carefully and identify the given information. Then, we can create an<br>equation and solve for the unknown variable.\" at Step 4 with training accuracy 67.0;<br>· \"I'm always down for solving a math word problem together. Just give me a moment to read<br>and understand the problem. Then, I'll create an equation that models the problem, which I'll<br>solve for the unknown variable. I also may or may not use some helpful diagrams or visuals<br>to understand the problem. Lastly, be sure to allow me some time to carefully check my work<br>before submitting any responses!\" at Step 29 with training accuracy 70.1.</p>",
            "id": 119,
            "page": 10,
            "text": "· \"Solve the following problems using the given information.\" at Step 2 with training accuracy 59.8; ● \"Solve the following problems by applying the given information and using the appropriate mathematical operations.' 7 at Step 3 with training accuracy 64.0; · \"Let's read the problem carefully and identify the given information. Then, we can create an equation and solve for the unknown variable.\" at Step 4 with training accuracy 67.0; · \"I'm always down for solving a math word problem together. Just give me a moment to read and understand the problem. Then, I'll create an equation that models the problem, which I'll solve for the unknown variable. I also may or may not use some helpful diagrams or visuals to understand the problem. Lastly, be sure to allow me some time to carefully check my work before submitting any responses!\" at Step 29 with training accuracy 70.1."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1698
                },
                {
                    "x": 2108,
                    "y": 1698
                },
                {
                    "x": 2108,
                    "y": 2250
                },
                {
                    "x": 441,
                    "y": 2250
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='120' style='font-size:18px'>Note that although our default setting is to run OPRO for 200 steps in prompt optimization, we<br>need much fewer steps if the goal is to find some outstanding instructions. An example is that the<br>Figure 1(a) experiment found \"Let's do the math!\" at Step 6 with training accuracy 78.2, almost<br>matching the \"Take a deep breath and work on this problem step-by-step.' , found at the 107th step<br>with training accuracy 80.2, at a point where the optimization curve is still trending upwards. This is<br>because a leap in our optimization curve does not always correspond to a much better instruction being<br>discovered; instead, it can be due to a large qualitative improvement of all 8 generated instructions in<br>this step. The latter usually happens several steps after the former: after a much better instruction is<br>discovered in one step, the meta-prompt gradually gets rid of worse instructions in the latter steps by<br>generating instructions similar to the much-better one. The top instructions kept in the meta-prompt<br>gradually improves in this procedure. At a point when the meta-prompt only triggers higher quality<br>instructions, the leap happens.</p>",
            "id": 120,
            "page": 10,
            "text": "Note that although our default setting is to run OPRO for 200 steps in prompt optimization, we need much fewer steps if the goal is to find some outstanding instructions. An example is that the Figure 1(a) experiment found \"Let's do the math!\" at Step 6 with training accuracy 78.2, almost matching the \"Take a deep breath and work on this problem step-by-step.' , found at the 107th step with training accuracy 80.2, at a point where the optimization curve is still trending upwards. This is because a leap in our optimization curve does not always correspond to a much better instruction being discovered; instead, it can be due to a large qualitative improvement of all 8 generated instructions in this step. The latter usually happens several steps after the former: after a much better instruction is discovered in one step, the meta-prompt gradually gets rid of worse instructions in the latter steps by generating instructions similar to the much-better one. The top instructions kept in the meta-prompt gradually improves in this procedure. At a point when the meta-prompt only triggers higher quality instructions, the leap happens."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2271
                },
                {
                    "x": 2108,
                    "y": 2271
                },
                {
                    "x": 2108,
                    "y": 2688
                },
                {
                    "x": 442,
                    "y": 2688
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='121' style='font-size:16px'>Finally, Figure 4(b) shows that the pre-trained PaLM 2-L can also serve as the optimizer LLM and<br>improve its own prediction performance. Different from other optimizer LLMs that are instruction-<br>tuned, the pre-trained P aLM 2-L performs better when the prompt is formatted in a few-shot manner.<br>Therefore, we include two initial instructions to start the optimization: the empty instruction (with<br>a training accuracy 32.2) and \"The answer is\" (with a training accuracy 33.3). See Figure 21 in<br>Appendix C for the meta-prompt format. The generated instructions follow the same style as \"The<br>answer is\": most instructions are also phrases suitable as the prefix of a sentence, like \"Here you<br>go:\" (generated at Step 11 with training accuracy 61.3) and \"Let's do it:\" (generated at Step 13 with<br>training accuracy 75.1).</p>",
            "id": 121,
            "page": 10,
            "text": "Finally, Figure 4(b) shows that the pre-trained PaLM 2-L can also serve as the optimizer LLM and improve its own prediction performance. Different from other optimizer LLMs that are instructiontuned, the pre-trained P aLM 2-L performs better when the prompt is formatted in a few-shot manner. Therefore, we include two initial instructions to start the optimization: the empty instruction (with a training accuracy 32.2) and \"The answer is\" (with a training accuracy 33.3). See Figure 21 in Appendix C for the meta-prompt format. The generated instructions follow the same style as \"The answer is\": most instructions are also phrases suitable as the prefix of a sentence, like \"Here you go:\" (generated at Step 11 with training accuracy 61.3) and \"Let's do it:\" (generated at Step 13 with training accuracy 75.1)."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2709
                },
                {
                    "x": 2109,
                    "y": 2709
                },
                {
                    "x": 2109,
                    "y": 2802
                },
                {
                    "x": 442,
                    "y": 2802
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='122' style='font-size:14px'>Table 4 summarizes top instructions found on GSM8K with different scorer and optimizer LLMs.<br>We observe that:</p>",
            "id": 122,
            "page": 10,
            "text": "Table 4 summarizes top instructions found on GSM8K with different scorer and optimizer LLMs. We observe that:"
        },
        {
            "bounding_box": [
                {
                    "x": 486,
                    "y": 2821
                },
                {
                    "x": 2111,
                    "y": 2821
                },
                {
                    "x": 2111,
                    "y": 3048
                },
                {
                    "x": 486,
                    "y": 3048
                }
            ],
            "category": "paragraph",
            "html": "<p id='123' style='font-size:14px'>· The styles of instructions found by different optimizer LLMs vary a lot: PaLM 2-L-IT and<br>text-bi son ones are concise, while GPT ones are long and detailed.<br>· Although some top instructions contain the \"step-by-step\" phrase, most others achieve a compa-<br>rable or better accuracy with different semantic meanings.</p>",
            "id": 123,
            "page": 10,
            "text": "· The styles of instructions found by different optimizer LLMs vary a lot: PaLM 2-L-IT and text-bi son ones are concise, while GPT ones are long and detailed. · Although some top instructions contain the \"step-by-step\" phrase, most others achieve a comparable or better accuracy with different semantic meanings."
        },
        {
            "bounding_box": [
                {
                    "x": 1252,
                    "y": 3132
                },
                {
                    "x": 1299,
                    "y": 3132
                },
                {
                    "x": 1299,
                    "y": 3172
                },
                {
                    "x": 1252,
                    "y": 3172
                }
            ],
            "category": "footer",
            "html": "<footer id='124' style='font-size:16px'>10</footer>",
            "id": 124,
            "page": 10,
            "text": "10"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 113
                },
                {
                    "x": 1099,
                    "y": 113
                },
                {
                    "x": 1099,
                    "y": 158
                },
                {
                    "x": 444,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='125' style='font-size:22px'>Large Language Models as Optimizers</header>",
            "id": 125,
            "page": 11,
            "text": "Large Language Models as Optimizers"
        },
        {
            "bounding_box": [
                {
                    "x": 676,
                    "y": 355
                },
                {
                    "x": 1881,
                    "y": 355
                },
                {
                    "x": 1881,
                    "y": 813
                },
                {
                    "x": 676,
                    "y": 813
                }
            ],
            "category": "figure",
            "html": "<figure><img id='126' style='font-size:14px' alt=\"80.0\nwww\naccuracy\naccuracy\n70.0\n60.0\ntraining\n60.0\n40.0\nGSM8K\nGSM8K (scorer and optimizer:\n(scorer: text-bison) training\nPaLM 2-L)\n50.0 20.0\n0 50 100 150 200 0 20 40 60 80\n# steps # steps\n(a) PaLM 2-L-IT optimizer (b) pre-trained PaLM 2-L optimizer\" data-coord=\"top-left:(676,355); bottom-right:(1881,813)\" /></figure>",
            "id": 126,
            "page": 11,
            "text": "80.0 www accuracy accuracy 70.0 60.0 training 60.0 40.0 GSM8K GSM8K (scorer and optimizer: (scorer: text-bison) training PaLM 2-L) 50.0 20.0 0 50 100 150 200 0 20 40 60 80 # steps # steps (a) PaLM 2-L-IT optimizer (b) pre-trained PaLM 2-L optimizer"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 862
                },
                {
                    "x": 2108,
                    "y": 862
                },
                {
                    "x": 2108,
                    "y": 958
                },
                {
                    "x": 440,
                    "y": 958
                }
            ],
            "category": "caption",
            "html": "<caption id='127' style='font-size:18px'>Figure 4: Prompt optimization on GSM8K with (a) the text-bi son scorer and the P aLM 2-L-IT<br>optimizer, and (b) pre-trained PaLM 2-L as both scorer and optimizer.</caption>",
            "id": 127,
            "page": 11,
            "text": "Figure 4: Prompt optimization on GSM8K with (a) the text-bi son scorer and the P aLM 2-L-IT optimizer, and (b) pre-trained PaLM 2-L as both scorer and optimizer."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 1036
                },
                {
                    "x": 680,
                    "y": 1036
                },
                {
                    "x": 680,
                    "y": 1084
                },
                {
                    "x": 445,
                    "y": 1084
                }
            ],
            "category": "paragraph",
            "html": "<p id='128' style='font-size:18px'>5.2.2 BBH</p>",
            "id": 128,
            "page": 11,
            "text": "5.2.2 BBH"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1117
                },
                {
                    "x": 2108,
                    "y": 1117
                },
                {
                    "x": 2108,
                    "y": 1350
                },
                {
                    "x": 441,
                    "y": 1350
                }
            ],
            "category": "paragraph",
            "html": "<p id='129' style='font-size:18px'>On BBH, the optimization starts from an empty string as the initial instruction by default. The<br>instructions are placed at A begin when the scorer is PaLM 2-L, and at Q_begin when the scorer<br>is text-bison. For each task, we utilize a subset of 20% examples for prompt optimization, and<br>the rest examples are for testing. We show experimental results on more variants of the instruction<br>position and initialization in Appendix E.</p>",
            "id": 129,
            "page": 11,
            "text": "On BBH, the optimization starts from an empty string as the initial instruction by default. The instructions are placed at A begin when the scorer is PaLM 2-L, and at Q_begin when the scorer is text-bison. For each task, we utilize a subset of 20% examples for prompt optimization, and the rest examples are for testing. We show experimental results on more variants of the instruction position and initialization in Appendix E."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1369
                },
                {
                    "x": 2107,
                    "y": 1369
                },
                {
                    "x": 2107,
                    "y": 1693
                },
                {
                    "x": 442,
                    "y": 1693
                }
            ],
            "category": "paragraph",
            "html": "<p id='130' style='font-size:18px'>Figure 5 visualizes the per-task accuracy difference on all 23 BBH tasks compared to the instruction<br>\"Let's think step by step.' (Kojima et al., 2022) and the empty instruction, and we present the concrete<br>accuracies in Table 7 of Appendix E. We show that the instructions found by OPRO outperform<br>\"Let's think step by step.' , almost all tasks by a large margin: our instructions outperform by over<br>on<br>5% on 19/23 tasks with the PaLM 2-L scorer, and on 15/23 tasks with the text-bison scorer.<br>Our prompt optimization algorithm also improves instructions from the empty starting point by over<br>5% on most tasks: 20/23 with the PaLM 2-L scorer and 15/23 with the text-bison scorer.</p>",
            "id": 130,
            "page": 11,
            "text": "Figure 5 visualizes the per-task accuracy difference on all 23 BBH tasks compared to the instruction \"Let's think step by step.' (Kojima , 2022) and the empty instruction, and we present the concrete accuracies in Table 7 of Appendix E. We show that the instructions found by OPRO outperform \"Let's think step by step.' , almost all tasks by a large margin: our instructions outperform by over on 5% on 19/23 tasks with the PaLM 2-L scorer, and on 15/23 tasks with the text-bison scorer. Our prompt optimization algorithm also improves instructions from the empty starting point by over 5% on most tasks: 20/23 with the PaLM 2-L scorer and 15/23 with the text-bison scorer."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1717
                },
                {
                    "x": 2105,
                    "y": 1717
                },
                {
                    "x": 2105,
                    "y": 1809
                },
                {
                    "x": 442,
                    "y": 1809
                }
            ],
            "category": "paragraph",
            "html": "<p id='131' style='font-size:16px'>Similar to GSM8K, we observe upward trends in optimization curves on almost all BBH tasks, as<br>shown in Figure 6. See Figure 23 and 24 in Appendix D for more curves on other BBH tasks.</p>",
            "id": 131,
            "page": 11,
            "text": "Similar to GSM8K, we observe upward trends in optimization curves on almost all BBH tasks, as shown in Figure 6. See Figure 23 and 24 in Appendix D for more curves on other BBH tasks."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1832
                },
                {
                    "x": 2108,
                    "y": 1832
                },
                {
                    "x": 2108,
                    "y": 1971
                },
                {
                    "x": 442,
                    "y": 1971
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='132' style='font-size:18px'>We next show some examples of instructions found through the course of optimization. On the task<br>ruin_names, starting from the empty instruction (with 64.0 training accuracy), with the text-bison<br>scorer and the P aLM 2-L-IT optimizer, the following instructions are generated:</p>",
            "id": 132,
            "page": 11,
            "text": "We next show some examples of instructions found through the course of optimization. On the task ruin_names, starting from the empty instruction (with 64.0 training accuracy), with the text-bison scorer and the P aLM 2-L-IT optimizer, the following instructions are generated:"
        },
        {
            "bounding_box": [
                {
                    "x": 485,
                    "y": 1991
                },
                {
                    "x": 2110,
                    "y": 1991
                },
                {
                    "x": 2110,
                    "y": 2424
                },
                {
                    "x": 485,
                    "y": 2424
                }
            ],
            "category": "paragraph",
            "html": "<p id='133' style='font-size:16px'>· \"Consider the following when editing artist or movie names humorously:\" at Step 1 with training<br>accuracy 72.0;<br>· \"When making humorous edits of artist or movie names, you can change one or more letters or<br>even create puns by adding new words that sound similar.\" at Step 18 with training accuracy<br>80.0;<br>· \"We can make humorous edits of artist/movie names by changing letters to create new words<br>that are similar in sound but have different meanings. For example, The Police can be changed<br>to The Polite, The Abyss can be changed to Toe Abyss, and Schindler's List can be changed to<br>Schindler's Lost.\" at Step 38 with training accuracy 82.0.</p>",
            "id": 133,
            "page": 11,
            "text": "· \"Consider the following when editing artist or movie names humorously:\" at Step 1 with training accuracy 72.0; · \"When making humorous edits of artist or movie names, you can change one or more letters or even create puns by adding new words that sound similar.\" at Step 18 with training accuracy 80.0; · \"We can make humorous edits of artist/movie names by changing letters to create new words that are similar in sound but have different meanings. For example, The Police can be changed to The Polite, The Abyss can be changed to Toe Abyss, and Schindler's List can be changed to Schindler's Lost.\" at Step 38 with training accuracy 82.0."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2446
                },
                {
                    "x": 2105,
                    "y": 2446
                },
                {
                    "x": 2105,
                    "y": 2539
                },
                {
                    "x": 441,
                    "y": 2539
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='134' style='font-size:20px'>Although the above instructions are semantically similar, a paraphrase by the optimizer LLM offers a<br>notable accuracy improvement. We further highlight this observation in Section 5.2.3.</p>",
            "id": 134,
            "page": 11,
            "text": "Although the above instructions are semantically similar, a paraphrase by the optimizer LLM offers a notable accuracy improvement. We further highlight this observation in Section 5.2.3."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2562
                },
                {
                    "x": 2109,
                    "y": 2562
                },
                {
                    "x": 2109,
                    "y": 2654
                },
                {
                    "x": 443,
                    "y": 2654
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='135' style='font-size:20px'>Below are some instructions generated when performing prompt optimization on temporal_sequences,<br>starting from the empty instruction (with the training accuracy of 64.0):</p>",
            "id": 135,
            "page": 11,
            "text": "Below are some instructions generated when performing prompt optimization on temporal_sequences, starting from the empty instruction (with the training accuracy of 64.0):"
        },
        {
            "bounding_box": [
                {
                    "x": 485,
                    "y": 2677
                },
                {
                    "x": 2108,
                    "y": 2677
                },
                {
                    "x": 2108,
                    "y": 3055
                },
                {
                    "x": 485,
                    "y": 3055
                }
            ],
            "category": "paragraph",
            "html": "<p id='136' style='font-size:18px'>· \"To solve this problem, we need to first identify the time period when the person was not seen<br>doing anything else. Then, we need to check if the place they went to was open during that time<br>period. If it was, then that is the time period when they could have gone to that place.\" at Step 2<br>with training accuracy 42.0;<br>· \"To find the time period when a person could have gone to a place, identify the time periods<br>when they were not seen doing anything else and the place was open. If there are multiple time<br>periods that match these criteria, then the person could have gone to the place during any of<br>these time periods.\" at Step 18 with training accuracy 54.0;</p>",
            "id": 136,
            "page": 11,
            "text": "· \"To solve this problem, we need to first identify the time period when the person was not seen doing anything else. Then, we need to check if the place they went to was open during that time period. If it was, then that is the time period when they could have gone to that place.\" at Step 2 with training accuracy 42.0; · \"To find the time period when a person could have gone to a place, identify the time periods when they were not seen doing anything else and the place was open. If there are multiple time periods that match these criteria, then the person could have gone to the place during any of these time periods.\" at Step 18 with training accuracy 54.0;"
        },
        {
            "bounding_box": [
                {
                    "x": 1251,
                    "y": 3133
                },
                {
                    "x": 1295,
                    "y": 3133
                },
                {
                    "x": 1295,
                    "y": 3172
                },
                {
                    "x": 1251,
                    "y": 3172
                }
            ],
            "category": "footer",
            "html": "<footer id='137' style='font-size:16px'>11</footer>",
            "id": 137,
            "page": 11,
            "text": "11"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 112
                },
                {
                    "x": 1099,
                    "y": 112
                },
                {
                    "x": 1099,
                    "y": 158
                },
                {
                    "x": 443,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='138' style='font-size:20px'>Large Language Models as Optimizers</header>",
            "id": 138,
            "page": 12,
            "text": "Large Language Models as Optimizers"
        },
        {
            "bounding_box": [
                {
                    "x": 456,
                    "y": 354
                },
                {
                    "x": 2083,
                    "y": 354
                },
                {
                    "x": 2083,
                    "y": 2371
                },
                {
                    "x": 456,
                    "y": 2371
                }
            ],
            "category": "figure",
            "html": "<figure><img id='139' style='font-size:14px' alt=\"60\n40\ndifference\ndifference\n40\n20\naccuracy\naccuracy\n0\n20\n-20\nexpressions\ndetection\nlies\nunderstanding\nsnarks\nexpressions\nobjects\nsnarks\ntable\nrecommendation\ndetection\nunderstanding\nhyperbaton\nobjects\nsequences\nlanguages\nnames\nunderstanding\nunderstanding\nhyperbaton\ncounting\nnames\ntable\nfallacies\ncounting\nlanguages\nqa\nqa\ndisambiguation lies\ndisambiguation two\na of two\na of\nweb shapes\ngeometric shapes\nruin objects\nruin objects\ngeometric objects\nreasoning_about_colored\ntracking_shuffled_objects_seven\nreasoning_about_colored fallacies\nseven\npenguins_in\nformal navigate\nformal navigate\nobject\nobject\ntranslation_error\nsalient_translation_error\npenguins in\ndyck\ndyck sequences\ntemporal\ntemporal web\ncausal judgement\ncausal judgement\nboolean word_sorting\nboolean word_sorting\nmultistep_ arithmetic\nmultistep_ arithmetic\ndate\nsports\nsports\ndate\nmovie\nmovie recommendation\ntracking_shuffled_ objects_ seven\nlogical_ deduction\nlogical_ deduction\nsalient_\n(a) PaLM 2-L scorer, ours minus 'Let's think step by step.' , (b) PaLM 2-L scorer, ours minus empty starting point\n40\n60\ndifference\ndifference\n40\n20\naccuracy\n20  accuracy\n0 0\nrecommendation\nexpressions\nrecommendation\nsnarks\nexpressions\nobjects\nsnarks\ntable\nobjects\nhyperbaton\ntable\nnames\nunderstanding\nunderstanding\nsequences\nunderstanding\nshapes\nshapes\nhyperbaton\nsequences\nnames\ncounting\ncounting\ntwo\nqa\nqa\ndisambiguation lies\ndisambiguation two\na of lies\nruin objects\nruin objects\ngeometric objects\ngeometric objects\nreasoning_about_colored a of\nseven\nseven\nseven\npenguins_in\npenguins_in\nformal navigate\nformal navigate\nobject detection\nsalient_translation_error\ndyck languages\ndyck languages\ntemporal web sorting\ntemporal web sorting\ncausal judgement\ncausal judgement\nboolean word_\nboolean word_\ntracking_shuffled_objects understanding\nmultistep_ arithmetic\ndeduction\ndate arithmetic\nsports\nsports\ndate\nreasoning_about_ colored\nmovie\nmovie\ntracking_shuffled_ objects_seven\nlogical_ deduction\nlogical_ multistep object detection\nsalient_ translation_error\n(c) text-bison scorer, ours minus 'Let's think step by step.' (d) text-bison scorer, ours minus empty starting point\" data-coord=\"top-left:(456,354); bottom-right:(2083,2371)\" /></figure>",
            "id": 139,
            "page": 12,
            "text": "60 40 difference difference 40 20 accuracy accuracy 0 20 -20 expressions detection lies understanding snarks expressions objects snarks table recommendation detection understanding hyperbaton objects sequences languages names understanding understanding hyperbaton counting names table fallacies counting languages qa qa disambiguation lies disambiguation two a of two a of web shapes geometric shapes ruin objects ruin objects geometric objects reasoning_about_colored tracking_shuffled_objects_seven reasoning_about_colored fallacies seven penguins_in formal navigate formal navigate object object translation_error salient_translation_error penguins in dyck dyck sequences temporal temporal web causal judgement causal judgement boolean word_sorting boolean word_sorting multistep_ arithmetic multistep_ arithmetic date sports sports date movie movie recommendation tracking_shuffled_ objects_ seven logical_ deduction logical_ deduction salient_ (a) PaLM 2-L scorer, ours minus \"Let's think step by step.' , (b) PaLM 2-L scorer, ours minus empty starting point 40 60 difference difference 40 20 accuracy 20  accuracy 0 0 recommendation expressions recommendation snarks expressions objects snarks table objects hyperbaton table names understanding understanding sequences understanding shapes shapes hyperbaton sequences names counting counting two qa qa disambiguation lies disambiguation two a of lies ruin objects ruin objects geometric objects geometric objects reasoning_about_colored a of seven seven seven penguins_in penguins_in formal navigate formal navigate object detection salient_translation_error dyck languages dyck languages temporal web sorting temporal web sorting causal judgement causal judgement boolean word_ boolean word_ tracking_shuffled_objects understanding multistep_ arithmetic deduction date arithmetic sports sports date reasoning_about_ colored movie movie tracking_shuffled_ objects_seven logical_ deduction logical_ multistep object detection salient_ translation_error (c) text-bison scorer, ours minus \"Let's think step by step.\" (d) text-bison scorer, ours minus empty starting point"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2412
                },
                {
                    "x": 2112,
                    "y": 2412
                },
                {
                    "x": 2112,
                    "y": 2559
                },
                {
                    "x": 442,
                    "y": 2559
                }
            ],
            "category": "caption",
            "html": "<caption id='140' style='font-size:18px'>Figure 5: On 23 BBH tasks, the accuracy differences among instructions found by prompt opti-<br>mization (with the PaLM 2-L-IT optimizer), \"Let's think step by step.' , and the empty string<br>,<br>(optimization starting point).</caption>",
            "id": 140,
            "page": 12,
            "text": "Figure 5: On 23 BBH tasks, the accuracy differences among instructions found by prompt optimization (with the PaLM 2-L-IT optimizer), \"Let's think step by step.' , and the empty string , (optimization starting point)."
        },
        {
            "bounding_box": [
                {
                    "x": 489,
                    "y": 2704
                },
                {
                    "x": 2110,
                    "y": 2704
                },
                {
                    "x": 2110,
                    "y": 2937
                },
                {
                    "x": 489,
                    "y": 2937
                }
            ],
            "category": "paragraph",
            "html": "<p id='141' style='font-size:16px'>· \"To determine the possible time period when a person went to a place, first identify all the time<br>periods when the person was not seen doing anything else and the place was open. Then, rule<br>out any time periods during which the person was seen doing something else. The remaining<br>time periods are the possible times when the person could have gone to the place.\" at Step 41<br>with training accuracy 72.0.</p>",
            "id": 141,
            "page": 12,
            "text": "· \"To determine the possible time period when a person went to a place, first identify all the time periods when the person was not seen doing anything else and the place was open. Then, rule out any time periods during which the person was seen doing something else. The remaining time periods are the possible times when the person could have gone to the place.\" at Step 41 with training accuracy 72.0."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2958
                },
                {
                    "x": 2112,
                    "y": 2958
                },
                {
                    "x": 2112,
                    "y": 3055
                },
                {
                    "x": 441,
                    "y": 3055
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='142' style='font-size:16px'>Table 5 presents the best instructions generated on movie_recommendation, ruin_names, and tem-<br>poral_sequences tasks with different combinations of the optimizer and the scorer LLMs. Again,</p>",
            "id": 142,
            "page": 12,
            "text": "Table 5 presents the best instructions generated on movie_recommendation, ruin_names, and temporal_sequences tasks with different combinations of the optimizer and the scorer LLMs. Again,"
        },
        {
            "bounding_box": [
                {
                    "x": 1252,
                    "y": 3134
                },
                {
                    "x": 1298,
                    "y": 3134
                },
                {
                    "x": 1298,
                    "y": 3172
                },
                {
                    "x": 1252,
                    "y": 3172
                }
            ],
            "category": "footer",
            "html": "<footer id='143' style='font-size:16px'>12</footer>",
            "id": 143,
            "page": 12,
            "text": "12"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 113
                },
                {
                    "x": 1099,
                    "y": 113
                },
                {
                    "x": 1099,
                    "y": 158
                },
                {
                    "x": 443,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='144' style='font-size:22px'>Large Language Models as Optimizers</header>",
            "id": 144,
            "page": 13,
            "text": "Large Language Models as Optimizers"
        },
        {
            "bounding_box": [
                {
                    "x": 675,
                    "y": 343
                },
                {
                    "x": 1877,
                    "y": 343
                },
                {
                    "x": 1877,
                    "y": 803
                },
                {
                    "x": 675,
                    "y": 803
                }
            ],
            "category": "figure",
            "html": "<figure><img id='145' style='font-size:14px' alt=\"90.0\naccuracy\naccuracy\n70.0 They\n80.0\ntraining\n50.0\n70.0 BBH\nBBH ruin_names training\n30.0 temporal_sequences\n0 50 100 150 200 0 50 100 150\n# steps # steps\n(a) BBH ruin_names (b) BBH temporal_sequences\" data-coord=\"top-left:(675,343); bottom-right:(1877,803)\" /></figure>",
            "id": 145,
            "page": 13,
            "text": "90.0 accuracy accuracy 70.0 They 80.0 training 50.0 70.0 BBH BBH ruin_names training 30.0 temporal_sequences 0 50 100 150 200 0 50 100 150 # steps # steps (a) BBH ruin_names (b) BBH temporal_sequences"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 849
                },
                {
                    "x": 2110,
                    "y": 849
                },
                {
                    "x": 2110,
                    "y": 995
                },
                {
                    "x": 440,
                    "y": 995
                }
            ],
            "category": "caption",
            "html": "<caption id='146' style='font-size:18px'>Figure 6: Training accuracy curves of prompt optimization on BBH ruin names and tempo-<br>ral_sequences with the text-bison scorer and the PaLM 2-L-IT optimizer. The optimizations<br>start from the empty string.</caption>",
            "id": 146,
            "page": 13,
            "text": "Figure 6: Training accuracy curves of prompt optimization on BBH ruin names and temporal_sequences with the text-bison scorer and the PaLM 2-L-IT optimizer. The optimizations start from the empty string."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1071
                },
                {
                    "x": 2107,
                    "y": 1071
                },
                {
                    "x": 2107,
                    "y": 1166
                },
                {
                    "x": 441,
                    "y": 1166
                }
            ],
            "category": "paragraph",
            "html": "<p id='147' style='font-size:20px'>different optimizer LLMs produce instructions of different styles. See Appendix E for results on<br>more BBH tasks.</p>",
            "id": 147,
            "page": 13,
            "text": "different optimizer LLMs produce instructions of different styles. See Appendix E for results on more BBH tasks."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 1214
                },
                {
                    "x": 2038,
                    "y": 1214
                },
                {
                    "x": 2038,
                    "y": 1305
                },
                {
                    "x": 444,
                    "y": 1305
                }
            ],
            "category": "paragraph",
            "html": "<p id='148' style='font-size:14px'>5.2.3 SEMANTICALLY SIMILAR INSTRUCTIONS MAY ACHIEVE DRASTICALLY DIFFERENT<br>ACCURACIES</p>",
            "id": 148,
            "page": 13,
            "text": "5.2.3 SEMANTICALLY SIMILAR INSTRUCTIONS MAY ACHIEVE DRASTICALLY DIFFERENT ACCURACIES"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1338
                },
                {
                    "x": 2109,
                    "y": 1338
                },
                {
                    "x": 2109,
                    "y": 1665
                },
                {
                    "x": 441,
                    "y": 1665
                }
            ],
            "category": "paragraph",
            "html": "<p id='149' style='font-size:20px'>One challenge of prompt optimization is the sensitivity of model performance to subtle changes in<br>the instruction. For example, with the PaLM 2-L scorer on the GSM8K test set, \"Let's think step<br>by step.\" achieves accuracy 71.8, \"Let's solve the problem together.\" has accuracy 60.5, while the<br>, , is only 49.4, although it is the<br>accuracy of \"Let's work together to solve this problem step by step.<br>semantic combination of the two upper instructions. This behavior increases both the variance across<br>single-step instructions and the oscillation during optimization, and motivates us to generate multiple<br>instructions at each step to improve the optimization stability.</p>",
            "id": 149,
            "page": 13,
            "text": "One challenge of prompt optimization is the sensitivity of model performance to subtle changes in the instruction. For example, with the PaLM 2-L scorer on the GSM8K test set, \"Let's think step by step.\" achieves accuracy 71.8, \"Let's solve the problem together.\" has accuracy 60.5, while the , , is only 49.4, although it is the accuracy of \"Let's work together to solve this problem step by step. semantic combination of the two upper instructions. This behavior increases both the variance across single-step instructions and the oscillation during optimization, and motivates us to generate multiple instructions at each step to improve the optimization stability."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1709
                },
                {
                    "x": 1394,
                    "y": 1709
                },
                {
                    "x": 1394,
                    "y": 1759
                },
                {
                    "x": 443,
                    "y": 1759
                }
            ],
            "category": "paragraph",
            "html": "<p id='150' style='font-size:16px'>5.2.4 TRANSFERABILITY OF FOUND INSTRUCTIONS</p>",
            "id": 150,
            "page": 13,
            "text": "5.2.4 TRANSFERABILITY OF FOUND INSTRUCTIONS"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1791
                },
                {
                    "x": 2110,
                    "y": 1791
                },
                {
                    "x": 2110,
                    "y": 1977
                },
                {
                    "x": 441,
                    "y": 1977
                }
            ],
            "category": "paragraph",
            "html": "<p id='151' style='font-size:18px'>We assess the transferability of found prompts to different datasets of the same domain, where we<br>evaluate the top instructions found for GSM8K on two more math reasoning benchmarks Multi-<br>Arith (Roy & Roth, 2016) and AQuA (Ling et al., 2017). Table 6 shows that our optimized prompts<br>also outperform baseline prompts with different scorer LLMs on these two benchmarks.</p>",
            "id": 151,
            "page": 13,
            "text": "We assess the transferability of found prompts to different datasets of the same domain, where we evaluate the top instructions found for GSM8K on two more math reasoning benchmarks MultiArith (Roy & Roth, 2016) and AQuA (Ling , 2017). Table 6 shows that our optimized prompts also outperform baseline prompts with different scorer LLMs on these two benchmarks."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2032
                },
                {
                    "x": 909,
                    "y": 2032
                },
                {
                    "x": 909,
                    "y": 2078
                },
                {
                    "x": 443,
                    "y": 2078
                }
            ],
            "category": "paragraph",
            "html": "<p id='152' style='font-size:18px'>5.3 ABLATION STUDIES</p>",
            "id": 152,
            "page": 13,
            "text": "5.3 ABLATION STUDIES"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2115
                },
                {
                    "x": 2108,
                    "y": 2115
                },
                {
                    "x": 2108,
                    "y": 2213
                },
                {
                    "x": 442,
                    "y": 2213
                }
            ],
            "category": "paragraph",
            "html": "<p id='153' style='font-size:16px'>We use text-bison as the scorer and PaLM 2-L as the optimizer for all ablation studies. The<br>tasks we evaluate are GSM8K (math reasoning) and BBH sports_understanding (non-math reasoning).</p>",
            "id": 153,
            "page": 13,
            "text": "We use text-bison as the scorer and PaLM 2-L as the optimizer for all ablation studies. The tasks we evaluate are GSM8K (math reasoning) and BBH sports_understanding (non-math reasoning)."
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 2234
                },
                {
                    "x": 2106,
                    "y": 2234
                },
                {
                    "x": 2106,
                    "y": 2330
                },
                {
                    "x": 440,
                    "y": 2330
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='154' style='font-size:20px'>Meta-prompt design. The meta-prompt design is crucial in achieving good prompt optimization<br>performance. We investigate the following core design choices:</p>",
            "id": 154,
            "page": 13,
            "text": "Meta-prompt design. The meta-prompt design is crucial in achieving good prompt optimization performance. We investigate the following core design choices:"
        },
        {
            "bounding_box": [
                {
                    "x": 486,
                    "y": 2348
                },
                {
                    "x": 2112,
                    "y": 2348
                },
                {
                    "x": 2112,
                    "y": 3056
                },
                {
                    "x": 486,
                    "y": 3056
                }
            ],
            "category": "paragraph",
            "html": "<p id='155' style='font-size:20px'>● The order of the previous instructions. We compare the following options: (1) from lowest to<br>highest (our default setting); (2) from highest to lowest; (3) random. Figures 7(a) and 7(b) show<br>that the default setting achieves better final accuracies and converges faster. One hypothesis is<br>that the optimizer LLM output is affected more by the past instructions closer to the end of the<br>meta-prompt. This is consistent with the recency bias observed in Zhao et al. (2021), which<br>states that LLMs are more likely to generate tokens similar to the end of the prompt.<br>· The effect ofinstruction scores. In terms of how to present the accuracy scores, we compare three<br>options: (1) rounding the accuracies to integers, which is equivalent to bucketizing the accuracy<br>scores to 100 buckets (our default setting); (2) bucketizing the accuracies to 20 buckets; (3)<br>not showing the accuracies, only showing the instructions in the ascending order. Figures 7(c)<br>and 7(d) show that the accuracy scores assists the optimizer LLM in better understanding the<br>quality difference among previous instructions, and thus the optimizer LLM proposes better new<br>instructions that are similar to the best ones in the input optimization trajectory.<br>· The effect of exemplars. We compare three options: (1) showing 3 exemplars from the task<br>(default); (2) showing 10 exemplars from the task; (3) no exemplars. Figures 7(e) and 7(f) show</p>",
            "id": 155,
            "page": 13,
            "text": "● The order of the previous instructions. We compare the following options: (1) from lowest to highest (our default setting); (2) from highest to lowest; (3) random. Figures 7(a) and 7(b) show that the default setting achieves better final accuracies and converges faster. One hypothesis is that the optimizer LLM output is affected more by the past instructions closer to the end of the meta-prompt. This is consistent with the recency bias observed in Zhao  (2021), which states that LLMs are more likely to generate tokens similar to the end of the prompt. · The effect ofinstruction scores. In terms of how to present the accuracy scores, we compare three options: (1) rounding the accuracies to integers, which is equivalent to bucketizing the accuracy scores to 100 buckets (our default setting); (2) bucketizing the accuracies to 20 buckets; (3) not showing the accuracies, only showing the instructions in the ascending order. Figures 7(c) and 7(d) show that the accuracy scores assists the optimizer LLM in better understanding the quality difference among previous instructions, and thus the optimizer LLM proposes better new instructions that are similar to the best ones in the input optimization trajectory. · The effect of exemplars. We compare three options: (1) showing 3 exemplars from the task (default); (2) showing 10 exemplars from the task; (3) no exemplars. Figures 7(e) and 7(f) show"
        },
        {
            "bounding_box": [
                {
                    "x": 1251,
                    "y": 3132
                },
                {
                    "x": 1298,
                    "y": 3132
                },
                {
                    "x": 1298,
                    "y": 3171
                },
                {
                    "x": 1251,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='156' style='font-size:16px'>13</footer>",
            "id": 156,
            "page": 13,
            "text": "13"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 113
                },
                {
                    "x": 1098,
                    "y": 113
                },
                {
                    "x": 1098,
                    "y": 158
                },
                {
                    "x": 443,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='157' style='font-size:20px'>Large Language Models as Optimizers</header>",
            "id": 157,
            "page": 14,
            "text": "Large Language Models as Optimizers"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 409
                },
                {
                    "x": 2104,
                    "y": 409
                },
                {
                    "x": 2104,
                    "y": 505
                },
                {
                    "x": 441,
                    "y": 505
                }
            ],
            "category": "caption",
            "html": "<caption id='158' style='font-size:18px'>Table 5: Top instructions with the highest accuracies found in prompt optimization on BBH<br>movie _recommendation, ruin names, and temporal_sequences.</caption>",
            "id": 158,
            "page": 14,
            "text": "Table 5: Top instructions with the highest accuracies found in prompt optimization on BBH movie _recommendation, ruin names, and temporal_sequences."
        },
        {
            "bounding_box": [
                {
                    "x": 466,
                    "y": 509
                },
                {
                    "x": 2083,
                    "y": 509
                },
                {
                    "x": 2083,
                    "y": 3017
                },
                {
                    "x": 466,
                    "y": 3017
                }
            ],
            "category": "table",
            "html": "<br><table id='159' style='font-size:14px'><tr><td>Scorer</td><td>Optimizer</td><td>Instruction position</td><td>Instruction</td><td>Acc</td></tr><tr><td colspan=\"5\">movie_recommendation</td></tr><tr><td>PaLM 2-L</td><td>PaLM 2-L-IT</td><td>A_begin</td><td>Based on your input, Ihave analyzed the given movies in terms of genre, plot, tone, audience rating, year of release, director, cast, and reviews. I have also taken into account the given options. The movie that is most similar to the given movies in terms of all these factors is:</td><td>90.8</td></tr><tr><td>PaLM 2-L</td><td>PaLM 2-L</td><td>A_begin</td><td>The best film:</td><td>88.4</td></tr><tr><td>PaLM 2-L</td><td>gpt-3 · 5-turbo</td><td>A_begin</td><td>Let's uncover the perfect movie recommendation from the options provided, ensuring an exceptional cinematic experience together as we select the most captivating and satisfying choice that will keep us thoroughly engaged and immersed until the very end.</td><td>88.0</td></tr><tr><td>text-bison</td><td>PaLM 2-L-IT</td><td>Q_begin</td><td>Whatis the highest-rated movie similar to the given movies, with a similar IMDb rating and released in the same year?</td><td>91.6</td></tr><tr><td>text-bison</td><td>gpt-3 5-turbo Q_begin</td><td></td><td>Based on the movie list provided, carefully consider your preferences and make a well-informed decision.</td><td>70.8</td></tr><tr><td colspan=\"5\">ruin_names</td></tr><tr><td>- PaLM 2-L</td><td>- PaLM 2-L-IT A_begin</td><td></td><td>Which is the funniest pun on the artist or movie name?</td><td>88.0</td></tr><tr><td>PaLM 2-L</td><td>PaLM 2-L A_begin</td><td></td><td>Answer for ruin:</td><td>83.6</td></tr><tr><td>PaLM 2-L</td><td>gpt-3 · 5-turbo A_begin</td><td></td><td>Prepare to have a side-splittingly funny time as we uncover the most clever and hilarious alternatives for these artist or movie names, challenging your wit to guess the correct one with a burst of creativity, humor, and imaginative twists!</td><td>86.8</td></tr><tr><td>text-bison</td><td>PaLM</td><td>2-L-IT Q_begin</td><td>A humorous edit of an artist or movie name can be created by replacing one or more letters to form a new word or phrase that sounds similar but has a different meaning. The new word or phrase should be relevant to the original word, but it should also be a surprise, which makes the edit funny. For example, the artist or movie name \"Rocky\" can be changed to \"Ricky, \" and \"Schindler's List\" can be changed to \"Schindler's Lift. \" Be creative and have fun!</td><td>83.6</td></tr><tr><td>text-bison</td><td>gpt-3 · 5-turbo Q_begin</td><td></td><td>Choose the option that offers the most clever and humorous alteration of the given artist or movie name. Let your creativity shine and select the answer that will undoubtedly bring a smile to your face! Make sure to think outside the box!</td><td>75.2</td></tr><tr><td colspan=\"5\">temporal_sequences (no PaLM 2-L as scorer results because its training accuracy on empty string is 100.0) text-bison</td></tr><tr><td></td><td>PaLM 2-L-IT Q_begin</td><td></td><td>To determine the time period when a person went to a place, first identify all the time periods when the person's whereabouts are unknown. Then, rule out any time periods during which the person was seen doing something else or the place was closed. The remaining time periods are the possible times when the person could have gone to the place.</td><td>80.4</td></tr><tr><td>text-bison</td><td>gpt-3 . 5-turbo Q_begin</td><td></td><td>Identify the optimal time slot for the individual to engage in the mentioned location/activity considering the given sightings and waking up time, taking into account the opening and closing times of the location and the duration of each event.</td><td>53.6</td></tr></table>",
            "id": 159,
            "page": 14,
            "text": "Scorer Optimizer Instruction position Instruction Acc  movie_recommendation  PaLM 2-L PaLM 2-L-IT A_begin Based on your input, Ihave analyzed the given movies in terms of genre, plot, tone, audience rating, year of release, director, cast, and reviews. I have also taken into account the given options. The movie that is most similar to the given movies in terms of all these factors is: 90.8  PaLM 2-L PaLM 2-L A_begin The best film: 88.4  PaLM 2-L gpt-3 · 5-turbo A_begin Let's uncover the perfect movie recommendation from the options provided, ensuring an exceptional cinematic experience together as we select the most captivating and satisfying choice that will keep us thoroughly engaged and immersed until the very end. 88.0  text-bison PaLM 2-L-IT Q_begin Whatis the highest-rated movie similar to the given movies, with a similar IMDb rating and released in the same year? 91.6  text-bison gpt-3 5-turbo Q_begin  Based on the movie list provided, carefully consider your preferences and make a well-informed decision. 70.8  ruin_names  - PaLM 2-L - PaLM 2-L-IT A_begin  Which is the funniest pun on the artist or movie name? 88.0  PaLM 2-L PaLM 2-L A_begin  Answer for ruin: 83.6  PaLM 2-L gpt-3 · 5-turbo A_begin  Prepare to have a side-splittingly funny time as we uncover the most clever and hilarious alternatives for these artist or movie names, challenging your wit to guess the correct one with a burst of creativity, humor, and imaginative twists! 86.8  text-bison PaLM 2-L-IT Q_begin A humorous edit of an artist or movie name can be created by replacing one or more letters to form a new word or phrase that sounds similar but has a different meaning. The new word or phrase should be relevant to the original word, but it should also be a surprise, which makes the edit funny. For example, the artist or movie name \"Rocky\" can be changed to \"Ricky, \" and \"Schindler's List\" can be changed to \"Schindler's Lift. \" Be creative and have fun! 83.6  text-bison gpt-3 · 5-turbo Q_begin  Choose the option that offers the most clever and humorous alteration of the given artist or movie name. Let your creativity shine and select the answer that will undoubtedly bring a smile to your face! Make sure to think outside the box! 75.2  temporal_sequences (no PaLM 2-L as scorer results because its training accuracy on empty string is 100.0) text-bison   PaLM 2-L-IT Q_begin  To determine the time period when a person went to a place, first identify all the time periods when the person's whereabouts are unknown. Then, rule out any time periods during which the person was seen doing something else or the place was closed. The remaining time periods are the possible times when the person could have gone to the place. 80.4  text-bison gpt-3 . 5-turbo Q_begin  Identify the optimal time slot for the individual to engage in the mentioned location/activity considering the given sightings and waking up time, taking into account the opening and closing times of the location and the duration of each event."
        },
        {
            "bounding_box": [
                {
                    "x": 1251,
                    "y": 3133
                },
                {
                    "x": 1299,
                    "y": 3133
                },
                {
                    "x": 1299,
                    "y": 3171
                },
                {
                    "x": 1251,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='160' style='font-size:16px'>14</footer>",
            "id": 160,
            "page": 14,
            "text": "14"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 113
                },
                {
                    "x": 1099,
                    "y": 113
                },
                {
                    "x": 1099,
                    "y": 158
                },
                {
                    "x": 443,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='161' style='font-size:22px'>Large Language Models as Optimizers</header>",
            "id": 161,
            "page": 15,
            "text": "Large Language Models as Optimizers"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 375
                },
                {
                    "x": 2109,
                    "y": 375
                },
                {
                    "x": 2109,
                    "y": 467
                },
                {
                    "x": 442,
                    "y": 467
                }
            ],
            "category": "caption",
            "html": "<caption id='162' style='font-size:18px'>Table 6: Transferability across datasets: accuracies of top instructions found for GSM8K on Multi-<br>Arith and AQuA.</caption>",
            "id": 162,
            "page": 15,
            "text": "Table 6: Transferability across datasets: accuracies of top instructions found for GSM8K on MultiArith and AQuA."
        },
        {
            "bounding_box": [
                {
                    "x": 452,
                    "y": 483
                },
                {
                    "x": 2103,
                    "y": 483
                },
                {
                    "x": 2103,
                    "y": 1610
                },
                {
                    "x": 452,
                    "y": 1610
                }
            ],
            "category": "table",
            "html": "<table id='163' style='font-size:14px'><tr><td rowspan=\"2\">Scorer</td><td rowspan=\"2\">Source</td><td rowspan=\"2\">Instruction position</td><td rowspan=\"2\">Instruction</td><td colspan=\"2\">Accuracy</td></tr><tr><td>MultiArith</td><td>AQuA</td></tr><tr><td colspan=\"6\">Baselines</td></tr><tr><td>PaLM 2-L</td><td>(Kojima et al., 2022)</td><td>A_begin</td><td>Let's think step by step.</td><td>- 85.7</td><td>44.9</td></tr><tr><td>PaLM 2-L</td><td>(Zhou et al., 2022b)</td><td>A_begin</td><td>Let's work this out in a step by step way to be sure we have the right answer.</td><td>72.8</td><td>48.4</td></tr><tr><td>PaLM 2-L</td><td></td><td>A_begin</td><td>Let's solve the problem.</td><td>87.5</td><td>44.1</td></tr><tr><td>PaLM 2-L</td><td></td><td>A_begin</td><td>(empty string)</td><td>69.3</td><td>37.8</td></tr><tr><td>text-bison</td><td>(Kojima et al., 2022)</td><td>Q_begin</td><td>Let's think step by step.</td><td>92.5</td><td>31.9</td></tr><tr><td>text-bison</td><td>(Zhou et al., 2022b)</td><td>Q_begin</td><td>Let's work this out in a step by step way to be sure we have the right answer.</td><td>93.7</td><td>32.3</td></tr><tr><td>text-bison</td><td></td><td>Q_begin</td><td>Let's solve the problem.</td><td>85.5</td><td>29.9</td></tr><tr><td>text-bison</td><td></td><td>Q_begin</td><td>(empty string)</td><td>82.2</td><td>33.5</td></tr><tr><td colspan=\"6\">Ours</td></tr><tr><td>PaLM 2-L</td><td>PaLM 2-L-IT on GSM8K</td><td>A_begin</td><td>Take a deep breath and work on this problem step-by-step.</td><td>- 95.3</td><td>54.3</td></tr><tr><td>text-bison</td><td>PaLM 2-L-IT on GSM8K</td><td>Q_begin</td><td>Let's work together to solve math word problems! First, we will read and discuss the problem together to make sure we understand it. Then, we will work together to find the solution. I will give you hints and help you work through the problem if you get stuck.</td><td>96.8</td><td>37.8</td></tr></table>",
            "id": 163,
            "page": 15,
            "text": "Scorer Source Instruction position Instruction Accuracy  MultiArith AQuA  Baselines  PaLM 2-L (Kojima , 2022) A_begin Let's think step by step. - 85.7 44.9  PaLM 2-L (Zhou , 2022b) A_begin Let's work this out in a step by step way to be sure we have the right answer. 72.8 48.4  PaLM 2-L  A_begin Let's solve the problem. 87.5 44.1  PaLM 2-L  A_begin (empty string) 69.3 37.8  text-bison (Kojima , 2022) Q_begin Let's think step by step. 92.5 31.9  text-bison (Zhou , 2022b) Q_begin Let's work this out in a step by step way to be sure we have the right answer. 93.7 32.3  text-bison  Q_begin Let's solve the problem. 85.5 29.9  text-bison  Q_begin (empty string) 82.2 33.5  Ours  PaLM 2-L PaLM 2-L-IT on GSM8K A_begin Take a deep breath and work on this problem step-by-step. - 95.3 54.3  text-bison PaLM 2-L-IT on GSM8K Q_begin Let's work together to solve math word problems! First, we will read and discuss the problem together to make sure we understand it. Then, we will work together to find the solution. I will give you hints and help you work through the problem if you get stuck. 96.8"
        },
        {
            "bounding_box": [
                {
                    "x": 522,
                    "y": 1813
                },
                {
                    "x": 2107,
                    "y": 1813
                },
                {
                    "x": 2107,
                    "y": 2090
                },
                {
                    "x": 522,
                    "y": 2090
                }
            ],
            "category": "paragraph",
            "html": "<p id='164' style='font-size:20px'>that presenting exemplars in the meta-prompt is critical, as it provides information on what the<br>task looks like and helps the optimizer model phrase new instructions better. However, more<br>exemplars do not necessarily improve the performance, as a few exemplars are usually sufficient<br>to describe the task. In addition, including more exemplars results in a longer meta-prompt<br>with a dominating exemplar part, which may distract the optimizer LLM from other important<br>components like the optimization trajectory.</p>",
            "id": 164,
            "page": 15,
            "text": "that presenting exemplars in the meta-prompt is critical, as it provides information on what the task looks like and helps the optimizer model phrase new instructions better. However, more exemplars do not necessarily improve the performance, as a few exemplars are usually sufficient to describe the task. In addition, including more exemplars results in a longer meta-prompt with a dominating exemplar part, which may distract the optimizer LLM from other important components like the optimization trajectory."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2112
                },
                {
                    "x": 2107,
                    "y": 2112
                },
                {
                    "x": 2107,
                    "y": 2481
                },
                {
                    "x": 441,
                    "y": 2481
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='165' style='font-size:20px'>The number of generated instructions per step. Computing a mini-batch of gradients reduces<br>the variance of a stochastic gradient descent procedure. Similarly, generating multiple instructions<br>in each step improves the optimization stability with LLMs. On the other hand, to achieve better<br>performance with a fixed budget for the number of instructions to evaluate, the number of per-step<br>instructions should not be too large, SO as to allow more optimization steps to incorporate richer<br>information of past instructions with their accuracies. Taking both aspects into consideration, Figure 8<br>compares the optimization performance of sampling 1 / 2 / 4 / 8 (default) / 16 instructions in each<br>step, showing that sampling 8 instructions at each step overall achieves the best performance.</p>",
            "id": 165,
            "page": 15,
            "text": "The number of generated instructions per step. Computing a mini-batch of gradients reduces the variance of a stochastic gradient descent procedure. Similarly, generating multiple instructions in each step improves the optimization stability with LLMs. On the other hand, to achieve better performance with a fixed budget for the number of instructions to evaluate, the number of per-step instructions should not be too large, SO as to allow more optimization steps to incorporate richer information of past instructions with their accuracies. Taking both aspects into consideration, Figure 8 compares the optimization performance of sampling 1 / 2 / 4 / 8 (default) / 16 instructions in each step, showing that sampling 8 instructions at each step overall achieves the best performance."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2503
                },
                {
                    "x": 2109,
                    "y": 2503
                },
                {
                    "x": 2109,
                    "y": 3053
                },
                {
                    "x": 441,
                    "y": 3053
                }
            ],
            "category": "paragraph",
            "html": "<p id='166' style='font-size:20px'>Starting point. We study the effect of different initial instructions for prompt optimization.<br>Our default setting is to start from an empty string when the scorer LLM is (instruction-tuned)<br>text-bison, and to start from either the empty string (on BBH tasks) or \"Let's solve the problem.\"<br>(on GSM8K) with instruction position A_begin when the scorer LLM is the (pre-trained) PaLM 2-L.<br>Figure 9(a) shows the performance of text -bi son as the scorer LLM with 3 options of initial<br>instructions: (1) the empty string; (2) \"Solve the following problem.\" ; or (3) \"Solve the following<br>problem.\" and \"Let's solve the problem.' , We observe that the accuracies do not differ much with<br>·<br>different starting points. Interestingly, the styles of the generated instructions are also similar. For<br>example, most of the generated instructions starting from (1) and (2) contain the phrase \"solve this<br>problem\", like \"Let's work together to solve this problem.\" in Step 4 with training accuracy 64.8 from<br>,<br>(1), and \"Let's solve the following problems using the given information.\" in Step 3 with training<br>accuracy 62.8 from (2).</p>",
            "id": 166,
            "page": 15,
            "text": "Starting point. We study the effect of different initial instructions for prompt optimization. Our default setting is to start from an empty string when the scorer LLM is (instruction-tuned) text-bison, and to start from either the empty string (on BBH tasks) or \"Let's solve the problem.\" (on GSM8K) with instruction position A_begin when the scorer LLM is the (pre-trained) PaLM 2-L. Figure 9(a) shows the performance of text -bi son as the scorer LLM with 3 options of initial instructions: (1) the empty string; (2) \"Solve the following problem.\" ; or (3) \"Solve the following problem.\" and \"Let's solve the problem.' , We observe that the accuracies do not differ much with · different starting points. Interestingly, the styles of the generated instructions are also similar. For example, most of the generated instructions starting from (1) and (2) contain the phrase \"solve this problem\", like \"Let's work together to solve this problem.\" in Step 4 with training accuracy 64.8 from , (1), and \"Let's solve the following problems using the given information.\" in Step 3 with training accuracy 62.8 from (2)."
        },
        {
            "bounding_box": [
                {
                    "x": 1251,
                    "y": 3132
                },
                {
                    "x": 1299,
                    "y": 3132
                },
                {
                    "x": 1299,
                    "y": 3171
                },
                {
                    "x": 1251,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='167' style='font-size:16px'>15</footer>",
            "id": 167,
            "page": 15,
            "text": "15"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 111
                },
                {
                    "x": 1100,
                    "y": 111
                },
                {
                    "x": 1100,
                    "y": 158
                },
                {
                    "x": 443,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='168' style='font-size:18px'>Large Language Models as Optimizers</header>",
            "id": 168,
            "page": 16,
            "text": "Large Language Models as Optimizers"
        },
        {
            "bounding_box": [
                {
                    "x": 479,
                    "y": 421
                },
                {
                    "x": 2070,
                    "y": 421
                },
                {
                    "x": 2070,
                    "y": 2834
                },
                {
                    "x": 479,
                    "y": 2834
                }
            ],
            "category": "figure",
            "html": "<figure><img id='169' style='font-size:20px' alt=\"70.0 100.0\naccuracy\naccuracy\n60.0 50.0\n50.0 0.0\n0 50 100 150 200 0 50 100 150 200\n# steps # steps\nascending (default) ascending (default)\ndescending descending\nrandom random\n(a) instruction ordering (GSM8K) (b) instruction ordering (BBH sports_understanding)\n70.0\n100.0\naccuracy\naccuracy\n60.0\n50.0\n50.0 0.0\n0 50 100 150 200 0 50 100 150 200\n# steps # steps\n100 buckets (default) 100 buckets (default)\n20 buckets 20 buckets\nno scores no scores\n(c) instruction scores (GSM8K) (d) instruction scores (BBH sports_understanding)\n70.0\n100.0\naccuracy\naccuracy\n60.0\n50.0\n50.0 0.0\n0 50 100 150 200 0 50 100 150 200\n# steps # steps\n3 exemplars (default) 3 exemplars (default)\n10 exemplars 10 exemplars\nno exemplars no exemplars\n(e) # exemplars (GSM8K) (f) # exemplars (BBH sports_understanding)\" data-coord=\"top-left:(479,421); bottom-right:(2070,2834)\" /></figure>",
            "id": 169,
            "page": 16,
            "text": "70.0 100.0 accuracy accuracy 60.0 50.0 50.0 0.0 0 50 100 150 200 0 50 100 150 200 # steps # steps ascending (default) ascending (default) descending descending random random (a) instruction ordering (GSM8K) (b) instruction ordering (BBH sports_understanding) 70.0 100.0 accuracy accuracy 60.0 50.0 50.0 0.0 0 50 100 150 200 0 50 100 150 200 # steps # steps 100 buckets (default) 100 buckets (default) 20 buckets 20 buckets no scores no scores (c) instruction scores (GSM8K) (d) instruction scores (BBH sports_understanding) 70.0 100.0 accuracy accuracy 60.0 50.0 50.0 0.0 0 50 100 150 200 0 50 100 150 200 # steps # steps 3 exemplars (default) 3 exemplars (default) 10 exemplars 10 exemplars no exemplars no exemplars (e) # exemplars (GSM8K) (f) # exemplars (BBH sports_understanding)"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 2855
                },
                {
                    "x": 2105,
                    "y": 2855
                },
                {
                    "x": 2105,
                    "y": 2953
                },
                {
                    "x": 440,
                    "y": 2953
                }
            ],
            "category": "caption",
            "html": "<br><caption id='170' style='font-size:16px'>Figure 7: Ablation studies: how each part of the meta-prompt matters. The dots are the average<br>values across 3 optimization repetitions, and the shaded regions represent standard deviations.</caption>",
            "id": 170,
            "page": 16,
            "text": "Figure 7: Ablation studies: how each part of the meta-prompt matters. The dots are the average values across 3 optimization repetitions, and the shaded regions represent standard deviations."
        },
        {
            "bounding_box": [
                {
                    "x": 1251,
                    "y": 3133
                },
                {
                    "x": 1300,
                    "y": 3133
                },
                {
                    "x": 1300,
                    "y": 3172
                },
                {
                    "x": 1251,
                    "y": 3172
                }
            ],
            "category": "footer",
            "html": "<footer id='171' style='font-size:14px'>16</footer>",
            "id": 171,
            "page": 16,
            "text": "16"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 112
                },
                {
                    "x": 1099,
                    "y": 112
                },
                {
                    "x": 1099,
                    "y": 158
                },
                {
                    "x": 443,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='172' style='font-size:22px'>Large Language Models as Optimizers</header>",
            "id": 172,
            "page": 17,
            "text": "Large Language Models as Optimizers"
        },
        {
            "bounding_box": [
                {
                    "x": 497,
                    "y": 333
                },
                {
                    "x": 2052,
                    "y": 333
                },
                {
                    "x": 2052,
                    "y": 1099
                },
                {
                    "x": 497,
                    "y": 1099
                }
            ],
            "category": "figure",
            "html": "<figure><img id='173' style='font-size:20px' alt=\"70.0\n100.0\naccuracy 60.0 accuracy 50.0\n50.0 0.0\n0 400 800 1200 1600 0 400 800 1200 1600\n# evaluated instructions # evaluated instructions\nx 1 8 (default) x 1 8 (default)\n2 16 2 16\n4 4\n(a) GSM8K (b) BBH sports_understanding\" data-coord=\"top-left:(497,333); bottom-right:(2052,1099)\" /></figure>",
            "id": 173,
            "page": 17,
            "text": "70.0 100.0 accuracy 60.0 accuracy 50.0 50.0 0.0 0 400 800 1200 1600 0 400 800 1200 1600 # evaluated instructions # evaluated instructions x 1 8 (default) x 1 8 (default) 2 16 2 16 4 4 (a) GSM8K (b) BBH sports_understanding"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 1138
                },
                {
                    "x": 2111,
                    "y": 1138
                },
                {
                    "x": 2111,
                    "y": 1376
                },
                {
                    "x": 440,
                    "y": 1376
                }
            ],
            "category": "caption",
            "html": "<caption id='174' style='font-size:16px'>Figure 8: Ablation studies: the number of generated instructions in each step. The dots are the<br>average values across 3 optimization repetitions, and the shaded regions represent standard deviations.<br>The x-axis represents the total number of evaluated instructions through the optimization; e.g., we<br>run 200 optimization steps when sampling 8 instructions in each step, run 400 steps when sampling 4<br>instructions in each step, etc.</caption>",
            "id": 174,
            "page": 17,
            "text": "Figure 8: Ablation studies: the number of generated instructions in each step. The dots are the average values across 3 optimization repetitions, and the shaded regions represent standard deviations. The x-axis represents the total number of evaluated instructions through the optimization; e.g., we run 200 optimization steps when sampling 8 instructions in each step, run 400 steps when sampling 4 instructions in each step, etc."
        },
        {
            "bounding_box": [
                {
                    "x": 494,
                    "y": 1482
                },
                {
                    "x": 2052,
                    "y": 1482
                },
                {
                    "x": 2052,
                    "y": 2225
                },
                {
                    "x": 494,
                    "y": 2225
                }
            ],
            "category": "figure",
            "html": "<figure><img id='175' style='font-size:14px' alt=\"80.0\n70.0\naccuracy\n60.0 accuracy 60.0\n50.0\n40.0\n0 50 100 150 200\n0 50 100 150 200\n# steps\n# steps\nfrom '' (default)\nfrom 'Let's solve the problem' (default)\nfrom 'Solve the following problem.' ''\nfrom\nfrom ', 'Solve the following problem.',\nand 'Let's solve the problem. ' from 'Let's think step by step.'\n(a) GSM8K, text-bison scorer, Q_begin (b) GSM8K, PaLM 2-L scorer, A_ begin\" data-coord=\"top-left:(494,1482); bottom-right:(2052,2225)\" /></figure>",
            "id": 175,
            "page": 17,
            "text": "80.0 70.0 accuracy 60.0 accuracy 60.0 50.0 40.0 0 50 100 150 200 0 50 100 150 200 # steps # steps from \"\" (default) from \"Let's solve the problem\" (default) from \"Solve the following problem.\" \"\" from from \", \"Solve the following problem.\", and \"Let's solve the problem. \" from \"Let's think step by step.\" (a) GSM8K, text-bison scorer, Q_begin (b) GSM8K, PaLM 2-L scorer, A_ begin"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 2275
                },
                {
                    "x": 2111,
                    "y": 2275
                },
                {
                    "x": 2111,
                    "y": 2372
                },
                {
                    "x": 440,
                    "y": 2372
                }
            ],
            "category": "caption",
            "html": "<caption id='176' style='font-size:18px'>Figure 9: Ablation studies: the initial instructions for prompt optimization. The dots are the<br>average values across 3 optimization repetitions, and the shaded regions represent standard deviations.</caption>",
            "id": 176,
            "page": 17,
            "text": "Figure 9: Ablation studies: the initial instructions for prompt optimization. The dots are the average values across 3 optimization repetitions, and the shaded regions represent standard deviations."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2500
                },
                {
                    "x": 2110,
                    "y": 2500
                },
                {
                    "x": 2110,
                    "y": 3055
                },
                {
                    "x": 441,
                    "y": 3055
                }
            ],
            "category": "paragraph",
            "html": "<p id='177' style='font-size:18px'>Figure 9(b) presents the results of of PaLM 2-L as the scorer LLM with the following options of<br>initial instructions: (1) \"Let's solve the problem.\"; (2) the empty string; or (3) \"Let's think step<br>by step.' · We notice that the performance differs much more with different initial instructions,<br>especially at the beginning of the optimization. Specifically, starting from (1) leads to better generated<br>instructions than (2) in the first 30 steps, while the instructions optimized from both (1) and (2)<br>are worse than (3) throughout. A similar observation holds when using P aLM 2-L as scorer and<br>gpt-3 · 5-turbo as optimizer for BBH tasks, by comparing the results starting from the empty<br>string (Appendix E.2) and from \"Let's solve the problem.\" (Appendix E.3). Taking a closer look into<br>the optimization process of (2), we find that although both \"solve the problem\" and \"step by step\"<br>show up in generated instructions at Step 5, it takes the optimizer LLM more steps to get rid of worse<br>instructions presented in the meta-prompt when starting from instructions with lower accuracies.<br>Therefore, one direction for future work is to accelerate convergence from weaker starting points.</p>",
            "id": 177,
            "page": 17,
            "text": "Figure 9(b) presents the results of of PaLM 2-L as the scorer LLM with the following options of initial instructions: (1) \"Let's solve the problem.\"; (2) the empty string; or (3) \"Let's think step by step.' · We notice that the performance differs much more with different initial instructions, especially at the beginning of the optimization. Specifically, starting from (1) leads to better generated instructions than (2) in the first 30 steps, while the instructions optimized from both (1) and (2) are worse than (3) throughout. A similar observation holds when using P aLM 2-L as scorer and gpt-3 · 5-turbo as optimizer for BBH tasks, by comparing the results starting from the empty string (Appendix E.2) and from \"Let's solve the problem.\" (Appendix E.3). Taking a closer look into the optimization process of (2), we find that although both \"solve the problem\" and \"step by step\" show up in generated instructions at Step 5, it takes the optimizer LLM more steps to get rid of worse instructions presented in the meta-prompt when starting from instructions with lower accuracies. Therefore, one direction for future work is to accelerate convergence from weaker starting points."
        },
        {
            "bounding_box": [
                {
                    "x": 1251,
                    "y": 3133
                },
                {
                    "x": 1298,
                    "y": 3133
                },
                {
                    "x": 1298,
                    "y": 3171
                },
                {
                    "x": 1251,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='178' style='font-size:16px'>17</footer>",
            "id": 178,
            "page": 17,
            "text": "17"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 112
                },
                {
                    "x": 1100,
                    "y": 112
                },
                {
                    "x": 1100,
                    "y": 158
                },
                {
                    "x": 443,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='179' style='font-size:22px'>Large Language Models as Optimizers</header>",
            "id": 179,
            "page": 18,
            "text": "Large Language Models as Optimizers"
        },
        {
            "bounding_box": [
                {
                    "x": 595,
                    "y": 339
                },
                {
                    "x": 1951,
                    "y": 339
                },
                {
                    "x": 1951,
                    "y": 1019
                },
                {
                    "x": 595,
                    "y": 1019
                }
            ],
            "category": "figure",
            "html": "<figure><img id='180' style='font-size:14px' alt=\"70.0\n100.0\naccuracy\n60.0 accuracy 50.0\nXX\n50.0 0.0\n0 50 100 150 200 0 50 100 150 200\n# steps # steps\nx 0.0 1.5 x 0.0 1.5\n0.5 2.0 0.5 2.0\n1.0 (default) 1.0 (default)\n(a) GSM8K (b) BBH sports_understanding\" data-coord=\"top-left:(595,339); bottom-right:(1951,1019)\" /></figure>",
            "id": 180,
            "page": 18,
            "text": "70.0 100.0 accuracy 60.0 accuracy 50.0 XX 50.0 0.0 0 50 100 150 200 0 50 100 150 200 # steps # steps x 0.0 1.5 x 0.0 1.5 0.5 2.0 0.5 2.0 1.0 (default) 1.0 (default) (a) GSM8K (b) BBH sports_understanding"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 1063
                },
                {
                    "x": 2109,
                    "y": 1063
                },
                {
                    "x": 2109,
                    "y": 1159
                },
                {
                    "x": 440,
                    "y": 1159
                }
            ],
            "category": "caption",
            "html": "<caption id='181' style='font-size:20px'>Figure 10: Ablation studies: temperature of the optimizer model. The dots are the average values<br>across 3 optimization repetitions, and the shaded regions represent standard deviations.</caption>",
            "id": 181,
            "page": 18,
            "text": "Figure 10: Ablation studies: temperature of the optimizer model. The dots are the average values across 3 optimization repetitions, and the shaded regions represent standard deviations."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1255
                },
                {
                    "x": 2109,
                    "y": 1255
                },
                {
                    "x": 2109,
                    "y": 1581
                },
                {
                    "x": 441,
                    "y": 1581
                }
            ],
            "category": "paragraph",
            "html": "<p id='182' style='font-size:20px'>Diversity per step. We evaluate the following temperatures of the optimizer LLM: {0.0, 0.5, 1.0<br>(default), 1.5, 2.0}. Figure 10 shows the default temperature 1.0 achieves the best performance.<br>Specifically, optimizations with smaller temperatures (0.0 and 0.5) lack exploration and thus creativity,<br>and the optimizer LLM often gets stuck at the same instruction for tens of steps, resulting in flat<br>optimization curves. On the other hand, with larger temperatures (1.5 and 2.0), the optimizer LLM<br>more often ignores the trajectory of previous instructions presented in the meta-prompt and thus lacks<br>exploitation, therefore the optimization curve does not have a steady upward trend.</p>",
            "id": 182,
            "page": 18,
            "text": "Diversity per step. We evaluate the following temperatures of the optimizer LLM: {0.0, 0.5, 1.0 (default), 1.5, 2.0}. Figure 10 shows the default temperature 1.0 achieves the best performance. Specifically, optimizations with smaller temperatures (0.0 and 0.5) lack exploration and thus creativity, and the optimizer LLM often gets stuck at the same instruction for tens of steps, resulting in flat optimization curves. On the other hand, with larger temperatures (1.5 and 2.0), the optimizer LLM more often ignores the trajectory of previous instructions presented in the meta-prompt and thus lacks exploitation, therefore the optimization curve does not have a steady upward trend."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1601
                },
                {
                    "x": 2108,
                    "y": 1601
                },
                {
                    "x": 2108,
                    "y": 2104
                },
                {
                    "x": 441,
                    "y": 2104
                }
            ],
            "category": "paragraph",
            "html": "<p id='183' style='font-size:18px'>Comparison with one-step instruction generation. Our current iterative procedure runs for multiple<br>steps and generates a new batch of solutions in each step. To validate the importance of leveraging<br>the optimization trajectory for generating new prompts, we compare to a baseline that generates all<br>instructions in a single step without entering into the optimization procedure. We compare these<br>two approaches on GSM8K and BBH sports_understanding with the PaLM 2-L-IT optimizer.<br>For GSM8K the scorer LLM is pre-trained PaLM 2-L and the initial instruction is \"Let's solve<br>the problem\", , and for BBH sports_ understanding the scorer LLM is text-bi son and the initial<br>instruction is the empty string. The baseline generates 50 instructions in a single step, thus its<br>meta-prompt only includes task exemplars, the initial instruction with its accuracy, and the same<br>meta-instructions as our full meta-prompt for performing optimization. All the other hyperparameters<br>remain the same.</p>",
            "id": 183,
            "page": 18,
            "text": "Comparison with one-step instruction generation. Our current iterative procedure runs for multiple steps and generates a new batch of solutions in each step. To validate the importance of leveraging the optimization trajectory for generating new prompts, we compare to a baseline that generates all instructions in a single step without entering into the optimization procedure. We compare these two approaches on GSM8K and BBH sports_understanding with the PaLM 2-L-IT optimizer. For GSM8K the scorer LLM is pre-trained PaLM 2-L and the initial instruction is \"Let's solve the problem\", , and for BBH sports_ understanding the scorer LLM is text-bi son and the initial instruction is the empty string. The baseline generates 50 instructions in a single step, thus its meta-prompt only includes task exemplars, the initial instruction with its accuracy, and the same meta-instructions as our full meta-prompt for performing optimization. All the other hyperparameters remain the same."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2128
                },
                {
                    "x": 2109,
                    "y": 2128
                },
                {
                    "x": 2109,
                    "y": 2498
                },
                {
                    "x": 441,
                    "y": 2498
                }
            ],
            "category": "paragraph",
            "html": "<p id='184' style='font-size:18px'>Our results show that this one-step instruction generation performs much worse than our optimization<br>approach. Specifically: (1) On GSM8K, the best instruction among all 50 is still \"Let's solve the<br>problem\", , with a 64.4 training accuracy and a 60.8 test accuracy. On the other hand, our approach<br>(corresponding to Figure 1(a) in the main paper) found \"Let's do the math!\" with a 78.2 training<br>accuracy and a 76.3 test accuracy at the 5th step by generating 8 instructions at each step. (2)<br>Similarly, on BBH sports_understanding, the best instruction among all 50 achieved a 84.0 training<br>accuracy and 80.0 test accuracy. This is again worse than the instruction found by our approach at<br>Step 4, which achieved a 88.0 training accuracy and a 84.5 test accuracy.</p>",
            "id": 184,
            "page": 18,
            "text": "Our results show that this one-step instruction generation performs much worse than our optimization approach. Specifically: (1) On GSM8K, the best instruction among all 50 is still \"Let's solve the problem\", , with a 64.4 training accuracy and a 60.8 test accuracy. On the other hand, our approach (corresponding to Figure 1(a) in the main paper) found \"Let's do the math!\" with a 78.2 training accuracy and a 76.3 test accuracy at the 5th step by generating 8 instructions at each step. (2) Similarly, on BBH sports_understanding, the best instruction among all 50 achieved a 84.0 training accuracy and 80.0 test accuracy. This is again worse than the instruction found by our approach at Step 4, which achieved a 88.0 training accuracy and a 84.5 test accuracy."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2568
                },
                {
                    "x": 1474,
                    "y": 2568
                },
                {
                    "x": 1474,
                    "y": 2615
                },
                {
                    "x": 444,
                    "y": 2615
                }
            ],
            "category": "paragraph",
            "html": "<p id='185' style='font-size:16px'>5.4 OVERFITTING ANALYSIS IN PROMPT OPTIMIZATION</p>",
            "id": 185,
            "page": 18,
            "text": "5.4 OVERFITTING ANALYSIS IN PROMPT OPTIMIZATION"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2661
                },
                {
                    "x": 2107,
                    "y": 2661
                },
                {
                    "x": 2107,
                    "y": 2753
                },
                {
                    "x": 441,
                    "y": 2753
                }
            ],
            "category": "paragraph",
            "html": "<p id='186' style='font-size:16px'>For simplicity, we do not set aside a validation set in our default setting of prompt optimization. We<br>made this decision based on the experiments when a validation set is present.</p>",
            "id": 186,
            "page": 18,
            "text": "For simplicity, we do not set aside a validation set in our default setting of prompt optimization. We made this decision based on the experiments when a validation set is present."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2777
                },
                {
                    "x": 2107,
                    "y": 2777
                },
                {
                    "x": 2107,
                    "y": 3055
                },
                {
                    "x": 441,
                    "y": 3055
                }
            ],
            "category": "paragraph",
            "html": "<p id='187' style='font-size:18px'>Overfitting may result in training accuracy being much higher than the validation/test accuracy. It<br>is difficult to avoid overfitting, but overfitting is less harmful when each candidate solution (natural<br>language instruction in the prompt optimization context) overfits to a similar extent. In this case, a<br>higher training accuracy solution still achieves a higher validation/test accuracy, and one can adopt<br>solutions with the highest training accuracies as the final result. Figure 11 shows this is the case for<br>OPRO in prompt optimization: when setting aside a validation set with the same size as the training</p>",
            "id": 187,
            "page": 18,
            "text": "Overfitting may result in training accuracy being much higher than the validation/test accuracy. It is difficult to avoid overfitting, but overfitting is less harmful when each candidate solution (natural language instruction in the prompt optimization context) overfits to a similar extent. In this case, a higher training accuracy solution still achieves a higher validation/test accuracy, and one can adopt solutions with the highest training accuracies as the final result. Figure 11 shows this is the case for OPRO in prompt optimization: when setting aside a validation set with the same size as the training"
        },
        {
            "bounding_box": [
                {
                    "x": 1251,
                    "y": 3133
                },
                {
                    "x": 1298,
                    "y": 3133
                },
                {
                    "x": 1298,
                    "y": 3172
                },
                {
                    "x": 1251,
                    "y": 3172
                }
            ],
            "category": "footer",
            "html": "<footer id='188' style='font-size:16px'>18</footer>",
            "id": 188,
            "page": 18,
            "text": "18"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 112
                },
                {
                    "x": 1099,
                    "y": 112
                },
                {
                    "x": 1099,
                    "y": 158
                },
                {
                    "x": 443,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='189' style='font-size:20px'>Large Language Models as Optimizers</header>",
            "id": 189,
            "page": 19,
            "text": "Large Language Models as Optimizers"
        },
        {
            "bounding_box": [
                {
                    "x": 496,
                    "y": 350
                },
                {
                    "x": 2052,
                    "y": 350
                },
                {
                    "x": 2052,
                    "y": 995
                },
                {
                    "x": 496,
                    "y": 995
                }
            ],
            "category": "figure",
            "html": "<figure><img id='190' style='font-size:18px' alt=\"90 80\naccuracy 70 accuracy 60\ntraining training\nvalidation validation\n50 40\n0 50 100 150 200 0 50 100\n# steps # steps\n(a) BBH snarks, PaLM 2-L as scorer, PaLM (b) BBH sports_understanding, text-bison\n2-L-IT as optimizer, starting from 'Let's solve as scorer, gpt-3 · 5-turbo as optimizer, start-\nthe problem.' ing from the empty string\" data-coord=\"top-left:(496,350); bottom-right:(2052,995)\" /></figure>",
            "id": 190,
            "page": 19,
            "text": "90 80 accuracy 70 accuracy 60 training training validation validation 50 40 0 50 100 150 200 0 50 100 # steps # steps (a) BBH snarks, PaLM 2-L as scorer, PaLM (b) BBH sports_understanding, text-bison 2-L-IT as optimizer, starting from \"Let's solve as scorer, gpt-3 · 5-turbo as optimizer, startthe problem.\" ing from the empty string"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1038
                },
                {
                    "x": 2109,
                    "y": 1038
                },
                {
                    "x": 2109,
                    "y": 1227
                },
                {
                    "x": 441,
                    "y": 1227
                }
            ],
            "category": "paragraph",
            "html": "<p id='191' style='font-size:18px'>Figure 11: Overfitting analysis. The exemplars are splitted to 1/3 training, 1/3 validation and 1/3<br>test. We compute the validation accuracy every 3 steps. The training/validation dots are the average<br>training/validation accuracies across 3 optimization repetitions, respectively, and the shaded regions<br>represent standard deviations.</p>",
            "id": 191,
            "page": 19,
            "text": "Figure 11: Overfitting analysis. The exemplars are splitted to 1/3 training, 1/3 validation and 1/3 test. We compute the validation accuracy every 3 steps. The training/validation dots are the average training/validation accuracies across 3 optimization repetitions, respectively, and the shaded regions represent standard deviations."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1371
                },
                {
                    "x": 2105,
                    "y": 1371
                },
                {
                    "x": 2105,
                    "y": 1464
                },
                {
                    "x": 441,
                    "y": 1464
                }
            ],
            "category": "paragraph",
            "html": "<p id='192' style='font-size:18px'>set, the validation accuracy curves trend up and down alongside the training curves in both prompt<br>optimization settings.</p>",
            "id": 192,
            "page": 19,
            "text": "set, the validation accuracy curves trend up and down alongside the training curves in both prompt optimization settings."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1486
                },
                {
                    "x": 2108,
                    "y": 1486
                },
                {
                    "x": 2108,
                    "y": 1674
                },
                {
                    "x": 441,
                    "y": 1674
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='193' style='font-size:18px'>Of course, overfitting still occurs in the instructions found by our prompt optimization: in Table 7<br>and 10, our training accuracies are often 5%-20% higher than our test accuracies, despite that our test<br>and overall accuracies are still mostly higher than human-written counterparts. Setting aside a larger<br>training set and optimizing for fewer steps (early stopping) may help reduce overfitting.</p>",
            "id": 193,
            "page": 19,
            "text": "Of course, overfitting still occurs in the instructions found by our prompt optimization: in Table 7 and 10, our training accuracies are often 5%-20% higher than our test accuracies, despite that our test and overall accuracies are still mostly higher than human-written counterparts. Setting aside a larger training set and optimizing for fewer steps (early stopping) may help reduce overfitting."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1792
                },
                {
                    "x": 1136,
                    "y": 1792
                },
                {
                    "x": 1136,
                    "y": 1840
                },
                {
                    "x": 443,
                    "y": 1840
                }
            ],
            "category": "paragraph",
            "html": "<p id='194' style='font-size:14px'>5.5 COMPARISON WITH EVOPROMPT</p>",
            "id": 194,
            "page": 19,
            "text": "5.5 COMPARISON WITH EVOPROMPT"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1904
                },
                {
                    "x": 2109,
                    "y": 1904
                },
                {
                    "x": 2109,
                    "y": 2411
                },
                {
                    "x": 441,
                    "y": 2411
                }
            ],
            "category": "paragraph",
            "html": "<p id='195' style='font-size:16px'>Some concurrent works on prompt optimization propose meta-prompts that explicitly ask the LLM to<br>perform mutation and crossovers of existing prompts (Fernando et al., 2023; Guo et al., 2023). In our<br>evaluation, we compare our approach to the Genetic Algorithm (GA) and Differential Evolution (DE)<br>versions of EvoPrompt (Guo et al., 2023). Specifically, in the GA meta-prompt, given two prompts,<br>the meta-prompt instructs the LLM to cross over the two prompts and generates a new one, then<br>mutates the newly generated prompt to produce the final prompt. DE extends the GA meta-prompt<br>to include more detailed instructions, e.g., asking the LLM to identify different parts between the<br>two given prompts before performing the mutation. This is in contrast with OPRO, which leverages<br>the optimization trajectory including multiple past prompts, instead of only 2 previous prompts.<br>Meanwhile, OPRO also provides the LLM with richer information to facilitate the understanding of<br>the optimization problem, including exemplars and task accuracies of different prompts.</p>",
            "id": 195,
            "page": 19,
            "text": "Some concurrent works on prompt optimization propose meta-prompts that explicitly ask the LLM to perform mutation and crossovers of existing prompts (Fernando , 2023; Guo , 2023). In our evaluation, we compare our approach to the Genetic Algorithm (GA) and Differential Evolution (DE) versions of EvoPrompt (Guo , 2023). Specifically, in the GA meta-prompt, given two prompts, the meta-prompt instructs the LLM to cross over the two prompts and generates a new one, then mutates the newly generated prompt to produce the final prompt. DE extends the GA meta-prompt to include more detailed instructions, e.g., asking the LLM to identify different parts between the two given prompts before performing the mutation. This is in contrast with OPRO, which leverages the optimization trajectory including multiple past prompts, instead of only 2 previous prompts. Meanwhile, OPRO also provides the LLM with richer information to facilitate the understanding of the optimization problem, including exemplars and task accuracies of different prompts."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2430
                },
                {
                    "x": 2107,
                    "y": 2430
                },
                {
                    "x": 2107,
                    "y": 2802
                },
                {
                    "x": 441,
                    "y": 2802
                }
            ],
            "category": "paragraph",
            "html": "<p id='196' style='font-size:16px'>Figure 12 presents the results on GSM8K and BBH sports_understanding benchmarks, where we use<br>gpt-3 · 5-turbo as the optimizer. On GSM8K, the initial instructions of all approaches are \"Let's<br>solve the problem.\" , and \"Here is the answer.\" which are simple and generic. Again, we observe that<br>,<br>OPRO performance steadily improves with more optimization steps. On the other hand, both versions<br>of EvoPrompt even degrade the performance on GSM8K. The main reason is because EvoPrompt<br>does not utilize exemplars for prompt optimization, thus it lacks the understanding of the task to<br>optimize for. In this way, EvoPrompt relies on good-quality and task-specific initial prompts to<br>optimize from.</p>",
            "id": 196,
            "page": 19,
            "text": "Figure 12 presents the results on GSM8K and BBH sports_understanding benchmarks, where we use gpt-3 · 5-turbo as the optimizer. On GSM8K, the initial instructions of all approaches are \"Let's solve the problem.\" , and \"Here is the answer.\" which are simple and generic. Again, we observe that , OPRO performance steadily improves with more optimization steps. On the other hand, both versions of EvoPrompt even degrade the performance on GSM8K. The main reason is because EvoPrompt does not utilize exemplars for prompt optimization, thus it lacks the understanding of the task to optimize for. In this way, EvoPrompt relies on good-quality and task-specific initial prompts to optimize from."
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 2821
                },
                {
                    "x": 2110,
                    "y": 2821
                },
                {
                    "x": 2110,
                    "y": 3055
                },
                {
                    "x": 440,
                    "y": 3055
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='197' style='font-size:18px'>Given this observation, we provide more task-specific initial instructions for experiments on BBH<br>sports_understanding, which are \"Solve the sports understanding problem.\" and \"Give me the answer<br>to sports understanding.\" In this case, EvoPrompt (DE) is able to find better prompts than the<br>initial ones, but the optimization curve is less stable than OPRO. This indicates that leveraging the<br>optimization trajectory helps the LLM to identify promising directions to improve existing prompts.</p>",
            "id": 197,
            "page": 19,
            "text": "Given this observation, we provide more task-specific initial instructions for experiments on BBH sports_understanding, which are \"Solve the sports understanding problem.\" and \"Give me the answer to sports understanding.\" In this case, EvoPrompt (DE) is able to find better prompts than the initial ones, but the optimization curve is less stable than OPRO. This indicates that leveraging the optimization trajectory helps the LLM to identify promising directions to improve existing prompts."
        },
        {
            "bounding_box": [
                {
                    "x": 1251,
                    "y": 3133
                },
                {
                    "x": 1298,
                    "y": 3133
                },
                {
                    "x": 1298,
                    "y": 3171
                },
                {
                    "x": 1251,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='198' style='font-size:14px'>19</footer>",
            "id": 198,
            "page": 19,
            "text": "19"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 112
                },
                {
                    "x": 1099,
                    "y": 112
                },
                {
                    "x": 1099,
                    "y": 158
                },
                {
                    "x": 443,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='199' style='font-size:22px'>Large Language Models as Optimizers</header>",
            "id": 199,
            "page": 20,
            "text": "Large Language Models as Optimizers"
        },
        {
            "bounding_box": [
                {
                    "x": 542,
                    "y": 351
                },
                {
                    "x": 2018,
                    "y": 351
                },
                {
                    "x": 2018,
                    "y": 1127
                },
                {
                    "x": 542,
                    "y": 1127
                }
            ],
            "category": "figure",
            "html": "<figure><img id='200' style='font-size:18px' alt=\"80 90\naccuracy\naccuracy\n50\n20 50\n0 50 100 150 0 50 100 150 200\n# steps # steps\nOPRO OPRO\nEvoPrompt (GA) EvoPrompt (GA)\nEvoPrompt (DE) EvoPrompt (DE)\n(a) GSM8K, PaLM 2-L scorer, A begin (b) BBH sports_understanding, text-bison\nscorer, Q_begin\" data-coord=\"top-left:(542,351); bottom-right:(2018,1127)\" /></figure>",
            "id": 200,
            "page": 20,
            "text": "80 90 accuracy accuracy 50 20 50 0 50 100 150 0 50 100 150 200 # steps # steps OPRO OPRO EvoPrompt (GA) EvoPrompt (GA) EvoPrompt (DE) EvoPrompt (DE) (a) GSM8K, PaLM 2-L scorer, A begin (b) BBH sports_understanding, text-bison scorer, Q_begin"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 1171
                },
                {
                    "x": 2110,
                    "y": 1171
                },
                {
                    "x": 2110,
                    "y": 1594
                },
                {
                    "x": 440,
                    "y": 1594
                }
            ],
            "category": "caption",
            "html": "<caption id='201' style='font-size:14px'>Figure 12: Comparison with EvoPrompt in prompt optimization. We use the gpt-3 · 5-turbo<br>optimizer for both experiments. \"EvoPrompt (GA)\" uses the meta-prompt from Guo et al. (2023),<br>Figure 1; \"EvoPrompt (DE)\" uses the meta-prompt from Guo et al. (2023), Figure 2. All optimizations<br>in (a) use the pre-trained PaLM 2-L scorer and start from two simple instructions \"Let's solve the<br>problem.\" and \"Here is the answer.' , · all optimizations in (b) use the text -bi son scorer and start<br>,<br>from two richer (task-specific) instructions \"Solve the sports understanding problem.\" and \"Give<br>me the answer to sports understanding.' The dots are the average values across 3 optimization<br>repetitions, and the shaded regions represent standard deviations. We use temperature 1.0 for OPRO<br>and temperature 0.5 for EvoPrompt, same as the default settings in respective works.</caption>",
            "id": 201,
            "page": 20,
            "text": "Figure 12: Comparison with EvoPrompt in prompt optimization. We use the gpt-3 · 5-turbo optimizer for both experiments. \"EvoPrompt (GA)\" uses the meta-prompt from Guo  (2023), Figure 1; \"EvoPrompt (DE)\" uses the meta-prompt from Guo  (2023), Figure 2. All optimizations in (a) use the pre-trained PaLM 2-L scorer and start from two simple instructions \"Let's solve the problem.\" and \"Here is the answer.' , · all optimizations in (b) use the text -bi son scorer and start , from two richer (task-specific) instructions \"Solve the sports understanding problem.\" and \"Give me the answer to sports understanding.' The dots are the average values across 3 optimization repetitions, and the shaded regions represent standard deviations. We use temperature 1.0 for OPRO and temperature 0.5 for EvoPrompt, same as the default settings in respective works."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 1681
                },
                {
                    "x": 885,
                    "y": 1681
                },
                {
                    "x": 885,
                    "y": 1734
                },
                {
                    "x": 444,
                    "y": 1734
                }
            ],
            "category": "paragraph",
            "html": "<p id='202' style='font-size:20px'>6 RELATED WORK</p>",
            "id": 202,
            "page": 20,
            "text": "6 RELATED WORK"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1791
                },
                {
                    "x": 2110,
                    "y": 1791
                },
                {
                    "x": 2110,
                    "y": 2663
                },
                {
                    "x": 441,
                    "y": 2663
                }
            ],
            "category": "paragraph",
            "html": "<p id='203' style='font-size:16px'>Prompt optimization. Prior works have developed soft prompt-tuning methods that optimize the<br>prompt represented as task-specific continuous vectors (Lester et al., 2021; Li & Liang, 2021; Liu et al.,<br>2021; Qin & Eisner, 2021), as well as performing discrete prompt optimization by gradient-guided<br>search (Shin et al., 2020; Wen et al., 2023; Gao et al., 2020; Chen et al., 2023d) and reinforcement<br>learning (Deng et al., 2022; Zhang et al., 2023). These approaches become inapplicable when there is<br>only API access to the LLM. Other works designed edit-based approaches for gradient-free prompt<br>optimization (Xu et al., 2022; Prasad et al., 2022), where the editing can be done with human-<br>defined operations (e.g., swapping two phrases) (Prasad et al., 2022) or language models (e.g., back<br>translation) (Xu et al., 2022). Some recent works investigate LLMs for prompt optimization (Zhou<br>et al., 2022b; Pryzant et al., 2023; Xu et al., 2023). Specifically, APE (Zhou et al., 2022b) first uses<br>the LLM to generate initial instructions. Afterwards, APE selects top instructions with the highest<br>accuracies, then prompts the LLM with each individual instruction to generate a semantically similar<br>variant of the initial instruction. APO (Pryzant et al., 2023) in each step instructs the LLM to produce<br>text feedback on how to update an old instruction. Different from edit-based approaches, the optimizer<br>LLM in our work directly generates new instructions at each optimization step, and the optimizer<br>LLM is merely asked to improve the task accuracy without being required to imitate past instructions.<br>Compared to Zhou et al. (2022b) and Pryzant et al. (2023), our optimization process incorporates<br>the past generated instructions with their scores in the meta-prompt, enabling the optimizer LLM to<br>discover common patterns of high-quality instructions.</p>",
            "id": 203,
            "page": 20,
            "text": "Prompt optimization. Prior works have developed soft prompt-tuning methods that optimize the prompt represented as task-specific continuous vectors (Lester , 2021; Li & Liang, 2021; Liu , 2021; Qin & Eisner, 2021), as well as performing discrete prompt optimization by gradient-guided search (Shin , 2020; Wen , 2023; Gao , 2020; Chen , 2023d) and reinforcement learning (Deng , 2022; Zhang , 2023). These approaches become inapplicable when there is only API access to the LLM. Other works designed edit-based approaches for gradient-free prompt optimization (Xu , 2022; Prasad , 2022), where the editing can be done with humandefined operations (e.g., swapping two phrases) (Prasad , 2022) or language models (e.g., back translation) (Xu , 2022). Some recent works investigate LLMs for prompt optimization (Zhou , 2022b; Pryzant , 2023; Xu , 2023). Specifically, APE (Zhou , 2022b) first uses the LLM to generate initial instructions. Afterwards, APE selects top instructions with the highest accuracies, then prompts the LLM with each individual instruction to generate a semantically similar variant of the initial instruction. APO (Pryzant , 2023) in each step instructs the LLM to produce text feedback on how to update an old instruction. Different from edit-based approaches, the optimizer LLM in our work directly generates new instructions at each optimization step, and the optimizer LLM is merely asked to improve the task accuracy without being required to imitate past instructions. Compared to Zhou  (2022b) and Pryzant  (2023), our optimization process incorporates the past generated instructions with their scores in the meta-prompt, enabling the optimizer LLM to discover common patterns of high-quality instructions."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2683
                },
                {
                    "x": 2110,
                    "y": 2683
                },
                {
                    "x": 2110,
                    "y": 3057
                },
                {
                    "x": 441,
                    "y": 3057
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='204' style='font-size:16px'>Prompting with natural language feedback. A recent line of work investigates approaches to<br>improve the LLM performance by prompting with natural language feedback to revise the model<br>output, which has shown effectiveness in reducing harmful LLM outputs (Bai et al., 2022; Ganguli<br>et al., 2023), improving reasoning (Shinn et al., 2023; Madaan et al., 2023) and code generation<br>performance (Chen et al., 2023e; Olausson et al., 2023; Shinn et al., 2023; Chen et al., 2023b),<br>dialogue applications (Nair et al., 2023; Madaan et al., 2023; Yuan et al., 2023), and SO on (Kim et al.,<br>2023; Wang et al., 2023). Specifically, Yuan et al. (2023) develops a human-in-the-loop framework<br>for deriving system-level feedback from a collection of instance-level feedback, which is then used</p>",
            "id": 204,
            "page": 20,
            "text": "Prompting with natural language feedback. A recent line of work investigates approaches to improve the LLM performance by prompting with natural language feedback to revise the model output, which has shown effectiveness in reducing harmful LLM outputs (Bai , 2022; Ganguli , 2023), improving reasoning (Shinn , 2023; Madaan , 2023) and code generation performance (Chen , 2023e; Olausson , 2023; Shinn , 2023; Chen , 2023b), dialogue applications (Nair , 2023; Madaan , 2023; Yuan , 2023), and SO on (Kim , 2023; Wang , 2023). Specifically, Yuan  (2023) develops a human-in-the-loop framework for deriving system-level feedback from a collection of instance-level feedback, which is then used"
        },
        {
            "bounding_box": [
                {
                    "x": 1248,
                    "y": 3132
                },
                {
                    "x": 1300,
                    "y": 3132
                },
                {
                    "x": 1300,
                    "y": 3172
                },
                {
                    "x": 1248,
                    "y": 3172
                }
            ],
            "category": "footer",
            "html": "<footer id='205' style='font-size:14px'>20</footer>",
            "id": 205,
            "page": 20,
            "text": "20"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 113
                },
                {
                    "x": 1099,
                    "y": 113
                },
                {
                    "x": 1099,
                    "y": 158
                },
                {
                    "x": 443,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='206' style='font-size:18px'>Large Language Models as Optimizers</header>",
            "id": 206,
            "page": 21,
            "text": "Large Language Models as Optimizers"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 345
                },
                {
                    "x": 2110,
                    "y": 345
                },
                {
                    "x": 2110,
                    "y": 532
                },
                {
                    "x": 441,
                    "y": 532
                }
            ],
            "category": "paragraph",
            "html": "<p id='207' style='font-size:16px'>for refining data. In our work, the optimizer LLM utilizes the optimization trajectory in the prompt,<br>which implicitly requires the LLM to summarize the common characteristics among solutions with<br>similar scores. We consider incorporating explicit natural language feedback on generated solutions<br>for later optimization steps as future work.</p>",
            "id": 207,
            "page": 21,
            "text": "for refining data. In our work, the optimizer LLM utilizes the optimization trajectory in the prompt, which implicitly requires the LLM to summarize the common characteristics among solutions with similar scores. We consider incorporating explicit natural language feedback on generated solutions for later optimization steps as future work."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 553
                },
                {
                    "x": 2109,
                    "y": 553
                },
                {
                    "x": 2109,
                    "y": 1061
                },
                {
                    "x": 441,
                    "y": 1061
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='208' style='font-size:16px'>Tuning language models for optimization. Some previous works tune or prompt language models<br>to behave as mutation and crossover operators in evolutionary algorithms. Meyerson et al. (2023)<br>utilizes language models with few-shot exemplars to propose evolutionary cross-overs on tasks such<br>as image and code generation. In Lehman et al. (2022), the large language model trained on code diff<br>generation is used as the mutation operator, and they further design a fine-tuning method to improve<br>performance in the Sodarace domain for robot simulation. EvoPrompting (Chen et al., 2023a) uses<br>large language models to evolve neural network architectures, where they combine evolutionary<br>search with soft prompt tuning. With respect to taking the trajectory as the input for optimization,<br>OptFormer (Chen et al., 2022) trains a transformer model on large collections of hyperparameter<br>optimization data. On the other hand, our work performs optimization solely by prompting without<br>additional training.</p>",
            "id": 208,
            "page": 21,
            "text": "Tuning language models for optimization. Some previous works tune or prompt language models to behave as mutation and crossover operators in evolutionary algorithms. Meyerson  (2023) utilizes language models with few-shot exemplars to propose evolutionary cross-overs on tasks such as image and code generation. In Lehman  (2022), the large language model trained on code diff generation is used as the mutation operator, and they further design a fine-tuning method to improve performance in the Sodarace domain for robot simulation. EvoPrompting (Chen , 2023a) uses large language models to evolve neural network architectures, where they combine evolutionary search with soft prompt tuning. With respect to taking the trajectory as the input for optimization, OptFormer (Chen , 2022) trains a transformer model on large collections of hyperparameter optimization data. On the other hand, our work performs optimization solely by prompting without additional training."
        },
        {
            "bounding_box": [
                {
                    "x": 447,
                    "y": 1123
                },
                {
                    "x": 819,
                    "y": 1123
                },
                {
                    "x": 819,
                    "y": 1175
                },
                {
                    "x": 447,
                    "y": 1175
                }
            ],
            "category": "paragraph",
            "html": "<p id='209' style='font-size:18px'>7 CONCLUSION</p>",
            "id": 209,
            "page": 21,
            "text": "7 CONCLUSION"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1225
                },
                {
                    "x": 2108,
                    "y": 1225
                },
                {
                    "x": 2108,
                    "y": 1592
                },
                {
                    "x": 442,
                    "y": 1592
                }
            ],
            "category": "paragraph",
            "html": "<p id='210' style='font-size:16px'>We embark on employing LLMs as optimizers, where the LLM progressively generates new solutions<br>to optimize an objective function. We first motivate OPRO with linear regression and traveling<br>salesman problems, then proceed to prompt optimization as a concrete application. Our evaluation<br>demonstrates that LLMs have the capacity of gradually improving the generated solutions based on<br>the past optimization trajectory. Interestingly, on small-scale traveling salesman problems, OPRO<br>performs on par with some hand-crafted heuristic algorithms. For prompt optimization, optimized<br>prompts outperform human-designed prompts on GSM8K and Big-Bench Hard by a significant<br>margin, sometimes over 50%.</p>",
            "id": 210,
            "page": 21,
            "text": "We embark on employing LLMs as optimizers, where the LLM progressively generates new solutions to optimize an objective function. We first motivate OPRO with linear regression and traveling salesman problems, then proceed to prompt optimization as a concrete application. Our evaluation demonstrates that LLMs have the capacity of gradually improving the generated solutions based on the past optimization trajectory. Interestingly, on small-scale traveling salesman problems, OPRO performs on par with some hand-crafted heuristic algorithms. For prompt optimization, optimized prompts outperform human-designed prompts on GSM8K and Big-Bench Hard by a significant margin, sometimes over 50%."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1614
                },
                {
                    "x": 2109,
                    "y": 1614
                },
                {
                    "x": 2109,
                    "y": 2305
                },
                {
                    "x": 441,
                    "y": 2305
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='211' style='font-size:14px'>A number of unresolved questions are open for future research on LLMs for optimization. In general,<br>how to reduce the sensitivity to initialization and better balance exploitation with exploration remains<br>a challenge. Specifically, for prompt optimization, one limitation of our current implementation is<br>that the optimizer LLM does not effectively utilize error cases in the training set to infer promising<br>directions to improve the generated instructions. In our experiments, we tried including error cases in<br>the meta-prompt rather than randomly sampling from the training set at each optimization step, but the<br>results are similar, indicating that the error cases alone are not informative enough for the optimizer<br>LLM to grasp the cause of the wrong prediction. Another limitation is that prompt optimization<br>requires a training set to compute the accuracy that guides the optimization process. Currently the<br>training set at least contains tens of samples, SO that the optimized prompt does not severely overfit<br>to the training samples. A promising direction is to incorporate richer feedback about the error<br>cases besides the aggregated accuracy, and summarize the key features that distinguish between<br>high-quality and low-quality generated prompts in the optimization trajectory. Such information may<br>inform the optimizer LLM of how to more efficiently improve over the past generated instructions,<br>and potentially further reduce the example set size needed for prompt optimization.</p>",
            "id": 211,
            "page": 21,
            "text": "A number of unresolved questions are open for future research on LLMs for optimization. In general, how to reduce the sensitivity to initialization and better balance exploitation with exploration remains a challenge. Specifically, for prompt optimization, one limitation of our current implementation is that the optimizer LLM does not effectively utilize error cases in the training set to infer promising directions to improve the generated instructions. In our experiments, we tried including error cases in the meta-prompt rather than randomly sampling from the training set at each optimization step, but the results are similar, indicating that the error cases alone are not informative enough for the optimizer LLM to grasp the cause of the wrong prediction. Another limitation is that prompt optimization requires a training set to compute the accuracy that guides the optimization process. Currently the training set at least contains tens of samples, SO that the optimized prompt does not severely overfit to the training samples. A promising direction is to incorporate richer feedback about the error cases besides the aggregated accuracy, and summarize the key features that distinguish between high-quality and low-quality generated prompts in the optimization trajectory. Such information may inform the optimizer LLM of how to more efficiently improve over the past generated instructions, and potentially further reduce the example set size needed for prompt optimization."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 2368
                },
                {
                    "x": 883,
                    "y": 2368
                },
                {
                    "x": 883,
                    "y": 2422
                },
                {
                    "x": 445,
                    "y": 2422
                }
            ],
            "category": "paragraph",
            "html": "<p id='212' style='font-size:20px'>ETHICS STATEMENT</p>",
            "id": 212,
            "page": 21,
            "text": "ETHICS STATEMENT"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2468
                },
                {
                    "x": 2109,
                    "y": 2468
                },
                {
                    "x": 2109,
                    "y": 2702
                },
                {
                    "x": 441,
                    "y": 2702
                }
            ],
            "category": "paragraph",
            "html": "<p id='213' style='font-size:14px'>This work uses synthetic math problems for linear regression and traveling salesman problems, and<br>uses public datasets like GSM8K and Big-Bench Hard for prompt optimization. These tasks have<br>been commonly used in similar works and should not be regarded controversial. There is a peril that<br>LLMs may generate harmful information that poses safety risks; how to safeguard model behavior<br>remains valuable future work.</p>",
            "id": 213,
            "page": 21,
            "text": "This work uses synthetic math problems for linear regression and traveling salesman problems, and uses public datasets like GSM8K and Big-Bench Hard for prompt optimization. These tasks have been commonly used in similar works and should not be regarded controversial. There is a peril that LLMs may generate harmful information that poses safety risks; how to safeguard model behavior remains valuable future work."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2765
                },
                {
                    "x": 1122,
                    "y": 2765
                },
                {
                    "x": 1122,
                    "y": 2819
                },
                {
                    "x": 444,
                    "y": 2819
                }
            ],
            "category": "paragraph",
            "html": "<p id='214' style='font-size:22px'>REPRODUCIBILITY STATEMENT</p>",
            "id": 214,
            "page": 21,
            "text": "REPRODUCIBILITY STATEMENT"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2866
                },
                {
                    "x": 2114,
                    "y": 2866
                },
                {
                    "x": 2114,
                    "y": 3053
                },
                {
                    "x": 442,
                    "y": 3053
                }
            ],
            "category": "paragraph",
            "html": "<p id='215' style='font-size:14px'>We evaluate on public benchmarks. The text-bison API is available at: https : / / cloud.<br>google · com/vertex-ai / docs / generative-ai / learn / models. The GPT models<br>are available here: http : / / openai · com/ api/. This work uses gpt-3 · 5-turbo-0613<br>and gpt -4-0613.</p>",
            "id": 215,
            "page": 21,
            "text": "We evaluate on public benchmarks. The text-bison API is available at: https : / / cloud. google · com/vertex-ai / docs / generative-ai / learn / models. The GPT models are available here: http : / / openai · com/ api/. This work uses gpt-3 · 5-turbo-0613 and gpt -4-0613."
        },
        {
            "bounding_box": [
                {
                    "x": 1249,
                    "y": 3132
                },
                {
                    "x": 1297,
                    "y": 3132
                },
                {
                    "x": 1297,
                    "y": 3171
                },
                {
                    "x": 1249,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='216' style='font-size:14px'>21</footer>",
            "id": 216,
            "page": 21,
            "text": "21"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 113
                },
                {
                    "x": 1099,
                    "y": 113
                },
                {
                    "x": 1099,
                    "y": 157
                },
                {
                    "x": 443,
                    "y": 157
                }
            ],
            "category": "header",
            "html": "<header id='217' style='font-size:22px'>Large Language Models as Optimizers</header>",
            "id": 217,
            "page": 22,
            "text": "Large Language Models as Optimizers"
        },
        {
            "bounding_box": [
                {
                    "x": 447,
                    "y": 344
                },
                {
                    "x": 915,
                    "y": 344
                },
                {
                    "x": 915,
                    "y": 392
                },
                {
                    "x": 447,
                    "y": 392
                }
            ],
            "category": "paragraph",
            "html": "<p id='218' style='font-size:18px'>ACKNOWLEDGMENTS</p>",
            "id": 218,
            "page": 22,
            "text": "ACKNOWLEDGMENTS"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 443
                },
                {
                    "x": 2111,
                    "y": 443
                },
                {
                    "x": 2111,
                    "y": 583
                },
                {
                    "x": 442,
                    "y": 583
                }
            ],
            "category": "paragraph",
            "html": "<p id='219' style='font-size:14px'>We thank Daiyi Peng, Yanqi Zhou, Jerry Wei, Shuo Chen, Tim Rocktaschel, Chrisantha Fernando,<br>Dylan Banarse, Henryk Michalewski, Simon Osindero, and Ed H. Chi for their valuable feedback,<br>and thank several anonymous reviewers for helpful comments.</p>",
            "id": 219,
            "page": 22,
            "text": "We thank Daiyi Peng, Yanqi Zhou, Jerry Wei, Shuo Chen, Tim Rocktaschel, Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Ed H. Chi for their valuable feedback, and thank several anonymous reviewers for helpful comments."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 649
                },
                {
                    "x": 735,
                    "y": 649
                },
                {
                    "x": 735,
                    "y": 699
                },
                {
                    "x": 445,
                    "y": 699
                }
            ],
            "category": "paragraph",
            "html": "<p id='220' style='font-size:20px'>REFERENCES</p>",
            "id": 220,
            "page": 22,
            "text": "REFERENCES"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 726
                },
                {
                    "x": 2110,
                    "y": 726
                },
                {
                    "x": 2110,
                    "y": 817
                },
                {
                    "x": 443,
                    "y": 817
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='221' style='font-size:16px'>Shun-ichi Amari. Backpropagation and stochastic gradient descent method. Neurocomputing, 5(4-5):<br>185-196, 1993.</p>",
            "id": 221,
            "page": 22,
            "text": "Shun-ichi Amari. Backpropagation and stochastic gradient descent method. Neurocomputing, 5(4-5): 185-196, 1993."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 847
                },
                {
                    "x": 2110,
                    "y": 847
                },
                {
                    "x": 2110,
                    "y": 985
                },
                {
                    "x": 442,
                    "y": 985
                }
            ],
            "category": "paragraph",
            "html": "<p id='222' style='font-size:16px'>Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,<br>Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv<br>preprint arXiv:2305.10403, 2023.</p>",
            "id": 222,
            "page": 22,
            "text": "Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen,  Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1015
                },
                {
                    "x": 2024,
                    "y": 1015
                },
                {
                    "x": 2024,
                    "y": 1063
                },
                {
                    "x": 443,
                    "y": 1063
                }
            ],
            "category": "paragraph",
            "html": "<p id='223' style='font-size:14px'>David Applegate, Ribert Bixby, Vasek Chvatal, and William Cook. Concorde tsp solver, 2006.</p>",
            "id": 223,
            "page": 22,
            "text": "David Applegate, Ribert Bixby, Vasek Chvatal, and William Cook. Concorde tsp solver, 2006."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1091
                },
                {
                    "x": 2108,
                    "y": 1091
                },
                {
                    "x": 2108,
                    "y": 1184
                },
                {
                    "x": 443,
                    "y": 1184
                }
            ],
            "category": "paragraph",
            "html": "<p id='224' style='font-size:14px'>Thomas Back and Hans-Paul Schwefel. An overview of evolutionary algorithms for parameter<br>optimization. Evolutionary computation, 1(1):1-23, 1993.</p>",
            "id": 224,
            "page": 22,
            "text": "Thomas Back and Hans-Paul Schwefel. An overview of evolutionary algorithms for parameter optimization. Evolutionary computation, 1(1):1-23, 1993."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1212
                },
                {
                    "x": 2108,
                    "y": 1212
                },
                {
                    "x": 2108,
                    "y": 1349
                },
                {
                    "x": 443,
                    "y": 1349
                }
            ],
            "category": "paragraph",
            "html": "<p id='225' style='font-size:14px'>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna<br>Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness<br>from ai feedback. arXiv preprint arXiv:2212.08073, 2022.</p>",
            "id": 225,
            "page": 22,
            "text": "Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon,  Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1378
                },
                {
                    "x": 2105,
                    "y": 1378
                },
                {
                    "x": 2105,
                    "y": 1471
                },
                {
                    "x": 443,
                    "y": 1471
                }
            ],
            "category": "paragraph",
            "html": "<p id='226' style='font-size:16px'>Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. Large language models as<br>tool makers. arXiv preprint arXiv:2305.17126, 2023.</p>",
            "id": 226,
            "page": 22,
            "text": "Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. Large language models as tool makers. arXiv preprint arXiv:2305.17126, 2023."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1502
                },
                {
                    "x": 2106,
                    "y": 1502
                },
                {
                    "x": 2106,
                    "y": 1594
                },
                {
                    "x": 443,
                    "y": 1594
                }
            ],
            "category": "paragraph",
            "html": "<p id='227' style='font-size:14px'>Angelica Chen, David M Dohan, and David R So. Evoprompting: Language models for code-level<br>neural architecture search. arXiv preprint arXiv:2302.14838, 2023a.</p>",
            "id": 227,
            "page": 22,
            "text": "Angelica Chen, David M Dohan, and David R So. Evoprompting: Language models for code-level neural architecture search. arXiv preprint arXiv:2302.14838, 2023a."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1622
                },
                {
                    "x": 2106,
                    "y": 1622
                },
                {
                    "x": 2106,
                    "y": 1761
                },
                {
                    "x": 442,
                    "y": 1761
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='228' style='font-size:16px'>Angelica Chen, Jeremy Scheurer, Tomasz Korbak, Jon Ander Campos, Jun Shern Chan, Samuel R<br>Bowman, Kyunghyun Cho, and Ethan Perez. Improving code generation by training with natural<br>language feedback. arXiv preprint arXiv:2303.16749, 2023b.</p>",
            "id": 228,
            "page": 22,
            "text": "Angelica Chen, Jeremy Scheurer, Tomasz Korbak, Jon Ander Campos, Jun Shern Chan, Samuel R Bowman, Kyunghyun Cho, and Ethan Perez. Improving code generation by training with natural language feedback. arXiv preprint arXiv:2303.16749, 2023b."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1792
                },
                {
                    "x": 2107,
                    "y": 1792
                },
                {
                    "x": 2107,
                    "y": 1882
                },
                {
                    "x": 442,
                    "y": 1882
                }
            ],
            "category": "paragraph",
            "html": "<p id='229' style='font-size:16px'>Jiuhai Chen, Lichang Chen, Heng Huang, and Tianyi Zhou. When do you need chain-of-thought<br>prompting for chatgpt? arXiv preprint arXiv:2304.03262, 2023c.</p>",
            "id": 229,
            "page": 22,
            "text": "Jiuhai Chen, Lichang Chen, Heng Huang, and Tianyi Zhou. When do you need chain-of-thought prompting for chatgpt? arXiv preprint arXiv:2304.03262, 2023c."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1910
                },
                {
                    "x": 2108,
                    "y": 1910
                },
                {
                    "x": 2108,
                    "y": 2046
                },
                {
                    "x": 442,
                    "y": 2046
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='230' style='font-size:16px'>Lichang Chen, Jiuhai Chen, Tom Goldstein, Heng Huang, and Tianyi Zhou. Instructzero: Efficient<br>instruction optimization for black-box large language models. arXiv preprint arXiv:2306.03082,<br>2023d.</p>",
            "id": 230,
            "page": 22,
            "text": "Lichang Chen, Jiuhai Chen, Tom Goldstein, Heng Huang, and Tianyi Zhou. Instructzero: Efficient instruction optimization for black-box large language models. arXiv preprint arXiv:2306.03082, 2023d."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2076
                },
                {
                    "x": 2109,
                    "y": 2076
                },
                {
                    "x": 2109,
                    "y": 2171
                },
                {
                    "x": 443,
                    "y": 2171
                }
            ],
            "category": "paragraph",
            "html": "<p id='231' style='font-size:16px'>Xinyun Chen and Yuandong Tian. Learning to perform local rewriting for combinatorial optimization.<br>Advances in Neural Information Processing Systems, 32, 2019.</p>",
            "id": 231,
            "page": 22,
            "text": "Xinyun Chen and Yuandong Tian. Learning to perform local rewriting for combinatorial optimization. Advances in Neural Information Processing Systems, 32, 2019."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2201
                },
                {
                    "x": 2109,
                    "y": 2201
                },
                {
                    "x": 2109,
                    "y": 2292
                },
                {
                    "x": 443,
                    "y": 2292
                }
            ],
            "category": "paragraph",
            "html": "<p id='232' style='font-size:18px'>Xinyun Chen, Maxwell Lin, Nathanael Scharli, and Denny Zhou. Teaching large language models to<br>self-debug. arXiv preprint arXiv:2304.05128, 2023e.</p>",
            "id": 232,
            "page": 22,
            "text": "Xinyun Chen, Maxwell Lin, Nathanael Scharli, and Denny Zhou. Teaching large language models to self-debug. arXiv preprint arXiv:2304.05128, 2023e."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2321
                },
                {
                    "x": 2110,
                    "y": 2321
                },
                {
                    "x": 2110,
                    "y": 2503
                },
                {
                    "x": 444,
                    "y": 2503
                }
            ],
            "category": "paragraph",
            "html": "<p id='233' style='font-size:16px'>Yutian Chen, Xingyou Song, Chansoo Lee, Zi Wang, Richard Zhang, David Dohan, Kazuya<br>Kawakami, Greg Kochanski, Arnaud Doucet, Marc' aurelio Ranzato, et al. Towards learning<br>universal hyperparameter optimizers with transformers. Advances in Neural Information Process-<br>ing Systems, 35:32053-32068, 2022.</p>",
            "id": 233,
            "page": 22,
            "text": "Yutian Chen, Xingyou Song, Chansoo Lee, Zi Wang, Richard Zhang, David Dohan, Kazuya Kawakami, Greg Kochanski, Arnaud Doucet, Marc' aurelio Ranzato,  Towards learning universal hyperparameter optimizers with transformers. Advances in Neural Information Processing Systems, 35:32053-32068, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2533
                },
                {
                    "x": 2109,
                    "y": 2533
                },
                {
                    "x": 2109,
                    "y": 2671
                },
                {
                    "x": 442,
                    "y": 2671
                }
            ],
            "category": "paragraph",
            "html": "<p id='234' style='font-size:14px'>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,<br>Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve<br>math word problems. arXiv preprint arXiv:2110.14168, 2021.</p>",
            "id": 234,
            "page": 22,
            "text": "Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,  Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2700
                },
                {
                    "x": 2110,
                    "y": 2700
                },
                {
                    "x": 2110,
                    "y": 2838
                },
                {
                    "x": 442,
                    "y": 2838
                }
            ],
            "category": "paragraph",
            "html": "<p id='235' style='font-size:18px'>Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song,<br>Eric P Xing, and Zhiting Hu. Rlprompt: Optimizing discrete text prompts with reinforcement<br>learning. arXiv preprint arXiv:2205.12548, 2022.</p>",
            "id": 235,
            "page": 22,
            "text": "Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric P Xing, and Zhiting Hu. Rlprompt: Optimizing discrete text prompts with reinforcement learning. arXiv preprint arXiv:2205.12548, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2868
                },
                {
                    "x": 2112,
                    "y": 2868
                },
                {
                    "x": 2112,
                    "y": 3051
                },
                {
                    "x": 444,
                    "y": 3051
                }
            ],
            "category": "paragraph",
            "html": "<p id='236' style='font-size:16px'>Michel Deudon, Pierre Cournut, Alexandre Lacoste, Yossiri Adulyasak, and Louis-Martin Rousseau.<br>Learning heuristics for the tsp by policy gradient. In International Conference on the Integration of<br>Constraint Programming, Artificial Intelligence, and Operations Research, pp. 170-181. Springer,<br>2018.</p>",
            "id": 236,
            "page": 22,
            "text": "Michel Deudon, Pierre Cournut, Alexandre Lacoste, Yossiri Adulyasak, and Louis-Martin Rousseau. Learning heuristics for the tsp by policy gradient. In International Conference on the Integration of Constraint Programming, Artificial Intelligence, and Operations Research, pp. 170-181. Springer, 2018."
        },
        {
            "bounding_box": [
                {
                    "x": 1249,
                    "y": 3132
                },
                {
                    "x": 1299,
                    "y": 3132
                },
                {
                    "x": 1299,
                    "y": 3171
                },
                {
                    "x": 1249,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='237' style='font-size:14px'>22</footer>",
            "id": 237,
            "page": 22,
            "text": "22"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 113
                },
                {
                    "x": 1098,
                    "y": 113
                },
                {
                    "x": 1098,
                    "y": 157
                },
                {
                    "x": 443,
                    "y": 157
                }
            ],
            "category": "header",
            "html": "<header id='238' style='font-size:22px'>Large Language Models as Optimizers</header>",
            "id": 238,
            "page": 23,
            "text": "Large Language Models as Optimizers"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 346
                },
                {
                    "x": 2112,
                    "y": 346
                },
                {
                    "x": 2112,
                    "y": 483
                },
                {
                    "x": 441,
                    "y": 483
                }
            ],
            "category": "paragraph",
            "html": "<p id='239' style='font-size:18px'>Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Tim Rock-<br>t�schel. Promptbreeder: Self-referential self-improvement via prompt evolution. arXiv preprint<br>arXiv:2309.16797, 2023.</p>",
            "id": 239,
            "page": 23,
            "text": "Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Tim Rockt�schel. Promptbreeder: Self-referential self-improvement via prompt evolution. arXiv preprint arXiv:2309.16797, 2023."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 519
                },
                {
                    "x": 2111,
                    "y": 519
                },
                {
                    "x": 2111,
                    "y": 658
                },
                {
                    "x": 442,
                    "y": 658
                }
            ],
            "category": "paragraph",
            "html": "<p id='240' style='font-size:16px'>Deep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas Liao, Kamile Lukosiute, Anna Chen,<br>Anna Goldie, Azalia Mirhoseini, Catherine Olsson, Danny Hernandez, et al. The capacity for<br>moral self-correction in large language models. arXiv preprint arXiv:2302.07459, 2023.</p>",
            "id": 240,
            "page": 23,
            "text": "Deep Ganguli, Amanda Askell, Nicholas Schiefer, Thomas Liao, Kamile Lukosiute, Anna Chen, Anna Goldie, Azalia Mirhoseini, Catherine Olsson, Danny Hernandez,  The capacity for moral self-correction in large language models. arXiv preprint arXiv:2302.07459, 2023."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 692
                },
                {
                    "x": 2107,
                    "y": 692
                },
                {
                    "x": 2107,
                    "y": 785
                },
                {
                    "x": 442,
                    "y": 785
                }
            ],
            "category": "paragraph",
            "html": "<p id='241' style='font-size:18px'>Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot<br>learners. arXiv preprint arXiv:2012.15723, 2020.</p>",
            "id": 241,
            "page": 23,
            "text": "Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners. arXiv preprint arXiv:2012.15723, 2020."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 820
                },
                {
                    "x": 2110,
                    "y": 820
                },
                {
                    "x": 2110,
                    "y": 913
                },
                {
                    "x": 442,
                    "y": 913
                }
            ],
            "category": "paragraph",
            "html": "<p id='242' style='font-size:18px'>Bruce Golden, Lawrence Bodin, T Doyle, and W Stewart Jr. Approximate traveling salesman<br>algorithms. Operations research, 28(3-part-ii):694-711, 1980.</p>",
            "id": 242,
            "page": 23,
            "text": "Bruce Golden, Lawrence Bodin, T Doyle, and W Stewart Jr. Approximate traveling salesman algorithms. Operations research, 28(3-part-ii):694-711, 1980."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 948
                },
                {
                    "x": 2109,
                    "y": 948
                },
                {
                    "x": 2109,
                    "y": 1087
                },
                {
                    "x": 441,
                    "y": 1087
                }
            ],
            "category": "paragraph",
            "html": "<p id='243' style='font-size:20px'>Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian,<br>and Yujiu Yang. Connecting large language models with evolutionary algorithms yields powerful<br>prompt optimizers. arXiv preprint arXiv:2309.08532, 2023.</p>",
            "id": 243,
            "page": 23,
            "text": "Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian, and Yujiu Yang. Connecting large language models with evolutionary algorithms yields powerful prompt optimizers. arXiv preprint arXiv:2309.08532, 2023."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1121
                },
                {
                    "x": 2110,
                    "y": 1121
                },
                {
                    "x": 2110,
                    "y": 1213
                },
                {
                    "x": 442,
                    "y": 1213
                }
            ],
            "category": "paragraph",
            "html": "<p id='244' style='font-size:16px'>Gregory Gutin and Abraham P Punnen. The traveling salesman problem and its variations, volume 12.<br>Springer Science & Business Media, 2006.</p>",
            "id": 244,
            "page": 23,
            "text": "Gregory Gutin and Abraham P Punnen. The traveling salesman problem and its variations, volume 12. Springer Science & Business Media, 2006."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1248
                },
                {
                    "x": 2106,
                    "y": 1248
                },
                {
                    "x": 2106,
                    "y": 1342
                },
                {
                    "x": 442,
                    "y": 1342
                }
            ],
            "category": "paragraph",
            "html": "<p id='245' style='font-size:16px'>Keld Helsgaun. An extension of the lin-kernighan-helsgaun tsp solver for constrained traveling<br>salesman and vehicle routing problems. Roskilde: Roskilde University, 12, 2017.</p>",
            "id": 245,
            "page": 23,
            "text": "Keld Helsgaun. An extension of the lin-kernighan-helsgaun tsp solver for constrained traveling salesman and vehicle routing problems. Roskilde: Roskilde University, 12, 2017."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1375
                },
                {
                    "x": 2105,
                    "y": 1375
                },
                {
                    "x": 2105,
                    "y": 1467
                },
                {
                    "x": 442,
                    "y": 1467
                }
            ],
            "category": "paragraph",
            "html": "<p id='246' style='font-size:16px'>Michael Junger, Gerhard Reinelt, and Giovanni Rinaldi. The traveling salesman problem. Handbooks<br>in operations research and management science, 7:225-330, 1995.</p>",
            "id": 246,
            "page": 23,
            "text": "Michael Junger, Gerhard Reinelt, and Giovanni Rinaldi. The traveling salesman problem. Handbooks in operations research and management science, 7:225-330, 1995."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1504
                },
                {
                    "x": 2108,
                    "y": 1504
                },
                {
                    "x": 2108,
                    "y": 1595
                },
                {
                    "x": 442,
                    "y": 1595
                }
            ],
            "category": "paragraph",
            "html": "<p id='247' style='font-size:16px'>Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks.<br>arXiv preprint arXiv:2303.17491, 2023.</p>",
            "id": 247,
            "page": 23,
            "text": "Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language models can solve computer tasks. arXiv preprint arXiv:2303.17491, 2023."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1630
                },
                {
                    "x": 2106,
                    "y": 1630
                },
                {
                    "x": 2106,
                    "y": 1723
                },
                {
                    "x": 442,
                    "y": 1723
                }
            ],
            "category": "paragraph",
            "html": "<p id='248' style='font-size:16px'>Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International<br>Conference on Learning Representations, 2015.</p>",
            "id": 248,
            "page": 23,
            "text": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1758
                },
                {
                    "x": 2106,
                    "y": 1758
                },
                {
                    "x": 2106,
                    "y": 1851
                },
                {
                    "x": 443,
                    "y": 1851
                }
            ],
            "category": "paragraph",
            "html": "<p id='249' style='font-size:14px'>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large<br>language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.</p>",
            "id": 249,
            "page": 23,
            "text": "Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1884
                },
                {
                    "x": 2113,
                    "y": 1884
                },
                {
                    "x": 2113,
                    "y": 2024
                },
                {
                    "x": 442,
                    "y": 2024
                }
            ],
            "category": "paragraph",
            "html": "<p id='250' style='font-size:14px'>Wouter Kool, Herke van Hoof, and Max Welling. Attention, learn to solve routing problems! In<br>International Conference on Learning Representations, 2019. URL https : / / openreview.<br>net / forum? id=ByxBF sRqYm.</p>",
            "id": 250,
            "page": 23,
            "text": "Wouter Kool, Herke van Hoof, and Max Welling. Attention, learn to solve routing problems! In International Conference on Learning Representations, 2019. URL https : / / openreview. net / forum? id=ByxBF sRqYm."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2058
                },
                {
                    "x": 2110,
                    "y": 2058
                },
                {
                    "x": 2110,
                    "y": 2152
                },
                {
                    "x": 443,
                    "y": 2152
                }
            ],
            "category": "paragraph",
            "html": "<p id='251' style='font-size:16px'>Joel Lehman, Jonathan Gordon, Shawn Jain, Kamal Ndousse, Cathy Yeh, and Kenneth 0 Stanley.<br>Evolution through large models. arXiv preprint arXiv:2206.08896, 2022.</p>",
            "id": 251,
            "page": 23,
            "text": "Joel Lehman, Jonathan Gordon, Shawn Jain, Kamal Ndousse, Cathy Yeh, and Kenneth 0 Stanley. Evolution through large models. arXiv preprint arXiv:2206.08896, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2185
                },
                {
                    "x": 2107,
                    "y": 2185
                },
                {
                    "x": 2107,
                    "y": 2278
                },
                {
                    "x": 442,
                    "y": 2278
                }
            ],
            "category": "paragraph",
            "html": "<p id='252' style='font-size:14px'>Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt<br>tuning. arXiv preprint arXiv:2104.08691, 2021.</p>",
            "id": 252,
            "page": 23,
            "text": "Brian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. arXiv preprint arXiv:2104.08691, 2021."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2313
                },
                {
                    "x": 2108,
                    "y": 2313
                },
                {
                    "x": 2108,
                    "y": 2407
                },
                {
                    "x": 442,
                    "y": 2407
                }
            ],
            "category": "paragraph",
            "html": "<p id='253' style='font-size:18px'>Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv<br>preprint arXiv:2101.00190, 2021.</p>",
            "id": 253,
            "page": 23,
            "text": "Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190, 2021."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2439
                },
                {
                    "x": 2110,
                    "y": 2439
                },
                {
                    "x": 2110,
                    "y": 2576
                },
                {
                    "x": 443,
                    "y": 2576
                }
            ],
            "category": "paragraph",
            "html": "<p id='254' style='font-size:18px'>Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale genera-<br>tion: Learning to solve and explain algebraic word problems. arXiv preprint arXiv:1705.04146,<br>2017.</p>",
            "id": 254,
            "page": 23,
            "text": "Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. arXiv preprint arXiv:1705.04146, 2017."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2614
                },
                {
                    "x": 2108,
                    "y": 2614
                },
                {
                    "x": 2108,
                    "y": 2707
                },
                {
                    "x": 443,
                    "y": 2707
                }
            ],
            "category": "paragraph",
            "html": "<p id='255' style='font-size:18px'>Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. Gpt<br>understands, too. arXiv preprint arXiv:2103.10385, 2021.</p>",
            "id": 255,
            "page": 23,
            "text": "Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. Gpt understands, too. arXiv preprint arXiv:2103.10385, 2021."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2741
                },
                {
                    "x": 2107,
                    "y": 2741
                },
                {
                    "x": 2107,
                    "y": 2878
                },
                {
                    "x": 443,
                    "y": 2878
                }
            ],
            "category": "paragraph",
            "html": "<p id='256' style='font-size:16px'>Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered<br>prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint<br>arXiv:2104.08786, 2021.</p>",
            "id": 256,
            "page": 23,
            "text": "Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786, 2021."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2914
                },
                {
                    "x": 2110,
                    "y": 2914
                },
                {
                    "x": 2110,
                    "y": 3050
                },
                {
                    "x": 442,
                    "y": 3050
                }
            ],
            "category": "paragraph",
            "html": "<p id='257' style='font-size:16px'>Xiao Ma, Swaroop Mishra, Ahmad Beirami, Alex Beutel, and Jilin Chen. Let's do a thought<br>experiment: Using counterfactuals to improve moral reasoning. arXiv preprint arXiv:2306.14308,<br>2023.</p>",
            "id": 257,
            "page": 23,
            "text": "Xiao Ma, Swaroop Mishra, Ahmad Beirami, Alex Beutel, and Jilin Chen. Let's do a thought experiment: Using counterfactuals to improve moral reasoning. arXiv preprint arXiv:2306.14308, 2023."
        },
        {
            "bounding_box": [
                {
                    "x": 1248,
                    "y": 3132
                },
                {
                    "x": 1299,
                    "y": 3132
                },
                {
                    "x": 1299,
                    "y": 3171
                },
                {
                    "x": 1248,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='258' style='font-size:14px'>23</footer>",
            "id": 258,
            "page": 23,
            "text": "23"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 114
                },
                {
                    "x": 1099,
                    "y": 114
                },
                {
                    "x": 1099,
                    "y": 157
                },
                {
                    "x": 443,
                    "y": 157
                }
            ],
            "category": "header",
            "html": "<header id='259' style='font-size:22px'>Large Language Models as Optimizers</header>",
            "id": 259,
            "page": 24,
            "text": "Large Language Models as Optimizers"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 348
                },
                {
                    "x": 2106,
                    "y": 348
                },
                {
                    "x": 2106,
                    "y": 438
                },
                {
                    "x": 443,
                    "y": 438
                }
            ],
            "category": "paragraph",
            "html": "<p id='260' style='font-size:14px'>Aman Madaan and Amir Yazdanbakhsh. Text and patterns: For effective chain of thought, it takes<br>two to tango. arXiv preprint arXiv:2209.07686, 2022.</p>",
            "id": 260,
            "page": 24,
            "text": "Aman Madaan and Amir Yazdanbakhsh. Text and patterns: For effective chain of thought, it takes two to tango. arXiv preprint arXiv:2209.07686, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 471
                },
                {
                    "x": 2108,
                    "y": 471
                },
                {
                    "x": 2108,
                    "y": 606
                },
                {
                    "x": 443,
                    "y": 606
                }
            ],
            "category": "paragraph",
            "html": "<p id='261' style='font-size:18px'>Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri<br>Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement<br>with self-feedback. arXiv preprint arXiv:2303.17651, 2023.</p>",
            "id": 261,
            "page": 24,
            "text": "Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,  Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 638
                },
                {
                    "x": 2107,
                    "y": 638
                },
                {
                    "x": 2107,
                    "y": 775
                },
                {
                    "x": 442,
                    "y": 775
                }
            ],
            "category": "paragraph",
            "html": "<p id='262' style='font-size:18px'>Elliot Meyerson, Mark J Nelson, Herbie Bradley, Arash Moradi, Amy K Hoover, and Joel<br>Lehman. Language model crossover: Variation through few-shot prompting. arXiv preprint<br>arXiv:2302.12170, 2023.</p>",
            "id": 262,
            "page": 24,
            "text": "Elliot Meyerson, Mark J Nelson, Herbie Bradley, Arash Moradi, Amy K Hoover, and Joel Lehman. Language model crossover: Variation through few-shot prompting. arXiv preprint arXiv:2302.12170, 2023."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 806
                },
                {
                    "x": 2110,
                    "y": 806
                },
                {
                    "x": 2110,
                    "y": 944
                },
                {
                    "x": 442,
                    "y": 944
                }
            ],
            "category": "paragraph",
            "html": "<p id='263' style='font-size:18px'>Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montserrat Gonzalez Arenas,<br>Kanishka Rao, Dorsa Sadigh, and Andy Zeng. Large language models as general pattern machines.<br>arXiv preprint arXiv:2307.04721, 2023.</p>",
            "id": 263,
            "page": 24,
            "text": "Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montserrat Gonzalez Arenas, Kanishka Rao, Dorsa Sadigh, and Andy Zeng. Large language models as general pattern machines. arXiv preprint arXiv:2307.04721, 2023."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 976
                },
                {
                    "x": 2107,
                    "y": 976
                },
                {
                    "x": 2107,
                    "y": 1069
                },
                {
                    "x": 444,
                    "y": 1069
                }
            ],
            "category": "paragraph",
            "html": "<p id='264' style='font-size:18px'>Varun Nair, Elliot Schumacher, Geoffrey Tso, and Anitha Kannan. Dera: Enhancing large language<br>model completions with dialog-enabled resolving agents. arXiv preprint arXiv:2303.17071, 2023.</p>",
            "id": 264,
            "page": 24,
            "text": "Varun Nair, Elliot Schumacher, Geoffrey Tso, and Anitha Kannan. Dera: Enhancing large language model completions with dialog-enabled resolving agents. arXiv preprint arXiv:2303.17071, 2023."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1099
                },
                {
                    "x": 2107,
                    "y": 1099
                },
                {
                    "x": 2107,
                    "y": 1236
                },
                {
                    "x": 442,
                    "y": 1236
                }
            ],
            "category": "paragraph",
            "html": "<p id='265' style='font-size:18px'>MohammadReza Nazari, Afshin Oroojlooy, Lawrence Snyder, and Martin Takac. Reinforcement<br>learning for solving the vehicle routing problem. In Advances in Neural Information Processing<br>Systems, pp. 9861-9871, 2018.</p>",
            "id": 265,
            "page": 24,
            "text": "MohammadReza Nazari, Afshin Oroojlooy, Lawrence Snyder, and Martin Takac. Reinforcement learning for solving the vehicle routing problem. In Advances in Neural Information Processing Systems, pp. 9861-9871, 2018."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1267
                },
                {
                    "x": 2110,
                    "y": 1267
                },
                {
                    "x": 2110,
                    "y": 1362
                },
                {
                    "x": 442,
                    "y": 1362
                }
            ],
            "category": "paragraph",
            "html": "<p id='266' style='font-size:20px'>Theo X Olausson, Jeevana Priya Inala, Chenglong Wang, Jianfeng Gao, and Armando Solar-Lezama.<br>Demystifying gpt self-repair for code generation. arXiv preprint arXiv:2306.09896, 2023.</p>",
            "id": 266,
            "page": 24,
            "text": "Theo X Olausson, Jeevana Priya Inala, Chenglong Wang, Jianfeng Gao, and Armando Solar-Lezama. Demystifying gpt self-repair for code generation. arXiv preprint arXiv:2306.09896, 2023."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1392
                },
                {
                    "x": 1605,
                    "y": 1392
                },
                {
                    "x": 1605,
                    "y": 1439
                },
                {
                    "x": 443,
                    "y": 1439
                }
            ],
            "category": "paragraph",
            "html": "<p id='267' style='font-size:16px'>Gurobi Optimization et al. Gurobi optimizer reference manual, 2020.</p>",
            "id": 267,
            "page": 24,
            "text": "Gurobi Optimization  Gurobi optimizer reference manual, 2020."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 1468
                },
                {
                    "x": 2107,
                    "y": 1468
                },
                {
                    "x": 2107,
                    "y": 1561
                },
                {
                    "x": 445,
                    "y": 1561
                }
            ],
            "category": "paragraph",
            "html": "<p id='268' style='font-size:18px'>Archiki Prasad, Peter Hase, Xiang Zhou, and Mohit Bansal. Grips: Gradient-free, edit-based<br>instruction search for prompting large language models. arXiv preprint arXiv:2203.07281, 2022.</p>",
            "id": 268,
            "page": 24,
            "text": "Archiki Prasad, Peter Hase, Xiang Zhou, and Mohit Bansal. Grips: Gradient-free, edit-based instruction search for prompting large language models. arXiv preprint arXiv:2203.07281, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1591
                },
                {
                    "x": 2106,
                    "y": 1591
                },
                {
                    "x": 2106,
                    "y": 1684
                },
                {
                    "x": 443,
                    "y": 1684
                }
            ],
            "category": "paragraph",
            "html": "<p id='269' style='font-size:16px'>Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt<br>optimization with\" gradient descent\" and beam search. arXiv preprint arXiv:2305.03495, 2023.</p>",
            "id": 269,
            "page": 24,
            "text": "Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang Zhu, and Michael Zeng. Automatic prompt optimization with\" gradient descent\" and beam search. arXiv preprint arXiv:2305.03495, 2023."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1714
                },
                {
                    "x": 2109,
                    "y": 1714
                },
                {
                    "x": 2109,
                    "y": 1804
                },
                {
                    "x": 442,
                    "y": 1804
                }
            ],
            "category": "paragraph",
            "html": "<p id='270' style='font-size:18px'>Ning Qian. On the momentum term in gradient descent learning algorithms. Neural networks, 12(1):<br>145-151, 1999.</p>",
            "id": 270,
            "page": 24,
            "text": "Ning Qian. On the momentum term in gradient descent learning algorithms. Neural networks, 12(1): 145-151, 1999."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1839
                },
                {
                    "x": 2111,
                    "y": 1839
                },
                {
                    "x": 2111,
                    "y": 1930
                },
                {
                    "x": 442,
                    "y": 1930
                }
            ],
            "category": "paragraph",
            "html": "<p id='271' style='font-size:16px'>Guanghui Qin and Jason Eisner. Learning how to ask: Querying lms with mixtures of soft prompts.<br>arXiv preprint arXiv:2104.06599, 2021.</p>",
            "id": 271,
            "page": 24,
            "text": "Guanghui Qin and Jason Eisner. Learning how to ask: Querying lms with mixtures of soft prompts. arXiv preprint arXiv:2104.06599, 2021."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1961
                },
                {
                    "x": 2106,
                    "y": 1961
                },
                {
                    "x": 2106,
                    "y": 2051
                },
                {
                    "x": 442,
                    "y": 2051
                }
            ],
            "category": "paragraph",
            "html": "<p id='272' style='font-size:18px'>Colin R Reeves. Modern heuristic techniques for combinatorial problems. John Wiley & Sons, Inc.,<br>1993.</p>",
            "id": 272,
            "page": 24,
            "text": "Colin R Reeves. Modern heuristic techniques for combinatorial problems. John Wiley & Sons, Inc., 1993."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2084
                },
                {
                    "x": 2108,
                    "y": 2084
                },
                {
                    "x": 2108,
                    "y": 2222
                },
                {
                    "x": 441,
                    "y": 2222
                }
            ],
            "category": "paragraph",
            "html": "<p id='273' style='font-size:18px'>Laria Reynolds and Kyle McDonell. Prompt programming for large language models: Beyond the<br>few-shot paradigm. In Extended Abstracts of the 2021 CHI Conference on Human Factors in<br>Computing Systems, pp. 1-7, 2021.</p>",
            "id": 273,
            "page": 24,
            "text": "Laria Reynolds and Kyle McDonell. Prompt programming for large language models: Beyond the few-shot paradigm. In Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems, pp. 1-7, 2021."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2254
                },
                {
                    "x": 2106,
                    "y": 2254
                },
                {
                    "x": 2106,
                    "y": 2345
                },
                {
                    "x": 441,
                    "y": 2345
                }
            ],
            "category": "paragraph",
            "html": "<p id='274' style='font-size:16px'>Luis Miguel Rios and Nikolaos V Sahinidis. Derivative-free optimization: a review of algorithms and<br>comparison of software implementations. Journal of Global Optimization, 56:1247-1293, 2013.</p>",
            "id": 274,
            "page": 24,
            "text": "Luis Miguel Rios and Nikolaos V Sahinidis. Derivative-free optimization: a review of algorithms and comparison of software implementations. Journal of Global Optimization, 56:1247-1293, 2013."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2375
                },
                {
                    "x": 2107,
                    "y": 2375
                },
                {
                    "x": 2107,
                    "y": 2468
                },
                {
                    "x": 442,
                    "y": 2468
                }
            ],
            "category": "paragraph",
            "html": "<p id='275' style='font-size:16px'>Daniel J Rosenkrantz, Richard E Stearns, and Philip M Lewis, II. An analysis of several heuristics<br>for the traveling salesman problem. SIAM journal on computing, 6(3):563-581, 1977.</p>",
            "id": 275,
            "page": 24,
            "text": "Daniel J Rosenkrantz, Richard E Stearns, and Philip M Lewis, II. An analysis of several heuristics for the traveling salesman problem. SIAM journal on computing, 6(3):563-581, 1977."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2500
                },
                {
                    "x": 2105,
                    "y": 2500
                },
                {
                    "x": 2105,
                    "y": 2589
                },
                {
                    "x": 443,
                    "y": 2589
                }
            ],
            "category": "paragraph",
            "html": "<p id='276' style='font-size:18px'>Subhro Roy and Dan Roth. Solving general arithmetic word problems. arXiv preprint<br>arXiv:1608.01413, 2016.</p>",
            "id": 276,
            "page": 24,
            "text": "Subhro Roy and Dan Roth. Solving general arithmetic word problems. arXiv preprint arXiv:1608.01413, 2016."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2621
                },
                {
                    "x": 2108,
                    "y": 2621
                },
                {
                    "x": 2108,
                    "y": 2760
                },
                {
                    "x": 443,
                    "y": 2760
                }
            ],
            "category": "paragraph",
            "html": "<p id='277' style='font-size:14px'>Timo Schick, Jane Dwivedi- Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer,<br>Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to<br>use tools. arXiv preprint arXiv:2302.04761, 2023.</p>",
            "id": 277,
            "page": 24,
            "text": "Timo Schick, Jane Dwivedi- Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2791
                },
                {
                    "x": 2107,
                    "y": 2791
                },
                {
                    "x": 2107,
                    "y": 2927
                },
                {
                    "x": 442,
                    "y": 2927
                }
            ],
            "category": "paragraph",
            "html": "<p id='278' style='font-size:20px'>Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. Autoprompt:<br>Eliciting knowledge from language models with automatically generated prompts. arXiv preprint<br>arXiv:2010.15980, 2020.</p>",
            "id": 278,
            "page": 24,
            "text": "Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace, and Sameer Singh. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. arXiv preprint arXiv:2010.15980, 2020."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2958
                },
                {
                    "x": 2108,
                    "y": 2958
                },
                {
                    "x": 2108,
                    "y": 3051
                },
                {
                    "x": 443,
                    "y": 3051
                }
            ],
            "category": "paragraph",
            "html": "<p id='279' style='font-size:14px'>Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic<br>memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023.</p>",
            "id": 279,
            "page": 24,
            "text": "Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366, 2023."
        },
        {
            "bounding_box": [
                {
                    "x": 1248,
                    "y": 3133
                },
                {
                    "x": 1299,
                    "y": 3133
                },
                {
                    "x": 1299,
                    "y": 3171
                },
                {
                    "x": 1248,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='280' style='font-size:14px'>24</footer>",
            "id": 280,
            "page": 24,
            "text": "24"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 112
                },
                {
                    "x": 1100,
                    "y": 112
                },
                {
                    "x": 1100,
                    "y": 158
                },
                {
                    "x": 442,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='281' style='font-size:20px'>Large Language Models as Optimizers</header>",
            "id": 281,
            "page": 25,
            "text": "Large Language Models as Optimizers"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 345
                },
                {
                    "x": 2110,
                    "y": 345
                },
                {
                    "x": 2110,
                    "y": 530
                },
                {
                    "x": 444,
                    "y": 530
                }
            ],
            "category": "paragraph",
            "html": "<p id='282' style='font-size:16px'>Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam<br>Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adria Garriga-Alonso, et al. Beyond the<br>imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint<br>arXiv:2206.04615, 2022.</p>",
            "id": 282,
            "page": 25,
            "text": "Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adria Garriga-Alonso,  Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 561
                },
                {
                    "x": 2111,
                    "y": 561
                },
                {
                    "x": 2111,
                    "y": 702
                },
                {
                    "x": 441,
                    "y": 702
                }
            ],
            "category": "paragraph",
            "html": "<p id='283' style='font-size:16px'>Mirac Suzgun, Nathan Scales, Nathanael Sch�rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung,<br>Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks<br>and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.</p>",
            "id": 283,
            "page": 25,
            "text": "Mirac Suzgun, Nathan Scales, Nathanael Sch�rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou,  Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 733
                },
                {
                    "x": 2110,
                    "y": 733
                },
                {
                    "x": 2110,
                    "y": 871
                },
                {
                    "x": 443,
                    "y": 871
                }
            ],
            "category": "paragraph",
            "html": "<p id='284' style='font-size:18px'>Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and<br>Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv<br>preprint arXiv:2305.16291, 2023.</p>",
            "id": 284,
            "page": 25,
            "text": "Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 901
                },
                {
                    "x": 2112,
                    "y": 901
                },
                {
                    "x": 2112,
                    "y": 1040
                },
                {
                    "x": 442,
                    "y": 1040
                }
            ],
            "category": "paragraph",
            "html": "<p id='285' style='font-size:16px'>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh-<br>ery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models.<br>arXiv preprint arXiv:2203.11171, 2022.</p>",
            "id": 285,
            "page": 25,
            "text": "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1073
                },
                {
                    "x": 2110,
                    "y": 1073
                },
                {
                    "x": 2110,
                    "y": 1210
                },
                {
                    "x": 442,
                    "y": 1210
                }
            ],
            "category": "paragraph",
            "html": "<p id='286' style='font-size:16px'>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny<br>Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint<br>arXiv:2201.11903, 2022.</p>",
            "id": 286,
            "page": 25,
            "text": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1243
                },
                {
                    "x": 2108,
                    "y": 1243
                },
                {
                    "x": 2108,
                    "y": 1382
                },
                {
                    "x": 443,
                    "y": 1382
                }
            ],
            "category": "paragraph",
            "html": "<p id='287' style='font-size:18px'>Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu,<br>Da Huang, Denny Zhou, et al. Larger language models do in-context learning differently. arXiv<br>preprint arXiv:2303.03846, 2023.</p>",
            "id": 287,
            "page": 25,
            "text": "Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou,  Larger language models do in-context learning differently. arXiv preprint arXiv:2303.03846, 2023."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1411
                },
                {
                    "x": 2112,
                    "y": 1411
                },
                {
                    "x": 2112,
                    "y": 1552
                },
                {
                    "x": 443,
                    "y": 1552
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='288' style='font-size:16px'>Yuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping, and Tom Goldstein.<br>Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery.<br>arXiv preprint arXiv:2302.03668, 2023.</p>",
            "id": 288,
            "page": 25,
            "text": "Yuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Hard prompts made easy: Gradient-based discrete optimization for prompt tuning and discovery. arXiv preprint arXiv:2302.03668, 2023."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1583
                },
                {
                    "x": 2108,
                    "y": 1583
                },
                {
                    "x": 2108,
                    "y": 1722
                },
                {
                    "x": 441,
                    "y": 1722
                }
            ],
            "category": "paragraph",
            "html": "<p id='289' style='font-size:18px'>Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin<br>Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv<br>preprint arXiv:2304.12244, 2023.</p>",
            "id": 289,
            "page": 25,
            "text": "Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1754
                },
                {
                    "x": 2110,
                    "y": 1754
                },
                {
                    "x": 2110,
                    "y": 1848
                },
                {
                    "x": 442,
                    "y": 1848
                }
            ],
            "category": "paragraph",
            "html": "<p id='290' style='font-size:18px'>Hanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Yanggang Wang, Haiyu Li, and Zhilin Yang. Gps:<br>Genetic prompt search for efficient few-shot learning. arXiv preprint arXiv:2210.17041, 2022.</p>",
            "id": 290,
            "page": 25,
            "text": "Hanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Yanggang Wang, Haiyu Li, and Zhilin Yang. Gps: Genetic prompt search for efficient few-shot learning. arXiv preprint arXiv:2210.17041, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1878
                },
                {
                    "x": 2108,
                    "y": 1878
                },
                {
                    "x": 2108,
                    "y": 1971
                },
                {
                    "x": 443,
                    "y": 1971
                }
            ],
            "category": "paragraph",
            "html": "<p id='291' style='font-size:16px'>Weizhe Yuan, Kyunghyun Cho, and Jason Weston. System-level natural language feedback. arXiv<br>preprint arXiv:2306.13588, 2023.</p>",
            "id": 291,
            "page": 25,
            "text": "Weizhe Yuan, Kyunghyun Cho, and Jason Weston. System-level natural language feedback. arXiv preprint arXiv:2306.13588, 2023."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2002
                },
                {
                    "x": 2110,
                    "y": 2002
                },
                {
                    "x": 2110,
                    "y": 2141
                },
                {
                    "x": 443,
                    "y": 2141
                }
            ],
            "category": "paragraph",
            "html": "<p id='292' style='font-size:18px'>Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schuurmans, and Joseph E Gonzalez. Tempera:<br>Test-time prompt editing via reinforcement learning. In The Eleventh International Conference on<br>Learning Representations, 2023.</p>",
            "id": 292,
            "page": 25,
            "text": "Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schuurmans, and Joseph E Gonzalez. Tempera: Test-time prompt editing via reinforcement learning. In The Eleventh International Conference on Learning Representations, 2023."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2172
                },
                {
                    "x": 2110,
                    "y": 2172
                },
                {
                    "x": 2110,
                    "y": 2310
                },
                {
                    "x": 441,
                    "y": 2310
                }
            ],
            "category": "paragraph",
            "html": "<p id='293' style='font-size:16px'>Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving<br>few-shot performance of language models. In International Conference on Machine Learning, pp.<br>12697-12706. PMLR, 2021.</p>",
            "id": 293,
            "page": 25,
            "text": "Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving few-shot performance of language models. In International Conference on Machine Learning, pp. 12697-12706. PMLR, 2021."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2341
                },
                {
                    "x": 2110,
                    "y": 2341
                },
                {
                    "x": 2110,
                    "y": 2482
                },
                {
                    "x": 442,
                    "y": 2482
                }
            ],
            "category": "paragraph",
            "html": "<p id='294' style='font-size:16px'>Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans,<br>Claire Cui, Olivier Bousquet, Quoc Le, et al. Least-to-most prompting enables complex reasoning<br>in large language models. arXiv preprint arXiv:2205.10625, 2022a.</p>",
            "id": 294,
            "page": 25,
            "text": "Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le,  Least-to-most prompting enables complex reasoning in large language models. arXiv preprint arXiv:2205.10625, 2022a."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2513
                },
                {
                    "x": 2110,
                    "y": 2513
                },
                {
                    "x": 2110,
                    "y": 2653
                },
                {
                    "x": 444,
                    "y": 2653
                }
            ],
            "category": "paragraph",
            "html": "<p id='295' style='font-size:16px'>Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan,<br>and Jimmy Ba. Large language models are human-level prompt engineers. arXiv preprint<br>arXiv:2211.01910, 2022b.</p>",
            "id": 295,
            "page": 25,
            "text": "Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level prompt engineers. arXiv preprint arXiv:2211.01910, 2022b."
        },
        {
            "bounding_box": [
                {
                    "x": 1248,
                    "y": 3132
                },
                {
                    "x": 1300,
                    "y": 3132
                },
                {
                    "x": 1300,
                    "y": 3171
                },
                {
                    "x": 1248,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='296' style='font-size:14px'>25</footer>",
            "id": 296,
            "page": 25,
            "text": "25"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 112
                },
                {
                    "x": 1099,
                    "y": 112
                },
                {
                    "x": 1099,
                    "y": 158
                },
                {
                    "x": 443,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='297' style='font-size:22px'>Large Language Models as Optimizers</header>",
            "id": 297,
            "page": 26,
            "text": "Large Language Models as Optimizers"
        },
        {
            "bounding_box": [
                {
                    "x": 446,
                    "y": 341
                },
                {
                    "x": 1024,
                    "y": 341
                },
                {
                    "x": 1024,
                    "y": 394
                },
                {
                    "x": 446,
                    "y": 394
                }
            ],
            "category": "paragraph",
            "html": "<p id='298' style='font-size:22px'>A SOME FAILURE CASES</p>",
            "id": 298,
            "page": 26,
            "text": "A SOME FAILURE CASES"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 476
                },
                {
                    "x": 2110,
                    "y": 476
                },
                {
                    "x": 2110,
                    "y": 617
                },
                {
                    "x": 442,
                    "y": 617
                }
            ],
            "category": "paragraph",
            "html": "<p id='299' style='font-size:20px'>Although LLMs show the power of optimizing basic math problems (Section 3) and prompts (Sec-<br>tion 4), we see some limitations across all optimizer LLMs that may impede their power of solving<br>more challenging problems. These limitations include:</p>",
            "id": 299,
            "page": 26,
            "text": "Although LLMs show the power of optimizing basic math problems (Section 3) and prompts (Section 4), we see some limitations across all optimizer LLMs that may impede their power of solving more challenging problems. These limitations include:"
        },
        {
            "bounding_box": [
                {
                    "x": 488,
                    "y": 637
                },
                {
                    "x": 2112,
                    "y": 637
                },
                {
                    "x": 2112,
                    "y": 869
                },
                {
                    "x": 488,
                    "y": 869
                }
            ],
            "category": "paragraph",
            "html": "<p id='300' style='font-size:18px'>· Hallucinating the values that need to come from math calculation: The optimizer LLMs<br>often output contents like \"the function value at (5, 3) is 15\" despite that the true value is not 15.<br>The model will get it right if external tools that can reliably calculate the value are triggered.<br>When and how to trigger such tool use cases remains an interesting topic (see e.g., (Schick et al.,<br>2023; Cai et al., 2023)).</p>",
            "id": 300,
            "page": 26,
            "text": "· Hallucinating the values that need to come from math calculation: The optimizer LLMs often output contents like \"the function value at (5, 3) is 15\" despite that the true value is not 15. The model will get it right if external tools that can reliably calculate the value are triggered. When and how to trigger such tool use cases remains an interesting topic (see e.g., (Schick , 2023; Cai , 2023))."
        },
        {
            "bounding_box": [
                {
                    "x": 489,
                    "y": 896
                },
                {
                    "x": 2110,
                    "y": 896
                },
                {
                    "x": 2110,
                    "y": 1268
                },
                {
                    "x": 489,
                    "y": 1268
                }
            ],
            "category": "paragraph",
            "html": "<p id='301' style='font-size:18px'>· Generating solutions already appeared in context even if we tell it to \"Give me a new (w, b)<br>pair that is different from all pairs above\": the optimizer LLMs do not 100% reliably follow<br>this instruction even if its own outputs often include sentences like \"I will provide a new pair<br>that is different\" making the output self-contradictory. The output is almost guaranteed to be<br>different from in-context old solutions when the model output contains a comparison of the new<br>pair and all old pairs, though. Thus (implicitly) triggering such behaviors may be a solution.<br>How to implement this feature without harming the instruction following performance of other<br>parts remains an interesting topic to study.</p>",
            "id": 301,
            "page": 26,
            "text": "· Generating solutions already appeared in context even if we tell it to \"Give me a new (w, b) pair that is different from all pairs above\": the optimizer LLMs do not 100% reliably follow this instruction even if its own outputs often include sentences like \"I will provide a new pair that is different\" making the output self-contradictory. The output is almost guaranteed to be different from in-context old solutions when the model output contains a comparison of the new pair and all old pairs, though. Thus (implicitly) triggering such behaviors may be a solution. How to implement this feature without harming the instruction following performance of other parts remains an interesting topic to study."
        },
        {
            "bounding_box": [
                {
                    "x": 488,
                    "y": 1293
                },
                {
                    "x": 2109,
                    "y": 1293
                },
                {
                    "x": 2109,
                    "y": 1758
                },
                {
                    "x": 488,
                    "y": 1758
                }
            ],
            "category": "paragraph",
            "html": "<p id='302' style='font-size:16px'>· In black-box math optimization, getting stuck at a point that is neither global nor local<br>optimal: This often occurs in two linear regression cases: (a) The in-context exemplars all share<br>the same w or 6 that is different from Wtrue or btrue. This case is more likely to be avoided when<br>a larger number of past solutions are included in the meta-prompt; (b) one or several of the best<br>previous solutions in the meta-prompt have ws and bs in quantitatively opposite directions from<br>the global optima Wtrue and btrue: for example, the ws are all smaller than Wtrue while the bs are<br>all larger than btrue. Since the optimizer model often proposes to only increase w or decrease 6<br>when the past solutions in meta-prompt share w or b, the optimization will get stuck if either<br>increasing w or decreasing 6 would increase the objective value. This issue is mitigated by<br>sampling multiple new solutions (thus more exploration) at each step.</p>",
            "id": 302,
            "page": 26,
            "text": "· In black-box math optimization, getting stuck at a point that is neither global nor local optimal: This often occurs in two linear regression cases: (a) The in-context exemplars all share the same w or 6 that is different from Wtrue or btrue. This case is more likely to be avoided when a larger number of past solutions are included in the meta-prompt; (b) one or several of the best previous solutions in the meta-prompt have ws and bs in quantitatively opposite directions from the global optima Wtrue and btrue: for example, the ws are all smaller than Wtrue while the bs are all larger than btrue. Since the optimizer model often proposes to only increase w or decrease 6 when the past solutions in meta-prompt share w or b, the optimization will get stuck if either increasing w or decreasing 6 would increase the objective value. This issue is mitigated by sampling multiple new solutions (thus more exploration) at each step."
        },
        {
            "bounding_box": [
                {
                    "x": 488,
                    "y": 1782
                },
                {
                    "x": 2110,
                    "y": 1782
                },
                {
                    "x": 2110,
                    "y": 2156
                },
                {
                    "x": 488,
                    "y": 2156
                }
            ],
            "category": "paragraph",
            "html": "<p id='303' style='font-size:20px'>· Hard to navigate a bumpy loss landscape: Like other optimizers, it is harder for the optimizer<br>LLM to optimize black-box functions when the loss landscape gets more complicated. For<br>example, when minimizing the Rosenbrock function f(x, y) = (a-x)2 +b(y - x2)2 with a = 20<br>(whose global optimal point is x = 20, y = 400) with 5 starting points in [10, 20] x [10, 20],<br>the optimization often gets stuck at around (0, 0). This is because the optimizer LLM sees a<br>decrease of objective value when it drastically decreases both x and y to 0. Then starting from<br>(0, 0), the optimizer LLM is hard to further navigate x and y along the narrow valley in the loss<br>landscape towards (20, 400) (Figure 13).</p>",
            "id": 303,
            "page": 26,
            "text": "· Hard to navigate a bumpy loss landscape: Like other optimizers, it is harder for the optimizer LLM to optimize black-box functions when the loss landscape gets more complicated. For example, when minimizing the Rosenbrock function f(x, y) = (a-x)2 +b(y - x2)2 with a = 20 (whose global optimal point is x = 20, y = 400) with 5 starting points in  x , the optimization often gets stuck at around (0, 0). This is because the optimizer LLM sees a decrease of objective value when it drastically decreases both x and y to 0. Then starting from (0, 0), the optimizer LLM is hard to further navigate x and y along the narrow valley in the loss landscape towards (20, 400) (Figure 13)."
        },
        {
            "bounding_box": [
                {
                    "x": 924,
                    "y": 2337
                },
                {
                    "x": 1584,
                    "y": 2337
                },
                {
                    "x": 1584,
                    "y": 2831
                },
                {
                    "x": 924,
                    "y": 2831
                }
            ],
            "category": "figure",
            "html": "<figure><img id='304' style='font-size:14px' alt=\"150000\n受100000\n드\n50000\n0\n5\n0 10\n100\n200 15 X\n300 20\ny 400\" data-coord=\"top-left:(924,2337); bottom-right:(1584,2831)\" /></figure>",
            "id": 304,
            "page": 26,
            "text": "150000 受100000 드 50000 0 5 0 10 100 200 15 X 300 20 y 400"
        },
        {
            "bounding_box": [
                {
                    "x": 439,
                    "y": 2899
                },
                {
                    "x": 2110,
                    "y": 2899
                },
                {
                    "x": 2110,
                    "y": 3046
                },
                {
                    "x": 439,
                    "y": 3046
                }
            ],
            "category": "caption",
            "html": "<caption id='305' style='font-size:18px'>Figure 13: A visualization of the landscape of the Rosenbrock function f(x,y) = (a-x)2 +b(y-x2)2<br>with a = 20 and 6 = 1. The global optima is at x = 20, y = 400 with function value 0. The function<br>value at x = 0, y = 0 is 400. The landscape has a narrow valley between (0, 0) and (20, 400).</caption>",
            "id": 305,
            "page": 26,
            "text": "Figure 13: A visualization of the landscape of the Rosenbrock function f(x,y) = (a-x)2 +b(y-x2)2 with a = 20 and 6 = 1. The global optima is at x = 20, y = 400 with function value 0. The function value at x = 0, y = 0 is 400. The landscape has a narrow valley between (0, 0) and (20, 400)."
        },
        {
            "bounding_box": [
                {
                    "x": 1249,
                    "y": 3132
                },
                {
                    "x": 1300,
                    "y": 3132
                },
                {
                    "x": 1300,
                    "y": 3172
                },
                {
                    "x": 1249,
                    "y": 3172
                }
            ],
            "category": "footer",
            "html": "<footer id='306' style='font-size:18px'>26</footer>",
            "id": 306,
            "page": 26,
            "text": "26"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 115
                },
                {
                    "x": 1097,
                    "y": 115
                },
                {
                    "x": 1097,
                    "y": 156
                },
                {
                    "x": 445,
                    "y": 156
                }
            ],
            "category": "header",
            "html": "<header id='307' style='font-size:20px'>Large Language Models as Optimizers</header>",
            "id": 307,
            "page": 27,
            "text": "Large Language Models as Optimizers"
        },
        {
            "bounding_box": [
                {
                    "x": 446,
                    "y": 343
                },
                {
                    "x": 1424,
                    "y": 343
                },
                {
                    "x": 1424,
                    "y": 392
                },
                {
                    "x": 446,
                    "y": 392
                }
            ],
            "category": "paragraph",
            "html": "<p id='308' style='font-size:20px'>B PROMPTING FORMATS FOR SCORER LLM</p>",
            "id": 308,
            "page": 27,
            "text": "B PROMPTING FORMATS FOR SCORER LLM"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 467
                },
                {
                    "x": 2109,
                    "y": 467
                },
                {
                    "x": 2109,
                    "y": 603
                },
                {
                    "x": 443,
                    "y": 603
                }
            ],
            "category": "paragraph",
            "html": "<p id='309' style='font-size:18px'>Figure 14, 15, and 16 show examples of the Q_begin, Q_end, and A_begin prompting formats when<br>the \"QA\" pattern is present. The \"QA\" pattern is eliminated when prompting instruction-tuned scorer<br>models like text-bison with the Q_begin and Q_end formats (Figure 17 and 18).</p>",
            "id": 309,
            "page": 27,
            "text": "Figure 14, 15, and 16 show examples of the Q_begin, Q_end, and A_begin prompting formats when the \"QA\" pattern is present. The \"QA\" pattern is eliminated when prompting instruction-tuned scorer models like text-bison with the Q_begin and Q_end formats (Figure 17 and 18)."
        },
        {
            "bounding_box": [
                {
                    "x": 472,
                    "y": 693
                },
                {
                    "x": 743,
                    "y": 693
                },
                {
                    "x": 743,
                    "y": 734
                },
                {
                    "x": 472,
                    "y": 734
                }
            ],
            "category": "paragraph",
            "html": "<p id='310' style='font-size:22px'>Q: {instruction}</p>",
            "id": 310,
            "page": 27,
            "text": "Q: {instruction}"
        },
        {
            "bounding_box": [
                {
                    "x": 471,
                    "y": 741
                },
                {
                    "x": 2131,
                    "y": 741
                },
                {
                    "x": 2131,
                    "y": 875
                },
                {
                    "x": 471,
                    "y": 875
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='311' style='font-size:16px'>Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for<br>her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh<br>duck egg. How much in dollars does she make every day at the farmers' market?</p>",
            "id": 311,
            "page": 27,
            "text": "Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?"
        },
        {
            "bounding_box": [
                {
                    "x": 470,
                    "y": 919
                },
                {
                    "x": 514,
                    "y": 919
                },
                {
                    "x": 514,
                    "y": 956
                },
                {
                    "x": 470,
                    "y": 956
                }
            ],
            "category": "paragraph",
            "html": "<p id='312' style='font-size:14px'>A:</p>",
            "id": 312,
            "page": 27,
            "text": "A:"
        },
        {
            "bounding_box": [
                {
                    "x": 493,
                    "y": 1021
                },
                {
                    "x": 2054,
                    "y": 1021
                },
                {
                    "x": 2054,
                    "y": 1068
                },
                {
                    "x": 493,
                    "y": 1068
                }
            ],
            "category": "paragraph",
            "html": "<p id='313' style='font-size:18px'>Figure 14: The Q_begin prompting format on a GSM8K test exemplar with the \"QA\" pattern.</p>",
            "id": 313,
            "page": 27,
            "text": "Figure 14: The Q_begin prompting format on a GSM8K test exemplar with the \"QA\" pattern."
        },
        {
            "bounding_box": [
                {
                    "x": 466,
                    "y": 1235
                },
                {
                    "x": 2131,
                    "y": 1235
                },
                {
                    "x": 2131,
                    "y": 1415
                },
                {
                    "x": 466,
                    "y": 1415
                }
            ],
            "category": "paragraph",
            "html": "<p id='314' style='font-size:16px'>Q: Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins<br>for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per<br>fresh duck egg. How much in dollars does she make every day at the farmers' market?<br>{instruction}</p>",
            "id": 314,
            "page": 27,
            "text": "Q: Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market? {instruction}"
        },
        {
            "bounding_box": [
                {
                    "x": 471,
                    "y": 1462
                },
                {
                    "x": 515,
                    "y": 1462
                },
                {
                    "x": 515,
                    "y": 1498
                },
                {
                    "x": 471,
                    "y": 1498
                }
            ],
            "category": "paragraph",
            "html": "<p id='315' style='font-size:14px'>A:</p>",
            "id": 315,
            "page": 27,
            "text": "A:"
        },
        {
            "bounding_box": [
                {
                    "x": 510,
                    "y": 1565
                },
                {
                    "x": 2038,
                    "y": 1565
                },
                {
                    "x": 2038,
                    "y": 1612
                },
                {
                    "x": 510,
                    "y": 1612
                }
            ],
            "category": "paragraph",
            "html": "<p id='316' style='font-size:18px'>Figure 15: The Q_end prompting format on a GSM8K test exemplar with the \"QA\" pattern.</p>",
            "id": 316,
            "page": 27,
            "text": "Figure 15: The Q_end prompting format on a GSM8K test exemplar with the \"QA\" pattern."
        },
        {
            "bounding_box": [
                {
                    "x": 469,
                    "y": 1778
                },
                {
                    "x": 2131,
                    "y": 1778
                },
                {
                    "x": 2131,
                    "y": 1915
                },
                {
                    "x": 469,
                    "y": 1915
                }
            ],
            "category": "paragraph",
            "html": "<p id='317' style='font-size:16px'>Q: Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins<br>for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per<br>fresh duck egg. How much in dollars does she make every day at the farmers' market?</p>",
            "id": 317,
            "page": 27,
            "text": "Q: Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?"
        },
        {
            "bounding_box": [
                {
                    "x": 467,
                    "y": 1959
                },
                {
                    "x": 744,
                    "y": 1959
                },
                {
                    "x": 744,
                    "y": 2001
                },
                {
                    "x": 467,
                    "y": 2001
                }
            ],
            "category": "paragraph",
            "html": "<p id='318' style='font-size:20px'>A: {instruction}</p>",
            "id": 318,
            "page": 27,
            "text": "A: {instruction}"
        },
        {
            "bounding_box": [
                {
                    "x": 681,
                    "y": 2070
                },
                {
                    "x": 1865,
                    "y": 2070
                },
                {
                    "x": 1865,
                    "y": 2115
                },
                {
                    "x": 681,
                    "y": 2115
                }
            ],
            "category": "paragraph",
            "html": "<p id='319' style='font-size:18px'>Figure 16: The A_begin prompting format on a GSM8K test exemplar.</p>",
            "id": 319,
            "page": 27,
            "text": "Figure 16: The A_begin prompting format on a GSM8K test exemplar."
        },
        {
            "bounding_box": [
                {
                    "x": 468,
                    "y": 2286
                },
                {
                    "x": 683,
                    "y": 2286
                },
                {
                    "x": 683,
                    "y": 2326
                },
                {
                    "x": 468,
                    "y": 2326
                }
            ],
            "category": "paragraph",
            "html": "<p id='320' style='font-size:22px'>{instruction}</p>",
            "id": 320,
            "page": 27,
            "text": "{instruction}"
        },
        {
            "bounding_box": [
                {
                    "x": 469,
                    "y": 2332
                },
                {
                    "x": 2130,
                    "y": 2332
                },
                {
                    "x": 2130,
                    "y": 2466
                },
                {
                    "x": 469,
                    "y": 2466
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='321' style='font-size:16px'>Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for<br>her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh<br>duck egg. How much in dollars does she make every day at the farmers' market?</p>",
            "id": 321,
            "page": 27,
            "text": "Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?"
        },
        {
            "bounding_box": [
                {
                    "x": 468,
                    "y": 2534
                },
                {
                    "x": 2078,
                    "y": 2534
                },
                {
                    "x": 2078,
                    "y": 2581
                },
                {
                    "x": 468,
                    "y": 2581
                }
            ],
            "category": "paragraph",
            "html": "<p id='322' style='font-size:16px'>Figure 17: The Q_begin prompting format on a GSM8K test exemplar without the \"QA\" pattern.</p>",
            "id": 322,
            "page": 27,
            "text": "Figure 17: The Q_begin prompting format on a GSM8K test exemplar without the \"QA\" pattern."
        },
        {
            "bounding_box": [
                {
                    "x": 467,
                    "y": 2747
                },
                {
                    "x": 2132,
                    "y": 2747
                },
                {
                    "x": 2132,
                    "y": 2928
                },
                {
                    "x": 467,
                    "y": 2928
                }
            ],
            "category": "paragraph",
            "html": "<p id='323' style='font-size:16px'>Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for<br>her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh<br>duck egg. How much in dollars does she make every day at the farmers' market?<br>{instruction}</p>",
            "id": 323,
            "page": 27,
            "text": "Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market? {instruction}"
        },
        {
            "bounding_box": [
                {
                    "x": 481,
                    "y": 2998
                },
                {
                    "x": 2068,
                    "y": 2998
                },
                {
                    "x": 2068,
                    "y": 3043
                },
                {
                    "x": 481,
                    "y": 3043
                }
            ],
            "category": "paragraph",
            "html": "<p id='324' style='font-size:16px'>Figure 18: The Q_end prompting format on a GSM8K test exemplar without the \"QA\" pattern.</p>",
            "id": 324,
            "page": 27,
            "text": "Figure 18: The Q_end prompting format on a GSM8K test exemplar without the \"QA\" pattern."
        },
        {
            "bounding_box": [
                {
                    "x": 1248,
                    "y": 3132
                },
                {
                    "x": 1299,
                    "y": 3132
                },
                {
                    "x": 1299,
                    "y": 3171
                },
                {
                    "x": 1248,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='325' style='font-size:16px'>27</footer>",
            "id": 325,
            "page": 27,
            "text": "27"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 115
                },
                {
                    "x": 1098,
                    "y": 115
                },
                {
                    "x": 1098,
                    "y": 156
                },
                {
                    "x": 445,
                    "y": 156
                }
            ],
            "category": "header",
            "html": "<header id='326' style='font-size:20px'>Large Language Models as Optimizers</header>",
            "id": 326,
            "page": 28,
            "text": "Large Language Models as Optimizers"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 344
                },
                {
                    "x": 887,
                    "y": 344
                },
                {
                    "x": 887,
                    "y": 392
                },
                {
                    "x": 445,
                    "y": 392
                }
            ],
            "category": "paragraph",
            "html": "<p id='327' style='font-size:22px'>C META-PROMPTS</p>",
            "id": 327,
            "page": 28,
            "text": "C META-PROMPTS"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 469
                },
                {
                    "x": 1307,
                    "y": 469
                },
                {
                    "x": 1307,
                    "y": 513
                },
                {
                    "x": 445,
                    "y": 513
                }
            ],
            "category": "paragraph",
            "html": "<p id='328' style='font-size:16px'>C.1 META-PROMPT FOR MATH OPTIMIZATION</p>",
            "id": 328,
            "page": 28,
            "text": "C.1 META-PROMPT FOR MATH OPTIMIZATION"
        },
        {
            "bounding_box": [
                {
                    "x": 461,
                    "y": 631
                },
                {
                    "x": 2136,
                    "y": 631
                },
                {
                    "x": 2136,
                    "y": 768
                },
                {
                    "x": 461,
                    "y": 768
                }
            ],
            "category": "paragraph",
            "html": "<p id='329' style='font-size:14px'>Now you will help me minimize a function with two input variables W, b. I have some (w, b) pairs<br>and the function values at those points. The pairs are arranged in descending order based on their<br>function values, where lower values are better.</p>",
            "id": 329,
            "page": 28,
            "text": "Now you will help me minimize a function with two input variables W, b. I have some (w, b) pairs and the function values at those points. The pairs are arranged in descending order based on their function values, where lower values are better."
        },
        {
            "bounding_box": [
                {
                    "x": 469,
                    "y": 815
                },
                {
                    "x": 679,
                    "y": 815
                },
                {
                    "x": 679,
                    "y": 988
                },
                {
                    "x": 469,
                    "y": 988
                }
            ],
            "category": "paragraph",
            "html": "<p id='330' style='font-size:16px'>input:<br>w=18, b=15<br>value:<br>10386334</p>",
            "id": 330,
            "page": 28,
            "text": "input: w=18, b=15 value: 10386334"
        },
        {
            "bounding_box": [
                {
                    "x": 469,
                    "y": 1040
                },
                {
                    "x": 678,
                    "y": 1040
                },
                {
                    "x": 678,
                    "y": 1212
                },
                {
                    "x": 469,
                    "y": 1212
                }
            ],
            "category": "paragraph",
            "html": "<p id='331' style='font-size:18px'>input:<br>w=17, b=18<br>value:<br>9204724</p>",
            "id": 331,
            "page": 28,
            "text": "input: w=17, b=18 value: 9204724"
        },
        {
            "bounding_box": [
                {
                    "x": 464,
                    "y": 1258
                },
                {
                    "x": 2131,
                    "y": 1258
                },
                {
                    "x": 2131,
                    "y": 1391
                },
                {
                    "x": 464,
                    "y": 1391
                }
            ],
            "category": "paragraph",
            "html": "<p id='332' style='font-size:14px'>Give me a new (W, b) pair that is different from all pairs above, and has a function value lower than<br>any of the above. Do not write code. The output must end with a pair [W, b], where W and b are<br>numerical values.</p>",
            "id": 332,
            "page": 28,
            "text": "Give me a new (W, b) pair that is different from all pairs above, and has a function value lower than any of the above. Do not write code. The output must end with a pair [W, b], where W and b are numerical values."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1455
                },
                {
                    "x": 2105,
                    "y": 1455
                },
                {
                    "x": 2105,
                    "y": 1546
                },
                {
                    "x": 443,
                    "y": 1546
                }
            ],
            "category": "paragraph",
            "html": "<p id='333' style='font-size:14px'>Figure 19: An example of the meta-prompt for linear regression. The blue text contains solution-score<br>pairs; the orange text are meta-instructions.</p>",
            "id": 333,
            "page": 28,
            "text": "Figure 19: An example of the meta-prompt for linear regression. The blue text contains solution-score pairs; the orange text are meta-instructions."
        },
        {
            "bounding_box": [
                {
                    "x": 461,
                    "y": 1725
                },
                {
                    "x": 2137,
                    "y": 1725
                },
                {
                    "x": 2137,
                    "y": 1998
                },
                {
                    "x": 461,
                    "y": 1998
                }
            ],
            "category": "paragraph",
            "html": "<p id='334' style='font-size:20px'>You are given a list of points with coordinates below: (0): (-4, 5), (1): (17, 76), (2): (-9, 0), (3): (-31,<br>-86), (4): (53, -35), (5): (26, 91), (6): (65, -33), (7): (26, 86), (8): (-13, -70), (9): (13, 79), (10): (-73,<br>-86), (11): (-45, 93), (12): (74, 24), (13): (67, -42), (14): (87, 51), (15): (83, 94), (16): (-7, 52), (17):<br>(-89, 47), (18): (0, -38), (19): (61, 58).<br>Below are some previous traces and their lengths. The traces are arranged in descending order based<br>on their lengths, where lower values are better.</p>",
            "id": 334,
            "page": 28,
            "text": "You are given a list of points with coordinates below: (0): (-4, 5), (1): (17, 76), (2): (-9, 0), (3): (-31, -86), (4): (53, -35), (5): (26, 91), (6): (65, -33), (7): (26, 86), (8): (-13, -70), (9): (13, 79), (10): (-73, -86), (11): (-45, 93), (12): (74, 24), (13): (67, -42), (14): (87, 51), (15): (83, 94), (16): (-7, 52), (17): (-89, 47), (18): (0, -38), (19): (61, 58). Below are some previous traces and their lengths. The traces are arranged in descending order based on their lengths, where lower values are better."
        },
        {
            "bounding_box": [
                {
                    "x": 464,
                    "y": 2042
                },
                {
                    "x": 1584,
                    "y": 2042
                },
                {
                    "x": 1584,
                    "y": 2171
                },
                {
                    "x": 464,
                    "y": 2171
                }
            ],
            "category": "paragraph",
            "html": "<p id='335' style='font-size:16px'><trace> 0133.16J92175A71881961411.15.12 </trace><br>length:<br>2254</p>",
            "id": 335,
            "page": 28,
            "text": "<trace> 0133.16J92175A71881961411.15.12 </trace> length: 2254"
        },
        {
            "bounding_box": [
                {
                    "x": 464,
                    "y": 2221
                },
                {
                    "x": 1583,
                    "y": 2221
                },
                {
                    "x": 1583,
                    "y": 2350
                },
                {
                    "x": 464,
                    "y": 2350
                }
            ],
            "category": "paragraph",
            "html": "<p id='336' style='font-size:18px'><trace> 0,18,4,11,9,7,14,17,12,16,005,19,13,16,16,2 </trace><br>length:<br>2017</p>",
            "id": 336,
            "page": 28,
            "text": "<trace> 0,18,4,11,9,7,14,17,12,16,005,19,13,16,16,2 </trace> length: 2017"
        },
        {
            "bounding_box": [
                {
                    "x": 464,
                    "y": 2400
                },
                {
                    "x": 1582,
                    "y": 2400
                },
                {
                    "x": 1582,
                    "y": 2528
                },
                {
                    "x": 464,
                    "y": 2528
                }
            ],
            "category": "paragraph",
            "html": "<p id='337' style='font-size:18px'><trace> 0.11.4.13.6.00.8.17.12.15.5,19,2,1,147,16.9 </trace><br>length:<br>1953</p>",
            "id": 337,
            "page": 28,
            "text": "<trace> 0.11.4.13.6.00.8.17.12.15.5,19,2,1,147,16.9 </trace> length: 1953"
        },
        {
            "bounding_box": [
                {
                    "x": 463,
                    "y": 2579
                },
                {
                    "x": 1582,
                    "y": 2579
                },
                {
                    "x": 1582,
                    "y": 2705
                },
                {
                    "x": 463,
                    "y": 2705
                }
            ],
            "category": "paragraph",
            "html": "<p id='338' style='font-size:16px'><trace> 010AJ8687.16J4.11215.1519.13.173 </trace><br>length:<br>1840</p>",
            "id": 338,
            "page": 28,
            "text": "<trace> 010AJ8687.16J4.11215.1519.13.173 </trace> length: 1840"
        },
        {
            "bounding_box": [
                {
                    "x": 465,
                    "y": 2754
                },
                {
                    "x": 2131,
                    "y": 2754
                },
                {
                    "x": 2131,
                    "y": 2889
                },
                {
                    "x": 465,
                    "y": 2889
                }
            ],
            "category": "paragraph",
            "html": "<p id='339' style='font-size:14px'>Give me a new trace that is different from all traces above, and has a length lower than any of the<br>above. The trace should traverse all points exactly once. The trace should start with <trace> and end<br>with </trace>.</p>",
            "id": 339,
            "page": 28,
            "text": "Give me a new trace that is different from all traces above, and has a length lower than any of the above. The trace should traverse all points exactly once. The trace should start with <trace> and end with </trace>."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2951
                },
                {
                    "x": 2106,
                    "y": 2951
                },
                {
                    "x": 2106,
                    "y": 3042
                },
                {
                    "x": 443,
                    "y": 3042
                }
            ],
            "category": "paragraph",
            "html": "<p id='340' style='font-size:14px'>Figure 20: An example of the meta-prompt for Traveling Salesman Problems with problem size<br>n = 20. The blue text contains solution-score pairs; the orange text are meta-instructions.</p>",
            "id": 340,
            "page": 28,
            "text": "Figure 20: An example of the meta-prompt for Traveling Salesman Problems with problem size n = 20. The blue text contains solution-score pairs; the orange text are meta-instructions."
        },
        {
            "bounding_box": [
                {
                    "x": 1249,
                    "y": 3133
                },
                {
                    "x": 1299,
                    "y": 3133
                },
                {
                    "x": 1299,
                    "y": 3171
                },
                {
                    "x": 1249,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='341' style='font-size:14px'>28</footer>",
            "id": 341,
            "page": 28,
            "text": "28"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 115
                },
                {
                    "x": 1098,
                    "y": 115
                },
                {
                    "x": 1098,
                    "y": 156
                },
                {
                    "x": 445,
                    "y": 156
                }
            ],
            "category": "header",
            "html": "<header id='342' style='font-size:22px'>Large Language Models as Optimizers</header>",
            "id": 342,
            "page": 29,
            "text": "Large Language Models as Optimizers"
        },
        {
            "bounding_box": [
                {
                    "x": 447,
                    "y": 349
                },
                {
                    "x": 1345,
                    "y": 349
                },
                {
                    "x": 1345,
                    "y": 392
                },
                {
                    "x": 447,
                    "y": 392
                }
            ],
            "category": "paragraph",
            "html": "<p id='343' style='font-size:18px'>C.2 META-PROMPT FOR PROMPT OPTIMIZATION</p>",
            "id": 343,
            "page": 29,
            "text": "C.2 META-PROMPT FOR PROMPT OPTIMIZATION"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 433
                },
                {
                    "x": 2109,
                    "y": 433
                },
                {
                    "x": 2109,
                    "y": 569
                },
                {
                    "x": 445,
                    "y": 569
                }
            ],
            "category": "paragraph",
            "html": "<p id='344' style='font-size:18px'>Different optimizer models work the best on different styles of meta-prompts. Figure 3 in the main<br>paper shows the meta-prompt for PaLM 2-L-IT; Figure 21 shows that for pre-trained PaLM 2-L;<br>Figure 22 shows that for GPT models.</p>",
            "id": 344,
            "page": 29,
            "text": "Different optimizer models work the best on different styles of meta-prompts. Figure 3 in the main paper shows the meta-prompt for PaLM 2-L-IT; Figure 21 shows that for pre-trained PaLM 2-L; Figure 22 shows that for GPT models."
        },
        {
            "bounding_box": [
                {
                    "x": 460,
                    "y": 630
                },
                {
                    "x": 2126,
                    "y": 630
                },
                {
                    "x": 2126,
                    "y": 719
                },
                {
                    "x": 460,
                    "y": 719
                }
            ],
            "category": "paragraph",
            "html": "<p id='345' style='font-size:18px'>Create a piece of text at the beginning of the answer to enhance the precision in solving diverse grade<br>school math problems.</p>",
            "id": 345,
            "page": 29,
            "text": "Create a piece of text at the beginning of the answer to enhance the precision in solving diverse grade school math problems."
        },
        {
            "bounding_box": [
                {
                    "x": 467,
                    "y": 761
                },
                {
                    "x": 1131,
                    "y": 761
                },
                {
                    "x": 1131,
                    "y": 806
                },
                {
                    "x": 467,
                    "y": 806
                }
            ],
            "category": "paragraph",
            "html": "<p id='346' style='font-size:18px'>Precision: 4 <TEXT>A dime</TEXT></p>",
            "id": 346,
            "page": 29,
            "text": "Precision: 4 <TEXT>A dime</TEXT>"
        },
        {
            "bounding_box": [
                {
                    "x": 470,
                    "y": 848
                },
                {
                    "x": 1520,
                    "y": 848
                },
                {
                    "x": 1520,
                    "y": 893
                },
                {
                    "x": 470,
                    "y": 893
                }
            ],
            "category": "paragraph",
            "html": "<p id='347' style='font-size:16px'>Precision: 17 <TEXT>The answer is a function. It is</TEXT></p>",
            "id": 347,
            "page": 29,
            "text": "Precision: 17 <TEXT>The answer is a function. It is</TEXT>"
        },
        {
            "bounding_box": [
                {
                    "x": 471,
                    "y": 936
                },
                {
                    "x": 1867,
                    "y": 936
                },
                {
                    "x": 1867,
                    "y": 981
                },
                {
                    "x": 471,
                    "y": 981
                }
            ],
            "category": "paragraph",
            "html": "<p id='348' style='font-size:16px'>Precision: 19 <TEXT>So how can we find out what this equation means?</TEXT></p>",
            "id": 348,
            "page": 29,
            "text": "Precision: 19 <TEXT>So how can we find out what this equation means?</TEXT>"
        },
        {
            "bounding_box": [
                {
                    "x": 469,
                    "y": 1024
                },
                {
                    "x": 1195,
                    "y": 1024
                },
                {
                    "x": 1195,
                    "y": 1066
                },
                {
                    "x": 469,
                    "y": 1066
                }
            ],
            "category": "paragraph",
            "html": "<p id='349' style='font-size:18px'>Precision: 20 <TEXT>Solutions:</TEXT></p>",
            "id": 349,
            "page": 29,
            "text": "Precision: 20 <TEXT>Solutions:</TEXT>"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 1129
                },
                {
                    "x": 2108,
                    "y": 1129
                },
                {
                    "x": 2108,
                    "y": 1267
                },
                {
                    "x": 445,
                    "y": 1267
                }
            ],
            "category": "paragraph",
            "html": "<p id='350' style='font-size:18px'>Figure 21: An example of the meta-prompt for prompt optimization with pre-trained PaLM 2-L<br>on GSM8K, where the generated instruction will be prepended to the beginning of the scorer LLM<br>output (A_begin in Section 4.1).</p>",
            "id": 350,
            "page": 29,
            "text": "Figure 21: An example of the meta-prompt for prompt optimization with pre-trained PaLM 2-L on GSM8K, where the generated instruction will be prepended to the beginning of the scorer LLM output (A_begin in Section 4.1)."
        },
        {
            "bounding_box": [
                {
                    "x": 467,
                    "y": 1373
                },
                {
                    "x": 2131,
                    "y": 1373
                },
                {
                    "x": 2131,
                    "y": 1464
                },
                {
                    "x": 467,
                    "y": 1464
                }
            ],
            "category": "paragraph",
            "html": "<p id='351' style='font-size:16px'>Your task is to generate the instruction <INS>. Below are some previous instructions with their scores.<br>The score ranges from 0 to 100.</p>",
            "id": 351,
            "page": 29,
            "text": "Your task is to generate the instruction <INS>. Below are some previous instructions with their scores. The score ranges from 0 to 100."
        },
        {
            "bounding_box": [
                {
                    "x": 469,
                    "y": 1511
                },
                {
                    "x": 782,
                    "y": 1511
                },
                {
                    "x": 782,
                    "y": 1686
                },
                {
                    "x": 469,
                    "y": 1686
                }
            ],
            "category": "paragraph",
            "html": "<p id='352' style='font-size:16px'>text:<br>Let's figure it out!<br>score:<br>61</p>",
            "id": 352,
            "page": 29,
            "text": "text: Let's figure it out! score: 61"
        },
        {
            "bounding_box": [
                {
                    "x": 467,
                    "y": 1733
                },
                {
                    "x": 879,
                    "y": 1733
                },
                {
                    "x": 879,
                    "y": 1907
                },
                {
                    "x": 467,
                    "y": 1907
                }
            ],
            "category": "paragraph",
            "html": "<p id='353' style='font-size:16px'>text:<br>Let's solve the problem.<br>score:<br>63</p>",
            "id": 353,
            "page": 29,
            "text": "text: Let's solve the problem. score: 63"
        },
        {
            "bounding_box": [
                {
                    "x": 467,
                    "y": 1955
                },
                {
                    "x": 1102,
                    "y": 1955
                },
                {
                    "x": 1102,
                    "y": 1999
                },
                {
                    "x": 467,
                    "y": 1999
                }
            ],
            "category": "paragraph",
            "html": "<p id='354' style='font-size:14px'>(... more instructions and scores )</p>",
            "id": 354,
            "page": 29,
            "text": "(... more instructions and scores )"
        },
        {
            "bounding_box": [
                {
                    "x": 470,
                    "y": 2042
                },
                {
                    "x": 914,
                    "y": 2042
                },
                {
                    "x": 914,
                    "y": 2087
                },
                {
                    "x": 470,
                    "y": 2087
                }
            ],
            "category": "paragraph",
            "html": "<p id='355' style='font-size:16px'>Below are some problems.</p>",
            "id": 355,
            "page": 29,
            "text": "Below are some problems."
        },
        {
            "bounding_box": [
                {
                    "x": 470,
                    "y": 2128
                },
                {
                    "x": 629,
                    "y": 2128
                },
                {
                    "x": 629,
                    "y": 2170
                },
                {
                    "x": 470,
                    "y": 2170
                }
            ],
            "category": "paragraph",
            "html": "<p id='356' style='font-size:20px'>Problem:</p>",
            "id": 356,
            "page": 29,
            "text": "Problem:"
        },
        {
            "bounding_box": [
                {
                    "x": 469,
                    "y": 2176
                },
                {
                    "x": 2132,
                    "y": 2176
                },
                {
                    "x": 2132,
                    "y": 2353
                },
                {
                    "x": 469,
                    "y": 2353
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='357' style='font-size:18px'>Q: Alannah, Beatrix, and Queen are preparing for the new school year and have been given books<br>by their parents. Alannah has 20 more books than Beatrix. Queen has 1/5 times more books than<br>Alannah. If Beatrix has 30 books, how many books do the three have together?<br>A: <INS></p>",
            "id": 357,
            "page": 29,
            "text": "Q: Alannah, Beatrix, and Queen are preparing for the new school year and have been given books by their parents. Alannah has 20 more books than Beatrix. Queen has 1/5 times more books than Alannah. If Beatrix has 30 books, how many books do the three have together? A: <INS>"
        },
        {
            "bounding_box": [
                {
                    "x": 469,
                    "y": 2396
                },
                {
                    "x": 835,
                    "y": 2396
                },
                {
                    "x": 835,
                    "y": 2484
                },
                {
                    "x": 469,
                    "y": 2484
                }
            ],
            "category": "paragraph",
            "html": "<p id='358' style='font-size:14px'>Ground truth answer:<br>140</p>",
            "id": 358,
            "page": 29,
            "text": "Ground truth answer: 140"
        },
        {
            "bounding_box": [
                {
                    "x": 467,
                    "y": 2534
                },
                {
                    "x": 894,
                    "y": 2534
                },
                {
                    "x": 894,
                    "y": 2575
                },
                {
                    "x": 467,
                    "y": 2575
                }
            ],
            "category": "paragraph",
            "html": "<p id='359' style='font-size:14px'>(... more exemplars · )</p>",
            "id": 359,
            "page": 29,
            "text": "(... more exemplars · )"
        },
        {
            "bounding_box": [
                {
                    "x": 467,
                    "y": 2618
                },
                {
                    "x": 2132,
                    "y": 2618
                },
                {
                    "x": 2132,
                    "y": 2753
                },
                {
                    "x": 467,
                    "y": 2753
                }
            ],
            "category": "paragraph",
            "html": "<p id='360' style='font-size:18px'>Generate an instruction that is different from all the instructions <INS> above, and has a higher score<br>than all the instructions <INS> above. The instruction should begin with <INS> and end with </INS>.<br>The instruction should be concise, effective, and generally applicable to all problems above.</p>",
            "id": 360,
            "page": 29,
            "text": "Generate an instruction that is different from all the instructions <INS> above, and has a higher score than all the instructions <INS> above. The instruction should begin with <INS> and end with </INS>. The instruction should be concise, effective, and generally applicable to all problems above."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2822
                },
                {
                    "x": 2112,
                    "y": 2822
                },
                {
                    "x": 2112,
                    "y": 3052
                },
                {
                    "x": 443,
                    "y": 3052
                }
            ],
            "category": "paragraph",
            "html": "<p id='361' style='font-size:18px'>Figure 22: An example of the meta-prompt for prompt optimization with GPT models<br>(gpt-3 . 5-turbo or gpt-4) on GSM8K, where the generated instruction will be prepended<br>to the beginning of the scorer LLM output (A_begin in Section 4.1). The blue text contains solution-<br>score pairs; the purple text describes the optimization task and output format; the orange text are<br>meta-instructions.</p>",
            "id": 361,
            "page": 29,
            "text": "Figure 22: An example of the meta-prompt for prompt optimization with GPT models (gpt-3 . 5-turbo or gpt-4) on GSM8K, where the generated instruction will be prepended to the beginning of the scorer LLM output (A_begin in Section 4.1). The blue text contains solutionscore pairs; the purple text describes the optimization task and output format; the orange text are meta-instructions."
        },
        {
            "bounding_box": [
                {
                    "x": 1249,
                    "y": 3133
                },
                {
                    "x": 1299,
                    "y": 3133
                },
                {
                    "x": 1299,
                    "y": 3170
                },
                {
                    "x": 1249,
                    "y": 3170
                }
            ],
            "category": "footer",
            "html": "<footer id='362' style='font-size:18px'>29</footer>",
            "id": 362,
            "page": 29,
            "text": "29"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 112
                },
                {
                    "x": 1099,
                    "y": 112
                },
                {
                    "x": 1099,
                    "y": 157
                },
                {
                    "x": 443,
                    "y": 157
                }
            ],
            "category": "header",
            "html": "<header id='363' style='font-size:20px'>Large Language Models as Optimizers</header>",
            "id": 363,
            "page": 30,
            "text": "Large Language Models as Optimizers"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 339
                },
                {
                    "x": 1955,
                    "y": 339
                },
                {
                    "x": 1955,
                    "y": 395
                },
                {
                    "x": 443,
                    "y": 395
                }
            ],
            "category": "paragraph",
            "html": "<p id='364' style='font-size:22px'>D PROMPT OPTIMIZATION CURVES ON THE REMAINING BBH TASKS</p>",
            "id": 364,
            "page": 30,
            "text": "D PROMPT OPTIMIZATION CURVES ON THE REMAINING BBH TASKS"
        },
        {
            "bounding_box": [
                {
                    "x": 459,
                    "y": 577
                },
                {
                    "x": 2092,
                    "y": 577
                },
                {
                    "x": 2092,
                    "y": 2888
                },
                {
                    "x": 459,
                    "y": 2888
                }
            ],
            "category": "figure",
            "html": "<figure><img id='365' style='font-size:14px' alt=\"90.0 80.0 60.0\naccuracy\n70.0 accuracy\n70.0 accuracy\n50.0\ntraining\nBBH BBH BBH\ncausal_judgement training\nboolean_ expressions training\ndate understanding\n50.0 60.0 40.0\n0 50 100 0 50 100 0 50 100 150\n# steps # steps # steps\n(a) BBH boolean_expressions (b) BBH causal _judgement (c) BBH date_understanding\n70.0\naccuracy\naccuracy\naccuracy\n60.0 100.0\n60.0\ntraining\n50.0\n98.0 BBH\nBBH BBH\ndisambiguation_qa training\nformal fallacies\ndyck_ languages training\n40.0 50.0\n0 50 100 0 50 100 0 20 40 60\n# steps # steps # steps\n(d) BBH disambiguation_qa (e) BBH dyck_languages (f) BBH formal fallacies\n65\n80.0 deduction\naccuracy\naccuracy\nseven_objects\n30.0\n60\n70.0 accuracy BBH logical\ntraining\n20.0\nBBH BBH\nhyperbaton training\ngeometric_shapes training\n60.0 55\n0 50 100 150 200 0 50 100 150 200 0 50 100 150 200\n# steps # steps # steps\n(g) BBH geometric_shapes (h) BBH hyperbaton (i) BBH logical_deduction_seven_objects\n30 70\n100\naccuracy\naccuracy\n90\n20 65\ntraining 80 accuracy\n10 60\n70 multistep_\nBBH\nBBH movie\nBBH navigate\narithmetic_ two training\nrecommendation training\n60 0 55\n0 50 100 150 200 0 50 100 150 200 0 40 80 120\n# steps # steps # steps\n① BBH movie recommendation (k) BBH multistep_arithmetic_two (1) BBH navigate\n70\naccuracy\naccuracy\naccuracy\n70 80-\n60\ntraining\n50\n60 70\nBBH reasoning_ about_\nBBH object_counting training\nBBH penguins_in_ a_table training\ncolored objects\n40\n0 50 100 0 50 100 0 20 40 60\n# steps # steps # steps\n(m) BBH object_counting (n) BBH penguins_in_a_table (o) BBH reasoning_about_colored_objects\" data-coord=\"top-left:(459,577); bottom-right:(2092,2888)\" /></figure>",
            "id": 365,
            "page": 30,
            "text": "90.0 80.0 60.0 accuracy 70.0 accuracy 70.0 accuracy 50.0 training BBH BBH BBH causal_judgement training boolean_ expressions training date understanding 50.0 60.0 40.0 0 50 100 0 50 100 0 50 100 150 # steps # steps # steps (a) BBH boolean_expressions (b) BBH causal _judgement (c) BBH date_understanding 70.0 accuracy accuracy accuracy 60.0 100.0 60.0 training 50.0 98.0 BBH BBH BBH disambiguation_qa training formal fallacies dyck_ languages training 40.0 50.0 0 50 100 0 50 100 0 20 40 60 # steps # steps # steps (d) BBH disambiguation_qa (e) BBH dyck_languages (f) BBH formal fallacies 65 80.0 deduction accuracy accuracy seven_objects 30.0 60 70.0 accuracy BBH logical training 20.0 BBH BBH hyperbaton training geometric_shapes training 60.0 55 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 # steps # steps # steps (g) BBH geometric_shapes (h) BBH hyperbaton (i) BBH logical_deduction_seven_objects 30 70 100 accuracy accuracy 90 20 65 training 80 accuracy 10 60 70 multistep_ BBH BBH movie BBH navigate arithmetic_ two training recommendation training 60 0 55 0 50 100 150 200 0 50 100 150 200 0 40 80 120 # steps # steps # steps ① BBH movie recommendation (k) BBH multistep_arithmetic_two (1) BBH navigate 70 accuracy accuracy accuracy 70 8060 training 50 60 70 BBH reasoning_ about_ BBH object_counting training BBH penguins_in_ a_table training colored objects 40 0 50 100 0 50 100 0 20 40 60 # steps # steps # steps (m) BBH object_counting (n) BBH penguins_in_a_table (o) BBH reasoning_about_colored_objects"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2904
                },
                {
                    "x": 2112,
                    "y": 2904
                },
                {
                    "x": 2112,
                    "y": 3046
                },
                {
                    "x": 442,
                    "y": 3046
                }
            ],
            "category": "caption",
            "html": "<br><caption id='366' style='font-size:16px'>Figure 23: Prompt optimization on 21 BBH tasks (except ruin names and temporal_sequences<br>already shown in Figure 6) with the text-bison scorer and the PaLM 2-L-IT optimizer, Part I.<br>Most curves have upward trends.</caption>",
            "id": 366,
            "page": 30,
            "text": "Figure 23: Prompt optimization on 21 BBH tasks (except ruin names and temporal_sequences already shown in Figure 6) with the text-bison scorer and the PaLM 2-L-IT optimizer, Part I. Most curves have upward trends."
        },
        {
            "bounding_box": [
                {
                    "x": 1250,
                    "y": 3133
                },
                {
                    "x": 1300,
                    "y": 3133
                },
                {
                    "x": 1300,
                    "y": 3172
                },
                {
                    "x": 1250,
                    "y": 3172
                }
            ],
            "category": "footer",
            "html": "<footer id='367' style='font-size:18px'>30</footer>",
            "id": 367,
            "page": 30,
            "text": "30"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 111
                },
                {
                    "x": 1100,
                    "y": 111
                },
                {
                    "x": 1100,
                    "y": 158
                },
                {
                    "x": 442,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='368' style='font-size:22px'>Large Language Models as Optimizers</header>",
            "id": 368,
            "page": 31,
            "text": "Large Language Models as Optimizers"
        },
        {
            "bounding_box": [
                {
                    "x": 459,
                    "y": 352
                },
                {
                    "x": 2086,
                    "y": 352
                },
                {
                    "x": 2086,
                    "y": 1268
                },
                {
                    "x": 459,
                    "y": 1268
                }
            ],
            "category": "figure",
            "html": "<figure><img id='369' style='font-size:14px' alt=\"100\naccuracy 80\naccuracy\naccuracy\n40 80\ntraining\n60\nBBH salient translation_ 70 BBH sports\nBBH snarks training\n30 error_detection training\nunderstanding\n40\n0 20 40 0 50 100 150 200 0 20 40\n# steps # steps # steps\n(a) BBH salient_translation_error_detection (b) BBH snarks (c) BBH sports_understanding\naccuracy\naccuracy\naccuracy\n60\n20\n20\ntraining\n10 50\nBBH tracking_shuffled_ 10\nBBH web_ of_lies training\nBBH word_sorting\nobjects_seven_objects training\n0 50 100 150 200 0 50 100 150 200 0 50 100 150 200\n# steps # steps # steps\n(d) BBH tracking_shuffled_ (e) BBH web_of_ lies (f) BBH word_sorting\nobjects_seven_objects\" data-coord=\"top-left:(459,352); bottom-right:(2086,1268)\" /></figure>",
            "id": 369,
            "page": 31,
            "text": "100 accuracy 80 accuracy accuracy 40 80 training 60 BBH salient translation_ 70 BBH sports BBH snarks training 30 error_detection training understanding 40 0 20 40 0 50 100 150 200 0 20 40 # steps # steps # steps (a) BBH salient_translation_error_detection (b) BBH snarks (c) BBH sports_understanding accuracy accuracy accuracy 60 20 20 training 10 50 BBH tracking_shuffled_ 10 BBH web_ of_lies training BBH word_sorting objects_seven_objects training 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 # steps # steps # steps (d) BBH tracking_shuffled_ (e) BBH web_of_ lies (f) BBH word_sorting objects_seven_objects"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1314
                },
                {
                    "x": 2108,
                    "y": 1314
                },
                {
                    "x": 2108,
                    "y": 1458
                },
                {
                    "x": 441,
                    "y": 1458
                }
            ],
            "category": "paragraph",
            "html": "<p id='370' style='font-size:20px'>Figure 24: Prompt optimization on 21 BBH tasks (except ruin _names and temporal_sequences in<br>Figure 6) with the text-bison scorer and the PaLM 2-L-IT optimizer, Part II. All curves have<br>upward trends.</p>",
            "id": 370,
            "page": 31,
            "text": "Figure 24: Prompt optimization on 21 BBH tasks (except ruin _names and temporal_sequences in Figure 6) with the text-bison scorer and the PaLM 2-L-IT optimizer, Part II. All curves have upward trends."
        },
        {
            "bounding_box": [
                {
                    "x": 439,
                    "y": 1540
                },
                {
                    "x": 2109,
                    "y": 1540
                },
                {
                    "x": 2109,
                    "y": 1758
                },
                {
                    "x": 439,
                    "y": 1758
                }
            ],
            "category": "paragraph",
            "html": "<p id='371' style='font-size:18px'>E PROMPT OPTIMIZATION ON BBH TASKS - TABULATED ACCURACIES AND<br>FOUND INSTRUCTIONS<br>E. 1 PALM 2-L-IT AS OPTIMIZER, OPTIMIZATION STARTING FROM THE EMPTY STRING</p>",
            "id": 371,
            "page": 31,
            "text": "E PROMPT OPTIMIZATION ON BBH TASKS - TABULATED ACCURACIES AND FOUND INSTRUCTIONS E. 1 PALM 2-L-IT AS OPTIMIZER, OPTIMIZATION STARTING FROM THE EMPTY STRING"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1790
                },
                {
                    "x": 2110,
                    "y": 1790
                },
                {
                    "x": 2110,
                    "y": 1978
                },
                {
                    "x": 441,
                    "y": 1978
                }
            ],
            "category": "paragraph",
            "html": "<p id='372' style='font-size:18px'>Table 8 and 9 show the instructions found by prompt optimization. A comparison of their accuracies<br>with baselines \"Let's think step by step.' (Kojima et al., 2022), \"Let's work this out in a step by step<br>way to be sure we have the right answer.\" (Zhou et al., 2022b), and the empty string is in Table 7; a<br>visualization is in Section 5.2 Figure 5.</p>",
            "id": 372,
            "page": 31,
            "text": "Table 8 and 9 show the instructions found by prompt optimization. A comparison of their accuracies with baselines \"Let's think step by step.' (Kojima , 2022), \"Let's work this out in a step by step way to be sure we have the right answer.\" (Zhou , 2022b), and the empty string is in Table 7; a visualization is in Section 5.2 Figure 5."
        },
        {
            "bounding_box": [
                {
                    "x": 1249,
                    "y": 3131
                },
                {
                    "x": 1298,
                    "y": 3131
                },
                {
                    "x": 1298,
                    "y": 3172
                },
                {
                    "x": 1249,
                    "y": 3172
                }
            ],
            "category": "footer",
            "html": "<footer id='373' style='font-size:16px'>31</footer>",
            "id": 373,
            "page": 31,
            "text": "31"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 112
                },
                {
                    "x": 1099,
                    "y": 112
                },
                {
                    "x": 1099,
                    "y": 158
                },
                {
                    "x": 445,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='374' style='font-size:20px'>Large Language Models as Optimizers</header>",
            "id": 374,
            "page": 32,
            "text": "Large Language Models as Optimizers"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 368
                },
                {
                    "x": 2112,
                    "y": 368
                },
                {
                    "x": 2112,
                    "y": 608
                },
                {
                    "x": 441,
                    "y": 608
                }
            ],
            "category": "caption",
            "html": "<caption id='375' style='font-size:16px'>Table 7: Accuracies on BBH tasks: our found instructions with the PaLM 2-L-IT optimizer VS<br>baseline. The optimization starts from the empty string. Because of the 20-80 train-test split, we<br>show accuracies with the format \"training / test / overall (training + test)\". The PaLM 2-L scores are<br>from A begin instructions; the text-bison scores are from Q_ begin instructions. Bold numbers<br>indicate the best for the corresponding task.</caption>",
            "id": 375,
            "page": 32,
            "text": "Table 7: Accuracies on BBH tasks: our found instructions with the PaLM 2-L-IT optimizer VS baseline. The optimization starts from the empty string. Because of the 20-80 train-test split, we show accuracies with the format \"training / test / overall (training + test)\". The PaLM 2-L scores are from A begin instructions; the text-bison scores are from Q_ begin instructions. Bold numbers indicate the best for the corresponding task."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 643
                },
                {
                    "x": 2121,
                    "y": 643
                },
                {
                    "x": 2121,
                    "y": 2170
                },
                {
                    "x": 441,
                    "y": 2170
                }
            ],
            "category": "table",
            "html": "<table id='376' style='font-size:14px'><tr><td>Task</td><td>Scorer</td><td>Our Acc</td><td>\"Let's think step by step.\" Acc</td><td>\"Let's work this out in a step by step way to be sure we have the right answer.\" Acc</td><td>empty string \"\" Acc</td></tr><tr><td></td><td></td><td>training / test / overall</td><td>training / test / overall</td><td>training / test / overall</td><td>training / test / overall</td></tr><tr><td>boolean_expressions</td><td>PaLM 2-L</td><td>90.0 / 83.5 / 84.8</td><td>90.0/ 83.0/ 84.4</td><td>82.0/74.0/75.6</td><td>74.0/71.0/71.6</td></tr><tr><td>causal_judgement</td><td>PaLM 2-L</td><td>84.8 / 58.0 / 63.1</td><td>73.0 / 55.3 /58.8</td><td>59.5/57.3/57.8</td><td>29.7 /49.3 /45.5</td></tr><tr><td>date_understanding</td><td>PaLM 2-L</td><td>86.0 / 84.5 / 84.8</td><td>76.0 / 80.0 /79.2</td><td>74.0/ 77.0/76.4</td><td>70.0 / 74.0 /73.2</td></tr><tr><td>disambiguation_qa</td><td>PaLM 2-L</td><td>80.0 / 69.0 /71.2</td><td>40.0 / 52.5/50.0</td><td>48.0 /47.0/47.2</td><td>54.0/ 57.5 /56.8</td></tr><tr><td>dyck_languages</td><td>PaLM 2-L</td><td>100.0 / 100.0 / 100.0</td><td>96.0 /94.5 /94.8</td><td>100.0 /93.5 /94.8</td><td>94.0/95.0/ 94.8</td></tr><tr><td>formal_ fallacies</td><td>PaLM 2-L</td><td>84.0 / 64.0 / 68.4</td><td>78.0 / 59.5 /63.2</td><td>68.0/ 63.0 / 64.0</td><td>66.0 / 59.0/ 60.4</td></tr><tr><td>geometric_shapes</td><td>PaLM 2-L</td><td>76.0 / 57.0 / 60.8</td><td>42.0 / 33.0/34.8</td><td>42.0 / 32.0/34.0</td><td>34.0 / 33.0/33.2</td></tr><tr><td>hyperbaton</td><td>PaLM 2-L</td><td>100.0 /96.0 / 96.8</td><td>78.0 /75.0/75.6</td><td>74.0/ 72.5 /72.8</td><td>88.0/ 89.0/ 88.8</td></tr><tr><td>logical_deduction_seven_ objects</td><td>PaLM 2-L</td><td>74.0 / 57.0 / 60.4</td><td>46.0/ 37.0/38.8</td><td>34.0 / 30.5 /31.2</td><td>46.0/ 45.5 / 45.6</td></tr><tr><td>movie_recommendation</td><td>PaLM 2-L</td><td>92.0 / 90.5 / 90.8</td><td>62.0 / 52.5 /54.4</td><td>52.0/ 48.0/ 48.8</td><td>80.0 / 83.0/ 82.4</td></tr><tr><td>multistep_arithmetic_two</td><td>PaLM 2-L</td><td>72.0 / 55.5 / 58.8</td><td>42.0 / 46.0 / 45.2</td><td>60.0/50.5/52.4</td><td>4.0/3.5/3.6</td></tr><tr><td>navigate</td><td>PaLM 2-L</td><td>92.0 /75.0 / 78.4</td><td>68.0 / 62.0/ 63.2</td><td>70.0/ 64.0/65.2</td><td>38.0/37.5/37.6</td></tr><tr><td>object_counting</td><td>PaLM 2-L</td><td>84.0 / 86.5 / 86.0</td><td>36.0 / 46.5 / 44.4</td><td>60.0/ 62.0 /61.6</td><td>28.0/27.0/27.2</td></tr><tr><td>penguins_in_a_table</td><td>PaLM 2-L</td><td>86.2 /71.8/ 74.7</td><td>79.3 / 64.1 /67.1</td><td>62.1 / 58.1 /58.9</td><td>72.4 / 69.2 / 69.9</td></tr><tr><td>reasoning_about_colored_objects</td><td>PaLM 2-L</td><td>98.0 / 85.5 / 88.0</td><td>82.0 /79.5/80.0</td><td>82.0 / 75.0/76.4</td><td>42.0 /35.0/ 36.4</td></tr><tr><td>ruin_names</td><td>PaLM 2-L</td><td>88.0 / 88.0 / 88.0</td><td>70.0/ 55.0/ 58.0</td><td>80.0/75.5 /76.4</td><td>88.0/ 76.5 /78.8</td></tr><tr><td>salient_ translation_error_ detection</td><td>PaLM 2-L</td><td>62.0 / 67.0 / 66.0</td><td>42.0 / 50.0 /48.4</td><td>58.0/ 46.0/48.4</td><td>56.0 / 56.5 /56.4</td></tr><tr><td>snarks</td><td>PaLM 2-L</td><td>85.7 / 83.2/ 83.7</td><td>60.0 / 62.2/61.8</td><td>54.3 / 53.1 /53.4 / 82.5 /</td><td>51.4 / 60.1 / 58.4</td></tr><tr><td>sports_ understanding</td><td>PaLM 2-L</td><td>98.0 / 88.0 / 90.0</td><td>50.0 / 46.5 /47.2</td><td>60.0/ 52.5 /54.0</td><td>52.0/41.5 / 43.6</td></tr><tr><td>temporal_sequences</td><td>PaLM 2-L</td><td>100.0 / 100.0 / 100.0</td><td>100.0 / 96.0 / 96.8</td><td>90.0/87.0/87.6</td><td>100.0 / 99.5 /99.6</td></tr><tr><td>tracking_shuffled_objects_seven_objects</td><td>PaLM 2-L</td><td>32.0 / 16.5/ 19.6</td><td>58.0 / 61.5 /60.8</td><td>54.0 / 55.5 /55.2</td><td>14.0 /23.5 /21.6</td></tr><tr><td>web_ of_lies</td><td>PaLM 2-L</td><td>62.0 / 52.0 / 54.0 94.0 84.8</td><td>46.0 / 41.5 / 42.4</td><td>24.0 /31.0/29.6</td><td>54.0 / 54.0 / 54.0</td></tr><tr><td>- word_sorting</td><td>PaLM 2-L -</td><td>54.0 / 54.5/ 54.4</td><td>2.0/4.5 / 4.0</td><td>12.0/9.5 / 10.0</td><td>20.0/ 22.5 /22.0</td></tr><tr><td>boolean_expressions</td><td>text-bison</td><td>98.0 /87.0 / 89.2</td><td>- - 72.0/ 61.5 / 63.6</td><td>88.0/ 78.0/ 80.0</td><td>- 80.0/ 68.5 /70.8</td></tr><tr><td>causal_judgement</td><td>text-bison</td><td>78.4 / 58.0 / 62.0</td><td>70.3 / 50.7 /54.5</td><td>73.0 / 55.3 /58.8</td><td>78.4 / 58.0 / 62.0</td></tr><tr><td>date_understanding</td><td>text-bison</td><td>60.0 / 50.0 / 52.0</td><td>44.0 / 45.5 /45.2</td><td>48.0/ 45.0/ 45.6</td><td>44.0 / 45.0 / 44.8</td></tr><tr><td>disambiguation_qa</td><td>text-bison</td><td>68.0 / 73.0 / 72.0</td><td>4.0 / 6.0/ 5.6</td><td>4.0 / 15.5 / 13.2</td><td>52.0/ 68.5 / 65.2</td></tr><tr><td>dyck_languages</td><td>text-bison</td><td>100.0 / 100.0 / 100.0</td><td>100.0 / 95.5 /96.4</td><td>100.0 / 94.5 /95.6</td><td>100.0 / 98.5 / 98.8</td></tr><tr><td>formal_ fallacies</td><td>text-bison</td><td>70.0 / 53.0/ 56.4</td><td>64.0 / 54.5 /56.4</td><td>84.0 / 82.5 /82.8</td><td>70.0 / 54.5 /57.6</td></tr><tr><td>geometric_shapes</td><td>text-bison</td><td>40.0 /19.5 / 23.6</td><td>22.0/ 13.0/ 14.8</td><td>18.0/ 12.0/ 13.2</td><td>20.0/ 14.5 /15.6</td></tr><tr><td>hyperbaton</td><td>text-bison</td><td>80.0 / 79.5 / 79.6</td><td>64.0 / 67.5 /66.8</td><td>64.0 / 69.0/68.0</td><td>64.0 / 64.0/ 64.0</td></tr><tr><td>logical_deduction_seven_objects</td><td>text-bison</td><td>66.0 / 53.5 / 56.0</td><td>56.0 / 58.0 / 57.6</td><td>56.0/ 56.0/ 56.0</td><td>58.0/ 56.5 / 56.8</td></tr><tr><td>movie_recommendation</td><td>text-bison</td><td>98.0 / 90.0 / 91.6</td><td>68.0/63.0/ 64.0</td><td>66.0/ 62.0/62.8</td><td>68.0/ 64.0/ 64.8</td></tr><tr><td>multistep_ arithmetic_two</td><td>text-bison</td><td>32.0 /16.5 / 19.6</td><td>12.0/ 18.0/ 16.8</td><td>18.0/ 17.5 /17.6</td><td>16.0/ 18.5 / 18.0</td></tr><tr><td>navigate</td><td>text-bison</td><td>72.0 / 61.0 / 63.2</td><td>56.0 / 55.0 /55.2</td><td>60.0/ 56.5/57.2</td><td>56.0 /57.0/ 56.8</td></tr><tr><td>object_counting</td><td>text-bison</td><td>72.0 / 62.0 / 64.0</td><td>58.0/57.0/57.2</td><td>62.0/55.5/56.8</td><td>50.0/57.0/ 55.6</td></tr><tr><td>penguins_in_a_table</td><td>text-bison</td><td>72.4 / 56.4/ 59.6</td><td>58.6/ 53.0/54.1</td><td>55.2/55.6/55.5</td><td>58.6/53.0/ 54.1</td></tr><tr><td>reasoning_ about_colored_objects</td><td>text-bison</td><td>82.0 / 77.0 / 78.0</td><td>76.0/ 72.5/73.2</td><td>78.0/ 73.0/74.0</td><td>74.0/ 69.5 / 70.4</td></tr><tr><td></td><td>text-bison</td><td>88.0 / 82.5 / 83.6</td><td>66.0 / 65.5 /65.6</td><td>66.0/ 62.5 /63.2</td><td>64.0 / 66.0 / 65.6</td></tr><tr><td></td><td>text-bison</td><td></td><td>42.0 / 47.5 / 46.4</td><td>42.0/ 49.5 /48.0</td><td>44.0 / 50.0 / 48.8</td></tr><tr><td></td><td></td><td>/ /</td><td></td><td></td><td>77.1 / 84.6 /73.1</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ruin_names salient_translation error_ detection snarks</td><td>text-bison text-bison</td><td>46.0 / 50.5 / 49.6 80.0 81.8 81.5</td><td>68.6/77.6/75.8 86.0 / 79.0/80.4</td><td>71.4 / 76.2 /75.3 90.0/81.0/ 82.8</td><td>38.0 / 44.5 / 43.2</td></tr><tr><td>sports_understanding temporal_sequences</td><td>text-bison text-bison</td><td>78.0 / 81.0 / 80.4 32.0 /15.5 / 18.8</td><td>36.0 / 43.5 /42.0 10.0 /17.0/15.6</td><td>32.0/45.0 / 42.4 10.0 / 18.0/16.4</td><td>36.0 / 43.0 / 41.6 12.0/ 15.5 / 14.8</td></tr><tr><td>macking_shuffled_objects_seven_objects web_ of_lies</td><td>text-bison</td><td>62.0 / 50.0/ 52.4</td><td>48.0 /45.5 / 46.0</td><td>48.0 / 44.0 / 44.8</td><td>52.0/51.5 /51.2</td></tr><tr><td>word_sorting</td><td>text-bison</td><td>24.0 /17.5 / 18.8</td><td>10.0 /12.0/11.6</td><td>4.0/ 8.0/7.2</td><td>4.0/7.5/6.8</td></tr></table>",
            "id": 376,
            "page": 32,
            "text": "Task Scorer Our Acc \"Let's think step by step.\" Acc \"Let's work this out in a step by step way to be sure we have the right answer.\" Acc empty string \"\" Acc    training / test / overall training / test / overall training / test / overall training / test / overall  boolean_expressions PaLM 2-L 90.0 / 83.5 / 84.8 90.0/ 83.0/ 84.4 82.0/74.0/75.6 74.0/71.0/71.6  causal_judgement PaLM 2-L 84.8 / 58.0 / 63.1 73.0 / 55.3 /58.8 59.5/57.3/57.8 29.7 /49.3 /45.5  date_understanding PaLM 2-L 86.0 / 84.5 / 84.8 76.0 / 80.0 /79.2 74.0/ 77.0/76.4 70.0 / 74.0 /73.2  disambiguation_qa PaLM 2-L 80.0 / 69.0 /71.2 40.0 / 52.5/50.0 48.0 /47.0/47.2 54.0/ 57.5 /56.8  dyck_languages PaLM 2-L 100.0 / 100.0 / 100.0 96.0 /94.5 /94.8 100.0 /93.5 /94.8 94.0/95.0/ 94.8  formal_ fallacies PaLM 2-L 84.0 / 64.0 / 68.4 78.0 / 59.5 /63.2 68.0/ 63.0 / 64.0 66.0 / 59.0/ 60.4  geometric_shapes PaLM 2-L 76.0 / 57.0 / 60.8 42.0 / 33.0/34.8 42.0 / 32.0/34.0 34.0 / 33.0/33.2  hyperbaton PaLM 2-L 100.0 /96.0 / 96.8 78.0 /75.0/75.6 74.0/ 72.5 /72.8 88.0/ 89.0/ 88.8  logical_deduction_seven_ objects PaLM 2-L 74.0 / 57.0 / 60.4 46.0/ 37.0/38.8 34.0 / 30.5 /31.2 46.0/ 45.5 / 45.6  movie_recommendation PaLM 2-L 92.0 / 90.5 / 90.8 62.0 / 52.5 /54.4 52.0/ 48.0/ 48.8 80.0 / 83.0/ 82.4  multistep_arithmetic_two PaLM 2-L 72.0 / 55.5 / 58.8 42.0 / 46.0 / 45.2 60.0/50.5/52.4 4.0/3.5/3.6  navigate PaLM 2-L 92.0 /75.0 / 78.4 68.0 / 62.0/ 63.2 70.0/ 64.0/65.2 38.0/37.5/37.6  object_counting PaLM 2-L 84.0 / 86.5 / 86.0 36.0 / 46.5 / 44.4 60.0/ 62.0 /61.6 28.0/27.0/27.2  penguins_in_a_table PaLM 2-L 86.2 /71.8/ 74.7 79.3 / 64.1 /67.1 62.1 / 58.1 /58.9 72.4 / 69.2 / 69.9  reasoning_about_colored_objects PaLM 2-L 98.0 / 85.5 / 88.0 82.0 /79.5/80.0 82.0 / 75.0/76.4 42.0 /35.0/ 36.4  ruin_names PaLM 2-L 88.0 / 88.0 / 88.0 70.0/ 55.0/ 58.0 80.0/75.5 /76.4 88.0/ 76.5 /78.8  salient_ translation_error_ detection PaLM 2-L 62.0 / 67.0 / 66.0 42.0 / 50.0 /48.4 58.0/ 46.0/48.4 56.0 / 56.5 /56.4  snarks PaLM 2-L 85.7 / 83.2/ 83.7 60.0 / 62.2/61.8 54.3 / 53.1 /53.4 / 82.5 / 51.4 / 60.1 / 58.4  sports_ understanding PaLM 2-L 98.0 / 88.0 / 90.0 50.0 / 46.5 /47.2 60.0/ 52.5 /54.0 52.0/41.5 / 43.6  temporal_sequences PaLM 2-L 100.0 / 100.0 / 100.0 100.0 / 96.0 / 96.8 90.0/87.0/87.6 100.0 / 99.5 /99.6  tracking_shuffled_objects_seven_objects PaLM 2-L 32.0 / 16.5/ 19.6 58.0 / 61.5 /60.8 54.0 / 55.5 /55.2 14.0 /23.5 /21.6  web_ of_lies PaLM 2-L 62.0 / 52.0 / 54.0 94.0 84.8 46.0 / 41.5 / 42.4 24.0 /31.0/29.6 54.0 / 54.0 / 54.0  - word_sorting PaLM 2-L - 54.0 / 54.5/ 54.4 2.0/4.5 / 4.0 12.0/9.5 / 10.0 20.0/ 22.5 /22.0  boolean_expressions text-bison 98.0 /87.0 / 89.2 - - 72.0/ 61.5 / 63.6 88.0/ 78.0/ 80.0 - 80.0/ 68.5 /70.8  causal_judgement text-bison 78.4 / 58.0 / 62.0 70.3 / 50.7 /54.5 73.0 / 55.3 /58.8 78.4 / 58.0 / 62.0  date_understanding text-bison 60.0 / 50.0 / 52.0 44.0 / 45.5 /45.2 48.0/ 45.0/ 45.6 44.0 / 45.0 / 44.8  disambiguation_qa text-bison 68.0 / 73.0 / 72.0 4.0 / 6.0/ 5.6 4.0 / 15.5 / 13.2 52.0/ 68.5 / 65.2  dyck_languages text-bison 100.0 / 100.0 / 100.0 100.0 / 95.5 /96.4 100.0 / 94.5 /95.6 100.0 / 98.5 / 98.8  formal_ fallacies text-bison 70.0 / 53.0/ 56.4 64.0 / 54.5 /56.4 84.0 / 82.5 /82.8 70.0 / 54.5 /57.6  geometric_shapes text-bison 40.0 /19.5 / 23.6 22.0/ 13.0/ 14.8 18.0/ 12.0/ 13.2 20.0/ 14.5 /15.6  hyperbaton text-bison 80.0 / 79.5 / 79.6 64.0 / 67.5 /66.8 64.0 / 69.0/68.0 64.0 / 64.0/ 64.0  logical_deduction_seven_objects text-bison 66.0 / 53.5 / 56.0 56.0 / 58.0 / 57.6 56.0/ 56.0/ 56.0 58.0/ 56.5 / 56.8  movie_recommendation text-bison 98.0 / 90.0 / 91.6 68.0/63.0/ 64.0 66.0/ 62.0/62.8 68.0/ 64.0/ 64.8  multistep_ arithmetic_two text-bison 32.0 /16.5 / 19.6 12.0/ 18.0/ 16.8 18.0/ 17.5 /17.6 16.0/ 18.5 / 18.0  navigate text-bison 72.0 / 61.0 / 63.2 56.0 / 55.0 /55.2 60.0/ 56.5/57.2 56.0 /57.0/ 56.8  object_counting text-bison 72.0 / 62.0 / 64.0 58.0/57.0/57.2 62.0/55.5/56.8 50.0/57.0/ 55.6  penguins_in_a_table text-bison 72.4 / 56.4/ 59.6 58.6/ 53.0/54.1 55.2/55.6/55.5 58.6/53.0/ 54.1  reasoning_ about_colored_objects text-bison 82.0 / 77.0 / 78.0 76.0/ 72.5/73.2 78.0/ 73.0/74.0 74.0/ 69.5 / 70.4   text-bison 88.0 / 82.5 / 83.6 66.0 / 65.5 /65.6 66.0/ 62.5 /63.2 64.0 / 66.0 / 65.6   text-bison  42.0 / 47.5 / 46.4 42.0/ 49.5 /48.0 44.0 / 50.0 / 48.8    / /   77.1 / 84.6 /73.1         ruin_names salient_translation error_ detection snarks text-bison text-bison 46.0 / 50.5 / 49.6 80.0 81.8 81.5 68.6/77.6/75.8 86.0 / 79.0/80.4 71.4 / 76.2 /75.3 90.0/81.0/ 82.8 38.0 / 44.5 / 43.2  sports_understanding temporal_sequences text-bison text-bison 78.0 / 81.0 / 80.4 32.0 /15.5 / 18.8 36.0 / 43.5 /42.0 10.0 /17.0/15.6 32.0/45.0 / 42.4 10.0 / 18.0/16.4 36.0 / 43.0 / 41.6 12.0/ 15.5 / 14.8  macking_shuffled_objects_seven_objects web_ of_lies text-bison 62.0 / 50.0/ 52.4 48.0 /45.5 / 46.0 48.0 / 44.0 / 44.8 52.0/51.5 /51.2  word_sorting text-bison 24.0 /17.5 / 18.8 10.0 /12.0/11.6 4.0/ 8.0/7.2"
        },
        {
            "bounding_box": [
                {
                    "x": 1250,
                    "y": 3132
                },
                {
                    "x": 1300,
                    "y": 3132
                },
                {
                    "x": 1300,
                    "y": 3171
                },
                {
                    "x": 1250,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='377' style='font-size:18px'>32</footer>",
            "id": 377,
            "page": 32,
            "text": "32"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 113
                },
                {
                    "x": 1098,
                    "y": 113
                },
                {
                    "x": 1098,
                    "y": 159
                },
                {
                    "x": 445,
                    "y": 159
                }
            ],
            "category": "header",
            "html": "<header id='378' style='font-size:22px'>Large Language Models as Optimizers</header>",
            "id": 378,
            "page": 33,
            "text": "Large Language Models as Optimizers"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 372
                },
                {
                    "x": 2111,
                    "y": 372
                },
                {
                    "x": 2111,
                    "y": 469
                },
                {
                    "x": 440,
                    "y": 469
                }
            ],
            "category": "caption",
            "html": "<caption id='379' style='font-size:18px'>Table 8: BBH task-wise instructions found by prompt optimization with the PaLM 2-L scorer and<br>the PaLM 2-L-IT optimizer. The optimization starts from the empty string.</caption>",
            "id": 379,
            "page": 33,
            "text": "Table 8: BBH task-wise instructions found by prompt optimization with the PaLM 2-L scorer and the PaLM 2-L-IT optimizer. The optimization starts from the empty string."
        },
        {
            "bounding_box": [
                {
                    "x": 1383,
                    "y": 481
                },
                {
                    "x": 1574,
                    "y": 481
                },
                {
                    "x": 1574,
                    "y": 515
                },
                {
                    "x": 1383,
                    "y": 515
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='380' style='font-size:16px'>Our Instruction</p>",
            "id": 380,
            "page": 33,
            "text": "Our Instruction"
        },
        {
            "bounding_box": [
                {
                    "x": 486,
                    "y": 492
                },
                {
                    "x": 2176,
                    "y": 492
                },
                {
                    "x": 2176,
                    "y": 3074
                },
                {
                    "x": 486,
                    "y": 3074
                }
            ],
            "category": "table",
            "html": "<br><table id='381' style='font-size:14px'><tr><td>Task</td><td></td></tr><tr><td>boolean_expressions</td><td>A Boolean expression is a well-formed expression consisting of variables, values, and logical operators. The expression must evaluate to a single True or False value. The order of precedence of the logical operators is as follows: NOT, AND, OR, XOR, IMP. Parentheses can be used to group subexpressions and to control the order of evaluation.</td></tr><tr><td>causal_judgement</td><td>When considering questions about causation, a typical person would consider the following factors: whether the action or event was a necessary condition for the outcome to occur, a sufficient condition, a proximate cause, or a foreseeable cause.</td></tr><tr><td>date_ understanding</td><td>To find the date X time ago from today, first find today's date. Then subtract X time from today's date. If the current date is the last day of a month, then the date a month ago is the last day of the previous month. If the current date is not the last day of a month, then the date a month ago is the same day of the previous month. For example, if today is March 31, 2023, then the date a month ago is February 28, 2023. If today is April 1, 2023, then the date a month ago is March 1, 2023.</td></tr><tr><td>disambiguation_qa</td><td>Identifying Antecedents of Pronouns: A Comprehensive Guide</td></tr><tr><td>dyck_languages</td><td>First, look for the opening parentheses. Then, count the number of opening parentheses. Finally, close the parentheses in the reverse order that they were opened.</td></tr><tr><td>formal_fallacies</td><td>A deductive argument is one where the conclusion follows necessarily from the premises. If the premises are true, then the conclusion must also be true. An invalid argument is one where it is possible for the premises to be true and the conclusion to be false.</td></tr><tr><td>geometric _shapes</td><td>A closed polygonal chain is a series of connected line segments. The line segments can be straight or curved. The first and last line segments are connected. The line segments do not intersect each other except at their endpoints. A closed polygon can be described by an SVG path element, which starts at a given point, goes to one or more additional points, and then ends at the starting point. The path element can consist of straight line segments, curved segments, or a mixture of both.</td></tr><tr><td>hyperbaton</td><td>The correct adjective order in English is opinion, size, shape, age, color, origin, material, and purpose. If you have more than one adjective of the same type, they are usually placed in order of importance. For example, you would say \"a large, old, Pakistani ship\" rather than \"an old, large, Pakistani ship. \" There are a few exceptions to these rules, but they are generally followed in most cases.</td></tr><tr><td>logical_ deduction _seven_objects</td><td>The following questions will test your ability to use deductive reasoning. You will be given a set of statements about a group of objects. You will then be asked to answer questions about the objects based on the statements. The statements in the questions are logically consistent, so you can use them to deduce the order of the objects. For each question, you must choose the option that is logically consistent with the information in the questions.</td></tr><tr><td>movie_recommendation</td><td>Based on your input, I have analyzed the given movies in terms of genre, plot, tone, audience rating, year of release, director, cast, and reviews. I have also taken into account the given options. The movie that is most similar to the given movies in terms of all these factors is:</td></tr><tr><td>multistep_arithmetic two</td><td>The order of operations in mathematics is PEMDAS, which stands for Parentheses, Exponents, Multiplication, Division, Addition, and Subtraction. When there are multiple operations of the same precedence, they must be performed from left to right. Note that multiplication and division have the same precedence, as do addition and subtraction.</td></tr><tr><td>navigate</td><td>You will return to the starting point if and only if (1) the total number of steps you take forward is equal to the total number of steps you take back, and (2) the total number of turns you make is a multiple of 180 degrees.</td></tr><tr><td>object_counting</td><td>Here is a list of the objects you mentioned and their corresponding counts:</td></tr><tr><td>penguins_in_a_table</td><td>Here is my new text:</td></tr><tr><td>reasoning_about _colored_objects</td><td>Starting from the leftmost object in the row, I observe the following objects arranged in this order:</td></tr><tr><td>ruin names</td><td>Which is the funniest pun on the artist or movie name?</td></tr><tr><td>salient_ translation _error_detection</td><td>Instructions: Read the German sentence and its English translation carefully, then identify the type of error in the translation and select the correct option. There are six possible types of errors: Named Entities, Numerical Values, Modifiers or Adjectives, Negation or Antonyms, Facts, and Dropped Content.</td></tr><tr><td>snarks</td><td>Identify the sarcastic statement by considering the following factors: incongruity, exaggeration, understatement, context, speaker's intent, and audience's reaction. I will also consider the speaker's tone of voice, facial expressions, and body language.</td></tr><tr><td>sports_understanding</td><td>I will determine if a sentence about an athlete is plausible by first checking ifit is grammatically correct. Ifitis, I will then check ifit is consistent with the athlete's sport, position, and real-world statistics. I will also check ifit is consistent with the rules of the athlete's sport. If the sentence is consistent with all of these things,</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></table>",
            "id": 381,
            "page": 33,
            "text": "Task   boolean_expressions A Boolean expression is a well-formed expression consisting of variables, values, and logical operators. The expression must evaluate to a single True or False value. The order of precedence of the logical operators is as follows: NOT, AND, OR, XOR, IMP. Parentheses can be used to group subexpressions and to control the order of evaluation.  causal_judgement When considering questions about causation, a typical person would consider the following factors: whether the action or event was a necessary condition for the outcome to occur, a sufficient condition, a proximate cause, or a foreseeable cause.  date_ understanding To find the date X time ago from today, first find today's date. Then subtract X time from today's date. If the current date is the last day of a month, then the date a month ago is the last day of the previous month. If the current date is not the last day of a month, then the date a month ago is the same day of the previous month. For example, if today is March 31, 2023, then the date a month ago is February 28, 2023. If today is April 1, 2023, then the date a month ago is March 1, 2023.  disambiguation_qa Identifying Antecedents of Pronouns: A Comprehensive Guide  dyck_languages First, look for the opening parentheses. Then, count the number of opening parentheses. Finally, close the parentheses in the reverse order that they were opened.  formal_fallacies A deductive argument is one where the conclusion follows necessarily from the premises. If the premises are true, then the conclusion must also be true. An invalid argument is one where it is possible for the premises to be true and the conclusion to be false.  geometric _shapes A closed polygonal chain is a series of connected line segments. The line segments can be straight or curved. The first and last line segments are connected. The line segments do not intersect each other except at their endpoints. A closed polygon can be described by an SVG path element, which starts at a given point, goes to one or more additional points, and then ends at the starting point. The path element can consist of straight line segments, curved segments, or a mixture of both.  hyperbaton The correct adjective order in English is opinion, size, shape, age, color, origin, material, and purpose. If you have more than one adjective of the same type, they are usually placed in order of importance. For example, you would say \"a large, old, Pakistani ship\" rather than \"an old, large, Pakistani ship. \" There are a few exceptions to these rules, but they are generally followed in most cases.  logical_ deduction _seven_objects The following questions will test your ability to use deductive reasoning. You will be given a set of statements about a group of objects. You will then be asked to answer questions about the objects based on the statements. The statements in the questions are logically consistent, so you can use them to deduce the order of the objects. For each question, you must choose the option that is logically consistent with the information in the questions.  movie_recommendation Based on your input, I have analyzed the given movies in terms of genre, plot, tone, audience rating, year of release, director, cast, and reviews. I have also taken into account the given options. The movie that is most similar to the given movies in terms of all these factors is:  multistep_arithmetic two The order of operations in mathematics is PEMDAS, which stands for Parentheses, Exponents, Multiplication, Division, Addition, and Subtraction. When there are multiple operations of the same precedence, they must be performed from left to right. Note that multiplication and division have the same precedence, as do addition and subtraction.  navigate You will return to the starting point if and only if (1) the total number of steps you take forward is equal to the total number of steps you take back, and (2) the total number of turns you make is a multiple of 180 degrees.  object_counting Here is a list of the objects you mentioned and their corresponding counts:  penguins_in_a_table Here is my new text:  reasoning_about _colored_objects Starting from the leftmost object in the row, I observe the following objects arranged in this order:  ruin names Which is the funniest pun on the artist or movie name?  salient_ translation _error_detection Instructions: Read the German sentence and its English translation carefully, then identify the type of error in the translation and select the correct option. There are six possible types of errors: Named Entities, Numerical Values, Modifiers or Adjectives, Negation or Antonyms, Facts, and Dropped Content.  snarks Identify the sarcastic statement by considering the following factors: incongruity, exaggeration, understatement, context, speaker's intent, and audience's reaction. I will also consider the speaker's tone of voice, facial expressions, and body language.  sports_understanding I will determine if a sentence about an athlete is plausible by first checking ifit is grammatically correct. Ifitis, I will then check ifit is consistent with the athlete's sport, position, and real-world statistics. I will also check ifit is consistent with the rules of the athlete's sport. If the sentence is consistent with all of these things,"
        },
        {
            "bounding_box": [
                {
                    "x": 1251,
                    "y": 3133
                },
                {
                    "x": 1299,
                    "y": 3133
                },
                {
                    "x": 1299,
                    "y": 3170
                },
                {
                    "x": 1251,
                    "y": 3170
                }
            ],
            "category": "footer",
            "html": "<footer id='382' style='font-size:20px'>33</footer>",
            "id": 382,
            "page": 33,
            "text": "33"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 113
                },
                {
                    "x": 1099,
                    "y": 113
                },
                {
                    "x": 1099,
                    "y": 159
                },
                {
                    "x": 445,
                    "y": 159
                }
            ],
            "category": "header",
            "html": "<header id='383' style='font-size:20px'>Large Language Models as Optimizers</header>",
            "id": 383,
            "page": 34,
            "text": "Large Language Models as Optimizers"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 370
                },
                {
                    "x": 2109,
                    "y": 370
                },
                {
                    "x": 2109,
                    "y": 471
                },
                {
                    "x": 440,
                    "y": 471
                }
            ],
            "category": "caption",
            "html": "<caption id='384' style='font-size:16px'>Table 9: BBH task-wise instructions found by prompt optimization with the text -bi son scorer<br>and the PaLM 2-L-IT optimizer. The optimization starts from the empty string.</caption>",
            "id": 384,
            "page": 34,
            "text": "Table 9: BBH task-wise instructions found by prompt optimization with the text -bi son scorer and the PaLM 2-L-IT optimizer. The optimization starts from the empty string."
        },
        {
            "bounding_box": [
                {
                    "x": 474,
                    "y": 541
                },
                {
                    "x": 2193,
                    "y": 541
                },
                {
                    "x": 2193,
                    "y": 3034
                },
                {
                    "x": 474,
                    "y": 3034
                }
            ],
            "category": "table",
            "html": "<table id='385' style='font-size:14px'><tr><td>Task</td><td>Our Instruction</td></tr><tr><td>boolean_expressions</td><td>Not (not False) and not not False is False</td></tr><tr><td>causal_judgement</td><td>A typical person would likely answer the questions about causation as follows:</td></tr><tr><td>date_ understanding</td><td>Today is February 28, 2023. Itis a Tuesday. Yesterday was Monday, February 27, 2023. Tomorrow will be Wednesday, March 1, 2023. A week ago, it was February 21, 2023, and a month ago, it was January 28, 2023. A year from now, it will be February 28, 2024. The day of the week is important to note because it will help us to correctly answer the questions below. Not all years are leap years that contain February 29.</td></tr><tr><td>disambiguation_qa</td><td>A pronoun is a word that stands in for a noun. The noun that a pronoun refers to is called its antecedent. To identify the antecedent of a pronoun, look for the noun that the pronoun could be referring to. If there is only one possible noun, then that is the antecedent. If there are two or more possible nouns, then the antecedent is ambiguous. Use the context of the sentence to help you determine the correct antecedent.</td></tr><tr><td>dyck_languages</td><td>{ }</td></tr><tr><td>formal fallacies</td><td>How to Evaluate Deductive Validity of an Argument</td></tr><tr><td>geometric_shapes</td><td>What shape is this SVG code drawing, and how many sides does it have?</td></tr><tr><td>hyperbaton</td><td>In English, adjectives are typically placed before nouns in a specific order. The order is: opinion, size, shape, age, color, origin, material, purpose, noun. For example, the sentence \"the big, old, red barn\" would be considered grammatically correct, while the sentence \"the old, big, red barn\" would not. Adjectives that come before nouns are called attributive adjectives, while adjectives that come after nouns are called predicative adjectives.</td></tr><tr><td>logical_ deduction _seven_objects</td><td>In this logical reasoning task, you will be given a series of paragraphs, each of which describes a set of objects arranged in a fixed order. The statements in each paragraph are logically consistent. You must read each paragraph carefully and use the information given to determine the logical relationships between the objects. You will then be asked a question about the order of the objects. Read each question carefully and choose the option that answers the question correctly.</td></tr><tr><td>movie_recommendation</td><td>What is the highest-rated movie similar to the given movies, with a similar IMDb rating and released in the same year?</td></tr><tr><td>multistep_arithmetic_two</td><td>Let's solve these equations using PEMDAS order of operations. Remember that PEMDAS stands for parentheses, exponents, multiplication and division, and addition and subtraction.</td></tr><tr><td>navigate</td><td>Starting at the origin, facing north, follow the instructions. If your displacement from the origin is zero and your direction is unchanged, then your answer is Yes. Otherwise, your answer is No.</td></tr><tr><td>object_counting</td><td>Let me help you count the items you have. Just list them one by one, separated by commas. I will then count each item and tell you how many items there are in total.</td></tr><tr><td>penguins_in_a_ table</td><td>This table shows information about penguins. The columns show the penguin's name, age, height (in cm), and weight (in kg). The penguins are listed in order of their age, from youngest to oldest.</td></tr><tr><td>reasoning_ about _colored_objects</td><td>First, read the input carefully. Then, identify all the objects mentioned, their colors, and their positions. Next, visualize the objects and their positions in your mind. Finally, answer the questions accurately based on the information given. Make sure to pay attention to the order of the objects.</td></tr><tr><td>ruin names</td><td>A humorous edit of an artist or movie name can be created by replacing one or more letters to form a new word or phrase that sounds similar but has a different meaning. The new word or phrase should be relevant to the original word, but it should also be a surprise, which makes the edit funny. For example, the artist or movie name \"Rocky\" can be changed to \"Ricky, and \"Schindler's List\" can be changed to \"Schindler's Lift. \" Be creative and have fun!</td></tr><tr><td>salient_ translation error_ detection</td><td>The following translations from German to English contain a particular error. The error may be one of the following types: Named Entities, Numerical Values, Modifiers or Adjectives, Negation or Antonyms, Facts, or Dropped Content. Please identify the error.</td></tr><tr><td>snarks</td><td>The statement</td></tr><tr><td>sports_understanding</td><td>To determine the plausibility of a sports sentence, I will first identify the sport, athletes, teams, and events mentioned in the sentence. Then, I will use my knowledge of the rules of the sport, the context of the sentence, common sense, and my knowledge of the world to determine whether the sentence is plausible. I will also consider the time period and location, as well as any other relevant information. Finally, I will return a score of 1 for plausible sentences and 0 for implausible ones.</td></tr><tr><td>temporal_sequences</td><td>To determine the time period when a person went to a place, first identify all the time periods when the person's whereabouts are unknown. Then, rule out any time periods during which the person was seen doing something else or the place was closed. The remaining time periods are the possible times when the person could have gone to the place.</td></tr><tr><td>tracking_shuffled_objects</td><td>At the start of the game, Claire has a blue ball. Throughout the game, pairs of people swap balls.</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></table>",
            "id": 385,
            "page": 34,
            "text": "Task Our Instruction  boolean_expressions Not (not False) and not not False is False  causal_judgement A typical person would likely answer the questions about causation as follows:  date_ understanding Today is February 28, 2023. Itis a Tuesday. Yesterday was Monday, February 27, 2023. Tomorrow will be Wednesday, March 1, 2023. A week ago, it was February 21, 2023, and a month ago, it was January 28, 2023. A year from now, it will be February 28, 2024. The day of the week is important to note because it will help us to correctly answer the questions below. Not all years are leap years that contain February 29.  disambiguation_qa A pronoun is a word that stands in for a noun. The noun that a pronoun refers to is called its antecedent. To identify the antecedent of a pronoun, look for the noun that the pronoun could be referring to. If there is only one possible noun, then that is the antecedent. If there are two or more possible nouns, then the antecedent is ambiguous. Use the context of the sentence to help you determine the correct antecedent.  dyck_languages { }  formal fallacies How to Evaluate Deductive Validity of an Argument  geometric_shapes What shape is this SVG code drawing, and how many sides does it have?  hyperbaton In English, adjectives are typically placed before nouns in a specific order. The order is: opinion, size, shape, age, color, origin, material, purpose, noun. For example, the sentence \"the big, old, red barn\" would be considered grammatically correct, while the sentence \"the old, big, red barn\" would not. Adjectives that come before nouns are called attributive adjectives, while adjectives that come after nouns are called predicative adjectives.  logical_ deduction _seven_objects In this logical reasoning task, you will be given a series of paragraphs, each of which describes a set of objects arranged in a fixed order. The statements in each paragraph are logically consistent. You must read each paragraph carefully and use the information given to determine the logical relationships between the objects. You will then be asked a question about the order of the objects. Read each question carefully and choose the option that answers the question correctly.  movie_recommendation What is the highest-rated movie similar to the given movies, with a similar IMDb rating and released in the same year?  multistep_arithmetic_two Let's solve these equations using PEMDAS order of operations. Remember that PEMDAS stands for parentheses, exponents, multiplication and division, and addition and subtraction.  navigate Starting at the origin, facing north, follow the instructions. If your displacement from the origin is zero and your direction is unchanged, then your answer is Yes. Otherwise, your answer is No.  object_counting Let me help you count the items you have. Just list them one by one, separated by commas. I will then count each item and tell you how many items there are in total.  penguins_in_a_ table This table shows information about penguins. The columns show the penguin's name, age, height (in cm), and weight (in kg). The penguins are listed in order of their age, from youngest to oldest.  reasoning_ about _colored_objects First, read the input carefully. Then, identify all the objects mentioned, their colors, and their positions. Next, visualize the objects and their positions in your mind. Finally, answer the questions accurately based on the information given. Make sure to pay attention to the order of the objects.  ruin names A humorous edit of an artist or movie name can be created by replacing one or more letters to form a new word or phrase that sounds similar but has a different meaning. The new word or phrase should be relevant to the original word, but it should also be a surprise, which makes the edit funny. For example, the artist or movie name \"Rocky\" can be changed to \"Ricky, and \"Schindler's List\" can be changed to \"Schindler's Lift. \" Be creative and have fun!  salient_ translation error_ detection The following translations from German to English contain a particular error. The error may be one of the following types: Named Entities, Numerical Values, Modifiers or Adjectives, Negation or Antonyms, Facts, or Dropped Content. Please identify the error.  snarks The statement  sports_understanding To determine the plausibility of a sports sentence, I will first identify the sport, athletes, teams, and events mentioned in the sentence. Then, I will use my knowledge of the rules of the sport, the context of the sentence, common sense, and my knowledge of the world to determine whether the sentence is plausible. I will also consider the time period and location, as well as any other relevant information. Finally, I will return a score of 1 for plausible sentences and 0 for implausible ones.  temporal_sequences To determine the time period when a person went to a place, first identify all the time periods when the person's whereabouts are unknown. Then, rule out any time periods during which the person was seen doing something else or the place was closed. The remaining time periods are the possible times when the person could have gone to the place.  tracking_shuffled_objects At the start of the game, Claire has a blue ball. Throughout the game, pairs of people swap balls."
        },
        {
            "bounding_box": [
                {
                    "x": 1250,
                    "y": 3132
                },
                {
                    "x": 1299,
                    "y": 3132
                },
                {
                    "x": 1299,
                    "y": 3170
                },
                {
                    "x": 1250,
                    "y": 3170
                }
            ],
            "category": "footer",
            "html": "<footer id='386' style='font-size:16px'>34</footer>",
            "id": 386,
            "page": 34,
            "text": "34"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 112
                },
                {
                    "x": 1099,
                    "y": 112
                },
                {
                    "x": 1099,
                    "y": 158
                },
                {
                    "x": 443,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='387' style='font-size:22px'>Large Language Models as Optimizers</header>",
            "id": 387,
            "page": 35,
            "text": "Large Language Models as Optimizers"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 342
                },
                {
                    "x": 2086,
                    "y": 342
                },
                {
                    "x": 2086,
                    "y": 396
                },
                {
                    "x": 443,
                    "y": 396
                }
            ],
            "category": "paragraph",
            "html": "<p id='388' style='font-size:14px'>E.2 G P T - 3 · 5 - TURBO AS OPTIMIZER, OPTIMIZATION STARTING FROM THE EMPTY STRING</p>",
            "id": 388,
            "page": 35,
            "text": "E.2 G P T - 3 · 5 - TURBO AS OPTIMIZER, OPTIMIZATION STARTING FROM THE EMPTY STRING"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 430
                },
                {
                    "x": 2109,
                    "y": 430
                },
                {
                    "x": 2109,
                    "y": 619
                },
                {
                    "x": 441,
                    "y": 619
                }
            ],
            "category": "paragraph",
            "html": "<p id='389' style='font-size:18px'>Table 11, 12 and 13 show the instructions found by prompt optimization. Their accuracies are listed<br>in Table 10. Figure 25 visualizes the difference between their accuracies and those of the baselines<br>\"Let's think step by step.' , and the empty string. The optimizations find instructions better than the<br>empty starting point, and most of the found instructions are better than \"Let's think step by step\".</p>",
            "id": 389,
            "page": 35,
            "text": "Table 11, 12 and 13 show the instructions found by prompt optimization. Their accuracies are listed in Table 10. Figure 25 visualizes the difference between their accuracies and those of the baselines \"Let's think step by step.' , and the empty string. The optimizations find instructions better than the empty starting point, and most of the found instructions are better than \"Let's think step by step\"."
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 638
                },
                {
                    "x": 2110,
                    "y": 638
                },
                {
                    "x": 2110,
                    "y": 963
                },
                {
                    "x": 440,
                    "y": 963
                }
            ],
            "category": "paragraph",
            "html": "<p id='390' style='font-size:16px'>One caveat in the A_ begin instructions (Table 11) is that a lot of the found instructions are imperative<br>like \"Solve<br>or interrogative sentences that are more suitable to be put into \"Q:\" rather than \"A:\" ,<br>the sequence by properly closing the parentheses.' , for dyck_languages and \"Which movie option<br>from the given choices ... ?\" for movie _recommendation. Such styles appear more often here than the<br>PaLM 2-L-IT optimizer results (Table 8), showing PaLM 2-L-IT understands the needed style<br>better. In Section E.3, we show the A begin optimization results with the non-empty starting point<br>\"Let's solve the problem.\" , Most results there are declarative sentences - more suitable for A begin.<br>·</p>",
            "id": 390,
            "page": 35,
            "text": "One caveat in the A_ begin instructions (Table 11) is that a lot of the found instructions are imperative like \"Solve or interrogative sentences that are more suitable to be put into \"Q:\" rather than \"A:\" , the sequence by properly closing the parentheses.' , for dyck_languages and \"Which movie option from the given choices ... ?\" for movie _recommendation. Such styles appear more often here than the PaLM 2-L-IT optimizer results (Table 8), showing PaLM 2-L-IT understands the needed style better. In Section E.3, we show the A begin optimization results with the non-empty starting point \"Let's solve the problem.\" , Most results there are declarative sentences - more suitable for A begin. ·"
        },
        {
            "bounding_box": [
                {
                    "x": 536,
                    "y": 1021
                },
                {
                    "x": 2015,
                    "y": 1021
                },
                {
                    "x": 2015,
                    "y": 2877
                },
                {
                    "x": 536,
                    "y": 2877
                }
            ],
            "category": "figure",
            "html": "<figure><img id='391' style='font-size:14px' alt=\"40\ndifference\n40\n20 L JILL difference\naccuracy\naccuracy\n20\n0\n-20\n0\nobjects\nexpressions\ndetection\nrecommendation\nhyperbaton\nunderstanding\nobjects\ntable\nhyperbaton\nobjects\nobjects\nsnarks\ntable\nexpressions\nobjects\nsequences\ndetection\nnavigate\nunderstanding\nshapes\nunderstanding\nsnarks\ntwo\nqa\nnames\nqa\ndisambiguation lies\ndisambiguation lies\nmultistep_arithmetic.\nweb\nruin names\nruin objects\ngeometric shapes\nreasoning_about_colored a of\ngeometric a of two\nreasoning_about_colored\npenguins_in\nseven fallacies\nseven\nformal fallacies\nformal navigate\nobject counting\npenguins in\ndyck languages\ndyck languages\ntemporal sequences\ncausal judgement\ntemporal web\ncausal judgement\nboolean word_sorting\nboolean word_sorting\ntranslation_ error\ndate\ndate arithmetic\nsports\nsports\nmultistep object_ counting\nmovie recommendation\nmovie\ntracking_shuffled_ objects_seven\ntracking_shuffled_ objects_seven\nlogical_ deduction understanding\nlogical_ deduction\nsalient_ translation_error\nsalient_\n(a) PaLM 2-L, ours minus 'Let's think step by step.' (b) PaLM 2-L, ours minus empty starting point\n40\n60\ndifference\ndifference\n40\n20\naccuracy\naccuracy\n20\n\n0\n0\nnavigate\ncausal_judgement\nnavigate\ntable\nexpressions\nobjects\nexpressions\nsnarks\nobjects\ntable\ndetection\nobjects\nunderstanding\ndetection\nsnarks\nunderstanding\nunderstanding\nhyperbaton\nhyperbaton\nnames\nunderstanding\nnames\nshapes\nqa\nqa\ndisambiguation two\ndisambiguation lies\nof lies\nweb\nweb sorting\nruin objects\ngeometric shapes\nruin objects\ngeometric objects\ntracking_shuffled_objects_seven a of\ntracking_shuffled_objects_severi a\nreasoning_about_colored\nseven\nseven\npenguins_in fallacies\npenguins_in\nformal\nformal fallacies\nobject counting\ndyck languages\ndyck languages\ntemporal sequences\ntemporal sequences\ncausal judgement\nboolean word_sorting\nboolean word_\ndeduction arithmetic two\ndate_\nsports\nsports\ndate arithmetic\nreasoning_about_ colored\nmultistep object counting\nmovie recommendation\nmovie recommendation\nlogical_ deduction\nlogical_ multistep_\nsalient_ translation_error\nsalient_ translation_error\n(c) text-bison, ours minus 'Let's think step by step.' (d) text-bison, ours minus empty starting point\" data-coord=\"top-left:(536,1021); bottom-right:(2015,2877)\" /></figure>",
            "id": 391,
            "page": 35,
            "text": "40 difference 40 20 L JILL difference accuracy accuracy 20 0 -20 0 objects expressions detection recommendation hyperbaton understanding objects table hyperbaton objects objects snarks table expressions objects sequences detection navigate understanding shapes understanding snarks two qa names qa disambiguation lies disambiguation lies multistep_arithmetic. web ruin names ruin objects geometric shapes reasoning_about_colored a of geometric a of two reasoning_about_colored penguins_in seven fallacies seven formal fallacies formal navigate object counting penguins in dyck languages dyck languages temporal sequences causal judgement temporal web causal judgement boolean word_sorting boolean word_sorting translation_ error date date arithmetic sports sports multistep object_ counting movie recommendation movie tracking_shuffled_ objects_seven tracking_shuffled_ objects_seven logical_ deduction understanding logical_ deduction salient_ translation_error salient_ (a) PaLM 2-L, ours minus \"Let's think step by step.' (b) PaLM 2-L, ours minus empty starting point 40 60 difference difference 40 20 accuracy accuracy 20  0 0 navigate causal_judgement navigate table expressions objects expressions snarks objects table detection objects understanding detection snarks understanding understanding hyperbaton hyperbaton names understanding names shapes qa qa disambiguation two disambiguation lies of lies web web sorting ruin objects geometric shapes ruin objects geometric objects tracking_shuffled_objects_seven a of tracking_shuffled_objects_severi a reasoning_about_colored seven seven penguins_in fallacies penguins_in formal formal fallacies object counting dyck languages dyck languages temporal sequences temporal sequences causal judgement boolean word_sorting boolean word_ deduction arithmetic two date_ sports sports date arithmetic reasoning_about_ colored multistep object counting movie recommendation movie recommendation logical_ deduction logical_ multistep_ salient_ translation_error salient_ translation_error (c) text-bison, ours minus \"Let's think step by step.\" (d) text-bison, ours minus empty starting point"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2903
                },
                {
                    "x": 2112,
                    "y": 2903
                },
                {
                    "x": 2112,
                    "y": 3048
                },
                {
                    "x": 441,
                    "y": 3048
                }
            ],
            "category": "caption",
            "html": "<caption id='392' style='font-size:20px'>Figure 25: On 23 BBH tasks, the accuracy differences among instructions found by prompt opti-<br>mization (with the gpt-3 5-turbo optimizer), \"Let's think step by step.' , and the empty string<br>(optimization starting point).</caption>",
            "id": 392,
            "page": 35,
            "text": "Figure 25: On 23 BBH tasks, the accuracy differences among instructions found by prompt optimization (with the gpt-3 5-turbo optimizer), \"Let's think step by step.' , and the empty string (optimization starting point)."
        },
        {
            "bounding_box": [
                {
                    "x": 1250,
                    "y": 3132
                },
                {
                    "x": 1300,
                    "y": 3132
                },
                {
                    "x": 1300,
                    "y": 3171
                },
                {
                    "x": 1250,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='393' style='font-size:18px'>35</footer>",
            "id": 393,
            "page": 35,
            "text": "35"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 113
                },
                {
                    "x": 1099,
                    "y": 113
                },
                {
                    "x": 1099,
                    "y": 158
                },
                {
                    "x": 443,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='394' style='font-size:20px'>Large Language Models as Optimizers</header>",
            "id": 394,
            "page": 36,
            "text": "Large Language Models as Optimizers"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 371
                },
                {
                    "x": 2108,
                    "y": 371
                },
                {
                    "x": 2108,
                    "y": 515
                },
                {
                    "x": 440,
                    "y": 515
                }
            ],
            "category": "caption",
            "html": "<caption id='395' style='font-size:16px'>Table 10: Accuracies on BBH tasks with the gpt-3 · 5-turbo optimizer that starts from the empty<br>string. The PaLM 2-L scores are from A_ begin (left) instructions; the text-bison scores include<br>Q_begin (left) and Q_end (right) instructions.</caption>",
            "id": 395,
            "page": 36,
            "text": "Table 10: Accuracies on BBH tasks with the gpt-3 · 5-turbo optimizer that starts from the empty string. The PaLM 2-L scores are from A_ begin (left) instructions; the text-bison scores include Q_begin (left) and Q_end (right) instructions."
        },
        {
            "bounding_box": [
                {
                    "x": 592,
                    "y": 523
                },
                {
                    "x": 1958,
                    "y": 523
                },
                {
                    "x": 1958,
                    "y": 2219
                },
                {
                    "x": 592,
                    "y": 2219
                }
            ],
            "category": "table",
            "html": "<table id='396' style='font-size:14px'><tr><td rowspan=\"2\">Task</td><td rowspan=\"2\">Scorer</td><td>Our Acc (begin)</td><td>Our Acc (end)</td></tr><tr><td>training / test / overall</td><td>training / test / overall</td></tr><tr><td>boolean_expressions</td><td>PaLM 2-L</td><td>92.0 / 86.5 / 87.6</td><td>N/A</td></tr><tr><td>causal_judgement</td><td>PaLM 2-L</td><td>81.1 / 58.7 / 63.1</td><td>N/A</td></tr><tr><td>date_understanding</td><td>PaLM 2-L</td><td>86.0 / 82.0 / 82.8</td><td>N/A</td></tr><tr><td>disambiguation_qa</td><td>PaLM 2-L</td><td>80.0 / 74.0 / 75.2</td><td>N/A</td></tr><tr><td>dyck_languages</td><td>PaLM 2-L</td><td>100.0 / 100.0 / 100.0</td><td>N/A</td></tr><tr><td>formal_fallacies</td><td>PaLM 2-L</td><td>88.0 / 63.5 / 68.4</td><td>N/A</td></tr><tr><td>geometric_shapes</td><td>PaLM 2-L</td><td>60.0 / 41.0 / 44.8</td><td>N/A</td></tr><tr><td>hyperbaton</td><td>PaLM 2-L</td><td>88.0 / 93.0 / 92.0</td><td>N/A</td></tr><tr><td>logical_deduction_seven_objects</td><td>PaLM 2-L</td><td>76.0 / 56.5 / 60.4</td><td>N/A</td></tr><tr><td>movie_recommendation</td><td>PaLM 2-L</td><td>84.0 / 86.0 / 85.6</td><td>N/A</td></tr><tr><td>multistep_arithmetic_two</td><td>PaLM 2-L</td><td>52.0 / 49.0 / 49.6</td><td>N/A</td></tr><tr><td>navigate</td><td>PaLM 2-L</td><td>76.0 / 67.0 / 68.8</td><td>N/A</td></tr><tr><td>object_counting</td><td>PaLM 2-L</td><td>78.0 / 79.0 / 78.8</td><td>N/A</td></tr><tr><td>penguins_in_a_table</td><td>PaLM 2-L</td><td>82.8 / 72.6 / 74.7</td><td>N/A</td></tr><tr><td>reasoning_about _colored_objects</td><td>PaLM 2-L</td><td>86.0 / 67.5 /71.2</td><td>N/A</td></tr><tr><td>ruin_names</td><td>PaLM 2-L</td><td>90.0 / 83.0 / 84.4</td><td>N/A</td></tr><tr><td>salient_manslation_error_detection</td><td>PaLM 2-L</td><td>62.0 / 65.0 / 64.4</td><td>N/A</td></tr><tr><td>snarks</td><td>PaLM 2-L</td><td>85.7 / 70.6 / 73.6</td><td>N/A</td></tr><tr><td>sports_understanding</td><td>PaLM 2-L</td><td>68.0 / 57.5 / 59.6</td><td>N/A</td></tr><tr><td>temporal_sequences</td><td>PaLM 2-L</td><td>100.0 / 99.5 / 99.6</td><td>N/A</td></tr><tr><td>tracking_stuffled_objects_seven_objects</td><td>PaLM 2-L</td><td>44.0 / 34.5 / 36.4</td><td>N/A</td></tr><tr><td>web_of_lies</td><td>PaLM 2-L</td><td>92.0 / 91.0 / 91.2</td><td>N/A</td></tr><tr><td>word_sorting</td><td>PaLM 2-L</td><td>62.0 / 52.0 / 54.0</td><td>N/A</td></tr><tr><td>- boolean_expressions</td><td>- - text-bison</td><td>- 84.0 / 78.5 / 79.6</td><td>- - 80.0/ 78.0/78.4</td></tr><tr><td>causal_judgement</td><td>text-bison</td><td>78.4 / 57.3 / 61.5</td><td>83.8 / 53.3 / 59.4</td></tr><tr><td>date_understanding</td><td>text-bison</td><td>52.0 / 45.0 / 46.4</td><td>64.0 / 52.4 / 54.8</td></tr><tr><td>disambiguation_qa</td><td>text-bison</td><td>68.0 / 75.5 / 74.0</td><td>64.0 / 71.5 / 70.0</td></tr><tr><td>dyck_languages</td><td>text-bison</td><td>100.0 / 99.5 / 99.6</td><td>100.0 / 100.0 / 100.0</td></tr><tr><td>formal_ fallacies</td><td>text-bison</td><td>70.0 / 54.5 / 57.6</td><td>74.0 / 53.5 / 57.6</td></tr><tr><td>geometric_shapes</td><td>text-bison</td><td>28.0 / 15.0 / 17.6</td><td>48.0 / 28.0 / 32.0</td></tr><tr><td>hyperbaton</td><td>text-bison</td><td>86.0 / 85.0 / 85.2</td><td>80.0 / 76.5 / 77.2</td></tr><tr><td>logical_deduction_seven_objects</td><td>text-bison</td><td>66.0 / 57.5 / 59.2</td><td>62.0 / 55.0 / 56.4</td></tr><tr><td>movie_recommendation</td><td>text-bison</td><td>76.0 / 69.5 / 70.8</td><td>82.0 / 70.5 /72.8</td></tr><tr><td>multistep_arithmetic_two</td><td>text-bison</td><td>28.0 / 20.5 / 22.0</td><td>28.0 / 22.5 /23.6</td></tr><tr><td>navigate</td><td>text-bison</td><td>72.0 / 61.0 / 63.2</td><td>68.0 / 59.5 / 61.2</td></tr><tr><td>object_counting</td><td>text-bison</td><td>68.0 / 71.0 / 70.4</td><td>72.0 / 69.0 / 69.6</td></tr><tr><td>penguins_in_a_table</td><td>text-bison</td><td>65.5 / 59.8 / 61.0</td><td>79.3 / 53.0 / 58.2</td></tr><tr><td>reasoning_about_colored_objects</td><td>text-bison</td><td>84.0 / 76.5 / 78.0</td><td>86.0 / 74.0 / 76.4</td></tr><tr><td>ruin_names</td><td>text-bison</td><td>80.0 / 74.0 / 75.2</td><td>74.0 / 75.0 / 74.8</td></tr><tr><td></td><td></td><td></td><td>48.0 / 51.0 / 50.4</td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td>salient_manslation_error_detection</td><td>text-bison text-bison</td><td>44.0 / 50.5 / 49.2 82.9 / 79.7 / 80.3</td><td>88.6 / 84.6 / 85.4</td></tr><tr><td>snarks sports_understanding temporal_sequences</td><td>text-bison text-bison</td><td>84.0 / 76.5 / 78.0 50.0 / 54.5 / 53.6</td><td>90.0 / 80.0 / 82.0 64.0 / 61.5 / 62.0</td></tr><tr><td>tracking_shuffled_objects_seven_objects</td><td>text-bison</td><td>22.0 / 18.5 / 19.2</td><td>30.0 / 21.5 / 23.2</td></tr><tr><td>web_of_lies</td><td>text-bison</td><td>64.0 / 57.5 / 58.8</td><td>68.0/ 55.0/ 57.6</td></tr><tr><td>word_sorting</td><td>text-bison</td><td>26.0/ 19.0 / 20.4</td><td>32.0/ 25.5/26.8</td></tr></table>",
            "id": 396,
            "page": 36,
            "text": "Task Scorer Our Acc (begin) Our Acc (end)  training / test / overall training / test / overall  boolean_expressions PaLM 2-L 92.0 / 86.5 / 87.6 N/A  causal_judgement PaLM 2-L 81.1 / 58.7 / 63.1 N/A  date_understanding PaLM 2-L 86.0 / 82.0 / 82.8 N/A  disambiguation_qa PaLM 2-L 80.0 / 74.0 / 75.2 N/A  dyck_languages PaLM 2-L 100.0 / 100.0 / 100.0 N/A  formal_fallacies PaLM 2-L 88.0 / 63.5 / 68.4 N/A  geometric_shapes PaLM 2-L 60.0 / 41.0 / 44.8 N/A  hyperbaton PaLM 2-L 88.0 / 93.0 / 92.0 N/A  logical_deduction_seven_objects PaLM 2-L 76.0 / 56.5 / 60.4 N/A  movie_recommendation PaLM 2-L 84.0 / 86.0 / 85.6 N/A  multistep_arithmetic_two PaLM 2-L 52.0 / 49.0 / 49.6 N/A  navigate PaLM 2-L 76.0 / 67.0 / 68.8 N/A  object_counting PaLM 2-L 78.0 / 79.0 / 78.8 N/A  penguins_in_a_table PaLM 2-L 82.8 / 72.6 / 74.7 N/A  reasoning_about _colored_objects PaLM 2-L 86.0 / 67.5 /71.2 N/A  ruin_names PaLM 2-L 90.0 / 83.0 / 84.4 N/A  salient_manslation_error_detection PaLM 2-L 62.0 / 65.0 / 64.4 N/A  snarks PaLM 2-L 85.7 / 70.6 / 73.6 N/A  sports_understanding PaLM 2-L 68.0 / 57.5 / 59.6 N/A  temporal_sequences PaLM 2-L 100.0 / 99.5 / 99.6 N/A  tracking_stuffled_objects_seven_objects PaLM 2-L 44.0 / 34.5 / 36.4 N/A  web_of_lies PaLM 2-L 92.0 / 91.0 / 91.2 N/A  word_sorting PaLM 2-L 62.0 / 52.0 / 54.0 N/A  - boolean_expressions - - text-bison - 84.0 / 78.5 / 79.6 - - 80.0/ 78.0/78.4  causal_judgement text-bison 78.4 / 57.3 / 61.5 83.8 / 53.3 / 59.4  date_understanding text-bison 52.0 / 45.0 / 46.4 64.0 / 52.4 / 54.8  disambiguation_qa text-bison 68.0 / 75.5 / 74.0 64.0 / 71.5 / 70.0  dyck_languages text-bison 100.0 / 99.5 / 99.6 100.0 / 100.0 / 100.0  formal_ fallacies text-bison 70.0 / 54.5 / 57.6 74.0 / 53.5 / 57.6  geometric_shapes text-bison 28.0 / 15.0 / 17.6 48.0 / 28.0 / 32.0  hyperbaton text-bison 86.0 / 85.0 / 85.2 80.0 / 76.5 / 77.2  logical_deduction_seven_objects text-bison 66.0 / 57.5 / 59.2 62.0 / 55.0 / 56.4  movie_recommendation text-bison 76.0 / 69.5 / 70.8 82.0 / 70.5 /72.8  multistep_arithmetic_two text-bison 28.0 / 20.5 / 22.0 28.0 / 22.5 /23.6  navigate text-bison 72.0 / 61.0 / 63.2 68.0 / 59.5 / 61.2  object_counting text-bison 68.0 / 71.0 / 70.4 72.0 / 69.0 / 69.6  penguins_in_a_table text-bison 65.5 / 59.8 / 61.0 79.3 / 53.0 / 58.2  reasoning_about_colored_objects text-bison 84.0 / 76.5 / 78.0 86.0 / 74.0 / 76.4  ruin_names text-bison 80.0 / 74.0 / 75.2 74.0 / 75.0 / 74.8     48.0 / 51.0 / 50.4       salient_manslation_error_detection text-bison text-bison 44.0 / 50.5 / 49.2 82.9 / 79.7 / 80.3 88.6 / 84.6 / 85.4  snarks sports_understanding temporal_sequences text-bison text-bison 84.0 / 76.5 / 78.0 50.0 / 54.5 / 53.6 90.0 / 80.0 / 82.0 64.0 / 61.5 / 62.0  tracking_shuffled_objects_seven_objects text-bison 22.0 / 18.5 / 19.2 30.0 / 21.5 / 23.2  web_of_lies text-bison 64.0 / 57.5 / 58.8 68.0/ 55.0/ 57.6  word_sorting text-bison 26.0/ 19.0 / 20.4"
        },
        {
            "bounding_box": [
                {
                    "x": 1250,
                    "y": 3133
                },
                {
                    "x": 1300,
                    "y": 3133
                },
                {
                    "x": 1300,
                    "y": 3172
                },
                {
                    "x": 1250,
                    "y": 3172
                }
            ],
            "category": "footer",
            "html": "<footer id='397' style='font-size:16px'>36</footer>",
            "id": 397,
            "page": 36,
            "text": "36"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 112
                },
                {
                    "x": 1099,
                    "y": 112
                },
                {
                    "x": 1099,
                    "y": 158
                },
                {
                    "x": 444,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='398' style='font-size:20px'>Large Language Models as Optimizers</header>",
            "id": 398,
            "page": 37,
            "text": "Large Language Models as Optimizers"
        },
        {
            "bounding_box": [
                {
                    "x": 439,
                    "y": 370
                },
                {
                    "x": 2112,
                    "y": 370
                },
                {
                    "x": 2112,
                    "y": 469
                },
                {
                    "x": 439,
                    "y": 469
                }
            ],
            "category": "caption",
            "html": "<caption id='399' style='font-size:18px'>Table 11: BBH task-wise instructions found by prompt optimization with the PaLM 2-L scorer and<br>the gpt-3 · 5-turbo optimizer. The optimizations start from the empty string.</caption>",
            "id": 399,
            "page": 37,
            "text": "Table 11: BBH task-wise instructions found by prompt optimization with the PaLM 2-L scorer and the gpt-3 · 5-turbo optimizer. The optimizations start from the empty string."
        },
        {
            "bounding_box": [
                {
                    "x": 1383,
                    "y": 481
                },
                {
                    "x": 1578,
                    "y": 481
                },
                {
                    "x": 1578,
                    "y": 515
                },
                {
                    "x": 1383,
                    "y": 515
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='400' style='font-size:16px'>Our Instruction</p>",
            "id": 400,
            "page": 37,
            "text": "Our Instruction"
        },
        {
            "bounding_box": [
                {
                    "x": 477,
                    "y": 496
                },
                {
                    "x": 2190,
                    "y": 496
                },
                {
                    "x": 2190,
                    "y": 2761
                },
                {
                    "x": 477,
                    "y": 2761
                }
            ],
            "category": "table",
            "html": "<br><table id='401' style='font-size:14px'><tr><td>Task</td><td></td></tr><tr><td>boolean_expressions</td><td>An accurate evaluation of logical expressions involves correctly applying Boolean operators, considering the order of operations, and analyzing the truth values of the operands in accordance with Boolean logic principles.</td></tr><tr><td>causal_judgement</td><td>Understanding causality is critical for accurately assessing cause and effect relationships in various scenarios, leading to well-informed judgments, precise conclusions, and definitive answers to questions about the outcomes involved.</td></tr><tr><td>date_understanding</td><td>What is the specific date mentioned or required in each given problem or question, taking into account all relevant information, available options, and the provided context? Please provide the accurate answer in the format MM/DD/YYYY.</td></tr><tr><td>disambiguation_qa</td><td>Accurately analyze and clarify the pronoun-antecedent relationship in the given sentences, identifying the appropriate referent to eliminate any potential confusion or ambiguity and ensure a precise understanding of the intended meaning.</td></tr><tr><td>dyck_ languages</td><td>Solve the sequence by properly closing the parentheses.</td></tr><tr><td>formal fallacies</td><td>In determining the deductive validity of arguments based on explicit premises, a meticulous analysis of the logical relationships and implications is essential for definitively establishing their soundness, confirming their validity or invalidity, and ensuring a reliable and robust assessment of the arguments at hand.</td></tr><tr><td>geometric_shapes</td><td>The SVG path element with the \"d\" attribute plays a crucial role in web development, allowing for the precise definition and rendering of various shapes on a webpage.</td></tr><tr><td>hyperbaton</td><td>Understanding the correct order of adjectives is crucial for constructing grammatically accurate and coherent sentences that effectively convey the intended meaning in diverse contexts while ensuring clarity, cohesion, and consistency throughout consistently and effortlessly.</td></tr><tr><td>logical_ deduction _seven_objects</td><td>By conducting a meticulous analysis of the given information and ensuring logical consistency within each paragraph, we can accurately determine the precise order or ranking of the mentioned objects, allowing us to confidently and consistently identify the correct answer in every presented scenario with utmost precision and confidence.</td></tr><tr><td>movie_recommendation</td><td>Which movie option from the given choices closely matches the mentioned films in terms of themes, storylines, and characteristics, guaranteeing the highest possible similarity score among them all?</td></tr><tr><td>multistep_arithmetic_two</td><td>Evaluate the given mathematical expressions step by step to determine the correct solutions accurately.</td></tr><tr><td>navigate</td><td>Is it possible to determine, with absolute certainty, whether strictly adhering to the given instructions will unfailingly bring you back to the original starting point without any exceptions, errors, or deviations?</td></tr><tr><td>object_counting</td><td>Determine the total number of objects or entities mentioned in the given list, covering various categories and types, to accurately calculate the overall count.</td></tr><tr><td>penguins_in_a_table</td><td>From the given table, what information can we gather about the mentioned animals and their respective attributes, including names, ages, heights, and weights?</td></tr><tr><td>reasoning_ about _colored_objects</td><td>By thoroughly examining the given information, accurately determine the answers for each question by considering the specific characteristics, colors, and positions of the mentioned objects.</td></tr><tr><td>ruin _names</td><td>Select the most amusing and clever alteration from the options provided for the given artist, movie, or title name, and accurately choose the correct answer to test your wit and creativity.</td></tr><tr><td>salient_ translation _error_ detection</td><td>Thoroughly examine the given translations from German to English and accurately identify any errors by carefully analyzing the text and selecting the appropriate option with meticulous attention to detail, precision, utmost accuracy, and comprehensive understanding of the language for precise evaluation and categorization.</td></tr><tr><td>snarks</td><td>Which option delivers the most devastatingly sarcastic response, brilliantly exposing the sheer absurdity and leaving absolutely no doubt whatsoever in all the given situations?</td></tr><tr><td>sports_understanding</td><td>Maintaining the accuracy, reliability, and integrity of sports event representation is essential for upholding the highest standards of credibility, trustworthiness, and overall quality in conveying information, without any compromise, misrepresentation, or distortion, thereby ensuring the factual accuracy of sports journalism.</td></tr><tr><td>temporal_sequences</td><td>Based on the provided timeline and observed activities, we can accurately determine the possible time range when each individual could have visited their intended destinations and answer questions about their visitation time.</td></tr><tr><td>tracking_shuffled_ objects _seven_objects</td><td>An important point to note is that each person in the group starts with one specific book at the beginning of the semester.</td></tr><tr><td>web_ of_ lies</td><td>Analyzing the consistency and accuracy of statements provided by each person is crucial for determining the truthfulness of individuals in every scenario.</td></tr><tr><td>word_sorting</td><td>Please sort the given words in alphabetical order: The list of words to be sorted contains -</td></tr></table>",
            "id": 401,
            "page": 37,
            "text": "Task   boolean_expressions An accurate evaluation of logical expressions involves correctly applying Boolean operators, considering the order of operations, and analyzing the truth values of the operands in accordance with Boolean logic principles.  causal_judgement Understanding causality is critical for accurately assessing cause and effect relationships in various scenarios, leading to well-informed judgments, precise conclusions, and definitive answers to questions about the outcomes involved.  date_understanding What is the specific date mentioned or required in each given problem or question, taking into account all relevant information, available options, and the provided context? Please provide the accurate answer in the format MM/DD/YYYY.  disambiguation_qa Accurately analyze and clarify the pronoun-antecedent relationship in the given sentences, identifying the appropriate referent to eliminate any potential confusion or ambiguity and ensure a precise understanding of the intended meaning.  dyck_ languages Solve the sequence by properly closing the parentheses.  formal fallacies In determining the deductive validity of arguments based on explicit premises, a meticulous analysis of the logical relationships and implications is essential for definitively establishing their soundness, confirming their validity or invalidity, and ensuring a reliable and robust assessment of the arguments at hand.  geometric_shapes The SVG path element with the \"d\" attribute plays a crucial role in web development, allowing for the precise definition and rendering of various shapes on a webpage.  hyperbaton Understanding the correct order of adjectives is crucial for constructing grammatically accurate and coherent sentences that effectively convey the intended meaning in diverse contexts while ensuring clarity, cohesion, and consistency throughout consistently and effortlessly.  logical_ deduction _seven_objects By conducting a meticulous analysis of the given information and ensuring logical consistency within each paragraph, we can accurately determine the precise order or ranking of the mentioned objects, allowing us to confidently and consistently identify the correct answer in every presented scenario with utmost precision and confidence.  movie_recommendation Which movie option from the given choices closely matches the mentioned films in terms of themes, storylines, and characteristics, guaranteeing the highest possible similarity score among them all?  multistep_arithmetic_two Evaluate the given mathematical expressions step by step to determine the correct solutions accurately.  navigate Is it possible to determine, with absolute certainty, whether strictly adhering to the given instructions will unfailingly bring you back to the original starting point without any exceptions, errors, or deviations?  object_counting Determine the total number of objects or entities mentioned in the given list, covering various categories and types, to accurately calculate the overall count.  penguins_in_a_table From the given table, what information can we gather about the mentioned animals and their respective attributes, including names, ages, heights, and weights?  reasoning_ about _colored_objects By thoroughly examining the given information, accurately determine the answers for each question by considering the specific characteristics, colors, and positions of the mentioned objects.  ruin _names Select the most amusing and clever alteration from the options provided for the given artist, movie, or title name, and accurately choose the correct answer to test your wit and creativity.  salient_ translation _error_ detection Thoroughly examine the given translations from German to English and accurately identify any errors by carefully analyzing the text and selecting the appropriate option with meticulous attention to detail, precision, utmost accuracy, and comprehensive understanding of the language for precise evaluation and categorization.  snarks Which option delivers the most devastatingly sarcastic response, brilliantly exposing the sheer absurdity and leaving absolutely no doubt whatsoever in all the given situations?  sports_understanding Maintaining the accuracy, reliability, and integrity of sports event representation is essential for upholding the highest standards of credibility, trustworthiness, and overall quality in conveying information, without any compromise, misrepresentation, or distortion, thereby ensuring the factual accuracy of sports journalism.  temporal_sequences Based on the provided timeline and observed activities, we can accurately determine the possible time range when each individual could have visited their intended destinations and answer questions about their visitation time.  tracking_shuffled_ objects _seven_objects An important point to note is that each person in the group starts with one specific book at the beginning of the semester.  web_ of_ lies Analyzing the consistency and accuracy of statements provided by each person is crucial for determining the truthfulness of individuals in every scenario.  word_sorting"
        },
        {
            "bounding_box": [
                {
                    "x": 1252,
                    "y": 3133
                },
                {
                    "x": 1298,
                    "y": 3133
                },
                {
                    "x": 1298,
                    "y": 3170
                },
                {
                    "x": 1252,
                    "y": 3170
                }
            ],
            "category": "footer",
            "html": "<footer id='402' style='font-size:18px'>37</footer>",
            "id": 402,
            "page": 37,
            "text": "37"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 112
                },
                {
                    "x": 1099,
                    "y": 112
                },
                {
                    "x": 1099,
                    "y": 159
                },
                {
                    "x": 445,
                    "y": 159
                }
            ],
            "category": "header",
            "html": "<header id='403' style='font-size:20px'>Large Language Models as Optimizers</header>",
            "id": 403,
            "page": 38,
            "text": "Large Language Models as Optimizers"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 370
                },
                {
                    "x": 2107,
                    "y": 370
                },
                {
                    "x": 2107,
                    "y": 474
                },
                {
                    "x": 441,
                    "y": 474
                }
            ],
            "category": "caption",
            "html": "<caption id='404' style='font-size:16px'>Table 12: BBH task-wise Q_begin instructions found by prompt optimization with the text-bi son<br>scorer and the gpt - 3 · 5-turbo optimizer. The optimizations start from the empty string.</caption>",
            "id": 404,
            "page": 38,
            "text": "Table 12: BBH task-wise Q_begin instructions found by prompt optimization with the text-bi son scorer and the gpt - 3 · 5-turbo optimizer. The optimizations start from the empty string."
        },
        {
            "bounding_box": [
                {
                    "x": 460,
                    "y": 515
                },
                {
                    "x": 2194,
                    "y": 515
                },
                {
                    "x": 2194,
                    "y": 2755
                },
                {
                    "x": 460,
                    "y": 2755
                }
            ],
            "category": "table",
            "html": "<table id='405' style='font-size:14px'><tr><td>Task</td><td>Our Instruction</td></tr><tr><td>boolean_expressions</td><td>Group sub-expressions with parentheses to accurately evaluate logical operations: not, and, and finally or. Determine the resulting value as either True or False.</td></tr><tr><td>causal_judgement</td><td>Consider the intentions and actions of the individuals involved.</td></tr><tr><td>date_understanding</td><td>Determine the one-day difference in the given date and express it in the format MM/DD/YYYY.</td></tr><tr><td>disambiguation_qa</td><td>Determine the precise antecedent of the pronoun in the given sentence and select the correct option or state ifit is ambiguous.</td></tr><tr><td>dyck_ languages</td><td>Ensure that all opening brackets have a corresponding closing bracket, and that the closing brackets are in the correct order.</td></tr><tr><td>formal fallacies</td><td>Thoroughly analyze the explicitly provided premises and determine the deductive validity of the argument based on all necessary conditions, implications, exclusions, and dependencies given.</td></tr><tr><td>geometric_shapes</td><td>Analyze the given SVG path element carefully and confidently select the correct option from the provided choices to accurately determine the corresponding shape. Pay close attention to the specific path details and confidently make the most suitable choice.</td></tr><tr><td>hyperbaton</td><td>Select the sentence that strictly adheres to the standard order of adjectives: opinion, size, age, shape, color, origin, material, and purpose. Ensure there are no deviations or alterations in the adjective order. Choose the option without any changes.</td></tr><tr><td>logical_ deduction _seven_objects</td><td>Analyze the given information to accurately determine the precise order and ranking of the mentioned objects/people, considering their relationships, positions, and any provided comparisons, for a definitive and logical progression with maximum accuracy and efficiency.</td></tr><tr><td>movie_recommendation</td><td>Based on the movie list provided, carefully consider your preferences and make a well-informed decision.</td></tr><tr><td>multistep_arithmetic_ two</td><td>First, simplify any expressions within parentheses following the correct order of operations to accurately evaluate the final answer with efficiency and precision.</td></tr><tr><td>navigate</td><td>Always face forward. Take 10 steps forward. Turn left. Take 5 steps forward. Take 3 steps backward. Finally, take 7 steps forward. Turn around and take 1 step forward. Repeat the previous sequence three times. Follow the given path precisely without any deviations. At the end, turn right and take 11 steps forward. If you follow these instructions, will you return to the starting point? Options: - Yes - No</td></tr><tr><td>object_counting</td><td>Determine the total count of mentioned vegetables accurately and state the final count as the answer.</td></tr><tr><td>penguins_in_a_table</td><td>Analyze the given table to accurately determine the required information based on the provided criteria and attributes of the penguins and giraffes. Utilize efficient problem-solving strategies to arrive at the correct answer.</td></tr><tr><td>reasoning_ about colored_objects</td><td>State the color of the object mentioned in the given arrangement with utmost accuracy.</td></tr><tr><td>ruin_names</td><td>Choose the option that offers the most clever and humorous alteration of the given artist or movie name. Let your creativity shine and select the answer that will undoubtedly bring a smile to your face! Make sure to think outside the box!</td></tr><tr><td>salient_ translation error_ detection</td><td>Analyze the translation and accurately identify the specific error type based on the source text, providing the most appropriate corresponding option.</td></tr><tr><td>snarks</td><td>Choose the option that wickedly embodies sarcasm.</td></tr><tr><td>sports_understanding</td><td>Determine the plausibility of the given statement by evaluating factual accuracy, logical consistency, and contextual relevance, then provide a succinct and well-justified response.</td></tr><tr><td>temporal_sequences</td><td>Identify the optimal time slot for the individual to engage in the mentioned location/activity considering the given sightings and waking up time, taking into account the opening and closing times of the location and the duration of each event.</td></tr><tr><td>tracking_shuffled_objects _seven_objects</td><td>Pay attention to the given information and track the swaps/exchanges carefully to accurately determine the final possession/position/outcome for the specified individual.</td></tr><tr><td>web_ of_ lies</td><td>To determine the truthfulness of the last person mentioned, analyze the consistency of each statement and count the number of individuals accusing the previous person of lying. If the count of accusers is even, that person tells the truth; ifitis odd, that person lies.</td></tr><tr><td>word_sorting</td><td>Alphabetically sort the given list of words, ensuring all words are included and in ascending order.</td></tr></table>",
            "id": 405,
            "page": 38,
            "text": "Task Our Instruction  boolean_expressions Group sub-expressions with parentheses to accurately evaluate logical operations: not, and, and finally or. Determine the resulting value as either True or False.  causal_judgement Consider the intentions and actions of the individuals involved.  date_understanding Determine the one-day difference in the given date and express it in the format MM/DD/YYYY.  disambiguation_qa Determine the precise antecedent of the pronoun in the given sentence and select the correct option or state ifit is ambiguous.  dyck_ languages Ensure that all opening brackets have a corresponding closing bracket, and that the closing brackets are in the correct order.  formal fallacies Thoroughly analyze the explicitly provided premises and determine the deductive validity of the argument based on all necessary conditions, implications, exclusions, and dependencies given.  geometric_shapes Analyze the given SVG path element carefully and confidently select the correct option from the provided choices to accurately determine the corresponding shape. Pay close attention to the specific path details and confidently make the most suitable choice.  hyperbaton Select the sentence that strictly adheres to the standard order of adjectives: opinion, size, age, shape, color, origin, material, and purpose. Ensure there are no deviations or alterations in the adjective order. Choose the option without any changes.  logical_ deduction _seven_objects Analyze the given information to accurately determine the precise order and ranking of the mentioned objects/people, considering their relationships, positions, and any provided comparisons, for a definitive and logical progression with maximum accuracy and efficiency.  movie_recommendation Based on the movie list provided, carefully consider your preferences and make a well-informed decision.  multistep_arithmetic_ two First, simplify any expressions within parentheses following the correct order of operations to accurately evaluate the final answer with efficiency and precision.  navigate Always face forward. Take 10 steps forward. Turn left. Take 5 steps forward. Take 3 steps backward. Finally, take 7 steps forward. Turn around and take 1 step forward. Repeat the previous sequence three times. Follow the given path precisely without any deviations. At the end, turn right and take 11 steps forward. If you follow these instructions, will you return to the starting point? Options: - Yes - No  object_counting Determine the total count of mentioned vegetables accurately and state the final count as the answer.  penguins_in_a_table Analyze the given table to accurately determine the required information based on the provided criteria and attributes of the penguins and giraffes. Utilize efficient problem-solving strategies to arrive at the correct answer.  reasoning_ about colored_objects State the color of the object mentioned in the given arrangement with utmost accuracy.  ruin_names Choose the option that offers the most clever and humorous alteration of the given artist or movie name. Let your creativity shine and select the answer that will undoubtedly bring a smile to your face! Make sure to think outside the box!  salient_ translation error_ detection Analyze the translation and accurately identify the specific error type based on the source text, providing the most appropriate corresponding option.  snarks Choose the option that wickedly embodies sarcasm.  sports_understanding Determine the plausibility of the given statement by evaluating factual accuracy, logical consistency, and contextual relevance, then provide a succinct and well-justified response.  temporal_sequences Identify the optimal time slot for the individual to engage in the mentioned location/activity considering the given sightings and waking up time, taking into account the opening and closing times of the location and the duration of each event.  tracking_shuffled_objects _seven_objects Pay attention to the given information and track the swaps/exchanges carefully to accurately determine the final possession/position/outcome for the specified individual.  web_ of_ lies To determine the truthfulness of the last person mentioned, analyze the consistency of each statement and count the number of individuals accusing the previous person of lying. If the count of accusers is even, that person tells the truth; ifitis odd, that person lies.  word_sorting"
        },
        {
            "bounding_box": [
                {
                    "x": 1251,
                    "y": 3132
                },
                {
                    "x": 1299,
                    "y": 3132
                },
                {
                    "x": 1299,
                    "y": 3171
                },
                {
                    "x": 1251,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='406' style='font-size:18px'>38</footer>",
            "id": 406,
            "page": 38,
            "text": "38"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 112
                },
                {
                    "x": 1098,
                    "y": 112
                },
                {
                    "x": 1098,
                    "y": 158
                },
                {
                    "x": 444,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='407' style='font-size:20px'>Large Language Models as Optimizers</header>",
            "id": 407,
            "page": 39,
            "text": "Large Language Models as Optimizers"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 370
                },
                {
                    "x": 2108,
                    "y": 370
                },
                {
                    "x": 2108,
                    "y": 474
                },
                {
                    "x": 441,
                    "y": 474
                }
            ],
            "category": "caption",
            "html": "<caption id='408' style='font-size:16px'>Table 13: BBH task-wise Q_end instructions found by prompt optimization with the text-bi son<br>scorer and the gpt - 3 · 5-turbo optimizer. The optimizations start from the empty string.</caption>",
            "id": 408,
            "page": 39,
            "text": "Table 13: BBH task-wise Q_end instructions found by prompt optimization with the text-bi son scorer and the gpt - 3 · 5-turbo optimizer. The optimizations start from the empty string."
        },
        {
            "bounding_box": [
                {
                    "x": 470,
                    "y": 524
                },
                {
                    "x": 2193,
                    "y": 524
                },
                {
                    "x": 2193,
                    "y": 2772
                },
                {
                    "x": 470,
                    "y": 2772
                }
            ],
            "category": "paragraph",
            "html": "<p id='409' style='font-size:14px'>Task Our Instruction<br>boolean_expressions Accurately use order of operations and parentheses to evaluate logical expressions and determine truth values efficiently.<br>causal_judgement Consider all relevant factors, prioritize overall well-being and ethical considerations, make well-informed decisions<br>while foreseeing potential consequences efficiently, and consistently strive for optimal outcomes with empathy and<br>adaptability in a thoughtful and comprehensive manner.<br>date_understanding Subtract the specified number of days from the given date and format the outcome as MM/DD/YYYY to accurately<br>determine the desired result in an efficient manner.<br>disambiguation_qa Clearly identify and select the unambiguous antecedent for the pronoun or designate it as \" Ambiguous\" ifit is unclear.<br>dyck languages Add the missing closing parentheses.<br>formal fallacies Determine the deductive validity of the argument presented based on the explicitly stated premises and reach a definitive<br>conclusion.<br>geometric_shapes Analyzing the given SVG path element, accurately determine its shape by closely examining its curves and coordinates,<br>then select the correct option.<br>hyperbaton Choose the option with the correct adjective order in each sentence, prioritizing specific attributes like size, color, and<br>origin. Place the most specific adjective before the more general ones for precise and standardized ordering across all<br>examples. Ensure accurate alignment of the adjectives based on their respective attributes for consistent and<br>standardized ordering.<br>logical_ deduction Determine the precise order of the given objects/participants based on the provided information and establish the final<br>_seven_objects ranking accurately, considering all relevant factors, while maintaining logical consistency with maximum efficiency.<br>movie_recommendation Choose the most similar option from the choices provided that closely aligns with the given movies' themes, genres, and<br>impact for the most accurate recommendation possible. Make your selection wisely.<br>multistep_arithmetic_two Carefully follow the order of operations to precisely simplify the expressions within parentheses and efficiently find the<br>accurate final answer.<br>navigate Always face forward. Take 10 steps forward. Turn right and walk for 5 steps. Then, make a left turn and continue for 9<br>steps. Proceed by walking 6 steps backward. Finally, turn around and take 200 steps. Accurately track your movements,<br>diligently adhere to the given path, and ensure to return to the starting point without any deviations or obstacles.<br>object_counting Determine the total count of items mentioned, including all listed items, using an efficient and concise method. State the<br>final count as your answer.<br>penguins_in_a_table Identify the animal with the maximum measurement (weight, age, or height) in the table and state its name and species.<br>reasoning_ about Determine the color of each item in the given scenario and select the correct color option from the provided choices for<br>_colored_objects accurate responses, ensuring utmost precision and completeness.<br>ruin names Choose the option that creatively and hilariously transforms the given artist or movie name.<br>salient_ translation Carefully analyze the translations and select the most suitable option from the given choices to rectify the specific error<br>error_ detection category, ensuring complete precision, accuracy, and faithful representation of the intended meaning, while considering<br>all relevant information in the source text.<br>snarks Choose the option that cleverly employs sarcasm to defy all expectations and leave everyone utterly dumbfounded,<br>questioning the very essence of their own perception.<br>sports_understanding Evaluate the plausibility of each given statement and provide a well-supported justification based on logical reasoning,<br>contextual understanding, and relevant evidence to arrive at a definitive and conclusive answer.<br>temporal_ sequences Identify the possible time slot for the desired activity based on the given information and sightings, then select the<br>correct option.<br>tracking_shuffled_ objects Thoroughly analyze the given scenarios, systematically consider all available information, and confidently determine<br>_seven_objects the final outcome with exceptional precision and optimal efficiency, while maintaining a strategic and logical approach<br>throughout the process.<br>web_ of_ lies Examine each person's statements meticulously to accurately determine the truth and confidently identify who is telling<br>the truth, enabling you to effectively solve the given problem.<br>word_ sorting Sort the given words alphabetically using spaces as separators while maintaining their original order and including all<br>words.</p>",
            "id": 409,
            "page": 39,
            "text": "Task Our Instruction boolean_expressions Accurately use order of operations and parentheses to evaluate logical expressions and determine truth values efficiently. causal_judgement Consider all relevant factors, prioritize overall well-being and ethical considerations, make well-informed decisions while foreseeing potential consequences efficiently, and consistently strive for optimal outcomes with empathy and adaptability in a thoughtful and comprehensive manner. date_understanding Subtract the specified number of days from the given date and format the outcome as MM/DD/YYYY to accurately determine the desired result in an efficient manner. disambiguation_qa Clearly identify and select the unambiguous antecedent for the pronoun or designate it as \" Ambiguous\" ifit is unclear. dyck languages Add the missing closing parentheses. formal fallacies Determine the deductive validity of the argument presented based on the explicitly stated premises and reach a definitive conclusion. geometric_shapes Analyzing the given SVG path element, accurately determine its shape by closely examining its curves and coordinates, then select the correct option. hyperbaton Choose the option with the correct adjective order in each sentence, prioritizing specific attributes like size, color, and origin. Place the most specific adjective before the more general ones for precise and standardized ordering across all examples. Ensure accurate alignment of the adjectives based on their respective attributes for consistent and standardized ordering. logical_ deduction Determine the precise order of the given objects/participants based on the provided information and establish the final _seven_objects ranking accurately, considering all relevant factors, while maintaining logical consistency with maximum efficiency. movie_recommendation Choose the most similar option from the choices provided that closely aligns with the given movies' themes, genres, and impact for the most accurate recommendation possible. Make your selection wisely. multistep_arithmetic_two Carefully follow the order of operations to precisely simplify the expressions within parentheses and efficiently find the accurate final answer. navigate Always face forward. Take 10 steps forward. Turn right and walk for 5 steps. Then, make a left turn and continue for 9 steps. Proceed by walking 6 steps backward. Finally, turn around and take 200 steps. Accurately track your movements, diligently adhere to the given path, and ensure to return to the starting point without any deviations or obstacles. object_counting Determine the total count of items mentioned, including all listed items, using an efficient and concise method. State the final count as your answer. penguins_in_a_table Identify the animal with the maximum measurement (weight, age, or height) in the table and state its name and species. reasoning_ about Determine the color of each item in the given scenario and select the correct color option from the provided choices for _colored_objects accurate responses, ensuring utmost precision and completeness. ruin names Choose the option that creatively and hilariously transforms the given artist or movie name. salient_ translation Carefully analyze the translations and select the most suitable option from the given choices to rectify the specific error error_ detection category, ensuring complete precision, accuracy, and faithful representation of the intended meaning, while considering all relevant information in the source text. snarks Choose the option that cleverly employs sarcasm to defy all expectations and leave everyone utterly dumbfounded, questioning the very essence of their own perception. sports_understanding Evaluate the plausibility of each given statement and provide a well-supported justification based on logical reasoning, contextual understanding, and relevant evidence to arrive at a definitive and conclusive answer. temporal_ sequences Identify the possible time slot for the desired activity based on the given information and sightings, then select the correct option. tracking_shuffled_ objects Thoroughly analyze the given scenarios, systematically consider all available information, and confidently determine _seven_objects the final outcome with exceptional precision and optimal efficiency, while maintaining a strategic and logical approach throughout the process. web_ of_ lies Examine each person's statements meticulously to accurately determine the truth and confidently identify who is telling the truth, enabling you to effectively solve the given problem. word_ sorting Sort the given words alphabetically using spaces as separators while maintaining their original order and including all words."
        },
        {
            "bounding_box": [
                {
                    "x": 1251,
                    "y": 3132
                },
                {
                    "x": 1298,
                    "y": 3132
                },
                {
                    "x": 1298,
                    "y": 3170
                },
                {
                    "x": 1251,
                    "y": 3170
                }
            ],
            "category": "footer",
            "html": "<footer id='410' style='font-size:18px'>39</footer>",
            "id": 410,
            "page": 39,
            "text": "39"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 112
                },
                {
                    "x": 1100,
                    "y": 112
                },
                {
                    "x": 1100,
                    "y": 158
                },
                {
                    "x": 443,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='411' style='font-size:22px'>Large Language Models as Optimizers</header>",
            "id": 411,
            "page": 40,
            "text": "Large Language Models as Optimizers"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 344
                },
                {
                    "x": 2049,
                    "y": 344
                },
                {
                    "x": 2049,
                    "y": 439
                },
                {
                    "x": 442,
                    "y": 439
                }
            ],
            "category": "paragraph",
            "html": "<p id='412' style='font-size:14px'>E.3 PALM 2-L AS SCORER, GPT-3 · 5-TURBO AS OPTIMIZER, OPTIMIZATION STARTING<br>FROM \"LET'S SOLVE THE PROBLEM. \"</p>",
            "id": 412,
            "page": 40,
            "text": "E.3 PALM 2-L AS SCORER, GPT-3 · 5-TURBO AS OPTIMIZER, OPTIMIZATION STARTING FROM \"LET'S SOLVE THE PROBLEM. \""
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 480
                },
                {
                    "x": 2108,
                    "y": 480
                },
                {
                    "x": 2108,
                    "y": 576
                },
                {
                    "x": 441,
                    "y": 576
                }
            ],
            "category": "paragraph",
            "html": "<p id='413' style='font-size:18px'>Figure 26 and Table 14 compare the accuracies of found instructions VS \"Let's solve the problem.\", ,<br>\"Let's think step by step.' , and the instructions in Table 11. Table 15 details the found instructions.</p>",
            "id": 413,
            "page": 40,
            "text": "Figure 26 and Table 14 compare the accuracies of found instructions VS \"Let's solve the problem.\", , \"Let's think step by step.' , and the instructions in Table 11. Table 15 details the found instructions."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 597
                },
                {
                    "x": 2111,
                    "y": 597
                },
                {
                    "x": 2111,
                    "y": 785
                },
                {
                    "x": 442,
                    "y": 785
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='414' style='font-size:18px'>The \"Let's\" pattern appears more often in the found instructions because of the starting points, and<br>the instructions are more often declarative that are more suitable for A begin, even if some are<br>semantically far from \"Let's solve the problem\". In fact, \"Let's\" was adopted by Zhou et al. (2022b)<br>as a fixed pattern in generated prompts, possibly because of the same reason.</p>",
            "id": 414,
            "page": 40,
            "text": "The \"Let's\" pattern appears more often in the found instructions because of the starting points, and the instructions are more often declarative that are more suitable for A begin, even if some are semantically far from \"Let's solve the problem\". In fact, \"Let's\" was adopted by Zhou  (2022b) as a fixed pattern in generated prompts, possibly because of the same reason."
        },
        {
            "bounding_box": [
                {
                    "x": 531,
                    "y": 856
                },
                {
                    "x": 2026,
                    "y": 856
                },
                {
                    "x": 2026,
                    "y": 2778
                },
                {
                    "x": 531,
                    "y": 2778
                }
            ],
            "category": "figure",
            "html": "<figure><img id='415' style='font-size:14px' alt=\"40\ndifference\ndifference\n40\naccuracy\naccuracy 20\n20\n0\ncausal_judgement\njudgement\nexpressions\nobjects\nsnarks\nrecommendation\nhyperbaton\nsnarks\nobjects\nobjects\ntable\nhyperbaton\ntable\ndetection\nunderstanding\nnames\nunderstanding\nunderstanding\nword_sorting\nunderstanding\nqa\nnames\nqa\ndisambiguation two\ndisambiguation lies\nof\nof lies\nweb\nweb shapes\nruin objects\nruin objects\ngeometric shapes\ngeometric objects\nreasoning_about_colored a\nseven fallacies\nseven\npenguins_in\nformal navigate\nformal navigate\nobject counting\nobject counting\nsalient_translation_error_ detection\npenguins in\ndyck languages\ndyck languages 0\ntemporal sequences\ncausal\ntemporal sequences\nboolean expressions\nboolean word_sorting\ntracking_shuffled_objects_ seven\ntracking_shuffled_objects_ seven\nmultistep_ arithmetic two\nmultistep_ arithmetic\nsports\ndate\nsports\ndate\nreasoning_about_ colored a\nmovie recommendation\nmovie\nlogical_ deduction\nlogical_ deduction\nsalient_ translation_error\n(a) ours minus 'Let's think step by step.' (b) ours minus 'Let's solve the problem.' starting\npoint\n20\ndifference\n0\naccuracy\n-20\ncausal_judgement\nexpressions\nobjects\ntable\nsnarks\nunderstanding\nhyperbaton\nunderstanding\nfallacies\nshapes\ntwo\nnames\nqa\ndisambiguation lies\na of\nweb\nruin objects\ngeometric objects\nreasoning_about_colored\nseven\npenguins_in\nformal navigate\nobject counting\nsalient_translation_error_ detection\ndyck languages\ntemporal sequences\nboolean word_sorting\nmultistep_ arithmetic\ndate\nsports\nmovie recommendation\ntracking_shuffled_ objects_seven\nlogical_ deduction\n(c) ours minus the instructions found with the empty\nstarting point\" data-coord=\"top-left:(531,856); bottom-right:(2026,2778)\" /></figure>",
            "id": 415,
            "page": 40,
            "text": "40 difference difference 40 accuracy accuracy 20 20 0 causal_judgement judgement expressions objects snarks recommendation hyperbaton snarks objects objects table hyperbaton table detection understanding names understanding understanding word_sorting understanding qa names qa disambiguation two disambiguation lies of of lies web web shapes ruin objects ruin objects geometric shapes geometric objects reasoning_about_colored a seven fallacies seven penguins_in formal navigate formal navigate object counting object counting salient_translation_error_ detection penguins in dyck languages dyck languages 0 temporal sequences causal temporal sequences boolean expressions boolean word_sorting tracking_shuffled_objects_ seven tracking_shuffled_objects_ seven multistep_ arithmetic two multistep_ arithmetic sports date sports date reasoning_about_ colored a movie recommendation movie logical_ deduction logical_ deduction salient_ translation_error (a) ours minus \"Let's think step by step.\" (b) ours minus \"Let's solve the problem.\" starting point 20 difference 0 accuracy -20 causal_judgement expressions objects table snarks understanding hyperbaton understanding fallacies shapes two names qa disambiguation lies a of web ruin objects geometric objects reasoning_about_colored seven penguins_in formal navigate object counting salient_translation_error_ detection dyck languages temporal sequences boolean word_sorting multistep_ arithmetic date sports movie recommendation tracking_shuffled_ objects_seven logical_ deduction (c) ours minus the instructions found with the empty starting point"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2809
                },
                {
                    "x": 2111,
                    "y": 2809
                },
                {
                    "x": 2111,
                    "y": 3047
                },
                {
                    "x": 441,
                    "y": 3047
                }
            ],
            "category": "caption",
            "html": "<caption id='416' style='font-size:20px'>Figure 26: On 23 BBH tasks, the accuracy differences among instructions found by prompt opti-<br>mization (with the text-bison scorer and the gpt-3 · 5-turbo optimizer), \"Let's think step by<br>step.' , and \"Let's solve the problem.' , (optimization starting point). The found instructions mostly<br>outperform the \"Let's think step by step.' baseline, the \"Let's solve the problem.\" starting point, and<br>the instructions in Table 11 found by prompt optimization from the empty string.</caption>",
            "id": 416,
            "page": 40,
            "text": "Figure 26: On 23 BBH tasks, the accuracy differences among instructions found by prompt optimization (with the text-bison scorer and the gpt-3 · 5-turbo optimizer), \"Let's think step by step.' , and \"Let's solve the problem.' , (optimization starting point). The found instructions mostly outperform the \"Let's think step by step.' baseline, the \"Let's solve the problem.\" starting point, and the instructions in Table 11 found by prompt optimization from the empty string."
        },
        {
            "bounding_box": [
                {
                    "x": 1248,
                    "y": 3133
                },
                {
                    "x": 1300,
                    "y": 3133
                },
                {
                    "x": 1300,
                    "y": 3171
                },
                {
                    "x": 1248,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='417' style='font-size:16px'>40</footer>",
            "id": 417,
            "page": 40,
            "text": "40"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 112
                },
                {
                    "x": 1100,
                    "y": 112
                },
                {
                    "x": 1100,
                    "y": 158
                },
                {
                    "x": 442,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='418' style='font-size:20px'>Large Language Models as Optimizers</header>",
            "id": 418,
            "page": 41,
            "text": "Large Language Models as Optimizers"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 372
                },
                {
                    "x": 2110,
                    "y": 372
                },
                {
                    "x": 2110,
                    "y": 468
                },
                {
                    "x": 440,
                    "y": 468
                }
            ],
            "category": "caption",
            "html": "<caption id='419' style='font-size:16px'>Table 14: Accuracies on BBH tasks with the PaLM 2-L scorer and the gpt-3 · 5-turbo optimizer<br>that starts from \"Let's solve the problem\". The scores are from A _begin instructions.</caption>",
            "id": 419,
            "page": 41,
            "text": "Table 14: Accuracies on BBH tasks with the PaLM 2-L scorer and the gpt-3 · 5-turbo optimizer that starts from \"Let's solve the problem\". The scores are from A _begin instructions."
        },
        {
            "bounding_box": [
                {
                    "x": 591,
                    "y": 490
                },
                {
                    "x": 1951,
                    "y": 490
                },
                {
                    "x": 1951,
                    "y": 1430
                },
                {
                    "x": 591,
                    "y": 1430
                }
            ],
            "category": "table",
            "html": "<table id='420' style='font-size:14px'><tr><td>Task</td><td>Scorer</td><td>Our Acc</td><td>\"Let's solve the , Acc problem.\"</td></tr><tr><td></td><td></td><td>training / test / overall</td><td>training / test / overall</td></tr><tr><td>boolean_expressions</td><td>PaLM 2-L</td><td>98.0 / 89.5 / 91.2</td><td>78.0 / 69.0/70.8</td></tr><tr><td>causal_judgement</td><td>PaLM 2-L</td><td>83.8 / 58.7 / 63.6</td><td>62.0 / 61.3 / 61.5</td></tr><tr><td>date_understanding</td><td>PaLM 2-L</td><td>90.0 / 82.0 / 83.6</td><td>74.0 / 71.0/71.6</td></tr><tr><td>disambiguation_qa</td><td>PaLM 2-L</td><td>78.0 / 68.0 / 70.0</td><td>52.0 / 54.5 / 54.0</td></tr><tr><td>dyck_languages</td><td>PaLM 2-L</td><td>100.0 / 100.0 / 100.0</td><td>94.0 / 97.0 / 96.4</td></tr><tr><td>formal_ fallacies</td><td>PaLM 2-L</td><td>84.0 / 62.0 / 66.4</td><td>68.0 / 54.0 / 56.8</td></tr><tr><td>geometric_shapes</td><td>PaLM 2-L</td><td>62.0 / 42.5 / 46.4</td><td>30.0 / 22.0 / 23.6</td></tr><tr><td>hyperbaton</td><td>PaLM 2-L</td><td>94.0 / 91.5 / 92.0</td><td>72.0 / 77.0 / 76.0</td></tr><tr><td>logical_deduction_seven_objects</td><td>PaLM 2-L</td><td>66.0 / 53.0 / 55.6</td><td>38.0 / 36.5 / 36.8</td></tr><tr><td>movie_recommendation</td><td>PaLM 2-L</td><td>88.0 / 88.0 / 88.0</td><td>66.0 / 76.0 / 74.0</td></tr><tr><td>multistep_arithmetic_two</td><td>PaLM 2-L</td><td>66.0 / 55.0 / 57.2</td><td>30.0 / 22.0 / 23.6</td></tr><tr><td>navigate</td><td>PaLM 2-L</td><td>76.0 / 67.0 / 68.8</td><td>54.0 / 63.5 / 61.6</td></tr><tr><td>object_counting</td><td>PaLM 2-L</td><td>96.0 / 92.5 / 93.2</td><td>58.0 / 58.0 / 58.0</td></tr><tr><td>penguins_in_a_table</td><td>PaLM 2-L</td><td>86.2 / 70.9 / 74.0</td><td>69.0 / 72.6 / 71.9</td></tr><tr><td>reasoning_about _colored_objects</td><td>PaLM 2-L</td><td>88.0 / 69.0 / 72.8</td><td>78.0 / 69.5 /71.2</td></tr><tr><td>ruin_names</td><td>PaLM 2-L</td><td>92.0 / 85.5 / 86.8</td><td>76.0 / 79.5 / 80.8</td></tr><tr><td>salient_manslation_error_detection</td><td>PaLM 2-L</td><td>66.0 / 67.5 / 67.2</td><td>30.0 / 35.5 / 34.4</td></tr><tr><td>snarks</td><td>PaLM 2-L</td><td>88.6 / 76.9 / 79.2</td><td>80.0/ 70.6/ 72.5</td></tr><tr><td>sports_understanding</td><td>PaLM 2-L</td><td>72.0 / 63.5 / 65.2</td><td>60.0/ 50.5 /52.4</td></tr><tr><td>temporal_sequences</td><td>PaLM 2-L</td><td>100.0 / 99.5 / 99.6</td><td>96.0 / 92.5 /93.2</td></tr><tr><td>tracking_shuffled_objects_seven_objects</td><td>PaLM 2-L</td><td>56.0 / 63.5 / 62.0</td><td>42.0 / 51.5 / 49.6</td></tr><tr><td>web_of_lies</td><td>PaLM 2-L</td><td>56.0 / 58.5 / 58.0</td><td>0.0 / 4.0/3.2</td></tr><tr><td>word_sorting</td><td>PaLM 2-L</td><td>52.0 / 44.5 / 46.0</td><td>18.0 / 20.5/20.0</td></tr></table>",
            "id": 420,
            "page": 41,
            "text": "Task Scorer Our Acc \"Let's solve the , Acc problem.\"    training / test / overall training / test / overall  boolean_expressions PaLM 2-L 98.0 / 89.5 / 91.2 78.0 / 69.0/70.8  causal_judgement PaLM 2-L 83.8 / 58.7 / 63.6 62.0 / 61.3 / 61.5  date_understanding PaLM 2-L 90.0 / 82.0 / 83.6 74.0 / 71.0/71.6  disambiguation_qa PaLM 2-L 78.0 / 68.0 / 70.0 52.0 / 54.5 / 54.0  dyck_languages PaLM 2-L 100.0 / 100.0 / 100.0 94.0 / 97.0 / 96.4  formal_ fallacies PaLM 2-L 84.0 / 62.0 / 66.4 68.0 / 54.0 / 56.8  geometric_shapes PaLM 2-L 62.0 / 42.5 / 46.4 30.0 / 22.0 / 23.6  hyperbaton PaLM 2-L 94.0 / 91.5 / 92.0 72.0 / 77.0 / 76.0  logical_deduction_seven_objects PaLM 2-L 66.0 / 53.0 / 55.6 38.0 / 36.5 / 36.8  movie_recommendation PaLM 2-L 88.0 / 88.0 / 88.0 66.0 / 76.0 / 74.0  multistep_arithmetic_two PaLM 2-L 66.0 / 55.0 / 57.2 30.0 / 22.0 / 23.6  navigate PaLM 2-L 76.0 / 67.0 / 68.8 54.0 / 63.5 / 61.6  object_counting PaLM 2-L 96.0 / 92.5 / 93.2 58.0 / 58.0 / 58.0  penguins_in_a_table PaLM 2-L 86.2 / 70.9 / 74.0 69.0 / 72.6 / 71.9  reasoning_about _colored_objects PaLM 2-L 88.0 / 69.0 / 72.8 78.0 / 69.5 /71.2  ruin_names PaLM 2-L 92.0 / 85.5 / 86.8 76.0 / 79.5 / 80.8  salient_manslation_error_detection PaLM 2-L 66.0 / 67.5 / 67.2 30.0 / 35.5 / 34.4  snarks PaLM 2-L 88.6 / 76.9 / 79.2 80.0/ 70.6/ 72.5  sports_understanding PaLM 2-L 72.0 / 63.5 / 65.2 60.0/ 50.5 /52.4  temporal_sequences PaLM 2-L 100.0 / 99.5 / 99.6 96.0 / 92.5 /93.2  tracking_shuffled_objects_seven_objects PaLM 2-L 56.0 / 63.5 / 62.0 42.0 / 51.5 / 49.6  web_of_lies PaLM 2-L 56.0 / 58.5 / 58.0 0.0 / 4.0/3.2  word_sorting PaLM 2-L 52.0 / 44.5 / 46.0"
        },
        {
            "bounding_box": [
                {
                    "x": 1248,
                    "y": 3131
                },
                {
                    "x": 1296,
                    "y": 3131
                },
                {
                    "x": 1296,
                    "y": 3173
                },
                {
                    "x": 1248,
                    "y": 3173
                }
            ],
            "category": "footer",
            "html": "<footer id='421' style='font-size:18px'>41</footer>",
            "id": 421,
            "page": 41,
            "text": "41"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 112
                },
                {
                    "x": 1099,
                    "y": 112
                },
                {
                    "x": 1099,
                    "y": 159
                },
                {
                    "x": 445,
                    "y": 159
                }
            ],
            "category": "header",
            "html": "<header id='422' style='font-size:20px'>Large Language Models as Optimizers</header>",
            "id": 422,
            "page": 42,
            "text": "Large Language Models as Optimizers"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 370
                },
                {
                    "x": 2110,
                    "y": 370
                },
                {
                    "x": 2110,
                    "y": 470
                },
                {
                    "x": 441,
                    "y": 470
                }
            ],
            "category": "caption",
            "html": "<caption id='423' style='font-size:16px'>Table 15: BBH task-wise Q begin instructions found by prompt optimization with the PaLM 2-L<br>scorer and the gpt - 3 · 5-turbo optimizer. The optimizations start from \"Let's solve the problem\".</caption>",
            "id": 423,
            "page": 42,
            "text": "Table 15: BBH task-wise Q begin instructions found by prompt optimization with the PaLM 2-L scorer and the gpt - 3 · 5-turbo optimizer. The optimizations start from \"Let's solve the problem\"."
        },
        {
            "bounding_box": [
                {
                    "x": 1384,
                    "y": 480
                },
                {
                    "x": 1577,
                    "y": 480
                },
                {
                    "x": 1577,
                    "y": 516
                },
                {
                    "x": 1384,
                    "y": 516
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='424' style='font-size:14px'>Our Instruction</p>",
            "id": 424,
            "page": 42,
            "text": "Our Instruction"
        },
        {
            "bounding_box": [
                {
                    "x": 476,
                    "y": 496
                },
                {
                    "x": 2189,
                    "y": 496
                },
                {
                    "x": 2189,
                    "y": 2809
                },
                {
                    "x": 476,
                    "y": 2809
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='425' style='font-size:14px'>Task<br>boolean_expressions Let's accurately assess the given conditions and determine their corresponding Boolean values.<br>causal_judgement Let's conduct a meticulous evaluation of the given scenarios, accurately determine the causal relationships, and provide<br>definitive answers through comprehensive analysis, ensuring a precise understanding of causation and a thorough<br>determination of events in each situation.<br>date_understanding Let's accurately determine the correct date based on the given information and select the corresponding option in the<br>standard MM/DD/YYYY format with utmost precision and reliability, ensuring the most definitive and reliable solution<br>possible for accurate representation in all scenarios without any room for ambiguity, error, or confusion, and providing<br>the highest level of accuracy and reliability.<br>disambiguation_qa Let's thoroughly analyze the given sentences to accurately determine the unambiguous antecedents of the pronouns<br>used, ensuring clear understanding, effective communication, and leaving no room for any confusion or ambiguity.<br>dyck_ languages Let's find the correct closing parentheses and brackets for the given sequences.<br>formal fallacies Let's thoroughly analyze the explicitly stated premises and draw definitive conclusions to accurately determine the<br>deductive validity of the arguments provided in each question, employing precise and logical reasoning in our<br>assessments for unwavering confidence in our determinations.<br>geometric_shapes Let's accurately determine the shape represented by the given SVG path element by carefully analyzing its path data<br>and considering all available options for a precise identification.<br>hyperbaton Let's quickly identify the correct adjective order.<br>logical_ deduction Let's methodically analyze the given information, employ logical reasoning, thoroughly evaluate all relevant details, and<br>_seven_objects accurately determine the solutions for each problem by considering all provided options comprehensively and<br>strategically, ensuring an efficient and effective approach towards arriving at the correct answers.<br>movie_recommendation Let's uncover the perfect movie recommendation from the options provided, ensuring an exceptional cinematic<br>experience together as we select the most captivating and satisfying choice that will keep us thoroughly engaged and<br>immersed until the very end.<br>multistep_arithmetic_two Let's tackle the following calculations.<br>navigate Let's accurately and efficiently determine the correct solution for each given scenario, ensuring the highest level of<br>precision, reliability, and consistency throughout.<br>object_counting Let's determine the total count of various items/objects/ingredients/animals mentioned in order to accurately and<br>efficiently find the answer.<br>penguins_in_a_table Let's analyze the given information and determine the correct answer.<br>reasoning_ about Let's systematically analyze the given information and carefully evaluate each answer choice to confidently determine<br>colored_ objects the accurate and optimal solutions, considering all available options and specific details provided in each question for<br>precise and concise responses, ensuring complete accuracy and clarity in our answers.<br>ruin _names Prepare to have a side-splittingly funny time as we uncover the most clever and hilarious alternatives for these artist or<br>movie names, challenging your wit to guess the correct one with a burst of creativity, humor, and imaginative twists!<br>salient_translation Let's meticulously analyze the provided translations, accurately identifying any errors or discrepancies, and conduct a<br>_error_ detection comprehensive evaluation to ensure the highest level of translation quality and fidelity. By considering contextual<br>nuances, cultural references, linguistic conventions, potential factual errors, and any dropped content, our ultimate aim<br>is to achieve precise and thorough assessments for optimal translation accuracy and adherence to the source text.<br>snarks Let's expertly determine the sarcastic statement among the given options and confidently provide the definitive answer<br>without any room for doubt or confusion, ensuring absolute precision, clarity, and unwavering expertise in our response,<br>while carefully analyzing the context, tone, and intention behind each statement to achieve unrivaled accuracy and<br>unwavering confidence.<br>sports_understanding Let's find the accurate information.<br>temporal_sequences The flawless approach<br>tracking_shuffled_ objects By meticulously analyzing the given scenarios and accurately determining the final outcomes through a series of trades,<br>_seven_objects swaps, and exchanges among the individuals involved, let's ascertain the conclusive results.<br>web_ of_ lies Let's scrutinize each statement provided to accurately determine the truth-teller and uncover the veracity behind their<br>words with unwavering analysis.<br>word_sorting Employing efficient and precise measures, sort the given list of words in alphabetical order to provide an optimal<br>solution for any sorting problem, ensuring maximum performance and effectiveness.</p>",
            "id": 425,
            "page": 42,
            "text": "Task boolean_expressions Let's accurately assess the given conditions and determine their corresponding Boolean values. causal_judgement Let's conduct a meticulous evaluation of the given scenarios, accurately determine the causal relationships, and provide definitive answers through comprehensive analysis, ensuring a precise understanding of causation and a thorough determination of events in each situation. date_understanding Let's accurately determine the correct date based on the given information and select the corresponding option in the standard MM/DD/YYYY format with utmost precision and reliability, ensuring the most definitive and reliable solution possible for accurate representation in all scenarios without any room for ambiguity, error, or confusion, and providing the highest level of accuracy and reliability. disambiguation_qa Let's thoroughly analyze the given sentences to accurately determine the unambiguous antecedents of the pronouns used, ensuring clear understanding, effective communication, and leaving no room for any confusion or ambiguity. dyck_ languages Let's find the correct closing parentheses and brackets for the given sequences. formal fallacies Let's thoroughly analyze the explicitly stated premises and draw definitive conclusions to accurately determine the deductive validity of the arguments provided in each question, employing precise and logical reasoning in our assessments for unwavering confidence in our determinations. geometric_shapes Let's accurately determine the shape represented by the given SVG path element by carefully analyzing its path data and considering all available options for a precise identification. hyperbaton Let's quickly identify the correct adjective order. logical_ deduction Let's methodically analyze the given information, employ logical reasoning, thoroughly evaluate all relevant details, and _seven_objects accurately determine the solutions for each problem by considering all provided options comprehensively and strategically, ensuring an efficient and effective approach towards arriving at the correct answers. movie_recommendation Let's uncover the perfect movie recommendation from the options provided, ensuring an exceptional cinematic experience together as we select the most captivating and satisfying choice that will keep us thoroughly engaged and immersed until the very end. multistep_arithmetic_two Let's tackle the following calculations. navigate Let's accurately and efficiently determine the correct solution for each given scenario, ensuring the highest level of precision, reliability, and consistency throughout. object_counting Let's determine the total count of various items/objects/ingredients/animals mentioned in order to accurately and efficiently find the answer. penguins_in_a_table Let's analyze the given information and determine the correct answer. reasoning_ about Let's systematically analyze the given information and carefully evaluate each answer choice to confidently determine colored_ objects the accurate and optimal solutions, considering all available options and specific details provided in each question for precise and concise responses, ensuring complete accuracy and clarity in our answers. ruin _names Prepare to have a side-splittingly funny time as we uncover the most clever and hilarious alternatives for these artist or movie names, challenging your wit to guess the correct one with a burst of creativity, humor, and imaginative twists! salient_translation Let's meticulously analyze the provided translations, accurately identifying any errors or discrepancies, and conduct a _error_ detection comprehensive evaluation to ensure the highest level of translation quality and fidelity. By considering contextual nuances, cultural references, linguistic conventions, potential factual errors, and any dropped content, our ultimate aim is to achieve precise and thorough assessments for optimal translation accuracy and adherence to the source text. snarks Let's expertly determine the sarcastic statement among the given options and confidently provide the definitive answer without any room for doubt or confusion, ensuring absolute precision, clarity, and unwavering expertise in our response, while carefully analyzing the context, tone, and intention behind each statement to achieve unrivaled accuracy and unwavering confidence. sports_understanding Let's find the accurate information. temporal_sequences The flawless approach tracking_shuffled_ objects By meticulously analyzing the given scenarios and accurately determining the final outcomes through a series of trades, _seven_objects swaps, and exchanges among the individuals involved, let's ascertain the conclusive results. web_ of_ lies Let's scrutinize each statement provided to accurately determine the truth-teller and uncover the veracity behind their words with unwavering analysis. word_sorting Employing efficient and precise measures, sort the given list of words in alphabetical order to provide an optimal solution for any sorting problem, ensuring maximum performance and effectiveness."
        },
        {
            "bounding_box": [
                {
                    "x": 1249,
                    "y": 3132
                },
                {
                    "x": 1297,
                    "y": 3132
                },
                {
                    "x": 1297,
                    "y": 3170
                },
                {
                    "x": 1249,
                    "y": 3170
                }
            ],
            "category": "footer",
            "html": "<footer id='426' style='font-size:18px'>42</footer>",
            "id": 426,
            "page": 42,
            "text": "42"
        }
    ]
}