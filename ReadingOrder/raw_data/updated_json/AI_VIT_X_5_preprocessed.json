{
    "id": "3296d5d6-0f94-11ef-8828-426932df3dcf",
    "pdf_path": "/root/data/pdf/2309.05653v3.pdf",
    "elements": [
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 111
                },
                {
                    "x": 899,
                    "y": 111
                },
                {
                    "x": 899,
                    "y": 159
                },
                {
                    "x": 444,
                    "y": 159
                }
            ],
            "category": "header",
            "html": "<header id='0' style='font-size:18px'>Preprint. Work in Progress</header>",
            "id": 0,
            "page": 1,
            "text": "Preprint. Work in Progress"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 329
                },
                {
                    "x": 2106,
                    "y": 329
                },
                {
                    "x": 2106,
                    "y": 486
                },
                {
                    "x": 442,
                    "y": 486
                }
            ],
            "category": "paragraph",
            "html": "<p id='1' style='font-size:22px'>MAMMOTH: BUILDING MATH GENERALIST MODELS<br>THROUGH HYBRID INSTRUCTION TUNING</p>",
            "id": 1,
            "page": 1,
            "text": "MAMMOTH: BUILDING MATH GENERALIST MODELS THROUGH HYBRID INSTRUCTION TUNING"
        },
        {
            "bounding_box": [
                {
                    "x": 478,
                    "y": 563
                },
                {
                    "x": 1670,
                    "y": 563
                },
                {
                    "x": 1670,
                    "y": 661
                },
                {
                    "x": 478,
                    "y": 661
                }
            ],
            "category": "paragraph",
            "html": "<p id='2' style='font-size:20px'>* Xiang Yue* *Xingwei Qu, †Ge Zhang, °Yao Fu, §Wenhao Huang,<br>*Huan Sun, *Yu Su, †Wenhu Chen*</p>",
            "id": 2,
            "page": 1,
            "text": "* Xiang Yue* *Xingwei Qu, †Ge Zhang, °Yao Fu, §Wenhao Huang, *Huan Sun, *Yu Su, †Wenhu Chen*"
        },
        {
            "bounding_box": [
                {
                    "x": 470,
                    "y": 663
                },
                {
                    "x": 2100,
                    "y": 663
                },
                {
                    "x": 2100,
                    "y": 758
                },
                {
                    "x": 470,
                    "y": 758
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='3' style='font-size:18px'>+University of Waterloo, *The Ohio State University, *HKUST, °University of Edinburgh, §01.AI<br>yue · 149@osu · edu, wenhuchen @uwaterloo · ca</p>",
            "id": 3,
            "page": 1,
            "text": "+University of Waterloo, *The Ohio State University, *HKUST, °University of Edinburgh, §01.AI yue · 149@osu · edu, wenhuchen @uwaterloo · ca"
        },
        {
            "bounding_box": [
                {
                    "x": 780,
                    "y": 826
                },
                {
                    "x": 1764,
                    "y": 826
                },
                {
                    "x": 1764,
                    "y": 868
                },
                {
                    "x": 780,
                    "y": 868
                }
            ],
            "category": "paragraph",
            "html": "<p id='4' style='font-size:16px'>https : / /tiger-ai-lab · github · io /MAmmo TH/</p>",
            "id": 4,
            "page": 1,
            "text": "https : / /tiger-ai-lab · github · io /MAmmo TH/"
        },
        {
            "bounding_box": [
                {
                    "x": 1157,
                    "y": 941
                },
                {
                    "x": 1396,
                    "y": 941
                },
                {
                    "x": 1396,
                    "y": 991
                },
                {
                    "x": 1157,
                    "y": 991
                }
            ],
            "category": "paragraph",
            "html": "<p id='5' style='font-size:20px'>ABSTRACT</p>",
            "id": 5,
            "page": 1,
            "text": "ABSTRACT"
        },
        {
            "bounding_box": [
                {
                    "x": 592,
                    "y": 1040
                },
                {
                    "x": 1960,
                    "y": 1040
                },
                {
                    "x": 1960,
                    "y": 1782
                },
                {
                    "x": 592,
                    "y": 1782
                }
            ],
            "category": "paragraph",
            "html": "<p id='6' style='font-size:18px'>We introduce MAmmo TH, a series of open-source large language models (LLMs)<br>specifically tailored for general math problem-solving. The MAmmoTH models are<br>trained on MathInstruct, our meticulously curated instruction tuning dataset.<br>MathInstruct is compiled from 13 math datasets with intermediate rationales,<br>six of which have rationales newly curated by us. It presents a unique hybrid<br>of chain-of-thought (CoT) and program-of-thought (PoT) rationales, and also en-<br>sures extensive coverage of diverse fields in math. The hybrid of CoT and PoT not<br>only unleashes the potential of tool use but also allows different thought processes<br>for different math problems. As a result, the MAmmo TH series substantially outper-<br>form existing open-source models on nine mathematical reasoning datasets across<br>all scales with an average accuracy gain between 16% and 32%. Remarkably,<br>our MAmmoTH-7B model reaches 33% on MATH (a competition-level dataset),<br>which exceeds the best open-source 7B model (WizardMath) by 23%, and the<br>MAmmoTH-34B model achieves 44% accuracy on MATH, even surpassing GPT-<br>4's CoT result. Our work underscores the importance of diverse problem coverage<br>and the use of hybrid rationales in developing superior math generalist models.</p>",
            "id": 6,
            "page": 1,
            "text": "We introduce MAmmo TH, a series of open-source large language models (LLMs) specifically tailored for general math problem-solving. The MAmmoTH models are trained on MathInstruct, our meticulously curated instruction tuning dataset. MathInstruct is compiled from 13 math datasets with intermediate rationales, six of which have rationales newly curated by us. It presents a unique hybrid of chain-of-thought (CoT) and program-of-thought (PoT) rationales, and also ensures extensive coverage of diverse fields in math. The hybrid of CoT and PoT not only unleashes the potential of tool use but also allows different thought processes for different math problems. As a result, the MAmmo TH series substantially outperform existing open-source models on nine mathematical reasoning datasets across all scales with an average accuracy gain between 16% and 32%. Remarkably, our MAmmoTH-7B model reaches 33% on MATH (a competition-level dataset), which exceeds the best open-source 7B model (WizardMath) by 23%, and the MAmmoTH-34B model achieves 44% accuracy on MATH, even surpassing GPT4's CoT result. Our work underscores the importance of diverse problem coverage and the use of hybrid rationales in developing superior math generalist models."
        },
        {
            "bounding_box": [
                {
                    "x": 446,
                    "y": 1876
                },
                {
                    "x": 2115,
                    "y": 1876
                },
                {
                    "x": 2115,
                    "y": 2764
                },
                {
                    "x": 446,
                    "y": 2764
                }
            ],
            "category": "figure",
            "html": "<figure><img id='7' style='font-size:14px' alt=\"MathInstruct Hybrid Instruction Tuning\nChain-of-Thought (CoT)\nWeng earns $12 an hour for babysitting. = 0.2 per minute.\nWeng earns 12/60\nYesterday, she just did 50 minutes of\nDoing 50 mins, she earned 0.2 x 50 = 10\nbabysitting. How much did she earn?\nProgram-of-Thought (PoT)\n2 hourly_rate = 12; time_ worked = 50/60;\n(X+ y1\nearnings = hourly_rate * time_worked;\nMAmmoTH print(round(earnings, 2))\nDiverse Math Problems\nIn-domain datasets Out-of-domain datasets\n80 80 (Ours)\n■ Base ■SoTA ■ MAmmoTH (Ours) ■ Base ■ SoTA ■ MAmmoTH\n+16 +32 +21\n+28\n+26\n+19 60 +24\n60 +20\n(%)\n(%)\nAccuracy\n40 40\n20\n20 Accuracy\n0 0\n7B 13B 30B 70B 7B 13B 30B 70B\" data-coord=\"top-left:(446,1876); bottom-right:(2115,2764)\" /></figure>",
            "id": 7,
            "page": 1,
            "text": "MathInstruct Hybrid Instruction Tuning Chain-of-Thought (CoT) Weng earns $12 an hour for babysitting. = 0.2 per minute. Weng earns 12/60 Yesterday, she just did 50 minutes of Doing 50 mins, she earned 0.2 x 50 = 10 babysitting. How much did she earn? Program-of-Thought (PoT) 2 hourly_rate = 12; time_ worked = 50/60; (X+ y1 earnings = hourly_rate * time_worked; MAmmoTH print(round(earnings, 2)) Diverse Math Problems In-domain datasets Out-of-domain datasets 80 80 (Ours) ■ Base ■SoTA ■ MAmmoTH (Ours) ■ Base ■ SoTA ■ MAmmoTH +16 +32 +21 +28 +26 +19 60 +24 60 +20 (%) (%) Accuracy 40 40 20 20 Accuracy 0 0 7B 13B 30B 70B 7B 13B 30B 70B"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 2812
                },
                {
                    "x": 2108,
                    "y": 2812
                },
                {
                    "x": 2108,
                    "y": 2954
                },
                {
                    "x": 440,
                    "y": 2954
                }
            ],
            "category": "paragraph",
            "html": "<p id='8' style='font-size:18px'>Figure 1: The superior performance of MAmmo TH, a series of models instruction-tuned to solve a<br>diverse set of mathematical problems using hybrid CoT and PoT rationales. MAmmo TH significantly<br>outperforms base and SoTA models on both in-domain and out-of-domain test sets, across all scales.</p>",
            "id": 8,
            "page": 1,
            "text": "Figure 1: The superior performance of MAmmo TH, a series of models instruction-tuned to solve a diverse set of mathematical problems using hybrid CoT and PoT rationales. MAmmo TH significantly outperforms base and SoTA models on both in-domain and out-of-domain test sets, across all scales."
        },
        {
            "bounding_box": [
                {
                    "x": 495,
                    "y": 3006
                },
                {
                    "x": 2099,
                    "y": 3006
                },
                {
                    "x": 2099,
                    "y": 3053
                },
                {
                    "x": 495,
                    "y": 3053
                }
            ],
            "category": "paragraph",
            "html": "<p id='9' style='font-size:16px'>* Xiang Yue and Wenhu Chen are the leading authors of the paper. They contributed equally to this project.</p>",
            "id": 9,
            "page": 1,
            "text": "* Xiang Yue and Wenhu Chen are the leading authors of the paper. They contributed equally to this project."
        },
        {
            "bounding_box": [
                {
                    "x": 65,
                    "y": 900
                },
                {
                    "x": 147,
                    "y": 900
                },
                {
                    "x": 147,
                    "y": 2321
                },
                {
                    "x": 65,
                    "y": 2321
                }
            ],
            "category": "footer",
            "html": "<br><footer id='10' style='font-size:14px'>2023<br>Oct<br>3<br>[cs.CL]<br>arXiv:2309.05653v3</footer>",
            "id": 10,
            "page": 1,
            "text": "2023 Oct 3 [cs.CL] arXiv:2309.05653v3"
        },
        {
            "bounding_box": [
                {
                    "x": 1261,
                    "y": 3133
                },
                {
                    "x": 1289,
                    "y": 3133
                },
                {
                    "x": 1289,
                    "y": 3172
                },
                {
                    "x": 1261,
                    "y": 3172
                }
            ],
            "category": "footer",
            "html": "<footer id='11' style='font-size:16px'>1</footer>",
            "id": 11,
            "page": 1,
            "text": "1"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 112
                },
                {
                    "x": 900,
                    "y": 112
                },
                {
                    "x": 900,
                    "y": 159
                },
                {
                    "x": 443,
                    "y": 159
                }
            ],
            "category": "header",
            "html": "<header id='12' style='font-size:22px'>Preprint. Work in Progress</header>",
            "id": 12,
            "page": 2,
            "text": "Preprint. Work in Progress"
        },
        {
            "bounding_box": [
                {
                    "x": 449,
                    "y": 342
                },
                {
                    "x": 863,
                    "y": 342
                },
                {
                    "x": 863,
                    "y": 392
                },
                {
                    "x": 449,
                    "y": 392
                }
            ],
            "category": "paragraph",
            "html": "<p id='13' style='font-size:20px'>1 INTRODUCTION</p>",
            "id": 13,
            "page": 2,
            "text": "1 INTRODUCTION"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 457
                },
                {
                    "x": 2108,
                    "y": 457
                },
                {
                    "x": 2108,
                    "y": 782
                },
                {
                    "x": 442,
                    "y": 782
                }
            ],
            "category": "paragraph",
            "html": "<p id='14' style='font-size:16px'>This work focuses on mathematical reasoning, a critical capability of modern large language models<br>(LLMs) (OpenAI, 2023; Anil et al., 2023). Despite the recent advances in this field, a noticeable gap<br>exists between closed-source and open-source LLMs-closed-source models like GPT-4 (OpenAI,<br>2023), PaLM-2 (Anil et al., 2023), and Claude 2 (Bai et al., 2022) dominate popular mathemat-<br>ical reasoning benchmarks such as GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al.,<br>2021b), while open-source models like Llama (Touvron et al., 2023a;b), Falcon (Penedo et al.,<br>2023), OPT (Zhang et al., 2022) lag behind on all benchmarks by a wide margin.</p>",
            "id": 14,
            "page": 2,
            "text": "This work focuses on mathematical reasoning, a critical capability of modern large language models (LLMs) (OpenAI, 2023; Anil , 2023). Despite the recent advances in this field, a noticeable gap exists between closed-source and open-source LLMs-closed-source models like GPT-4 (OpenAI, 2023), PaLM-2 (Anil , 2023), and Claude 2 (Bai , 2022) dominate popular mathematical reasoning benchmarks such as GSM8K (Cobbe , 2021) and MATH (Hendrycks , 2021b), while open-source models like Llama (Touvron , 2023a;b), Falcon (Penedo , 2023), OPT (Zhang , 2022) lag behind on all benchmarks by a wide margin."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 806
                },
                {
                    "x": 2108,
                    "y": 806
                },
                {
                    "x": 2108,
                    "y": 1264
                },
                {
                    "x": 441,
                    "y": 1264
                }
            ],
            "category": "paragraph",
            "html": "<p id='15' style='font-size:16px'>Current efforts to bridge this gap are twofold: (1) Continued pre-training like Galactica (Taylor et al.,<br>2022) and MINERVA (Lewkowycz et al., 2022), which continues to train an LLM on math-related<br>web data of more than 100B tokens. This approach improves a model's general scientific reasoning<br>capability but incurs a high computation cost. (2) Dataset-specific fine-tuning like rejection sampling<br>fine-tuning (RFT) (Yuan et al., 2023) and WizardMath (Luo et al., 2023), which fine-tunes LLMs<br>using supervised data specific to certain datasets. Although such approaches improve in-domain<br>performance, they cannot generalize to a wider range of math reasoning tasks beyond their fine-<br>tuning data. For instance, both RFT and WizardMath can increase the accuracy on GSM8K (Cobbe<br>et al., 2021) by 30%+, one of their fine-tuning datasets, but hurt the accuracy on out-of-domain<br>datasets like MMLU-Math (Hendrycks et al., 2021a) or AQuA (Ling et al., 2017) by up to 10%.</p>",
            "id": 15,
            "page": 2,
            "text": "Current efforts to bridge this gap are twofold: (1) Continued pre-training like Galactica (Taylor , 2022) and MINERVA (Lewkowycz , 2022), which continues to train an LLM on math-related web data of more than 100B tokens. This approach improves a model's general scientific reasoning capability but incurs a high computation cost. (2) Dataset-specific fine-tuning like rejection sampling fine-tuning (RFT) (Yuan , 2023) and WizardMath (Luo , 2023), which fine-tunes LLMs using supervised data specific to certain datasets. Although such approaches improve in-domain performance, they cannot generalize to a wider range of math reasoning tasks beyond their finetuning data. For instance, both RFT and WizardMath can increase the accuracy on GSM8K (Cobbe , 2021) by 30%+, one of their fine-tuning datasets, but hurt the accuracy on out-of-domain datasets like MMLU-Math (Hendrycks , 2021a) or AQuA (Ling , 2017) by up to 10%."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1286
                },
                {
                    "x": 2108,
                    "y": 1286
                },
                {
                    "x": 2108,
                    "y": 1931
                },
                {
                    "x": 441,
                    "y": 1931
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='16' style='font-size:18px'>In this paper, we aim to propose a lightweight yet generalizable math instruction-tuning approach to<br>enhance the general (i.e., not limited to the fine-tuning tasks) mathematical reasoning capabilities of<br>LLMs. Existing methods (Luo et al., 2023; Yuan et al., 2023; Taylor et al., 2022) primarily focus on<br>Chain-of- Thought (CoT) approaches (Wei et al., 2022b; Nye et al., 2022) to solve math problems<br>through step-by-step natural language descriptions. This approach excels in its generality to cover<br>most math subjects but struggles with computation precision, and complex mathematical or algorith-<br>mic reasoning procedures (e.g., solving quadratic equation roots and calculating matrix eigenvalues).<br>In contrast, prompts in the format of code like Program-of- Thought (PoT) approaches (Chen et al.,<br>2022) and PAL (Madaan et al., 2022; Gao et al., 2023) utilize external tools (i.e., Python interpreter)<br>to greatly simplify the math solving process. This approach advocates offloading the computation<br>process to the external Python interpreter to solve complex mathematical and algorithmic reason-<br>ing procedures (e.g., solving quadratic equations with sympy or calculating matrix eigenvalues with<br>numpy). However, PoT falls short in dealing with more abstract reasoning scenarios, like common-<br>sense reasoning, formal logic, and abstract algebra, especially when there exist no built-in APIs.</p>",
            "id": 16,
            "page": 2,
            "text": "In this paper, we aim to propose a lightweight yet generalizable math instruction-tuning approach to enhance the general (i.e., not limited to the fine-tuning tasks) mathematical reasoning capabilities of LLMs. Existing methods (Luo , 2023; Yuan , 2023; Taylor , 2022) primarily focus on Chain-of- Thought (CoT) approaches (Wei , 2022b; Nye , 2022) to solve math problems through step-by-step natural language descriptions. This approach excels in its generality to cover most math subjects but struggles with computation precision, and complex mathematical or algorithmic reasoning procedures (e.g., solving quadratic equation roots and calculating matrix eigenvalues). In contrast, prompts in the format of code like Program-of- Thought (PoT) approaches (Chen , 2022) and PAL (Madaan , 2022; Gao , 2023) utilize external tools (i.e., Python interpreter) to greatly simplify the math solving process. This approach advocates offloading the computation process to the external Python interpreter to solve complex mathematical and algorithmic reasoning procedures (e.g., solving quadratic equations with sympy or calculating matrix eigenvalues with numpy). However, PoT falls short in dealing with more abstract reasoning scenarios, like commonsense reasoning, formal logic, and abstract algebra, especially when there exist no built-in APIs."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1950
                },
                {
                    "x": 2106,
                    "y": 1950
                },
                {
                    "x": 2106,
                    "y": 2274
                },
                {
                    "x": 441,
                    "y": 2274
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='17' style='font-size:14px'>To leverage the strengths of both CoT and PoT approaches, we introduce a new math hybrid<br>instruction-tuning dataset MathInst ruct, which has two main characteristics: (1) broad COV-<br>erage of different math fields and complexity levels, and (2) hybrid CoT & PoT rationales.<br>MathInstruct is based on seven existing math rationale datasets and six newly-curated datasets<br>(see details in Table 1). We use MathInst ruct to fine-tune Llama (Touvron et al., 2023a;b;<br>Roziere et al., 2023) models of different scales ranging from 7B to 70B. The resulting MAmmo TH<br>models ( Figure 1) demonstrate unprecedented potential in serving as math generalists.</p>",
            "id": 17,
            "page": 2,
            "text": "To leverage the strengths of both CoT and PoT approaches, we introduce a new math hybrid instruction-tuning dataset MathInst ruct, which has two main characteristics: (1) broad COVerage of different math fields and complexity levels, and (2) hybrid CoT & PoT rationales. MathInstruct is based on seven existing math rationale datasets and six newly-curated datasets (see details in Table 1). We use MathInst ruct to fine-tune Llama (Touvron , 2023a;b; Roziere , 2023) models of different scales ranging from 7B to 70B. The resulting MAmmo TH models ( Figure 1) demonstrate unprecedented potential in serving as math generalists."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2295
                },
                {
                    "x": 2108,
                    "y": 2295
                },
                {
                    "x": 2108,
                    "y": 2755
                },
                {
                    "x": 441,
                    "y": 2755
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='18' style='font-size:16px'>We evaluate MAmmoTH on a spectrum of datasets, including in-domain (IND) test sets-<br>GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2021b), AQuA-RAT (Ling et al., 2017),<br>NumGLUE (Mishra et al., 2022b)-and out-of-domain (OOD) test sets-SVAMP (Patel et al.,<br>2021), SAT (Zhong et al., 2023), MMLU-Math (Hendrycks et al., 2021a), Mathematics (Davies<br>et al., 2021), and SimulEq (Koncel-Kedziorski et al., 2016). Compared with existing meth-<br>ods, our models generalize better to OOD datasets and substantially improve the performance<br>of open-source LLMs in mathematical reasoning. Notably, on the popular competition-level<br>MATH dataset (Hendrycks et al., 2021b), our 7B model can beat WizardMath (open-source MATH<br>SoTA) (Luo et al., 2023) by 3.5x (35.2% VS 10.7%), and our 34B MAmmo TH-Coder (fine-tuned on<br>Code Llama (Roziere et al., 2023)) can even beat the result of GPT-4 (using CoT).</p>",
            "id": 18,
            "page": 2,
            "text": "We evaluate MAmmoTH on a spectrum of datasets, including in-domain (IND) test setsGSM8K (Cobbe , 2021), MATH (Hendrycks , 2021b), AQuA-RAT (Ling , 2017), NumGLUE (Mishra , 2022b)-and out-of-domain (OOD) test sets-SVAMP (Patel , 2021), SAT (Zhong , 2023), MMLU-Math (Hendrycks , 2021a), Mathematics (Davies , 2021), and SimulEq (Koncel-Kedziorski , 2016). Compared with existing methods, our models generalize better to OOD datasets and substantially improve the performance of open-source LLMs in mathematical reasoning. Notably, on the popular competition-level MATH dataset (Hendrycks , 2021b), our 7B model can beat WizardMath (open-source MATH SoTA) (Luo , 2023) by 3.5x (35.2% VS 10.7%), and our 34B MAmmo TH-Coder (fine-tuned on Code Llama (Roziere , 2023)) can even beat the result of GPT-4 (using CoT)."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2776
                },
                {
                    "x": 2108,
                    "y": 2776
                },
                {
                    "x": 2108,
                    "y": 3055
                },
                {
                    "x": 441,
                    "y": 3055
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='19' style='font-size:16px'>We highlight our contributions from two perspectives: (1) From the data engineering perspective,<br>we present Math Instruct, a high-quality math instruction tuning dataset, combining a vari-<br>ety of math problems and hybrid rationales. (2) From the modeling perspective, we investigate<br>the impact of various data sources and input-output formats through training and evaluating over<br>50 different models and baselines ranging from 7B to 70B. Our models, including MAmmo TH and<br>MAmmo TH-Coder, achieve substantial accuracy gains over existing open-source models.</p>",
            "id": 19,
            "page": 2,
            "text": "We highlight our contributions from two perspectives: (1) From the data engineering perspective, we present Math Instruct, a high-quality math instruction tuning dataset, combining a variety of math problems and hybrid rationales. (2) From the modeling perspective, we investigate the impact of various data sources and input-output formats through training and evaluating over 50 different models and baselines ranging from 7B to 70B. Our models, including MAmmo TH and MAmmo TH-Coder, achieve substantial accuracy gains over existing open-source models."
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3134
                },
                {
                    "x": 1289,
                    "y": 3134
                },
                {
                    "x": 1289,
                    "y": 3171
                },
                {
                    "x": 1260,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='20' style='font-size:14px'>2</footer>",
            "id": 20,
            "page": 2,
            "text": "2"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 112
                },
                {
                    "x": 899,
                    "y": 112
                },
                {
                    "x": 899,
                    "y": 159
                },
                {
                    "x": 444,
                    "y": 159
                }
            ],
            "category": "header",
            "html": "<header id='21' style='font-size:22px'>Preprint. Work in Progress</header>",
            "id": 21,
            "page": 3,
            "text": "Preprint. Work in Progress"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 336
                },
                {
                    "x": 2100,
                    "y": 336
                },
                {
                    "x": 2100,
                    "y": 1053
                },
                {
                    "x": 440,
                    "y": 1053
                }
            ],
            "category": "table",
            "html": "<table id='22' style='font-size:14px'><tr><td>Training Dataset</td><td>Type</td><td>Annotation</td><td># Samples</td><td>Characteristics</td><td>Fields</td></tr><tr><td>GSM8K (Cobbe et al., 2021)</td><td>CoT</td><td>Human</td><td>7K</td><td>Grade Schol Exam</td><td></td></tr><tr><td>GSM8K-RFT (Yuan et al., 2023)</td><td>CoT</td><td>Llama</td><td>28K</td><td>Llama + Validated</td><td></td></tr><tr><td>AQuA-RAT (Ling et al., 2017)</td><td>CoT</td><td>Human</td><td>90K</td><td>GRE/GMAT Exam</td><td></td></tr><tr><td>MATH (Hendrycks et al., 2021b)</td><td>CoT</td><td>Human</td><td>7K</td><td>Math Competition</td><td></td></tr><tr><td>TheoremQA (Chen et al., 2023) ☆ Camel-Math</td><td>CoT</td><td>GPT-4</td><td>600</td><td>GPT4 + Validated</td><td></td></tr><tr><td>(Li et al., 2023a) College-Math ☆</td><td>CoT</td><td>GPT-4</td><td>50K</td><td>GPT4 (Unvalidated)</td><td></td></tr><tr><td>☆</td><td>CoT</td><td>GPT-4</td><td>1.8K</td><td>GPT4 (Unvalidated)</td><td></td></tr><tr><td>GSM8K AQuA-RAT ☆</td><td>PoT</td><td>GPT4</td><td>14K</td><td>GPT4 + Validated GPT4 + Validated</td><td></td></tr><tr><td>MATH ☆</td><td>PoT</td><td>GPT4</td><td>9.7K</td><td>GPT4 + Validated</td><td></td></tr><tr><td>TheoremQA ☆</td><td>PoT</td><td>GPT4</td><td>7K</td><td></td><td></td></tr><tr><td></td><td>PoT</td><td>GPT4</td><td>700</td><td>GPT4 + Validated</td><td></td></tr><tr><td>MathQA (Amini et al., 2019)</td><td>PoT</td><td>Human</td><td>25K</td><td>AQuA-RAT Subset</td><td></td></tr><tr><td>NumGLUE (Mishra et al., 2022a) MathInstruct</td><td>PoT</td><td>Human</td><td>13K 260K</td><td>Lila Annotated</td><td></td></tr></table>",
            "id": 22,
            "page": 3,
            "text": "Training Dataset Type Annotation # Samples Characteristics Fields  GSM8K (Cobbe , 2021) CoT Human 7K Grade Schol Exam   GSM8K-RFT (Yuan , 2023) CoT Llama 28K Llama + Validated   AQuA-RAT (Ling , 2017) CoT Human 90K GRE/GMAT Exam   MATH (Hendrycks , 2021b) CoT Human 7K Math Competition   TheoremQA (Chen , 2023) ☆ Camel-Math CoT GPT-4 600 GPT4 + Validated   (Li , 2023a) College-Math ☆ CoT GPT-4 50K GPT4 (Unvalidated)   ☆ CoT GPT-4 1.8K GPT4 (Unvalidated)   GSM8K AQuA-RAT ☆ PoT GPT4 14K GPT4 + Validated GPT4 + Validated   MATH ☆ PoT GPT4 9.7K GPT4 + Validated   TheoremQA ☆ PoT GPT4 7K     PoT GPT4 700 GPT4 + Validated   MathQA (Amini , 2019) PoT Human 25K AQuA-RAT Subset   NumGLUE (Mishra , 2022a) MathInstruct PoT Human 13K 260K Lila Annotated"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 1094
                },
                {
                    "x": 2107,
                    "y": 1094
                },
                {
                    "x": 2107,
                    "y": 1294
                },
                {
                    "x": 440,
                    "y": 1294
                }
            ],
            "category": "paragraph",
            "html": "<p id='23' style='font-size:20px'>Table 1: Overview of our MathInst ruct. ☆ means with NEW rationales curated by us by<br>prompting GPT-4. We have filtered out augmented samples that have answers inconsistent with the<br>original dataset's annotations. Different colored squares represent different fields in mathematics:<br>Pre-Algebra; Inter-Algebra; Algebra; Probability; ■ NumTheory; Calculus; Geometry.</p>",
            "id": 23,
            "page": 3,
            "text": "Table 1: Overview of our MathInst ruct. ☆ means with NEW rationales curated by us by prompting GPT-4. We have filtered out augmented samples that have answers inconsistent with the original dataset's annotations. Different colored squares represent different fields in mathematics: Pre-Algebra; Inter-Algebra; Algebra; Probability; ■ NumTheory; Calculus; Geometry."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 1394
                },
                {
                    "x": 878,
                    "y": 1394
                },
                {
                    "x": 878,
                    "y": 1447
                },
                {
                    "x": 445,
                    "y": 1447
                }
            ],
            "category": "paragraph",
            "html": "<p id='24' style='font-size:22px'>2 OUR APPROACH</p>",
            "id": 24,
            "page": 3,
            "text": "2 OUR APPROACH"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 1515
                },
                {
                    "x": 811,
                    "y": 1515
                },
                {
                    "x": 811,
                    "y": 1561
                },
                {
                    "x": 445,
                    "y": 1561
                }
            ],
            "category": "paragraph",
            "html": "<p id='25' style='font-size:18px'>2.1 BACKGROUND</p>",
            "id": 25,
            "page": 3,
            "text": "2.1 BACKGROUND"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1608
                },
                {
                    "x": 2108,
                    "y": 1608
                },
                {
                    "x": 2108,
                    "y": 2256
                },
                {
                    "x": 442,
                    "y": 2256
                }
            ],
            "category": "paragraph",
            "html": "<p id='26' style='font-size:20px'>Mathematical reasoning serves as a vital gauge for assessing the ability of LLMs to execute com-<br>plex multi-hop and quantitative reasoning. Previously, this has been a challenging task for neural<br>networks, which struggle to solve even basic addition and subtraction problems (Yang et al., 2023).<br>However, recent LLMs have considerable advancements in mathematical reasoning. Key break-<br>throughs have been made through CoT prompting (Wei et al., 2022b; Nye et al., 2022) and PoT<br>prompting (Chen et al., 2022; Gao et al., 2023). CoT prompting encourages LLMs to solve prob-<br>lems incrementally on a scratchpad, enhancing both accuracy and explainability in mathematical<br>reasoning. This approach contrasts with traditional methods that generate answers directly. PoT<br>prompting, on the other hand, formulates the intermediate reasoning process as a program, executed<br>with an external tool like Python, to compute the answer. This method improves robustness in solv-<br>ing complex mathematical problems by offloading the calculations to external tools. However, most<br>existing work (Zhou et al., 2023a) in PoT is limited to proprietary models like GPT-4 (OpenAI,<br>2023) and Codex (Chen et al., 2021). The PoT potential of open-source models is yet to be seen.<br>Our work aims at optimizing LLMs' CoT and PoT reasoning capabilities through instruction tuning.</p>",
            "id": 26,
            "page": 3,
            "text": "Mathematical reasoning serves as a vital gauge for assessing the ability of LLMs to execute complex multi-hop and quantitative reasoning. Previously, this has been a challenging task for neural networks, which struggle to solve even basic addition and subtraction problems (Yang , 2023). However, recent LLMs have considerable advancements in mathematical reasoning. Key breakthroughs have been made through CoT prompting (Wei , 2022b; Nye , 2022) and PoT prompting (Chen , 2022; Gao , 2023). CoT prompting encourages LLMs to solve problems incrementally on a scratchpad, enhancing both accuracy and explainability in mathematical reasoning. This approach contrasts with traditional methods that generate answers directly. PoT prompting, on the other hand, formulates the intermediate reasoning process as a program, executed with an external tool like Python, to compute the answer. This method improves robustness in solving complex mathematical problems by offloading the calculations to external tools. However, most existing work (Zhou , 2023a) in PoT is limited to proprietary models like GPT-4 (OpenAI, 2023) and Codex (Chen , 2021). The PoT potential of open-source models is yet to be seen. Our work aims at optimizing LLMs' CoT and PoT reasoning capabilities through instruction tuning."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2380
                },
                {
                    "x": 1778,
                    "y": 2380
                },
                {
                    "x": 1778,
                    "y": 2430
                },
                {
                    "x": 443,
                    "y": 2430
                }
            ],
            "category": "paragraph",
            "html": "<p id='27' style='font-size:16px'>2.2 CURATING A DIVERSE AND HYBRID INSTRUCTION TUNING DATASET</p>",
            "id": 27,
            "page": 3,
            "text": "2.2 CURATING A DIVERSE AND HYBRID INSTRUCTION TUNING DATASET"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2476
                },
                {
                    "x": 2107,
                    "y": 2476
                },
                {
                    "x": 2107,
                    "y": 2617
                },
                {
                    "x": 441,
                    "y": 2617
                }
            ],
            "category": "paragraph",
            "html": "<p id='28' style='font-size:20px'>Our study aims to compile a list of high-quality and diverse math instruction-tuning datasets, stand-<br>ing out with three main characteristics: (1) broad coverage of different mathematical fields and<br>complexity levels, and (2) hybrid CoT & PoT rationales.</p>",
            "id": 28,
            "page": 3,
            "text": "Our study aims to compile a list of high-quality and diverse math instruction-tuning datasets, standing out with three main characteristics: (1) broad coverage of different mathematical fields and complexity levels, and (2) hybrid CoT & PoT rationales."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2638
                },
                {
                    "x": 2107,
                    "y": 2638
                },
                {
                    "x": 2107,
                    "y": 3055
                },
                {
                    "x": 442,
                    "y": 3055
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='29' style='font-size:18px'>Broad Coverage of Different Math Fields and Complexity Levels: We aim for a broad repre-<br>sentation of math fields and complexity levels in our dataset. This ensures exposure to a diverse set<br>of mathematical knowledge, fostering versatility in our models. Based on these criteria, we narrow<br>down our choices to a few high-quality datasets that are widely adopted and encompass different<br>math fields and complexity levels, such as GSM8K, MATH, AQuA, Camel, and TheoremQA. Fur-<br>thermore, we notice a lack of coverage for college-level math knowledge, such as abstract algebra<br>and formal logic, in existing datasets. To rectify this, we use GPT-4 to synthesize CoT rationales for<br>questions in TheoremQA and create question-CoT pairs through Self-Instruct (Wang et al., 2023h),<br>utilizing a few seed exemplars found online.</p>",
            "id": 29,
            "page": 3,
            "text": "Broad Coverage of Different Math Fields and Complexity Levels: We aim for a broad representation of math fields and complexity levels in our dataset. This ensures exposure to a diverse set of mathematical knowledge, fostering versatility in our models. Based on these criteria, we narrow down our choices to a few high-quality datasets that are widely adopted and encompass different math fields and complexity levels, such as GSM8K, MATH, AQuA, Camel, and TheoremQA. Furthermore, we notice a lack of coverage for college-level math knowledge, such as abstract algebra and formal logic, in existing datasets. To rectify this, we use GPT-4 to synthesize CoT rationales for questions in TheoremQA and create question-CoT pairs through Self-Instruct (Wang , 2023h), utilizing a few seed exemplars found online."
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3132
                },
                {
                    "x": 1289,
                    "y": 3132
                },
                {
                    "x": 1289,
                    "y": 3172
                },
                {
                    "x": 1260,
                    "y": 3172
                }
            ],
            "category": "footer",
            "html": "<footer id='30' style='font-size:14px'>3</footer>",
            "id": 30,
            "page": 3,
            "text": "3"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 111
                },
                {
                    "x": 900,
                    "y": 111
                },
                {
                    "x": 900,
                    "y": 159
                },
                {
                    "x": 443,
                    "y": 159
                }
            ],
            "category": "header",
            "html": "<header id='31' style='font-size:20px'>Preprint. Work in Progress</header>",
            "id": 31,
            "page": 4,
            "text": "Preprint. Work in Progress"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 343
                },
                {
                    "x": 2109,
                    "y": 343
                },
                {
                    "x": 2109,
                    "y": 715
                },
                {
                    "x": 442,
                    "y": 715
                }
            ],
            "category": "paragraph",
            "html": "<p id='32' style='font-size:16px'>Hybrid CoT and PoT Rationales: Contrary to previous work (Yuan et al., 2023; Luo et al., 2023;<br>Lee et al., 2023; Wang et al., 2023g) that focus on CoT, our dataset strategically combines both.<br>This integration enhances the dataset's versatility, catering to varying mathematical problem-solving<br>approaches. However, most existing datasets provide limited program rationales, leading to an im-<br>balance between CoT and PoT rationales. To fill the gap, we utilize GPT-4 to supplement the PoT<br>rationales for selected datasets, including MATH, AQuA, GSM8K, and TheoremQA. We then fil-<br>ter these GPT-4 synthesized programs by comparing their executed results with human-annotated<br>ground truth, which ensures the high quality of the added rationales.</p>",
            "id": 32,
            "page": 4,
            "text": "Hybrid CoT and PoT Rationales: Contrary to previous work (Yuan , 2023; Luo , 2023; Lee , 2023; Wang , 2023g) that focus on CoT, our dataset strategically combines both. This integration enhances the dataset's versatility, catering to varying mathematical problem-solving approaches. However, most existing datasets provide limited program rationales, leading to an imbalance between CoT and PoT rationales. To fill the gap, we utilize GPT-4 to supplement the PoT rationales for selected datasets, including MATH, AQuA, GSM8K, and TheoremQA. We then filter these GPT-4 synthesized programs by comparing their executed results with human-annotated ground truth, which ensures the high quality of the added rationales."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 736
                },
                {
                    "x": 2108,
                    "y": 736
                },
                {
                    "x": 2108,
                    "y": 924
                },
                {
                    "x": 442,
                    "y": 924
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='33' style='font-size:18px'>Following these guidelines, our instruction dataset, detailed in Table 1, encompasses 260K (instruc-<br>tion, response) pairs, covering a wide range of core mathematical fields (arithmetic, algebra, proba-<br>bility, calculus, and geometry, etc.), including hybrid CoT and PoT rationales, and offering diversity<br>in both language and difficulty levels. This attests to its high quality and unique characteristics.</p>",
            "id": 33,
            "page": 4,
            "text": "Following these guidelines, our instruction dataset, detailed in Table 1, encompasses 260K (instruction, response) pairs, covering a wide range of core mathematical fields (arithmetic, algebra, probability, calculus, and geometry, etc.), including hybrid CoT and PoT rationales, and offering diversity in both language and difficulty levels. This attests to its high quality and unique characteristics."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 982
                },
                {
                    "x": 864,
                    "y": 982
                },
                {
                    "x": 864,
                    "y": 1030
                },
                {
                    "x": 445,
                    "y": 1030
                }
            ],
            "category": "paragraph",
            "html": "<p id='34' style='font-size:14px'>2.3 TRAINING SETUP</p>",
            "id": 34,
            "page": 4,
            "text": "2.3 TRAINING SETUP"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1070
                },
                {
                    "x": 2109,
                    "y": 1070
                },
                {
                    "x": 2109,
                    "y": 1533
                },
                {
                    "x": 442,
                    "y": 1533
                }
            ],
            "category": "paragraph",
            "html": "<p id='35' style='font-size:14px'>We unify all the subsets in our MathInst ruct to conform to the structure of an Alpaca-like<br>instruction dataset (Taori et al., 2023). This standardization ensures that the fine-tuned models can<br>process data consistently, regardless of the original dataset formats. We choose the open-source<br>models Llama-2 (Touvron et al., 2023b) and Code Llama (Roziere et al., 2023) as our base models.<br>We fine-tune these models including 7B, 13B, 34B, and 70B on MathInstruct, which allows us<br>to validate our Ma thInst ruct at multiple scales. We fine-tune all the models with Huggingface<br>transformers library (Wolf et al., 2019). We use a learning rate of 2e-5 for the 7B and 13B models,<br>and 1e-5 for the 34B and 70B models. We set the batch size at 128 and used a cosine scheduler with<br>a 3% warm-up period for three epochs. To efficiently train the computationally intensive 34B and<br>70B models, we employ DeepSpeed training with ZeRO-3 stage (Rajbhandari et al., 2020).</p>",
            "id": 35,
            "page": 4,
            "text": "We unify all the subsets in our MathInst ruct to conform to the structure of an Alpaca-like instruction dataset (Taori , 2023). This standardization ensures that the fine-tuned models can process data consistently, regardless of the original dataset formats. We choose the open-source models Llama-2 (Touvron , 2023b) and Code Llama (Roziere , 2023) as our base models. We fine-tune these models including 7B, 13B, 34B, and 70B on MathInstruct, which allows us to validate our Ma thInst ruct at multiple scales. We fine-tune all the models with Huggingface transformers library (Wolf , 2019). We use a learning rate of 2e-5 for the 7B and 13B models, and 1e-5 for the 34B and 70B models. We set the batch size at 128 and used a cosine scheduler with a 3% warm-up period for three epochs. To efficiently train the computationally intensive 34B and 70B models, we employ DeepSpeed training with ZeRO-3 stage (Rajbhandari , 2020)."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 1592
                },
                {
                    "x": 915,
                    "y": 1592
                },
                {
                    "x": 915,
                    "y": 1640
                },
                {
                    "x": 444,
                    "y": 1640
                }
            ],
            "category": "paragraph",
            "html": "<p id='36' style='font-size:16px'>2.4 EVALUATION SETUP</p>",
            "id": 36,
            "page": 4,
            "text": "2.4 EVALUATION SETUP"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1680
                },
                {
                    "x": 2107,
                    "y": 1680
                },
                {
                    "x": 2107,
                    "y": 1819
                },
                {
                    "x": 442,
                    "y": 1819
                }
            ],
            "category": "paragraph",
            "html": "<p id='37' style='font-size:14px'>Our hybrid training enables models to solve problems using either the CoT or PoT approach. By<br>default, the model provides the CoT solution. To switch to the PoT approach, one can add the trigger<br>phrase \"Let's write a program to solve the problem\" following the question.</p>",
            "id": 37,
            "page": 4,
            "text": "Our hybrid training enables models to solve problems using either the CoT or PoT approach. By default, the model provides the CoT solution. To switch to the PoT approach, one can add the trigger phrase \"Let's write a program to solve the problem\" following the question."
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 1841
                },
                {
                    "x": 2108,
                    "y": 1841
                },
                {
                    "x": 2108,
                    "y": 2210
                },
                {
                    "x": 440,
                    "y": 2210
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='38' style='font-size:16px'>Our preliminary evaluation reveals that PoT generally outperforms CoT, notably in open-form ques-<br>tions like GSM8K and MATH, as programmable solutions are better at solving complex mathemati-<br>cal and algorithmic reasoning procedures. However, PoT struggles with abstract reasoning scenarios<br>such as commonsense reasoning, formal logic, and abstract algebra, particularly in the absence of<br>built-in APIs. To further combine the power of both approaches, we introduce a simple hybrid de-<br>coding strategy: The model first attempts PoT prompting. If the program is not executable, we falls<br>back to CoT prompting. This heuristic significantly enhances our model's overall performance (see<br>more discussions in section 3.4).</p>",
            "id": 38,
            "page": 4,
            "text": "Our preliminary evaluation reveals that PoT generally outperforms CoT, notably in open-form questions like GSM8K and MATH, as programmable solutions are better at solving complex mathematical and algorithmic reasoning procedures. However, PoT struggles with abstract reasoning scenarios such as commonsense reasoning, formal logic, and abstract algebra, particularly in the absence of built-in APIs. To further combine the power of both approaches, we introduce a simple hybrid decoding strategy: The model first attempts PoT prompting. If the program is not executable, we falls back to CoT prompting. This heuristic significantly enhances our model's overall performance (see more discussions in section 3.4)."
        },
        {
            "bounding_box": [
                {
                    "x": 446,
                    "y": 2282
                },
                {
                    "x": 839,
                    "y": 2282
                },
                {
                    "x": 839,
                    "y": 2336
                },
                {
                    "x": 446,
                    "y": 2336
                }
            ],
            "category": "paragraph",
            "html": "<p id='39' style='font-size:22px'>3 EXPERIMENTS</p>",
            "id": 39,
            "page": 4,
            "text": "3 EXPERIMENTS"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 2389
                },
                {
                    "x": 979,
                    "y": 2389
                },
                {
                    "x": 979,
                    "y": 2437
                },
                {
                    "x": 445,
                    "y": 2437
                }
            ],
            "category": "paragraph",
            "html": "<p id='40' style='font-size:14px'>3.1 EVALUATION DATASETS</p>",
            "id": 40,
            "page": 4,
            "text": "3.1 EVALUATION DATASETS"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2476
                },
                {
                    "x": 2107,
                    "y": 2476
                },
                {
                    "x": 2107,
                    "y": 2616
                },
                {
                    "x": 442,
                    "y": 2616
                }
            ],
            "category": "paragraph",
            "html": "<p id='41' style='font-size:14px'>We have selected diverse evaluation datasets (Table 2), encompassing a variety of in-domain and<br>out-of-domain samples across diverse fields of mathematics, to assess the models' capabilities in<br>general mathematical reasoning.</p>",
            "id": 41,
            "page": 4,
            "text": "We have selected diverse evaluation datasets (Table 2), encompassing a variety of in-domain and out-of-domain samples across diverse fields of mathematics, to assess the models' capabilities in general mathematical reasoning."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2639
                },
                {
                    "x": 2108,
                    "y": 2639
                },
                {
                    "x": 2108,
                    "y": 3055
                },
                {
                    "x": 441,
                    "y": 3055
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='42' style='font-size:16px'>For the in-domain datasets, we consider GSM8K (Cobbe et al., 2021), MATH (Hendrycks<br>et al., 2021b), AQuA-RAT (Ling et al., 2017), and NumGLUE (Mishra et al., 2022b). For<br>the out-of-domain datasets, we choose SVAMP (Patel et al., 2021), Mathematics (Davies et al.,<br>2021), SimulEq (Koncel-Kedziorski et al., 2016), SAT-Math (Zhong et al., 2023), and MMLU-<br>Math (Hendrycks et al., 2021a). The wide selection of evaluation datasets includes math problems<br>from elementary, high school, and college levels. Some of the datasets even include formal logic and<br>commonsense reasoning. The choice of these datasets is to ensure a comprehensive evaluation of<br>the models' capabilities to generalize to unfamiliar situations and different math fields. The chosen<br>evaluation datasets consist of both open-formed questions and multi-choice questions.</p>",
            "id": 42,
            "page": 4,
            "text": "For the in-domain datasets, we consider GSM8K (Cobbe , 2021), MATH (Hendrycks , 2021b), AQuA-RAT (Ling , 2017), and NumGLUE (Mishra , 2022b). For the out-of-domain datasets, we choose SVAMP (Patel , 2021), Mathematics (Davies , 2021), SimulEq (Koncel-Kedziorski , 2016), SAT-Math (Zhong , 2023), and MMLUMath (Hendrycks , 2021a). The wide selection of evaluation datasets includes math problems from elementary, high school, and college levels. Some of the datasets even include formal logic and commonsense reasoning. The choice of these datasets is to ensure a comprehensive evaluation of the models' capabilities to generalize to unfamiliar situations and different math fields. The chosen evaluation datasets consist of both open-formed questions and multi-choice questions."
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3135
                },
                {
                    "x": 1288,
                    "y": 3135
                },
                {
                    "x": 1288,
                    "y": 3167
                },
                {
                    "x": 1260,
                    "y": 3167
                }
            ],
            "category": "footer",
            "html": "<footer id='43' style='font-size:14px'>4</footer>",
            "id": 43,
            "page": 4,
            "text": "4"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 112
                },
                {
                    "x": 899,
                    "y": 112
                },
                {
                    "x": 899,
                    "y": 157
                },
                {
                    "x": 444,
                    "y": 157
                }
            ],
            "category": "header",
            "html": "<header id='44' style='font-size:20px'>Preprint. Work in Progress</header>",
            "id": 44,
            "page": 5,
            "text": "Preprint. Work in Progress"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 335
                },
                {
                    "x": 2101,
                    "y": 335
                },
                {
                    "x": 2101,
                    "y": 826
                },
                {
                    "x": 442,
                    "y": 826
                }
            ],
            "category": "table",
            "html": "<table id='45' style='font-size:14px'><tr><td>Eval Dataset</td><td># Samples</td><td>In-Domain?</td><td>Answer Form</td><td>Fields</td></tr><tr><td>GSM8K (Cobbe et al., 2021)</td><td>1319</td><td>YES</td><td>Open-formed</td><td></td></tr><tr><td>MATH (Hendrycks et al., 2021b) AQuA-RAT (Ling et al., 2017) NumGLUE (Mishra et al., 2022b)</td><td>5000 254 1042</td><td>YES YES YES</td><td>Open-formed Multi-choice Open-formed</td><td></td></tr><tr><td></td><td>1000</td><td>NO</td><td>Open-formed</td><td></td></tr><tr><td>SVAMP (Patel et al., 2021) Mathematics (Davies et al., 2021)</td><td>1000</td><td>NO</td><td>Open-formed</td><td></td></tr><tr><td>SimulEq (Koncel-Kedziorski et al., 2016) SAT-Math (Zhong et al., 2023) MMLU-Math (Hendrycks et al., 2021a)</td><td>514 220 974</td><td>NO NO NO</td><td>Open-formed Multi-choice Multi-choice</td><td></td></tr></table>",
            "id": 45,
            "page": 5,
            "text": "Eval Dataset # Samples In-Domain? Answer Form Fields  GSM8K (Cobbe , 2021) 1319 YES Open-formed   MATH (Hendrycks , 2021b) AQuA-RAT (Ling , 2017) NumGLUE (Mishra , 2022b) 5000 254 1042 YES YES YES Open-formed Multi-choice Open-formed    1000 NO Open-formed   SVAMP (Patel , 2021) Mathematics (Davies , 2021) 1000 NO Open-formed   SimulEq (Koncel-Kedziorski , 2016) SAT-Math (Zhong , 2023) MMLU-Math (Hendrycks , 2021a) 514 220 974 NO NO NO Open-formed Multi-choice Multi-choice"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 860
                },
                {
                    "x": 2107,
                    "y": 860
                },
                {
                    "x": 2107,
                    "y": 1047
                },
                {
                    "x": 442,
                    "y": 1047
                }
            ],
            "category": "paragraph",
            "html": "<p id='46' style='font-size:18px'>Table 2: Comprehensive overview of our evaluation datasets, featuring a variety of in-domain and<br>out-of-domain problems across diverse fields of mathematics. Different colored squares repre-<br>sent different fields in mathematics: Pre-Algebra; Inter-Algebra; Algebra; Probability;<br>■ NumTheory; Calculus; Geometry.</p>",
            "id": 46,
            "page": 5,
            "text": "Table 2: Comprehensive overview of our evaluation datasets, featuring a variety of in-domain and out-of-domain problems across diverse fields of mathematics. Different colored squares represent different fields in mathematics: Pre-Algebra; Inter-Algebra; Algebra; Probability; ■ NumTheory; Calculus; Geometry."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 1139
                },
                {
                    "x": 756,
                    "y": 1139
                },
                {
                    "x": 756,
                    "y": 1186
                },
                {
                    "x": 444,
                    "y": 1186
                }
            ],
            "category": "paragraph",
            "html": "<p id='47' style='font-size:16px'>3.2 BASELINES</p>",
            "id": 47,
            "page": 5,
            "text": "3.2 BASELINES"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 1228
                },
                {
                    "x": 1462,
                    "y": 1228
                },
                {
                    "x": 1462,
                    "y": 1276
                },
                {
                    "x": 445,
                    "y": 1276
                }
            ],
            "category": "paragraph",
            "html": "<p id='48' style='font-size:18px'>We partition our baselines into the following four categories:</p>",
            "id": 48,
            "page": 5,
            "text": "We partition our baselines into the following four categories:"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 1294
                },
                {
                    "x": 2112,
                    "y": 1294
                },
                {
                    "x": 2112,
                    "y": 2089
                },
                {
                    "x": 440,
                    "y": 2089
                }
            ],
            "category": "paragraph",
            "html": "<p id='49' style='font-size:18px'>· Closed-source LLMs: We consider 4 closed-source LLMs including GPT-4 (OpenAI, 2023),<br>GPT-4 (Code Interpreter), PaLM-2 Unicorn (Anil et al., 2023), Claude-2 (Bai et al., 2022) and<br>Codex (Chen et al., 2021). GPT-4, PaLM-2, and Claude-2 use CoT prompting while GPT-4 (Code<br>Interpreter) and Codex use PoT prompting.<br>● Llama Base: For the base models, we consider Llama-1/2 (Touvron et al., 2023a;b), Llama-2-<br>Chat (Touvron et al., 2023b).<br>· Coder Model: To compare with different coder models, we choose Code-Llama (Roziere et al.,<br>2023), CodeT5+ (Wang et al., 2023i) and CodeGen (Nijkamp et al., 2023).<br>· STEM Pre-training: We cover Galactica (Taylor et al., 2022) mainly to understand the perfor-<br>mance of models specialized in STEM knowledge.<br>· Instruction Tuning: We include Orca-Platypus (Mukherjee et al., 2023), Vicuna-1.5 (Zheng<br>et al., 2023b), Tulu (Wang et al., 2023g), Platypus-2 (Lee et al., 2023) and Guanaco (Dettmers<br>et al., 2023). We cover a wide spectrum of models trained with different types of datasets.<br>· Dataset-Specific Tuning: We include both RFT (Yuan et al., 2023) and WizardMath (Luo et al.,<br>2023), which specifically tune the models to adapt to GSM8K and MATH datasets. We include<br>them to understand their generalization.</p>",
            "id": 49,
            "page": 5,
            "text": "· Closed-source LLMs: We consider 4 closed-source LLMs including GPT-4 (OpenAI, 2023), GPT-4 (Code Interpreter), PaLM-2 Unicorn (Anil , 2023), Claude-2 (Bai , 2022) and Codex (Chen , 2021). GPT-4, PaLM-2, and Claude-2 use CoT prompting while GPT-4 (Code Interpreter) and Codex use PoT prompting. ● Llama Base: For the base models, we consider Llama-1/2 (Touvron , 2023a;b), Llama-2Chat (Touvron , 2023b). · Coder Model: To compare with different coder models, we choose Code-Llama (Roziere , 2023), CodeT5+ (Wang , 2023i) and CodeGen (Nijkamp , 2023). · STEM Pre-training: We cover Galactica (Taylor , 2022) mainly to understand the performance of models specialized in STEM knowledge. · Instruction Tuning: We include Orca-Platypus (Mukherjee , 2023), Vicuna-1.5 (Zheng , 2023b), Tulu (Wang , 2023g), Platypus-2 (Lee , 2023) and Guanaco (Dettmers , 2023). We cover a wide spectrum of models trained with different types of datasets. · Dataset-Specific Tuning: We include both RFT (Yuan , 2023) and WizardMath (Luo , 2023), which specifically tune the models to adapt to GSM8K and MATH datasets. We include them to understand their generalization."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2113
                },
                {
                    "x": 2108,
                    "y": 2113
                },
                {
                    "x": 2108,
                    "y": 2576
                },
                {
                    "x": 442,
                    "y": 2576
                }
            ],
            "category": "paragraph",
            "html": "<p id='50' style='font-size:18px'>For most baselines, we choose CoT prompting to maximize their performance due to their incom-<br>petence in program generation. All the 'Code Model' use PoT prompting. For GSM8K, MATH,<br>AQuA, and NumGLUE, we will evaluate both 8-shot in-context-learning and zero-shot setups to<br>report the highest score. For SVAMP, Mathematics, SimulEq, SAT, and MMLU, we use 5-shot<br>in-context-learning to maintain consistency with prior work (Wei et al., 2022b; Chen et al., 2023).<br>Our few-shot exemplars are mostly taken from PHP1 (Zheng et al., 2023a). For MAmmoTH and<br>MAmmo TH-Coder, we always evaluate under 0-shot setting. For all models, we allow a maximum<br>sequence length of 2048 tokens for decoding. For multiple-choice questions, if the generated answer<br>lacks an option, we map it by re-prompting the model: \"Please find the closest option to [generated<br>answer]. The options are [options]\".</p>",
            "id": 50,
            "page": 5,
            "text": "For most baselines, we choose CoT prompting to maximize their performance due to their incompetence in program generation. All the 'Code Model' use PoT prompting. For GSM8K, MATH, AQuA, and NumGLUE, we will evaluate both 8-shot in-context-learning and zero-shot setups to report the highest score. For SVAMP, Mathematics, SimulEq, SAT, and MMLU, we use 5-shot in-context-learning to maintain consistency with prior work (Wei , 2022b; Chen , 2023). Our few-shot exemplars are mostly taken from PHP1 (Zheng , 2023a). For MAmmoTH and MAmmo TH-Coder, we always evaluate under 0-shot setting. For all models, we allow a maximum sequence length of 2048 tokens for decoding. For multiple-choice questions, if the generated answer lacks an option, we map it by re-prompting the model: \"Please find the closest option to [generated answer]. The options are [options]\"."
        },
        {
            "bounding_box": [
                {
                    "x": 446,
                    "y": 2638
                },
                {
                    "x": 831,
                    "y": 2638
                },
                {
                    "x": 831,
                    "y": 2686
                },
                {
                    "x": 446,
                    "y": 2686
                }
            ],
            "category": "paragraph",
            "html": "<p id='51' style='font-size:16px'>3.3 MAIN RESULTS</p>",
            "id": 51,
            "page": 5,
            "text": "3.3 MAIN RESULTS"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2725
                },
                {
                    "x": 2108,
                    "y": 2725
                },
                {
                    "x": 2108,
                    "y": 2958
                },
                {
                    "x": 441,
                    "y": 2958
                }
            ],
            "category": "paragraph",
            "html": "<p id='52' style='font-size:14px'>We report our in-domain and out-of-domain results in Table 3 and Table 4 respectively. Overall, we<br>can see that MAmmo TH and MAmmo TH-Coder are able to outperform the SoTA model at different<br>scales. In general, the performance gain for OOD datasets is more significant than IND datasets.<br>These results show us the potential of our models as a mathematical generalist. On several datasets,<br>MAmmo TH-Coder-34B and MAmmo TH-70B are even surpassing closed-source LLMs.</p>",
            "id": 52,
            "page": 5,
            "text": "We report our in-domain and out-of-domain results in Table 3 and Table 4 respectively. Overall, we can see that MAmmo TH and MAmmo TH-Coder are able to outperform the SoTA model at different scales. In general, the performance gain for OOD datasets is more significant than IND datasets. These results show us the potential of our models as a mathematical generalist. On several datasets, MAmmo TH-Coder-34B and MAmmo TH-70B are even surpassing closed-source LLMs."
        },
        {
            "bounding_box": [
                {
                    "x": 499,
                    "y": 3007
                },
                {
                    "x": 1336,
                    "y": 3007
                },
                {
                    "x": 1336,
                    "y": 3052
                },
                {
                    "x": 499,
                    "y": 3052
                }
            ],
            "category": "paragraph",
            "html": "<p id='53' style='font-size:22px'>https://gimd.com/inanyang-Zhang/Prograssing-Einl</p>",
            "id": 53,
            "page": 5,
            "text": "https://gimd.com/inanyang-Zhang/Prograssing-Einl"
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3131
                },
                {
                    "x": 1290,
                    "y": 3131
                },
                {
                    "x": 1290,
                    "y": 3170
                },
                {
                    "x": 1260,
                    "y": 3170
                }
            ],
            "category": "footer",
            "html": "<footer id='54' style='font-size:16px'>5</footer>",
            "id": 54,
            "page": 5,
            "text": "5"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 112
                },
                {
                    "x": 899,
                    "y": 112
                },
                {
                    "x": 899,
                    "y": 158
                },
                {
                    "x": 445,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='55' style='font-size:20px'>Preprint. Work in Progress</header>",
            "id": 55,
            "page": 6,
            "text": "Preprint. Work in Progress"
        },
        {
            "bounding_box": [
                {
                    "x": 438,
                    "y": 332
                },
                {
                    "x": 2106,
                    "y": 332
                },
                {
                    "x": 2106,
                    "y": 2626
                },
                {
                    "x": 438,
                    "y": 2626
                }
            ],
            "category": "table",
            "html": "<table id='56' style='font-size:14px'><tr><td>Model</td><td>Base</td><td>Math-SFT?</td><td>GSM8K</td><td>MATH</td><td>AQuA</td><td>NumGLUE</td><td>Avg</td></tr><tr><td colspan=\"8\">Closed-source Model</td></tr><tr><td>GPT-4</td><td>-</td><td>Unknown</td><td>92.0+</td><td>42.5+</td><td>72.6+</td><td>-</td><td>-</td></tr><tr><td>GPT-4 (Code-Interpreter)</td><td>-</td><td>Unknown</td><td>97.0+</td><td>69.7t</td><td>-</td><td>-</td><td>-</td></tr><tr><td>PaLM-2</td><td>-</td><td>Unknown</td><td>80.7t</td><td>34.3t</td><td>64.1</td><td>-</td><td>-</td></tr><tr><td>Claude-2</td><td>-</td><td>Unknown</td><td>85.2t</td><td>32.5t</td><td>60.9</td><td>-</td><td>-</td></tr><tr><td>Codex (PoT)</td><td>-</td><td>No</td><td>71.6t</td><td>36.8t</td><td>54.1+</td><td>-</td><td>-</td></tr><tr><td>ART (InstructGPT)</td><td></td><td>Unknown</td><td>71.0</td><td>-</td><td>54.2</td><td>-</td><td>-</td></tr><tr><td colspan=\"8\">7B Parameter Model</td></tr><tr><td>Llama-1</td><td>-</td><td>No</td><td>10.7t</td><td>2.9t</td><td>22.6</td><td>24.7</td><td>15.5</td></tr><tr><td>Llama-2</td><td>-</td><td>No</td><td>14.6t</td><td>2.5t</td><td>30.3</td><td>29.9</td><td>19.3</td></tr><tr><td>Galactica-6.7B</td><td>GAL</td><td>GAL-Instruct</td><td>10.2</td><td>2.2</td><td>25.6</td><td>25.8</td><td>15.9</td></tr><tr><td>Code-Llama (PoT)</td><td>-</td><td>No</td><td>25.2</td><td>13.0</td><td>24.0</td><td>26.8</td><td>22.2</td></tr><tr><td>AQuA-SFT</td><td>Llama-2</td><td>AQuA</td><td>11.2</td><td>3.6</td><td>35.6</td><td>12.2</td><td>15.6</td></tr><tr><td>Llama-1 RFT</td><td>Llama-1</td><td>GSM8K</td><td>46.5T</td><td>5.2</td><td>18.8</td><td>21.1</td><td>22.9</td></tr><tr><td>WizardMath</td><td>Llama-2</td><td>GSM8K+MATH</td><td>54.9T</td><td>10.7t</td><td>26.3</td><td>36.1</td><td>32.0</td></tr><tr><td>MAmmoTH</td><td>Llama-2</td><td>MathInstruct</td><td>53.6</td><td>31.5</td><td>44.5</td><td>61.2</td><td>47.7</td></tr><tr><td>MAmmoTH-Coder</td><td>Code-Llama</td><td>MathInstruct</td><td>59.4</td><td>33.4</td><td>47.2</td><td>66.4</td><td>51.6</td></tr><tr><td></td><td></td><td></td><td>+5</td><td>+21</td><td>+12</td><td>+30</td><td>+20</td></tr><tr><td colspan=\"8\">13-15B Parameter Model</td></tr><tr><td>Llama-1</td><td>、</td><td>No</td><td>17.8t</td><td>3.9t</td><td>26.0</td><td>24.8</td><td>18.1</td></tr><tr><td>Llama-2</td><td>-</td><td>No</td><td>28.7t</td><td>3.9t</td><td>25.1</td><td>8.8</td><td>16.6</td></tr><tr><td>Code-Llama (PoT)</td><td>-</td><td>No</td><td>36.1</td><td>16.4</td><td>28.7</td><td>29.2</td><td>27.6</td></tr><tr><td>CodeT5+ (PoT)</td><td>-</td><td>No</td><td>12.5</td><td>2.4</td><td>20.5</td><td>19.4</td><td>13.7</td></tr><tr><td>CodeGen+ (PoT)</td><td>-</td><td>No</td><td>12.7</td><td>3.4</td><td>24.5</td><td>22.5</td><td>15.7</td></tr><tr><td>Vicuna-1.5</td><td>Llama-2</td><td>No</td><td>28.4t</td><td>5.8</td><td>24.8</td><td>36.9</td><td>23.9</td></tr><tr><td>Llama-1 RFT</td><td>Llama-1</td><td>GSM8K</td><td>52.1t</td><td>5.1</td><td>16.1</td><td>24.5</td><td>24.4</td></tr><tr><td>Orca-Platypus</td><td>Llama-2</td><td>Platypus</td><td>38.4</td><td>3.0</td><td>18.9</td><td>35.3</td><td>23.9</td></tr><tr><td>Platypus</td><td>Llama-2</td><td>Platypus</td><td>25.7</td><td>2.5</td><td>33.4</td><td>42.3</td><td>25.9</td></tr><tr><td>WizardMath</td><td>Llama-2</td><td>GSM8K+MATH</td><td>63.9T</td><td>14.0+</td><td>21.2</td><td>40.8</td><td>34.9</td></tr><tr><td>MAmmoTH</td><td>Llama-2</td><td>MathInstruct</td><td>62.0</td><td>34.2</td><td>51.6</td><td>68.7</td><td>54.1</td></tr><tr><td>MAmmoTH-Coder</td><td>Code-Llama</td><td>MathInstruct</td><td>64.7</td><td>36.3</td><td>46.9</td><td>66.8</td><td>53.7</td></tr><tr><td>△</td><td></td><td></td><td>+1</td><td>+20</td><td>+18</td><td>+26</td><td>+19</td></tr><tr><td colspan=\"8\">30-34B Parameter Model</td></tr><tr><td>Llama-1</td><td>-</td><td>No</td><td>35.6t</td><td>7.1t</td><td>33.4</td><td>28.4</td><td>26.1</td></tr><tr><td>Code-Llama (PoT)</td><td>-</td><td>No</td><td>44.0</td><td>23.1</td><td>25.2</td><td>29.3</td><td>30.4</td></tr><tr><td>Llama-1 RFT</td><td>Llama-1</td><td>GSM8K</td><td>56.5T</td><td>7.4t</td><td>18.5</td><td>24.3</td><td>26.6</td></tr><tr><td>Galactica-30B</td><td>GAL</td><td>GAL-Instruct</td><td>41.7</td><td>12.7</td><td>28.7</td><td>34.7</td><td>29.4</td></tr><tr><td>Platypus</td><td>Llama-1</td><td>Platypus</td><td>37.8</td><td>9.3</td><td>27.9</td><td>40.5</td><td>28.8</td></tr><tr><td>Tulu</td><td>Llama-2</td><td>Tulu</td><td>51.0</td><td>10.8</td><td>25.5</td><td>43.4</td><td>32.6</td></tr><tr><td>MAmmoTH-Coder</td><td>Code-Llama</td><td>MathInstruct</td><td>72.7</td><td>43.6</td><td>54.7</td><td>71.6</td><td>60.7</td></tr><tr><td></td><td></td><td></td><td>+16</td><td>+21</td><td>+21</td><td>+28</td><td>+28</td></tr><tr><td colspan=\"8\">65-70B Parameter Model</td></tr><tr><td>Llama-1</td><td>-</td><td>No</td><td>50.9¢</td><td>10.6+</td><td>35.0</td><td>50.2</td><td>36.6</td></tr><tr><td>Llama-2</td><td></td><td>No</td><td>56.8t</td><td>13.5t</td><td>40.9</td><td>50.4</td><td>40.4</td></tr><tr><td>Llama-2-Chat</td><td>Llama-2</td><td>No</td><td>54.9</td><td>18.6</td><td>37.0</td><td>51.6</td><td>40.5</td></tr><tr><td>Guanaco</td><td>Llama-2</td><td>No</td><td>59.2</td><td>4.1</td><td>45.2</td><td>53.5</td><td>40.5</td></tr><tr><td>WizardMath</td><td>Llama-2</td><td>GSM8K+MATH</td><td>81.6T</td><td>22.7t</td><td>20.0</td><td>48.9</td><td>43.3</td></tr><tr><td>Platypus</td><td>Llama-2</td><td>Platypus</td><td>70.6</td><td>15.6</td><td>51.2</td><td>55.4</td><td>48.1</td></tr><tr><td>MAmmoTH</td><td>Llama-2</td><td>MathInstruct</td><td>76.9</td><td>41.8</td><td>65.0</td><td>74.4</td><td>64.5</td></tr><tr><td></td><td></td><td></td><td>-5</td><td>+19</td><td>+14</td><td>+19</td><td>+16</td></tr></table>",
            "id": 56,
            "page": 6,
            "text": "Model Base Math-SFT? GSM8K MATH AQuA NumGLUE Avg  Closed-source Model  GPT-4 - Unknown 92.0+ 42.5+ 72.6+ -  GPT-4 (Code-Interpreter) - Unknown 97.0+ 69.7t - -  PaLM-2 - Unknown 80.7t 34.3t 64.1 -  Claude-2 - Unknown 85.2t 32.5t 60.9 -  Codex (PoT) - No 71.6t 36.8t 54.1+ -  ART (InstructGPT)  Unknown 71.0 - 54.2 -  7B Parameter Model  Llama-1 - No 10.7t 2.9t 22.6 24.7 15.5  Llama-2 - No 14.6t 2.5t 30.3 29.9 19.3  Galactica-6.7B GAL GAL-Instruct 10.2 2.2 25.6 25.8 15.9  Code-Llama (PoT) - No 25.2 13.0 24.0 26.8 22.2  AQuA-SFT Llama-2 AQuA 11.2 3.6 35.6 12.2 15.6  Llama-1 RFT Llama-1 GSM8K 46.5T 5.2 18.8 21.1 22.9  WizardMath Llama-2 GSM8K+MATH 54.9T 10.7t 26.3 36.1 32.0  MAmmoTH Llama-2 MathInstruct 53.6 31.5 44.5 61.2 47.7  MAmmoTH-Coder Code-Llama MathInstruct 59.4 33.4 47.2 66.4 51.6     +5 +21 +12 +30 +20  13-15B Parameter Model  Llama-1 、 No 17.8t 3.9t 26.0 24.8 18.1  Llama-2 - No 28.7t 3.9t 25.1 8.8 16.6  Code-Llama (PoT) - No 36.1 16.4 28.7 29.2 27.6  CodeT5+ (PoT) - No 12.5 2.4 20.5 19.4 13.7  CodeGen+ (PoT) - No 12.7 3.4 24.5 22.5 15.7  Vicuna-1.5 Llama-2 No 28.4t 5.8 24.8 36.9 23.9  Llama-1 RFT Llama-1 GSM8K 52.1t 5.1 16.1 24.5 24.4  Orca-Platypus Llama-2 Platypus 38.4 3.0 18.9 35.3 23.9  Platypus Llama-2 Platypus 25.7 2.5 33.4 42.3 25.9  WizardMath Llama-2 GSM8K+MATH 63.9T 14.0+ 21.2 40.8 34.9  MAmmoTH Llama-2 MathInstruct 62.0 34.2 51.6 68.7 54.1  MAmmoTH-Coder Code-Llama MathInstruct 64.7 36.3 46.9 66.8 53.7  △   +1 +20 +18 +26 +19  30-34B Parameter Model  Llama-1 - No 35.6t 7.1t 33.4 28.4 26.1  Code-Llama (PoT) - No 44.0 23.1 25.2 29.3 30.4  Llama-1 RFT Llama-1 GSM8K 56.5T 7.4t 18.5 24.3 26.6  Galactica-30B GAL GAL-Instruct 41.7 12.7 28.7 34.7 29.4  Platypus Llama-1 Platypus 37.8 9.3 27.9 40.5 28.8  Tulu Llama-2 Tulu 51.0 10.8 25.5 43.4 32.6  MAmmoTH-Coder Code-Llama MathInstruct 72.7 43.6 54.7 71.6 60.7     +16 +21 +21 +28 +28  65-70B Parameter Model  Llama-1 - No 50.9¢ 10.6+ 35.0 50.2 36.6  Llama-2  No 56.8t 13.5t 40.9 50.4 40.4  Llama-2-Chat Llama-2 No 54.9 18.6 37.0 51.6 40.5  Guanaco Llama-2 No 59.2 4.1 45.2 53.5 40.5  WizardMath Llama-2 GSM8K+MATH 81.6T 22.7t 20.0 48.9 43.3  Platypus Llama-2 Platypus 70.6 15.6 51.2 55.4 48.1  MAmmoTH Llama-2 MathInstruct 76.9 41.8 65.0 74.4 64.5     -5 +19 +14 +19"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2659
                },
                {
                    "x": 2105,
                    "y": 2659
                },
                {
                    "x": 2105,
                    "y": 2895
                },
                {
                    "x": 442,
                    "y": 2895
                }
            ],
            "category": "paragraph",
            "html": "<p id='57' style='font-size:16px'>Table 3: The table compiles all the in-domain evaluation results. Results marked as 1 are copied<br>from other papers, which can be found on paperswithcode leaderboards. Math-SFT? means whether<br>the model has been instruction-tuned on any math reasoning datasets. Pink numbers highlight the<br>highest number within the corresponding scale and dataset. Note that there does not exist a 30B+<br>version for Llama-2 or a 70B version for Code-Llama.</p>",
            "id": 57,
            "page": 6,
            "text": "Table 3: The table compiles all the in-domain evaluation results. Results marked as 1 are copied from other papers, which can be found on paperswithcode leaderboards. Math-SFT? means whether the model has been instruction-tuned on any math reasoning datasets. Pink numbers highlight the highest number within the corresponding scale and dataset. Note that there does not exist a 30B+ version for Llama-2 or a 70B version for Code-Llama."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2958
                },
                {
                    "x": 2107,
                    "y": 2958
                },
                {
                    "x": 2107,
                    "y": 3054
                },
                {
                    "x": 441,
                    "y": 3054
                }
            ],
            "category": "paragraph",
            "html": "<p id='58' style='font-size:18px'>From Table 3, we can observe that our main competitors for IND datasets are WizardMath (Luo<br>et al., 2023) and Platypus (Lee et al., 2023). WizardMath's training is heavily rooted in GSM8K</p>",
            "id": 58,
            "page": 6,
            "text": "From Table 3, we can observe that our main competitors for IND datasets are WizardMath (Luo , 2023) and Platypus (Lee , 2023). WizardMath's training is heavily rooted in GSM8K"
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3136
                },
                {
                    "x": 1290,
                    "y": 3136
                },
                {
                    "x": 1290,
                    "y": 3172
                },
                {
                    "x": 1260,
                    "y": 3172
                }
            ],
            "category": "footer",
            "html": "<footer id='59' style='font-size:18px'>6</footer>",
            "id": 59,
            "page": 6,
            "text": "6"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 112
                },
                {
                    "x": 899,
                    "y": 112
                },
                {
                    "x": 899,
                    "y": 158
                },
                {
                    "x": 444,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='60' style='font-size:20px'>Preprint. Work in Progress</header>",
            "id": 60,
            "page": 7,
            "text": "Preprint. Work in Progress"
        },
        {
            "bounding_box": [
                {
                    "x": 509,
                    "y": 331
                },
                {
                    "x": 2045,
                    "y": 331
                },
                {
                    "x": 2045,
                    "y": 2539
                },
                {
                    "x": 509,
                    "y": 2539
                }
            ],
            "category": "table",
            "html": "<table id='61' style='font-size:14px'><tr><td>Model</td><td>SVAMP</td><td>Mathematics</td><td>SimulEq</td><td>SAT-Math</td><td>MMLU-Math</td><td>Avg</td></tr><tr><td colspan=\"7\">Closed-source Model</td></tr><tr><td>GPT-4</td><td>97.0†</td><td>-</td><td>-</td><td>95t</td><td></td><td>-</td></tr><tr><td>Codex (PoT)</td><td>85.2+</td><td>-</td><td>-</td><td>68t</td><td></td><td></td></tr><tr><td colspan=\"7\">7B Parameter Model</td></tr><tr><td>Llama-1</td><td>24.5</td><td>6.2</td><td>4.6</td><td>22.7</td><td>30.6</td><td>17.7</td></tr><tr><td>Llama-2</td><td>34.5</td><td>6.0</td><td>5.0</td><td>26.8</td><td>29.8</td><td>20.4</td></tr><tr><td>Code-Llama (PoT)</td><td>49.4</td><td>21.7</td><td>3.5</td><td>28.6</td><td>26.9</td><td>26.0</td></tr><tr><td>Llama-1 RFT</td><td>21.1</td><td>5.1</td><td>11.0</td><td>12.5</td><td>21.7</td><td>14.3</td></tr><tr><td>Galactica-6.7B</td><td>25.6</td><td>4.6</td><td>4.2</td><td>17.5</td><td>28.0</td><td>16.0</td></tr><tr><td>WizardMath</td><td>36.1</td><td>9.3</td><td>12.8</td><td>25.4</td><td>31.1</td><td>28.6</td></tr><tr><td>Toolformer</td><td>29.4t</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>MAmmoTH</td><td>67.7</td><td>46.3</td><td>41.2</td><td>42.7</td><td>42.6</td><td>48.1</td></tr><tr><td>MAmmoTH-Coder</td><td>71.4</td><td>55.4</td><td>45.9</td><td>40.5</td><td>48.3</td><td>52.3</td></tr><tr><td></td><td>+22</td><td>+34</td><td>+33</td><td>+14</td><td>+17</td><td>+24</td></tr><tr><td colspan=\"7\">13B Parameter Model</td></tr><tr><td>Llama-1</td><td>34.7</td><td>6.9</td><td>5.4</td><td>27.7</td><td>30.7</td><td>21.0</td></tr><tr><td>Llama-2</td><td>35.1</td><td>11.5</td><td>5.8</td><td>32.7</td><td>34.4</td><td>23.9</td></tr><tr><td>Code-Llama (PoT)</td><td>60.0</td><td>21.3</td><td>3.8</td><td>25.9</td><td>27.7</td><td>27.7</td></tr><tr><td>Vicuna-1.5</td><td>55.7</td><td>10</td><td>6.6</td><td>34.0</td><td>34.1</td><td>28.1</td></tr><tr><td>Llama-1 RFT</td><td>46.5</td><td>6.7</td><td>10.1</td><td>13.2</td><td>21.6</td><td>19.6</td></tr><tr><td>WizardMath</td><td>51.9</td><td>14.1</td><td>14.9</td><td>24.5</td><td>32.1</td><td>27.5</td></tr><tr><td>Platypus</td><td>55.4</td><td>11.4</td><td>7.4</td><td>36.8</td><td>35.5</td><td>29.3</td></tr><tr><td>Orca-Platypus</td><td>56.8</td><td>12.6</td><td>7.9</td><td>29.5</td><td>41.6</td><td>29.7</td></tr><tr><td>MAmmoTH</td><td>72.4</td><td>49.2</td><td>43.2</td><td>46.8</td><td>47.6</td><td>51.8</td></tr><tr><td>MAmmoTH-Coder</td><td>73.7</td><td>61.5</td><td>47.1</td><td>48.6</td><td>48.3</td><td>55.8</td></tr><tr><td></td><td>+14</td><td>+40</td><td>+33</td><td>+12</td><td>+7</td><td>+26</td></tr><tr><td colspan=\"7\">30-34B Parameter Model</td></tr><tr><td>Llama-1</td><td>48.8</td><td>12.8</td><td>11.2</td><td>33.4</td><td>39.0</td><td>29.0</td></tr><tr><td>Code-Llama (PoT)</td><td>69.1</td><td>34.5</td><td>6.8</td><td>26.8</td><td>21.6</td><td>31.7</td></tr><tr><td>Llama-1 RFT</td><td>55.4</td><td>7.6</td><td>12.8</td><td>20.4</td><td>37.9</td><td>26.8</td></tr><tr><td>Galactica-30B</td><td>41.6</td><td>11.8</td><td>13.2</td><td>37.7</td><td>37.9</td><td>28.4</td></tr><tr><td>Tulu</td><td>59.0</td><td>10.7</td><td>10.3</td><td>31.3</td><td>39.8</td><td>30.2</td></tr><tr><td>Platypus</td><td>51.7</td><td>13.8</td><td>13.6</td><td>38.6</td><td>41.0</td><td>31.7</td></tr><tr><td>MAmmoTH-Coder</td><td>84.3</td><td>65.4</td><td>51.8</td><td>60.9</td><td>53.8</td><td>63.2</td></tr><tr><td>△</td><td>+15</td><td>+31</td><td>+38</td><td>+22</td><td>+13</td><td>+32</td></tr><tr><td colspan=\"7\">65-70B Parameter Model</td></tr><tr><td>Llama-1</td><td>55.3</td><td>14.2</td><td>15.2</td><td>37.4</td><td>44.1</td><td>33.2</td></tr><tr><td>Llama-2</td><td>63.8</td><td>20.5</td><td>14.0</td><td>51.3</td><td>47.1</td><td>39.3</td></tr><tr><td>Llama-2-Chat</td><td>71.5</td><td>19.2</td><td>21.7</td><td>44.1</td><td>46.9</td><td>40.6</td></tr><tr><td>WizardMath</td><td>71.8</td><td>17.1</td><td>37.9</td><td>13.2</td><td>27.4</td><td>33.4</td></tr><tr><td>Guanaco</td><td>66.8</td><td>17.8</td><td>20.2</td><td>50.0</td><td>47.3</td><td>40.4</td></tr><tr><td>Platypus</td><td>51.8</td><td>26.3</td><td>21.7</td><td>55.9</td><td>52.5</td><td>41.6</td></tr><tr><td>MAmmoTH</td><td>82.4</td><td>55.6</td><td>51.4</td><td>66.4</td><td>56.7</td><td>62.5</td></tr><tr><td>△</td><td>+11</td><td>+29</td><td>+14</td><td>+11</td><td>+4</td><td>+21</td></tr></table>",
            "id": 61,
            "page": 7,
            "text": "Model SVAMP Mathematics SimulEq SAT-Math MMLU-Math Avg  Closed-source Model  GPT-4 97.0† - - 95t   Codex (PoT) 85.2+ - - 68t    7B Parameter Model  Llama-1 24.5 6.2 4.6 22.7 30.6 17.7  Llama-2 34.5 6.0 5.0 26.8 29.8 20.4  Code-Llama (PoT) 49.4 21.7 3.5 28.6 26.9 26.0  Llama-1 RFT 21.1 5.1 11.0 12.5 21.7 14.3  Galactica-6.7B 25.6 4.6 4.2 17.5 28.0 16.0  WizardMath 36.1 9.3 12.8 25.4 31.1 28.6  Toolformer 29.4t - - - -  MAmmoTH 67.7 46.3 41.2 42.7 42.6 48.1  MAmmoTH-Coder 71.4 55.4 45.9 40.5 48.3 52.3   +22 +34 +33 +14 +17 +24  13B Parameter Model  Llama-1 34.7 6.9 5.4 27.7 30.7 21.0  Llama-2 35.1 11.5 5.8 32.7 34.4 23.9  Code-Llama (PoT) 60.0 21.3 3.8 25.9 27.7 27.7  Vicuna-1.5 55.7 10 6.6 34.0 34.1 28.1  Llama-1 RFT 46.5 6.7 10.1 13.2 21.6 19.6  WizardMath 51.9 14.1 14.9 24.5 32.1 27.5  Platypus 55.4 11.4 7.4 36.8 35.5 29.3  Orca-Platypus 56.8 12.6 7.9 29.5 41.6 29.7  MAmmoTH 72.4 49.2 43.2 46.8 47.6 51.8  MAmmoTH-Coder 73.7 61.5 47.1 48.6 48.3 55.8   +14 +40 +33 +12 +7 +26  30-34B Parameter Model  Llama-1 48.8 12.8 11.2 33.4 39.0 29.0  Code-Llama (PoT) 69.1 34.5 6.8 26.8 21.6 31.7  Llama-1 RFT 55.4 7.6 12.8 20.4 37.9 26.8  Galactica-30B 41.6 11.8 13.2 37.7 37.9 28.4  Tulu 59.0 10.7 10.3 31.3 39.8 30.2  Platypus 51.7 13.8 13.6 38.6 41.0 31.7  MAmmoTH-Coder 84.3 65.4 51.8 60.9 53.8 63.2  △ +15 +31 +38 +22 +13 +32  65-70B Parameter Model  Llama-1 55.3 14.2 15.2 37.4 44.1 33.2  Llama-2 63.8 20.5 14.0 51.3 47.1 39.3  Llama-2-Chat 71.5 19.2 21.7 44.1 46.9 40.6  WizardMath 71.8 17.1 37.9 13.2 27.4 33.4  Guanaco 66.8 17.8 20.2 50.0 47.3 40.4  Platypus 51.8 26.3 21.7 55.9 52.5 41.6  MAmmoTH 82.4 55.6 51.4 66.4 56.7 62.5  △ +11 +29 +14 +11 +4"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2575
                },
                {
                    "x": 2107,
                    "y": 2575
                },
                {
                    "x": 2107,
                    "y": 2673
                },
                {
                    "x": 441,
                    "y": 2673
                }
            ],
            "category": "caption",
            "html": "<caption id='62' style='font-size:16px'>Table 4: The table compiles all the out-of-domain evaluation results. Results marked as + are copied<br>from other papers, which can be found on paperswithcode leaderboards.</caption>",
            "id": 62,
            "page": 7,
            "text": "Table 4: The table compiles all the out-of-domain evaluation results. Results marked as + are copied from other papers, which can be found on paperswithcode leaderboards."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2728
                },
                {
                    "x": 2107,
                    "y": 2728
                },
                {
                    "x": 2107,
                    "y": 3053
                },
                {
                    "x": 442,
                    "y": 3053
                }
            ],
            "category": "paragraph",
            "html": "<p id='63' style='font-size:16px'>and MATH datasets. Therefore, WizardMath's results are highly competitive on these two datasets.<br>However, the dataset-specific training can be detrimental to OOD datasets like AQuA. In contrast,<br>Platypus fine-tunes LLMs on a wide range of text and math reasoning datasets. it improves the open-<br>source SoTA on several datasets. Similarly, MAmmo TH can achieve universal improvement across<br>the board. A major observation is that MAmmoTH is particularly strong at solving more complex<br>math problems in MATH, where the gain of our model over WizardMath (open-source SoTA on<br>MATH) can exceed 25% at different scales.</p>",
            "id": 63,
            "page": 7,
            "text": "and MATH datasets. Therefore, WizardMath's results are highly competitive on these two datasets. However, the dataset-specific training can be detrimental to OOD datasets like AQuA. In contrast, Platypus fine-tunes LLMs on a wide range of text and math reasoning datasets. it improves the opensource SoTA on several datasets. Similarly, MAmmo TH can achieve universal improvement across the board. A major observation is that MAmmoTH is particularly strong at solving more complex math problems in MATH, where the gain of our model over WizardMath (open-source SoTA on MATH) can exceed 25% at different scales."
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3134
                },
                {
                    "x": 1289,
                    "y": 3134
                },
                {
                    "x": 1289,
                    "y": 3170
                },
                {
                    "x": 1260,
                    "y": 3170
                }
            ],
            "category": "footer",
            "html": "<footer id='64' style='font-size:16px'>7</footer>",
            "id": 64,
            "page": 7,
            "text": "7"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 111
                },
                {
                    "x": 899,
                    "y": 111
                },
                {
                    "x": 899,
                    "y": 159
                },
                {
                    "x": 444,
                    "y": 159
                }
            ],
            "category": "header",
            "html": "<header id='65' style='font-size:22px'>Preprint. Work in Progress</header>",
            "id": 65,
            "page": 8,
            "text": "Preprint. Work in Progress"
        },
        {
            "bounding_box": [
                {
                    "x": 483,
                    "y": 327
                },
                {
                    "x": 2066,
                    "y": 327
                },
                {
                    "x": 2066,
                    "y": 801
                },
                {
                    "x": 483,
                    "y": 801
                }
            ],
            "category": "figure",
            "html": "<figure><img id='66' style='font-size:14px' alt=\"60.0\n48.1 47.9\n42.6 41.6 41.0\n39.9\nAccuracy 40.0\n32.8 32.0\n29.6 29.8\n27.0\n22.9\n20.4 19.9\n20.0\n8.6\n0.0\nGSM + MATH Out-of-domain Overall\n■ Llama-2 Base WizardMath (GSM + MATH CoT) MAmmoTH (MathInstruct - CoT)\n■ MAmmoTH (MathInstruct - PoT) MAmmoTH (MathInstruct - Hybrid)\" data-coord=\"top-left:(483,327); bottom-right:(2066,801)\" /></figure>",
            "id": 66,
            "page": 8,
            "text": "60.0 48.1 47.9 42.6 41.6 41.0 39.9 Accuracy 40.0 32.8 32.0 29.6 29.8 27.0 22.9 20.4 19.9 20.0 8.6 0.0 GSM + MATH Out-of-domain Overall ■ Llama-2 Base WizardMath (GSM + MATH CoT) MAmmoTH (MathInstruct - CoT) ■ MAmmoTH (MathInstruct - PoT) MAmmoTH (MathInstruct - Hybrid)"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 851
                },
                {
                    "x": 2107,
                    "y": 851
                },
                {
                    "x": 2107,
                    "y": 1177
                },
                {
                    "x": 443,
                    "y": 1177
                }
            ],
            "category": "paragraph",
            "html": "<p id='67' style='font-size:20px'>Figure 2: Investigation of the influence of CoT & PoT hybrid training on the 7B Llama-2 model.<br>\"Out-of-domain\" refers to the five datasets detailed in Table 2. Key insights include: 1) The SoTA<br>model, utilizing dataset-specific CoT fine-tuning on GSM and MATH, displays strong performance<br>within its domains but struggles in OOD scenarios; 2) Diverse data sources in MathInstruct<br>enable better math generalist model; 3) Fine-tuning on the PoT subsets generally outperforms fine-<br>tuning on the CoT subsets; 4) Hybrid training yields the best-performing model. The breakdown<br>results on each dataset can be found in Appendix Table 6.</p>",
            "id": 67,
            "page": 8,
            "text": "Figure 2: Investigation of the influence of CoT & PoT hybrid training on the 7B Llama-2 model. \"Out-of-domain\" refers to the five datasets detailed in Table 2. Key insights include: 1) The SoTA model, utilizing dataset-specific CoT fine-tuning on GSM and MATH, displays strong performance within its domains but struggles in OOD scenarios; 2) Diverse data sources in MathInstruct enable better math generalist model; 3) Fine-tuning on the PoT subsets generally outperforms finetuning on the CoT subsets; 4) Hybrid training yields the best-performing model. The breakdown results on each dataset can be found in Appendix Table 6."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1248
                },
                {
                    "x": 2107,
                    "y": 1248
                },
                {
                    "x": 2107,
                    "y": 1617
                },
                {
                    "x": 441,
                    "y": 1617
                }
            ],
            "category": "paragraph",
            "html": "<p id='68' style='font-size:16px'>From Table 4, we can observe that our main competitor for OOD datasets is Platypus (Lee et al.,<br>2023). Similar to in-domain results, Platypus is able to yield gains over the baseline models univer-<br>sally across the board, especially on the MMLU-Math dataset, which is tied with MAmmo TH-70B. It<br>is worth noting that the performance gains of our model on OOD datasets are even more significant<br>than on in-domain datasets. This demonstrates our models' remarkable generalizability to unseen<br>math problems. Notably, MAmmo TH-7B also boosts the CoT performance of WizardMath-7B greatly<br>on MMLU-Math by 9%, which contains a substantial number of questions beyond the subjects we<br>covered in our training dataset.</p>",
            "id": 68,
            "page": 8,
            "text": "From Table 4, we can observe that our main competitor for OOD datasets is Platypus (Lee , 2023). Similar to in-domain results, Platypus is able to yield gains over the baseline models universally across the board, especially on the MMLU-Math dataset, which is tied with MAmmo TH-70B. It is worth noting that the performance gains of our model on OOD datasets are even more significant than on in-domain datasets. This demonstrates our models' remarkable generalizability to unseen math problems. Notably, MAmmo TH-7B also boosts the CoT performance of WizardMath-7B greatly on MMLU-Math by 9%, which contains a substantial number of questions beyond the subjects we covered in our training dataset."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1640
                },
                {
                    "x": 2108,
                    "y": 1640
                },
                {
                    "x": 2108,
                    "y": 1965
                },
                {
                    "x": 441,
                    "y": 1965
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='69' style='font-size:16px'>Comparison between Different Base Models. In our experiments, we experimented with both<br>Llama-2 and Code-Llama as the base models. From the two tables, we can observe that Code-<br>Llama is consistently better than Llama-2, especially on OOD datasets. The gap between MAmmo TH<br>and MAmmo TH-Coder can even reach up to 5%. Surprisingly, the average performance on<br>OOD datasets of MAmmo TH-Coder (34B) is actually higher than MAmmo TH (70B). We believe<br>MAmmo TH-Coder benefits greatly from the continuous code training of Code-Llama, which not<br>only enhances the PoT capabilities but also improves Llama's general reasoning skills.</p>",
            "id": 69,
            "page": 8,
            "text": "Comparison between Different Base Models. In our experiments, we experimented with both Llama-2 and Code-Llama as the base models. From the two tables, we can observe that CodeLlama is consistently better than Llama-2, especially on OOD datasets. The gap between MAmmo TH and MAmmo TH-Coder can even reach up to 5%. Surprisingly, the average performance on OOD datasets of MAmmo TH-Coder (34B) is actually higher than MAmmo TH (70B). We believe MAmmo TH-Coder benefits greatly from the continuous code training of Code-Llama, which not only enhances the PoT capabilities but also improves Llama's general reasoning skills."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 2052
                },
                {
                    "x": 1207,
                    "y": 2052
                },
                {
                    "x": 1207,
                    "y": 2100
                },
                {
                    "x": 445,
                    "y": 2100
                }
            ],
            "category": "paragraph",
            "html": "<p id='70' style='font-size:14px'>3.4 ABLATION STUDY ON DATA SOURCE</p>",
            "id": 70,
            "page": 8,
            "text": "3.4 ABLATION STUDY ON DATA SOURCE"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2153
                },
                {
                    "x": 2106,
                    "y": 2153
                },
                {
                    "x": 2106,
                    "y": 2289
                },
                {
                    "x": 442,
                    "y": 2289
                }
            ],
            "category": "paragraph",
            "html": "<p id='71' style='font-size:16px'>Ablation of the Data Source. In order to better understand what factors contribute to the great gain<br>of MAmmo TH over existing baselines, we set up a group of control experiments in Figure 2. We study<br>the following setups:</p>",
            "id": 71,
            "page": 8,
            "text": "Ablation of the Data Source. In order to better understand what factors contribute to the great gain of MAmmo TH over existing baselines, we set up a group of control experiments in Figure 2. We study the following setups:"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2314
                },
                {
                    "x": 2107,
                    "y": 2314
                },
                {
                    "x": 2107,
                    "y": 2498
                },
                {
                    "x": 441,
                    "y": 2498
                }
            ],
            "category": "paragraph",
            "html": "<p id='72' style='font-size:16px'>(1) MAmmo TH (MathInstruct- CoT): This experiment aims to understand how much our curated<br>CoT data could improve the generalization over the SoTA model WizardMath (Luo et al., 2023)<br>trained specifically on GSM + MATH. As can be seen, while sacrificing accuracy on GSM + MATH<br>by 3%, our CoT subset fine-tuning improves the overall nine-dataset accuracy from 27% to 32%.</p>",
            "id": 72,
            "page": 8,
            "text": "(1) MAmmo TH (MathInstruct- CoT): This experiment aims to understand how much our curated CoT data could improve the generalization over the SoTA model WizardMath (Luo , 2023) trained specifically on GSM + MATH. As can be seen, while sacrificing accuracy on GSM + MATH by 3%, our CoT subset fine-tuning improves the overall nine-dataset accuracy from 27% to 32%."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2522
                },
                {
                    "x": 2107,
                    "y": 2522
                },
                {
                    "x": 2107,
                    "y": 2704
                },
                {
                    "x": 441,
                    "y": 2704
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='73' style='font-size:18px'>(2) MAmmo TH (MathInstruct- PoT): This experiment aims to understand the advantage of our<br>PoT subset. As can be observed, our PoT subset fine-tuning can significantly improve the overall ac-<br>curacy from 27% to 41 %. This ablation reflects the importance of unlocking the program generation<br>capabilities of our model.</p>",
            "id": 73,
            "page": 8,
            "text": "(2) MAmmo TH (MathInstruct- PoT): This experiment aims to understand the advantage of our PoT subset. As can be observed, our PoT subset fine-tuning can significantly improve the overall accuracy from 27% to 41 %. This ablation reflects the importance of unlocking the program generation capabilities of our model."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2732
                },
                {
                    "x": 2104,
                    "y": 2732
                },
                {
                    "x": 2104,
                    "y": 2821
                },
                {
                    "x": 443,
                    "y": 2821
                }
            ],
            "category": "paragraph",
            "html": "<p id='74' style='font-size:16px'>(3) MAmmo TH (MathInst ruct- Hybrid): We further combine CoT and PoT as the hybrid training<br>data to achieve the best overall performance of 47.9%. This combined gain comes from two aspects:</p>",
            "id": 74,
            "page": 8,
            "text": "(3) MAmmo TH (MathInst ruct- Hybrid): We further combine CoT and PoT as the hybrid training data to achieve the best overall performance of 47.9%. This combined gain comes from two aspects:"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2845
                },
                {
                    "x": 2108,
                    "y": 2845
                },
                {
                    "x": 2108,
                    "y": 3056
                },
                {
                    "x": 442,
                    "y": 3056
                }
            ],
            "category": "paragraph",
            "html": "<p id='75' style='font-size:18px'>· The CoT subset helps maintain generic language-based reasoning skills to handle scenarios where<br>PoT cannot handle well, e.g., abstract reasoning multi-choice questions in AQuA and MMLU.<br>· The PoT subset can teach the model how to utilize Python APIs to solve complex math problems<br>with high precision, e.g., the MATH problems requiring complex computation.</p>",
            "id": 75,
            "page": 8,
            "text": "· The CoT subset helps maintain generic language-based reasoning skills to handle scenarios where PoT cannot handle well, e.g., abstract reasoning multi-choice questions in AQuA and MMLU. · The PoT subset can teach the model how to utilize Python APIs to solve complex math problems with high precision, e.g., the MATH problems requiring complex computation."
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3134
                },
                {
                    "x": 1289,
                    "y": 3134
                },
                {
                    "x": 1289,
                    "y": 3171
                },
                {
                    "x": 1260,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='76' style='font-size:16px'>8</footer>",
            "id": 76,
            "page": 8,
            "text": "8"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 112
                },
                {
                    "x": 899,
                    "y": 112
                },
                {
                    "x": 899,
                    "y": 159
                },
                {
                    "x": 445,
                    "y": 159
                }
            ],
            "category": "header",
            "html": "<header id='77' style='font-size:20px'>Preprint. Work in Progress</header>",
            "id": 77,
            "page": 9,
            "text": "Preprint. Work in Progress"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 336
                },
                {
                    "x": 2094,
                    "y": 336
                },
                {
                    "x": 2094,
                    "y": 820
                },
                {
                    "x": 442,
                    "y": 820
                }
            ],
            "category": "table",
            "html": "<table id='78' style='font-size:14px'><tr><td>Training Data</td><td>GSM</td><td>MATH</td><td>AQuA</td><td>NumG</td><td>SVA</td><td>Mat</td><td>Sim</td><td>SAT</td><td>MMLU</td><td>AVG</td></tr><tr><td></td><td>14.6</td><td>2.5</td><td>30.3</td><td>29.9</td><td>34.5</td><td>6.0</td><td>5.0</td><td>26.8</td><td>29.8</td><td>-25.3</td></tr><tr><td>G</td><td>56.6</td><td>9.2</td><td>24.4</td><td>32.1</td><td>65.4</td><td>20.5</td><td>12.3</td><td>27.2</td><td>25.2</td><td>-22.7</td></tr><tr><td>G + M</td><td>58.1</td><td>28.2</td><td>26.0</td><td>34.7</td><td>64.8</td><td>50.1</td><td>17.1</td><td>28.6</td><td>28.4</td><td>-19.5</td></tr><tr><td>G + M + C</td><td>57.4</td><td>28.5</td><td>26.2</td><td>37.5</td><td>65.3</td><td>50.4</td><td>17.7</td><td>29.3</td><td>28.7</td><td>-19.2</td></tr><tr><td>G + M + C + A</td><td>57.5</td><td>29.1</td><td>46.9</td><td>42.2</td><td>65.8</td><td>49.6</td><td>32.7</td><td>42.3</td><td>43.1</td><td>-4.8</td></tr><tr><td>G + M + C+ A + N</td><td>56.5</td><td>28.9</td><td>38.2</td><td>63.7</td><td>64.1</td><td>47.9</td><td>40.8</td><td>38.6</td><td>44.5</td><td>-3.4</td></tr><tr><td>Existing Data</td><td>31.4</td><td>18.4</td><td>40.3</td><td>53.3</td><td>61.8</td><td>27.9</td><td>45.6</td><td>32.7</td><td>38.4</td><td>-9.0</td></tr><tr><td>MathInstruct</td><td>53.6</td><td>31.5</td><td>44.5</td><td>61.2</td><td>67.7</td><td>46.3</td><td>41.2</td><td>42.7</td><td>42.6</td><td>47.9</td></tr></table>",
            "id": 78,
            "page": 9,
            "text": "Training Data GSM MATH AQuA NumG SVA Mat Sim SAT MMLU AVG   14.6 2.5 30.3 29.9 34.5 6.0 5.0 26.8 29.8 -25.3  G 56.6 9.2 24.4 32.1 65.4 20.5 12.3 27.2 25.2 -22.7  G + M 58.1 28.2 26.0 34.7 64.8 50.1 17.1 28.6 28.4 -19.5  G + M + C 57.4 28.5 26.2 37.5 65.3 50.4 17.7 29.3 28.7 -19.2  G + M + C + A 57.5 29.1 46.9 42.2 65.8 49.6 32.7 42.3 43.1 -4.8  G + M + C+ A + N 56.5 28.9 38.2 63.7 64.1 47.9 40.8 38.6 44.5 -3.4  Existing Data 31.4 18.4 40.3 53.3 61.8 27.9 45.6 32.7 38.4 -9.0  MathInstruct 53.6 31.5 44.5 61.2 67.7 46.3 41.2 42.7 42.6"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 851
                },
                {
                    "x": 2107,
                    "y": 851
                },
                {
                    "x": 2107,
                    "y": 1039
                },
                {
                    "x": 442,
                    "y": 1039
                }
            ],
            "category": "paragraph",
            "html": "<p id='79' style='font-size:16px'>Table 5: Influence of different major subsets in Ma thInstruct based on Llama-2 7B. G: GSM8K,<br>M: MATH, C: Camel, A: AQuA, N: NumGLUE. \"Existing data\": the subset of MathInstruct<br>in Table 1 by excluding all the NEW rationales curated by us. We shorten Mathematics as Mat,<br>SimulEq as Sim, NumGLUE as NumG, and SVAMP as SVA to save space.</p>",
            "id": 79,
            "page": 9,
            "text": "Table 5: Influence of different major subsets in Ma thInstruct based on Llama-2 7B. G: GSM8K, M: MATH, C: Camel, A: AQuA, N: NumGLUE. \"Existing data\": the subset of MathInstruct in Table 1 by excluding all the NEW rationales curated by us. We shorten Mathematics as Mat, SimulEq as Sim, NumGLUE as NumG, and SVAMP as SVA to save space."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1132
                },
                {
                    "x": 2105,
                    "y": 1132
                },
                {
                    "x": 2105,
                    "y": 1319
                },
                {
                    "x": 441,
                    "y": 1319
                }
            ],
            "category": "paragraph",
            "html": "<p id='80' style='font-size:18px'>We put some case studies in Appendix B to demonstrate the respective advantages of PoT and CoT<br>in solving different types of math problems. To summarize, we attribute our substantial gain to: 1)<br>diverse data sources covering different math fields and complexity levels and 2) a hybrid of CoT &<br>PoT instruction tuning and decoding strategy.</p>",
            "id": 80,
            "page": 9,
            "text": "We put some case studies in Appendix B to demonstrate the respective advantages of PoT and CoT in solving different types of math problems. To summarize, we attribute our substantial gain to: 1) diverse data sources covering different math fields and complexity levels and 2) a hybrid of CoT & PoT instruction tuning and decoding strategy."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1376
                },
                {
                    "x": 2107,
                    "y": 1376
                },
                {
                    "x": 2107,
                    "y": 1792
                },
                {
                    "x": 443,
                    "y": 1792
                }
            ],
            "category": "paragraph",
            "html": "<p id='81' style='font-size:18px'>Influence of Major Subsets. Given the diverse sources of MathInstruct used in training<br>MAmmo TH, it is important to understand how each dataset contributes to the overall performance of<br>the model. We focus on five significant subsets: GSM8K, MATH, Camel, AQuA and NumGLUE.<br>We conduct an experiment gradually adding each dataset into training and compare the performance<br>with the one fine-tuned on the whole MathInstruct. As we can see from Table 5, when the data<br>is not very diverse in training at the beginning (e.g., GSM8K only), the overall generalization perfor-<br>mance is very bad: the model only fits in-distribution data and struggles to answer questions beyond<br>GSM questions. And when gradually adding other major subsets, besides seeing the improvements<br>on its own test sets overall, we could observe MAmmo TH becomes a better math generalist.</p>",
            "id": 81,
            "page": 9,
            "text": "Influence of Major Subsets. Given the diverse sources of MathInstruct used in training MAmmo TH, it is important to understand how each dataset contributes to the overall performance of the model. We focus on five significant subsets: GSM8K, MATH, Camel, AQuA and NumGLUE. We conduct an experiment gradually adding each dataset into training and compare the performance with the one fine-tuned on the whole MathInstruct. As we can see from Table 5, when the data is not very diverse in training at the beginning (e.g., GSM8K only), the overall generalization performance is very bad: the model only fits in-distribution data and struggles to answer questions beyond GSM questions. And when gradually adding other major subsets, besides seeing the improvements on its own test sets overall, we could observe MAmmo TH becomes a better math generalist."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1814
                },
                {
                    "x": 2108,
                    "y": 1814
                },
                {
                    "x": 2108,
                    "y": 1999
                },
                {
                    "x": 441,
                    "y": 1999
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='82' style='font-size:16px'>These results underscore the significant impact of diverse data sources on MAmmo TH performance,<br>a core aspect of making MAmmoTH a math generalist. The results also provide valuable insights<br>for future data curation and collection efforts (e.g., we should always collect diverse data and avoid<br>collecting only specific types of data).</p>",
            "id": 82,
            "page": 9,
            "text": "These results underscore the significant impact of diverse data sources on MAmmo TH performance, a core aspect of making MAmmoTH a math generalist. The results also provide valuable insights for future data curation and collection efforts (e.g., we should always collect diverse data and avoid collecting only specific types of data)."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2021
                },
                {
                    "x": 2107,
                    "y": 2021
                },
                {
                    "x": 2107,
                    "y": 2207
                },
                {
                    "x": 441,
                    "y": 2207
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='83' style='font-size:14px'>To help understand the contribution of the 6 newly curated datasets as shown in Table 1, we remove<br>them from MathInstruct, and train a model on the existing data. As shown in the last two rows<br>of Table 5, our new curated data substantially improves the performance on many datasets and leads<br>to a 9% overall increase, which reflects the importance of the NEWLY curated dataset.</p>",
            "id": 83,
            "page": 9,
            "text": "To help understand the contribution of the 6 newly curated datasets as shown in Table 1, we remove them from MathInstruct, and train a model on the existing data. As shown in the last two rows of Table 5, our new curated data substantially improves the performance on many datasets and leads to a 9% overall increase, which reflects the importance of the NEWLY curated dataset."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2266
                },
                {
                    "x": 2108,
                    "y": 2266
                },
                {
                    "x": 2108,
                    "y": 2546
                },
                {
                    "x": 441,
                    "y": 2546
                }
            ],
            "category": "paragraph",
            "html": "<p id='84' style='font-size:18px'>Influence of Hybrid Decoding. To demonstrate the effectiveness of the hybrid decoding method,<br>we conduct an experiment as outlined in subsection 2.4. By default, we initially attempt the PoT<br>decoding method for a given question. Ifit fails to generate an executable query, we then transition to<br>the CoT decoding method. The performance of different decoding methods (CoT, PoT, and Hybrid)<br>is shown in Table 7. This hybrid decoding improves performance on every test set, showcasing that<br>our model can effectively leverage the strengths of both CoT and PoT decoding strategies.</p>",
            "id": 84,
            "page": 9,
            "text": "Influence of Hybrid Decoding. To demonstrate the effectiveness of the hybrid decoding method, we conduct an experiment as outlined in subsection 2.4. By default, we initially attempt the PoT decoding method for a given question. Ifit fails to generate an executable query, we then transition to the CoT decoding method. The performance of different decoding methods (CoT, PoT, and Hybrid) is shown in Table 7. This hybrid decoding improves performance on every test set, showcasing that our model can effectively leverage the strengths of both CoT and PoT decoding strategies."
        },
        {
            "bounding_box": [
                {
                    "x": 446,
                    "y": 2623
                },
                {
                    "x": 819,
                    "y": 2623
                },
                {
                    "x": 819,
                    "y": 2671
                },
                {
                    "x": 446,
                    "y": 2671
                }
            ],
            "category": "paragraph",
            "html": "<p id='85' style='font-size:22px'>4 CONCLUSION</p>",
            "id": 85,
            "page": 9,
            "text": "4 CONCLUSION"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2730
                },
                {
                    "x": 2108,
                    "y": 2730
                },
                {
                    "x": 2108,
                    "y": 3053
                },
                {
                    "x": 442,
                    "y": 3053
                }
            ],
            "category": "paragraph",
            "html": "<p id='86' style='font-size:18px'>In this paper, we propose a novel math instruction tuning approach to activate open-source LLMs'<br>mathematical reasoning capabilities. Through a comprehensive study, we show that our models<br>can outperform the SoTA performance at different scales by a huge margin. Our models benefit<br>massively from: 1) the broad coverage of different math fields and complexity levels, and 2) a<br>hybrid of CoT and PoT training. Our instruction tuning dataset contains 260K samples, which<br>makes fine-tuning highly affordable even for academic labs. Our work paves the road for future<br>studies to activate LLMs' core capabilities in specialized domains.</p>",
            "id": 86,
            "page": 9,
            "text": "In this paper, we propose a novel math instruction tuning approach to activate open-source LLMs' mathematical reasoning capabilities. Through a comprehensive study, we show that our models can outperform the SoTA performance at different scales by a huge margin. Our models benefit massively from: 1) the broad coverage of different math fields and complexity levels, and 2) a hybrid of CoT and PoT training. Our instruction tuning dataset contains 260K samples, which makes fine-tuning highly affordable even for academic labs. Our work paves the road for future studies to activate LLMs' core capabilities in specialized domains."
        },
        {
            "bounding_box": [
                {
                    "x": 1259,
                    "y": 3133
                },
                {
                    "x": 1289,
                    "y": 3133
                },
                {
                    "x": 1289,
                    "y": 3169
                },
                {
                    "x": 1259,
                    "y": 3169
                }
            ],
            "category": "footer",
            "html": "<footer id='87' style='font-size:14px'>9</footer>",
            "id": 87,
            "page": 9,
            "text": "9"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 113
                },
                {
                    "x": 898,
                    "y": 113
                },
                {
                    "x": 898,
                    "y": 157
                },
                {
                    "x": 444,
                    "y": 157
                }
            ],
            "category": "header",
            "html": "<header id='88' style='font-size:22px'>Preprint. Work in Progress</header>",
            "id": 88,
            "page": 10,
            "text": "Preprint. Work in Progress"
        },
        {
            "bounding_box": [
                {
                    "x": 447,
                    "y": 342
                },
                {
                    "x": 734,
                    "y": 342
                },
                {
                    "x": 734,
                    "y": 391
                },
                {
                    "x": 447,
                    "y": 391
                }
            ],
            "category": "paragraph",
            "html": "<p id='89' style='font-size:20px'>REFERENCES</p>",
            "id": 89,
            "page": 10,
            "text": "REFERENCES"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 422
                },
                {
                    "x": 2108,
                    "y": 422
                },
                {
                    "x": 2108,
                    "y": 698
                },
                {
                    "x": 443,
                    "y": 698
                }
            ],
            "category": "paragraph",
            "html": "<p id='90' style='font-size:18px'>Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh<br>Hajishirzi. MathQA: Towards interpretable math word problem solving with operation-based<br>formalisms. In Proceedings of the 2019 Conference of the North American Chapter of the<br>Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long<br>and Short Papers), pp. 2357-2367, 2019. doi: 10.18653/y1/N19-1245. URL https : //<br>aclanthology · org/N19-1245.</p>",
            "id": 90,
            "page": 10,
            "text": "Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. MathQA: Towards interpretable math word problem solving with operation-based formalisms. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 2357-2367, 2019. doi: 10.18653/y1/N19-1245. URL https : // aclanthology · org/N19-1245."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 733
                },
                {
                    "x": 2105,
                    "y": 733
                },
                {
                    "x": 2105,
                    "y": 871
                },
                {
                    "x": 442,
                    "y": 871
                }
            ],
            "category": "paragraph",
            "html": "<p id='91' style='font-size:16px'>Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,<br>Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report.<br>ArXiv preprint, abs/2305.10403, 2023. URL https : / / arxiv · org/ abs / 2305 · 10403.</p>",
            "id": 91,
            "page": 10,
            "text": "Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen,  Palm 2 technical report. ArXiv preprint, abs/2305.10403, 2023. URL https : / / arxiv · org/ abs / 2305 · 10403."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 904
                },
                {
                    "x": 2108,
                    "y": 904
                },
                {
                    "x": 2108,
                    "y": 1087
                },
                {
                    "x": 443,
                    "y": 1087
                }
            ],
            "category": "paragraph",
            "html": "<p id='92' style='font-size:14px'>Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,<br>Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harm-<br>lessness from ai feedback. ArXiv preprint, abs/2212.08073, 2022. URL https : / / arxiv ·<br>org/ abs / 2212 · 08073.</p>",
            "id": 92,
            "page": 10,
            "text": "Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon,  Constitutional ai: Harmlessness from ai feedback. ArXiv preprint, abs/2212.08073, 2022. URL https : / / arxiv · org/ abs / 2212 · 08073."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1124
                },
                {
                    "x": 2107,
                    "y": 1124
                },
                {
                    "x": 2107,
                    "y": 1308
                },
                {
                    "x": 442,
                    "y": 1308
                }
            ],
            "category": "paragraph",
            "html": "<p id='93' style='font-size:16px'>Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared<br>Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large<br>language models trained on code. ArXiv preprint, abs/2107.03374, 2021. URL https : / /<br>arxiv · org/ abs / 2107 · 03374.</p>",
            "id": 93,
            "page": 10,
            "text": "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman,  Evaluating large language models trained on code. ArXiv preprint, abs/2107.03374, 2021. URL https : / / arxiv · org/ abs / 2107 · 03374."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 1343
                },
                {
                    "x": 2106,
                    "y": 1343
                },
                {
                    "x": 2106,
                    "y": 1482
                },
                {
                    "x": 444,
                    "y": 1482
                }
            ],
            "category": "paragraph",
            "html": "<p id='94' style='font-size:16px'>Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompt-<br>ing: Disentangling computation from reasoning for numerical reasoning tasks. ArXiv preprint,<br>abs/2211.12588, 2022. URL https : / / arxiv · org/ abs / 2211 · 12588.</p>",
            "id": 94,
            "page": 10,
            "text": "Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. ArXiv preprint, abs/2211.12588, 2022. URL https : / / arxiv · org/ abs / 2211 · 12588."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1516
                },
                {
                    "x": 2105,
                    "y": 1516
                },
                {
                    "x": 2105,
                    "y": 1654
                },
                {
                    "x": 443,
                    "y": 1654
                }
            ],
            "category": "paragraph",
            "html": "<p id='95' style='font-size:16px'>Wenhu Chen, Ming Yin, Max Ku, Elaine Wan, Xueguang Ma, Jianyu Xu, Tony Xia, Xinyi<br>Wang, and Pan Lu. Theoremqa: A theorem-driven question answering dataset. ArXiv preprint,<br>abs/2305.12524, 2023. URL https : / /arxiv · org/ abs / 2305 · 12524.</p>",
            "id": 95,
            "page": 10,
            "text": "Wenhu Chen, Ming Yin, Max Ku, Elaine Wan, Xueguang Ma, Jianyu Xu, Tony Xia, Xinyi Wang, and Pan Lu. Theoremqa: A theorem-driven question answering dataset. ArXiv preprint, abs/2305.12524, 2023. URL https : / /arxiv · org/ abs / 2305 · 12524."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1690
                },
                {
                    "x": 2107,
                    "y": 1690
                },
                {
                    "x": 2107,
                    "y": 1829
                },
                {
                    "x": 441,
                    "y": 1829
                }
            ],
            "category": "paragraph",
            "html": "<p id='96' style='font-size:16px'>Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi<br>Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language mod-<br>els. ArXiv preprint, abs/2210.11416, 2022. URL https : / / arxiv · org/ abs / 2210 · 11416.</p>",
            "id": 96,
            "page": 10,
            "text": "Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma,  Scaling instruction-finetuned language models. ArXiv preprint, abs/2210.11416, 2022. URL https : / / arxiv · org/ abs / 2210 · 11416."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1863
                },
                {
                    "x": 2108,
                    "y": 1863
                },
                {
                    "x": 2108,
                    "y": 2048
                },
                {
                    "x": 443,
                    "y": 2048
                }
            ],
            "category": "paragraph",
            "html": "<p id='97' style='font-size:16px'>Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,<br>Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to<br>solve math word problems. ArXiv preprint, abs/2110.14168, 2021. URL https : / / arxiv.<br>org/ abs / 2110 · 141 68.</p>",
            "id": 97,
            "page": 10,
            "text": "Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,  Training verifiers to solve math word problems. ArXiv preprint, abs/2110.14168, 2021. URL https : / / arxiv. org/ abs / 2110 · 141 68."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2083
                },
                {
                    "x": 2106,
                    "y": 2083
                },
                {
                    "x": 2106,
                    "y": 2265
                },
                {
                    "x": 444,
                    "y": 2265
                }
            ],
            "category": "paragraph",
            "html": "<p id='98' style='font-size:14px'>Alex Davies, Petar Velickovic, Lars Buesing, Sam Blackwell, Daniel Zheng, Nenad Tomasev,<br>Richard Tanburn, Peter Battaglia, Charles Blundell, Andras Juhasz, et al. Advancing mathe-<br>matics by guiding human intuition with ai. Nature, 600(7887):70-74, 2021. URL https :<br>/ / www · nature · com/ articles / s41586-021-0408 6-x.</p>",
            "id": 98,
            "page": 10,
            "text": "Alex Davies, Petar Velickovic, Lars Buesing, Sam Blackwell, Daniel Zheng, Nenad Tomasev, Richard Tanburn, Peter Battaglia, Charles Blundell, Andras Juhasz,  Advancing mathematics by guiding human intuition with ai. Nature, 600(7887):70-74, 2021. URL https : / / www · nature · com/ articles / s41586-021-0408 6-x."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2301
                },
                {
                    "x": 2107,
                    "y": 2301
                },
                {
                    "x": 2107,
                    "y": 2439
                },
                {
                    "x": 443,
                    "y": 2439
                }
            ],
            "category": "paragraph",
            "html": "<p id='99' style='font-size:14px'>Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning<br>of quantized llms. ArXiv preprint, abs/2305.14314, 2023. URL https : / / arxiv · org/ abs/<br>2305 · 14314.</p>",
            "id": 99,
            "page": 10,
            "text": "Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning of quantized llms. ArXiv preprint, abs/2305.14314, 2023. URL https : / / arxiv · org/ abs/ 2305 · 14314."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2476
                },
                {
                    "x": 2105,
                    "y": 2476
                },
                {
                    "x": 2105,
                    "y": 2660
                },
                {
                    "x": 443,
                    "y": 2660
                }
            ],
            "category": "paragraph",
            "html": "<p id='100' style='font-size:18px'>Andrew Drozdov, Nathanael Scharli, Ekin Akyurek, Nathan Scales, Xinying Song, Xinyun Chen,<br>Olivier Bousquet, and Denny Zhou. Compositional semantic parsing with large language mod-<br>els. International Conference on Learning Representations (ICLR), 2023. URL https :<br>/ / openreview · net / forum? id=gJW8hSGBys8.</p>",
            "id": 100,
            "page": 10,
            "text": "Andrew Drozdov, Nathanael Scharli, Ekin Akyurek, Nathan Scales, Xinying Song, Xinyun Chen, Olivier Bousquet, and Denny Zhou. Compositional semantic parsing with large language models. International Conference on Learning Representations (ICLR), 2023. URL https : / / openreview · net / forum? id=gJW8hSGBys8."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2695
                },
                {
                    "x": 2106,
                    "y": 2695
                },
                {
                    "x": 2106,
                    "y": 2880
                },
                {
                    "x": 443,
                    "y": 2880
                }
            ],
            "category": "paragraph",
            "html": "<p id='101' style='font-size:18px'>Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and<br>Graham Neubig. Pal: Program-aided language models. In International Conference on Machine<br>Learning, pp. 10764-10799. PMLR, 2023. URL https : / /proceedings · mlr · press/<br>v202 / gao23f/ gao23f · pdf.</p>",
            "id": 101,
            "page": 10,
            "text": "Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models. In International Conference on Machine Learning, pp. 10764-10799. PMLR, 2023. URL https : / /proceedings · mlr · press/ v202 / gao23f/ gao23f · pdf."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2916
                },
                {
                    "x": 2107,
                    "y": 2916
                },
                {
                    "x": 2107,
                    "y": 3053
                },
                {
                    "x": 442,
                    "y": 3053
                }
            ],
            "category": "paragraph",
            "html": "<p id='102' style='font-size:16px'>Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen.<br>Critic: Large language models can self-correct with tool-interactive critiquing. ArXiv preprint,<br>abs/2305.11738, 2023. URL https : / / arxiv · org/ abs/2305 · 11 738.</p>",
            "id": 102,
            "page": 10,
            "text": "Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. Critic: Large language models can self-correct with tool-interactive critiquing. ArXiv preprint, abs/2305.11738, 2023. URL https : / / arxiv · org/ abs/2305 · 11 738."
        },
        {
            "bounding_box": [
                {
                    "x": 1252,
                    "y": 3133
                },
                {
                    "x": 1300,
                    "y": 3133
                },
                {
                    "x": 1300,
                    "y": 3172
                },
                {
                    "x": 1252,
                    "y": 3172
                }
            ],
            "category": "footer",
            "html": "<footer id='103' style='font-size:14px'>10</footer>",
            "id": 103,
            "page": 10,
            "text": "10"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 113
                },
                {
                    "x": 899,
                    "y": 113
                },
                {
                    "x": 899,
                    "y": 157
                },
                {
                    "x": 443,
                    "y": 157
                }
            ],
            "category": "header",
            "html": "<header id='104' style='font-size:22px'>Preprint. Work in Progress</header>",
            "id": 104,
            "page": 11,
            "text": "Preprint. Work in Progress"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 345
                },
                {
                    "x": 2107,
                    "y": 345
                },
                {
                    "x": 2107,
                    "y": 532
                },
                {
                    "x": 442,
                    "y": 532
                }
            ],
            "category": "paragraph",
            "html": "<p id='105' style='font-size:18px'>Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob<br>Steinhardt. Measuring massive multitask language understanding. In 9th International Confer-<br>ence on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021a.<br>URL https : / / openreview · net / forum?id=d7KB jmI 3GmQ.</p>",
            "id": 105,
            "page": 11,
            "text": "Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021, 2021a. URL https : / / openreview · net / forum?id=d7KB jmI 3GmQ."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 563
                },
                {
                    "x": 2107,
                    "y": 563
                },
                {
                    "x": 2107,
                    "y": 839
                },
                {
                    "x": 443,
                    "y": 839
                }
            ],
            "category": "paragraph",
            "html": "<p id='106' style='font-size:16px'>Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric<br>Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solv-<br>ing with the math dataset. In Thirty-fifth Conference on Neural Information Pro-<br>cessing Systems Datasets and Benchmarks Track (Round 2), 2021b. URL https :<br>/ / datasets-benchmarks-proteedings · neurips · cc/paper /2021/file/<br>be83ab3ecd0db 773eb2dc1b0a1 7836a1 -Paper-round2 · pdf.</p>",
            "id": 106,
            "page": 11,
            "text": "Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021b. URL https : / / datasets-benchmarks-proteedings · neurips · cc/paper /2021/file/ be83ab3ecd0db 773eb2dc1b0a1 7836a1 -Paper-round2 · pdf."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 874
                },
                {
                    "x": 2108,
                    "y": 874
                },
                {
                    "x": 2108,
                    "y": 1057
                },
                {
                    "x": 443,
                    "y": 1057
                }
            ],
            "category": "paragraph",
            "html": "<p id='107' style='font-size:16px'>Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. Learning to<br>solve arithmetic word problems with verb categorization. In Proceedings of the 2014 Conference<br>on Empirical Methods in Natural Language Processing (EMNLP), pp. 523-533, 2014. doi: 10.<br>3115/v1/D14-1058. URL https : / / aclanthology · org/D14-1058.</p>",
            "id": 107,
            "page": 11,
            "text": "Mohammad Javad Hosseini, Hannaneh Hajishirzi, Oren Etzioni, and Nate Kushman. Learning to solve arithmetic word problems with verb categorization. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 523-533, 2014. doi: 10. 3115/v1/D14-1058. URL https : / / aclanthology · org/D14-1058."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1091
                },
                {
                    "x": 2106,
                    "y": 1091
                },
                {
                    "x": 2106,
                    "y": 1184
                },
                {
                    "x": 442,
                    "y": 1184
                }
            ],
            "category": "paragraph",
            "html": "<p id='108' style='font-size:18px'>Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large<br>language models are zero-shot reasoners. NeurIPS, 2022.</p>",
            "id": 108,
            "page": 11,
            "text": "Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. NeurIPS, 2022."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1216
                },
                {
                    "x": 2106,
                    "y": 1216
                },
                {
                    "x": 2106,
                    "y": 1402
                },
                {
                    "x": 441,
                    "y": 1402
                }
            ],
            "category": "paragraph",
            "html": "<p id='109' style='font-size:18px'>Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena Du-<br>mas Ang. Parsing algebraic word problems into equations. Transactions of the Association<br>for Computational Linguistics, 3:585-597, 2015. doi: 10.1162/tacl_a_00160. URL https :<br>/ / aclanthology · org/Q15-1042.</p>",
            "id": 109,
            "page": 11,
            "text": "Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena Dumas Ang. Parsing algebraic word problems into equations. Transactions of the Association for Computational Linguistics, 3:585-597, 2015. doi: 10.1162/tacl_a_00160. URL https : / / aclanthology · org/Q15-1042."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1435
                },
                {
                    "x": 2107,
                    "y": 1435
                },
                {
                    "x": 2107,
                    "y": 1664
                },
                {
                    "x": 441,
                    "y": 1664
                }
            ],
            "category": "paragraph",
            "html": "<p id='110' style='font-size:20px'>Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi.<br>MAWPS: A math word problem repository. In Proceedings of the 2016 Conference of the North<br>American Chapter of the Associationfor Computational Linguistics: Human Language Technolo-<br>gies, pp. 1152-1157, 2016. doi: 10.18653/v1/N16-1136. URL https : / / aclanthology ·<br>org/N1 6-1136.</p>",
            "id": 110,
            "page": 11,
            "text": "Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. MAWPS: A math word problem repository. In Proceedings of the 2016 Conference of the North American Chapter of the Associationfor Computational Linguistics: Human Language Technologies, pp. 1152-1157, 2016. doi: 10.18653/v1/N16-1136. URL https : / / aclanthology · org/N1 6-1136."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1699
                },
                {
                    "x": 2107,
                    "y": 1699
                },
                {
                    "x": 2107,
                    "y": 1835
                },
                {
                    "x": 443,
                    "y": 1835
                }
            ],
            "category": "paragraph",
            "html": "<p id='111' style='font-size:14px'>Ariel N Lee, Cole J Hunter, and Nataniel Ruiz. Platypus: Quick, cheap, and powerful refinement<br>of llms. ArXiv preprint, abs/2308.07317, 2023. URL https : / / arxiv · org/ abs /2308 ·<br>07317.</p>",
            "id": 111,
            "page": 11,
            "text": "Ariel N Lee, Cole J Hunter, and Nataniel Ruiz. Platypus: Quick, cheap, and powerful refinement of llms. ArXiv preprint, abs/2308.07317, 2023. URL https : / / arxiv · org/ abs /2308 · 07317."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 1870
                },
                {
                    "x": 2106,
                    "y": 1870
                },
                {
                    "x": 2106,
                    "y": 2055
                },
                {
                    "x": 444,
                    "y": 2055
                }
            ],
            "category": "paragraph",
            "html": "<p id='112' style='font-size:18px'>Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ra-<br>masesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative<br>reasoning problems with language models. Advances in Neural Information Processing Systems,<br>35:3843-3857, 2022. URL https : / / openreview · net /pdf?id=IFXTZERXdM7.</p>",
            "id": 112,
            "page": 11,
            "text": "Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo,  Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems, 35:3843-3857, 2022. URL https : / / openreview · net /pdf?id=IFXTZERXdM7."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2089
                },
                {
                    "x": 2106,
                    "y": 2089
                },
                {
                    "x": 2106,
                    "y": 2227
                },
                {
                    "x": 442,
                    "y": 2227
                }
            ],
            "category": "paragraph",
            "html": "<p id='113' style='font-size:14px'>Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem.<br>Camel: Communicative agents for\" mind\" exploration of large scale language model society.<br>ArXiv preprint, abs/2303.17760, 2023a. URL https : / / arxiv · org/ abs / 2303 · 17760.</p>",
            "id": 113,
            "page": 11,
            "text": "Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Communicative agents for\" mind\" exploration of large scale language model society. ArXiv preprint, abs/2303.17760, 2023a. URL https : / / arxiv · org/ abs / 2303 · 17760."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2260
                },
                {
                    "x": 2107,
                    "y": 2260
                },
                {
                    "x": 2107,
                    "y": 2446
                },
                {
                    "x": 444,
                    "y": 2446
                }
            ],
            "category": "paragraph",
            "html": "<p id='114' style='font-size:16px'>Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. Making<br>language models better reasoners with step-aware verifier. In Proceedings of the 61st Annual<br>Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 5315-<br>5333, 2023b. URL https : / / aclanthology · org/ 2023 · acl-long · 291 · pdf.</p>",
            "id": 114,
            "page": 11,
            "text": "Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. Making language models better reasoners with step-aware verifier. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 53155333, 2023b. URL https : / / aclanthology · org/ 2023 · acl-long · 291 · pdf."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2477
                },
                {
                    "x": 2106,
                    "y": 2477
                },
                {
                    "x": 2106,
                    "y": 2663
                },
                {
                    "x": 443,
                    "y": 2663
                }
            ],
            "category": "paragraph",
            "html": "<p id='115' style='font-size:18px'>Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale gener-<br>ation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual<br>Meeting ofthe Associationfor Computational Linguistics (Volume 1: Long Papers), pp. 158-167,<br>2017. doi: 10.18653/v1/P17-1015. URL https : / / aclanthology · org/P17-1015.</p>",
            "id": 115,
            "page": 11,
            "text": "Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale generation: Learning to solve and explain algebraic word problems. In Proceedings of the 55th Annual Meeting ofthe Associationfor Computational Linguistics (Volume 1: Long Papers), pp. 158-167, 2017. doi: 10.18653/v1/P17-1015. URL https : / / aclanthology · org/P17-1015."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2696
                },
                {
                    "x": 2106,
                    "y": 2696
                },
                {
                    "x": 2106,
                    "y": 2834
                },
                {
                    "x": 442,
                    "y": 2834
                }
            ],
            "category": "paragraph",
            "html": "<p id='116' style='font-size:18px'>Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V<br>Le, Barret Zoph, Jason Wei, et al. The flan collection: Designing data and methods for effective in-<br>struction tuning. ICML, 2023. URL https : / / openreview · net /pdf?id=ZX4uS605XV.</p>",
            "id": 116,
            "page": 11,
            "text": "Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei,  The flan collection: Designing data and methods for effective instruction tuning. ICML, 2023. URL https : / / openreview · net /pdf?id=ZX4uS605XV."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 2868
                },
                {
                    "x": 2107,
                    "y": 2868
                },
                {
                    "x": 2107,
                    "y": 3052
                },
                {
                    "x": 445,
                    "y": 3052
                }
            ],
            "category": "paragraph",
            "html": "<p id='117' style='font-size:20px'>Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qing-<br>wei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning<br>for large language models via reinforced evol-instruct. ArXiv preprint, abs/2308.09583, 2023.<br>URL https : / / arxiv · org/ abs/ 2308 · 09583.</p>",
            "id": 117,
            "page": 11,
            "text": "Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. ArXiv preprint, abs/2308.09583, 2023. URL https : / / arxiv · org/ abs/ 2308 · 09583."
        },
        {
            "bounding_box": [
                {
                    "x": 1253,
                    "y": 3133
                },
                {
                    "x": 1297,
                    "y": 3133
                },
                {
                    "x": 1297,
                    "y": 3172
                },
                {
                    "x": 1253,
                    "y": 3172
                }
            ],
            "category": "footer",
            "html": "<footer id='118' style='font-size:14px'>11</footer>",
            "id": 118,
            "page": 11,
            "text": "11"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 113
                },
                {
                    "x": 899,
                    "y": 113
                },
                {
                    "x": 899,
                    "y": 157
                },
                {
                    "x": 444,
                    "y": 157
                }
            ],
            "category": "header",
            "html": "<header id='119' style='font-size:22px'>Preprint. Work in Progress</header>",
            "id": 119,
            "page": 12,
            "text": "Preprint. Work in Progress"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 346
                },
                {
                    "x": 2109,
                    "y": 346
                },
                {
                    "x": 2109,
                    "y": 532
                },
                {
                    "x": 443,
                    "y": 532
                }
            ],
            "category": "paragraph",
            "html": "<p id='120' style='font-size:14px'>Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, and Graham Neubig. Language mod-<br>els of code are few-shot commonsense learners. In Proceedings of the 2022 Conference on<br>Empirical Methods in Natural Language Processing, pp. 1384-1403, 2022. URL https :<br>/ / aclanthology · org/ 2022 · emnlp-main · 90 · pdf.</p>",
            "id": 120,
            "page": 12,
            "text": "Aman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, and Graham Neubig. Language models of code are few-shot commonsense learners. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 1384-1403, 2022. URL https : / / aclanthology · org/ 2022 · emnlp-main · 90 · pdf."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 569
                },
                {
                    "x": 2108,
                    "y": 569
                },
                {
                    "x": 2108,
                    "y": 798
                },
                {
                    "x": 442,
                    "y": 798
                }
            ],
            "category": "paragraph",
            "html": "<p id='121' style='font-size:18px'>Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tan-<br>may Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, and Ashwin Kalyan. LILA:<br>A unified benchmark for mathematical reasoning. In Proceedings of the 2022 Conference on<br>Empirical Methods in Natural Language Processing, pp. 5807-5832, 2022a. URL https :<br>/ / aclanthology · org/2022 · emnlp-main · 392.</p>",
            "id": 121,
            "page": 12,
            "text": "Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang, Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal, Peter Clark, and Ashwin Kalyan. LILA: A unified benchmark for mathematical reasoning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 5807-5832, 2022a. URL https : / / aclanthology · org/2022 · emnlp-main · 392."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 835
                },
                {
                    "x": 2108,
                    "y": 835
                },
                {
                    "x": 2108,
                    "y": 1065
                },
                {
                    "x": 442,
                    "y": 1065
                }
            ],
            "category": "paragraph",
            "html": "<p id='122' style='font-size:18px'>Swaroop Mishra, Arindam Mitra, Neeraj Varshney, Bhavdeep Sachdeva, Peter Clark, Chitta Baral,<br>and Ashwin Kalyan. NumGLUE: A suite of fundamental yet challenging mathematical reasoning<br>tasks. In Proceedings ofthe 60th Annual Meeting ofthe Associationfor Computational Linguistics<br>(Volume 1: Long Papers), pp. 3505-3523, 2022b. doi: 10.18653/v1/2022.acl-long.246. URL<br>https : / / aclanthology · org/2022 · acl-long · 246.</p>",
            "id": 122,
            "page": 12,
            "text": "Swaroop Mishra, Arindam Mitra, Neeraj Varshney, Bhavdeep Sachdeva, Peter Clark, Chitta Baral, and Ashwin Kalyan. NumGLUE: A suite of fundamental yet challenging mathematical reasoning tasks. In Proceedings ofthe 60th Annual Meeting ofthe Associationfor Computational Linguistics (Volume 1: Long Papers), pp. 3505-3523, 2022b. doi: 10.18653/v1/2022.acl-long.246. URL https : / / aclanthology · org/2022 · acl-long · 246."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1102
                },
                {
                    "x": 2107,
                    "y": 1102
                },
                {
                    "x": 2107,
                    "y": 1241
                },
                {
                    "x": 441,
                    "y": 1241
                }
            ],
            "category": "paragraph",
            "html": "<p id='123' style='font-size:16px'>Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and<br>Ahmed Awadallah. Orca: Progressive learning from complex explanation traces of gpt-4. ArXiv<br>preprint, abs/2306.02707, 2023. URL https : / / arxiv · org/ abs / 2306 · 02707.</p>",
            "id": 123,
            "page": 12,
            "text": "Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. Orca: Progressive learning from complex explanation traces of gpt-4. ArXiv preprint, abs/2306.02707, 2023. URL https : / / arxiv · org/ abs / 2306 · 02707."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1278
                },
                {
                    "x": 2106,
                    "y": 1278
                },
                {
                    "x": 2106,
                    "y": 1463
                },
                {
                    "x": 442,
                    "y": 1463
                }
            ],
            "category": "paragraph",
            "html": "<p id='124' style='font-size:20px'>Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese,<br>and Caiming Xiong. Codegen: An open large language model for code with multi-turn program<br>synthesis. In International Conference on Learning Representations (ICLR), 2023. URL https :<br>/ / openreview net /pdf ?id=iaYcJKpY2B_.</p>",
            "id": 124,
            "page": 12,
            "text": "Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. Codegen: An open large language model for code with multi-turn program synthesis. In International Conference on Learning Representations (ICLR), 2023. URL https : / / openreview net /pdf ?id=iaYcJKpY2B_."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1499
                },
                {
                    "x": 2107,
                    "y": 1499
                },
                {
                    "x": 2107,
                    "y": 1685
                },
                {
                    "x": 443,
                    "y": 1685
                }
            ],
            "category": "paragraph",
            "html": "<p id='125' style='font-size:16px'>Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David<br>Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work:<br>Scratchpads for intermediate computation with language models. In Deep Learning for Code<br>Workshop, 2022. URL https : / / arxiv · org/ abs / 2112 · 00114.</p>",
            "id": 125,
            "page": 12,
            "text": "Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan,  Show your work: Scratchpads for intermediate computation with language models. In Deep Learning for Code Workshop, 2022. URL https : / / arxiv · org/ abs / 2112 · 00114."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1722
                },
                {
                    "x": 2095,
                    "y": 1722
                },
                {
                    "x": 2095,
                    "y": 1813
                },
                {
                    "x": 442,
                    "y": 1813
                }
            ],
            "category": "paragraph",
            "html": "<p id='126' style='font-size:14px'>OpenAI. Gpt-4 technical report. ArXiv preprint, abs/2303.08774, 2023. URL https : / / arxiv.<br>org/ abs / 2303 · 0877 4.</p>",
            "id": 126,
            "page": 12,
            "text": "OpenAI. Gpt-4 technical report. ArXiv preprint, abs/2303.08774, 2023. URL https : / / arxiv. org/ abs / 2303 · 0877 4."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 1853
                },
                {
                    "x": 2108,
                    "y": 1853
                },
                {
                    "x": 2108,
                    "y": 2081
                },
                {
                    "x": 444,
                    "y": 2081
                }
            ],
            "category": "paragraph",
            "html": "<p id='127' style='font-size:16px'>Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple<br>math word problems? In Proceedings of the 2021 Conference of the North American Chapter<br>of the Association for Computational Linguistics: Human Language Technologies, pp. 2080-<br>2094, 2021. doi: 10.18653/v1/2021.naacl-main.168. URL https : / / aclanthology · org/<br>2021 · naacl -main · 1 68.</p>",
            "id": 127,
            "page": 12,
            "text": "Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really able to solve simple math word problems? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 20802094, 2021. doi: 10.18653/v1/2021.naacl-main.168. URL https : / / aclanthology · org/ 2021 · naacl -main · 1 68."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2118
                },
                {
                    "x": 2107,
                    "y": 2118
                },
                {
                    "x": 2107,
                    "y": 2305
                },
                {
                    "x": 443,
                    "y": 2305
                }
            ],
            "category": "paragraph",
            "html": "<p id='128' style='font-size:14px'>Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli,<br>Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb<br>dataset for falcon llm: outperforming curated corpora with web data, and web data only. ArXiv<br>preprint, abs/2306.01116, 2023. URL https : / / arxiv · org/ abs / 2306 · 01116.</p>",
            "id": 128,
            "page": 12,
            "text": "Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. ArXiv preprint, abs/2306.01116, 2023. URL https : / / arxiv · org/ abs / 2306 · 01116."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2340
                },
                {
                    "x": 2106,
                    "y": 2340
                },
                {
                    "x": 2106,
                    "y": 2477
                },
                {
                    "x": 442,
                    "y": 2477
                }
            ],
            "category": "paragraph",
            "html": "<p id='129' style='font-size:16px'>Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning<br>with gpt-4. ArXiv preprint, abs/2304.03277, 2023. URL https : / / arxiv · org/ abs / 2304 .<br>03277.</p>",
            "id": 129,
            "page": 12,
            "text": "Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4. ArXiv preprint, abs/2304.03277, 2023. URL https : / / arxiv · org/ abs / 2304 . 03277."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2516
                },
                {
                    "x": 2107,
                    "y": 2516
                },
                {
                    "x": 2107,
                    "y": 2701
                },
                {
                    "x": 442,
                    "y": 2701
                }
            ],
            "category": "paragraph",
            "html": "<p id='130' style='font-size:16px'>Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory opti-<br>mizations toward training trillion parameter models. In SC20: International Conference for<br>High Performance Computing, Networking, Storage and Analysis, pp. 1-16. IEEE, 2020. URL<br>https : / /dl · acm · org/doi/10 · 5555/ 3433701 · 3433727.</p>",
            "id": 130,
            "page": 12,
            "text": "Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 1-16. IEEE, 2020. URL https : / /dl · acm · org/doi/10 · 5555/ 3433701 · 3433727."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2738
                },
                {
                    "x": 2105,
                    "y": 2738
                },
                {
                    "x": 2105,
                    "y": 2876
                },
                {
                    "x": 441,
                    "y": 2876
                }
            ],
            "category": "paragraph",
            "html": "<p id='131' style='font-size:16px'>Subhro Roy and Dan Roth. Solving general arithmetic word problems. In Proceedings of the 2015<br>Conference on Empirical Methods in Natural Language Processing, pp. 1743-1752, 2015. doi:<br>10.18653/v1/D15-1202. URL https : / / aclanthology · org /D15-1202.</p>",
            "id": 131,
            "page": 12,
            "text": "Subhro Roy and Dan Roth. Solving general arithmetic word problems. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 1743-1752, 2015. doi: 10.18653/v1/D15-1202. URL https : / / aclanthology · org /D15-1202."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2916
                },
                {
                    "x": 2106,
                    "y": 2916
                },
                {
                    "x": 2106,
                    "y": 3052
                },
                {
                    "x": 442,
                    "y": 3052
                }
            ],
            "category": "paragraph",
            "html": "<p id='132' style='font-size:16px'>Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi<br>Adi, Jingyu Liu, Tal Remez, Jeremy Rapin, et al. Code llama: Open foundation models for code.<br>ArXiv preprint, abs/2308.12950, 2023. URL https : / / arxiv · org/ abs / 2308 · 12950.</p>",
            "id": 132,
            "page": 12,
            "text": "Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jeremy Rapin,  Code llama: Open foundation models for code. ArXiv preprint, abs/2308.12950, 2023. URL https : / / arxiv · org/ abs / 2308 · 12950."
        },
        {
            "bounding_box": [
                {
                    "x": 1253,
                    "y": 3133
                },
                {
                    "x": 1299,
                    "y": 3133
                },
                {
                    "x": 1299,
                    "y": 3171
                },
                {
                    "x": 1253,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='133' style='font-size:14px'>12</footer>",
            "id": 133,
            "page": 12,
            "text": "12"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 113
                },
                {
                    "x": 898,
                    "y": 113
                },
                {
                    "x": 898,
                    "y": 157
                },
                {
                    "x": 444,
                    "y": 157
                }
            ],
            "category": "header",
            "html": "<header id='134' style='font-size:22px'>Preprint. Work in Progress</header>",
            "id": 134,
            "page": 13,
            "text": "Preprint. Work in Progress"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 345
                },
                {
                    "x": 2107,
                    "y": 345
                },
                {
                    "x": 2107,
                    "y": 805
                },
                {
                    "x": 441,
                    "y": 805
                }
            ],
            "category": "paragraph",
            "html": "<p id='135' style='font-size:20px'>Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai,<br>Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish<br>Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V.<br>Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica,<br>Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj,<br>Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan,<br>Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. Multitask<br>prompted training enables zero-shot task generalization. In The Tenth International Confer-<br>ence on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022, 2022. URL<br>https : / / openreview · net / forum?id=9Vrb9D0WI4.</p>",
            "id": 135,
            "page": 13,
            "text": "Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V. Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M. Rush. Multitask prompted training enables zero-shot task generalization. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022, 2022. URL https : / / openreview · net / forum?id=9Vrb9D0WI4."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 834
                },
                {
                    "x": 2107,
                    "y": 834
                },
                {
                    "x": 2107,
                    "y": 1017
                },
                {
                    "x": 443,
                    "y": 1017
                }
            ],
            "category": "paragraph",
            "html": "<p id='136' style='font-size:16px'>Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung,<br>Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks<br>and whether chain-of-thought can solve them. ArXiv preprint, abs/2210.09261, 2022. URL<br>https : / / arxiv · org/ abs / 2210 · 09261.</p>",
            "id": 136,
            "page": 13,
            "text": "Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou,  Challenging big-bench tasks and whether chain-of-thought can solve them. ArXiv preprint, abs/2210.09261, 2022. URL https : / / arxiv · org/ abs / 2210 · 09261."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1046
                },
                {
                    "x": 2104,
                    "y": 1046
                },
                {
                    "x": 2104,
                    "y": 1184
                },
                {
                    "x": 442,
                    "y": 1184
                }
            ],
            "category": "paragraph",
            "html": "<p id='137' style='font-size:18px'>Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy<br>Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.<br>https : / / github · com/tatsu-lab / stanford_alpaca, 2023.</p>",
            "id": 137,
            "page": 13,
            "text": "Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https : / / github · com/tatsu-lab / stanford_alpaca, 2023."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1212
                },
                {
                    "x": 2106,
                    "y": 1212
                },
                {
                    "x": 2106,
                    "y": 1393
                },
                {
                    "x": 442,
                    "y": 1393
                }
            ],
            "category": "paragraph",
            "html": "<p id='138' style='font-size:16px'>Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia,<br>Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for<br>science. ArXiv preprint, abs/2211.09085, 2022. URL https : / / arxiv · org/ abs / 2211 ·<br>09085.</p>",
            "id": 138,
            "page": 13,
            "text": "Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for science. ArXiv preprint, abs/2211.09085, 2022. URL https : / / arxiv · org/ abs / 2211 · 09085."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1424
                },
                {
                    "x": 2106,
                    "y": 1424
                },
                {
                    "x": 2106,
                    "y": 1608
                },
                {
                    "x": 443,
                    "y": 1608
                }
            ],
            "category": "paragraph",
            "html": "<p id='139' style='font-size:16px'>Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee<br>Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and<br>efficient foundation language models. ArXiv preprint, abs/2302.13971, 2023a. URL https :<br>/ / arxiv · org/ abs / 2302 · 13971.</p>",
            "id": 139,
            "page": 13,
            "text": "Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar,  Llama: Open and efficient foundation language models. ArXiv preprint, abs/2302.13971, 2023a. URL https : / / arxiv · org/ abs / 2302 · 13971."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1639
                },
                {
                    "x": 2106,
                    "y": 1639
                },
                {
                    "x": 2106,
                    "y": 1820
                },
                {
                    "x": 443,
                    "y": 1820
                }
            ],
            "category": "paragraph",
            "html": "<p id='140' style='font-size:16px'>Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-<br>lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foun-<br>dation and fine-tuned chat models. ArXiv preprint, abs/2307.09288, 2023b. URL https :<br>/ / arxiv · org/ abs / 2307 · 09288.</p>",
            "id": 140,
            "page": 13,
            "text": "Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,  Llama 2: Open foundation and fine-tuned chat models. ArXiv preprint, abs/2307.09288, 2023b. URL https : / / arxiv · org/ abs / 2307 · 09288."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1852
                },
                {
                    "x": 2105,
                    "y": 1852
                },
                {
                    "x": 2105,
                    "y": 2034
                },
                {
                    "x": 443,
                    "y": 2034
                }
            ],
            "category": "paragraph",
            "html": "<p id='141' style='font-size:16px'>Boshi Wang, Xiang Deng, and Huan Sun. Iteratively prompt pre-trained language models for chain<br>of thought. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language<br>Processing, pp. 2714-2730. Association for Computational Linguistics, 2022a. URL https :<br>/ / aclanthology · org/ 2022 · emnlp-main · 1 74.</p>",
            "id": 141,
            "page": 13,
            "text": "Boshi Wang, Xiang Deng, and Huan Sun. Iteratively prompt pre-trained language models for chain of thought. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 2714-2730. Association for Computational Linguistics, 2022a. URL https : / / aclanthology · org/ 2022 · emnlp-main · 1 74."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2062
                },
                {
                    "x": 2107,
                    "y": 2062
                },
                {
                    "x": 2107,
                    "y": 2294
                },
                {
                    "x": 444,
                    "y": 2294
                }
            ],
            "category": "paragraph",
            "html": "<p id='142' style='font-size:18px'>Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and Huan Sun.<br>Towards understanding chain-of-thought prompting: An empirical study of what matters. In Pro-<br>ceedings of the 61st Annual Meeting of the Associationfor Computational Linguistics (Volume 1:<br>Long Papers), pp. 2717-2739. Association for Computational Linguistics, 2023a. doi: 10.18653/<br>v1/2023.acl-long.153. URL https : / / acl anthology · org/ 2023 · acl-long · 153.</p>",
            "id": 142,
            "page": 13,
            "text": "Boshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You Wu, Luke Zettlemoyer, and Huan Sun. Towards understanding chain-of-thought prompting: An empirical study of what matters. In Proceedings of the 61st Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Papers), pp. 2717-2739. Association for Computational Linguistics, 2023a. doi: 10.18653/ v1/2023.acl-long.153. URL https : / / acl anthology · org/ 2023 · acl-long · 153."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2322
                },
                {
                    "x": 2107,
                    "y": 2322
                },
                {
                    "x": 2107,
                    "y": 2459
                },
                {
                    "x": 442,
                    "y": 2459
                }
            ],
            "category": "paragraph",
            "html": "<p id='143' style='font-size:14px'>Boshi Wang, Xiang Yue, and Huan Sun. Can chatgpt defend the truth? automatic dialectical eval-<br>uation elicits llms' deficiencies in reasoning. ArXiv preprint, abs/2305.13160, 2023b. URL<br>https : / / arxiv · org / abs / 2305 · 131 60.</p>",
            "id": 143,
            "page": 13,
            "text": "Boshi Wang, Xiang Yue, and Huan Sun. Can chatgpt defend the truth? automatic dialectical evaluation elicits llms' deficiencies in reasoning. ArXiv preprint, abs/2305.13160, 2023b. URL https : / / arxiv · org / abs / 2305 · 131 60."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2489
                },
                {
                    "x": 2105,
                    "y": 2489
                },
                {
                    "x": 2105,
                    "y": 2669
                },
                {
                    "x": 442,
                    "y": 2669
                }
            ],
            "category": "paragraph",
            "html": "<p id='144' style='font-size:18px'>Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim.<br>Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language<br>models. ArXiv preprint, abs/2305.04091, 2023c. URL https : / / arxiv · org/ abs / 2305.<br>04091.</p>",
            "id": 144,
            "page": 13,
            "text": "Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim. Plan-and-solve prompting: Improving zero-shot chain-of-thought reasoning by large language models. ArXiv preprint, abs/2305.04091, 2023c. URL https : / / arxiv · org/ abs / 2305. 04091."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2701
                },
                {
                    "x": 2107,
                    "y": 2701
                },
                {
                    "x": 2107,
                    "y": 2839
                },
                {
                    "x": 442,
                    "y": 2839
                }
            ],
            "category": "paragraph",
            "html": "<p id='145' style='font-size:16px'>Peiyi Wang, Lei Li, Liang Chen, Feifan Song, Binghuai Lin, Yunbo Cao, Tianyu Liu, and Zhi-<br>fang Sui. Making large language models better reasoners with alignment. ArXiv preprint,<br>abs/2309.02144, 2023d. URL https : / / arxiv · org/ abs / 2309 · 02144.</p>",
            "id": 145,
            "page": 13,
            "text": "Peiyi Wang, Lei Li, Liang Chen, Feifan Song, Binghuai Lin, Yunbo Cao, Tianyu Liu, and Zhifang Sui. Making large language models better reasoners with alignment. ArXiv preprint, abs/2309.02144, 2023d. URL https : / / arxiv · org/ abs / 2309 · 02144."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2869
                },
                {
                    "x": 2106,
                    "y": 2869
                },
                {
                    "x": 2106,
                    "y": 3052
                },
                {
                    "x": 444,
                    "y": 3052
                }
            ],
            "category": "paragraph",
            "html": "<p id='146' style='font-size:18px'>Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R<br>Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang. Scibench: Evaluating college-level scien-<br>tific problem-solving abilities of large language models. ArXiv preprint, abs/2307.10635, 2023e.<br>URL https : / / arxiv · org/ abs / 2307 · 10635.</p>",
            "id": 146,
            "page": 13,
            "text": "Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang. Scibench: Evaluating college-level scientific problem-solving abilities of large language models. ArXiv preprint, abs/2307.10635, 2023e. URL https : / / arxiv · org/ abs / 2307 · 10635."
        },
        {
            "bounding_box": [
                {
                    "x": 1253,
                    "y": 3133
                },
                {
                    "x": 1299,
                    "y": 3133
                },
                {
                    "x": 1299,
                    "y": 3171
                },
                {
                    "x": 1253,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='147' style='font-size:14px'>13</footer>",
            "id": 147,
            "page": 13,
            "text": "13"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 113
                },
                {
                    "x": 899,
                    "y": 113
                },
                {
                    "x": 899,
                    "y": 157
                },
                {
                    "x": 443,
                    "y": 157
                }
            ],
            "category": "header",
            "html": "<header id='148' style='font-size:22px'>Preprint. Work in Progress</header>",
            "id": 148,
            "page": 14,
            "text": "Preprint. Work in Progress"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 346
                },
                {
                    "x": 2107,
                    "y": 346
                },
                {
                    "x": 2107,
                    "y": 529
                },
                {
                    "x": 443,
                    "y": 529
                }
            ],
            "category": "paragraph",
            "html": "<p id='149' style='font-size:20px'>Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Self-<br>consistency improves chain of thought reasoning in language models. International Conference<br>on Learning Representations (ICLR), 2023f. URL https : / / openreview net /pdf?id=<br>1PL1NIMMrw.</p>",
            "id": 149,
            "page": 14,
            "text": "Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, and Denny Zhou. Selfconsistency improves chain of thought reasoning in language models. International Conference on Learning Representations (ICLR), 2023f. URL https : / / openreview net /pdf?id= 1PL1NIMMrw."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 567
                },
                {
                    "x": 2108,
                    "y": 567
                },
                {
                    "x": 2108,
                    "y": 1027
                },
                {
                    "x": 441,
                    "y": 1027
                }
            ],
            "category": "paragraph",
            "html": "<p id='150' style='font-size:20px'>Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei,<br>Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Es-<br>haan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob An-<br>derson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi,<br>Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravse-<br>haj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan<br>Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. Super-NaturalInstructions: Gener-<br>alization via declarative instructions on 1600+ NLP tasks. In Proceedings of the 2022 Con-<br>ference on Empirical Methods in Natural Language Processing, pp. 5085-5109, 2022b. URL<br>https : / / aclanthology · org/2022 · emnlp-main · 340.</p>",
            "id": 150,
            "page": 14,
            "text": "Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. Super-NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 5085-5109, 2022b. URL https : / / aclanthology · org/2022 · emnlp-main · 340."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 1060
                },
                {
                    "x": 2108,
                    "y": 1060
                },
                {
                    "x": 2108,
                    "y": 1247
                },
                {
                    "x": 444,
                    "y": 1247
                }
            ],
            "category": "paragraph",
            "html": "<p id='151' style='font-size:16px'>Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi<br>Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy, et al. How far can camels<br>go? exploring the state of instruction tuning on open resources. ArXiv preprint, abs/2306.04751,<br>2023g. URL https : / / arxiv · org/ abs / 2306 · 04751.</p>",
            "id": 151,
            "page": 14,
            "text": "Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A Smith, Iz Beltagy,  How far can camels go? exploring the state of instruction tuning on open resources. ArXiv preprint, abs/2306.04751, 2023g. URL https : / / arxiv · org/ abs / 2306 · 04751."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 1283
                },
                {
                    "x": 2106,
                    "y": 1283
                },
                {
                    "x": 2106,
                    "y": 1468
                },
                {
                    "x": 445,
                    "y": 1468
                }
            ],
            "category": "paragraph",
            "html": "<p id='152' style='font-size:16px'>Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and<br>Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions.<br>The 61st Annual Meeting of the Associationfor Computational Linguistics (ACL 2023), 2023h.<br>URL https : / /aclanthology · org/2023 · acl-long · 754 · pdf.</p>",
            "id": 152,
            "page": 14,
            "text": "Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. The 61st Annual Meeting of the Associationfor Computational Linguistics (ACL 2023), 2023h. URL https : / /aclanthology · org/2023 · acl-long · 754 · pdf."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 1503
                },
                {
                    "x": 2106,
                    "y": 1503
                },
                {
                    "x": 2106,
                    "y": 1644
                },
                {
                    "x": 444,
                    "y": 1644
                }
            ],
            "category": "paragraph",
            "html": "<p id='153' style='font-size:16px'>Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi DQ Bui, Junnan Li, and Steven CH Hoi.<br>Codet5+: Open code large language models for code understanding and generation. ArXiv<br>preprint, abs/2305.07922, 2023i. URL https : / / arxiv · org/ abs / 2305 · 07922.</p>",
            "id": 153,
            "page": 14,
            "text": "Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi DQ Bui, Junnan Li, and Steven CH Hoi. Codet5+: Open code large language models for code understanding and generation. ArXiv preprint, abs/2305.07922, 2023i. URL https : / / arxiv · org/ abs / 2305 · 07922."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 1681
                },
                {
                    "x": 2105,
                    "y": 1681
                },
                {
                    "x": 2105,
                    "y": 1864
                },
                {
                    "x": 444,
                    "y": 1864
                }
            ],
            "category": "paragraph",
            "html": "<p id='154' style='font-size:18px'>Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,<br>Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. In The Tenth<br>International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29,<br>2022, 2022a. URL https : / /openreview net / forum?id=gEZrGCozdqR.</p>",
            "id": 154,
            "page": 14,
            "text": "Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022, 2022a. URL https : / /openreview net / forum?id=gEZrGCozdqR."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 1901
                },
                {
                    "x": 2105,
                    "y": 1901
                },
                {
                    "x": 2105,
                    "y": 2085
                },
                {
                    "x": 444,
                    "y": 2085
                }
            ],
            "category": "paragraph",
            "html": "<p id='155' style='font-size:18px'>Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,<br>Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models.<br>Advances in Neural Information Processing Systems, 35:24824-24837, 2022b. URL https :<br>/ / openreview · net/pdf?id=_VjQIMeSB_J.</p>",
            "id": 155,
            "page": 14,
            "text": "Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,  Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824-24837, 2022b. URL https : / / openreview · net/pdf?id=_VjQIMeSB_J."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2121
                },
                {
                    "x": 2106,
                    "y": 2121
                },
                {
                    "x": 2106,
                    "y": 2258
                },
                {
                    "x": 442,
                    "y": 2258
                }
            ],
            "category": "paragraph",
            "html": "<p id='156' style='font-size:16px'>Jerry Wei, Da Huang, Yifeng Lu, Denny Zhou, and Quoc V Le. Simple synthetic data reduces<br>sycophancy in large language models. ArXiv preprint, abs/2308.03958, 2023. URL https :<br>/ / arxiv · org/ abs / 2308 · 03958.</p>",
            "id": 156,
            "page": 14,
            "text": "Jerry Wei, Da Huang, Yifeng Lu, Denny Zhou, and Quoc V Le. Simple synthetic data reduces sycophancy in large language models. ArXiv preprint, abs/2308.03958, 2023. URL https : / / arxiv · org/ abs / 2308 · 03958."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2296
                },
                {
                    "x": 2105,
                    "y": 2296
                },
                {
                    "x": 2105,
                    "y": 2481
                },
                {
                    "x": 444,
                    "y": 2481
                }
            ],
            "category": "paragraph",
            "html": "<p id='157' style='font-size:16px'>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi,<br>Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, et al. Huggingface's transform-<br>ers: State-of-the-art natural language processing. ArXiv preprint, abs/1910.03771, 2019. URL<br>https : / / arxiv · org / abs/ 1910 · 03771.</p>",
            "id": 157,
            "page": 14,
            "text": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz,  Huggingface's transformers: State-of-the-art natural language processing. ArXiv preprint, abs/1910.03771, 2019. URL https : / / arxiv · org / abs/ 1910 · 03771."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2518
                },
                {
                    "x": 2105,
                    "y": 2518
                },
                {
                    "x": 2105,
                    "y": 2702
                },
                {
                    "x": 443,
                    "y": 2702
                }
            ],
            "category": "paragraph",
            "html": "<p id='158' style='font-size:18px'>Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of in-<br>context learning as implicit bayesian inference. In The Tenth International Conference on<br>Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022, 2022. URL https ··<br>/ / openreview · net/ forum?id=RdJVFCH jUMI.</p>",
            "id": 158,
            "page": 14,
            "text": "Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. An explanation of incontext learning as implicit bayesian inference. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022, 2022. URL https ·· / / openreview · net/ forum?id=RdJVFCH jUMI."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2738
                },
                {
                    "x": 2106,
                    "y": 2738
                },
                {
                    "x": 2106,
                    "y": 2877
                },
                {
                    "x": 443,
                    "y": 2877
                }
            ],
            "category": "paragraph",
            "html": "<p id='159' style='font-size:16px'>Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min- Yen Kan, Junxian He, and Qizhe<br>Xie. Decomposition enhances reasoning via self-evaluation guided decoding. ArXiv preprint,<br>abs/2305.00633, 2023. URL https : / / arxiv · org/ abs / 2305 · 00633.</p>",
            "id": 159,
            "page": 14,
            "text": "Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min- Yen Kan, Junxian He, and Qizhe Xie. Decomposition enhances reasoning via self-evaluation guided decoding. ArXiv preprint, abs/2305.00633, 2023. URL https : / / arxiv · org/ abs / 2305 · 00633."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2916
                },
                {
                    "x": 2107,
                    "y": 2916
                },
                {
                    "x": 2107,
                    "y": 3053
                },
                {
                    "x": 442,
                    "y": 3053
                }
            ],
            "category": "paragraph",
            "html": "<p id='160' style='font-size:18px'>Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and<br>Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions.<br>ArXiv preprint, abs/2304.12244, 2023. URL https : / / arxiv · org/ abs / 2304 · 12244.</p>",
            "id": 160,
            "page": 14,
            "text": "Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models to follow complex instructions. ArXiv preprint, abs/2304.12244, 2023. URL https : / / arxiv · org/ abs / 2304 · 12244."
        },
        {
            "bounding_box": [
                {
                    "x": 1252,
                    "y": 3133
                },
                {
                    "x": 1300,
                    "y": 3133
                },
                {
                    "x": 1300,
                    "y": 3171
                },
                {
                    "x": 1252,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='161' style='font-size:14px'>14</footer>",
            "id": 161,
            "page": 14,
            "text": "14"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 113
                },
                {
                    "x": 899,
                    "y": 113
                },
                {
                    "x": 899,
                    "y": 158
                },
                {
                    "x": 443,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='162' style='font-size:22px'>Preprint. Work in Progress</header>",
            "id": 162,
            "page": 15,
            "text": "Preprint. Work in Progress"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 345
                },
                {
                    "x": 2106,
                    "y": 345
                },
                {
                    "x": 2106,
                    "y": 485
                },
                {
                    "x": 441,
                    "y": 485
                }
            ],
            "category": "paragraph",
            "html": "<p id='163' style='font-size:16px'>Zhen Yang, Ming Ding, Qingsong Lv, Zhihuan Jiang, Zehai He, Yuyi Guo, Jinfeng Bai, and Jie Tang.<br>Gpt can solve mathematical problems without a calculator. ArXiv preprint, abs/2309.03241, 2023.<br>URL https : / / arxiv · org / abs / 2309 · 03241.</p>",
            "id": 163,
            "page": 15,
            "text": "Zhen Yang, Ming Ding, Qingsong Lv, Zhihuan Jiang, Zehai He, Yuyi Guo, Jinfeng Bai, and Jie Tang. Gpt can solve mathematical problems without a calculator. ArXiv preprint, abs/2309.03241, 2023. URL https : / / arxiv · org / abs / 2309 · 03241."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 518
                },
                {
                    "x": 2108,
                    "y": 518
                },
                {
                    "x": 2108,
                    "y": 699
                },
                {
                    "x": 441,
                    "y": 699
                }
            ],
            "category": "paragraph",
            "html": "<p id='164' style='font-size:18px'>Shunyu Yao, Jeffrey Zhao, Dian Yu, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. Re-<br>act: Synergizing reasoning and acting in language models. In International Conference on<br>Learning Representations (ICLR), 2023. URL https : / / openreview · net /pdf?id=WE_<br>vluYUL-X.</p>",
            "id": 164,
            "page": 15,
            "text": "Shunyu Yao, Jeffrey Zhao, Dian Yu, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR), 2023. URL https : / / openreview · net /pdf?id=WE_ vluYUL-X."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 733
                },
                {
                    "x": 2107,
                    "y": 733
                },
                {
                    "x": 2107,
                    "y": 918
                },
                {
                    "x": 443,
                    "y": 918
                }
            ],
            "category": "paragraph",
            "html": "<p id='165' style='font-size:16px'>Qinyuan Ye, Bill Yuchen Lin, and Xiang Ren. CrossFit: A few-shot learning challenge for cross-<br>task generalization in NLP. In Proceedings of the 2021 Conference on Empirical Methods in<br>Natural Language Processing, pp. 7163-7189, 2021. doi: 10.18653/v1/2021.emnlp-main.572.<br>URL https : / / aclanthology · org/2021 · emnlp-main · 572.</p>",
            "id": 165,
            "page": 15,
            "text": "Qinyuan Ye, Bill Yuchen Lin, and Xiang Ren. CrossFit: A few-shot learning challenge for crosstask generalization in NLP. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 7163-7189, 2021. doi: 10.18653/v1/2021.emnlp-main.572. URL https : / / aclanthology · org/2021 · emnlp-main · 572."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 948
                },
                {
                    "x": 2108,
                    "y": 948
                },
                {
                    "x": 2108,
                    "y": 1131
                },
                {
                    "x": 441,
                    "y": 1131
                }
            ],
            "category": "paragraph",
            "html": "<p id='166' style='font-size:18px'>Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok,<br>Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical<br>questions for large language models. ArXiv preprint, abs/2309.12284, 2023. URL https :<br>/ / arxiv · org/ abs /2309 · 12284.</p>",
            "id": 166,
            "page": 15,
            "text": "Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. ArXiv preprint, abs/2309.12284, 2023. URL https : / / arxiv · org/ abs /2309 · 12284."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1163
                },
                {
                    "x": 2107,
                    "y": 1163
                },
                {
                    "x": 2107,
                    "y": 1303
                },
                {
                    "x": 441,
                    "y": 1303
                }
            ],
            "category": "paragraph",
            "html": "<p id='167' style='font-size:18px'>Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou. Scaling<br>relationship on learning mathematical reasoning with large language models. ArXiv preprint,<br>abs/2308.01825, 2023. URL https : / / arxiv · org/ abs / 2308 · 01825.</p>",
            "id": 167,
            "page": 15,
            "text": "Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Chuanqi Tan, and Chang Zhou. Scaling relationship on learning mathematical reasoning with large language models. ArXiv preprint, abs/2308.01825, 2023. URL https : / / arxiv · org/ abs / 2308 · 01825."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1336
                },
                {
                    "x": 2105,
                    "y": 1336
                },
                {
                    "x": 2105,
                    "y": 1518
                },
                {
                    "x": 442,
                    "y": 1518
                }
            ],
            "category": "paragraph",
            "html": "<p id='168' style='font-size:16px'>Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christo-<br>pher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer<br>language models. ArXiv preprint, abs/2205.01068, 2022. URL https : / / arxiv · org/ abs/<br>2205 · 01068.</p>",
            "id": 168,
            "page": 15,
            "text": "Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,  Opt: Open pre-trained transformer language models. ArXiv preprint, abs/2205.01068, 2022. URL https : / / arxiv · org/ abs/ 2205 · 01068."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1550
                },
                {
                    "x": 2106,
                    "y": 1550
                },
                {
                    "x": 2106,
                    "y": 1689
                },
                {
                    "x": 441,
                    "y": 1689
                }
            ],
            "category": "paragraph",
            "html": "<p id='169' style='font-size:16px'>Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, and Yu Li. Progressive-hint prompting<br>improves reasoning in large language models. ArXiv preprint, abs/2304.09797, 2023a. URL<br>https : / / arxiv · org/ abs/ 2304 · 09797.</p>",
            "id": 169,
            "page": 15,
            "text": "Chuanyang Zheng, Zhengying Liu, Enze Xie, Zhenguo Li, and Yu Li. Progressive-hint prompting improves reasoning in large language models. ArXiv preprint, abs/2304.09797, 2023a. URL https : / / arxiv · org/ abs/ 2304 · 09797."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1723
                },
                {
                    "x": 2105,
                    "y": 1723
                },
                {
                    "x": 2105,
                    "y": 1902
                },
                {
                    "x": 442,
                    "y": 1902
                }
            ],
            "category": "paragraph",
            "html": "<p id='170' style='font-size:18px'>Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,<br>Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and<br>chatbot arena. ArXiv preprint, abs/2306.05685, 2023b. URL https : / / arxiv · org/ abs/<br>2306 · 05685.</p>",
            "id": 170,
            "page": 15,
            "text": "Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing,  Judging llm-as-a-judge with mt-bench and chatbot arena. ArXiv preprint, abs/2306.05685, 2023b. URL https : / / arxiv · org/ abs/ 2306 · 05685."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1937
                },
                {
                    "x": 2107,
                    "y": 1937
                },
                {
                    "x": 2107,
                    "y": 2118
                },
                {
                    "x": 443,
                    "y": 2118
                }
            ],
            "category": "paragraph",
            "html": "<p id='171' style='font-size:16px'>Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied,<br>Weizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation<br>models. ArXiv preprint, abs/2304.06364, 2023. URL https : / / arxiv · org/ abs - 2304 ·<br>06364.</p>",
            "id": 171,
            "page": 15,
            "text": "Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation models. ArXiv preprint, abs/2304.06364, 2023. URL https : / / arxiv · org/ abs - 2304 · 06364."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2154
                },
                {
                    "x": 2107,
                    "y": 2154
                },
                {
                    "x": 2107,
                    "y": 2336
                },
                {
                    "x": 443,
                    "y": 2336
                }
            ],
            "category": "paragraph",
            "html": "<p id='172' style='font-size:20px'>Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia,<br>Linqi Song, Mingjie Zhan, et al. Solving challenging math word problems using gpt-4 code<br>interpreter with code-based self-verification. ArXiv preprint, abs/2308.07921, 2023a. URL<br>https : / / arxiv · org/ abs/ 2308 · 07921.</p>",
            "id": 172,
            "page": 15,
            "text": "Aojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun Luo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song, Mingjie Zhan,  Solving challenging math word problems using gpt-4 code interpreter with code-based self-verification. ArXiv preprint, abs/2308.07921, 2023a. URL https : / / arxiv · org/ abs/ 2308 · 07921."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2368
                },
                {
                    "x": 2105,
                    "y": 2368
                },
                {
                    "x": 2105,
                    "y": 2507
                },
                {
                    "x": 441,
                    "y": 2507
                }
            ],
            "category": "paragraph",
            "html": "<p id='173' style='font-size:16px'>Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat,<br>Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. ArXiv preprint, abs/2305.11206, 2023b.<br>URL https : / / arxiv · org/ abs / 2305 · 11206.</p>",
            "id": 173,
            "page": 15,
            "text": "Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu,  Lima: Less is more for alignment. ArXiv preprint, abs/2305.11206, 2023b. URL https : / / arxiv · org/ abs / 2305 · 11206."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2538
                },
                {
                    "x": 2106,
                    "y": 2538
                },
                {
                    "x": 2106,
                    "y": 2725
                },
                {
                    "x": 442,
                    "y": 2725
                }
            ],
            "category": "paragraph",
            "html": "<p id='174' style='font-size:18px'>Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuur-<br>mans, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex rea-<br>soning in large language models. International Conference on Learning Representations (ICLR),<br>2023c. URL https : / / openreview · net /pdf?id=WZH7099tgfM.</p>",
            "id": 174,
            "page": 15,
            "text": "Denny Zhou, Nathanael Scharli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting enables complex reasoning in large language models. International Conference on Learning Representations (ICLR), 2023c. URL https : / / openreview · net /pdf?id=WZH7099tgfM."
        },
        {
            "bounding_box": [
                {
                    "x": 1253,
                    "y": 3132
                },
                {
                    "x": 1300,
                    "y": 3132
                },
                {
                    "x": 1300,
                    "y": 3171
                },
                {
                    "x": 1253,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='175' style='font-size:14px'>15</footer>",
            "id": 175,
            "page": 15,
            "text": "15"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 111
                },
                {
                    "x": 900,
                    "y": 111
                },
                {
                    "x": 900,
                    "y": 159
                },
                {
                    "x": 443,
                    "y": 159
                }
            ],
            "category": "header",
            "html": "<header id='176' style='font-size:22px'>Preprint. Work in Progress</header>",
            "id": 176,
            "page": 16,
            "text": "Preprint. Work in Progress"
        },
        {
            "bounding_box": [
                {
                    "x": 446,
                    "y": 340
                },
                {
                    "x": 897,
                    "y": 340
                },
                {
                    "x": 897,
                    "y": 394
                },
                {
                    "x": 446,
                    "y": 394
                }
            ],
            "category": "paragraph",
            "html": "<p id='177' style='font-size:22px'>A RELATED W ORK</p>",
            "id": 177,
            "page": 16,
            "text": "A RELATED W ORK"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 444
                },
                {
                    "x": 1279,
                    "y": 444
                },
                {
                    "x": 1279,
                    "y": 492
                },
                {
                    "x": 445,
                    "y": 492
                }
            ],
            "category": "paragraph",
            "html": "<p id='178' style='font-size:18px'>A.1 MATHEMATICAL REASONING DATASETS</p>",
            "id": 178,
            "page": 16,
            "text": "A.1 MATHEMATICAL REASONING DATASETS"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 527
                },
                {
                    "x": 2109,
                    "y": 527
                },
                {
                    "x": 2109,
                    "y": 1083
                },
                {
                    "x": 440,
                    "y": 1083
                }
            ],
            "category": "paragraph",
            "html": "<p id='179' style='font-size:20px'>Our work builds upon the existing mathematical reasoning literature. Early on, mathematical rea-<br>soning is mostly focused on solving synthetic basic math problems like AddSub (Hosseini et al.,<br>2014) and other arithmetic reasoning datasets (Koncel-Kedziorski et al., 2015; Roy & Roth, 2015;<br>Patel et al., 2021). Later on, more difficult math word problem datasets (Cobbe et al., 2021; Amini<br>et al., 2019; Ling et al., 2017; Hendrycks et al., 2021b) have been proposed to focus on addressing<br>realistic math word problems. NumGLUE (Mishra et al., 2022b) and LiLA (Mishra et al., 2022a)<br>compile the existing literature to build a more diversified dataset collection. However, these datasets<br>are mostly focused on grade school math problems. To further test LLMs' limits in addressing more<br>complex math problems, MMLU (Hendrycks et al., 2021a) includes college math problems in its<br>evaluation suite. More recently, (Chen et al., 2023; Wang et al., 2023e) have proposed to tackle<br>more challenging college-level science and math problems. Our instruction tuning dataset is built<br>upon existing work to include a diversified collection of math problems from different subfields.</p>",
            "id": 179,
            "page": 16,
            "text": "Our work builds upon the existing mathematical reasoning literature. Early on, mathematical reasoning is mostly focused on solving synthetic basic math problems like AddSub (Hosseini , 2014) and other arithmetic reasoning datasets (Koncel-Kedziorski , 2015; Roy & Roth, 2015; Patel , 2021). Later on, more difficult math word problem datasets (Cobbe , 2021; Amini , 2019; Ling , 2017; Hendrycks , 2021b) have been proposed to focus on addressing realistic math word problems. NumGLUE (Mishra , 2022b) and LiLA (Mishra , 2022a) compile the existing literature to build a more diversified dataset collection. However, these datasets are mostly focused on grade school math problems. To further test LLMs' limits in addressing more complex math problems, MMLU (Hendrycks , 2021a) includes college math problems in its evaluation suite. More recently, (Chen , 2023; Wang , 2023e) have proposed to tackle more challenging college-level science and math problems. Our instruction tuning dataset is built upon existing work to include a diversified collection of math problems from different subfields."
        },
        {
            "bounding_box": [
                {
                    "x": 446,
                    "y": 1136
                },
                {
                    "x": 1401,
                    "y": 1136
                },
                {
                    "x": 1401,
                    "y": 1184
                },
                {
                    "x": 446,
                    "y": 1184
                }
            ],
            "category": "paragraph",
            "html": "<p id='180' style='font-size:18px'>A.2 REASONING WITH LARGE LANGUAGE MODELS</p>",
            "id": 180,
            "page": 16,
            "text": "A.2 REASONING WITH LARGE LANGUAGE MODELS"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1220
                },
                {
                    "x": 2108,
                    "y": 1220
                },
                {
                    "x": 2108,
                    "y": 1864
                },
                {
                    "x": 441,
                    "y": 1864
                }
            ],
            "category": "paragraph",
            "html": "<p id='181' style='font-size:20px'>LLMs have demonstrated great capabilities to reason with the help of Chain-of- Thought prompt-<br>ing (Wei et al., 2022b; Kojima et al., 2022; Wang et al., 2023f). Suzgun et al. (2022) have shown<br>that CoT can already surpass human performance on challenging BIG-Bench tasks. Later on, sev-<br>eral other works (Drozdov et al., 2023; Zhou et al., 2023c; Nye et al., 2022; Wang et al., 2022a;<br>2023a; Li et al., 2023b; Wang et al., 2023d; Yu et al., 2023) also propose different approaches to<br>utilize LLMs to solve reasoning tasks by allowing intermediate steps. ReAct Yao et al. (2023) pro-<br>poses to leverage external tools like search engines to enhance LLM reasoning skills. Another trend<br>is to enable LLMs' capabilities to use programs as thought processes like PoT (Chen et al., 2022).<br>Some follow-up works include self-critic (Gou et al., 2023), self-eval (Xie et al., 2023), plan-and-<br>solve (Wang et al., 2023c). These methods propose to enhance LLMs' capabilities to solve math<br>problems with PoT. Self-critic (Gou et al., 2023) and self-eval (Xie et al., 2022) both adopt self-<br>evaluation to enhance the robustness of the generated program. Plan-and-solve (Wang et al., 2023c)<br>instead adopts more detailed planning instructions to help LLMs create a high-level reasoning plan.<br>These methods all prove to bring decent improvements over PoT.</p>",
            "id": 181,
            "page": 16,
            "text": "LLMs have demonstrated great capabilities to reason with the help of Chain-of- Thought prompting (Wei , 2022b; Kojima , 2022; Wang , 2023f). Suzgun  (2022) have shown that CoT can already surpass human performance on challenging BIG-Bench tasks. Later on, several other works (Drozdov , 2023; Zhou , 2023c; Nye , 2022; Wang , 2022a; 2023a; Li , 2023b; Wang , 2023d; Yu , 2023) also propose different approaches to utilize LLMs to solve reasoning tasks by allowing intermediate steps. ReAct Yao  (2023) proposes to leverage external tools like search engines to enhance LLM reasoning skills. Another trend is to enable LLMs' capabilities to use programs as thought processes like PoT (Chen , 2022). Some follow-up works include self-critic (Gou , 2023), self-eval (Xie , 2023), plan-andsolve (Wang , 2023c). These methods propose to enhance LLMs' capabilities to solve math problems with PoT. Self-critic (Gou , 2023) and self-eval (Xie , 2022) both adopt selfevaluation to enhance the robustness of the generated program. Plan-and-solve (Wang , 2023c) instead adopts more detailed planning instructions to help LLMs create a high-level reasoning plan. These methods all prove to bring decent improvements over PoT."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 1918
                },
                {
                    "x": 1394,
                    "y": 1918
                },
                {
                    "x": 1394,
                    "y": 1968
                },
                {
                    "x": 444,
                    "y": 1968
                }
            ],
            "category": "paragraph",
            "html": "<p id='182' style='font-size:16px'>A.3 INSTRUCTION TUNING IN LANGUAGE MODELS</p>",
            "id": 182,
            "page": 16,
            "text": "A.3 INSTRUCTION TUNING IN LANGUAGE MODELS"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2005
                },
                {
                    "x": 2108,
                    "y": 2005
                },
                {
                    "x": 2108,
                    "y": 2652
                },
                {
                    "x": 441,
                    "y": 2652
                }
            ],
            "category": "paragraph",
            "html": "<p id='183' style='font-size:20px'>Instruction tuning is part of a line of work designed to \"align\" language models with more use-<br>ful objectives and human preferences. The instruction tuning step is seen as a major step to acti-<br>vate LLMs' certain capabilities to respond to human instructions. Previously, instruction tuning is<br>mainly focused on enhancing LLMs' general-purpose instruction following abilities. Since 2021,<br>CrossFit (Ye et al., 2021) and NaturalInstruction (Wang et al., 2022b), FLAN (Wei et al., 2022a)<br>and T0 (Sanh et al., 2022) are amongst the first wave of instruction tuning effort to understand<br>LLMs' generalization capabilities. Later on, FLAN-v2 (Chung et al., 2022; Longpre et al., 2023)<br>have been proposed to understand the effect of scaling up the instruction datasets to understand its<br>impact on model performance. These approaches mainly adopt human-annotated datasets to build<br>the instruction following dataset. More recently, multiple works (Wang et al., 2023h; Xu et al.,<br>2023; Peng et al., 2023; Zhou et al., 2023b; Wang et al., 2023g) propose to utilize synthetic instruc-<br>tion following data distilled from GPT-3/4 to align open-source LLMs. The most similar effort to<br>ours is Platypus (Lee et al., 2023) which aims to utilize a domain-specialized dataset to construct a<br>small-scale instruction following dataset to enhance LLMs' reasoning capabilities.</p>",
            "id": 183,
            "page": 16,
            "text": "Instruction tuning is part of a line of work designed to \"align\" language models with more useful objectives and human preferences. The instruction tuning step is seen as a major step to activate LLMs' certain capabilities to respond to human instructions. Previously, instruction tuning is mainly focused on enhancing LLMs' general-purpose instruction following abilities. Since 2021, CrossFit (Ye , 2021) and NaturalInstruction (Wang , 2022b), FLAN (Wei , 2022a) and T0 (Sanh , 2022) are amongst the first wave of instruction tuning effort to understand LLMs' generalization capabilities. Later on, FLAN-v2 (Chung , 2022; Longpre , 2023) have been proposed to understand the effect of scaling up the instruction datasets to understand its impact on model performance. These approaches mainly adopt human-annotated datasets to build the instruction following dataset. More recently, multiple works (Wang , 2023h; Xu , 2023; Peng , 2023; Zhou , 2023b; Wang , 2023g) propose to utilize synthetic instruction following data distilled from GPT-3/4 to align open-source LLMs. The most similar effort to ours is Platypus (Lee , 2023) which aims to utilize a domain-specialized dataset to construct a small-scale instruction following dataset to enhance LLMs' reasoning capabilities."
        },
        {
            "bounding_box": [
                {
                    "x": 1252,
                    "y": 3131
                },
                {
                    "x": 1300,
                    "y": 3131
                },
                {
                    "x": 1300,
                    "y": 3172
                },
                {
                    "x": 1252,
                    "y": 3172
                }
            ],
            "category": "footer",
            "html": "<footer id='184' style='font-size:14px'>16</footer>",
            "id": 184,
            "page": 16,
            "text": "16"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 112
                },
                {
                    "x": 899,
                    "y": 112
                },
                {
                    "x": 899,
                    "y": 159
                },
                {
                    "x": 444,
                    "y": 159
                }
            ],
            "category": "header",
            "html": "<header id='185' style='font-size:20px'>Preprint. Work in Progress</header>",
            "id": 185,
            "page": 17,
            "text": "Preprint. Work in Progress"
        },
        {
            "bounding_box": [
                {
                    "x": 448,
                    "y": 341
                },
                {
                    "x": 814,
                    "y": 341
                },
                {
                    "x": 814,
                    "y": 393
                },
                {
                    "x": 448,
                    "y": 393
                }
            ],
            "category": "paragraph",
            "html": "<p id='186' style='font-size:22px'>B CASE STUDY</p>",
            "id": 186,
            "page": 17,
            "text": "B CASE STUDY"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 442
                },
                {
                    "x": 2109,
                    "y": 442
                },
                {
                    "x": 2109,
                    "y": 768
                },
                {
                    "x": 443,
                    "y": 768
                }
            ],
            "category": "paragraph",
            "html": "<p id='187' style='font-size:16px'>We conduct a comparison between our PoT results VS. CoT results in Figure 3, Figure 4 and<br>Figure 5. In the first example, even though PoT and CoT can both solve the problem, CoT gives<br>a very tedious solution to derive the answer. Such solution is not only slow but also unstable. In<br>the second and third case, we can further see the advantages of PoT over CoT by utilizing external<br>tools and Python packages to greatly simplify the solution. Figure 6 shows some types of questions<br>(especially the formal logic question) that are not easily handled by programs. In order to address<br>these types of questions, CoT is a better choice.</p>",
            "id": 187,
            "page": 17,
            "text": "We conduct a comparison between our PoT results VS. CoT results in Figure 3, Figure 4 and Figure 5. In the first example, even though PoT and CoT can both solve the problem, CoT gives a very tedious solution to derive the answer. Such solution is not only slow but also unstable. In the second and third case, we can further see the advantages of PoT over CoT by utilizing external tools and Python packages to greatly simplify the solution. Figure 6 shows some types of questions (especially the formal logic question) that are not easily handled by programs. In order to address these types of questions, CoT is a better choice."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 814
                },
                {
                    "x": 2106,
                    "y": 814
                },
                {
                    "x": 2106,
                    "y": 1726
                },
                {
                    "x": 443,
                    "y": 1726
                }
            ],
            "category": "figure",
            "html": "<figure><img id='188' style='font-size:14px' alt=\"(3!)!\nWhat is the value of ?\n3!\nPoT CoT\n(3!)!\nFirst, we need to understand the given expression:\nfrom sympy import factorial 3!\nStep 1: Simplify the expression inside the parentheses.\n3! = 3 x2 x1 = 6\nvalue = factorial(factorial(3)) / factorial(3)\n(6)!\nprint(value) So, the expression becomes:\n6\nStep 2: Recall the property of factorials that states (n!)k = n! for any\n>>> 120 positive integer k and any positive integer n > k. In our case, we have\nn = 6 and k = 6.\nUsing this property, we can rewrite the expression as: 으\n6\nStep 3: Simplify the expression further by applying the property\nmentioned in Step 2. 6! = 6 x5 x4 x3 x2 x1 = 720\n720\nSo, the expression becomes:\n6\nStep 4: Divide 720 by 6. 720 ÷ 6 = 120\nSo, the final simplified expression is: 120\n(3!)!\nTherefore, the value of 1S 120\n3!\nThe answer is: 120\" data-coord=\"top-left:(443,814); bottom-right:(2106,1726)\" /></figure>",
            "id": 188,
            "page": 17,
            "text": "(3!)! What is the value of ? 3! PoT CoT (3!)! First, we need to understand the given expression: from sympy import factorial 3! Step 1: Simplify the expression inside the parentheses. 3! = 3 x2 x1 = 6 value = factorial(factorial(3)) / factorial(3) (6)! print(value) So, the expression becomes: 6 Step 2: Recall the property of factorials that states (n!)k = n! for any >>> 120 positive integer k and any positive integer n > k. In our case, we have n = 6 and k = 6. Using this property, we can rewrite the expression as: 으 6 Step 3: Simplify the expression further by applying the property mentioned in Step 2. 6! = 6 x5 x4 x3 x2 x1 = 720 720 So, the expression becomes: 6 Step 4: Divide 720 by 6. 720 ÷ 6 = 120 So, the final simplified expression is: 120 (3!)! Therefore, the value of 1S 120 3! The answer is: 120"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1773
                },
                {
                    "x": 2106,
                    "y": 1773
                },
                {
                    "x": 2106,
                    "y": 1868
                },
                {
                    "x": 442,
                    "y": 1868
                }
            ],
            "category": "caption",
            "html": "<caption id='189' style='font-size:16px'>Figure 3: Example 1: PoT and CoT can both solve the problem, however, CoT gives a very tedious<br>solution to derive the answer.</caption>",
            "id": 189,
            "page": 17,
            "text": "Figure 3: Example 1: PoT and CoT can both solve the problem, however, CoT gives a very tedious solution to derive the answer."
        },
        {
            "bounding_box": [
                {
                    "x": 447,
                    "y": 1926
                },
                {
                    "x": 2106,
                    "y": 1926
                },
                {
                    "x": 2106,
                    "y": 2576
                },
                {
                    "x": 447,
                    "y": 2576
                }
            ],
            "category": "figure",
            "html": "<figure><img id='190' style='font-size:14px' alt=\"Determine the number of ways to arrange the letters of the word TEPEE.\nPoT CoT\nfrom sympy import factorial\nFirst we count the arrangements if all the letters are unique, which is 4!.\nThen since the E's are not unique, we divide by 2! for the arrangements\nword = 'TEEPE'\n4!\nof the two E's, for an answer of 60\nn = len(word)\n2!\nd = word.count('E')\nways = factorial(n)/factorial(d) The answer is: 60\nprint(ways)\n>>> 20\nx\nV\" data-coord=\"top-left:(447,1926); bottom-right:(2106,2576)\" /></figure>",
            "id": 190,
            "page": 17,
            "text": "Determine the number of ways to arrange the letters of the word TEPEE. PoT CoT from sympy import factorial First we count the arrangements if all the letters are unique, which is 4!. Then since the E's are not unique, we divide by 2! for the arrangements word = \"TEEPE\" 4! of the two E's, for an answer of 60 n = len(word) 2! d = word.count('E') ways = factorial(n)/factorial(d) The answer is: 60 print(ways) >>> 20 x V"
        },
        {
            "bounding_box": [
                {
                    "x": 660,
                    "y": 2618
                },
                {
                    "x": 1882,
                    "y": 2618
                },
                {
                    "x": 1882,
                    "y": 2668
                },
                {
                    "x": 660,
                    "y": 2668
                }
            ],
            "category": "caption",
            "html": "<caption id='191' style='font-size:18px'>Figure 4: Example 2: PoT generates the correct solution while CoT fails.</caption>",
            "id": 191,
            "page": 17,
            "text": "Figure 4: Example 2: PoT generates the correct solution while CoT fails."
        },
        {
            "bounding_box": [
                {
                    "x": 1253,
                    "y": 3132
                },
                {
                    "x": 1300,
                    "y": 3132
                },
                {
                    "x": 1300,
                    "y": 3171
                },
                {
                    "x": 1253,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='192' style='font-size:16px'>17</footer>",
            "id": 192,
            "page": 17,
            "text": "17"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 113
                },
                {
                    "x": 899,
                    "y": 113
                },
                {
                    "x": 899,
                    "y": 158
                },
                {
                    "x": 445,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='193' style='font-size:22px'>Preprint. Work in Progress</header>",
            "id": 193,
            "page": 18,
            "text": "Preprint. Work in Progress"
        },
        {
            "bounding_box": [
                {
                    "x": 519,
                    "y": 346
                },
                {
                    "x": 2021,
                    "y": 346
                },
                {
                    "x": 2021,
                    "y": 1587
                },
                {
                    "x": 519,
                    "y": 1587
                }
            ],
            "category": "figure",
            "html": "<figure><img id='194' style='font-size:14px' alt=\"Question: a is chosen from the set {1,2,3,4}, and b is chosen from the set\n{5,6,7,8,9}. What is the probability that the product ab is a prime number?\nPoT CoT\nfrom sympy import isprime First, we need to find the total number of possible combinations of\nchoosing a number from each set. There are 4 numbers in the first set\n# define the sets\nand 5 numbers in the second set. So, there are 4 · 5 = 20 possible\na set = {1,2,3,4}\ncombinations.\nb_ set = {5,6,7,8,9}\nn a = len(a_set) Now, let's consider the prime numbers. A prime number is a number\nn b = len(b_set) greater than 1 that has no divisors other than 1 and itself. We need to\nprime_ count = 0 find the pairs of (a, b) where either a or b is a prime number, or both.\nprime_ ab count = 0\n1. If a is a prime number, there are 4 prime numbers in the first set: 2,\nprime_ a count = prime_ ab count\n3, 5, and 7. So, there are 4 possible combinations where a is a prime\nprime_ b count = prime_ ab count\nnumber.\n# iterate over all possible pairs\n2. Ifb is a prime number, there are 5 prime numbers in the second set:\nfor a in a set: 3, 5, 7, 11, and 13. So, there are 5 possible combinations where b is a\nfor b in b set: prime number.\nif isprime(a) or isprime(b): 3. Ifboth a and b are prime numbers, we need to find the pairs where\nif isprime(a*b): the prime numbers are 2, 3, 5, or 7. There are 4 pairs in this case: (2,3),\n# increment the counters (2, 5), (2, 7), and (3, 5).\nprime_ count += 1\nNow, let's add up the number of combinations where either a or b is a\nprime_ ab count += 1\nprime number: 4 (a is prime) + 5 (b is prime) + 4 (both are prime) = 13.\n# check if a is prime\nif isprime(a): So, there are 13 combinations where either a or b is a prime number.\nprime_a_ count += 1 Since there are 20 possible combinations in total, the probability that\n13\nif isprime(b): the product ab is a prime number is\n20\nprime_b_count += 1 13\nTherefore, the probability is\nprobability = prime_ ab count / (n_a * n b) 20\n13\nprint(probability) The answer is:\n20\n>>> 0.1\nV X\" data-coord=\"top-left:(519,346); bottom-right:(2021,1587)\" /></figure>",
            "id": 194,
            "page": 18,
            "text": "Question: a is chosen from the set {1,2,3,4}, and b is chosen from the set {5,6,7,8,9}. What is the probability that the product ab is a prime number? PoT CoT from sympy import isprime First, we need to find the total number of possible combinations of choosing a number from each set. There are 4 numbers in the first set # define the sets and 5 numbers in the second set. So, there are 4 · 5 = 20 possible a set = {1,2,3,4} combinations. b_ set = {5,6,7,8,9} n a = len(a_set) Now, let's consider the prime numbers. A prime number is a number n b = len(b_set) greater than 1 that has no divisors other than 1 and itself. We need to prime_ count = 0 find the pairs of (a, b) where either a or b is a prime number, or both. prime_ ab count = 0 1. If a is a prime number, there are 4 prime numbers in the first set: 2, prime_ a count = prime_ ab count 3, 5, and 7. So, there are 4 possible combinations where a is a prime prime_ b count = prime_ ab count number. # iterate over all possible pairs 2. Ifb is a prime number, there are 5 prime numbers in the second set: for a in a set: 3, 5, 7, 11, and 13. So, there are 5 possible combinations where b is a for b in b set: prime number. if isprime(a) or isprime(b): 3. Ifboth a and b are prime numbers, we need to find the pairs where if isprime(a*b): the prime numbers are 2, 3, 5, or 7. There are 4 pairs in this case: (2,3), # increment the counters (2, 5), (2, 7), and (3, 5). prime_ count += 1 Now, let's add up the number of combinations where either a or b is a prime_ ab count += 1 prime number: 4 (a is prime) + 5 (b is prime) + 4 (both are prime) = 13. # check if a is prime if isprime(a): So, there are 13 combinations where either a or b is a prime number. prime_a_ count += 1 Since there are 20 possible combinations in total, the probability that 13 if isprime(b): the product ab is a prime number is 20 prime_b_count += 1 13 Therefore, the probability is probability = prime_ ab count / (n_a * n b) 20 13 print(probability) The answer is: 20 >>> 0.1 V X"
        },
        {
            "bounding_box": [
                {
                    "x": 662,
                    "y": 1632
                },
                {
                    "x": 1881,
                    "y": 1632
                },
                {
                    "x": 1881,
                    "y": 1684
                },
                {
                    "x": 662,
                    "y": 1684
                }
            ],
            "category": "caption",
            "html": "<caption id='195' style='font-size:18px'>Figure 5: Example 3: PoT generates the correct solution while CoT fails.</caption>",
            "id": 195,
            "page": 18,
            "text": "Figure 5: Example 3: PoT generates the correct solution while CoT fails."
        },
        {
            "bounding_box": [
                {
                    "x": 531,
                    "y": 1805
                },
                {
                    "x": 2020,
                    "y": 1805
                },
                {
                    "x": 2020,
                    "y": 2745
                },
                {
                    "x": 531,
                    "y": 2745
                }
            ],
            "category": "paragraph",
            "html": "<p id='196' style='font-size:14px'>If n is such that 36 ≤ n ≤ 72, then x = (n2 + 2Vn(n + 4) + 16)/(n + 4Vn + 4) satisfies.<br>Answer Choices: (A) 20 < x < 54 (B) 23 < x < 58 (C) 25 < x < 64 (D) 28 < x < 60 (E) None of these<br>The expression in the numerator can be rewritten as (n + 2Vn + 4)2, and so the expression is equal to<br>n + 2Vn + 4. We substitute the smallest value ofn, which is 36, and get 42. We substitute the largest<br>value ofn, which is 72, and get 88. So the answer is (E) None of these CoT<br>Which of the following could NOT be the sides of an isosceles triangle?<br>Answer Choices: (A) 2,2,3 (B) 2,2,4 (C) 4,12,12 (D) 5,5,5<br>This is because in an isosceles triangle, the lengths oftwo sides are equal, and the length of the third side must be<br>less than the sum of the lengths of the other two sides. In this case, 4 is equal to the sum of the other two<br>sides (2+2), not less. Therefore, the answer is (B) 2,2,4. CoT<br>Select the best translation into predicate logic. Some CEO is wealthier than everyone. (Cx: x is a CEO; Px: x is a<br>person; Wxy: x is wealthier than y)<br>Answer Choices: (A) (ヨx)[Cx · (Ay)(Py · Wxy)] (B) (ヨx)[Cx U (Ay)(Py つ Wxy)]<br>(C) (ヨx)[Cx · (Ay)(Py U Wxy)] (D) (Px)[Ax U (ヨy)(Cy U Wxy)]<br>3x: There exists some x Cx: x is a CEO, and<br>Ay: For all y Py: y is a person, and Wxy: x is wealthier than y<br>So, in plain English, this expression says: \"There exists some x such that x is a CEO and for all y, ify is a person<br>then x is wealthier than y. \" Which matches the original statement.<br>Therefore, the answer is (A) (ヨx)[Cx · (Ay)(Py · Wxy)] CoT</p>",
            "id": 196,
            "page": 18,
            "text": "If n is such that 36 ≤ n ≤ 72, then x = (n2 + 2Vn(n + 4) + 16)/(n + 4Vn + 4) satisfies. Answer Choices: (A) 20 < x < 54 (B) 23 < x < 58 (C) 25 < x < 64 (D) 28 < x < 60 (E) None of these The expression in the numerator can be rewritten as (n + 2Vn + 4)2, and so the expression is equal to n + 2Vn + 4. We substitute the smallest value ofn, which is 36, and get 42. We substitute the largest value ofn, which is 72, and get 88. So the answer is (E) None of these CoT Which of the following could NOT be the sides of an isosceles triangle? Answer Choices: (A) 2,2,3 (B) 2,2,4 (C) 4,12,12 (D) 5,5,5 This is because in an isosceles triangle, the lengths oftwo sides are equal, and the length of the third side must be less than the sum of the lengths of the other two sides. In this case, 4 is equal to the sum of the other two sides (2+2), not less. Therefore, the answer is (B) 2,2,4. CoT Select the best translation into predicate logic. Some CEO is wealthier than everyone. (Cx: x is a CEO; Px: x is a person; Wxy: x is wealthier than y) Answer Choices: (A) (ヨx)[Cx · (Ay)(Py · Wxy)] (B) (ヨx)[Cx U (Ay)(Py つ Wxy)] (C) (ヨx)[Cx · (Ay)(Py U Wxy)] (D) (Px)[Ax U (ヨy)(Cy U Wxy)] 3x: There exists some x Cx: x is a CEO, and Ay: For all y Py: y is a person, and Wxy: x is wealthier than y So, in plain English, this expression says: \"There exists some x such that x is a CEO and for all y, ify is a person then x is wealthier than y. \" Which matches the original statement. Therefore, the answer is (A) (ヨx)[Cx · (Ay)(Py · Wxy)] CoT"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2798
                },
                {
                    "x": 2107,
                    "y": 2798
                },
                {
                    "x": 2107,
                    "y": 2894
                },
                {
                    "x": 442,
                    "y": 2894
                }
            ],
            "category": "paragraph",
            "html": "<p id='197' style='font-size:20px'>Figure 6: Example 4: Some types of questions (e.g., formal logic) are hard to be solved by PoT but<br>could be handled by CoT.</p>",
            "id": 197,
            "page": 18,
            "text": "Figure 6: Example 4: Some types of questions (e.g., formal logic) are hard to be solved by PoT but could be handled by CoT."
        },
        {
            "bounding_box": [
                {
                    "x": 1254,
                    "y": 3133
                },
                {
                    "x": 1300,
                    "y": 3133
                },
                {
                    "x": 1300,
                    "y": 3171
                },
                {
                    "x": 1254,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='198' style='font-size:16px'>18</footer>",
            "id": 198,
            "page": 18,
            "text": "18"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 111
                },
                {
                    "x": 899,
                    "y": 111
                },
                {
                    "x": 899,
                    "y": 159
                },
                {
                    "x": 444,
                    "y": 159
                }
            ],
            "category": "header",
            "html": "<header id='199' style='font-size:20px'>Preprint. Work in Progress</header>",
            "id": 199,
            "page": 19,
            "text": "Preprint. Work in Progress"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 336
                },
                {
                    "x": 2106,
                    "y": 336
                },
                {
                    "x": 2106,
                    "y": 630
                },
                {
                    "x": 441,
                    "y": 630
                }
            ],
            "category": "table",
            "html": "<table id='200' style='font-size:14px'><tr><td>Model</td><td>GSM</td><td>MATH</td><td>AQuA</td><td>NumG</td><td>SVA</td><td>Mat</td><td>Sim</td><td>SAT</td><td>MMLU</td><td>Overall</td></tr><tr><td>Base</td><td>14.6</td><td>2.5</td><td>30.3</td><td>29.9</td><td>34.5</td><td>6.0</td><td>5.0</td><td>26.8</td><td>29.8</td><td>19.9</td></tr><tr><td>WizzardMath</td><td>54.9</td><td>10.7</td><td>26.3</td><td>36.1</td><td>36.1</td><td>9.3</td><td>12.8</td><td>25.4</td><td>31.1</td><td>27.0</td></tr><tr><td>MAmmoTH (MathInstruct- CoT)</td><td>49.2</td><td>9.9</td><td>42.2</td><td>37.1</td><td>48.5</td><td>9.5</td><td>17.3</td><td>34.1</td><td>39.8</td><td>32.0</td></tr><tr><td>MAmmoTH (MathInstruct- PoT)</td><td>50.8</td><td>28.9</td><td>28.6</td><td>52.7</td><td>65.0</td><td>46.7</td><td>42.0</td><td>25.9</td><td>28.3</td><td>41.0</td></tr><tr><td>MAmmoTH (MathInstruct)</td><td>53.6</td><td>31.5</td><td>44.5</td><td>61.2</td><td>67.7</td><td>46.3</td><td>41.2</td><td>42.7</td><td>42.6</td><td>47.9</td></tr></table>",
            "id": 200,
            "page": 19,
            "text": "Model GSM MATH AQuA NumG SVA Mat Sim SAT MMLU Overall  Base 14.6 2.5 30.3 29.9 34.5 6.0 5.0 26.8 29.8 19.9  WizzardMath 54.9 10.7 26.3 36.1 36.1 9.3 12.8 25.4 31.1 27.0  MAmmoTH (MathInstruct- CoT) 49.2 9.9 42.2 37.1 48.5 9.5 17.3 34.1 39.8 32.0  MAmmoTH (MathInstruct- PoT) 50.8 28.9 28.6 52.7 65.0 46.7 42.0 25.9 28.3 41.0  MAmmoTH (MathInstruct) 53.6 31.5 44.5 61.2 67.7 46.3 41.2 42.7 42.6"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 664
                },
                {
                    "x": 2104,
                    "y": 664
                },
                {
                    "x": 2104,
                    "y": 758
                },
                {
                    "x": 443,
                    "y": 758
                }
            ],
            "category": "paragraph",
            "html": "<p id='201' style='font-size:18px'>Table 6: Breakdown results of Figure 2. Investigation of the influence of CoT & PoT hybrid training<br>on the 7B Llama-2 model.</p>",
            "id": 201,
            "page": 19,
            "text": "Table 6: Breakdown results of Figure 2. Investigation of the influence of CoT & PoT hybrid training on the 7B Llama-2 model."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 792
                },
                {
                    "x": 2106,
                    "y": 792
                },
                {
                    "x": 2106,
                    "y": 1618
                },
                {
                    "x": 442,
                    "y": 1618
                }
            ],
            "category": "table",
            "html": "<table id='202' style='font-size:14px'><tr><td>Model</td><td>Decoding</td><td>GSM</td><td>MATH</td><td>AQuA</td><td>NumG</td><td>SVA</td><td>Mat</td><td>Sim</td><td>SAT</td><td>MMLU</td><td>AVG</td></tr><tr><td rowspan=\"3\">MAmmoTH-7B</td><td>CoT</td><td>50.5</td><td>10.4</td><td>43.7</td><td>44.0</td><td>47.3</td><td>9.2</td><td>18.9</td><td>32.7</td><td>39.9</td><td>33.0</td></tr><tr><td>PoT</td><td>51.6</td><td>28.7</td><td>43.3</td><td>52.3</td><td>65.1</td><td>41.9</td><td>48.2</td><td>39.1</td><td>44.6</td><td>46.1</td></tr><tr><td>Hybrid</td><td>53.6</td><td>31.5</td><td>44.5</td><td>61.2</td><td>67.7</td><td>46.3</td><td>41.2</td><td>42.7</td><td>42.6</td><td>47.9</td></tr><tr><td rowspan=\"3\">MAmmoTH-Coder-7B</td><td>CoT</td><td>22.4</td><td>7.9</td><td>36.2</td><td>36.0</td><td>37.0</td><td>8.2</td><td>7.2</td><td>32.7</td><td>34.6</td><td>24.7</td></tr><tr><td>PoT</td><td>58.8</td><td>32.1</td><td>47.2</td><td>57.1</td><td>71.1</td><td>53.9</td><td>44.6</td><td>40.0</td><td>47.8</td><td>50.3</td></tr><tr><td>Hybrid</td><td>59.4</td><td>33.4</td><td>47.2</td><td>66.4</td><td>71.4</td><td>55.4</td><td>45.9</td><td>40.5</td><td>48.3</td><td>52.0</td></tr><tr><td rowspan=\"3\">MAmmoTH-13B</td><td>CoT</td><td>56.3</td><td>12.9</td><td>45.3</td><td>45.6</td><td>53.8</td><td>11.7</td><td>22.4</td><td>43.6</td><td>42.3</td><td>37.1</td></tr><tr><td>PoT</td><td>61.3</td><td>32.6</td><td>48.8</td><td>59.6</td><td>72.2</td><td>48.5</td><td>40.3</td><td>46.8</td><td>45.4</td><td>50.6</td></tr><tr><td>Hybrid</td><td>62.0</td><td>34.2</td><td>51.6</td><td>68.7</td><td>72.4</td><td>49.2</td><td>43.2</td><td>46.8</td><td>47.6</td><td>52.9</td></tr><tr><td rowspan=\"3\">MAmmoTH-Coder-13B</td><td>CoT</td><td>32.1</td><td>10.2</td><td>40.6</td><td>36.2</td><td>43.0</td><td>9.6</td><td>10.1</td><td>40.9</td><td>36.6</td><td>28.8</td></tr><tr><td>PoT</td><td>64.3</td><td>35.2</td><td>46.8</td><td>54.2</td><td>73.2</td><td>60.0</td><td>44.2</td><td>48.2</td><td>48.2</td><td>52.7</td></tr><tr><td>Hybrid</td><td>64.7</td><td>36.3</td><td>46.9</td><td>66.8</td><td>73.7</td><td>61.5</td><td>47.1</td><td>48.6</td><td>48.3</td><td>54.9</td></tr><tr><td rowspan=\"3\">MAmmoTH-Coder-33B</td><td>CoT</td><td>34.3</td><td>11.6</td><td>39.0</td><td>36.2</td><td>44.6</td><td>10.8</td><td>10.9</td><td>46.4</td><td>42.9</td><td>30.7</td></tr><tr><td>PoT</td><td>72.3</td><td>42.8</td><td>53.8</td><td>59.6</td><td>84.0</td><td>64.7</td><td>50.6</td><td>58.6</td><td>52.7</td><td>59.9</td></tr><tr><td>Hybrid</td><td>72.7</td><td>43.6</td><td>54.7</td><td>71.6</td><td>84.3</td><td>65.4</td><td>51.8</td><td>60.9</td><td>53.8</td><td>62.1</td></tr><tr><td rowspan=\"3\">MAmmo TH-70B</td><td>CoT</td><td>72.4</td><td>21.1</td><td>57.9</td><td>58.9</td><td>71.6</td><td>20.0</td><td>31.9</td><td>57.3</td><td>52.1</td><td>49.2</td></tr><tr><td>PoT</td><td>76.7</td><td>40.1</td><td>60.2</td><td>64.3</td><td>81.7</td><td>55.3</td><td>45.3</td><td>64.1</td><td>53.5</td><td>60.1</td></tr><tr><td>Hybrid</td><td>76.9</td><td>41.8</td><td>65.0</td><td>74.4</td><td>82.4</td><td>55.6</td><td>51.4</td><td>66.4</td><td>56.7</td><td>63.4</td></tr></table>",
            "id": 202,
            "page": 19,
            "text": "Model Decoding GSM MATH AQuA NumG SVA Mat Sim SAT MMLU AVG  MAmmoTH-7B CoT 50.5 10.4 43.7 44.0 47.3 9.2 18.9 32.7 39.9 33.0  PoT 51.6 28.7 43.3 52.3 65.1 41.9 48.2 39.1 44.6 46.1  Hybrid 53.6 31.5 44.5 61.2 67.7 46.3 41.2 42.7 42.6 47.9  MAmmoTH-Coder-7B CoT 22.4 7.9 36.2 36.0 37.0 8.2 7.2 32.7 34.6 24.7  PoT 58.8 32.1 47.2 57.1 71.1 53.9 44.6 40.0 47.8 50.3  Hybrid 59.4 33.4 47.2 66.4 71.4 55.4 45.9 40.5 48.3 52.0  MAmmoTH-13B CoT 56.3 12.9 45.3 45.6 53.8 11.7 22.4 43.6 42.3 37.1  PoT 61.3 32.6 48.8 59.6 72.2 48.5 40.3 46.8 45.4 50.6  Hybrid 62.0 34.2 51.6 68.7 72.4 49.2 43.2 46.8 47.6 52.9  MAmmoTH-Coder-13B CoT 32.1 10.2 40.6 36.2 43.0 9.6 10.1 40.9 36.6 28.8  PoT 64.3 35.2 46.8 54.2 73.2 60.0 44.2 48.2 48.2 52.7  Hybrid 64.7 36.3 46.9 66.8 73.7 61.5 47.1 48.6 48.3 54.9  MAmmoTH-Coder-33B CoT 34.3 11.6 39.0 36.2 44.6 10.8 10.9 46.4 42.9 30.7  PoT 72.3 42.8 53.8 59.6 84.0 64.7 50.6 58.6 52.7 59.9  Hybrid 72.7 43.6 54.7 71.6 84.3 65.4 51.8 60.9 53.8 62.1  MAmmo TH-70B CoT 72.4 21.1 57.9 58.9 71.6 20.0 31.9 57.3 52.1 49.2  PoT 76.7 40.1 60.2 64.3 81.7 55.3 45.3 64.1 53.5 60.1  Hybrid 76.9 41.8 65.0 74.4 82.4 55.6 51.4 66.4 56.7"
        },
        {
            "bounding_box": [
                {
                    "x": 728,
                    "y": 1653
                },
                {
                    "x": 1819,
                    "y": 1653
                },
                {
                    "x": 1819,
                    "y": 1701
                },
                {
                    "x": 728,
                    "y": 1701
                }
            ],
            "category": "caption",
            "html": "<caption id='203' style='font-size:18px'>Table 7: Influence of different decoding methods on each dataset.</caption>",
            "id": 203,
            "page": 19,
            "text": "Table 7: Influence of different decoding methods on each dataset."
        },
        {
            "bounding_box": [
                {
                    "x": 446,
                    "y": 1782
                },
                {
                    "x": 817,
                    "y": 1782
                },
                {
                    "x": 817,
                    "y": 1832
                },
                {
                    "x": 446,
                    "y": 1832
                }
            ],
            "category": "paragraph",
            "html": "<p id='204' style='font-size:22px'>C LIMITATIONS</p>",
            "id": 204,
            "page": 19,
            "text": "C LIMITATIONS"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1883
                },
                {
                    "x": 2106,
                    "y": 1883
                },
                {
                    "x": 2106,
                    "y": 2162
                },
                {
                    "x": 442,
                    "y": 2162
                }
            ],
            "category": "paragraph",
            "html": "<p id='205' style='font-size:18px'>Despite their training on a diverse set of mathematical rationale datasets, the MAmmo TH models<br>might exhibit limitations when faced with problems outside their primary domain of expertise like<br>mathematical analysis, complex analysis, graph theory, numerical analysis, etc. Thus, our models<br>are not suitable for solving more complex problems in these fields. Also, they have not been trained<br>with proof-type problems, thus their theorem-proving capability is also limited. In the future, we<br>would like to expand the models' skill set to cover more fields and theorem-proving problems.</p>",
            "id": 205,
            "page": 19,
            "text": "Despite their training on a diverse set of mathematical rationale datasets, the MAmmo TH models might exhibit limitations when faced with problems outside their primary domain of expertise like mathematical analysis, complex analysis, graph theory, numerical analysis, etc. Thus, our models are not suitable for solving more complex problems in these fields. Also, they have not been trained with proof-type problems, thus their theorem-proving capability is also limited. In the future, we would like to expand the models' skill set to cover more fields and theorem-proving problems."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2183
                },
                {
                    "x": 2106,
                    "y": 2183
                },
                {
                    "x": 2106,
                    "y": 2507
                },
                {
                    "x": 442,
                    "y": 2507
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='206' style='font-size:18px'>There is also a risk of the MAmmoTH models generating potentially harmful, offensive, or biased<br>content, especially if they are asked to answer questions beyond math. The MAmmoTH series could<br>be misused for malicious purposes, such as spreading misinformation or probing sensitive topics.<br>Developers should conduct safety testing and tuning tailored to their specific applications before<br>deploying any MAmmo TH model. While we have made every effort to ensure the cleanliness and<br>purity of our training data, we cannot guarantee absolute perfection. Itis unlikely but not impossible<br>that some inappropriate questions slipped through the curation process.</p>",
            "id": 206,
            "page": 19,
            "text": "There is also a risk of the MAmmoTH models generating potentially harmful, offensive, or biased content, especially if they are asked to answer questions beyond math. The MAmmoTH series could be misused for malicious purposes, such as spreading misinformation or probing sensitive topics. Developers should conduct safety testing and tuning tailored to their specific applications before deploying any MAmmo TH model. While we have made every effort to ensure the cleanliness and purity of our training data, we cannot guarantee absolute perfection. Itis unlikely but not impossible that some inappropriate questions slipped through the curation process."
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 2528
                },
                {
                    "x": 2108,
                    "y": 2528
                },
                {
                    "x": 2108,
                    "y": 2805
                },
                {
                    "x": 440,
                    "y": 2805
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='207' style='font-size:18px'>Future work may continue to explore how to further improve the robustness and generalizability<br>of MAmmoTH in mathematical reasoning. For example, recent work identifies \"sycophancy\" and<br>\"Clever Hans effect\" in reasoning: LLMs cannot maintain truthful solutions to reasoning tasks when<br>challenged by the user's absurdly invalid arguments and critiques (Wang et al., 2023b). Potential<br>methods to improve the models' reasoning robustness could involve the exploration of synthetic<br>data intervention methods as explored in (Wei et al., 2023).</p>",
            "id": 207,
            "page": 19,
            "text": "Future work may continue to explore how to further improve the robustness and generalizability of MAmmoTH in mathematical reasoning. For example, recent work identifies \"sycophancy\" and \"Clever Hans effect\" in reasoning: LLMs cannot maintain truthful solutions to reasoning tasks when challenged by the user's absurdly invalid arguments and critiques (Wang , 2023b). Potential methods to improve the models' reasoning robustness could involve the exploration of synthetic data intervention methods as explored in (Wei , 2023)."
        },
        {
            "bounding_box": [
                {
                    "x": 1253,
                    "y": 3131
                },
                {
                    "x": 1300,
                    "y": 3131
                },
                {
                    "x": 1300,
                    "y": 3172
                },
                {
                    "x": 1253,
                    "y": 3172
                }
            ],
            "category": "footer",
            "html": "<footer id='208' style='font-size:16px'>19</footer>",
            "id": 208,
            "page": 19,
            "text": "19"
        }
    ]
}