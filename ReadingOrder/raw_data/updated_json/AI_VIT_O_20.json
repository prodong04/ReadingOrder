{
    "id": "62aed86a-0f92-11ef-8230-426932df3dcf",
    "pdf_path": "/root/data/pdf/2107.00652v3.pdf",
    "elements": [
        {
            "bounding_box": [
                {
                    "x": 365,
                    "y": 433
                },
                {
                    "x": 2120,
                    "y": 433
                },
                {
                    "x": 2120,
                    "y": 578
                },
                {
                    "x": 365,
                    "y": 578
                }
            ],
            "category": "paragraph",
            "html": "<p id='0' style='font-size:22px'>CSWin Transformer: A General Vision Transformer Backbone with<br>Cross-Shaped Windows</p>",
            "id": 0,
            "page": 1,
            "text": "CSWin Transformer: A General Vision Transformer Backbone with\nCross-Shaped Windows"
        },
        {
            "bounding_box": [
                {
                    "x": 571,
                    "y": 673
                },
                {
                    "x": 1902,
                    "y": 673
                },
                {
                    "x": 1902,
                    "y": 908
                },
                {
                    "x": 571,
                    "y": 908
                }
            ],
            "category": "paragraph",
            "html": "<p id='1' style='font-size:20px'>Xiaoyi Dong1* Jianmin Bao2, Dongdong Chen3, Weiming Zhang1,<br>Nenghai Yu1 , Lu Yuan3, Dong Chen2, Baining Guo2<br>1University of Science and Technology of China<br>2Microsoft Research Asia 3Microsoft Cloud + AI</p>",
            "id": 1,
            "page": 1,
            "text": "Xiaoyi Dong1* Jianmin Bao2, Dongdong Chen3, Weiming Zhang1,\nNenghai Yu1 , Lu Yuan3, Dong Chen2, Baining Guo2\n1University of Science and Technology of China\n2Microsoft Research Asia 3Microsoft Cloud + AI"
        },
        {
            "bounding_box": [
                {
                    "x": 573,
                    "y": 918
                },
                {
                    "x": 1914,
                    "y": 918
                },
                {
                    "x": 1914,
                    "y": 1024
                },
                {
                    "x": 573,
                    "y": 1024
                }
            ],
            "category": "paragraph",
            "html": "<p id='2' style='font-size:14px'>{dlight@mail · , zhangwm@ , ynh@} · ustc · edu · cn cddlyf@gmail · com<br>{jianbao, luyuan, doch, bainguo }@microsoft · com</p>",
            "id": 2,
            "page": 1,
            "text": "{dlight@mail · , zhangwm@ , ynh@} · ustc · edu · cn cddlyf@gmail · com\n{jianbao, luyuan, doch, bainguo }@microsoft · com"
        },
        {
            "bounding_box": [
                {
                    "x": 602,
                    "y": 1140
                },
                {
                    "x": 799,
                    "y": 1140
                },
                {
                    "x": 799,
                    "y": 1192
                },
                {
                    "x": 602,
                    "y": 1192
                }
            ],
            "category": "paragraph",
            "html": "<p id='3' style='font-size:18px'>Abstract</p>",
            "id": 3,
            "page": 1,
            "text": "Abstract"
        },
        {
            "bounding_box": [
                {
                    "x": 198,
                    "y": 1273
                },
                {
                    "x": 1202,
                    "y": 1273
                },
                {
                    "x": 1202,
                    "y": 2776
                },
                {
                    "x": 198,
                    "y": 2776
                }
            ],
            "category": "paragraph",
            "html": "<p id='4' style='font-size:18px'>We present CSWin Transformer, an efficient and effec-<br>tive Transformer-based backbone for general-purpose vision<br>tasks. A challenging issue in Transformer design is that<br>global self-attention is very expensive to compute whereas<br>local self-attention often limits the field of interactions of<br>each token. To address this issue, we develop the Cross-<br>Shaped Window self-attention mechanism for computing<br>self-attention in the horizontal and vertical stripes in parallel<br>that form a cross-shaped window, with each stripe obtained<br>by splitting the inputfeature into stripes of equal width. We<br>provide a mathematical analysis of the effect of the stripe<br>width and vary the stripe width for different layers of the<br>Transformer network which achieves strong modeling capa-<br>bility while limiting the computation cost. We also introduce<br>Locally-enhanced Positional Encoding (LePE), which han-<br>dles the local positional information better than existing<br>encoding schemes. LePE naturally supports arbitrary input<br>resolutions, and is thus especially effective and friendly for<br>downstream tasks. Incorporated with these designs and a hi-<br>erarchical structure, CSWin Transformer demonstrates com-<br>petitive performance on common vision tasks. Specifically,<br>it achieves 85.4% Top-1 accuracy on ImageNet-1K without<br>any extra training data or label, 53.9 box AP and 46.4 mask<br>AP on the COCO detection task, and 52.2 mIOU on the<br>ADE20K semantic segmentation task, surpassing previous<br>state-of-the-art Swin Transformer backbone by +1.2, +2.0,<br>+1.4, and +2.0 respectively under the similar FLOPs setting.<br>By further pretraining on the larger dataset ImageNet-21K,<br>we achieve 87.5% Top-1 accuracy on ImageNet-1K and high<br>segmentation performance on ADE20K with 55.7 mIoU.</p>",
            "id": 4,
            "page": 1,
            "text": "We present CSWin Transformer, an efficient and effec-\ntive Transformer-based backbone for general-purpose vision\ntasks. A challenging issue in Transformer design is that\nglobal self-attention is very expensive to compute whereas\nlocal self-attention often limits the field of interactions of\neach token. To address this issue, we develop the Cross-\nShaped Window self-attention mechanism for computing\nself-attention in the horizontal and vertical stripes in parallel\nthat form a cross-shaped window, with each stripe obtained\nby splitting the inputfeature into stripes of equal width. We\nprovide a mathematical analysis of the effect of the stripe\nwidth and vary the stripe width for different layers of the\nTransformer network which achieves strong modeling capa-\nbility while limiting the computation cost. We also introduce\nLocally-enhanced Positional Encoding (LePE), which han-\ndles the local positional information better than existing\nencoding schemes. LePE naturally supports arbitrary input\nresolutions, and is thus especially effective and friendly for\ndownstream tasks. Incorporated with these designs and a hi-\nerarchical structure, CSWin Transformer demonstrates com-\npetitive performance on common vision tasks. Specifically,\nit achieves 85.4% Top-1 accuracy on ImageNet-1K without\nany extra training data or label, 53.9 box AP and 46.4 mask\nAP on the COCO detection task, and 52.2 mIOU on the\nADE20K semantic segmentation task, surpassing previous\nstate-of-the-art Swin Transformer backbone by +1.2, +2.0,\n+1.4, and +2.0 respectively under the similar FLOPs setting.\nBy further pretraining on the larger dataset ImageNet-21K,\nwe achieve 87.5% Top-1 accuracy on ImageNet-1K and high\nsegmentation performance on ADE20K with 55.7 mIoU."
        },
        {
            "bounding_box": [
                {
                    "x": 254,
                    "y": 2931
                },
                {
                    "x": 1068,
                    "y": 2931
                },
                {
                    "x": 1068,
                    "y": 2973
                },
                {
                    "x": 254,
                    "y": 2973
                }
            ],
            "category": "paragraph",
            "html": "<p id='5' style='font-size:16px'>*Work done during an internship at Microsoft Research Asia.</p>",
            "id": 5,
            "page": 1,
            "text": "*Work done during an internship at Microsoft Research Asia."
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 1138
                },
                {
                    "x": 1611,
                    "y": 1138
                },
                {
                    "x": 1611,
                    "y": 1193
                },
                {
                    "x": 1278,
                    "y": 1193
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='6' style='font-size:20px'>1. Introduction</p>",
            "id": 6,
            "page": 1,
            "text": "1. Introduction"
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 1224
                },
                {
                    "x": 2282,
                    "y": 1224
                },
                {
                    "x": 2282,
                    "y": 1723
                },
                {
                    "x": 1278,
                    "y": 1723
                }
            ],
            "category": "paragraph",
            "html": "<p id='7' style='font-size:18px'>Transformer-based architectures [17, 38, 53, 60] have re-<br>cently achieved competitive performances compared to their<br>CNN counterparts in various vision tasks. By leveraging<br>the multi-head self-attention mechanism, these vision Trans-<br>formers demonstrate a high capability in modeling the long-<br>range dependencies, which is especially helpful for handling<br>high-resolution inputs in downstream tasks, e.g., object de-<br>tection and segmentation. Despite the success, the Trans-<br>former architecture with full-attention mechanism [17] is<br>computationally inefficient.</p>",
            "id": 7,
            "page": 1,
            "text": "Transformer-based architectures [17, 38, 53, 60] have re-\ncently achieved competitive performances compared to their\nCNN counterparts in various vision tasks. By leveraging\nthe multi-head self-attention mechanism, these vision Trans-\nformers demonstrate a high capability in modeling the long-\nrange dependencies, which is especially helpful for handling\nhigh-resolution inputs in downstream tasks, e.g., object de-\ntection and segmentation. Despite the success, the Trans-\nformer architecture with full-attention mechanism [17] is\ncomputationally inefficient."
        },
        {
            "bounding_box": [
                {
                    "x": 1277,
                    "y": 1728
                },
                {
                    "x": 2279,
                    "y": 1728
                },
                {
                    "x": 2279,
                    "y": 2324
                },
                {
                    "x": 1277,
                    "y": 2324
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='8' style='font-size:18px'>To improve the efficiency, one typical way is to limit<br>the attention region of each token from full-attention to lo-<br>cal/windowed attention [38, 55]. To bridge the connection<br>between windows, researchers further proposed halo and<br>shift operations to exchange information through nearby win-<br>dows. However, the receptive field is enlarged quite slowly<br>and it requires stacking a great number of blocks to achieve<br>global self-attention. A sufficiently large receptive field is<br>crucial to the performance especially for the downstream<br>tasks(e.g., object detection and segmentation). Therefore it<br>is important to achieve large receptive filed efficiently while<br>keeping the computation cost low.</p>",
            "id": 8,
            "page": 1,
            "text": "To improve the efficiency, one typical way is to limit\nthe attention region of each token from full-attention to lo-\ncal/windowed attention [38, 55]. To bridge the connection\nbetween windows, researchers further proposed halo and\nshift operations to exchange information through nearby win-\ndows. However, the receptive field is enlarged quite slowly\nand it requires stacking a great number of blocks to achieve\nglobal self-attention. A sufficiently large receptive field is\ncrucial to the performance especially for the downstream\ntasks(e.g., object detection and segmentation). Therefore it\nis important to achieve large receptive filed efficiently while\nkeeping the computation cost low."
        },
        {
            "bounding_box": [
                {
                    "x": 1277,
                    "y": 2329
                },
                {
                    "x": 2279,
                    "y": 2329
                },
                {
                    "x": 2279,
                    "y": 2978
                },
                {
                    "x": 1277,
                    "y": 2978
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='9' style='font-size:16px'>In this paper, we present the Cross-Shaped Window<br>(CSWin) self-attention, which is illustrated in Figure 1 and<br>compared with existing self-attention mechanisms. With<br>CSWin self-attention, we perform the self-attention calcu-<br>lation in the horizontal and vertical stripes in parallel, with<br>each stripe obtained by splitting the input feature into stripes<br>of equal width. This stripe width is an important parameter<br>of the cross-shaped window because it allows us to achieve<br>strong modelling capability while limiting the computation<br>cost. Specifically, we adjust the stripe width according to the<br>depth of the network: small widths for shallow layers and<br>larger widths for deep layers. A larger stripe width encour-<br>ages a stronger connection between long-range elements and</p>",
            "id": 9,
            "page": 1,
            "text": "In this paper, we present the Cross-Shaped Window\n(CSWin) self-attention, which is illustrated in Figure 1 and\ncompared with existing self-attention mechanisms. With\nCSWin self-attention, we perform the self-attention calcu-\nlation in the horizontal and vertical stripes in parallel, with\neach stripe obtained by splitting the input feature into stripes\nof equal width. This stripe width is an important parameter\nof the cross-shaped window because it allows us to achieve\nstrong modelling capability while limiting the computation\ncost. Specifically, we adjust the stripe width according to the\ndepth of the network: small widths for shallow layers and\nlarger widths for deep layers. A larger stripe width encour-\nages a stronger connection between long-range elements and"
        },
        {
            "bounding_box": [
                {
                    "x": 59,
                    "y": 911
                },
                {
                    "x": 149,
                    "y": 911
                },
                {
                    "x": 149,
                    "y": 2315
                },
                {
                    "x": 59,
                    "y": 2315
                }
            ],
            "category": "footer",
            "html": "<br><footer id='10' style='font-size:14px'>2022<br>Jan<br>6<br>[cs.CV]<br>arXiv:2107.00652v3</footer>",
            "id": 10,
            "page": 1,
            "text": "2022\nJan\n6\n[cs.CV]\narXiv:2107.00652v3"
        },
        {
            "bounding_box": [
                {
                    "x": 234,
                    "y": 269
                },
                {
                    "x": 2239,
                    "y": 269
                },
                {
                    "x": 2239,
                    "y": 907
                },
                {
                    "x": 234,
                    "y": 907
                }
            ],
            "category": "figure",
            "html": "<figure><img id='11' style='font-size:14px' alt=\"SW\nNext\nBlock\nhk hk\nhK/2 hk\nhk\nh1 Concat Slide Local Shifted Local\nh1 h1 h1 h1\nSplit Head\nFull Attention\nSW\nNext\nBlock\nhk\nhk hk hk hk\nhk/2+1\nh1 h1 h1 h1\nDynaic Stripe Window + Parallel Grouing Heads = CSWin Criss-Cross Local + Global Sequential Axial\" data-coord=\"top-left:(234,269); bottom-right:(2239,907)\" /></figure>",
            "id": 11,
            "page": 2,
            "text": "SW\nNext\nBlock\nhk hk\nhK/2 hk\nhk\nh1 Concat Slide Local Shifted Local\nh1 h1 h1 h1\nSplit Head\nFull Attention\nSW\nNext\nBlock\nhk\nhk hk hk hk\nhk/2+1\nh1 h1 h1 h1\nDynaic Stripe Window + Parallel Grouing Heads = CSWin Criss-Cross Local + Global Sequential Axial"
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 922
                },
                {
                    "x": 2279,
                    "y": 922
                },
                {
                    "x": 2279,
                    "y": 1061
                },
                {
                    "x": 199,
                    "y": 1061
                }
            ],
            "category": "caption",
            "html": "<caption id='12' style='font-size:16px'>Figure 1. Illustration of different self-attention mechanisms, our CSWin is fundamentally different from two aspects. First, we split<br>multi-heads ({h1, · · · , hk }) into two groups and perform self-attention in horizontal and vertical stripes simultaneously. Second, we adjust<br>the stripe width according to the depth network, which can achieve better trade-off between computation cost and capability</caption>",
            "id": 12,
            "page": 2,
            "text": "Figure 1. Illustration of different self-attention mechanisms, our CSWin is fundamentally different from two aspects. First, we split\nmulti-heads ({h1, · · · , hk }) into two groups and perform self-attention in horizontal and vertical stripes simultaneously. Second, we adjust\nthe stripe width according to the depth network, which can achieve better trade-off between computation cost and capability"
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 1091
                },
                {
                    "x": 1201,
                    "y": 1091
                },
                {
                    "x": 1201,
                    "y": 1287
                },
                {
                    "x": 200,
                    "y": 1287
                }
            ],
            "category": "paragraph",
            "html": "<p id='13' style='font-size:18px'>achieves better network capacity with a small increase in<br>computation cost. We will provide a mathematical analysis<br>of how the stripe width affects the modeling capability and<br>computation cost.</p>",
            "id": 13,
            "page": 2,
            "text": "achieves better network capacity with a small increase in\ncomputation cost. We will provide a mathematical analysis\nof how the stripe width affects the modeling capability and\ncomputation cost."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 1304
                },
                {
                    "x": 1202,
                    "y": 1304
                },
                {
                    "x": 1202,
                    "y": 2004
                },
                {
                    "x": 200,
                    "y": 2004
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='14' style='font-size:18px'>It is worthwhile to note that with CSWin self-attention<br>mechanism, the self-attention in horizontal and vertical<br>stripes are calculated in parallel. We split the multi-heads<br>into parallel groups and apply different self-attention op-<br>erations onto different groups. This parallel strategy intro-<br>duces no extra computation cost while enlarging the area<br>for computing self-attention within each Transformer block.<br>This strategy is fundamentally different from existing self-<br>attention mechanisms [25, 38, 56, 69] that apply the same<br>attention operation across multi-heads((Figure 1 b,c,d,e), and<br>perform different attention operations sequentially(Figure 1<br>c,e). We will show through ablation analysis that this differ-<br>ence makes CSWin self-attention much more effective for<br>general vision tasks.</p>",
            "id": 14,
            "page": 2,
            "text": "It is worthwhile to note that with CSWin self-attention\nmechanism, the self-attention in horizontal and vertical\nstripes are calculated in parallel. We split the multi-heads\ninto parallel groups and apply different self-attention op-\nerations onto different groups. This parallel strategy intro-\nduces no extra computation cost while enlarging the area\nfor computing self-attention within each Transformer block.\nThis strategy is fundamentally different from existing self-\nattention mechanisms [25, 38, 56, 69] that apply the same\nattention operation across multi-heads((Figure 1 b,c,d,e), and\nperform different attention operations sequentially(Figure 1\nc,e). We will show through ablation analysis that this differ-\nence makes CSWin self-attention much more effective for\ngeneral vision tasks."
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 2015
                },
                {
                    "x": 1202,
                    "y": 2015
                },
                {
                    "x": 1202,
                    "y": 2763
                },
                {
                    "x": 199,
                    "y": 2763
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='15' style='font-size:20px'>Based on the CSWin self-attention mechanism, we fol-<br>low the hierarchical design and propose a new vision<br>Transformer architecture named \"CSWin Transformer\" for<br>general-purpose vision tasks. This architecture provides<br>significantly stronger modeling power while limiting compu-<br>tation cost. To further enhance this vision Transformer, we<br>introduce an effective positional encoding, Locally-enhanced<br>Positional Encoding (LePE), which is especially effective<br>and friendly for input varying downstream tasks such as ob-<br>ject detection and segmentation. Compared with previous<br>positional encoding methods [12, 46, 56], our LePE imposes<br>the positional information within each Transformer block<br>and directly operates on the attention results instead of the<br>attention calculation. The LePE makes CSWin Transformer<br>more effective and friendly for the downstream tasks.</p>",
            "id": 15,
            "page": 2,
            "text": "Based on the CSWin self-attention mechanism, we fol-\nlow the hierarchical design and propose a new vision\nTransformer architecture named \"CSWin Transformer\" for\ngeneral-purpose vision tasks. This architecture provides\nsignificantly stronger modeling power while limiting compu-\ntation cost. To further enhance this vision Transformer, we\nintroduce an effective positional encoding, Locally-enhanced\nPositional Encoding (LePE), which is especially effective\nand friendly for input varying downstream tasks such as ob-\nject detection and segmentation. Compared with previous\npositional encoding methods [12, 46, 56], our LePE imposes\nthe positional information within each Transformer block\nand directly operates on the attention results instead of the\nattention calculation. The LePE makes CSWin Transformer\nmore effective and friendly for the downstream tasks."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 2778
                },
                {
                    "x": 1204,
                    "y": 2778
                },
                {
                    "x": 1204,
                    "y": 2976
                },
                {
                    "x": 200,
                    "y": 2976
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='16' style='font-size:18px'>As a general vision Transformer backbone, the CSWin<br>Transformer demonstrates strong performance on image clas-<br>sification, object detection and semantic segmentation tasks.<br>Under the similar FLOPs and model size, CSWin Trans-</p>",
            "id": 16,
            "page": 2,
            "text": "As a general vision Transformer backbone, the CSWin\nTransformer demonstrates strong performance on image clas-\nsification, object detection and semantic segmentation tasks.\nUnder the similar FLOPs and model size, CSWin Trans-"
        },
        {
            "bounding_box": [
                {
                    "x": 1277,
                    "y": 1091
                },
                {
                    "x": 2280,
                    "y": 1091
                },
                {
                    "x": 2280,
                    "y": 1839
                },
                {
                    "x": 1277,
                    "y": 1839
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='17' style='font-size:18px'>former variants significantly outperforms previous state-<br>of-the-art (SOTA) vision Transformers. For example, our<br>base variant CSWin-B achieves 85.4% Top-1 accuracy on<br>ImageNet-1K without any extra training data or label, 53.9<br>box AP and 46.4 mask AP on the COCO detection task, 51.7<br>mIOU on the ADE20K semantic segmentation task, surpass-<br>ing previous state-of-the-art Swin Transformer counterpart<br>by +1.2, +2.0, 1.4 and +2.0 respectively. Under a smaller<br>FLOPs setting, our tiny variant CSWin-T even shows larger<br>performance gains, i.e.,, +1.4 point on ImageNet classifica-<br>tion, +3.0 box AP, +2.0 mask AP on COCO detection and<br>+4.6 on ADE20K segmentation. Furthermore, when pretrain-<br>ing CSWin Transformer on the larger dataset ImageNet-21K,<br>we achieve 87.5% Top-1 accuracy on ImageNet-1K and high<br>segmentation performance on ADE20K with 55.7 mIoU.</p>",
            "id": 17,
            "page": 2,
            "text": "former variants significantly outperforms previous state-\nof-the-art (SOTA) vision Transformers. For example, our\nbase variant CSWin-B achieves 85.4% Top-1 accuracy on\nImageNet-1K without any extra training data or label, 53.9\nbox AP and 46.4 mask AP on the COCO detection task, 51.7\nmIOU on the ADE20K semantic segmentation task, surpass-\ning previous state-of-the-art Swin Transformer counterpart\nby +1.2, +2.0, 1.4 and +2.0 respectively. Under a smaller\nFLOPs setting, our tiny variant CSWin-T even shows larger\nperformance gains, i.e.,, +1.4 point on ImageNet classifica-\ntion, +3.0 box AP, +2.0 mask AP on COCO detection and\n+4.6 on ADE20K segmentation. Furthermore, when pretrain-\ning CSWin Transformer on the larger dataset ImageNet-21K,\nwe achieve 87.5% Top-1 accuracy on ImageNet-1K and high\nsegmentation performance on ADE20K with 55.7 mIoU."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1898
                },
                {
                    "x": 1636,
                    "y": 1898
                },
                {
                    "x": 1636,
                    "y": 1949
                },
                {
                    "x": 1280,
                    "y": 1949
                }
            ],
            "category": "paragraph",
            "html": "<p id='18' style='font-size:22px'>2. Related Work</p>",
            "id": 18,
            "page": 2,
            "text": "2. Related Work"
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 1978
                },
                {
                    "x": 2281,
                    "y": 1978
                },
                {
                    "x": 2281,
                    "y": 2979
                },
                {
                    "x": 1276,
                    "y": 2979
                }
            ],
            "category": "paragraph",
            "html": "<p id='19' style='font-size:20px'>Vision Transformers. Convolutional neural networks<br>(CNN) have dominated the computer vision field for many<br>years and achieved tremendous successes [7, 22, 26-28, 35,<br>45, 47, 49-51]. Recently, the pioneering work ViT [17]<br>demonstrates that pure Transformer-based architectures can<br>also achieve very competitive results, indicating the potential<br>of handling the vision tasks and natural language processing<br>(NLP) tasks under a unified framework. Built upon the suc-<br>cess of ViT, many efforts have been devoted to designing bet-<br>ter Transformer based architectures for various vision tasks,<br>including low-level image processing [5,57], image classifi-<br>cation [11, 11, 13, 18, 20.23, 31, 53, 54, 58, 60, 64 -66], object<br>detection [3, 73] and semantic segmentation [48, 59, 70].<br>Rather than concentrating on one special task, some recent<br>works [38, 58, 69] try to design a general vision Transformer<br>backbone for general-purpose vision tasks. They all follow<br>the hierarchical Transformer architecture but adopt differ-<br>ent self-attention mechanisms. The main benefit of the hi-<br>erarchical design is to utilize the multi-scale features and<br>reduce the computation complexity by progressively decreas-</p>",
            "id": 19,
            "page": 2,
            "text": "Vision Transformers. Convolutional neural networks\n(CNN) have dominated the computer vision field for many\nyears and achieved tremendous successes [7, 22, 26-28, 35,\n45, 47, 49-51]. Recently, the pioneering work ViT [17]\ndemonstrates that pure Transformer-based architectures can\nalso achieve very competitive results, indicating the potential\nof handling the vision tasks and natural language processing\n(NLP) tasks under a unified framework. Built upon the suc-\ncess of ViT, many efforts have been devoted to designing bet-\nter Transformer based architectures for various vision tasks,\nincluding low-level image processing [5,57], image classifi-\ncation [11, 11, 13, 18, 20.23, 31, 53, 54, 58, 60, 64 -66], object\ndetection [3, 73] and semantic segmentation [48, 59, 70].\nRather than concentrating on one special task, some recent\nworks [38, 58, 69] try to design a general vision Transformer\nbackbone for general-purpose vision tasks. They all follow\nthe hierarchical Transformer architecture but adopt differ-\nent self-attention mechanisms. The main benefit of the hi-\nerarchical design is to utilize the multi-scale features and\nreduce the computation complexity by progressively decreas-"
        },
        {
            "bounding_box": [
                {
                    "x": 239,
                    "y": 283
                },
                {
                    "x": 2232,
                    "y": 283
                },
                {
                    "x": 2232,
                    "y": 747
                },
                {
                    "x": 239,
                    "y": 747
                }
            ],
            "category": "figure",
            "html": "<figure><img id='20' style='font-size:14px' alt=\"H W H W H W H W MLP\nH x W x 3 x C x x 2C x 16 x 4C x x 8C\n4 4 8\n32 32\n8 16\nLN\nToken\nConvolutional\nCSwin CSwin CSwin CSwin\nEmbedding Conv( Conv |Conv\nTransformer Block Transformer Block Transformer Block Transformer Block\nCross-Shaped\n(SW1) (SW2) (sw3) (SW4) Window Self-Attention\nxN1 x N2 x N3 x LN\nStage 1 Stage 2 Stage 3 Stage 4 CSwin Transformer Block\" data-coord=\"top-left:(239,283); bottom-right:(2232,747)\" /></figure>",
            "id": 20,
            "page": 3,
            "text": "H W H W H W H W MLP\nH x W x 3 x C x x 2C x 16 x 4C x x 8C\n4 4 8\n32 32\n8 16\nLN\nToken\nConvolutional\nCSwin CSwin CSwin CSwin\nEmbedding Conv( Conv |Conv\nTransformer Block Transformer Block Transformer Block Transformer Block\nCross-Shaped\n(SW1) (SW2) (sw3) (SW4) Window Self-Attention\nxN1 x N2 x N3 x LN\nStage 1 Stage 2 Stage 3 Stage 4 CSwin Transformer Block"
        },
        {
            "bounding_box": [
                {
                    "x": 268,
                    "y": 768
                },
                {
                    "x": 2209,
                    "y": 768
                },
                {
                    "x": 2209,
                    "y": 815
                },
                {
                    "x": 268,
                    "y": 815
                }
            ],
            "category": "caption",
            "html": "<br><caption id='21' style='font-size:14px'>Figure 2. Left: the overall architecture of our proposed CSWin Transformer, Right: the illustration of CSWin Transformer block.</caption>",
            "id": 21,
            "page": 3,
            "text": "Figure 2. Left: the overall architecture of our proposed CSWin Transformer, Right: the illustration of CSWin Transformer block."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 835
                },
                {
                    "x": 1198,
                    "y": 835
                },
                {
                    "x": 1198,
                    "y": 1032
                },
                {
                    "x": 202,
                    "y": 1032
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='22' style='font-size:18px'>ing the number of tokens. In this paper,we propose a new<br>hierarchical vision Transformer backbone by introducing<br>cross-shaped window self-attention and locally-enhanced<br>positional encoding.</p>",
            "id": 22,
            "page": 3,
            "text": "ing the number of tokens. In this paper,we propose a new\nhierarchical vision Transformer backbone by introducing\ncross-shaped window self-attention and locally-enhanced\npositional encoding."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 1057
                },
                {
                    "x": 1200,
                    "y": 1057
                },
                {
                    "x": 1200,
                    "y": 2106
                },
                {
                    "x": 200,
                    "y": 2106
                }
            ],
            "category": "paragraph",
            "html": "<p id='23' style='font-size:18px'>Efficient Self-attentions. In the NLP field, many efficient<br>attention mechanisms [1, 9, 10,32, 34, 42, 44, 52] have been<br>designed to improve the Transformer efficiency for han-<br>dling long sequences. Since the image resolution is often<br>very high in vision tasks, designing efficient self-attention<br>mechanisms is also very crucial. However, many existing vi-<br>sion Transformers [17, 53, 60, 66] still adopt the original full<br>self-attention, whose computation complexity is quadratic<br>to the image size. To reduce the complexity, the recent<br>vision Transformers [38, 55] adopt the local self-attention<br>mechanism [43] and its shifted/haloed version to add the<br>interaction across different local windows. Besides, axial<br>self-attention [25] and criss-cross attention [30] propose cal-<br>culating attention within stripe windows along horizontal<br>or/and vertical axis. While the performance of axial atten-<br>tion is limited by its sequential mechanism and restricted<br>window size, criss-cross attention is inefficient in practice<br>due to its overlapped window design and ineffective due to<br>its restricted window size. They are the most related works<br>with our CSWin, which could be viewed as a much general<br>and efficient format of these previous works.</p>",
            "id": 23,
            "page": 3,
            "text": "Efficient Self-attentions. In the NLP field, many efficient\nattention mechanisms [1, 9, 10,32, 34, 42, 44, 52] have been\ndesigned to improve the Transformer efficiency for han-\ndling long sequences. Since the image resolution is often\nvery high in vision tasks, designing efficient self-attention\nmechanisms is also very crucial. However, many existing vi-\nsion Transformers [17, 53, 60, 66] still adopt the original full\nself-attention, whose computation complexity is quadratic\nto the image size. To reduce the complexity, the recent\nvision Transformers [38, 55] adopt the local self-attention\nmechanism [43] and its shifted/haloed version to add the\ninteraction across different local windows. Besides, axial\nself-attention [25] and criss-cross attention [30] propose cal-\nculating attention within stripe windows along horizontal\nor/and vertical axis. While the performance of axial atten-\ntion is limited by its sequential mechanism and restricted\nwindow size, criss-cross attention is inefficient in practice\ndue to its overlapped window design and ineffective due to\nits restricted window size. They are the most related works\nwith our CSWin, which could be viewed as a much general\nand efficient format of these previous works."
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 2128
                },
                {
                    "x": 1200,
                    "y": 2128
                },
                {
                    "x": 1200,
                    "y": 2980
                },
                {
                    "x": 199,
                    "y": 2980
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='24' style='font-size:18px'>Positional Encoding. Since self-attention is permutation-<br>invariant and ignores the token positional information, po-<br>sitional encoding is widely used in Transformers to add<br>such positional information back. Typical positional en-<br>coding mechanisms include absolute positional encoding<br>(APE) [56], relative positional encoding (RPE) [38, 46] and<br>conditional positional encoding (CPE) [12]. APE and RPE<br>are often defined as the sinusoidal functions of a series of<br>frequencies or the learnable parameters, which are designed<br>for a specific input size and are not friendly to varying input<br>resolutions. CPE takes the feature as input and can generate<br>the positional encoding for arbitrary input resolutions. Then<br>the generated positional encoding will be added onto the<br>input feature. Our LePE shares a similar spirit as CPE, but<br>proposes to add the positional encoding as a parallel mod-<br>ule to the self-attention operation and operates on projected<br>values in each Transformer block. This design decouples</p>",
            "id": 24,
            "page": 3,
            "text": "Positional Encoding. Since self-attention is permutation-\ninvariant and ignores the token positional information, po-\nsitional encoding is widely used in Transformers to add\nsuch positional information back. Typical positional en-\ncoding mechanisms include absolute positional encoding\n(APE) [56], relative positional encoding (RPE) [38, 46] and\nconditional positional encoding (CPE) [12]. APE and RPE\nare often defined as the sinusoidal functions of a series of\nfrequencies or the learnable parameters, which are designed\nfor a specific input size and are not friendly to varying input\nresolutions. CPE takes the feature as input and can generate\nthe positional encoding for arbitrary input resolutions. Then\nthe generated positional encoding will be added onto the\ninput feature. Our LePE shares a similar spirit as CPE, but\nproposes to add the positional encoding as a parallel mod-\nule to the self-attention operation and operates on projected\nvalues in each Transformer block. This design decouples"
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 837
                },
                {
                    "x": 2274,
                    "y": 837
                },
                {
                    "x": 2274,
                    "y": 932
                },
                {
                    "x": 1280,
                    "y": 932
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='25' style='font-size:18px'>positional encoding from the self-attention calculation, and<br>can enforce stronger local inductive bias.</p>",
            "id": 25,
            "page": 3,
            "text": "positional encoding from the self-attention calculation, and\ncan enforce stronger local inductive bias."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 976
                },
                {
                    "x": 1504,
                    "y": 976
                },
                {
                    "x": 1504,
                    "y": 1024
                },
                {
                    "x": 1281,
                    "y": 1024
                }
            ],
            "category": "paragraph",
            "html": "<p id='26' style='font-size:22px'>3. Method</p>",
            "id": 26,
            "page": 3,
            "text": "3. Method"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 1048
                },
                {
                    "x": 1775,
                    "y": 1048
                },
                {
                    "x": 1775,
                    "y": 1094
                },
                {
                    "x": 1279,
                    "y": 1094
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='27' style='font-size:20px'>3.1. Overall Architecture</p>",
            "id": 27,
            "page": 3,
            "text": "3.1. Overall Architecture"
        },
        {
            "bounding_box": [
                {
                    "x": 1277,
                    "y": 1112
                },
                {
                    "x": 2280,
                    "y": 1112
                },
                {
                    "x": 2280,
                    "y": 2111
                },
                {
                    "x": 1277,
                    "y": 2111
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='28' style='font-size:16px'>The overall architecture of CSWin Transformer is illus-<br>trated in Figure 2. For an input image with size of H x W x3,<br>we follow [60] and leverage the overlapped convolutional<br>token embedding (7 x 7 convolution layer with stride 4) )<br>H W<br>to obtain x patch tokens, and the dimension of each<br>4 4<br>token is C. To produce a hierarchical representation, the<br>whole network consists of four stages. A convolution layer<br>(3 x 3, stride 2) is used between two adjacent stages to re-<br>duce the number of tokens and double the channel dimension.<br>H W<br>Therefore, the constructed feature maps have x<br>2i+1 2i+1<br>tokens for the ith stage, which is similar to traditional CNN<br>backbones like VGG/ResNet. Each stage consists of Ni<br>sequential CSWin Transformer Blocks and maintains the<br>number of tokens. CSWin Transformer Block has the over-<br>all similar topology as the vanilla multi-head self-attention<br>Transformer block with two differences: 1) It replaces the<br>self-attention mechanism with our proposed Cross-Shaped<br>Window Self-Attention; 2) In order to introduce the local<br>inductive bias, LePE is added as a parallel module to the<br>self-attention branch.</p>",
            "id": 28,
            "page": 3,
            "text": "The overall architecture of CSWin Transformer is illus-\ntrated in Figure 2. For an input image with size of H x W x3,\nwe follow [60] and leverage the overlapped convolutional\ntoken embedding (7 x 7 convolution layer with stride 4) )\nH W\nto obtain x patch tokens, and the dimension of each\n4 4\ntoken is C. To produce a hierarchical representation, the\nwhole network consists of four stages. A convolution layer\n(3 x 3, stride 2) is used between two adjacent stages to re-\nduce the number of tokens and double the channel dimension.\nH W\nTherefore, the constructed feature maps have x\n2i+1 2i+1\ntokens for the ith stage, which is similar to traditional CNN\nbackbones like VGG/ResNet. Each stage consists of Ni\nsequential CSWin Transformer Blocks and maintains the\nnumber of tokens. CSWin Transformer Block has the over-\nall similar topology as the vanilla multi-head self-attention\nTransformer block with two differences: 1) It replaces the\nself-attention mechanism with our proposed Cross-Shaped\nWindow Self-Attention; 2) In order to introduce the local\ninductive bias, LePE is added as a parallel module to the\nself-attention branch."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 2150
                },
                {
                    "x": 2101,
                    "y": 2150
                },
                {
                    "x": 2101,
                    "y": 2200
                },
                {
                    "x": 1281,
                    "y": 2200
                }
            ],
            "category": "paragraph",
            "html": "<p id='29' style='font-size:22px'>3.2. Cross-Shaped Window Self-A ttention</p>",
            "id": 29,
            "page": 3,
            "text": "3.2. Cross-Shaped Window Self-A ttention"
        },
        {
            "bounding_box": [
                {
                    "x": 1277,
                    "y": 2228
                },
                {
                    "x": 2280,
                    "y": 2228
                },
                {
                    "x": 2280,
                    "y": 2978
                },
                {
                    "x": 1277,
                    "y": 2978
                }
            ],
            "category": "paragraph",
            "html": "<p id='30' style='font-size:18px'>Despite the strong long-range context modeling capa-<br>bility, the computation complexity of the original full self-<br>attention mechanism is quadratic to feature map size. There-<br>fore, it will suffer from huge computation cost for vision<br>tasks that take high resolution feature maps as input, such<br>as object detection and segmentation. To alleviate this issue,<br>existing works [38, 55] suggest to perform self-attention in a<br>local attention window and apply halo or shifted window to<br>enlarge the receptive filed. However, the token within each<br>Transformer block still has limited attention area and re-<br>quires stacking more blocks to achieve global receptive filed.<br>To enlarge the attention area and achieve global self-attention<br>more efficiently, we present the cross-shaped window self-<br>attention mechanism, which is achieved by performing self-<br>attention in horizontal and vertical stripes in parallel that</p>",
            "id": 30,
            "page": 3,
            "text": "Despite the strong long-range context modeling capa-\nbility, the computation complexity of the original full self-\nattention mechanism is quadratic to feature map size. There-\nfore, it will suffer from huge computation cost for vision\ntasks that take high resolution feature maps as input, such\nas object detection and segmentation. To alleviate this issue,\nexisting works [38, 55] suggest to perform self-attention in a\nlocal attention window and apply halo or shifted window to\nenlarge the receptive filed. However, the token within each\nTransformer block still has limited attention area and re-\nquires stacking more blocks to achieve global receptive filed.\nTo enlarge the attention area and achieve global self-attention\nmore efficiently, we present the cross-shaped window self-\nattention mechanism, which is achieved by performing self-\nattention in horizontal and vertical stripes in parallel that"
        },
        {
            "bounding_box": [
                {
                    "x": 197,
                    "y": 523
                },
                {
                    "x": 2280,
                    "y": 523
                },
                {
                    "x": 2280,
                    "y": 707
                },
                {
                    "x": 197,
                    "y": 707
                }
            ],
            "category": "paragraph",
            "html": "<p id='31' style='font-size:14px'>Figure 3. Comparison among different positional encoding mechanisms: APE and CPE introduce the positional information before feeding<br>into the Transformer blocks, while RPE and our LePE operate in each Transformer block. Different from RPE that adds the positional<br>information into the attention calculation, our LePE operates directly upon V and acts as a parallel module. * Here we only draw the<br>self-attention part to represent the Transformer block for simplicity.</p>",
            "id": 31,
            "page": 4,
            "text": "Figure 3. Comparison among different positional encoding mechanisms: APE and CPE introduce the positional information before feeding\ninto the Transformer blocks, while RPE and our LePE operate in each Transformer block. Different from RPE that adds the positional\ninformation into the attention calculation, our LePE operates directly upon V and acts as a parallel module. * Here we only draw the\nself-attention part to represent the Transformer block for simplicity."
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 749
                },
                {
                    "x": 693,
                    "y": 749
                },
                {
                    "x": 693,
                    "y": 794
                },
                {
                    "x": 204,
                    "y": 794
                }
            ],
            "category": "paragraph",
            "html": "<p id='32' style='font-size:18px'>form a cross-shaped window.</p>",
            "id": 32,
            "page": 4,
            "text": "form a cross-shaped window."
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 800
                },
                {
                    "x": 1200,
                    "y": 800
                },
                {
                    "x": 1200,
                    "y": 1044
                },
                {
                    "x": 203,
                    "y": 1044
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='33' style='font-size:20px'>Horizontal and Vertical Stripes. According to the multi-<br>head self-attention mechanism, the input feature X E<br>R(Hx W)xC will be first linearly projected to K heads, and<br>then each head will perform local self-attention within either<br>the horizontal or vertical stripes.</p>",
            "id": 33,
            "page": 4,
            "text": "Horizontal and Vertical Stripes. According to the multi-\nhead self-attention mechanism, the input feature X E\nR(Hx W)xC will be first linearly projected to K heads, and\nthen each head will perform local self-attention within either\nthe horizontal or vertical stripes."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1051
                },
                {
                    "x": 1202,
                    "y": 1051
                },
                {
                    "x": 1202,
                    "y": 1448
                },
                {
                    "x": 201,
                    "y": 1448
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='34' style='font-size:20px'>For horizontal stripes self-attention, X is evenly parti-<br>tioned into non-overlapping horizontal stripes [X1 , · · , XM]<br>of equal width sw, and each of them contains sw x W to-<br>kens. Here, sw is the stripe width and can be adjusted to<br>balance the learning capacity and computation complexity.<br>Formally, suppose the projected queries, keys and values of<br>the kth head all have dimension dk, then the output of the<br>horizontal stripes self-attention for kth head is defined as:</p>",
            "id": 34,
            "page": 4,
            "text": "For horizontal stripes self-attention, X is evenly parti-\ntioned into non-overlapping horizontal stripes [X1 , · · , XM]\nof equal width sw, and each of them contains sw x W to-\nkens. Here, sw is the stripe width and can be adjusted to\nbalance the learning capacity and computation complexity.\nFormally, suppose the projected queries, keys and values of\nthe kth head all have dimension dk, then the output of the\nhorizontal stripes self-attention for kth head is defined as:"
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 1717
                },
                {
                    "x": 1199,
                    "y": 1717
                },
                {
                    "x": 1199,
                    "y": 2024
                },
                {
                    "x": 200,
                    "y": 2024
                }
            ],
            "category": "paragraph",
            "html": "<p id='35' style='font-size:20px'>Where where Xi E R(swxW)xC and M = H/sw, i =<br>1,. · M. Wk E RCxdk, WK E RCxdk WV E RCxdk<br>,<br>,<br>represent the projection matrices of queries, keys and values<br>for the kth head respectively, and dk is set as C/K. The<br>vertical stripes self-attention can be similarly derived, and<br>its output for kth head is denoted as V-Attentionk (X).</p>",
            "id": 35,
            "page": 4,
            "text": "Where where Xi E R(swxW)xC and M = H/sw, i =\n1,. · M. Wk E RCxdk, WK E RCxdk WV E RCxdk\n,\n,\nrepresent the projection matrices of queries, keys and values\nfor the kth head respectively, and dk is set as C/K. The\nvertical stripes self-attention can be similarly derived, and\nits output for kth head is denoted as V-Attentionk (X)."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2028
                },
                {
                    "x": 1202,
                    "y": 2028
                },
                {
                    "x": 1202,
                    "y": 2345
                },
                {
                    "x": 201,
                    "y": 2345
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='36' style='font-size:20px'>Assuming natural images do not have directional bias,<br>we equally split the K heads into two parallel groups (each<br>has K/2 heads, K is often an even value). The first group<br>of heads perform horizontal stripes self-attention while the<br>second group of heads perform vertical stripes self-attention.<br>Finally the output of these two parallel groups will be con-<br>catenated back together.</p>",
            "id": 36,
            "page": 4,
            "text": "Assuming natural images do not have directional bias,\nwe equally split the K heads into two parallel groups (each\nhas K/2 heads, K is often an even value). The first group\nof heads perform horizontal stripes self-attention while the\nsecond group of heads perform vertical stripes self-attention.\nFinally the output of these two parallel groups will be con-\ncatenated back together."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2574
                },
                {
                    "x": 1200,
                    "y": 2574
                },
                {
                    "x": 1200,
                    "y": 2977
                },
                {
                    "x": 201,
                    "y": 2977
                }
            ],
            "category": "paragraph",
            "html": "<p id='37' style='font-size:20px'>Where WO E RCxC is the commonly used projection<br>matrix that projects the self-attention results into the tar-<br>get output dimension (set as C by default). As described<br>above, one key insight in our self-attention mechanism de-<br>sign is splitting the multi-heads into different groups and<br>applying different self-attention operations accordingly. In<br>other words, the attention area of each token within one<br>Transformer block is enlarged via multi-head grouping. By</p>",
            "id": 37,
            "page": 4,
            "text": "Where WO E RCxC is the commonly used projection\nmatrix that projects the self-attention results into the tar-\nget output dimension (set as C by default). As described\nabove, one key insight in our self-attention mechanism de-\nsign is splitting the multi-heads into different groups and\napplying different self-attention operations accordingly. In\nother words, the attention area of each token within one\nTransformer block is enlarged via multi-head grouping. By"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 750
                },
                {
                    "x": 2278,
                    "y": 750
                },
                {
                    "x": 2278,
                    "y": 1048
                },
                {
                    "x": 1279,
                    "y": 1048
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='38' style='font-size:22px'>contrast, existing self-attention mechanisms apply the same<br>self-attention operations across different multi-heads. In the<br>experiment parts, we will show that this design will bring<br>better performance.<br>Computation Complexity Analysis. The computation<br>complexity of CSWin self-attention is:</p>",
            "id": 38,
            "page": 4,
            "text": "contrast, existing self-attention mechanisms apply the same\nself-attention operations across different multi-heads. In the\nexperiment parts, we will show that this design will bring\nbetter performance.\nComputation Complexity Analysis. The computation\ncomplexity of CSWin self-attention is:"
        },
        {
            "bounding_box": [
                {
                    "x": 1277,
                    "y": 1166
                },
                {
                    "x": 2279,
                    "y": 1166
                },
                {
                    "x": 2279,
                    "y": 2513
                },
                {
                    "x": 1277,
                    "y": 2513
                }
            ],
            "category": "paragraph",
            "html": "<p id='39' style='font-size:20px'>For high-resolution inputs, considering H, W will be<br>larger than C in the early stages and smaller than C in the<br>later stages, we choose small sw for early stages and larger<br>sw for later stages. In other words, adjusting sw provides the<br>flexibility to enlarge the attention area of each token in later<br>stages in an efficient way. Besides, to make the intermediate<br>feature map size divisible by sw for 224 x 224 input, we<br>empirically set sw to 1, 2, 7, 7 for four stages by default.<br>Locally-Enhanced Positional Encoding. Since the self-<br>attention operation is permutation-invariant, it will ignore<br>the important positional information within the 2D image.<br>To add such information back, different positional encoding<br>mechanisms have been utilized in existing vision Transform-<br>ers. In Figure 3, we show some typical positional encoding<br>mechanisms and compare them with our proposed locally-<br>enhanced positional encoding. In details, APE [56] and<br>CPE [12] add the positional information into the input token<br>before feeding into the Transformer blocks, while RPE [46]<br>and our LePE incorporate the positional information within<br>each Transformer block. But different from RPE that adds<br>the positional information within the attention calculation<br>(i.e., Softmax(QKT)), we consider a more straightforward<br>manner and impose the positional information upon the lin-<br>early projected values. Meanwhile, we notice that RPE<br>introduces bias in a per head manner, while our LePE is a<br>per-channel bias, which may show more potential to serve<br>as positional embeddings.</p>",
            "id": 39,
            "page": 4,
            "text": "For high-resolution inputs, considering H, W will be\nlarger than C in the early stages and smaller than C in the\nlater stages, we choose small sw for early stages and larger\nsw for later stages. In other words, adjusting sw provides the\nflexibility to enlarge the attention area of each token in later\nstages in an efficient way. Besides, to make the intermediate\nfeature map size divisible by sw for 224 x 224 input, we\nempirically set sw to 1, 2, 7, 7 for four stages by default.\nLocally-Enhanced Positional Encoding. Since the self-\nattention operation is permutation-invariant, it will ignore\nthe important positional information within the 2D image.\nTo add such information back, different positional encoding\nmechanisms have been utilized in existing vision Transform-\ners. In Figure 3, we show some typical positional encoding\nmechanisms and compare them with our proposed locally-\nenhanced positional encoding. In details, APE [56] and\nCPE [12] add the positional information into the input token\nbefore feeding into the Transformer blocks, while RPE [46]\nand our LePE incorporate the positional information within\neach Transformer block. But different from RPE that adds\nthe positional information within the attention calculation\n(i.e., Softmax(QKT)), we consider a more straightforward\nmanner and impose the positional information upon the lin-\nearly projected values. Meanwhile, we notice that RPE\nintroduces bias in a per head manner, while our LePE is a\nper-channel bias, which may show more potential to serve\nas positional embeddings."
        },
        {
            "bounding_box": [
                {
                    "x": 1277,
                    "y": 2516
                },
                {
                    "x": 2278,
                    "y": 2516
                },
                {
                    "x": 2278,
                    "y": 2714
                },
                {
                    "x": 1277,
                    "y": 2714
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='40' style='font-size:14px'>Mathematically, we denote the input sequence as x<br>(x1, · · · , xn) of n elements, and the output of the attention<br>2 = (21, · · · , zn) of the same length, where Xi, Zi E RC.<br>Self-attention computation could be formulated as:</p>",
            "id": 40,
            "page": 4,
            "text": "Mathematically, we denote the input sequence as x\n(x1, · · · , xn) of n elements, and the output of the attention\n2 = (21, · · · , zn) of the same length, where Xi, Zi E RC.\nSelf-attention computation could be formulated as:"
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 2878
                },
                {
                    "x": 2278,
                    "y": 2878
                },
                {
                    "x": 2278,
                    "y": 2977
                },
                {
                    "x": 1281,
                    "y": 2977
                }
            ],
            "category": "paragraph",
            "html": "<p id='41' style='font-size:18px'>where qi, ki, Vi are the queue, key and value get by a<br>linear transformation of the input Xi and d is the feature</p>",
            "id": 41,
            "page": 4,
            "text": "where qi, ki, Vi are the queue, key and value get by a\nlinear transformation of the input Xi and d is the feature"
        },
        {
            "bounding_box": [
                {
                    "x": 213,
                    "y": 301
                },
                {
                    "x": 1198,
                    "y": 301
                },
                {
                    "x": 1198,
                    "y": 585
                },
                {
                    "x": 213,
                    "y": 585
                }
            ],
            "category": "table",
            "html": "<table id='42' style='font-size:18px'><tr><td>Models</td><td>#Dim</td><td>#Blocks</td><td>sw</td><td>#heads</td><td>#Param.</td><td>FLOPs</td></tr><tr><td>CSWin-T</td><td>64</td><td>1,2,21,1</td><td>1,2,7,7</td><td>2,4,8,16</td><td>23M</td><td>4.3G</td></tr><tr><td>CSWin-S</td><td>64</td><td>2,4,32,21,2,7,7</td><td></td><td>2,4,8,16</td><td>35M</td><td>6.9G</td></tr><tr><td>CSWin-B</td><td>96</td><td>2,4,32,2</td><td>1,2,7,7</td><td>4,8,16,32</td><td>78M</td><td>15.0G</td></tr><tr><td>CSWin-L</td><td>144</td><td>2,4,32,2</td><td>1,2,7,7</td><td>6,12,24,48</td><td>173M</td><td>31.5G</td></tr></table>",
            "id": 42,
            "page": 5,
            "text": "Models #Dim #Blocks sw #heads #Param. FLOPs\n CSWin-T 64 1,2,21,1 1,2,7,7 2,4,8,16 23M 4.3G\n CSWin-S 64 2,4,32,21,2,7,7  2,4,8,16 35M 6.9G\n CSWin-B 96 2,4,32,2 1,2,7,7 4,8,16,32 78M 15.0G\n CSWin-L 144 2,4,32,2 1,2,7,7 6,12,24,48 173M"
        },
        {
            "bounding_box": [
                {
                    "x": 205,
                    "y": 598
                },
                {
                    "x": 1194,
                    "y": 598
                },
                {
                    "x": 1194,
                    "y": 684
                },
                {
                    "x": 205,
                    "y": 684
                }
            ],
            "category": "caption",
            "html": "<br><caption id='43' style='font-size:14px'>Table 1. Detailed configurations of different variants of CSWin<br>Transformer. The FLOPs are calculated with 224 x 224 input.</caption>",
            "id": 43,
            "page": 5,
            "text": "Table 1. Detailed configurations of different variants of CSWin\nTransformer. The FLOPs are calculated with 224 x 224 input."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 707
                },
                {
                    "x": 1197,
                    "y": 707
                },
                {
                    "x": 1197,
                    "y": 852
                },
                {
                    "x": 201,
                    "y": 852
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='44' style='font-size:16px'>dimension. Then our Locally-Enhanced position encoding<br>performs as a learnable per-element bias and Eq.4 could be<br>formulated as:</p>",
            "id": 44,
            "page": 5,
            "text": "dimension. Then our Locally-Enhanced position encoding\nperforms as a learnable per-element bias and Eq.4 could be\nformulated as:"
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 1043
                },
                {
                    "x": 1200,
                    "y": 1043
                },
                {
                    "x": 1200,
                    "y": 1293
                },
                {
                    "x": 200,
                    "y": 1293
                }
            ],
            "category": "paragraph",
            "html": "<p id='45' style='font-size:16px'>where 잘 represents the kth element of vector Zi. To make<br>the LePE suitable to varying input size, we set a distance<br>threshold to the LePE and set it to 0 if the Chebyshev dis-<br>tance of token i and j is greater than a threshold T (T = 3in<br>the default setting).</p>",
            "id": 45,
            "page": 5,
            "text": "where 잘 represents the kth element of vector Zi. To make\nthe LePE suitable to varying input size, we set a distance\nthreshold to the LePE and set it to 0 if the Chebyshev dis-\ntance of token i and j is greater than a threshold T (T = 3in\nthe default setting)."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 1330
                },
                {
                    "x": 816,
                    "y": 1330
                },
                {
                    "x": 816,
                    "y": 1382
                },
                {
                    "x": 202,
                    "y": 1382
                }
            ],
            "category": "paragraph",
            "html": "<p id='46' style='font-size:18px'>3.3. CSWin Transformer Block</p>",
            "id": 46,
            "page": 5,
            "text": "3.3. CSWin Transformer Block"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 1410
                },
                {
                    "x": 1199,
                    "y": 1410
                },
                {
                    "x": 1199,
                    "y": 1558
                },
                {
                    "x": 202,
                    "y": 1558
                }
            ],
            "category": "paragraph",
            "html": "<p id='47' style='font-size:18px'>Equipped with the above self-attention mechanism and<br>positional embedding mechanism, CSWin Transformer<br>block is formally defined as:</p>",
            "id": 47,
            "page": 5,
            "text": "Equipped with the above self-attention mechanism and\npositional embedding mechanism, CSWin Transformer\nblock is formally defined as:"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 1797
                },
                {
                    "x": 1200,
                    "y": 1797
                },
                {
                    "x": 1200,
                    "y": 1899
                },
                {
                    "x": 202,
                    "y": 1899
                }
            ],
            "category": "paragraph",
            "html": "<p id='48' style='font-size:16px'>where Xl denotes the output of l-th Transformer block or<br>the precedent convolutional layer of each stage.</p>",
            "id": 48,
            "page": 5,
            "text": "where Xl denotes the output of l-th Transformer block or\nthe precedent convolutional layer of each stage."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1934
                },
                {
                    "x": 716,
                    "y": 1934
                },
                {
                    "x": 716,
                    "y": 1985
                },
                {
                    "x": 201,
                    "y": 1985
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='49' style='font-size:20px'>3.4. Architecture Variants</p>",
            "id": 49,
            "page": 5,
            "text": "3.4. Architecture Variants"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2017
                },
                {
                    "x": 1201,
                    "y": 2017
                },
                {
                    "x": 1201,
                    "y": 2465
                },
                {
                    "x": 201,
                    "y": 2465
                }
            ],
            "category": "paragraph",
            "html": "<p id='50' style='font-size:16px'>For a fair comparison with other vision Transformers<br>under similar settings, we build four different variants of<br>CSWin Transformer as shown in Table 1: CSWin-T (Tiny),<br>CSWin-S (Small), CSWin-B (Base), CSWin-L (Large). They<br>are designed by changing the base channel dimension C and<br>the block number of each stage. In all these variants, the<br>expansion ratio of each MLP is set as 4. The head number of<br>the four stages is set as 2, 4, 8, 16 in the first three variants<br>and 6, 12, 24, 48 in the last variant respectively.</p>",
            "id": 50,
            "page": 5,
            "text": "For a fair comparison with other vision Transformers\nunder similar settings, we build four different variants of\nCSWin Transformer as shown in Table 1: CSWin-T (Tiny),\nCSWin-S (Small), CSWin-B (Base), CSWin-L (Large). They\nare designed by changing the base channel dimension C and\nthe block number of each stage. In all these variants, the\nexpansion ratio of each MLP is set as 4. The head number of\nthe four stages is set as 2, 4, 8, 16 in the first three variants\nand 6, 12, 24, 48 in the last variant respectively."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2502
                },
                {
                    "x": 533,
                    "y": 2502
                },
                {
                    "x": 533,
                    "y": 2555
                },
                {
                    "x": 201,
                    "y": 2555
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='51' style='font-size:22px'>4. Experiments</p>",
            "id": 51,
            "page": 5,
            "text": "4. Experiments"
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 2579
                },
                {
                    "x": 1201,
                    "y": 2579
                },
                {
                    "x": 1201,
                    "y": 2977
                },
                {
                    "x": 200,
                    "y": 2977
                }
            ],
            "category": "paragraph",
            "html": "<p id='52' style='font-size:16px'>To show the effectiveness of CSWin Transformer as a gen-<br>eral vision backbone, we conduct experiments on ImageNet-<br>1K [16] classification, COCO [37] object detection, and<br>ADE20K [72] semantic segmentation. We also perform<br>comprehensive ablation studies to analyze each component<br>of CSWin Transformer. As most of the methods we com-<br>pared did not report downstream inference speed, we use an<br>extra section to report it for simplicity.</p>",
            "id": 52,
            "page": 5,
            "text": "To show the effectiveness of CSWin Transformer as a gen-\neral vision backbone, we conduct experiments on ImageNet-\n1K [16] classification, COCO [37] object detection, and\nADE20K [72] semantic segmentation. We also perform\ncomprehensive ablation studies to analyze each component\nof CSWin Transformer. As most of the methods we com-\npared did not report downstream inference speed, we use an\nextra section to report it for simplicity."
        },
        {
            "bounding_box": [
                {
                    "x": 1299,
                    "y": 295
                },
                {
                    "x": 2266,
                    "y": 295
                },
                {
                    "x": 2266,
                    "y": 1645
                },
                {
                    "x": 1299,
                    "y": 1645
                }
            ],
            "category": "table",
            "html": "<br><table id='53' style='font-size:14px'><tr><td>Method</td><td>Image Size</td><td>#Param.</td><td>FLOPs</td><td>Throughput</td><td>Top-1</td></tr><tr><td>Eff-B4 [51]</td><td>3802</td><td>19M</td><td>4.2G</td><td>349/s</td><td>82.9</td></tr><tr><td>Eff-B5 [51]</td><td>4562</td><td>30M</td><td>9.9G</td><td>169/s</td><td>83.6</td></tr><tr><td>Eff-B6 [51]</td><td>5282</td><td>43M</td><td>19.0G</td><td>96/s</td><td>84.0</td></tr><tr><td>DeiT-S [53]</td><td>2242</td><td>22M</td><td>4.6G</td><td>940/s</td><td>79.8</td></tr><tr><td>DeiT-B [53]</td><td>2242</td><td>87M</td><td>17.5G</td><td>292/s</td><td>81.8</td></tr><tr><td>DeiT-B [53]</td><td>3842</td><td>86M</td><td>55.4G</td><td>85/s</td><td>83.1</td></tr><tr><td>PVT-S [58]</td><td>2242</td><td>25M</td><td>3.8G</td><td>820/s</td><td>79.8</td></tr><tr><td>PVT-M [58]</td><td>2242</td><td>44M</td><td>6.7G</td><td>526/s</td><td>81.2</td></tr><tr><td>PVT-L [58]</td><td>2242</td><td>61M</td><td>9.8G</td><td>367/s</td><td>81.7</td></tr><tr><td>T2Tt-14 [66]</td><td>2242</td><td>22M</td><td>6.1G</td><td>-</td><td>81.7</td></tr><tr><td>T2Tt-19 [66]</td><td>2242</td><td>39M</td><td>9.8G</td><td>-</td><td>82.2</td></tr><tr><td>T2Tt-24 [66]</td><td>2242</td><td>64M</td><td>15.0G</td><td></td><td>82.6</td></tr><tr><td>CvT-13 [60]</td><td>2242</td><td>20M</td><td>4.5G</td><td>-</td><td>81.6</td></tr><tr><td>CvT-21 [60]</td><td>2242</td><td>32M</td><td>7.1G</td><td>-</td><td>82.5</td></tr><tr><td>CvT-21 [60]</td><td>3842</td><td>32M</td><td>24.9G</td><td>-</td><td>83.3</td></tr><tr><td>Swin-T [38]</td><td>2242</td><td>29M</td><td>4.5G</td><td>755/s</td><td>81.3</td></tr><tr><td>Swin-S [38]</td><td>2242</td><td>50M</td><td>8.7G</td><td>437/s</td><td>83.0</td></tr><tr><td>Swin-B [38]</td><td>2242</td><td>88M</td><td>15.4G</td><td>278/s</td><td>83.3</td></tr><tr><td>Swin-B [38]</td><td>3842</td><td>88M</td><td>47.0G</td><td>85/s</td><td>84.2</td></tr><tr><td>CSWin-T</td><td>2242</td><td>23M</td><td>4.3G</td><td>701/s</td><td>82.7</td></tr><tr><td>CSWin-S</td><td>2242</td><td>35M</td><td>6.9G</td><td>437/s</td><td>83.6</td></tr><tr><td>CSWin-B</td><td>2242</td><td>78M</td><td>15.0G</td><td>250/s</td><td>84.2</td></tr><tr><td>CSWin-B</td><td>3842</td><td>78M</td><td>47.0G</td><td></td><td>85.4</td></tr></table>",
            "id": 53,
            "page": 5,
            "text": "Method Image Size #Param. FLOPs Throughput Top-1\n Eff-B4 [51] 3802 19M 4.2G 349/s 82.9\n Eff-B5 [51] 4562 30M 9.9G 169/s 83.6\n Eff-B6 [51] 5282 43M 19.0G 96/s 84.0\n DeiT-S [53] 2242 22M 4.6G 940/s 79.8\n DeiT-B [53] 2242 87M 17.5G 292/s 81.8\n DeiT-B [53] 3842 86M 55.4G 85/s 83.1\n PVT-S [58] 2242 25M 3.8G 820/s 79.8\n PVT-M [58] 2242 44M 6.7G 526/s 81.2\n PVT-L [58] 2242 61M 9.8G 367/s 81.7\n T2Tt-14 [66] 2242 22M 6.1G - 81.7\n T2Tt-19 [66] 2242 39M 9.8G - 82.2\n T2Tt-24 [66] 2242 64M 15.0G  82.6\n CvT-13 [60] 2242 20M 4.5G - 81.6\n CvT-21 [60] 2242 32M 7.1G - 82.5\n CvT-21 [60] 3842 32M 24.9G - 83.3\n Swin-T [38] 2242 29M 4.5G 755/s 81.3\n Swin-S [38] 2242 50M 8.7G 437/s 83.0\n Swin-B [38] 2242 88M 15.4G 278/s 83.3\n Swin-B [38] 3842 88M 47.0G 85/s 84.2\n CSWin-T 2242 23M 4.3G 701/s 82.7\n CSWin-S 2242 35M 6.9G 437/s 83.6\n CSWin-B 2242 78M 15.0G 250/s 84.2\n CSWin-B 3842 78M 47.0G"
        },
        {
            "bounding_box": [
                {
                    "x": 1334,
                    "y": 1645
                },
                {
                    "x": 2212,
                    "y": 1645
                },
                {
                    "x": 2212,
                    "y": 1690
                },
                {
                    "x": 1334,
                    "y": 1690
                }
            ],
            "category": "caption",
            "html": "<br><caption id='54' style='font-size:14px'>Table 2. Comparison of different models on ImageNet-1K.</caption>",
            "id": 54,
            "page": 5,
            "text": "Table 2. Comparison of different models on ImageNet-1K."
        },
        {
            "bounding_box": [
                {
                    "x": 1299,
                    "y": 1719
                },
                {
                    "x": 2317,
                    "y": 1719
                },
                {
                    "x": 2317,
                    "y": 2133
                },
                {
                    "x": 1299,
                    "y": 2133
                }
            ],
            "category": "table",
            "html": "<table id='55' style='font-size:14px'><tr><td>Method</td><td>Param</td><td>Size FLOPs Top-1|</td><td></td><td>Method</td><td>Param Size</td><td></td><td>FLOPs</td><td>Top-1</td></tr><tr><td>R-101x3</td><td>388M</td><td>3842 204.6G</td><td>84.4</td><td>R-152x4</td><td>937M</td><td>4802</td><td>840.5G</td><td>85.4</td></tr><tr><td>ViT-B/16</td><td>86M</td><td>3842 55.4G</td><td>84.0</td><td>ViT-L/16</td><td>307M</td><td>3842</td><td>190.7G</td><td>85.2</td></tr><tr><td>Swin-B</td><td>88M</td><td>2242 15.4G 3842 47.1G</td><td>85.2 86.4</td><td>Swin-L</td><td>197M</td><td>2242 3842</td><td>34.5G 103.9G</td><td>86.3 87.3</td></tr><tr><td>CSWin-B</td><td>78M</td><td>2242 15.0G 3842 47.0G</td><td>85.9 87.0</td><td>CSWin-L</td><td>173M</td><td>2242 3842</td><td>31.5G 96.8G</td><td>86.5 87.5</td></tr></table>",
            "id": 55,
            "page": 5,
            "text": "Method Param Size FLOPs Top-1|  Method Param Size  FLOPs Top-1\n R-101x3 388M 3842 204.6G 84.4 R-152x4 937M 4802 840.5G 85.4\n ViT-B/16 86M 3842 55.4G 84.0 ViT-L/16 307M 3842 190.7G 85.2\n Swin-B 88M 2242 15.4G 3842 47.1G 85.2 86.4 Swin-L 197M 2242 3842 34.5G 103.9G 86.3 87.3\n CSWin-B 78M 2242 15.0G 3842 47.0G 85.9 87.0 CSWin-L 173M 2242 3842 31.5G 96.8G"
        },
        {
            "bounding_box": [
                {
                    "x": 1284,
                    "y": 2140
                },
                {
                    "x": 2276,
                    "y": 2140
                },
                {
                    "x": 2276,
                    "y": 2229
                },
                {
                    "x": 1284,
                    "y": 2229
                }
            ],
            "category": "caption",
            "html": "<br><caption id='56' style='font-size:16px'>Table 3. ImageNet-1K fine-tuning results by pre-training on<br>ImageNet-21K datasets.</caption>",
            "id": 56,
            "page": 5,
            "text": "Table 3. ImageNet-1K fine-tuning results by pre-training on\nImageNet-21K datasets."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 2294
                },
                {
                    "x": 1906,
                    "y": 2294
                },
                {
                    "x": 1906,
                    "y": 2344
                },
                {
                    "x": 1281,
                    "y": 2344
                }
            ],
            "category": "paragraph",
            "html": "<p id='57' style='font-size:20px'>4.1. ImageNet-1K Classification</p>",
            "id": 57,
            "page": 5,
            "text": "4.1. ImageNet-1K Classification"
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 2378
                },
                {
                    "x": 2279,
                    "y": 2378
                },
                {
                    "x": 2279,
                    "y": 2978
                },
                {
                    "x": 1278,
                    "y": 2978
                }
            ],
            "category": "paragraph",
            "html": "<p id='58' style='font-size:16px'>For fair comparison, we follow the training strategy<br>in DeiT [53] as other baseline Transformer architectures<br>[38, 60]. Specifically, all our models are trained for 300<br>epochs with the input size of 224 x 224. We use the AdamW<br>optimizer with weight decay of 0.05 for CSWin-T/S and<br>0.1 for CSWin-B. The default batch size and initial learning<br>rate are set to 1024 and 0.001, and the cosine learning rate<br>scheduler with 20 epochs linear warm-up is used. We apply<br>increasing stochastic depth [29] augmentation for CSWin-T,<br>CSWin-S, and CSWin-B with the maximum rate as 0.1, 0.3,<br>0.5 respectively. When reporting the results of 384 x 384<br>input, we fine-tune the models for 30 epochs with the weight</p>",
            "id": 58,
            "page": 5,
            "text": "For fair comparison, we follow the training strategy\nin DeiT [53] as other baseline Transformer architectures\n[38, 60]. Specifically, all our models are trained for 300\nepochs with the input size of 224 x 224. We use the AdamW\noptimizer with weight decay of 0.05 for CSWin-T/S and\n0.1 for CSWin-B. The default batch size and initial learning\nrate are set to 1024 and 0.001, and the cosine learning rate\nscheduler with 20 epochs linear warm-up is used. We apply\nincreasing stochastic depth [29] augmentation for CSWin-T,\nCSWin-S, and CSWin-B with the maximum rate as 0.1, 0.3,\n0.5 respectively. When reporting the results of 384 x 384\ninput, we fine-tune the models for 30 epochs with the weight"
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 293
                },
                {
                    "x": 2267,
                    "y": 293
                },
                {
                    "x": 2267,
                    "y": 1279
                },
                {
                    "x": 222,
                    "y": 1279
                }
            ],
            "category": "table",
            "html": "<table id='59' style='font-size:14px'><tr><td rowspan=\"2\">Backbone</td><td rowspan=\"2\">#Params (M)</td><td rowspan=\"2\">FLOPs (G)</td><td colspan=\"6\">Mask R-CNN 1x schedule</td><td colspan=\"6\">Mask R-CNN 3x + MS schedule</td></tr><tr><td>APb</td><td>AP50</td><td>AP75</td><td>APm</td><td>APm</td><td>APm</td><td>APb</td><td>AP50</td><td>AP75</td><td>APm</td><td>APm</td><td>APm</td></tr><tr><td>Res50 [22]</td><td>44</td><td>260</td><td>38.0</td><td>58.6</td><td>41.4</td><td>34.4</td><td>55.1</td><td>36.7</td><td>41.0</td><td>61.7</td><td>44.9</td><td>37.1</td><td>58.4</td><td>40.1</td></tr><tr><td>PVT-S [58]</td><td>44</td><td>245</td><td>40.4</td><td>62.9</td><td>43.8</td><td>37.8</td><td>60.1</td><td>40.3</td><td>43.0</td><td>65.3</td><td>46.9</td><td>39.9</td><td>62.5</td><td>42.8</td></tr><tr><td>ViL-S [69]</td><td>45</td><td>218</td><td>44.9</td><td>67.1</td><td>49.3</td><td>41.0</td><td>64.2</td><td>44.1</td><td>47.1</td><td>68.7</td><td>51.5</td><td>42.7</td><td>65.9</td><td>46.2</td></tr><tr><td>TwinsP-S [11]</td><td>44</td><td>245</td><td>42.9</td><td>65.8</td><td>47.1</td><td>40.0</td><td>62.7</td><td>42.9</td><td>46.8</td><td>69.3</td><td>51.8</td><td>42.6</td><td>66.3</td><td>46.0</td></tr><tr><td>Twins-S [11]</td><td>44</td><td>228</td><td>43.4</td><td>66.0</td><td>47.3</td><td>40.3</td><td>63.2</td><td>43.4</td><td>46.8</td><td>69.2</td><td>51.2</td><td>42.6</td><td>66.3</td><td>45.8</td></tr><tr><td>Swin-T [38]</td><td>48</td><td>264</td><td>42.2</td><td>64.6</td><td>46.2</td><td>39.1</td><td>61.6</td><td>42.0</td><td>46.0</td><td>68.2</td><td>50.2</td><td>41.6</td><td>65.1</td><td>44.8</td></tr><tr><td>CSWin-T</td><td>42</td><td>279</td><td>46.7</td><td>68.6</td><td>51.3</td><td>42.2</td><td>65.6</td><td>45.4</td><td>49.0</td><td>70.7</td><td>53.7</td><td>43.6</td><td>67.9</td><td>46.6</td></tr><tr><td>Res101 [22]</td><td>63</td><td>336</td><td>40.4</td><td>61.1</td><td>44.2</td><td>36.4</td><td>57.7</td><td>38.8</td><td>42.8</td><td>63.2</td><td>47.1</td><td>38.5</td><td>60.1</td><td>41.3</td></tr><tr><td>X101-32 [63]</td><td>63</td><td>340</td><td>41.9</td><td>62.5</td><td>45.9</td><td>37.5</td><td>59.4</td><td>40.2</td><td>44.0</td><td>64.4</td><td>48.0</td><td>39.2</td><td>61.4</td><td>41.9</td></tr><tr><td>PVT-M [58]</td><td>64</td><td>302</td><td>42.0</td><td>64.4</td><td>45.6</td><td>39.0</td><td>61.6</td><td>42.1</td><td>44.2</td><td>66.0</td><td>48.2</td><td>40.5</td><td>63.1</td><td>43.5</td></tr><tr><td>ViL-M [69]</td><td>60</td><td>261</td><td>43.4</td><td></td><td>-</td><td>39.7</td><td></td><td>T</td><td>44.6</td><td>66.3</td><td>48.5</td><td>40.7</td><td>63.8</td><td>43.7</td></tr><tr><td>TwinsP-B [11]</td><td>64</td><td>302</td><td>44.6</td><td>66.7</td><td>48.9</td><td>40.9</td><td>63.8</td><td>44.2</td><td>47.9</td><td>70.1</td><td>52.5</td><td>43.2</td><td>67.2</td><td>46.3</td></tr><tr><td>Twins-B [11]</td><td>76</td><td>340</td><td>45.2</td><td>67.6</td><td>49.3</td><td>41.5</td><td>64.5</td><td>44.8</td><td>48.0</td><td>69.5</td><td>52.7</td><td>43.0</td><td>66.8</td><td>46.6</td></tr><tr><td>Swin-S [38]</td><td>69</td><td>354</td><td>44.8</td><td>66.6</td><td>48.9</td><td>40.9</td><td>63.4</td><td>44.2</td><td>48.5</td><td>70.2</td><td>53.5</td><td>43.3</td><td>67.3</td><td>46.6</td></tr><tr><td>CSWin-S</td><td>54</td><td>342</td><td>47.9</td><td>70.1</td><td>52.6</td><td>43.2</td><td>67.1</td><td>46.2</td><td>50.0</td><td>71.3</td><td>54.7</td><td>44.5</td><td>68.4</td><td>47.7</td></tr><tr><td>X101-64 [63]</td><td>101</td><td>493</td><td>42.8</td><td>63.8</td><td>47.3</td><td>38.4</td><td>60.6</td><td>41.3</td><td>44.4</td><td>64.9</td><td>48.8</td><td>39.7</td><td>61.9</td><td>42.6</td></tr><tr><td>PVT-L [58]</td><td>81</td><td>364</td><td>42.9</td><td>65.0</td><td>46.6</td><td>39.5</td><td>61.9</td><td>42.5</td><td>44.5</td><td>66.0</td><td>48.3</td><td>40.7</td><td>63.4</td><td>43.7</td></tr><tr><td>ViL-B [69]</td><td>76</td><td>365</td><td>45.1</td><td></td><td></td><td>41.0</td><td></td><td></td><td>45.7</td><td>67.2</td><td>49.9</td><td>41.3</td><td>64.4</td><td>44.5</td></tr><tr><td>TwinsP-L [11]</td><td>81</td><td>364</td><td>45.4</td><td></td><td></td><td>41.5</td><td></td><td></td><td>-</td><td></td><td>-</td><td>-</td><td>-</td><td></td></tr><tr><td>Twins-L [11]</td><td>111</td><td>474</td><td>45.9</td><td></td><td></td><td>41.6</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Swin-B [38]</td><td>107</td><td>496</td><td>46.9</td><td></td><td></td><td>42.3</td><td></td><td></td><td>48.5</td><td>69.8</td><td>53.2</td><td>43.4</td><td>66.8</td><td>46.9</td></tr><tr><td>CSWin-B</td><td>97</td><td>526</td><td>48.7</td><td>70.4</td><td>53.9</td><td>43.9</td><td>67.8</td><td>47.3</td><td>50.8</td><td>72.1</td><td>55.8</td><td>44.9</td><td>69.1</td><td>48.3</td></tr></table>",
            "id": 59,
            "page": 6,
            "text": "Backbone #Params (M) FLOPs (G) Mask R-CNN 1x schedule Mask R-CNN 3x + MS schedule\n APb AP50 AP75 APm APm APm APb AP50 AP75 APm APm APm\n Res50 [22] 44 260 38.0 58.6 41.4 34.4 55.1 36.7 41.0 61.7 44.9 37.1 58.4 40.1\n PVT-S [58] 44 245 40.4 62.9 43.8 37.8 60.1 40.3 43.0 65.3 46.9 39.9 62.5 42.8\n ViL-S [69] 45 218 44.9 67.1 49.3 41.0 64.2 44.1 47.1 68.7 51.5 42.7 65.9 46.2\n TwinsP-S [11] 44 245 42.9 65.8 47.1 40.0 62.7 42.9 46.8 69.3 51.8 42.6 66.3 46.0\n Twins-S [11] 44 228 43.4 66.0 47.3 40.3 63.2 43.4 46.8 69.2 51.2 42.6 66.3 45.8\n Swin-T [38] 48 264 42.2 64.6 46.2 39.1 61.6 42.0 46.0 68.2 50.2 41.6 65.1 44.8\n CSWin-T 42 279 46.7 68.6 51.3 42.2 65.6 45.4 49.0 70.7 53.7 43.6 67.9 46.6\n Res101 [22] 63 336 40.4 61.1 44.2 36.4 57.7 38.8 42.8 63.2 47.1 38.5 60.1 41.3\n X101-32 [63] 63 340 41.9 62.5 45.9 37.5 59.4 40.2 44.0 64.4 48.0 39.2 61.4 41.9\n PVT-M [58] 64 302 42.0 64.4 45.6 39.0 61.6 42.1 44.2 66.0 48.2 40.5 63.1 43.5\n ViL-M [69] 60 261 43.4  - 39.7  T 44.6 66.3 48.5 40.7 63.8 43.7\n TwinsP-B [11] 64 302 44.6 66.7 48.9 40.9 63.8 44.2 47.9 70.1 52.5 43.2 67.2 46.3\n Twins-B [11] 76 340 45.2 67.6 49.3 41.5 64.5 44.8 48.0 69.5 52.7 43.0 66.8 46.6\n Swin-S [38] 69 354 44.8 66.6 48.9 40.9 63.4 44.2 48.5 70.2 53.5 43.3 67.3 46.6\n CSWin-S 54 342 47.9 70.1 52.6 43.2 67.1 46.2 50.0 71.3 54.7 44.5 68.4 47.7\n X101-64 [63] 101 493 42.8 63.8 47.3 38.4 60.6 41.3 44.4 64.9 48.8 39.7 61.9 42.6\n PVT-L [58] 81 364 42.9 65.0 46.6 39.5 61.9 42.5 44.5 66.0 48.3 40.7 63.4 43.7\n ViL-B [69] 76 365 45.1   41.0   45.7 67.2 49.9 41.3 64.4 44.5\n TwinsP-L [11] 81 364 45.4   41.5   -  - - - \n Twins-L [11] 111 474 45.9   41.6        \n Swin-B [38] 107 496 46.9   42.3   48.5 69.8 53.2 43.4 66.8 46.9\n CSWin-B 97 526 48.7 70.4 53.9 43.9 67.8 47.3 50.8 72.1 55.8 44.9 69.1"
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 1284
                },
                {
                    "x": 2278,
                    "y": 1284
                },
                {
                    "x": 2278,
                    "y": 1378
                },
                {
                    "x": 203,
                    "y": 1378
                }
            ],
            "category": "caption",
            "html": "<br><caption id='60' style='font-size:16px'>Table 4. Object detection and instance segmentation performance on the COCO val2017 with the Mask R-CNN framework. The FLOPs (G)<br>are measured at resolution 800 x 1280, and the models are pre-trained on the ImageNet-1K. ResNet/ResNeXt results are copied from [58].</caption>",
            "id": 60,
            "page": 6,
            "text": "Table 4. Object detection and instance segmentation performance on the COCO val2017 with the Mask R-CNN framework. The FLOPs (G)\nare measured at resolution 800 x 1280, and the models are pre-trained on the ImageNet-1K. ResNet/ResNeXt results are copied from [58]."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 1420
                },
                {
                    "x": 1108,
                    "y": 1420
                },
                {
                    "x": 1108,
                    "y": 1465
                },
                {
                    "x": 202,
                    "y": 1465
                }
            ],
            "category": "paragraph",
            "html": "<p id='61' style='font-size:20px'>decay of 1e-8, learning rate of 1e-5, batch size of 512.</p>",
            "id": 61,
            "page": 6,
            "text": "decay of 1e-8, learning rate of 1e-5, batch size of 512."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 1474
                },
                {
                    "x": 1201,
                    "y": 1474
                },
                {
                    "x": 1201,
                    "y": 1719
                },
                {
                    "x": 202,
                    "y": 1719
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='62' style='font-size:18px'>In Table 11, we compare our CSWin Transformer with<br>state-of-the-art CNN and Transformer architectures. With<br>the limitation of pages, we only compare with a few classical<br>methods here and make a comprehensive comparison in the<br>supplemental materials.</p>",
            "id": 62,
            "page": 6,
            "text": "In Table 11, we compare our CSWin Transformer with\nstate-of-the-art CNN and Transformer architectures. With\nthe limitation of pages, we only compare with a few classical\nmethods here and make a comprehensive comparison in the\nsupplemental materials."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1725
                },
                {
                    "x": 1202,
                    "y": 1725
                },
                {
                    "x": 1202,
                    "y": 2171
                },
                {
                    "x": 201,
                    "y": 2171
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='63' style='font-size:20px'>It shows that our CSWin Transformers outperform pre-<br>vious state-of-the-art vision Transformers by large margins.<br>For example, CSWin-T achieves 82.7% Top-1 accuracy with<br>only 4.3G FLOPs, surpassing CvT-13, Swin-T and DeiT-S<br>by 1.1%, 1.4% and 2.9% respectively. And for the small and<br>base model setting, our CSWin-S and CSWin-B also achieve<br>the best performance. When finetuned on the 384 x 384<br>input, a similar trend is observed, which well demonstrates<br>the powerful learning capacity of our CSWin Transformers.</p>",
            "id": 63,
            "page": 6,
            "text": "It shows that our CSWin Transformers outperform pre-\nvious state-of-the-art vision Transformers by large margins.\nFor example, CSWin-T achieves 82.7% Top-1 accuracy with\nonly 4.3G FLOPs, surpassing CvT-13, Swin-T and DeiT-S\nby 1.1%, 1.4% and 2.9% respectively. And for the small and\nbase model setting, our CSWin-S and CSWin-B also achieve\nthe best performance. When finetuned on the 384 x 384\ninput, a similar trend is observed, which well demonstrates\nthe powerful learning capacity of our CSWin Transformers."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2173
                },
                {
                    "x": 1202,
                    "y": 2173
                },
                {
                    "x": 1202,
                    "y": 2522
                },
                {
                    "x": 201,
                    "y": 2522
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='64' style='font-size:18px'>Compared with state-of-the-art CNNs, we find our CSWin<br>Transformer is the only Transformer based architecture that<br>achieves comparable or even better results than Efficient-<br>Net [51] under the small and base settings, while using less<br>computation complexity · It is also worth noting that neu-<br>ral architecture search is used in EfficientNet but not in our<br>CSWin Transformer design.</p>",
            "id": 64,
            "page": 6,
            "text": "Compared with state-of-the-art CNNs, we find our CSWin\nTransformer is the only Transformer based architecture that\nachieves comparable or even better results than Efficient-\nNet [51] under the small and base settings, while using less\ncomputation complexity · It is also worth noting that neu-\nral architecture search is used in EfficientNet but not in our\nCSWin Transformer design."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2527
                },
                {
                    "x": 1202,
                    "y": 2527
                },
                {
                    "x": 1202,
                    "y": 2978
                },
                {
                    "x": 201,
                    "y": 2978
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='65' style='font-size:20px'>We further pre-train CSWin Transformer on ImageNet-<br>21K dataset, which contains 14.2M images and 21K classes.<br>Models are trained for 90 epochs with the input size of<br>224 x 224. We use the AdamW optimizer with weight decay<br>of 0.1 for CSWin-B and 0.2 for CSWin-L, and the default<br>batch size and initial learning rate are set to 2048 and 0.001.<br>When fine-tuning on ImageNet-1K, we train the models for<br>30 epochs with the weight decay of 1e-8, learning rate of<br>1e-5, batch size of 512. The increasing stochastic depth [29]</p>",
            "id": 65,
            "page": 6,
            "text": "We further pre-train CSWin Transformer on ImageNet-\n21K dataset, which contains 14.2M images and 21K classes.\nModels are trained for 90 epochs with the input size of\n224 x 224. We use the AdamW optimizer with weight decay\nof 0.1 for CSWin-B and 0.2 for CSWin-L, and the default\nbatch size and initial learning rate are set to 2048 and 0.001.\nWhen fine-tuning on ImageNet-1K, we train the models for\n30 epochs with the weight decay of 1e-8, learning rate of\n1e-5, batch size of 512. The increasing stochastic depth [29]"
        },
        {
            "bounding_box": [
                {
                    "x": 1294,
                    "y": 1415
                },
                {
                    "x": 2267,
                    "y": 1415
                },
                {
                    "x": 2267,
                    "y": 1903
                },
                {
                    "x": 1294,
                    "y": 1903
                }
            ],
            "category": "table",
            "html": "<br><table id='66' style='font-size:14px'><tr><td rowspan=\"2\">Backbone</td><td rowspan=\"2\">#Params (M)</td><td rowspan=\"2\">FLOPs (G)</td><td colspan=\"6\">Cascade Mask R-CNN 3x +MS</td></tr><tr><td>APb</td><td>AP50</td><td>AP75</td><td>APm</td><td>APm</td><td>APm</td></tr><tr><td>Res50 [22]</td><td>82</td><td>739</td><td>46.3</td><td>64.3</td><td>50.5</td><td>40.1</td><td>61.7</td><td>43.4</td></tr><tr><td>Swin-T [38]</td><td>86</td><td>745</td><td>50.5</td><td>69.3</td><td>54.9</td><td>43.7</td><td>66.6</td><td>47.1</td></tr><tr><td>CSWin-T</td><td>80</td><td>757</td><td>52.5</td><td>71.5</td><td>57.1</td><td>45.3</td><td>68.8</td><td>48.9</td></tr><tr><td>X101-32 [63]</td><td>101</td><td>819</td><td>48.1</td><td>66.5</td><td>52.4</td><td>41.6</td><td>63.9</td><td>45.2</td></tr><tr><td>Swin-S [38]</td><td>107</td><td>838</td><td>51.8</td><td>70.4</td><td>56.3</td><td>44.7</td><td>67.9</td><td>48.5</td></tr><tr><td>CSWin-S</td><td>92</td><td>820</td><td>53.7</td><td>72.2</td><td>58.4</td><td>46.4</td><td>69.6</td><td>50.6</td></tr><tr><td>X101-64 [63]</td><td>140</td><td>972</td><td>48.3</td><td>66.4</td><td>52.3</td><td>41.7</td><td>64.0</td><td>45.1</td></tr><tr><td>Swin-B [38]</td><td>145</td><td>982</td><td>51.9</td><td>70.9</td><td>56.5</td><td>45.0</td><td>68.4</td><td>48.7</td></tr><tr><td>CSWin-B</td><td>135</td><td>1004</td><td>53.9</td><td>72.6</td><td>58.5</td><td>46.4</td><td>70.0</td><td>50.4</td></tr></table>",
            "id": 66,
            "page": 6,
            "text": "Backbone #Params (M) FLOPs (G) Cascade Mask R-CNN 3x +MS\n APb AP50 AP75 APm APm APm\n Res50 [22] 82 739 46.3 64.3 50.5 40.1 61.7 43.4\n Swin-T [38] 86 745 50.5 69.3 54.9 43.7 66.6 47.1\n CSWin-T 80 757 52.5 71.5 57.1 45.3 68.8 48.9\n X101-32 [63] 101 819 48.1 66.5 52.4 41.6 63.9 45.2\n Swin-S [38] 107 838 51.8 70.4 56.3 44.7 67.9 48.5\n CSWin-S 92 820 53.7 72.2 58.4 46.4 69.6 50.6\n X101-64 [63] 140 972 48.3 66.4 52.3 41.7 64.0 45.1\n Swin-B [38] 145 982 51.9 70.9 56.5 45.0 68.4 48.7\n CSWin-B 135 1004 53.9 72.6 58.5 46.4 70.0"
        },
        {
            "bounding_box": [
                {
                    "x": 1284,
                    "y": 1913
                },
                {
                    "x": 2273,
                    "y": 1913
                },
                {
                    "x": 2273,
                    "y": 1996
                },
                {
                    "x": 1284,
                    "y": 1996
                }
            ],
            "category": "caption",
            "html": "<br><caption id='67' style='font-size:16px'>Table 5. Object detection and instance segmentation performance<br>on the COCO val2017 with Cascade Mask R-CNN.</caption>",
            "id": 67,
            "page": 6,
            "text": "Table 5. Object detection and instance segmentation performance\non the COCO val2017 with Cascade Mask R-CNN."
        },
        {
            "bounding_box": [
                {
                    "x": 1282,
                    "y": 2068
                },
                {
                    "x": 2274,
                    "y": 2068
                },
                {
                    "x": 2274,
                    "y": 2110
                },
                {
                    "x": 1282,
                    "y": 2110
                }
            ],
            "category": "paragraph",
            "html": "<p id='68' style='font-size:16px'>augmentation for both CSWin-B and CSWin-L is set to 0.1.</p>",
            "id": 68,
            "page": 6,
            "text": "augmentation for both CSWin-B and CSWin-L is set to 0.1."
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 2119
                },
                {
                    "x": 2278,
                    "y": 2119
                },
                {
                    "x": 2278,
                    "y": 2364
                },
                {
                    "x": 1279,
                    "y": 2364
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='69' style='font-size:20px'>Table.3 reports the results of pre-training on ImageNet-<br>21K. Compared to the results of CSWin-B pre-trained on<br>ImageNet-1K, the large-scale data of ImageNet-21K brings<br>a 1.6%~1.7% gain. CSWin-B and CSWin-L achieve 87.0%<br>and 87.5% top-1 accuracy, surpassing previous methods.</p>",
            "id": 69,
            "page": 6,
            "text": "Table.3 reports the results of pre-training on ImageNet-\n21K. Compared to the results of CSWin-B pre-trained on\nImageNet-1K, the large-scale data of ImageNet-21K brings\na 1.6%~1.7% gain. CSWin-B and CSWin-L achieve 87.0%\nand 87.5% top-1 accuracy, surpassing previous methods."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 2398
                },
                {
                    "x": 1846,
                    "y": 2398
                },
                {
                    "x": 1846,
                    "y": 2447
                },
                {
                    "x": 1281,
                    "y": 2447
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='70' style='font-size:22px'>4.2. COCO Object Detection</p>",
            "id": 70,
            "page": 6,
            "text": "4.2. COCO Object Detection"
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2477
                },
                {
                    "x": 2279,
                    "y": 2477
                },
                {
                    "x": 2279,
                    "y": 2774
                },
                {
                    "x": 1280,
                    "y": 2774
                }
            ],
            "category": "paragraph",
            "html": "<p id='71' style='font-size:20px'>Next, we evaluate CSWin Transformer on the COCO<br>objection detection task with the Mask R-CNN [21] and<br>Cascade Mask R-CNN [2] framework respectively. Specifi-<br>cally, we pretrain the backbones on the ImageNet-1K dataset<br>and follow the finetuning strategy used in Swin Transformer<br>[38] on the COCO training set.</p>",
            "id": 71,
            "page": 6,
            "text": "Next, we evaluate CSWin Transformer on the COCO\nobjection detection task with the Mask R-CNN [21] and\nCascade Mask R-CNN [2] framework respectively. Specifi-\ncally, we pretrain the backbones on the ImageNet-1K dataset\nand follow the finetuning strategy used in Swin Transformer\n[38] on the COCO training set."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 2779
                },
                {
                    "x": 2279,
                    "y": 2779
                },
                {
                    "x": 2279,
                    "y": 2975
                },
                {
                    "x": 1281,
                    "y": 2975
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='72' style='font-size:20px'>We compare CSWin Transformer with various backbones:<br>previous CNN backbones ResNet [22], ResNeXt(X) [62],<br>and Transformer backbones PVT [58], Twins [11], and<br>Swin [38]. Table 4 reports the results of the Mask R-CNN</p>",
            "id": 72,
            "page": 6,
            "text": "We compare CSWin Transformer with various backbones:\nprevious CNN backbones ResNet [22], ResNeXt(X) [62],\nand Transformer backbones PVT [58], Twins [11], and\nSwin [38]. Table 4 reports the results of the Mask R-CNN"
        },
        {
            "bounding_box": [
                {
                    "x": 218,
                    "y": 295
                },
                {
                    "x": 1199,
                    "y": 295
                },
                {
                    "x": 1199,
                    "y": 1441
                },
                {
                    "x": 218,
                    "y": 1441
                }
            ],
            "category": "table",
            "html": "<table id='73' style='font-size:14px'><tr><td>Backbone</td><td colspan=\"3\">Semantic FPN 80k #Param. FLOPsmIoU</td><td colspan=\"3\">Upernet 160k #Param. FLOPsSS/MS mIoU</td></tr><tr><td>Res50 [22]</td><td>28.5</td><td>183</td><td>36.7</td><td></td><td></td><td></td></tr><tr><td>PVT-S [58]</td><td>28.2</td><td>161</td><td>39.8</td><td></td><td></td><td></td></tr><tr><td>TwinsP-S [11]</td><td>28.4</td><td>162</td><td>44.3</td><td>54.6</td><td>919</td><td>46.2/47.5</td></tr><tr><td>Twins-S [11]</td><td>28.3</td><td>144</td><td>43.2</td><td>54.4</td><td>901</td><td>46.2/47.1</td></tr><tr><td>Swin-T [38]</td><td>31.9</td><td>182</td><td>41.5</td><td>59.9</td><td>945</td><td>44.5/45.8</td></tr><tr><td>CSWin-T</td><td>26.1</td><td>202</td><td>48.2</td><td>59.9</td><td>959</td><td>49.3/50.7</td></tr><tr><td>Res101 [22]</td><td>47.5</td><td>260</td><td>38.8</td><td>86.0</td><td>1029</td><td>--/44.9</td></tr><tr><td>PVT-M [58]</td><td>48.0</td><td>219</td><td>41.6</td><td></td><td></td><td></td></tr><tr><td>TwinsP-B [11]</td><td>48.1</td><td>220</td><td>44.9</td><td>74.3</td><td>977</td><td>47.1/48.4</td></tr><tr><td>Twins-B [11]</td><td>60.4</td><td>261</td><td>45.3</td><td>88.5</td><td>1020</td><td>47.7/48.9</td></tr><tr><td>Swin-S [38]</td><td>53.2</td><td>274</td><td>45.2</td><td>81.3</td><td>1038</td><td>47.6/49.5</td></tr><tr><td>CSWin-S</td><td>38.5</td><td>271</td><td>49.2</td><td>64.6</td><td>1027</td><td>50.4/51.5</td></tr><tr><td>X101-64 [63]</td><td>86.4</td><td></td><td>40.2</td><td></td><td></td><td></td></tr><tr><td>PVT-L [58]</td><td>65.1</td><td>283</td><td>42.1</td><td></td><td></td><td></td></tr><tr><td>TwinsP-L [11]</td><td>65.3</td><td>283</td><td>46.4</td><td>91.5</td><td>1041</td><td>48.6/49.8</td></tr><tr><td>Twins-L [11]</td><td>103.7</td><td>404</td><td>46.7</td><td>133.0</td><td>1164</td><td>48.8/50.2</td></tr><tr><td>Swin-B [38]</td><td>91.2</td><td>422</td><td>46.0</td><td>121.0</td><td>1188</td><td>48.1/49.7</td></tr><tr><td>CSWin-B</td><td>81.2</td><td>464</td><td>49.9</td><td>109.2</td><td>1222</td><td>51.1/52.2</td></tr><tr><td>Swin-B+ [38]</td><td></td><td></td><td></td><td>121.0</td><td>1841</td><td>50.0/51.7</td></tr><tr><td>Swin-Lt [38]</td><td></td><td></td><td></td><td>234.0</td><td>3230</td><td>52.1/53.5</td></tr><tr><td>CSWin-Bi</td><td>- -</td><td></td><td>-</td><td>109.2</td><td>1941</td><td>51.8/52.6</td></tr><tr><td>CSWin-Lt</td><td></td><td></td><td></td><td>207.7</td><td>2745</td><td>54.0/55.7</td></tr></table>",
            "id": 73,
            "page": 7,
            "text": "Backbone Semantic FPN 80k #Param. FLOPsmIoU Upernet 160k #Param. FLOPsSS/MS mIoU\n Res50 [22] 28.5 183 36.7   \n PVT-S [58] 28.2 161 39.8   \n TwinsP-S [11] 28.4 162 44.3 54.6 919 46.2/47.5\n Twins-S [11] 28.3 144 43.2 54.4 901 46.2/47.1\n Swin-T [38] 31.9 182 41.5 59.9 945 44.5/45.8\n CSWin-T 26.1 202 48.2 59.9 959 49.3/50.7\n Res101 [22] 47.5 260 38.8 86.0 1029 --/44.9\n PVT-M [58] 48.0 219 41.6   \n TwinsP-B [11] 48.1 220 44.9 74.3 977 47.1/48.4\n Twins-B [11] 60.4 261 45.3 88.5 1020 47.7/48.9\n Swin-S [38] 53.2 274 45.2 81.3 1038 47.6/49.5\n CSWin-S 38.5 271 49.2 64.6 1027 50.4/51.5\n X101-64 [63] 86.4  40.2   \n PVT-L [58] 65.1 283 42.1   \n TwinsP-L [11] 65.3 283 46.4 91.5 1041 48.6/49.8\n Twins-L [11] 103.7 404 46.7 133.0 1164 48.8/50.2\n Swin-B [38] 91.2 422 46.0 121.0 1188 48.1/49.7\n CSWin-B 81.2 464 49.9 109.2 1222 51.1/52.2\n Swin-B+ [38]    121.0 1841 50.0/51.7\n Swin-Lt [38]    234.0 3230 52.1/53.5\n CSWin-Bi - -  - 109.2 1941 51.8/52.6\n CSWin-Lt    207.7 2745"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 1445
                },
                {
                    "x": 1200,
                    "y": 1445
                },
                {
                    "x": 1200,
                    "y": 1717
                },
                {
                    "x": 202,
                    "y": 1717
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='74' style='font-size:14px'>Table 6. Performance comparison of different backbones on the<br>ADE20K segmentation task. Two different frameworks semantic<br>FPN and Upernet are used. FLOPs are calculated with resolution<br>512 x 2048. ResNet/ResNeXt results and Swin FPN results are<br>copied from [58] and [11] respectively. 1 means the model is pre-<br>trained on ImageNet-21K and finetuned with 640x 640 resolution.</p>",
            "id": 74,
            "page": 7,
            "text": "Table 6. Performance comparison of different backbones on the\nADE20K segmentation task. Two different frameworks semantic\nFPN and Upernet are used. FLOPs are calculated with resolution\n512 x 2048. ResNet/ResNeXt results and Swin FPN results are\ncopied from [58] and [11] respectively. 1 means the model is pre-\ntrained on ImageNet-21K and finetuned with 640x 640 resolution."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1772
                },
                {
                    "x": 1200,
                    "y": 1772
                },
                {
                    "x": 1200,
                    "y": 2169
                },
                {
                    "x": 201,
                    "y": 2169
                }
            ],
            "category": "paragraph",
            "html": "<p id='75' style='font-size:18px'>framework with \"1x\" (12 training epoch) and \"3 x +MS\"<br>(36 training epoch with multi-scale training) schedule. It<br>shows that our CSWin Transformer variants clearly outper-<br>forms all the CNN and Transformer counterparts. In details,<br>our CSWin-T outperforms Swin-T by +4.5 box AP, +3.1<br>mask AP with the 1x schedule and +3.0 box AP, +2.0 mask<br>AP with the 3x schedule respectively. We also achieve<br>similar performance gain on small and base configuration.</p>",
            "id": 75,
            "page": 7,
            "text": "framework with \"1x\" (12 training epoch) and \"3 x +MS\"\n(36 training epoch with multi-scale training) schedule. It\nshows that our CSWin Transformer variants clearly outper-\nforms all the CNN and Transformer counterparts. In details,\nour CSWin-T outperforms Swin-T by +4.5 box AP, +3.1\nmask AP with the 1x schedule and +3.0 box AP, +2.0 mask\nAP with the 3x schedule respectively. We also achieve\nsimilar performance gain on small and base configuration."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2174
                },
                {
                    "x": 1201,
                    "y": 2174
                },
                {
                    "x": 1201,
                    "y": 2418
                },
                {
                    "x": 201,
                    "y": 2418
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='76' style='font-size:18px'>Table 5 reports the results with the Cascade Mask R-<br>CNN framework. Though Cascade Mask R-CNN is overall<br>stronger than Mask R-CNN, we observe CSWin Transform-<br>ers still surpass the counterparts by promising margins under<br>different model configurations.</p>",
            "id": 76,
            "page": 7,
            "text": "Table 5 reports the results with the Cascade Mask R-\nCNN framework. Though Cascade Mask R-CNN is overall\nstronger than Mask R-CNN, we observe CSWin Transform-\ners still surpass the counterparts by promising margins under\ndifferent model configurations."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2451
                },
                {
                    "x": 938,
                    "y": 2451
                },
                {
                    "x": 938,
                    "y": 2500
                },
                {
                    "x": 202,
                    "y": 2500
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='77' style='font-size:22px'>4.3. ADE20K Semantic Segmentation</p>",
            "id": 77,
            "page": 7,
            "text": "4.3. ADE20K Semantic Segmentation"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2529
                },
                {
                    "x": 1202,
                    "y": 2529
                },
                {
                    "x": 1202,
                    "y": 2977
                },
                {
                    "x": 201,
                    "y": 2977
                }
            ],
            "category": "paragraph",
            "html": "<p id='78' style='font-size:18px'>We further investigate the capability of CSWin Trans-<br>former for Semantic Segmentation on the ADE20K [72]<br>dataset. Here we employ the semantic FPN [33] and Uper-<br>net [61] as the basic framework. For fair comparison, we<br>follow previous works [38, 58] and train Semantic FPN 80k<br>iterations with batch size as 16, and Upernet 160k iterations<br>with batch size as 16, more details are provided in the sup-<br>plementary material. In Table 6, we report the results of<br>different methods in terms of mIoU and Multi-scale tested</p>",
            "id": 78,
            "page": 7,
            "text": "We further investigate the capability of CSWin Trans-\nformer for Semantic Segmentation on the ADE20K [72]\ndataset. Here we employ the semantic FPN [33] and Uper-\nnet [61] as the basic framework. For fair comparison, we\nfollow previous works [38, 58] and train Semantic FPN 80k\niterations with batch size as 16, and Upernet 160k iterations\nwith batch size as 16, more details are provided in the sup-\nplementary material. In Table 6, we report the results of\ndifferent methods in terms of mIoU and Multi-scale tested"
        },
        {
            "bounding_box": [
                {
                    "x": 1296,
                    "y": 296
                },
                {
                    "x": 2292,
                    "y": 296
                },
                {
                    "x": 2292,
                    "y": 687
                },
                {
                    "x": 1296,
                    "y": 687
                }
            ],
            "category": "table",
            "html": "<br><table id='79' style='font-size:14px'><tr><td rowspan=\"2\">Model</td><td colspan=\"4\">Cascade Mask R-CNN on COCO</td><td colspan=\"4\">UperNet on ADE20K</td></tr><tr><td>#Param.</td><td>FLOPs</td><td>FPS</td><td>APb/m</td><td>#Param.</td><td>FLOPs</td><td>FPS</td><td>mloU</td></tr><tr><td>Swin-T</td><td>86M</td><td>745G</td><td>15.3</td><td>50.5/43.7</td><td>60M</td><td>945G</td><td>18.5</td><td>44.5</td></tr><tr><td>CSWin-T</td><td>80M</td><td>757G</td><td>14.2</td><td>52.5/45.3</td><td>60M</td><td>959G</td><td>17.3</td><td>49.3</td></tr><tr><td>Swin-S</td><td>107M</td><td>838G</td><td>12.0</td><td>51.8/44.7</td><td>81M</td><td>1038G</td><td>15.2</td><td>47.6</td></tr><tr><td>CSWin-S</td><td>92M</td><td>820G</td><td>11.7</td><td>53.7/46.4</td><td>65M</td><td>1027G</td><td>15.6</td><td>50.4</td></tr><tr><td>Swin-B</td><td>145M</td><td>982G</td><td>11.2</td><td>51.9/45.0</td><td>121M</td><td>1188G</td><td>9.92</td><td>48.1</td></tr><tr><td>CSWin-B</td><td>135M</td><td>1004G</td><td>9.6</td><td>53.9/46.4</td><td>109M</td><td>1222G</td><td>9.08</td><td>51.1</td></tr></table>",
            "id": 79,
            "page": 7,
            "text": "Model Cascade Mask R-CNN on COCO UperNet on ADE20K\n #Param. FLOPs FPS APb/m #Param. FLOPs FPS mloU\n Swin-T 86M 745G 15.3 50.5/43.7 60M 945G 18.5 44.5\n CSWin-T 80M 757G 14.2 52.5/45.3 60M 959G 17.3 49.3\n Swin-S 107M 838G 12.0 51.8/44.7 81M 1038G 15.2 47.6\n CSWin-S 92M 820G 11.7 53.7/46.4 65M 1027G 15.6 50.4\n Swin-B 145M 982G 11.2 51.9/45.0 121M 1188G 9.92 48.1\n CSWin-B 135M 1004G 9.6 53.9/46.4 109M 1222G 9.08"
        },
        {
            "bounding_box": [
                {
                    "x": 1339,
                    "y": 690
                },
                {
                    "x": 2211,
                    "y": 690
                },
                {
                    "x": 2211,
                    "y": 730
                },
                {
                    "x": 1339,
                    "y": 730
                }
            ],
            "category": "caption",
            "html": "<br><caption id='80' style='font-size:14px'>Table 7. FPS comparison with Swin on downstream tasks.</caption>",
            "id": 80,
            "page": 7,
            "text": "Table 7. FPS comparison with Swin on downstream tasks."
        },
        {
            "bounding_box": [
                {
                    "x": 1277,
                    "y": 783
                },
                {
                    "x": 2280,
                    "y": 783
                },
                {
                    "x": 2280,
                    "y": 1382
                },
                {
                    "x": 1277,
                    "y": 1382
                }
            ],
            "category": "paragraph",
            "html": "<p id='81' style='font-size:20px'>mIoU (MS mloU). It can be seen that, our CSWin Trans-<br>formers significantly outperform previous state-of-the-arts<br>under different configurations. In details, CSWin-T, CSWin-<br>S, CSWin-B achieve +6.7, +4.0, +3.9 higher mIOU than the<br>Swin counterparts with the Semantic FPN framework, and<br>+4.8, +2.8, +3.0 higher mIOU with the Upernet framework.<br>Compared to the CNN counterparts, the performance gain<br>is very promising and demonstrates the potential of vision<br>Transformers again. When using the ImageNet-21K pre-<br>trained model, our CSWin-L further achieves 55.7 mIoU<br>and surpasses the previous best model by +2.2 mIoU, while<br>using less computation complexity.</p>",
            "id": 81,
            "page": 7,
            "text": "mIoU (MS mloU). It can be seen that, our CSWin Trans-\nformers significantly outperform previous state-of-the-arts\nunder different configurations. In details, CSWin-T, CSWin-\nS, CSWin-B achieve +6.7, +4.0, +3.9 higher mIOU than the\nSwin counterparts with the Semantic FPN framework, and\n+4.8, +2.8, +3.0 higher mIOU with the Upernet framework.\nCompared to the CNN counterparts, the performance gain\nis very promising and demonstrates the potential of vision\nTransformers again. When using the ImageNet-21K pre-\ntrained model, our CSWin-L further achieves 55.7 mIoU\nand surpasses the previous best model by +2.2 mIoU, while\nusing less computation complexity."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 1424
                },
                {
                    "x": 1692,
                    "y": 1424
                },
                {
                    "x": 1692,
                    "y": 1473
                },
                {
                    "x": 1281,
                    "y": 1473
                }
            ],
            "category": "paragraph",
            "html": "<p id='82' style='font-size:22px'>4.4. Inference Speed.</p>",
            "id": 82,
            "page": 7,
            "text": "4.4. Inference Speed."
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 1504
                },
                {
                    "x": 2280,
                    "y": 1504
                },
                {
                    "x": 2280,
                    "y": 2150
                },
                {
                    "x": 1278,
                    "y": 2150
                }
            ],
            "category": "paragraph",
            "html": "<p id='83' style='font-size:16px'>Here we report the inference speed of our CSWin and<br>Swin works. For downstream tasks, we report the FPS of<br>Cascade Mask R-CNN for object detection on COCO and<br>UperNet for semantic segmentation on ADE20K. In most<br>cases, the speed of our model is only slightly slower than<br>Swin (less than 10%), but our model outperforms Swin by<br>large margins. For example, on COCO, CSWin-S are +1.9%<br>box AP and +1.7% mask AP higher than Swin-S with sim-<br>ilar inference speed(11.7 FPS VS. 12 FPS). Note that our<br>CSWin-T performs better than Swin-B on box AP(+0.6%),<br>mask AP(+0.3%) with much faster inference speed(14.2<br>FPS VS. 11.2 FPS), indicating our CSWin achieves better<br>accuracy/FPS trade-offs.</p>",
            "id": 83,
            "page": 7,
            "text": "Here we report the inference speed of our CSWin and\nSwin works. For downstream tasks, we report the FPS of\nCascade Mask R-CNN for object detection on COCO and\nUperNet for semantic segmentation on ADE20K. In most\ncases, the speed of our model is only slightly slower than\nSwin (less than 10%), but our model outperforms Swin by\nlarge margins. For example, on COCO, CSWin-S are +1.9%\nbox AP and +1.7% mask AP higher than Swin-S with sim-\nilar inference speed(11.7 FPS VS. 12 FPS). Note that our\nCSWin-T performs better than Swin-B on box AP(+0.6%),\nmask AP(+0.3%) with much faster inference speed(14.2\nFPS VS. 11.2 FPS), indicating our CSWin achieves better\naccuracy/FPS trade-offs."
        },
        {
            "bounding_box": [
                {
                    "x": 1282,
                    "y": 2194
                },
                {
                    "x": 1663,
                    "y": 2194
                },
                {
                    "x": 1663,
                    "y": 2244
                },
                {
                    "x": 1282,
                    "y": 2244
                }
            ],
            "category": "paragraph",
            "html": "<p id='84' style='font-size:22px'>4.5. Ablation Study</p>",
            "id": 84,
            "page": 7,
            "text": "4.5. Ablation Study"
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2275
                },
                {
                    "x": 2279,
                    "y": 2275
                },
                {
                    "x": 2279,
                    "y": 2721
                },
                {
                    "x": 1280,
                    "y": 2721
                }
            ],
            "category": "paragraph",
            "html": "<p id='85' style='font-size:16px'>To better understand CSWin Transformers, we compare<br>each key component with the previous works under a com-<br>pletely fair setting that we use the same architecture and<br>hyper-parameter for the following experiments, and only<br>vary one component for each ablation. For time considera-<br>tion, we use Mask R-CNN with 1x schedule as the default<br>setting for detection and instance segmentation evaluation,<br>and Semantic FPN with 80k iterations and single-scale test<br>for segmentation evaluation.</p>",
            "id": 85,
            "page": 7,
            "text": "To better understand CSWin Transformers, we compare\neach key component with the previous works under a com-\npletely fair setting that we use the same architecture and\nhyper-parameter for the following experiments, and only\nvary one component for each ablation. For time considera-\ntion, we use Mask R-CNN with 1x schedule as the default\nsetting for detection and instance segmentation evaluation,\nand Semantic FPN with 80k iterations and single-scale test\nfor segmentation evaluation."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2727
                },
                {
                    "x": 2280,
                    "y": 2727
                },
                {
                    "x": 2280,
                    "y": 2976
                },
                {
                    "x": 1280,
                    "y": 2976
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='86' style='font-size:18px'>Parallel Multi-Head Grouping. We first study the effec-<br>tiveness of our novel \"Parallel Multi-Head Grouping\" strat-<br>egy. Here we compare Axial-Attention [25] and Criss-Cross-<br>Attention [30] under the CSWin-T backbone. \"Attention<br>region\" is used as the computation cost metric for detailed</p>",
            "id": 86,
            "page": 7,
            "text": "Parallel Multi-Head Grouping. We first study the effec-\ntiveness of our novel \"Parallel Multi-Head Grouping\" strat-\negy. Here we compare Axial-Attention [25] and Criss-Cross-\nAttention [30] under the CSWin-T backbone. \"Attention\nregion\" is used as the computation cost metric for detailed"
        },
        {
            "bounding_box": [
                {
                    "x": 219,
                    "y": 296
                },
                {
                    "x": 2282,
                    "y": 296
                },
                {
                    "x": 2282,
                    "y": 726
                },
                {
                    "x": 219,
                    "y": 726
                }
            ],
            "category": "table",
            "html": "<table id='87' style='font-size:14px'><tr><td rowspan=\"2\">Model</td><td rowspan=\"2\">Attention Reigon</td><td colspan=\"4\">ImageNet</td><td colspan=\"5\">COCO</td><td colspan=\"4\">ADE20K</td></tr><tr><td>#Param.</td><td>FLOPs</td><td>FPS</td><td>Top1(%)</td><td>#Param.</td><td>FLOPs</td><td>FPS</td><td>APb</td><td>APm</td><td>#Param.</td><td>FLOPs</td><td>FPS</td><td>mIoU(%)</td></tr><tr><td rowspan=\"2\">Axial CSWin (fix sw=1)</td><td>H</td><td>23M</td><td>4.2G</td><td>735</td><td>81.8</td><td>42M</td><td>258G</td><td>27.9</td><td>43.4</td><td>39.4</td><td>26M</td><td>186G</td><td>50.3</td><td>42.6</td></tr><tr><td>H</td><td>23M</td><td>4.1G</td><td>721</td><td>81.9</td><td>42M</td><td>258G</td><td>26.8</td><td>45.2</td><td>40.8</td><td>26M</td><td>179G</td><td>49.1</td><td>47.5</td></tr><tr><td rowspan=\"2\">Criss-Cross CSWin (fix sw=2)</td><td>H*2-1</td><td>23M</td><td>4.2G</td><td>187</td><td>82.2</td><td>42M</td><td>263G</td><td>5.5</td><td>45.2</td><td>40.9</td><td>26M</td><td>186G</td><td>17.6</td><td>47.4</td></tr><tr><td>H*2</td><td>23M</td><td>4.2G</td><td>718</td><td>82.2</td><td>42M</td><td>263G</td><td>25.1</td><td>45.6</td><td>41.4</td><td>26M</td><td>186G</td><td>47.2</td><td>47.6</td></tr><tr><td rowspan=\"2\">CSWin (sw=1,2,7,7; Seq) CSWin (sw=1,2,7,7)</td><td>swxH</td><td>23M</td><td>4.3G</td><td>711</td><td>82.4</td><td>42M</td><td>279G</td><td>22.3</td><td>45.1</td><td>41.1</td><td>26M</td><td>202G</td><td>45.2</td><td>46.2</td></tr><tr><td>SwxH</td><td>23M</td><td>4.3G</td><td>701</td><td>82.7</td><td>42M</td><td>279G</td><td>21.1</td><td>46.7</td><td>42.2</td><td>26M</td><td>202G</td><td>44.8</td><td>48.2</td></tr></table>",
            "id": 87,
            "page": 8,
            "text": "Model Attention Reigon ImageNet COCO ADE20K\n #Param. FLOPs FPS Top1(%) #Param. FLOPs FPS APb APm #Param. FLOPs FPS mIoU(%)\n Axial CSWin (fix sw=1) H 23M 4.2G 735 81.8 42M 258G 27.9 43.4 39.4 26M 186G 50.3 42.6\n H 23M 4.1G 721 81.9 42M 258G 26.8 45.2 40.8 26M 179G 49.1 47.5\n Criss-Cross CSWin (fix sw=2) H*2-1 23M 4.2G 187 82.2 42M 263G 5.5 45.2 40.9 26M 186G 17.6 47.4\n H*2 23M 4.2G 718 82.2 42M 263G 25.1 45.6 41.4 26M 186G 47.2 47.6\n CSWin (sw=1,2,7,7; Seq) CSWin (sw=1,2,7,7) swxH 23M 4.3G 711 82.4 42M 279G 22.3 45.1 41.1 26M 202G 45.2 46.2\n SwxH 23M 4.3G 701 82.7 42M 279G 21.1 46.7 42.2 26M 202G 44.8"
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 725
                },
                {
                    "x": 2275,
                    "y": 725
                },
                {
                    "x": 2275,
                    "y": 812
                },
                {
                    "x": 204,
                    "y": 812
                }
            ],
            "category": "caption",
            "html": "<br><caption id='88' style='font-size:16px'>Table 8. Stripes-Based attention mechanism comparison. 'Seq' means sequential multi-head attention like Axial-attention. 'Attention<br>Region' means the average number of tokens that each head calculates attention with.</caption>",
            "id": 88,
            "page": 8,
            "text": "Table 8. Stripes-Based attention mechanism comparison. 'Seq' means sequential multi-head attention like Axial-attention. 'Attention\nRegion' means the average number of tokens that each head calculates attention with."
        },
        {
            "bounding_box": [
                {
                    "x": 206,
                    "y": 851
                },
                {
                    "x": 1200,
                    "y": 851
                },
                {
                    "x": 1200,
                    "y": 1239
                },
                {
                    "x": 206,
                    "y": 1239
                }
            ],
            "category": "figure",
            "html": "<figure><img id='89' style='font-size:14px' alt=\"83\nsw=[14,14,14, 7] sw=[28,28,14, 7]\n82.8 sw=[1,2,7,7] sw=7\nAccuracy\n82.6\n82.4\nsw=2\n82.2\nsw=1\n82\nX\n81.8\n4 4.2 4.4 4.6 4.8 5 5.2 5.4 5.6\nFLOPS(G)\" data-coord=\"top-left:(206,851); bottom-right:(1200,1239)\" /></figure>",
            "id": 89,
            "page": 8,
            "text": "83\nsw=[14,14,14, 7] sw=[28,28,14, 7]\n82.8 sw=[1,2,7,7] sw=7\nAccuracy\n82.6\n82.4\nsw=2\n82.2\nsw=1\n82\nX\n81.8\n4 4.2 4.4 4.6 4.8 5 5.2 5.4 5.6\nFLOPS(G)"
        },
        {
            "bounding_box": [
                {
                    "x": 359,
                    "y": 1244
                },
                {
                    "x": 1036,
                    "y": 1244
                },
                {
                    "x": 1036,
                    "y": 1290
                },
                {
                    "x": 359,
                    "y": 1290
                }
            ],
            "category": "caption",
            "html": "<br><caption id='90' style='font-size:16px'>Figure 4. Ablation on dynamic window size.</caption>",
            "id": 90,
            "page": 8,
            "text": "Figure 4. Ablation on dynamic window size."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 1334
                },
                {
                    "x": 1202,
                    "y": 1334
                },
                {
                    "x": 1202,
                    "y": 1429
                },
                {
                    "x": 202,
                    "y": 1429
                }
            ],
            "category": "paragraph",
            "html": "<p id='91' style='font-size:16px'>comparison. To simplify, we assume the attention is calcu-<br>lated on a square input that H = W.</p>",
            "id": 91,
            "page": 8,
            "text": "comparison. To simplify, we assume the attention is calcu-\nlated on a square input that H = W."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1436
                },
                {
                    "x": 1201,
                    "y": 1436
                },
                {
                    "x": 1201,
                    "y": 1975
                },
                {
                    "x": 201,
                    "y": 1975
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='92' style='font-size:18px'>In Table.8, we find that the \"parallel multi-head grouping\"<br>is efficient and effective, especially for downstream tasks.<br>When we replace the Parallel manner with Sequential, the<br>performance of CSWin degrades on all tasks. When compar-<br>ing with previous methods under the similar attention region<br>constrain, our sw = 1 CSWin performs slightly better than<br>Axial on ImageNet, while outperforming it by a large margin<br>on downstream tasks. Our sw = 2 CSWin performs slightly<br>better than Criss-Cross Attention, while the speed of CSWin<br>is 2x ~ 5x faster than it on different tasks, this further<br>proves that our \"parallel\" design is much more efficient.</p>",
            "id": 92,
            "page": 8,
            "text": "In Table.8, we find that the \"parallel multi-head grouping\"\nis efficient and effective, especially for downstream tasks.\nWhen we replace the Parallel manner with Sequential, the\nperformance of CSWin degrades on all tasks. When compar-\ning with previous methods under the similar attention region\nconstrain, our sw = 1 CSWin performs slightly better than\nAxial on ImageNet, while outperforming it by a large margin\non downstream tasks. Our sw = 2 CSWin performs slightly\nbetter than Criss-Cross Attention, while the speed of CSWin\nis 2x ~ 5x faster than it on different tasks, this further\nproves that our \"parallel\" design is much more efficient."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 1979
                },
                {
                    "x": 1202,
                    "y": 1979
                },
                {
                    "x": 1202,
                    "y": 2326
                },
                {
                    "x": 202,
                    "y": 2326
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='93' style='font-size:20px'>Dynamic Stripe Width · In Fig.4 we study the trade off<br>between stripe width and accuracy. We find that with the in-<br>crease of stripe width, the compution cost(FLOPS) increase,<br>and the Top-1 classification accuracy improves greatly at the<br>beginning and slows down when the width is large enough.<br>Our default setting [1,2,7,7] achieves a good trade-off be-<br>tween accuracy and FLOPs.</p>",
            "id": 93,
            "page": 8,
            "text": "Dynamic Stripe Width · In Fig.4 we study the trade off\nbetween stripe width and accuracy. We find that with the in-\ncrease of stripe width, the compution cost(FLOPS) increase,\nand the Top-1 classification accuracy improves greatly at the\nbeginning and slows down when the width is large enough.\nOur default setting [1,2,7,7] achieves a good trade-off be-\ntween accuracy and FLOPs."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 2329
                },
                {
                    "x": 1201,
                    "y": 2329
                },
                {
                    "x": 1201,
                    "y": 2875
                },
                {
                    "x": 200,
                    "y": 2875
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='94' style='font-size:20px'>Attention Mechanism Comparison. Following the above<br>analysis on each component of CSWin self-attention, we<br>further compare with existing self-attention mechanisms. As<br>some of the methods need even layers in each stage, for a<br>fair comparison, we use the Swin-T [38] as backbone and<br>only change the self-attention mechanism. In detail, we<br>use 2, 2, 6, 2 blocks for the four stages with the 96 base chan-<br>nel, non-overlapped token embedding [17], and RPE [38].<br>The results are reported in Table 9. Obviously, our CSWin<br>self-attention mechanism performs better than existing self-<br>attention mechanisms across all the tasks.</p>",
            "id": 94,
            "page": 8,
            "text": "Attention Mechanism Comparison. Following the above\nanalysis on each component of CSWin self-attention, we\nfurther compare with existing self-attention mechanisms. As\nsome of the methods need even layers in each stage, for a\nfair comparison, we use the Swin-T [38] as backbone and\nonly change the self-attention mechanism. In detail, we\nuse 2, 2, 6, 2 blocks for the four stages with the 96 base chan-\nnel, non-overlapped token embedding [17], and RPE [38].\nThe results are reported in Table 9. Obviously, our CSWin\nself-attention mechanism performs better than existing self-\nattention mechanisms across all the tasks."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2876
                },
                {
                    "x": 1201,
                    "y": 2876
                },
                {
                    "x": 1201,
                    "y": 2978
                },
                {
                    "x": 202,
                    "y": 2978
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='95' style='font-size:20px'>Positional Encoding Comparison. The proposed LePE<br>is specially designed to enhance the local positional infor-</p>",
            "id": 95,
            "page": 8,
            "text": "Positional Encoding Comparison. The proposed LePE\nis specially designed to enhance the local positional infor-"
        },
        {
            "bounding_box": [
                {
                    "x": 1297,
                    "y": 857
                },
                {
                    "x": 2301,
                    "y": 857
                },
                {
                    "x": 2301,
                    "y": 1202
                },
                {
                    "x": 1297,
                    "y": 1202
                }
            ],
            "category": "table",
            "html": "<br><table id='96' style='font-size:16px'><tr><td></td><td>ImageNet</td><td colspan=\"2\">COCO</td><td rowspan=\"2\">ADE20K mIoU(%)</td></tr><tr><td></td><td>Top1(%)</td><td>APb</td><td>APm</td></tr><tr><td>Sliding window [43]</td><td>81.4</td><td></td><td></td><td></td></tr><tr><td>Shifted window [38]</td><td>81.3</td><td>42.2</td><td>39.1</td><td>41.5</td></tr><tr><td>Spatially Sep [11]</td><td>81.5</td><td>42.7</td><td>39.5</td><td>42.9</td></tr><tr><td>Sequential Axial [25]</td><td>81.5</td><td>40.4</td><td>37.6</td><td>39.8</td></tr><tr><td>Criss-Cross [30]</td><td>81.7</td><td>42.9</td><td>39.7</td><td>43.0</td></tr><tr><td>Cross-shaped window</td><td>82.2</td><td>43.4</td><td>40.2</td><td>43.4</td></tr></table>",
            "id": 96,
            "page": 8,
            "text": "ImageNet COCO ADE20K mIoU(%)\n  Top1(%) APb APm\n Sliding window [43] 81.4   \n Shifted window [38] 81.3 42.2 39.1 41.5\n Spatially Sep [11] 81.5 42.7 39.5 42.9\n Sequential Axial [25] 81.5 40.4 37.6 39.8\n Criss-Cross [30] 81.7 42.9 39.7 43.0\n Cross-shaped window 82.2 43.4 40.2"
        },
        {
            "bounding_box": [
                {
                    "x": 1325,
                    "y": 1210
                },
                {
                    "x": 2228,
                    "y": 1210
                },
                {
                    "x": 2228,
                    "y": 1249
                },
                {
                    "x": 1325,
                    "y": 1249
                }
            ],
            "category": "caption",
            "html": "<br><caption id='97' style='font-size:16px'>Table 9. Comparison of different self-attention mechanisms.</caption>",
            "id": 97,
            "page": 8,
            "text": "Table 9. Comparison of different self-attention mechanisms."
        },
        {
            "bounding_box": [
                {
                    "x": 1298,
                    "y": 1285
                },
                {
                    "x": 2301,
                    "y": 1285
                },
                {
                    "x": 2301,
                    "y": 1630
                },
                {
                    "x": 1298,
                    "y": 1630
                }
            ],
            "category": "table",
            "html": "<table id='98' style='font-size:14px'><tr><td rowspan=\"2\"></td><td rowspan=\"2\">ImageNet Top1(%)</td><td colspan=\"2\">COCO</td><td rowspan=\"2\">ADE20K mIoU(%)</td></tr><tr><td>APb</td><td>APm</td></tr><tr><td>No PE</td><td>82.5</td><td>44.8</td><td>41.1</td><td>47.0</td></tr><tr><td>APE [17]</td><td>82.6</td><td>45.1</td><td>41.1</td><td>45.7</td></tr><tr><td>CPE [12]</td><td>82.2</td><td>45.8</td><td>41.6</td><td>46.1</td></tr><tr><td>CPE* [12]</td><td>82.4</td><td>45.4</td><td>41.3</td><td>46.6</td></tr><tr><td>RPE [46]</td><td>82.7</td><td>45.5</td><td>41.3</td><td>46.6</td></tr><tr><td>LePE</td><td>82.7</td><td>46.7</td><td>42.2</td><td>48.2</td></tr></table>",
            "id": 98,
            "page": 8,
            "text": "ImageNet Top1(%) COCO ADE20K mIoU(%)\n APb APm\n No PE 82.5 44.8 41.1 47.0\n APE [17] 82.6 45.1 41.1 45.7\n CPE [12] 82.2 45.8 41.6 46.1\n CPE* [12] 82.4 45.4 41.3 46.6\n RPE [46] 82.7 45.5 41.3 46.6\n LePE 82.7 46.7 42.2"
        },
        {
            "bounding_box": [
                {
                    "x": 1285,
                    "y": 1639
                },
                {
                    "x": 2273,
                    "y": 1639
                },
                {
                    "x": 2273,
                    "y": 1680
                },
                {
                    "x": 1285,
                    "y": 1680
                }
            ],
            "category": "caption",
            "html": "<br><caption id='99' style='font-size:20px'>Table 10. Comparison of different positional encoding mechanisms.</caption>",
            "id": 99,
            "page": 8,
            "text": "Table 10. Comparison of different positional encoding mechanisms."
        },
        {
            "bounding_box": [
                {
                    "x": 1277,
                    "y": 1737
                },
                {
                    "x": 2282,
                    "y": 1737
                },
                {
                    "x": 2282,
                    "y": 2531
                },
                {
                    "x": 1277,
                    "y": 2531
                }
            ],
            "category": "paragraph",
            "html": "<p id='100' style='font-size:20px'>mation on downstream tasks for various input resolutions.<br>Here we use CSWin-T as the backbone and only very<br>the position encoding. In Table 10, we compare our LePE<br>with other recent positional encoding mechanisms(APE [17],<br>CPE [12], and RPE [46]) for image classification, object<br>detection and image segmentation. Besides, we also test<br>the variants without positional encoding (No PE) and CPE*,<br>which is obtained by applying CPE before every Transformer<br>block. According to the comparison results, we see that: 1)<br>Positional encoding can bring performance gain by introduc-<br>ing the local inductive bias; 2) Though RPE achieves similar<br>performance on the classification task with fixed input resolu-<br>tion, our LePE performs better (+1.2 box AP and +0.9 mask<br>AP on COCO, +0.9 mIoU on ADE20K) on downstream<br>tasks where the input resolution varies; 3) Compared to APE<br>and CPE, our LePE also achieves better performance.</p>",
            "id": 100,
            "page": 8,
            "text": "mation on downstream tasks for various input resolutions.\nHere we use CSWin-T as the backbone and only very\nthe position encoding. In Table 10, we compare our LePE\nwith other recent positional encoding mechanisms(APE [17],\nCPE [12], and RPE [46]) for image classification, object\ndetection and image segmentation. Besides, we also test\nthe variants without positional encoding (No PE) and CPE*,\nwhich is obtained by applying CPE before every Transformer\nblock. According to the comparison results, we see that: 1)\nPositional encoding can bring performance gain by introduc-\ning the local inductive bias; 2) Though RPE achieves similar\nperformance on the classification task with fixed input resolu-\ntion, our LePE performs better (+1.2 box AP and +0.9 mask\nAP on COCO, +0.9 mIoU on ADE20K) on downstream\ntasks where the input resolution varies; 3) Compared to APE\nand CPE, our LePE also achieves better performance."
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 2587
                },
                {
                    "x": 1579,
                    "y": 2587
                },
                {
                    "x": 1579,
                    "y": 2641
                },
                {
                    "x": 1279,
                    "y": 2641
                }
            ],
            "category": "paragraph",
            "html": "<p id='101' style='font-size:22px'>5. Conclusion</p>",
            "id": 101,
            "page": 8,
            "text": "5. Conclusion"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 2677
                },
                {
                    "x": 2280,
                    "y": 2677
                },
                {
                    "x": 2280,
                    "y": 2978
                },
                {
                    "x": 1279,
                    "y": 2978
                }
            ],
            "category": "paragraph",
            "html": "<p id='102' style='font-size:18px'>In this paper, we have presented a new Vision Trans-<br>former architecture named CSWin Transformer. The core<br>design of CSWin Transformer is the CSWin Self-Attention,<br>which performs self-attention in the horizontal and vertical<br>stripes by splitting the multi-heads into parallel groups. This<br>multi-head grouping design can enlarge the attention area</p>",
            "id": 102,
            "page": 8,
            "text": "In this paper, we have presented a new Vision Trans-\nformer architecture named CSWin Transformer. The core\ndesign of CSWin Transformer is the CSWin Self-Attention,\nwhich performs self-attention in the horizontal and vertical\nstripes by splitting the multi-heads into parallel groups. This\nmulti-head grouping design can enlarge the attention area"
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 307
                },
                {
                    "x": 1203,
                    "y": 307
                },
                {
                    "x": 1203,
                    "y": 755
                },
                {
                    "x": 200,
                    "y": 755
                }
            ],
            "category": "paragraph",
            "html": "<p id='103' style='font-size:16px'>of each token within one Transformer block efficiently. On<br>the other hand, the mathematical analysis also allows us to<br>increase the stripe width along the network depth to further<br>enlarge the attention area with subtle extra computation cost.<br>We further introduce locally-enhanced positional encoding<br>into CSWin Transformer for downstream tasks. We achieved<br>the state-of-the-art performance on various vision tasks un-<br>der constrained computation complexity. We are looking<br>forward to applying it for more vision tasks.</p>",
            "id": 103,
            "page": 9,
            "text": "of each token within one Transformer block efficiently. On\nthe other hand, the mathematical analysis also allows us to\nincrease the stripe width along the network depth to further\nenlarge the attention area with subtle extra computation cost.\nWe further introduce locally-enhanced positional encoding\ninto CSWin Transformer for downstream tasks. We achieved\nthe state-of-the-art performance on various vision tasks un-\nder constrained computation complexity. We are looking\nforward to applying it for more vision tasks."
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 795
                },
                {
                    "x": 447,
                    "y": 795
                },
                {
                    "x": 447,
                    "y": 849
                },
                {
                    "x": 204,
                    "y": 849
                }
            ],
            "category": "paragraph",
            "html": "<p id='104' style='font-size:20px'>References</p>",
            "id": 104,
            "page": 9,
            "text": "References"
        },
        {
            "bounding_box": [
                {
                    "x": 218,
                    "y": 871
                },
                {
                    "x": 1200,
                    "y": 871
                },
                {
                    "x": 1200,
                    "y": 2984
                },
                {
                    "x": 218,
                    "y": 2984
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='105' style='font-size:16px'>[1] Iz Beltagy, Matthew E Peters, and Arman Cohan. Long-<br>former: The long-document transformer. arXiv preprint<br>arXiv:2004.05150, 2020. 3<br>[2] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delving<br>into high quality object detection. In Proceedings of the IEEE<br>conference on computer vision and pattern recognition, pages<br>6154-6162, 2018. 6, 12<br>[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas<br>Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-<br>end object detection with transformers. In European Con-<br>ference on Computer Vision, pages 213-229. Springer, 2020.<br>2<br>[4] Chun-Fu Chen, Quanfu Fan, and Rameswar Panda. Crossvit:<br>Cross-attention multi-scale vision transformer for image clas-<br>sification, 2021. 13<br>[5] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping<br>Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, and<br>Wen Gao. Pre-trained image processing transformer. arXiv<br>preprint arXiv:2012.00364, 2020. 2<br>[6] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu<br>Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu,<br>Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tian-<br>heng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue<br>Wu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang,<br>Chen Change Loy, and Dahua Lin. MMDetection: Open<br>mmlab detection toolbox and benchmark. arXiv preprint<br>arXiv:1906.07155, 2019. 12<br>[7] Yunpeng Chen, Jianan Li, Huaxin Xiao, Xiaojie Jin,<br>Shuicheng Yan, and Jiashi Feng. Dual path networks. arXiv<br>preprint arXiv:1707.01629, 2017. 2<br>[8] Zhengsu Chen, Lingxi Xie, Jianwei Niu, Xuefeng Liu,<br>Longhui Wei, and Qi Tian. Visformer: The vision-friendly<br>transformer, 2021. 13<br>[9] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.<br>Generating long sequences with sparse transformers. arXiv<br>preprint arXiv:1904.10509, 2019. 3<br>[10] Krzysztof Choromanski, Valerii Likhosherstov, David Do-<br>han, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter<br>Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser,<br>et al. Rethinking attention with performers. arXiv preprint<br>arXiv:2009.14794, 2020. 3<br>[11] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing<br>Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Twins: Re-<br>visiting spatial attention design in vision transformers. arXiv<br>preprint arXiv:2104.13840, 2021. 2, 6, 7, 8, 13</p>",
            "id": 105,
            "page": 9,
            "text": "[1] Iz Beltagy, Matthew E Peters, and Arman Cohan. Long-\nformer: The long-document transformer. arXiv preprint\narXiv:2004.05150, 2020. 3\n[2] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delving\ninto high quality object detection. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages\n6154-6162, 2018. 6, 12\n[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In European Con-\nference on Computer Vision, pages 213-229. Springer, 2020.\n2\n[4] Chun-Fu Chen, Quanfu Fan, and Rameswar Panda. Crossvit:\nCross-attention multi-scale vision transformer for image clas-\nsification, 2021. 13\n[5] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping\nDeng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, and\nWen Gao. Pre-trained image processing transformer. arXiv\npreprint arXiv:2012.00364, 2020. 2\n[6] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu\nXiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu,\nJiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tian-\nheng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue\nWu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang,\nChen Change Loy, and Dahua Lin. MMDetection: Open\nmmlab detection toolbox and benchmark. arXiv preprint\narXiv:1906.07155, 2019. 12\n[7] Yunpeng Chen, Jianan Li, Huaxin Xiao, Xiaojie Jin,\nShuicheng Yan, and Jiashi Feng. Dual path networks. arXiv\npreprint arXiv:1707.01629, 2017. 2\n[8] Zhengsu Chen, Lingxi Xie, Jianwei Niu, Xuefeng Liu,\nLonghui Wei, and Qi Tian. Visformer: The vision-friendly\ntransformer, 2021. 13\n[9] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.\nGenerating long sequences with sparse transformers. arXiv\npreprint arXiv:1904.10509, 2019. 3\n[10] Krzysztof Choromanski, Valerii Likhosherstov, David Do-\nhan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter\nHawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser,\net al. Rethinking attention with performers. arXiv preprint\narXiv:2009.14794, 2020. 3\n[11] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing\nRen, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Twins: Re-\nvisiting spatial attention design in vision transformers. arXiv\npreprint arXiv:2104.13840, 2021. 2, 6, 7, 8, 13"
        },
        {
            "bounding_box": [
                {
                    "x": 1276,
                    "y": 301
                },
                {
                    "x": 2291,
                    "y": 301
                },
                {
                    "x": 2291,
                    "y": 2976
                },
                {
                    "x": 1276,
                    "y": 2976
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='106' style='font-size:14px'>[12] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Xi-<br>aolin Wei, Huaxia Xia, and Chunhua Shen. Conditional<br>positional encodings for vision transformers. arXiv preprint<br>arXiv:2102.10882, 2021. 2, 3, 4, 8, 13<br>[13] Xiangxiang Chu, Bo Zhang, Zhi Tian, Xiaolin Wei, and<br>Huaxia Xia. Do we really need explicit position encodings<br>for vision transformers? arXiv e-prints, pages arXiv-2102,<br>2021. 2<br>[14] MMSegmentation Contributors. Mmsegmentation, an open<br>source semantic segmentation toolbox. https : / / github.<br>com/ open-mmlab/ mmsegmentation, 2020. 12<br>[15] Ekin D. Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V.<br>Le. Randaugment: Practical automated data augmentation<br>with a reduced search space, 2019. 12<br>[16] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li<br>Fei-Fei. Imagenet: A large-scale hierarchical image database.<br>In 2009 IEEE conference on computer vision and pattern<br>recognition, pages 248-255. Ieee, 2009. 5<br>[17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,<br>Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,<br>Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-<br>vain Gelly, et al. An image is worth 16x16 words: Trans-<br>formers for image recognition at scale. arXiv preprint<br>arXiv:2010.11929, 2020. 1 2, 3, 8, 13<br>[18] Alaaeldin El-Nouby, Natalia Neverova, Ivan Laptev, and<br>Herve Jegou. Training vision transformers for image retrieval.<br>arXiv preprint arXiv:2102.05644, 2021. 2<br>[19] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao<br>Li, Zhicheng Yan, Jitendra Malik, and Christoph Feicht-<br>enhofer. Multiscale vision transformers. arXiv preprint<br>arXiv:2104.11227, 2021. 13<br>[20] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu,<br>and Yunhe Wang. Transformer in transformer. arXiv preprint<br>arXiv:2103.00112, 2021. 2, 13<br>[21] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Gir-<br>shick. Mask r-cnn. In Proceedings of the IEEE international<br>conference on computer vision, pages 2961-2969, 2017. 6,<br>12<br>[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.<br>Deep residual learning for image recognition. In Proceed-<br>ings of the IEEE conference on computer vision and pattern<br>recognition, pages 770-778, 2016. 2, 6, 7<br>[23] Shuting He, Hao Luo, Pichao Wang, Fan Wang, Hao Li,<br>and Wei Jiang. Transreid: Transformer-based object re-<br>identification. arXiv preprint arXiv:2102.04378, 2021. 2<br>[24] Byeongho Heo, Sangdoo Yun, Dongyoon Han, Sanghyuk<br>Chun, Junsuk Choe, and Seong Joon Oh. Rethinking spatial<br>dimensions of vision transformers, 2021. 13<br>[25] Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim<br>Salimans. Axial attention in multidimensional transformers.<br>arXiv preprint arXiv:1912.12180, 2019. 2, 3, 7, 8<br>[26] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry<br>Kalenichenko, Weijun Wang, Tobias Weyand, Marco An-<br>dreetto, and Hartwig Adam. Mobilenets: Efficient convolu-<br>tional neural networks for mobile vision applications. arXiv<br>preprint arXiv:1704.04861, 2017. 2</p>",
            "id": 106,
            "page": 9,
            "text": "[12] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Xi-\naolin Wei, Huaxia Xia, and Chunhua Shen. Conditional\npositional encodings for vision transformers. arXiv preprint\narXiv:2102.10882, 2021. 2, 3, 4, 8, 13\n[13] Xiangxiang Chu, Bo Zhang, Zhi Tian, Xiaolin Wei, and\nHuaxia Xia. Do we really need explicit position encodings\nfor vision transformers? arXiv e-prints, pages arXiv-2102,\n2021. 2\n[14] MMSegmentation Contributors. Mmsegmentation, an open\nsource semantic segmentation toolbox. https : / / github.\ncom/ open-mmlab/ mmsegmentation, 2020. 12\n[15] Ekin D. Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V.\nLe. Randaugment: Practical automated data augmentation\nwith a reduced search space, 2019. 12\n[16] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li\nFei-Fei. Imagenet: A large-scale hierarchical image database.\nIn 2009 IEEE conference on computer vision and pattern\nrecognition, pages 248-255. Ieee, 2009. 5\n[17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 1 2, 3, 8, 13\n[18] Alaaeldin El-Nouby, Natalia Neverova, Ivan Laptev, and\nHerve Jegou. Training vision transformers for image retrieval.\narXiv preprint arXiv:2102.05644, 2021. 2\n[19] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao\nLi, Zhicheng Yan, Jitendra Malik, and Christoph Feicht-\nenhofer. Multiscale vision transformers. arXiv preprint\narXiv:2104.11227, 2021. 13\n[20] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu,\nand Yunhe Wang. Transformer in transformer. arXiv preprint\narXiv:2103.00112, 2021. 2, 13\n[21] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Gir-\nshick. Mask r-cnn. In Proceedings of the IEEE international\nconference on computer vision, pages 2961-2969, 2017. 6,\n12\n[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770-778, 2016. 2, 6, 7\n[23] Shuting He, Hao Luo, Pichao Wang, Fan Wang, Hao Li,\nand Wei Jiang. Transreid: Transformer-based object re-\nidentification. arXiv preprint arXiv:2102.04378, 2021. 2\n[24] Byeongho Heo, Sangdoo Yun, Dongyoon Han, Sanghyuk\nChun, Junsuk Choe, and Seong Joon Oh. Rethinking spatial\ndimensions of vision transformers, 2021. 13\n[25] Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim\nSalimans. Axial attention in multidimensional transformers.\narXiv preprint arXiv:1912.12180, 2019. 2, 3, 7, 8\n[26] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry\nKalenichenko, Weijun Wang, Tobias Weyand, Marco An-\ndreetto, and Hartwig Adam. Mobilenets: Efficient convolu-\ntional neural networks for mobile vision applications. arXiv\npreprint arXiv:1704.04861, 2017. 2"
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 293
                },
                {
                    "x": 1203,
                    "y": 293
                },
                {
                    "x": 1203,
                    "y": 2972
                },
                {
                    "x": 200,
                    "y": 2972
                }
            ],
            "category": "paragraph",
            "html": "<p id='107' style='font-size:14px'>[27] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation<br>networks. In Proceedings ofthe IEEE conference on computer<br>vision and pattern recognition, pages 7132-7141, 2018. 2<br>[28] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-<br>ian Q Weinberger. Densely connected convolutional networks.<br>In Proceedings of the IEEE conference on computer vision<br>and pattern recognition, pages 4700-4708, 2017. 2<br>[29] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kil-<br>ian Q Weinberger. Deep networks with stochastic depth. In<br>European conference on computer vision, pages 646-661.<br>Springer, 2016. 5, 6, 12<br>[30] Zilong Huang, Xinggang Wang, Yunchao Wei, Lichao Huang,<br>Humphrey Shi, Wenyu Liu, and Thomas S. Huang. Ccnet:<br>Criss-cross attention for semantic segmentation. IEEE Trans-<br>actions on Pattern Analysis and Machine Intelligence, pages<br>1-1, 2020. 3, 7, 8<br>[31] Zihang Jiang, Qibin Hou, Li Yuan, Daquan Zhou, Xiaojie<br>Jin, Anran Wang, and Jiashi Feng. Token labeling: Training<br>a 85.5% top-1 accuracy vision transformer with 56m param-<br>eters on imagenet. arXiv preprint arXiv:2104.10858, 2021.<br>2<br>[32] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and<br>Fran�ois Fleuret. Transformers are rnns: Fast autoregressive<br>transformers with linear attention. In International Confer-<br>ence on Machine Learning, pages 5156-5165. PMLR, 2020.<br>3<br>[33] Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr<br>Dollar. Panoptic feature pyramid networks. In Proceedings of<br>the IEEE/CVF Conference on Computer Vision and Pattern<br>Recognition, pages 6399-6408, 2019. 7, 12<br>[34] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.<br>Reformer: The efficient transformer. arXiv preprint<br>arXiv:2001.04451, 2020. 3<br>[35] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Im-<br>agenet classification with deep convolutional neural networks.<br>Advances in neural information processing systems, 25:1097-<br>1105, 2012. 2<br>[36] Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, and<br>Luc Van Gool. Localvit: Bringing locality to vision trans-<br>formers, 2021. 13<br>[37] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,<br>Pietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence<br>Zitnick. Microsoft coco: Common objects in context. In<br>European conference on computer vision, pages 740-755.<br>Springer, 2014. 5<br>[38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng<br>Zhang, Stephen Lin, and Baining Guo. Swin transformer:<br>Hierarchical vision transformer using shifted windows. arXiv<br>preprint arXiv:2103.14030, 2021. 1, 2, 3, 5, 6, 7, 8, 12, 13<br>[39] Ilya Loshchilov and Frank Hutter. Decoupled weight decay<br>regularization, 2019. 12<br>[40] Boris T Polyak and Anatoli B Juditsky. Acceleration of<br>stochastic approximation by averaging. SIAM journal on<br>control and optimization, 30(4):838-855, 1992. 12<br>[41] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaim-<br>ing He, and Piotr Dollar. Designing network design spaces,<br>2020. 13</p>",
            "id": 107,
            "page": 10,
            "text": "[27] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation\nnetworks. In Proceedings ofthe IEEE conference on computer\nvision and pattern recognition, pages 7132-7141, 2018. 2\n[28] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-\nian Q Weinberger. Densely connected convolutional networks.\nIn Proceedings of the IEEE conference on computer vision\nand pattern recognition, pages 4700-4708, 2017. 2\n[29] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kil-\nian Q Weinberger. Deep networks with stochastic depth. In\nEuropean conference on computer vision, pages 646-661.\nSpringer, 2016. 5, 6, 12\n[30] Zilong Huang, Xinggang Wang, Yunchao Wei, Lichao Huang,\nHumphrey Shi, Wenyu Liu, and Thomas S. Huang. Ccnet:\nCriss-cross attention for semantic segmentation. IEEE Trans-\nactions on Pattern Analysis and Machine Intelligence, pages\n1-1, 2020. 3, 7, 8\n[31] Zihang Jiang, Qibin Hou, Li Yuan, Daquan Zhou, Xiaojie\nJin, Anran Wang, and Jiashi Feng. Token labeling: Training\na 85.5% top-1 accuracy vision transformer with 56m param-\neters on imagenet. arXiv preprint arXiv:2104.10858, 2021.\n2\n[32] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and\nFran�ois Fleuret. Transformers are rnns: Fast autoregressive\ntransformers with linear attention. In International Confer-\nence on Machine Learning, pages 5156-5165. PMLR, 2020.\n3\n[33] Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr\nDollar. Panoptic feature pyramid networks. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 6399-6408, 2019. 7, 12\n[34] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.\nReformer: The efficient transformer. arXiv preprint\narXiv:2001.04451, 2020. 3\n[35] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Im-\nagenet classification with deep convolutional neural networks.\nAdvances in neural information processing systems, 25:1097-\n1105, 2012. 2\n[36] Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, and\nLuc Van Gool. Localvit: Bringing locality to vision trans-\nformers, 2021. 13\n[37] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Dollar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nEuropean conference on computer vision, pages 740-755.\nSpringer, 2014. 5\n[38] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. arXiv\npreprint arXiv:2103.14030, 2021. 1, 2, 3, 5, 6, 7, 8, 12, 13\n[39] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization, 2019. 12\n[40] Boris T Polyak and Anatoli B Juditsky. Acceleration of\nstochastic approximation by averaging. SIAM journal on\ncontrol and optimization, 30(4):838-855, 1992. 12\n[41] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaim-\ning He, and Piotr Dollar. Designing network design spaces,\n2020. 13"
        },
        {
            "bounding_box": [
                {
                    "x": 1275,
                    "y": 304
                },
                {
                    "x": 2291,
                    "y": 304
                },
                {
                    "x": 2291,
                    "y": 2975
                },
                {
                    "x": 1275,
                    "y": 2975
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='108' style='font-size:14px'>[42] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and<br>Timothy P Lillicrap. Compressive transformers for long-range<br>sequence modelling. arXiv preprint arXiv:1911.05507, 2019.<br>3<br>[43] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan<br>Bello, Anselm Levskaya, and Jonathon Shlens. Stand-<br>alone self-attention in vision models. arXiv preprint<br>arXiv: 1906.05909, 2019. 3, 8<br>[44] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David<br>Grangier. Efficient content-based sparse attention with routing<br>transformers. Transactions of the Association for Computa-<br>tional Linguistics, 9:53-68, 2021. 3<br>[45] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-<br>moginov, and Liang-Chieh Chen. Mobilenetv2: Inverted<br>residuals and linear bottlenecks. In Proceedings of the IEEE<br>conference on computer vision and pattern recognition, pages<br>4510-4520, 2018. 2<br>[46] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-<br>attention with relative position representations. arXiv preprint<br>arXiv:1803.02155, 2018. 2, 3, 4, 8<br>[47] Karen Simonyan and Andrew Zisserman. Very deep convo-<br>lutional networks for large-scale image recognition. arXiv<br>preprint arXiv:1 409.1556, 2014. 2<br>[48] Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia<br>Schmid. Segmenter: Transformer for semantic segmentation.<br>arXiv preprint arXiv:2105.05633, 2021. 2<br>[49] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep high-<br>resolution representation learning for human pose estimation.<br>In Proceedings of the IEEE/CVF Conference on Computer<br>Vision and Pattern Recognition, pages 5693-5703, 2019. 2<br>[50] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,<br>Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent<br>Vanhoucke, and Andrew Rabinovich. Going deeper with<br>convolutions. In Proceedings of the IEEE conference on<br>computer vision and pattern recognition, pages 1-9, 2015. 2<br>[51] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model<br>scaling for convolutional neural networks. In International<br>Conference on Machine Learning, pages 6105-6114. PMLR,<br>2019. 2, 5, 6, 13<br>[52] Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng<br>Juan. Sparse sinkhorn attention. In International Conference<br>on Machine Learning, pages 9438-9447. PMLR, 2020. 3<br>[53] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco<br>Massa, Alexandre Sablayrolles, and Herve Jegou. Training<br>data-efficient image transformers & distillation through atten-<br>3, 5, 12,<br>tion. arXiv preprint arXiv:2012.12877, 2020. 1, 2,<br>13<br>[54] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles,<br>Gabriel Synnaeve, and Herve Jegou. Going deeper with<br>image transformers. arXiv preprint arXiv:2103.17239, 2021.<br>2<br>[55] Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki<br>Parmar, Blake Hechtman, and Jonathon Shlens. Scaling local<br>self-attention for parameter efficient visual backbones. arXiv<br>preprint arXiv:2103.12731, 2021. 1, 3<br>[56] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-<br>reit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Il-</p>",
            "id": 108,
            "page": 10,
            "text": "[42] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and\nTimothy P Lillicrap. Compressive transformers for long-range\nsequence modelling. arXiv preprint arXiv:1911.05507, 2019.\n3\n[43] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan\nBello, Anselm Levskaya, and Jonathon Shlens. Stand-\nalone self-attention in vision models. arXiv preprint\narXiv: 1906.05909, 2019. 3, 8\n[44] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David\nGrangier. Efficient content-based sparse attention with routing\ntransformers. Transactions of the Association for Computa-\ntional Linguistics, 9:53-68, 2021. 3\n[45] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-\nmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted\nresiduals and linear bottlenecks. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages\n4510-4520, 2018. 2\n[46] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-\nattention with relative position representations. arXiv preprint\narXiv:1803.02155, 2018. 2, 3, 4, 8\n[47] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. arXiv\npreprint arXiv:1 409.1556, 2014. 2\n[48] Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia\nSchmid. Segmenter: Transformer for semantic segmentation.\narXiv preprint arXiv:2105.05633, 2021. 2\n[49] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep high-\nresolution representation learning for human pose estimation.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 5693-5703, 2019. 2\n[50] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\nVanhoucke, and Andrew Rabinovich. Going deeper with\nconvolutions. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 1-9, 2015. 2\n[51] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model\nscaling for convolutional neural networks. In International\nConference on Machine Learning, pages 6105-6114. PMLR,\n2019. 2, 5, 6, 13\n[52] Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng\nJuan. Sparse sinkhorn attention. In International Conference\non Machine Learning, pages 9438-9447. PMLR, 2020. 3\n[53] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herve Jegou. Training\ndata-efficient image transformers & distillation through atten-\n3, 5, 12,\ntion. arXiv preprint arXiv:2012.12877, 2020. 1, 2,\n13\n[54] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles,\nGabriel Synnaeve, and Herve Jegou. Going deeper with\nimage transformers. arXiv preprint arXiv:2103.17239, 2021.\n2\n[55] Ashish Vaswani, Prajit Ramachandran, Aravind Srinivas, Niki\nParmar, Blake Hechtman, and Jonathon Shlens. Scaling local\nself-attention for parameter efficient visual backbones. arXiv\npreprint arXiv:2103.12731, 2021. 1, 3\n[56] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Il-"
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 309
                },
                {
                    "x": 1201,
                    "y": 309
                },
                {
                    "x": 1201,
                    "y": 395
                },
                {
                    "x": 286,
                    "y": 395
                }
            ],
            "category": "paragraph",
            "html": "<p id='109' style='font-size:14px'>lia Polosukhin. Attention is all you need. arXiv preprint<br>arXiv:1706.03762, 2017. 2, 3, 4</p>",
            "id": 109,
            "page": 11,
            "text": "lia Polosukhin. Attention is all you need. arXiv preprint\narXiv:1706.03762, 2017. 2, 3, 4"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 390
                },
                {
                    "x": 1203,
                    "y": 390
                },
                {
                    "x": 1203,
                    "y": 2972
                },
                {
                    "x": 202,
                    "y": 2972
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='110' style='font-size:18px'>[57] Ziyu Wan, Jingbo Zhang, Dongdong Chen, and Jing Liao.<br>High-fidelity pluralistic image completion with transformers.<br>arXiv preprint arXiv:2103.14031, 2021. 2<br>[58] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao<br>Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyra-<br>mid vision transformer: A versatile backbone for dense predic-<br>tion without convolutions. arXiv preprint arXiv:2102.12122,<br>2021. 2, 5, 6, 7, 12, 13<br>[59] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen,<br>Baoshan Cheng, Hao Shen, and Huaxia Xia. End-to-end<br>video instance segmentation with transformers. arXiv preprint<br>arXiv:2011.14503, 2020. 2<br>[60] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang<br>Dai, Lu Yuan, and Lei Zhang. Cvt: Introducing convolutions<br>to vision transformers. arXiv preprint arXiv:2103.15808,<br>2021. 1, 2, 3, 5, 13<br>[61] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and<br>Jian Sun. Unified perceptual parsing for scene understanding.<br>In Proceedings of the European Conference on Computer<br>Vision (ECCV), pages 418-434, 2018. 7, 12<br>[62] Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and<br>Kaiming He. Aggregated residual transformations for deep<br>neural networks. In Proceedings of the IEEE conference on<br>computer vision and pattern recognition, pages 1492-1500,<br>2017. 6<br>[63] Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and<br>Kaiming He. Aggregated residual transformations for deep<br>neural networks, 2017. 6, 7<br>[64] Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. Co-<br>scale conv-attentional image transformers. arXiv preprint<br>arXiv:2104.06399, 2021. 2, 13<br>[65] Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei<br>Yu, and Wei Wu. Incorporating convolution designs into<br>visual transformers. arXiv preprint arXiv:2103.11816, 2021.<br>2<br>[66] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,<br>Zihang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng<br>Yan. Tokens-to-token vit: Training vision transformers from<br>scratch on imagenet. arXiv preprint arXiv:2101.11986, 2021.<br>2, 3, 5, 13<br>[67] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk<br>Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu-<br>larization strategy to train strong classifiers with localizable<br>features, 2019. 12<br>[68] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and<br>David Lopez-Paz. mixup: Beyond empirical risk minimiza-<br>tion, 2018. 12<br>[69] Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu<br>Yuan, Lei Zhang, and Jianfeng Gao. Multi-scale vision long-<br>former: A new vision transformer for high-resolution image<br>encoding. arXiv preprint arXiv:2103.15358, 2021. 2, 6, 13<br>[70] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu,<br>Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xi-<br>ang, Philip HS Torr, et al. Rethinking semantic segmentation<br>from a sequence-to-sequence perspective with transformers.<br>arXiv preprint arXiv:2012.15840, 2020. 2</p>",
            "id": 110,
            "page": 11,
            "text": "[57] Ziyu Wan, Jingbo Zhang, Dongdong Chen, and Jing Liao.\nHigh-fidelity pluralistic image completion with transformers.\narXiv preprint arXiv:2103.14031, 2021. 2\n[58] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyra-\nmid vision transformer: A versatile backbone for dense predic-\ntion without convolutions. arXiv preprint arXiv:2102.12122,\n2021. 2, 5, 6, 7, 12, 13\n[59] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen,\nBaoshan Cheng, Hao Shen, and Huaxia Xia. End-to-end\nvideo instance segmentation with transformers. arXiv preprint\narXiv:2011.14503, 2020. 2\n[60] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang\nDai, Lu Yuan, and Lei Zhang. Cvt: Introducing convolutions\nto vision transformers. arXiv preprint arXiv:2103.15808,\n2021. 1, 2, 3, 5, 13\n[61] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and\nJian Sun. Unified perceptual parsing for scene understanding.\nIn Proceedings of the European Conference on Computer\nVision (ECCV), pages 418-434, 2018. 7, 12\n[62] Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and\nKaiming He. Aggregated residual transformations for deep\nneural networks. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 1492-1500,\n2017. 6\n[63] Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and\nKaiming He. Aggregated residual transformations for deep\nneural networks, 2017. 6, 7\n[64] Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. Co-\nscale conv-attentional image transformers. arXiv preprint\narXiv:2104.06399, 2021. 2, 13\n[65] Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei\nYu, and Wei Wu. Incorporating convolution designs into\nvisual transformers. arXiv preprint arXiv:2103.11816, 2021.\n2\n[66] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\nZihang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng\nYan. Tokens-to-token vit: Training vision transformers from\nscratch on imagenet. arXiv preprint arXiv:2101.11986, 2021.\n2, 3, 5, 13\n[67] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk\nChun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu-\nlarization strategy to train strong classifiers with localizable\nfeatures, 2019. 12\n[68] Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and\nDavid Lopez-Paz. mixup: Beyond empirical risk minimiza-\ntion, 2018. 12\n[69] Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu\nYuan, Lei Zhang, and Jianfeng Gao. Multi-scale vision long-\nformer: A new vision transformer for high-resolution image\nencoding. arXiv preprint arXiv:2103.15358, 2021. 2, 6, 13\n[70] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu,\nZekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xi-\nang, Philip HS Torr, et al. Rethinking semantic segmentation\nfrom a sequence-to-sequence perspective with transformers.\narXiv preprint arXiv:2012.15840, 2020. 2"
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 303
                },
                {
                    "x": 2284,
                    "y": 303
                },
                {
                    "x": 2284,
                    "y": 776
                },
                {
                    "x": 1281,
                    "y": 776
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='111' style='font-size:18px'>[71] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and<br>Yi Yang. Random erasing data augmentation, 2017. 12<br>[72] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Bar-<br>riuso, and Antonio Torralba. Scene parsing through ade20k<br>dataset. In Proceedings of the IEEE conference on computer<br>vision and pattern recognition, pages 633-641, 2017. 5, 7<br>[73] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang<br>Wang, and Jifeng Dai. Deformable detr: Deformable trans-<br>formers for end-to-end object detection. arXiv preprint<br>arXiv:2010.04159, 2020. 2</p>",
            "id": 111,
            "page": 11,
            "text": "[71] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and\nYi Yang. Random erasing data augmentation, 2017. 12\n[72] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Bar-\nriuso, and Antonio Torralba. Scene parsing through ade20k\ndataset. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 633-641, 2017. 5, 7\n[73] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang\nWang, and Jifeng Dai. Deformable detr: Deformable trans-\nformers for end-to-end object detection. arXiv preprint\narXiv:2010.04159, 2020. 2"
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 301
                },
                {
                    "x": 624,
                    "y": 301
                },
                {
                    "x": 624,
                    "y": 352
                },
                {
                    "x": 204,
                    "y": 352
                }
            ],
            "category": "paragraph",
            "html": "<p id='112' style='font-size:22px'>Experiment Details</p>",
            "id": 112,
            "page": 12,
            "text": "Experiment Details"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 387
                },
                {
                    "x": 1198,
                    "y": 387
                },
                {
                    "x": 1198,
                    "y": 480
                },
                {
                    "x": 201,
                    "y": 480
                }
            ],
            "category": "paragraph",
            "html": "<p id='113' style='font-size:16px'>In this section, we provide more detailed experimental<br>settings about ImageNet and downstream tasks.</p>",
            "id": 113,
            "page": 12,
            "text": "In this section, we provide more detailed experimental\nsettings about ImageNet and downstream tasks."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 481
                },
                {
                    "x": 1201,
                    "y": 481
                },
                {
                    "x": 1201,
                    "y": 1181
                },
                {
                    "x": 200,
                    "y": 1181
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='114' style='font-size:18px'>ImageNet-1K Classification. For a fair comparison, we<br>follow the training strategy in DeiT [53]. Specifically, all<br>our models are trained for 300 epochs with the input size<br>of 224 x 224. We use the AdamW optimizer with weight<br>decay of 0.05 for CSWin-T/S and 0.1 for CSWin-B. The<br>default batch size and initial learning rate are set to 2048 and<br>2e - 3 respectively, and the cosine learning rate scheduler<br>with 20 epochs linear warm-up is used. We adopt most of the<br>augmentation in [53], including RandAugment [15] (rand-<br>m9-mstd0.5-inc1) , Mixup [68] (prob = 0.8), CutMix [67]<br>(prob = 1.0), Random Erasing [71] (prob = 0.25) and Ex-<br>ponential Moving Average [40] (ema-decay = 0.99984),<br>increasing stochastic depth [29] (prob = 0.2, 0.4, 0.5 for<br>CSWin-T, CSWin-S, and CSWin-B respectively).</p>",
            "id": 114,
            "page": 12,
            "text": "ImageNet-1K Classification. For a fair comparison, we\nfollow the training strategy in DeiT [53]. Specifically, all\nour models are trained for 300 epochs with the input size\nof 224 x 224. We use the AdamW optimizer with weight\ndecay of 0.05 for CSWin-T/S and 0.1 for CSWin-B. The\ndefault batch size and initial learning rate are set to 2048 and\n2e - 3 respectively, and the cosine learning rate scheduler\nwith 20 epochs linear warm-up is used. We adopt most of the\naugmentation in [53], including RandAugment [15] (rand-\nm9-mstd0.5-inc1) , Mixup [68] (prob = 0.8), CutMix [67]\n(prob = 1.0), Random Erasing [71] (prob = 0.25) and Ex-\nponential Moving Average [40] (ema-decay = 0.99984),\nincreasing stochastic depth [29] (prob = 0.2, 0.4, 0.5 for\nCSWin-T, CSWin-S, and CSWin-B respectively)."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1183
                },
                {
                    "x": 1202,
                    "y": 1183
                },
                {
                    "x": 1202,
                    "y": 1475
                },
                {
                    "x": 201,
                    "y": 1475
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='115' style='font-size:16px'>When fine-tuning with 384 x 384 input, we follow the<br>setting in [38] that fine-tune the models for 30 epochs with<br>the weight decay of 1e-8, learning rate of 5e-6, batch size<br>of 256. We notice that a large ratio of stochastic depth is<br>beneficial for fine-tuning and keeping it the same as the<br>training stage.</p>",
            "id": 115,
            "page": 12,
            "text": "When fine-tuning with 384 x 384 input, we follow the\nsetting in [38] that fine-tune the models for 30 epochs with\nthe weight decay of 1e-8, learning rate of 5e-6, batch size\nof 256. We notice that a large ratio of stochastic depth is\nbeneficial for fine-tuning and keeping it the same as the\ntraining stage."
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 1484
                },
                {
                    "x": 1201,
                    "y": 1484
                },
                {
                    "x": 1201,
                    "y": 1528
                },
                {
                    "x": 203,
                    "y": 1528
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='116' style='font-size:18px'>COCO Object Detection and Instance Segmentation.</p>",
            "id": 116,
            "page": 12,
            "text": "COCO Object Detection and Instance Segmentation."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1529
                },
                {
                    "x": 1200,
                    "y": 1529
                },
                {
                    "x": 1200,
                    "y": 2522
                },
                {
                    "x": 201,
                    "y": 2522
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='117' style='font-size:14px'>We use two classical object detection frameworks: Mask<br>R-CNN [21] and Cascade Mask R-CNN [2] based on the<br>implementation from mmdetection [6]. For Mask R-CNN,<br>we train it with ImageNet-1K pretrained model with two set-<br>tings: 1x schedule and 3x+MS schedule. For 1x schedule,<br>we train the model with single-scale input (image is resized<br>to the shorter side of 800 pixels, while the longer side does<br>not exceed 1333 pixels) for 12 epochs. We use Adam W [39]<br>optimizer with a learning rate of 0.0001, weight decay of<br>0.05 and batch size of 16. The learning rate declines at the<br>8 and 11 epoch with decay rate 0.1. The stochastic depth<br>is also same as the ImageNet-1K setting that 0.1, 0.3, 0.5<br>for CSWin-T, CSWin-S, and CSWin-B respectively. For<br>3x+MS schedule, we train the model with multi-scale input<br>(image is resized to the shorter side between 480 and 800<br>while the longer side is no longer than 1333) for 36 epochs.<br>The other settings are same as the 1 x except we decay the<br>learning rate at epoch 27 and 33. When it comes to Cascade<br>Mask R-CNN, we use the same 3x +MS schedule as Mask<br>R-CNN.</p>",
            "id": 117,
            "page": 12,
            "text": "We use two classical object detection frameworks: Mask\nR-CNN [21] and Cascade Mask R-CNN [2] based on the\nimplementation from mmdetection [6]. For Mask R-CNN,\nwe train it with ImageNet-1K pretrained model with two set-\ntings: 1x schedule and 3x+MS schedule. For 1x schedule,\nwe train the model with single-scale input (image is resized\nto the shorter side of 800 pixels, while the longer side does\nnot exceed 1333 pixels) for 12 epochs. We use Adam W [39]\noptimizer with a learning rate of 0.0001, weight decay of\n0.05 and batch size of 16. The learning rate declines at the\n8 and 11 epoch with decay rate 0.1. The stochastic depth\nis also same as the ImageNet-1K setting that 0.1, 0.3, 0.5\nfor CSWin-T, CSWin-S, and CSWin-B respectively. For\n3x+MS schedule, we train the model with multi-scale input\n(image is resized to the shorter side between 480 and 800\nwhile the longer side is no longer than 1333) for 36 epochs.\nThe other settings are same as the 1 x except we decay the\nlearning rate at epoch 27 and 33. When it comes to Cascade\nMask R-CNN, we use the same 3x +MS schedule as Mask\nR-CNN."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2527
                },
                {
                    "x": 1200,
                    "y": 2527
                },
                {
                    "x": 1200,
                    "y": 2978
                },
                {
                    "x": 202,
                    "y": 2978
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='118' style='font-size:16px'>ADE20K Semantic segmentation. Here we consider two<br>semantic segmentation frameworks: UperNet [61] and Se-<br>mantic FPN [33] based on the implementation from mm-<br>segmentaion [14]. For UperNet, we follow the setting in<br>[38] and use AdamW [39] optimizer with initial learning<br>rate 6e-5 weight decay of 0.01 and batch size of 16 (8<br>,<br>GPUs with 2 images per GPU) for 160K iterations. The<br>learning rate warmups with 1500 iterations at the beginning<br>and decays with a linear decay strategy. We use the default</p>",
            "id": 118,
            "page": 12,
            "text": "ADE20K Semantic segmentation. Here we consider two\nsemantic segmentation frameworks: UperNet [61] and Se-\nmantic FPN [33] based on the implementation from mm-\nsegmentaion [14]. For UperNet, we follow the setting in\n[38] and use AdamW [39] optimizer with initial learning\nrate 6e-5 weight decay of 0.01 and batch size of 16 (8\n,\nGPUs with 2 images per GPU) for 160K iterations. The\nlearning rate warmups with 1500 iterations at the beginning\nand decays with a linear decay strategy. We use the default"
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 310
                },
                {
                    "x": 2280,
                    "y": 310
                },
                {
                    "x": 2280,
                    "y": 701
                },
                {
                    "x": 1278,
                    "y": 701
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='119' style='font-size:16px'>augmentation setting in mmsegmentation including random<br>horizontal flipping, random re-scaling (ratio range [0.5, 2.0])<br>and random photo-metric distortion. All the models are<br>trained with input size 512 x 512. The stochastic depth is<br>set to 0.2, 0.4, 0.6 for CSWin-T, CSWin-S, and CSWin-B<br>respectively. When it comes to testing, we report both single-<br>scale test result and multi-scale test ([0.5, 0.75, 1.0, 1.25,<br>1.5, 1.75]x of that in training).</p>",
            "id": 119,
            "page": 12,
            "text": "augmentation setting in mmsegmentation including random\nhorizontal flipping, random re-scaling (ratio range [0.5, 2.0])\nand random photo-metric distortion. All the models are\ntrained with input size 512 x 512. The stochastic depth is\nset to 0.2, 0.4, 0.6 for CSWin-T, CSWin-S, and CSWin-B\nrespectively. When it comes to testing, we report both single-\nscale test result and multi-scale test ([0.5, 0.75, 1.0, 1.25,\n1.5, 1.75]x of that in training)."
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 706
                },
                {
                    "x": 2279,
                    "y": 706
                },
                {
                    "x": 2279,
                    "y": 902
                },
                {
                    "x": 1279,
                    "y": 902
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='120' style='font-size:16px'>For Semantic FPN, we follow the setting in [58]. We<br>use AdamW [39] optimizer with initial learning rate 1e-4<br>weight decay of 1e-4 and batch size of 16 (4 GPUs with 4<br>images per GPU) for 80K iterations.</p>",
            "id": 120,
            "page": 12,
            "text": "For Semantic FPN, we follow the setting in [58]. We\nuse AdamW [39] optimizer with initial learning rate 1e-4\nweight decay of 1e-4 and batch size of 16 (4 GPUs with 4\nimages per GPU) for 80K iterations."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 949
                },
                {
                    "x": 1690,
                    "y": 949
                },
                {
                    "x": 1690,
                    "y": 1001
                },
                {
                    "x": 1280,
                    "y": 1001
                }
            ],
            "category": "paragraph",
            "html": "<p id='121' style='font-size:20px'>More Experimetns</p>",
            "id": 121,
            "page": 12,
            "text": "More Experimetns"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 1032
                },
                {
                    "x": 2281,
                    "y": 1032
                },
                {
                    "x": 2281,
                    "y": 1277
                },
                {
                    "x": 1279,
                    "y": 1277
                }
            ],
            "category": "paragraph",
            "html": "<p id='122' style='font-size:14px'>With the limitation of pages, we only compare with a few<br>classical methods in our paper, here we make a comprehen-<br>sive comparison with more current methods on ImageNet-<br>1K. We find that our CSWin performs best in concurrent<br>works.</p>",
            "id": 122,
            "page": 12,
            "text": "With the limitation of pages, we only compare with a few\nclassical methods in our paper, here we make a comprehen-\nsive comparison with more current methods on ImageNet-\n1K. We find that our CSWin performs best in concurrent\nworks."
        },
        {
            "bounding_box": [
                {
                    "x": 219,
                    "y": 1111
                },
                {
                    "x": 2271,
                    "y": 1111
                },
                {
                    "x": 2271,
                    "y": 2068
                },
                {
                    "x": 219,
                    "y": 2068
                }
            ],
            "category": "table",
            "html": "<table id='123' style='font-size:14px'><tr><td colspan=\"4\">ImageNet-1K 2242 trained models</td><td colspan=\"4\">ImageNet-1K 2242 trained models</td><td colspan=\"4\">ImageNet-1K 2242 trained models</td></tr><tr><td>Method</td><td>#Param.</td><td>FLOPs</td><td>Top-1</td><td>Method</td><td>#Param.</td><td>FLOPs</td><td>Top-1</td><td>Method</td><td>#Param.</td><td>FLOPs</td><td>Top-1</td></tr><tr><td>Reg-4G [41]</td><td>21M</td><td>4.0G</td><td>80.0</td><td>Reg-8G [41]</td><td>39M</td><td>8.0G</td><td>81.7</td><td>Reg-16G [41]</td><td>84M</td><td>16.0G</td><td>82.9</td></tr><tr><td>Eff-B4* [51]</td><td>19M</td><td>4.2G</td><td>82.9</td><td>Eff-B5* [51]</td><td>30M</td><td>9.9G</td><td>83.6</td><td>Eff-B6* [51]</td><td>43M</td><td>19.0G</td><td>84.0</td></tr><tr><td>DeiT-S [53]</td><td>22M</td><td>4.6G</td><td>79.8</td><td>PVT-M [58]</td><td>44M</td><td>6.7G</td><td>81.2</td><td>DeiT-B [53]</td><td>87M</td><td>17.5G</td><td>81.8</td></tr><tr><td>PVT-S [58]</td><td>25M</td><td>3.8G</td><td>79.8</td><td>PVT-L [58]</td><td>61M</td><td>9.8G</td><td>81.7</td><td>PiT-B [24]</td><td>74M</td><td>12.5G</td><td>82.0</td></tr><tr><td>T2T-14 [66]</td><td>22M</td><td>5.2G</td><td>81.5</td><td>T2T-19 [66]</td><td>39M</td><td>8.9G</td><td>81.9</td><td>T2T-24 [66]</td><td>64M</td><td>14.1G</td><td>82.3</td></tr><tr><td>ViL-S [69]</td><td>25M</td><td>4.9G</td><td>82.0</td><td>T2Tt-19 [66]</td><td>39M</td><td>9.8G</td><td>82.2</td><td>T2Tt-24 [66]</td><td>64M</td><td>15.0G</td><td>82.6</td></tr><tr><td>TNT-S [20]</td><td>24M</td><td>5.2G</td><td>81.3</td><td>ViL-M [69]</td><td>40M</td><td>8.7G</td><td>83.3</td><td>CPVT-B [12]</td><td>88M</td><td>17.6G</td><td>82.3</td></tr><tr><td>CViT-15 [4]</td><td>27M</td><td>5.6G</td><td>81.0</td><td>MViT-B [19]</td><td>37M</td><td>7.8G</td><td>83.0</td><td>TNT-B [20]</td><td>66M</td><td>14.1G</td><td>82.8</td></tr><tr><td>Visf-S [8]</td><td>40M</td><td>4.9G</td><td>82.3</td><td>CViT-18 [4]</td><td>43M</td><td>9.0G</td><td>82.5</td><td>ViL-B [69]</td><td>56M</td><td>13.4G</td><td>83.2</td></tr><tr><td>LViT-S [36]</td><td>22M</td><td>4.6G</td><td>80.8</td><td>CViTc-18 [4]</td><td>44M</td><td>9.5G</td><td>82.8</td><td>Twins-L [11]</td><td>99M</td><td>14.8G</td><td>83.7</td></tr><tr><td>CoaTL-S [64]</td><td>20M</td><td>4.0G</td><td>81.9</td><td>Twins-B [11]</td><td>56M</td><td>8.3G</td><td>83.2</td><td>Swin-B [38]</td><td>88M</td><td>15.4G</td><td>83.3</td></tr><tr><td>CPVT-S [12]</td><td>23M</td><td>4.6G</td><td>81.5</td><td>Swin-S [38]</td><td>50M</td><td>8.7G</td><td>83.0</td><td>CSWin-B</td><td>78M</td><td>15.0G</td><td>84.2</td></tr><tr><td>Swin-T [38]</td><td>29M</td><td>4.5G</td><td>81.3</td><td>CvT-21 [60]</td><td>32M</td><td>7.1G</td><td>82.5</td><td></td><td></td><td></td><td></td></tr><tr><td>CvT-13 [60]</td><td>20M</td><td>4.5G</td><td>81.6</td><td>CSWin-S</td><td>35M</td><td>6.9G</td><td>83.6</td><td></td><td></td><td></td><td></td></tr><tr><td>CSWin-T</td><td>23M</td><td>4.3G</td><td>82.7</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan=\"4\">ImageNet-1K 3842 finetuned models models</td><td colspan=\"4\">ImageNet-1K 3842 finetuned</td><td colspan=\"4\">ImageNet-1K 3842 finetuned models</td></tr><tr><td>CvT-13 [60]</td><td>20M</td><td>16.3G</td><td>83.0</td><td>CvT-21 [60]</td><td>32M</td><td>24.9G</td><td>83.3</td><td>ViT-B/16 [17]</td><td>86M</td><td>49.3G</td><td>77.9</td></tr><tr><td>T2T-14 [66]</td><td>22M</td><td>17.1G</td><td>83.3</td><td>CViTc-18 [4]</td><td>45M</td><td>32.4G</td><td>83.9</td><td>DeiT-B [53]</td><td>86M</td><td>55.4G</td><td>83.1</td></tr><tr><td>CViTc-15 [4]</td><td>28M</td><td>21.4G</td><td>83.5</td><td>CSWin-S</td><td>35M</td><td>22.0G</td><td>85.0</td><td>Swin-B [38]</td><td>88M</td><td>47.0G</td><td>84.2</td></tr><tr><td>CSWin-T</td><td>23M</td><td>14.0G</td><td>84.3</td><td></td><td></td><td></td><td></td><td>CSWin-B</td><td>78M</td><td>47.0G</td><td>85.4</td></tr><tr><td colspan=\"4\">(a) Tiny Model</td><td colspan=\"4\">(b) Small Model</td><td colspan=\"4\">(c) Base Model</td></tr></table>",
            "id": 123,
            "page": 13,
            "text": "ImageNet-1K 2242 trained models ImageNet-1K 2242 trained models ImageNet-1K 2242 trained models\n Method #Param. FLOPs Top-1 Method #Param. FLOPs Top-1 Method #Param. FLOPs Top-1\n Reg-4G [41] 21M 4.0G 80.0 Reg-8G [41] 39M 8.0G 81.7 Reg-16G [41] 84M 16.0G 82.9\n Eff-B4* [51] 19M 4.2G 82.9 Eff-B5* [51] 30M 9.9G 83.6 Eff-B6* [51] 43M 19.0G 84.0\n DeiT-S [53] 22M 4.6G 79.8 PVT-M [58] 44M 6.7G 81.2 DeiT-B [53] 87M 17.5G 81.8\n PVT-S [58] 25M 3.8G 79.8 PVT-L [58] 61M 9.8G 81.7 PiT-B [24] 74M 12.5G 82.0\n T2T-14 [66] 22M 5.2G 81.5 T2T-19 [66] 39M 8.9G 81.9 T2T-24 [66] 64M 14.1G 82.3\n ViL-S [69] 25M 4.9G 82.0 T2Tt-19 [66] 39M 9.8G 82.2 T2Tt-24 [66] 64M 15.0G 82.6\n TNT-S [20] 24M 5.2G 81.3 ViL-M [69] 40M 8.7G 83.3 CPVT-B [12] 88M 17.6G 82.3\n CViT-15 [4] 27M 5.6G 81.0 MViT-B [19] 37M 7.8G 83.0 TNT-B [20] 66M 14.1G 82.8\n Visf-S [8] 40M 4.9G 82.3 CViT-18 [4] 43M 9.0G 82.5 ViL-B [69] 56M 13.4G 83.2\n LViT-S [36] 22M 4.6G 80.8 CViTc-18 [4] 44M 9.5G 82.8 Twins-L [11] 99M 14.8G 83.7\n CoaTL-S [64] 20M 4.0G 81.9 Twins-B [11] 56M 8.3G 83.2 Swin-B [38] 88M 15.4G 83.3\n CPVT-S [12] 23M 4.6G 81.5 Swin-S [38] 50M 8.7G 83.0 CSWin-B 78M 15.0G 84.2\n Swin-T [38] 29M 4.5G 81.3 CvT-21 [60] 32M 7.1G 82.5    \n CvT-13 [60] 20M 4.5G 81.6 CSWin-S 35M 6.9G 83.6    \n CSWin-T 23M 4.3G 82.7        \n ImageNet-1K 3842 finetuned models models ImageNet-1K 3842 finetuned ImageNet-1K 3842 finetuned models\n CvT-13 [60] 20M 16.3G 83.0 CvT-21 [60] 32M 24.9G 83.3 ViT-B/16 [17] 86M 49.3G 77.9\n T2T-14 [66] 22M 17.1G 83.3 CViTc-18 [4] 45M 32.4G 83.9 DeiT-B [53] 86M 55.4G 83.1\n CViTc-15 [4] 28M 21.4G 83.5 CSWin-S 35M 22.0G 85.0 Swin-B [38] 88M 47.0G 84.2\n CSWin-T 23M 14.0G 84.3     CSWin-B 78M 47.0G 85.4\n (a) Tiny Model (b) Small Model"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2099
                },
                {
                    "x": 2278,
                    "y": 2099
                },
                {
                    "x": 2278,
                    "y": 2195
                },
                {
                    "x": 202,
                    "y": 2195
                }
            ],
            "category": "caption",
            "html": "<caption id='124' style='font-size:18px'>Table 11. Comparison of different models on ImageNet-1K classification. * means the EfficientNet are trained with other input sizes. Here<br>the models are grouped based on the computation complexity.</caption>",
            "id": 124,
            "page": 13,
            "text": "Table 11. Comparison of different models on ImageNet-1K classification. * means the EfficientNet are trained with other input sizes. Here\nthe models are grouped based on the computation complexity."
        }
    ]
}