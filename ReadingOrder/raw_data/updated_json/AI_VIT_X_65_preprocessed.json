{
    "id": "3299d448-0f94-11ef-8828-426932df3dcf",
    "pdf_path": "/root/data/pdf/1909.09586v1.pdf",
    "elements": [
        {
            "bounding_box": [
                {
                    "x": 636,
                    "y": 677
                },
                {
                    "x": 1831,
                    "y": 677
                },
                {
                    "x": 1831,
                    "y": 937
                },
                {
                    "x": 636,
                    "y": 937
                }
            ],
            "category": "paragraph",
            "html": "<p id='0' style='font-size:22px'>- Understanding LSTM -<br>a tutorial into Long Short- Term Memory<br>Recurrent Neural Networks</p>",
            "id": 0,
            "page": 1,
            "text": "- Understanding LSTM a tutorial into Long Short- Term Memory Recurrent Neural Networks"
        },
        {
            "bounding_box": [
                {
                    "x": 972,
                    "y": 998
                },
                {
                    "x": 1497,
                    "y": 998
                },
                {
                    "x": 1497,
                    "y": 1055
                },
                {
                    "x": 972,
                    "y": 1055
                }
            ],
            "category": "paragraph",
            "html": "<p id='1' style='font-size:20px'>Ralf C. Staudemeyer</p>",
            "id": 1,
            "page": 1,
            "text": "Ralf C. Staudemeyer"
        },
        {
            "bounding_box": [
                {
                    "x": 639,
                    "y": 1058
                },
                {
                    "x": 1826,
                    "y": 1058
                },
                {
                    "x": 1826,
                    "y": 1228
                },
                {
                    "x": 639,
                    "y": 1228
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='2' style='font-size:20px'>Faculty of Computer Science<br>Schmalkalden University of Applied Sciences, Germany<br>E-Mail: r.staudemeyer@hs-sm.de</p>",
            "id": 2,
            "page": 1,
            "text": "Faculty of Computer Science Schmalkalden University of Applied Sciences, Germany E-Mail: r.staudemeyer@hs-sm.de"
        },
        {
            "bounding_box": [
                {
                    "x": 958,
                    "y": 1258
                },
                {
                    "x": 1512,
                    "y": 1258
                },
                {
                    "x": 1512,
                    "y": 1310
                },
                {
                    "x": 958,
                    "y": 1310
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='3' style='font-size:18px'>Eric Rothstein Morris</p>",
            "id": 3,
            "page": 1,
            "text": "Eric Rothstein Morris"
        },
        {
            "bounding_box": [
                {
                    "x": 595,
                    "y": 1317
                },
                {
                    "x": 1875,
                    "y": 1317
                },
                {
                    "x": 1875,
                    "y": 1427
                },
                {
                    "x": 595,
                    "y": 1427
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='4' style='font-size:22px'>(Singapore University of Technology and Design, Singapore<br>E-Mail: eric_rothstein@sutd.edu.sg)</p>",
            "id": 4,
            "page": 1,
            "text": "(Singapore University of Technology and Design, Singapore E-Mail: eric_rothstein@sutd.edu.sg)"
        },
        {
            "bounding_box": [
                {
                    "x": 1019,
                    "y": 1472
                },
                {
                    "x": 1450,
                    "y": 1472
                },
                {
                    "x": 1450,
                    "y": 1525
                },
                {
                    "x": 1019,
                    "y": 1525
                }
            ],
            "category": "paragraph",
            "html": "<p id='5' style='font-size:20px'>September 23, 2019</p>",
            "id": 5,
            "page": 1,
            "text": "September 23, 2019"
        },
        {
            "bounding_box": [
                {
                    "x": 1146,
                    "y": 1624
                },
                {
                    "x": 1323,
                    "y": 1624
                },
                {
                    "x": 1323,
                    "y": 1666
                },
                {
                    "x": 1146,
                    "y": 1666
                }
            ],
            "category": "paragraph",
            "html": "<p id='6' style='font-size:16px'>Abstract</p>",
            "id": 6,
            "page": 1,
            "text": "Abstract"
        },
        {
            "bounding_box": [
                {
                    "x": 617,
                    "y": 1683
                },
                {
                    "x": 1858,
                    "y": 1683
                },
                {
                    "x": 1858,
                    "y": 2093
                },
                {
                    "x": 617,
                    "y": 2093
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='7' style='font-size:14px'>Long Short-Term Memory Recurrent Neural Networks (LSTM-RNN)<br>are one of the most powerful dynamic classifiers publicly known. The net-<br>work itself and the related learning algorithms are reasonably well docu-<br>mented to get an idea how it works. This paper will shed more light into<br>understanding how LSTM-RNNs evolved and why they work impressively<br>well, focusing on the early, ground-breaking publications. We significantly<br>improved documentation and fixed a number of errors and inconsistencies<br>that accumulated in previous publications. To support understanding we<br>as well revised and unified the notation used.</p>",
            "id": 7,
            "page": 1,
            "text": "Long Short-Term Memory Recurrent Neural Networks (LSTM-RNN) are one of the most powerful dynamic classifiers publicly known. The network itself and the related learning algorithms are reasonably well documented to get an idea how it works. This paper will shed more light into understanding how LSTM-RNNs evolved and why they work impressively well, focusing on the early, ground-breaking publications. We significantly improved documentation and fixed a number of errors and inconsistencies that accumulated in previous publications. To support understanding we as well revised and unified the notation used."
        },
        {
            "bounding_box": [
                {
                    "x": 515,
                    "y": 2169
                },
                {
                    "x": 995,
                    "y": 2169
                },
                {
                    "x": 995,
                    "y": 2233
                },
                {
                    "x": 515,
                    "y": 2233
                }
            ],
            "category": "paragraph",
            "html": "<p id='8' style='font-size:20px'>1 Introduction</p>",
            "id": 8,
            "page": 1,
            "text": "1 Introduction"
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 2271
                },
                {
                    "x": 1961,
                    "y": 2271
                },
                {
                    "x": 1961,
                    "y": 2669
                },
                {
                    "x": 511,
                    "y": 2669
                }
            ],
            "category": "paragraph",
            "html": "<p id='9' style='font-size:16px'>This article is an tutorial-like introduction initially developed as supplementary<br>material for lectures focused on Artificial Intelligence. The interested reader<br>can deepen his /her knowledge by understanding Long Short- Term Memory Re-<br>current Neural Networks (LSTM-RNN) considering its evolution since the early<br>nineties. Todays publications on LSTM-RNN use a slightly different notation<br>and a much more summarized representation of the derivations. Nevertheless<br>the authors found the presented approach very helpful and we are confident this<br>publication will find its audience.</p>",
            "id": 9,
            "page": 1,
            "text": "This article is an tutorial-like introduction initially developed as supplementary material for lectures focused on Artificial Intelligence. The interested reader can deepen his /her knowledge by understanding Long Short- Term Memory Recurrent Neural Networks (LSTM-RNN) considering its evolution since the early nineties. Todays publications on LSTM-RNN use a slightly different notation and a much more summarized representation of the derivations. Nevertheless the authors found the presented approach very helpful and we are confident this publication will find its audience."
        },
        {
            "bounding_box": [
                {
                    "x": 513,
                    "y": 2672
                },
                {
                    "x": 1960,
                    "y": 2672
                },
                {
                    "x": 1960,
                    "y": 2918
                },
                {
                    "x": 513,
                    "y": 2918
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='10' style='font-size:16px'>Machine learning is concerned with the development of algorithms that au-<br>tomatically improve by practice. Ideally, the more the learning algorithm is run,<br>the better the algorithm becomes. It is the task of the learning algorithm to<br>create a classifier function from the training data presented. The performance<br>of this built classifier is then measured by applying it to previously unseen data.</p>",
            "id": 10,
            "page": 1,
            "text": "Machine learning is concerned with the development of algorithms that automatically improve by practice. Ideally, the more the learning algorithm is run, the better the algorithm becomes. It is the task of the learning algorithm to create a classifier function from the training data presented. The performance of this built classifier is then measured by applying it to previously unseen data."
        },
        {
            "bounding_box": [
                {
                    "x": 515,
                    "y": 2923
                },
                {
                    "x": 1960,
                    "y": 2923
                },
                {
                    "x": 1960,
                    "y": 3019
                },
                {
                    "x": 515,
                    "y": 3019
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='11' style='font-size:18px'>Artificial Neural Networks (ANN) are inspired by biological learning sys-<br>tems and loosely model their basic functions. Biological learning systems are</p>",
            "id": 11,
            "page": 1,
            "text": "Artificial Neural Networks (ANN) are inspired by biological learning systems and loosely model their basic functions. Biological learning systems are"
        },
        {
            "bounding_box": [
                {
                    "x": 60,
                    "y": 1097
                },
                {
                    "x": 145,
                    "y": 1097
                },
                {
                    "x": 145,
                    "y": 2533
                },
                {
                    "x": 60,
                    "y": 2533
                }
            ],
            "category": "footer",
            "html": "<br><footer id='12' style='font-size:14px'>2019<br>Sep<br>12<br>[cs.NE]<br>arXiv:1909.09586v1</footer>",
            "id": 12,
            "page": 1,
            "text": "2019 Sep 12 [cs.NE] arXiv:1909.09586v1"
        },
        {
            "bounding_box": [
                {
                    "x": 1221,
                    "y": 3098
                },
                {
                    "x": 1248,
                    "y": 3098
                },
                {
                    "x": 1248,
                    "y": 3138
                },
                {
                    "x": 1221,
                    "y": 3138
                }
            ],
            "category": "footer",
            "html": "<footer id='13' style='font-size:14px'>1</footer>",
            "id": 13,
            "page": 1,
            "text": "1"
        },
        {
            "bounding_box": [
                {
                    "x": 508,
                    "y": 528
                },
                {
                    "x": 1961,
                    "y": 528
                },
                {
                    "x": 1961,
                    "y": 924
                },
                {
                    "x": 508,
                    "y": 924
                }
            ],
            "category": "paragraph",
            "html": "<p id='14' style='font-size:16px'>complex webs of interconnected neurons. Neurons are simple units accepting<br>a vector of real-valued inputs and producing a single real-valued output. The<br>most common standard neural network type are feed-forward neural networks.<br>Here sets of neurons are organised in layers: one input layer, one output layer,<br>and at least one intermediate hidden layer. Feed-forward neural networks are<br>limited to static classification tasks. Therefore, they are limited to provide a<br>static mapping between input and output. To model time prediction tasks we<br>need a so-called dynamic classifier.</p>",
            "id": 14,
            "page": 2,
            "text": "complex webs of interconnected neurons. Neurons are simple units accepting a vector of real-valued inputs and producing a single real-valued output. The most common standard neural network type are feed-forward neural networks. Here sets of neurons are organised in layers: one input layer, one output layer, and at least one intermediate hidden layer. Feed-forward neural networks are limited to static classification tasks. Therefore, they are limited to provide a static mapping between input and output. To model time prediction tasks we need a so-called dynamic classifier."
        },
        {
            "bounding_box": [
                {
                    "x": 509,
                    "y": 928
                },
                {
                    "x": 1960,
                    "y": 928
                },
                {
                    "x": 1960,
                    "y": 1425
                },
                {
                    "x": 509,
                    "y": 1425
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='15' style='font-size:20px'>We can extend feed-forward neural networks towards dynamic classification.<br>To gain this property we need to feed signals from previous timesteps back into<br>the network. These networks with recurrent connections are called Recurrent<br>Neural Networks (RNN) [74], [75]. RNNs are limited to look back in time for<br>approximately ten timesteps [38], [56]. This is due to the fed back signal is<br>either vanishing or exploding. This issue was addressed with Long Short-Term<br>Memory Recurrent Neural Networks (LSTM-RNN) [22], [41], [23], [60]. LSTM<br>networks are to a certain extend biologically plausible [58] and capable to learn<br>more than 1,000 timesteps, depending on the complexity of the built network<br>[41].</p>",
            "id": 15,
            "page": 2,
            "text": "We can extend feed-forward neural networks towards dynamic classification. To gain this property we need to feed signals from previous timesteps back into the network. These networks with recurrent connections are called Recurrent Neural Networks (RNN) , . RNNs are limited to look back in time for approximately ten timesteps , . This is due to the fed back signal is either vanishing or exploding. This issue was addressed with Long Short-Term Memory Recurrent Neural Networks (LSTM-RNN) , , , . LSTM networks are to a certain extend biologically plausible  and capable to learn more than 1,000 timesteps, depending on the complexity of the built network ."
        },
        {
            "bounding_box": [
                {
                    "x": 509,
                    "y": 1427
                },
                {
                    "x": 1961,
                    "y": 1427
                },
                {
                    "x": 1961,
                    "y": 1672
                },
                {
                    "x": 509,
                    "y": 1672
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='16' style='font-size:18px'>In the early, ground-breaking papers by Hochreiter [41] and Graves [34], the<br>authors used different notations which made further development prone to errors<br>and inconvenient to follow. To address this we developed a unified notation and<br>did draw descriptive figures to support the interested reader in understanding<br>the related equations of the early publications.</p>",
            "id": 16,
            "page": 2,
            "text": "In the early, ground-breaking papers by Hochreiter  and Graves , the authors used different notations which made further development prone to errors and inconvenient to follow. To address this we developed a unified notation and did draw descriptive figures to support the interested reader in understanding the related equations of the early publications."
        },
        {
            "bounding_box": [
                {
                    "x": 510,
                    "y": 1677
                },
                {
                    "x": 1960,
                    "y": 1677
                },
                {
                    "x": 1960,
                    "y": 2070
                },
                {
                    "x": 510,
                    "y": 2070
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='17' style='font-size:16px'>In the following, we slowly dive into the world of neural networks and specifi-<br>cally LSTM-RNNs with a selection of its most promising extensions documented<br>SO far. We successively explain how neural networks evolved from a single per-<br>ceptron to something as powerful as LSTM. This includes vanilla LSTM, al-<br>though not used in practice anymore, as the fundamental evolutionary step.<br>With this article, we support beginners in the machine learning community to<br>understand how LSTM works with the intention motivate its further develop-<br>ment.</p>",
            "id": 17,
            "page": 2,
            "text": "In the following, we slowly dive into the world of neural networks and specifically LSTM-RNNs with a selection of its most promising extensions documented SO far. We successively explain how neural networks evolved from a single perceptron to something as powerful as LSTM. This includes vanilla LSTM, although not used in practice anymore, as the fundamental evolutionary step. With this article, we support beginners in the machine learning community to understand how LSTM works with the intention motivate its further development."
        },
        {
            "bounding_box": [
                {
                    "x": 512,
                    "y": 2075
                },
                {
                    "x": 1957,
                    "y": 2075
                },
                {
                    "x": 1957,
                    "y": 2171
                },
                {
                    "x": 512,
                    "y": 2171
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='18' style='font-size:14px'>This is the first document that covers LSTM and its extensions in such great<br>detail.</p>",
            "id": 18,
            "page": 2,
            "text": "This is the first document that covers LSTM and its extensions in such great detail."
        },
        {
            "bounding_box": [
                {
                    "x": 515,
                    "y": 2249
                },
                {
                    "x": 886,
                    "y": 2249
                },
                {
                    "x": 886,
                    "y": 2309
                },
                {
                    "x": 515,
                    "y": 2309
                }
            ],
            "category": "paragraph",
            "html": "<p id='19' style='font-size:22px'>2 Notation</p>",
            "id": 19,
            "page": 2,
            "text": "2 Notation"
        },
        {
            "bounding_box": [
                {
                    "x": 513,
                    "y": 2351
                },
                {
                    "x": 1328,
                    "y": 2351
                },
                {
                    "x": 1328,
                    "y": 2399
                },
                {
                    "x": 513,
                    "y": 2399
                }
            ],
            "category": "paragraph",
            "html": "<p id='20' style='font-size:14px'>In this article we use the following notation:</p>",
            "id": 20,
            "page": 2,
            "text": "In this article we use the following notation:"
        },
        {
            "bounding_box": [
                {
                    "x": 574,
                    "y": 2446
                },
                {
                    "x": 1957,
                    "y": 2446
                },
                {
                    "x": 1957,
                    "y": 3002
                },
                {
                    "x": 574,
                    "y": 3002
                }
            ],
            "category": "paragraph",
            "html": "<p id='21' style='font-size:14px'>● The learning rate of the network is 7.<br>● A time unit is T. Initial times of an epoch are denoted by t' and final<br>times by t.<br>● The set of units of the network is N, with generic (unless stated otherwise)<br>units u, v,l,k E N.<br>● The set of input units is I, with input unit i E I.<br>● The set of output units is 0, with output unit 0 E 0.<br>● The set of non-input units is U.</p>",
            "id": 21,
            "page": 2,
            "text": "● The learning rate of the network is 7. ● A time unit is T. Initial times of an epoch are denoted by t' and final times by t. ● The set of units of the network is N, with generic (unless stated otherwise) units u, v,l,k E N. ● The set of input units is I, with input unit i E I. ● The set of output units is 0, with output unit 0 E 0. ● The set of non-input units is U."
        },
        {
            "bounding_box": [
                {
                    "x": 1221,
                    "y": 3098
                },
                {
                    "x": 1250,
                    "y": 3098
                },
                {
                    "x": 1250,
                    "y": 3136
                },
                {
                    "x": 1221,
                    "y": 3136
                }
            ],
            "category": "footer",
            "html": "<footer id='22' style='font-size:14px'>2</footer>",
            "id": 22,
            "page": 2,
            "text": "2"
        },
        {
            "bounding_box": [
                {
                    "x": 570,
                    "y": 522
                },
                {
                    "x": 1965,
                    "y": 522
                },
                {
                    "x": 1965,
                    "y": 1631
                },
                {
                    "x": 570,
                    "y": 1631
                }
            ],
            "category": "paragraph",
            "html": "<p id='23' style='font-size:14px'>● The output of a unit u (also called the activation of u) is Yu, and unlike<br>the input, it is a single value.<br>● The set of units with connections to a unit u; i.e., its predecessors, is<br>Pre (u)<br>● The set of units with connections from a unit u; i.e., its successors, is<br>Suc (u)<br>● The weight that connects the unit v to the unit u is W[v,u].<br>● The input of a unit u coming from a unit v is denoted by X[v,u]<br>● The weighted input of the unit u is zu.<br>● The bias of the unit u is bu.<br>● The state of the unit u is Su.<br>● The squashing function of the unit u is fu.<br>● The error of the unit u is eu.<br>● The error signal of the unit u is Vu.<br>● The output sensitivity of the unit k with respect to the weight W[u,v] is</p>",
            "id": 23,
            "page": 3,
            "text": "● The output of a unit u (also called the activation of u) is Yu, and unlike the input, it is a single value. ● The set of units with connections to a unit u; i.e., its predecessors, is Pre (u) ● The set of units with connections from a unit u; i.e., its successors, is Suc (u) ● The weight that connects the unit v to the unit u is W[v,u]. ● The input of a unit u coming from a unit v is denoted by X[v,u] ● The weighted input of the unit u is zu. ● The bias of the unit u is bu. ● The state of the unit u is Su. ● The squashing function of the unit u is fu. ● The error of the unit u is eu. ● The error signal of the unit u is Vu. ● The output sensitivity of the unit k with respect to the weight W[u,v] is"
        },
        {
            "bounding_box": [
                {
                    "x": 612,
                    "y": 1599
                },
                {
                    "x": 704,
                    "y": 1599
                },
                {
                    "x": 704,
                    "y": 1664
                },
                {
                    "x": 612,
                    "y": 1664
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='24' style='font-size:22px'>pkuv.</p>",
            "id": 24,
            "page": 3,
            "text": "pkuv."
        },
        {
            "bounding_box": [
                {
                    "x": 516,
                    "y": 1731
                },
                {
                    "x": 1701,
                    "y": 1731
                },
                {
                    "x": 1701,
                    "y": 1794
                },
                {
                    "x": 516,
                    "y": 1794
                }
            ],
            "category": "paragraph",
            "html": "<p id='25' style='font-size:20px'>3 Perceptron and Delta Learning Rule</p>",
            "id": 25,
            "page": 3,
            "text": "3 Perceptron and Delta Learning Rule"
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 1833
                },
                {
                    "x": 1961,
                    "y": 1833
                },
                {
                    "x": 1961,
                    "y": 2082
                },
                {
                    "x": 511,
                    "y": 2082
                }
            ],
            "category": "paragraph",
            "html": "<p id='26' style='font-size:16px'>Artificial Neural Networks consist of a densely interconnected group of simple<br>neuron-like threshold switching units. Each unit takes a number of real-valued<br>inputs and produces a single real-valued output. Based on the connectivity<br>between the threshold units and element parameters, these networks can model<br>complex global behaviour.</p>",
            "id": 26,
            "page": 3,
            "text": "Artificial Neural Networks consist of a densely interconnected group of simple neuron-like threshold switching units. Each unit takes a number of real-valued inputs and produces a single real-valued output. Based on the connectivity between the threshold units and element parameters, these networks can model complex global behaviour."
        },
        {
            "bounding_box": [
                {
                    "x": 515,
                    "y": 2142
                },
                {
                    "x": 1037,
                    "y": 2142
                },
                {
                    "x": 1037,
                    "y": 2199
                },
                {
                    "x": 515,
                    "y": 2199
                }
            ],
            "category": "paragraph",
            "html": "<p id='27' style='font-size:18px'>3.1 The Perceptron</p>",
            "id": 27,
            "page": 3,
            "text": "3.1 The Perceptron"
        },
        {
            "bounding_box": [
                {
                    "x": 509,
                    "y": 2222
                },
                {
                    "x": 1962,
                    "y": 2222
                },
                {
                    "x": 1962,
                    "y": 2720
                },
                {
                    "x": 509,
                    "y": 2720
                }
            ],
            "category": "paragraph",
            "html": "<p id='28' style='font-size:16px'>The most basic type of artificial neuron is called a perceptron. Perceptrons<br>consist of a number of external input links, a threshold, and a single external<br>output link. Additionally, perceptrons have an internal input, 6, called bias. The<br>perceptron takes a vector of real-valued input values, all of which are weighted<br>by a multiplier. In a previous perceptron training phase, the perceptron learns<br>these weights on the basis of training data. It sums all weighted input values<br>and 'fires' if the resultant value is above a pre-defined threshold. The output of<br>the perceptron is always Boolean, and it is considered to have fired if the output<br>is '1 , The deactivated value of the perceptron is -1', and the threshold value<br>is, in most cases, '0'.</p>",
            "id": 28,
            "page": 3,
            "text": "The most basic type of artificial neuron is called a perceptron. Perceptrons consist of a number of external input links, a threshold, and a single external output link. Additionally, perceptrons have an internal input, 6, called bias. The perceptron takes a vector of real-valued input values, all of which are weighted by a multiplier. In a previous perceptron training phase, the perceptron learns these weights on the basis of training data. It sums all weighted input values and 'fires' if the resultant value is above a pre-defined threshold. The output of the perceptron is always Boolean, and it is considered to have fired if the output is '1 , The deactivated value of the perceptron is -1', and the threshold value is, in most cases, '0'."
        },
        {
            "bounding_box": [
                {
                    "x": 512,
                    "y": 2725
                },
                {
                    "x": 1961,
                    "y": 2725
                },
                {
                    "x": 1961,
                    "y": 2872
                },
                {
                    "x": 512,
                    "y": 2872
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='29' style='font-size:14px'>As we only have one unit for the perceptron, we omit the subindexes that<br>refer to the unit. Given the input vector x = <x1, ..., xn> and trained weights<br>W1, ..., Wn, the perceptron outputs y; which is computed by the formula</p>",
            "id": 29,
            "page": 3,
            "text": "As we only have one unit for the perceptron, we omit the subindexes that refer to the unit. Given the input vector x = <x1, ..., xn> and trained weights W1, ..., Wn, the perceptron outputs y; which is computed by the formula"
        },
        {
            "bounding_box": [
                {
                    "x": 1221,
                    "y": 3099
                },
                {
                    "x": 1251,
                    "y": 3099
                },
                {
                    "x": 1251,
                    "y": 3136
                },
                {
                    "x": 1221,
                    "y": 3136
                }
            ],
            "category": "footer",
            "html": "<footer id='30' style='font-size:14px'>3</footer>",
            "id": 30,
            "page": 3,
            "text": "3"
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 525
                },
                {
                    "x": 1962,
                    "y": 525
                },
                {
                    "x": 1962,
                    "y": 675
                },
                {
                    "x": 511,
                    "y": 675
                }
            ],
            "category": "paragraph",
            "html": "<p id='31' style='font-size:16px'>We refer to 2 = Ei=1 Wixi as the weighted input, and to s = 2 + 6 as the state<br>of the perceptron. For the perceptron to fire, its state s must exceed the value<br>of the threshold.</p>",
            "id": 31,
            "page": 4,
            "text": "We refer to 2 = Ei=1 Wixi as the weighted input, and to s = 2 + 6 as the state of the perceptron. For the perceptron to fire, its state s must exceed the value of the threshold."
        },
        {
            "bounding_box": [
                {
                    "x": 512,
                    "y": 678
                },
                {
                    "x": 1960,
                    "y": 678
                },
                {
                    "x": 1960,
                    "y": 975
                },
                {
                    "x": 512,
                    "y": 975
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='32' style='font-size:20px'>Single perceptron units can already represent a number of useful functions.<br>Examples are the Boolean functions AND, OR, NAND and NOR. Other func-<br>tions are only representable using networks of neurons. Single perceptrons are<br>limited to learning only functions that are linearly separable. In general, a prob-<br>lem is linear and the classes are linearly separable in an n-dimensional space if<br>the decision surface is an (n - 1)-dimensional hyperplane.</p>",
            "id": 32,
            "page": 4,
            "text": "Single perceptron units can already represent a number of useful functions. Examples are the Boolean functions AND, OR, NAND and NOR. Other functions are only representable using networks of neurons. Single perceptrons are limited to learning only functions that are linearly separable. In general, a problem is linear and the classes are linearly separable in an n-dimensional space if the decision surface is an (n - 1)-dimensional hyperplane."
        },
        {
            "bounding_box": [
                {
                    "x": 580,
                    "y": 979
                },
                {
                    "x": 1662,
                    "y": 979
                },
                {
                    "x": 1662,
                    "y": 1026
                },
                {
                    "x": 580,
                    "y": 1026
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='33' style='font-size:16px'>The general structure of a perceptron is shown in Figure 1.</p>",
            "id": 33,
            "page": 4,
            "text": "The general structure of a perceptron is shown in Figure 1."
        },
        {
            "bounding_box": [
                {
                    "x": 514,
                    "y": 1061
                },
                {
                    "x": 1956,
                    "y": 1061
                },
                {
                    "x": 1956,
                    "y": 2050
                },
                {
                    "x": 514,
                    "y": 2050
                }
            ],
            "category": "figure",
            "html": "<figure><img id='34' style='font-size:14px' alt=\"output\n(output to next layer)\n1 if �i=1 wixi +6>0;\ny\n-1 otherwise.\nbias b activation function\nn\n� s wixi +6\ni=1\nweighted sum\nW2 n\nW1 W\ninputs\n(from previous layer)\nx1 x2\" data-coord=\"top-left:(514,1061); bottom-right:(1956,2050)\" /></figure>",
            "id": 34,
            "page": 4,
            "text": "output (output to next layer) 1 if �i=1 wixi +6>0; y -1 otherwise. bias b activation function n � s wixi +6 i=1 weighted sum W2 n W1 W inputs (from previous layer) x1 x2"
        },
        {
            "bounding_box": [
                {
                    "x": 510,
                    "y": 2103
                },
                {
                    "x": 1960,
                    "y": 2103
                },
                {
                    "x": 1960,
                    "y": 2255
                },
                {
                    "x": 510,
                    "y": 2255
                }
            ],
            "category": "caption",
            "html": "<caption id='35' style='font-size:20px'>Figure 1: The general structure of the most basic type of artificial neuron,<br>called a perceptron. Single perceptrons are limited to learning linearly separable<br>functions.</caption>",
            "id": 35,
            "page": 4,
            "text": "Figure 1: The general structure of the most basic type of artificial neuron, called a perceptron. Single perceptrons are limited to learning linearly separable functions."
        },
        {
            "bounding_box": [
                {
                    "x": 513,
                    "y": 2357
                },
                {
                    "x": 1118,
                    "y": 2357
                },
                {
                    "x": 1118,
                    "y": 2416
                },
                {
                    "x": 513,
                    "y": 2416
                }
            ],
            "category": "paragraph",
            "html": "<p id='36' style='font-size:22px'>3.2 Linear Separability</p>",
            "id": 36,
            "page": 4,
            "text": "3.2 Linear Separability"
        },
        {
            "bounding_box": [
                {
                    "x": 510,
                    "y": 2438
                },
                {
                    "x": 1962,
                    "y": 2438
                },
                {
                    "x": 1962,
                    "y": 2839
                },
                {
                    "x": 510,
                    "y": 2839
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='37' style='font-size:18px'>To understand linear separability, it is helpful to visualise the possible inputs<br>of a perceptron on the axes of a two-dimensional graph. Figure 2 shows repre-<br>sentations of the Boolean functions OR and XOR. The OR function is linearly<br>separable, whereas the XOR function is not. In the figure, pluses are used for<br>an input where the perceptron fires and minuses, where it does not. If the<br>pluses and minuses can be completely separated by a single line, the problem<br>is linearly separable in two dimensions. The weights of the trained perceptron<br>should represent that line.</p>",
            "id": 37,
            "page": 4,
            "text": "To understand linear separability, it is helpful to visualise the possible inputs of a perceptron on the axes of a two-dimensional graph. Figure 2 shows representations of the Boolean functions OR and XOR. The OR function is linearly separable, whereas the XOR function is not. In the figure, pluses are used for an input where the perceptron fires and minuses, where it does not. If the pluses and minuses can be completely separated by a single line, the problem is linearly separable in two dimensions. The weights of the trained perceptron should represent that line."
        },
        {
            "bounding_box": [
                {
                    "x": 1223,
                    "y": 3100
                },
                {
                    "x": 1250,
                    "y": 3100
                },
                {
                    "x": 1250,
                    "y": 3138
                },
                {
                    "x": 1223,
                    "y": 3138
                }
            ],
            "category": "footer",
            "html": "<footer id='38' style='font-size:18px'>4</footer>",
            "id": 38,
            "page": 4,
            "text": "4"
        },
        {
            "bounding_box": [
                {
                    "x": 510,
                    "y": 1128
                },
                {
                    "x": 1965,
                    "y": 1128
                },
                {
                    "x": 1965,
                    "y": 2224
                },
                {
                    "x": 510,
                    "y": 2224
                }
            ],
            "category": "figure",
            "html": "<figure><img id='39' style='font-size:16px' alt=\"logical OR logical XOR\n(linearly separable) (not linearly separable)\n+ + +\n0 + 0 +\n0 0 1\nInput 1 Input 2 Output Input 1 Input 2 Output\n1 1 1 1 1 0\n1 0 1 1 0 1\n0 1 1 0 1 1\n0 0 0 0 0 0\" data-coord=\"top-left:(510,1128); bottom-right:(1965,2224)\" /></figure>",
            "id": 39,
            "page": 5,
            "text": "logical OR logical XOR (linearly separable) (not linearly separable) + + + 0 + 0 + 0 0 1 Input 1 Input 2 Output Input 1 Input 2 Output 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 1 0 0 0 0 0 0"
        },
        {
            "bounding_box": [
                {
                    "x": 509,
                    "y": 2254
                },
                {
                    "x": 1962,
                    "y": 2254
                },
                {
                    "x": 1962,
                    "y": 2406
                },
                {
                    "x": 509,
                    "y": 2406
                }
            ],
            "category": "caption",
            "html": "<caption id='40' style='font-size:20px'>Figure 2: Representations of the Boolean functions OR and XOR. The figures<br>show that the OR function is linearly separable, whereas the XOR function is<br>not.</caption>",
            "id": 40,
            "page": 5,
            "text": "Figure 2: Representations of the Boolean functions OR and XOR. The figures show that the OR function is linearly separable, whereas the XOR function is not."
        },
        {
            "bounding_box": [
                {
                    "x": 1220,
                    "y": 3099
                },
                {
                    "x": 1251,
                    "y": 3099
                },
                {
                    "x": 1251,
                    "y": 3137
                },
                {
                    "x": 1220,
                    "y": 3137
                }
            ],
            "category": "footer",
            "html": "<footer id='41' style='font-size:14px'>5</footer>",
            "id": 41,
            "page": 5,
            "text": "5"
        },
        {
            "bounding_box": [
                {
                    "x": 513,
                    "y": 523
                },
                {
                    "x": 1266,
                    "y": 523
                },
                {
                    "x": 1266,
                    "y": 579
                },
                {
                    "x": 513,
                    "y": 579
                }
            ],
            "category": "paragraph",
            "html": "<p id='42' style='font-size:22px'>3.3 The Delta Learning Rule</p>",
            "id": 42,
            "page": 6,
            "text": "3.3 The Delta Learning Rule"
        },
        {
            "bounding_box": [
                {
                    "x": 510,
                    "y": 603
                },
                {
                    "x": 1962,
                    "y": 603
                },
                {
                    "x": 1962,
                    "y": 954
                },
                {
                    "x": 510,
                    "y": 954
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='43' style='font-size:20px'>Perceptron training is learning by imitation, which is called 'supervised learn-<br>ing'. During the training phase, the perceptron produces an output and com-<br>pares it with a derived output value provided by the training data. In cases<br>of misclassification, it then modifies the weights accordingly. [55] show that in<br>a finite time, the perceptron will converge to reproduce the correct behaviour,<br>provided that the training examples are linearly separable. Convergence is not<br>assured if the training data is not linearly separable.</p>",
            "id": 43,
            "page": 6,
            "text": "Perceptron training is learning by imitation, which is called 'supervised learning'. During the training phase, the perceptron produces an output and compares it with a derived output value provided by the training data. In cases of misclassification, it then modifies the weights accordingly.  show that in a finite time, the perceptron will converge to reproduce the correct behaviour, provided that the training examples are linearly separable. Convergence is not assured if the training data is not linearly separable."
        },
        {
            "bounding_box": [
                {
                    "x": 509,
                    "y": 954
                },
                {
                    "x": 1961,
                    "y": 954
                },
                {
                    "x": 1961,
                    "y": 1351
                },
                {
                    "x": 509,
                    "y": 1351
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='44' style='font-size:18px'>A variety of training algorithms for perceptrons exist, of which the most<br>common are the perceptron learning rule and the delta learning rule. Both<br>start with random weights and both guarantee convergence to an acceptable<br>hypothesis. Using the perceptron learning rule algorithm, the perceptron can<br>learn from a set of samples A sample is a pair <x, d> where x is the input and<br>d is its label. For the sample <x, d〉, given the input x = <x1, · · · , xn〉, the old<br>weight vector W = 〈W1, · · · , Wn> is updated to the new vector W' using the<br>rule</p>",
            "id": 44,
            "page": 6,
            "text": "A variety of training algorithms for perceptrons exist, of which the most common are the perceptron learning rule and the delta learning rule. Both start with random weights and both guarantee convergence to an acceptable hypothesis. Using the perceptron learning rule algorithm, the perceptron can learn from a set of samples A sample is a pair <x, d> where x is the input and d is its label. For the sample <x, d〉, given the input x = <x1, · · · , xn〉, the old weight vector W = 〈W1, · · · , Wn> is updated to the new vector W' using the rule"
        },
        {
            "bounding_box": [
                {
                    "x": 514,
                    "y": 1429
                },
                {
                    "x": 605,
                    "y": 1429
                },
                {
                    "x": 605,
                    "y": 1474
                },
                {
                    "x": 514,
                    "y": 1474
                }
            ],
            "category": "paragraph",
            "html": "<p id='45' style='font-size:16px'>with</p>",
            "id": 45,
            "page": 6,
            "text": "with"
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 1554
                },
                {
                    "x": 1960,
                    "y": 1554
                },
                {
                    "x": 1960,
                    "y": 1851
                },
                {
                    "x": 511,
                    "y": 1851
                }
            ],
            "category": "paragraph",
            "html": "<p id='46' style='font-size:18px'>where y is the output calculated using the input x and the weights W and 7<br>is the learning rate. The learning rate is a constant that controls the degree to<br>which the weights are changed. As stated before, the initial weight vector wo<br>has random values. The algorithm will only converge towards an optimum if<br>the training data is linearly separable, and the learning rate is sufficiently small.<br>The perceptron rule fails if the training examples are not linearly separable.</p>",
            "id": 46,
            "page": 6,
            "text": "where y is the output calculated using the input x and the weights W and 7 is the learning rate. The learning rate is a constant that controls the degree to which the weights are changed. As stated before, the initial weight vector wo has random values. The algorithm will only converge towards an optimum if the training data is linearly separable, and the learning rate is sufficiently small. The perceptron rule fails if the training examples are not linearly separable."
        },
        {
            "bounding_box": [
                {
                    "x": 510,
                    "y": 1853
                },
                {
                    "x": 1961,
                    "y": 1853
                },
                {
                    "x": 1961,
                    "y": 2249
                },
                {
                    "x": 510,
                    "y": 2249
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='47' style='font-size:18px'>The delta learning rule was specifically designed to handle linearly separable<br>and linearly non-separable training examples. It also calculates the errors be-<br>tween calculated output and output data from training samples, and modifies<br>the weights accordingly. The modification of weights is achieved by using the<br>gradient optimisation descent algorithm, which alters them in the direction that<br>produces the steepest descent along the error surface towards the global min-<br>imum error. The delta learning rule is the basis of the error backpropagation<br>algorithm, which we will discuss later in this section.</p>",
            "id": 47,
            "page": 6,
            "text": "The delta learning rule was specifically designed to handle linearly separable and linearly non-separable training examples. It also calculates the errors between calculated output and output data from training samples, and modifies the weights accordingly. The modification of weights is achieved by using the gradient optimisation descent algorithm, which alters them in the direction that produces the steepest descent along the error surface towards the global minimum error. The delta learning rule is the basis of the error backpropagation algorithm, which we will discuss later in this section."
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 2310
                },
                {
                    "x": 1359,
                    "y": 2310
                },
                {
                    "x": 1359,
                    "y": 2366
                },
                {
                    "x": 511,
                    "y": 2366
                }
            ],
            "category": "paragraph",
            "html": "<p id='48' style='font-size:22px'>3.4 The Sigmoid Threshold Unit</p>",
            "id": 48,
            "page": 6,
            "text": "3.4 The Sigmoid Threshold Unit"
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 2390
                },
                {
                    "x": 1960,
                    "y": 2390
                },
                {
                    "x": 1960,
                    "y": 2542
                },
                {
                    "x": 511,
                    "y": 2542
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='49' style='font-size:18px'>The sigmoid threshold unit is a different kind of artificial neuron, very similar<br>to the perceptron, but uses a sigmoid function to calculate the output. The<br>output y is computed by the formula</p>",
            "id": 49,
            "page": 6,
            "text": "The sigmoid threshold unit is a different kind of artificial neuron, very similar to the perceptron, but uses a sigmoid function to calculate the output. The output y is computed by the formula"
        },
        {
            "bounding_box": [
                {
                    "x": 514,
                    "y": 2717
                },
                {
                    "x": 604,
                    "y": 2717
                },
                {
                    "x": 604,
                    "y": 2761
                },
                {
                    "x": 514,
                    "y": 2761
                }
            ],
            "category": "paragraph",
            "html": "<p id='50' style='font-size:16px'>with</p>",
            "id": 50,
            "page": 6,
            "text": "with"
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 2898
                },
                {
                    "x": 1959,
                    "y": 2898
                },
                {
                    "x": 1959,
                    "y": 2999
                },
                {
                    "x": 511,
                    "y": 2999
                }
            ],
            "category": "paragraph",
            "html": "<p id='51' style='font-size:16px'>where 6 is the bias and l is a positive constant that determines the steepness<br>of the sigmoid function. The major effect on the perceptron is that the output</p>",
            "id": 51,
            "page": 6,
            "text": "where 6 is the bias and l is a positive constant that determines the steepness of the sigmoid function. The major effect on the perceptron is that the output"
        },
        {
            "bounding_box": [
                {
                    "x": 1221,
                    "y": 3101
                },
                {
                    "x": 1250,
                    "y": 3101
                },
                {
                    "x": 1250,
                    "y": 3136
                },
                {
                    "x": 1221,
                    "y": 3136
                }
            ],
            "category": "footer",
            "html": "<footer id='52' style='font-size:14px'>6</footer>",
            "id": 52,
            "page": 6,
            "text": "6"
        },
        {
            "bounding_box": [
                {
                    "x": 510,
                    "y": 527
                },
                {
                    "x": 1964,
                    "y": 527
                },
                {
                    "x": 1964,
                    "y": 1036
                },
                {
                    "x": 510,
                    "y": 1036
                }
            ],
            "category": "paragraph",
            "html": "<p id='53' style='font-size:16px'>of the sigmoid threshold unit now has more than two possible values; now, the<br>output is \"squashed\" by a continuous function that ranges between 0 and 1.<br>Accordingly, the function is called the 'squashing' function, because it<br>(1-e-lxs)<br>maps a very large input domain onto a small range of outputs. For a low total<br>input value, the output of the sigmoid function is close to zero, whereas it is close<br>to one for a high total input value. The slope of the sigmoid function is adjusted<br>by the threshold value. The advantage of neural networks using sigmoid units<br>is that they are capable of representing non-linear functions. Cascaded linear<br>units, like the perceptron, are limited to representing linear functions. A sigmoid<br>threshold unit is sketched in Figure 3.</p>",
            "id": 53,
            "page": 7,
            "text": "of the sigmoid threshold unit now has more than two possible values; now, the output is \"squashed\" by a continuous function that ranges between 0 and 1. Accordingly, the function is called the 'squashing' function, because it (1-e-lxs) maps a very large input domain onto a small range of outputs. For a low total input value, the output of the sigmoid function is close to zero, whereas it is close to one for a high total input value. The slope of the sigmoid function is adjusted by the threshold value. The advantage of neural networks using sigmoid units is that they are capable of representing non-linear functions. Cascaded linear units, like the perceptron, are limited to representing linear functions. A sigmoid threshold unit is sketched in Figure 3."
        },
        {
            "bounding_box": [
                {
                    "x": 505,
                    "y": 1066
                },
                {
                    "x": 2013,
                    "y": 1066
                },
                {
                    "x": 2013,
                    "y": 2313
                },
                {
                    "x": 505,
                    "y": 2313
                }
            ],
            "category": "figure",
            "html": "<figure><img id='54' style='font-size:18px' alt=\"output\n(output to next layer)\nneuron\nI\ny =\n(1-e-lxs)\nbias b sigmoid threshold function\n(compute output)\nn\n� s wixi + 6\ni=1\nweighted sum\n(net input)\nW1 w 2 n\nW\ninputs\n(from previous layer)\nx1 x2 Xn\" data-coord=\"top-left:(505,1066); bottom-right:(2013,2313)\" /></figure>",
            "id": 54,
            "page": 7,
            "text": "output (output to next layer) neuron I y = (1-e-lxs) bias b sigmoid threshold function (compute output) n � s wixi + 6 i=1 weighted sum (net input) W1 w 2 n W inputs (from previous layer) x1 x2 Xn"
        },
        {
            "bounding_box": [
                {
                    "x": 510,
                    "y": 2370
                },
                {
                    "x": 1961,
                    "y": 2370
                },
                {
                    "x": 1961,
                    "y": 2521
                },
                {
                    "x": 510,
                    "y": 2521
                }
            ],
            "category": "caption",
            "html": "<caption id='55' style='font-size:14px'>Figure 3: The sigmoid threshold unit is capable of representing non-linear func-<br>tions. Its output is a continuous function of its input, which ranges between 0<br>and 1.</caption>",
            "id": 55,
            "page": 7,
            "text": "Figure 3: The sigmoid threshold unit is capable of representing non-linear functions. Its output is a continuous function of its input, which ranges between 0 and 1."
        },
        {
            "bounding_box": [
                {
                    "x": 513,
                    "y": 2636
                },
                {
                    "x": 1970,
                    "y": 2636
                },
                {
                    "x": 1970,
                    "y": 2778
                },
                {
                    "x": 513,
                    "y": 2778
                }
            ],
            "category": "paragraph",
            "html": "<p id='56' style='font-size:20px'>4 Feed-Forward Neural Networks and Backprop-<br>agation</p>",
            "id": 56,
            "page": 7,
            "text": "4 Feed-Forward Neural Networks and Backpropagation"
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 2813
                },
                {
                    "x": 1962,
                    "y": 2813
                },
                {
                    "x": 1962,
                    "y": 3017
                },
                {
                    "x": 511,
                    "y": 3017
                }
            ],
            "category": "paragraph",
            "html": "<p id='57' style='font-size:14px'>In feed-forward neural networks (FFNNs), sets of neurons are organised in lay-<br>ers, where each neuron computes a weighted sum of its inputs. Input neurons<br>take signals from the environment, and output neurons present signals to the<br>environment. Neurons that are not directly connected to the environment, but</p>",
            "id": 57,
            "page": 7,
            "text": "In feed-forward neural networks (FFNNs), sets of neurons are organised in layers, where each neuron computes a weighted sum of its inputs. Input neurons take signals from the environment, and output neurons present signals to the environment. Neurons that are not directly connected to the environment, but"
        },
        {
            "bounding_box": [
                {
                    "x": 513,
                    "y": 530
                },
                {
                    "x": 1708,
                    "y": 530
                },
                {
                    "x": 1708,
                    "y": 576
                },
                {
                    "x": 513,
                    "y": 576
                }
            ],
            "category": "paragraph",
            "html": "<p id='58' style='font-size:14px'>which are connected to other neurons, are called hidden neurons.</p>",
            "id": 58,
            "page": 8,
            "text": "which are connected to other neurons, are called hidden neurons."
        },
        {
            "bounding_box": [
                {
                    "x": 512,
                    "y": 581
                },
                {
                    "x": 1960,
                    "y": 581
                },
                {
                    "x": 1960,
                    "y": 726
                },
                {
                    "x": 512,
                    "y": 726
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='59' style='font-size:16px'>Feed-forward neural networks are loop-free and fully connected. This means<br>that each neuron provides an input to each neuron in the following layer, and<br>that none of the weights give an input to a neuron in a previous layer.</p>",
            "id": 59,
            "page": 8,
            "text": "Feed-forward neural networks are loop-free and fully connected. This means that each neuron provides an input to each neuron in the following layer, and that none of the weights give an input to a neuron in a previous layer."
        },
        {
            "bounding_box": [
                {
                    "x": 510,
                    "y": 732
                },
                {
                    "x": 1961,
                    "y": 732
                },
                {
                    "x": 1961,
                    "y": 1024
                },
                {
                    "x": 510,
                    "y": 1024
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='60' style='font-size:18px'>The simplest type of neural feed-forward networks are single-layer perceptron<br>networks. Single-layer neural networks consist of a set of input neurons, defined<br>as the input layer, and a set of output neurons, defined as the output layer. The<br>outputs of the input-layer neurons are directly connected to the neurons of the<br>output layer. The weights are applied to the connections between the input and<br>output layer.</p>",
            "id": 60,
            "page": 8,
            "text": "The simplest type of neural feed-forward networks are single-layer perceptron networks. Single-layer neural networks consist of a set of input neurons, defined as the input layer, and a set of output neurons, defined as the output layer. The outputs of the input-layer neurons are directly connected to the neurons of the output layer. The weights are applied to the connections between the input and output layer."
        },
        {
            "bounding_box": [
                {
                    "x": 510,
                    "y": 1028
                },
                {
                    "x": 1959,
                    "y": 1028
                },
                {
                    "x": 1959,
                    "y": 1225
                },
                {
                    "x": 510,
                    "y": 1225
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='61' style='font-size:20px'>In the single-layer perceptron network, every single perceptron calculates<br>the sum of the products of the weights and the inputs. The perceptron fires '1'<br>if the value is above the threshold value; otherwise, the perceptron takes the<br>deactivated value, which is usually '-1'. The threshold value is typically zero.</p>",
            "id": 61,
            "page": 8,
            "text": "In the single-layer perceptron network, every single perceptron calculates the sum of the products of the weights and the inputs. The perceptron fires '1' if the value is above the threshold value; otherwise, the perceptron takes the deactivated value, which is usually '-1'. The threshold value is typically zero."
        },
        {
            "bounding_box": [
                {
                    "x": 509,
                    "y": 1228
                },
                {
                    "x": 1960,
                    "y": 1228
                },
                {
                    "x": 1960,
                    "y": 1572
                },
                {
                    "x": 509,
                    "y": 1572
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='62' style='font-size:18px'>Sets of neurons organised in several layers can form multilayer, forward-<br>connected networks. The input and output layers are connected via at least one<br>hidden layer, built from set(s) of hidden neurons. The multilayer feed-forward<br>neural network sketched in Figure 4, with one input layer and three output<br>layers (two hidden and one output), is classified as a 3-layer feed-forward neural<br>network. For most problems, feed-forward neural networks with more than two<br>layers offer no advantage.</p>",
            "id": 62,
            "page": 8,
            "text": "Sets of neurons organised in several layers can form multilayer, forwardconnected networks. The input and output layers are connected via at least one hidden layer, built from set(s) of hidden neurons. The multilayer feed-forward neural network sketched in Figure 4, with one input layer and three output layers (two hidden and one output), is classified as a 3-layer feed-forward neural network. For most problems, feed-forward neural networks with more than two layers offer no advantage."
        },
        {
            "bounding_box": [
                {
                    "x": 510,
                    "y": 1578
                },
                {
                    "x": 1959,
                    "y": 1578
                },
                {
                    "x": 1959,
                    "y": 1723
                },
                {
                    "x": 510,
                    "y": 1723
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='63' style='font-size:18px'>Multilayer feed-forward networks using sigmoid threshold functions are able<br>to express non-linear decision surfaces. Any function can be closely approxi-<br>mated by these networks, given enough hidden units.</p>",
            "id": 63,
            "page": 8,
            "text": "Multilayer feed-forward networks using sigmoid threshold functions are able to express non-linear decision surfaces. Any function can be closely approximated by these networks, given enough hidden units."
        },
        {
            "bounding_box": [
                {
                    "x": 787,
                    "y": 1755
                },
                {
                    "x": 1677,
                    "y": 1755
                },
                {
                    "x": 1677,
                    "y": 2479
                },
                {
                    "x": 787,
                    "y": 2479
                }
            ],
            "category": "figure",
            "html": "<figure><img id='64' style='font-size:22px' alt=\"output layer\nhidden layerj\nhidden layeri\ninput layer\nx1 x2 Xn\" data-coord=\"top-left:(787,1755); bottom-right:(1677,2479)\" /></figure>",
            "id": 64,
            "page": 8,
            "text": "output layer hidden layerj hidden layeri input layer x1 x2 Xn"
        },
        {
            "bounding_box": [
                {
                    "x": 509,
                    "y": 2529
                },
                {
                    "x": 1961,
                    "y": 2529
                },
                {
                    "x": 1961,
                    "y": 2685
                },
                {
                    "x": 509,
                    "y": 2685
                }
            ],
            "category": "caption",
            "html": "<caption id='65' style='font-size:18px'>Figure 4: A multilayer feed-forward neural network with one input layer, two<br>hidden layers, and an output layer. Using neurons with sigmoid threshold func-<br>tions, these neural networks are able to express non-linear decision surfaces.</caption>",
            "id": 65,
            "page": 8,
            "text": "Figure 4: A multilayer feed-forward neural network with one input layer, two hidden layers, and an output layer. Using neurons with sigmoid threshold functions, these neural networks are able to express non-linear decision surfaces."
        },
        {
            "bounding_box": [
                {
                    "x": 510,
                    "y": 2732
                },
                {
                    "x": 1962,
                    "y": 2732
                },
                {
                    "x": 1962,
                    "y": 2980
                },
                {
                    "x": 510,
                    "y": 2980
                }
            ],
            "category": "paragraph",
            "html": "<p id='66' style='font-size:18px'>The most common neural network learning technique is the error backprop-<br>agation algorithm. It uses gradient descent to learn the weights in multilayer<br>networks. It works in small iterative steps, starting backwards from the output<br>layer towards the input layer. A requirement is that the activation function of<br>the neuron is differentiable.</p>",
            "id": 66,
            "page": 8,
            "text": "The most common neural network learning technique is the error backpropagation algorithm. It uses gradient descent to learn the weights in multilayer networks. It works in small iterative steps, starting backwards from the output layer towards the input layer. A requirement is that the activation function of the neuron is differentiable."
        },
        {
            "bounding_box": [
                {
                    "x": 1221,
                    "y": 3099
                },
                {
                    "x": 1250,
                    "y": 3099
                },
                {
                    "x": 1250,
                    "y": 3136
                },
                {
                    "x": 1221,
                    "y": 3136
                }
            ],
            "category": "footer",
            "html": "<footer id='67' style='font-size:14px'>8</footer>",
            "id": 67,
            "page": 8,
            "text": "8"
        },
        {
            "bounding_box": [
                {
                    "x": 510,
                    "y": 529
                },
                {
                    "x": 1960,
                    "y": 529
                },
                {
                    "x": 1960,
                    "y": 726
                },
                {
                    "x": 510,
                    "y": 726
                }
            ],
            "category": "paragraph",
            "html": "<p id='68' style='font-size:20px'>Usually, the weights of a feed-forward neural network are initialised to small,<br>normalised random numbers using bias values. Then, error backpropagation<br>applies all training samples to the neural network and computes the input and<br>output of each unit for all (hidden and) output layers.</p>",
            "id": 68,
            "page": 9,
            "text": "Usually, the weights of a feed-forward neural network are initialised to small, normalised random numbers using bias values. Then, error backpropagation applies all training samples to the neural network and computes the input and output of each unit for all (hidden and) output layers."
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 729
                },
                {
                    "x": 1959,
                    "y": 729
                },
                {
                    "x": 1959,
                    "y": 1075
                },
                {
                    "x": 511,
                    "y": 1075
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='69' style='font-size:20px'>The set of units of the network is N 스 I □ H □ 0, where □ is disjoint<br>union, and I, H, 0 are the sets of input, hidden and output units, respectively.<br>We denote input units by i, hidden units by h and output units by 0. For<br>convenience, we define the set of non-input units U 스 H □ 0. For a non-input<br>unit u E U, the input to u is denoted by Xu, its state by Su, its bias by bu and<br>its output by Yu· Given units u,v E U, the weight that connects u with v is<br>denoted by Wuv.</p>",
            "id": 69,
            "page": 9,
            "text": "The set of units of the network is N 스 I □ H □ 0, where □ is disjoint union, and I, H, 0 are the sets of input, hidden and output units, respectively. We denote input units by i, hidden units by h and output units by 0. For convenience, we define the set of non-input units U 스 H □ 0. For a non-input unit u E U, the input to u is denoted by Xu, its state by Su, its bias by bu and its output by Yu· Given units u,v E U, the weight that connects u with v is denoted by Wuv."
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 1079
                },
                {
                    "x": 1960,
                    "y": 1079
                },
                {
                    "x": 1960,
                    "y": 1324
                },
                {
                    "x": 511,
                    "y": 1324
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='70' style='font-size:18px'>To model the external input that the neural network receives, we use the<br>external input vector x = <x1, .. · , xn〉. For each component of the external<br>input vector we find a corresponding input unit that models it, SO the output<br>of the ith input unit should be equal ith component of the input to the network<br>(i.e., xi), and consequently 피 = n.</p>",
            "id": 70,
            "page": 9,
            "text": "To model the external input that the neural network receives, we use the external input vector x = <x1, .. · , xn〉. For each component of the external input vector we find a corresponding input unit that models it, SO the output of the ith input unit should be equal ith component of the input to the network (i.e., xi), and consequently 피 = n."
        },
        {
            "bounding_box": [
                {
                    "x": 512,
                    "y": 1328
                },
                {
                    "x": 1958,
                    "y": 1328
                },
                {
                    "x": 1958,
                    "y": 1424
                },
                {
                    "x": 512,
                    "y": 1424
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='71' style='font-size:20px'>For the non-input unit u E U, the output of u, written Yu, is defined using<br>the sigmoid activation function by</p>",
            "id": 71,
            "page": 9,
            "text": "For the non-input unit u E U, the output of u, written Yu, is defined using the sigmoid activation function by"
        },
        {
            "bounding_box": [
                {
                    "x": 515,
                    "y": 1565
                },
                {
                    "x": 1360,
                    "y": 1565
                },
                {
                    "x": 1360,
                    "y": 1611
                },
                {
                    "x": 515,
                    "y": 1611
                }
            ],
            "category": "paragraph",
            "html": "<p id='72' style='font-size:16px'>where Su is the state of u, and it is defined by</p>",
            "id": 72,
            "page": 9,
            "text": "where Su is the state of u, and it is defined by"
        },
        {
            "bounding_box": [
                {
                    "x": 515,
                    "y": 1717
                },
                {
                    "x": 1948,
                    "y": 1717
                },
                {
                    "x": 1948,
                    "y": 1767
                },
                {
                    "x": 515,
                    "y": 1767
                }
            ],
            "category": "paragraph",
            "html": "<p id='73' style='font-size:18px'>where bu is the bias of u, and Zu is the weighted input of u, defined in turn by</p>",
            "id": 73,
            "page": 9,
            "text": "where bu is the bias of u, and Zu is the weighted input of u, defined in turn by"
        },
        {
            "bounding_box": [
                {
                    "x": 510,
                    "y": 2036
                },
                {
                    "x": 1958,
                    "y": 2036
                },
                {
                    "x": 1958,
                    "y": 2228
                },
                {
                    "x": 510,
                    "y": 2228
                }
            ],
            "category": "paragraph",
            "html": "<p id='74' style='font-size:20px'>where X[v,u] is the information that v passes as input to u, and Pre (u) is the set<br>of units v that preceed u; that is, input units, and hidden units that feed their<br>outputs Yv (see Equation (1)) multiplied by the corresponding weight W[v,u] to<br>the unit u.</p>",
            "id": 74,
            "page": 9,
            "text": "where X[v,u] is the information that v passes as input to u, and Pre (u) is the set of units v that preceed u; that is, input units, and hidden units that feed their outputs Yv (see Equation (1)) multiplied by the corresponding weight W[v,u] to the unit u."
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 2235
                },
                {
                    "x": 1959,
                    "y": 2235
                },
                {
                    "x": 1959,
                    "y": 2430
                },
                {
                    "x": 511,
                    "y": 2430
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='75' style='font-size:20px'>Starting from the input layer, the inputs are propagated forwards through<br>the network until the output units are reached at the output layer. Then,<br>the output units produce an observable output (the network output) y. More<br>precisely, for 0 E 0, its output Yo corresponds to the oth component of y.</p>",
            "id": 75,
            "page": 9,
            "text": "Starting from the input layer, the inputs are propagated forwards through the network until the output units are reached at the output layer. Then, the output units produce an observable output (the network output) y. More precisely, for 0 E 0, its output Yo corresponds to the oth component of y."
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 2436
                },
                {
                    "x": 1960,
                    "y": 2436
                },
                {
                    "x": 1960,
                    "y": 2730
                },
                {
                    "x": 511,
                    "y": 2730
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='76' style='font-size:20px'>Next, the backpropagation learning algorithm propagates the error back-<br>wards, and the weights and biases are updated such that we reduce the error<br>with respect to the present training sample. Starting from the output layer,<br>the algorithm compares the network output Yo with the corresponding desired<br>target output do. It calculates the error eo for each output neuron using some<br>error function to be minimised. The error eo is computed as</p>",
            "id": 76,
            "page": 9,
            "text": "Next, the backpropagation learning algorithm propagates the error backwards, and the weights and biases are updated such that we reduce the error with respect to the present training sample. Starting from the output layer, the algorithm compares the network output Yo with the corresponding desired target output do. It calculates the error eo for each output neuron using some error function to be minimised. The error eo is computed as"
        },
        {
            "bounding_box": [
                {
                    "x": 516,
                    "y": 2839
                },
                {
                    "x": 1683,
                    "y": 2839
                },
                {
                    "x": 1683,
                    "y": 2884
                },
                {
                    "x": 516,
                    "y": 2884
                }
            ],
            "category": "paragraph",
            "html": "<p id='77' style='font-size:16px'>and we have the following notion of overall error of the network</p>",
            "id": 77,
            "page": 9,
            "text": "and we have the following notion of overall error of the network"
        },
        {
            "bounding_box": [
                {
                    "x": 1221,
                    "y": 3099
                },
                {
                    "x": 1251,
                    "y": 3099
                },
                {
                    "x": 1251,
                    "y": 3135
                },
                {
                    "x": 1221,
                    "y": 3135
                }
            ],
            "category": "footer",
            "html": "<footer id='78' style='font-size:14px'>9</footer>",
            "id": 78,
            "page": 9,
            "text": "9"
        },
        {
            "bounding_box": [
                {
                    "x": 517,
                    "y": 530
                },
                {
                    "x": 1484,
                    "y": 530
                },
                {
                    "x": 1484,
                    "y": 578
                },
                {
                    "x": 517,
                    "y": 578
                }
            ],
            "category": "paragraph",
            "html": "<p id='79' style='font-size:18px'>To update the weight W[u,v], we will use the formula</p>",
            "id": 79,
            "page": 10,
            "text": "To update the weight W[u,v], we will use the formula"
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 745
                },
                {
                    "x": 1961,
                    "y": 745
                },
                {
                    "x": 1961,
                    "y": 943
                },
                {
                    "x": 511,
                    "y": 943
                }
            ],
            "category": "paragraph",
            "html": "<p id='80' style='font-size:16px'>dyu asu to<br>where 7 is the learning rate. We now make use of the factors and<br>dyu dsu<br>calculate the weight update by deriving the error with respect to the activation,<br>and the activation in terms of the state, and in turn the derivative of the state<br>with respect to the weight:</p>",
            "id": 80,
            "page": 10,
            "text": "dyu asu to where 7 is the learning rate. We now make use of the factors and dyu dsu calculate the weight update by deriving the error with respect to the activation, and the activation in terms of the state, and in turn the derivative of the state with respect to the weight:"
        },
        {
            "bounding_box": [
                {
                    "x": 513,
                    "y": 1094
                },
                {
                    "x": 1904,
                    "y": 1094
                },
                {
                    "x": 1904,
                    "y": 1142
                },
                {
                    "x": 513,
                    "y": 1142
                }
            ],
            "category": "paragraph",
            "html": "<p id='81' style='font-size:14px'>The derivative of the error with respect to the activation for output units is</p>",
            "id": 81,
            "page": 10,
            "text": "The derivative of the error with respect to the activation for output units is"
        },
        {
            "bounding_box": [
                {
                    "x": 510,
                    "y": 1290
                },
                {
                    "x": 1958,
                    "y": 1290
                },
                {
                    "x": 1958,
                    "y": 1381
                },
                {
                    "x": 510,
                    "y": 1381
                }
            ],
            "category": "paragraph",
            "html": "<p id='82' style='font-size:14px'>now, the derivative of the activation with respect to the state for output units<br>is</p>",
            "id": 82,
            "page": 10,
            "text": "now, the derivative of the activation with respect to the state for output units is"
        },
        {
            "bounding_box": [
                {
                    "x": 512,
                    "y": 1485
                },
                {
                    "x": 1957,
                    "y": 1485
                },
                {
                    "x": 1957,
                    "y": 1579
                },
                {
                    "x": 512,
                    "y": 1579
                }
            ],
            "category": "paragraph",
            "html": "<p id='83' style='font-size:14px'>and the derivative of the state with respect to a weight that connects the hidden<br>unit h to the output unit 0 is</p>",
            "id": 83,
            "page": 10,
            "text": "and the derivative of the state with respect to a weight that connects the hidden unit h to the output unit 0 is"
        },
        {
            "bounding_box": [
                {
                    "x": 514,
                    "y": 1733
                },
                {
                    "x": 1520,
                    "y": 1733
                },
                {
                    "x": 1520,
                    "y": 1782
                },
                {
                    "x": 514,
                    "y": 1782
                }
            ],
            "category": "paragraph",
            "html": "<p id='84' style='font-size:16px'>Let us define, for the output unit 0, the error signal by</p>",
            "id": 84,
            "page": 10,
            "text": "Let us define, for the output unit 0, the error signal by"
        },
        {
            "bounding_box": [
                {
                    "x": 516,
                    "y": 1932
                },
                {
                    "x": 1060,
                    "y": 1932
                },
                {
                    "x": 1060,
                    "y": 1977
                },
                {
                    "x": 516,
                    "y": 1977
                }
            ],
            "category": "paragraph",
            "html": "<p id='85' style='font-size:16px'>for output units we have that</p>",
            "id": 85,
            "page": 10,
            "text": "for output units we have that"
        },
        {
            "bounding_box": [
                {
                    "x": 513,
                    "y": 2088
                },
                {
                    "x": 1958,
                    "y": 2088
                },
                {
                    "x": 1958,
                    "y": 2182
                },
                {
                    "x": 513,
                    "y": 2182
                }
            ],
            "category": "paragraph",
            "html": "<p id='86' style='font-size:14px'>and we see that we can update the weight between the hidden unit h and the<br>output unit 0 by</p>",
            "id": 86,
            "page": 10,
            "text": "and we see that we can update the weight between the hidden unit h and the output unit 0 by"
        },
        {
            "bounding_box": [
                {
                    "x": 512,
                    "y": 2250
                },
                {
                    "x": 1960,
                    "y": 2250
                },
                {
                    "x": 1960,
                    "y": 2501
                },
                {
                    "x": 512,
                    "y": 2501
                }
            ],
            "category": "paragraph",
            "html": "<p id='87' style='font-size:16px'>Now, for a hidden unit h, if we consider that its notion of error is related<br>to how much it contributed to the production of a faulty output, then we can<br>backpropagate the error from the output units that h sends signals to; more pre-<br>dE<br>cisely, for an input unit 2, we need to expand the equation △W[i,h] = -naW[i,h]<br>to</p>",
            "id": 87,
            "page": 10,
            "text": "Now, for a hidden unit h, if we consider that its notion of error is related to how much it contributed to the production of a faulty output, then we can backpropagate the error from the output units that h sends signals to; more predE cisely, for an input unit 2, we need to expand the equation △W[i,h] = -naW[i,h] to"
        },
        {
            "bounding_box": [
                {
                    "x": 512,
                    "y": 2620
                },
                {
                    "x": 1958,
                    "y": 2620
                },
                {
                    "x": 1958,
                    "y": 2764
                },
                {
                    "x": 512,
                    "y": 2764
                }
            ],
            "category": "paragraph",
            "html": "<p id='88' style='font-size:16px'>where Suc (h) is the set of units that succeed h; that is, the units that are fed<br>with the output of h as part of their input. By solving the partial derivatives,<br>we obtain</p>",
            "id": 88,
            "page": 10,
            "text": "where Suc (h) is the set of units that succeed h; that is, the units that are fed with the output of h as part of their input. By solving the partial derivatives, we obtain"
        },
        {
            "bounding_box": [
                {
                    "x": 1213,
                    "y": 3098
                },
                {
                    "x": 1260,
                    "y": 3098
                },
                {
                    "x": 1260,
                    "y": 3136
                },
                {
                    "x": 1213,
                    "y": 3136
                }
            ],
            "category": "footer",
            "html": "<footer id='89' style='font-size:14px'>10</footer>",
            "id": 89,
            "page": 10,
            "text": "10"
        },
        {
            "bounding_box": [
                {
                    "x": 896,
                    "y": 507
                },
                {
                    "x": 1559,
                    "y": 507
                },
                {
                    "x": 1559,
                    "y": 1249
                },
                {
                    "x": 896,
                    "y": 1249
                }
            ],
            "category": "figure",
            "html": "<figure><img id='90' style='font-size:18px' alt=\"output layer\nhidden layer\ninput layer\" data-coord=\"top-left:(896,507); bottom-right:(1559,1249)\" /></figure>",
            "id": 90,
            "page": 11,
            "text": "output layer hidden layer input layer"
        },
        {
            "bounding_box": [
                {
                    "x": 697,
                    "y": 1285
                },
                {
                    "x": 1770,
                    "y": 1285
                },
                {
                    "x": 1770,
                    "y": 1338
                },
                {
                    "x": 697,
                    "y": 1338
                }
            ],
            "category": "caption",
            "html": "<caption id='91' style='font-size:16px'>Figure 5: This figure shows a feed-forward neural network.</caption>",
            "id": 91,
            "page": 11,
            "text": "Figure 5: This figure shows a feed-forward neural network."
        },
        {
            "bounding_box": [
                {
                    "x": 513,
                    "y": 1419
                },
                {
                    "x": 1463,
                    "y": 1419
                },
                {
                    "x": 1463,
                    "y": 1470
                },
                {
                    "x": 513,
                    "y": 1470
                }
            ],
            "category": "paragraph",
            "html": "<p id='92' style='font-size:14px'>If we define the error signal of the hidden unit h by</p>",
            "id": 92,
            "page": 11,
            "text": "If we define the error signal of the hidden unit h by"
        },
        {
            "bounding_box": [
                {
                    "x": 513,
                    "y": 1644
                },
                {
                    "x": 1639,
                    "y": 1644
                },
                {
                    "x": 1639,
                    "y": 1692
                },
                {
                    "x": 513,
                    "y": 1692
                }
            ],
            "category": "paragraph",
            "html": "<p id='93' style='font-size:14px'>then we have a uniform expression for weight change; that is,</p>",
            "id": 93,
            "page": 11,
            "text": "then we have a uniform expression for weight change; that is,"
        },
        {
            "bounding_box": [
                {
                    "x": 512,
                    "y": 1826
                },
                {
                    "x": 1959,
                    "y": 1826
                },
                {
                    "x": 1959,
                    "y": 1928
                },
                {
                    "x": 512,
                    "y": 1928
                }
            ],
            "category": "paragraph",
            "html": "<p id='94' style='font-size:16px'>We calculate △W[v,u] again and again until all network outputs are within<br>an acceptable range, or some other terminating condition is reached.</p>",
            "id": 94,
            "page": 11,
            "text": "We calculate △W[v,u] again and again until all network outputs are within an acceptable range, or some other terminating condition is reached."
        },
        {
            "bounding_box": [
                {
                    "x": 515,
                    "y": 2001
                },
                {
                    "x": 1441,
                    "y": 2001
                },
                {
                    "x": 1441,
                    "y": 2063
                },
                {
                    "x": 515,
                    "y": 2063
                }
            ],
            "category": "paragraph",
            "html": "<p id='95' style='font-size:22px'>5 Recurrent Neural Networks</p>",
            "id": 95,
            "page": 11,
            "text": "5 Recurrent Neural Networks"
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 2102
                },
                {
                    "x": 1961,
                    "y": 2102
                },
                {
                    "x": 1961,
                    "y": 2401
                },
                {
                    "x": 511,
                    "y": 2401
                }
            ],
            "category": "paragraph",
            "html": "<p id='96' style='font-size:16px'>Recurrent neural networks (RNNs) [74, 75] are dynamic systems; they have an<br>internal state at each time step of the classification. This is due to circular<br>connections between higher- and lower-layer neurons and optional self-feedback<br>connections. These feedback connections enable RNNs to propagate data from<br>earlier events to current processing steps. Thus, RNNs build a memory of time<br>series events.</p>",
            "id": 96,
            "page": 11,
            "text": "Recurrent neural networks (RNNs)  are dynamic systems; they have an internal state at each time step of the classification. This is due to circular connections between higher- and lower-layer neurons and optional self-feedback connections. These feedback connections enable RNNs to propagate data from earlier events to current processing steps. Thus, RNNs build a memory of time series events."
        },
        {
            "bounding_box": [
                {
                    "x": 514,
                    "y": 2463
                },
                {
                    "x": 1110,
                    "y": 2463
                },
                {
                    "x": 1110,
                    "y": 2518
                },
                {
                    "x": 514,
                    "y": 2518
                }
            ],
            "category": "paragraph",
            "html": "<p id='97' style='font-size:20px'>5.1 Basic Architecture</p>",
            "id": 97,
            "page": 11,
            "text": "5.1 Basic Architecture"
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 2543
                },
                {
                    "x": 1962,
                    "y": 2543
                },
                {
                    "x": 1962,
                    "y": 2997
                },
                {
                    "x": 511,
                    "y": 2997
                }
            ],
            "category": "paragraph",
            "html": "<p id='98' style='font-size:16px'>RNNs range from partly to fully connected, and two simple RNNs are suggested<br>by [46] and [16]. The Elman network is similar to a three-layer neural network,<br>but additionally, the outputs of the hidden layer are saved in so-called 'context<br>cells'. The output of a context cell is circularly fed back to the hidden neuron<br>along with the originating signal. Every hidden neuron has its own context cell<br>and receives input both from the input layer and the context cells. Elman net-<br>works can be trained with standard error backpropagation, the output from the<br>context cells being simply regarded as an additional input. Figures 5 and 6 show<br>a standard feed-forward network in comparison with such an Elman network.</p>",
            "id": 98,
            "page": 11,
            "text": "RNNs range from partly to fully connected, and two simple RNNs are suggested by  and . The Elman network is similar to a three-layer neural network, but additionally, the outputs of the hidden layer are saved in so-called 'context cells'. The output of a context cell is circularly fed back to the hidden neuron along with the originating signal. Every hidden neuron has its own context cell and receives input both from the input layer and the context cells. Elman networks can be trained with standard error backpropagation, the output from the context cells being simply regarded as an additional input. Figures 5 and 6 show a standard feed-forward network in comparison with such an Elman network."
        },
        {
            "bounding_box": [
                {
                    "x": 1213,
                    "y": 3099
                },
                {
                    "x": 1257,
                    "y": 3099
                },
                {
                    "x": 1257,
                    "y": 3138
                },
                {
                    "x": 1213,
                    "y": 3138
                }
            ],
            "category": "footer",
            "html": "<footer id='99' style='font-size:14px'>11</footer>",
            "id": 99,
            "page": 11,
            "text": "11"
        },
        {
            "bounding_box": [
                {
                    "x": 706,
                    "y": 503
                },
                {
                    "x": 1755,
                    "y": 503
                },
                {
                    "x": 1755,
                    "y": 1228
                },
                {
                    "x": 706,
                    "y": 1228
                }
            ],
            "category": "figure",
            "html": "<figure><img id='100' style='font-size:14px' alt=\"output\nhidden\ncontext cells\ninput\n�1 X2\" data-coord=\"top-left:(706,503); bottom-right:(1755,1228)\" /></figure>",
            "id": 100,
            "page": 12,
            "text": "output hidden context cells input �1 X2"
        },
        {
            "bounding_box": [
                {
                    "x": 739,
                    "y": 1284
                },
                {
                    "x": 1730,
                    "y": 1284
                },
                {
                    "x": 1730,
                    "y": 1338
                },
                {
                    "x": 739,
                    "y": 1338
                }
            ],
            "category": "caption",
            "html": "<caption id='101' style='font-size:16px'>Figure 6: This figure shows an Elman neural network.</caption>",
            "id": 101,
            "page": 12,
            "text": "Figure 6: This figure shows an Elman neural network."
        },
        {
            "bounding_box": [
                {
                    "x": 877,
                    "y": 1366
                },
                {
                    "x": 1591,
                    "y": 1366
                },
                {
                    "x": 1591,
                    "y": 2071
                },
                {
                    "x": 877,
                    "y": 2071
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='102' style='font-size:14px' alt=\"soft feedback\" data-coord=\"top-left:(877,1366); bottom-right:(1591,2071)\" /></figure>",
            "id": 102,
            "page": 12,
            "text": "soft feedback"
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 2115
                },
                {
                    "x": 1954,
                    "y": 2115
                },
                {
                    "x": 1954,
                    "y": 2216
                },
                {
                    "x": 511,
                    "y": 2216
                }
            ],
            "category": "caption",
            "html": "<caption id='103' style='font-size:16px'>Figure 7: This figure shows a partially recurrent neural network with self-<br>feedback in the hidden layer.</caption>",
            "id": 103,
            "page": 12,
            "text": "Figure 7: This figure shows a partially recurrent neural network with selffeedback in the hidden layer."
        },
        {
            "bounding_box": [
                {
                    "x": 512,
                    "y": 2298
                },
                {
                    "x": 1959,
                    "y": 2298
                },
                {
                    "x": 1959,
                    "y": 2494
                },
                {
                    "x": 512,
                    "y": 2494
                }
            ],
            "category": "paragraph",
            "html": "<p id='104' style='font-size:16px'>Jordan networks have a similar structure to Elman networks, but the context<br>cells are instead fed by the output layer. A partial recurrent neural network with<br>a fully connected recurrent hidden layer is shown in Figure 7. Figure 8 shows a<br>fully connected RNN.</p>",
            "id": 104,
            "page": 12,
            "text": "Jordan networks have a similar structure to Elman networks, but the context cells are instead fed by the output layer. A partial recurrent neural network with a fully connected recurrent hidden layer is shown in Figure 7. Figure 8 shows a fully connected RNN."
        },
        {
            "bounding_box": [
                {
                    "x": 510,
                    "y": 2497
                },
                {
                    "x": 1960,
                    "y": 2497
                },
                {
                    "x": 1960,
                    "y": 2998
                },
                {
                    "x": 510,
                    "y": 2998
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='105' style='font-size:20px'>RNNs need to be trained differently to the feed-forward neural networks<br>(FFNNs) described in Section 4. This is because, for RNNs, we need to propa-<br>gate information through the recurrent connections in-between steps. The most<br>common and well-documented learning algorithms for training RNNs in tempo-<br>ral, supervised learning tasks are backpropagation through time (BPTT) and<br>real-time recurrent learning (RTRL). In BPTT, the network is unfolded in time<br>to construct an FFNN. Then, the generalised delta rule is applied to update the<br>weights. This is an offline learning algorithm in the sense that we first collect<br>the data and then build the model from the system. In RTRL, the gradient<br>information is forward propagated. Here, the data is collected online from the</p>",
            "id": 105,
            "page": 12,
            "text": "RNNs need to be trained differently to the feed-forward neural networks (FFNNs) described in Section 4. This is because, for RNNs, we need to propagate information through the recurrent connections in-between steps. The most common and well-documented learning algorithms for training RNNs in temporal, supervised learning tasks are backpropagation through time (BPTT) and real-time recurrent learning (RTRL). In BPTT, the network is unfolded in time to construct an FFNN. Then, the generalised delta rule is applied to update the weights. This is an offline learning algorithm in the sense that we first collect the data and then build the model from the system. In RTRL, the gradient information is forward propagated. Here, the data is collected online from the"
        },
        {
            "bounding_box": [
                {
                    "x": 1213,
                    "y": 3097
                },
                {
                    "x": 1261,
                    "y": 3097
                },
                {
                    "x": 1261,
                    "y": 3138
                },
                {
                    "x": 1213,
                    "y": 3138
                }
            ],
            "category": "footer",
            "html": "<footer id='106' style='font-size:16px'>12</footer>",
            "id": 106,
            "page": 12,
            "text": "12"
        },
        {
            "bounding_box": [
                {
                    "x": 865,
                    "y": 510
                },
                {
                    "x": 1605,
                    "y": 510
                },
                {
                    "x": 1605,
                    "y": 1027
                },
                {
                    "x": 865,
                    "y": 1027
                }
            ],
            "category": "figure",
            "html": "<figure><img id='107' alt=\"\" data-coord=\"top-left:(865,510); bottom-right:(1605,1027)\" /></figure>",
            "id": 107,
            "page": 13,
            "text": ""
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 1075
                },
                {
                    "x": 1957,
                    "y": 1075
                },
                {
                    "x": 1957,
                    "y": 1175
                },
                {
                    "x": 511,
                    "y": 1175
                }
            ],
            "category": "caption",
            "html": "<caption id='108' style='font-size:18px'>Figure 8: This figure shows a fully recurrent neural network (RNN) with self-<br>feedback connections.</caption>",
            "id": 108,
            "page": 13,
            "text": "Figure 8: This figure shows a fully recurrent neural network (RNN) with selffeedback connections."
        },
        {
            "bounding_box": [
                {
                    "x": 513,
                    "y": 1253
                },
                {
                    "x": 1957,
                    "y": 1253
                },
                {
                    "x": 1957,
                    "y": 1350
                },
                {
                    "x": 513,
                    "y": 1350
                }
            ],
            "category": "paragraph",
            "html": "<p id='109' style='font-size:18px'>system and the model is learned during collection. Therefore, RTRL is an online<br>learning algorithm.</p>",
            "id": 109,
            "page": 13,
            "text": "system and the model is learned during collection. Therefore, RTRL is an online learning algorithm."
        },
        {
            "bounding_box": [
                {
                    "x": 514,
                    "y": 1427
                },
                {
                    "x": 1713,
                    "y": 1427
                },
                {
                    "x": 1713,
                    "y": 1488
                },
                {
                    "x": 514,
                    "y": 1488
                }
            ],
            "category": "paragraph",
            "html": "<p id='110' style='font-size:22px'>6 Training Recurrent Neural Networks</p>",
            "id": 110,
            "page": 13,
            "text": "6 Training Recurrent Neural Networks"
        },
        {
            "bounding_box": [
                {
                    "x": 512,
                    "y": 1527
                },
                {
                    "x": 1960,
                    "y": 1527
                },
                {
                    "x": 1960,
                    "y": 1828
                },
                {
                    "x": 512,
                    "y": 1828
                }
            ],
            "category": "paragraph",
            "html": "<p id='111' style='font-size:18px'>The most common methods to train recurrent neural networks are Backpropa-<br>gation Through Time (BPTT) [62, 74, 75] and Real-Time Recurrent Learning<br>(RTRL) [75, 76], whereas BPTT is the most common method. The main differ-<br>ence between BPTT and RTRL is the way the weight changes are calculated.<br>The original formulation of LSTM-RNNs used a combination of BPTT and<br>RTRL. Therefore we cover both learning algorithms in short.</p>",
            "id": 111,
            "page": 13,
            "text": "The most common methods to train recurrent neural networks are Backpropagation Through Time (BPTT)  and Real-Time Recurrent Learning (RTRL) , whereas BPTT is the most common method. The main difference between BPTT and RTRL is the way the weight changes are calculated. The original formulation of LSTM-RNNs used a combination of BPTT and RTRL. Therefore we cover both learning algorithms in short."
        },
        {
            "bounding_box": [
                {
                    "x": 514,
                    "y": 1888
                },
                {
                    "x": 1441,
                    "y": 1888
                },
                {
                    "x": 1441,
                    "y": 1944
                },
                {
                    "x": 514,
                    "y": 1944
                }
            ],
            "category": "paragraph",
            "html": "<p id='112' style='font-size:20px'>6.1 Backpropagation Through Time</p>",
            "id": 112,
            "page": 13,
            "text": "6.1 Backpropagation Through Time"
        },
        {
            "bounding_box": [
                {
                    "x": 510,
                    "y": 1968
                },
                {
                    "x": 1960,
                    "y": 1968
                },
                {
                    "x": 1960,
                    "y": 2316
                },
                {
                    "x": 510,
                    "y": 2316
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='113' style='font-size:18px'>The BPTT algorithm makes use of the fact that, for a finite period of time, there<br>is an FFNN with identical behaviour for every RNN. To obtain this FFNN, we<br>need to unfold the RNN in time. Figure 9a shows a simple, fully recurrent<br>neural network with a single two-neuron layer. The corresponding feed-forward<br>neural network, shown in Figure 9b, requires a separate layer for each time step<br>with the same weights for all layers. If weights are identical to the RNN, both<br>networks show the same behaviour.</p>",
            "id": 113,
            "page": 13,
            "text": "The BPTT algorithm makes use of the fact that, for a finite period of time, there is an FFNN with identical behaviour for every RNN. To obtain this FFNN, we need to unfold the RNN in time. Figure 9a shows a simple, fully recurrent neural network with a single two-neuron layer. The corresponding feed-forward neural network, shown in Figure 9b, requires a separate layer for each time step with the same weights for all layers. If weights are identical to the RNN, both networks show the same behaviour."
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 2320
                },
                {
                    "x": 1961,
                    "y": 2320
                },
                {
                    "x": 1961,
                    "y": 2665
                },
                {
                    "x": 511,
                    "y": 2665
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='114' style='font-size:16px'>The unfolded network can be trained using the backpropagation algorithm<br>described in Section 4. At the end of a training sequence, the network is unfolded<br>in time. The error is calculated for the output units with existing target values<br>using some chosen error measure. Then, the error is injected backwards into<br>the network and the weight updates for all time steps calculated. The weights<br>in the recurrent version of the network are updated with the sum of its deltas<br>over all time steps.</p>",
            "id": 114,
            "page": 13,
            "text": "The unfolded network can be trained using the backpropagation algorithm described in Section 4. At the end of a training sequence, the network is unfolded in time. The error is calculated for the output units with existing target values using some chosen error measure. Then, the error is injected backwards into the network and the weight updates for all time steps calculated. The weights in the recurrent version of the network are updated with the sum of its deltas over all time steps."
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 2668
                },
                {
                    "x": 1961,
                    "y": 2668
                },
                {
                    "x": 1961,
                    "y": 2967
                },
                {
                    "x": 511,
                    "y": 2967
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='115' style='font-size:16px'>We calculate the error signal for a unit for all time steps in a single pass,<br>using the following iterative backpropagation algorithm. We consider discrete<br>time steps 1,2,3..., indexed by the variable T. The network starts at a point in<br>time t' and runs until a final time t. This time frame between t' and t is called<br>an epoch. Let U be the set of non input units, and let fu be the differentiable,<br>non-linear squashing function of the unit u E U; the output Yu(T) of u at time</p>",
            "id": 115,
            "page": 13,
            "text": "We calculate the error signal for a unit for all time steps in a single pass, using the following iterative backpropagation algorithm. We consider discrete time steps 1,2,3..., indexed by the variable T. The network starts at a point in time t' and runs until a final time t. This time frame between t' and t is called an epoch. Let U be the set of non input units, and let fu be the differentiable, non-linear squashing function of the unit u E U; the output Yu(T) of u at time"
        },
        {
            "bounding_box": [
                {
                    "x": 1212,
                    "y": 3096
                },
                {
                    "x": 1261,
                    "y": 3096
                },
                {
                    "x": 1261,
                    "y": 3139
                },
                {
                    "x": 1212,
                    "y": 3139
                }
            ],
            "category": "footer",
            "html": "<footer id='116' style='font-size:14px'>13</footer>",
            "id": 116,
            "page": 13,
            "text": "13"
        },
        {
            "bounding_box": [
                {
                    "x": 646,
                    "y": 525
                },
                {
                    "x": 1827,
                    "y": 525
                },
                {
                    "x": 1827,
                    "y": 1247
                },
                {
                    "x": 646,
                    "y": 1247
                }
            ],
            "category": "figure",
            "html": "<figure><img id='117' style='font-size:18px' alt=\"time\na) b) C1,t C2,t\nt\nW1,2 W2,1 W2,2\nW1,1\nC1,t-1\nt-1\nC2,t-1\nW1,1 W1,2\nW2,2\nC2,1 T\nC1,1\nW2,1\nC1,2 C2,2 2\nW1,1 W1,2 W2,1 W2,2\nC2,1 1\nC1,1\" data-coord=\"top-left:(646,525); bottom-right:(1827,1247)\" /></figure>",
            "id": 117,
            "page": 14,
            "text": "time a) b) C1,t C2,t t W1,2 W2,1 W2,2 W1,1 C1,t-1 t-1 C2,t-1 W1,1 W1,2 W2,2 C2,1 T C1,1 W2,1 C1,2 C2,2 2 W1,1 W1,2 W2,1 W2,2 C2,1 1 C1,1"
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 1284
                },
                {
                    "x": 1961,
                    "y": 1284
                },
                {
                    "x": 1961,
                    "y": 1488
                },
                {
                    "x": 511,
                    "y": 1488
                }
            ],
            "category": "caption",
            "html": "<caption id='118' style='font-size:14px'>Figure 9: Figure a shows a simple fully recurrent neural network with a two-<br>neuron layer. The same network unfolded over time with a separate layer for<br>each time step is shown in Figure b. The latter representation is a feed-forward<br>neural network.</caption>",
            "id": 118,
            "page": 14,
            "text": "Figure 9: Figure a shows a simple fully recurrent neural network with a twoneuron layer. The same network unfolded over time with a separate layer for each time step is shown in Figure b. The latter representation is a feed-forward neural network."
        },
        {
            "bounding_box": [
                {
                    "x": 513,
                    "y": 1550
                },
                {
                    "x": 755,
                    "y": 1550
                },
                {
                    "x": 755,
                    "y": 1600
                },
                {
                    "x": 513,
                    "y": 1600
                }
            ],
            "category": "paragraph",
            "html": "<p id='119' style='font-size:16px'>T is given by</p>",
            "id": 119,
            "page": 14,
            "text": "T is given by"
        },
        {
            "bounding_box": [
                {
                    "x": 514,
                    "y": 1666
                },
                {
                    "x": 960,
                    "y": 1666
                },
                {
                    "x": 960,
                    "y": 1713
                },
                {
                    "x": 514,
                    "y": 1713
                }
            ],
            "category": "paragraph",
            "html": "<p id='120' style='font-size:18px'>with the weighted input</p>",
            "id": 120,
            "page": 14,
            "text": "with the weighted input"
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 1976
                },
                {
                    "x": 1961,
                    "y": 1976
                },
                {
                    "x": 1961,
                    "y": 2423
                },
                {
                    "x": 511,
                    "y": 2423
                }
            ],
            "category": "paragraph",
            "html": "<p id='121' style='font-size:16px'>where v E UnPre (u) and i E I, the set of input units. Note that the inputs to u<br>at time ヶ+1 are of two types: the environmental input that arrives at time ヶ+1<br>via the input units, and the recurrent output from all non-input units in the<br>network produced at time T. If the network is fully connected, then U nPre (u)<br>is equal to the set U of non-input units. Let T(T) be the set of non-input units<br>for which, at time T, the output value Yu(T) of the unit u E T(T) should match<br>some target value du(T). The cost function is the summed error Etotal(t',t) for<br>the epoch t', t' +1, ..,t, which we want to minimise using a learning algorithm.<br>Such total error is defined by</p>",
            "id": 121,
            "page": 14,
            "text": "where v E UnPre (u) and i E I, the set of input units. Note that the inputs to u at time ヶ+1 are of two types: the environmental input that arrives at time ヶ+1 via the input units, and the recurrent output from all non-input units in the network produced at time T. If the network is fully connected, then U nPre (u) is equal to the set U of non-input units. Let T(T) be the set of non-input units for which, at time T, the output value Yu(T) of the unit u E T(T) should match some target value du(T). The cost function is the summed error Etotal(t',t) for the epoch t', t' +1, ..,t, which we want to minimise using a learning algorithm. Such total error is defined by"
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 2600
                },
                {
                    "x": 1958,
                    "y": 2600
                },
                {
                    "x": 1958,
                    "y": 2698
                },
                {
                    "x": 511,
                    "y": 2698
                }
            ],
            "category": "paragraph",
            "html": "<p id='122' style='font-size:14px'>with the error E(T) at time T defined using the squared error as an objective<br>function by</p>",
            "id": 122,
            "page": 14,
            "text": "with the error E(T) at time T defined using the squared error as an objective function by"
        },
        {
            "bounding_box": [
                {
                    "x": 514,
                    "y": 2826
                },
                {
                    "x": 1785,
                    "y": 2826
                },
                {
                    "x": 1785,
                    "y": 2875
                },
                {
                    "x": 514,
                    "y": 2875
                }
            ],
            "category": "paragraph",
            "html": "<p id='123' style='font-size:14px'>and with the error eu (T) of the non-input unit u at time T defined by</p>",
            "id": 123,
            "page": 14,
            "text": "and with the error eu (T) of the non-input unit u at time T defined by"
        },
        {
            "bounding_box": [
                {
                    "x": 1213,
                    "y": 3099
                },
                {
                    "x": 1260,
                    "y": 3099
                },
                {
                    "x": 1260,
                    "y": 3137
                },
                {
                    "x": 1213,
                    "y": 3137
                }
            ],
            "category": "footer",
            "html": "<footer id='124' style='font-size:14px'>14</footer>",
            "id": 124,
            "page": 14,
            "text": "14"
        },
        {
            "bounding_box": [
                {
                    "x": 512,
                    "y": 530
                },
                {
                    "x": 1958,
                    "y": 530
                },
                {
                    "x": 1958,
                    "y": 625
                },
                {
                    "x": 512,
                    "y": 625
                }
            ],
            "category": "paragraph",
            "html": "<p id='125' style='font-size:14px'>To adjust the weights, we use the error signal Vu(T) of a non-input unit u at a<br>time T, which is defined by</p>",
            "id": 125,
            "page": 15,
            "text": "To adjust the weights, we use the error signal Vu(T) of a non-input unit u at a time T, which is defined by"
        },
        {
            "bounding_box": [
                {
                    "x": 516,
                    "y": 803
                },
                {
                    "x": 1486,
                    "y": 803
                },
                {
                    "x": 1486,
                    "y": 851
                },
                {
                    "x": 516,
                    "y": 851
                }
            ],
            "category": "paragraph",
            "html": "<p id='126' style='font-size:14px'>When we unroll Vu over time, we obtain the equality</p>",
            "id": 126,
            "page": 15,
            "text": "When we unroll Vu over time, we obtain the equality"
        },
        {
            "bounding_box": [
                {
                    "x": 513,
                    "y": 1055
                },
                {
                    "x": 1959,
                    "y": 1055
                },
                {
                    "x": 1959,
                    "y": 1204
                },
                {
                    "x": 513,
                    "y": 1204
                }
            ],
            "category": "paragraph",
            "html": "<p id='127' style='font-size:16px'>After the backpropagation computation is performed down to time t', we cal-<br>culate the weight update △W[u,v] in the recurrent version of the network. This<br>is done by summing the corresponding weight updates for all time steps:</p>",
            "id": 127,
            "page": 15,
            "text": "After the backpropagation computation is performed down to time t', we calculate the weight update △W[u,v] in the recurrent version of the network. This is done by summing the corresponding weight updates for all time steps:"
        },
        {
            "bounding_box": [
                {
                    "x": 514,
                    "y": 1386
                },
                {
                    "x": 605,
                    "y": 1386
                },
                {
                    "x": 605,
                    "y": 1429
                },
                {
                    "x": 514,
                    "y": 1429
                }
            ],
            "category": "paragraph",
            "html": "<p id='128' style='font-size:16px'>with</p>",
            "id": 128,
            "page": 15,
            "text": "with"
        },
        {
            "bounding_box": [
                {
                    "x": 576,
                    "y": 1718
                },
                {
                    "x": 1581,
                    "y": 1718
                },
                {
                    "x": 1581,
                    "y": 1771
                },
                {
                    "x": 576,
                    "y": 1771
                }
            ],
            "category": "paragraph",
            "html": "<p id='129' style='font-size:16px'>BPTT is described in more detail in [74], [62] and [76].</p>",
            "id": 129,
            "page": 15,
            "text": "BPTT is described in more detail in ,  and ."
        },
        {
            "bounding_box": [
                {
                    "x": 514,
                    "y": 1831
                },
                {
                    "x": 1405,
                    "y": 1831
                },
                {
                    "x": 1405,
                    "y": 1885
                },
                {
                    "x": 514,
                    "y": 1885
                }
            ],
            "category": "paragraph",
            "html": "<p id='130' style='font-size:18px'>6.2 Real- Time Recurrent Learning</p>",
            "id": 130,
            "page": 15,
            "text": "6.2 Real- Time Recurrent Learning"
        },
        {
            "bounding_box": [
                {
                    "x": 513,
                    "y": 1911
                },
                {
                    "x": 1959,
                    "y": 1911
                },
                {
                    "x": 1959,
                    "y": 2257
                },
                {
                    "x": 513,
                    "y": 2257
                }
            ],
            "category": "paragraph",
            "html": "<p id='131' style='font-size:16px'>The RTRL algorithm does not require error propagation. All the information<br>necessary to compute the gradient is collected as the input stream is presented<br>to the network. This makes a dedicated training interval obsolete. The algo-<br>rithm comes at significant computational cost per update cycle, and the stored<br>information is non-local; i.e., we need an additional notion called sensitivity<br>of the output, which we'll explain later. Nevertheless, the memory required<br>depends only on the size of the network and not on the size of the input.</p>",
            "id": 131,
            "page": 15,
            "text": "The RTRL algorithm does not require error propagation. All the information necessary to compute the gradient is collected as the input stream is presented to the network. This makes a dedicated training interval obsolete. The algorithm comes at significant computational cost per update cycle, and the stored information is non-local; i.e., we need an additional notion called sensitivity of the output, which we'll explain later. Nevertheless, the memory required depends only on the size of the network and not on the size of the input."
        },
        {
            "bounding_box": [
                {
                    "x": 513,
                    "y": 2262
                },
                {
                    "x": 1960,
                    "y": 2262
                },
                {
                    "x": 1960,
                    "y": 2557
                },
                {
                    "x": 513,
                    "y": 2557
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='132' style='font-size:14px'>Following the notation from the previous section, we will now define for the<br>network units v E I U U and u, k E U, and the time steps t' ≤ T ≤ t. Unlike<br>BPTT, in RTRL we assume the existence of a label dk(T) at every time T<br>(given that it is an online algorithm) for every non-input unit k, SO the training<br>objective is to minimise the overall network error, which is given at time step T<br>by</p>",
            "id": 132,
            "page": 15,
            "text": "Following the notation from the previous section, we will now define for the network units v E I U U and u, k E U, and the time steps t' ≤ T ≤ t. Unlike BPTT, in RTRL we assume the existence of a label dk(T) at every time T (given that it is an online algorithm) for every non-input unit k, SO the training objective is to minimise the overall network error, which is given at time step T by"
        },
        {
            "bounding_box": [
                {
                    "x": 510,
                    "y": 2691
                },
                {
                    "x": 1959,
                    "y": 2691
                },
                {
                    "x": 1959,
                    "y": 2790
                },
                {
                    "x": 510,
                    "y": 2790
                }
            ],
            "category": "paragraph",
            "html": "<p id='133' style='font-size:14px'>We conclude from Equation 8 that the gradient of the total error is also the<br>sum of the gradient for all previous time steps and the current time step:</p>",
            "id": 133,
            "page": 15,
            "text": "We conclude from Equation 8 that the gradient of the total error is also the sum of the gradient for all previous time steps and the current time step:"
        },
        {
            "bounding_box": [
                {
                    "x": 514,
                    "y": 2922
                },
                {
                    "x": 1960,
                    "y": 2922
                },
                {
                    "x": 1960,
                    "y": 3021
                },
                {
                    "x": 514,
                    "y": 3021
                }
            ],
            "category": "paragraph",
            "html": "<p id='134' style='font-size:14px'>During presentation of the time series to the network, we need to accumulate<br>the values of the gradient at each time step. Thus, we can also keep track of</p>",
            "id": 134,
            "page": 15,
            "text": "During presentation of the time series to the network, we need to accumulate the values of the gradient at each time step. Thus, we can also keep track of"
        },
        {
            "bounding_box": [
                {
                    "x": 1213,
                    "y": 3099
                },
                {
                    "x": 1261,
                    "y": 3099
                },
                {
                    "x": 1261,
                    "y": 3137
                },
                {
                    "x": 1213,
                    "y": 3137
                }
            ],
            "category": "footer",
            "html": "<footer id='135' style='font-size:14px'>15</footer>",
            "id": 135,
            "page": 15,
            "text": "15"
        },
        {
            "bounding_box": [
                {
                    "x": 512,
                    "y": 531
                },
                {
                    "x": 1959,
                    "y": 531
                },
                {
                    "x": 1959,
                    "y": 629
                },
                {
                    "x": 512,
                    "y": 629
                }
            ],
            "category": "paragraph",
            "html": "<p id='136' style='font-size:16px'>the weight changes △W[u,v](T). After presentation, the overall weight change<br>for W[u,v] is then given by</p>",
            "id": 136,
            "page": 16,
            "text": "the weight changes △W[u,v](T). After presentation, the overall weight change for W[u,v] is then given by"
        },
        {
            "bounding_box": [
                {
                    "x": 576,
                    "y": 846
                },
                {
                    "x": 1441,
                    "y": 846
                },
                {
                    "x": 1441,
                    "y": 893
                },
                {
                    "x": 576,
                    "y": 893
                }
            ],
            "category": "paragraph",
            "html": "<p id='137' style='font-size:14px'>To get the weight changes we need to calculate</p>",
            "id": 137,
            "page": 16,
            "text": "To get the weight changes we need to calculate"
        },
        {
            "bounding_box": [
                {
                    "x": 512,
                    "y": 1076
                },
                {
                    "x": 1957,
                    "y": 1076
                },
                {
                    "x": 1957,
                    "y": 1172
                },
                {
                    "x": 512,
                    "y": 1172
                }
            ],
            "category": "paragraph",
            "html": "<p id='138' style='font-size:14px'>for each time step t. After expanding this equation via gradient descent and by<br>applying Equation 9, we find that</p>",
            "id": 138,
            "page": 16,
            "text": "for each time step t. After expanding this equation via gradient descent and by applying Equation 9, we find that"
        },
        {
            "bounding_box": [
                {
                    "x": 512,
                    "y": 1505
                },
                {
                    "x": 1957,
                    "y": 1505
                },
                {
                    "x": 1957,
                    "y": 1603
                },
                {
                    "x": 512,
                    "y": 1603
                }
            ],
            "category": "paragraph",
            "html": "<p id='139' style='font-size:14px'>Since the error ek (T) = dk(T) - yk (T) is always known, we need to find a way<br>to calculate the second factor only. We define the quantity</p>",
            "id": 139,
            "page": 16,
            "text": "Since the error ek (T) = dk(T) - yk (T) is always known, we need to find a way to calculate the second factor only. We define the quantity"
        },
        {
            "bounding_box": [
                {
                    "x": 513,
                    "y": 1787
                },
                {
                    "x": 1958,
                    "y": 1787
                },
                {
                    "x": 1958,
                    "y": 2029
                },
                {
                    "x": 513,
                    "y": 2029
                }
            ],
            "category": "paragraph",
            "html": "<p id='140' style='font-size:14px'>which measures the sensitivity of the output of unit k at time T to a small change<br>in the weight W[u,v], in due consideration of the effect of such a change in the<br>weight over the entire network trajectory from time t' to t. The weight W[u,v]<br>does not have to be connected to unit k, which makes the algorithm non-local.<br>Local changes in the network can have an effect anywhere in the network.</p>",
            "id": 140,
            "page": 16,
            "text": "which measures the sensitivity of the output of unit k at time T to a small change in the weight W[u,v], in due consideration of the effect of such a change in the weight over the entire network trajectory from time t' to t. The weight W[u,v] does not have to be connected to unit k, which makes the algorithm non-local. Local changes in the network can have an effect anywhere in the network."
        },
        {
            "bounding_box": [
                {
                    "x": 517,
                    "y": 2036
                },
                {
                    "x": 1958,
                    "y": 2036
                },
                {
                    "x": 1958,
                    "y": 2130
                },
                {
                    "x": 517,
                    "y": 2130
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='141' style='font-size:14px'>In RTRL, the gradient information is forward-propagated. Using Equa-<br>tions 6 and 7, the output yk(t+ 1) at time step t + 1 is given by</p>",
            "id": 141,
            "page": 16,
            "text": "In RTRL, the gradient information is forward-propagated. Using Equations 6 and 7, the output yk(t+ 1) at time step t + 1 is given by"
        },
        {
            "bounding_box": [
                {
                    "x": 515,
                    "y": 2268
                },
                {
                    "x": 959,
                    "y": 2268
                },
                {
                    "x": 959,
                    "y": 2316
                },
                {
                    "x": 515,
                    "y": 2316
                }
            ],
            "category": "paragraph",
            "html": "<p id='142' style='font-size:16px'>with the weighted input</p>",
            "id": 142,
            "page": 16,
            "text": "with the weighted input"
        },
        {
            "bounding_box": [
                {
                    "x": 576,
                    "y": 2608
                },
                {
                    "x": 1956,
                    "y": 2608
                },
                {
                    "x": 1956,
                    "y": 2657
                },
                {
                    "x": 576,
                    "y": 2657
                }
            ],
            "category": "paragraph",
            "html": "<p id='143' style='font-size:14px'>By differentiating Equations 15, 16 and 17, we can calculate results for all</p>",
            "id": 143,
            "page": 16,
            "text": "By differentiating Equations 15, 16 and 17, we can calculate results for all"
        },
        {
            "bounding_box": [
                {
                    "x": 1213,
                    "y": 3096
                },
                {
                    "x": 1261,
                    "y": 3096
                },
                {
                    "x": 1261,
                    "y": 3139
                },
                {
                    "x": 1213,
                    "y": 3139
                }
            ],
            "category": "footer",
            "html": "<footer id='144' style='font-size:16px'>16</footer>",
            "id": 144,
            "page": 16,
            "text": "16"
        },
        {
            "bounding_box": [
                {
                    "x": 514,
                    "y": 530
                },
                {
                    "x": 947,
                    "y": 530
                },
                {
                    "x": 947,
                    "y": 578
                },
                {
                    "x": 514,
                    "y": 578
                }
            ],
            "category": "paragraph",
            "html": "<p id='145' style='font-size:14px'>time steps ≥ t + 1 with</p>",
            "id": 145,
            "page": 17,
            "text": "time steps ≥ t + 1 with"
        },
        {
            "bounding_box": [
                {
                    "x": 514,
                    "y": 1635
                },
                {
                    "x": 1274,
                    "y": 1635
                },
                {
                    "x": 1274,
                    "y": 1684
                },
                {
                    "x": 514,
                    "y": 1684
                }
            ],
            "category": "paragraph",
            "html": "<p id='146' style='font-size:18px'>where Suk is the Kronecker delta; that is,</p>",
            "id": 146,
            "page": 17,
            "text": "where Suk is the Kronecker delta; that is,"
        },
        {
            "bounding_box": [
                {
                    "x": 513,
                    "y": 1873
                },
                {
                    "x": 1959,
                    "y": 1873
                },
                {
                    "x": 1959,
                    "y": 1968
                },
                {
                    "x": 513,
                    "y": 1968
                }
            ],
            "category": "paragraph",
            "html": "<p id='147' style='font-size:14px'>Assuming that the initial state of the network has no functional dependency on<br>the weights, the derivative for the first time step is</p>",
            "id": 147,
            "page": 17,
            "text": "Assuming that the initial state of the network has no functional dependency on the weights, the derivative for the first time step is"
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 2143
                },
                {
                    "x": 1960,
                    "y": 2143
                },
                {
                    "x": 1960,
                    "y": 2343
                },
                {
                    "x": 511,
                    "y": 2343
                }
            ],
            "category": "paragraph",
            "html": "<p id='148' style='font-size:16px'>Equation 18 shows how pku(t + 1) can be calculated in terms of pkuv(t). In<br>this sense, the learning algorithm becomes incremental, SO that we can learn<br>as we receive new inputs (in real time), and we no longer need to perform<br>back-propagation through time.</p>",
            "id": 148,
            "page": 17,
            "text": "Equation 18 shows how pku(t + 1) can be calculated in terms of pkuv(t). In this sense, the learning algorithm becomes incremental, SO that we can learn as we receive new inputs (in real time), and we no longer need to perform back-propagation through time."
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 2345
                },
                {
                    "x": 1959,
                    "y": 2345
                },
                {
                    "x": 1959,
                    "y": 2693
                },
                {
                    "x": 511,
                    "y": 2693
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='149' style='font-size:18px'>Knowing the initial value for pkuv at time t' from Equation 19, we can re-<br>cursively calculate the quantities pkuv for the first and all subsequent time steps<br>using Equation 18. Note that pkuv (T) uses the values of W[u,v] at t', and not<br>values in-between t' and T. Combining these values with the error vector e(T)<br>for that time step, using Equation 14, we can finally calculate the negative error<br>gradient ▽WE(T). The final weight change for W[u,v] can be calculated using<br>Equations 14 and 13.</p>",
            "id": 149,
            "page": 17,
            "text": "Knowing the initial value for pkuv at time t' from Equation 19, we can recursively calculate the quantities pkuv for the first and all subsequent time steps using Equation 18. Note that pkuv (T) uses the values of W[u,v] at t', and not values in-between t' and T. Combining these values with the error vector e(T) for that time step, using Equation 14, we can finally calculate the negative error gradient ▽WE(T). The final weight change for W[u,v] can be calculated using Equations 14 and 13."
        },
        {
            "bounding_box": [
                {
                    "x": 580,
                    "y": 2692
                },
                {
                    "x": 1953,
                    "y": 2692
                },
                {
                    "x": 1953,
                    "y": 2747
                },
                {
                    "x": 580,
                    "y": 2747
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='150' style='font-size:18px'>A more detailed description of the RTRL algorithm is given in [75] and [76].</p>",
            "id": 150,
            "page": 17,
            "text": "A more detailed description of the RTRL algorithm is given in  and ."
        },
        {
            "bounding_box": [
                {
                    "x": 515,
                    "y": 2816
                },
                {
                    "x": 1719,
                    "y": 2816
                },
                {
                    "x": 1719,
                    "y": 2879
                },
                {
                    "x": 515,
                    "y": 2879
                }
            ],
            "category": "paragraph",
            "html": "<p id='151' style='font-size:22px'>7 Solving the Vanishing Error Problem</p>",
            "id": 151,
            "page": 17,
            "text": "7 Solving the Vanishing Error Problem"
        },
        {
            "bounding_box": [
                {
                    "x": 513,
                    "y": 2919
                },
                {
                    "x": 1961,
                    "y": 2919
                },
                {
                    "x": 1961,
                    "y": 3022
                },
                {
                    "x": 513,
                    "y": 3022
                }
            ],
            "category": "paragraph",
            "html": "<p id='152' style='font-size:14px'>Standard RNN cannot bridge more than 5-10 time steps ([22]). This is due to<br>that back-propagated error signals tend to either grow or shrink with every time</p>",
            "id": 152,
            "page": 17,
            "text": "Standard RNN cannot bridge more than 5-10 time steps (). This is due to that back-propagated error signals tend to either grow or shrink with every time"
        },
        {
            "bounding_box": [
                {
                    "x": 1213,
                    "y": 3098
                },
                {
                    "x": 1260,
                    "y": 3098
                },
                {
                    "x": 1260,
                    "y": 3138
                },
                {
                    "x": 1213,
                    "y": 3138
                }
            ],
            "category": "footer",
            "html": "<footer id='153' style='font-size:16px'>17</footer>",
            "id": 153,
            "page": 17,
            "text": "17"
        },
        {
            "bounding_box": [
                {
                    "x": 513,
                    "y": 530
                },
                {
                    "x": 1960,
                    "y": 530
                },
                {
                    "x": 1960,
                    "y": 725
                },
                {
                    "x": 513,
                    "y": 725
                }
            ],
            "category": "paragraph",
            "html": "<p id='154' style='font-size:16px'>step. Over many time steps the error therefore typically blows-up or vanishes<br>([5, 42]). Blown-up error signals lead straight to oscillating weights, whereas<br>with a vanishing error, learning takes an unacceptable amount of time, or does<br>not work at all.</p>",
            "id": 154,
            "page": 18,
            "text": "step. Over many time steps the error therefore typically blows-up or vanishes (). Blown-up error signals lead straight to oscillating weights, whereas with a vanishing error, learning takes an unacceptable amount of time, or does not work at all."
        },
        {
            "bounding_box": [
                {
                    "x": 513,
                    "y": 730
                },
                {
                    "x": 1958,
                    "y": 730
                },
                {
                    "x": 1958,
                    "y": 877
                },
                {
                    "x": 513,
                    "y": 877
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='155' style='font-size:14px'>The explanation of how gradients are computed by the standard backpropa-<br>gation algorithm and the basic vanishing error analysis is as follows: we update<br>weights after the network has trained from time t' to time t using the formula</p>",
            "id": 155,
            "page": 18,
            "text": "The explanation of how gradients are computed by the standard backpropagation algorithm and the basic vanishing error analysis is as follows: we update weights after the network has trained from time t' to time t using the formula"
        },
        {
            "bounding_box": [
                {
                    "x": 514,
                    "y": 1055
                },
                {
                    "x": 604,
                    "y": 1055
                },
                {
                    "x": 604,
                    "y": 1097
                },
                {
                    "x": 514,
                    "y": 1097
                }
            ],
            "category": "paragraph",
            "html": "<p id='156' style='font-size:14px'>with</p>",
            "id": 156,
            "page": 18,
            "text": "with"
        },
        {
            "bounding_box": [
                {
                    "x": 510,
                    "y": 1243
                },
                {
                    "x": 1959,
                    "y": 1243
                },
                {
                    "x": 1959,
                    "y": 1340
                },
                {
                    "x": 510,
                    "y": 1340
                }
            ],
            "category": "paragraph",
            "html": "<p id='157' style='font-size:14px'>where the backpropagated error signal at time T (with t' ≤T<t) of the unit u<br>is</p>",
            "id": 157,
            "page": 18,
            "text": "where the backpropagated error signal at time T (with t' ≤T<t) of the unit u is"
        },
        {
            "bounding_box": [
                {
                    "x": 512,
                    "y": 1501
                },
                {
                    "x": 1959,
                    "y": 1501
                },
                {
                    "x": 1959,
                    "y": 1748
                },
                {
                    "x": 512,
                    "y": 1748
                }
            ],
            "category": "paragraph",
            "html": "<p id='158' style='font-size:14px'>Consequently, given a fully recurrent neural network with a set of non-input<br>units U, the error signal that occurs at any chosen output-layer neuron 0 E 0,<br>at time-step T, is propagated back through time for t-t' time-steps, with t' <t<br>to an arbitrary neuron v. This causes the error to be scaled by the following<br>factor:</p>",
            "id": 158,
            "page": 18,
            "text": "Consequently, given a fully recurrent neural network with a set of non-input units U, the error signal that occurs at any chosen output-layer neuron 0 E 0, at time-step T, is propagated back through time for t-t' time-steps, with t' <t to an arbitrary neuron v. This causes the error to be scaled by the following factor:"
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 1951
                },
                {
                    "x": 1960,
                    "y": 1951
                },
                {
                    "x": 1960,
                    "y": 2103
                },
                {
                    "x": 511,
                    "y": 2103
                }
            ],
            "category": "paragraph",
            "html": "<p id='159' style='font-size:14px'>To solve the above equation, we unroll it over time. For t' ≤Ⓣ ≤t, let UT be<br>a non-input-layer neuron in one of the replicas in the unrolled network at time<br>T. Now, by setting Ut = v and ut' = 0, we obtain the equation</p>",
            "id": 159,
            "page": 18,
            "text": "To solve the above equation, we unroll it over time. For t' ≤Ⓣ ≤t, let UT be a non-input-layer neuron in one of the replicas in the unrolled network at time T. Now, by setting Ut = v and ut' = 0, we obtain the equation"
        },
        {
            "bounding_box": [
                {
                    "x": 515,
                    "y": 2311
                },
                {
                    "x": 1260,
                    "y": 2311
                },
                {
                    "x": 1260,
                    "y": 2359
                },
                {
                    "x": 515,
                    "y": 2359
                }
            ],
            "category": "paragraph",
            "html": "<p id='160' style='font-size:16px'>Observing Equation 21, it follows that if</p>",
            "id": 160,
            "page": 18,
            "text": "Observing Equation 21, it follows that if"
        },
        {
            "bounding_box": [
                {
                    "x": 513,
                    "y": 2489
                },
                {
                    "x": 1958,
                    "y": 2489
                },
                {
                    "x": 1958,
                    "y": 2635
                },
                {
                    "x": 513,
                    "y": 2635
                }
            ],
            "category": "paragraph",
            "html": "<p id='161' style='font-size:16px'>for all T, then the product will grow exponentially, causing the error to blow-up;<br>moreover, conflicting error signals arriving at neuron v can lead to oscillating<br>weights and unstable learning. If now</p>",
            "id": 161,
            "page": 18,
            "text": "for all T, then the product will grow exponentially, causing the error to blow-up; moreover, conflicting error signals arriving at neuron v can lead to oscillating weights and unstable learning. If now"
        },
        {
            "bounding_box": [
                {
                    "x": 512,
                    "y": 2764
                },
                {
                    "x": 1958,
                    "y": 2764
                },
                {
                    "x": 1958,
                    "y": 2909
                },
                {
                    "x": 512,
                    "y": 2909
                }
            ],
            "category": "paragraph",
            "html": "<p id='162' style='font-size:16px'>for all T, then the product decreases exponentially, causing the error to vanish,<br>preventing the network from learning within an acceptable time period. Finally,<br>the equation</p>",
            "id": 162,
            "page": 18,
            "text": "for all T, then the product decreases exponentially, causing the error to vanish, preventing the network from learning within an acceptable time period. Finally, the equation"
        },
        {
            "bounding_box": [
                {
                    "x": 1213,
                    "y": 3098
                },
                {
                    "x": 1260,
                    "y": 3098
                },
                {
                    "x": 1260,
                    "y": 3137
                },
                {
                    "x": 1213,
                    "y": 3137
                }
            ],
            "category": "footer",
            "html": "<footer id='163' style='font-size:14px'>18</footer>",
            "id": 163,
            "page": 18,
            "text": "18"
        },
        {
            "bounding_box": [
                {
                    "x": 513,
                    "y": 530
                },
                {
                    "x": 1849,
                    "y": 530
                },
                {
                    "x": 1849,
                    "y": 575
                },
                {
                    "x": 513,
                    "y": 575
                }
            ],
            "category": "paragraph",
            "html": "<p id='164' style='font-size:14px'>shows that if the local error vanishes, then the global error also vanishes.</p>",
            "id": 164,
            "page": 19,
            "text": "shows that if the local error vanishes, then the global error also vanishes."
        },
        {
            "bounding_box": [
                {
                    "x": 513,
                    "y": 583
                },
                {
                    "x": 1958,
                    "y": 583
                },
                {
                    "x": 1958,
                    "y": 728
                },
                {
                    "x": 513,
                    "y": 728
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='165' style='font-size:16px'>A more detailed theoretical analysis of the problem with long-term depen-<br>dencies is presented in [39]. The paper also briefly outlines several proposals on<br>how to address this problem.</p>",
            "id": 165,
            "page": 19,
            "text": "A more detailed theoretical analysis of the problem with long-term dependencies is presented in . The paper also briefly outlines several proposals on how to address this problem."
        },
        {
            "bounding_box": [
                {
                    "x": 514,
                    "y": 805
                },
                {
                    "x": 1654,
                    "y": 805
                },
                {
                    "x": 1654,
                    "y": 865
                },
                {
                    "x": 514,
                    "y": 865
                }
            ],
            "category": "paragraph",
            "html": "<p id='166' style='font-size:22px'>8 Long Short- Term Neural Networks</p>",
            "id": 166,
            "page": 19,
            "text": "8 Long Short- Term Neural Networks"
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 906
                },
                {
                    "x": 1959,
                    "y": 906
                },
                {
                    "x": 1959,
                    "y": 1205
                },
                {
                    "x": 511,
                    "y": 1205
                }
            ],
            "category": "paragraph",
            "html": "<p id='167' style='font-size:16px'>One solution that addresses the vanishing error problem is a gradient-based<br>method called long short-term memory (LSTM) published by [41], [42], [22]<br>and [23]. LSTM can learn how to bridge minimal time lags of more than 1,000<br>discrete time steps. The solution uses constant error carousels (CECs), which<br>enforce a constant error flow within special cells. Access to the cells is handled<br>by multiplicative gate units, which learn when to grant access.</p>",
            "id": 167,
            "page": 19,
            "text": "One solution that addresses the vanishing error problem is a gradient-based method called long short-term memory (LSTM) published by , ,  and . LSTM can learn how to bridge minimal time lags of more than 1,000 discrete time steps. The solution uses constant error carousels (CECs), which enforce a constant error flow within special cells. Access to the cells is handled by multiplicative gate units, which learn when to grant access."
        },
        {
            "bounding_box": [
                {
                    "x": 513,
                    "y": 1266
                },
                {
                    "x": 1259,
                    "y": 1266
                },
                {
                    "x": 1259,
                    "y": 1322
                },
                {
                    "x": 513,
                    "y": 1322
                }
            ],
            "category": "paragraph",
            "html": "<p id='168' style='font-size:18px'>8.1 Constant Error Carousel</p>",
            "id": 168,
            "page": 19,
            "text": "8.1 Constant Error Carousel"
        },
        {
            "bounding_box": [
                {
                    "x": 512,
                    "y": 1348
                },
                {
                    "x": 1959,
                    "y": 1348
                },
                {
                    "x": 1959,
                    "y": 1496
                },
                {
                    "x": 512,
                    "y": 1496
                }
            ],
            "category": "paragraph",
            "html": "<p id='169' style='font-size:14px'>Suppose that we have only one unit u with a single connection to itself. The<br>local error back flow of u at a single time-step T follows from Equation 20 and<br>is given by</p>",
            "id": 169,
            "page": 19,
            "text": "Suppose that we have only one unit u with a single connection to itself. The local error back flow of u at a single time-step T follows from Equation 20 and is given by"
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 1573
                },
                {
                    "x": 1955,
                    "y": 1573
                },
                {
                    "x": 1955,
                    "y": 1670
                },
                {
                    "x": 511,
                    "y": 1670
                }
            ],
            "category": "paragraph",
            "html": "<p id='170' style='font-size:14px'>From Equations 22 and 23 we see that, in order to ensure a constant error flow<br>through u, we need to have</p>",
            "id": 170,
            "page": 19,
            "text": "From Equations 22 and 23 we see that, in order to ensure a constant error flow through u, we need to have"
        },
        {
            "bounding_box": [
                {
                    "x": 513,
                    "y": 1807
                },
                {
                    "x": 1019,
                    "y": 1807
                },
                {
                    "x": 1019,
                    "y": 1854
                },
                {
                    "x": 513,
                    "y": 1854
                }
            ],
            "category": "paragraph",
            "html": "<p id='171' style='font-size:16px'>and by integration we have</p>",
            "id": 171,
            "page": 19,
            "text": "and by integration we have"
        },
        {
            "bounding_box": [
                {
                    "x": 512,
                    "y": 2036
                },
                {
                    "x": 1958,
                    "y": 2036
                },
                {
                    "x": 1958,
                    "y": 2137
                },
                {
                    "x": 512,
                    "y": 2137
                }
            ],
            "category": "paragraph",
            "html": "<p id='172' style='font-size:14px'>From this, we learn that fu must be linear, and that u's activation must remain<br>constant over time; i.e.,</p>",
            "id": 172,
            "page": 19,
            "text": "From this, we learn that fu must be linear, and that u's activation must remain constant over time; i.e.,"
        },
        {
            "bounding_box": [
                {
                    "x": 512,
                    "y": 2267
                },
                {
                    "x": 1960,
                    "y": 2267
                },
                {
                    "x": 1960,
                    "y": 2566
                },
                {
                    "x": 512,
                    "y": 2566
                }
            ],
            "category": "paragraph",
            "html": "<p id='173' style='font-size:16px'>This is ensured by using the identity function fu = id, and by setting W[u,u] =<br>1.0. This preservation of error is called the constant error carousel (CEC), and<br>it is the central feature of LSTM, where short-term memory storage is achieved<br>for extended periods of time. Clearly, we still need to handle the connections<br>from other units to the unit u, and this is where the different components of<br>LSTM networks come into the picture.</p>",
            "id": 173,
            "page": 19,
            "text": "This is ensured by using the identity function fu = id, and by setting W[u,u] = 1.0. This preservation of error is called the constant error carousel (CEC), and it is the central feature of LSTM, where short-term memory storage is achieved for extended periods of time. Clearly, we still need to handle the connections from other units to the unit u, and this is where the different components of LSTM networks come into the picture."
        },
        {
            "bounding_box": [
                {
                    "x": 514,
                    "y": 2628
                },
                {
                    "x": 1036,
                    "y": 2628
                },
                {
                    "x": 1036,
                    "y": 2684
                },
                {
                    "x": 514,
                    "y": 2684
                }
            ],
            "category": "paragraph",
            "html": "<p id='174' style='font-size:18px'>8.2 Memory Blocks</p>",
            "id": 174,
            "page": 19,
            "text": "8.2 Memory Blocks"
        },
        {
            "bounding_box": [
                {
                    "x": 512,
                    "y": 2709
                },
                {
                    "x": 1960,
                    "y": 2709
                },
                {
                    "x": 1960,
                    "y": 3010
                },
                {
                    "x": 512,
                    "y": 3010
                }
            ],
            "category": "paragraph",
            "html": "<p id='175' style='font-size:16px'>In the absence of new inputs to the cell, we now know that the CEC's backflow<br>remains constant. However, as part of a neural network, the CEC is not only<br>connected to itself, but also to other units in the neural network. We need<br>to take these additional weighted inputs and outputs into account. Incoming<br>connections to neuron u can have conflicting weight update signals, because<br>the same weight is used for storing and ignoring inputs. For weighted output</p>",
            "id": 175,
            "page": 19,
            "text": "In the absence of new inputs to the cell, we now know that the CEC's backflow remains constant. However, as part of a neural network, the CEC is not only connected to itself, but also to other units in the neural network. We need to take these additional weighted inputs and outputs into account. Incoming connections to neuron u can have conflicting weight update signals, because the same weight is used for storing and ignoring inputs. For weighted output"
        },
        {
            "bounding_box": [
                {
                    "x": 1213,
                    "y": 3097
                },
                {
                    "x": 1260,
                    "y": 3097
                },
                {
                    "x": 1260,
                    "y": 3137
                },
                {
                    "x": 1213,
                    "y": 3137
                }
            ],
            "category": "footer",
            "html": "<footer id='176' style='font-size:14px'>19</footer>",
            "id": 176,
            "page": 19,
            "text": "19"
        },
        {
            "bounding_box": [
                {
                    "x": 510,
                    "y": 531
                },
                {
                    "x": 1957,
                    "y": 531
                },
                {
                    "x": 1957,
                    "y": 625
                },
                {
                    "x": 510,
                    "y": 625
                }
            ],
            "category": "paragraph",
            "html": "<p id='177' style='font-size:14px'>connections from neuron u, the same weights can be used to both retrieve u's<br>contents and prevent u's output flow to other neurons in the network.</p>",
            "id": 177,
            "page": 20,
            "text": "connections from neuron u, the same weights can be used to both retrieve u's contents and prevent u's output flow to other neurons in the network."
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 630
                },
                {
                    "x": 1960,
                    "y": 630
                },
                {
                    "x": 1960,
                    "y": 825
                },
                {
                    "x": 511,
                    "y": 825
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='178' style='font-size:18px'>To address the problem of conflicting weight updates, LSTM extends the<br>CEC with input and output gates connected to the network input layer and<br>to other memory cells. This results in a more complex LSTM unit, called a<br>memory block; its standard architecture is shown in Figure 11.</p>",
            "id": 178,
            "page": 20,
            "text": "To address the problem of conflicting weight updates, LSTM extends the CEC with input and output gates connected to the network input layer and to other memory cells. This results in a more complex LSTM unit, called a memory block; its standard architecture is shown in Figure 11."
        },
        {
            "bounding_box": [
                {
                    "x": 509,
                    "y": 830
                },
                {
                    "x": 1961,
                    "y": 830
                },
                {
                    "x": 1961,
                    "y": 1276
                },
                {
                    "x": 509,
                    "y": 1276
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='179' style='font-size:16px'>The input gates, which are simple sigmoid threshold units with an activation<br>function range of [0, 1], control the signals from the network to the memory cell<br>by scaling them appropriately; when the gate is closed, activation is close to<br>zero. Additionally, these can learn to protect the contents stored in u from<br>disturbance by irrelevant signals. The activation of a CEC by the input gate is<br>defined as the cell state. The output gates can learn how to control access to<br>the memory cell contents, which protects other memory cells from disturbances<br>originating from u. So we can see that the basic function of multiplicative gate<br>units is to either allow or deny access to constant error flow through the CEC.</p>",
            "id": 179,
            "page": 20,
            "text": "The input gates, which are simple sigmoid threshold units with an activation function range of , control the signals from the network to the memory cell by scaling them appropriately; when the gate is closed, activation is close to zero. Additionally, these can learn to protect the contents stored in u from disturbance by irrelevant signals. The activation of a CEC by the input gate is defined as the cell state. The output gates can learn how to control access to the memory cell contents, which protects other memory cells from disturbances originating from u. So we can see that the basic function of multiplicative gate units is to either allow or deny access to constant error flow through the CEC."
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 1349
                },
                {
                    "x": 1961,
                    "y": 1349
                },
                {
                    "x": 1961,
                    "y": 1490
                },
                {
                    "x": 511,
                    "y": 1490
                }
            ],
            "category": "paragraph",
            "html": "<p id='180' style='font-size:22px'>9 Training LSTM-RNNs - the Hybrid Learning<br>Approach</p>",
            "id": 180,
            "page": 20,
            "text": "9 Training LSTM-RNNs - the Hybrid Learning Approach"
        },
        {
            "bounding_box": [
                {
                    "x": 510,
                    "y": 1527
                },
                {
                    "x": 1962,
                    "y": 1527
                },
                {
                    "x": 1962,
                    "y": 1928
                },
                {
                    "x": 510,
                    "y": 1928
                }
            ],
            "category": "paragraph",
            "html": "<p id='181' style='font-size:16px'>In order to preserve the CEC in LSTM memory block cells, the original formu-<br>lation of LSTM used a combination of two learning algorithms: BPTT to train<br>network components located after cells, and RTRL to train network components<br>located before and including cells. The latter units work with RTRL because<br>there are some partial derivatives (related to the state of the cell) that need to<br>be computed during every step, no matter if a target value is given or not at that<br>step. For now, we only allow the gradient of the cell to be propagated through<br>time, truncating the rest of the gradients for the other recurrent connections.</p>",
            "id": 181,
            "page": 20,
            "text": "In order to preserve the CEC in LSTM memory block cells, the original formulation of LSTM used a combination of two learning algorithms: BPTT to train network components located after cells, and RTRL to train network components located before and including cells. The latter units work with RTRL because there are some partial derivatives (related to the state of the cell) that need to be computed during every step, no matter if a target value is given or not at that step. For now, we only allow the gradient of the cell to be propagated through time, truncating the rest of the gradients for the other recurrent connections."
        },
        {
            "bounding_box": [
                {
                    "x": 510,
                    "y": 1929
                },
                {
                    "x": 1961,
                    "y": 1929
                },
                {
                    "x": 1961,
                    "y": 2128
                },
                {
                    "x": 510,
                    "y": 2128
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='182' style='font-size:14px'>We define discrete time steps in the form T = 1, 2, 3, .... Each step has a<br>forward pass and a backward pass; in the forward pass the output/activation<br>of all units are calculated, whereas in the backward pass, the calculation of the<br>error signals for all weights is performed.</p>",
            "id": 182,
            "page": 20,
            "text": "We define discrete time steps in the form T = 1, 2, 3, .... Each step has a forward pass and a backward pass; in the forward pass the output/activation of all units are calculated, whereas in the backward pass, the calculation of the error signals for all weights is performed."
        },
        {
            "bounding_box": [
                {
                    "x": 515,
                    "y": 2187
                },
                {
                    "x": 1095,
                    "y": 2187
                },
                {
                    "x": 1095,
                    "y": 2243
                },
                {
                    "x": 515,
                    "y": 2243
                }
            ],
            "category": "paragraph",
            "html": "<p id='183' style='font-size:20px'>9.1 The Forward Pass</p>",
            "id": 183,
            "page": 20,
            "text": "9.1 The Forward Pass"
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 2266
                },
                {
                    "x": 1957,
                    "y": 2266
                },
                {
                    "x": 1957,
                    "y": 2368
                },
                {
                    "x": 511,
                    "y": 2368
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='184' style='font-size:14px'>Let M be the set of memory blocks. Let mc be the c-th memory cell in the<br>memory block m, and W[u,v] be a weight connecting unit u to unit v.</p>",
            "id": 184,
            "page": 20,
            "text": "Let M be the set of memory blocks. Let mc be the c-th memory cell in the memory block m, and W[u,v] be a weight connecting unit u to unit v."
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 2373
                },
                {
                    "x": 1961,
                    "y": 2373
                },
                {
                    "x": 1961,
                    "y": 2667
                },
                {
                    "x": 511,
                    "y": 2667
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='185' style='font-size:18px'>In the original formulation of LSTM, each memory block m is associated with<br>one input gate inm and one output gate outm. The internal state of a memory<br>cell mc at time T + 1 is updated according to its state Smc (T) and according<br>to the weighted input Zmc (T + 1) multiplied by the activation of the input gate<br>Yinm (T + 1). Then, we use the activation of the output gate Zoutm (T + 1) to<br>calculate the activation of the cell ymc (T + 1).</p>",
            "id": 185,
            "page": 20,
            "text": "In the original formulation of LSTM, each memory block m is associated with one input gate inm and one output gate outm. The internal state of a memory cell mc at time T + 1 is updated according to its state Smc (T) and according to the weighted input Zmc (T + 1) multiplied by the activation of the input gate Yinm (T + 1). Then, we use the activation of the output gate Zoutm (T + 1) to calculate the activation of the cell ymc (T + 1)."
        },
        {
            "bounding_box": [
                {
                    "x": 578,
                    "y": 2672
                },
                {
                    "x": 1637,
                    "y": 2672
                },
                {
                    "x": 1637,
                    "y": 2717
                },
                {
                    "x": 578,
                    "y": 2717
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='186' style='font-size:16px'>of the input gate inm is computed as<br>The activation Yinm</p>",
            "id": 186,
            "page": 20,
            "text": "of the input gate inm is computed as The activation Yinm"
        },
        {
            "bounding_box": [
                {
                    "x": 1210,
                    "y": 3096
                },
                {
                    "x": 1260,
                    "y": 3096
                },
                {
                    "x": 1260,
                    "y": 3138
                },
                {
                    "x": 1210,
                    "y": 3138
                }
            ],
            "category": "footer",
            "html": "<footer id='187' style='font-size:14px'>20</footer>",
            "id": 187,
            "page": 20,
            "text": "20"
        },
        {
            "bounding_box": [
                {
                    "x": 640,
                    "y": 588
                },
                {
                    "x": 1854,
                    "y": 588
                },
                {
                    "x": 1854,
                    "y": 2493
                },
                {
                    "x": 640,
                    "y": 2493
                }
            ],
            "category": "figure",
            "html": "<figure><img id='188' style='font-size:20px' alt=\"outputs to\nrecurrent next layer\nconnections\n.Yout\noutput\nI I\ngating\nh(x)\nmemorising\nCEC 1.0\nII\n·yin input\nI /\ngating\ng(x)\" data-coord=\"top-left:(640,588); bottom-right:(1854,2493)\" /></figure>",
            "id": 188,
            "page": 21,
            "text": "outputs to recurrent next layer connections .Yout output I I gating h(x) memorising CEC 1.0 II ·yin input I / gating g(x)"
        },
        {
            "bounding_box": [
                {
                    "x": 508,
                    "y": 2545
                },
                {
                    "x": 1963,
                    "y": 2545
                },
                {
                    "x": 1963,
                    "y": 2956
                },
                {
                    "x": 508,
                    "y": 2956
                }
            ],
            "category": "caption",
            "html": "<caption id='189' style='font-size:16px'>Figure 10: A standard LSTM memory block. The block contains (at least) one<br>cell with a recurrent self-connection (CEC) and weight of '1'. The state of the<br>cell is denoted as Sc. Read and write access is regulated by the input gate, Yin,<br>and the output gate, Yout· The internal cell state is calculated by multiplying<br>the result of the squashed input, g, by the result of the input gate, Yin, and<br>then adding the state of the last time step, sc(t - 1). Finally, the cell output<br>is calculated by multiplying the cell state, Sc, by the activation of the output<br>gate, Yout·</caption>",
            "id": 189,
            "page": 21,
            "text": "Figure 10: A standard LSTM memory block. The block contains (at least) one cell with a recurrent self-connection (CEC) and weight of '1'. The state of the cell is denoted as Sc. Read and write access is regulated by the input gate, Yin, and the output gate, Yout· The internal cell state is calculated by multiplying the result of the squashed input, g, by the result of the input gate, Yin, and then adding the state of the last time step, sc(t - 1). Finally, the cell output is calculated by multiplying the cell state, Sc, by the activation of the output gate, Yout·"
        },
        {
            "bounding_box": [
                {
                    "x": 1209,
                    "y": 3097
                },
                {
                    "x": 1258,
                    "y": 3097
                },
                {
                    "x": 1258,
                    "y": 3139
                },
                {
                    "x": 1209,
                    "y": 3139
                }
            ],
            "category": "footer",
            "html": "<footer id='190' style='font-size:14px'>21</footer>",
            "id": 190,
            "page": 21,
            "text": "21"
        },
        {
            "bounding_box": [
                {
                    "x": 646,
                    "y": 676
                },
                {
                    "x": 1832,
                    "y": 676
                },
                {
                    "x": 1832,
                    "y": 2439
                },
                {
                    "x": 646,
                    "y": 2439
                }
            ],
            "category": "figure",
            "html": "<figure><img id='191' style='font-size:20px' alt=\"outputs to\nnext layer\nYmc (T + 1)\nYoutm (Ⓣ+1)\n� II output\ngating\nh(x)\nSmc (T)\n.Smc (T + 1) CEC 1.0\nmemorising\n�\nYinm (T + 1)\ninput\n〉 II\ngating\n시\ng(x)\n� YU(T)\nu: non-input unit\nYi(T + 1)\ni: input unit\" data-coord=\"top-left:(646,676); bottom-right:(1832,2439)\" /></figure>",
            "id": 191,
            "page": 22,
            "text": "outputs to next layer Ymc (T + 1) Youtm (Ⓣ+1) � II output gating h(x) Smc (T) .Smc (T + 1) CEC 1.0 memorising � Yinm (T + 1) input 〉 II gating 시 g(x) � YU(T) u: non-input unit Yi(T + 1) i: input unit"
        },
        {
            "bounding_box": [
                {
                    "x": 508,
                    "y": 2462
                },
                {
                    "x": 1965,
                    "y": 2462
                },
                {
                    "x": 1965,
                    "y": 2872
                },
                {
                    "x": 508,
                    "y": 2872
                }
            ],
            "category": "caption",
            "html": "<br><caption id='192' style='font-size:16px'>Figure 11: A standard LSTM memory block. The block contains (at least)<br>one cell with a recurrent self-connection (CEC) and weight of '1'. The state<br>of the cell is denoted as Sc. Read and write access is regulated by the input<br>gate, Yin, and the output gate, Yout· The internal cell state is calculated by<br>multiplying the result of the squashed input, g(x), by the result of the input<br>gate and then adding the state of the current time step, Smc (T), to the next,<br>Smc (T + 1). Finally, the cell output is calculated by multiplying the cell state<br>by the activation of the output gate.</caption>",
            "id": 192,
            "page": 22,
            "text": "Figure 11: A standard LSTM memory block. The block contains (at least) one cell with a recurrent self-connection (CEC) and weight of '1'. The state of the cell is denoted as Sc. Read and write access is regulated by the input gate, Yin, and the output gate, Yout· The internal cell state is calculated by multiplying the result of the squashed input, g(x), by the result of the input gate and then adding the state of the current time step, Smc (T), to the next, Smc (T + 1). Finally, the cell output is calculated by multiplying the cell state by the activation of the output gate."
        },
        {
            "bounding_box": [
                {
                    "x": 1209,
                    "y": 3096
                },
                {
                    "x": 1261,
                    "y": 3096
                },
                {
                    "x": 1261,
                    "y": 3138
                },
                {
                    "x": 1209,
                    "y": 3138
                }
            ],
            "category": "footer",
            "html": "<footer id='193' style='font-size:14px'>22</footer>",
            "id": 193,
            "page": 22,
            "text": "22"
        },
        {
            "bounding_box": [
                {
                    "x": 521,
                    "y": 1075
                },
                {
                    "x": 1968,
                    "y": 1075
                },
                {
                    "x": 1968,
                    "y": 2349
                },
                {
                    "x": 521,
                    "y": 2349
                }
            ],
            "category": "figure",
            "html": "<figure><img id='194' style='font-size:14px' alt=\"recurrent\nconnections\noutput gate\nh(x)\nCEC\ninput gate\ng(x)\" data-coord=\"top-left:(521,1075); bottom-right:(1968,2349)\" /></figure>",
            "id": 194,
            "page": 23,
            "text": "recurrent connections output gate h(x) CEC input gate g(x)"
        },
        {
            "bounding_box": [
                {
                    "x": 535,
                    "y": 2403
                },
                {
                    "x": 1934,
                    "y": 2403
                },
                {
                    "x": 1934,
                    "y": 2454
                },
                {
                    "x": 535,
                    "y": 2454
                }
            ],
            "category": "caption",
            "html": "<caption id='195' style='font-size:20px'>Figure 12: A three cell LSTM memory block with recurrent self-connections</caption>",
            "id": 195,
            "page": 23,
            "text": "Figure 12: A three cell LSTM memory block with recurrent self-connections"
        },
        {
            "bounding_box": [
                {
                    "x": 1209,
                    "y": 3097
                },
                {
                    "x": 1260,
                    "y": 3097
                },
                {
                    "x": 1260,
                    "y": 3139
                },
                {
                    "x": 1209,
                    "y": 3139
                }
            ],
            "category": "footer",
            "html": "<footer id='196' style='font-size:16px'>23</footer>",
            "id": 196,
            "page": 23,
            "text": "23"
        },
        {
            "bounding_box": [
                {
                    "x": 514,
                    "y": 530
                },
                {
                    "x": 990,
                    "y": 530
                },
                {
                    "x": 990,
                    "y": 578
                },
                {
                    "x": 514,
                    "y": 578
                }
            ],
            "category": "paragraph",
            "html": "<p id='197' style='font-size:16px'>with the input gate input</p>",
            "id": 197,
            "page": 24,
            "text": "with the input gate input"
        },
        {
            "bounding_box": [
                {
                    "x": 515,
                    "y": 852
                },
                {
                    "x": 1286,
                    "y": 852
                },
                {
                    "x": 1286,
                    "y": 901
                },
                {
                    "x": 515,
                    "y": 901
                }
            ],
            "category": "paragraph",
            "html": "<p id='198' style='font-size:16px'>The activation of the output gate outm is</p>",
            "id": 198,
            "page": 24,
            "text": "The activation of the output gate outm is"
        },
        {
            "bounding_box": [
                {
                    "x": 514,
                    "y": 1020
                },
                {
                    "x": 1012,
                    "y": 1020
                },
                {
                    "x": 1012,
                    "y": 1069
                },
                {
                    "x": 514,
                    "y": 1069
                }
            ],
            "category": "paragraph",
            "html": "<p id='199' style='font-size:16px'>with the output gate input</p>",
            "id": 199,
            "page": 24,
            "text": "with the output gate input"
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 1342
                },
                {
                    "x": 1956,
                    "y": 1342
                },
                {
                    "x": 1956,
                    "y": 1444
                },
                {
                    "x": 511,
                    "y": 1444
                }
            ],
            "category": "paragraph",
            "html": "<p id='200' style='font-size:16px'>The results of the gates are scaled using the non-linear squashing function<br>finm = foutm = f, defined by</p>",
            "id": 200,
            "page": 24,
            "text": "The results of the gates are scaled using the non-linear squashing function finm = foutm = f, defined by"
        },
        {
            "bounding_box": [
                {
                    "x": 512,
                    "y": 1598
                },
                {
                    "x": 1955,
                    "y": 1598
                },
                {
                    "x": 1955,
                    "y": 1697
                },
                {
                    "x": 512,
                    "y": 1697
                }
            ],
            "category": "paragraph",
            "html": "<p id='201' style='font-size:16px'>SO that they are within the range [0,1]. Thus, the input for the memory cell<br>will only be able to pass if the signal at the input gate is sufficiently close to '1'.</p>",
            "id": 201,
            "page": 24,
            "text": "SO that they are within the range . Thus, the input for the memory cell will only be able to pass if the signal at the input gate is sufficiently close to '1'."
        },
        {
            "bounding_box": [
                {
                    "x": 516,
                    "y": 1705
                },
                {
                    "x": 1951,
                    "y": 1705
                },
                {
                    "x": 1951,
                    "y": 1793
                },
                {
                    "x": 516,
                    "y": 1793
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='202' style='font-size:16px'>For a memory cell mc in the memory block m, the weighted input Zmc (T+1)<br>is defined by</p>",
            "id": 202,
            "page": 24,
            "text": "For a memory cell mc in the memory block m, the weighted input Zmc (T+1) is defined by"
        },
        {
            "bounding_box": [
                {
                    "x": 513,
                    "y": 2072
                },
                {
                    "x": 1959,
                    "y": 2072
                },
                {
                    "x": 1959,
                    "y": 2270
                },
                {
                    "x": 513,
                    "y": 2270
                }
            ],
            "category": "paragraph",
            "html": "<p id='203' style='font-size:16px'>As we mentioned before, the internal state Smc (ヶ+1) of the unit in the memory<br>cell at time T + 1 is computed differently; the weighted input is squashed and<br>then multiplied by the activation of the input gate, and then the state of the<br>last time step Smc (T) is added. The corresponding equation is</p>",
            "id": 203,
            "page": 24,
            "text": "As we mentioned before, the internal state Smc (ヶ+1) of the unit in the memory cell at time T + 1 is computed differently; the weighted input is squashed and then multiplied by the activation of the input gate, and then the state of the last time step Smc (T) is added. The corresponding equation is"
        },
        {
            "bounding_box": [
                {
                    "x": 514,
                    "y": 2388
                },
                {
                    "x": 1839,
                    "y": 2388
                },
                {
                    "x": 1839,
                    "y": 2443
                },
                {
                    "x": 514,
                    "y": 2443
                }
            ],
            "category": "paragraph",
            "html": "<p id='204' style='font-size:16px'>with Smc (0) = 0 and the non-linear squashing function for the cell input</p>",
            "id": 204,
            "page": 24,
            "text": "with Smc (0) = 0 and the non-linear squashing function for the cell input"
        },
        {
            "bounding_box": [
                {
                    "x": 515,
                    "y": 2593
                },
                {
                    "x": 1547,
                    "y": 2593
                },
                {
                    "x": 1547,
                    "y": 2640
                },
                {
                    "x": 515,
                    "y": 2640
                }
            ],
            "category": "paragraph",
            "html": "<p id='205' style='font-size:18px'>which, in this case, scales the result to the range [-2, 2].</p>",
            "id": 205,
            "page": 24,
            "text": "which, in this case, scales the result to the range [-2, 2]."
        },
        {
            "bounding_box": [
                {
                    "x": 514,
                    "y": 2650
                },
                {
                    "x": 1959,
                    "y": 2650
                },
                {
                    "x": 1959,
                    "y": 2741
                },
                {
                    "x": 514,
                    "y": 2741
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='206' style='font-size:18px'>The output ymc is now calculated by squashing and multiplying the cell state<br>Smc by the activation of the output gate Youtm :</p>",
            "id": 206,
            "page": 24,
            "text": "The output ymc is now calculated by squashing and multiplying the cell state Smc by the activation of the output gate Youtm :"
        },
        {
            "bounding_box": [
                {
                    "x": 515,
                    "y": 2859
                },
                {
                    "x": 1227,
                    "y": 2859
                },
                {
                    "x": 1227,
                    "y": 2906
                },
                {
                    "x": 515,
                    "y": 2906
                }
            ],
            "category": "paragraph",
            "html": "<p id='207' style='font-size:18px'>with the non-linear squashing function</p>",
            "id": 207,
            "page": 24,
            "text": "with the non-linear squashing function"
        },
        {
            "bounding_box": [
                {
                    "x": 1210,
                    "y": 3096
                },
                {
                    "x": 1260,
                    "y": 3096
                },
                {
                    "x": 1260,
                    "y": 3139
                },
                {
                    "x": 1210,
                    "y": 3139
                }
            ],
            "category": "footer",
            "html": "<footer id='208' style='font-size:14px'>24</footer>",
            "id": 208,
            "page": 24,
            "text": "24"
        },
        {
            "bounding_box": [
                {
                    "x": 515,
                    "y": 529
                },
                {
                    "x": 859,
                    "y": 529
                },
                {
                    "x": 859,
                    "y": 579
                },
                {
                    "x": 515,
                    "y": 579
                }
            ],
            "category": "paragraph",
            "html": "<p id='209' style='font-size:18px'>with range [-1,1].</p>",
            "id": 209,
            "page": 25,
            "text": "with range [-1,1]."
        },
        {
            "bounding_box": [
                {
                    "x": 513,
                    "y": 581
                },
                {
                    "x": 1958,
                    "y": 581
                },
                {
                    "x": 1958,
                    "y": 726
                },
                {
                    "x": 513,
                    "y": 726
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='210' style='font-size:14px'>Assuming a layered, recurrent neural network with standard input, standard<br>output and hidden layer consisting of memory blocks, the activation of the<br>output unit 0 is computed as</p>",
            "id": 210,
            "page": 25,
            "text": "Assuming a layered, recurrent neural network with standard input, standard output and hidden layer consisting of memory blocks, the activation of the output unit 0 is computed as"
        },
        {
            "bounding_box": [
                {
                    "x": 514,
                    "y": 845
                },
                {
                    "x": 603,
                    "y": 845
                },
                {
                    "x": 603,
                    "y": 886
                },
                {
                    "x": 514,
                    "y": 886
                }
            ],
            "category": "paragraph",
            "html": "<p id='211' style='font-size:14px'>with</p>",
            "id": 211,
            "page": 25,
            "text": "with"
        },
        {
            "bounding_box": [
                {
                    "x": 513,
                    "y": 1004
                },
                {
                    "x": 1957,
                    "y": 1004
                },
                {
                    "x": 1957,
                    "y": 1101
                },
                {
                    "x": 513,
                    "y": 1101
                }
            ],
            "category": "paragraph",
            "html": "<p id='212' style='font-size:14px'>where G is the set of gate units, and we can again use the logistic sigmoid in<br>Equation 28 as a squashing function fo.</p>",
            "id": 212,
            "page": 25,
            "text": "where G is the set of gate units, and we can again use the logistic sigmoid in Equation 28 as a squashing function fo."
        },
        {
            "bounding_box": [
                {
                    "x": 515,
                    "y": 1160
                },
                {
                    "x": 968,
                    "y": 1160
                },
                {
                    "x": 968,
                    "y": 1213
                },
                {
                    "x": 515,
                    "y": 1213
                }
            ],
            "category": "paragraph",
            "html": "<p id='213' style='font-size:22px'>9.2 Forget Gates</p>",
            "id": 213,
            "page": 25,
            "text": "9.2 Forget Gates"
        },
        {
            "bounding_box": [
                {
                    "x": 513,
                    "y": 1241
                },
                {
                    "x": 1959,
                    "y": 1241
                },
                {
                    "x": 1959,
                    "y": 1536
                },
                {
                    "x": 513,
                    "y": 1536
                }
            ],
            "category": "paragraph",
            "html": "<p id='214' style='font-size:14px'>The self-connection in a standard LSTM network has a fixed weight set to '1' in<br>order to preserve the cell state over time. Unfortunately, the cell states Sm tend<br>to grow linearly during the progression of a time series presented in a continuous<br>input stream. The main negative effect is that the entire memory cell loses its<br>memorising capability, and begins to function like an ordinary RNN network<br>neuron.</p>",
            "id": 214,
            "page": 25,
            "text": "The self-connection in a standard LSTM network has a fixed weight set to '1' in order to preserve the cell state over time. Unfortunately, the cell states Sm tend to grow linearly during the progression of a time series presented in a continuous input stream. The main negative effect is that the entire memory cell loses its memorising capability, and begins to function like an ordinary RNN network neuron."
        },
        {
            "bounding_box": [
                {
                    "x": 514,
                    "y": 1542
                },
                {
                    "x": 1959,
                    "y": 1542
                },
                {
                    "x": 1959,
                    "y": 1737
                },
                {
                    "x": 514,
                    "y": 1737
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='215' style='font-size:14px'>By manually resetting the state of the cell at the beginning of each sequence,<br>the cell state growth can be limited, but this is not practical for continuous input<br>where there is no distinguishable end, or subdivision is very complex and error<br>prone.</p>",
            "id": 215,
            "page": 25,
            "text": "By manually resetting the state of the cell at the beginning of each sequence, the cell state growth can be limited, but this is not practical for continuous input where there is no distinguishable end, or subdivision is very complex and error prone."
        },
        {
            "bounding_box": [
                {
                    "x": 514,
                    "y": 1741
                },
                {
                    "x": 1960,
                    "y": 1741
                },
                {
                    "x": 1960,
                    "y": 2036
                },
                {
                    "x": 514,
                    "y": 2036
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='216' style='font-size:14px'>To address this problem, [22] suggested that an adaptive forget gate could<br>be attached to the self-connection. Forget gates can learn to reset the internal<br>state of the memory cell when the stored information is no longer needed. To<br>this end, we replace the weight '1.0' of the self-connection from the CEC with<br>a multiplicative, forget gate activation which is computed using a similar<br>Y⌀,<br>method as for the other gates:</p>",
            "id": 216,
            "page": 25,
            "text": "To address this problem,  suggested that an adaptive forget gate could be attached to the self-connection. Forget gates can learn to reset the internal state of the memory cell when the stored information is no longer needed. To this end, we replace the weight '1.0' of the self-connection from the CEC with a multiplicative, forget gate activation which is computed using a similar Y⌀, method as for the other gates:"
        },
        {
            "bounding_box": [
                {
                    "x": 514,
                    "y": 2153
                },
                {
                    "x": 1956,
                    "y": 2153
                },
                {
                    "x": 1956,
                    "y": 2248
                },
                {
                    "x": 514,
                    "y": 2248
                }
            ],
            "category": "paragraph",
            "html": "<p id='217' style='font-size:14px'>where f is the squashing function from Equation 28 with a range [0,1], 6�m is<br>the bias of the forget gate, and</p>",
            "id": 217,
            "page": 25,
            "text": "where f is the squashing function from Equation 28 with a range , 6�m is the bias of the forget gate, and"
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 2522
                },
                {
                    "x": 1956,
                    "y": 2522
                },
                {
                    "x": 1956,
                    "y": 2619
                },
                {
                    "x": 511,
                    "y": 2619
                }
            ],
            "category": "paragraph",
            "html": "<p id='218' style='font-size:16px'>Originally, 6�m is set to 0, however, following the recommendation by [47], we<br>fix 6�m to 1, in order to improve the performance of LSTM (see Section 10.3).</p>",
            "id": 218,
            "page": 25,
            "text": "Originally, 6�m is set to 0, however, following the recommendation by , we fix 6�m to 1, in order to improve the performance of LSTM (see Section 10.3)."
        },
        {
            "bounding_box": [
                {
                    "x": 577,
                    "y": 2625
                },
                {
                    "x": 1782,
                    "y": 2625
                },
                {
                    "x": 1782,
                    "y": 2668
                },
                {
                    "x": 577,
                    "y": 2668
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='219' style='font-size:16px'>The updated equation for calculating the internal cell state Smc<br>is</p>",
            "id": 219,
            "page": 25,
            "text": "The updated equation for calculating the internal cell state Smc is"
        },
        {
            "bounding_box": [
                {
                    "x": 514,
                    "y": 2872
                },
                {
                    "x": 1959,
                    "y": 2872
                },
                {
                    "x": 1959,
                    "y": 3020
                },
                {
                    "x": 514,
                    "y": 3020
                }
            ],
            "category": "paragraph",
            "html": "<p id='220' style='font-size:16px'>with Smc (0) = 0 and using the squashing function in Equation 31, with a range<br>[-2, 2]. The extended forward pass is given simply by exchanging Equation 30<br>for Equation 38.</p>",
            "id": 220,
            "page": 25,
            "text": "with Smc (0) = 0 and using the squashing function in Equation 31, with a range [-2, 2]. The extended forward pass is given simply by exchanging Equation 30 for Equation 38."
        },
        {
            "bounding_box": [
                {
                    "x": 1210,
                    "y": 3097
                },
                {
                    "x": 1260,
                    "y": 3097
                },
                {
                    "x": 1260,
                    "y": 3138
                },
                {
                    "x": 1210,
                    "y": 3138
                }
            ],
            "category": "footer",
            "html": "<footer id='221' style='font-size:14px'>25</footer>",
            "id": 221,
            "page": 25,
            "text": "25"
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 527
                },
                {
                    "x": 1961,
                    "y": 527
                },
                {
                    "x": 1961,
                    "y": 829
                },
                {
                    "x": 511,
                    "y": 829
                }
            ],
            "category": "paragraph",
            "html": "<p id='222' style='font-size:16px'>The bias weights of input and output gates are initialised with negative<br>values, and the weights of the forget gate are initialised with positive values.<br>From this, it follows that at the beginning of training, the forget gate activation<br>will be close to '1.0'. The memory cell will behave like a standard LSTM memory<br>cell without a forget gate. This prevents the LSTM memory cell from forgetting,<br>before it has actually learned anything.</p>",
            "id": 222,
            "page": 26,
            "text": "The bias weights of input and output gates are initialised with negative values, and the weights of the forget gate are initialised with positive values. From this, it follows that at the beginning of training, the forget gate activation will be close to '1.0'. The memory cell will behave like a standard LSTM memory cell without a forget gate. This prevents the LSTM memory cell from forgetting, before it has actually learned anything."
        },
        {
            "bounding_box": [
                {
                    "x": 514,
                    "y": 887
                },
                {
                    "x": 1020,
                    "y": 887
                },
                {
                    "x": 1020,
                    "y": 942
                },
                {
                    "x": 514,
                    "y": 942
                }
            ],
            "category": "paragraph",
            "html": "<p id='223' style='font-size:18px'>9.3 Backward Pass</p>",
            "id": 223,
            "page": 26,
            "text": "9.3 Backward Pass"
        },
        {
            "bounding_box": [
                {
                    "x": 512,
                    "y": 969
                },
                {
                    "x": 1961,
                    "y": 969
                },
                {
                    "x": 1961,
                    "y": 1218
                },
                {
                    "x": 512,
                    "y": 1218
                }
            ],
            "category": "paragraph",
            "html": "<p id='224' style='font-size:16px'>LSTM incorporates elements from both BPTT and RTRL. Thus, we separate<br>units into two types: those units whose weight changes are computed using a<br>variation of BPTT (i.e, output units, hidden units, and the output gates), and<br>those whose weight changes are computed using a variation of RTRL (i.e., the<br>input gates, the forget gates and the cells).</p>",
            "id": 224,
            "page": 26,
            "text": "LSTM incorporates elements from both BPTT and RTRL. Thus, we separate units into two types: those units whose weight changes are computed using a variation of BPTT (i.e, output units, hidden units, and the output gates), and those whose weight changes are computed using a variation of RTRL (i.e., the input gates, the forget gates and the cells)."
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 1223
                },
                {
                    "x": 1958,
                    "y": 1223
                },
                {
                    "x": 1958,
                    "y": 1317
                },
                {
                    "x": 511,
                    "y": 1317
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='225' style='font-size:14px'>Following the notation used in previous sections, and using Equations 8<br>and 10, the overall network error at time step T is</p>",
            "id": 225,
            "page": 26,
            "text": "Following the notation used in previous sections, and using Equations 8 and 10, the overall network error at time step T is"
        },
        {
            "bounding_box": [
                {
                    "x": 512,
                    "y": 1517
                },
                {
                    "x": 1959,
                    "y": 1517
                },
                {
                    "x": 1959,
                    "y": 1618
                },
                {
                    "x": 512,
                    "y": 1618
                }
            ],
            "category": "paragraph",
            "html": "<p id='226' style='font-size:14px'>Let us first consider units that work with BPTT. We define the notion of<br>individual error of a unit u at time T by</p>",
            "id": 226,
            "page": 26,
            "text": "Let us first consider units that work with BPTT. We define the notion of individual error of a unit u at time T by"
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 1789
                },
                {
                    "x": 1958,
                    "y": 1789
                },
                {
                    "x": 1958,
                    "y": 1884
                },
                {
                    "x": 511,
                    "y": 1884
                }
            ],
            "category": "paragraph",
            "html": "<p id='227' style='font-size:14px'>where Zu is the weighted input of the unit. We can expand the notion of weight<br>contribution as follows</p>",
            "id": 227,
            "page": 26,
            "text": "where Zu is the weighted input of the unit. We can expand the notion of weight contribution as follows"
        },
        {
            "bounding_box": [
                {
                    "x": 510,
                    "y": 2167
                },
                {
                    "x": 1961,
                    "y": 2167
                },
                {
                    "x": 1961,
                    "y": 2338
                },
                {
                    "x": 510,
                    "y": 2338
                }
            ],
            "category": "paragraph",
            "html": "<p id='228' style='font-size:14px'>dzu (T)<br>The factor corresponds to the input signal that comes from the unit v to<br>dW[u,v]<br>the unit u. However, depending on the nature of u, the individual error varies.<br>If u is equal to an output unit 0, then</p>",
            "id": 228,
            "page": 26,
            "text": "dzu (T) The factor corresponds to the input signal that comes from the unit v to dW[u,v] the unit u. However, depending on the nature of u, the individual error varies. If u is equal to an output unit 0, then"
        },
        {
            "bounding_box": [
                {
                    "x": 514,
                    "y": 2461
                },
                {
                    "x": 1386,
                    "y": 2461
                },
                {
                    "x": 1386,
                    "y": 2511
                },
                {
                    "x": 514,
                    "y": 2511
                }
            ],
            "category": "paragraph",
            "html": "<p id='229' style='font-size:16px'>thus, the weight contribution of output units is</p>",
            "id": 229,
            "page": 26,
            "text": "thus, the weight contribution of output units is"
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 2634
                },
                {
                    "x": 1959,
                    "y": 2634
                },
                {
                    "x": 1959,
                    "y": 2726
                },
                {
                    "x": 511,
                    "y": 2726
                }
            ],
            "category": "paragraph",
            "html": "<p id='230' style='font-size:14px'>Now, if u is equal to a hidden unit h located between cells and output units,<br>then</p>",
            "id": 230,
            "page": 26,
            "text": "Now, if u is equal to a hidden unit h located between cells and output units, then"
        },
        {
            "bounding_box": [
                {
                    "x": 514,
                    "y": 2871
                },
                {
                    "x": 1957,
                    "y": 2871
                },
                {
                    "x": 1957,
                    "y": 2919
                },
                {
                    "x": 514,
                    "y": 2919
                }
            ],
            "category": "paragraph",
            "html": "<p id='231' style='font-size:16px'>where 0 is the set of output units, and the weight contribution of hidden units</p>",
            "id": 231,
            "page": 26,
            "text": "where 0 is the set of output units, and the weight contribution of hidden units"
        },
        {
            "bounding_box": [
                {
                    "x": 513,
                    "y": 2925
                },
                {
                    "x": 553,
                    "y": 2925
                },
                {
                    "x": 553,
                    "y": 2963
                },
                {
                    "x": 513,
                    "y": 2963
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='232' style='font-size:14px'>is</p>",
            "id": 232,
            "page": 26,
            "text": "is"
        },
        {
            "bounding_box": [
                {
                    "x": 1210,
                    "y": 3098
                },
                {
                    "x": 1261,
                    "y": 3098
                },
                {
                    "x": 1261,
                    "y": 3138
                },
                {
                    "x": 1210,
                    "y": 3138
                }
            ],
            "category": "footer",
            "html": "<footer id='233' style='font-size:14px'>26</footer>",
            "id": 233,
            "page": 26,
            "text": "26"
        },
        {
            "bounding_box": [
                {
                    "x": 513,
                    "y": 528
                },
                {
                    "x": 1891,
                    "y": 528
                },
                {
                    "x": 1891,
                    "y": 578
                },
                {
                    "x": 513,
                    "y": 578
                }
            ],
            "category": "paragraph",
            "html": "<p id='234' style='font-size:14px'>Finally, if u is equal to the output gate outm of the memory block m, then</p>",
            "id": 234,
            "page": 27,
            "text": "Finally, if u is equal to the output gate outm of the memory block m, then"
        },
        {
            "bounding_box": [
                {
                    "x": 512,
                    "y": 790
                },
                {
                    "x": 1961,
                    "y": 790
                },
                {
                    "x": 1961,
                    "y": 1002
                },
                {
                    "x": 512,
                    "y": 1002
                }
            ],
            "category": "paragraph",
            "html": "<p id='235' style='font-size:14px'>where t프 means the equality only holds if the error is truncated SO that it does<br>not propagate \"too much\"; that is, it prevents the error from propagating back<br>to the unit via its own feedback connection. Finally, the weight contribution for<br>output gates is</p>",
            "id": 235,
            "page": 27,
            "text": "where t프 means the equality only holds if the error is truncated SO that it does not propagate \"too much\"; that is, it prevents the error from propagating back to the unit via its own feedback connection. Finally, the weight contribution for output gates is"
        },
        {
            "bounding_box": [
                {
                    "x": 510,
                    "y": 1132
                },
                {
                    "x": 1960,
                    "y": 1132
                },
                {
                    "x": 1960,
                    "y": 1334
                },
                {
                    "x": 510,
                    "y": 1334
                }
            ],
            "category": "paragraph",
            "html": "<p id='236' style='font-size:14px'>Let us now consider units that work with RTRL. In this case, the individual<br>errors of the input gate and the forget gate revolve around the individual error<br>of the cells in the memory block. We define the individual error of the cell mc<br>of the memory block m by</p>",
            "id": 236,
            "page": 27,
            "text": "Let us now consider units that work with RTRL. In this case, the individual errors of the input gate and the forget gate revolve around the individual error of the cells in the memory block. We define the individual error of the cell mc of the memory block m by"
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 1907
                },
                {
                    "x": 1960,
                    "y": 1907
                },
                {
                    "x": 1960,
                    "y": 2152
                },
                {
                    "x": 511,
                    "y": 2152
                }
            ],
            "category": "paragraph",
            "html": "<p id='237' style='font-size:16px'>Note that this equation does not consider the recurrent connection between<br>the cell and other units, propagating back in time only the error through its<br>recurrent connection (accounting for the influence of the forget gate). We use<br>the following partial derivatives to expand the weight contribution for the cell<br>as follows</p>",
            "id": 237,
            "page": 27,
            "text": "Note that this equation does not consider the recurrent connection between the cell and other units, propagating back in time only the error through its recurrent connection (accounting for the influence of the forget gate). We use the following partial derivatives to expand the weight contribution for the cell as follows"
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 2510
                },
                {
                    "x": 1699,
                    "y": 2510
                },
                {
                    "x": 1699,
                    "y": 2559
                },
                {
                    "x": 511,
                    "y": 2559
                }
            ],
            "category": "paragraph",
            "html": "<p id='238' style='font-size:16px'>and the weight contribution for forget and input gates as follows</p>",
            "id": 238,
            "page": 27,
            "text": "and the weight contribution for forget and input gates as follows"
        },
        {
            "bounding_box": [
                {
                    "x": 1209,
                    "y": 3098
                },
                {
                    "x": 1260,
                    "y": 3098
                },
                {
                    "x": 1260,
                    "y": 3138
                },
                {
                    "x": 1209,
                    "y": 3138
                }
            ],
            "category": "footer",
            "html": "<footer id='239' style='font-size:16px'>27</footer>",
            "id": 239,
            "page": 27,
            "text": "27"
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 522
                },
                {
                    "x": 1960,
                    "y": 522
                },
                {
                    "x": 1960,
                    "y": 638
                },
                {
                    "x": 511,
                    "y": 638
                }
            ],
            "category": "paragraph",
            "html": "<p id='240' style='font-size:14px'>asmc (ヶ+1)<br>Now, we need to define what is the value of As expected, these also<br>W[u,u,v]<br>depend on the nature of the unit u. If u is equal to the cell mc, then</p>",
            "id": 240,
            "page": 28,
            "text": "asmc (ヶ+1) Now, we need to define what is the value of As expected, these also W[u,u,v] depend on the nature of the unit u. If u is equal to the cell mc, then"
        },
        {
            "bounding_box": [
                {
                    "x": 514,
                    "y": 820
                },
                {
                    "x": 1363,
                    "y": 820
                },
                {
                    "x": 1363,
                    "y": 869
                },
                {
                    "x": 514,
                    "y": 869
                }
            ],
            "category": "paragraph",
            "html": "<p id='241' style='font-size:16px'>Now, if u is equal to the input gate inm, then</p>",
            "id": 241,
            "page": 28,
            "text": "Now, if u is equal to the input gate inm, then"
        },
        {
            "bounding_box": [
                {
                    "x": 514,
                    "y": 1050
                },
                {
                    "x": 1358,
                    "y": 1050
                },
                {
                    "x": 1358,
                    "y": 1101
                },
                {
                    "x": 514,
                    "y": 1101
                }
            ],
            "category": "paragraph",
            "html": "<p id='242' style='font-size:14px'>Finally, if u is equal to a forget gate 4m, then</p>",
            "id": 242,
            "page": 28,
            "text": "Finally, if u is equal to a forget gate 4m, then"
        },
        {
            "bounding_box": [
                {
                    "x": 513,
                    "y": 1285
                },
                {
                    "x": 1958,
                    "y": 1285
                },
                {
                    "x": 1958,
                    "y": 1383
                },
                {
                    "x": 513,
                    "y": 1383
                }
            ],
            "category": "paragraph",
            "html": "<p id='243' style='font-size:14px'>with Smc (0) = 0. A more detailed version of the LSTM backward pass with<br>forget gates is described in [22].</p>",
            "id": 243,
            "page": 28,
            "text": "with Smc (0) = 0. A more detailed version of the LSTM backward pass with forget gates is described in ."
        },
        {
            "bounding_box": [
                {
                    "x": 514,
                    "y": 1444
                },
                {
                    "x": 935,
                    "y": 1444
                },
                {
                    "x": 935,
                    "y": 1503
                },
                {
                    "x": 514,
                    "y": 1503
                }
            ],
            "category": "paragraph",
            "html": "<p id='244' style='font-size:20px'>9.4 Complexity</p>",
            "id": 244,
            "page": 28,
            "text": "9.4 Complexity"
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 1524
                },
                {
                    "x": 1960,
                    "y": 1524
                },
                {
                    "x": 1960,
                    "y": 1923
                },
                {
                    "x": 511,
                    "y": 1923
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='245' style='font-size:16px'>In this section, we present a complexity measure following the same principles<br>that Gers used in [22]; namely, we assume that every memory block contains the<br>same number of cells (usually one), and that output units only receive signals<br>from cell units and not from other units in the network. Let B, C,In, Out be<br>the number of of memory blocks, memory cells in each block, input units and<br>output units, respectively. Now, for each memory block we need to resolve the<br>(recurrent) connections for each cell, input gate, forget gate and output gate.<br>Solving these connections yields a complexity measure of</p>",
            "id": 245,
            "page": 28,
            "text": "In this section, we present a complexity measure following the same principles that Gers used in ; namely, we assume that every memory block contains the same number of cells (usually one), and that output units only receive signals from cell units and not from other units in the network. Let B, C,In, Out be the number of of memory blocks, memory cells in each block, input units and output units, respectively. Now, for each memory block we need to resolve the (recurrent) connections for each cell, input gate, forget gate and output gate. Solving these connections yields a complexity measure of"
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 2177
                },
                {
                    "x": 1957,
                    "y": 2177
                },
                {
                    "x": 1957,
                    "y": 2271
                },
                {
                    "x": 511,
                    "y": 2271
                }
            ],
            "category": "paragraph",
            "html": "<p id='246' style='font-size:14px'>We also need to solve the connections from input units and to output units;<br>these are, respectively</p>",
            "id": 246,
            "page": 28,
            "text": "We also need to solve the connections from input units and to output units; these are, respectively"
        },
        {
            "bounding_box": [
                {
                    "x": 513,
                    "y": 2355
                },
                {
                    "x": 592,
                    "y": 2355
                },
                {
                    "x": 592,
                    "y": 2395
                },
                {
                    "x": 513,
                    "y": 2395
                }
            ],
            "category": "paragraph",
            "html": "<p id='247' style='font-size:14px'>and</p>",
            "id": 247,
            "page": 28,
            "text": "and"
        },
        {
            "bounding_box": [
                {
                    "x": 512,
                    "y": 2477
                },
                {
                    "x": 1958,
                    "y": 2477
                },
                {
                    "x": 1958,
                    "y": 2675
                },
                {
                    "x": 512,
                    "y": 2675
                }
            ],
            "category": "paragraph",
            "html": "<p id='248' style='font-size:16px'>The numbers B,C,In and Out do not change as the network executes, and, at<br>each step, the number of weight updates is bounded by the number of connec-<br>tions; thus, we can say that LSTM's computational complexity per step and<br>weight is 0(1).</p>",
            "id": 248,
            "page": 28,
            "text": "The numbers B,C,In and Out do not change as the network executes, and, at each step, the number of weight updates is bounded by the number of connections; thus, we can say that LSTM's computational complexity per step and weight is 0(1)."
        },
        {
            "bounding_box": [
                {
                    "x": 516,
                    "y": 2737
                },
                {
                    "x": 1686,
                    "y": 2737
                },
                {
                    "x": 1686,
                    "y": 2790
                },
                {
                    "x": 516,
                    "y": 2790
                }
            ],
            "category": "paragraph",
            "html": "<p id='249' style='font-size:18px'>9.5 Strengths and limitations of LSTM-RNNs</p>",
            "id": 249,
            "page": 28,
            "text": "9.5 Strengths and limitations of LSTM-RNNs"
        },
        {
            "bounding_box": [
                {
                    "x": 513,
                    "y": 2818
                },
                {
                    "x": 1960,
                    "y": 2818
                },
                {
                    "x": 1960,
                    "y": 3016
                },
                {
                    "x": 513,
                    "y": 3016
                }
            ],
            "category": "paragraph",
            "html": "<p id='250' style='font-size:14px'>According to [23], LSTM excels on tasks in which a limited amount of data<br>must be remembered for a long time. This property is attributed to the use<br>of memory blocks. Memory blocks are interesting constructions: they have<br>access control in the form of input and output gates; which prevent irrelevant</p>",
            "id": 250,
            "page": 28,
            "text": "According to , LSTM excels on tasks in which a limited amount of data must be remembered for a long time. This property is attributed to the use of memory blocks. Memory blocks are interesting constructions: they have access control in the form of input and output gates; which prevent irrelevant"
        },
        {
            "bounding_box": [
                {
                    "x": 1210,
                    "y": 3098
                },
                {
                    "x": 1260,
                    "y": 3098
                },
                {
                    "x": 1260,
                    "y": 3137
                },
                {
                    "x": 1210,
                    "y": 3137
                }
            ],
            "category": "footer",
            "html": "<footer id='251' style='font-size:14px'>28</footer>",
            "id": 251,
            "page": 28,
            "text": "28"
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 529
                },
                {
                    "x": 1960,
                    "y": 529
                },
                {
                    "x": 1960,
                    "y": 827
                },
                {
                    "x": 511,
                    "y": 827
                }
            ],
            "category": "paragraph",
            "html": "<p id='252' style='font-size:16px'>information from entering or leaving the memory block. Memory blocks also<br>have a forget gate which weights the information inside the cells, SO whenever<br>previous information becomes irrelevant for some cells, the forget gate can reset<br>the state of the different cell inside the block. Forget gates also enable continuous<br>prediction [54], because they can make cells completely forget their previous<br>state; preventing biases in prediction.</p>",
            "id": 252,
            "page": 29,
            "text": "information from entering or leaving the memory block. Memory blocks also have a forget gate which weights the information inside the cells, SO whenever previous information becomes irrelevant for some cells, the forget gate can reset the state of the different cell inside the block. Forget gates also enable continuous prediction , because they can make cells completely forget their previous state; preventing biases in prediction."
        },
        {
            "bounding_box": [
                {
                    "x": 510,
                    "y": 828
                },
                {
                    "x": 1960,
                    "y": 828
                },
                {
                    "x": 1960,
                    "y": 1129
                },
                {
                    "x": 510,
                    "y": 1129
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='253' style='font-size:16px'>Like other algorithms, LSTM requires the topology of the network to be<br>fixed a priori. The number of memory blocks in networks does not change dy-<br>namically, SO the memory of the network is ultimately limited. Moreover, [23]<br>point out that it is unlikely to overcome this limitation by increasing the net-<br>work size homogeneously, and suggest that modularisation promotes effective<br>learning. The process of modularisation is, however, \"not generally clear\"</p>",
            "id": 253,
            "page": 29,
            "text": "Like other algorithms, LSTM requires the topology of the network to be fixed a priori. The number of memory blocks in networks does not change dynamically, SO the memory of the network is ultimately limited. Moreover,  point out that it is unlikely to overcome this limitation by increasing the network size homogeneously, and suggest that modularisation promotes effective learning. The process of modularisation is, however, \"not generally clear\""
        },
        {
            "bounding_box": [
                {
                    "x": 518,
                    "y": 1203
                },
                {
                    "x": 1460,
                    "y": 1203
                },
                {
                    "x": 1460,
                    "y": 1267
                },
                {
                    "x": 518,
                    "y": 1267
                }
            ],
            "category": "paragraph",
            "html": "<p id='254' style='font-size:22px'>10 Problem specific topologies</p>",
            "id": 254,
            "page": 29,
            "text": "10 Problem specific topologies"
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 1303
                },
                {
                    "x": 1961,
                    "y": 1303
                },
                {
                    "x": 1961,
                    "y": 1903
                },
                {
                    "x": 511,
                    "y": 1903
                }
            ],
            "category": "paragraph",
            "html": "<p id='255' style='font-size:18px'>LSTM-RNN permits many different variants and topologies. These partially<br>problem specific and can be derived [3] from the basic method [41], [21] COV-<br>ered in Section 8 and 9. More recently the basic method is referenced to as<br>'vanilla' LSTM, which used in practise these days only with various extensions<br>and modifications. In the following sections we cover the most common in use,<br>namely bidirectional LSTM (BLSTM-CTC) ([34], [27], [31]), Grid LSTM (or<br>N-LSTM) [49] and Gated Recurrent Unit (GRU) ([10], [13]). There are vari-<br>ous variants of Grid LSTM. The most important to note are Multidimensional<br>LSTM ( [29], [35]), Stacked LSTM ([18], [33], [68]). Specifically we would like to<br>also point out the more recent variant Sequence-to-Sequence ([68], [36], [8], [80],<br>[69]) and attention-based learning [12], which are both important to mention in<br>the context of cognitive learning tasks.</p>",
            "id": 255,
            "page": 29,
            "text": "LSTM-RNN permits many different variants and topologies. These partially problem specific and can be derived  from the basic method ,  COVered in Section 8 and 9. More recently the basic method is referenced to as 'vanilla' LSTM, which used in practise these days only with various extensions and modifications. In the following sections we cover the most common in use, namely bidirectional LSTM (BLSTM-CTC) (, , ), Grid LSTM (or N-LSTM)  and Gated Recurrent Unit (GRU) (, ). There are various variants of Grid LSTM. The most important to note are Multidimensional LSTM ( , ), Stacked LSTM (, , ). Specifically we would like to also point out the more recent variant Sequence-to-Sequence (, , , , ) and attention-based learning , which are both important to mention in the context of cognitive learning tasks."
        },
        {
            "bounding_box": [
                {
                    "x": 517,
                    "y": 1964
                },
                {
                    "x": 1169,
                    "y": 1964
                },
                {
                    "x": 1169,
                    "y": 2017
                },
                {
                    "x": 517,
                    "y": 2017
                }
            ],
            "category": "paragraph",
            "html": "<p id='256' style='font-size:20px'>10.1 Bidirectional LSTM</p>",
            "id": 256,
            "page": 29,
            "text": "10.1 Bidirectional LSTM"
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 2046
                },
                {
                    "x": 1959,
                    "y": 2046
                },
                {
                    "x": 1959,
                    "y": 2442
                },
                {
                    "x": 511,
                    "y": 2442
                }
            ],
            "category": "paragraph",
            "html": "<p id='257' style='font-size:14px'>Conventional RNNs analyse, for any given point in a sequence, just one di-<br>rection during processing: the past. The work published in [34] explores the<br>possibility of analysing both the future as well as the past of a given point in<br>the context of LSTM. At a very high level, bidirectional means that the input<br>is presented forwards and backwards to two separate LSTM networks, both of<br>which are connected to the same output layer. According to [34], bidirectional<br>training possesses an architectural advantage over unidirectional training if used<br>to classify phonemes.</p>",
            "id": 257,
            "page": 29,
            "text": "Conventional RNNs analyse, for any given point in a sequence, just one direction during processing: the past. The work published in  explores the possibility of analysing both the future as well as the past of a given point in the context of LSTM. At a very high level, bidirectional means that the input is presented forwards and backwards to two separate LSTM networks, both of which are connected to the same output layer. According to , bidirectional training possesses an architectural advantage over unidirectional training if used to classify phonemes."
        },
        {
            "bounding_box": [
                {
                    "x": 510,
                    "y": 2445
                },
                {
                    "x": 1960,
                    "y": 2445
                },
                {
                    "x": 1960,
                    "y": 2641
                },
                {
                    "x": 510,
                    "y": 2641
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='258' style='font-size:14px'>Bidirectional LSTM removes the one-step truncation originally present in<br>LSTM, and implements a full error gradient calculation. This full error gradient<br>approach eased the implementation of bidirectional LSTM, and allowed it to be<br>trained using standard BPTT.</p>",
            "id": 258,
            "page": 29,
            "text": "Bidirectional LSTM removes the one-step truncation originally present in LSTM, and implements a full error gradient calculation. This full error gradient approach eased the implementation of bidirectional LSTM, and allowed it to be trained using standard BPTT."
        },
        {
            "bounding_box": [
                {
                    "x": 512,
                    "y": 2644
                },
                {
                    "x": 1960,
                    "y": 2644
                },
                {
                    "x": 1960,
                    "y": 2944
                },
                {
                    "x": 512,
                    "y": 2944
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='259' style='font-size:16px'>In 2006 [28] introduced an RNN objective function named Connectionist<br>Temporal Classification (CTC). The advantage of CTC is that it enables the<br>LSTM-RNN to handle input data not segmented into sequences. This is impor-<br>tant if the correct segmentation of data is difficult to achieve (e.g. separation<br>of letters in handwriting). Later this lead to the now common variant BLSTM-<br>CTC as documented by [52, 19, 27].</p>",
            "id": 259,
            "page": 29,
            "text": "In 2006  introduced an RNN objective function named Connectionist Temporal Classification (CTC). The advantage of CTC is that it enables the LSTM-RNN to handle input data not segmented into sequences. This is important if the correct segmentation of data is difficult to achieve (e.g. separation of letters in handwriting). Later this lead to the now common variant BLSTMCTC as documented by ."
        },
        {
            "bounding_box": [
                {
                    "x": 1210,
                    "y": 3097
                },
                {
                    "x": 1260,
                    "y": 3097
                },
                {
                    "x": 1260,
                    "y": 3136
                },
                {
                    "x": 1210,
                    "y": 3136
                }
            ],
            "category": "footer",
            "html": "<footer id='260' style='font-size:14px'>29</footer>",
            "id": 260,
            "page": 29,
            "text": "29"
        },
        {
            "bounding_box": [
                {
                    "x": 516,
                    "y": 525
                },
                {
                    "x": 968,
                    "y": 525
                },
                {
                    "x": 968,
                    "y": 575
                },
                {
                    "x": 516,
                    "y": 575
                }
            ],
            "category": "paragraph",
            "html": "<p id='261' style='font-size:20px'>10.2 Grid LSTM</p>",
            "id": 261,
            "page": 30,
            "text": "10.2 Grid LSTM"
        },
        {
            "bounding_box": [
                {
                    "x": 514,
                    "y": 606
                },
                {
                    "x": 1961,
                    "y": 606
                },
                {
                    "x": 1961,
                    "y": 854
                },
                {
                    "x": 514,
                    "y": 854
                }
            ],
            "category": "paragraph",
            "html": "<p id='262' style='font-size:16px'>Grid LSTM presented by [49] is an attempt to generalise the advantages of<br>LSTM - including its ability to select or ignore inputs- into deep networks of<br>a unified architecture. An N-dimensional grid LSTM or N-LSTM is a network<br>arranged in a grid of N dimensions, with LSTM cells along and in-between some<br>(or all) of the dimensions, enabling communication among consecutive layers.</p>",
            "id": 262,
            "page": 30,
            "text": "Grid LSTM presented by  is an attempt to generalise the advantages of LSTM - including its ability to select or ignore inputs- into deep networks of a unified architecture. An N-dimensional grid LSTM or N-LSTM is a network arranged in a grid of N dimensions, with LSTM cells along and in-between some (or all) of the dimensions, enabling communication among consecutive layers."
        },
        {
            "bounding_box": [
                {
                    "x": 512,
                    "y": 859
                },
                {
                    "x": 1961,
                    "y": 859
                },
                {
                    "x": 1961,
                    "y": 1150
                },
                {
                    "x": 512,
                    "y": 1150
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='263' style='font-size:18px'>Grid LSTM is analogous to the stacked LSTM [33], but it adds cells along the<br>depth dimension too, i.e., in-between layers. Additionally, N-LSTM networks<br>with N > 2 are analogous to multidimensional LSTM [29], but they differ<br>again by the cells along the depth dimension, and by the ability of grid LSTM<br>networks to modulate the interaction among layers such that it is not prone to<br>the instability present in Multidimensional LSTM.</p>",
            "id": 263,
            "page": 30,
            "text": "Grid LSTM is analogous to the stacked LSTM , but it adds cells along the depth dimension too, i.e., in-between layers. Additionally, N-LSTM networks with N > 2 are analogous to multidimensional LSTM , but they differ again by the cells along the depth dimension, and by the ability of grid LSTM networks to modulate the interaction among layers such that it is not prone to the instability present in Multidimensional LSTM."
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 1154
                },
                {
                    "x": 1961,
                    "y": 1154
                },
                {
                    "x": 1961,
                    "y": 1501
                },
                {
                    "x": 511,
                    "y": 1501
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='264' style='font-size:16px'>Consider a trained LSTM network with weights W, whose hidden cells emit<br>a collection of signals represented by the vector yh and whose memory units<br>emit a collection of signals represented by the vector yh. Whenever this LSTM<br>network is provided an input vector x, there is a change in the signals emitted<br>by both hidden units and memory cells; let Yh , and Sm represent the new values<br>of signals. Let P be a projection matrix, the concatenation of the new input<br>signals and the recurrent signals is given by</p>",
            "id": 264,
            "page": 30,
            "text": "Consider a trained LSTM network with weights W, whose hidden cells emit a collection of signals represented by the vector yh and whose memory units emit a collection of signals represented by the vector yh. Whenever this LSTM network is provided an input vector x, there is a change in the signals emitted by both hidden units and memory cells; let Yh , and Sm represent the new values of signals. Let P be a projection matrix, the concatenation of the new input signals and the recurrent signals is given by"
        },
        {
            "bounding_box": [
                {
                    "x": 513,
                    "y": 1680
                },
                {
                    "x": 1957,
                    "y": 1680
                },
                {
                    "x": 1957,
                    "y": 1778
                },
                {
                    "x": 513,
                    "y": 1778
                }
            ],
            "category": "paragraph",
            "html": "<p id='265' style='font-size:14px'>An LSTM transform, which changes the values of hidden and memory signals<br>as previously mentioned, can be formulated as follows:</p>",
            "id": 265,
            "page": 30,
            "text": "An LSTM transform, which changes the values of hidden and memory signals as previously mentioned, can be formulated as follows:"
        },
        {
            "bounding_box": [
                {
                    "x": 514,
                    "y": 1922
                },
                {
                    "x": 1957,
                    "y": 1922
                },
                {
                    "x": 1957,
                    "y": 2022
                },
                {
                    "x": 514,
                    "y": 2022
                }
            ],
            "category": "paragraph",
            "html": "<p id='266' style='font-size:16px'>Before we explain in detail the architecture of Grid LSTM blocks, we quickly<br>review Stacked LSTM and Multidimensional LSTM architectures.</p>",
            "id": 266,
            "page": 30,
            "text": "Before we explain in detail the architecture of Grid LSTM blocks, we quickly review Stacked LSTM and Multidimensional LSTM architectures."
        },
        {
            "bounding_box": [
                {
                    "x": 516,
                    "y": 2081
                },
                {
                    "x": 1002,
                    "y": 2081
                },
                {
                    "x": 1002,
                    "y": 2127
                },
                {
                    "x": 516,
                    "y": 2127
                }
            ],
            "category": "paragraph",
            "html": "<p id='267' style='font-size:14px'>10.2.1 Stacked LSTM</p>",
            "id": 267,
            "page": 30,
            "text": "10.2.1 Stacked LSTM"
        },
        {
            "bounding_box": [
                {
                    "x": 513,
                    "y": 2156
                },
                {
                    "x": 1960,
                    "y": 2156
                },
                {
                    "x": 1960,
                    "y": 2355
                },
                {
                    "x": 513,
                    "y": 2355
                }
            ],
            "category": "paragraph",
            "html": "<p id='268' style='font-size:16px'>A stacked LSTM [33], as its name suggests, stacks LSTM layers on top of each<br>other in order to increase capacity. At a high level, to stack N LSTM networks,<br>we make the first network have X1 as defined in Equation (52), but we make<br>the i-th network have Xi defined by</p>",
            "id": 268,
            "page": 30,
            "text": "A stacked LSTM , as its name suggests, stacks LSTM layers on top of each other in order to increase capacity. At a high level, to stack N LSTM networks, we make the first network have X1 as defined in Equation (52), but we make the i-th network have Xi defined by"
        },
        {
            "bounding_box": [
                {
                    "x": 512,
                    "y": 2538
                },
                {
                    "x": 1958,
                    "y": 2538
                },
                {
                    "x": 1958,
                    "y": 2635
                },
                {
                    "x": 512,
                    "y": 2635
                }
            ],
            "category": "paragraph",
            "html": "<p id='269' style='font-size:18px'>instead, replacing the input signals x with the hidden signals from the previous<br>LSTM transform, effectively \"stacking\" them.</p>",
            "id": 269,
            "page": 30,
            "text": "instead, replacing the input signals x with the hidden signals from the previous LSTM transform, effectively \"stacking\" them."
        },
        {
            "bounding_box": [
                {
                    "x": 517,
                    "y": 2692
                },
                {
                    "x": 1207,
                    "y": 2692
                },
                {
                    "x": 1207,
                    "y": 2741
                },
                {
                    "x": 517,
                    "y": 2741
                }
            ],
            "category": "paragraph",
            "html": "<p id='270' style='font-size:16px'>10.2.2 Multidimensional LSTM</p>",
            "id": 270,
            "page": 30,
            "text": "10.2.2 Multidimensional LSTM"
        },
        {
            "bounding_box": [
                {
                    "x": 513,
                    "y": 2768
                },
                {
                    "x": 2026,
                    "y": 2768
                },
                {
                    "x": 2026,
                    "y": 3022
                },
                {
                    "x": 513,
                    "y": 3022
                }
            ],
            "category": "paragraph",
            "html": "<p id='271' style='font-size:14px'>In Multidimensional LSTM networks [29], inputs are structured in an N-dimensional<br>grid instead of being sequences of values; for example, a solid expressed as a<br>three-dimensional array of voxels. To use this structure of inputs, Multidimen-<br>sional LSTM networks increase the number of recurrent connections from 1 to<br>N; thus, an N-dimensional LSTM receives N hidden vectors yh1, · · · , yhN and</p>",
            "id": 271,
            "page": 30,
            "text": "In Multidimensional LSTM networks , inputs are structured in an N-dimensional grid instead of being sequences of values; for example, a solid expressed as a three-dimensional array of voxels. To use this structure of inputs, Multidimensional LSTM networks increase the number of recurrent connections from 1 to N; thus, an N-dimensional LSTM receives N hidden vectors yh1, · · · , yhN and"
        },
        {
            "bounding_box": [
                {
                    "x": 1210,
                    "y": 3098
                },
                {
                    "x": 1260,
                    "y": 3098
                },
                {
                    "x": 1260,
                    "y": 3136
                },
                {
                    "x": 1210,
                    "y": 3136
                }
            ],
            "category": "footer",
            "html": "<footer id='272' style='font-size:14px'>30</footer>",
            "id": 272,
            "page": 30,
            "text": "30"
        },
        {
            "bounding_box": [
                {
                    "x": 513,
                    "y": 531
                },
                {
                    "x": 1959,
                    "y": 531
                },
                {
                    "x": 1959,
                    "y": 676
                },
                {
                    "x": 513,
                    "y": 676
                }
            ],
            "category": "paragraph",
            "html": "<p id='273' style='font-size:14px'>N memory vectors Sm1, · · · , SmN as input, then the network outputs a single<br>hidden vector Yh and a single memory vector Sm. For multidimensional LSTM<br>networks, we define X by</p>",
            "id": 273,
            "page": 31,
            "text": "N memory vectors Sm1, · · · , SmN as input, then the network outputs a single hidden vector Yh and a single memory vector Sm. For multidimensional LSTM networks, we define X by"
        },
        {
            "bounding_box": [
                {
                    "x": 514,
                    "y": 983
                },
                {
                    "x": 1478,
                    "y": 983
                },
                {
                    "x": 1478,
                    "y": 1029
                },
                {
                    "x": 514,
                    "y": 1029
                }
            ],
            "category": "paragraph",
            "html": "<p id='274' style='font-size:16px'>and the memory signal vector Sm is calculated using</p>",
            "id": 274,
            "page": 31,
            "text": "and the memory signal vector Sm is calculated using"
        },
        {
            "bounding_box": [
                {
                    "x": 514,
                    "y": 1236
                },
                {
                    "x": 1958,
                    "y": 1236
                },
                {
                    "x": 1958,
                    "y": 1434
                },
                {
                    "x": 514,
                    "y": 1434
                }
            ],
            "category": "paragraph",
            "html": "<p id='275' style='font-size:18px'>where ⊙ is the Hadamard product, V is a vector consisting of N forget signals<br>(one for each yhi), and inm and zm respectively correspond to the signals of<br>the input gate and the weighted input of the memory cell (see Equation (38) to<br>compare Equation (54) with the standard calculation of Sm).</p>",
            "id": 275,
            "page": 31,
            "text": "where ⊙ is the Hadamard product, V is a vector consisting of N forget signals (one for each yhi), and inm and zm respectively correspond to the signals of the input gate and the weighted input of the memory cell (see Equation (38) to compare Equation (54) with the standard calculation of Sm)."
        },
        {
            "bounding_box": [
                {
                    "x": 516,
                    "y": 1492
                },
                {
                    "x": 1092,
                    "y": 1492
                },
                {
                    "x": 1092,
                    "y": 1538
                },
                {
                    "x": 516,
                    "y": 1538
                }
            ],
            "category": "paragraph",
            "html": "<p id='276' style='font-size:16px'>10.2.3 Grid LSTM Blocks</p>",
            "id": 276,
            "page": 31,
            "text": "10.2.3 Grid LSTM Blocks"
        },
        {
            "bounding_box": [
                {
                    "x": 513,
                    "y": 1568
                },
                {
                    "x": 1959,
                    "y": 1568
                },
                {
                    "x": 1959,
                    "y": 1864
                },
                {
                    "x": 513,
                    "y": 1864
                }
            ],
            "category": "paragraph",
            "html": "<p id='277' style='font-size:14px'>Due to the high number of connections, large multidimensional LSTM net-<br>works are usually unstable [49]. Grid LSTM offers an alternate way of comput-<br>ing the new memory vector. However, unlike multidimensional LSTM, a Grid<br>LSTM block outputs N hidden vectors yh', · · · , yh'N and N memory vectors<br>, , hidden<br>Sm1, · · · , SmN that are all distinct. To do so, the model concatenates the<br>vectors from the N dimensions as follows</p>",
            "id": 277,
            "page": 31,
            "text": "Due to the high number of connections, large multidimensional LSTM networks are usually unstable . Grid LSTM offers an alternate way of computing the new memory vector. However, unlike multidimensional LSTM, a Grid LSTM block outputs N hidden vectors yh', · · · , yh'N and N memory vectors , , hidden Sm1, · · · , SmN that are all distinct. To do so, the model concatenates the vectors from the N dimensions as follows"
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 2109
                },
                {
                    "x": 1956,
                    "y": 2109
                },
                {
                    "x": 1956,
                    "y": 2203
                },
                {
                    "x": 511,
                    "y": 2203
                }
            ],
            "category": "paragraph",
            "html": "<p id='278' style='font-size:16px'>The grid LSTM block computes N LSTM transforms, one for each dimension,<br>as follows</p>",
            "id": 278,
            "page": 31,
            "text": "The grid LSTM block computes N LSTM transforms, one for each dimension, as follows"
        },
        {
            "bounding_box": [
                {
                    "x": 512,
                    "y": 2479
                },
                {
                    "x": 1960,
                    "y": 2479
                },
                {
                    "x": 1960,
                    "y": 2677
                },
                {
                    "x": 512,
                    "y": 2677
                }
            ],
            "category": "paragraph",
            "html": "<p id='279' style='font-size:18px'>Each transform applies standard LSTM across its respective dimension. Having<br>X as input to all transforms represents the sharing of hidden signals across the<br>different dimension of the grid; note that each transform independently manages<br>its memory signals.</p>",
            "id": 279,
            "page": 31,
            "text": "Each transform applies standard LSTM across its respective dimension. Having X as input to all transforms represents the sharing of hidden signals across the different dimension of the grid; note that each transform independently manages its memory signals."
        },
        {
            "bounding_box": [
                {
                    "x": 518,
                    "y": 2739
                },
                {
                    "x": 1409,
                    "y": 2739
                },
                {
                    "x": 1409,
                    "y": 2793
                },
                {
                    "x": 518,
                    "y": 2793
                }
            ],
            "category": "paragraph",
            "html": "<p id='280' style='font-size:20px'>10.3 Gated Recurrent Unit (GRU)</p>",
            "id": 280,
            "page": 31,
            "text": "10.3 Gated Recurrent Unit (GRU)"
        },
        {
            "bounding_box": [
                {
                    "x": 514,
                    "y": 2819
                },
                {
                    "x": 1958,
                    "y": 2819
                },
                {
                    "x": 1958,
                    "y": 3020
                },
                {
                    "x": 514,
                    "y": 3020
                }
            ],
            "category": "paragraph",
            "html": "<p id='281' style='font-size:18px'>[10] propose the Gated Recurrent Unit (GRU) architecture for RNN as an al-<br>ternative to LSTM. GRU has empirically been found to outperform LSTM on<br>nearly all tasks, except language modelling with naive initialization [47]. GRU<br>units, unlike LSTM memory blocks, do not have a memory cell; although they</p>",
            "id": 281,
            "page": 31,
            "text": " propose the Gated Recurrent Unit (GRU) architecture for RNN as an alternative to LSTM. GRU has empirically been found to outperform LSTM on nearly all tasks, except language modelling with naive initialization . GRU units, unlike LSTM memory blocks, do not have a memory cell; although they"
        },
        {
            "bounding_box": [
                {
                    "x": 1209,
                    "y": 3098
                },
                {
                    "x": 1257,
                    "y": 3098
                },
                {
                    "x": 1257,
                    "y": 3138
                },
                {
                    "x": 1209,
                    "y": 3138
                }
            ],
            "category": "footer",
            "html": "<footer id='282' style='font-size:14px'>31</footer>",
            "id": 282,
            "page": 31,
            "text": "31"
        },
        {
            "bounding_box": [
                {
                    "x": 512,
                    "y": 530
                },
                {
                    "x": 1959,
                    "y": 530
                },
                {
                    "x": 1959,
                    "y": 678
                },
                {
                    "x": 512,
                    "y": 678
                }
            ],
            "category": "paragraph",
            "html": "<p id='283' style='font-size:14px'>do have gating units: a reset gate and an update gate. More precisely, let H<br>be the set of GRU units; if u E H, then we define the activation Yresu (ヶ+ 1) of<br>the reset gate resu at time T +1 by</p>",
            "id": 283,
            "page": 32,
            "text": "do have gating units: a reset gate and an update gate. More precisely, let H be the set of GRU units; if u E H, then we define the activation Yresu (ヶ+ 1) of the reset gate resu at time T +1 by"
        },
        {
            "bounding_box": [
                {
                    "x": 513,
                    "y": 812
                },
                {
                    "x": 1958,
                    "y": 812
                },
                {
                    "x": 1958,
                    "y": 960
                },
                {
                    "x": 513,
                    "y": 960
                }
            ],
            "category": "paragraph",
            "html": "<p id='284' style='font-size:16px'>where fresu is the squashing function of the reset gate (usually a sigmoid func-<br>tion), and Sresu (T + 1) is the state of the reset gate resu at time T + 1, which<br>is defined by</p>",
            "id": 284,
            "page": 32,
            "text": "where fresu is the squashing function of the reset gate (usually a sigmoid function), and Sresu (T + 1) is the state of the reset gate resu at time T + 1, which is defined by"
        },
        {
            "bounding_box": [
                {
                    "x": 512,
                    "y": 1096
                },
                {
                    "x": 1957,
                    "y": 1096
                },
                {
                    "x": 1957,
                    "y": 1193
                },
                {
                    "x": 512,
                    "y": 1193
                }
            ],
            "category": "paragraph",
            "html": "<p id='285' style='font-size:16px'>where bresu is the bias of the reset gate, and Zresu (Ⓣ+ 1) is the weighted input<br>of the reset gate at time T + 1, which is in turn defined by</p>",
            "id": 285,
            "page": 32,
            "text": "where bresu is the bias of the reset gate, and Zresu (Ⓣ+ 1) is the weighted input of the reset gate at time T + 1, which is in turn defined by"
        },
        {
            "bounding_box": [
                {
                    "x": 514,
                    "y": 1485
                },
                {
                    "x": 1116,
                    "y": 1485
                },
                {
                    "x": 1116,
                    "y": 1531
                },
                {
                    "x": 514,
                    "y": 1531
                }
            ],
            "category": "paragraph",
            "html": "<p id='286' style='font-size:14px'>where I is the set of input units.</p>",
            "id": 286,
            "page": 32,
            "text": "where I is the set of input units."
        },
        {
            "bounding_box": [
                {
                    "x": 515,
                    "y": 1536
                },
                {
                    "x": 1959,
                    "y": 1536
                },
                {
                    "x": 1959,
                    "y": 1632
                },
                {
                    "x": 515,
                    "y": 1632
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='287' style='font-size:16px'>Similarly, we define define the activation Yupd\", (Ⓣ+1) of the update gate<br>updu at time T + 1 by</p>",
            "id": 287,
            "page": 32,
            "text": "Similarly, we define define the activation Yupd\", (Ⓣ+1) of the update gate updu at time T + 1 by"
        },
        {
            "bounding_box": [
                {
                    "x": 513,
                    "y": 1766
                },
                {
                    "x": 1957,
                    "y": 1766
                },
                {
                    "x": 1957,
                    "y": 1914
                },
                {
                    "x": 513,
                    "y": 1914
                }
            ],
            "category": "paragraph",
            "html": "<p id='288' style='font-size:18px'>where fupdu is the squashing function of the update gate (again, usually a sig-<br>moid function), and Supdu (T + 1) is the state of the update gate updu at time<br>T + 1, defined by</p>",
            "id": 288,
            "page": 32,
            "text": "where fupdu is the squashing function of the update gate (again, usually a sigmoid function), and Supdu (T + 1) is the state of the update gate updu at time T + 1, defined by"
        },
        {
            "bounding_box": [
                {
                    "x": 513,
                    "y": 2050
                },
                {
                    "x": 1957,
                    "y": 2050
                },
                {
                    "x": 1957,
                    "y": 2148
                },
                {
                    "x": 513,
                    "y": 2148
                }
            ],
            "category": "paragraph",
            "html": "<p id='289' style='font-size:16px'>where bupdu is the bias of the update gate, and Zupdu (ヶ+1) is the weighted input<br>of the update gate at time T + 1, which in turn is defined by</p>",
            "id": 289,
            "page": 32,
            "text": "where bupdu is the bias of the update gate, and Zupdu (ヶ+1) is the weighted input of the update gate at time T + 1, which in turn is defined by"
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 2437
                },
                {
                    "x": 1958,
                    "y": 2437
                },
                {
                    "x": 1958,
                    "y": 2637
                },
                {
                    "x": 511,
                    "y": 2637
                }
            ],
            "category": "paragraph",
            "html": "<p id='290' style='font-size:14px'>GRU reset and input gates behave like normal units in a recurrent network.<br>The main characteristic of GRU is the way the activation of the GRU units is<br>defined. A GRU unit u E H has an associated candidate activation Yu(T+ 1)<br>at time T 十 1, formally defined by</p>",
            "id": 290,
            "page": 32,
            "text": "GRU reset and input gates behave like normal units in a recurrent network. The main characteristic of GRU is the way the activation of the GRU units is defined. A GRU unit u E H has an associated candidate activation Yu(T+ 1) at time T 十 1, formally defined by"
        },
        {
            "bounding_box": [
                {
                    "x": 504,
                    "y": 2665
                },
                {
                    "x": 1988,
                    "y": 2665
                },
                {
                    "x": 1988,
                    "y": 2993
                },
                {
                    "x": 504,
                    "y": 2993
                }
            ],
            "category": "figure",
            "html": "<figure><img id='291' style='font-size:18px' alt=\"yu(T+1) = fu � W[u,i]Yi(T +1) + Yresu (Ⓣ+1) � (W[u,h]yh (T)) + bu\niEI hEH Bias )\nExternal input at time T + 1 Gated recurrent connection\n(65)\" data-coord=\"top-left:(504,2665); bottom-right:(1988,2993)\" /></figure>",
            "id": 291,
            "page": 32,
            "text": "yu(T+1) = fu � W[u,i]Yi(T +1) + Yresu (Ⓣ+1) � (W[u,h]yh (T)) + bu iEI hEH Bias ) External input at time T + 1 Gated recurrent connection (65)"
        },
        {
            "bounding_box": [
                {
                    "x": 1209,
                    "y": 3098
                },
                {
                    "x": 1259,
                    "y": 3098
                },
                {
                    "x": 1259,
                    "y": 3137
                },
                {
                    "x": 1209,
                    "y": 3137
                }
            ],
            "category": "footer",
            "html": "<footer id='292' style='font-size:16px'>32</footer>",
            "id": 292,
            "page": 32,
            "text": "32"
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 530
                },
                {
                    "x": 1957,
                    "y": 530
                },
                {
                    "x": 1957,
                    "y": 626
                },
                {
                    "x": 511,
                    "y": 626
                }
            ],
            "category": "paragraph",
            "html": "<p id='293' style='font-size:16px'>where fu is usually tanh, and the activation Yu(T + 1) of the GRU unit u at<br>time T + 1 is defined by</p>",
            "id": 293,
            "page": 33,
            "text": "where fu is usually tanh, and the activation Yu(T + 1) of the GRU unit u at time T + 1 is defined by"
        },
        {
            "bounding_box": [
                {
                    "x": 513,
                    "y": 760
                },
                {
                    "x": 1957,
                    "y": 760
                },
                {
                    "x": 1957,
                    "y": 915
                },
                {
                    "x": 513,
                    "y": 915
                }
            ],
            "category": "paragraph",
            "html": "<p id='294' style='font-size:16px'>Note the similarities between Equations (38) and (66). The factor Yupdu (Ⓣ+1)<br>appears to emulate the function of the forget gate of LSTM, while the factor<br>(1- Yupdu (T+1)) appears to emulate the function of the the input gate of LSTM.</p>",
            "id": 294,
            "page": 33,
            "text": "Note the similarities between Equations (38) and (66). The factor Yupdu (Ⓣ+1) appears to emulate the function of the forget gate of LSTM, while the factor (1- Yupdu (T+1)) appears to emulate the function of the the input gate of LSTM."
        },
        {
            "bounding_box": [
                {
                    "x": 517,
                    "y": 988
                },
                {
                    "x": 1488,
                    "y": 988
                },
                {
                    "x": 1488,
                    "y": 1049
                },
                {
                    "x": 517,
                    "y": 1049
                }
            ],
            "category": "paragraph",
            "html": "<p id='295' style='font-size:22px'>11 Applications of LSTM-RNN</p>",
            "id": 295,
            "page": 33,
            "text": "11 Applications of LSTM-RNN"
        },
        {
            "bounding_box": [
                {
                    "x": 512,
                    "y": 1090
                },
                {
                    "x": 1957,
                    "y": 1090
                },
                {
                    "x": 1957,
                    "y": 1189
                },
                {
                    "x": 512,
                    "y": 1189
                }
            ],
            "category": "paragraph",
            "html": "<p id='296' style='font-size:14px'>In this final section we cover a selection of well-known publications which proved<br>relevant over time.</p>",
            "id": 296,
            "page": 33,
            "text": "In this final section we cover a selection of well-known publications which proved relevant over time."
        },
        {
            "bounding_box": [
                {
                    "x": 518,
                    "y": 1249
                },
                {
                    "x": 1169,
                    "y": 1249
                },
                {
                    "x": 1169,
                    "y": 1306
                },
                {
                    "x": 518,
                    "y": 1306
                }
            ],
            "category": "paragraph",
            "html": "<p id='297' style='font-size:22px'>11.1 Early learning tasks</p>",
            "id": 297,
            "page": 33,
            "text": "11.1 Early learning tasks"
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 1330
                },
                {
                    "x": 1961,
                    "y": 1330
                },
                {
                    "x": 1961,
                    "y": 1733
                },
                {
                    "x": 511,
                    "y": 1733
                }
            ],
            "category": "paragraph",
            "html": "<p id='298' style='font-size:16px'>In early experiments LSTM proved applicable to various learning tasks, pre-<br>viously considered impossible to learn. This included recalling high precision<br>real numbers over extended noisy sequences [41], learning context free lan-<br>guages [21], and various tasks that require precise timing and counting [23].<br>In [43] LSTM was successfully introduced to meta-learning with a program<br>search tasks to approximate a learning algorithm for quadratic functions. The<br>successful application of reinforcement learning to solve non-Markovian learning<br>tasks with long-term dependencies was shown by [2].</p>",
            "id": 298,
            "page": 33,
            "text": "In early experiments LSTM proved applicable to various learning tasks, previously considered impossible to learn. This included recalling high precision real numbers over extended noisy sequences , learning context free languages , and various tasks that require precise timing and counting . In  LSTM was successfully introduced to meta-learning with a program search tasks to approximate a learning algorithm for quadratic functions. The successful application of reinforcement learning to solve non-Markovian learning tasks with long-term dependencies was shown by ."
        },
        {
            "bounding_box": [
                {
                    "x": 517,
                    "y": 1792
                },
                {
                    "x": 1271,
                    "y": 1792
                },
                {
                    "x": 1271,
                    "y": 1848
                },
                {
                    "x": 517,
                    "y": 1848
                }
            ],
            "category": "paragraph",
            "html": "<p id='299' style='font-size:20px'>11.2 Cognitive learning tasks</p>",
            "id": 299,
            "page": 33,
            "text": "11.2 Cognitive learning tasks"
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 1871
                },
                {
                    "x": 1959,
                    "y": 1871
                },
                {
                    "x": 1959,
                    "y": 2175
                },
                {
                    "x": 511,
                    "y": 2175
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='300' style='font-size:18px'>LSTM-RNNs proved great strengths in solving a large variety of cognitive learn-<br>ing tasks. Speech and handwriting recognition, and more recently machine<br>translation are the most predominant in literature. Other cognitive learn-<br>ing tasks include emotion recognition from speech [78], text generation [67],<br>handwriting generation [24], constituency parsing [71], and conversational mod-<br>elling [72].</p>",
            "id": 300,
            "page": 33,
            "text": "LSTM-RNNs proved great strengths in solving a large variety of cognitive learning tasks. Speech and handwriting recognition, and more recently machine translation are the most predominant in literature. Other cognitive learning tasks include emotion recognition from speech , text generation , handwriting generation , constituency parsing , and conversational modelling ."
        },
        {
            "bounding_box": [
                {
                    "x": 514,
                    "y": 2231
                },
                {
                    "x": 1086,
                    "y": 2231
                },
                {
                    "x": 1086,
                    "y": 2280
                },
                {
                    "x": 514,
                    "y": 2280
                }
            ],
            "category": "paragraph",
            "html": "<p id='301' style='font-size:18px'>11.2.1 Speech recognition</p>",
            "id": 301,
            "page": 33,
            "text": "11.2.1 Speech recognition"
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 2306
                },
                {
                    "x": 1959,
                    "y": 2306
                },
                {
                    "x": 1959,
                    "y": 2957
                },
                {
                    "x": 511,
                    "y": 2957
                }
            ],
            "category": "paragraph",
            "html": "<p id='302' style='font-size:16px'>A first indication of the capabilities of neural networks in tasks related to nat-<br>ural language was given by [4] with a neural language modelling task. In 2003<br>good results applying standard LSTM-RNN networks with a mix of LSTM and<br>sigmoidal units to speech recognition tasks were obtained by [25, 26]. Better<br>results comparable to Hidden-Markov-Model (HMM)-based systems [7] were<br>achieved using bidirectional training with BLSTM [6, 34]. A variant named<br>BLSTM-CTC [28, 19, 17] finally outperformed HMMs, with recent improve-<br>ments documented in [44, 77]. A deep variant of stacked BLSTM-CTC was<br>used in 2013 by [33] and later extended with a modified CTC objective func-<br>tion by [30], both achieving outstanding results. The performance of different<br>LSTM-RNN architectures on large vocabulary speech recognition tasks was in-<br>vestigated by [63], with best results using an LSTM/ HMM hybrid architecture.<br>Comparable results were achieved by [20].</p>",
            "id": 302,
            "page": 33,
            "text": "A first indication of the capabilities of neural networks in tasks related to natural language was given by  with a neural language modelling task. In 2003 good results applying standard LSTM-RNN networks with a mix of LSTM and sigmoidal units to speech recognition tasks were obtained by . Better results comparable to Hidden-Markov-Model (HMM)-based systems  were achieved using bidirectional training with BLSTM . A variant named BLSTM-CTC  finally outperformed HMMs, with recent improvements documented in . A deep variant of stacked BLSTM-CTC was used in 2013 by  and later extended with a modified CTC objective function by , both achieving outstanding results. The performance of different LSTM-RNN architectures on large vocabulary speech recognition tasks was investigated by , with best results using an LSTM/ HMM hybrid architecture. Comparable results were achieved by ."
        },
        {
            "bounding_box": [
                {
                    "x": 1210,
                    "y": 3097
                },
                {
                    "x": 1260,
                    "y": 3097
                },
                {
                    "x": 1260,
                    "y": 3138
                },
                {
                    "x": 1210,
                    "y": 3138
                }
            ],
            "category": "footer",
            "html": "<footer id='303' style='font-size:14px'>33</footer>",
            "id": 303,
            "page": 33,
            "text": "33"
        },
        {
            "bounding_box": [
                {
                    "x": 510,
                    "y": 527
                },
                {
                    "x": 1962,
                    "y": 527
                },
                {
                    "x": 1962,
                    "y": 980
                },
                {
                    "x": 510,
                    "y": 980
                }
            ],
            "category": "paragraph",
            "html": "<p id='304' style='font-size:18px'>More recently LSTM was improving results using the sequence-to-sequence<br>framework ([68]) and attention-based learning ([11] [12]). In 2015 [8] introduced<br>an specialised architecture for speech recognition with two functions, the first<br>called 'listener' and the latter called 'attend and spell'. The 'listener' function<br>uses BLSTM with a pyramid structure (pBLSTM), similar to clockwork RNNs<br>introduced by [50]. The other function, 'attend and spell', uses an attention-<br>based LSTM transducer developed by [1] and [12]. Both functions are trained<br>with methods introduced in the sequence-to-sequence framework [68] and in<br>attention-based learning [1].</p>",
            "id": 304,
            "page": 34,
            "text": "More recently LSTM was improving results using the sequence-to-sequence framework () and attention-based learning ( ). In 2015  introduced an specialised architecture for speech recognition with two functions, the first called 'listener' and the latter called 'attend and spell'. The 'listener' function uses BLSTM with a pyramid structure (pBLSTM), similar to clockwork RNNs introduced by . The other function, 'attend and spell', uses an attentionbased LSTM transducer developed by  and . Both functions are trained with methods introduced in the sequence-to-sequence framework  and in attention-based learning ."
        },
        {
            "bounding_box": [
                {
                    "x": 515,
                    "y": 1034
                },
                {
                    "x": 1207,
                    "y": 1034
                },
                {
                    "x": 1207,
                    "y": 1085
                },
                {
                    "x": 515,
                    "y": 1085
                }
            ],
            "category": "paragraph",
            "html": "<p id='305' style='font-size:20px'>11.2.2 Handwriting recognition</p>",
            "id": 305,
            "page": 34,
            "text": "11.2.2 Handwriting recognition"
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 1108
                },
                {
                    "x": 1960,
                    "y": 1108
                },
                {
                    "x": 1960,
                    "y": 1613
                },
                {
                    "x": 511,
                    "y": 1613
                }
            ],
            "category": "paragraph",
            "html": "<p id='306' style='font-size:16px'>In 2007 [52] introduced BLSTM-CTC and applied it to online handwriting recog-<br>nition, with results later outperforming Hidden-Markov-based recognition sys-<br>tems presented by [32]. [27] combined BLSTM-CTC with a probabilistic lan-<br>guage model and by this developed a system capable of directly transcribing raw<br>online handwriting data. In a real-world use case this system showed a very high<br>automation rate with an error rate comparable to a human on this kind of task<br>( [57]). In another approach [35] combined BLSTM-CTC with multidimensional<br>LSTM and applied it to an offline handwriting recognition task, as well outper-<br>forming classifiers based on Hidden-Markov models. In 2013 [81, 61] applied the<br>very successful regularisation method dropout as proposed by [37, 64]).</p>",
            "id": 306,
            "page": 34,
            "text": "In 2007  introduced BLSTM-CTC and applied it to online handwriting recognition, with results later outperforming Hidden-Markov-based recognition systems presented by .  combined BLSTM-CTC with a probabilistic language model and by this developed a system capable of directly transcribing raw online handwriting data. In a real-world use case this system showed a very high automation rate with an error rate comparable to a human on this kind of task ( ). In another approach  combined BLSTM-CTC with multidimensional LSTM and applied it to an offline handwriting recognition task, as well outperforming classifiers based on Hidden-Markov models. In 2013  applied the very successful regularisation method dropout as proposed by )."
        },
        {
            "bounding_box": [
                {
                    "x": 515,
                    "y": 1668
                },
                {
                    "x": 1112,
                    "y": 1668
                },
                {
                    "x": 1112,
                    "y": 1716
                },
                {
                    "x": 515,
                    "y": 1716
                }
            ],
            "category": "paragraph",
            "html": "<p id='307' style='font-size:14px'>11.2.3 Machine translation</p>",
            "id": 307,
            "page": 34,
            "text": "11.2.3 Machine translation"
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 1742
                },
                {
                    "x": 1960,
                    "y": 1742
                },
                {
                    "x": 1960,
                    "y": 2195
                },
                {
                    "x": 511,
                    "y": 2195
                }
            ],
            "category": "paragraph",
            "html": "<p id='308' style='font-size:16px'>In 2014 [10] the authors applied the RNN encoder-decoder neural network ar-<br>chitecture to machine translation and improved the performance of a statistical<br>machine translation system. The RNN Encoder-Decoder architecture is based<br>on an approach communicated by [48]. A very similar deep LSTM architecture,<br>referred to as sequence-to-sequence learning, was investigated by [68] confirming<br>these results. [53] addressed the rare word problem using sequence-to-sequence,<br>which improves the ability to translate words not in the vocabulary. The archi-<br>tecture was further improved by [1] addressing issues related to the translation<br>of long sentences by implementing an attention mechanism into the decoder.</p>",
            "id": 308,
            "page": 34,
            "text": "In 2014  the authors applied the RNN encoder-decoder neural network architecture to machine translation and improved the performance of a statistical machine translation system. The RNN Encoder-Decoder architecture is based on an approach communicated by . A very similar deep LSTM architecture, referred to as sequence-to-sequence learning, was investigated by  confirming these results.  addressed the rare word problem using sequence-to-sequence, which improves the ability to translate words not in the vocabulary. The architecture was further improved by  addressing issues related to the translation of long sentences by implementing an attention mechanism into the decoder."
        },
        {
            "bounding_box": [
                {
                    "x": 514,
                    "y": 2251
                },
                {
                    "x": 1055,
                    "y": 2251
                },
                {
                    "x": 1055,
                    "y": 2303
                },
                {
                    "x": 514,
                    "y": 2303
                }
            ],
            "category": "paragraph",
            "html": "<p id='309' style='font-size:20px'>11.2.4 Image processing</p>",
            "id": 309,
            "page": 34,
            "text": "11.2.4 Image processing"
        },
        {
            "bounding_box": [
                {
                    "x": 510,
                    "y": 2326
                },
                {
                    "x": 1961,
                    "y": 2326
                },
                {
                    "x": 1961,
                    "y": 2878
                },
                {
                    "x": 510,
                    "y": 2878
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='310' style='font-size:18px'>In 2012 BSLTM was applied to keyword spotting and mode detection distin-<br>guishing different types of content in handwritten documents, such as text,<br>formulas, diagrams and figures, outperforming HMMs and SVMs [44, 45, 59].<br>At approximately the same period of time [51] investigated the classification<br>of high-resolution images from the ImageNet database with considerable better<br>results then previous approaches. In 2015 the more recent LSTM variant using<br>the Sequence-to-Sequence framework was successfully trained by [73, 79] to gen-<br>erate natural sentences in plain English describing images. Also in 2015 [14] the<br>authors combined LSTMs with a deep hierarchical visual feature extractor and<br>applied the model to image interpretation and classification tasks, like activity<br>recognition and image/video description.</p>",
            "id": 310,
            "page": 34,
            "text": "In 2012 BSLTM was applied to keyword spotting and mode detection distinguishing different types of content in handwritten documents, such as text, formulas, diagrams and figures, outperforming HMMs and SVMs . At approximately the same period of time  investigated the classification of high-resolution images from the ImageNet database with considerable better results then previous approaches. In 2015 the more recent LSTM variant using the Sequence-to-Sequence framework was successfully trained by  to generate natural sentences in plain English describing images. Also in 2015  the authors combined LSTMs with a deep hierarchical visual feature extractor and applied the model to image interpretation and classification tasks, like activity recognition and image/video description."
        },
        {
            "bounding_box": [
                {
                    "x": 1210,
                    "y": 3096
                },
                {
                    "x": 1259,
                    "y": 3096
                },
                {
                    "x": 1259,
                    "y": 3138
                },
                {
                    "x": 1210,
                    "y": 3138
                }
            ],
            "category": "footer",
            "html": "<footer id='311' style='font-size:14px'>34</footer>",
            "id": 311,
            "page": 34,
            "text": "34"
        },
        {
            "bounding_box": [
                {
                    "x": 516,
                    "y": 523
                },
                {
                    "x": 1182,
                    "y": 523
                },
                {
                    "x": 1182,
                    "y": 579
                },
                {
                    "x": 516,
                    "y": 579
                }
            ],
            "category": "paragraph",
            "html": "<p id='312' style='font-size:18px'>11.3 Other learning tasks</p>",
            "id": 312,
            "page": 35,
            "text": "11.3 Other learning tasks"
        },
        {
            "bounding_box": [
                {
                    "x": 513,
                    "y": 604
                },
                {
                    "x": 1958,
                    "y": 604
                },
                {
                    "x": 1958,
                    "y": 806
                },
                {
                    "x": 513,
                    "y": 806
                }
            ],
            "category": "paragraph",
            "html": "<p id='313' style='font-size:16px'>Early papers applied LSTM-RNN to a number of real world problems pushing its<br>evolution further. Covered problems include protein secondary structure predic-<br>tion [40, 9] and music generation [15]. Network security was covered in [65, 66]<br>were the authors apply LSTM-RNN to the DARPA intrusion detection dataset.</p>",
            "id": 313,
            "page": 35,
            "text": "Early papers applied LSTM-RNN to a number of real world problems pushing its evolution further. Covered problems include protein secondary structure prediction  and music generation . Network security was covered in  were the authors apply LSTM-RNN to the DARPA intrusion detection dataset."
        },
        {
            "bounding_box": [
                {
                    "x": 512,
                    "y": 810
                },
                {
                    "x": 1960,
                    "y": 810
                },
                {
                    "x": 1960,
                    "y": 1004
                },
                {
                    "x": 512,
                    "y": 1004
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='314' style='font-size:16px'>In [80, 70] the authors apply computational tasks to LSTM-RNN. In 2014<br>the authors of [80] evaluate short computer programs using the Sequence-to-<br>Sequence framework. One year later the authors of [70] use a modified version<br>of the framework to learn solutions of combinatorial optimisation problems.</p>",
            "id": 314,
            "page": 35,
            "text": "In  the authors apply computational tasks to LSTM-RNN. In 2014 the authors of  evaluate short computer programs using the Sequence-toSequence framework. One year later the authors of  use a modified version of the framework to learn solutions of combinatorial optimisation problems."
        },
        {
            "bounding_box": [
                {
                    "x": 517,
                    "y": 1079
                },
                {
                    "x": 1008,
                    "y": 1079
                },
                {
                    "x": 1008,
                    "y": 1142
                },
                {
                    "x": 517,
                    "y": 1142
                }
            ],
            "category": "paragraph",
            "html": "<p id='315' style='font-size:20px'>12 Conclusions</p>",
            "id": 315,
            "page": 35,
            "text": "12 Conclusions"
        },
        {
            "bounding_box": [
                {
                    "x": 510,
                    "y": 1178
                },
                {
                    "x": 1962,
                    "y": 1178
                },
                {
                    "x": 1962,
                    "y": 1584
                },
                {
                    "x": 510,
                    "y": 1584
                }
            ],
            "category": "paragraph",
            "html": "<p id='316' style='font-size:16px'>In this article, we covered the derivation of LSTM in detail, summarising the<br>most relevant literature. Specifically, we highlighted the vanishing error prob-<br>lem, which is a serious shortcoming of RNNs. LSTM provides a possible solution<br>to this problem by introducing a constant error flow through the internal states<br>of special memory cells. In this way, LSTM is able to tackle long time-lag prob-<br>lems, bridging time intervals in excess of 1,000 time steps. Finally, we introduced<br>two LSTM extensions that enable LSTM to learn self-resets and precise timing.<br>With self-resets, LSTM is able to free memory of irrelevant information.</p>",
            "id": 316,
            "page": 35,
            "text": "In this article, we covered the derivation of LSTM in detail, summarising the most relevant literature. Specifically, we highlighted the vanishing error problem, which is a serious shortcoming of RNNs. LSTM provides a possible solution to this problem by introducing a constant error flow through the internal states of special memory cells. In this way, LSTM is able to tackle long time-lag problems, bridging time intervals in excess of 1,000 time steps. Finally, we introduced two LSTM extensions that enable LSTM to learn self-resets and precise timing. With self-resets, LSTM is able to free memory of irrelevant information."
        },
        {
            "bounding_box": [
                {
                    "x": 516,
                    "y": 1655
                },
                {
                    "x": 1082,
                    "y": 1655
                },
                {
                    "x": 1082,
                    "y": 1722
                },
                {
                    "x": 516,
                    "y": 1722
                }
            ],
            "category": "paragraph",
            "html": "<p id='317' style='font-size:22px'>Acknowledgements</p>",
            "id": 317,
            "page": 35,
            "text": "Acknowledgements"
        },
        {
            "bounding_box": [
                {
                    "x": 511,
                    "y": 1755
                },
                {
                    "x": 1960,
                    "y": 1755
                },
                {
                    "x": 1960,
                    "y": 2105
                },
                {
                    "x": 511,
                    "y": 2105
                }
            ],
            "category": "paragraph",
            "html": "<p id='318' style='font-size:16px'>This work was mainly pushed as a private project from Ralf C. Staudemeyer<br>spanning a period of ten years from 2007-17. During the time 2013-15 it was<br>partially supported by post-doctoral fellowship research funds provided by the<br>South African National Research Foundation, Rhodes University, the University<br>of South Africa, and the University of Passau. The co-author Eric Rothstein<br>Morris picked-up the loose ends, developed the unified notation for this article<br>in 2015-16.</p>",
            "id": 318,
            "page": 35,
            "text": "This work was mainly pushed as a private project from Ralf C. Staudemeyer spanning a period of ten years from 2007-17. During the time 2013-15 it was partially supported by post-doctoral fellowship research funds provided by the South African National Research Foundation, Rhodes University, the University of South Africa, and the University of Passau. The co-author Eric Rothstein Morris picked-up the loose ends, developed the unified notation for this article in 2015-16."
        },
        {
            "bounding_box": [
                {
                    "x": 512,
                    "y": 2106
                },
                {
                    "x": 1961,
                    "y": 2106
                },
                {
                    "x": 1961,
                    "y": 2409
                },
                {
                    "x": 512,
                    "y": 2409
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='319' style='font-size:14px'>We acknowledge support for this work from Ralf's Ph.D. supervisor Chris-<br>tian W. Omlin for raising the authors interest to investigate the capabilities of<br>Long Short-Term Memory Recurrent Neural Networks. Very special thanks go<br>to Arne Janza for doing the internal review. Without his dedicated support to<br>eliminate a number of hard to find logical inconsistencies this publication would<br>not have found its way to the reader.</p>",
            "id": 319,
            "page": 35,
            "text": "We acknowledge support for this work from Ralf's Ph.D. supervisor Christian W. Omlin for raising the authors interest to investigate the capabilities of Long Short-Term Memory Recurrent Neural Networks. Very special thanks go to Arne Janza for doing the internal review. Without his dedicated support to eliminate a number of hard to find logical inconsistencies this publication would not have found its way to the reader."
        },
        {
            "bounding_box": [
                {
                    "x": 515,
                    "y": 2481
                },
                {
                    "x": 840,
                    "y": 2481
                },
                {
                    "x": 840,
                    "y": 2544
                },
                {
                    "x": 515,
                    "y": 2544
                }
            ],
            "category": "paragraph",
            "html": "<p id='320' style='font-size:20px'>References</p>",
            "id": 320,
            "page": 35,
            "text": "References"
        },
        {
            "bounding_box": [
                {
                    "x": 531,
                    "y": 2581
                },
                {
                    "x": 1962,
                    "y": 2581
                },
                {
                    "x": 1962,
                    "y": 2922
                },
                {
                    "x": 531,
                    "y": 2922
                }
            ],
            "category": "paragraph",
            "html": "<p id='321' style='font-size:16px'>[1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine<br>translation by jointly learning to align and translate. In Proc. of the Int.<br>Conf. on Learning Representations (ICLR 2015), volume 26, page 15, sep<br>2015.<br>[2] Bram Bakker. Reinforcement learning with long short-term memory. In<br>Advances in Neural Information Processing Systems (NIPS'02), 2002.</p>",
            "id": 321,
            "page": 35,
            "text": " Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In Proc. of the Int. Conf. on Learning Representations (ICLR 2015), volume 26, page 15, sep 2015.  Bram Bakker. Reinforcement learning with long short-term memory. In Advances in Neural Information Processing Systems (NIPS'02), 2002."
        },
        {
            "bounding_box": [
                {
                    "x": 1209,
                    "y": 3097
                },
                {
                    "x": 1260,
                    "y": 3097
                },
                {
                    "x": 1260,
                    "y": 3139
                },
                {
                    "x": 1209,
                    "y": 3139
                }
            ],
            "category": "footer",
            "html": "<footer id='322' style='font-size:14px'>35</footer>",
            "id": 322,
            "page": 35,
            "text": "35"
        },
        {
            "bounding_box": [
                {
                    "x": 524,
                    "y": 512
                },
                {
                    "x": 1963,
                    "y": 512
                },
                {
                    "x": 1963,
                    "y": 3034
                },
                {
                    "x": 524,
                    "y": 3034
                }
            ],
            "category": "paragraph",
            "html": "<p id='323' style='font-size:18px'>[3] Justin Bayer, Daan Wierstra, Julian Togelius, and Jurgen Schmidhuber.<br>Evolving memory cell structures for sequence learning. In Int. Conf. on<br>Artificial Neural Networks, pages 755-764, 2009.<br>[4] Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and Christian Janvin.<br>A Neural Probabilistic Language Model. The Journal of Machine Learning<br>Research, 3:1137-1155, 2003.<br>[5] Yoshua Bengio, Patrice Simard, Paolo Frasconi, and Paolo Frasconi Yoshua<br>Bengio, Patrice Simard. Learning long-term dependencies with gradient<br>descent is difficult. IEEE trans. on Neural Networks / A publication of the<br>IEEE Neural Networks Council, 5(2):157-66, jan 1994.<br>[6] N Beringer, A Graves, F Schiel, and J Schmidhuber. Classifying un-<br>prompted speech by retraining LSTM Nets. In W Duch, J Kacprzyk, E Oja,<br>and S Zadrozny, editors, Artificial Neural Networks: Biological Inspirations<br>(ICANN), volume 3696 LNCS, pages 575-581. Springer-Verlag Berlin Hei-<br>delberg, 2005.<br>[7] Herve A. Bourlard and Nelson Morgan. Connectionist Speech Recognition<br>- a hybrid approach. Kluwer Academic Publishers, Boston, MA, 1994.<br>[8] William Chan, Navdeep Jaitly, Quoc V. Le, and Oriol Vinyals. Listen,<br>Attend and Spell. arXiv preprint, pages 1-16, aug 2015.<br>[9] Jinmiao Chen and Narendra S. Chaudhari. Capturing Long-Term Depen-<br>dencies for Protein Secondary Structure Prediction. In Fu-Liang Yin, Jun<br>Wang, and Chengan Guo, editors, Advances in Neural Networks - Proc. of<br>the Int. Symp. on Neural Networks (ISNN 2004), pages 494-500, Berlin,<br>Heidelberg, 2004. Springer Berlin Heidelberg.<br>[10] Kyunghyun Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bah-<br>danau, Fethi Bougares, Holger Schwenk, Yoshua Bengio, Bart van Mer-<br>rienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger<br>Schwenk, and Yoshua Bengio. Learning phrase representations using RNN<br>encoder-decoder for statistical machine translation. In Proc. of the Conf.<br>on Empirical Methods in Natural Language Processing (EMNLP'14), pages<br>1724-1734, Stroudsburg, PA, USA, 2014. Association for Computational<br>Linguistics.<br>[11] Jan Chorowski, Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.<br>End-to-end continuous speech recognition using attention-based recurrent<br>NN: first results. Deep Learning and Representation Learning Workshop<br>(NIPS 2014), pages 1-10, dec 2014.<br>[12] Jan K Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho,<br>and Yoshua Bengio. Attention-based models for speech recognition. In<br>C Cortes, N D Lawrence, D D Lee, M Sugiyama, and R Garnett, editors,<br>Advances in Neural Information Processing Systems 28, pages 577-585.<br>Curran Associates, Inc., jun 2015.<br>[13] Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio.<br>Empirical evaluation of gated recurrent neural networks on sequence mod-<br>eling. In arXiv, pages 1-9, dec 2014.</p>",
            "id": 323,
            "page": 36,
            "text": " Justin Bayer, Daan Wierstra, Julian Togelius, and Jurgen Schmidhuber. Evolving memory cell structures for sequence learning. In Int. Conf. on Artificial Neural Networks, pages 755-764, 2009.  Yoshua Bengio, Rejean Ducharme, Pascal Vincent, and Christian Janvin. A Neural Probabilistic Language Model. The Journal of Machine Learning Research, 3:1137-1155, 2003.  Yoshua Bengio, Patrice Simard, Paolo Frasconi, and Paolo Frasconi Yoshua Bengio, Patrice Simard. Learning long-term dependencies with gradient descent is difficult. IEEE trans. on Neural Networks / A publication of the IEEE Neural Networks Council, 5(2):157-66, jan 1994.  N Beringer, A Graves, F Schiel, and J Schmidhuber. Classifying unprompted speech by retraining LSTM Nets. In W Duch, J Kacprzyk, E Oja, and S Zadrozny, editors, Artificial Neural Networks: Biological Inspirations (ICANN), volume 3696 LNCS, pages 575-581. Springer-Verlag Berlin Heidelberg, 2005.  Herve A. Bourlard and Nelson Morgan. Connectionist Speech Recognition - a hybrid approach. Kluwer Academic Publishers, Boston, MA, 1994.  William Chan, Navdeep Jaitly, Quoc V. Le, and Oriol Vinyals. Listen, Attend and Spell. arXiv preprint, pages 1-16, aug 2015.  Jinmiao Chen and Narendra S. Chaudhari. Capturing Long-Term Dependencies for Protein Secondary Structure Prediction. In Fu-Liang Yin, Jun Wang, and Chengan Guo, editors, Advances in Neural Networks - Proc. of the Int. Symp. on Neural Networks (ISNN 2004), pages 494-500, Berlin, Heidelberg, 2004. Springer Berlin Heidelberg.  Kyunghyun Cho, Bart Van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder-decoder for statistical machine translation. In Proc. of the Conf. on Empirical Methods in Natural Language Processing (EMNLP'14), pages 1724-1734, Stroudsburg, PA, USA, 2014. Association for Computational Linguistics.  Jan Chorowski, Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. End-to-end continuous speech recognition using attention-based recurrent NN: first results. Deep Learning and Representation Learning Workshop (NIPS 2014), pages 1-10, dec 2014.  Jan K Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio. Attention-based models for speech recognition. In C Cortes, N D Lawrence, D D Lee, M Sugiyama, and R Garnett, editors, Advances in Neural Information Processing Systems 28, pages 577-585. Curran Associates, Inc., jun 2015.  Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. In arXiv, pages 1-9, dec 2014."
        },
        {
            "bounding_box": [
                {
                    "x": 1210,
                    "y": 3098
                },
                {
                    "x": 1261,
                    "y": 3098
                },
                {
                    "x": 1261,
                    "y": 3138
                },
                {
                    "x": 1210,
                    "y": 3138
                }
            ],
            "category": "footer",
            "html": "<footer id='324' style='font-size:14px'>36</footer>",
            "id": 324,
            "page": 36,
            "text": "36"
        },
        {
            "bounding_box": [
                {
                    "x": 516,
                    "y": 511
                },
                {
                    "x": 1964,
                    "y": 511
                },
                {
                    "x": 1964,
                    "y": 2964
                },
                {
                    "x": 516,
                    "y": 2964
                }
            ],
            "category": "paragraph",
            "html": "<p id='325' style='font-size:18px'>[14] Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus<br>Rohrbach, Subhashini Venugopalan, Kate Saenko, Trevor Darrell, U T<br>Austin, Umass Lowell, U C Berkeley, Lisa Anne Hendricks, Sergio Guadar-<br>rama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, and<br>Trevor Darrell. Long-Term Recurrent Convolutional Networks for Visual<br>Recognition and Description. In Proc. of the Conf. on Computer Vision<br>and Pattern Recognition (CVPR'15), pages 2625-2634, jun 2015.<br>[15] Douglas Eck and Jurgen Schmidhuber. Finding temporal structure in mu-<br>sic: Blues improvisation with LSTM recurrent networks. In Proc. of the<br>12th Workshop on Neural Networks for Signal Processing, pages 747-756.<br>IEEE, IEEE, 2002.<br>[16] Jeffrey L. Elman. Finding structure in time. Cognitive Science, 14(2):179-<br>211, mar 1990.<br>[17] Florian Eyben, Martin Wollmer, Bjorn Schuller, and Alex Graves.<br>From speech to letters - using a novel neural network architecture for<br>grapheme based ASR. In Workshop on Automatic Speech Recognition<br>& Understanding, pages 376-380. IEEE, dec 2009.<br>[18] Santiago Fernandez, Alex Graves, and Jurgen Schmidhuber. Sequence la-<br>belling in structured domains with hierarchical recurrent neural networks.<br>In Proc. of the 20th Int. Joint Conf. on Artificial Intelligence (IJCAI'07),<br>pages 774-779, 2007.<br>[19] Santiago Fernandez, Alex Graves, and Jurgen Schmidhuber. Phoneme<br>recognition in TIMIT with BLSTM-CTC. Arxiv preprint arXiv08043269,<br>abs/0804.3:8, 2008.<br>[20] T Geiger, Zixing Zhang, Felix Weninger, Gerhard Rigoll, Jurgen T Geiger,<br>Zixing Zhang, Felix Weninger, Bjorn Schuller, and Gerhard Rigoll. Ro-<br>bust speech recognition using long short-term memory recurrent neu-<br>ral networks for hybrid acoustic modelling. Proc. of the Ann. Conf. of<br>International Speech Communication Association (INTERSPEECH 2014),<br>(September):631-635, 2014.<br>[21] Felix A. Gers and Jurgen Schmidhuber. LSTM recurrent networks learn<br>simple context-free and context-sensitive languages. IEEE Trans. on Neural<br>Networks, 12(6):1333-1340, jan 2001.<br>[22] Felix A. Gers, Jurgen Schmidhuber, and Fred Cummins. Learning to For-<br>get: Continual Prediction with LSTM. Neural Computation, 12(10):2451-<br>2471, oct 2000.<br>[23] Felix A. Gers, Nicol N. Schraudolph, and Jurgen Schmidhuber. Learning<br>precise timing with LSTM recurrent networks. Journal of Machine Learning<br>Research (JMLR), 3(1):115-143, 2002.<br>[24] Alex Graves. Generating sequences with recurrent neural networks. Proc.<br>of the 23rd Int. Conf. on Information and Knowledge Management (CIKM<br>'14), pages 101-110, aug 2014.</p>",
            "id": 325,
            "page": 37,
            "text": " Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, Trevor Darrell, U T Austin, Umass Lowell, U C Berkeley, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, and Trevor Darrell. Long-Term Recurrent Convolutional Networks for Visual Recognition and Description. In Proc. of the Conf. on Computer Vision and Pattern Recognition (CVPR'15), pages 2625-2634, jun 2015.  Douglas Eck and Jurgen Schmidhuber. Finding temporal structure in music: Blues improvisation with LSTM recurrent networks. In Proc. of the 12th Workshop on Neural Networks for Signal Processing, pages 747-756. IEEE, IEEE, 2002.  Jeffrey L. Elman. Finding structure in time. Cognitive Science, 14(2):179211, mar 1990.  Florian Eyben, Martin Wollmer, Bjorn Schuller, and Alex Graves. From speech to letters - using a novel neural network architecture for grapheme based ASR. In Workshop on Automatic Speech Recognition & Understanding, pages 376-380. IEEE, dec 2009.  Santiago Fernandez, Alex Graves, and Jurgen Schmidhuber. Sequence labelling in structured domains with hierarchical recurrent neural networks. In Proc. of the 20th Int. Joint Conf. on Artificial Intelligence (IJCAI'07), pages 774-779, 2007.  Santiago Fernandez, Alex Graves, and Jurgen Schmidhuber. Phoneme recognition in TIMIT with BLSTM-CTC. Arxiv preprint arXiv08043269, abs/0804.3:8, 2008.  T Geiger, Zixing Zhang, Felix Weninger, Gerhard Rigoll, Jurgen T Geiger, Zixing Zhang, Felix Weninger, Bjorn Schuller, and Gerhard Rigoll. Robust speech recognition using long short-term memory recurrent neural networks for hybrid acoustic modelling. Proc. of the Ann. Conf. of International Speech Communication Association (INTERSPEECH 2014), (September):631-635, 2014.  Felix A. Gers and Jurgen Schmidhuber. LSTM recurrent networks learn simple context-free and context-sensitive languages. IEEE Trans. on Neural Networks, 12(6):1333-1340, jan 2001.  Felix A. Gers, Jurgen Schmidhuber, and Fred Cummins. Learning to Forget: Continual Prediction with LSTM. Neural Computation, 12(10):24512471, oct 2000.  Felix A. Gers, Nicol N. Schraudolph, and Jurgen Schmidhuber. Learning precise timing with LSTM recurrent networks. Journal of Machine Learning Research (JMLR), 3(1):115-143, 2002.  Alex Graves. Generating sequences with recurrent neural networks. Proc. of the 23rd Int. Conf. on Information and Knowledge Management (CIKM '14), pages 101-110, aug 2014."
        },
        {
            "bounding_box": [
                {
                    "x": 1209,
                    "y": 3097
                },
                {
                    "x": 1260,
                    "y": 3097
                },
                {
                    "x": 1260,
                    "y": 3139
                },
                {
                    "x": 1209,
                    "y": 3139
                }
            ],
            "category": "footer",
            "html": "<footer id='326' style='font-size:14px'>37</footer>",
            "id": 326,
            "page": 37,
            "text": "37"
        },
        {
            "bounding_box": [
                {
                    "x": 517,
                    "y": 508
                },
                {
                    "x": 1964,
                    "y": 508
                },
                {
                    "x": 1964,
                    "y": 3035
                },
                {
                    "x": 517,
                    "y": 3035
                }
            ],
            "category": "paragraph",
            "html": "<p id='327' style='font-size:18px'>[25] Alex Graves, Nicole Beringer, and Jurgen Schmidhuber. A comparison<br>between spiking and differentiable recurrent neural networks on spoken<br>digit recognition. In Proc. of the 23rd IASTED Int. Conf. on Modelling,<br>Identification, and Control, Grindelwald, 2003.<br>[26] Alex Graves, Nicole Beringer, and Jurgen Schmidhuber. Rapid retraining<br>on speech data with LSTM recurrent networks. Technical Report IDSIA-<br>09-05, IDSIA, 2005.<br>[27] Alex Graves, S Fernandez, and Marcus Liwicki. Unconstrained online hand-<br>writing recognition with recurrent neural networks. Neural Information<br>Processing Systems (NIPS'07), 20:577-584, 2007.<br>[28] Alex Graves, Santiago Fernandez, Faustino Gomez, and Jurgen Schmid-<br>huber. Connectionist temporal classification: Labelling unsegmented se-<br>quence data with recurrent neural networks. In Proc. of the 23rd Int. Conf.<br>on Machine Learning (ICML '06), number January, pages 369-376, New<br>York, New York, USA, 2006. ACM Press.<br>[29] Alex Graves, Santiago Fernandez, and Jurgen Schmidhuber. Multi-<br>Dimensional Recurrent Neural Networks. Proc. of the Int. Conf. on<br>Artificial Neural Networks (ICANN'07), 4668(1):549-558, may 2007.<br>[30] Alex Graves and Navdeep Jaitly. Towards End-To-End Speech Recogni-<br>tion with Recurrent Neural Networks. JMLR Workshop and Conference<br>Proceedings, 32(1):1764-1772, 2014.<br>[31] Alex Graves, Navdeep Jaitly, and Abdel Rahman Mohamed. Hybrid speech<br>recognition with Deep Bidirectional LSTM. In Proc. of the workshop on<br>Automatic Speech Recognition and Understanding (ASRU'13), pages 273-<br>278, 2013.<br>[32] Alex Graves, Marcus Liwicki, Santiago Fernandez, Roman Bertolami, Horst<br>Bunke, and Jurgen Schmidhuber. A novel connectionist system for uncon-<br>strained handwriting recognition. IEEE trans. on Pattern Analysis and<br>Machine Intelligence, 31(5):855-68, may 2009.<br>[33] Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recog-<br>nition with deep recurrent neural networks. In Int. Conf. on Acoustics,<br>Speech and Signal Processing (ICASSP'13), number 3, pages 6645 6649.<br>IEEE, may 2013.<br>[34] Alex Graves and Jurgen Schmidhuber. Framewise phoneme classification<br>with bidirectional LSTM networks. In Proc. of the Int. Joint Conf. on<br>Neural Networks, volume 18, pages 2047-2052, Oxford, UK, UK, jun 2005.<br>Elsevier Science Ltd.<br>[35] Alex Graves and Jurgen Schmidhuber. Offline handwriting recognition<br>with multidimensional recurrent neural networks. In Advances in Neural<br>Information Processing Systems 21 (NIPS'09), pages 545-552. MIT Press,<br>2009.<br>[36] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural Turing Machines.<br>Arxiv, pages 1-26, 2014.</p>",
            "id": 327,
            "page": 38,
            "text": " Alex Graves, Nicole Beringer, and Jurgen Schmidhuber. A comparison between spiking and differentiable recurrent neural networks on spoken digit recognition. In Proc. of the 23rd IASTED Int. Conf. on Modelling, Identification, and Control, Grindelwald, 2003.  Alex Graves, Nicole Beringer, and Jurgen Schmidhuber. Rapid retraining on speech data with LSTM recurrent networks. Technical Report IDSIA09-05, IDSIA, 2005.  Alex Graves, S Fernandez, and Marcus Liwicki. Unconstrained online handwriting recognition with recurrent neural networks. Neural Information Processing Systems (NIPS'07), 20:577-584, 2007.  Alex Graves, Santiago Fernandez, Faustino Gomez, and Jurgen Schmidhuber. Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks. In Proc. of the 23rd Int. Conf. on Machine Learning (ICML '06), number January, pages 369-376, New York, New York, USA, 2006. ACM Press.  Alex Graves, Santiago Fernandez, and Jurgen Schmidhuber. MultiDimensional Recurrent Neural Networks. Proc. of the Int. Conf. on Artificial Neural Networks (ICANN'07), 4668(1):549-558, may 2007.  Alex Graves and Navdeep Jaitly. Towards End-To-End Speech Recognition with Recurrent Neural Networks. JMLR Workshop and Conference Proceedings, 32(1):1764-1772, 2014.  Alex Graves, Navdeep Jaitly, and Abdel Rahman Mohamed. Hybrid speech recognition with Deep Bidirectional LSTM. In Proc. of the workshop on Automatic Speech Recognition and Understanding (ASRU'13), pages 273278, 2013.  Alex Graves, Marcus Liwicki, Santiago Fernandez, Roman Bertolami, Horst Bunke, and Jurgen Schmidhuber. A novel connectionist system for unconstrained handwriting recognition. IEEE trans. on Pattern Analysis and Machine Intelligence, 31(5):855-68, may 2009.  Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton. Speech recognition with deep recurrent neural networks. In Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP'13), number 3, pages 6645 6649. IEEE, may 2013.  Alex Graves and Jurgen Schmidhuber. Framewise phoneme classification with bidirectional LSTM networks. In Proc. of the Int. Joint Conf. on Neural Networks, volume 18, pages 2047-2052, Oxford, UK, UK, jun 2005. Elsevier Science Ltd.  Alex Graves and Jurgen Schmidhuber. Offline handwriting recognition with multidimensional recurrent neural networks. In Advances in Neural Information Processing Systems 21 (NIPS'09), pages 545-552. MIT Press, 2009.  Alex Graves, Greg Wayne, and Ivo Danihelka. Neural Turing Machines. Arxiv, pages 1-26, 2014."
        },
        {
            "bounding_box": [
                {
                    "x": 1209,
                    "y": 3097
                },
                {
                    "x": 1260,
                    "y": 3097
                },
                {
                    "x": 1260,
                    "y": 3138
                },
                {
                    "x": 1209,
                    "y": 3138
                }
            ],
            "category": "footer",
            "html": "<footer id='328' style='font-size:14px'>38</footer>",
            "id": 328,
            "page": 38,
            "text": "38"
        },
        {
            "bounding_box": [
                {
                    "x": 517,
                    "y": 511
                },
                {
                    "x": 1964,
                    "y": 511
                },
                {
                    "x": 1964,
                    "y": 2988
                },
                {
                    "x": 517,
                    "y": 2988
                }
            ],
            "category": "paragraph",
            "html": "<p id='329' style='font-size:18px'>[37] Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever,<br>and Ruslan R. Salakhutdinov. Improving neural networks by preventing<br>co-adaptation of feature detectors. ArXiv e-prints, pages 1-18, 2012.<br>[38] Josef Hochreiter. Untersuchungen zu dynamischen neuronalen Netzen.<br>Master's thesis, Institut fur Informatik, Technische Universitat, Munchen,<br>(April 1991):1-71, 1991.<br>[39] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jurgen Schmidhuber.<br>Gradient flow in recurrent nets: the difficulty of learning long-term depen-<br>dencies. A Field Guide to Dynamical Recurrent Neural Networks, page 15,<br>2001.<br>[40] Sepp Hochreiter, Martin Heusel, and Klaus Obermayer. Fast model-<br>based protein homology detection without alignment. Bioinformatics,<br>23(14):1728-1736, jul 2007.<br>[41] Sepp; Hochreiter and Jurgen Schmidhuber. Long Short-Term Memory.<br>Neural computation, 9(8):1735-1780, 1997.<br>[42] Sepp Hochreiter and Jurgen Schmidhuber. LSTM can solve hard long time<br>lag problems. Neural Information Processing Systems, pages 473-479, 1997.<br>[43] Sepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to learn<br>using gradient descent. In Georg Dorffner, Horst Bischof, and Kurt Hornik,<br>editors, Proc. of the Int. Conf. of Artificial Neural Networks (ICANN 2001),<br>pages 87-94, Berlin, Heidelberg, 2001. Springer Berlin Heidelberg.<br>[44] Emanuel Indermuehle, Volkmar Frinken, Andreas Fischer, and Horst<br>Bunke. Keyword spotting in online handwritten documents containing text<br>and non-text using BLSTM neural networks. In Int. Conf. on Document<br>Analysis and Recognition (ICDAR), pages 73-77. IEEE, 2011.<br>[45] Emanuel Indermuhle, Volkmar Frinken, and Horst Bunke. Mode detection<br>in online handwritten documents using BLSTM neural networks. In Int.<br>Conf. on Frontiers in Handwriting Recognition (ICFHR), pages 302-307.<br>IEEE, 2012.<br>[46] Michael I. Jordan. Attractor dynamics and parallelism in a connectionist<br>sequential machine. In Proc. of the Eigth Annual Conf. of the Cognitive<br>Science Society, pages 531-546. IEEE Press, jan 1986.<br>[47] Rafal Jozefowicz, Wojciech Zaremba, and Ilya Sutskever. An empirical<br>exploration of Recurrent Network Architectures. In Proc. of the 32nd Int.<br>Conf on Machine Learning, pp. 23422350, 2015, pages 2342--2350, 2015.<br>[48] Nal Kalchbrenner and Phil Blunsom. Recurrent Continuous Translation<br>Models. In Proc. of the Conf. on Empirical Methods in Natural Language<br>Processing (EMNLP'13), volume 3, page 413. Proceedings of the 2013 Con-<br>ference on Empirical Methods in Natural Language Processing, 2013.<br>[49] Nal Kalchbrenner, Ivo Danihelka, and Alex Graves. Grid Long Short-Term<br>Memory. In arXiv preprint, page 14, jul 2016.</p>",
            "id": 329,
            "page": 39,
            "text": " Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R. Salakhutdinov. Improving neural networks by preventing co-adaptation of feature detectors. ArXiv e-prints, pages 1-18, 2012.  Josef Hochreiter. Untersuchungen zu dynamischen neuronalen Netzen. Master's thesis, Institut fur Informatik, Technische Universitat, Munchen, (April 1991):1-71, 1991.  Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jurgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies. A Field Guide to Dynamical Recurrent Neural Networks, page 15, 2001.  Sepp Hochreiter, Martin Heusel, and Klaus Obermayer. Fast modelbased protein homology detection without alignment. Bioinformatics, 23(14):1728-1736, jul 2007.  Sepp; Hochreiter and Jurgen Schmidhuber. Long Short-Term Memory. Neural computation, 9(8):1735-1780, 1997.  Sepp Hochreiter and Jurgen Schmidhuber. LSTM can solve hard long time lag problems. Neural Information Processing Systems, pages 473-479, 1997.  Sepp Hochreiter, A Steven Younger, and Peter R Conwell. Learning to learn using gradient descent. In Georg Dorffner, Horst Bischof, and Kurt Hornik, editors, Proc. of the Int. Conf. of Artificial Neural Networks (ICANN 2001), pages 87-94, Berlin, Heidelberg, 2001. Springer Berlin Heidelberg.  Emanuel Indermuehle, Volkmar Frinken, Andreas Fischer, and Horst Bunke. Keyword spotting in online handwritten documents containing text and non-text using BLSTM neural networks. In Int. Conf. on Document Analysis and Recognition (ICDAR), pages 73-77. IEEE, 2011.  Emanuel Indermuhle, Volkmar Frinken, and Horst Bunke. Mode detection in online handwritten documents using BLSTM neural networks. In Int. Conf. on Frontiers in Handwriting Recognition (ICFHR), pages 302-307. IEEE, 2012.  Michael I. Jordan. Attractor dynamics and parallelism in a connectionist sequential machine. In Proc. of the Eigth Annual Conf. of the Cognitive Science Society, pages 531-546. IEEE Press, jan 1986.  Rafal Jozefowicz, Wojciech Zaremba, and Ilya Sutskever. An empirical exploration of Recurrent Network Architectures. In Proc. of the 32nd Int. Conf on Machine Learning, pp. 23422350, 2015, pages 2342--2350, 2015.  Nal Kalchbrenner and Phil Blunsom. Recurrent Continuous Translation Models. In Proc. of the Conf. on Empirical Methods in Natural Language Processing (EMNLP'13), volume 3, page 413. Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, 2013.  Nal Kalchbrenner, Ivo Danihelka, and Alex Graves. Grid Long Short-Term Memory. In arXiv preprint, page 14, jul 2016."
        },
        {
            "bounding_box": [
                {
                    "x": 1209,
                    "y": 3097
                },
                {
                    "x": 1260,
                    "y": 3097
                },
                {
                    "x": 1260,
                    "y": 3138
                },
                {
                    "x": 1209,
                    "y": 3138
                }
            ],
            "category": "footer",
            "html": "<footer id='330' style='font-size:14px'>39</footer>",
            "id": 330,
            "page": 39,
            "text": "39"
        },
        {
            "bounding_box": [
                {
                    "x": 516,
                    "y": 511
                },
                {
                    "x": 1962,
                    "y": 511
                },
                {
                    "x": 1962,
                    "y": 3032
                },
                {
                    "x": 516,
                    "y": 3032
                }
            ],
            "category": "paragraph",
            "html": "<p id='331' style='font-size:18px'>[50] Jan Koutnik, Klaus Greff, Faustino Gomez, and Jurgen Schmidhuber. A<br>Clockwork RNN. In Proc. of the 31st Int. Conf. on Machine Learning<br>(ICML 2014), volume 32, pages 1863-1871, 2014.<br>[51] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet Clas-<br>sification with Deep Convolutional Neural Networks. In F Pereira, C J C<br>Burges, L Bottou, and K Q Weinberger, editors, Advances in Neural<br>Information Processing Systems 25, pages 1-9. Curran Associates, Inc.,<br>2012.<br>[52] M Liwicki, A Graves, H Bunke, and J Schmidhuber. A novel approach<br>to on-line handwriting recognition based on bidirectional long short-term<br>memory networks. In Proc. of the 9th Int. Conf. on Document Analysis<br>and Recognition, pages 367{ }371, 2007.<br>[53] Minh- Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, and Woj-<br>ciech Zaremba. Addressing the rare word problem in neural machine trans-<br>lation. Arxiv, pages 1-11, 2014.<br>[54] Qi Lyu and Jun Zhu. Revisit long short-term memory: an optimization<br>perspective. In Deep Learning and Representation Learning Workshop<br>(NIPS 2014), pages 1-9, 2014.<br>[55] Marvin L. Minsky and Seymour A. Papert. Perceptrons: An introduction<br>to computational geometry. Expanded. MIT Press, Cambridge, 1988.<br>[56] Michael C Mozer. Induction of Multiscale Temporal Structure. In Advances<br>in Neural Information Processing Systems 4, pages 275-282. Morgan Kauf-<br>mann, 1992.<br>[57] Thibauld Nion, Fares Menasri, Jerome Louradour, Cedric Sibade, Thomas<br>Retornaz, Pierre Yves Metaireau, and Christopher Kermorvant. Handwrit-<br>ten information extraction from historical census documents. Proc. of the<br>Int. Conf. on Document Analysis and Recognition (ICDAR 2013), pages<br>822-826, 2013.<br>[58] Randall C O'Reilly and Michael J Frank. Making working memory work: a<br>computational model of learning in the prefrontal cortex and basal ganglia.<br>Neural Computation, 18(2):283-328, feb 2006.<br>[59] Sebastian Otte, Dirk Krechel, Marcus Liwicki, and Andreas Dengel. Local<br>Feature Based Online Mode Detection with Recurrent Neural Networks.<br>Int. Conf. on Frontiers in Handwriting Recognition, pages 533-537, 2012.<br>[60] JA A Perez-Ortiz, FA A Felix A. Gers, Douglas Eck, J??rgen U. Schmid-<br>huber, Juan Antonio P??rez-Ortiz, FA A Felix A. Gers, Douglas Eck, and<br>J??rgen U. Schmidhuber. Kalman filters improve LSTM network per-<br>formance in problems unsolvable by traditional recurrent nets. Neural<br>Networks, 16(2):241-250, 2003.<br>[61] Vu Pham, Theodore Theodore Theodore Bluche, Christopher Kermorvant,<br>and Jerome Jerome Jerome Louradour. Dropout improves recurrent neural<br>networks for handwriting recognition. In Proc. of the Int. Conf. on Frontiers<br>in Handwriting Recognition (ICFHR'13), volume 2014-Decem, pages 285-<br>290. IEEE, nov 2013.</p>",
            "id": 331,
            "page": 40,
            "text": " Jan Koutnik, Klaus Greff, Faustino Gomez, and Jurgen Schmidhuber. A Clockwork RNN. In Proc. of the 31st Int. Conf. on Machine Learning (ICML 2014), volume 32, pages 1863-1871, 2014.  Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. ImageNet Classification with Deep Convolutional Neural Networks. In F Pereira, C J C Burges, L Bottou, and K Q Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 1-9. Curran Associates, Inc., 2012.  M Liwicki, A Graves, H Bunke, and J Schmidhuber. A novel approach to on-line handwriting recognition based on bidirectional long short-term memory networks. In Proc. of the 9th Int. Conf. on Document Analysis and Recognition, pages 367{ }371, 2007.  Minh- Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, and Wojciech Zaremba. Addressing the rare word problem in neural machine translation. Arxiv, pages 1-11, 2014.  Qi Lyu and Jun Zhu. Revisit long short-term memory: an optimization perspective. In Deep Learning and Representation Learning Workshop (NIPS 2014), pages 1-9, 2014.  Marvin L. Minsky and Seymour A. Papert. Perceptrons: An introduction to computational geometry. Expanded. MIT Press, Cambridge, 1988.  Michael C Mozer. Induction of Multiscale Temporal Structure. In Advances in Neural Information Processing Systems 4, pages 275-282. Morgan Kaufmann, 1992.  Thibauld Nion, Fares Menasri, Jerome Louradour, Cedric Sibade, Thomas Retornaz, Pierre Yves Metaireau, and Christopher Kermorvant. Handwritten information extraction from historical census documents. Proc. of the Int. Conf. on Document Analysis and Recognition (ICDAR 2013), pages 822-826, 2013.  Randall C O'Reilly and Michael J Frank. Making working memory work: a computational model of learning in the prefrontal cortex and basal ganglia. Neural Computation, 18(2):283-328, feb 2006.  Sebastian Otte, Dirk Krechel, Marcus Liwicki, and Andreas Dengel. Local Feature Based Online Mode Detection with Recurrent Neural Networks. Int. Conf. on Frontiers in Handwriting Recognition, pages 533-537, 2012.  JA A Perez-Ortiz, FA A Felix A. Gers, Douglas Eck, J??rgen U. Schmidhuber, Juan Antonio P??rez-Ortiz, FA A Felix A. Gers, Douglas Eck, and J??rgen U. Schmidhuber. Kalman filters improve LSTM network performance in problems unsolvable by traditional recurrent nets. Neural Networks, 16(2):241-250, 2003.  Vu Pham, Theodore Theodore Theodore Bluche, Christopher Kermorvant, and Jerome Jerome Jerome Louradour. Dropout improves recurrent neural networks for handwriting recognition. In Proc. of the Int. Conf. on Frontiers in Handwriting Recognition (ICFHR'13), volume 2014-Decem, pages 285290. IEEE, nov 2013."
        },
        {
            "bounding_box": [
                {
                    "x": 1213,
                    "y": 3098
                },
                {
                    "x": 1260,
                    "y": 3098
                },
                {
                    "x": 1260,
                    "y": 3137
                },
                {
                    "x": 1213,
                    "y": 3137
                }
            ],
            "category": "footer",
            "html": "<footer id='332' style='font-size:14px'>40</footer>",
            "id": 332,
            "page": 40,
            "text": "40"
        },
        {
            "bounding_box": [
                {
                    "x": 517,
                    "y": 511
                },
                {
                    "x": 1964,
                    "y": 511
                },
                {
                    "x": 1964,
                    "y": 3028
                },
                {
                    "x": 517,
                    "y": 3028
                }
            ],
            "category": "paragraph",
            "html": "<p id='333' style='font-size:18px'>[62] David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Learning<br>internal representations by error propagation. In J L McClelland and D E<br>Rumelhart, editors, Parallel distributed processing: Explorations in the<br>microstructure of cognition, volume 1, pages 318-362. MIT Press, jan 1985.<br>[63] Haim Sak, Andrew Senior, and Fran�oise Beaufays. Long short-term mem-<br>ory based recurrent neural network architectures for large scale acoustic<br>modeling. In Interspeech 2014, number September, pages 338-342, feb<br>2014.<br>[64] Nitisch Srivastava. Improving Neural Networks with Dropout. PhD thesis,<br>University of Toronto, 2013.<br>[65] Ralf C. Staudemeyer. The importance of time: Modelling network<br>intrusions with long short-term memory recurrent neural networks. PhD<br>thesis, 2012.<br>[66] Ralf C. Staudemeyer. Applying long short-term memory recurrent neu-<br>ral networks to intrusion detection. South African Computer Journal,<br>56(1):136-154, jul 2015.<br>[67] Ilya Sutskever, James Martens, and Geoffrey E. Hinton. Generating text<br>with recurrent neural networks. In Proc. of the 28th Int. Conf. on Machine<br>Learning (ICML-11)., pages 1017-1024, 2011.<br>[68] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to Sequence<br>learning with neural networks. Advances in Neural Information Processing<br>Systems (NIPS'14), pages 3104-3112, sep 2014.<br>[69] Oriol Vinyals, Samy Bengio, and Manjunath Kudlur. Order Matters: Se-<br>quence to Sequence for sets. In Proc. of the 4th Int. Conf. on Learning<br>Representations (ICLR'17), pages 1-11, 2016.<br>[70] Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer Networks.<br>Neural Information Processing Systems 2015, pages 1-9, 2015.<br>[71] Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and<br>Geoffrey Hinton. Grammar as a foreign language. In C Cortes, N D<br>Lawrence, D D Lee, M Sugiyama, and R Garnett, editors, Advances in<br>Neural Information Processing Systems 28, pages 2773-2781. Curran As-<br>sociates, Inc., dec 2014.<br>[72] Oriol Vinyals and Quoc V. Le. A neural conversational model. arXiv, 37,<br>jun 2015.<br>[73] Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show<br>and Tell: A Neural Image Caption Generator. In Conf. on Computer Vision<br>and Pattern Recognition (CVPR'15), pages 3156-3164, jun 2015.<br>[74] Paul J. Werbos. Backpropagation through time: What it does and how to<br>do it. Proc. of the IEEE, 78(10):1550-1560, 1990.<br>[75] Ronald J. Williams and David Zipser. A learning algorithm for continually<br>running fully recurrent neural networks. Neural Computation, 1(2):270-<br>280, jun 1989.</p>",
            "id": 333,
            "page": 41,
            "text": " David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Learning internal representations by error propagation. In J L McClelland and D E Rumelhart, editors, Parallel distributed processing: Explorations in the microstructure of cognition, volume 1, pages 318-362. MIT Press, jan 1985.  Haim Sak, Andrew Senior, and Fran�oise Beaufays. Long short-term memory based recurrent neural network architectures for large scale acoustic modeling. In Interspeech 2014, number September, pages 338-342, feb 2014.  Nitisch Srivastava. Improving Neural Networks with Dropout. PhD thesis, University of Toronto, 2013.  Ralf C. Staudemeyer. The importance of time: Modelling network intrusions with long short-term memory recurrent neural networks. PhD thesis, 2012.  Ralf C. Staudemeyer. Applying long short-term memory recurrent neural networks to intrusion detection. South African Computer Journal, 56(1):136-154, jul 2015.  Ilya Sutskever, James Martens, and Geoffrey E. Hinton. Generating text with recurrent neural networks. In Proc. of the 28th Int. Conf. on Machine Learning (ICML-11)., pages 1017-1024, 2011.  Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to Sequence learning with neural networks. Advances in Neural Information Processing Systems (NIPS'14), pages 3104-3112, sep 2014.  Oriol Vinyals, Samy Bengio, and Manjunath Kudlur. Order Matters: Sequence to Sequence for sets. In Proc. of the 4th Int. Conf. on Learning Representations (ICLR'17), pages 1-11, 2016.  Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. Pointer Networks. Neural Information Processing Systems 2015, pages 1-9, 2015.  Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov, Ilya Sutskever, and Geoffrey Hinton. Grammar as a foreign language. In C Cortes, N D Lawrence, D D Lee, M Sugiyama, and R Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2773-2781. Curran Associates, Inc., dec 2014.  Oriol Vinyals and Quoc V. Le. A neural conversational model. arXiv, 37, jun 2015.  Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and Tell: A Neural Image Caption Generator. In Conf. on Computer Vision and Pattern Recognition (CVPR'15), pages 3156-3164, jun 2015.  Paul J. Werbos. Backpropagation through time: What it does and how to do it. Proc. of the IEEE, 78(10):1550-1560, 1990.  Ronald J. Williams and David Zipser. A learning algorithm for continually running fully recurrent neural networks. Neural Computation, 1(2):270280, jun 1989."
        },
        {
            "bounding_box": [
                {
                    "x": 1212,
                    "y": 3098
                },
                {
                    "x": 1257,
                    "y": 3098
                },
                {
                    "x": 1257,
                    "y": 3138
                },
                {
                    "x": 1212,
                    "y": 3138
                }
            ],
            "category": "footer",
            "html": "<footer id='334' style='font-size:14px'>41</footer>",
            "id": 334,
            "page": 41,
            "text": "41"
        },
        {
            "bounding_box": [
                {
                    "x": 513,
                    "y": 518
                },
                {
                    "x": 1965,
                    "y": 518
                },
                {
                    "x": 1965,
                    "y": 1699
                },
                {
                    "x": 513,
                    "y": 1699
                }
            ],
            "category": "paragraph",
            "html": "<p id='335' style='font-size:18px'>[76] Ronald J. Williams and David Zipser. Gradient-based learning algo-<br>rithms for recurrent networks and their computational complexity. In<br>Back-propagation: Theory, Architectures and Applications, pages 1-45.<br>L. Erlbaum Associates Inc., jan 1995.<br>[77] Martin Woellmer, Bjorn Schuller, and Gerhard Rigoll. Keyword spotting<br>exploiting Long Short-Term Memory. Speech Communication, 55(2):252-<br>265, feb 2013.<br>[78] Martin Wollmer, Florian Eyben, Stephan Reiter, Bjorn Schuller, Cate Cox,<br>Ellen Douglas-Cowie, and Roddy Cowie. Abandoning emotion classes -<br>Towards continuous emotion recognition with modelling of long-range de-<br>pendencies. In Proc. of the Ann. Conf. of the Int. Speech Communication<br>Association (INTERSPEECH'08), number January, pages 597-600, 2008.<br>[79] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville,<br>Ruslan Salakhutdinov, Richard Zemel, and Yoshua Bengio. Show, Attend<br>and Tell: Neural image caption generation with visual attention. IEEE<br>Transactions on Neural Networks, 5(2):157-166, feb 2015.<br>[80] Wojciech Zaremba and Ilya Sutskever. Learning to Execute. arXiv preprint,<br>pages 1-25, oct 2014.<br>[81] Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent Neural<br>Network Regularization. Icrl, (2013):1-8, sep 2014.</p>",
            "id": 335,
            "page": 42,
            "text": " Ronald J. Williams and David Zipser. Gradient-based learning algorithms for recurrent networks and their computational complexity. In Back-propagation: Theory, Architectures and Applications, pages 1-45. L. Erlbaum Associates Inc., jan 1995.  Martin Woellmer, Bjorn Schuller, and Gerhard Rigoll. Keyword spotting exploiting Long Short-Term Memory. Speech Communication, 55(2):252265, feb 2013.  Martin Wollmer, Florian Eyben, Stephan Reiter, Bjorn Schuller, Cate Cox, Ellen Douglas-Cowie, and Roddy Cowie. Abandoning emotion classes Towards continuous emotion recognition with modelling of long-range dependencies. In Proc. of the Ann. Conf. of the Int. Speech Communication Association (INTERSPEECH'08), number January, pages 597-600, 2008.  Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, and Yoshua Bengio. Show, Attend and Tell: Neural image caption generation with visual attention. IEEE Transactions on Neural Networks, 5(2):157-166, feb 2015.  Wojciech Zaremba and Ilya Sutskever. Learning to Execute. arXiv preprint, pages 1-25, oct 2014.  Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent Neural Network Regularization. Icrl, (2013):1-8, sep 2014."
        },
        {
            "bounding_box": [
                {
                    "x": 1212,
                    "y": 3097
                },
                {
                    "x": 1260,
                    "y": 3097
                },
                {
                    "x": 1260,
                    "y": 3138
                },
                {
                    "x": 1212,
                    "y": 3138
                }
            ],
            "category": "footer",
            "html": "<footer id='336' style='font-size:14px'>42</footer>",
            "id": 336,
            "page": 42,
            "text": "42"
        }
    ]
}