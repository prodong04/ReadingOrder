{
    "id": "32b39a40-0f94-11ef-8828-426932df3dcf",
    "pdf_path": "/root/data/pdf/2311.08355v2.pdf",
    "elements": [
        {
            "bounding_box": [
                {
                    "x": 472,
                    "y": 322
                },
                {
                    "x": 2036,
                    "y": 322
                },
                {
                    "x": 2036,
                    "y": 392
                },
                {
                    "x": 472,
                    "y": 392
                }
            ],
            "category": "paragraph",
            "html": "<p id='0' style='font-size:22px'>Mustango: Toward Controllable Text-to-Music Generation</p>",
            "id": 0,
            "page": 1,
            "text": "Mustango: Toward Controllable Text-to-Music Generation"
        },
        {
            "bounding_box": [
                {
                    "x": 617,
                    "y": 490
                },
                {
                    "x": 1884,
                    "y": 490
                },
                {
                    "x": 1884,
                    "y": 729
                },
                {
                    "x": 617,
                    "y": 729
                }
            ],
            "category": "paragraph",
            "html": "<p id='1' style='font-size:22px'>Jan Melechovsky1*, Zixun Guo2*, Deepanway Ghosal1,<br>Navonil Majumder1, Dorien Herremans1, Soujanya Poria1†<br>1 Singapore University of Technology and Design, Singapore<br>2 Queen Mary University of London, UK</p>",
            "id": 1,
            "page": 1,
            "text": "Jan Melechovsky1*, Zixun Guo2*, Deepanway Ghosal1, Navonil Majumder1, Dorien Herremans1, Soujanya Poria1† 1 Singapore University of Technology and Design, Singapore 2 Queen Mary University of London, UK"
        },
        {
            "bounding_box": [
                {
                    "x": 739,
                    "y": 756
                },
                {
                    "x": 1768,
                    "y": 756
                },
                {
                    "x": 1768,
                    "y": 864
                },
                {
                    "x": 739,
                    "y": 864
                }
            ],
            "category": "paragraph",
            "html": "<p id='2' style='font-size:16px'>O: https: / /github. com/AMAAI-Lab/mustango<br>https: / /huggingface . co/spaces/declare-lab/mustangc</p>",
            "id": 2,
            "page": 1,
            "text": "O: https: / /github. com/AMAAI-Lab/mustango https: / /huggingface . co/spaces/declare-lab/mustangc"
        },
        {
            "bounding_box": [
                {
                    "x": 652,
                    "y": 978
                },
                {
                    "x": 847,
                    "y": 978
                },
                {
                    "x": 847,
                    "y": 1028
                },
                {
                    "x": 652,
                    "y": 1028
                }
            ],
            "category": "paragraph",
            "html": "<p id='3' style='font-size:20px'>Abstract</p>",
            "id": 3,
            "page": 1,
            "text": "Abstract"
        },
        {
            "bounding_box": [
                {
                    "x": 358,
                    "y": 1063
                },
                {
                    "x": 1143,
                    "y": 1063
                },
                {
                    "x": 1143,
                    "y": 2823
                },
                {
                    "x": 358,
                    "y": 2823
                }
            ],
            "category": "paragraph",
            "html": "<p id='4' style='font-size:16px'>The quality of the text-to-music models has<br>reached new heights due to recent advance-<br>ments in diffusion models. The controllabil-<br>ity of various musical aspects, however, has<br>barely been explored. In this paper, we pro-<br>pose Mustango: a music-domain-knowledge-<br>inspired text-to-music system based on dif-<br>fusion. Mustango aims to control the gen-<br>erated music, not only with general text<br>captions, but with more rich captions that<br>can include specific instructions related to<br>chords, beats, tempo, and key. At the core<br>of Mustango is MuNet, a Music-Domain-<br>Knowledge-Informed UNet guidance module<br>that steers the generated music to include the<br>music-specific conditions, which we predict<br>from the text prompt, as well as the general<br>text embedding, during the reverse diffusion<br>process. To overcome the limited availability<br>of open datasets of music with text captions,<br>we propose a novel data augmentation method<br>that includes altering the harmonic, rhythmic,<br>and dynamic aspects of music audio and using<br>state-of-the-art Music Information Retrieval<br>methods to extract the music features which<br>will then be appended to the existing descrip-<br>tions in text format. We release the resulting<br>MusicBench dataset which contains over 52K<br>instances and includes music-theory -based de-<br>scriptions in the caption text. Through exten-<br>sive experiments, we show that the quality of<br>the music generated by Mustango is state-of-<br>the-art, and the controllability through music-<br>specific text prompts greatly outperforms other<br>models such as Musi cGen and AudioLDM2.</p>",
            "id": 4,
            "page": 1,
            "text": "The quality of the text-to-music models has reached new heights due to recent advancements in diffusion models. The controllability of various musical aspects, however, has barely been explored. In this paper, we propose Mustango: a music-domain-knowledgeinspired text-to-music system based on diffusion. Mustango aims to control the generated music, not only with general text captions, but with more rich captions that can include specific instructions related to chords, beats, tempo, and key. At the core of Mustango is MuNet, a Music-DomainKnowledge-Informed UNet guidance module that steers the generated music to include the music-specific conditions, which we predict from the text prompt, as well as the general text embedding, during the reverse diffusion process. To overcome the limited availability of open datasets of music with text captions, we propose a novel data augmentation method that includes altering the harmonic, rhythmic, and dynamic aspects of music audio and using state-of-the-art Music Information Retrieval methods to extract the music features which will then be appended to the existing descriptions in text format. We release the resulting MusicBench dataset which contains over 52K instances and includes music-theory -based descriptions in the caption text. Through extensive experiments, we show that the quality of the music generated by Mustango is state-ofthe-art, and the controllability through musicspecific text prompts greatly outperforms other models such as Musi cGen and AudioLDM2."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 2859
                },
                {
                    "x": 646,
                    "y": 2859
                },
                {
                    "x": 646,
                    "y": 2910
                },
                {
                    "x": 291,
                    "y": 2910
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='5' style='font-size:20px'>1 Introduction</p>",
            "id": 5,
            "page": 1,
            "text": "1 Introduction"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 2948
                },
                {
                    "x": 1216,
                    "y": 2948
                },
                {
                    "x": 1216,
                    "y": 3115
                },
                {
                    "x": 288,
                    "y": 3115
                }
            ],
            "category": "paragraph",
            "html": "<p id='6' style='font-size:18px'>Recently, diffusion models (Ho et al., 2020) have<br>shown prowess in image (OpenAI, 2023a), au-<br>dio (Liu et al., 2023a,b; Ghosal et al., 2023; Borsos</p>",
            "id": 6,
            "page": 1,
            "text": "Recently, diffusion models (Ho , 2020) have shown prowess in image (OpenAI, 2023a), audio (Liu , 2023a,b; Ghosal , 2023; Borsos"
        },
        {
            "bounding_box": [
                {
                    "x": 357,
                    "y": 3140
                },
                {
                    "x": 1191,
                    "y": 3140
                },
                {
                    "x": 1191,
                    "y": 3235
                },
                {
                    "x": 357,
                    "y": 3235
                }
            ],
            "category": "paragraph",
            "html": "<p id='7' style='font-size:14px'>* Co-first authors. Both authors contributed equally.<br>t Both authors contributed equally and led this project.</p>",
            "id": 7,
            "page": 1,
            "text": "* Co-first authors. Both authors contributed equally. t Both authors contributed equally and led this project."
        },
        {
            "bounding_box": [
                {
                    "x": 1266,
                    "y": 918
                },
                {
                    "x": 2198,
                    "y": 918
                },
                {
                    "x": 2198,
                    "y": 2783
                },
                {
                    "x": 1266,
                    "y": 2783
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='8' style='font-size:18px'>et al., 2023) and music (Huang et al., 2023; Schnei-<br>der et al., 2023) generation tasks. Generating mu-<br>sic directly from a diffusion model poses some<br>unique challenges. First, music adheres to spe-<br>cific rules related to, for instance, tempo, key, and<br>chord progressions. Evaluating whether or not the<br>generated music follows these conditions remains<br>challenging. For instance, MusicLM (Agostinelli<br>et al., 2023), a text-to-music model, ensures that<br>the generated music matches the text prompts in<br>terms of instrumentation and music vibe. However,<br>the musicality of the generated music (e.g., mu-<br>sically meaningful harmonies and steady tempo)<br>remains only partially addressed. Secondly, the<br>availability of paired music and textual description<br>datasets is limited (Agostinelli et al., 2023; Huang<br>et al., 2023). Although the textual descriptions in<br>the existing datasets include details like instrumen-<br>tation or vibe, more representational descriptions<br>that capture the structural, melodic, and harmonic<br>aspects of music are missing. We thus argue that<br>including this information during generation may<br>improve the current text-to-music models in terms<br>of musicality and controllability (e.g., following<br>metrical structure and chord progressions). More<br>information on related work can be found in Ap-<br>pendix E. Beyond existing text-to-music systems'<br>capability (e.g., setting correct instrumentation),<br>our proposed Mustango model enables musicians,<br>producers, and sound designers to create music<br>clips with specific text-specified conditions like fol-<br>lowing a chord progression, setting tempo, and key<br>selection.</p>",
            "id": 8,
            "page": 1,
            "text": ", 2023) and music (Huang , 2023; Schneider , 2023) generation tasks. Generating music directly from a diffusion model poses some unique challenges. First, music adheres to specific rules related to, for instance, tempo, key, and chord progressions. Evaluating whether or not the generated music follows these conditions remains challenging. For instance, MusicLM (Agostinelli , 2023), a text-to-music model, ensures that the generated music matches the text prompts in terms of instrumentation and music vibe. However, the musicality of the generated music (e.g., musically meaningful harmonies and steady tempo) remains only partially addressed. Secondly, the availability of paired music and textual description datasets is limited (Agostinelli , 2023; Huang , 2023). Although the textual descriptions in the existing datasets include details like instrumentation or vibe, more representational descriptions that capture the structural, melodic, and harmonic aspects of music are missing. We thus argue that including this information during generation may improve the current text-to-music models in terms of musicality and controllability (e.g., following metrical structure and chord progressions). More information on related work can be found in Appendix E. Beyond existing text-to-music systems' capability (e.g., setting correct instrumentation), our proposed Mustango model enables musicians, producers, and sound designers to create music clips with specific text-specified conditions like following a chord progression, setting tempo, and key selection."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 2787
                },
                {
                    "x": 2200,
                    "y": 2787
                },
                {
                    "x": 2200,
                    "y": 3236
                },
                {
                    "x": 1267,
                    "y": 3236
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='9' style='font-size:16px'>In this paper, we release the MusicBench dataset<br>which is derived from the MusicCaps (Agostinelli<br>et al., 2023) dataset and propose Mustango to ad-<br>dress these challenges. To create the MusicBench<br>dataset, we use two augmentation methods: de-<br>scription enrichment and music diversification.<br>The aim of description enrichment is to augment<br>the existing text descriptions with beats and down-</p>",
            "id": 9,
            "page": 1,
            "text": "In this paper, we release the MusicBench dataset which is derived from the MusicCaps (Agostinelli , 2023) dataset and propose Mustango to address these challenges. To create the MusicBench dataset, we use two augmentation methods: description enrichment and music diversification. The aim of description enrichment is to augment the existing text descriptions with beats and down-"
        },
        {
            "bounding_box": [
                {
                    "x": 58,
                    "y": 1054
                },
                {
                    "x": 148,
                    "y": 1054
                },
                {
                    "x": 148,
                    "y": 2570
                },
                {
                    "x": 58,
                    "y": 2570
                }
            ],
            "category": "footer",
            "html": "<br><footer id='10' style='font-size:14px'>2024<br>Mar<br>16<br>[eess.AS]<br>arXiv:2311.08355v2</footer>",
            "id": 10,
            "page": 1,
            "text": "2024 Mar 16 [eess.AS] arXiv:2311.08355v2"
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 299
                },
                {
                    "x": 1214,
                    "y": 299
                },
                {
                    "x": 1214,
                    "y": 2055
                },
                {
                    "x": 286,
                    "y": 2055
                }
            ],
            "category": "paragraph",
            "html": "<p id='11' style='font-size:20px'>beats location (inferred from tempo information<br>in the text prompt), underlying chord progression,<br>key, and tempo as control information. During<br>inference, these additional descriptive texts could<br>steer the music generation towards user-specified<br>music quality. We use state-of-the-art music in-<br>formation retrieval (MIR) methods (Mauch and<br>Dixon, 2010; Heydari et al., 2021; Bogdanov et al.,<br>2013) to extract such control information from our<br>training data. Furthermore, to diversify the mu-<br>sic samples in the training set, we augment this<br>dataset with variants of the existing music, altered<br>along three aspects-tempo, pitch, and volume-<br>that essentially determine the rhythmic, harmonic,<br>and interpretive aspects of music. The text descrip-<br>tions are also altered accordingly. The resulting<br>MusicBench dataset is 11 times the size of the orig-<br>inal MusicCaps (Agostinelli et al., 2023) dataset.<br>Our proposed controllable text-to-music model<br>Mustango incorporates a novel MuNet (music-<br>domain-knowledge-informed UNet) that can in-<br>still the input chords, beats, key, and tempo, along<br>with the textual description, in the generated mu-<br>sic during the reverse-diffusion process. The re-<br>sults in §4 indicate Mustango creates more mu-<br>sically meaningful output and shows improved<br>controllability (e.g., changing chords) over the<br>existing text-to-music models. Our MusicBench<br>dataset, Mustango implementation, and compar-<br>ative music samples are available through https:<br>/ /github.com/AMAAI-Lab/mustango.</p>",
            "id": 11,
            "page": 2,
            "text": "beats location (inferred from tempo information in the text prompt), underlying chord progression, key, and tempo as control information. During inference, these additional descriptive texts could steer the music generation towards user-specified music quality. We use state-of-the-art music information retrieval (MIR) methods (Mauch and Dixon, 2010; Heydari , 2021; Bogdanov , 2013) to extract such control information from our training data. Furthermore, to diversify the music samples in the training set, we augment this dataset with variants of the existing music, altered along three aspects-tempo, pitch, and volumethat essentially determine the rhythmic, harmonic, and interpretive aspects of music. The text descriptions are also altered accordingly. The resulting MusicBench dataset is 11 times the size of the original MusicCaps (Agostinelli , 2023) dataset. Our proposed controllable text-to-music model Mustango incorporates a novel MuNet (musicdomain-knowledge-informed UNet) that can instill the input chords, beats, key, and tempo, along with the textual description, in the generated music during the reverse-diffusion process. The results in §4 indicate Mustango creates more musically meaningful output and shows improved controllability (e.g., changing chords) over the existing text-to-music models. Our MusicBench dataset, Mustango implementation, and comparative music samples are available through https: / /github.com/AMAAI-Lab/mustango."
        },
        {
            "bounding_box": [
                {
                    "x": 329,
                    "y": 2057
                },
                {
                    "x": 1121,
                    "y": 2057
                },
                {
                    "x": 1121,
                    "y": 2107
                },
                {
                    "x": 329,
                    "y": 2107
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='12' style='font-size:16px'>The overall contributions of this paper are:</p>",
            "id": 12,
            "page": 2,
            "text": "The overall contributions of this paper are:"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 2115
                },
                {
                    "x": 1214,
                    "y": 2115
                },
                {
                    "x": 1214,
                    "y": 2911
                },
                {
                    "x": 287,
                    "y": 2911
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='13' style='font-size:20px'>(i) We propose Mustango, a text-to-music diffu-<br>sion model with our novel MuNet module to ex-<br>plicitly guide the music generation towards input<br>tempo, key, chords, and general textual description.<br>(ii) We release the Musi cBench dataset with ~53K<br>pairs of music audio and description with informa-<br>tion on musical attributes like chords, key, and<br>beats. This is achieved by altering the music sam-<br>ples of MusicCaps along the harmony, tempo, and<br>volume dimensions and enriching the captions with<br>the aforementioned musically-relevant attributes.<br>(iii) We empirically verify that our Mustango<br>model is able to generate high quality music faith-<br>ful to the input text descriptions, chords, and beats.</p>",
            "id": 13,
            "page": 2,
            "text": "(i) We propose Mustango, a text-to-music diffusion model with our novel MuNet module to explicitly guide the music generation towards input tempo, key, chords, and general textual description. (ii) We release the Musi cBench dataset with ~53K pairs of music audio and description with information on musical attributes like chords, key, and beats. This is achieved by altering the music samples of MusicCaps along the harmony, tempo, and volume dimensions and enriching the captions with the aforementioned musically-relevant attributes. (iii) We empirically verify that our Mustango model is able to generate high quality music faithful to the input text descriptions, chords, and beats."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 2964
                },
                {
                    "x": 735,
                    "y": 2964
                },
                {
                    "x": 735,
                    "y": 3020
                },
                {
                    "x": 288,
                    "y": 3020
                }
            ],
            "category": "paragraph",
            "html": "<p id='14' style='font-size:22px'>2 Dataset Creation</p>",
            "id": 14,
            "page": 2,
            "text": "2 Dataset Creation"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 3067
                },
                {
                    "x": 1217,
                    "y": 3067
                },
                {
                    "x": 1217,
                    "y": 3236
                },
                {
                    "x": 288,
                    "y": 3236
                }
            ],
            "category": "paragraph",
            "html": "<p id='15' style='font-size:18px'>In this section, the methods of music feature extrac-<br>tion and data augmentation are introduced. Then,<br>the application of these methods and the details of</p>",
            "id": 15,
            "page": 2,
            "text": "In this section, the methods of music feature extraction and data augmentation are introduced. Then, the application of these methods and the details of"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 304
                },
                {
                    "x": 1736,
                    "y": 304
                },
                {
                    "x": 1736,
                    "y": 351
                },
                {
                    "x": 1268,
                    "y": 351
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='16' style='font-size:14px'>our dataset are discussed.</p>",
            "id": 16,
            "page": 2,
            "text": "our dataset are discussed."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 407
                },
                {
                    "x": 2072,
                    "y": 407
                },
                {
                    "x": 2072,
                    "y": 513
                },
                {
                    "x": 1269,
                    "y": 513
                }
            ],
            "category": "paragraph",
            "html": "<p id='17' style='font-size:20px'>2.1 Feature Extraction and Description<br>Enrichment</p>",
            "id": 17,
            "page": 2,
            "text": "2.1 Feature Extraction and Description Enrichment"
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 538
                },
                {
                    "x": 2197,
                    "y": 538
                },
                {
                    "x": 2197,
                    "y": 1835
                },
                {
                    "x": 1267,
                    "y": 1835
                }
            ],
            "category": "paragraph",
            "html": "<p id='18' style='font-size:18px'>We extract four common music features: beats and<br>downbeats, chords, keys, and tempo, and use them<br>to enhance the text prompts and guide music gener-<br>ation. We use BeatNet (Heydari et al., 2021) to ex-<br>tract the beat and downbeat features, 6 E R Lbeats x2<br>,<br>where the first dimension represents the type of<br>beat according to the meter (e.g., 1, 2, 3) and the<br>second represents the timing of each correspond-<br>ing beat in seconds. The second feature tempo,<br>measured in beats per minute (BPM), is estimated<br>by averaging the reciprocal of the time interval be-<br>tween beats. Chordino (Mauch and Dixon, 2010) is<br>used to extract the chord features, c E RLchords x3<br>,<br>where the first dimension represents the roots of the<br>chord sequence, the second represents the chord<br>type (e.g., major, minor, maj7, etc.), and the third<br>represents whether the chords are inverted. Finally,<br>Essentia's (Bogdanov et al., 2013) KeyExtractor<br>algorithm 1 is used to extract the key. The extracted<br>features are used to enrich the textual descriptions<br>and guide the reverse diffusion process. We notice<br>a similar data enrichment approach in concurrent<br>research (Gardner et al., 2023).</p>",
            "id": 18,
            "page": 2,
            "text": "We extract four common music features: beats and downbeats, chords, keys, and tempo, and use them to enhance the text prompts and guide music generation. We use BeatNet (Heydari , 2021) to extract the beat and downbeat features, 6 E R Lbeats x2 , where the first dimension represents the type of beat according to the meter (e.g., 1, 2, 3) and the second represents the timing of each corresponding beat in seconds. The second feature tempo, measured in beats per minute (BPM), is estimated by averaging the reciprocal of the time interval between beats. Chordino (Mauch and Dixon, 2010) is used to extract the chord features, c E RLchords x3 , where the first dimension represents the roots of the chord sequence, the second represents the chord type (e.g., major, minor, maj7, etc.), and the third represents whether the chords are inverted. Finally, Essentia's (Bogdanov , 2013) KeyExtractor algorithm 1 is used to extract the key. The extracted features are used to enrich the textual descriptions and guide the reverse diffusion process. We notice a similar data enrichment approach in concurrent research (Gardner , 2023)."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 1845
                },
                {
                    "x": 2194,
                    "y": 1845
                },
                {
                    "x": 2194,
                    "y": 2348
                },
                {
                    "x": 1267,
                    "y": 2348
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='19' style='font-size:18px'>These features are then expressed in text format<br>following several text templates (e.g., 'The song<br>is in the key of A minor. The tempo of this song<br>is Adagio. The beat counts to 4. The chord pro-<br>gression is Am, Cmaj7, G.'). We refer to these<br>as control sentences and they will be appended<br>to the original text prompt to form the enhanced<br>prompts. A full list of the different control sentence<br>templates can be found in Appendix I (Table 5).</p>",
            "id": 19,
            "page": 2,
            "text": "These features are then expressed in text format following several text templates (e.g., 'The song is in the key of A minor. The tempo of this song is Adagio. The beat counts to 4. The chord progression is Am, Cmaj7, G.'). We refer to these as control sentences and they will be appended to the original text prompt to form the enhanced prompts. A full list of the different control sentence templates can be found in Appendix I (Table 5)."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 2399
                },
                {
                    "x": 2166,
                    "y": 2399
                },
                {
                    "x": 2166,
                    "y": 2450
                },
                {
                    "x": 1269,
                    "y": 2450
                }
            ],
            "category": "paragraph",
            "html": "<p id='20' style='font-size:20px'>2.2 Augmentation and Music Diversification</p>",
            "id": 20,
            "page": 2,
            "text": "2.2 Augmentation and Music Diversification"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 2475
                },
                {
                    "x": 2197,
                    "y": 2475
                },
                {
                    "x": 2197,
                    "y": 3148
                },
                {
                    "x": 1268,
                    "y": 3148
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='21' style='font-size:18px'>Our dataset augmentation for both music audio and<br>text prompts increases the total amount of training<br>data 11-fold to improve both audio quality and con-<br>trollability of our model. Standard text-to-audio<br>augmentations may not suit the nature of music<br>audio. For example, the augmentation method used<br>for Tango (Ghosal et al., 2023), whereby two audio<br>samples normalized to similar audio levels are su-<br>perimposed and their prompts concatenated, would<br>not work for music due to overlapping rhythms, dis-<br>sonance in harmony, and overall musical concept<br>mismatch.</p>",
            "id": 21,
            "page": 2,
            "text": "Our dataset augmentation for both music audio and text prompts increases the total amount of training data 11-fold to improve both audio quality and controllability of our model. Standard text-to-audio augmentations may not suit the nature of music audio. For example, the augmentation method used for Tango (Ghosal , 2023), whereby two audio samples normalized to similar audio levels are superimposed and their prompts concatenated, would not work for music due to overlapping rhythms, dissonance in harmony, and overall musical concept mismatch."
        },
        {
            "bounding_box": [
                {
                    "x": 1324,
                    "y": 3189
                },
                {
                    "x": 2093,
                    "y": 3189
                },
                {
                    "x": 2093,
                    "y": 3230
                },
                {
                    "x": 1324,
                    "y": 3230
                }
            ],
            "category": "footer",
            "html": "<footer id='22' style='font-size:14px'>1<br>essentia.upf. edu/reference/std_KeyExtractor html</footer>",
            "id": 22,
            "page": 2,
            "text": "1 essentia.upf. edu/reference/std_KeyExtractor html"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 299
                },
                {
                    "x": 1215,
                    "y": 299
                },
                {
                    "x": 1215,
                    "y": 1259
                },
                {
                    "x": 287,
                    "y": 1259
                }
            ],
            "category": "paragraph",
            "html": "<p id='23' style='font-size:20px'>Therefore, we alter individual music samples<br>along one of the three dimensions- pitch, speed,<br>and volume-which determine the melodic, rhyth-<br>mic, and dynamic aspects of music. We use<br>PyRubberband2 to shift the pitch of the music au-<br>dio within a range of 士3 semitones following a<br>uniform distribution. We decided to use this range<br>in order to keep the timbre of instruments relatively<br>untouched, as larger pitch shifts could result in un-<br>natural timbre. We change the speed of the music<br>audio by ±(5 to 25)%, drawn from a uniform dis-<br>tribution as well. Finally, we alter the volume of<br>the audio by introducing a gradual volume change<br>(both crescendo and decrescendo) with the mini-<br>mum volume drawn from a uniform distribution<br>from 0.1 to 0.5 times the original track's amplitude,<br>while the maximum volume is kept untouched.</p>",
            "id": 23,
            "page": 3,
            "text": "Therefore, we alter individual music samples along one of the three dimensions- pitch, speed, and volume-which determine the melodic, rhythmic, and dynamic aspects of music. We use PyRubberband2 to shift the pitch of the music audio within a range of 士3 semitones following a uniform distribution. We decided to use this range in order to keep the timbre of instruments relatively untouched, as larger pitch shifts could result in unnatural timbre. We change the speed of the music audio by ±(5 to 25)%, drawn from a uniform distribution as well. Finally, we alter the volume of the audio by introducing a gradual volume change (both crescendo and decrescendo) with the minimum volume drawn from a uniform distribution from 0.1 to 0.5 times the original track's amplitude, while the maximum volume is kept untouched."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 1264
                },
                {
                    "x": 1213,
                    "y": 1264
                },
                {
                    "x": 1213,
                    "y": 1770
                },
                {
                    "x": 288,
                    "y": 1770
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='24' style='font-size:18px'>The text descriptions are enhanced and modi-<br>fied in tandem with the alterations to the music<br>audio. However, to enhance the robustness of the<br>model, we randomly discard one to four sentences<br>from the prompt that describe the aforementioned<br>music features. More details are illustrated in the<br>Appendix. Finally, we used ChatGPT (OpenAI,<br>2023b) to rephrase the text prompts to add variety<br>to the text prompts.</p>",
            "id": 24,
            "page": 3,
            "text": "The text descriptions are enhanced and modified in tandem with the alterations to the music audio. However, to enhance the robustness of the model, we randomly discard one to four sentences from the prompt that describe the aforementioned music features. More details are illustrated in the Appendix. Finally, we used ChatGPT (OpenAI, 2023b) to rephrase the text prompts to add variety to the text prompts."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 1820
                },
                {
                    "x": 641,
                    "y": 1820
                },
                {
                    "x": 641,
                    "y": 1869
                },
                {
                    "x": 290,
                    "y": 1869
                }
            ],
            "category": "paragraph",
            "html": "<p id='25' style='font-size:20px'>2.3 MusicBench</p>",
            "id": 25,
            "page": 3,
            "text": "2.3 MusicBench"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 1896
                },
                {
                    "x": 1214,
                    "y": 1896
                },
                {
                    "x": 1214,
                    "y": 2458
                },
                {
                    "x": 287,
                    "y": 2458
                }
            ],
            "category": "paragraph",
            "html": "<p id='26' style='font-size:22px'>In this study, we make use of the Music-<br>Caps (Agostinelli et al., 2023) dataset, which com-<br>prises a collection of 5,521 audio clips featuring<br>music. Each clip is 10 seconds long and is sourced<br>from the train and evaluation splits of the Au-<br>dioSet (Gemmeke et al., 2017) dataset. These audio<br>clips are accompanied by on average four-sentence-<br>long English caption that describe the music. How-<br>ever, due to the inaccessibility of some audio files,<br>our dataset comes from 5,479 samples.</p>",
            "id": 26,
            "page": 3,
            "text": "In this study, we make use of the MusicCaps (Agostinelli , 2023) dataset, which comprises a collection of 5,521 audio clips featuring music. Each clip is 10 seconds long and is sourced from the train and evaluation splits of the AudioSet (Gemmeke , 2017) dataset. These audio clips are accompanied by on average four-sentencelong English caption that describe the music. However, due to the inaccessibility of some audio files, our dataset comes from 5,479 samples."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 2468
                },
                {
                    "x": 1214,
                    "y": 2468
                },
                {
                    "x": 1214,
                    "y": 2912
                },
                {
                    "x": 287,
                    "y": 2912
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='27' style='font-size:20px'>We split our dataset as shown in Fig. 1. First,<br>we split the data into TrainA and TestA sets. Sub-<br>sequently, four control sentences corresponding<br>to the music features are spliced with the original<br>prompts to obtain the TrainB and TestB sets from<br>TrainA and TestA, respectively. Then, by instruct-<br>ing ChatGPT to rephrase the TrainB text prompts,<br>we get the final TrainC set.</p>",
            "id": 27,
            "page": 3,
            "text": "We split our dataset as shown in Fig. 1. First, we split the data into TrainA and TestA sets. Subsequently, four control sentences corresponding to the music features are spliced with the original prompts to obtain the TrainB and TestB sets from TrainA and TestA, respectively. Then, by instructing ChatGPT to rephrase the TrainB text prompts, we get the final TrainC set."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 2920
                },
                {
                    "x": 1216,
                    "y": 2920
                },
                {
                    "x": 1216,
                    "y": 3146
                },
                {
                    "x": 287,
                    "y": 3146
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='28' style='font-size:22px'>In addition, before performing audio augmen-<br>tation, we filter out 'low quality' samples by re-<br>moving samples that mention the terms 'quality'<br>(as it is typically related to poor quality) or 'low</p>",
            "id": 28,
            "page": 3,
            "text": "In addition, before performing audio augmentation, we filter out 'low quality' samples by removing samples that mention the terms 'quality' (as it is typically related to poor quality) or 'low"
        },
        {
            "bounding_box": [
                {
                    "x": 344,
                    "y": 3183
                },
                {
                    "x": 839,
                    "y": 3183
                },
                {
                    "x": 839,
                    "y": 3235
                },
                {
                    "x": 344,
                    "y": 3235
                }
            ],
            "category": "paragraph",
            "html": "<p id='29' style='font-size:16px'>2<br>github.com/bmcfee/pyrubberband</p>",
            "id": 29,
            "page": 3,
            "text": "2 github.com/bmcfee/pyrubberband"
        },
        {
            "bounding_box": [
                {
                    "x": 1311,
                    "y": 279
                },
                {
                    "x": 2150,
                    "y": 279
                },
                {
                    "x": 2150,
                    "y": 1138
                },
                {
                    "x": 1311,
                    "y": 1138
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='30' style='font-size:14px' alt=\"MusicCaps Filtered\n5479 instances 3413 instances\nFilter out\nAudio augmentation\n'low quality'\nTestA TrainA\n400 instances 5079 instances Audio augmented\n37k instances\nExtract features, add all control prompts Extract features\nAdd control prompts\nselect 0/1/2/3/4 sentences\nwith 25/30/20/15/10% probability\nTestB TrainB\n400 instances 5079 instances\nControls added\n37k instances\nRephrase captions with ChatGPT\nTrainC ChatGPT rephrased\n5079 instances 37k instances\nRandomly pick original/chatgpt caption\nwith probability 15/85 %\n厂\nFinal train set\n53k instances\" data-coord=\"top-left:(1311,279); bottom-right:(2150,1138)\" /></figure>",
            "id": 30,
            "page": 3,
            "text": "MusicCaps Filtered 5479 instances 3413 instances Filter out Audio augmentation \"low quality\" TestA TrainA 400 instances 5079 instances Audio augmented 37k instances Extract features, add all control prompts Extract features Add control prompts select 0/1/2/3/4 sentences with 25/30/20/15/10% probability TestB TrainB 400 instances 5079 instances Controls added 37k instances Rephrase captions with ChatGPT TrainC ChatGPT rephrased 5079 instances 37k instances Randomly pick original/chatgpt caption with probability 15/85 % 厂 Final train set 53k instances"
        },
        {
            "bounding_box": [
                {
                    "x": 1333,
                    "y": 1168
                },
                {
                    "x": 2121,
                    "y": 1168
                },
                {
                    "x": 2121,
                    "y": 1218
                },
                {
                    "x": 1333,
                    "y": 1218
                }
            ],
            "category": "caption",
            "html": "<br><caption id='31' style='font-size:20px'>Figure 1: Composition of MusicBench dataset.</caption>",
            "id": 31,
            "page": 3,
            "text": "Figure 1: Composition of MusicBench dataset."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 1315
                },
                {
                    "x": 2198,
                    "y": 1315
                },
                {
                    "x": 2198,
                    "y": 3235
                },
                {
                    "x": 1267,
                    "y": 3235
                }
            ],
            "category": "paragraph",
            "html": "<p id='32' style='font-size:20px'>fidelity' in the captions of TrainA set, to get 3,413<br>instances. The higher quality samples are altered<br>(see §2.2) to form a set of 37k augmentation sam-<br>ples, comprising 6 pitch-shifted, 4 tempo-altered<br>and 1 volume-altered sample per original sample.<br>In the case of pitch-shifted samples, instead of ran-<br>domly sampling from a uniform distribution, we<br>used all 6 unique semitone shifts (from -3 to +3, ex-<br>cluding 0). Thereafter we randomly select control<br>prompts to concatenate with the original captions.<br>We pick 0/1/2/3/4 prompts with a probability of<br>25 /30/20/15/10%, respectively. We do this to in-<br>crease the robustness of the model, as the model<br>should be able to take inputs both with and with-<br>out control sentences specifying the four music<br>features. Then, to further increase text input robust-<br>ness, we rephrase all of the captions using ChatGPT<br>(see Appendix H). We find this step a necessary<br>addition in our augmentation pipeline as the au-<br>dio augmentation produces 11 similar samples that<br>share a big portion of their caption with the original<br>MusicCaps caption. By paraphrasing, we create<br>more unique instances. In our final training dataset,<br>we use both of the rephrased and non-rephrased<br>prompts with a probability of 85 / 15%, respectively.<br>Finally, we take this augmented set and concate-<br>nate it with sets TrainA, TrainB, and TrainC to get<br>our final training set consisting of 52,768 samples,<br>hereafter referred to as MusicBench. We note that<br>TestA and TestB sets consist of 200 'low quality'<br>(as explained above), and 200 'high quality' sam-<br>ples. This means that the test set distribution is<br>slightly different from that of train set. Our inten-<br>tion was to create a difficult evaluation set to test</p>",
            "id": 32,
            "page": 3,
            "text": "fidelity' in the captions of TrainA set, to get 3,413 instances. The higher quality samples are altered (see §2.2) to form a set of 37k augmentation samples, comprising 6 pitch-shifted, 4 tempo-altered and 1 volume-altered sample per original sample. In the case of pitch-shifted samples, instead of randomly sampling from a uniform distribution, we used all 6 unique semitone shifts (from -3 to +3, excluding 0). Thereafter we randomly select control prompts to concatenate with the original captions. We pick 0/1/2/3/4 prompts with a probability of 25 /30/20/15/10%, respectively. We do this to increase the robustness of the model, as the model should be able to take inputs both with and without control sentences specifying the four music features. Then, to further increase text input robustness, we rephrase all of the captions using ChatGPT (see Appendix H). We find this step a necessary addition in our augmentation pipeline as the audio augmentation produces 11 similar samples that share a big portion of their caption with the original MusicCaps caption. By paraphrasing, we create more unique instances. In our final training dataset, we use both of the rephrased and non-rephrased prompts with a probability of 85 / 15%, respectively. Finally, we take this augmented set and concatenate it with sets TrainA, TrainB, and TrainC to get our final training set consisting of 52,768 samples, hereafter referred to as MusicBench. We note that TestA and TestB sets consist of 200 'low quality' (as explained above), and 200 'high quality' samples. This means that the test set distribution is slightly different from that of train set. Our intention was to create a difficult evaluation set to test"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 305
                },
                {
                    "x": 1214,
                    "y": 305
                },
                {
                    "x": 1214,
                    "y": 407
                },
                {
                    "x": 288,
                    "y": 407
                }
            ],
            "category": "paragraph",
            "html": "<p id='33' style='font-size:16px'>the controllability of Mustango in tougher condi-<br>tions.</p>",
            "id": 33,
            "page": 4,
            "text": "the controllability of Mustango in tougher conditions."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 483
                },
                {
                    "x": 325,
                    "y": 483
                },
                {
                    "x": 325,
                    "y": 526
                },
                {
                    "x": 290,
                    "y": 526
                }
            ],
            "category": "paragraph",
            "html": "<p id='34' style='font-size:16px'>3</p>",
            "id": 34,
            "page": 4,
            "text": "3"
        },
        {
            "bounding_box": [
                {
                    "x": 489,
                    "y": 480
                },
                {
                    "x": 699,
                    "y": 480
                },
                {
                    "x": 699,
                    "y": 533
                },
                {
                    "x": 489,
                    "y": 533
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='35' style='font-size:22px'>Mustango</p>",
            "id": 35,
            "page": 4,
            "text": "Mustango"
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 595
                },
                {
                    "x": 1208,
                    "y": 595
                },
                {
                    "x": 1208,
                    "y": 697
                },
                {
                    "x": 289,
                    "y": 697
                }
            ],
            "category": "paragraph",
            "html": "<p id='36' style='font-size:16px'>Mustango consists of two components: 1) Latent<br>Diffusion Model; 2) MuNet.</p>",
            "id": 36,
            "page": 4,
            "text": "Mustango consists of two components: 1) Latent Diffusion Model; 2) MuNet."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 746
                },
                {
                    "x": 1003,
                    "y": 746
                },
                {
                    "x": 1003,
                    "y": 797
                },
                {
                    "x": 289,
                    "y": 797
                }
            ],
            "category": "paragraph",
            "html": "<p id='37' style='font-size:16px'>3.1 Latent Diffusion Model (LDM)</p>",
            "id": 37,
            "page": 4,
            "text": "3.1 Latent Diffusion Model (LDM)"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 822
                },
                {
                    "x": 1212,
                    "y": 822
                },
                {
                    "x": 1212,
                    "y": 1439
                },
                {
                    "x": 287,
                    "y": 1439
                }
            ],
            "category": "paragraph",
            "html": "<p id='38' style='font-size:14px'>Inspired by Tango (Ghosal et al., 2023) and<br>AudioLDM (Liu et al., 2023a), we leverage the latent<br>diffusion model (LDM) to reduce computational<br>complexity meanwhile maintaining the expressive-<br>ness of the diffusion model. More specifically, we<br>aim to construct the latent audio prior zo extracted<br>using an extra variational autoencoder (VAE) with<br>condition C , which in our case refers to a joint<br>music and text condition. Similar to Tango, we<br>leverage the pre-trained VAE from AudioLDM to<br>obtain the latent code of the audio.</p>",
            "id": 38,
            "page": 4,
            "text": "Inspired by Tango (Ghosal , 2023) and AudioLDM (Liu , 2023a), we leverage the latent diffusion model (LDM) to reduce computational complexity meanwhile maintaining the expressiveness of the diffusion model. More specifically, we aim to construct the latent audio prior zo extracted using an extra variational autoencoder (VAE) with condition C , which in our case refers to a joint music and text condition. Similar to Tango, we leverage the pre-trained VAE from AudioLDM to obtain the latent code of the audio."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 1446
                },
                {
                    "x": 1213,
                    "y": 1446
                },
                {
                    "x": 1213,
                    "y": 1779
                },
                {
                    "x": 288,
                    "y": 1779
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='39' style='font-size:14px'>Through the forward-diffusion process (Marko-<br>vian Hierarchical VAE), the latent audio prior zo<br>turns into a standard gaussian noise ZN ~ N(0, I),<br>as shown in Eq. (1) where a pre-scheduled gaussian<br>noise (0 < B1 < B2 < · · · < BN < 1) is gradually<br>added at each forward step:</p>",
            "id": 39,
            "page": 4,
            "text": "Through the forward-diffusion process (Markovian Hierarchical VAE), the latent audio prior zo turns into a standard gaussian noise ZN ~ N(0, I), as shown in Eq. (1) where a pre-scheduled gaussian noise (0 < B1 < B2 < · · · < BN < 1) is gradually added at each forward step:"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 1942
                },
                {
                    "x": 1214,
                    "y": 1942
                },
                {
                    "x": 1214,
                    "y": 2507
                },
                {
                    "x": 287,
                    "y": 2507
                }
            ],
            "category": "paragraph",
            "html": "<p id='40' style='font-size:16px'>For the reverse process, which reconstructs 20<br>from Gaussian noise ZN ~ N(0,I), we propose<br>MuNet (see §3.2), which is able to steer the gen-<br>erated music towards the given condition C. Intu-<br>itively, backward diffusion aims to iteratively re-<br>construct the latent audio prior Zn-1 from the pre-<br>vious step Zn until zo, using a denoiser �(�) (zn, C).<br>This denoiser is driven by classifier-free guidance,<br>similar to Tango. The reverse diffusion process is<br>outlined in Appendix A.</p>",
            "id": 40,
            "page": 4,
            "text": "For the reverse process, which reconstructs 20 from Gaussian noise ZN ~ N(0,I), we propose MuNet (see §3.2), which is able to steer the generated music towards the given condition C. Intuitively, backward diffusion aims to iteratively reconstruct the latent audio prior Zn-1 from the previous step Zn until zo, using a denoiser �(�) (zn, C). This denoiser is driven by classifier-free guidance, similar to Tango. The reverse diffusion process is outlined in Appendix A."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 2516
                },
                {
                    "x": 1214,
                    "y": 2516
                },
                {
                    "x": 1214,
                    "y": 2686
                },
                {
                    "x": 287,
                    "y": 2686
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='41' style='font-size:16px'>This reconstruction is trained using a noise-<br>estimation loss where �(�) is the estimated noise<br>and Yn is the weight of reverse step n:</p>",
            "id": 41,
            "page": 4,
            "text": "This reconstruction is trained using a noiseestimation loss where �(�) is the estimated noise and Yn is the weight of reverse step n:"
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 2880
                },
                {
                    "x": 514,
                    "y": 2880
                },
                {
                    "x": 514,
                    "y": 2929
                },
                {
                    "x": 289,
                    "y": 2929
                }
            ],
            "category": "paragraph",
            "html": "<p id='42' style='font-size:14px'>3.2 MuNet</p>",
            "id": 42,
            "page": 4,
            "text": "3.2 MuNet"
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 2955
                },
                {
                    "x": 1213,
                    "y": 2955
                },
                {
                    "x": 1213,
                    "y": 3233
                },
                {
                    "x": 289,
                    "y": 3233
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='43' style='font-size:16px'>The reverse-diffusion process, briefly described in<br>§3.1, is conditioned on both musical attributes (beat<br>6 and chord c) and text T (C := {T, 6, 다). This is<br>realized through the Music-Domain-Knowledge-<br>Informed UNet (MuNet) denoiser as follows:</p>",
            "id": 43,
            "page": 4,
            "text": "The reverse-diffusion process, briefly described in §3.1, is conditioned on both musical attributes (beat 6 and chord c) and text T (C := {T, 6, 다). This is realized through the Music-Domain-KnowledgeInformed UNet (MuNet) denoiser as follows:"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 688
                },
                {
                    "x": 2195,
                    "y": 688
                },
                {
                    "x": 2195,
                    "y": 1247
                },
                {
                    "x": 1268,
                    "y": 1247
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='44' style='font-size:14px'>where, MHA is the multi-headed attention<br>block (Vaswani et al., 2017) for the cross atten-<br>tions, where Q, K, and V are query, key, and value,<br>respectively, and FLAN-T5 is the text encoder<br>model (Chung et al., 2022), adopted from Tango.<br>We prioritize applying cross-attention to the beat<br>first, as we consider a consistent rhythm to be the<br>fundamental basis for the generated music. Sub-<br>sequently, we can focus on conditioning based on<br>chords.</p>",
            "id": 44,
            "page": 4,
            "text": "where, MHA is the multi-headed attention block (Vaswani , 2017) for the cross attentions, where Q, K, and V are query, key, and value, respectively, and FLAN-T5 is the text encoder model (Chung , 2022), adopted from Tango. We prioritize applying cross-attention to the beat first, as we consider a consistent rhythm to be the fundamental basis for the generated music. Subsequently, we can focus on conditioning based on chords."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 1264
                },
                {
                    "x": 2197,
                    "y": 1264
                },
                {
                    "x": 2197,
                    "y": 1996
                },
                {
                    "x": 1267,
                    "y": 1996
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='45' style='font-size:16px'>MuNet consists of a UNet (Ronneberger et al.,<br>2015) -consisting of multiple downsampling, mid-<br>dle, and upsampling blocks-and multiple con-<br>ditioning cross-attention blocks. We use two en-<br>coders, Encb and Encc, to encode the beat and<br>chord features which leverage both the state-of-<br>the-art Fundamental Music Embedding (FME) as<br>well as an onset-and-beat-based positional encod-<br>ing (Guo et al., 2023) which we name Music Posi-<br>tional Encoding (MPE). These ensure the musical<br>features are properly captured and several funda-<br>mental music properties (e.g., intervals between<br>pitches are translational invariant) are preserved.</p>",
            "id": 45,
            "page": 4,
            "text": "MuNet consists of a UNet (Ronneberger , 2015) -consisting of multiple downsampling, middle, and upsampling blocks-and multiple conditioning cross-attention blocks. We use two encoders, Encb and Encc, to encode the beat and chord features which leverage both the state-ofthe-art Fundamental Music Embedding (FME) as well as an onset-and-beat-based positional encoding (Guo , 2023) which we name Music Positional Encoding (MPE). These ensure the musical features are properly captured and several fundamental music properties (e.g., intervals between pitches are translational invariant) are preserved."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 2009
                },
                {
                    "x": 2193,
                    "y": 2009
                },
                {
                    "x": 2193,
                    "y": 2514
                },
                {
                    "x": 1267,
                    "y": 2514
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='46' style='font-size:16px'>We introduce the two encoders Encb and Encc<br>that extract the beat and chord embeddings from<br>the raw input. The beat encoder Encb, defined in<br>Eq. (3), encodes the beat types 6[:, 0] (§2.1) using<br>One-Hot Encoding (OHb) and the beat timings<br>6[:, 1] with Music Positional Embedding. By con-<br>catenating these beat types and timing encodings<br>and passing them through a trainable linear layer<br>(Wb), we obtain the final beat features:</p>",
            "id": 46,
            "page": 4,
            "text": "We introduce the two encoders Encb and Encc that extract the beat and chord embeddings from the raw input. The beat encoder Encb, defined in Eq. (3), encodes the beat types 6[:, 0] (§2.1) using One-Hot Encoding (OHb) and the beat timings 6[:, 1] with Music Positional Embedding. By concatenating these beat types and timing encodings and passing them through a trainable linear layer (Wb), we obtain the final beat features:"
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 2898
                },
                {
                    "x": 2196,
                    "y": 2898
                },
                {
                    "x": 2196,
                    "y": 3234
                },
                {
                    "x": 1267,
                    "y": 3234
                }
            ],
            "category": "paragraph",
            "html": "<p id='47' style='font-size:18px'>In the chord encoder in Eq. (4), we obtain the<br>chord embeddings by first concatenating i) FME-<br>embedded (Guo et al., 2023) chord roots c[:, 0]<br>(see §2.1); ii) One-Hot encoded chord type (c[:, 1]);<br>iii) One-Hot encoded chord inversions (c[:, 1]);<br>and iv) MPE-embedded (Guo et al., 2023) tim-</p>",
            "id": 47,
            "page": 4,
            "text": "In the chord encoder in Eq. (4), we obtain the chord embeddings by first concatenating i) FMEembedded (Guo , 2023) chord roots c[:, 0] (see §2.1); ii) One-Hot encoded chord type (c[:, 1]); iii) One-Hot encoded chord inversions (c[:, 1]); and iv) MPE-embedded (Guo , 2023) tim-"
        },
        {
            "bounding_box": [
                {
                    "x": 475,
                    "y": 304
                },
                {
                    "x": 2001,
                    "y": 304
                },
                {
                    "x": 2001,
                    "y": 1127
                },
                {
                    "x": 475,
                    "y": 1127
                }
            ],
            "category": "figure",
            "html": "<figure><img id='48' style='font-size:14px' alt=\"Audio\n~ Encoder\nZO Z1 Z.2 ZN-1\nForward Process\nVAE Diffusion Model ZN N (⌀, I)\nReverse Process\nAudio\n20 21 순2 �N-1 E\nDecoder\nMuNet MuNet MuNet\nHiFi Chord Predictor\nm\nGAN\nDec. Enc.\nT5 T5\n21-1\nChord Embedding D, Bm, G, A7\nQ Enc°(c)\n(1) Q Q\nN(��,B� UNet ← MHA MHA MHA Beats and\nBeat Embedding downbeats locations\nFLAN-T5(T) Encb(b)\nK,V K,V K,V\nSoothing techno music with 120 DeBERTa\nMuNet Enc°(c) Encb(b) FLAN-T5(T) FLAN-T5 bpm and in the key of D major. Beat\nFollow this chord progression: D, Predictor\nBm, G, A7.\nLegend:\nInference,\nInference only Train only Train + Inference when required Frozen Params. Trainable Params.\" data-coord=\"top-left:(475,304); bottom-right:(2001,1127)\" /></figure>",
            "id": 48,
            "page": 5,
            "text": "Audio ~ Encoder ZO Z1 Z.2 ZN-1 Forward Process VAE Diffusion Model ZN N (⌀, I) Reverse Process Audio 20 21 순2 �N-1 E Decoder MuNet MuNet MuNet HiFi Chord Predictor m GAN Dec. Enc. T5 T5 21-1 Chord Embedding D, Bm, G, A7 Q Enc°(c) (1) Q Q N(��,B� UNet ← MHA MHA MHA Beats and Beat Embedding downbeats locations FLAN-T5(T) Encb(b) K,V K,V K,V Soothing techno music with 120 DeBERTa MuNet Enc°(c) Encb(b) FLAN-T5(T) FLAN-T5 bpm and in the key of D major. Beat Follow this chord progression: D, Predictor Bm, G, A7. Legend: Inference, Inference only Train only Train + Inference when required Frozen Params. Trainable Params."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 1163
                },
                {
                    "x": 2192,
                    "y": 1163
                },
                {
                    "x": 2192,
                    "y": 1265
                },
                {
                    "x": 287,
                    "y": 1265
                }
            ],
            "category": "caption",
            "html": "<caption id='49' style='font-size:14px'>Figure 2: Depiction of our proposed Mustango model. Beats and chords are inferred from the caption when they<br>are not provided as input.</caption>",
            "id": 49,
            "page": 5,
            "text": "Figure 2: Depiction of our proposed Mustango model. Beats and chords are inferred from the caption when they are not provided as input."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 1352
                },
                {
                    "x": 1214,
                    "y": 1352
                },
                {
                    "x": 1214,
                    "y": 1856
                },
                {
                    "x": 287,
                    "y": 1856
                }
            ],
            "category": "paragraph",
            "html": "<p id='50' style='font-size:18px'>ing of the chords (c[:, 3]). Subsequently, this con-<br>catenated representation is passed through a train-<br>able linear layer (Wc). Notably, we incorporate<br>a music-domain-knowledge informed music em-<br>bedding through the use of the Fundamental Music<br>Embedding from Guo et al. (2023), which effec-<br>tively captures the translational invariant property<br>of pitches and intervals, resulting in a more musi-<br>cally meaningful representation of the chord.</p>",
            "id": 50,
            "page": 5,
            "text": "ing of the chords (c[:, 3]). Subsequently, this concatenated representation is passed through a trainable linear layer (Wc). Notably, we incorporate a music-domain-knowledge informed music embedding through the use of the Fundamental Music Embedding from Guo  (2023), which effectively captures the translational invariant property of pitches and intervals, resulting in a more musically meaningful representation of the chord."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 1867
                },
                {
                    "x": 1213,
                    "y": 1867
                },
                {
                    "x": 1213,
                    "y": 2369
                },
                {
                    "x": 288,
                    "y": 2369
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='51' style='font-size:18px'>After obtaining the encoded beat and chord em-<br>beddings, we use two additional cross-attention<br>layers to integrate these music conditions during<br>the denoising process, whereas Tango used one<br>cross-attention layer to incorporate only text condi-<br>tions. This enables MuNet to leverage both music<br>and text features during the denoising process, re-<br>sulting in more controllable and meaningful music<br>generation.</p>",
            "id": 51,
            "page": 5,
            "text": "After obtaining the encoded beat and chord embeddings, we use two additional cross-attention layers to integrate these music conditions during the denoising process, whereas Tango used one cross-attention layer to incorporate only text conditions. This enables MuNet to leverage both music and text features during the denoising process, resulting in more controllable and meaningful music generation."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 2419
                },
                {
                    "x": 585,
                    "y": 2419
                },
                {
                    "x": 585,
                    "y": 2471
                },
                {
                    "x": 289,
                    "y": 2471
                }
            ],
            "category": "paragraph",
            "html": "<p id='52' style='font-size:20px'>3.3 Inference</p>",
            "id": 52,
            "page": 5,
            "text": "3.3 Inference"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 2499
                },
                {
                    "x": 1214,
                    "y": 2499
                },
                {
                    "x": 1214,
                    "y": 2891
                },
                {
                    "x": 287,
                    "y": 2891
                }
            ],
            "category": "paragraph",
            "html": "<p id='53' style='font-size:18px'>During the training phase, we use teacher forc-<br>ing and hence utilize the ground truth beats and<br>chord features to condition the music generation<br>process. However, during inference, we employ<br>two transformer-based text-to-music-feature gener-<br>ators that have been trained independently to pre-<br>dict the beat and chord features as follows:</p>",
            "id": 53,
            "page": 5,
            "text": "During the training phase, we use teacher forcing and hence utilize the ground truth beats and chord features to condition the music generation process. However, during inference, we employ two transformer-based text-to-music-feature generators that have been trained independently to predict the beat and chord features as follows:"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 2899
                },
                {
                    "x": 1215,
                    "y": 2899
                },
                {
                    "x": 1215,
                    "y": 3235
                },
                {
                    "x": 287,
                    "y": 3235
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='54' style='font-size:18px'>Beats: We use the DeBERTa Large model (He<br>et al., 2022) as the beats predictor. The model takes<br>the text caption as input and predicts: i) the beat<br>count (meter) of corresponding music, and ii) the<br>sequence of interval duration between the beats.<br>We predict them from the token representations of</p>",
            "id": 54,
            "page": 5,
            "text": "Beats: We use the DeBERTa Large model (He , 2022) as the beats predictor. The model takes the text caption as input and predicts: i) the beat count (meter) of corresponding music, and ii) the sequence of interval duration between the beats. We predict them from the token representations of"
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 1353
                },
                {
                    "x": 2196,
                    "y": 1353
                },
                {
                    "x": 2196,
                    "y": 2026
                },
                {
                    "x": 1267,
                    "y": 2026
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='55' style='font-size:16px'>the final layer of the model. The beat count takes<br>an integer value between 1 and 4 for the instances<br>in our training data. Hence, we predict the beat<br>using a four-class classification setup from the first<br>token of the output layer. The interval durations<br>are predicted as a float value from the second token<br>onwards. As an example, if the beat count is pre-<br>dicted as 2 and the interval durations are predicted<br>as t1, t2, t3, · · · , then the predicted beats are as fol-<br>lows: 1 at t1, 2 at t1 + t2, 1 at t1 + t2 + t3, etc. We<br>keep the predicted beats time up to 10 seconds and<br>ignore predicted timestamps beyond that.</p>",
            "id": 55,
            "page": 5,
            "text": "the final layer of the model. The beat count takes an integer value between 1 and 4 for the instances in our training data. Hence, we predict the beat using a four-class classification setup from the first token of the output layer. The interval durations are predicted as a float value from the second token onwards. As an example, if the beat count is predicted as 2 and the interval durations are predicted as t1, t2, t3, · · · , then the predicted beats are as follows: 1 at t1, 2 at t1 + t2, 1 at t1 + t2 + t3, etc. We keep the predicted beats time up to 10 seconds and ignore predicted timestamps beyond that."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 2033
                },
                {
                    "x": 2196,
                    "y": 2033
                },
                {
                    "x": 2196,
                    "y": 2764
                },
                {
                    "x": 1267,
                    "y": 2764
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='56' style='font-size:16px'>Chords: We use the sequence to sequence<br>FLAN-T5 Large model (Chung et al., 2022) as<br>the chords predictor. The model takes the concate-<br>nation of the text caption and the verbalized beats<br>as input. The verbalized beats are prepared for the<br>example we illustrated earlier as follows: Times-<br>tamps: t1, t1 + t2, t1 + t2 + t3 · · · , Max Beat:<br>2. The model is trained to generate the verbalized<br>chords sequence with timestamps, which would<br>look like something as follows: Am at 1.11; E at<br>4.14; C#maj7 at 7.18. We again keep the predicted<br>chord time up to 10 seconds and ignore timestamps<br>predicted beyond that.</p>",
            "id": 56,
            "page": 5,
            "text": "Chords: We use the sequence to sequence FLAN-T5 Large model (Chung , 2022) as the chords predictor. The model takes the concatenation of the text caption and the verbalized beats as input. The verbalized beats are prepared for the example we illustrated earlier as follows: Timestamps: t1, t1 + t2, t1 + t2 + t3 · · · , Max Beat: 2. The model is trained to generate the verbalized chords sequence with timestamps, which would look like something as follows: Am at 1.11; E at 4.14; C#maj7 at 7.18. We again keep the predicted chord time up to 10 seconds and ignore timestamps predicted beyond that."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 2807
                },
                {
                    "x": 1625,
                    "y": 2807
                },
                {
                    "x": 1625,
                    "y": 2862
                },
                {
                    "x": 1269,
                    "y": 2862
                }
            ],
            "category": "paragraph",
            "html": "<p id='57' style='font-size:22px'>4 Experiments</p>",
            "id": 57,
            "page": 5,
            "text": "4 Experiments"
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 2898
                },
                {
                    "x": 2197,
                    "y": 2898
                },
                {
                    "x": 2197,
                    "y": 3236
                },
                {
                    "x": 1267,
                    "y": 3236
                }
            ],
            "category": "paragraph",
            "html": "<p id='58' style='font-size:18px'>We conduct extensive objective and subjective eval-<br>uations to answer these research questions: i) How<br>is the audio quality of the music generated by<br>Mustango? ii) Does Mustango generate music<br>with better music quality compared to other base-<br>lines? iii) Is Mustango more controllable in terms</p>",
            "id": 58,
            "page": 5,
            "text": "We conduct extensive objective and subjective evaluations to answer these research questions: i) How is the audio quality of the music generated by Mustango? ii) Does Mustango generate music with better music quality compared to other baselines? iii) Is Mustango more controllable in terms"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 303
                },
                {
                    "x": 1214,
                    "y": 303
                },
                {
                    "x": 1214,
                    "y": 523
                },
                {
                    "x": 287,
                    "y": 523
                }
            ],
            "category": "paragraph",
            "html": "<p id='59' style='font-size:16px'>of music-specific instructions? iv) Is our data aug-<br>mentation approach effective - can models trained<br>on only this dataset compete with large-scale pre-<br>trained models?</p>",
            "id": 59,
            "page": 6,
            "text": "of music-specific instructions? iv) Is our data augmentation approach effective - can models trained on only this dataset compete with large-scale pretrained models?"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 563
                },
                {
                    "x": 1036,
                    "y": 563
                },
                {
                    "x": 1036,
                    "y": 615
                },
                {
                    "x": 288,
                    "y": 615
                }
            ],
            "category": "paragraph",
            "html": "<p id='60' style='font-size:20px'>4.1 Baselines and Mustango Variants</p>",
            "id": 60,
            "page": 6,
            "text": "4.1 Baselines and Mustango Variants"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 633
                },
                {
                    "x": 1213,
                    "y": 633
                },
                {
                    "x": 1213,
                    "y": 1767
                },
                {
                    "x": 287,
                    "y": 1767
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='61' style='font-size:18px'>We first compare Mustango with Tango since it<br>shares a similar architecture with Mustango, ex-<br>cept for the extra conditioning module: MuNet.<br>To judge the efficacy of Mustango, we train<br>the following three models from scratch: i)<br>Tango trained on MusicCaps TrainA, ii) Tango<br>trained on MusicBench, iii) Mustango trained on<br>MusicBench. Additionally, we finetune Tango and<br>Mustango from pre-trained Tango checkpoints: iv)<br>pre-trained Tango fine-tuned on AudioCaps and<br>MusicCaps3, v) pre-trained Tango fine-tuned on<br>AudioCaps , then finetuned on MusicBench, vi)<br>Mustango initialized from pre-trained Tango and<br>finetuned on MusicBench. Furthermore, we com-<br>pare Mustango with state-of-the-art Text-to-Music<br>model of MusicGen (Copet et al., 2023) and a Text-<br>to-Audio model of AudioLDM2 (Liu et al., 2023b).<br>For MusicGen baselines, we use the small and<br>medium checkpoints. For Audi oLDM2, we compare<br>with their music-specific checkpoint.</p>",
            "id": 61,
            "page": 6,
            "text": "We first compare Mustango with Tango since it shares a similar architecture with Mustango, except for the extra conditioning module: MuNet. To judge the efficacy of Mustango, we train the following three models from scratch: i) Tango trained on MusicCaps TrainA, ii) Tango trained on MusicBench, iii) Mustango trained on MusicBench. Additionally, we finetune Tango and Mustango from pre-trained Tango checkpoints: iv) pre-trained Tango fine-tuned on AudioCaps and MusicCaps3, v) pre-trained Tango fine-tuned on AudioCaps , then finetuned on MusicBench, vi) Mustango initialized from pre-trained Tango and finetuned on MusicBench. Furthermore, we compare Mustango with state-of-the-art Text-to-Music model of MusicGen (Copet , 2023) and a Textto-Audio model of AudioLDM2 (Liu , 2023b). For MusicGen baselines, we use the small and medium checkpoints. For Audi oLDM2, we compare with their music-specific checkpoint."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 1801
                },
                {
                    "x": 1169,
                    "y": 1801
                },
                {
                    "x": 1169,
                    "y": 1854
                },
                {
                    "x": 288,
                    "y": 1854
                }
            ],
            "category": "paragraph",
            "html": "<p id='62' style='font-size:20px'>4.2 Training and Additional Evaluation Set</p>",
            "id": 62,
            "page": 6,
            "text": "4.2 Training and Additional Evaluation Set"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 1876
                },
                {
                    "x": 1213,
                    "y": 1876
                },
                {
                    "x": 1213,
                    "y": 2211
                },
                {
                    "x": 287,
                    "y": 2211
                }
            ],
            "category": "paragraph",
            "html": "<p id='63' style='font-size:14px'>All the models were trained at a learning rate of<br>4.5e - 5 using the AdamW (Loshchilov and Hutter,<br>2017) optimizer until convergence. Our Beat and<br>Chord predictors are also trained on MusicBench.<br>More details on training the classifier-free guid-<br>ance, and parameters are reported in Appendix B.</p>",
            "id": 63,
            "page": 6,
            "text": "All the models were trained at a learning rate of 4.5e - 5 using the AdamW (Loshchilov and Hutter, 2017) optimizer until convergence. Our Beat and Chord predictors are also trained on MusicBench. More details on training the classifier-free guidance, and parameters are reported in Appendix B."
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 2215
                },
                {
                    "x": 1211,
                    "y": 2215
                },
                {
                    "x": 1211,
                    "y": 2778
                },
                {
                    "x": 286,
                    "y": 2778
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='64' style='font-size:16px'>Given that some of the fine-tuned models used in<br>our experiments were exposed to the entire Music-<br>Caps dataset in the initial Tango pre-trained check-<br>point, we can only fairly evaluate those models on<br>a different and independently created evaluation set.<br>We thus curated 1,000 pseudo-captioned evaluation<br>samples from the music files of Free Music Archive<br>(FMA) (Defferrard et al., 2016), which we refer to<br>as FMACaps. The details of creating FMACaps are<br>reported in Appendix F.</p>",
            "id": 64,
            "page": 6,
            "text": "Given that some of the fine-tuned models used in our experiments were exposed to the entire MusicCaps dataset in the initial Tango pre-trained checkpoint, we can only fairly evaluate those models on a different and independently created evaluation set. We thus curated 1,000 pseudo-captioned evaluation samples from the music files of Free Music Archive (FMA) (Defferrard , 2016), which we refer to as FMACaps. The details of creating FMACaps are reported in Appendix F."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 2814
                },
                {
                    "x": 947,
                    "y": 2814
                },
                {
                    "x": 947,
                    "y": 2866
                },
                {
                    "x": 289,
                    "y": 2866
                }
            ],
            "category": "paragraph",
            "html": "<p id='65' style='font-size:20px'>4.3 Inference Settings and Time</p>",
            "id": 65,
            "page": 6,
            "text": "4.3 Inference Settings and Time"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 2888
                },
                {
                    "x": 1213,
                    "y": 2888
                },
                {
                    "x": 1213,
                    "y": 3111
                },
                {
                    "x": 287,
                    "y": 3111
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='66' style='font-size:14px'>In all our experiments, we use 200 diffusion steps<br>with a classifier-free guidance scale of three for<br>all variants of Mustango, Tango, and AudioLDM2.<br>In MusicGen, we generate audio sequences of 10</p>",
            "id": 66,
            "page": 6,
            "text": "In all our experiments, we use 200 diffusion steps with a classifier-free guidance scale of three for all variants of Mustango, Tango, and AudioLDM2. In MusicGen, we generate audio sequences of 10"
        },
        {
            "bounding_box": [
                {
                    "x": 340,
                    "y": 3139
                },
                {
                    "x": 1127,
                    "y": 3139
                },
                {
                    "x": 1127,
                    "y": 3231
                },
                {
                    "x": 340,
                    "y": 3231
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='67' style='font-size:16px'>3hf. co/declare- Lab/tango-full-ft-aucio-music-caps<br>4hf. co/declare- lab/tango-full-ft-audiocaps</p>",
            "id": 67,
            "page": 6,
            "text": "3hf. co/declare- Lab/tango-full-ft-aucio-music-caps 4hf. co/declare- lab/tango-full-ft-audiocaps"
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 304
                },
                {
                    "x": 2197,
                    "y": 304
                },
                {
                    "x": 2197,
                    "y": 695
                },
                {
                    "x": 1267,
                    "y": 695
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='68' style='font-size:14px'>seconds to match the outputs of the other models.<br>We further performed a simple time measurement<br>experiment to assess computing time of presented<br>models. With batch size of 1, we inferred 20 sam-<br>ples and took the average inference time on a single<br>Tesla V100 GPU. The obtained results are: Tango<br>34 sec, MusicGen-M 51 sec, Mustango 76 sec.</p>",
            "id": 68,
            "page": 6,
            "text": "seconds to match the outputs of the other models. We further performed a simple time measurement experiment to assess computing time of presented models. With batch size of 1, we inferred 20 samples and took the average inference time on a single Tesla V100 GPU. The obtained results are: Tango 34 sec, MusicGen-M 51 sec, Mustango 76 sec."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 730
                },
                {
                    "x": 2055,
                    "y": 730
                },
                {
                    "x": 2055,
                    "y": 784
                },
                {
                    "x": 1268,
                    "y": 784
                }
            ],
            "category": "paragraph",
            "html": "<p id='69' style='font-size:22px'>4.4 Objective Evaluation Methodology</p>",
            "id": 69,
            "page": 6,
            "text": "4.4 Objective Evaluation Methodology"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 805
                },
                {
                    "x": 2194,
                    "y": 805
                },
                {
                    "x": 2194,
                    "y": 1140
                },
                {
                    "x": 1268,
                    "y": 1140
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='70' style='font-size:20px'>Audio Quality Estimation The quality of the<br>generated audio samples is evaluated using three<br>objective metrics: Frechet Distance (FD), Frechet<br>Audio Distance (FAD) (Kilgour et al., 2019), and<br>Kullback-Leibler divergence (KL) as used earlier<br>in AudioLDM and Tango.</p>",
            "id": 70,
            "page": 6,
            "text": "Audio Quality Estimation The quality of the generated audio samples is evaluated using three objective metrics: Frechet Distance (FD), Frechet Audio Distance (FAD) (Kilgour , 2019), and Kullback-Leibler divergence (KL) as used earlier in AudioLDM and Tango."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 1171
                },
                {
                    "x": 2196,
                    "y": 1171
                },
                {
                    "x": 2196,
                    "y": 1793
                },
                {
                    "x": 1268,
                    "y": 1793
                }
            ],
            "category": "paragraph",
            "html": "<p id='71' style='font-size:16px'>Controllability Evaluation We evaluate each<br>model's controllability using TestB (see §2.3) and<br>a version of FMACaps that has all the control sen-<br>tences for each sample in the prompt. We first<br>generate music based on the text prompts and then<br>extract the musical features mentioned in §2.1. Sub-<br>sequently, we define nine metrics (all represented in<br>percentage; in the case of binary values, 100 stands<br>for true and 0 stands for false) to evaluate whether<br>the music properties in the generated music match<br>the text prompts. The metrics are:</p>",
            "id": 71,
            "page": 6,
            "text": "Controllability Evaluation We evaluate each model's controllability using TestB (see §2.3) and a version of FMACaps that has all the control sentences for each sample in the prompt. We first generate music based on the text prompts and then extract the musical features mentioned in §2.1. Subsequently, we define nine metrics (all represented in percentage; in the case of binary values, 100 stands for true and 0 stands for false) to evaluate whether the music properties in the generated music match the text prompts. The metrics are:"
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 1826
                },
                {
                    "x": 2193,
                    "y": 1826
                },
                {
                    "x": 2193,
                    "y": 1932
                },
                {
                    "x": 1269,
                    "y": 1932
                }
            ],
            "category": "paragraph",
            "html": "<p id='72' style='font-size:18px'>● Tempo Bin (TB): The predicted beats per minute<br>(bpm) fall into the ground truth tempo bin.</p>",
            "id": 72,
            "page": 6,
            "text": "● Tempo Bin (TB): The predicted beats per minute (bpm) fall into the ground truth tempo bin."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 1940
                },
                {
                    "x": 2196,
                    "y": 1940
                },
                {
                    "x": 2196,
                    "y": 2103
                },
                {
                    "x": 1268,
                    "y": 2103
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='73' style='font-size:16px'>● Tempo Bin with Tolerance (TBT): The pre-<br>dicted bpm falls into the ground truth tempo bin or<br>a neighboring one.</p>",
            "id": 73,
            "page": 6,
            "text": "● Tempo Bin with Tolerance (TBT): The predicted bpm falls into the ground truth tempo bin or a neighboring one."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 2112
                },
                {
                    "x": 2190,
                    "y": 2112
                },
                {
                    "x": 2190,
                    "y": 2215
                },
                {
                    "x": 1269,
                    "y": 2215
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='74' style='font-size:20px'>● Correct Key (CK): The predicted key matches<br>the ground truth key.</p>",
            "id": 74,
            "page": 6,
            "text": "● Correct Key (CK): The predicted key matches the ground truth key."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 2223
                },
                {
                    "x": 2195,
                    "y": 2223
                },
                {
                    "x": 2195,
                    "y": 2384
                },
                {
                    "x": 1269,
                    "y": 2384
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='75' style='font-size:20px'>● Correct Key with Duplicates (CKD): The pre-<br>dicted key matches the ground truth key or an equiv-<br>alent key (i.e., a major key and its relative minor).</p>",
            "id": 75,
            "page": 6,
            "text": "● Correct Key with Duplicates (CKD): The predicted key matches the ground truth key or an equivalent key (i.e., a major key and its relative minor)."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 2394
                },
                {
                    "x": 2192,
                    "y": 2394
                },
                {
                    "x": 2192,
                    "y": 2559
                },
                {
                    "x": 1269,
                    "y": 2559
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='76' style='font-size:18px'>● Perfect Chord Match (PCM): The predicted<br>chord sequence perfectly matches ground truth in<br>terms of length, order, chord root, and chord type.</p>",
            "id": 76,
            "page": 6,
            "text": "● Perfect Chord Match (PCM): The predicted chord sequence perfectly matches ground truth in terms of length, order, chord root, and chord type."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 2563
                },
                {
                    "x": 2193,
                    "y": 2563
                },
                {
                    "x": 2193,
                    "y": 2781
                },
                {
                    "x": 1269,
                    "y": 2781
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='77' style='font-size:18px'>● Exact Chord Match (ECM): The predicted<br>chord sequence matches the ground truth exactly<br>in terms of order, chord root, and chord type, with<br>tolerance for missing and excess chord instances.</p>",
            "id": 77,
            "page": 6,
            "text": "● Exact Chord Match (ECM): The predicted chord sequence matches the ground truth exactly in terms of order, chord root, and chord type, with tolerance for missing and excess chord instances."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 2787
                },
                {
                    "x": 2195,
                    "y": 2787
                },
                {
                    "x": 2195,
                    "y": 2949
                },
                {
                    "x": 1269,
                    "y": 2949
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='78' style='font-size:16px'>● Chord Match in any Order (CMO): The por-<br>tion of predicted chord sequence matching the<br>ground truth chord root and type, in any order.</p>",
            "id": 78,
            "page": 6,
            "text": "● Chord Match in any Order (CMO): The portion of predicted chord sequence matching the ground truth chord root and type, in any order."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 2957
                },
                {
                    "x": 2194,
                    "y": 2957
                },
                {
                    "x": 2194,
                    "y": 3236
                },
                {
                    "x": 1268,
                    "y": 3236
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='79' style='font-size:20px'>● Chord Match in any Order major/minor Type<br>(CMOT): The portion of predicted chord sequence<br>matching the ground truth in terms of chord root<br>and binary major/minor chord type, in any order<br>(e.g., D, D6, D7, Dmaj7 are all considered major).</p>",
            "id": 79,
            "page": 6,
            "text": "● Chord Match in any Order major/minor Type (CMOT): The portion of predicted chord sequence matching the ground truth in terms of chord root and binary major/minor chord type, in any order (e.g., D, D6, D7, Dmaj7 are all considered major)."
        },
        {
            "bounding_box": [
                {
                    "x": 296,
                    "y": 287
                },
                {
                    "x": 2181,
                    "y": 287
                },
                {
                    "x": 2181,
                    "y": 770
                },
                {
                    "x": 296,
                    "y": 770
                }
            ],
            "category": "table",
            "html": "<table id='80' style='font-size:14px'><tr><td rowspan=\"2\">Model</td><td rowspan=\"2\">Datasets</td><td rowspan=\"2\">Pre-trained</td><td rowspan=\"2\">#Params</td><td colspan=\"3\">TestA</td><td colspan=\"3\">TestB</td><td colspan=\"3\">FMACaps</td></tr><tr><td>FD ↓</td><td>FAD ↓</td><td>KL ↓</td><td>FD ↓</td><td>FAD ↓</td><td>KL ↓</td><td>FD ↓</td><td>FAD ↓</td><td>KL ↓</td></tr><tr><td>MusicGen-S</td><td>-</td><td>X</td><td>300M</td><td>35.40</td><td>6.82</td><td>1.81</td><td>36.40</td><td>7.54</td><td>1.75</td><td>23.21</td><td>5.13</td><td>1.31</td></tr><tr><td>MusicGen-M</td><td>-</td><td>X</td><td>1.5B</td><td>36.49</td><td>6.98</td><td>1.71</td><td>35.54</td><td>6.99</td><td>1.71</td><td>22.61</td><td>5.01</td><td>1.33</td></tr><tr><td>AudioLDM2</td><td>-</td><td>X</td><td>346M</td><td>32.76</td><td>5.29</td><td>1.68</td><td>33.66</td><td>5.42</td><td>1.75</td><td>19.99</td><td>3.01</td><td>1.33</td></tr><tr><td>Tango</td><td>MusicCaps</td><td>X</td><td>866M</td><td>30.80</td><td>2.84</td><td>1.34</td><td>30.39</td><td>2.92</td><td>1.33</td><td>28.32</td><td>3.75</td><td>1.22</td></tr><tr><td>Tango</td><td>MusicCaps</td><td>V</td><td>866M</td><td>34.87</td><td>4.05</td><td>1.25</td><td>37.85</td><td>4.52</td><td>1.32</td><td>28.81</td><td>2.92</td><td>1.21</td></tr><tr><td>Tango</td><td>MusicBench</td><td>X</td><td>866M</td><td>28.50</td><td>2.29</td><td>1.33</td><td>28.27</td><td>2.17</td><td>1.32</td><td>26.31</td><td>2.31</td><td>1.16</td></tr><tr><td>Tango</td><td>MusicBench</td><td>V</td><td>866M</td><td>25.38</td><td>1.91</td><td>1.19</td><td>24.60</td><td>1.77</td><td>1.13</td><td>24.48</td><td>2.96</td><td>1.15</td></tr><tr><td>Mustango</td><td>MusicBench</td><td>X</td><td>1.4B</td><td>26.58</td><td>2.09</td><td>1.21</td><td>25.24</td><td>1.57</td><td>1.18</td><td>24.24</td><td>2.94</td><td>1.16</td></tr><tr><td>Mustango</td><td>MusicBench</td><td>V</td><td>1.4B</td><td>26.35</td><td>1.46</td><td>1.21</td><td>25.97</td><td>1.67</td><td>1.12</td><td>25.18</td><td>2.34</td><td>1.16</td></tr></table>",
            "id": 80,
            "page": 7,
            "text": "Model Datasets Pre-trained #Params TestA TestB FMACaps  FD ↓ FAD ↓ KL ↓ FD ↓ FAD ↓ KL ↓ FD ↓ FAD ↓ KL ↓  MusicGen-S - X 300M 35.40 6.82 1.81 36.40 7.54 1.75 23.21 5.13 1.31  MusicGen-M - X 1.5B 36.49 6.98 1.71 35.54 6.99 1.71 22.61 5.01 1.33  AudioLDM2 - X 346M 32.76 5.29 1.68 33.66 5.42 1.75 19.99 3.01 1.33  Tango MusicCaps X 866M 30.80 2.84 1.34 30.39 2.92 1.33 28.32 3.75 1.22  Tango MusicCaps V 866M 34.87 4.05 1.25 37.85 4.52 1.32 28.81 2.92 1.21  Tango MusicBench X 866M 28.50 2.29 1.33 28.27 2.17 1.32 26.31 2.31 1.16  Tango MusicBench V 866M 25.38 1.91 1.19 24.60 1.77 1.13 24.48 2.96 1.15  Mustango MusicBench X 1.4B 26.58 2.09 1.21 25.24 1.57 1.18 24.24 2.94 1.16  Mustango MusicBench V 1.4B 26.35 1.46 1.21 25.97 1.67 1.12 25.18 2.34"
        },
        {
            "bounding_box": [
                {
                    "x": 479,
                    "y": 802
                },
                {
                    "x": 1998,
                    "y": 802
                },
                {
                    "x": 1998,
                    "y": 849
                },
                {
                    "x": 479,
                    "y": 849
                }
            ],
            "category": "caption",
            "html": "<caption id='81' style='font-size:14px'>Table 1: Objective evaluation results of the models on TestA, TestB, and FMACaps datasets.</caption>",
            "id": 81,
            "page": 7,
            "text": "Table 1: Objective evaluation results of the models on TestA, TestB, and FMACaps datasets."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 943
                },
                {
                    "x": 1212,
                    "y": 943
                },
                {
                    "x": 1212,
                    "y": 1053
                },
                {
                    "x": 287,
                    "y": 1053
                }
            ],
            "category": "paragraph",
            "html": "<p id='82' style='font-size:16px'>● Beat Match (BM): The percentage of predicted<br>beat counts that match the ground truth.</p>",
            "id": 82,
            "page": 7,
            "text": "● Beat Match (BM): The percentage of predicted beat counts that match the ground truth."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 1169
                },
                {
                    "x": 966,
                    "y": 1169
                },
                {
                    "x": 966,
                    "y": 1224
                },
                {
                    "x": 289,
                    "y": 1224
                }
            ],
            "category": "paragraph",
            "html": "<p id='83' style='font-size:20px'>4.5 Objective Evaluation Results</p>",
            "id": 83,
            "page": 7,
            "text": "4.5 Objective Evaluation Results"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 1255
                },
                {
                    "x": 1213,
                    "y": 1255
                },
                {
                    "x": 1213,
                    "y": 2269
                },
                {
                    "x": 287,
                    "y": 2269
                }
            ],
            "category": "paragraph",
            "html": "<p id='84' style='font-size:18px'>Audio Quality: The results for TestA, TestB and<br>FMACaps are presented in Table 1. Both Tango<br>variants trained on MusicCaps are inferior to the<br>other four models, which depicts the efficacy of<br>our augmentation strategy. Pre-trained Tango fine-<br>tuned on MusicBench and Mustango pre-trained<br>seem to perform very similarly in FD and KL, but<br>Mustango pre-trained shows a big improvement in<br>FAD, which suggests better-perceived quality and<br>musicality as FAD is a human perception-inspired<br>metric. Lastly, the performance of Mustango<br>trained from scratch is comparable in FD and KL to<br>both pre-trained versions of Mustango and Tango<br>trained on MusicBench, which shows that training<br>with our augmented dataset can be an alternative<br>to large-scale audio pre-training for music gener-<br>ation. Mustango also outperforms MusicGen and<br>AudioLDM2 in FAD and KL across all three sets.</p>",
            "id": 84,
            "page": 7,
            "text": "Audio Quality: The results for TestA, TestB and FMACaps are presented in Table 1. Both Tango variants trained on MusicCaps are inferior to the other four models, which depicts the efficacy of our augmentation strategy. Pre-trained Tango finetuned on MusicBench and Mustango pre-trained seem to perform very similarly in FD and KL, but Mustango pre-trained shows a big improvement in FAD, which suggests better-perceived quality and musicality as FAD is a human perception-inspired metric. Lastly, the performance of Mustango trained from scratch is comparable in FD and KL to both pre-trained versions of Mustango and Tango trained on MusicBench, which shows that training with our augmented dataset can be an alternative to large-scale audio pre-training for music generation. Mustango also outperforms MusicGen and AudioLDM2 in FAD and KL across all three sets."
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 2277
                },
                {
                    "x": 1213,
                    "y": 2277
                },
                {
                    "x": 1213,
                    "y": 3234
                },
                {
                    "x": 286,
                    "y": 3234
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='85' style='font-size:16px'>We note that the results for MusicGen and<br>AudioLDM2 differ from what was reported in their<br>original papers in evaluation on MusicCaps. This<br>is due to MusicBench representing a different,<br>more challenging, split of data than the Music-<br>Caps evaluation set, as described in §2.3. Addi-<br>tionally, we note that the results of both MusicGen<br>and AudioLDM2 show more improvement than<br>Mustango when evaluated on FMACaps as com-<br>pared to TestA and TestB. This is due to the fact<br>that Mustango was trained on MusicBench, thus<br>TestA and TestB represent similar distributions to<br>the training set, while FMACaps is of a slightly dif-<br>ferent distribution. The MusicGen and AudioLDM2<br>models on the other hand, were trained with vari-<br>ous large-scale data, hence they perform well on<br>an unseen set.</p>",
            "id": 85,
            "page": 7,
            "text": "We note that the results for MusicGen and AudioLDM2 differ from what was reported in their original papers in evaluation on MusicCaps. This is due to MusicBench representing a different, more challenging, split of data than the MusicCaps evaluation set, as described in §2.3. Additionally, we note that the results of both MusicGen and AudioLDM2 show more improvement than Mustango when evaluated on FMACaps as compared to TestA and TestB. This is due to the fact that Mustango was trained on MusicBench, thus TestA and TestB represent similar distributions to the training set, while FMACaps is of a slightly different distribution. The MusicGen and AudioLDM2 models on the other hand, were trained with various large-scale data, hence they perform well on an unseen set."
        },
        {
            "bounding_box": [
                {
                    "x": 1266,
                    "y": 942
                },
                {
                    "x": 2198,
                    "y": 942
                },
                {
                    "x": 2198,
                    "y": 2068
                },
                {
                    "x": 1266,
                    "y": 2068
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='86' style='font-size:18px'>Controllability: The evaluation results on con-<br>trollability are shown in Table 2. On TestB, in terms<br>of Tempo metrics, all the models perform compa-<br>rably, except for MusicGen, which performs better.<br>In Beat metrics, the models perform similarly to<br>each other. Mustango placed second closely be-<br>hind MusicGen. The similarity in performance<br>among the models could be caused by the Music-<br>Caps dataset already containing enough informa-<br>tion about tempo, with words such as \"slow\" , \"fast\",<br>\"moderate\" , etc. This information being passed<br>through the text encoding might be a sufficient<br>control command. Furthermore, the inaccuracy of<br>the beat extractor combined with the fact that not<br>all music pieces in MusicCaps have clearly audi-<br>ble beats might further contribute to the Beat and<br>Tempo metrics results. Thus, having more open-<br>sourced high-quality music data would greatly ben-<br>efit development of even more controllable sys-<br>tems.</p>",
            "id": 86,
            "page": 7,
            "text": "Controllability: The evaluation results on controllability are shown in Table 2. On TestB, in terms of Tempo metrics, all the models perform comparably, except for MusicGen, which performs better. In Beat metrics, the models perform similarly to each other. Mustango placed second closely behind MusicGen. The similarity in performance among the models could be caused by the MusicCaps dataset already containing enough information about tempo, with words such as \"slow\" , \"fast\", \"moderate\" , etc. This information being passed through the text encoding might be a sufficient control command. Furthermore, the inaccuracy of the beat extractor combined with the fact that not all music pieces in MusicCaps have clearly audible beats might further contribute to the Beat and Tempo metrics results. Thus, having more opensourced high-quality music data would greatly benefit development of even more controllable systems."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 2076
                },
                {
                    "x": 2197,
                    "y": 2076
                },
                {
                    "x": 2197,
                    "y": 2812
                },
                {
                    "x": 1268,
                    "y": 2812
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='87' style='font-size:18px'>In Key metrics, we can observe that models<br>trained on MusicBench perform significantly bet-<br>ter than the ones trained on MusicCaps. Addition-<br>ally, Mustango outperforms all the other models<br>on TestB and placed second on FMACaps. Finally,<br>in Chord controllability, Mustango outperforms all<br>the other models by a big margin. On FMACaps, we<br>further see that the Chord metrics are even better<br>for Mustango with CMOT reaching 75.83. Overall,<br>the results gathered from both TestB and modi-<br>fied FMACaps correlate in most aspects. Overall,<br>Mustango performs fine in Beat and Tempo met-<br>rics, and it excels in Key and Chord controllability.</p>",
            "id": 87,
            "page": 7,
            "text": "In Key metrics, we can observe that models trained on MusicBench perform significantly better than the ones trained on MusicCaps. Additionally, Mustango outperforms all the other models on TestB and placed second on FMACaps. Finally, in Chord controllability, Mustango outperforms all the other models by a big margin. On FMACaps, we further see that the Chord metrics are even better for Mustango with CMOT reaching 75.83. Overall, the results gathered from both TestB and modified FMACaps correlate in most aspects. Overall, Mustango performs fine in Beat and Tempo metrics, and it excels in Key and Chord controllability."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 2869
                },
                {
                    "x": 2069,
                    "y": 2869
                },
                {
                    "x": 2069,
                    "y": 2923
                },
                {
                    "x": 1269,
                    "y": 2923
                }
            ],
            "category": "paragraph",
            "html": "<p id='88' style='font-size:22px'>4.6 Subjective Evaluation Methodology</p>",
            "id": 88,
            "page": 7,
            "text": "4.6 Subjective Evaluation Methodology"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 2953
                },
                {
                    "x": 2198,
                    "y": 2953
                },
                {
                    "x": 2198,
                    "y": 3235
                },
                {
                    "x": 1268,
                    "y": 3235
                }
            ],
            "category": "paragraph",
            "html": "<p id='89' style='font-size:16px'>We conducted two rounds of subjective evalua-<br>tion, each consisting of a general and an expert<br>listening test that focuses on controllability. The<br>first round is aimed at comparing Mustango vari-<br>ants with Tango and in the second run we com-</p>",
            "id": 89,
            "page": 7,
            "text": "We conducted two rounds of subjective evaluation, each consisting of a general and an expert listening test that focuses on controllability. The first round is aimed at comparing Mustango variants with Tango and in the second run we com-"
        },
        {
            "bounding_box": [
                {
                    "x": 295,
                    "y": 287
                },
                {
                    "x": 2186,
                    "y": 287
                },
                {
                    "x": 2186,
                    "y": 716
                },
                {
                    "x": 295,
                    "y": 716
                }
            ],
            "category": "table",
            "html": "<table id='90' style='font-size:14px'><tr><td rowspan=\"3\">Model</td><td rowspan=\"3\">Datasets</td><td rowspan=\"3\">Pre- trained</td><td colspan=\"9\">TestB</td><td colspan=\"9\">FMACaps</td></tr><tr><td colspan=\"2\">Tempo</td><td colspan=\"2\">Key</td><td colspan=\"4\">Chord</td><td>Beat</td><td colspan=\"2\">Tempo</td><td colspan=\"2\">Key</td><td colspan=\"4\">Chord</td><td>Beat</td></tr><tr><td>TB</td><td>TBT</td><td>CK</td><td>CKD</td><td>PCM</td><td>ECM</td><td>CMO</td><td>CMOT</td><td>BM</td><td>TB</td><td>TBT</td><td>CK</td><td>CKD</td><td>PCM</td><td>ECM</td><td>CMO</td><td>CMOT</td><td>BM</td></tr><tr><td>MusicGen-S</td><td></td><td></td><td>39.50</td><td>56.00</td><td>17.5</td><td>19.00</td><td>3.17</td><td>6.03</td><td>12.56</td><td>21.74</td><td>36.75</td><td>45.0</td><td>61.9</td><td>19.9</td><td>21.1</td><td>3.62</td><td>6.49</td><td>10.99</td><td>22.30</td><td>42.4</td></tr><tr><td>MusicGen-M</td><td></td><td>X</td><td>41.00</td><td>60.25</td><td>25.5</td><td>26.25</td><td>3.97</td><td>8.21</td><td>14.42</td><td>26.76</td><td>45.00</td><td>42.7</td><td>63.5</td><td>23.5</td><td>24.3</td><td>6.38</td><td>10.60</td><td>16.24</td><td>31.51</td><td>42.9</td></tr><tr><td>AudioLDM2</td><td></td><td>X</td><td>21.25</td><td>47.75</td><td>6.50</td><td>10.25</td><td>0.79</td><td>2.67</td><td>4.87</td><td>10.55</td><td>39.75</td><td>24.2</td><td>48.7</td><td>5.9</td><td>9.9</td><td>1.06</td><td>1.96</td><td>3.27</td><td>8.84</td><td>38.1</td></tr><tr><td>Tango</td><td>MusicCaps</td><td>X</td><td>26.00</td><td>55.25</td><td>4.00</td><td>7.00</td><td>0.53</td><td>2.09</td><td>4.30</td><td>11.13</td><td>41.00</td><td>22.5</td><td>49.6</td><td>3.6</td><td>8.6</td><td>0.64</td><td>1.43</td><td>4.03</td><td>10.82</td><td>41.1</td></tr><tr><td>Tango</td><td>MusicCaps</td><td></td><td>27.50</td><td>52.00</td><td>7.75</td><td>11.25</td><td>1.06</td><td>3.07</td><td>6.72</td><td>13.99</td><td>36.75</td><td>24.2</td><td>48.6</td><td>5.9</td><td>8.6</td><td>1.17</td><td>2.74</td><td>5.17</td><td>12.69</td><td>35.4</td></tr><tr><td>Tango</td><td>MusicBench</td><td>X</td><td>24.75</td><td>50.75</td><td>34.25</td><td>34.50</td><td>5.56</td><td>12.03</td><td>21.54</td><td>32.21</td><td>34.25</td><td>25.5</td><td>51.0</td><td>38.1</td><td>38.4</td><td>6.60</td><td>13.45</td><td>21.18</td><td>41.49</td><td>36.4</td></tr><tr><td>Tango</td><td>MusicBench</td><td></td><td>26.00</td><td>48.75</td><td>30.25</td><td>31.00</td><td>6.61</td><td>13.33</td><td>22.53</td><td>39.31</td><td>38.50</td><td>22.8</td><td>45.6</td><td>30.6</td><td>31.7</td><td>7.55</td><td>14.72</td><td>22.35</td><td>44.46</td><td>36.0</td></tr><tr><td>Mustango</td><td>MusicBench</td><td></td><td>25.50</td><td>52.00</td><td>41.75</td><td>42.50</td><td>17.99</td><td>32.61</td><td>48.74</td><td>68.46</td><td>42.00</td><td>24.1</td><td>50.9</td><td>36.8</td><td>37.3</td><td>23.94</td><td>35.43</td><td>49.59</td><td>75.83</td><td>42.6</td></tr><tr><td>Mustango</td><td>MusicBench</td><td></td><td>21.25</td><td>48.25</td><td>34.50</td><td>35.50</td><td>11.64</td><td>20.82</td><td>32.93</td><td>50.56</td><td>34.75</td><td>26.2</td><td>52.2</td><td>33.9</td><td>34.7</td><td>15.21</td><td>25.48</td><td>37.50</td><td>61.55</td><td>39.1</td></tr></table>",
            "id": 90,
            "page": 8,
            "text": "Model Datasets Pre- trained TestB FMACaps  Tempo Key Chord Beat Tempo Key Chord Beat  TB TBT CK CKD PCM ECM CMO CMOT BM TB TBT CK CKD PCM ECM CMO CMOT BM  MusicGen-S   39.50 56.00 17.5 19.00 3.17 6.03 12.56 21.74 36.75 45.0 61.9 19.9 21.1 3.62 6.49 10.99 22.30 42.4  MusicGen-M  X 41.00 60.25 25.5 26.25 3.97 8.21 14.42 26.76 45.00 42.7 63.5 23.5 24.3 6.38 10.60 16.24 31.51 42.9  AudioLDM2  X 21.25 47.75 6.50 10.25 0.79 2.67 4.87 10.55 39.75 24.2 48.7 5.9 9.9 1.06 1.96 3.27 8.84 38.1  Tango MusicCaps X 26.00 55.25 4.00 7.00 0.53 2.09 4.30 11.13 41.00 22.5 49.6 3.6 8.6 0.64 1.43 4.03 10.82 41.1  Tango MusicCaps  27.50 52.00 7.75 11.25 1.06 3.07 6.72 13.99 36.75 24.2 48.6 5.9 8.6 1.17 2.74 5.17 12.69 35.4  Tango MusicBench X 24.75 50.75 34.25 34.50 5.56 12.03 21.54 32.21 34.25 25.5 51.0 38.1 38.4 6.60 13.45 21.18 41.49 36.4  Tango MusicBench  26.00 48.75 30.25 31.00 6.61 13.33 22.53 39.31 38.50 22.8 45.6 30.6 31.7 7.55 14.72 22.35 44.46 36.0  Mustango MusicBench  25.50 52.00 41.75 42.50 17.99 32.61 48.74 68.46 42.00 24.1 50.9 36.8 37.3 23.94 35.43 49.59 75.83 42.6  Mustango MusicBench  21.25 48.25 34.50 35.50 11.64 20.82 32.93 50.56 34.75 26.2 52.2 33.9 34.7 15.21 25.48 37.50 61.55"
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 747
                },
                {
                    "x": 2190,
                    "y": 747
                },
                {
                    "x": 2190,
                    "y": 850
                },
                {
                    "x": 290,
                    "y": 850
                }
            ],
            "category": "caption",
            "html": "<caption id='91' style='font-size:16px'>Table 2: Controllability evaluation results of the models on TestB and full-control variant of FMACaps. Higher<br>numbers indicate better controllability.</caption>",
            "id": 91,
            "page": 8,
            "text": "Table 2: Controllability evaluation results of the models on TestB and full-control variant of FMACaps. Higher numbers indicate better controllability."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 943
                },
                {
                    "x": 1216,
                    "y": 943
                },
                {
                    "x": 1216,
                    "y": 1046
                },
                {
                    "x": 288,
                    "y": 1046
                }
            ],
            "category": "paragraph",
            "html": "<p id='92' style='font-size:20px'>pare Mustango with the state-of-the-art models:<br>MusicGen and AudioLDM2.</p>",
            "id": 92,
            "page": 8,
            "text": "pare Mustango with the state-of-the-art models: MusicGen and AudioLDM2."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 1062
                },
                {
                    "x": 1213,
                    "y": 1062
                },
                {
                    "x": 1213,
                    "y": 2251
                },
                {
                    "x": 287,
                    "y": 2251
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='93' style='font-size:20px'>In the first round of the general listening test,<br>subjects listened to ten generated music samples<br>for each of the four models (pre-trained Mustango,<br>Mustango, Tango trained with MusicCaps and Mu-<br>sicBench) and were provided with the input text<br>caption. The ten text prompts were custom-made<br>by music experts in the style of MusicCaps, and<br>are shown in Table 7 in the Appendix. The partic-<br>ipants were asked to rate the: i) audio rendering<br>quality (AQ), ii) relevance of the audio with the<br>input text prompt (REL), iii) overall musical qual-<br>ity (OMQ), iv) rhythm consistency (RC), and v)<br>harmony and consonance of music (HC). For the<br>expert listening test, we added two additional music<br>control-specific aspects to rate the degree to which<br>the chords and tempo from the generated music<br>match the text prompt. We denote them as MCM<br>and MTM (musical chord/tempo match). All the as-<br>pects were rated on a 7-point Likert scale using the<br>PsyToolkit interface (Stoet, 2010). The full ques-<br>tions and interface used are shown in Appendix G.</p>",
            "id": 93,
            "page": 8,
            "text": "In the first round of the general listening test, subjects listened to ten generated music samples for each of the four models (pre-trained Mustango, Mustango, Tango trained with MusicCaps and MusicBench) and were provided with the input text caption. The ten text prompts were custom-made by music experts in the style of MusicCaps, and are shown in Table 7 in the Appendix. The participants were asked to rate the: i) audio rendering quality (AQ), ii) relevance of the audio with the input text prompt (REL), iii) overall musical quality (OMQ), iv) rhythm consistency (RC), and v) harmony and consonance of music (HC). For the expert listening test, we added two additional music control-specific aspects to rate the degree to which the chords and tempo from the generated music match the text prompt. We denote them as MCM and MTM (musical chord/tempo match). All the aspects were rated on a 7-point Likert scale using the PsyToolkit interface (Stoet, 2010). The full questions and interface used are shown in Appendix G."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 2264
                },
                {
                    "x": 1213,
                    "y": 2264
                },
                {
                    "x": 1213,
                    "y": 2770
                },
                {
                    "x": 288,
                    "y": 2770
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='94' style='font-size:20px'>For the expert listening test in the first round,<br>we found experts with at least five years of formal<br>musical training who can identify music attributes<br>from music audio. They were presented with 80<br>samples generated using 20 custom text prompts<br>for each of the four models as shown in Appendix J.<br>Samples consisted of ten contrasting pairs (e.g.,<br>same prompts with different chord changes) that<br>aimed to target musical controllability.</p>",
            "id": 94,
            "page": 8,
            "text": "For the expert listening test in the first round, we found experts with at least five years of formal musical training who can identify music attributes from music audio. They were presented with 80 samples generated using 20 custom text prompts for each of the four models as shown in Appendix J. Samples consisted of ten contrasting pairs (e.g., same prompts with different chord changes) that aimed to target musical controllability."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 2787
                },
                {
                    "x": 1213,
                    "y": 2787
                },
                {
                    "x": 1213,
                    "y": 3235
                },
                {
                    "x": 287,
                    "y": 3235
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='95' style='font-size:18px'>In the second round of the general listening test,<br>we used the same 10 captions as in the first run<br>with five additional captions taken from FMACaps.<br>In the expert test, we kept the same 10 contrasting<br>pairs as in the first round. For both of these tests, we<br>downsampled the MusicGen samples to 16 kHz to<br>eliminate audio quality bias in listeners' responses,<br>and we excluded the AQ metric from these tests.</p>",
            "id": 95,
            "page": 8,
            "text": "In the second round of the general listening test, we used the same 10 captions as in the first run with five additional captions taken from FMACaps. In the expert test, we kept the same 10 contrasting pairs as in the first round. For both of these tests, we downsampled the MusicGen samples to 16 kHz to eliminate audio quality bias in listeners' responses, and we excluded the AQ metric from these tests."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 941
                },
                {
                    "x": 1959,
                    "y": 941
                },
                {
                    "x": 1959,
                    "y": 991
                },
                {
                    "x": 1268,
                    "y": 991
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='96' style='font-size:22px'>4.7 Subjective Evaluation Results</p>",
            "id": 96,
            "page": 8,
            "text": "4.7 Subjective Evaluation Results"
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 1078
                },
                {
                    "x": 2195,
                    "y": 1078
                },
                {
                    "x": 2195,
                    "y": 1869
                },
                {
                    "x": 1267,
                    "y": 1869
                }
            ],
            "category": "paragraph",
            "html": "<p id='97' style='font-size:20px'>A total of 48 participants participated in the first<br>round of the general listening test, of which 26 had<br>more than five years of formal musical training.<br>The results in Table 3 show the average ratings<br>for each of the metrics defined above. We can<br>clearly see that the Tango baseline model is out-<br>performed in all metrics by the models trained on<br>MusicBench. Interestingly, Mustango trained from<br>scratch performs the best in terms of audio quality,<br>rhythm presence, and harmony. The differences in<br>ratings are minimal between the three top models,<br>clearly confirming that our augmentation method<br>is effective in furthering the output quality and that<br>Mustango is able to reach state-of-the-art quality.</p>",
            "id": 97,
            "page": 8,
            "text": "A total of 48 participants participated in the first round of the general listening test, of which 26 had more than five years of formal musical training. The results in Table 3 show the average ratings for each of the metrics defined above. We can clearly see that the Tango baseline model is outperformed in all metrics by the models trained on MusicBench. Interestingly, Mustango trained from scratch performs the best in terms of audio quality, rhythm presence, and harmony. The differences in ratings are minimal between the three top models, clearly confirming that our augmentation method is effective in furthering the output quality and that Mustango is able to reach state-of-the-art quality."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 1906
                },
                {
                    "x": 2193,
                    "y": 1906
                },
                {
                    "x": 2193,
                    "y": 2469
                },
                {
                    "x": 1268,
                    "y": 2469
                }
            ],
            "category": "paragraph",
            "html": "<p id='98' style='font-size:20px'>A total of four experts participated in the con-<br>trollability listening study. The results of the expert<br>listening study in Table 3 further confirm that both<br>Mustango models outperform the Tango baselines<br>in all metrics, especially in terms of the chords of<br>the generated music matching with the input text<br>caption (Chord Match or MCM). This further sup-<br>ports the controllability results presented in Table 2<br>and shows that our proposed Mustango model can<br>indeed understand music-specific text prompts.</p>",
            "id": 98,
            "page": 8,
            "text": "A total of four experts participated in the controllability listening study. The results of the expert listening study in Table 3 further confirm that both Mustango models outperform the Tango baselines in all metrics, especially in terms of the chords of the generated music matching with the input text caption (Chord Match or MCM). This further supports the controllability results presented in Table 2 and shows that our proposed Mustango model can indeed understand music-specific text prompts."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 2503
                },
                {
                    "x": 2195,
                    "y": 2503
                },
                {
                    "x": 2195,
                    "y": 3233
                },
                {
                    "x": 1267,
                    "y": 3233
                }
            ],
            "category": "paragraph",
            "html": "<p id='99' style='font-size:20px'>In the second run, a total of 17 general audience<br>listeners and 4 experts participated. The results<br>are depicted in the lower part of Table 3. We per-<br>formed a series of paired t-tests on the obtained<br>results and conclude that Mustango outperforms<br>MusicGen and AudioLDM2 in terms of REL, with<br>a statistically significant difference; and performs<br>similarly in OMQ, HC, and MTM to MusicGen,<br>where the t-tests showed no stastically significant<br>differences (both in general audience and expert<br>test). Moreover, Mustango dominates in MCM. In<br>RC, Musi cGen outperformed both AudioLDM2 and<br>Mustango.</p>",
            "id": 99,
            "page": 8,
            "text": "In the second run, a total of 17 general audience listeners and 4 experts participated. The results are depicted in the lower part of Table 3. We performed a series of paired t-tests on the obtained results and conclude that Mustango outperforms MusicGen and AudioLDM2 in terms of REL, with a statistically significant difference; and performs similarly in OMQ, HC, and MTM to MusicGen, where the t-tests showed no stastically significant differences (both in general audience and expert test). Moreover, Mustango dominates in MCM. In RC, Musi cGen outperformed both AudioLDM2 and Mustango."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 290
                },
                {
                    "x": 2191,
                    "y": 290
                },
                {
                    "x": 2191,
                    "y": 741
                },
                {
                    "x": 290,
                    "y": 741
                }
            ],
            "category": "table",
            "html": "<table id='100' style='font-size:14px'><tr><td rowspan=\"2\">Model</td><td rowspan=\"2\">Datasets</td><td rowspan=\"2\">Pre-trained</td><td colspan=\"5\">General audience</td><td colspan=\"7\">Music experts</td></tr><tr><td>REL</td><td>AQ</td><td>OMQ</td><td>RC</td><td>HC</td><td>REL</td><td>MCM</td><td>MTM</td><td>AQ</td><td>OMQ</td><td>RC</td><td>HC</td></tr><tr><td>Tango</td><td>MusicCaps</td><td></td><td>4.09</td><td>3.68</td><td>3.55</td><td>3.91</td><td>3.80</td><td>4.35</td><td>2.75</td><td>3.88</td><td>3.35</td><td>2.83</td><td>3.95</td><td>3.84</td></tr><tr><td>Tango</td><td>MusicBench</td><td></td><td>4.96</td><td>4.26</td><td>4.40</td><td>4.49</td><td>4.61</td><td>4.91</td><td>3.61</td><td>3.86</td><td>3.88</td><td>3.54</td><td>4.01</td><td>4.34</td></tr><tr><td>Mustango</td><td>MusicBench</td><td></td><td>4.85</td><td>4.10</td><td>4.02</td><td>4.24</td><td>4.43</td><td>5.49</td><td>5.76</td><td>4.98</td><td>4.30</td><td>4.28</td><td>4.65</td><td>5.18</td></tr><tr><td>Mustango</td><td>MusicBench</td><td>X</td><td>4.79</td><td>4.20</td><td>4.23</td><td>4.51</td><td>4.63</td><td>5.75</td><td>6.06</td><td>5.11</td><td>4.80</td><td>4.80</td><td>4.75</td><td>5.59</td></tr><tr><td>MusicGen-M</td><td>-</td><td>-</td><td>4.55</td><td>-</td><td>4.40</td><td>5.11</td><td>4.63</td><td>4.41</td><td>2.99</td><td>4.83</td><td>-</td><td>5.01</td><td>5.61</td><td>5.31</td></tr><tr><td>AudioLDM2</td><td>-</td><td>-</td><td>3.99</td><td>-</td><td>3.89</td><td>4.38</td><td>4.11</td><td>3.71</td><td>2.48</td><td>3.53</td><td>-</td><td>3.29</td><td>3.84</td><td>3.40</td></tr><tr><td>Mustango</td><td>MusicBench</td><td>X</td><td>5.18</td><td>-</td><td>4.15</td><td>4.31</td><td>4.47</td><td>5.79</td><td>6.10</td><td>4.84</td><td>-</td><td>4.53</td><td>4.14</td><td>5.11</td></tr></table>",
            "id": 100,
            "page": 9,
            "text": "Model Datasets Pre-trained General audience Music experts  REL AQ OMQ RC HC REL MCM MTM AQ OMQ RC HC  Tango MusicCaps  4.09 3.68 3.55 3.91 3.80 4.35 2.75 3.88 3.35 2.83 3.95 3.84  Tango MusicBench  4.96 4.26 4.40 4.49 4.61 4.91 3.61 3.86 3.88 3.54 4.01 4.34  Mustango MusicBench  4.85 4.10 4.02 4.24 4.43 5.49 5.76 4.98 4.30 4.28 4.65 5.18  Mustango MusicBench X 4.79 4.20 4.23 4.51 4.63 5.75 6.06 5.11 4.80 4.80 4.75 5.59  MusicGen-M - - 4.55 - 4.40 5.11 4.63 4.41 2.99 4.83 - 5.01 5.61 5.31  AudioLDM2 - - 3.99 - 3.89 4.38 4.11 3.71 2.48 3.53 - 3.29 3.84 3.40  Mustango MusicBench X 5.18 - 4.15 4.31 4.47 5.79 6.10 4.84 - 4.53 4.14"
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 775
                },
                {
                    "x": 2193,
                    "y": 775
                },
                {
                    "x": 2193,
                    "y": 874
                },
                {
                    "x": 289,
                    "y": 874
                }
            ],
            "category": "caption",
            "html": "<caption id='101' style='font-size:14px'>Table 3: Average ratings for each metric in the general and expert listening study. Top part of the table shows the first<br>run of listening tests, the bottom part represents the second round of comparison with Musi cGen and AudioLDM2.</caption>",
            "id": 101,
            "page": 9,
            "text": "Table 3: Average ratings for each metric in the general and expert listening study. Top part of the table shows the first run of listening tests, the bottom part represents the second round of comparison with Musi cGen and AudioLDM2."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 965
                },
                {
                    "x": 693,
                    "y": 965
                },
                {
                    "x": 693,
                    "y": 1017
                },
                {
                    "x": 289,
                    "y": 1017
                }
            ],
            "category": "paragraph",
            "html": "<p id='102' style='font-size:22px'>4.8 Ablation Study</p>",
            "id": 102,
            "page": 9,
            "text": "4.8 Ablation Study"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 1039
                },
                {
                    "x": 1212,
                    "y": 1039
                },
                {
                    "x": 1212,
                    "y": 1205
                },
                {
                    "x": 287,
                    "y": 1205
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='103' style='font-size:16px'>Although without explicitly ablating one module at<br>a time due to resource constraints, we are able to<br>answer the following research questions:</p>",
            "id": 103,
            "page": 9,
            "text": "Although without explicitly ablating one module at a time due to resource constraints, we are able to answer the following research questions:"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 1233
                },
                {
                    "x": 1212,
                    "y": 1233
                },
                {
                    "x": 1212,
                    "y": 1911
                },
                {
                    "x": 287,
                    "y": 1911
                }
            ],
            "category": "paragraph",
            "html": "<p id='104' style='font-size:20px'>Is Pre-training Mustango Necessary? In one<br>of the experiment settings in §4.1, we initialized<br>Mustango with a pre-trained Tango checkpoint<br>and subsequently fine-tune it using the AudioCaps<br>dataset. This Tango model was pre-trained using<br>1.2 million text-audio paired samples and it en-<br>capsulates a broad understanding of general audio<br>and text. However, we observed this did not prove<br>beneficial for music generation (see Tables 1 to 3).<br>Nevertheless, these checkpoints may find utility in<br>composing music with soundscapes, such as \"Hip-<br>hop music with a lion's roar in the background.\"</p>",
            "id": 104,
            "page": 9,
            "text": "Is Pre-training Mustango Necessary? In one of the experiment settings in §4.1, we initialized Mustango with a pre-trained Tango checkpoint and subsequently fine-tune it using the AudioCaps dataset. This Tango model was pre-trained using 1.2 million text-audio paired samples and it encapsulates a broad understanding of general audio and text. However, we observed this did not prove beneficial for music generation (see Tables 1 to 3). Nevertheless, these checkpoints may find utility in composing music with soundscapes, such as \"Hiphop music with a lion's roar in the background.\""
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 1941
                },
                {
                    "x": 1212,
                    "y": 1941
                },
                {
                    "x": 1212,
                    "y": 2621
                },
                {
                    "x": 287,
                    "y": 2621
                }
            ],
            "category": "paragraph",
            "html": "<p id='105' style='font-size:18px'>Is MuNet Helpful? We prove the effectiveness<br>of MuNet in §4.5 and §4.7, we show that the use<br>of MuNet significantly enhances the performance<br>of Mustango in terms of controllability under both<br>objective and subjective evaluations. Moreover,<br>several objective metrics which are not explic-<br>itly targeted at controllability (i.e., FD, FAD, and<br>KL-divergence), consistently show superior perfor-<br>mance when MuNet is incorporated. With classier-<br>free guidance, MuNet does not compromise the<br>overall quality of the generated music when the<br>control sentences in the prompts are absent.</p>",
            "id": 105,
            "page": 9,
            "text": "Is MuNet Helpful? We prove the effectiveness of MuNet in §4.5 and §4.7, we show that the use of MuNet significantly enhances the performance of Mustango in terms of controllability under both objective and subjective evaluations. Moreover, several objective metrics which are not explicitly targeted at controllability (i.e., FD, FAD, and KL-divergence), consistently show superior performance when MuNet is incorporated. With classierfree guidance, MuNet does not compromise the overall quality of the generated music when the control sentences in the prompts are absent."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 2656
                },
                {
                    "x": 625,
                    "y": 2656
                },
                {
                    "x": 625,
                    "y": 2705
                },
                {
                    "x": 289,
                    "y": 2705
                }
            ],
            "category": "paragraph",
            "html": "<p id='106' style='font-size:16px'>4.9 Discussions</p>",
            "id": 106,
            "page": 9,
            "text": "4.9 Discussions"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 2729
                },
                {
                    "x": 1213,
                    "y": 2729
                },
                {
                    "x": 1213,
                    "y": 3235
                },
                {
                    "x": 288,
                    "y": 3235
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='107' style='font-size:18px'>As both objective and subjective evaluation results<br>show, Mustango gives state-of-the-art performance<br>in music quality and drastic improvement in mu-<br>sic controllability, despite being trained on a pub-<br>licly available dataset of relatively small size as<br>compared to other available text-to-music systems<br>such as MusicGen, which are usually trained on<br>private large-scale licensed dataset. Although these<br>text-to-music systems usually generate music with</p>",
            "id": 107,
            "page": 9,
            "text": "As both objective and subjective evaluation results show, Mustango gives state-of-the-art performance in music quality and drastic improvement in music controllability, despite being trained on a publicly available dataset of relatively small size as compared to other available text-to-music systems such as MusicGen, which are usually trained on private large-scale licensed dataset. Although these text-to-music systems usually generate music with"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 966
                },
                {
                    "x": 2194,
                    "y": 966
                },
                {
                    "x": 2194,
                    "y": 1131
                },
                {
                    "x": 1268,
                    "y": 1131
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='108' style='font-size:20px'>better audio quality or longer-term structure, which<br>sheds light on further improvement direction of<br>Mustango.</p>",
            "id": 108,
            "page": 9,
            "text": "better audio quality or longer-term structure, which sheds light on further improvement direction of Mustango."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 1178
                },
                {
                    "x": 1593,
                    "y": 1178
                },
                {
                    "x": 1593,
                    "y": 1232
                },
                {
                    "x": 1268,
                    "y": 1232
                }
            ],
            "category": "paragraph",
            "html": "<p id='109' style='font-size:20px'>5 Conclusion</p>",
            "id": 109,
            "page": 9,
            "text": "5 Conclusion"
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 1271
                },
                {
                    "x": 2199,
                    "y": 1271
                },
                {
                    "x": 2199,
                    "y": 2173
                },
                {
                    "x": 1267,
                    "y": 2173
                }
            ],
            "category": "paragraph",
            "html": "<p id='110' style='font-size:16px'>In conclusion, Mustango presents a significant ad-<br>vancement in the field of controllable text-to-music<br>generation. Mustango is a controllable diffusion-<br>based text-to-music system inspired by music-<br>domain knowledge which is able to generate mu-<br>sic that follows certain music properties embed-<br>ded within user-specified text prompts. The in-<br>tegration of the MuNet module within Mustango<br>enables greater music controllability over state-<br>of-the-art text-to-music systems such as Tango,<br>AudioLDM2 and MusicGen. We also made our<br>dataset MusicBench and model publicly available.<br>MusicBench contains 11 times more data than<br>the original MusicCaps dataset and includes text<br>prompts that contain music-theory -based descrip-<br>tion and augmented music audio.</p>",
            "id": 110,
            "page": 9,
            "text": "In conclusion, Mustango presents a significant advancement in the field of controllable text-to-music generation. Mustango is a controllable diffusionbased text-to-music system inspired by musicdomain knowledge which is able to generate music that follows certain music properties embedded within user-specified text prompts. The integration of the MuNet module within Mustango enables greater music controllability over stateof-the-art text-to-music systems such as Tango, AudioLDM2 and MusicGen. We also made our dataset MusicBench and model publicly available. MusicBench contains 11 times more data than the original MusicCaps dataset and includes text prompts that contain music-theory -based description and augmented music audio."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 2218
                },
                {
                    "x": 1603,
                    "y": 2218
                },
                {
                    "x": 1603,
                    "y": 2271
                },
                {
                    "x": 1269,
                    "y": 2271
                }
            ],
            "category": "paragraph",
            "html": "<p id='111' style='font-size:22px'>6 Limitations</p>",
            "id": 111,
            "page": 9,
            "text": "6 Limitations"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 2309
                },
                {
                    "x": 2196,
                    "y": 2309
                },
                {
                    "x": 2196,
                    "y": 2934
                },
                {
                    "x": 1268,
                    "y": 2934
                }
            ],
            "category": "paragraph",
            "html": "<p id='112' style='font-size:16px'>Our music generation method is limited to Western<br>music in terms of controllability since the control<br>information mentioned in the paper (e.g., chord,<br>key) might be missing or appear in a different form<br>in other non- Western music (e.g., Indian or Chinese<br>classical music). We also assumed the availability<br>of paired text captions of music, which was used to<br>train our model. Mustango is also currently limited<br>to generating music of up to ten seconds due to<br>computational constraints. Adapting Mustango for<br>generating long-form music is left for future work.</p>",
            "id": 112,
            "page": 9,
            "text": "Our music generation method is limited to Western music in terms of controllability since the control information mentioned in the paper (e.g., chord, key) might be missing or appear in a different form in other non- Western music (e.g., Indian or Chinese classical music). We also assumed the availability of paired text captions of music, which was used to train our model. Mustango is also currently limited to generating music of up to ten seconds due to computational constraints. Adapting Mustango for generating long-form music is left for future work."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 2975
                },
                {
                    "x": 1841,
                    "y": 2975
                },
                {
                    "x": 1841,
                    "y": 3030
                },
                {
                    "x": 1270,
                    "y": 3030
                }
            ],
            "category": "paragraph",
            "html": "<p id='113' style='font-size:22px'>7 Ethical Considerations</p>",
            "id": 113,
            "page": 9,
            "text": "7 Ethical Considerations"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 3068
                },
                {
                    "x": 2194,
                    "y": 3068
                },
                {
                    "x": 2194,
                    "y": 3237
                },
                {
                    "x": 1268,
                    "y": 3237
                }
            ],
            "category": "paragraph",
            "html": "<p id='114' style='font-size:18px'>Our training data is based on the MusicCaps<br>dataset (Agostinelli et al., 2023). The 5.5k music<br>samples in Music-Caps are sourced from Youtube</p>",
            "id": 114,
            "page": 9,
            "text": "Our training data is based on the MusicCaps dataset (Agostinelli , 2023). The 5.5k music samples in Music-Caps are sourced from Youtube"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 302
                },
                {
                    "x": 1212,
                    "y": 302
                },
                {
                    "x": 1212,
                    "y": 520
                },
                {
                    "x": 287,
                    "y": 520
                }
            ],
            "category": "paragraph",
            "html": "<p id='115' style='font-size:16px'>under Creative Commons license. We perform our<br>custom data augmentation strategies solely on this<br>dataset. We did not use any other privately-licensed<br>dataset.</p>",
            "id": 115,
            "page": 10,
            "text": "under Creative Commons license. We perform our custom data augmentation strategies solely on this dataset. We did not use any other privately-licensed dataset."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 529
                },
                {
                    "x": 1213,
                    "y": 529
                },
                {
                    "x": 1213,
                    "y": 753
                },
                {
                    "x": 287,
                    "y": 753
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='116' style='font-size:16px'>Our listening tests involved human annotators<br>for which the data collection protocol was approved<br>by an independent ethics review board. More de-<br>tails can be found in the Appendix G.</p>",
            "id": 116,
            "page": 10,
            "text": "Our listening tests involved human annotators for which the data collection protocol was approved by an independent ethics review board. More details can be found in the Appendix G."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 798
                },
                {
                    "x": 712,
                    "y": 798
                },
                {
                    "x": 712,
                    "y": 855
                },
                {
                    "x": 288,
                    "y": 855
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='117' style='font-size:22px'>Acknowledgements</p>",
            "id": 117,
            "page": 10,
            "text": "Acknowledgements"
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 887
                },
                {
                    "x": 1212,
                    "y": 887
                },
                {
                    "x": 1212,
                    "y": 1736
                },
                {
                    "x": 286,
                    "y": 1736
                }
            ],
            "category": "paragraph",
            "html": "<p id='118' style='font-size:18px'>This project has received funding from SUTD<br>MOE grant SKI 2021_04_06 and AcRF MoE Tier-<br>2 grant (Project no. T2MOE2008, and Grantor ref-<br>erence no. MOE-T2EP20220-0017) titled: \"CSK<br>NLP: Leveraging Commonsense Knowledge for<br>NLP\", for the support. This work is also supported<br>by the Microsoft Research Accelerate Foundation<br>Models Academic Research program. We also<br>thank Huggingface for sponsoring the live demo<br>on Huggingface Space. Zixun Guo is a research<br>student at the UKRI Centre for Doctoral Train-<br>ing in Artificial Intelligence and Music, supported<br>jointly by UK Research and Innovation [Grant num-<br>ber EP/S022694/1] and Queen Mary University of<br>London.</p>",
            "id": 118,
            "page": 10,
            "text": "This project has received funding from SUTD MOE grant SKI 2021_04_06 and AcRF MoE Tier2 grant (Project no. T2MOE2008, and Grantor reference no. MOE-T2EP20220-0017) titled: \"CSK NLP: Leveraging Commonsense Knowledge for NLP\", for the support. This work is also supported by the Microsoft Research Accelerate Foundation Models Academic Research program. We also thank Huggingface for sponsoring the live demo on Huggingface Space. Zixun Guo is a research student at the UKRI Centre for Doctoral Training in Artificial Intelligence and Music, supported jointly by UK Research and Innovation [Grant number EP/S022694/1] and Queen Mary University of London."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 1833
                },
                {
                    "x": 531,
                    "y": 1833
                },
                {
                    "x": 531,
                    "y": 1886
                },
                {
                    "x": 289,
                    "y": 1886
                }
            ],
            "category": "paragraph",
            "html": "<p id='119' style='font-size:20px'>References</p>",
            "id": 119,
            "page": 10,
            "text": "References"
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 1916
                },
                {
                    "x": 1214,
                    "y": 1916
                },
                {
                    "x": 1214,
                    "y": 2149
                },
                {
                    "x": 290,
                    "y": 2149
                }
            ],
            "category": "paragraph",
            "html": "<p id='120' style='font-size:14px'>Andrea Agostinelli, Timo I Denk, Zalan Borsos,<br>Jesse Engel, Mauro Verzetti, Antoine Caillon,<br>Qingqing Huang, Aren Jansen, Adam Roberts, Marco<br>Tagliasacchi, et al. 2023. Musiclm: Generating mu-<br>sic from text. arXiv preprint arXiv:2301.11325.</p>",
            "id": 120,
            "page": 10,
            "text": "Andrea Agostinelli, Timo I Denk, Zalan Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi,  2023. Musiclm: Generating music from text. arXiv preprint arXiv:2301.11325."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 2183
                },
                {
                    "x": 1214,
                    "y": 2183
                },
                {
                    "x": 1214,
                    "y": 2691
                },
                {
                    "x": 289,
                    "y": 2691
                }
            ],
            "category": "paragraph",
            "html": "<p id='121' style='font-size:16px'>Dmitry Bogdanov, Nicolas Wack, Emilia Gomez Gutier-<br>rez, Sankalp Gulati, Herrera Boyer, Oscar Mayor,<br>Gerard Roma Trepat, Justin Salamon, Jose Ricardo<br>Zapata Gonzalez, Xavier Serra, et al. 2013. Essen-<br>tia: An audio analysis library for music information<br>retrieval. In Britto A, Gouyon F, Dixon S, editors.<br>14th Conference of the International Society for Mu-<br>sic Information Retrieval (ISMIR); 2013 Nov 4-8;<br>Curitiba, Brazil. [place unknown]: ISMIR; 2013. p.<br>493-8. International Society for Music Information<br>Retrieval (ISMIR).</p>",
            "id": 121,
            "page": 10,
            "text": "Dmitry Bogdanov, Nicolas Wack, Emilia Gomez Gutierrez, Sankalp Gulati, Herrera Boyer, Oscar Mayor, Gerard Roma Trepat, Justin Salamon, Jose Ricardo Zapata Gonzalez, Xavier Serra,  2013. Essentia: An audio analysis library for music information retrieval. In Britto A, Gouyon F, Dixon S, editors. 14th Conference of the International Society for Music Information Retrieval (ISMIR); 2013 Nov 4-8; Curitiba, Brazil. [place unknown]: ISMIR; 2013. p. 493-8. International Society for Music Information Retrieval (ISMIR)."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 2729
                },
                {
                    "x": 1215,
                    "y": 2729
                },
                {
                    "x": 1215,
                    "y": 3054
                },
                {
                    "x": 289,
                    "y": 3054
                }
            ],
            "category": "paragraph",
            "html": "<p id='122' style='font-size:14px'>Zalan Borsos, Raphael Marinier, Damien Vincent,<br>Eugene Kharitonov, Olivier Pietquin, Matt Shar-<br>ifi, Dominik Roblek, Olivier Teboul, David Grang-<br>ier, Marco Tagliasacchi, et al. 2023. Audiolm: a<br>language modeling approach to audio generation.<br>IEEE/ACM Transactions on Audio, Speech, and Lan-<br>guage Processing.</p>",
            "id": 122,
            "page": 10,
            "text": "Zalan Borsos, Raphael Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Sharifi, Dominik Roblek, Olivier Teboul, David Grangier, Marco Tagliasacchi,  2023. Audiolm: a language modeling approach to audio generation. IEEE/ACM Transactions on Audio, Speech, and Language Processing."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 3092
                },
                {
                    "x": 1212,
                    "y": 3092
                },
                {
                    "x": 1212,
                    "y": 3235
                },
                {
                    "x": 290,
                    "y": 3235
                }
            ],
            "category": "paragraph",
            "html": "<p id='123' style='font-size:18px'>Hyung Won Chung, Le Hou, Shayne Longpre, Barret<br>Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi<br>Wang, Mostafa Dehghani, Siddhartha Brahma, et al.</p>",
            "id": 123,
            "page": 10,
            "text": "Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, "
        },
        {
            "bounding_box": [
                {
                    "x": 1314,
                    "y": 307
                },
                {
                    "x": 2195,
                    "y": 307
                },
                {
                    "x": 2195,
                    "y": 398
                },
                {
                    "x": 1314,
                    "y": 398
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='124' style='font-size:18px'>2022. Scaling instruction-finetuned language models.<br>arXiv preprint arXiv:2210.11416.</p>",
            "id": 124,
            "page": 10,
            "text": "2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 432
                },
                {
                    "x": 2195,
                    "y": 432
                },
                {
                    "x": 2195,
                    "y": 803
                },
                {
                    "x": 1271,
                    "y": 803
                }
            ],
            "category": "paragraph",
            "html": "<p id='125' style='font-size:18px'>Yu-An Chung, Yu Zhang, Wei Han, Chung-Cheng<br>Chiu, James Qin, Ruoming Pang, and Yonghui Wu.<br>2021. w2v-bert: Combining contrastive learning<br>and masked language modeling for self-supervised<br>speech pre-training. In IEEE Automatic Speech<br>Recognition and Understanding Workshop, ASRU<br>2021, Cartagena, Colombia, December 13-17, 2021,<br>pages 244-250. IEEE.</p>",
            "id": 125,
            "page": 10,
            "text": "Yu-An Chung, Yu Zhang, Wei Han, Chung-Cheng Chiu, James Qin, Ruoming Pang, and Yonghui Wu. 2021. w2v-bert: Combining contrastive learning and masked language modeling for self-supervised speech pre-training. In IEEE Automatic Speech Recognition and Understanding Workshop, ASRU 2021, Cartagena, Colombia, December 13-17, 2021, pages 244-250. IEEE."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 836
                },
                {
                    "x": 2193,
                    "y": 836
                },
                {
                    "x": 2193,
                    "y": 1022
                },
                {
                    "x": 1268,
                    "y": 1022
                }
            ],
            "category": "paragraph",
            "html": "<p id='126' style='font-size:14px'>Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David<br>Kant, Gabriel Synnaeve, Yossi Adi, and Alexandre<br>Defossez. 2023. Simple and controllable music gen-<br>eration. arXiv preprint arXiv:2306.05284.</p>",
            "id": 126,
            "page": 10,
            "text": "Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and Alexandre Defossez. 2023. Simple and controllable music generation. arXiv preprint arXiv:2306.05284."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 1056
                },
                {
                    "x": 2195,
                    "y": 1056
                },
                {
                    "x": 2195,
                    "y": 1196
                },
                {
                    "x": 1269,
                    "y": 1196
                }
            ],
            "category": "paragraph",
            "html": "<p id='127' style='font-size:14px'>Michael Defferrard, Kirell Benzi, Pierre Vandergheynst,<br>and Xavier Bresson. 2016. Fma: A dataset for music<br>analysis. arXiv preprint arXiv:1612.01840.</p>",
            "id": 127,
            "page": 10,
            "text": "Michael Defferrard, Kirell Benzi, Pierre Vandergheynst, and Xavier Bresson. 2016. Fma: A dataset for music analysis. arXiv preprint arXiv:1612.01840."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 1231
                },
                {
                    "x": 2193,
                    "y": 1231
                },
                {
                    "x": 2193,
                    "y": 1415
                },
                {
                    "x": 1269,
                    "y": 1415
                }
            ],
            "category": "paragraph",
            "html": "<p id='128' style='font-size:14px'>Josh Gardner, Simon Durand, Daniel Stoller, and<br>Rachel M Bittner. 2023. Llark: A multimodal<br>foundation model for music. arXiv preprint<br>arXiv:2310.07160.</p>",
            "id": 128,
            "page": 10,
            "text": "Josh Gardner, Simon Durand, Daniel Stoller, and Rachel M Bittner. 2023. Llark: A multimodal foundation model for music. arXiv preprint arXiv:2310.07160."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 1452
                },
                {
                    "x": 2196,
                    "y": 1452
                },
                {
                    "x": 2196,
                    "y": 1773
                },
                {
                    "x": 1271,
                    "y": 1773
                }
            ],
            "category": "paragraph",
            "html": "<p id='129' style='font-size:14px'>Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman,<br>Aren Jansen, Wade Lawrence, R Channing Moore,<br>Manoj Plakal, and Marvin Ritter. 2017. Audio set:<br>An ontology and human-labeled dataset for audio<br>events. In 2017 IEEE international conference on<br>acoustics, speech and signal processing (ICASSP),<br>pages 776-780. IEEE.</p>",
            "id": 129,
            "page": 10,
            "text": "Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore, Manoj Plakal, and Marvin Ritter. 2017. Audio set: An ontology and human-labeled dataset for audio events. In 2017 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 776-780. IEEE."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 1808
                },
                {
                    "x": 2195,
                    "y": 1808
                },
                {
                    "x": 2195,
                    "y": 1994
                },
                {
                    "x": 1270,
                    "y": 1994
                }
            ],
            "category": "paragraph",
            "html": "<p id='130' style='font-size:14px'>Deepanway Ghosal, Navonil Majumder, Ambuj<br>Mehrish, and Soujanya Poria. 2023. Text-to-audio<br>generation using instruction tuned llm and latent dif-<br>fusion model. arXiv preprint arXiv:2304.13731.</p>",
            "id": 130,
            "page": 10,
            "text": "Deepanway Ghosal, Navonil Majumder, Ambuj Mehrish, and Soujanya Poria. 2023. Text-to-audio generation using instruction tuned llm and latent diffusion model. arXiv preprint arXiv:2304.13731."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 2027
                },
                {
                    "x": 2195,
                    "y": 2027
                },
                {
                    "x": 2195,
                    "y": 2303
                },
                {
                    "x": 1269,
                    "y": 2303
                }
            ],
            "category": "paragraph",
            "html": "<p id='131' style='font-size:14px'>Zixun Guo, J. Kang, and D. Herremans. 2023. A<br>domain-knowledge-inspired music embedding space<br>and a novel attention mechanism for symbolic mu-<br>sic modeling. In Proc. of the 37th AAAI Conference<br>on Artificial Intelligence, Washington DC. AAAI,<br>AAAI.</p>",
            "id": 131,
            "page": 10,
            "text": "Zixun Guo, J. Kang, and D. Herremans. 2023. A domain-knowledge-inspired music embedding space and a novel attention mechanism for symbolic music modeling. In Proc. of the 37th AAAI Conference on Artificial Intelligence, Washington DC. AAAI, AAAI."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 2341
                },
                {
                    "x": 2196,
                    "y": 2341
                },
                {
                    "x": 2196,
                    "y": 2571
                },
                {
                    "x": 1270,
                    "y": 2571
                }
            ],
            "category": "paragraph",
            "html": "<p id='132' style='font-size:18px'>Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2022.<br>Debertav3: Improving deberta using electra-style pre-<br>training with gradient-disentangled embedding shar-<br>ing. In The Eleventh International Conference on<br>Learning Representations.</p>",
            "id": 132,
            "page": 10,
            "text": "Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2022. Debertav3: Improving deberta using electra-style pretraining with gradient-disentangled embedding sharing. In The Eleventh International Conference on Learning Representations."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 2606
                },
                {
                    "x": 2197,
                    "y": 2606
                },
                {
                    "x": 2197,
                    "y": 2787
                },
                {
                    "x": 1271,
                    "y": 2787
                }
            ],
            "category": "paragraph",
            "html": "<p id='133' style='font-size:14px'>Dorien Herremans, Ching-Hua Chuan, and Elaine Chew.<br>2017. A functional taxonomy of music generation<br>systems. ACM Computing Surveys (CSUR), 50(5):1-<br>30.</p>",
            "id": 133,
            "page": 10,
            "text": "Dorien Herremans, Ching-Hua Chuan, and Elaine Chew. 2017. A functional taxonomy of music generation systems. ACM Computing Surveys (CSUR), 50(5):130."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 2827
                },
                {
                    "x": 2196,
                    "y": 2827
                },
                {
                    "x": 2196,
                    "y": 2966
                },
                {
                    "x": 1270,
                    "y": 2966
                }
            ],
            "category": "paragraph",
            "html": "<p id='134' style='font-size:16px'>Mojtaba Heydari, Frank Cwitkowitz, and Zhiyao Duan.<br>2021. Beatnet: Crnn and particle filtering for online<br>joint beat downbeat and meter tracking.</p>",
            "id": 134,
            "page": 10,
            "text": "Mojtaba Heydari, Frank Cwitkowitz, and Zhiyao Duan. 2021. Beatnet: Crnn and particle filtering for online joint beat downbeat and meter tracking."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 3001
                },
                {
                    "x": 2196,
                    "y": 3001
                },
                {
                    "x": 2196,
                    "y": 3229
                },
                {
                    "x": 1272,
                    "y": 3229
                }
            ],
            "category": "paragraph",
            "html": "<p id='135' style='font-size:14px'>Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020.<br>Denoising diffusion probabilistic models. In Ad-<br>vances in Neural Information Processing Systems,<br>volume 33, pages 6840-6851. Curran Associates,<br>Inc.</p>",
            "id": 135,
            "page": 10,
            "text": "Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems, volume 33, pages 6840-6851. Curran Associates, Inc."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 304
                },
                {
                    "x": 1214,
                    "y": 304
                },
                {
                    "x": 1214,
                    "y": 627
                },
                {
                    "x": 288,
                    "y": 627
                }
            ],
            "category": "paragraph",
            "html": "<p id='136' style='font-size:18px'>Qingqing Huang, Aren Jansen, Joonseok Lee, Ravi<br>Ganti, Judith Yue Li, and Daniel P. W. Ellis. 2022.<br>Mulan: A joint embedding of music audio and natural<br>language. In Proceedings of the 23rd International<br>Society for Music Information Retrieval Conference,<br>ISMIR 2022, Bengaluru, India, December 4-8, 2022,<br>pages 559-566.</p>",
            "id": 136,
            "page": 11,
            "text": "Qingqing Huang, Aren Jansen, Joonseok Lee, Ravi Ganti, Judith Yue Li, and Daniel P. W. Ellis. 2022. Mulan: A joint embedding of music audio and natural language. In Proceedings of the 23rd International Society for Music Information Retrieval Conference, ISMIR 2022, Bengaluru, India, December 4-8, 2022, pages 559-566."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 661
                },
                {
                    "x": 1213,
                    "y": 661
                },
                {
                    "x": 1213,
                    "y": 937
                },
                {
                    "x": 289,
                    "y": 937
                }
            ],
            "category": "paragraph",
            "html": "<p id='137' style='font-size:18px'>Qingqing Huang, Daniel S Park, Tao Wang, Timo I<br>Denk, Andy Ly, Nanxin Chen, Zhengdong Zhang,<br>Zhishuai Zhang, Jiahui Yu, Christian Frank, et al.<br>2023. Noise2music: Text-conditioned music gen-<br>eration with diffusion models. arXiv preprint<br>arXiv:2302.03917.</p>",
            "id": 137,
            "page": 11,
            "text": "Qingqing Huang, Daniel S Park, Tao Wang, Timo I Denk, Andy Ly, Nanxin Chen, Zhengdong Zhang, Zhishuai Zhang, Jiahui Yu, Christian Frank,  2023. Noise2music: Text-conditioned music generation with diffusion models. arXiv preprint arXiv:2302.03917."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 972
                },
                {
                    "x": 1214,
                    "y": 972
                },
                {
                    "x": 1214,
                    "y": 1201
                },
                {
                    "x": 290,
                    "y": 1201
                }
            ],
            "category": "paragraph",
            "html": "<p id='138' style='font-size:14px'>Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and<br>Matthew Sharifi. 2019. Frechet audio distance: A<br>reference-free metric for evaluating music enhance-<br>ment algorithms. In INTERSPEECH, pages 2350-<br>2354.</p>",
            "id": 138,
            "page": 11,
            "text": "Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and Matthew Sharifi. 2019. Frechet audio distance: A reference-free metric for evaluating music enhancement algorithms. In INTERSPEECH, pages 23502354."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 1238
                },
                {
                    "x": 1213,
                    "y": 1238
                },
                {
                    "x": 1213,
                    "y": 1425
                },
                {
                    "x": 289,
                    "y": 1425
                }
            ],
            "category": "paragraph",
            "html": "<p id='139' style='font-size:16px'>Peike Li, Boyu Chen, Yao Yao, Yikai Wang, Allen<br>Wang, and Alex Wang. 2023. Jen-1: Text-guided<br>universal music generation with omnidirectional dif-<br>fusion models. arXiv preprint arXiv:2308.04729.</p>",
            "id": 139,
            "page": 11,
            "text": "Peike Li, Boyu Chen, Yao Yao, Yikai Wang, Allen Wang, and Alex Wang. 2023. Jen-1: Text-guided universal music generation with omnidirectional diffusion models. arXiv preprint arXiv:2308.04729."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 1460
                },
                {
                    "x": 1213,
                    "y": 1460
                },
                {
                    "x": 1213,
                    "y": 1689
                },
                {
                    "x": 290,
                    "y": 1689
                }
            ],
            "category": "paragraph",
            "html": "<p id='140' style='font-size:14px'>Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo<br>Liu, Danilo Mandic, Wenwu Wang, and Mark D<br>Plumbley. 2023a. Audioldm: Text-to-audio gener-<br>ation with latent diffusion models. arXiv preprint<br>arXiv:2301.12503.</p>",
            "id": 140,
            "page": 11,
            "text": "Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark D Plumbley. 2023a. Audioldm: Text-to-audio generation with latent diffusion models. arXiv preprint arXiv:2301.12503."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 1725
                },
                {
                    "x": 1213,
                    "y": 1725
                },
                {
                    "x": 1213,
                    "y": 2001
                },
                {
                    "x": 290,
                    "y": 2001
                }
            ],
            "category": "paragraph",
            "html": "<p id='141' style='font-size:18px'>Haohe Liu, Qiao Tian, Yi Yuan, Xubo Liu, Xin-<br>hao Mei, Qiuqiang Kong, Yuping Wang, Wenwu<br>Wang, Yuxuan Wang, and Mark D Plumbley. 2023b.<br>Audioldm 2: Learning holistic audio generation<br>with self-supervised pretraining. arXiv preprint<br>arXiv:2308.05734.</p>",
            "id": 141,
            "page": 11,
            "text": "Haohe Liu, Qiao Tian, Yi Yuan, Xubo Liu, Xinhao Mei, Qiuqiang Kong, Yuping Wang, Wenwu Wang, Yuxuan Wang, and Mark D Plumbley. 2023b. Audioldm 2: Learning holistic audio generation with self-supervised pretraining. arXiv preprint arXiv:2308.05734."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 2035
                },
                {
                    "x": 1213,
                    "y": 2035
                },
                {
                    "x": 1213,
                    "y": 2175
                },
                {
                    "x": 289,
                    "y": 2175
                }
            ],
            "category": "paragraph",
            "html": "<p id='142' style='font-size:18px'>Ilya Loshchilov and Frank Hutter. 2017. Decou-<br>pled weight decay regularization. arXiv preprint<br>arXiv:1711.05101.</p>",
            "id": 142,
            "page": 11,
            "text": "Ilya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 2210
                },
                {
                    "x": 1213,
                    "y": 2210
                },
                {
                    "x": 1213,
                    "y": 2534
                },
                {
                    "x": 289,
                    "y": 2534
                }
            ],
            "category": "paragraph",
            "html": "<p id='143' style='font-size:18px'>Matthias Mauch and Simon Dixon. 2010. Approximate<br>note transcription for the improved identification of<br>difficult chords. In Proceedings of the 11th Interna-<br>tional Society for Music Information Retrieval Con-<br>ference, ISMIR 2010, Utrecht, Netherlands, August<br>9-13, 2010, pages 135-140. International Society for<br>Music Information Retrieval.</p>",
            "id": 143,
            "page": 11,
            "text": "Matthias Mauch and Simon Dixon. 2010. Approximate note transcription for the improved identification of difficult chords. In Proceedings of the 11th International Society for Music Information Retrieval Conference, ISMIR 2010, Utrecht, Netherlands, August 9-13, 2010, pages 135-140. International Society for Music Information Retrieval."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 2568
                },
                {
                    "x": 767,
                    "y": 2568
                },
                {
                    "x": 767,
                    "y": 2616
                },
                {
                    "x": 288,
                    "y": 2616
                }
            ],
            "category": "paragraph",
            "html": "<p id='144' style='font-size:20px'>OpenAI. 2023a. DALL.E2.</p>",
            "id": 144,
            "page": 11,
            "text": "OpenAI. 2023a. DALL.E2."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 2652
                },
                {
                    "x": 953,
                    "y": 2652
                },
                {
                    "x": 953,
                    "y": 2700
                },
                {
                    "x": 289,
                    "y": 2700
                }
            ],
            "category": "paragraph",
            "html": "<p id='145' style='font-size:22px'>OpenAI. 2023b. Introducing ChatGPT.</p>",
            "id": 145,
            "page": 11,
            "text": "OpenAI. 2023b. Introducing ChatGPT."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 2734
                },
                {
                    "x": 1214,
                    "y": 2734
                },
                {
                    "x": 1214,
                    "y": 2966
                },
                {
                    "x": 291,
                    "y": 2966
                }
            ],
            "category": "paragraph",
            "html": "<p id='146' style='font-size:16px'>Marco Pasini and Jan Schluter. 2022. Musika! fast infi-<br>nite waveform music generation. In Proceedings of<br>the 23rd International Society for Music Information<br>Retrieval Conference, ISMIR 2022, Bengaluru, India,<br>December 4-8, 2022, pages 543-550.</p>",
            "id": 146,
            "page": 11,
            "text": "Marco Pasini and Jan Schluter. 2022. Musika! fast infinite waveform music generation. In Proceedings of the 23rd International Society for Music Information Retrieval Conference, ISMIR 2022, Bengaluru, India, December 4-8, 2022, pages 543-550."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 3001
                },
                {
                    "x": 1213,
                    "y": 3001
                },
                {
                    "x": 1213,
                    "y": 3233
                },
                {
                    "x": 289,
                    "y": 3233
                }
            ],
            "category": "paragraph",
            "html": "<p id='147' style='font-size:18px'>Olaf Ronneberger, Philipp Fischer, and Thomas Brox.<br>2015. U-net: Convolutional networks for biomedical<br>image segmentation. In Medical Image Computing<br>and Computer-Assisted Intervention-MICCAI 2015:<br>18th International Conference, Munich, Germany,</p>",
            "id": 147,
            "page": 11,
            "text": "Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention-MICCAI 2015: 18th International Conference, Munich, Germany,"
        },
        {
            "bounding_box": [
                {
                    "x": 1314,
                    "y": 304
                },
                {
                    "x": 2190,
                    "y": 304
                },
                {
                    "x": 2190,
                    "y": 399
                },
                {
                    "x": 1314,
                    "y": 399
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='148' style='font-size:18px'>October 5-9, 2015, Proceedings, Part III 18, pages<br>234-241. Springer.</p>",
            "id": 148,
            "page": 11,
            "text": "October 5-9, 2015, Proceedings, Part III 18, pages 234-241. Springer."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 433
                },
                {
                    "x": 2195,
                    "y": 433
                },
                {
                    "x": 2195,
                    "y": 618
                },
                {
                    "x": 1270,
                    "y": 618
                }
            ],
            "category": "paragraph",
            "html": "<p id='149' style='font-size:16px'>Flavio Schneider, Zhijing Jin, and Bernhard Scholkopf.<br>2023. Mo\\^ usai: Text-to-music generation<br>with long-context latent diffusion. arXiv preprint<br>arXiv:2301.11757.</p>",
            "id": 149,
            "page": 11,
            "text": "Flavio Schneider, Zhijing Jin, and Bernhard Scholkopf. 2023. Mo\\^ usai: Text-to-music generation with long-context latent diffusion. arXiv preprint arXiv:2301.11757."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 653
                },
                {
                    "x": 2193,
                    "y": 653
                },
                {
                    "x": 2193,
                    "y": 792
                },
                {
                    "x": 1270,
                    "y": 792
                }
            ],
            "category": "paragraph",
            "html": "<p id='150' style='font-size:18px'>Gijsbert Stoet. 2010. Psytoolkit: A software package<br>for programming psychological experiments using<br>linux. Behavior research methods, 42:1096-1104.</p>",
            "id": 150,
            "page": 11,
            "text": "Gijsbert Stoet. 2010. Psytoolkit: A software package for programming psychological experiments using linux. Behavior research methods, 42:1096-1104."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 827
                },
                {
                    "x": 2196,
                    "y": 827
                },
                {
                    "x": 2196,
                    "y": 1058
                },
                {
                    "x": 1271,
                    "y": 1058
                }
            ],
            "category": "paragraph",
            "html": "<p id='151' style='font-size:16px'>Kun Su, Judith Yue Li, Qingqing Huang, Dima Kuzmin,<br>Joonseok Lee, Chris Donahue, Fei Sha, Aren Jansen,<br>Yu Wang, Mauro Verzetti, et al. 2023. V2meow:<br>Meowing to the visual beat via music generation.<br>arXiv preprint arXiv:2305.06594.</p>",
            "id": 151,
            "page": 11,
            "text": "Kun Su, Judith Yue Li, Qingqing Huang, Dima Kuzmin, Joonseok Lee, Chris Donahue, Fei Sha, Aren Jansen, Yu Wang, Mauro Verzetti,  2023. V2meow: Meowing to the visual beat via music generation. arXiv preprint arXiv:2305.06594."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 1094
                },
                {
                    "x": 2196,
                    "y": 1094
                },
                {
                    "x": 2196,
                    "y": 1323
                },
                {
                    "x": 1271,
                    "y": 1323
                }
            ],
            "category": "paragraph",
            "html": "<p id='152' style='font-size:18px'>Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam<br>Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,<br>Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.<br>2022. Lamda: Language models for dialog applica-<br>tions. arXiv preprint arXiv:2201.08239.</p>",
            "id": 152,
            "page": 11,
            "text": "Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,  2022. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 1358
                },
                {
                    "x": 2194,
                    "y": 1358
                },
                {
                    "x": 2194,
                    "y": 1589
                },
                {
                    "x": 1271,
                    "y": 1589
                }
            ],
            "category": "paragraph",
            "html": "<p id='153' style='font-size:14px'>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob<br>Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz<br>Kaiser, and Illia Polosukhin. 2017. Attention is all<br>you need. Advances in neural information processing<br>systems, 30.</p>",
            "id": 153,
            "page": 11,
            "text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 1625
                },
                {
                    "x": 2197,
                    "y": 1625
                },
                {
                    "x": 2197,
                    "y": 1947
                },
                {
                    "x": 1273,
                    "y": 1947
                }
            ],
            "category": "paragraph",
            "html": "<p id='154' style='font-size:18px'>Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Tay-<br>lor Berg-Kirkpatrick, and Shlomo Dubnov. 2023.<br>Large-scale contrastive language-audio pretraining<br>with feature fusion and keyword-to-caption augmen-<br>tation. In ICASSP 2023-2023 IEEE International<br>Conference on Acoustics, Speech and Signal Process-<br>ing (ICASSP), pages 1-5. IEEE.</p>",
            "id": 154,
            "page": 11,
            "text": "Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, and Shlomo Dubnov. 2023. Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1-5. IEEE."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1981
                },
                {
                    "x": 2196,
                    "y": 1981
                },
                {
                    "x": 2196,
                    "y": 2209
                },
                {
                    "x": 1272,
                    "y": 2209
                }
            ],
            "category": "paragraph",
            "html": "<p id='155' style='font-size:16px'>Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan<br>Skoglund, and Marco Tagliasacchi. 2022. Sound-<br>stream: An end-to-end neural audio codec. IEEE<br>ACM Trans. Audio Speech Lang. Process., 30:495-<br>507.</p>",
            "id": 155,
            "page": 11,
            "text": "Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. 2022. Soundstream: An end-to-end neural audio codec. IEEE ACM Trans. Audio Speech Lang. Process., 30:495507."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 2248
                },
                {
                    "x": 2196,
                    "y": 2248
                },
                {
                    "x": 2196,
                    "y": 2433
                },
                {
                    "x": 1271,
                    "y": 2433
                }
            ],
            "category": "paragraph",
            "html": "<p id='156' style='font-size:16px'>Pengfei Zhu, Chao Pang, Shuohuan Wang, Yekun Chai,<br>Yu Sun, Hao Tian, and Hua Wu. 2023. Ernie-music:<br>Text-to-waveform music generation with diffusion<br>models. ArXiv, abs/2302.04456.</p>",
            "id": 156,
            "page": 11,
            "text": "Pengfei Zhu, Chao Pang, Shuohuan Wang, Yekun Chai, Yu Sun, Hao Tian, and Hua Wu. 2023. Ernie-music: Text-to-waveform music generation with diffusion models. ArXiv, abs/2302.04456."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 300
                },
                {
                    "x": 935,
                    "y": 300
                },
                {
                    "x": 935,
                    "y": 352
                },
                {
                    "x": 289,
                    "y": 352
                }
            ],
            "category": "paragraph",
            "html": "<p id='157' style='font-size:18px'>A Reverse Diffusion Process</p>",
            "id": 157,
            "page": 12,
            "text": "A Reverse Diffusion Process"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 388
                },
                {
                    "x": 1454,
                    "y": 388
                },
                {
                    "x": 1454,
                    "y": 439
                },
                {
                    "x": 288,
                    "y": 439
                }
            ],
            "category": "paragraph",
            "html": "<p id='158' style='font-size:14px'>The reverse process to iteratively reconstruct zo is as following:</p>",
            "id": 158,
            "page": 12,
            "text": "The reverse process to iteratively reconstruct zo is as following:"
        },
        {
            "bounding_box": [
                {
                    "x": 285,
                    "y": 1107
                },
                {
                    "x": 2191,
                    "y": 1107
                },
                {
                    "x": 2191,
                    "y": 1221
                },
                {
                    "x": 285,
                    "y": 1221
                }
            ],
            "category": "paragraph",
            "html": "<p id='159' style='font-size:18px'>where w is the guidance scale in Eq. (10) used during inference. During training however, E Co (zn, C) is<br>directly used for noise estimation where the conditions C are randomly dropped as specified in §4.2.</p>",
            "id": 159,
            "page": 12,
            "text": "where w is the guidance scale in Eq. (10) used during inference. During training however, E Co (zn, C) is directly used for noise estimation where the conditions C are randomly dropped as specified in §4.2."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 1262
                },
                {
                    "x": 727,
                    "y": 1262
                },
                {
                    "x": 727,
                    "y": 1317
                },
                {
                    "x": 290,
                    "y": 1317
                }
            ],
            "category": "paragraph",
            "html": "<p id='160' style='font-size:20px'>B Training Details</p>",
            "id": 160,
            "page": 12,
            "text": "B Training Details"
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 1351
                },
                {
                    "x": 2189,
                    "y": 1351
                },
                {
                    "x": 2189,
                    "y": 1461
                },
                {
                    "x": 286,
                    "y": 1461
                }
            ],
            "category": "paragraph",
            "html": "<p id='161' style='font-size:18px'>To further improve the robustness of the classifier-free guidance in Mustango, we use these three dropouts<br>during training:</p>",
            "id": 161,
            "page": 12,
            "text": "To further improve the robustness of the classifier-free guidance in Mustango, we use these three dropouts during training:"
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 1493
                },
                {
                    "x": 2194,
                    "y": 1493
                },
                {
                    "x": 2194,
                    "y": 1834
                },
                {
                    "x": 286,
                    "y": 1834
                }
            ],
            "category": "paragraph",
            "html": "<p id='162' style='font-size:16px'>1. With 5% probability, drop all the inputs (text, beats, and chords);<br>2. With 5% probability, drop an input feature (applied to each of the inputs separately);<br>3. We determine the probability of masking a prompt as min(100, 10시)%, where N represents the<br>number of sentences in the current prompt, and M is the average number of sentences per prompt. Once a<br>prompt is chosen for masking, we randomly draw an integer X from a uniform distribution in the range<br>[20, 50] and proceed to remove X% of the input sentences in the prompt.</p>",
            "id": 162,
            "page": 12,
            "text": "1. With 5% probability, drop all the inputs (text, beats, and chords); 2. With 5% probability, drop an input feature (applied to each of the inputs separately); 3. We determine the probability of masking a prompt as min(100, 10시)%, where N represents the number of sentences in the current prompt, and M is the average number of sentences per prompt. Once a prompt is chosen for masking, we randomly draw an integer X from a uniform distribution in the range  and proceed to remove X% of the input sentences in the prompt."
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 1865
                },
                {
                    "x": 2195,
                    "y": 1865
                },
                {
                    "x": 2195,
                    "y": 2088
                },
                {
                    "x": 286,
                    "y": 2088
                }
            ],
            "category": "paragraph",
            "html": "<p id='163' style='font-size:16px'>The idea behind the first two dropouts is to enable the model to work with incomplete, faulty, or missing<br>input information. The third dropout is aimed at improving robustness for short text inputs. We apply<br>these dropouts to Tango as well, with a small modification: since Tango does not use music feature inputs,<br>we replace the first two dropouts with a single 10 % probability of dropping all text.</p>",
            "id": 163,
            "page": 12,
            "text": "The idea behind the first two dropouts is to enable the model to work with incomplete, faulty, or missing input information. The third dropout is aimed at improving robustness for short text inputs. We apply these dropouts to Tango as well, with a small modification: since Tango does not use music feature inputs, we replace the first two dropouts with a single 10 % probability of dropping all text."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 2094
                },
                {
                    "x": 2196,
                    "y": 2094
                },
                {
                    "x": 2196,
                    "y": 2204
                },
                {
                    "x": 288,
                    "y": 2204
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='164' style='font-size:16px'>To train Mustango and Tango baselines, we used various GPU resources: 4 Nvidia Tesla V100 GPUs,<br>and 8 Quadro RTX 8000 GPUs. Training time ranged from 5 to 10 days with effective batch size of 32.</p>",
            "id": 164,
            "page": 12,
            "text": "To train Mustango and Tango baselines, we used various GPU resources: 4 Nvidia Tesla V100 GPUs, and 8 Quadro RTX 8000 GPUs. Training time ranged from 5 to 10 days with effective batch size of 32."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 2244
                },
                {
                    "x": 1026,
                    "y": 2244
                },
                {
                    "x": 1026,
                    "y": 2301
                },
                {
                    "x": 289,
                    "y": 2301
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='165' style='font-size:20px'>C Performance of the Predictors</p>",
            "id": 165,
            "page": 12,
            "text": "C Performance of the Predictors"
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 2333
                },
                {
                    "x": 2194,
                    "y": 2333
                },
                {
                    "x": 2194,
                    "y": 2670
                },
                {
                    "x": 286,
                    "y": 2670
                }
            ],
            "category": "paragraph",
            "html": "<p id='166' style='font-size:16px'>During the inference phase, we utilize pre-trained predictors for chord and beat predictions based on textual<br>prompts. These predictors exhibit exceptional performance when the prompts explicitly contain chord and<br>beat information, achieving accuracy of 94.5 % on the TestB dataset. However, our interest extends to<br>evaluating their performance in scenarios where control sentences are absent from the prompt-essentially,<br>do these predictors generate noisy chords and beats? The concern is that such noise might propagate from<br>the predictors to Mustango, significantly impacting the overall quality of the generated music.</p>",
            "id": 166,
            "page": 12,
            "text": "During the inference phase, we utilize pre-trained predictors for chord and beat predictions based on textual prompts. These predictors exhibit exceptional performance when the prompts explicitly contain chord and beat information, achieving accuracy of 94.5 % on the TestB dataset. However, our interest extends to evaluating their performance in scenarios where control sentences are absent from the prompt-essentially, do these predictors generate noisy chords and beats? The concern is that such noise might propagate from the predictors to Mustango, significantly impacting the overall quality of the generated music."
        },
        {
            "bounding_box": [
                {
                    "x": 285,
                    "y": 2674
                },
                {
                    "x": 2194,
                    "y": 2674
                },
                {
                    "x": 2194,
                    "y": 3005
                },
                {
                    "x": 285,
                    "y": 3005
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='167' style='font-size:16px'>In our experiments, TestA serves as a scenario where control sentences are not included in the textual<br>prompts. Upon comparing the performance (Table 1) of Tango and Mustango on TestA, we observe<br>that the latter outperforms the former across most metrics. This observation indicates that the control<br>predictors do not compromise the performance of Mustango relative to Tango. The adaptability of these<br>predictors to specific themes or styles in the absence of control sentences remains a potential avenue for<br>future exploration, a topic we briefly touch upon below.</p>",
            "id": 167,
            "page": 12,
            "text": "In our experiments, TestA serves as a scenario where control sentences are not included in the textual prompts. Upon comparing the performance (Table 1) of Tango and Mustango on TestA, we observe that the latter outperforms the former across most metrics. This observation indicates that the control predictors do not compromise the performance of Mustango relative to Tango. The adaptability of these predictors to specific themes or styles in the absence of control sentences remains a potential avenue for future exploration, a topic we briefly touch upon below."
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 3014
                },
                {
                    "x": 2194,
                    "y": 3014
                },
                {
                    "x": 2194,
                    "y": 3238
                },
                {
                    "x": 286,
                    "y": 3238
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='168' style='font-size:16px'>First, we investigate the effect of the Chord predictor on the generated output in a little comparison<br>experiment. We take both TestA and TestB samples synthesized by Mustango and extract features from<br>them. Then, we evaluate the chord control metrics of PCM, ECM, CMO, and CMOT using chords<br>predicted by chord predictor VS chords detected in the audio from feature extraction. The metrics on TestA</p>",
            "id": 168,
            "page": 12,
            "text": "First, we investigate the effect of the Chord predictor on the generated output in a little comparison experiment. We take both TestA and TestB samples synthesized by Mustango and extract features from them. Then, we evaluate the chord control metrics of PCM, ECM, CMO, and CMOT using chords predicted by chord predictor VS chords detected in the audio from feature extraction. The metrics on TestA"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 295
                },
                {
                    "x": 2195,
                    "y": 295
                },
                {
                    "x": 2195,
                    "y": 561
                },
                {
                    "x": 288,
                    "y": 561
                }
            ],
            "category": "paragraph",
            "html": "<p id='169' style='font-size:14px'>are PCM - 16.15, ECM - 33.95, CMO - 39.81, and CMOT - 47.82. The metrics on TestB are PCM - 17.75,<br>ECM - 32.07, CMO - 47.36, and CMOT - 66.80. These results show that Mustango tends to follow the<br>chords predicted by the chord predictor quite often. While the results on TestA are a bit lower than on<br>TestB, they are still higher than Tango results on TestB as shown in Table 2.<br>Second, we take a look at some specific examples:</p>",
            "id": 169,
            "page": 13,
            "text": "are PCM - 16.15, ECM - 33.95, CMO - 39.81, and CMOT - 47.82. The metrics on TestB are PCM - 17.75, ECM - 32.07, CMO - 47.36, and CMOT - 66.80. These results show that Mustango tends to follow the chords predicted by the chord predictor quite often. While the results on TestA are a bit lower than on TestB, they are still higher than Tango results on TestB as shown in Table 2. Second, we take a look at some specific examples:"
        },
        {
            "bounding_box": [
                {
                    "x": 329,
                    "y": 600
                },
                {
                    "x": 2150,
                    "y": 600
                },
                {
                    "x": 2150,
                    "y": 939
                },
                {
                    "x": 329,
                    "y": 939
                }
            ],
            "category": "paragraph",
            "html": "<p id='170' style='font-size:20px'>Prompt: \"This folk song features a female voice singing the main melody. This is accompanied by a<br>tabla playing the percussion. A guitar strums chords. For most parts of the song, only one chord is<br>played. At the last bar, a different chord is played. This song has minimal instruments. This song<br>has a story-telling mood. This song can be played in a village scene in an Indian movie. The chord<br>sequence is Bbm, Ab. The beat is 3. The tempo of this song is Allegro. The key of this song is Bb<br>minor.\"</p>",
            "id": 170,
            "page": 13,
            "text": "Prompt: \"This folk song features a female voice singing the main melody. This is accompanied by a tabla playing the percussion. A guitar strums chords. For most parts of the song, only one chord is played. At the last bar, a different chord is played. This song has minimal instruments. This song has a story-telling mood. This song can be played in a village scene in an Indian movie. The chord sequence is Bbm, Ab. The beat is 3. The tempo of this song is Allegro. The key of this song is Bb minor.\""
        },
        {
            "bounding_box": [
                {
                    "x": 328,
                    "y": 944
                },
                {
                    "x": 2151,
                    "y": 944
                },
                {
                    "x": 2151,
                    "y": 1164
                },
                {
                    "x": 328,
                    "y": 1164
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='171' style='font-size:16px'>Without control sentences in italics (TestA): chords predicted: [\"G\" , \"C\" , \"G\" \"C\" \"G\" \"C\"],<br>, , ,<br>chords predicted time: [0.46, 1.21, 3.25, 5.48, 7.24, 8.92]. chords extracted from audio: [\"G6\",<br>\"C\", \"G\" \"C\" \"G\" \"Cmaj7\"], chords time extracted from audio: [0.46, 1.58, 3.07, 5.94, 7.62,<br>,<br>, ,<br>9.66]</p>",
            "id": 171,
            "page": 13,
            "text": "Without control sentences in italics (TestA): chords predicted: [\"G\" , \"C\" , \"G\" \"C\" \"G\" \"C\"], , , , chords predicted time: [0.46, 1.21, 3.25, 5.48, 7.24, 8.92]. chords extracted from audio: [\"G6\", \"C\", \"G\" \"C\" \"G\" \"Cmaj7\"], chords time extracted from audio: [0.46, 1.58, 3.07, 5.94, 7.62, , , , 9.66]"
        },
        {
            "bounding_box": [
                {
                    "x": 330,
                    "y": 1171
                },
                {
                    "x": 2153,
                    "y": 1171
                },
                {
                    "x": 2153,
                    "y": 1338
                },
                {
                    "x": 330,
                    "y": 1338
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='172' style='font-size:20px'>With control sentences in italics (TestB): chords predicted: [\"Bbm\" , \"Ab\"], chords predicted<br>time: [0.46, 7.24], chords extracted from audio: [\"F#maj7\", \"Ab\"], chords time extracted from<br>audio: [0.46, 7.43].</p>",
            "id": 172,
            "page": 13,
            "text": "With control sentences in italics (TestB): chords predicted: [\"Bbm\" , \"Ab\"], chords predicted time: [0.46, 7.24], chords extracted from audio: [\"F#maj7\", \"Ab\"], chords time extracted from audio: [0.46, 7.43]."
        },
        {
            "bounding_box": [
                {
                    "x": 325,
                    "y": 1392
                },
                {
                    "x": 2154,
                    "y": 1392
                },
                {
                    "x": 2154,
                    "y": 1675
                },
                {
                    "x": 325,
                    "y": 1675
                }
            ],
            "category": "paragraph",
            "html": "<p id='173' style='font-size:20px'>Prompt: \"A female singer sings this bluesy melody. The song is medium tempo with minimal guitar<br>accompaniment and no other instrumentation. The song's medium tempo is very emotional and<br>passionate. The song is a modern pop hit but with poor audio quality. The key of this song is G minor.<br>The time signature is 3/4. This song goes at 168.0 beats per minute. The chord progression in this<br>song is Am7, G7, Cm, G, A7.\"</p>",
            "id": 173,
            "page": 13,
            "text": "Prompt: \"A female singer sings this bluesy melody. The song is medium tempo with minimal guitar accompaniment and no other instrumentation. The song's medium tempo is very emotional and passionate. The song is a modern pop hit but with poor audio quality. The key of this song is G minor. The time signature is 3/4. This song goes at 168.0 beats per minute. The chord progression in this song is Am7, G7, Cm, G, A7.\""
        },
        {
            "bounding_box": [
                {
                    "x": 330,
                    "y": 1679
                },
                {
                    "x": 2152,
                    "y": 1679
                },
                {
                    "x": 2152,
                    "y": 1844
                },
                {
                    "x": 330,
                    "y": 1844
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='174' style='font-size:20px'>Without control sentences in italics (TestA): chords predicted: [\"C#m7\", \"C#m7\", \"C#m7\",<br>\"C#m7\" , C#m7\"], chords predicted time: [0.46, 3.25, 6.32, 8.17, 9.29], chords extracted from<br>audio: [\"F#\", , \"C#m \" \"F#m\" , \"C#m7\"], chords time extracted from audio: [0.46, 1.21, 4.55, 5.39]<br>,</p>",
            "id": 174,
            "page": 13,
            "text": "Without control sentences in italics (TestA): chords predicted: [\"C#m7\", \"C#m7\", \"C#m7\", \"C#m7\" , C#m7\"], chords predicted time: [0.46, 3.25, 6.32, 8.17, 9.29], chords extracted from audio: [\"F#\", , \"C#m \" \"F#m\" , \"C#m7\"], chords time extracted from audio: [0.46, 1.21, 4.55, 5.39] ,"
        },
        {
            "bounding_box": [
                {
                    "x": 328,
                    "y": 1852
                },
                {
                    "x": 2154,
                    "y": 1852
                },
                {
                    "x": 2154,
                    "y": 2069
                },
                {
                    "x": 328,
                    "y": 2069
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='175' style='font-size:20px'>With control sentences in italics (TestB): chords predicted: [\"Am7\", \"G7\" , \"Cm\" , \"G\" \" A7\"],<br>,<br>chords predicted time: [0.46, 1.67, 3.53, 5.48, 8.92], chords extracted from audio: [\"Am \" \"G\",<br>,<br>\"A6\", \" Gmaj7\"], chords time extracted from audio: [0.46, 1.67, 3.72, 5.94, 8.73,<br>\"C\", \"Gmaj7\" ,<br>9.85]</p>",
            "id": 175,
            "page": 13,
            "text": "With control sentences in italics (TestB): chords predicted: [\"Am7\", \"G7\" , \"Cm\" , \"G\" \" A7\"], , chords predicted time: [0.46, 1.67, 3.53, 5.48, 8.92], chords extracted from audio: [\"Am \" \"G\", , \"A6\", \" Gmaj7\"], chords time extracted from audio: [0.46, 1.67, 3.72, 5.94, 8.73, \"C\", \"Gmaj7\" , 9.85]"
        },
        {
            "bounding_box": [
                {
                    "x": 285,
                    "y": 2093
                },
                {
                    "x": 2195,
                    "y": 2093
                },
                {
                    "x": 2195,
                    "y": 2600
                },
                {
                    "x": 285,
                    "y": 2600
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='176' style='font-size:18px'>The two depicted samples give us some specific insights into the predicted chords and chords detected<br>in the generated audio. Most of the time, Mustango follows the chords provided by the chord predictor in<br>most cases. We can observe some substitutions in the actual chords detected from the audio compared<br>to the predicted chords, e.g., G became G6, C became Cmaj7, and C#m7 became C#m. These chord<br>substitutions are very close musically and could even be a consequence of the feature extraction system<br>not being 100% accurate. The substitution of Bbm for F#maj7 is more of a change at first glance, but given<br>that 2 out of 3 notes in Bbm are also contained in the 4-note F#maj Mustango, we see this substitution as<br>understandable too. However, we note that this substitution would not be considered a valid one in any of<br>our proposed chord control metrics.</p>",
            "id": 176,
            "page": 13,
            "text": "The two depicted samples give us some specific insights into the predicted chords and chords detected in the generated audio. Most of the time, Mustango follows the chords provided by the chord predictor in most cases. We can observe some substitutions in the actual chords detected from the audio compared to the predicted chords, e.g., G became G6, C became Cmaj7, and C#m7 became C#m. These chord substitutions are very close musically and could even be a consequence of the feature extraction system not being 100% accurate. The substitution of Bbm for F#maj7 is more of a change at first glance, but given that 2 out of 3 notes in Bbm are also contained in the 4-note F#maj Mustango, we see this substitution as understandable too. However, we note that this substitution would not be considered a valid one in any of our proposed chord control metrics."
        },
        {
            "bounding_box": [
                {
                    "x": 284,
                    "y": 2602
                },
                {
                    "x": 2195,
                    "y": 2602
                },
                {
                    "x": 2195,
                    "y": 2884
                },
                {
                    "x": 284,
                    "y": 2884
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='177' style='font-size:20px'>Last but not least, in the absence of explicit control sentences in the prompt, we observe that the chords<br>predicted by the chord predictor usually follow specific patterns. The generated samples follow a pattern<br>of two chords that alternate (A, B, A, B, A, B). Another type of an observed pattern is one chord repeated<br>(A, A, A, A, A, A). A more elaborate study on the Chord predictor behavior should be a topic for future<br>work.</p>",
            "id": 177,
            "page": 13,
            "text": "Last but not least, in the absence of explicit control sentences in the prompt, we observe that the chords predicted by the chord predictor usually follow specific patterns. The generated samples follow a pattern of two chords that alternate (A, B, A, B, A, B). Another type of an observed pattern is one chord repeated (A, A, A, A, A, A). A more elaborate study on the Chord predictor behavior should be a topic for future work."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 2921
                },
                {
                    "x": 1177,
                    "y": 2921
                },
                {
                    "x": 1177,
                    "y": 2980
                },
                {
                    "x": 287,
                    "y": 2980
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='178' style='font-size:22px'>D Insights from the Human Annotation</p>",
            "id": 178,
            "page": 13,
            "text": "D Insights from the Human Annotation"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 3009
                },
                {
                    "x": 2197,
                    "y": 3009
                },
                {
                    "x": 2197,
                    "y": 3238
                },
                {
                    "x": 287,
                    "y": 3238
                }
            ],
            "category": "paragraph",
            "html": "<p id='179' style='font-size:20px'>Here, we take a look at some generated examples from the expert listening test, specifically a blues<br>sample with the following prompt: \"An instrumental blues melody played by a lead guitar<br>and a strumming acoustic guitar. The acoustic guitarist 's strumming keeps the rhythm<br>steady. The chord sequence is G7, F7, C7, G7. This song goes at 100 beats per minute. \"</p>",
            "id": 179,
            "page": 13,
            "text": "Here, we take a look at some generated examples from the expert listening test, specifically a blues sample with the following prompt: \"An instrumental blues melody played by a lead guitar and a strumming acoustic guitar. The acoustic guitarist 's strumming keeps the rhythm steady. The chord sequence is G7, F7, C7, G7. This song goes at 100 beats per minute. \""
        },
        {
            "bounding_box": [
                {
                    "x": 567,
                    "y": 286
                },
                {
                    "x": 1905,
                    "y": 286
                },
                {
                    "x": 1905,
                    "y": 821
                },
                {
                    "x": 567,
                    "y": 821
                }
            ],
            "category": "figure",
            "html": "<figure><img id='180' style='font-size:20px' alt=\"司机的前\" data-coord=\"top-left:(567,286); bottom-right:(1905,821)\" /></figure>",
            "id": 180,
            "page": 14,
            "text": "司机的前"
        },
        {
            "bounding_box": [
                {
                    "x": 486,
                    "y": 853
                },
                {
                    "x": 1989,
                    "y": 853
                },
                {
                    "x": 1989,
                    "y": 907
                },
                {
                    "x": 486,
                    "y": 907
                }
            ],
            "category": "caption",
            "html": "<caption id='181' style='font-size:14px'>Figure 3: Mel-spectrogram of a blues sample generated by Tango trained on Musi cBench.</caption>",
            "id": 181,
            "page": 14,
            "text": "Figure 3: Mel-spectrogram of a blues sample generated by Tango trained on Musi cBench."
        },
        {
            "bounding_box": [
                {
                    "x": 285,
                    "y": 1431
                },
                {
                    "x": 2197,
                    "y": 1431
                },
                {
                    "x": 2197,
                    "y": 1888
                },
                {
                    "x": 285,
                    "y": 1888
                }
            ],
            "category": "paragraph",
            "html": "<p id='182' style='font-size:18px'>In Figure 3 we can see the mel-spectrogram generated by pre-trained Tango finetuned on Musi cBench.<br>As is clear from the spectrogram and the waveform attached, the music appears a bit abruptly in contrast<br>to the sample generated by Mustango depicted in Figure 4 where the rhythm is very consistent. This<br>seems to reflect the results of our expert listening study from Table 3. The predicted beat timestamps by<br>our Beat predictor that condition the diffusion process are as follows: beats predicted: [[0.26, 0.87, 1.52,<br>2.09, 2.76, 3.41, 4.0, 4.57, 5.1, 5.65, 6.22, 6.79, 7.36, 7.79, 8.3, 8.8, 9.3, 9.75], 3]. These predicted beat<br>timestamps show that there is a beat roughly every 0.6 seconds, which corresponds to 100 beats per<br>minute tempo. This is the tempo ordered and properly predicted to condition the model.</p>",
            "id": 182,
            "page": 14,
            "text": "In Figure 3 we can see the mel-spectrogram generated by pre-trained Tango finetuned on Musi cBench. As is clear from the spectrogram and the waveform attached, the music appears a bit abruptly in contrast to the sample generated by Mustango depicted in Figure 4 where the rhythm is very consistent. This seems to reflect the results of our expert listening study from Table 3. The predicted beat timestamps by our Beat predictor that condition the diffusion process are as follows: beats predicted: [[0.26, 0.87, 1.52, 2.09, 2.76, 3.41, 4.0, 4.57, 5.1, 5.65, 6.22, 6.79, 7.36, 7.79, 8.3, 8.8, 9.3, 9.75], 3]. These predicted beat timestamps show that there is a beat roughly every 0.6 seconds, which corresponds to 100 beats per minute tempo. This is the tempo ordered and properly predicted to condition the model."
        },
        {
            "bounding_box": [
                {
                    "x": 282,
                    "y": 2100
                },
                {
                    "x": 2199,
                    "y": 2100
                },
                {
                    "x": 2199,
                    "y": 3237
                },
                {
                    "x": 282,
                    "y": 3237
                }
            ],
            "category": "paragraph",
            "html": "<p id='183' style='font-size:16px'>When it comes to chords, Tango would sometimes not follow the chords, make them sound unclear, or<br>not give them enough time to sound through. On the other hand, Mustango seems to follow the predicted<br>chords as well as their starting time. We take a look at the same blues example. The predicted chord<br>condition from the Chord predictor is as follows: chords predicted: [\"G7\", , \"F7\" , \"C7\" , \"G7\"], chords<br>predicted time: [0.46, 2.04, 4.37, 8.17]. We can see that the chord onset time is nicely spread in time.<br>This is also clear from listening to the sample and seeing the spectrogram with perceived chord starts in<br>Figure 4. To confirm this, we extracted the chord features from the generated audio to compare. The<br>\"C\",<br>chord feature extracted from the audio sample generated by Mustango is: chords: [\"G7\", \"F7\" ,<br>\"G7\"], chords time: [0.46, 1.76, 4.74, 8.45] Interestingly, the match of timing and chord sequence is<br>very clear here. The substitution of the C7 chord for C can be a minor mistake either on the generation<br>part or the feature extraction part. If we consider the chord metrics from the controllability evaluation in<br>§4.4, this would yield a score of 100 for CMOT and a score of 75 for CMO and ECM. In contrast, the<br>sample generated by pre-trained Tango finetuned on MusicBench sounds more unstable and does not give<br>enough time to chords to sound through. The chord feature extracted from the audio sample generated by<br>pre-trained Tango finetuned on MusicBench is: chords: [\"Fm6\" , , \"Dm\" \"G\" \"C\" , \"Gm\"], chords<br>\"G\"<br>, ,<br>time: [0.46, 2.69, 3.53, 5.76, 6.69, 9.66]. We can see that there are 6 chords extracted from the audio<br>sample instead of the ordered 4, and they do not match too well, as we see a minor type of F chord instead<br>of a major; G also appears in a minor variant once; and there is an additional Dm chord too. This would<br>yield a CMOT score of 75, but CMO and ECM scores of 0. The perceived chord starts can be seen in<br>Figure 3.</p>",
            "id": 183,
            "page": 14,
            "text": "When it comes to chords, Tango would sometimes not follow the chords, make them sound unclear, or not give them enough time to sound through. On the other hand, Mustango seems to follow the predicted chords as well as their starting time. We take a look at the same blues example. The predicted chord condition from the Chord predictor is as follows: chords predicted: [\"G7\", , \"F7\" , \"C7\" , \"G7\"], chords predicted time: [0.46, 2.04, 4.37, 8.17]. We can see that the chord onset time is nicely spread in time. This is also clear from listening to the sample and seeing the spectrogram with perceived chord starts in Figure 4. To confirm this, we extracted the chord features from the generated audio to compare. The \"C\", chord feature extracted from the audio sample generated by Mustango is: chords: [\"G7\", \"F7\" , \"G7\"], chords time: [0.46, 1.76, 4.74, 8.45] Interestingly, the match of timing and chord sequence is very clear here. The substitution of the C7 chord for C can be a minor mistake either on the generation part or the feature extraction part. If we consider the chord metrics from the controllability evaluation in §4.4, this would yield a score of 100 for CMOT and a score of 75 for CMO and ECM. In contrast, the sample generated by pre-trained Tango finetuned on MusicBench sounds more unstable and does not give enough time to chords to sound through. The chord feature extracted from the audio sample generated by pre-trained Tango finetuned on MusicBench is: chords: [\"Fm6\" , , \"Dm\" \"G\" \"C\" , \"Gm\"], chords \"G\" , , time: [0.46, 2.69, 3.53, 5.76, 6.69, 9.66]. We can see that there are 6 chords extracted from the audio sample instead of the ordered 4, and they do not match too well, as we see a minor type of F chord instead of a major; G also appears in a minor variant once; and there is an additional Dm chord too. This would yield a CMOT score of 75, but CMO and ECM scores of 0. The perceived chord starts can be seen in Figure 3."
        },
        {
            "bounding_box": [
                {
                    "x": 570,
                    "y": 290
                },
                {
                    "x": 1905,
                    "y": 290
                },
                {
                    "x": 1905,
                    "y": 814
                },
                {
                    "x": 570,
                    "y": 814
                }
            ],
            "category": "figure",
            "html": "<figure><img id='184' alt=\"\" data-coord=\"top-left:(570,290); bottom-right:(1905,814)\" /></figure>",
            "id": 184,
            "page": 15,
            "text": ""
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 850
                },
                {
                    "x": 2194,
                    "y": 850
                },
                {
                    "x": 2194,
                    "y": 951
                },
                {
                    "x": 287,
                    "y": 951
                }
            ],
            "category": "caption",
            "html": "<caption id='185' style='font-size:16px'>Figure 4: Mel-spectrogram of a blues sample generated by Mustango with vertical lines showing perceived chord<br>starts.</caption>",
            "id": 185,
            "page": 15,
            "text": "Figure 4: Mel-spectrogram of a blues sample generated by Mustango with vertical lines showing perceived chord starts."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 1001
                },
                {
                    "x": 2195,
                    "y": 1001
                },
                {
                    "x": 2195,
                    "y": 1723
                },
                {
                    "x": 288,
                    "y": 1723
                }
            ],
            "category": "table",
            "html": "<table id='186' style='font-size:14px'><tr><td>Model</td><td>Dataset</td><td>Core Architecture</td><td>Area of Focus</td></tr><tr><td>MusicLM (Agostinelli et al., 2023) Noise2Music (Huang et al., 2023) Ernie-Music (Zhu et al., 2023) MusicGEN (Copet et al., 2023) Mousai (Schneider et al., 2023) JEN1 (Li et al., 2023) Mustango (ours)</td><td>Private Dataset including an open- sourced test set: MusiCaps Private Dataset obtained via pseudo la- belling Private Dataset consisting of online mu- sic and corresponding comments Private Dataset Private Dataset; Data Collection Pipeline partially open-sourced Private Dataset Public Dataset + Music-Domain- Knowledge-Enhanced Data Augmenta- tion</td><td>Hierarchical Seq2Seq Modeling 2-stage Diffusion Diffusion (without using Audio Latent) Autoregressive Transformer 2-Stage Latent Diffusion Latent Diffusion, Multi-Task Learning Latent Diffusion</td><td>Audio Quality, Text-Music Relevance Audio Quality, Text-Music Relevance Audio Quality, Text-Music Relevance, Diversity Audio Quality, Text-Music Relevance, Music Quality, Controllability (Follows given melodies) Audio Quality, Text-Music Relevance, Music Quality, Efficiency, Long-Term Structure, Diversity Audio Quality, Text-Music Relevance, Music Quality, Efficiency Audio Quality, Text-Music Relevance, Music Quality, Music Controllability (Follows user-specific text prompts in- cluding tempo, chord changes, etc)</td></tr></table>",
            "id": 186,
            "page": 15,
            "text": "Model Dataset Core Architecture Area of Focus  MusicLM (Agostinelli , 2023) Noise2Music (Huang , 2023) Ernie-Music (Zhu , 2023) MusicGEN (Copet , 2023) Mousai (Schneider , 2023) JEN1 (Li , 2023) Mustango (ours) Private Dataset including an open- sourced test set: MusiCaps Private Dataset obtained via pseudo la- belling Private Dataset consisting of online mu- sic and corresponding comments Private Dataset Private Dataset; Data Collection Pipeline partially open-sourced Private Dataset Public Dataset + Music-Domain- Knowledge-Enhanced Data Augmenta- tion Hierarchical Seq2Seq Modeling 2-stage Diffusion Diffusion (without using Audio Latent) Autoregressive Transformer 2-Stage Latent Diffusion Latent Diffusion, Multi-Task Learning Latent Diffusion"
        },
        {
            "bounding_box": [
                {
                    "x": 597,
                    "y": 1758
                },
                {
                    "x": 1875,
                    "y": 1758
                },
                {
                    "x": 1875,
                    "y": 1812
                },
                {
                    "x": 597,
                    "y": 1812
                }
            ],
            "category": "caption",
            "html": "<caption id='187' style='font-size:16px'>Table 4: High-level comparison among various recent text-to-music models.</caption>",
            "id": 187,
            "page": 15,
            "text": "Table 4: High-level comparison among various recent text-to-music models."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 1899
                },
                {
                    "x": 700,
                    "y": 1899
                },
                {
                    "x": 700,
                    "y": 1956
                },
                {
                    "x": 289,
                    "y": 1956
                }
            ],
            "category": "paragraph",
            "html": "<p id='188' style='font-size:22px'>E Related Works</p>",
            "id": 188,
            "page": 15,
            "text": "E Related Works"
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 1991
                },
                {
                    "x": 2194,
                    "y": 1991
                },
                {
                    "x": 2194,
                    "y": 2271
                },
                {
                    "x": 286,
                    "y": 2271
                }
            ],
            "category": "paragraph",
            "html": "<p id='189' style='font-size:18px'>In this section, we describe existing state-of-the-art research on text-to-audio generation, followed by<br>the more specific domain of text-to-music generation. For audio generation, AudioLM (Borsos et al.,<br>2023) uses the state-of-the-art semantic model w2v-Bert (Chung et al., 2021) to generate the semantic<br>tokens from audio prompt. These tokens condition the generation of acoustic tokens that are decoded<br>using acoustic model SoundStream (Zeghidour et al., 2022) to generate audio.</p>",
            "id": 189,
            "page": 15,
            "text": "In this section, we describe existing state-of-the-art research on text-to-audio generation, followed by the more specific domain of text-to-music generation. For audio generation, AudioLM (Borsos , 2023) uses the state-of-the-art semantic model w2v-Bert (Chung , 2021) to generate the semantic tokens from audio prompt. These tokens condition the generation of acoustic tokens that are decoded using acoustic model SoundStream (Zeghidour , 2022) to generate audio."
        },
        {
            "bounding_box": [
                {
                    "x": 285,
                    "y": 2277
                },
                {
                    "x": 2196,
                    "y": 2277
                },
                {
                    "x": 2196,
                    "y": 2667
                },
                {
                    "x": 285,
                    "y": 2667
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='190' style='font-size:20px'>AudioLDM (Liu et al., 2023a) is a text-to-audio framework that leverages CLAP (Wu et al., 2023),<br>a joint audio-text representation model, and a latent diffusion model (LDM). Specifically, an LDM is<br>trained to generate the latent representations of melspectrograms which are obtained using a VAE. During<br>diffusion, the CLAP embeddings are utilized to guide the generation. Tango (Ghosal et al., 2023) leverages<br>the pre-trained VAE from AudioLDM and replaces the CLAP model with an instruction fine-tuned large<br>language model: FLAN-T5 to achieve comparable or better results while training with a much smaller<br>dataset.</p>",
            "id": 190,
            "page": 15,
            "text": "AudioLDM (Liu , 2023a) is a text-to-audio framework that leverages CLAP (Wu , 2023), a joint audio-text representation model, and a latent diffusion model (LDM). Specifically, an LDM is trained to generate the latent representations of melspectrograms which are obtained using a VAE. During diffusion, the CLAP embeddings are utilized to guide the generation. Tango (Ghosal , 2023) leverages the pre-trained VAE from AudioLDM and replaces the CLAP model with an instruction fine-tuned large language model: FLAN-T5 to achieve comparable or better results while training with a much smaller dataset."
        },
        {
            "bounding_box": [
                {
                    "x": 284,
                    "y": 2671
                },
                {
                    "x": 2196,
                    "y": 2671
                },
                {
                    "x": 2196,
                    "y": 3239
                },
                {
                    "x": 284,
                    "y": 3239
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='191' style='font-size:20px'>In the field of music generation, there is a long history of generated MIDI music (Herremans et al.,<br>2017). Using MIDI may be useful for producers to work with in Digital Audio Workstations, yet it has the<br>disadvantage that datasets are extremely limited. In recent years, the focus ofconditional music generation<br>within the audio domain has centered around musical conditions, such as note intensity or tempo (Pasini<br>and Schluter, 2022). More recently, however, models that directly generate audio music from text captions<br>have emerged. A summary of these papers are provided in Table 4. MusicLM (Agostinelli et al., 2023)<br>uses two pre-trained models, MuLan (Huang et al., 2022), a joint text-music embedding model, and<br>w2v-Bert (Chung et al., 2021), a masked language model to address the challenge of maintaining both<br>synthesizing quality and coherence during music generation. These two pre-trained models are then<br>utilized to condition the acoustic model SoundStream (Zeghidour et al., 2022) which in turn can generate</p>",
            "id": 191,
            "page": 15,
            "text": "In the field of music generation, there is a long history of generated MIDI music (Herremans , 2017). Using MIDI may be useful for producers to work with in Digital Audio Workstations, yet it has the disadvantage that datasets are extremely limited. In recent years, the focus ofconditional music generation within the audio domain has centered around musical conditions, such as note intensity or tempo (Pasini and Schluter, 2022). More recently, however, models that directly generate audio music from text captions have emerged. A summary of these papers are provided in Table 4. MusicLM (Agostinelli , 2023) uses two pre-trained models, MuLan (Huang , 2022), a joint text-music embedding model, and w2v-Bert (Chung , 2021), a masked language model to address the challenge of maintaining both synthesizing quality and coherence during music generation. These two pre-trained models are then utilized to condition the acoustic model SoundStream (Zeghidour , 2022) which in turn can generate"
        },
        {
            "bounding_box": [
                {
                    "x": 285,
                    "y": 301
                },
                {
                    "x": 2196,
                    "y": 301
                },
                {
                    "x": 2196,
                    "y": 579
                },
                {
                    "x": 285,
                    "y": 579
                }
            ],
            "category": "paragraph",
            "html": "<p id='192' style='font-size:16px'>acoustic tokens autoregressively. These acoustic tokens are then decoded by SoundStream to become the<br>final audio output. MusicLM outperforms two existing commercially available text-to-music software:<br>Muberts and Riffusion' in terms of Frechet Audio Distance, Faithfulness to the text description, KL<br>divergence, and Mulan Cycle Consistency. Since no publications are linked to these latter two systems,<br>the model details are not available.</p>",
            "id": 192,
            "page": 16,
            "text": "acoustic tokens autoregressively. These acoustic tokens are then decoded by SoundStream to become the final audio output. MusicLM outperforms two existing commercially available text-to-music software: Muberts and Riffusion' in terms of Frechet Audio Distance, Faithfulness to the text description, KL divergence, and Mulan Cycle Consistency. Since no publications are linked to these latter two systems, the model details are not available."
        },
        {
            "bounding_box": [
                {
                    "x": 284,
                    "y": 584
                },
                {
                    "x": 2198,
                    "y": 584
                },
                {
                    "x": 2198,
                    "y": 974
                },
                {
                    "x": 284,
                    "y": 974
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='193' style='font-size:16px'>Another text-to-music model is Noise2Music (Huang et al., 2023). To obtain training data for the model,<br>the authors propose a method to obtain a large amount of paired music and text data in which LaMDA-<br>LF (Thoppilan et al., 2022), a large language model, is used to generate multiple generic candidate text<br>descriptions. The aforementioned joint text-music embedding MuLan is then utilized to select the best<br>candidates for existing music data. The obtained music and text pairs are then used to train a two-stage<br>diffusion model, where the first diffusion model generates an intermediate representation and the second<br>generates the final audio output.</p>",
            "id": 193,
            "page": 16,
            "text": "Another text-to-music model is Noise2Music (Huang , 2023). To obtain training data for the model, the authors propose a method to obtain a large amount of paired music and text data in which LaMDALF (Thoppilan , 2022), a large language model, is used to generate multiple generic candidate text descriptions. The aforementioned joint text-music embedding MuLan is then utilized to select the best candidates for existing music data. The obtained music and text pairs are then used to train a two-stage diffusion model, where the first diffusion model generates an intermediate representation and the second generates the final audio output."
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 978
                },
                {
                    "x": 2198,
                    "y": 978
                },
                {
                    "x": 2198,
                    "y": 1202
                },
                {
                    "x": 286,
                    "y": 1202
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='194' style='font-size:16px'>Ernie-Music (Zhu et al., 2023) uses a diffusion model to generate music audio from free-form text.<br>It is trained using a private dataset which consists of online music and the top-rated comments from<br>the comment section. The authors recruited 10 casual music listeners to participate in a listening study.<br>Results showed that Ernie-Music outperforms two non-diffusion-based generative systems.</p>",
            "id": 194,
            "page": 16,
            "text": "Ernie-Music (Zhu , 2023) uses a diffusion model to generate music audio from free-form text. It is trained using a private dataset which consists of online music and the top-rated comments from the comment section. The authors recruited 10 casual music listeners to participate in a listening study. Results showed that Ernie-Music outperforms two non-diffusion-based generative systems."
        },
        {
            "bounding_box": [
                {
                    "x": 285,
                    "y": 1207
                },
                {
                    "x": 2198,
                    "y": 1207
                },
                {
                    "x": 2198,
                    "y": 1883
                },
                {
                    "x": 285,
                    "y": 1883
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='195' style='font-size:16px'>In recent months, a number of text-to-music models have come out. Schneider et al. (2023) proposes a<br>2-stage diffusion model in which the first diffusion magnitude autoencoder (DMAE) learns a meaningful<br>latent representation of music (64 times smaller than the input), while in the second diffusion model, text<br>condition along with the latent acquired at the first stage is included to guide the final music generation.<br>MusicGen (Copet et al., 2023) utilizes a single-stage transformer LM with efficient token interleaving<br>patterns to achieve high-quality generation and better controlabillity over the output. MusicGen can be<br>conditioned by a text prompt, or by an audio fragment in the form of a chromagram. The system was<br>trained with a licensed dataset. The JEN-1 model (Li et al., 2023) is an omnidirectional diffusion model<br>designed to perform various tasks such as text-guided music generation, music inpainting, and continuation.<br>Another interesting recent model is that of Su et al. (2023), which focuses on generating music pieces to<br>complement video, conditioned on both video and text inputs. Unlike text, video conditioning can contain<br>a lot of temporal information, such as beats and emotions, which are important for music.</p>",
            "id": 195,
            "page": 16,
            "text": "In recent months, a number of text-to-music models have come out. Schneider  (2023) proposes a 2-stage diffusion model in which the first diffusion magnitude autoencoder (DMAE) learns a meaningful latent representation of music (64 times smaller than the input), while in the second diffusion model, text condition along with the latent acquired at the first stage is included to guide the final music generation. MusicGen (Copet , 2023) utilizes a single-stage transformer LM with efficient token interleaving patterns to achieve high-quality generation and better controlabillity over the output. MusicGen can be conditioned by a text prompt, or by an audio fragment in the form of a chromagram. The system was trained with a licensed dataset. The JEN-1 model (Li , 2023) is an omnidirectional diffusion model designed to perform various tasks such as text-guided music generation, music inpainting, and continuation. Another interesting recent model is that of Su  (2023), which focuses on generating music pieces to complement video, conditioned on both video and text inputs. Unlike text, video conditioning can contain a lot of temporal information, such as beats and emotions, which are important for music."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 1919
                },
                {
                    "x": 950,
                    "y": 1919
                },
                {
                    "x": 950,
                    "y": 1979
                },
                {
                    "x": 290,
                    "y": 1979
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='196' style='font-size:18px'>F FMACaps dataset creation</p>",
            "id": 196,
            "page": 16,
            "text": "F FMACaps dataset creation"
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 2008
                },
                {
                    "x": 2198,
                    "y": 2008
                },
                {
                    "x": 2198,
                    "y": 2631
                },
                {
                    "x": 286,
                    "y": 2631
                }
            ],
            "category": "paragraph",
            "html": "<p id='197' style='font-size:16px'>We source the new music files from the Free Music Archive (FMA) (Defferrard et al., 2016), a large<br>dataset of popular songs. In particular, we took 1,000 random samples from FMA-large and clipped out a<br>random 10-second fragment from each of them. Then, we used Essentia's tagging models (Bogdanov<br>et al., 2013) to assign tags to audio. Specifically, we used the models for general auto-tagging, mood,<br>genre, instrumentation, voice, and voice gender which provide us with a rich set of tags along with their<br>probabilities. Then, a music expert wrote text descriptions for 25 of the samples based on the audio as<br>well as the extracted tags. Next, we instructed ChatGPT to perform an in-context learning task to get<br>pseudo-prompts from tags for the rest of the dataset. Finally, we added relevant control sentences to the<br>prompts after extracting relevant music features, as described in §2.1. Similar to our training set, we<br>added 0/1/2/3/4 control sentences with a probability of 25/30/20/15/10% respectively. We refer to this<br>evaluation set as FMACaps.</p>",
            "id": 197,
            "page": 16,
            "text": "We source the new music files from the Free Music Archive (FMA) (Defferrard , 2016), a large dataset of popular songs. In particular, we took 1,000 random samples from FMA-large and clipped out a random 10-second fragment from each of them. Then, we used Essentia's tagging models (Bogdanov , 2013) to assign tags to audio. Specifically, we used the models for general auto-tagging, mood, genre, instrumentation, voice, and voice gender which provide us with a rich set of tags along with their probabilities. Then, a music expert wrote text descriptions for 25 of the samples based on the audio as well as the extracted tags. Next, we instructed ChatGPT to perform an in-context learning task to get pseudo-prompts from tags for the rest of the dataset. Finally, we added relevant control sentences to the prompts after extracting relevant music features, as described in §2.1. Similar to our training set, we added 0/1/2/3/4 control sentences with a probability of 25/30/20/15/10% respectively. We refer to this evaluation set as FMACaps."
        },
        {
            "bounding_box": [
                {
                    "x": 285,
                    "y": 2669
                },
                {
                    "x": 1575,
                    "y": 2669
                },
                {
                    "x": 1575,
                    "y": 2731
                },
                {
                    "x": 285,
                    "y": 2731
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='198' style='font-size:20px'>G User Interface and Questions used for Listening Studies</p>",
            "id": 198,
            "page": 16,
            "text": "G User Interface and Questions used for Listening Studies"
        },
        {
            "bounding_box": [
                {
                    "x": 285,
                    "y": 2759
                },
                {
                    "x": 2196,
                    "y": 2759
                },
                {
                    "x": 2196,
                    "y": 3045
                },
                {
                    "x": 285,
                    "y": 3045
                }
            ],
            "category": "paragraph",
            "html": "<p id='199' style='font-size:16px'>The human evaluation participants for the listening tests were recruited through email contacts of various<br>music research and machine learning communities. Their age ranged from 15 to 60+ years. No specific<br>information on the country of residence was collected. Participation was free of reward and voluntary. All<br>the participants were informed about this before their participation. The data collection protocol for the<br>listening test was approved by an independent ethics review board.</p>",
            "id": 199,
            "page": 16,
            "text": "The human evaluation participants for the listening tests were recruited through email contacts of various music research and machine learning communities. Their age ranged from 15 to 60+ years. No specific information on the country of residence was collected. Participation was free of reward and voluntary. All the participants were informed about this before their participation. The data collection protocol for the listening test was approved by an independent ethics review board."
        },
        {
            "bounding_box": [
                {
                    "x": 338,
                    "y": 3139
                },
                {
                    "x": 1125,
                    "y": 3139
                },
                {
                    "x": 1125,
                    "y": 3233
                },
                {
                    "x": 338,
                    "y": 3233
                }
            ],
            "category": "footer",
            "html": "<footer id='200' style='font-size:14px'>5https://github. com/MubertAI /Mubert- Text-to-Music<br>6https:/ /www.riffusion. com/</footer>",
            "id": 200,
            "page": 16,
            "text": "5https://github. com/MubertAI /Mubert- Text-to-Music 6https:/ /www.riffusion. com/"
        },
        {
            "bounding_box": [
                {
                    "x": 633,
                    "y": 550
                },
                {
                    "x": 1858,
                    "y": 550
                },
                {
                    "x": 1858,
                    "y": 1358
                },
                {
                    "x": 633,
                    "y": 1358
                }
            ],
            "category": "figure",
            "html": "<figure><img id='201' style='font-size:14px' alt=\"0:10 / 0:10\nPrompt:\nA piano plays a soothing popular instrumental song that could serve as background music in a restaurant. There is only piano\nplaying, no other instruments. There is a piano melody with background piano chords of Am, Fmaj7, Cmaj7, and G. The tempo\nis unhurried. The melody is gentle and soothing, evoking a sense of nostalgia and comfort.\nPlease listen to the whole audio clip and rate the following:\nItem Very poor Neutral Very good\nHow well does the Prompt match the generated audio? ○ ○ ○ ○ ○ ○ ○\nHow good is the overall musical quality? ○ ○ ○ ○ ○ ○ ○\nHow clear is the audio rendering/quality (absence of noise)? ○ ○ ○ ○ ○ ○ ○\nHow consistent is the rhythm? ○ ○ ○ ○ ○ ○ ○\nHow harmonic/consonant does the music sound? ○ ○ ○ ○ ○ ○ ○\nClick this button to continue\" data-coord=\"top-left:(633,550); bottom-right:(1858,1358)\" /></figure>",
            "id": 201,
            "page": 17,
            "text": "0:10 / 0:10 Prompt: A piano plays a soothing popular instrumental song that could serve as background music in a restaurant. There is only piano playing, no other instruments. There is a piano melody with background piano chords of Am, Fmaj7, Cmaj7, and G. The tempo is unhurried. The melody is gentle and soothing, evoking a sense of nostalgia and comfort. Please listen to the whole audio clip and rate the following: Item Very poor Neutral Very good How well does the Prompt match the generated audio? ○ ○ ○ ○ ○ ○ ○ How good is the overall musical quality? ○ ○ ○ ○ ○ ○ ○ How clear is the audio rendering/quality (absence of noise)? ○ ○ ○ ○ ○ ○ ○ How consistent is the rhythm? ○ ○ ○ ○ ○ ○ ○ How harmonic/consonant does the music sound? ○ ○ ○ ○ ○ ○ ○ Click this button to continue"
        },
        {
            "bounding_box": [
                {
                    "x": 711,
                    "y": 1392
                },
                {
                    "x": 1766,
                    "y": 1392
                },
                {
                    "x": 1766,
                    "y": 1448
                },
                {
                    "x": 711,
                    "y": 1448
                }
            ],
            "category": "caption",
            "html": "<caption id='202' style='font-size:18px'>Figure 5: Question interface used for the general listening test.</caption>",
            "id": 202,
            "page": 17,
            "text": "Figure 5: Question interface used for the general listening test."
        },
        {
            "bounding_box": [
                {
                    "x": 623,
                    "y": 1988
                },
                {
                    "x": 1860,
                    "y": 1988
                },
                {
                    "x": 1860,
                    "y": 2880
                },
                {
                    "x": 623,
                    "y": 2880
                }
            ],
            "category": "figure",
            "html": "<figure><img id='203' style='font-size:16px' alt=\"0:10 / 0:10\nPrompt:\nA horn and a bass guitar groove to a reggae tune. The combination of the horn section's catchy melodies and the buoyant\nbassline creates an irresistible groove. The bassline is bouncy and lively. The song is played at the pace of Adagio. An electric\nkeyboard plays the chords Am, Dm, G, C.\nPlease listen to the whole audio clip and rate the following:\nItem Very poor Neutral Very good\nMatching: How well does the Prompt match the generated audio? ○ ○ ○ ○ ○ ○ ○\nMatching: How well do the chords match the chords in the Prompt? ○ ○ ○ ○ ○ ○ ○\nMatching: How well does the tempo match the tempo in the Prompt? ○ ○ ○ ○ ○ ○ ○\nAudio: How good is the overall musical quality? ○ ○ ○ ○ ○ ○ ○\nAudio: How clear is the audio rendering/quality (absence of noise)? ○ ○ ○ ○ ○ ○ ○\nAudio: How consistent is the rhythm? ○ ○ ○ ○ ○ ○ ○\nAudio: How harmonic/consonant does the music sound? ○ ○ ○ ○ ○ ○ ○\nClick this button to continue\" data-coord=\"top-left:(623,1988); bottom-right:(1860,2880)\" /></figure>",
            "id": 203,
            "page": 17,
            "text": "0:10 / 0:10 Prompt: A horn and a bass guitar groove to a reggae tune. The combination of the horn section's catchy melodies and the buoyant bassline creates an irresistible groove. The bassline is bouncy and lively. The song is played at the pace of Adagio. An electric keyboard plays the chords Am, Dm, G, C. Please listen to the whole audio clip and rate the following: Item Very poor Neutral Very good Matching: How well does the Prompt match the generated audio? ○ ○ ○ ○ ○ ○ ○ Matching: How well do the chords match the chords in the Prompt? ○ ○ ○ ○ ○ ○ ○ Matching: How well does the tempo match the tempo in the Prompt? ○ ○ ○ ○ ○ ○ ○ Audio: How good is the overall musical quality? ○ ○ ○ ○ ○ ○ ○ Audio: How clear is the audio rendering/quality (absence of noise)? ○ ○ ○ ○ ○ ○ ○ Audio: How consistent is the rhythm? ○ ○ ○ ○ ○ ○ ○ Audio: How harmonic/consonant does the music sound? ○ ○ ○ ○ ○ ○ ○ Click this button to continue"
        },
        {
            "bounding_box": [
                {
                    "x": 495,
                    "y": 2905
                },
                {
                    "x": 1976,
                    "y": 2905
                },
                {
                    "x": 1976,
                    "y": 2964
                },
                {
                    "x": 495,
                    "y": 2964
                }
            ],
            "category": "caption",
            "html": "<br><caption id='204' style='font-size:20px'>Figure 6: Question interface used for the controllability listening test with music experts.</caption>",
            "id": 204,
            "page": 17,
            "text": "Figure 6: Question interface used for the controllability listening test with music experts."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 297
                },
                {
                    "x": 857,
                    "y": 297
                },
                {
                    "x": 857,
                    "y": 359
                },
                {
                    "x": 288,
                    "y": 359
                }
            ],
            "category": "paragraph",
            "html": "<p id='205' style='font-size:18px'>H ChatGPT Rephrasing</p>",
            "id": 205,
            "page": 18,
            "text": "H ChatGPT Rephrasing"
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 390
                },
                {
                    "x": 2190,
                    "y": 390
                },
                {
                    "x": 2190,
                    "y": 497
                },
                {
                    "x": 290,
                    "y": 497
                }
            ],
            "category": "paragraph",
            "html": "<p id='206' style='font-size:14px'>In our data augmentation pipeline, we rephrase captions using ChatGPT. The instructions to the API are<br>as follows:</p>",
            "id": 206,
            "page": 18,
            "text": "In our data augmentation pipeline, we rephrase captions using ChatGPT. The instructions to the API are as follows:"
        },
        {
            "bounding_box": [
                {
                    "x": 326,
                    "y": 513
                },
                {
                    "x": 2154,
                    "y": 513
                },
                {
                    "x": 2154,
                    "y": 799
                },
                {
                    "x": 326,
                    "y": 799
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='207' style='font-size:14px'>\"I have a song for which the caption is the following:\" *caption* \"I have made some changes to<br>the audio file which are optionally described towards the end of the caption. Can you rephrase the<br>caption more naturally in a single paragraph using all the musical terms provided above? You should<br>generate only the caption and nothing else. Do not use the word modification in your generation. The<br>length of the new caption should be no more than eight sentences. \"</p>",
            "id": 207,
            "page": 18,
            "text": "\"I have a song for which the caption is the following:\" *caption* \"I have made some changes to the audio file which are optionally described towards the end of the caption. Can you rephrase the caption more naturally in a single paragraph using all the musical terms provided above? You should generate only the caption and nothing else. Do not use the word modification in your generation. The length of the new caption should be no more than eight sentences. \""
        },
        {
            "bounding_box": [
                {
                    "x": 333,
                    "y": 885
                },
                {
                    "x": 820,
                    "y": 885
                },
                {
                    "x": 820,
                    "y": 939
                },
                {
                    "x": 333,
                    "y": 939
                }
            ],
            "category": "paragraph",
            "html": "<p id='208' style='font-size:16px'>Examples of rephrasing:</p>",
            "id": 208,
            "page": 18,
            "text": "Examples of rephrasing:"
        },
        {
            "bounding_box": [
                {
                    "x": 330,
                    "y": 967
                },
                {
                    "x": 509,
                    "y": 967
                },
                {
                    "x": 509,
                    "y": 1016
                },
                {
                    "x": 330,
                    "y": 1016
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='209' style='font-size:20px'>Original</p>",
            "id": 209,
            "page": 18,
            "text": "Original"
        },
        {
            "bounding_box": [
                {
                    "x": 331,
                    "y": 1021
                },
                {
                    "x": 2152,
                    "y": 1021
                },
                {
                    "x": 2152,
                    "y": 1302
                },
                {
                    "x": 331,
                    "y": 1302
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='210' style='font-size:14px'>This folk song features a male voice singing the main melody in an emotional mood. This is<br>accompanied by an accordion playing fills in the background. A violin plays a droning melody. There<br>is no percussion in this song. This song can be played at a Central Asian classical concert. The chord<br>sequence is B, Dmaj7, B, Gbm. This song goes at 95.0 beats per minute. The beat counts to 2. This<br>song is in the key of B minor.</p>",
            "id": 210,
            "page": 18,
            "text": "This folk song features a male voice singing the main melody in an emotional mood. This is accompanied by an accordion playing fills in the background. A violin plays a droning melody. There is no percussion in this song. This song can be played at a Central Asian classical concert. The chord sequence is B, Dmaj7, B, Gbm. This song goes at 95.0 beats per minute. The beat counts to 2. This song is in the key of B minor."
        },
        {
            "bounding_box": [
                {
                    "x": 332,
                    "y": 1372
                },
                {
                    "x": 554,
                    "y": 1372
                },
                {
                    "x": 554,
                    "y": 1422
                },
                {
                    "x": 332,
                    "y": 1422
                }
            ],
            "category": "paragraph",
            "html": "<p id='211' style='font-size:20px'>Rephrased</p>",
            "id": 211,
            "page": 18,
            "text": "Rephrased"
        },
        {
            "bounding_box": [
                {
                    "x": 330,
                    "y": 1426
                },
                {
                    "x": 2153,
                    "y": 1426
                },
                {
                    "x": 2153,
                    "y": 1710
                },
                {
                    "x": 330,
                    "y": 1710
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='212' style='font-size:14px'>A male voice sings the main melody with emotional intensity in this Central Asian folk song. The<br>accordion fills the background while a violin provides a droning melody. There is no percussion in<br>this piece, which can be played at a classical concert in Central Asia. With a chord sequence of B,<br>Dmaj7, B, Gbm, the song has a beat count of 2 at a tempo of 95.0 beats per minute. The song is in<br>the key of B minor.</p>",
            "id": 212,
            "page": 18,
            "text": "A male voice sings the main melody with emotional intensity in this Central Asian folk song. The accordion fills the background while a violin provides a droning melody. There is no percussion in this piece, which can be played at a classical concert in Central Asia. With a chord sequence of B, Dmaj7, B, Gbm, the song has a beat count of 2 at a tempo of 95.0 beats per minute. The song is in the key of B minor."
        },
        {
            "bounding_box": [
                {
                    "x": 331,
                    "y": 1777
                },
                {
                    "x": 507,
                    "y": 1777
                },
                {
                    "x": 507,
                    "y": 1827
                },
                {
                    "x": 331,
                    "y": 1827
                }
            ],
            "category": "paragraph",
            "html": "<p id='213' style='font-size:20px'>Original</p>",
            "id": 213,
            "page": 18,
            "text": "Original"
        },
        {
            "bounding_box": [
                {
                    "x": 328,
                    "y": 1835
                },
                {
                    "x": 2152,
                    "y": 1835
                },
                {
                    "x": 2152,
                    "y": 2057
                },
                {
                    "x": 328,
                    "y": 2057
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='214' style='font-size:14px'>This folk song features a male voice singing the main melody in an emotional mood. This is<br>accompanied by an accordion playing fills in the background. A violin plays a droning melody. There<br>is no percussion in this song. This song can be played at a Central Asian classical concert. The key is<br>C minor. The chord progression in this song is C, D#maj7, C, Gm.</p>",
            "id": 214,
            "page": 18,
            "text": "This folk song features a male voice singing the main melody in an emotional mood. This is accompanied by an accordion playing fills in the background. A violin plays a droning melody. There is no percussion in this song. This song can be played at a Central Asian classical concert. The key is C minor. The chord progression in this song is C, D#maj7, C, Gm."
        },
        {
            "bounding_box": [
                {
                    "x": 331,
                    "y": 2125
                },
                {
                    "x": 553,
                    "y": 2125
                },
                {
                    "x": 553,
                    "y": 2175
                },
                {
                    "x": 331,
                    "y": 2175
                }
            ],
            "category": "paragraph",
            "html": "<p id='215' style='font-size:16px'>Rephrased</p>",
            "id": 215,
            "page": 18,
            "text": "Rephrased"
        },
        {
            "bounding_box": [
                {
                    "x": 329,
                    "y": 2181
                },
                {
                    "x": 2153,
                    "y": 2181
                },
                {
                    "x": 2153,
                    "y": 2405
                },
                {
                    "x": 329,
                    "y": 2405
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='216' style='font-size:14px'>This emotional folk song, perfect for a Central Asian classical concert, showcases a male voice<br>singing the main melody accompanied by a droning violin and accordion fills in the background.<br>With no percussion present, the key of C minor sets the tone, and the chord progression follows suit<br>with C, D#maj7, C, Gm.</p>",
            "id": 216,
            "page": 18,
            "text": "This emotional folk song, perfect for a Central Asian classical concert, showcases a male voice singing the main melody accompanied by a droning violin and accordion fills in the background. With no percussion present, the key of C minor sets the tone, and the chord progression follows suit with C, D#maj7, C, Gm."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 2459
                },
                {
                    "x": 1495,
                    "y": 2459
                },
                {
                    "x": 1495,
                    "y": 2520
                },
                {
                    "x": 287,
                    "y": 2520
                }
            ],
            "category": "paragraph",
            "html": "<p id='217' style='font-size:16px'>I Control-Sentence Templates to Enhance the Prompts</p>",
            "id": 217,
            "page": 18,
            "text": "I Control-Sentence Templates to Enhance the Prompts"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 1072
                },
                {
                    "x": 2098,
                    "y": 1072
                },
                {
                    "x": 2098,
                    "y": 2176
                },
                {
                    "x": 287,
                    "y": 2176
                }
            ],
            "category": "table",
            "html": "<table id='218' style='font-size:14px'><tr><td>Feature</td><td>Input</td><td>Output sentences</td></tr><tr><td>Tempo</td><td>int i</td><td>· The bpm is i. · The tempo of this song is i beats per minute. · This song goes at 2 beats per minute.</td></tr><tr><td>Tempo</td><td>string w E ['Grave' , 'Largo' , 'Adagio', 'Andante' 'Moderato' , 'Allegro', 'Vivace' 'Presto' 'Prestissimo']</td><td>· This song is in w. · The tempo of this song is w. · This song is played in w. · The song is played at the pace of w.</td></tr><tr><td>Beat count</td><td>int 6</td><td>· The time signature is 64. · The beat is b. · The beat counts to b.</td></tr><tr><td>Chords</td><td>text list of chords s</td><td>· The chord sequence is s. · The chord progression in this song is s.</td></tr><tr><td>Key</td><td>string rootnote string m E ['major', 'minor']</td><td>· The key is rootnote m · The key of this song is rootnote m. · This song is in the key of rootnote m</td></tr><tr><td>Volume change</td><td>float f indicating start/end time of crescendo/decrescendo, string w E ['crescendo', 'decrescendo'], and u E ['increase', 'decrease']</td><td>· There is a w from start until f seconds · The song starts with a w. · u the volume progressively! · There is a w from f seconds on. · At seconds f, the song starts to gradually u in volume. · Midway through the song, a w starts.</td></tr></table>",
            "id": 218,
            "page": 19,
            "text": "Feature Input Output sentences  Tempo int i · The bpm is i. · The tempo of this song is i beats per minute. · This song goes at 2 beats per minute.  Tempo string w E ['Grave' , 'Largo' , 'Adagio', 'Andante' 'Moderato' , 'Allegro', 'Vivace' 'Presto' 'Prestissimo'] · This song is in w. · The tempo of this song is w. · This song is played in w. · The song is played at the pace of w.  Beat count int 6 · The time signature is 64. · The beat is b. · The beat counts to b.  Chords text list of chords s · The chord sequence is s. · The chord progression in this song is s.  Key string rootnote string m E ['major', 'minor'] · The key is rootnote m · The key of this song is rootnote m. · This song is in the key of rootnote m  Volume change float f indicating start/end time of crescendo/decrescendo, string w E ['crescendo', 'decrescendo'], and u E ['increase', 'decrease']"
        },
        {
            "bounding_box": [
                {
                    "x": 285,
                    "y": 2202
                },
                {
                    "x": 2198,
                    "y": 2202
                },
                {
                    "x": 2198,
                    "y": 2412
                },
                {
                    "x": 285,
                    "y": 2412
                }
            ],
            "category": "caption",
            "html": "<caption id='219' style='font-size:18px'>Table 5: Rules used to create text sentences from input parameters detected from the data (key, chords, beats,<br>tempo), and those used to augment the data (crescendo, etc.). Note that the tempo strings w were assigned based on<br>music-theory binning in terms of bpm: Grave (0, 40], Largo (40, 60], Adagio (60, 70], Andante (70, 90], Moderato<br>(90, 110], Allegro (110, 140], Vivace (140, 160], Presto (160, 210], Prestissimo (210, 00).</caption>",
            "id": 219,
            "page": 19,
            "text": "Table 5: Rules used to create text sentences from input parameters detected from the data (key, chords, beats, tempo), and those used to augment the data (crescendo, etc.). Note that the tempo strings w were assigned based on music-theory binning in terms of bpm: Grave (0, 40], Largo (40, 60], Adagio (60, 70], Andante (70, 90], Moderato (90, 110], Allegro (110, 140], Vivace (140, 160], Presto (160, 210], Prestissimo (210, 00)."
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 296
                },
                {
                    "x": 1309,
                    "y": 296
                },
                {
                    "x": 1309,
                    "y": 357
                },
                {
                    "x": 286,
                    "y": 357
                }
            ],
            "category": "caption",
            "html": "<caption id='220' style='font-size:20px'>J Custom Captions used for Listening Studies</caption>",
            "id": 220,
            "page": 20,
            "text": "J Custom Captions used for Listening Studies"
        },
        {
            "bounding_box": [
                {
                    "x": 304,
                    "y": 430
                },
                {
                    "x": 2205,
                    "y": 430
                },
                {
                    "x": 2205,
                    "y": 2109
                },
                {
                    "x": 304,
                    "y": 2109
                }
            ],
            "category": "paragraph",
            "html": "<p id='221' style='font-size:14px'>1 This piece is an instrumental reggae song that is very chill and slow. There is no singer. It is relaxing to hear the groove<br>with the bass guitar. The song includes reggae electric guitar, horn, and percussion like bongos. The keyboard provides<br>lush chords. The time signature is 4/4. The chord progression is G, F, C.<br>2 This instrumental blues song goes very slow at a bpm of 50. You can hear the bass, harmonica and guitar grooving. The<br>harmonica plays a solo over the harmonious guitar and bass.<br>3 This classical piece is a waltz played by a string quartet. It includes two violins, a viola, and a cello, the beat counts to 3.<br>It sounds elegant, and has a strong first beat. It has a natural and danceable rhythm. The mood is romantic. The chord<br>progression is Em, Am, D, G.<br>4 African drums are playing a complex rhythm while a male vocalist chants a ritual. The atmosphere is mesmerizing. The<br>complex drumming pattern is a mesmerizing blend of syncopation, polyrhythms, and intricate patterns. It takes place<br>somewhere in the wilderness, or in an indigenous village.<br>5 This rock piece with guitars and drums is loud but fades out later on and becomes softer. It sounds powerful yet<br>melancholic. Itis instrumental only. A bass guitar provides a steady beat, enhancing the groove and energy of the song.<br>6 A single bass instrument is playing a running baseline. It has a jazzy feeling to it and sounds mellow. This could be<br>played in a jazz club. The tempo is 120 bpm.<br>7 This is a hip hop song. It has two rappers taking turns, one female and one male. An electronic synth melody sample in<br>the background keeps on looping. We can hear electronic beats and sometimes record-scratching sound effects.<br>8 A smooth jazz song with saxophone, drums and guitar with a chord progression of Dm7, G7, Cmaj7. The song is relaxed<br>and slow. There are no vocals, it is instrumental only. The saxophone produces a velvety tone that delivers an emotive<br>melody.<br>9 A piano plays a soothing popular instrumental song that could serve as background music in a restaurant. There is only<br>piano playing, no other instruments. There is a piano melody with background piano chords of Am, Fmaj7, Cmaj7, and<br>G. The tempo is unhurried. The melody is gentle and soothing, evoking a sense of nostalgia and comfort.<br>10 Indian folk music with a sitar and female vocals. It evokes a sense of zen and elevation. A sitar player begins with a<br>gentle and melodic introduction, plucking the strings with precision and emotion. There are rhythmic beats of traditional<br>hand percussion instruments, such as the tabla. It could be played at a cultural festival to showcase Indian culture.<br>11 This is a melodic and energetic rock ballad with a male vocalist. It has a country vibe and is of alternative or popfolk<br>genre. The electric and acoustic guitars and the bass create the background, while the drums give a regular beat. The<br>singer's voice is complemented by a piano.<br>12 This is a slow classical piece with violins and pianos. It has a film score feel and is instrumental only. The orchestration<br>is soft, with strings and flutes.<br>13 This fast and energetic rock song is performed by a male singer. The genre is alternative or punk rock. The background<br>is formed by a guitar, an electric guitar, bass, and drums. There is also a synthesizer.<br>14 This is a slow and ambient instrumental piece with a soundscape that feels like space. The atmosphere is meditative and<br>relaxing but with a certain darkness to it. The genre is electronic soundtrack, and the music is completely instrumental<br>with a synthesizer, bass, and drums forming the background. This song goes at 167.0 beats per minute.<br>15 This is an instrumental piece with Indian and classical elements. The sitar, violin, and flute play prominent roles in<br>creating a meditative and relaxing mood. The percussion and guitar provide a background rhythm to this world and jazz<br>fusion.</p>",
            "id": 221,
            "page": 20,
            "text": "1 This piece is an instrumental reggae song that is very chill and slow. There is no singer. It is relaxing to hear the groove with the bass guitar. The song includes reggae electric guitar, horn, and percussion like bongos. The keyboard provides lush chords. The time signature is 4/4. The chord progression is G, F, C. 2 This instrumental blues song goes very slow at a bpm of 50. You can hear the bass, harmonica and guitar grooving. The harmonica plays a solo over the harmonious guitar and bass. 3 This classical piece is a waltz played by a string quartet. It includes two violins, a viola, and a cello, the beat counts to 3. It sounds elegant, and has a strong first beat. It has a natural and danceable rhythm. The mood is romantic. The chord progression is Em, Am, D, G. 4 African drums are playing a complex rhythm while a male vocalist chants a ritual. The atmosphere is mesmerizing. The complex drumming pattern is a mesmerizing blend of syncopation, polyrhythms, and intricate patterns. It takes place somewhere in the wilderness, or in an indigenous village. 5 This rock piece with guitars and drums is loud but fades out later on and becomes softer. It sounds powerful yet melancholic. Itis instrumental only. A bass guitar provides a steady beat, enhancing the groove and energy of the song. 6 A single bass instrument is playing a running baseline. It has a jazzy feeling to it and sounds mellow. This could be played in a jazz club. The tempo is 120 bpm. 7 This is a hip hop song. It has two rappers taking turns, one female and one male. An electronic synth melody sample in the background keeps on looping. We can hear electronic beats and sometimes record-scratching sound effects. 8 A smooth jazz song with saxophone, drums and guitar with a chord progression of Dm7, G7, Cmaj7. The song is relaxed and slow. There are no vocals, it is instrumental only. The saxophone produces a velvety tone that delivers an emotive melody. 9 A piano plays a soothing popular instrumental song that could serve as background music in a restaurant. There is only piano playing, no other instruments. There is a piano melody with background piano chords of Am, Fmaj7, Cmaj7, and G. The tempo is unhurried. The melody is gentle and soothing, evoking a sense of nostalgia and comfort. 10 Indian folk music with a sitar and female vocals. It evokes a sense of zen and elevation. A sitar player begins with a gentle and melodic introduction, plucking the strings with precision and emotion. There are rhythmic beats of traditional hand percussion instruments, such as the tabla. It could be played at a cultural festival to showcase Indian culture. 11 This is a melodic and energetic rock ballad with a male vocalist. It has a country vibe and is of alternative or popfolk genre. The electric and acoustic guitars and the bass create the background, while the drums give a regular beat. The singer's voice is complemented by a piano. 12 This is a slow classical piece with violins and pianos. It has a film score feel and is instrumental only. The orchestration is soft, with strings and flutes. 13 This fast and energetic rock song is performed by a male singer. The genre is alternative or punk rock. The background is formed by a guitar, an electric guitar, bass, and drums. There is also a synthesizer. 14 This is a slow and ambient instrumental piece with a soundscape that feels like space. The atmosphere is meditative and relaxing but with a certain darkness to it. The genre is electronic soundtrack, and the music is completely instrumental with a synthesizer, bass, and drums forming the background. This song goes at 167.0 beats per minute. 15 This is an instrumental piece with Indian and classical elements. The sitar, violin, and flute play prominent roles in creating a meditative and relaxing mood. The percussion and guitar provide a background rhythm to this world and jazz fusion."
        },
        {
            "bounding_box": [
                {
                    "x": 285,
                    "y": 2181
                },
                {
                    "x": 2194,
                    "y": 2181
                },
                {
                    "x": 2194,
                    "y": 2286
                },
                {
                    "x": 285,
                    "y": 2286
                }
            ],
            "category": "paragraph",
            "html": "<p id='222' style='font-size:16px'>Table 6: Custom captions used for the general listening test. Captions in the top part were used in both first and<br>second runs, captions in the bottom part were used in the second run only.</p>",
            "id": 222,
            "page": 20,
            "text": "Table 6: Custom captions used for the general listening test. Captions in the top part were used in both first and second runs, captions in the bottom part were used in the second run only."
        },
        {
            "bounding_box": [
                {
                    "x": 285,
                    "y": 2339
                },
                {
                    "x": 2198,
                    "y": 2339
                },
                {
                    "x": 2198,
                    "y": 2906
                },
                {
                    "x": 285,
                    "y": 2906
                }
            ],
            "category": "paragraph",
            "html": "<p id='223' style='font-size:18px'>Table 7 presents the 20 text prompts used for the expert listening studies. They consist of 10 contrasting<br>pairs written by music experts. Care was given to make sure that they were realistic and that there were<br>no contradicting elements in the prompts. For instance, caption 1 in Table 7 contrasts with caption 2.<br>They share the same original caption \"An instrumental blues melody played by a lead guitar and a<br>strumming acoustic guitar. The acoustic guitarist's strumming keeps the rhythm steady. · However, the<br>control sentences are different: \"The chord sequence is G7, F7, C7, G7. This song goes at 100 beats per<br>minute. , , \"The chord sequence is Dm, Am, Em. This song goes at 60 beats per minute. , , Both<br>versus<br>·<br>chord sequences come from blues progressions, but they belong to a different key/mode. The tempo of<br>caption 2 is significantly slower. Such captions are ideally suited to test if the control sentences influence<br>the generated music.</p>",
            "id": 223,
            "page": 20,
            "text": "Table 7 presents the 20 text prompts used for the expert listening studies. They consist of 10 contrasting pairs written by music experts. Care was given to make sure that they were realistic and that there were no contradicting elements in the prompts. For instance, caption 1 in Table 7 contrasts with caption 2. They share the same original caption \"An instrumental blues melody played by a lead guitar and a strumming acoustic guitar. The acoustic guitarist's strumming keeps the rhythm steady. · However, the control sentences are different: \"The chord sequence is G7, F7, C7, G7. This song goes at 100 beats per minute. , , \"The chord sequence is Dm, Am, Em. This song goes at 60 beats per minute. , , Both versus · chord sequences come from blues progressions, but they belong to a different key/mode. The tempo of caption 2 is significantly slower. Such captions are ideally suited to test if the control sentences influence the generated music."
        },
        {
            "bounding_box": [
                {
                    "x": 305,
                    "y": 504
                },
                {
                    "x": 2206,
                    "y": 504
                },
                {
                    "x": 2206,
                    "y": 2916
                },
                {
                    "x": 305,
                    "y": 2916
                }
            ],
            "category": "paragraph",
            "html": "<p id='224' style='font-size:14px'>1 An instrumental blues melody played by a lead guitar and a strumming acoustic guitar. The acoustic guitarist's strumming<br>keeps the rhythm steady. The chord sequence is G7, F7, C7, G7. This song goes at 100 beats per minute.<br>2 An instrumental blues melody played by a lead guitar and a strumming acoustic guitar. The acoustic guitarist's strumming<br>keeps the rhythm steady. The chord sequence is Dm, Am, Em. This song goes at 60 beats per minute.<br>3 A piano plays a popular melody over the chords of Am, Fmaj7, Cmaj7, G. There is only piano playing, no other<br>instruments or voice. The tempo is Adagio.<br>4 A piano plays a popular melody over the chords of Gm, Bb, Eb. There is only piano playing, no other instruments or<br>voice. The tempo is Vivace.<br>5 This is an intense and loud punk song with guitars and drums. It is instrumental only. It is very energetic and powerful.<br>The thunderous beats of the drummer provide a pounding rhythm. A guitar solo melody emerges from the chaotic<br>background of the chords. The chord progression is A, D, E. The tempo of the song is 160 bpm.<br>6 This is an intense and loud punk song with guitars and drums. It is instrumental only. It is very energetic and powerful.<br>The thunderous beats of the drummer provide a pounding rhythm. A guitar solo melody emerges from the chaotic<br>background of the chords. The chord progression is C, B, A, G. The tempo of the song is 100 bpm.<br>7 A slow paced jazz song played by a saxophone, piano, guitar and drums follows a chord progression of Em7b5, A7,<br>Dm7. The pianist produces delicate harmonies and subtle embellishments. The drummer provides a brushed rhythm.<br>The guitar strums softly, while the saxophone plays a solo over the chords. This song goes at 80 beats per minute.<br>8 A slow paced jazz song played by a saxophone, piano, guitar and drums follows a chord progression of B7, G7, E7, C7.<br>The drummer provides a brushed rhythm. The guitar strums softly, while the saxophone plays a solo over the chords.<br>This song goes at 115 beats per minute.<br>9 This is a techno piece with drums and beats and a leading melody. A synth plays chords. The music kicks off with a<br>powerful and relentless drumbeat. Over the pounding beats, a leading melody emerges. It has strong danceability and<br>can be played in a club. The tempo is 120 bpm. The chords played by the synth are Am, Cm, Dm, Gm.<br>10 This is a techno piece with drums and beats and a leading melody. A synth plays chords. The music kicks off with a<br>powerful and relentless drumbeat. Over the pounding beats, a leading melody emerges. It has strong danceability and<br>can be played in a club. The tempo is 160 bpm. The chords played by the synth are C, F, G.<br>11 A horn and a bass guitar groove to a reggae tune. The combination of the horn section 's catchy melodies and the buoyant<br>bassline creates an irresistible groove. The bassline is bouncy and lively. The song is played at the pace of Adagio. An<br>electric keyboard plays the chords Am, Dm, G, C.<br>12 A horn and a bass guitar groove to a reggae tune. The combination of the horn section's catchy melodies and the buoyant<br>bassline creates an irresistible groove. The bassline is bouncy and lively. The song is played at the pace of Moderato. An<br>electric keyboard plays the chords E, B, A.<br>13 This is a metal song with a guitar, drums and bass guitar. The bassist, wielding a solid-bodied bass guitar, adds depth<br>and power to the sonic landscape. The drummer commands a massive drum kit. With a relentless force, they pound<br>out thunderous rhythms, driving the music forward. As the song begins, the guitar roars to life, delivering a series of<br>distorted chords. It follows the chords of Em, C, G, D. The tempo is 120 bpm.<br>14 This is a metal song with a guitar, drums and bass guitar. The bassist, wielding a solid-bodied bass guitar, adds depth<br>and power to the sonic landscape. The drummer commands a massive drum kit. With a relentless force, they pound<br>out thunderous rhythms, driving the music forward. As the song begins, the guitar roars to life, delivering a series of<br>distorted chords. It follows the chords of A, F#m, D, E. The tempo is 170 bpm.<br>15 A man sings a captivating folk song while strumming chords on an acoustic guitar. This fits a campfire evening happening.<br>The chord progression is G, C, D, G. The tempo is 100 beats per minute.<br>16 A man sings a captivating folk song while strumming chords on an acoustic guitar. This fits a campfire evening happening.<br>The chord progression is Am, Em, Dm, Am. The tempo is 70 beats per minute.<br>17 This is a classical music piece played by a string trio. The instruments involved are violin, viola, and cello. The violin<br>plays the lead melody. The cello's soulful and melodic contributions add depth and gravitas to the performance. The<br>time signature is 3/4. The tempo of this song is Presto. The chord sequence is E, C#m, A, B.<br>18 This is a classical music piece played by a string trio. The instruments involved are violin, viola, and cello. The violin<br>plays the lead melody. The cello's soulful and melodic contributions add depth and gravitas to the performance. The<br>time signature is 4/4. The tempo of this song is Andante. The chord sequence is Am, Dm, E7, Am.<br>19 This is a pop song with a female singer singing the leading melody and synthesizers looping samples as background.<br>These loops provide the song's electronic foundation, creating a rich and layered sonic landscape. The charismatic<br>female singer has a dynamic and emotive voice. The tempo is Moderato. The chord sequence is C, G, Am, F.<br>20 This is a pop song with a female singer singing the leading melody and synthesizers looping samples as background.<br>These loops provide the song's electronic foundation, creating a rich and layered sonic landscape. The charismatic<br>female singer has a dynamic and emotive voice. The tempo is Presto. The key is A minor and the chord sequences are<br>Am, Dm, E.</p>",
            "id": 224,
            "page": 21,
            "text": "1 An instrumental blues melody played by a lead guitar and a strumming acoustic guitar. The acoustic guitarist's strumming keeps the rhythm steady. The chord sequence is G7, F7, C7, G7. This song goes at 100 beats per minute. 2 An instrumental blues melody played by a lead guitar and a strumming acoustic guitar. The acoustic guitarist's strumming keeps the rhythm steady. The chord sequence is Dm, Am, Em. This song goes at 60 beats per minute. 3 A piano plays a popular melody over the chords of Am, Fmaj7, Cmaj7, G. There is only piano playing, no other instruments or voice. The tempo is Adagio. 4 A piano plays a popular melody over the chords of Gm, Bb, Eb. There is only piano playing, no other instruments or voice. The tempo is Vivace. 5 This is an intense and loud punk song with guitars and drums. It is instrumental only. It is very energetic and powerful. The thunderous beats of the drummer provide a pounding rhythm. A guitar solo melody emerges from the chaotic background of the chords. The chord progression is A, D, E. The tempo of the song is 160 bpm. 6 This is an intense and loud punk song with guitars and drums. It is instrumental only. It is very energetic and powerful. The thunderous beats of the drummer provide a pounding rhythm. A guitar solo melody emerges from the chaotic background of the chords. The chord progression is C, B, A, G. The tempo of the song is 100 bpm. 7 A slow paced jazz song played by a saxophone, piano, guitar and drums follows a chord progression of Em7b5, A7, Dm7. The pianist produces delicate harmonies and subtle embellishments. The drummer provides a brushed rhythm. The guitar strums softly, while the saxophone plays a solo over the chords. This song goes at 80 beats per minute. 8 A slow paced jazz song played by a saxophone, piano, guitar and drums follows a chord progression of B7, G7, E7, C7. The drummer provides a brushed rhythm. The guitar strums softly, while the saxophone plays a solo over the chords. This song goes at 115 beats per minute. 9 This is a techno piece with drums and beats and a leading melody. A synth plays chords. The music kicks off with a powerful and relentless drumbeat. Over the pounding beats, a leading melody emerges. It has strong danceability and can be played in a club. The tempo is 120 bpm. The chords played by the synth are Am, Cm, Dm, Gm. 10 This is a techno piece with drums and beats and a leading melody. A synth plays chords. The music kicks off with a powerful and relentless drumbeat. Over the pounding beats, a leading melody emerges. It has strong danceability and can be played in a club. The tempo is 160 bpm. The chords played by the synth are C, F, G. 11 A horn and a bass guitar groove to a reggae tune. The combination of the horn section 's catchy melodies and the buoyant bassline creates an irresistible groove. The bassline is bouncy and lively. The song is played at the pace of Adagio. An electric keyboard plays the chords Am, Dm, G, C. 12 A horn and a bass guitar groove to a reggae tune. The combination of the horn section's catchy melodies and the buoyant bassline creates an irresistible groove. The bassline is bouncy and lively. The song is played at the pace of Moderato. An electric keyboard plays the chords E, B, A. 13 This is a metal song with a guitar, drums and bass guitar. The bassist, wielding a solid-bodied bass guitar, adds depth and power to the sonic landscape. The drummer commands a massive drum kit. With a relentless force, they pound out thunderous rhythms, driving the music forward. As the song begins, the guitar roars to life, delivering a series of distorted chords. It follows the chords of Em, C, G, D. The tempo is 120 bpm. 14 This is a metal song with a guitar, drums and bass guitar. The bassist, wielding a solid-bodied bass guitar, adds depth and power to the sonic landscape. The drummer commands a massive drum kit. With a relentless force, they pound out thunderous rhythms, driving the music forward. As the song begins, the guitar roars to life, delivering a series of distorted chords. It follows the chords of A, F#m, D, E. The tempo is 170 bpm. 15 A man sings a captivating folk song while strumming chords on an acoustic guitar. This fits a campfire evening happening. The chord progression is G, C, D, G. The tempo is 100 beats per minute. 16 A man sings a captivating folk song while strumming chords on an acoustic guitar. This fits a campfire evening happening. The chord progression is Am, Em, Dm, Am. The tempo is 70 beats per minute. 17 This is a classical music piece played by a string trio. The instruments involved are violin, viola, and cello. The violin plays the lead melody. The cello's soulful and melodic contributions add depth and gravitas to the performance. The time signature is 3/4. The tempo of this song is Presto. The chord sequence is E, C#m, A, B. 18 This is a classical music piece played by a string trio. The instruments involved are violin, viola, and cello. The violin plays the lead melody. The cello's soulful and melodic contributions add depth and gravitas to the performance. The time signature is 4/4. The tempo of this song is Andante. The chord sequence is Am, Dm, E7, Am. 19 This is a pop song with a female singer singing the leading melody and synthesizers looping samples as background. These loops provide the song's electronic foundation, creating a rich and layered sonic landscape. The charismatic female singer has a dynamic and emotive voice. The tempo is Moderato. The chord sequence is C, G, Am, F. 20 This is a pop song with a female singer singing the leading melody and synthesizers looping samples as background. These loops provide the song's electronic foundation, creating a rich and layered sonic landscape. The charismatic female singer has a dynamic and emotive voice. The tempo is Presto. The key is A minor and the chord sequences are Am, Dm, E."
        },
        {
            "bounding_box": [
                {
                    "x": 642,
                    "y": 2945
                },
                {
                    "x": 1830,
                    "y": 2945
                },
                {
                    "x": 1830,
                    "y": 3003
                },
                {
                    "x": 642,
                    "y": 3003
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='225' style='font-size:18px'>Table 7: Custom opposing captions created for the control experiment.</p>",
            "id": 225,
            "page": 21,
            "text": "Table 7: Custom opposing captions created for the control experiment."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 296
                },
                {
                    "x": 1271,
                    "y": 296
                },
                {
                    "x": 1271,
                    "y": 354
                },
                {
                    "x": 288,
                    "y": 354
                }
            ],
            "category": "paragraph",
            "html": "<p id='226' style='font-size:22px'>K Additional Examples of Generated Music</p>",
            "id": 226,
            "page": 22,
            "text": "K Additional Examples of Generated Music"
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 384
                },
                {
                    "x": 2194,
                    "y": 384
                },
                {
                    "x": 2194,
                    "y": 502
                },
                {
                    "x": 286,
                    "y": 502
                }
            ],
            "category": "paragraph",
            "html": "<p id='227' style='font-size:20px'>Here we show additional samples generated from pre-trained Tango fine-tuned on MusicCaps, Tango<br>finetuned on MusicBench, Mustango, Musi cGen-M, and AudioLDM2, all generated from the same prompts.</p>",
            "id": 227,
            "page": 22,
            "text": "Here we show additional samples generated from pre-trained Tango fine-tuned on MusicCaps, Tango finetuned on MusicBench, Mustango, Musi cGen-M, and AudioLDM2, all generated from the same prompts."
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 551
                },
                {
                    "x": 2197,
                    "y": 551
                },
                {
                    "x": 2197,
                    "y": 728
                },
                {
                    "x": 286,
                    "y": 728
                }
            ],
            "category": "paragraph",
            "html": "<p id='228' style='font-size:18px'>Prompt: A horn and a bass guitar groove to a reggae tune. The combination of the horn section's<br>catchy melodies and the buoyant bassline creates an irresistible groove. The bassline is bouncy and lively.<br>The song is played at the pace of Adagio. An electric keyboard plays the chords Am, Dm, G, and C.</p>",
            "id": 228,
            "page": 22,
            "text": "Prompt: A horn and a bass guitar groove to a reggae tune. The combination of the horn section's catchy melodies and the buoyant bassline creates an irresistible groove. The bassline is bouncy and lively. The song is played at the pace of Adagio. An electric keyboard plays the chords Am, Dm, G, and C."
        },
        {
            "bounding_box": [
                {
                    "x": 574,
                    "y": 764
                },
                {
                    "x": 1896,
                    "y": 764
                },
                {
                    "x": 1896,
                    "y": 1293
                },
                {
                    "x": 574,
                    "y": 1293
                }
            ],
            "category": "figure",
            "html": "<figure><img id='229' style='font-size:14px' alt=\"sedo ob\" data-coord=\"top-left:(574,764); bottom-right:(1896,1293)\" /></figure>",
            "id": 229,
            "page": 22,
            "text": "sedo ob"
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 1330
                },
                {
                    "x": 2194,
                    "y": 1330
                },
                {
                    "x": 2194,
                    "y": 1485
                },
                {
                    "x": 286,
                    "y": 1485
                }
            ],
            "category": "paragraph",
            "html": "<p id='230' style='font-size:14px'>Figure 7: Mel-spectrogram of a reggae sample generated by pre-trained Tango fine-tuned on MusicCaps with<br>vertical lines showing perceived chord starts. The blue box shows an area of dissonance in the music. Overall, the<br>audio is a bit noisy.</p>",
            "id": 230,
            "page": 22,
            "text": "Figure 7: Mel-spectrogram of a reggae sample generated by pre-trained Tango fine-tuned on MusicCaps with vertical lines showing perceived chord starts. The blue box shows an area of dissonance in the music. Overall, the audio is a bit noisy."
        },
        {
            "bounding_box": [
                {
                    "x": 575,
                    "y": 1580
                },
                {
                    "x": 1902,
                    "y": 1580
                },
                {
                    "x": 1902,
                    "y": 2113
                },
                {
                    "x": 575,
                    "y": 2113
                }
            ],
            "category": "figure",
            "html": "<figure><img id='231' alt=\"\" data-coord=\"top-left:(575,1580); bottom-right:(1902,2113)\" /></figure>",
            "id": 231,
            "page": 22,
            "text": ""
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 2148
                },
                {
                    "x": 2194,
                    "y": 2148
                },
                {
                    "x": 2194,
                    "y": 2252
                },
                {
                    "x": 287,
                    "y": 2252
                }
            ],
            "category": "caption",
            "html": "<caption id='232' style='font-size:14px'>Figure 8: Mel-spectrogram of a reggae sample generated by pre-trained Tango fine-tuned on Musi cBench with<br>vertical lines showing perceived chord starts. There are too many chords here.</caption>",
            "id": 232,
            "page": 22,
            "text": "Figure 8: Mel-spectrogram of a reggae sample generated by pre-trained Tango fine-tuned on Musi cBench with vertical lines showing perceived chord starts. There are too many chords here."
        },
        {
            "bounding_box": [
                {
                    "x": 574,
                    "y": 2343
                },
                {
                    "x": 1902,
                    "y": 2343
                },
                {
                    "x": 1902,
                    "y": 2877
                },
                {
                    "x": 574,
                    "y": 2877
                }
            ],
            "category": "figure",
            "html": "<figure><img id='233' alt=\"\" data-coord=\"top-left:(574,2343); bottom-right:(1902,2877)\" /></figure>",
            "id": 233,
            "page": 22,
            "text": ""
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 2914
                },
                {
                    "x": 2193,
                    "y": 2914
                },
                {
                    "x": 2193,
                    "y": 3021
                },
                {
                    "x": 287,
                    "y": 3021
                }
            ],
            "category": "caption",
            "html": "<caption id='234' style='font-size:16px'>Figure 9: Mel-spectrogram of a reggae sample generated by Mustango with vertical lines showing perceived chord<br>starts. The chords match the prompt.</caption>",
            "id": 234,
            "page": 22,
            "text": "Figure 9: Mel-spectrogram of a reggae sample generated by Mustango with vertical lines showing perceived chord starts. The chords match the prompt."
        },
        {
            "bounding_box": [
                {
                    "x": 285,
                    "y": 298
                },
                {
                    "x": 2199,
                    "y": 298
                },
                {
                    "x": 2199,
                    "y": 583
                },
                {
                    "x": 285,
                    "y": 583
                }
            ],
            "category": "paragraph",
            "html": "<p id='235' style='font-size:22px'>Prompt: This is a metal song with a guitar, drums and bass guitar. The bassist, wielding a solid-bodied<br>bass guitar, adds depth and power to the sonic landscape. The drummer commands a massive drum kit.<br>With a relentless force, they pound out thunderous rhythms, driving the music forward. As the song<br>begins, the guitar roars to life, delivering a series of distorted chords. It follows the chords of Em, C, G, D.<br>The tempo is 120 bpm.</p>",
            "id": 235,
            "page": 23,
            "text": "Prompt: This is a metal song with a guitar, drums and bass guitar. The bassist, wielding a solid-bodied bass guitar, adds depth and power to the sonic landscape. The drummer commands a massive drum kit. With a relentless force, they pound out thunderous rhythms, driving the music forward. As the song begins, the guitar roars to life, delivering a series of distorted chords. It follows the chords of Em, C, G, D. The tempo is 120 bpm."
        },
        {
            "bounding_box": [
                {
                    "x": 574,
                    "y": 622
                },
                {
                    "x": 1901,
                    "y": 622
                },
                {
                    "x": 1901,
                    "y": 1153
                },
                {
                    "x": 574,
                    "y": 1153
                }
            ],
            "category": "figure",
            "html": "<figure><img id='236' alt=\"\" data-coord=\"top-left:(574,622); bottom-right:(1901,1153)\" /></figure>",
            "id": 236,
            "page": 23,
            "text": ""
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 1184
                },
                {
                    "x": 2194,
                    "y": 1184
                },
                {
                    "x": 2194,
                    "y": 1291
                },
                {
                    "x": 286,
                    "y": 1291
                }
            ],
            "category": "caption",
            "html": "<caption id='237' style='font-size:18px'>Figure 10: Mel-spectrogram of a metal song sample generated by pre-trained Tango fine-tuned on MusicCaps. Itis<br>very noisy from the very start.</caption>",
            "id": 237,
            "page": 23,
            "text": "Figure 10: Mel-spectrogram of a metal song sample generated by pre-trained Tango fine-tuned on MusicCaps. Itis very noisy from the very start."
        },
        {
            "bounding_box": [
                {
                    "x": 574,
                    "y": 1383
                },
                {
                    "x": 1902,
                    "y": 1383
                },
                {
                    "x": 1902,
                    "y": 1914
                },
                {
                    "x": 574,
                    "y": 1914
                }
            ],
            "category": "figure",
            "html": "<figure><img id='238' style='font-size:14px' alt=\"dO\" data-coord=\"top-left:(574,1383); bottom-right:(1902,1914)\" /></figure>",
            "id": 238,
            "page": 23,
            "text": "dO"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 1950
                },
                {
                    "x": 2196,
                    "y": 1950
                },
                {
                    "x": 2196,
                    "y": 2053
                },
                {
                    "x": 287,
                    "y": 2053
                }
            ],
            "category": "caption",
            "html": "<caption id='239' style='font-size:16px'>Figure 11: Mel-spectrogram of a metal song sample generated by pre-trained Tango fine-tuned on Musi cBench.<br>The song starts with 4 beats from the drummer, but there is a bit of noise from the start.</caption>",
            "id": 239,
            "page": 23,
            "text": "Figure 11: Mel-spectrogram of a metal song sample generated by pre-trained Tango fine-tuned on Musi cBench. The song starts with 4 beats from the drummer, but there is a bit of noise from the start."
        },
        {
            "bounding_box": [
                {
                    "x": 574,
                    "y": 2146
                },
                {
                    "x": 1902,
                    "y": 2146
                },
                {
                    "x": 1902,
                    "y": 2680
                },
                {
                    "x": 574,
                    "y": 2680
                }
            ],
            "category": "figure",
            "html": "<figure><img id='240' alt=\"\" data-coord=\"top-left:(574,2146); bottom-right:(1902,2680)\" /></figure>",
            "id": 240,
            "page": 23,
            "text": ""
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 2716
                },
                {
                    "x": 2192,
                    "y": 2716
                },
                {
                    "x": 2192,
                    "y": 2821
                },
                {
                    "x": 287,
                    "y": 2821
                }
            ],
            "category": "caption",
            "html": "<caption id='241' style='font-size:20px'>Figure 12: Mel-spectrogram of a metal sample generated by Mustango. The song starts with 4 distinguishable beats<br>from the drummer, then the guitars join.</caption>",
            "id": 241,
            "page": 23,
            "text": "Figure 12: Mel-spectrogram of a metal sample generated by Mustango. The song starts with 4 distinguishable beats from the drummer, then the guitars join."
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 299
                },
                {
                    "x": 2198,
                    "y": 299
                },
                {
                    "x": 2198,
                    "y": 468
                },
                {
                    "x": 286,
                    "y": 468
                }
            ],
            "category": "paragraph",
            "html": "<p id='242' style='font-size:20px'>Prompt: This is a classical music piece. There are violins playing a lead theme, with a double bass and<br>cymbals in the background. It is a melancholic, rather sad piece. The music builds up in volume gradually.<br>The key is A minor. The chord sequence is Am, C, Am.</p>",
            "id": 242,
            "page": 24,
            "text": "Prompt: This is a classical music piece. There are violins playing a lead theme, with a double bass and cymbals in the background. It is a melancholic, rather sad piece. The music builds up in volume gradually. The key is A minor. The chord sequence is Am, C, Am."
        },
        {
            "bounding_box": [
                {
                    "x": 572,
                    "y": 508
                },
                {
                    "x": 1901,
                    "y": 508
                },
                {
                    "x": 1901,
                    "y": 1041
                },
                {
                    "x": 572,
                    "y": 1041
                }
            ],
            "category": "figure",
            "html": "<figure><img id='243' alt=\"\" data-coord=\"top-left:(572,508); bottom-right:(1901,1041)\" /></figure>",
            "id": 243,
            "page": 24,
            "text": ""
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 1073
                },
                {
                    "x": 2192,
                    "y": 1073
                },
                {
                    "x": 2192,
                    "y": 1178
                },
                {
                    "x": 287,
                    "y": 1178
                }
            ],
            "category": "paragraph",
            "html": "<p id='244' style='font-size:18px'>Figure 13: Mel-spectrogram of a classical music piece generated by Mustango. The effect of gradual volume<br>increase (crescendo) is apparent (red color envelope around waveform).</p>",
            "id": 244,
            "page": 24,
            "text": "Figure 13: Mel-spectrogram of a classical music piece generated by Mustango. The effect of gradual volume increase (crescendo) is apparent (red color envelope around waveform)."
        },
        {
            "bounding_box": [
                {
                    "x": 573,
                    "y": 1274
                },
                {
                    "x": 1901,
                    "y": 1274
                },
                {
                    "x": 1901,
                    "y": 1799
                },
                {
                    "x": 573,
                    "y": 1799
                }
            ],
            "category": "figure",
            "html": "<figure><img id='245' style='font-size:14px' alt=\"\n12\" data-coord=\"top-left:(573,1274); bottom-right:(1901,1799)\" /></figure>",
            "id": 245,
            "page": 24,
            "text": " 12"
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 1832
                },
                {
                    "x": 2194,
                    "y": 1832
                },
                {
                    "x": 2194,
                    "y": 1942
                },
                {
                    "x": 286,
                    "y": 1942
                }
            ],
            "category": "caption",
            "html": "<caption id='246' style='font-size:16px'>Figure 14: Mel-spectrogram of a classical music piece generated by Musi cGen-M. The effect of gradual volume<br>increase (crescendo) is not clear (red color envelope around waveform).</caption>",
            "id": 246,
            "page": 24,
            "text": "Figure 14: Mel-spectrogram of a classical music piece generated by Musi cGen-M. The effect of gradual volume increase (crescendo) is not clear (red color envelope around waveform)."
        },
        {
            "bounding_box": [
                {
                    "x": 572,
                    "y": 2035
                },
                {
                    "x": 1900,
                    "y": 2035
                },
                {
                    "x": 1900,
                    "y": 2565
                },
                {
                    "x": 572,
                    "y": 2565
                }
            ],
            "category": "figure",
            "html": "<figure><img id='247' alt=\"\" data-coord=\"top-left:(572,2035); bottom-right:(1900,2565)\" /></figure>",
            "id": 247,
            "page": 24,
            "text": ""
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 2596
                },
                {
                    "x": 2192,
                    "y": 2596
                },
                {
                    "x": 2192,
                    "y": 2702
                },
                {
                    "x": 286,
                    "y": 2702
                }
            ],
            "category": "caption",
            "html": "<caption id='248' style='font-size:18px'>Figure 15: Mel-spectrogram of a classical music piece generated by Audi oLDM-2. The effect of gradual volume<br>increase (crescendo) is not present (red color envelope around waveform).</caption>",
            "id": 248,
            "page": 24,
            "text": "Figure 15: Mel-spectrogram of a classical music piece generated by Audi oLDM-2. The effect of gradual volume increase (crescendo) is not present (red color envelope around waveform)."
        }
    ]
}