{
    "id": "32b1befa-0f94-11ef-8828-426932df3dcf",
    "pdf_path": "/root/data/pdf/1708.05031v2.pdf",
    "elements": [
        {
            "bounding_box": [
                {
                    "x": 740,
                    "y": 288
                },
                {
                    "x": 1808,
                    "y": 288
                },
                {
                    "x": 1808,
                    "y": 379
                },
                {
                    "x": 740,
                    "y": 379
                }
            ],
            "category": "paragraph",
            "html": "<p id='0' style='font-size:22px'>Neural Collaborative Filtering</p>",
            "id": 0,
            "page": 1,
            "text": "Neural Collaborative Filtering"
        },
        {
            "bounding_box": [
                {
                    "x": 382,
                    "y": 483
                },
                {
                    "x": 935,
                    "y": 483
                },
                {
                    "x": 935,
                    "y": 679
                },
                {
                    "x": 382,
                    "y": 679
                }
            ],
            "category": "paragraph",
            "html": "<p id='1' style='font-size:20px'>Xiangnan He<br>National University of<br>Singapore, Singapore<br>xiangnanhe@gmail.com</p>",
            "id": 1,
            "page": 1,
            "text": "Xiangnan He National University of Singapore, Singapore xiangnanhe@gmail.com"
        },
        {
            "bounding_box": [
                {
                    "x": 406,
                    "y": 698
                },
                {
                    "x": 912,
                    "y": 698
                },
                {
                    "x": 912,
                    "y": 886
                },
                {
                    "x": 406,
                    "y": 886
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='2' style='font-size:22px'>Liqiang Nie<br>Shandong University<br>China<br>nieliqiang@gmail.com</p>",
            "id": 2,
            "page": 1,
            "text": "Liqiang Nie Shandong University China nieliqiang@gmail.com"
        },
        {
            "bounding_box": [
                {
                    "x": 219,
                    "y": 946
                },
                {
                    "x": 493,
                    "y": 946
                },
                {
                    "x": 493,
                    "y": 996
                },
                {
                    "x": 219,
                    "y": 996
                }
            ],
            "category": "paragraph",
            "html": "<p id='3' style='font-size:18px'>ABSTRACT</p>",
            "id": 3,
            "page": 1,
            "text": "ABSTRACT"
        },
        {
            "bounding_box": [
                {
                    "x": 216,
                    "y": 1014
                },
                {
                    "x": 1229,
                    "y": 1014
                },
                {
                    "x": 1229,
                    "y": 1363
                },
                {
                    "x": 216,
                    "y": 1363
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='4' style='font-size:16px'>In recent years, deep neural networks have yielded immense<br>success on speech recognition, computer vision and natural<br>language processing. However, the exploration of deep neu-<br>ral networks on recommender systems has received relatively<br>less scrutiny. In this work, we strive to develop techniques<br>based on neural networks to tackle the key problem in rec-<br>ommendation collaborative filtering on the basis of<br>implicit feedback.</p>",
            "id": 4,
            "page": 1,
            "text": "In recent years, deep neural networks have yielded immense success on speech recognition, computer vision and natural language processing. However, the exploration of deep neural networks on recommender systems has received relatively less scrutiny. In this work, we strive to develop techniques based on neural networks to tackle the key problem in recommendation collaborative filtering on the basis of implicit feedback."
        },
        {
            "bounding_box": [
                {
                    "x": 217,
                    "y": 1365
                },
                {
                    "x": 1230,
                    "y": 1365
                },
                {
                    "x": 1230,
                    "y": 1711
                },
                {
                    "x": 217,
                    "y": 1711
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='5' style='font-size:16px'>Although some recent work has employed deep learning<br>for recommendation, they primarily used it to model auxil-<br>iary information, such as textual descriptions of items and<br>acoustic features of musics. When it comes to model the key<br>factor in collaborative filtering the interaction between<br>user and item features, they still resorted to matrix factor-<br>ization and applied an inner product on the latent features<br>of users and items.</p>",
            "id": 5,
            "page": 1,
            "text": "Although some recent work has employed deep learning for recommendation, they primarily used it to model auxiliary information, such as textual descriptions of items and acoustic features of musics. When it comes to model the key factor in collaborative filtering the interaction between user and item features, they still resorted to matrix factorization and applied an inner product on the latent features of users and items."
        },
        {
            "bounding_box": [
                {
                    "x": 217,
                    "y": 1713
                },
                {
                    "x": 1228,
                    "y": 1713
                },
                {
                    "x": 1228,
                    "y": 2240
                },
                {
                    "x": 217,
                    "y": 2240
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='6' style='font-size:16px'>By replacing the inner product with a neural architecture<br>that can learn an arbitrary function from data, we present<br>a general framework named NCF, short for Neural network-<br>based Collaborative Filtering. NCF is generic and can ex-<br>press and generalize matrix factorization under its frame-<br>work. To supercharge NCF modelling with non-linearities,<br>we propose to leverage a multi-layer perceptron to learn the<br>user-item interaction function. Extensive experiments on<br>two real- world datasets show significant improvements of our<br>proposed NCF framework over the state-of-the-art methods.<br>Empirical evidence shows that using deeper layers of neural<br>networks offers better recommendation performance.</p>",
            "id": 6,
            "page": 1,
            "text": "By replacing the inner product with a neural architecture that can learn an arbitrary function from data, we present a general framework named NCF, short for Neural networkbased Collaborative Filtering. NCF is generic and can express and generalize matrix factorization under its framework. To supercharge NCF modelling with non-linearities, we propose to leverage a multi-layer perceptron to learn the user-item interaction function. Extensive experiments on two real- world datasets show significant improvements of our proposed NCF framework over the state-of-the-art methods. Empirical evidence shows that using deeper layers of neural networks offers better recommendation performance."
        },
        {
            "bounding_box": [
                {
                    "x": 218,
                    "y": 2267
                },
                {
                    "x": 444,
                    "y": 2267
                },
                {
                    "x": 444,
                    "y": 2322
                },
                {
                    "x": 218,
                    "y": 2322
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='7' style='font-size:22px'>Keywords</p>",
            "id": 7,
            "page": 1,
            "text": "Keywords"
        },
        {
            "bounding_box": [
                {
                    "x": 219,
                    "y": 2336
                },
                {
                    "x": 1225,
                    "y": 2336
                },
                {
                    "x": 1225,
                    "y": 2424
                },
                {
                    "x": 219,
                    "y": 2424
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='8' style='font-size:18px'>Collaborative Filtering, Neural Networks, Deep Learning,<br>Matrix Factorization, Implicit Feedback</p>",
            "id": 8,
            "page": 1,
            "text": "Collaborative Filtering, Neural Networks, Deep Learning, Matrix Factorization, Implicit Feedback"
        },
        {
            "bounding_box": [
                {
                    "x": 219,
                    "y": 2453
                },
                {
                    "x": 1226,
                    "y": 2453
                },
                {
                    "x": 1226,
                    "y": 2577
                },
                {
                    "x": 219,
                    "y": 2577
                }
            ],
            "category": "paragraph",
            "html": "<p id='9' style='font-size:16px'>*NExT research is supported by the National Research<br>Foundation, Prime Minister's Office, Singapore under its<br>IRC@SG Funding Initiative.</p>",
            "id": 9,
            "page": 1,
            "text": "*NExT research is supported by the National Research Foundation, Prime Minister's Office, Singapore under its IRC@SG Funding Initiative."
        },
        {
            "bounding_box": [
                {
                    "x": 216,
                    "y": 2676
                },
                {
                    "x": 1117,
                    "y": 2676
                },
                {
                    "x": 1117,
                    "y": 2865
                },
                {
                    "x": 216,
                    "y": 2865
                }
            ],
            "category": "paragraph",
            "html": "<p id='10' style='font-size:14px'>ⓒ2017 International World Wide Web Conference Committee<br>(IW3C2), published under Creative Commons CC BY 4.0 License.<br>www 2017, April 3-7, 2017, Perth, Australia.<br>ACM 978-1-4503-4913-0/17/04.<br>http://ix.doi.org/10.1145/3038912.3052569</p>",
            "id": 10,
            "page": 1,
            "text": "ⓒ2017 International World Wide Web Conference Committee (IW3C2), published under Creative Commons CC BY 4.0 License. www 2017, April 3-7, 2017, Perth, Australia. ACM 978-1-4503-4913-0/17/04. http://ix.doi.org/10.1145/3038912.3052569"
        },
        {
            "bounding_box": [
                {
                    "x": 251,
                    "y": 2907
                },
                {
                    "x": 323,
                    "y": 2907
                },
                {
                    "x": 323,
                    "y": 2955
                },
                {
                    "x": 251,
                    "y": 2955
                }
            ],
            "category": "paragraph",
            "html": "<p id='11' style='font-size:18px'>CC</p>",
            "id": 11,
            "page": 1,
            "text": "CC"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2892
                },
                {
                    "x": 473,
                    "y": 2892
                },
                {
                    "x": 473,
                    "y": 2939
                },
                {
                    "x": 442,
                    "y": 2939
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='12' style='font-size:20px'>i</p>",
            "id": 12,
            "page": 1,
            "text": "i"
        },
        {
            "bounding_box": [
                {
                    "x": 431,
                    "y": 2964
                },
                {
                    "x": 482,
                    "y": 2964
                },
                {
                    "x": 482,
                    "y": 2995
                },
                {
                    "x": 431,
                    "y": 2995
                }
            ],
            "category": "paragraph",
            "html": "<p id='13' style='font-size:14px'>BY</p>",
            "id": 13,
            "page": 1,
            "text": "BY"
        },
        {
            "bounding_box": [
                {
                    "x": 1023,
                    "y": 482
                },
                {
                    "x": 1512,
                    "y": 482
                },
                {
                    "x": 1512,
                    "y": 678
                },
                {
                    "x": 1023,
                    "y": 678
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='14' style='font-size:20px'>Lizi Liao<br>National University of<br>Singapore, Singapore<br>liaolizi.llz@gmail.com</p>",
            "id": 14,
            "page": 1,
            "text": "Lizi Liao National University of Singapore, Singapore liaolizi.llz@gmail.com"
        },
        {
            "bounding_box": [
                {
                    "x": 1062,
                    "y": 694
                },
                {
                    "x": 1480,
                    "y": 694
                },
                {
                    "x": 1480,
                    "y": 884
                },
                {
                    "x": 1062,
                    "y": 884
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='15' style='font-size:18px'>Xia Hu<br>Texas A&M University<br>USA<br>hu@cse.tamu.edu</p>",
            "id": 15,
            "page": 1,
            "text": "Xia Hu Texas A&M University USA hu@cse.tamu.edu"
        },
        {
            "bounding_box": [
                {
                    "x": 1586,
                    "y": 482
                },
                {
                    "x": 2217,
                    "y": 482
                },
                {
                    "x": 2217,
                    "y": 672
                },
                {
                    "x": 1586,
                    "y": 672
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='16' style='font-size:20px'>Hanwang Zhang<br>Columbia University<br>USA<br>hanwangzhang@gmail.com</p>",
            "id": 16,
            "page": 1,
            "text": "Hanwang Zhang Columbia University USA hanwangzhang@gmail.com"
        },
        {
            "bounding_box": [
                {
                    "x": 1657,
                    "y": 695
                },
                {
                    "x": 2107,
                    "y": 695
                },
                {
                    "x": 2107,
                    "y": 891
                },
                {
                    "x": 1657,
                    "y": 891
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='17' style='font-size:20px'>Tat-Seng Chua<br>National University of<br>Singapore, Singapore<br>dcscts@nus.edu.sg</p>",
            "id": 17,
            "page": 1,
            "text": "Tat-Seng Chua National University of Singapore, Singapore dcscts@nus.edu.sg"
        },
        {
            "bounding_box": [
                {
                    "x": 1316,
                    "y": 944
                },
                {
                    "x": 1809,
                    "y": 944
                },
                {
                    "x": 1809,
                    "y": 998
                },
                {
                    "x": 1316,
                    "y": 998
                }
            ],
            "category": "paragraph",
            "html": "<p id='18' style='font-size:20px'>1. INTRODUCTION</p>",
            "id": 18,
            "page": 1,
            "text": "1. INTRODUCTION"
        },
        {
            "bounding_box": [
                {
                    "x": 1313,
                    "y": 1009
                },
                {
                    "x": 2323,
                    "y": 1009
                },
                {
                    "x": 2323,
                    "y": 1576
                },
                {
                    "x": 1313,
                    "y": 1576
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='19' style='font-size:16px'>In the era of information explosion, recommender systems<br>play a pivotal role in alleviating information overload, hav-<br>ing been widely adopted by many online services, including<br>E-commerce, online news and social media sites. The key to<br>a personalized recommender system is in modelling users'<br>preference on items based on their past interactions (e.g.,<br>ratings and clicks), known as collaborative filtering [31, 46].<br>Among the various collaborative filtering techniques, matrix<br>factorization (MF) [14, 21] is the most popular one, which<br>projects users and items into a shared latent space, using<br>a vector of latent features to represent a user or an item.<br>Thereafter a user's interaction on an item is modelled as the<br>inner product of their latent vectors.</p>",
            "id": 19,
            "page": 1,
            "text": "In the era of information explosion, recommender systems play a pivotal role in alleviating information overload, having been widely adopted by many online services, including E-commerce, online news and social media sites. The key to a personalized recommender system is in modelling users' preference on items based on their past interactions (e.g., ratings and clicks), known as collaborative filtering . Among the various collaborative filtering techniques, matrix factorization (MF)  is the most popular one, which projects users and items into a shared latent space, using a vector of latent features to represent a user or an item. Thereafter a user's interaction on an item is modelled as the inner product of their latent vectors."
        },
        {
            "bounding_box": [
                {
                    "x": 1312,
                    "y": 1575
                },
                {
                    "x": 2323,
                    "y": 1575
                },
                {
                    "x": 2323,
                    "y": 2448
                },
                {
                    "x": 1312,
                    "y": 2448
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='20' style='font-size:16px'>Popularized by the Netflix Prize, MF has become the de<br>facto approach to latent factor model-based recommenda-<br>tion. Much research effort has been devoted to enhancing<br>MF, such as integrating it with neighbor-based models [21],<br>combining it with topic models of item content [38], and ex-<br>tending it to factorization machines [26] for a generic mod-<br>elling of features. Despite the effectiveness of MF for collab-<br>orative filtering, it is well-known that its performance can be<br>hindered by the simple choice of the interaction function<br>inner product. For example, for the task of rating prediction<br>on explicit feedback, it is well known that the performance<br>of the MF model can be improved by incorporating user<br>and item bias terms into the interaction function1 While<br>it seems to be just a trivial tweak for the inner product<br>operator [14], it points to the positive effect of designing a<br>better, dedicated interaction function for modelling the la-<br>tent feature interactions between users and items. The inner<br>product, which simply combines the multiplication of latent<br>features linearly, may not be sufficient to capture the com-<br>plex structure of user interaction data.</p>",
            "id": 20,
            "page": 1,
            "text": "Popularized by the Netflix Prize, MF has become the de facto approach to latent factor model-based recommendation. Much research effort has been devoted to enhancing MF, such as integrating it with neighbor-based models , combining it with topic models of item content , and extending it to factorization machines  for a generic modelling of features. Despite the effectiveness of MF for collaborative filtering, it is well-known that its performance can be hindered by the simple choice of the interaction function inner product. For example, for the task of rating prediction on explicit feedback, it is well known that the performance of the MF model can be improved by incorporating user and item bias terms into the interaction function1 While it seems to be just a trivial tweak for the inner product operator , it points to the positive effect of designing a better, dedicated interaction function for modelling the latent feature interactions between users and items. The inner product, which simply combines the multiplication of latent features linearly, may not be sufficient to capture the complex structure of user interaction data."
        },
        {
            "bounding_box": [
                {
                    "x": 1313,
                    "y": 2448
                },
                {
                    "x": 2322,
                    "y": 2448
                },
                {
                    "x": 2322,
                    "y": 2885
                },
                {
                    "x": 1313,
                    "y": 2885
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='21' style='font-size:16px'>This paper explores the use of deep neural networks for<br>learning the interaction function from data, rather than a<br>handcraft that has been done by many previous work [18,<br>21]. The neural network has been proven to be capable of<br>approximating any continuous function [17], and more re-<br>cently deep neural networks (DNNs) have been found to be<br>effective in several domains, ranging from computer vision,<br>speech recognition, to text processing [5, 10, 15, 47]. How-<br>ever, there is relatively little work on employing DNNs for<br>recommendation in contrast to the vast amount of literature</p>",
            "id": 21,
            "page": 1,
            "text": "This paper explores the use of deep neural networks for learning the interaction function from data, rather than a handcraft that has been done by many previous work . The neural network has been proven to be capable of approximating any continuous function , and more recently deep neural networks (DNNs) have been found to be effective in several domains, ranging from computer vision, speech recognition, to text processing . However, there is relatively little work on employing DNNs for recommendation in contrast to the vast amount of literature"
        },
        {
            "bounding_box": [
                {
                    "x": 61,
                    "y": 896
                },
                {
                    "x": 149,
                    "y": 896
                },
                {
                    "x": 149,
                    "y": 2335
                },
                {
                    "x": 61,
                    "y": 2335
                }
            ],
            "category": "footer",
            "html": "<br><footer id='22' style='font-size:14px'>2017<br>Aug<br>26<br>[cs.IR]<br>1v2<br>arXiv:1708.0503</footer>",
            "id": 22,
            "page": 1,
            "text": "2017 Aug 26 [cs.IR] 1v2 arXiv:1708.0503"
        },
        {
            "bounding_box": [
                {
                    "x": 1314,
                    "y": 2918
                },
                {
                    "x": 2277,
                    "y": 2918
                },
                {
                    "x": 2277,
                    "y": 2998
                },
                {
                    "x": 1314,
                    "y": 2998
                }
            ],
            "category": "footer",
            "html": "<footer id='23' style='font-size:16px'>1http: // alex. smola. org/teaching/berbeley2012/slides/8_<br>Recommender · pdf</footer>",
            "id": 23,
            "page": 1,
            "text": "1http: // alex. smola. org/teaching/berbeley2012/slides/8_ Recommender · pdf"
        },
        {
            "bounding_box": [
                {
                    "x": 215,
                    "y": 231
                },
                {
                    "x": 1228,
                    "y": 231
                },
                {
                    "x": 1228,
                    "y": 581
                },
                {
                    "x": 215,
                    "y": 581
                }
            ],
            "category": "paragraph",
            "html": "<p id='24' style='font-size:18px'>on MF methods. Although some recent advances [37, 38,<br>45] have applied DNNs to recommendation tasks and shown<br>promising results, they mostly used DNNs to model auxil-<br>iary information, such as textual description of items, audio<br>features of musics, and visual content of images. With re-<br>gards to modelling the key collaborative filtering effect, they<br>still resorted to MF, combining user and item latent features<br>using an inner product.</p>",
            "id": 24,
            "page": 2,
            "text": "on MF methods. Although some recent advances  have applied DNNs to recommendation tasks and shown promising results, they mostly used DNNs to model auxiliary information, such as textual description of items, audio features of musics, and visual content of images. With regards to modelling the key collaborative filtering effect, they still resorted to MF, combining user and item latent features using an inner product."
        },
        {
            "bounding_box": [
                {
                    "x": 217,
                    "y": 582
                },
                {
                    "x": 1229,
                    "y": 582
                },
                {
                    "x": 1229,
                    "y": 1105
                },
                {
                    "x": 217,
                    "y": 1105
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='25' style='font-size:16px'>This work addresses the aforementioned research prob-<br>lems by formalizing a neural network modelling approach for<br>collaborative filtering. We focus on implicit feedback, which<br>indirectly reflects users' preference through behaviours like<br>watching videos, purchasing products and clicking items.<br>Compared to explicit feedback (i.e., ratings and reviews),<br>implicit feedback can be tracked automatically and is thus<br>much easier to collect for content providers. However, it is<br>more challenging to utilize, since user satisfaction is not ob-<br>served and there is a natural scarcity of negative feedback.<br>In this paper, we explore the central theme of how to utilize<br>DNNs to model noisy implicit feedback signals.</p>",
            "id": 25,
            "page": 2,
            "text": "This work addresses the aforementioned research problems by formalizing a neural network modelling approach for collaborative filtering. We focus on implicit feedback, which indirectly reflects users' preference through behaviours like watching videos, purchasing products and clicking items. Compared to explicit feedback (i.e., ratings and reviews), implicit feedback can be tracked automatically and is thus much easier to collect for content providers. However, it is more challenging to utilize, since user satisfaction is not observed and there is a natural scarcity of negative feedback. In this paper, we explore the central theme of how to utilize DNNs to model noisy implicit feedback signals."
        },
        {
            "bounding_box": [
                {
                    "x": 261,
                    "y": 1106
                },
                {
                    "x": 1117,
                    "y": 1106
                },
                {
                    "x": 1117,
                    "y": 1147
                },
                {
                    "x": 261,
                    "y": 1147
                }
            ],
            "category": "caption",
            "html": "<br><caption id='26' style='font-size:14px'>The main contributions of this work are as follows.</caption>",
            "id": 26,
            "page": 2,
            "text": "The main contributions of this work are as follows."
        },
        {
            "bounding_box": [
                {
                    "x": 263,
                    "y": 1160
                },
                {
                    "x": 1230,
                    "y": 1160
                },
                {
                    "x": 1230,
                    "y": 1688
                },
                {
                    "x": 263,
                    "y": 1688
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='27' style='font-size:16px'>1. We present a neural network architecture to model<br>latent features of users and items and devise a gen-<br>eral framework NCF for collaborative filtering based<br>on neural networks.<br>2. We show that MF can be interpreted as a specialization<br>of NCF and utilize a multi-layer perceptron to endow<br>NCF modelling with a high level of non-linearities.<br>3. We perform extensive experiments on two real-world<br>datasets to demonstrate the effectiveness of our NCF<br>approaches and the promise of deep learning for col-<br>laborative filtering.</p>",
            "id": 27,
            "page": 2,
            "text": "1. We present a neural network architecture to model latent features of users and items and devise a general framework NCF for collaborative filtering based on neural networks. 2. We show that MF can be interpreted as a specialization of NCF and utilize a multi-layer perceptron to endow NCF modelling with a high level of non-linearities. 3. We perform extensive experiments on two real-world datasets to demonstrate the effectiveness of our NCF approaches and the promise of deep learning for collaborative filtering."
        },
        {
            "bounding_box": [
                {
                    "x": 220,
                    "y": 1733
                },
                {
                    "x": 720,
                    "y": 1733
                },
                {
                    "x": 720,
                    "y": 1784
                },
                {
                    "x": 220,
                    "y": 1784
                }
            ],
            "category": "paragraph",
            "html": "<p id='28' style='font-size:22px'>2. PRELIMINARIES</p>",
            "id": 28,
            "page": 2,
            "text": "2. PRELIMINARIES"
        },
        {
            "bounding_box": [
                {
                    "x": 218,
                    "y": 1795
                },
                {
                    "x": 1228,
                    "y": 1795
                },
                {
                    "x": 1228,
                    "y": 1971
                },
                {
                    "x": 218,
                    "y": 1971
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='29' style='font-size:18px'>We first formalize the problem and discuss existing solu-<br>tions for collaborative filtering with implicit feedback. We<br>then shortly recapitulate the widely used MF model, high-<br>lighting its limitation caused by using an inner product.</p>",
            "id": 29,
            "page": 2,
            "text": "We first formalize the problem and discuss existing solutions for collaborative filtering with implicit feedback. We then shortly recapitulate the widely used MF model, highlighting its limitation caused by using an inner product."
        },
        {
            "bounding_box": [
                {
                    "x": 217,
                    "y": 2001
                },
                {
                    "x": 946,
                    "y": 2001
                },
                {
                    "x": 946,
                    "y": 2054
                },
                {
                    "x": 217,
                    "y": 2054
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='30' style='font-size:22px'>2.1 Learning from Implicit Data</p>",
            "id": 30,
            "page": 2,
            "text": "2.1 Learning from Implicit Data"
        },
        {
            "bounding_box": [
                {
                    "x": 218,
                    "y": 2061
                },
                {
                    "x": 1227,
                    "y": 2061
                },
                {
                    "x": 1227,
                    "y": 2195
                },
                {
                    "x": 218,
                    "y": 2195
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='31' style='font-size:16px'>Let M and N denote the number of users and items,<br>respectively. We define the user-item interaction matrix<br>Y E RMxN from users' implicit feedback as,</p>",
            "id": 31,
            "page": 2,
            "text": "Let M and N denote the number of users and items, respectively. We define the user-item interaction matrix Y E RMxN from users' implicit feedback as,"
        },
        {
            "bounding_box": [
                {
                    "x": 216,
                    "y": 2345
                },
                {
                    "x": 1227,
                    "y": 2345
                },
                {
                    "x": 1227,
                    "y": 2738
                },
                {
                    "x": 216,
                    "y": 2738
                }
            ],
            "category": "paragraph",
            "html": "<p id='32' style='font-size:16px'>Here a value of 1 for Yui indicates that there is an interac-<br>tion between user u and item 2; however, it does not mean u<br>actually likes i. Similarly, a value of 0 does not necessarily<br>mean u does not like i, it can be that the user is not aware<br>of the item. This poses challenges in learning from implicit<br>data, since it provides only noisy signals about users' pref-<br>erence. While observed entries at least reflect users' interest<br>on items, the unobserved entries can be just missing data<br>and there is a natural scarcity of negative feedback.</p>",
            "id": 32,
            "page": 2,
            "text": "Here a value of 1 for Yui indicates that there is an interaction between user u and item 2; however, it does not mean u actually likes i. Similarly, a value of 0 does not necessarily mean u does not like i, it can be that the user is not aware of the item. This poses challenges in learning from implicit data, since it provides only noisy signals about users' preference. While observed entries at least reflect users' interest on items, the unobserved entries can be just missing data and there is a natural scarcity of negative feedback."
        },
        {
            "bounding_box": [
                {
                    "x": 218,
                    "y": 2739
                },
                {
                    "x": 1229,
                    "y": 2739
                },
                {
                    "x": 1229,
                    "y": 3004
                },
                {
                    "x": 218,
                    "y": 3004
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='33' style='font-size:16px'>The recommendation problem with implicit feedback is<br>formulated as the problem of estimating the scores of unob-<br>served entries in Y, which are used for ranking the items.<br>Model-based approaches assume that data can be generated<br>(or described) by an underlying model. Formally, they can<br>be abstracted as learning Yui = f(u, i|日), where Yui denotes</p>",
            "id": 33,
            "page": 2,
            "text": "The recommendation problem with implicit feedback is formulated as the problem of estimating the scores of unobserved entries in Y, which are used for ranking the items. Model-based approaches assume that data can be generated (or described) by an underlying model. Formally, they can be abstracted as learning Yui = f(u, i|日), where Yui denotes"
        },
        {
            "bounding_box": [
                {
                    "x": 1361,
                    "y": 215
                },
                {
                    "x": 2264,
                    "y": 215
                },
                {
                    "x": 2264,
                    "y": 632
                },
                {
                    "x": 1361,
                    "y": 632
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='34' style='font-size:14px' alt=\"i1 i2 i3 i4 i5\np'4\nu1 1 1 1 0 1 P1\nP4\nu2 0 1 1 0 0 users\nu3 0 1 1 1 0\nP2\n1 0 1 1 1\nitems P3\n(a) user-item matrix (b) user latent space\" data-coord=\"top-left:(1361,215); bottom-right:(2264,632)\" /></figure>",
            "id": 34,
            "page": 2,
            "text": "i1 i2 i3 i4 i5 p'4 u1 1 1 1 0 1 P1 P4 u2 0 1 1 0 0 users u3 0 1 1 1 0 P2 1 0 1 1 1 items P3 (a) user-item matrix (b) user latent space"
        },
        {
            "bounding_box": [
                {
                    "x": 1313,
                    "y": 646
                },
                {
                    "x": 2324,
                    "y": 646
                },
                {
                    "x": 2324,
                    "y": 868
                },
                {
                    "x": 1313,
                    "y": 868
                }
            ],
            "category": "caption",
            "html": "<br><caption id='35' style='font-size:18px'>Figure 1: An example illustrates MF's limitation.<br>From data matrix (a), u4 is most similar to u1, fol-<br>lowed by u3, and lastly u2· However in the latent<br>space (b), placing P4 closest to P1 makes p4 closer to<br>P2 than P3, incurring a large ranking loss.</caption>",
            "id": 35,
            "page": 2,
            "text": "Figure 1: An example illustrates MF's limitation. From data matrix (a), u4 is most similar to u1, followed by u3, and lastly u2· However in the latent space (b), placing P4 closest to P1 makes p4 closer to P2 than P3, incurring a large ranking loss."
        },
        {
            "bounding_box": [
                {
                    "x": 1314,
                    "y": 909
                },
                {
                    "x": 2321,
                    "y": 909
                },
                {
                    "x": 2321,
                    "y": 1083
                },
                {
                    "x": 1314,
                    "y": 1083
                }
            ],
            "category": "paragraph",
            "html": "<p id='36' style='font-size:16px'>the predicted score of interaction Yui, 日 denotes model pa-<br>rameters, and f denotes the function that maps model pa-<br>rameters to the predicted score (which we term as an inter-<br>action function).</p>",
            "id": 36,
            "page": 2,
            "text": "the predicted score of interaction Yui, 日 denotes model parameters, and f denotes the function that maps model parameters to the predicted score (which we term as an interaction function)."
        },
        {
            "bounding_box": [
                {
                    "x": 1313,
                    "y": 1085
                },
                {
                    "x": 2323,
                    "y": 1085
                },
                {
                    "x": 2323,
                    "y": 1783
                },
                {
                    "x": 1313,
                    "y": 1783
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='37' style='font-size:18px'>To estimate parameters 日, existing approaches generally<br>follow the machine learning paradigm that optimizes an ob-<br>jective function. Two types of objective functions are most<br>commonly used in literature pointwise loss [14, 19] and<br>pairwise loss [27, 33]. As a natural extension of abundant<br>work on explicit feedback [21, 46], methods on pointwise<br>learning usually follow a regression framework by minimiz-<br>ing the squared loss between Yui and its target value Yui·<br>To handle the absence of negative data, they have either<br>treated all unobserved entries as negative feedback, or sam-<br>pled negative instances from unobserved entries [14]. For<br>pairwise learning [27, 44], the idea is that observed entries<br>should be ranked higher than the unobserved ones. As such,<br>instead of minimizing the loss between Yui and Yui, pairwise<br>learning maximizes the margin between observed entry Yui<br>and unobserved entry Yuj.</p>",
            "id": 37,
            "page": 2,
            "text": "To estimate parameters 日, existing approaches generally follow the machine learning paradigm that optimizes an objective function. Two types of objective functions are most commonly used in literature pointwise loss  and pairwise loss . As a natural extension of abundant work on explicit feedback , methods on pointwise learning usually follow a regression framework by minimizing the squared loss between Yui and its target value Yui· To handle the absence of negative data, they have either treated all unobserved entries as negative feedback, or sampled negative instances from unobserved entries . For pairwise learning , the idea is that observed entries should be ranked higher than the unobserved ones. As such, instead of minimizing the loss between Yui and Yui, pairwise learning maximizes the margin between observed entry Yui and unobserved entry Yuj."
        },
        {
            "bounding_box": [
                {
                    "x": 1312,
                    "y": 1783
                },
                {
                    "x": 2323,
                    "y": 1783
                },
                {
                    "x": 2323,
                    "y": 1959
                },
                {
                    "x": 1312,
                    "y": 1959
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='38' style='font-size:16px'>Moving one step forward, our NCF framework parame-<br>terizes the interaction function f using neural networks to<br>estimate Yui. As such, it naturally supports both pointwise<br>and pairwise learning.</p>",
            "id": 38,
            "page": 2,
            "text": "Moving one step forward, our NCF framework parameterizes the interaction function f using neural networks to estimate Yui. As such, it naturally supports both pointwise and pairwise learning."
        },
        {
            "bounding_box": [
                {
                    "x": 1315,
                    "y": 1982
                },
                {
                    "x": 1881,
                    "y": 1982
                },
                {
                    "x": 1881,
                    "y": 2033
                },
                {
                    "x": 1315,
                    "y": 2033
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='39' style='font-size:20px'>2.2 Matrix Factorization</p>",
            "id": 39,
            "page": 2,
            "text": "2.2 Matrix Factorization"
        },
        {
            "bounding_box": [
                {
                    "x": 1313,
                    "y": 2044
                },
                {
                    "x": 2323,
                    "y": 2044
                },
                {
                    "x": 2323,
                    "y": 2221
                },
                {
                    "x": 1313,
                    "y": 2221
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='40' style='font-size:14px'>MF associates each user and item with a real-valued vector<br>of latent features. Let Pu and qi denote the latent vector for<br>user u and item 2, respectively; MF estimates an interaction<br>and qi:<br>Yui as the inner product of Pu</p>",
            "id": 40,
            "page": 2,
            "text": "MF associates each user and item with a real-valued vector of latent features. Let Pu and qi denote the latent vector for user u and item 2, respectively; MF estimates an interaction and qi: Yui as the inner product of Pu"
        },
        {
            "bounding_box": [
                {
                    "x": 1313,
                    "y": 2365
                },
                {
                    "x": 2322,
                    "y": 2365
                },
                {
                    "x": 2322,
                    "y": 2625
                },
                {
                    "x": 1313,
                    "y": 2625
                }
            ],
            "category": "paragraph",
            "html": "<p id='41' style='font-size:16px'>where K denotes the dimension of the latent space. As we<br>can see, MF models the two-way interaction of user and item<br>latent factors, assuming each dimension of the latent space<br>is independent of each other and linearly combining them<br>with the same weight. As such, MF can be deemed as a<br>linear model of latent factors.</p>",
            "id": 41,
            "page": 2,
            "text": "where K denotes the dimension of the latent space. As we can see, MF models the two-way interaction of user and item latent factors, assuming each dimension of the latent space is independent of each other and linearly combining them with the same weight. As such, MF can be deemed as a linear model of latent factors."
        },
        {
            "bounding_box": [
                {
                    "x": 1314,
                    "y": 2628
                },
                {
                    "x": 2323,
                    "y": 2628
                },
                {
                    "x": 2323,
                    "y": 2935
                },
                {
                    "x": 1314,
                    "y": 2935
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='42' style='font-size:16px'>Figure 1 illustrates how the inner product function can<br>limit the expressiveness of MF. There are two settings to be<br>stated clearly beforehand to understand the example well.<br>First, since MF maps users and items to the same latent<br>space, the similarity between two users can also be measured<br>with an inner product, or equivalently2 , the cosine of the<br>angle between their latent vectors. Second, without loss of</p>",
            "id": 42,
            "page": 2,
            "text": "Figure 1 illustrates how the inner product function can limit the expressiveness of MF. There are two settings to be stated clearly beforehand to understand the example well. First, since MF maps users and items to the same latent space, the similarity between two users can also be measured with an inner product, or equivalently2 , the cosine of the angle between their latent vectors. Second, without loss of"
        },
        {
            "bounding_box": [
                {
                    "x": 1316,
                    "y": 2954
                },
                {
                    "x": 2093,
                    "y": 2954
                },
                {
                    "x": 2093,
                    "y": 3002
                },
                {
                    "x": 1316,
                    "y": 3002
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='43' style='font-size:16px'>2 Assuming latent vectors are of a unit length.</p>",
            "id": 43,
            "page": 2,
            "text": "2 Assuming latent vectors are of a unit length."
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 221
                },
                {
                    "x": 1173,
                    "y": 221
                },
                {
                    "x": 1173,
                    "y": 774
                },
                {
                    "x": 223,
                    "y": 774
                }
            ],
            "category": "figure",
            "html": "<figure><img id='44' style='font-size:14px' alt=\"Training\nScore y ui Target\nui\nOutput Layer\nLayer X\n↑\nNeural CF Layers\nLayer 2\nLayer 1\nEmbedding Layer User Latent Vector Item Latent Vector\nMxK = {Puk } QNxK = {qik}\nInput Layer (Sparse) 0 0 0 1 0 0 ...... 0 0 0 0 1 0 ·\nUser (u) Item (i)\" data-coord=\"top-left:(223,221); bottom-right:(1173,774)\" /></figure>",
            "id": 44,
            "page": 3,
            "text": "Training Score y ui Target ui Output Layer Layer X ↑ Neural CF Layers Layer 2 Layer 1 Embedding Layer User Latent Vector Item Latent Vector MxK = {Puk } QNxK = {qik} Input Layer (Sparse) 0 0 0 1 0 0 ...... 0 0 0 0 1 0 · User (u) Item (i)"
        },
        {
            "bounding_box": [
                {
                    "x": 230,
                    "y": 784
                },
                {
                    "x": 1213,
                    "y": 784
                },
                {
                    "x": 1213,
                    "y": 829
                },
                {
                    "x": 230,
                    "y": 829
                }
            ],
            "category": "caption",
            "html": "<br><caption id='45' style='font-size:18px'>Figure 2: Neural collaborative filtering framework</caption>",
            "id": 45,
            "page": 3,
            "text": "Figure 2: Neural collaborative filtering framework"
        },
        {
            "bounding_box": [
                {
                    "x": 217,
                    "y": 874
                },
                {
                    "x": 1224,
                    "y": 874
                },
                {
                    "x": 1224,
                    "y": 962
                },
                {
                    "x": 217,
                    "y": 962
                }
            ],
            "category": "paragraph",
            "html": "<p id='46' style='font-size:14px'>generality, we use the Jaccard coefficient3 as the ground-<br>truth similarity of two users that MF needs to recover.</p>",
            "id": 46,
            "page": 3,
            "text": "generality, we use the Jaccard coefficient3 as the groundtruth similarity of two users that MF needs to recover."
        },
        {
            "bounding_box": [
                {
                    "x": 217,
                    "y": 966
                },
                {
                    "x": 1226,
                    "y": 966
                },
                {
                    "x": 1226,
                    "y": 1441
                },
                {
                    "x": 217,
                    "y": 1441
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='47' style='font-size:14px'>Let us first focus on the first three rows (users) in Fig-<br>ure 1a. It is easy to have S23 (0.66) > S12 (0.5) > S13 (0.4).<br>As such, the geometric relations of P1, P2, and P3 in the la-<br>tent space can be plotted as in Figure 1b. Now, let us con-<br>sider a new user u4, whose input is given as the dashed line<br>in Figure 1a. We can have S41 (0.6) > S43 (0.4) > S42 (0.2),<br>meaning that u4 is most similar to u1, followed by u3, and<br>lastly u2. However, if a MF model places p4 closest to P1<br>(the two options are shown in Figure 1b with dashed lines),<br>it will result in p4 closer to P2 than P3, which unfortunately<br>will incur a large ranking loss.</p>",
            "id": 47,
            "page": 3,
            "text": "Let us first focus on the first three rows (users) in Figure 1a. It is easy to have S23 (0.66) > S12 (0.5) > S13 (0.4). As such, the geometric relations of P1, P2, and P3 in the latent space can be plotted as in Figure 1b. Now, let us consider a new user u4, whose input is given as the dashed line in Figure 1a. We can have S41 (0.6) > S43 (0.4) > S42 (0.2), meaning that u4 is most similar to u1, followed by u3, and lastly u2. However, if a MF model places p4 closest to P1 (the two options are shown in Figure 1b with dashed lines), it will result in p4 closer to P2 than P3, which unfortunately will incur a large ranking loss."
        },
        {
            "bounding_box": [
                {
                    "x": 218,
                    "y": 1444
                },
                {
                    "x": 1228,
                    "y": 1444
                },
                {
                    "x": 1228,
                    "y": 1838
                },
                {
                    "x": 218,
                    "y": 1838
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='48' style='font-size:14px'>The above example shows the possible limitation of MF<br>caused by the use of a simple and fixed inner product to esti-<br>mate complex user-item interactions in the low-dimensional<br>latent space. We note that one way to resolve the issue is<br>to use a large number of latent factors K. However, it may<br>adversely hurt the generalization of the model (e.g., over-<br>fitting the data), especially in sparse settings [26]. In this<br>work, we address the limitation by learning the interaction<br>function using DNNs from data.</p>",
            "id": 48,
            "page": 3,
            "text": "The above example shows the possible limitation of MF caused by the use of a simple and fixed inner product to estimate complex user-item interactions in the low-dimensional latent space. We note that one way to resolve the issue is to use a large number of latent factors K. However, it may adversely hurt the generalization of the model (e.g., overfitting the data), especially in sparse settings . In this work, we address the limitation by learning the interaction function using DNNs from data."
        },
        {
            "bounding_box": [
                {
                    "x": 218,
                    "y": 1892
                },
                {
                    "x": 1255,
                    "y": 1892
                },
                {
                    "x": 1255,
                    "y": 1942
                },
                {
                    "x": 218,
                    "y": 1942
                }
            ],
            "category": "paragraph",
            "html": "<p id='49' style='font-size:22px'>3. NEURAL COLLABORATIVE FILTERING</p>",
            "id": 49,
            "page": 3,
            "text": "3. NEURAL COLLABORATIVE FILTERING"
        },
        {
            "bounding_box": [
                {
                    "x": 217,
                    "y": 1955
                },
                {
                    "x": 1228,
                    "y": 1955
                },
                {
                    "x": 1228,
                    "y": 2435
                },
                {
                    "x": 217,
                    "y": 2435
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='50' style='font-size:14px'>We first present the general NCF framework, elaborat-<br>ing how to learn NCF with a probabilistic model that em-<br>phasizes the binary property of implicit data. We then<br>show that MF can be expressed and generalized under NCF.<br>To explore DNNs for collaborative filtering, we then pro-<br>pose an instantiation of NCF, using a multi-layer perceptron<br>(MLP) to learn the user-item interaction function. Lastly,<br>we present a new neural matrix factorization model, which<br>ensembles MF and MLP under the NCF framework; it uni-<br>fies the strengths of linearity of MF and non-linearity of<br>MLP for modelling the user-item latent structures.</p>",
            "id": 50,
            "page": 3,
            "text": "We first present the general NCF framework, elaborating how to learn NCF with a probabilistic model that emphasizes the binary property of implicit data. We then show that MF can be expressed and generalized under NCF. To explore DNNs for collaborative filtering, we then propose an instantiation of NCF, using a multi-layer perceptron (MLP) to learn the user-item interaction function. Lastly, we present a new neural matrix factorization model, which ensembles MF and MLP under the NCF framework; it unifies the strengths of linearity of MF and non-linearity of MLP for modelling the user-item latent structures."
        },
        {
            "bounding_box": [
                {
                    "x": 218,
                    "y": 2462
                },
                {
                    "x": 775,
                    "y": 2462
                },
                {
                    "x": 775,
                    "y": 2516
                },
                {
                    "x": 218,
                    "y": 2516
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='51' style='font-size:22px'>3.1 General Framework</p>",
            "id": 51,
            "page": 3,
            "text": "3.1 General Framework"
        },
        {
            "bounding_box": [
                {
                    "x": 218,
                    "y": 2526
                },
                {
                    "x": 1228,
                    "y": 2526
                },
                {
                    "x": 1228,
                    "y": 2832
                },
                {
                    "x": 218,
                    "y": 2832
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='52' style='font-size:14px'>To permit a full neural treatment of collaborative filtering,<br>we adopt a multi-layer representation to model a user-item<br>interaction Yui as shown in Figure 2, where the output of one<br>layer serves as the input of the next one. The bottom input<br>layer consists of two feature vectors VU and VI that describe<br>user u and item 2, respectively; they can be customized to<br>support a wide range of modelling of users and items, such</p>",
            "id": 52,
            "page": 3,
            "text": "To permit a full neural treatment of collaborative filtering, we adopt a multi-layer representation to model a user-item interaction Yui as shown in Figure 2, where the output of one layer serves as the input of the next one. The bottom input layer consists of two feature vectors VU and VI that describe user u and item 2, respectively; they can be customized to support a wide range of modelling of users and items, such"
        },
        {
            "bounding_box": [
                {
                    "x": 219,
                    "y": 2858
                },
                {
                    "x": 1226,
                    "y": 2858
                },
                {
                    "x": 1226,
                    "y": 2943
                },
                {
                    "x": 219,
                    "y": 2943
                }
            ],
            "category": "paragraph",
            "html": "<p id='53' style='font-size:14px'>3Let Ru be the set of items that user u has interacted with,<br>then the Jaccard similarity of users i and j is defined as</p>",
            "id": 53,
            "page": 3,
            "text": "3Let Ru be the set of items that user u has interacted with, then the Jaccard similarity of users i and j is defined as"
        },
        {
            "bounding_box": [
                {
                    "x": 1313,
                    "y": 232
                },
                {
                    "x": 2324,
                    "y": 232
                },
                {
                    "x": 2324,
                    "y": 580
                },
                {
                    "x": 1313,
                    "y": 580
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='54' style='font-size:14px'>as context-aware [28, 1], content-based [3], and neighbor-<br>based [26]. Since this work focuses on the pure collaborative<br>filtering setting, we use only the identity of a user and an<br>item as the input feature, transforming it to a binarized<br>sparse vector with one-hot encoding. Note that with such a<br>generic feature representation for inputs, our method can be<br>easily adjusted to address the cold-start problem by using<br>content features to represent users and items.</p>",
            "id": 54,
            "page": 3,
            "text": "as context-aware , content-based , and neighborbased . Since this work focuses on the pure collaborative filtering setting, we use only the identity of a user and an item as the input feature, transforming it to a binarized sparse vector with one-hot encoding. Note that with such a generic feature representation for inputs, our method can be easily adjusted to address the cold-start problem by using content features to represent users and items."
        },
        {
            "bounding_box": [
                {
                    "x": 1313,
                    "y": 582
                },
                {
                    "x": 2323,
                    "y": 582
                },
                {
                    "x": 2323,
                    "y": 1409
                },
                {
                    "x": 1313,
                    "y": 1409
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='55' style='font-size:16px'>Above the input layer is the embedding layer; it is a fully<br>connected layer that projects the sparse representation to<br>a dense vector. The obtained user (item) embedding can<br>be seen as the latent vector for user (item) in the context<br>of latent factor model. The user embedding and item em-<br>bedding are then fed into a multi-layer neural architecture,<br>which we term as neural collaborative filtering layers, to map<br>the latent vectors to prediction scores. Each layer of the neu-<br>ral CF layers can be customized to discover certain latent<br>structures of user-item interactions. The dimension of the<br>last hidden layer X determines the model's capability. The<br>final output layer is the predicted score Yui, and training<br>is performed by minimizing the pointwise loss between Yui<br>and its target value Yui· We note that another way to train<br>the model is by performing pairwise learning, such as using<br>the Bayesian Personalized Ranking [27] and margin-based<br>loss [33]. As the focus of the paper is on the neural network<br>modelling part, we leave the extension to pairwise learning<br>of NCF as a future work.</p>",
            "id": 55,
            "page": 3,
            "text": "Above the input layer is the embedding layer; it is a fully connected layer that projects the sparse representation to a dense vector. The obtained user (item) embedding can be seen as the latent vector for user (item) in the context of latent factor model. The user embedding and item embedding are then fed into a multi-layer neural architecture, which we term as neural collaborative filtering layers, to map the latent vectors to prediction scores. Each layer of the neural CF layers can be customized to discover certain latent structures of user-item interactions. The dimension of the last hidden layer X determines the model's capability. The final output layer is the predicted score Yui, and training is performed by minimizing the pointwise loss between Yui and its target value Yui· We note that another way to train the model is by performing pairwise learning, such as using the Bayesian Personalized Ranking  and margin-based loss . As the focus of the paper is on the neural network modelling part, we leave the extension to pairwise learning of NCF as a future work."
        },
        {
            "bounding_box": [
                {
                    "x": 1356,
                    "y": 1410
                },
                {
                    "x": 2196,
                    "y": 1410
                },
                {
                    "x": 2196,
                    "y": 1453
                },
                {
                    "x": 1356,
                    "y": 1453
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='56' style='font-size:14px'>We now formulate the NCF's predictive model as</p>",
            "id": 56,
            "page": 3,
            "text": "We now formulate the NCF's predictive model as"
        },
        {
            "bounding_box": [
                {
                    "x": 1313,
                    "y": 1549
                },
                {
                    "x": 2323,
                    "y": 1549
                },
                {
                    "x": 2323,
                    "y": 1770
                },
                {
                    "x": 1313,
                    "y": 1770
                }
            ],
            "category": "paragraph",
            "html": "<p id='57' style='font-size:14px'>where P E RMxK<br>and Q E RNxK denoting the latent fac-<br>,<br>tor matrix for users and items, respectively; and Of denotes<br>the model parameters of the interaction function f. Since<br>the function f is defined as a multi-layer neural network, it<br>can be formulated as</p>",
            "id": 57,
            "page": 3,
            "text": "where P E RMxK and Q E RNxK denoting the latent fac, tor matrix for users and items, respectively; and Of denotes the model parameters of the interaction function f. Since the function f is defined as a multi-layer neural network, it can be formulated as"
        },
        {
            "bounding_box": [
                {
                    "x": 1313,
                    "y": 1886
                },
                {
                    "x": 2323,
                    "y": 1886
                },
                {
                    "x": 2323,
                    "y": 2018
                },
                {
                    "x": 1313,
                    "y": 2018
                }
            ],
            "category": "paragraph",
            "html": "<p id='58' style='font-size:16px'>where ⌀out and ⌀x respectively denote the mapping function<br>for the output layer and x-th neural collaborative filtering<br>(CF) layer, and there are X neural CF layers in total.</p>",
            "id": 58,
            "page": 3,
            "text": "where ⌀out and ⌀x respectively denote the mapping function for the output layer and x-th neural collaborative filtering (CF) layer, and there are X neural CF layers in total."
        },
        {
            "bounding_box": [
                {
                    "x": 1325,
                    "y": 2046
                },
                {
                    "x": 1742,
                    "y": 2046
                },
                {
                    "x": 1742,
                    "y": 2094
                },
                {
                    "x": 1325,
                    "y": 2094
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='59' style='font-size:20px'>3.1.1 Learning NCF</p>",
            "id": 59,
            "page": 3,
            "text": "3.1.1 Learning NCF"
        },
        {
            "bounding_box": [
                {
                    "x": 1315,
                    "y": 2106
                },
                {
                    "x": 2332,
                    "y": 2106
                },
                {
                    "x": 2332,
                    "y": 2193
                },
                {
                    "x": 1315,
                    "y": 2193
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='60' style='font-size:18px'>To learn model parameters, existing pointwise methods [14,<br>39] largely perform a regression with squared loss:</p>",
            "id": 60,
            "page": 3,
            "text": "To learn model parameters, existing pointwise methods  largely perform a regression with squared loss:"
        },
        {
            "bounding_box": [
                {
                    "x": 1313,
                    "y": 2344
                },
                {
                    "x": 2322,
                    "y": 2344
                },
                {
                    "x": 2322,
                    "y": 2869
                },
                {
                    "x": 1313,
                    "y": 2869
                }
            ],
            "category": "paragraph",
            "html": "<p id='61' style='font-size:16px'>where y denotes the set of observed interactions in Y, and<br>y- denotes the set of negative instances, which can be all (or<br>sampled from) unobserved interactions; and Wui is a hyper-<br>parameter denoting the weight of training instance (u, i).<br>While the squared loss can be explained by assuming that<br>observations are generated from a Gaussian distribution [29],<br>we point out that it may not tally well with implicit data.<br>This is because for implicit data, the target value Yui is<br>a binarized 1 or 0 denoting whether u has interacted with<br>i. In what follows, we present a probabilistic approach for<br>learning the pointwise NCF that pays special attention to<br>the binary property of implicit data.</p>",
            "id": 61,
            "page": 3,
            "text": "where y denotes the set of observed interactions in Y, and y- denotes the set of negative instances, which can be all (or sampled from) unobserved interactions; and Wui is a hyperparameter denoting the weight of training instance (u, i). While the squared loss can be explained by assuming that observations are generated from a Gaussian distribution , we point out that it may not tally well with implicit data. This is because for implicit data, the target value Yui is a binarized 1 or 0 denoting whether u has interacted with i. In what follows, we present a probabilistic approach for learning the pointwise NCF that pays special attention to the binary property of implicit data."
        },
        {
            "bounding_box": [
                {
                    "x": 1313,
                    "y": 2871
                },
                {
                    "x": 2324,
                    "y": 2871
                },
                {
                    "x": 2324,
                    "y": 3003
                },
                {
                    "x": 1313,
                    "y": 3003
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='62' style='font-size:14px'>Considering the one-class nature of implicit feedback, we<br>can view the value of Yui as a label 1 means item i is<br>relevant to u, and 0 otherwise. The prediction score Yui</p>",
            "id": 62,
            "page": 3,
            "text": "Considering the one-class nature of implicit feedback, we can view the value of Yui as a label 1 means item i is relevant to u, and 0 otherwise. The prediction score Yui"
        },
        {
            "bounding_box": [
                {
                    "x": 217,
                    "y": 232
                },
                {
                    "x": 1228,
                    "y": 232
                },
                {
                    "x": 1228,
                    "y": 537
                },
                {
                    "x": 217,
                    "y": 537
                }
            ],
            "category": "paragraph",
            "html": "<p id='63' style='font-size:14px'>then represents how likely i is relevant to u. To endow NCF<br>with such a probabilistic explanation, we need to constrain<br>the output Yui in the range of [0, 1], which can be easily<br>achieved by using a probabilistic function (e.g., the Logistic<br>or Probit function) as the activation function for the output<br>layer ⌀out. With the above settings, we then define the<br>likelihood function as</p>",
            "id": 63,
            "page": 4,
            "text": "then represents how likely i is relevant to u. To endow NCF with such a probabilistic explanation, we need to constrain the output Yui in the range of , which can be easily achieved by using a probabilistic function (e.g., the Logistic or Probit function) as the activation function for the output layer ⌀out. With the above settings, we then define the likelihood function as"
        },
        {
            "bounding_box": [
                {
                    "x": 220,
                    "y": 676
                },
                {
                    "x": 1189,
                    "y": 676
                },
                {
                    "x": 1189,
                    "y": 722
                },
                {
                    "x": 220,
                    "y": 722
                }
            ],
            "category": "paragraph",
            "html": "<p id='64' style='font-size:16px'>Taking the negative logarithm of the likelihood, we reach</p>",
            "id": 64,
            "page": 4,
            "text": "Taking the negative logarithm of the likelihood, we reach"
        },
        {
            "bounding_box": [
                {
                    "x": 218,
                    "y": 979
                },
                {
                    "x": 1228,
                    "y": 979
                },
                {
                    "x": 1228,
                    "y": 1680
                },
                {
                    "x": 218,
                    "y": 1680
                }
            ],
            "category": "paragraph",
            "html": "<p id='65' style='font-size:14px'>This is the objective function to minimize for the NCF meth-<br>ods, and its optimization can be done by performing stochas-<br>tic gradient descent (SGD). Careful readers might have real-<br>ized that it is the same as the binary cross-entropy loss, also<br>known as log loss. By employing a probabilistic treatment<br>for NCF, we address recommendation with implicit feedback<br>as a binary classification problem. As the classification-<br>aware log loss has rarely been investigated in recommen-<br>dation literature, we explore it in this work and empirically<br>show its effectiveness in Section 4.3. For the negative in-<br>stances y- , we uniformly sample them from unobserved in-<br>teractions in each iteration and control the sampling ratio<br>w.r.t. the number of observed interactions. While a non-<br>uniform sampling strategy (e.g., item popularity-biased [14,<br>12]) might further improve the performance, we leave the<br>exploration as a future work.</p>",
            "id": 65,
            "page": 4,
            "text": "This is the objective function to minimize for the NCF methods, and its optimization can be done by performing stochastic gradient descent (SGD). Careful readers might have realized that it is the same as the binary cross-entropy loss, also known as log loss. By employing a probabilistic treatment for NCF, we address recommendation with implicit feedback as a binary classification problem. As the classificationaware log loss has rarely been investigated in recommendation literature, we explore it in this work and empirically show its effectiveness in Section 4.3. For the negative instances y- , we uniformly sample them from unobserved interactions in each iteration and control the sampling ratio w.r.t. the number of observed interactions. While a nonuniform sampling strategy (e.g., item popularity-biased ) might further improve the performance, we leave the exploration as a future work."
        },
        {
            "bounding_box": [
                {
                    "x": 218,
                    "y": 1703
                },
                {
                    "x": 1218,
                    "y": 1703
                },
                {
                    "x": 1218,
                    "y": 1754
                },
                {
                    "x": 218,
                    "y": 1754
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='66' style='font-size:20px'>3.2 Generalized Matrix Factorization (GMF)</p>",
            "id": 66,
            "page": 4,
            "text": "3.2 Generalized Matrix Factorization (GMF)"
        },
        {
            "bounding_box": [
                {
                    "x": 217,
                    "y": 1768
                },
                {
                    "x": 1227,
                    "y": 1768
                },
                {
                    "x": 1227,
                    "y": 1982
                },
                {
                    "x": 217,
                    "y": 1982
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='67' style='font-size:14px'>We now show how MF can be interpreted as a special case<br>of our NCF framework. As MF is the most popular model<br>for recommendation and has been investigated extensively<br>in literature, being able to recover it allows NCF to mimic<br>a large family of factorization models [26].</p>",
            "id": 67,
            "page": 4,
            "text": "We now show how MF can be interpreted as a special case of our NCF framework. As MF is the most popular model for recommendation and has been investigated extensively in literature, being able to recover it allows NCF to mimic a large family of factorization models ."
        },
        {
            "bounding_box": [
                {
                    "x": 218,
                    "y": 1986
                },
                {
                    "x": 1227,
                    "y": 1986
                },
                {
                    "x": 1227,
                    "y": 2201
                },
                {
                    "x": 218,
                    "y": 2201
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='68' style='font-size:14px'>Due to the one-hot encoding of user (item) ID of the input<br>layer, the obtained embedding vector can be seen as the<br>latent vector of user (item). Let the user latent vector Pu<br>be PTVU and item latent vector qi be QT VT. We define the<br>mapping function of the first neural CF layer as</p>",
            "id": 68,
            "page": 4,
            "text": "Due to the one-hot encoding of user (item) ID of the input layer, the obtained embedding vector can be seen as the latent vector of user (item). Let the user latent vector Pu be PTVU and item latent vector qi be QT VT. We define the mapping function of the first neural CF layer as"
        },
        {
            "bounding_box": [
                {
                    "x": 219,
                    "y": 2296
                },
                {
                    "x": 1224,
                    "y": 2296
                },
                {
                    "x": 1224,
                    "y": 2384
                },
                {
                    "x": 219,
                    "y": 2384
                }
            ],
            "category": "paragraph",
            "html": "<p id='69' style='font-size:14px'>where ⊙ denotes the element-wise product of vectors. We<br>then project the vector to the output layer:</p>",
            "id": 69,
            "page": 4,
            "text": "where ⊙ denotes the element-wise product of vectors. We then project the vector to the output layer:"
        },
        {
            "bounding_box": [
                {
                    "x": 218,
                    "y": 2477
                },
                {
                    "x": 1225,
                    "y": 2477
                },
                {
                    "x": 1225,
                    "y": 2649
                },
                {
                    "x": 218,
                    "y": 2649
                }
            ],
            "category": "paragraph",
            "html": "<p id='70' style='font-size:14px'>where aout and h denote the activation function and edge<br>weights of the output layer, respectively. Intuitively, if we<br>use an identity function for aout and enforce h to be a uni-<br>form vector of 1, we can exactly recover the MF model.</p>",
            "id": 70,
            "page": 4,
            "text": "where aout and h denote the activation function and edge weights of the output layer, respectively. Intuitively, if we use an identity function for aout and enforce h to be a uniform vector of 1, we can exactly recover the MF model."
        },
        {
            "bounding_box": [
                {
                    "x": 218,
                    "y": 2653
                },
                {
                    "x": 1228,
                    "y": 2653
                },
                {
                    "x": 1228,
                    "y": 3002
                },
                {
                    "x": 218,
                    "y": 3002
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='71' style='font-size:14px'>Under the NCF framework, MF can be easily general-<br>ized and extended. For example, if we allow h to be learnt<br>from data without the uniform constraint, it will result in<br>a variant of MF that allows varying importance of latent<br>dimensions. And if we use a non-linear function for aout, it<br>will generalize MF to a non-linear setting which might be<br>more expressive than the linear MF model. In this work, we<br>implement a generalized version of MF under NCF that uses</p>",
            "id": 71,
            "page": 4,
            "text": "Under the NCF framework, MF can be easily generalized and extended. For example, if we allow h to be learnt from data without the uniform constraint, it will result in a variant of MF that allows varying importance of latent dimensions. And if we use a non-linear function for aout, it will generalize MF to a non-linear setting which might be more expressive than the linear MF model. In this work, we implement a generalized version of MF under NCF that uses"
        },
        {
            "bounding_box": [
                {
                    "x": 1314,
                    "y": 233
                },
                {
                    "x": 2324,
                    "y": 233
                },
                {
                    "x": 2324,
                    "y": 366
                },
                {
                    "x": 1314,
                    "y": 366
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='72' style='font-size:14px'>the sigmoid function �(x) = 1/(1 +e-x) as aout and learns<br>h from data with the log loss (Section 3.1.1). We term it as<br>GMF, short for Generalized Matrix Factorization.</p>",
            "id": 72,
            "page": 4,
            "text": "the sigmoid function �(x) = 1/(1 +e-x) as aout and learns h from data with the log loss (Section 3.1.1). We term it as GMF, short for Generalized Matrix Factorization."
        },
        {
            "bounding_box": [
                {
                    "x": 1315,
                    "y": 397
                },
                {
                    "x": 2102,
                    "y": 397
                },
                {
                    "x": 2102,
                    "y": 448
                },
                {
                    "x": 1315,
                    "y": 448
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='73' style='font-size:22px'>3.3 Multi-Layer Perceptron (MLP)</p>",
            "id": 73,
            "page": 4,
            "text": "3.3 Multi-Layer Perceptron (MLP)"
        },
        {
            "bounding_box": [
                {
                    "x": 1313,
                    "y": 459
                },
                {
                    "x": 2323,
                    "y": 459
                },
                {
                    "x": 2323,
                    "y": 1115
                },
                {
                    "x": 1313,
                    "y": 1115
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='74' style='font-size:14px'>Since NCF adopts two pathways to model users and items,<br>it is intuitive to combine the features of two pathways by<br>concatenating them. This design has been widely adopted<br>in multimodal deep learning work [47, 34]. However, simply<br>a vector concatenation does not account for any interactions<br>between user and item latent features, which is insufficient<br>for modelling the collaborative filtering effect. To address<br>this issue, we propose to add hidden layers on the concate-<br>nated vector, using a standard MLP to learn the interaction<br>between user and item latent features. In this sense, we can<br>endow the model a large level of flexibility and non-linearity<br>to learn the interactions between Pu and qi, rather than the<br>way of GMF that uses only a fixed element-wise product<br>on them. More precisely, the MLP model under our NCF<br>framework is defined as</p>",
            "id": 74,
            "page": 4,
            "text": "Since NCF adopts two pathways to model users and items, it is intuitive to combine the features of two pathways by concatenating them. This design has been widely adopted in multimodal deep learning work . However, simply a vector concatenation does not account for any interactions between user and item latent features, which is insufficient for modelling the collaborative filtering effect. To address this issue, we propose to add hidden layers on the concatenated vector, using a standard MLP to learn the interaction between user and item latent features. In this sense, we can endow the model a large level of flexibility and non-linearity to learn the interactions between Pu and qi, rather than the way of GMF that uses only a fixed element-wise product on them. More precisely, the MLP model under our NCF framework is defined as"
        },
        {
            "bounding_box": [
                {
                    "x": 1313,
                    "y": 1507
                },
                {
                    "x": 2322,
                    "y": 1507
                },
                {
                    "x": 2322,
                    "y": 2337
                },
                {
                    "x": 1313,
                    "y": 2337
                }
            ],
            "category": "paragraph",
            "html": "<p id='75' style='font-size:14px'>where Wx, bx, and ax denote the weight matrix, bias vec-<br>tor, and activation function for the x-th layer's perceptron,<br>respectively. For activation functions of MLP layers, one<br>can freely choose sigmoid, hyperbolic tangent (tanh), and<br>Rectifier (ReLU), among others. We would like to ana-<br>lyze each function: 1) The sigmoid function restricts each<br>neuron to be in (0,1), which may limit the model's perfor-<br>mance; and it is known to suffer from saturation, where<br>neurons stop learning when their output is near either 0 or<br>1. 2) Even though tanh is a better choice and has been<br>widely adopted [6, 44], it only alleviates the issues of sig-<br>moid to a certain extent, since it can be seen as a rescaled<br>version of sigmoid (tanh(x/2) = 2ヶ(x) - 1). And 3) as<br>such, we opt for ReLU, which is more biologically plausi-<br>ble and proven to be non-saturated [9]; moreover, it encour-<br>ages sparse activations, being well-suited for sparse data and<br>making the model less likely to be overfitting. Our empirical<br>results show that ReLU yields slightly better performance<br>than tanh, which in turn is significantly better than sigmoid.</p>",
            "id": 75,
            "page": 4,
            "text": "where Wx, bx, and ax denote the weight matrix, bias vector, and activation function for the x-th layer's perceptron, respectively. For activation functions of MLP layers, one can freely choose sigmoid, hyperbolic tangent (tanh), and Rectifier (ReLU), among others. We would like to analyze each function: 1) The sigmoid function restricts each neuron to be in (0,1), which may limit the model's performance; and it is known to suffer from saturation, where neurons stop learning when their output is near either 0 or 1. 2) Even though tanh is a better choice and has been widely adopted , it only alleviates the issues of sigmoid to a certain extent, since it can be seen as a rescaled version of sigmoid (tanh(x/2) = 2ヶ(x) - 1). And 3) as such, we opt for ReLU, which is more biologically plausible and proven to be non-saturated ; moreover, it encourages sparse activations, being well-suited for sparse data and making the model less likely to be overfitting. Our empirical results show that ReLU yields slightly better performance than tanh, which in turn is significantly better than sigmoid."
        },
        {
            "bounding_box": [
                {
                    "x": 1313,
                    "y": 2339
                },
                {
                    "x": 2323,
                    "y": 2339
                },
                {
                    "x": 2323,
                    "y": 2687
                },
                {
                    "x": 1313,
                    "y": 2687
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='76' style='font-size:14px'>As for the design of network structure, a common solution<br>is to follow a tower pattern, where the bottom layer is the<br>widest and each successive layer has a smaller number of<br>neurons (as in Figure 2). The premise is that by using a<br>small number of hidden units for higher layers, they can<br>learn more abstractive features of data [10]. We empirically<br>implement the tower structure, halving the layer size for<br>each successive higher layer.</p>",
            "id": 76,
            "page": 4,
            "text": "As for the design of network structure, a common solution is to follow a tower pattern, where the bottom layer is the widest and each successive layer has a smaller number of neurons (as in Figure 2). The premise is that by using a small number of hidden units for higher layers, they can learn more abstractive features of data . We empirically implement the tower structure, halving the layer size for each successive higher layer."
        },
        {
            "bounding_box": [
                {
                    "x": 1315,
                    "y": 2718
                },
                {
                    "x": 1979,
                    "y": 2718
                },
                {
                    "x": 1979,
                    "y": 2769
                },
                {
                    "x": 1315,
                    "y": 2769
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='77' style='font-size:20px'>3.4 Fusion of GMF and MLP</p>",
            "id": 77,
            "page": 4,
            "text": "3.4 Fusion of GMF and MLP"
        },
        {
            "bounding_box": [
                {
                    "x": 1314,
                    "y": 2781
                },
                {
                    "x": 2324,
                    "y": 2781
                },
                {
                    "x": 2324,
                    "y": 3002
                },
                {
                    "x": 1314,
                    "y": 3002
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='78' style='font-size:14px'>So far we have developed two instantiations of NCF<br>GMF that applies a linear kernel to model the latent feature<br>interactions, and MLP that uses a non-linear kernel to learn<br>the interaction function from data. The question then arises:<br>how can we fuse GMF and MLP under the NCF framework,</p>",
            "id": 78,
            "page": 4,
            "text": "So far we have developed two instantiations of NCF GMF that applies a linear kernel to model the latent feature interactions, and MLP that uses a non-linear kernel to learn the interaction function from data. The question then arises: how can we fuse GMF and MLP under the NCF framework,"
        },
        {
            "bounding_box": [
                {
                    "x": 215,
                    "y": 209
                },
                {
                    "x": 1278,
                    "y": 209
                },
                {
                    "x": 1278,
                    "y": 863
                },
                {
                    "x": 215,
                    "y": 863
                }
            ],
            "category": "figure",
            "html": "<figure><img id='79' style='font-size:14px' alt=\"Training\nScore Yui yui Target\nLog loss\no\nNeuMF Layer\nConcatenation\nMLP Layer X\nGMF Layer ReLU\nMLP Layer 2\nElement-wise\nReLU\nProduct\nMLP Layer 1\nConcatenation\nMF User Vector MLP User Vector MF Item Vector MLP Item Vector\n0 0 0 1 0 0 0 0 0 0 1 0\nUser (u) Item (i)\" data-coord=\"top-left:(215,209); bottom-right:(1278,863)\" /></figure>",
            "id": 79,
            "page": 5,
            "text": "Training Score Yui yui Target Log loss o NeuMF Layer Concatenation MLP Layer X GMF Layer ReLU MLP Layer 2 Element-wise ReLU Product MLP Layer 1 Concatenation MF User Vector MLP User Vector MF Item Vector MLP Item Vector 0 0 0 1 0 0 0 0 0 0 1 0 User (u) Item (i)"
        },
        {
            "bounding_box": [
                {
                    "x": 285,
                    "y": 874
                },
                {
                    "x": 1153,
                    "y": 874
                },
                {
                    "x": 1153,
                    "y": 919
                },
                {
                    "x": 285,
                    "y": 919
                }
            ],
            "category": "caption",
            "html": "<br><caption id='80' style='font-size:16px'>Figure 3: Neural matrix factorization model</caption>",
            "id": 80,
            "page": 5,
            "text": "Figure 3: Neural matrix factorization model"
        },
        {
            "bounding_box": [
                {
                    "x": 216,
                    "y": 945
                },
                {
                    "x": 1223,
                    "y": 945
                },
                {
                    "x": 1223,
                    "y": 1031
                },
                {
                    "x": 216,
                    "y": 1031
                }
            ],
            "category": "paragraph",
            "html": "<p id='81' style='font-size:14px'>SO that they can mutually reinforce each other to better<br>model the complex user-iterm interactions?</p>",
            "id": 81,
            "page": 5,
            "text": "SO that they can mutually reinforce each other to better model the complex user-iterm interactions?"
        },
        {
            "bounding_box": [
                {
                    "x": 216,
                    "y": 1034
                },
                {
                    "x": 1228,
                    "y": 1034
                },
                {
                    "x": 1228,
                    "y": 1294
                },
                {
                    "x": 216,
                    "y": 1294
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='82' style='font-size:16px'>A straightforward solution is to let GMF and MLP share<br>the same embedding layer, and then combine the outputs of<br>their interaction functions. This way shares a similar spirit<br>with the well-known Neural Tensor Network (NTN) [33].<br>Specifically, the model for combining GMF with a one-layer<br>MLP can be formulated as</p>",
            "id": 82,
            "page": 5,
            "text": "A straightforward solution is to let GMF and MLP share the same embedding layer, and then combine the outputs of their interaction functions. This way shares a similar spirit with the well-known Neural Tensor Network (NTN) . Specifically, the model for combining GMF with a one-layer MLP can be formulated as"
        },
        {
            "bounding_box": [
                {
                    "x": 216,
                    "y": 1434
                },
                {
                    "x": 1229,
                    "y": 1434
                },
                {
                    "x": 1229,
                    "y": 1695
                },
                {
                    "x": 216,
                    "y": 1695
                }
            ],
            "category": "paragraph",
            "html": "<p id='83' style='font-size:16px'>However, sharing embeddings of GMF and MLP might<br>limit the performance of the fused model. For example,<br>it implies that GMF and MLP must use the same size of<br>embeddings; for datasets where the optimal embedding size<br>of the two models varies a lot, this solution may fail to obtain<br>the optimal ensemble.</p>",
            "id": 83,
            "page": 5,
            "text": "However, sharing embeddings of GMF and MLP might limit the performance of the fused model. For example, it implies that GMF and MLP must use the same size of embeddings; for datasets where the optimal embedding size of the two models varies a lot, this solution may fail to obtain the optimal ensemble."
        },
        {
            "bounding_box": [
                {
                    "x": 218,
                    "y": 1696
                },
                {
                    "x": 1230,
                    "y": 1696
                },
                {
                    "x": 1230,
                    "y": 1915
                },
                {
                    "x": 218,
                    "y": 1915
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='84' style='font-size:16px'>To provide more flexibility to the fused model, we allow<br>GMF and MLP to learn separate embeddings, and combine<br>the two models by concatenating their last hidden layer.<br>Figure 3 illustrates our proposal, the formulation of which<br>is given as follows</p>",
            "id": 84,
            "page": 5,
            "text": "To provide more flexibility to the fused model, we allow GMF and MLP to learn separate embeddings, and combine the two models by concatenating their last hidden layer. Figure 3 illustrates our proposal, the formulation of which is given as follows"
        },
        {
            "bounding_box": [
                {
                    "x": 216,
                    "y": 2248
                },
                {
                    "x": 1228,
                    "y": 2248
                },
                {
                    "x": 1228,
                    "y": 2691
                },
                {
                    "x": 216,
                    "y": 2691
                }
            ],
            "category": "paragraph",
            "html": "<p id='85' style='font-size:16px'>where pG and pM denote the user embedding for GMF<br>and MLP parts, respectively; and similar notations of q.G<br>and qM for item embeddings. As discussed before, we use<br>ReLU as the activation function of MLP layers. This model<br>combines the linearity of MF and non-linearity of DNNs for<br>modelling user-item latent structures. We dub this model<br>\"NeuMF\", short for Neural Matrix Factorization. The deriva-<br>tive of the model w.r.t. each model parameter can be cal-<br>culated with standard back-propagation, which is omitted<br>here due to space limitation.</p>",
            "id": 85,
            "page": 5,
            "text": "where pG and pM denote the user embedding for GMF and MLP parts, respectively; and similar notations of q.G and qM for item embeddings. As discussed before, we use ReLU as the activation function of MLP layers. This model combines the linearity of MF and non-linearity of DNNs for modelling user-item latent structures. We dub this model \"NeuMF\", short for Neural Matrix Factorization. The derivative of the model w.r.t. each model parameter can be calculated with standard back-propagation, which is omitted here due to space limitation."
        },
        {
            "bounding_box": [
                {
                    "x": 230,
                    "y": 2722
                },
                {
                    "x": 599,
                    "y": 2722
                },
                {
                    "x": 599,
                    "y": 2771
                },
                {
                    "x": 230,
                    "y": 2771
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='86' style='font-size:18px'>3.4.1 Pre-training</p>",
            "id": 86,
            "page": 5,
            "text": "3.4.1 Pre-training"
        },
        {
            "bounding_box": [
                {
                    "x": 218,
                    "y": 2780
                },
                {
                    "x": 1262,
                    "y": 2780
                },
                {
                    "x": 1262,
                    "y": 3003
                },
                {
                    "x": 218,
                    "y": 3003
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='87' style='font-size:16px'>Due to the non-convexity of the objective function of NeuMF,<br>gradient-based optimization methods only find locally-optimal<br>solutions. It is reported that the initialization plays an im-<br>portant role for the convergence and performance of deep<br>learning models [7]. Since NeuMF is an ensemble of GMF</p>",
            "id": 87,
            "page": 5,
            "text": "Due to the non-convexity of the objective function of NeuMF, gradient-based optimization methods only find locally-optimal solutions. It is reported that the initialization plays an important role for the convergence and performance of deep learning models . Since NeuMF is an ensemble of GMF"
        },
        {
            "bounding_box": [
                {
                    "x": 1314,
                    "y": 234
                },
                {
                    "x": 2318,
                    "y": 234
                },
                {
                    "x": 2318,
                    "y": 316
                },
                {
                    "x": 1314,
                    "y": 316
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='88' style='font-size:14px'>and MLP, we propose to initialize NeuMF using the pre-<br>trained models of GMF and MLP.</p>",
            "id": 88,
            "page": 5,
            "text": "and MLP, we propose to initialize NeuMF using the pretrained models of GMF and MLP."
        },
        {
            "bounding_box": [
                {
                    "x": 1313,
                    "y": 320
                },
                {
                    "x": 2324,
                    "y": 320
                },
                {
                    "x": 2324,
                    "y": 538
                },
                {
                    "x": 1313,
                    "y": 538
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='89' style='font-size:14px'>We first train GMF and MLP with random initializations<br>until convergence. We then use their model parameters as<br>the initialization for the corresponding parts of NeuMF's<br>parameters. The only tweak is on the output layer, where<br>we concatenate weights of the two models with</p>",
            "id": 89,
            "page": 5,
            "text": "We first train GMF and MLP with random initializations until convergence. We then use their model parameters as the initialization for the corresponding parts of NeuMF's parameters. The only tweak is on the output layer, where we concatenate weights of the two models with"
        },
        {
            "bounding_box": [
                {
                    "x": 1313,
                    "y": 683
                },
                {
                    "x": 2323,
                    "y": 683
                },
                {
                    "x": 2323,
                    "y": 860
                },
                {
                    "x": 1313,
                    "y": 860
                }
            ],
            "category": "paragraph",
            "html": "<p id='90' style='font-size:14px'>and hMLP denote the h vector of the pre-<br>where hGMF<br>trained GMF and MLP model, respectively; and a is a<br>hyper-parameter determining the trade-off between the two<br>pre-trained models.</p>",
            "id": 90,
            "page": 5,
            "text": "and hMLP denote the h vector of the prewhere hGMF trained GMF and MLP model, respectively; and a is a hyper-parameter determining the trade-off between the two pre-trained models."
        },
        {
            "bounding_box": [
                {
                    "x": 1313,
                    "y": 863
                },
                {
                    "x": 2324,
                    "y": 863
                },
                {
                    "x": 2324,
                    "y": 1433
                },
                {
                    "x": 1313,
                    "y": 1433
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='91' style='font-size:16px'>For training GMF and MLP from scratch, we adopt the<br>Adaptive Moment Estimation (Adam) [20], which adapts<br>the learning rate for each parameter by performing smaller<br>updates for frequent and larger updates for infrequent pa-<br>rameters. The Adam method yields faster convergence for<br>both models than the vanilla SGD and relieves the pain of<br>tuning the learning rate. After feeding pre-trained parame-<br>ters into NeuMF, we optimize it with the vanilla SGD, rather<br>than Adam. This is because Adam needs to save momentum<br>information for updating parameters properly. As we ini-<br>tialize NeuMF with pre-trained model parameters only and<br>forgo saving the momentum information, it is unsuitable to<br>further optimize NeuMF with momentum-based methods.</p>",
            "id": 91,
            "page": 5,
            "text": "For training GMF and MLP from scratch, we adopt the Adaptive Moment Estimation (Adam) , which adapts the learning rate for each parameter by performing smaller updates for frequent and larger updates for infrequent parameters. The Adam method yields faster convergence for both models than the vanilla SGD and relieves the pain of tuning the learning rate. After feeding pre-trained parameters into NeuMF, we optimize it with the vanilla SGD, rather than Adam. This is because Adam needs to save momentum information for updating parameters properly. As we initialize NeuMF with pre-trained model parameters only and forgo saving the momentum information, it is unsuitable to further optimize NeuMF with momentum-based methods."
        },
        {
            "bounding_box": [
                {
                    "x": 1315,
                    "y": 1479
                },
                {
                    "x": 1777,
                    "y": 1479
                },
                {
                    "x": 1777,
                    "y": 1529
                },
                {
                    "x": 1315,
                    "y": 1529
                }
            ],
            "category": "paragraph",
            "html": "<p id='92' style='font-size:20px'>4. EXPERIMENTS</p>",
            "id": 92,
            "page": 5,
            "text": "4. EXPERIMENTS"
        },
        {
            "bounding_box": [
                {
                    "x": 1314,
                    "y": 1542
                },
                {
                    "x": 2321,
                    "y": 1542
                },
                {
                    "x": 2321,
                    "y": 1631
                },
                {
                    "x": 1314,
                    "y": 1631
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='93' style='font-size:14px'>In this section, we conduct experiments with the aim of<br>answering the following research questions:</p>",
            "id": 93,
            "page": 5,
            "text": "In this section, we conduct experiments with the aim of answering the following research questions:"
        },
        {
            "bounding_box": [
                {
                    "x": 1304,
                    "y": 1660
                },
                {
                    "x": 2319,
                    "y": 1660
                },
                {
                    "x": 2319,
                    "y": 1749
                },
                {
                    "x": 1304,
                    "y": 1749
                }
            ],
            "category": "paragraph",
            "html": "<p id='94' style='font-size:16px'>RQ1 Do our proposed NCF methods outperform the state-<br>of-the-art implicit collaborative filtering methods?</p>",
            "id": 94,
            "page": 5,
            "text": "RQ1 Do our proposed NCF methods outperform the stateof-the-art implicit collaborative filtering methods?"
        },
        {
            "bounding_box": [
                {
                    "x": 1306,
                    "y": 1776
                },
                {
                    "x": 2325,
                    "y": 1776
                },
                {
                    "x": 2325,
                    "y": 1913
                },
                {
                    "x": 1306,
                    "y": 1913
                }
            ],
            "category": "paragraph",
            "html": "<p id='95' style='font-size:16px'>RQ2 How does our proposed optimization framework (log<br>loss with negative sampling) work for the recommen-<br>dation task?</p>",
            "id": 95,
            "page": 5,
            "text": "RQ2 How does our proposed optimization framework (log loss with negative sampling) work for the recommendation task?"
        },
        {
            "bounding_box": [
                {
                    "x": 1305,
                    "y": 1943
                },
                {
                    "x": 2323,
                    "y": 1943
                },
                {
                    "x": 2323,
                    "y": 2033
                },
                {
                    "x": 1305,
                    "y": 2033
                }
            ],
            "category": "paragraph",
            "html": "<p id='96' style='font-size:18px'>RQ3 Are deeper layers of hidden units helpful for learning<br>from user-item interaction data?</p>",
            "id": 96,
            "page": 5,
            "text": "RQ3 Are deeper layers of hidden units helpful for learning from user-item interaction data?"
        },
        {
            "bounding_box": [
                {
                    "x": 1313,
                    "y": 2062
                },
                {
                    "x": 2321,
                    "y": 2062
                },
                {
                    "x": 2321,
                    "y": 2154
                },
                {
                    "x": 1313,
                    "y": 2154
                }
            ],
            "category": "paragraph",
            "html": "<p id='97' style='font-size:16px'>In what follows, we first present the experimental settings,<br>followed by answering the above three research questions.</p>",
            "id": 97,
            "page": 5,
            "text": "In what follows, we first present the experimental settings, followed by answering the above three research questions."
        },
        {
            "bounding_box": [
                {
                    "x": 1315,
                    "y": 2178
                },
                {
                    "x": 1909,
                    "y": 2178
                },
                {
                    "x": 1909,
                    "y": 2231
                },
                {
                    "x": 1315,
                    "y": 2231
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='98' style='font-size:22px'>4.1 Experimental Settings</p>",
            "id": 98,
            "page": 5,
            "text": "4.1 Experimental Settings"
        },
        {
            "bounding_box": [
                {
                    "x": 1314,
                    "y": 2238
                },
                {
                    "x": 2322,
                    "y": 2238
                },
                {
                    "x": 2322,
                    "y": 2370
                },
                {
                    "x": 1314,
                    "y": 2370
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='99' style='font-size:14px'>Datasets. We experimented with two publicly accessible<br>datasets: MovieLens4 and Pinterest5. The characteristics of<br>the two datasets are summarized in Table 1.</p>",
            "id": 99,
            "page": 5,
            "text": "Datasets. We experimented with two publicly accessible datasets: MovieLens4 and Pinterest5. The characteristics of the two datasets are summarized in Table 1."
        },
        {
            "bounding_box": [
                {
                    "x": 1313,
                    "y": 2372
                },
                {
                    "x": 2325,
                    "y": 2372
                },
                {
                    "x": 2325,
                    "y": 2763
                },
                {
                    "x": 1313,
                    "y": 2763
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='100' style='font-size:16px'>1. MovieLens. This movie rating dataset has been<br>widely used to evaluate collaborative filtering algorithms.<br>We used the version containing one million ratings, where<br>each user has at least 20 ratings. While it is an explicit<br>feedback data, we have intentionally chosen it to investigate<br>the performance of learning from the implicit signal [21] of<br>explicit feedback. To this end, we transformed it into im-<br>plicit data, where each entry is marked as 0 or 1 indicating<br>whether the user has rated the item.</p>",
            "id": 100,
            "page": 5,
            "text": "1. MovieLens. This movie rating dataset has been widely used to evaluate collaborative filtering algorithms. We used the version containing one million ratings, where each user has at least 20 ratings. While it is an explicit feedback data, we have intentionally chosen it to investigate the performance of learning from the implicit signal  of explicit feedback. To this end, we transformed it into implicit data, where each entry is marked as 0 or 1 indicating whether the user has rated the item."
        },
        {
            "bounding_box": [
                {
                    "x": 1314,
                    "y": 2763
                },
                {
                    "x": 2320,
                    "y": 2763
                },
                {
                    "x": 2320,
                    "y": 2852
                },
                {
                    "x": 1314,
                    "y": 2852
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='101' style='font-size:18px'>2. Pinterest. This implicit feedback data is constructed<br>by [8] for evaluating content-based image recommendation.</p>",
            "id": 101,
            "page": 5,
            "text": "2. Pinterest. This implicit feedback data is constructed by  for evaluating content-based image recommendation."
        },
        {
            "bounding_box": [
                {
                    "x": 1315,
                    "y": 2875
                },
                {
                    "x": 2224,
                    "y": 2875
                },
                {
                    "x": 2224,
                    "y": 3002
                },
                {
                    "x": 1315,
                    "y": 3002
                }
            ],
            "category": "paragraph",
            "html": "<p id='102' style='font-size:14px'>4http : / / grouplens · org/datasets/movielens / 1m/<br>5https : //sites · google · com/site/xueatalphabeta/<br>academic-projects</p>",
            "id": 102,
            "page": 5,
            "text": "4http : / / grouplens · org/datasets/movielens / 1m/ 5https : //sites · google · com/site/xueatalphabeta/ academic-projects"
        },
        {
            "bounding_box": [
                {
                    "x": 276,
                    "y": 220
                },
                {
                    "x": 1160,
                    "y": 220
                },
                {
                    "x": 1160,
                    "y": 264
                },
                {
                    "x": 276,
                    "y": 264
                }
            ],
            "category": "caption",
            "html": "<caption id='103' style='font-size:14px'>Table 1: Statistics of the evaluation datasets.</caption>",
            "id": 103,
            "page": 6,
            "text": "Table 1: Statistics of the evaluation datasets."
        },
        {
            "bounding_box": [
                {
                    "x": 220,
                    "y": 274
                },
                {
                    "x": 1238,
                    "y": 274
                },
                {
                    "x": 1238,
                    "y": 405
                },
                {
                    "x": 220,
                    "y": 405
                }
            ],
            "category": "table",
            "html": "<br><table id='104' style='font-size:14px'><tr><td>Dataset</td><td>Interaction#</td><td>Item#</td><td>User#</td><td>Sparsity</td></tr><tr><td>MovieLens</td><td>1,000,209</td><td>3,706</td><td>6,040</td><td>95.53%</td></tr><tr><td>Pinterest</td><td>1,500,809</td><td>9,916</td><td>55,187</td><td>99.73%</td></tr></table>",
            "id": 104,
            "page": 6,
            "text": "Dataset Interaction# Item# User# Sparsity  MovieLens 1,000,209 3,706 6,040 95.53%  Pinterest 1,500,809 9,916 55,187"
        },
        {
            "bounding_box": [
                {
                    "x": 218,
                    "y": 435
                },
                {
                    "x": 1228,
                    "y": 435
                },
                {
                    "x": 1228,
                    "y": 787
                },
                {
                    "x": 218,
                    "y": 787
                }
            ],
            "category": "paragraph",
            "html": "<p id='105' style='font-size:14px'>The original data is very large but highly sparse. For exam-<br>ple, over 20% of users have only one pin, making it difficult<br>to evaluate collaborative filtering algorithms. As such, we<br>filtered the dataset in the same way as the MovieLens data<br>that retained only users with at least 20 interactions (pins).<br>This results in a subset of the data that contains 55, 187<br>users and 1, 500, 809 interactions. Each interaction denotes<br>whether the user has pinned the image to her own board.</p>",
            "id": 105,
            "page": 6,
            "text": "The original data is very large but highly sparse. For example, over 20% of users have only one pin, making it difficult to evaluate collaborative filtering algorithms. As such, we filtered the dataset in the same way as the MovieLens data that retained only users with at least 20 interactions (pins). This results in a subset of the data that contains 55, 187 users and 1, 500, 809 interactions. Each interaction denotes whether the user has pinned the image to her own board."
        },
        {
            "bounding_box": [
                {
                    "x": 217,
                    "y": 802
                },
                {
                    "x": 1230,
                    "y": 802
                },
                {
                    "x": 1230,
                    "y": 1553
                },
                {
                    "x": 217,
                    "y": 1553
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='106' style='font-size:16px'>Evaluation Protocols. To evaluate the performance of<br>item recommendation, we adopted the leave-one-out evalu-<br>ation, which has been widely used in literature [1, 14, 27].<br>For each user, we held-out her latest interaction as the test<br>set and utilized the remaining data for training. Since it is<br>too time-consuming to rank all items for every user during<br>evaluation, we followed the common strategy [6, 21] that<br>randomly samples 100 items that are not interacted by the<br>user, ranking the test item among the 100 items. The perfor-<br>mance of a ranked list is judged by Hit Ratio (HR) and Nor-<br>malized Discounted Cumulative Gain (NDCG) [11]. With-<br>out special mention, we truncated the ranked list at 10 for<br>both metrics. As such, the HR intuitively measures whether<br>the test item is present on the top-10 list, and the NDCG<br>accounts for the position of the hit by assigning higher scores<br>to hits at top ranks. We calculated both metrics for each<br>test user and reported the average score.</p>",
            "id": 106,
            "page": 6,
            "text": "Evaluation Protocols. To evaluate the performance of item recommendation, we adopted the leave-one-out evaluation, which has been widely used in literature . For each user, we held-out her latest interaction as the test set and utilized the remaining data for training. Since it is too time-consuming to rank all items for every user during evaluation, we followed the common strategy  that randomly samples 100 items that are not interacted by the user, ranking the test item among the 100 items. The performance of a ranked list is judged by Hit Ratio (HR) and Normalized Discounted Cumulative Gain (NDCG) . Without special mention, we truncated the ranked list at 10 for both metrics. As such, the HR intuitively measures whether the test item is present on the top-10 list, and the NDCG accounts for the position of the hit by assigning higher scores to hits at top ranks. We calculated both metrics for each test user and reported the average score."
        },
        {
            "bounding_box": [
                {
                    "x": 219,
                    "y": 1570
                },
                {
                    "x": 1242,
                    "y": 1570
                },
                {
                    "x": 1242,
                    "y": 1657
                },
                {
                    "x": 219,
                    "y": 1657
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='107' style='font-size:18px'>Baselines. We compared our proposed NCF methods (GMF,<br>MLP and NeuMF) with the following methods:</p>",
            "id": 107,
            "page": 6,
            "text": "Baselines. We compared our proposed NCF methods (GMF, MLP and NeuMF) with the following methods:"
        },
        {
            "bounding_box": [
                {
                    "x": 220,
                    "y": 1659
                },
                {
                    "x": 1229,
                    "y": 1659
                },
                {
                    "x": 1229,
                    "y": 1791
                },
                {
                    "x": 220,
                    "y": 1791
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='108' style='font-size:16px'>- ItemPop. Items are ranked by their popularity judged<br>by the number of interactions. This is a non-personalized<br>method to benchmark the recommendation performance [27].</p>",
            "id": 108,
            "page": 6,
            "text": "- ItemPop. Items are ranked by their popularity judged by the number of interactions. This is a non-personalized method to benchmark the recommendation performance ."
        },
        {
            "bounding_box": [
                {
                    "x": 218,
                    "y": 1790
                },
                {
                    "x": 1226,
                    "y": 1790
                },
                {
                    "x": 1226,
                    "y": 1920
                },
                {
                    "x": 218,
                    "y": 1920
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='109' style='font-size:16px'>- ItemKNN [31]. This is the standard item-based col-<br>laborative filtering method. We followed the setting of [19]<br>to adapt it for implicit data.</p>",
            "id": 109,
            "page": 6,
            "text": "- ItemKNN . This is the standard item-based collaborative filtering method. We followed the setting of  to adapt it for implicit data."
        },
        {
            "bounding_box": [
                {
                    "x": 218,
                    "y": 1920
                },
                {
                    "x": 1230,
                    "y": 1920
                },
                {
                    "x": 1230,
                    "y": 2139
                },
                {
                    "x": 218,
                    "y": 2139
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='110' style='font-size:16px'>- BPR [27]. This method optimizes the MF model of<br>Equation 2 with a pairwise ranking loss, which is tailored<br>to learn from implicit feedback. It is a highly competitive<br>baseline for item recommendation. We used a fixed learning<br>rate, varying it and reporting the best performance.</p>",
            "id": 110,
            "page": 6,
            "text": "- BPR . This method optimizes the MF model of Equation 2 with a pairwise ranking loss, which is tailored to learn from implicit feedback. It is a highly competitive baseline for item recommendation. We used a fixed learning rate, varying it and reporting the best performance."
        },
        {
            "bounding_box": [
                {
                    "x": 217,
                    "y": 2141
                },
                {
                    "x": 1228,
                    "y": 2141
                },
                {
                    "x": 1228,
                    "y": 2445
                },
                {
                    "x": 217,
                    "y": 2445
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='111' style='font-size:16px'>- eALS [14]. This is a state-of-the-art MF method for<br>item recommendation. It optimizes the squared loss of Equa-<br>tion 5, treating all unobserved interactions as negative in-<br>stances and weighting them non-uniformly by the item pop-<br>ularity. Since eALS shows superior performance over the<br>uniform-weighting method WMF [19], we do not further re-<br>port WMF's performance.</p>",
            "id": 111,
            "page": 6,
            "text": "- eALS . This is a state-of-the-art MF method for item recommendation. It optimizes the squared loss of Equation 5, treating all unobserved interactions as negative instances and weighting them non-uniformly by the item popularity. Since eALS shows superior performance over the uniform-weighting method WMF , we do not further report WMF's performance."
        },
        {
            "bounding_box": [
                {
                    "x": 218,
                    "y": 2453
                },
                {
                    "x": 1228,
                    "y": 2453
                },
                {
                    "x": 1228,
                    "y": 2716
                },
                {
                    "x": 218,
                    "y": 2716
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='112' style='font-size:16px'>As our proposed methods aim to model the relationship<br>between users and items, we mainly compare with user-<br>item models. We leave out the comparison with item-item<br>models, such as SLIM [25] and CDAE [44], because the per-<br>formance difference may be caused by the user models for<br>personalization (as they are item-item model).</p>",
            "id": 112,
            "page": 6,
            "text": "As our proposed methods aim to model the relationship between users and items, we mainly compare with useritem models. We leave out the comparison with item-item models, such as SLIM  and CDAE , because the performance difference may be caused by the user models for personalization (as they are item-item model)."
        },
        {
            "bounding_box": [
                {
                    "x": 218,
                    "y": 2736
                },
                {
                    "x": 1229,
                    "y": 2736
                },
                {
                    "x": 1229,
                    "y": 2871
                },
                {
                    "x": 218,
                    "y": 2871
                }
            ],
            "category": "paragraph",
            "html": "<p id='113' style='font-size:14px'>Parameter Settings. We implemented our proposed meth-<br>ods based on Keras6 · To determine hyper-parameters of<br>NCF methods, we randomly sampled one interaction for</p>",
            "id": 113,
            "page": 6,
            "text": "Parameter Settings. We implemented our proposed methods based on Keras6 · To determine hyper-parameters of NCF methods, we randomly sampled one interaction for"
        },
        {
            "bounding_box": [
                {
                    "x": 219,
                    "y": 2914
                },
                {
                    "x": 971,
                    "y": 2914
                },
                {
                    "x": 971,
                    "y": 3000
                },
                {
                    "x": 219,
                    "y": 3000
                }
            ],
            "category": "paragraph",
            "html": "<p id='114' style='font-size:20px'>6https : //github. com/hexiangnan/neural_<br>collaborative_filtering</p>",
            "id": 114,
            "page": 6,
            "text": "6https : //github. com/hexiangnan/neural_ collaborative_filtering"
        },
        {
            "bounding_box": [
                {
                    "x": 1313,
                    "y": 231
                },
                {
                    "x": 2324,
                    "y": 231
                },
                {
                    "x": 2324,
                    "y": 1064
                },
                {
                    "x": 1313,
                    "y": 1064
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='115' style='font-size:16px'>each user as the validation data and tuned hyper-parameters<br>on it. All NCF models are learnt by optimizing the log loss<br>of Equation 7, where we sampled four negative instances<br>per positive instance. For NCF models that are trained<br>from scratch, we randomly initialized model parameters with<br>a Gaussian distribution (with a mean of 0 and standard<br>deviation of 0.01), optimizing the model with mini-batch<br>Adam [20]. We tested the batch size of [128, 256, 512, 1024],<br>and the learning rate of [0.0001 ,0.0005, 0.001, 0.005]. Since<br>the last hidden layer of NCF determines the model capa-<br>bility, we term it as predictive factors and evaluated the<br>factors of [8, 16, 32, 64]. It is worth noting that large factors<br>may cause overfitting and degrade the performance. With-<br>out special mention, we employed three hidden layers for<br>MLP; for example, if the size of predictive factors is 8, then<br>the architecture of the neural CF layers is 32 → 16 → 8, and<br>the embedding size is 16. For the NeuMF with pre-training,<br>a was set to 0.5, allowing the pre-trained GMF and MLP to<br>contribute equally to NeuMF's initialization.</p>",
            "id": 115,
            "page": 6,
            "text": "each user as the validation data and tuned hyper-parameters on it. All NCF models are learnt by optimizing the log loss of Equation 7, where we sampled four negative instances per positive instance. For NCF models that are trained from scratch, we randomly initialized model parameters with a Gaussian distribution (with a mean of 0 and standard deviation of 0.01), optimizing the model with mini-batch Adam . We tested the batch size of , and the learning rate of [0.0001 ,0.0005, 0.001, 0.005]. Since the last hidden layer of NCF determines the model capability, we term it as predictive factors and evaluated the factors of . It is worth noting that large factors may cause overfitting and degrade the performance. Without special mention, we employed three hidden layers for MLP; for example, if the size of predictive factors is 8, then the architecture of the neural CF layers is 32 → 16 → 8, and the embedding size is 16. For the NeuMF with pre-training, a was set to 0.5, allowing the pre-trained GMF and MLP to contribute equally to NeuMF's initialization."
        },
        {
            "bounding_box": [
                {
                    "x": 1314,
                    "y": 1105
                },
                {
                    "x": 2132,
                    "y": 1105
                },
                {
                    "x": 2132,
                    "y": 1160
                },
                {
                    "x": 1314,
                    "y": 1160
                }
            ],
            "category": "paragraph",
            "html": "<p id='116' style='font-size:22px'>4.2 Performance Comparison (RQ1)</p>",
            "id": 116,
            "page": 6,
            "text": "4.2 Performance Comparison (RQ1)"
        },
        {
            "bounding_box": [
                {
                    "x": 1313,
                    "y": 1167
                },
                {
                    "x": 2324,
                    "y": 1167
                },
                {
                    "x": 2324,
                    "y": 1516
                },
                {
                    "x": 1313,
                    "y": 1516
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='117' style='font-size:16px'>Figure 4 shows the performance of HR@10 and NDCG@10<br>with respect to the number of predictive factors. For MF<br>methods BPR and eALS, the number of predictive factors<br>is equal to the number of latent factors. For ItemKNN, we<br>tested different neighbor sizes and reported the best per-<br>formance. Due to the weak performance of ItemPop, it is<br>omitted in Figure 4 to better highlight the performance dif-<br>ference of personalized methods.</p>",
            "id": 117,
            "page": 6,
            "text": "Figure 4 shows the performance of HR@10 and NDCG@10 with respect to the number of predictive factors. For MF methods BPR and eALS, the number of predictive factors is equal to the number of latent factors. For ItemKNN, we tested different neighbor sizes and reported the best performance. Due to the weak performance of ItemPop, it is omitted in Figure 4 to better highlight the performance difference of personalized methods."
        },
        {
            "bounding_box": [
                {
                    "x": 1312,
                    "y": 1518
                },
                {
                    "x": 2326,
                    "y": 1518
                },
                {
                    "x": 2326,
                    "y": 2477
                },
                {
                    "x": 1312,
                    "y": 2477
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='118' style='font-size:16px'>First, we can see that NeuMF achieves the best perfor-<br>mance on both datasets, significantly outperforming the state-<br>of-the-art methods eALS and BPR by a large margin (on<br>average, the relative improvement over eALS and BPR is<br>4.5% and 4.9%, respectively). For Pinterest, even with a<br>small predictive factor of 8, NeuMF substantially outper-<br>forms that of eALS and BPR with a large factor of 64. This<br>indicates the high expressiveness of NeuMF by fusing the<br>linear MF and non-linear MLP models. Second, the other<br>two NCF methods GMF and MLP also show quite<br>strong performance. Between them, MLP slightly under-<br>performs GMF. Note that MLP can be further improved by<br>adding more hidden layers (see Section 4.4), and here we<br>only show the performance of three layers. For small pre-<br>dictive factors, GMF outperforms eALS on both datasets;<br>although GMF suffers from overfitting for large factors, its<br>best performance obtained is better than (or on par with)<br>that of eALS. Lastly, GMF shows consistent improvements<br>over BPR, admitting the effectiveness of the classification-<br>aware log loss for the recommendation task, since GMF and<br>BPR learn the same MF model but with different objective<br>functions.</p>",
            "id": 118,
            "page": 6,
            "text": "First, we can see that NeuMF achieves the best performance on both datasets, significantly outperforming the stateof-the-art methods eALS and BPR by a large margin (on average, the relative improvement over eALS and BPR is 4.5% and 4.9%, respectively). For Pinterest, even with a small predictive factor of 8, NeuMF substantially outperforms that of eALS and BPR with a large factor of 64. This indicates the high expressiveness of NeuMF by fusing the linear MF and non-linear MLP models. Second, the other two NCF methods GMF and MLP also show quite strong performance. Between them, MLP slightly underperforms GMF. Note that MLP can be further improved by adding more hidden layers (see Section 4.4), and here we only show the performance of three layers. For small predictive factors, GMF outperforms eALS on both datasets; although GMF suffers from overfitting for large factors, its best performance obtained is better than (or on par with) that of eALS. Lastly, GMF shows consistent improvements over BPR, admitting the effectiveness of the classificationaware log loss for the recommendation task, since GMF and BPR learn the same MF model but with different objective functions."
        },
        {
            "bounding_box": [
                {
                    "x": 1312,
                    "y": 2475
                },
                {
                    "x": 2323,
                    "y": 2475
                },
                {
                    "x": 2323,
                    "y": 3004
                },
                {
                    "x": 1312,
                    "y": 3004
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='119' style='font-size:16px'>Figure 5 shows the performance of Top-K recommended<br>lists where the ranking position K ranges from 1 to 10. To<br>make the figure more clear, we show the performance of<br>NeuMF rather than all three NCF methods. As can be<br>seen, NeuMF demonstrates consistent improvements over<br>other methods across positions, and we further conducted<br>one-sample paired t-tests, verifying that all improvements<br>are statistically significant for p < 0.01. For baseline meth-<br>ods, eALS outperforms BPR on MovieLens with about 5.1%<br>relative improvement, while underperforms BPR on Pinter-<br>est in terms of NDCG. This is consistent with [14]'s finding<br>that BPR can be a strong performer for ranking performance</p>",
            "id": 119,
            "page": 6,
            "text": "Figure 5 shows the performance of Top-K recommended lists where the ranking position K ranges from 1 to 10. To make the figure more clear, we show the performance of NeuMF rather than all three NCF methods. As can be seen, NeuMF demonstrates consistent improvements over other methods across positions, and we further conducted one-sample paired t-tests, verifying that all improvements are statistically significant for p < 0.01. For baseline methods, eALS outperforms BPR on MovieLens with about 5.1% relative improvement, while underperforms BPR on Pinterest in terms of NDCG. This is consistent with 's finding that BPR can be a strong performer for ranking performance"
        },
        {
            "bounding_box": [
                {
                    "x": 222,
                    "y": 217
                },
                {
                    "x": 2315,
                    "y": 217
                },
                {
                    "x": 2315,
                    "y": 699
                },
                {
                    "x": 222,
                    "y": 699
                }
            ],
            "category": "figure",
            "html": "<figure><img id='120' style='font-size:14px' alt=\"MovieLens MovieLens Pinterest Pinterest\n0.75 0.46 0.9 0.56\n0.7 0.42 0.87 0.54\n△\n10 NDCOGON @0.84 NDCOCIAN\n10\n@0.65\n또\n또\nItemKNN - △- BPR\nItemKNN - △- BPR\nItemKNN - △- BPR ItemKNN - △- BPR ○ - GMF 0.5 eALS - ○ - GMF\n0.6 0.34\n0.81 eALS\neALS ○ GMF eALS ○ - GMF X MLP NeuMF -*- MLP NeuMF\n*- MLP NeuMF MLP NeuMF\n0.55 0.3 0.78 0.48\n8 16 32 64 8 16 32 64 8 16 32 64 8 16 32 64\nFactors Factors Factors Factors\n(a) MovieLens HR@10 (b) MovieLens NDCG@10 (c) Pinterest HR@10 Pinterest NDCG@10\" data-coord=\"top-left:(222,217); bottom-right:(2315,699)\" /></figure>",
            "id": 120,
            "page": 7,
            "text": "MovieLens MovieLens Pinterest Pinterest 0.75 0.46 0.9 0.56 0.7 0.42 0.87 0.54 △ 10 NDCOGON @0.84 NDCOCIAN 10 @0.65 또 또 ItemKNN - △- BPR ItemKNN - △- BPR ItemKNN - △- BPR ItemKNN - △- BPR ○ - GMF 0.5 eALS - ○ - GMF 0.6 0.34 0.81 eALS eALS ○ GMF eALS ○ - GMF X MLP NeuMF -*- MLP NeuMF *- MLP NeuMF MLP NeuMF 0.55 0.3 0.78 0.48 8 16 32 64 8 16 32 64 8 16 32 64 8 16 32 64 Factors Factors Factors Factors (a) MovieLens HR@10 (b) MovieLens NDCG@10 (c) Pinterest HR@10 Pinterest NDCG@10"
        },
        {
            "bounding_box": [
                {
                    "x": 215,
                    "y": 669
                },
                {
                    "x": 2317,
                    "y": 669
                },
                {
                    "x": 2317,
                    "y": 782
                },
                {
                    "x": 215,
                    "y": 782
                }
            ],
            "category": "caption",
            "html": "<br><caption id='121' style='font-size:18px'>(d)<br>Figure 4: Performance of HR@10 and NDCG@10 w.r.t. the number of predictive factors on the two datasets.</caption>",
            "id": 121,
            "page": 7,
            "text": "(d) Figure 4: Performance of HR@10 and NDCG@10 w.r.t. the number of predictive factors on the two datasets."
        },
        {
            "bounding_box": [
                {
                    "x": 229,
                    "y": 817
                },
                {
                    "x": 2314,
                    "y": 817
                },
                {
                    "x": 2314,
                    "y": 1290
                },
                {
                    "x": 229,
                    "y": 1290
                }
            ],
            "category": "figure",
            "html": "<figure><img id='122' style='font-size:14px' alt=\"MovieLens MovieLens Pinterest Pinterest\n0.9 0.58\n0.7\n0.42\n△ 0.7 0.46\n0.55\n0.34\nItemPop Ke ItemPop\nHR@K\nHR@K\nItemKNN ItemKNN\n0.5 go.34 - △- BPR\n0.4 NDCGOK △\n- △- BPR\neALS\neALS\nNeuMF NeuMF\n0.25 ItemPop ItemKNN ItemPop ItemKNN 0.3 0.22\n0.18\n△- BPR eALS △- BPR eALS\nNeuMF NeuMF\n0.1 0.1 0.1 0.1\n1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10\nK K K K\" data-coord=\"top-left:(229,817); bottom-right:(2314,1290)\" /></figure>",
            "id": 122,
            "page": 7,
            "text": "MovieLens MovieLens Pinterest Pinterest 0.9 0.58 0.7 0.42 △ 0.7 0.46 0.55 0.34 ItemPop Ke ItemPop HR@K HR@K ItemKNN ItemKNN 0.5 go.34 - △- BPR 0.4 NDCGOK △ - △- BPR eALS eALS NeuMF NeuMF 0.25 ItemPop ItemKNN ItemPop ItemKNN 0.3 0.22 0.18 △- BPR eALS △- BPR eALS NeuMF NeuMF 0.1 0.1 0.1 0.1 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 K K K K"
        },
        {
            "bounding_box": [
                {
                    "x": 261,
                    "y": 1277
                },
                {
                    "x": 2299,
                    "y": 1277
                },
                {
                    "x": 2299,
                    "y": 1389
                },
                {
                    "x": 261,
                    "y": 1389
                }
            ],
            "category": "caption",
            "html": "<br><caption id='123' style='font-size:18px'>(a) MovieLens HR@K (b) MovieLens NDCG@K (c) Pinterest HR@K (d) Pinterest NDCG@K<br>Figure 5: Evaluation of Top-K item recommendation where K ranges from 1 to 10 on the two datasets.</caption>",
            "id": 123,
            "page": 7,
            "text": "(a) MovieLens HR@K (b) MovieLens NDCG@K (c) Pinterest HR@K (d) Pinterest NDCG@K Figure 5: Evaluation of Top-K item recommendation where K ranges from 1 to 10 on the two datasets."
        },
        {
            "bounding_box": [
                {
                    "x": 217,
                    "y": 1469
                },
                {
                    "x": 1227,
                    "y": 1469
                },
                {
                    "x": 1227,
                    "y": 1688
                },
                {
                    "x": 217,
                    "y": 1688
                }
            ],
            "category": "paragraph",
            "html": "<p id='124' style='font-size:18px'>owing to its pairwise ranking-aware learner. The neighbor-<br>based ItemKNN underperforms model-based methods. And<br>ItemPop performs the worst, indicating the necessity of mod-<br>eling users' personalized preferences, rather than just recom-<br>mending popular items to users.</p>",
            "id": 124,
            "page": 7,
            "text": "owing to its pairwise ranking-aware learner. The neighborbased ItemKNN underperforms model-based methods. And ItemPop performs the worst, indicating the necessity of modeling users' personalized preferences, rather than just recommending popular items to users."
        },
        {
            "bounding_box": [
                {
                    "x": 229,
                    "y": 1714
                },
                {
                    "x": 777,
                    "y": 1714
                },
                {
                    "x": 777,
                    "y": 1764
                },
                {
                    "x": 229,
                    "y": 1764
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='125' style='font-size:20px'>4.2.1 Utility of Pre-training</p>",
            "id": 125,
            "page": 7,
            "text": "4.2.1 Utility of Pre-training"
        },
        {
            "bounding_box": [
                {
                    "x": 219,
                    "y": 1772
                },
                {
                    "x": 1226,
                    "y": 1772
                },
                {
                    "x": 1226,
                    "y": 2300
                },
                {
                    "x": 219,
                    "y": 2300
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='126' style='font-size:18px'>To demonstrate the utility of pre-training for NeuMF, we<br>compared the performance of two versions of NeuMF<br>with and without pre-training. For NeuMF without pre-<br>training, we used the Adam to learn it with random ini-<br>tializations. As shown in Table 2, the NeuMF with pre-<br>training achieves better performance in most cases; only<br>for MovieLens with a small predictive factors of 8, the pre-<br>training method performs slightly worse. The relative im-<br>provements of the NeuMF with pre-training are 2.2% and<br>1.1% for MovieLens and Pinterest, respectively. This re-<br>sult justifies the usefulness of our pre-training method for<br>initializing NeuMF.</p>",
            "id": 126,
            "page": 7,
            "text": "To demonstrate the utility of pre-training for NeuMF, we compared the performance of two versions of NeuMF with and without pre-training. For NeuMF without pretraining, we used the Adam to learn it with random initializations. As shown in Table 2, the NeuMF with pretraining achieves better performance in most cases; only for MovieLens with a small predictive factors of 8, the pretraining method performs slightly worse. The relative improvements of the NeuMF with pre-training are 2.2% and 1.1% for MovieLens and Pinterest, respectively. This result justifies the usefulness of our pre-training method for initializing NeuMF."
        },
        {
            "bounding_box": [
                {
                    "x": 218,
                    "y": 2313
                },
                {
                    "x": 1224,
                    "y": 2313
                },
                {
                    "x": 1224,
                    "y": 2403
                },
                {
                    "x": 218,
                    "y": 2403
                }
            ],
            "category": "caption",
            "html": "<br><caption id='127' style='font-size:18px'>Table 2: Performance of NeuMF with and without<br>pre-training.</caption>",
            "id": 127,
            "page": 7,
            "text": "Table 2: Performance of NeuMF with and without pre-training."
        },
        {
            "bounding_box": [
                {
                    "x": 221,
                    "y": 2410
                },
                {
                    "x": 1262,
                    "y": 2410
                },
                {
                    "x": 1262,
                    "y": 2920
                },
                {
                    "x": 221,
                    "y": 2920
                }
            ],
            "category": "table",
            "html": "<br><table id='128' style='font-size:16px'><tr><td></td><td colspan=\"2\">With Pre-training</td><td colspan=\"2\">Without Pre-training</td></tr><tr><td>Factors</td><td>HR@10</td><td>NDCG@10</td><td>HR@10</td><td>NDCG@10</td></tr><tr><td colspan=\"5\">MovieLens</td></tr><tr><td>8</td><td>0.684</td><td>0.403</td><td>0.688</td><td>0.410</td></tr><tr><td>16</td><td>0.707</td><td>0.426</td><td>0.696</td><td>0.420</td></tr><tr><td>32</td><td>0.726</td><td>0.445</td><td>0.701</td><td>0.425</td></tr><tr><td>64</td><td>0.730</td><td>0.447</td><td>0.705</td><td>0.426</td></tr><tr><td colspan=\"5\">Pinterest</td></tr><tr><td>8</td><td>0.878</td><td>0.555</td><td>0.869</td><td>0.546</td></tr><tr><td>16</td><td>0.880</td><td>0.558</td><td>0.871</td><td>0.547</td></tr><tr><td>32</td><td>0.879</td><td>0.555</td><td>0.870</td><td>0.549</td></tr><tr><td>64</td><td>0.877</td><td>0.552</td><td>0.872</td><td>0.551</td></tr></table>",
            "id": 128,
            "page": 7,
            "text": "With Pre-training Without Pre-training  Factors HR@10 NDCG@10 HR@10 NDCG@10  MovieLens  8 0.684 0.403 0.688 0.410  16 0.707 0.426 0.696 0.420  32 0.726 0.445 0.701 0.425  64 0.730 0.447 0.705 0.426  Pinterest  8 0.878 0.555 0.869 0.546  16 0.880 0.558 0.871 0.547  32 0.879 0.555 0.870 0.549  64 0.877 0.552 0.872"
        },
        {
            "bounding_box": [
                {
                    "x": 1312,
                    "y": 1459
                },
                {
                    "x": 2293,
                    "y": 1459
                },
                {
                    "x": 2293,
                    "y": 1515
                },
                {
                    "x": 1312,
                    "y": 1515
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='129' style='font-size:22px'>4.3 Log Loss with Negative Sampling (RQ2)</p>",
            "id": 129,
            "page": 7,
            "text": "4.3 Log Loss with Negative Sampling (RQ2)"
        },
        {
            "bounding_box": [
                {
                    "x": 1312,
                    "y": 1519
                },
                {
                    "x": 2323,
                    "y": 1519
                },
                {
                    "x": 2323,
                    "y": 2396
                },
                {
                    "x": 1312,
                    "y": 2396
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='130' style='font-size:18px'>To deal with the one-class nature of implicit feedback,<br>we cast recommendation as a binary classification task. By<br>viewing NCF as a probabilistic model, we optimized it with<br>the log loss. Figure 6 shows the training loss (averaged<br>over all instances) and recommendation performance of NCF<br>methods of each iteration on MovieLens. Results on Pinter-<br>est show the same trend and thus they are omitted due to<br>space limitation. First, we can see that with more iterations,<br>the training loss of NCF models gradually decreases and<br>the recommendation performance is improved. The most<br>effective updates are occurred in the first 10 iterations, and<br>more iterations may overfit a model (e.g., although the train-<br>ing loss of NeuMF keeps decreasing after 10 iterations, its<br>recommendation performance actually degrades). Second,<br>among the three NCF methods, NeuMF achieves the lowest<br>training loss, followed by MLP, and then GMF. The rec-<br>ommendation performance also shows the same trend that<br>NeuMF > MLP > GMF. The above findings provide empir-<br>ical evidence for the rationality and effectiveness of optimiz-<br>ing the log loss for learning from implicit data.</p>",
            "id": 130,
            "page": 7,
            "text": "To deal with the one-class nature of implicit feedback, we cast recommendation as a binary classification task. By viewing NCF as a probabilistic model, we optimized it with the log loss. Figure 6 shows the training loss (averaged over all instances) and recommendation performance of NCF methods of each iteration on MovieLens. Results on Pinterest show the same trend and thus they are omitted due to space limitation. First, we can see that with more iterations, the training loss of NCF models gradually decreases and the recommendation performance is improved. The most effective updates are occurred in the first 10 iterations, and more iterations may overfit a model (e.g., although the training loss of NeuMF keeps decreasing after 10 iterations, its recommendation performance actually degrades). Second, among the three NCF methods, NeuMF achieves the lowest training loss, followed by MLP, and then GMF. The recommendation performance also shows the same trend that NeuMF > MLP > GMF. The above findings provide empirical evidence for the rationality and effectiveness of optimizing the log loss for learning from implicit data."
        },
        {
            "bounding_box": [
                {
                    "x": 1311,
                    "y": 2396
                },
                {
                    "x": 2324,
                    "y": 2396
                },
                {
                    "x": 2324,
                    "y": 2966
                },
                {
                    "x": 1311,
                    "y": 2966
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='131' style='font-size:18px'>An advantage of pointwise log loss over pairwise objective<br>functions [27, 33] is the flexible sampling ratio for negative<br>instances. While pairwise objective functions can pair only<br>one sampled negative instance with a positive instance, we<br>can flexibly control the sampling ratio of a pointwise loss. To<br>illustrate the impact of negative sampling for NCF methods,<br>we show the performance of NCF methods w.r.t. different<br>negative sampling ratios in Figure 7. It can be clearly seen<br>that just one negative sample per positive instance is insuf-<br>ficient to achieve optimal performance, and sampling more<br>negative instances is beneficial. Comparing GMF to BPR,<br>we can see the performance of GMF with a sampling ratio<br>of one is on par with BPR, while GMF significantly betters</p>",
            "id": 131,
            "page": 7,
            "text": "An advantage of pointwise log loss over pairwise objective functions  is the flexible sampling ratio for negative instances. While pairwise objective functions can pair only one sampled negative instance with a positive instance, we can flexibly control the sampling ratio of a pointwise loss. To illustrate the impact of negative sampling for NCF methods, we show the performance of NCF methods w.r.t. different negative sampling ratios in Figure 7. It can be clearly seen that just one negative sample per positive instance is insufficient to achieve optimal performance, and sampling more negative instances is beneficial. Comparing GMF to BPR, we can see the performance of GMF with a sampling ratio of one is on par with BPR, while GMF significantly betters"
        },
        {
            "bounding_box": [
                {
                    "x": 372,
                    "y": 212
                },
                {
                    "x": 2174,
                    "y": 212
                },
                {
                    "x": 2174,
                    "y": 767
                },
                {
                    "x": 372,
                    "y": 767
                }
            ],
            "category": "figure",
            "html": "<figure><img id='132' style='font-size:14px' alt=\"MovieLens MovieLens MovieLens\n0.5 0.5\nGMF\n0.7\nMLP\n0.4\nLoss 0.4 NeuMF\nHR@10\nTraining 0.5 NDCG10 0.3\n0.3\n0.2\n0.3 - - GMF - - - GMF\n0.2\nMLP 0.1 MLP\nNeuMF NeuMF\n0.1 0.1 0\n0 10 20 30 40 50 0 10 20 30 40 50 0 10 20 30 40 50\nIteration Iteration Iteration\n(a) Training Loss (b) HR@10 (c) NDCG@10\" data-coord=\"top-left:(372,212); bottom-right:(2174,767)\" /></figure>",
            "id": 132,
            "page": 8,
            "text": "MovieLens MovieLens MovieLens 0.5 0.5 GMF 0.7 MLP 0.4 Loss 0.4 NeuMF HR@10 Training 0.5 NDCG10 0.3 0.3 0.2 0.3 - - GMF - - - GMF 0.2 MLP 0.1 MLP NeuMF NeuMF 0.1 0.1 0 0 10 20 30 40 50 0 10 20 30 40 50 0 10 20 30 40 50 Iteration Iteration Iteration (a) Training Loss (b) HR@10 (c) NDCG@10"
        },
        {
            "bounding_box": [
                {
                    "x": 213,
                    "y": 740
                },
                {
                    "x": 2324,
                    "y": 740
                },
                {
                    "x": 2324,
                    "y": 867
                },
                {
                    "x": 213,
                    "y": 867
                }
            ],
            "category": "caption",
            "html": "<br><caption id='133' style='font-size:18px'>Figure 6: Training loss and recommendation performance of NCF methods w.r.t. the number of iterations<br>on MovieLens (factors=8).</caption>",
            "id": 133,
            "page": 8,
            "text": "Figure 6: Training loss and recommendation performance of NCF methods w.r.t. the number of iterations on MovieLens (factors=8)."
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 887
                },
                {
                    "x": 2318,
                    "y": 887
                },
                {
                    "x": 2318,
                    "y": 1371
                },
                {
                    "x": 225,
                    "y": 1371
                }
            ],
            "category": "figure",
            "html": "<figure><img id='134' style='font-size:14px' alt=\"MovieLens MovieLens Pinterest Pinterest\n0.72 0.44 0.89 0.57\n0.7 0.88 0.56\n0.42\n○- -Q )- -○ ○\n-○ O-\n-○-\nNDCG@10\nNDCG@10\nHR@10 0.68 0.87 0.55\n0.4\n*\n0.66 HR@10\n0.86 0.54\nNeuMF NeuMF\nNeuMF\n- ○ - GMF 0.38 -○ - GMF -○ - GMF\n0.64 0.85 0.53 NeuMF - ○ - GMF\n-*- MLP -*- MLP -*- MLP\nBPR BPR BPR -*- MLP BPR\n0.62 0.36 0.84 0.52\n1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10\nNumber of Negatives Number of Negatives Number of Negatives Number of Negatives\n(a) MovieLens HR@10 (b) MovieLens NDCG@10 (c) Pinterest HR@10 (d) Pinterest NDCG@10\" data-coord=\"top-left:(225,887); bottom-right:(2318,1371)\" /></figure>",
            "id": 134,
            "page": 8,
            "text": "MovieLens MovieLens Pinterest Pinterest 0.72 0.44 0.89 0.57 0.7 0.88 0.56 0.42 ○- -Q )- -○ ○ -○ O-○NDCG@10 NDCG@10 HR@10 0.68 0.87 0.55 0.4 * 0.66 HR@10 0.86 0.54 NeuMF NeuMF NeuMF - ○ - GMF 0.38 -○ - GMF -○ - GMF 0.64 0.85 0.53 NeuMF - ○ - GMF -*- MLP -*- MLP -*- MLP BPR BPR BPR -*- MLP BPR 0.62 0.36 0.84 0.52 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 1 2 3 4 5 6 7 8 9 10 Number of Negatives Number of Negatives Number of Negatives Number of Negatives (a) MovieLens HR@10 (b) MovieLens NDCG@10 (c) Pinterest HR@10 (d) Pinterest NDCG@10"
        },
        {
            "bounding_box": [
                {
                    "x": 215,
                    "y": 1342
                },
                {
                    "x": 2327,
                    "y": 1342
                },
                {
                    "x": 2327,
                    "y": 1525
                },
                {
                    "x": 215,
                    "y": 1525
                }
            ],
            "category": "caption",
            "html": "<br><caption id='135' style='font-size:20px'>Figure 7: Performance of NCF methods w.r.t. the number of negative samples per positive instance (fac-<br>tors=16). The performance of BPR is also shown, which samples only one negative instance to pair with a<br>positive instance for learning.</caption>",
            "id": 135,
            "page": 8,
            "text": "Figure 7: Performance of NCF methods w.r.t. the number of negative samples per positive instance (factors=16). The performance of BPR is also shown, which samples only one negative instance to pair with a positive instance for learning."
        },
        {
            "bounding_box": [
                {
                    "x": 217,
                    "y": 1564
                },
                {
                    "x": 1227,
                    "y": 1564
                },
                {
                    "x": 1227,
                    "y": 1873
                },
                {
                    "x": 217,
                    "y": 1873
                }
            ],
            "category": "paragraph",
            "html": "<p id='136' style='font-size:18px'>BPR with larger sampling ratios. This shows the advan-<br>tage of pointwise log loss over the pairwise BPR loss. For<br>both datasets, the optimal sampling ratio is around 3 to 6.<br>On Pinterest, we find that when the sampling ratio is larger<br>than 7, the performance of NCF methods starts to drop. It<br>reveals that setting the sampling ratio too aggressively may<br>adversely hurt the performance.</p>",
            "id": 136,
            "page": 8,
            "text": "BPR with larger sampling ratios. This shows the advantage of pointwise log loss over the pairwise BPR loss. For both datasets, the optimal sampling ratio is around 3 to 6. On Pinterest, we find that when the sampling ratio is larger than 7, the performance of NCF methods starts to drop. It reveals that setting the sampling ratio too aggressively may adversely hurt the performance."
        },
        {
            "bounding_box": [
                {
                    "x": 219,
                    "y": 1892
                },
                {
                    "x": 1052,
                    "y": 1892
                },
                {
                    "x": 1052,
                    "y": 1945
                },
                {
                    "x": 219,
                    "y": 1945
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='137' style='font-size:22px'>4.4 Is Deep Learning Helpful? (RQ3)</p>",
            "id": 137,
            "page": 8,
            "text": "4.4 Is Deep Learning Helpful? (RQ3)"
        },
        {
            "bounding_box": [
                {
                    "x": 218,
                    "y": 1954
                },
                {
                    "x": 1228,
                    "y": 1954
                },
                {
                    "x": 1228,
                    "y": 2693
                },
                {
                    "x": 218,
                    "y": 2693
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='138' style='font-size:16px'>As there is little work on learning user-item interaction<br>function with neural networks, it is curious to see whether<br>using a deep network structure is beneficial to the recom-<br>mendation task. Towards this end, we further investigated<br>MLP with different number of hidden layers. The results<br>are summarized in Table 3 and 4. The MLP-3 indicates<br>the MLP method with three hidden layers (besides the em-<br>bedding layer), and similar notations for others. As we can<br>see, even for models with the same capability, stacking more<br>layers are beneficial to performance. This result is highly<br>encouraging, indicating the effectiveness of using deep mod-<br>els for collaborative recommendation. We attribute the im-<br>provement to the high non-linearities brought by stacking<br>more non-linear layers. To verify this, we further tried stack-<br>ing linear layers, using an identity function as the activation<br>function. The performance is much worse than using the<br>ReLU unit.</p>",
            "id": 138,
            "page": 8,
            "text": "As there is little work on learning user-item interaction function with neural networks, it is curious to see whether using a deep network structure is beneficial to the recommendation task. Towards this end, we further investigated MLP with different number of hidden layers. The results are summarized in Table 3 and 4. The MLP-3 indicates the MLP method with three hidden layers (besides the embedding layer), and similar notations for others. As we can see, even for models with the same capability, stacking more layers are beneficial to performance. This result is highly encouraging, indicating the effectiveness of using deep models for collaborative recommendation. We attribute the improvement to the high non-linearities brought by stacking more non-linear layers. To verify this, we further tried stacking linear layers, using an identity function as the activation function. The performance is much worse than using the ReLU unit."
        },
        {
            "bounding_box": [
                {
                    "x": 217,
                    "y": 2694
                },
                {
                    "x": 1228,
                    "y": 2694
                },
                {
                    "x": 1228,
                    "y": 3003
                },
                {
                    "x": 217,
                    "y": 3003
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='139' style='font-size:18px'>For MLP-0 that has no hidden layers (i.e., the embedding<br>layer is directly projected to predictions), the performance is<br>very weak and is not better than the non-personalized Item-<br>Pop. This verifies our argument in Section 3.3 that simply<br>concatenating user and item latent vectors is insufficient for<br>modelling their feature interactions, and thus the necessity<br>of transforming it with hidden layers.</p>",
            "id": 139,
            "page": 8,
            "text": "For MLP-0 that has no hidden layers (i.e., the embedding layer is directly projected to predictions), the performance is very weak and is not better than the non-personalized ItemPop. This verifies our argument in Section 3.3 that simply concatenating user and item latent vectors is insufficient for modelling their feature interactions, and thus the necessity of transforming it with hidden layers."
        },
        {
            "bounding_box": [
                {
                    "x": 1328,
                    "y": 1554
                },
                {
                    "x": 2334,
                    "y": 1554
                },
                {
                    "x": 2334,
                    "y": 1654
                },
                {
                    "x": 1328,
                    "y": 1654
                }
            ],
            "category": "caption",
            "html": "<br><caption id='140' style='font-size:16px'>Table 3: HR@10 of MLP with different layers.<br>Factors MLP-0 MLP-1 MLP-2 MLP-3 MLP-4</caption>",
            "id": 140,
            "page": 8,
            "text": "Table 3: HR@10 of MLP with different layers. Factors MLP-0 MLP-1 MLP-2 MLP-3 MLP-4"
        },
        {
            "bounding_box": [
                {
                    "x": 1312,
                    "y": 1636
                },
                {
                    "x": 2356,
                    "y": 1636
                },
                {
                    "x": 2356,
                    "y": 2074
                },
                {
                    "x": 1312,
                    "y": 2074
                }
            ],
            "category": "table",
            "html": "<br><table id='141' style='font-size:16px'><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan=\"6\">MovieLens</td></tr><tr><td>8</td><td>0.452</td><td>0.628</td><td>0.655</td><td>0.671</td><td>0.678</td></tr><tr><td>16</td><td>0.454</td><td>0.663</td><td>0.674</td><td>0.684</td><td>0.690</td></tr><tr><td>32</td><td>0.453</td><td>0.682</td><td>0.687</td><td>0.692</td><td>0.699</td></tr><tr><td>64</td><td>0.453</td><td>0.687</td><td>0.696</td><td>0.702</td><td>0.707</td></tr><tr><td colspan=\"6\">Pinterest</td></tr><tr><td>8</td><td>0.275</td><td>0.848</td><td>0.855</td><td>0.859</td><td>0.862</td></tr><tr><td>16</td><td>0.274</td><td>0.855</td><td>0.861</td><td>0.865</td><td>0.867</td></tr><tr><td>32</td><td>0.273</td><td>0.861</td><td>0.863</td><td>0.868</td><td>0.867</td></tr><tr><td>64</td><td>0.274</td><td>0.864</td><td>0.867</td><td>0.869</td><td>0.873</td></tr></table>",
            "id": 141,
            "page": 8,
            "text": "MovieLens  8 0.452 0.628 0.655 0.671 0.678  16 0.454 0.663 0.674 0.684 0.690  32 0.453 0.682 0.687 0.692 0.699  64 0.453 0.687 0.696 0.702 0.707  Pinterest  8 0.275 0.848 0.855 0.859 0.862  16 0.274 0.855 0.861 0.865 0.867  32 0.273 0.861 0.863 0.868 0.867  64 0.274 0.864 0.867 0.869"
        },
        {
            "bounding_box": [
                {
                    "x": 1314,
                    "y": 2151
                },
                {
                    "x": 1819,
                    "y": 2151
                },
                {
                    "x": 1819,
                    "y": 2203
                },
                {
                    "x": 1314,
                    "y": 2203
                }
            ],
            "category": "paragraph",
            "html": "<p id='142' style='font-size:20px'>5. RELATED WORK</p>",
            "id": 142,
            "page": 8,
            "text": "5. RELATED WORK"
        },
        {
            "bounding_box": [
                {
                    "x": 1313,
                    "y": 2214
                },
                {
                    "x": 2326,
                    "y": 2214
                },
                {
                    "x": 2326,
                    "y": 3003
                },
                {
                    "x": 1313,
                    "y": 3003
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='143' style='font-size:20px'>While early literature on recommendation has largely fo-<br>cused on explicit feedback [30, 31], recent attention is in-<br>creasingly shifting towards implicit data [1, 14, 23]. The<br>collaborative filtering (CF) task with implicit feedback is<br>usually formulated as an item recommendation problem, for<br>which the aim is to recommend a short list of items to users.<br>In contrast to rating prediction that has been widely solved<br>by work on explicit feedback, addressing the item recommen-<br>dation problem is more practical but challenging [1, 11]. One<br>key insight is to model the missing data, which are always<br>ignored by the work on explicit feedback [21, 48]. To tailor<br>latent factor models for item recommendation with implicit<br>feedback, early work [19, 27] applies a uniform weighting<br>where two strategies have been proposed which either<br>treated all missing data as negative instances [19] or sam-<br>pled negative instances from missing data [27]. Recently, He<br>et al. [14] and Liang et al. [23] proposed dedicated models<br>to weight missing data, and Rendle et al. [1] developed an</p>",
            "id": 143,
            "page": 8,
            "text": "While early literature on recommendation has largely focused on explicit feedback , recent attention is increasingly shifting towards implicit data . The collaborative filtering (CF) task with implicit feedback is usually formulated as an item recommendation problem, for which the aim is to recommend a short list of items to users. In contrast to rating prediction that has been widely solved by work on explicit feedback, addressing the item recommendation problem is more practical but challenging . One key insight is to model the missing data, which are always ignored by the work on explicit feedback . To tailor latent factor models for item recommendation with implicit feedback, early work  applies a uniform weighting where two strategies have been proposed which either treated all missing data as negative instances  or sampled negative instances from missing data . Recently, He   and Liang   proposed dedicated models to weight missing data, and Rendle   developed an"
        },
        {
            "bounding_box": [
                {
                    "x": 219,
                    "y": 220
                },
                {
                    "x": 1254,
                    "y": 220
                },
                {
                    "x": 1254,
                    "y": 318
                },
                {
                    "x": 219,
                    "y": 318
                }
            ],
            "category": "caption",
            "html": "<caption id='144' style='font-size:14px'>Table 4: NDCG@10 of MLP with different layers.<br>Factors MLP-0 MLP-1 MLP-2 MLP-3 MLP-4</caption>",
            "id": 144,
            "page": 9,
            "text": "Table 4: NDCG@10 of MLP with different layers. Factors MLP-0 MLP-1 MLP-2 MLP-3 MLP-4"
        },
        {
            "bounding_box": [
                {
                    "x": 219,
                    "y": 319
                },
                {
                    "x": 1263,
                    "y": 319
                },
                {
                    "x": 1263,
                    "y": 737
                },
                {
                    "x": 219,
                    "y": 737
                }
            ],
            "category": "table",
            "html": "<br><table id='145' style='font-size:14px'><tr><td colspan=\"6\">MovieLens</td></tr><tr><td>8</td><td>0.253</td><td>0.359</td><td>0.383</td><td>0.399</td><td>0.406</td></tr><tr><td>16</td><td>0.252</td><td>0.391</td><td>0.402</td><td>0.410</td><td>0.415</td></tr><tr><td>32</td><td>0.252</td><td>0.406</td><td>0.410</td><td>0.425</td><td>0.423</td></tr><tr><td>64</td><td>0.251</td><td>0.409</td><td>0.417</td><td>0.426</td><td>0.432</td></tr><tr><td colspan=\"6\">Pinterest</td></tr><tr><td>8</td><td>0.141</td><td>0.526</td><td>0.534</td><td>0.536</td><td>0.539</td></tr><tr><td>16</td><td>0.141</td><td>0.532</td><td>0.536</td><td>0.538</td><td>0.544</td></tr><tr><td>32</td><td>0.142</td><td>0.537</td><td>0.538</td><td>0.542</td><td>0.546</td></tr><tr><td>64</td><td>0.141</td><td>0.538</td><td>0.542</td><td>0.545</td><td>0.550</td></tr></table>",
            "id": 145,
            "page": 9,
            "text": "MovieLens  8 0.253 0.359 0.383 0.399 0.406  16 0.252 0.391 0.402 0.410 0.415  32 0.252 0.406 0.410 0.425 0.423  64 0.251 0.409 0.417 0.426 0.432  Pinterest  8 0.141 0.526 0.534 0.536 0.539  16 0.141 0.532 0.536 0.538 0.544  32 0.142 0.537 0.538 0.542 0.546  64 0.141 0.538 0.542 0.545"
        },
        {
            "bounding_box": [
                {
                    "x": 216,
                    "y": 820
                },
                {
                    "x": 1228,
                    "y": 820
                },
                {
                    "x": 1228,
                    "y": 992
                },
                {
                    "x": 216,
                    "y": 992
                }
            ],
            "category": "paragraph",
            "html": "<p id='146' style='font-size:16px'>implicit coordinate descent (iCD) solution for feature-based<br>factorization models, achieving state-of-the-art performance<br>for item recommendation. In the following, we discuss rec-<br>ommendation works that use neural networks.</p>",
            "id": 146,
            "page": 9,
            "text": "implicit coordinate descent (iCD) solution for feature-based factorization models, achieving state-of-the-art performance for item recommendation. In the following, we discuss recommendation works that use neural networks."
        },
        {
            "bounding_box": [
                {
                    "x": 217,
                    "y": 995
                },
                {
                    "x": 1230,
                    "y": 995
                },
                {
                    "x": 1230,
                    "y": 1869
                },
                {
                    "x": 217,
                    "y": 1869
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='147' style='font-size:16px'>The early pioneer work by Salakhutdinov et al. [30] pro-<br>posed a two-layer Restricted Boltzmann Machines (RBMs)<br>to model users' explicit ratings on items. The work was been<br>later extended to model the ordinal nature of ratings [36].<br>Recently, autoencoders have become a popular choice for<br>building recommendation systems [32, 22, 35]. The idea of<br>user-based AutoRec [32] is to learn hidden structures that<br>can reconstruct a user's ratings given her historical ratings<br>as inputs. In terms of user personalization, this approach<br>shares a similar spirit as the item-item model [31, 25] that<br>represents a user as her rated items. To avoid autoencoders<br>learning an identity function and failing to generalize to un-<br>seen data, denoising autoencoders (DAEs) have been applied<br>to learn from intentionally corrupted inputs [22, 35]. More<br>recently, Zheng et al. [48] presented a neural autoregressive<br>method for CF. While the previous effort has lent support<br>to the effectiveness of neural networks for addressing CF,<br>most of them focused on explicit ratings and modelled the<br>observed data only. As a result, they can easily fail to learn<br>users' preference from the positive-only implicit data.</p>",
            "id": 147,
            "page": 9,
            "text": "The early pioneer work by Salakhutdinov   proposed a two-layer Restricted Boltzmann Machines (RBMs) to model users' explicit ratings on items. The work was been later extended to model the ordinal nature of ratings . Recently, autoencoders have become a popular choice for building recommendation systems . The idea of user-based AutoRec  is to learn hidden structures that can reconstruct a user's ratings given her historical ratings as inputs. In terms of user personalization, this approach shares a similar spirit as the item-item model  that represents a user as her rated items. To avoid autoencoders learning an identity function and failing to generalize to unseen data, denoising autoencoders (DAEs) have been applied to learn from intentionally corrupted inputs . More recently, Zheng   presented a neural autoregressive method for CF. While the previous effort has lent support to the effectiveness of neural networks for addressing CF, most of them focused on explicit ratings and modelled the observed data only. As a result, they can easily fail to learn users' preference from the positive-only implicit data."
        },
        {
            "bounding_box": [
                {
                    "x": 217,
                    "y": 1868
                },
                {
                    "x": 1242,
                    "y": 1868
                },
                {
                    "x": 1242,
                    "y": 2914
                },
                {
                    "x": 217,
                    "y": 2914
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='148' style='font-size:18px'>Although some recent works [6, 37, 38, 43, 45] have ex-<br>plored deep learning models for recommendation based on<br>implicit feedback, they primarily used DNNs for modelling<br>auxiliary information, such as textual description ofitems [38],<br>acoustic features of musics [37, 43], cross-domain behaviors<br>of users [6], and the rich information in knowledge bases [45].<br>The features learnt by DNNs are then integrated with MF<br>for CF. The work that is most relevant to our work is [44],<br>which presents a collaborative denoising autoencoder (CDAE)<br>for CF with implicit feedback. In contrast to the DAE-based<br>CF [35], CDAE additionally plugs a user node to the input<br>of autoencoders for reconstructing the user's ratings. As<br>shown by the authors, CDAE is equivalent to the SVD++<br>model [21] when the identity function is applied to acti-<br>vate the hidden layers of CDAE. This implies that although<br>CDAE is a neural modelling approach for CF, it still applies<br>a linear kernel (i.e., inner product) to model user-item inter-<br>actions. This may partially explain why using deep layers for<br>CDAE does not improve the performance (cf. Section 6 of<br>[44]). Distinct from CDAE, our NCF adopts a two-pathway<br>architecture, modelling user-item interactions with a multi-<br>layer feedforward neural network. This allows NCF to learn<br>an arbitrary function from the data, being more powerful<br>and expressive than the fixed inner product function.</p>",
            "id": 148,
            "page": 9,
            "text": "Although some recent works  have explored deep learning models for recommendation based on implicit feedback, they primarily used DNNs for modelling auxiliary information, such as textual description ofitems , acoustic features of musics , cross-domain behaviors of users , and the rich information in knowledge bases . The features learnt by DNNs are then integrated with MF for CF. The work that is most relevant to our work is , which presents a collaborative denoising autoencoder (CDAE) for CF with implicit feedback. In contrast to the DAE-based CF , CDAE additionally plugs a user node to the input of autoencoders for reconstructing the user's ratings. As shown by the authors, CDAE is equivalent to the SVD++ model  when the identity function is applied to activate the hidden layers of CDAE. This implies that although CDAE is a neural modelling approach for CF, it still applies a linear kernel (i.e., inner product) to model user-item interactions. This may partially explain why using deep layers for CDAE does not improve the performance (cf. Section 6 of ). Distinct from CDAE, our NCF adopts a two-pathway architecture, modelling user-item interactions with a multilayer feedforward neural network. This allows NCF to learn an arbitrary function from the data, being more powerful and expressive than the fixed inner product function."
        },
        {
            "bounding_box": [
                {
                    "x": 217,
                    "y": 2914
                },
                {
                    "x": 1227,
                    "y": 2914
                },
                {
                    "x": 1227,
                    "y": 3002
                },
                {
                    "x": 217,
                    "y": 3002
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='149' style='font-size:16px'>Along a similar line, learning the relations of two enti-<br>ties has been intensively studied in literature of knowledge</p>",
            "id": 149,
            "page": 9,
            "text": "Along a similar line, learning the relations of two entities has been intensively studied in literature of knowledge"
        },
        {
            "bounding_box": [
                {
                    "x": 1313,
                    "y": 232
                },
                {
                    "x": 2323,
                    "y": 232
                },
                {
                    "x": 2323,
                    "y": 625
                },
                {
                    "x": 1313,
                    "y": 625
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='150' style='font-size:16px'>graphs [2, 33]. Many relational machine learning methods<br>have been devised [24]. The one that is most similar to our<br>proposal is the Neural Tensor Network (NTN) [33], which<br>uses neural networks to learn the interaction of two entities<br>and shows strong performance. Here we focus on a differ-<br>ent problem setting of CF. While the idea of NeuMF that<br>combines MF with MLP is partially inspired by NTN, our<br>NeuMF is more flexible and generic than NTN, in terms of<br>allowing MF and MLP learning different sets of embeddings.</p>",
            "id": 150,
            "page": 9,
            "text": "graphs . Many relational machine learning methods have been devised . The one that is most similar to our proposal is the Neural Tensor Network (NTN) , which uses neural networks to learn the interaction of two entities and shows strong performance. Here we focus on a different problem setting of CF. While the idea of NeuMF that combines MF with MLP is partially inspired by NTN, our NeuMF is more flexible and generic than NTN, in terms of allowing MF and MLP learning different sets of embeddings."
        },
        {
            "bounding_box": [
                {
                    "x": 1313,
                    "y": 626
                },
                {
                    "x": 2324,
                    "y": 626
                },
                {
                    "x": 2324,
                    "y": 1020
                },
                {
                    "x": 1313,
                    "y": 1020
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='151' style='font-size:16px'>More recently, Google publicized their Wide & Deep learn-<br>ing approach for App recommendation [4]. The deep compo-<br>nent similarly uses a MLP on feature embeddings, which has<br>been reported to have strong generalization ability. While<br>their work has focused on incorporating various features<br>of users and items, we target at exploring DNNs for pure<br>collaborative filtering systems. We show that DNNs are a<br>promising choice for modelling user-item interactions, which<br>to our knowledge has not been investigated before.</p>",
            "id": 151,
            "page": 9,
            "text": "More recently, Google publicized their Wide & Deep learning approach for App recommendation . The deep component similarly uses a MLP on feature embeddings, which has been reported to have strong generalization ability. While their work has focused on incorporating various features of users and items, we target at exploring DNNs for pure collaborative filtering systems. We show that DNNs are a promising choice for modelling user-item interactions, which to our knowledge has not been investigated before."
        },
        {
            "bounding_box": [
                {
                    "x": 1314,
                    "y": 1070
                },
                {
                    "x": 2259,
                    "y": 1070
                },
                {
                    "x": 2259,
                    "y": 1120
                },
                {
                    "x": 1314,
                    "y": 1120
                }
            ],
            "category": "paragraph",
            "html": "<p id='152' style='font-size:18px'>6. CONCLUSION AND FUTURE WORK</p>",
            "id": 152,
            "page": 9,
            "text": "6. CONCLUSION AND FUTURE WORK"
        },
        {
            "bounding_box": [
                {
                    "x": 1313,
                    "y": 1133
                },
                {
                    "x": 2324,
                    "y": 1133
                },
                {
                    "x": 2324,
                    "y": 1612
                },
                {
                    "x": 1313,
                    "y": 1612
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='153' style='font-size:14px'>In this work, we explored neural network architectures<br>for collaborative filtering. We devised a general framework<br>NCF and proposed three instantiations GMF, MLP and<br>NeuMF that model user-item interactions in different<br>ways. Our framework is simple and generic; it is not limited<br>to the models presented in this paper, but is designed to<br>serve as a guideline for developing deep learning methods for<br>recommendation. This work complements the mainstream<br>shallow models for collaborative filtering, opening up a new<br>avenue of research possibilities for recommendation based<br>on deep learning.</p>",
            "id": 153,
            "page": 9,
            "text": "In this work, we explored neural network architectures for collaborative filtering. We devised a general framework NCF and proposed three instantiations GMF, MLP and NeuMF that model user-item interactions in different ways. Our framework is simple and generic; it is not limited to the models presented in this paper, but is designed to serve as a guideline for developing deep learning methods for recommendation. This work complements the mainstream shallow models for collaborative filtering, opening up a new avenue of research possibilities for recommendation based on deep learning."
        },
        {
            "bounding_box": [
                {
                    "x": 1313,
                    "y": 1612
                },
                {
                    "x": 2324,
                    "y": 1612
                },
                {
                    "x": 2324,
                    "y": 2356
                },
                {
                    "x": 1313,
                    "y": 2356
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='154' style='font-size:18px'>In future, we will study pairwise learners for NCF mod-<br>els and extend NCF to model auxiliary information, such<br>as user reviews [11], knowledge bases [45], and temporal sig-<br>nals [1]. While existing personalization models have primar-<br>ily focused on individuals, it is interesting to develop models<br>for groups of users, which help the decision-making for social<br>groups [15, 42]. Moreover, we are particularly interested in<br>building recommender systems for multi-media items, an in-<br>teresting task but has received relatively less scrutiny in the<br>recommendation community [3]. Multi-media items, such as<br>images and videos, contain much richer visual semantics [16,<br>41] that can reflect users' interest. To build a multi-media<br>recommender system, we need to develop effective methods<br>to learn from multi-view and multi-modal data [13, 40]. An-<br>other emerging direction is to explore the potential of recur-<br>rent neural networks and hashing methods [46] for providing<br>efficient online recommendation [14, 1].</p>",
            "id": 154,
            "page": 9,
            "text": "In future, we will study pairwise learners for NCF models and extend NCF to model auxiliary information, such as user reviews , knowledge bases , and temporal signals . While existing personalization models have primarily focused on individuals, it is interesting to develop models for groups of users, which help the decision-making for social groups . Moreover, we are particularly interested in building recommender systems for multi-media items, an interesting task but has received relatively less scrutiny in the recommendation community . Multi-media items, such as images and videos, contain much richer visual semantics  that can reflect users' interest. To build a multi-media recommender system, we need to develop effective methods to learn from multi-view and multi-modal data . Another emerging direction is to explore the potential of recurrent neural networks and hashing methods  for providing efficient online recommendation ."
        },
        {
            "bounding_box": [
                {
                    "x": 1318,
                    "y": 2370
                },
                {
                    "x": 1762,
                    "y": 2370
                },
                {
                    "x": 1762,
                    "y": 2420
                },
                {
                    "x": 1318,
                    "y": 2420
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='155' style='font-size:22px'>Acknowledgement</p>",
            "id": 155,
            "page": 9,
            "text": "Acknowledgement"
        },
        {
            "bounding_box": [
                {
                    "x": 1314,
                    "y": 2423
                },
                {
                    "x": 2324,
                    "y": 2423
                },
                {
                    "x": 2324,
                    "y": 2556
                },
                {
                    "x": 1314,
                    "y": 2556
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='156' style='font-size:14px'>The authors thank the anonymous reviewers for their valu-<br>able comments, which are beneficial to the authors' thoughts<br>on recommendation systems and the revision of the paper.</p>",
            "id": 156,
            "page": 9,
            "text": "The authors thank the anonymous reviewers for their valuable comments, which are beneficial to the authors' thoughts on recommendation systems and the revision of the paper."
        },
        {
            "bounding_box": [
                {
                    "x": 1316,
                    "y": 2605
                },
                {
                    "x": 1746,
                    "y": 2605
                },
                {
                    "x": 1746,
                    "y": 2653
                },
                {
                    "x": 1316,
                    "y": 2653
                }
            ],
            "category": "paragraph",
            "html": "<p id='157' style='font-size:20px'>7. REFERENCES</p>",
            "id": 157,
            "page": 9,
            "text": "7. REFERENCES"
        },
        {
            "bounding_box": [
                {
                    "x": 1333,
                    "y": 2652
                },
                {
                    "x": 2272,
                    "y": 2652
                },
                {
                    "x": 2272,
                    "y": 2999
                },
                {
                    "x": 1333,
                    "y": 2999
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='158' style='font-size:14px'>[1] I. Bayer, X. He, B. Kanagal, and S. Rendle. A generic<br>coordinate descent framework for learning from implicit<br>feedback. In www, 2017.<br>[2] A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston, and<br>0. Yakhnenko. Translating embeddings for modeling<br>multi-relational data. In NIPS, pages 2787-2795, 2013.<br>[3] T. Chen, X. He, and M.-Y. Kan. Context-aware image<br>tweet modelling and recommendation. In MM, pages<br>1018-1027, 2016.</p>",
            "id": 158,
            "page": 9,
            "text": " I. Bayer, X. He, B. Kanagal, and S. Rendle. A generic coordinate descent framework for learning from implicit feedback. In www, 2017.  A. Bordes, N. Usunier, A. Garcia-Duran, J. Weston, and 0. Yakhnenko. Translating embeddings for modeling multi-relational data. In NIPS, pages 2787-2795, 2013.  T. Chen, X. He, and M.-Y. Kan. Context-aware image tweet modelling and recommendation. In MM, pages 1018-1027, 2016."
        },
        {
            "bounding_box": [
                {
                    "x": 221,
                    "y": 209
                },
                {
                    "x": 1232,
                    "y": 209
                },
                {
                    "x": 1232,
                    "y": 2996
                },
                {
                    "x": 221,
                    "y": 2996
                }
            ],
            "category": "paragraph",
            "html": "<p id='159' style='font-size:18px'>[4] H.-T. Cheng, L. Koc, J. Harmsen, T. Shaked, T. Chandra,<br>H. Aradhye, G. Anderson, G. Corrado, W. Chai, M. Ispir,<br>et al. Wide & deep learning for recommender systems.<br>arXiv preprint arXiv:1606.07792, 2016.<br>[5] R. Collobert and J. Weston. A unified architecture for<br>natural language processing: Deep neural networks with<br>multitask learning. In ICML, pages 160-167, 2008.<br>[6] A. M. Elkahky, Y. Song, and X. He. A multi-view deep<br>learning approach for cross domain user modeling in<br>recommendation systems. In www, pages 278-288, 2015.<br>[7] D. Erhan, Y. Bengio, A. Courville, P.-A. Manzagol,<br>P. Vincent, and S. Bengio. Why does unsupervised<br>pre-training help deep learning? Journal of Machine<br>Learning Research, 11:625-660, 2010.<br>[8] X. Geng, H. Zhang, J. Bian, and T.-S. Chua. Learning<br>image and user features for recommendation in social<br>networks. In ICCV, pages 4274-4282, 2015.<br>[9] X. Glorot, A. Bordes, and Y. Bengio. Deep sparse rectifier<br>neural networks. In AISTATS, pages 315-323, 2011.<br>[10] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual<br>learning for image recognition. In CVPR, 2016.<br>[11] X. He, T. Chen, M.-Y. Kan, and X. Chen. TriRank:<br>Review-aware explainable recommendation by modeling<br>aspects. In CIKM, pages 1661-1670, 2015.<br>[12] X. He, M. Gao, M.-Y. Kan, Y. Liu, and K. Sugiyama.<br>Predicting the popularity of web 2.0 items based on user<br>comments. In SIGIR, pages 233-242, 2014.<br>[13] X. He, M.-Y. Kan, P. Xie, and X. Chen. Comment-based<br>multi-view clustering of web 2.0 items. In www, pages<br>771-782, 2014.<br>[14] X. He, H. Zhang, M.-Y. Kan, and T.-S. Chua. Fast matrix<br>factorization for online recommendation with implicit<br>feedback. In SIGIR, pages 549-558, 2016.<br>[15] R. Hong, Z. Hu, L. Liu, M. Wang, S. Yan, and Q. Tian.<br>Understanding blooming human groups in social networks.<br>IEEE Transactions on Multimedia, 17(11):1980-1988, 2015.<br>[16] R. Hong, Y. Yang, M. Wang, and X. S. Hua. Learning<br>visual semantic relationships for efficient visual retrieval.<br>IEEE Transactions on Big Data, 1(4):152-161, 2015.<br>[17] K. Hornik, M. Stinchcombe, and H. White. Multilayer<br>feedforward networks are universal approximators. Neural<br>Networks, 2(5):359-366, 1989.<br>[18] L. Hu, A. Sun, and Y. Liu. Your neighbors affect your<br>ratings: On geographical neighborhood influence to rating<br>prediction. In SIGIR, pages 345-354, 2014.<br>[19] Y. Hu, Y. Koren, and C. Volinsky. Collaborative filtering<br>for implicit feedback datasets. In ICDM, pages 263-272,<br>2008.<br>[20] D. Kingma and J. Ba. Adam: A method for stochastic<br>optimization. In ICLR, pages 1-15, 2014.<br>[21] Y. Koren. Factorization meets the neighborhood: A<br>multifaceted collaborative filtering model. In KDD, pages<br>426-434, 2008.<br>[22] S. Li, J. Kawale, and Y. Fu. Deep collaborative filtering via<br>marginalized denoising auto-encoder. In CIKM, pages<br>811-820, 2015.<br>[23] D. Liang, L. Charlin, J. McInerney, and D. M. Blei.<br>Modeling user exposure in recommendation. In www,<br>pages 951-961, 2016.<br>[24] M. Nickel, K. Murphy, V. Tresp, and E. Gabrilovich. A<br>review of relational machine learning for knowledge graphs.<br>Proceedings of the IEEE, 104:11-33, 2016.<br>[25] X. Ning and G. Karypis. Slim: Sparse linear methods for<br>top-n recommender systems. In ICDM, pages 497-506,<br>2011.<br>[26] S. Rendle. Factorization machines. In ICDM, pages<br>995-1000, 2010.<br>[27] S. Rendle, C. Freudenthaler, Z. Gantner, and<br>L. Schmidt-Thieme. Bpr: Bayesian personalized ranking<br>from implicit feedback. In UAI, pages 452-461, 2009.<br>[28] S. Rendle, Z. Gantner, C. Freudenthaler, and</p>",
            "id": 159,
            "page": 10,
            "text": " H.-T. Cheng, L. Koc, J. Harmsen, T. Shaked, T. Chandra, H. Aradhye, G. Anderson, G. Corrado, W. Chai, M. Ispir,  Wide & deep learning for recommender systems. arXiv preprint arXiv:1606.07792, 2016.  R. Collobert and J. Weston. A unified architecture for natural language processing: Deep neural networks with multitask learning. In ICML, pages 160-167, 2008.  A. M. Elkahky, Y. Song, and X. He. A multi-view deep learning approach for cross domain user modeling in recommendation systems. In www, pages 278-288, 2015.  D. Erhan, Y. Bengio, A. Courville, P.-A. Manzagol, P. Vincent, and S. Bengio. Why does unsupervised pre-training help deep learning? Journal of Machine Learning Research, 11:625-660, 2010.  X. Geng, H. Zhang, J. Bian, and T.-S. Chua. Learning image and user features for recommendation in social networks. In ICCV, pages 4274-4282, 2015.  X. Glorot, A. Bordes, and Y. Bengio. Deep sparse rectifier neural networks. In AISTATS, pages 315-323, 2011.  K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016.  X. He, T. Chen, M.-Y. Kan, and X. Chen. TriRank: Review-aware explainable recommendation by modeling aspects. In CIKM, pages 1661-1670, 2015.  X. He, M. Gao, M.-Y. Kan, Y. Liu, and K. Sugiyama. Predicting the popularity of web 2.0 items based on user comments. In SIGIR, pages 233-242, 2014.  X. He, M.-Y. Kan, P. Xie, and X. Chen. Comment-based multi-view clustering of web 2.0 items. In www, pages 771-782, 2014.  X. He, H. Zhang, M.-Y. Kan, and T.-S. Chua. Fast matrix factorization for online recommendation with implicit feedback. In SIGIR, pages 549-558, 2016.  R. Hong, Z. Hu, L. Liu, M. Wang, S. Yan, and Q. Tian. Understanding blooming human groups in social networks. IEEE Transactions on Multimedia, 17(11):1980-1988, 2015.  R. Hong, Y. Yang, M. Wang, and X. S. Hua. Learning visual semantic relationships for efficient visual retrieval. IEEE Transactions on Big Data, 1(4):152-161, 2015.  K. Hornik, M. Stinchcombe, and H. White. Multilayer feedforward networks are universal approximators. Neural Networks, 2(5):359-366, 1989.  L. Hu, A. Sun, and Y. Liu. Your neighbors affect your ratings: On geographical neighborhood influence to rating prediction. In SIGIR, pages 345-354, 2014.  Y. Hu, Y. Koren, and C. Volinsky. Collaborative filtering for implicit feedback datasets. In ICDM, pages 263-272, 2008.  D. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR, pages 1-15, 2014.  Y. Koren. Factorization meets the neighborhood: A multifaceted collaborative filtering model. In KDD, pages 426-434, 2008.  S. Li, J. Kawale, and Y. Fu. Deep collaborative filtering via marginalized denoising auto-encoder. In CIKM, pages 811-820, 2015.  D. Liang, L. Charlin, J. McInerney, and D. M. Blei. Modeling user exposure in recommendation. In www, pages 951-961, 2016.  M. Nickel, K. Murphy, V. Tresp, and E. Gabrilovich. A review of relational machine learning for knowledge graphs. Proceedings of the IEEE, 104:11-33, 2016.  X. Ning and G. Karypis. Slim: Sparse linear methods for top-n recommender systems. In ICDM, pages 497-506, 2011.  S. Rendle. Factorization machines. In ICDM, pages 995-1000, 2010.  S. Rendle, C. Freudenthaler, Z. Gantner, and L. Schmidt-Thieme. Bpr: Bayesian personalized ranking from implicit feedback. In UAI, pages 452-461, 2009.  S. Rendle, Z. Gantner, C. Freudenthaler, and"
        },
        {
            "bounding_box": [
                {
                    "x": 1388,
                    "y": 235
                },
                {
                    "x": 2292,
                    "y": 235
                },
                {
                    "x": 2292,
                    "y": 347
                },
                {
                    "x": 1388,
                    "y": 347
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='160' style='font-size:14px'>L. Schmidt-Thieme. Fast context-aware recommendations<br>with factorization machines. In SIGIR, pages 635-644,<br>2011.</p>",
            "id": 160,
            "page": 10,
            "text": "L. Schmidt-Thieme. Fast context-aware recommendations with factorization machines. In SIGIR, pages 635-644, 2011."
        },
        {
            "bounding_box": [
                {
                    "x": 1313,
                    "y": 269
                },
                {
                    "x": 2325,
                    "y": 269
                },
                {
                    "x": 2325,
                    "y": 2753
                },
                {
                    "x": 1313,
                    "y": 2753
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='161' style='font-size:18px'>[29] R. Salakhutdinov and A. Mnih. Probabilistic matrix<br>factorization. In NIPS, pages 1-8, 2008.<br>[30] R. Salakhutdinov, A. Mnih, and G. Hinton. Restricted<br>boltzmann machines for collaborative filtering. In ICDM,<br>pages 791-798, 2007.<br>[31] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl.<br>Item-based collaborative filtering recommendation<br>algorithms. In www, pages 285-295, 2001.<br>[32] S. Sedhain, A. K. Menon, S. Sanner, and L. Xie. Autorec:<br>Autoencoders meet collaborative filtering. In www, pages<br>111-112, 2015.<br>[33] R. Socher, D. Chen, C. D. Manning, and A. Ng. Reasoning<br>with neural tensor networks for knowledge base completion.<br>In NIPS, pages 926-934, 2013.<br>[34] N. Srivastava and R. R. Salakhutdinov. Multimodal<br>learning with deep boltzmann machines. In NIPS, pages<br>2222-2230, 2012.<br>[35] F. Strub and J. Mary. Collaborative filtering with stacked<br>denoising autoencoders and sparse inputs. In NIPS<br>Workshop on Machine Learning for eCommerce, 2015.<br>[36] T. T. Truyen, D. Q. Phung, and S. Venkatesh. Ordinal<br>boltzmann machines for collaborative filtering. In UAI,<br>pages 548-556, 2009.<br>[37] A. Van den Oord, S. Dieleman, and B. Schrauwen. Deep<br>content-based music recommendation. In NIPS, pages<br>2643-2651, 2013.<br>[38] H. Wang, N. Wang, and D.-Y. Yeung. Collaborative deep<br>learning for recommender systems. In KDD, pages<br>1235-1244, 2015.<br>[39] M. Wang, W. Fu, S. Hao, D. Tao, and X. Wu. Scalable<br>semi-supervised learning by efficient anchor graph<br>regularization. IEEE Transactions on Knowledge and Data<br>Engineering, 28(7):1864-1877, 2016.<br>[40] M. Wang, H. Li, D. Tao, K. Lu, and X. Wu. Multimodal<br>graph-based reranking for web image search. IEEE<br>Transactions on Image Processing, 21(11):4649-4661, 2012.<br>[41] M. Wang, X. Liu, and X. Wu. Visual classification by 11<br>hypergraph modeling. IEEE Transactions on Knowledge<br>and Data Engineering, 27(9):2564-2574, 2015.<br>[42] X. Wang, L. Nie, X. Song, D. Zhang, and T.-S. Chua.<br>Unifying virtual and physical worlds: Learning towards<br>local and global consistency. ACM Transactions on<br>Information Systems, 2017.<br>[43] X. Wang and Y. Wang. Improving content-based and<br>hybrid music recommendation using deep learning. In MM,<br>pages 627-636, 2014.<br>[44] Y. Wu, C. DuBois, A. X. Zheng, and M. Ester.<br>Collaborative denoising auto-encoders for top-n<br>recommender systems. In WSDM, pages 153-162, 2016.<br>[45] F. Zhang, N. J. Yuan, D. Lian, X. Xie, and W.-Y. Ma.<br>Collaborative knowledge base embedding for recommender<br>systems. In KDD, pages 353-362, 2016.<br>[46] H. Zhang, F. Shen, W. Liu, X. He, H. Luan, and T.-S.<br>Chua. Discrete collaborative filtering. In SIGIR, pages<br>325-334, 2016.<br>[47] H. Zhang, Y. Yang, H. Luan, S. Yang, and T.-S. Chua.<br>Start from scratch: Towards automatically identifying,<br>modeling, and naming visual attributes. In MM, pages<br>187-196, 2014.<br>[48] Y. Zheng, B. Tang, W. Ding, and H. Zhou. A neural<br>autoregressive approach to collaborative filtering. In ICML,<br>pages 764-773, 2016.</p>",
            "id": 161,
            "page": 10,
            "text": " R. Salakhutdinov and A. Mnih. Probabilistic matrix factorization. In NIPS, pages 1-8, 2008.  R. Salakhutdinov, A. Mnih, and G. Hinton. Restricted boltzmann machines for collaborative filtering. In ICDM, pages 791-798, 2007.  B. Sarwar, G. Karypis, J. Konstan, and J. Riedl. Item-based collaborative filtering recommendation algorithms. In www, pages 285-295, 2001.  S. Sedhain, A. K. Menon, S. Sanner, and L. Xie. Autorec: Autoencoders meet collaborative filtering. In www, pages 111-112, 2015.  R. Socher, D. Chen, C. D. Manning, and A. Ng. Reasoning with neural tensor networks for knowledge base completion. In NIPS, pages 926-934, 2013.  N. Srivastava and R. R. Salakhutdinov. Multimodal learning with deep boltzmann machines. In NIPS, pages 2222-2230, 2012.  F. Strub and J. Mary. Collaborative filtering with stacked denoising autoencoders and sparse inputs. In NIPS Workshop on Machine Learning for eCommerce, 2015.  T. T. Truyen, D. Q. Phung, and S. Venkatesh. Ordinal boltzmann machines for collaborative filtering. In UAI, pages 548-556, 2009.  A. Van den Oord, S. Dieleman, and B. Schrauwen. Deep content-based music recommendation. In NIPS, pages 2643-2651, 2013.  H. Wang, N. Wang, and D.-Y. Yeung. Collaborative deep learning for recommender systems. In KDD, pages 1235-1244, 2015.  M. Wang, W. Fu, S. Hao, D. Tao, and X. Wu. Scalable semi-supervised learning by efficient anchor graph regularization. IEEE Transactions on Knowledge and Data Engineering, 28(7):1864-1877, 2016.  M. Wang, H. Li, D. Tao, K. Lu, and X. Wu. Multimodal graph-based reranking for web image search. IEEE Transactions on Image Processing, 21(11):4649-4661, 2012.  M. Wang, X. Liu, and X. Wu. Visual classification by 11 hypergraph modeling. IEEE Transactions on Knowledge and Data Engineering, 27(9):2564-2574, 2015.  X. Wang, L. Nie, X. Song, D. Zhang, and T.-S. Chua. Unifying virtual and physical worlds: Learning towards local and global consistency. ACM Transactions on Information Systems, 2017.  X. Wang and Y. Wang. Improving content-based and hybrid music recommendation using deep learning. In MM, pages 627-636, 2014.  Y. Wu, C. DuBois, A. X. Zheng, and M. Ester. Collaborative denoising auto-encoders for top-n recommender systems. In WSDM, pages 153-162, 2016.  F. Zhang, N. J. Yuan, D. Lian, X. Xie, and W.-Y. Ma. Collaborative knowledge base embedding for recommender systems. In KDD, pages 353-362, 2016.  H. Zhang, F. Shen, W. Liu, X. He, H. Luan, and T.-S. Chua. Discrete collaborative filtering. In SIGIR, pages 325-334, 2016.  H. Zhang, Y. Yang, H. Luan, S. Yang, and T.-S. Chua. Start from scratch: Towards automatically identifying, modeling, and naming visual attributes. In MM, pages 187-196, 2014.  Y. Zheng, B. Tang, W. Ding, and H. Zhou. A neural autoregressive approach to collaborative filtering. In ICML, pages 764-773, 2016."
        }
    ]
}