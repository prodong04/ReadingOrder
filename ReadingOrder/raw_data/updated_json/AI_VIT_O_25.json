{
    "id": "629e1a8e-0f92-11ef-8230-426932df3dcf",
    "pdf_path": "/root/data/pdf/2005.12872v3.pdf",
    "elements": [
        {
            "bounding_box": [
                {
                    "x": 141,
                    "y": 133
                },
                {
                    "x": 1585,
                    "y": 133
                },
                {
                    "x": 1585,
                    "y": 197
                },
                {
                    "x": 141,
                    "y": 197
                }
            ],
            "category": "paragraph",
            "html": "<p id='0' style='font-size:22px'>End-to-End Object Detection with Transformers</p>",
            "id": 0,
            "page": 1,
            "text": "End-to-End Object Detection with Transformers"
        },
        {
            "bounding_box": [
                {
                    "x": 214,
                    "y": 293
                },
                {
                    "x": 1510,
                    "y": 293
                },
                {
                    "x": 1510,
                    "y": 391
                },
                {
                    "x": 214,
                    "y": 391
                }
            ],
            "category": "paragraph",
            "html": "<p id='1' style='font-size:20px'>Nicolas Carion*, Francisco Massa* , Gabriel Synnaeve, Nicolas Usunier,<br>Alexander Kirillov, and Sergey Zagoruyko</p>",
            "id": 1,
            "page": 1,
            "text": "Nicolas Carion*, Francisco Massa* , Gabriel Synnaeve, Nicolas Usunier,\nAlexander Kirillov, and Sergey Zagoruyko"
        },
        {
            "bounding_box": [
                {
                    "x": 751,
                    "y": 435
                },
                {
                    "x": 971,
                    "y": 435
                },
                {
                    "x": 971,
                    "y": 474
                },
                {
                    "x": 751,
                    "y": 474
                }
            ],
            "category": "paragraph",
            "html": "<p id='2' style='font-size:16px'>Facebook AI</p>",
            "id": 2,
            "page": 1,
            "text": "Facebook AI"
        },
        {
            "bounding_box": [
                {
                    "x": 252,
                    "y": 613
                },
                {
                    "x": 1470,
                    "y": 613
                },
                {
                    "x": 1470,
                    "y": 1493
                },
                {
                    "x": 252,
                    "y": 1493
                }
            ],
            "category": "paragraph",
            "html": "<p id='3' style='font-size:16px'>Abstract. We present a new method that views object detection as a<br>direct set prediction problem. Our approach streamlines the detection<br>pipeline, effectively removing the need for many hand-designed compo-<br>nents like a non-maximum suppression procedure or anchor generation<br>that explicitly encode our prior knowledge about the task. The main<br>ingredients of the new framework, called DEtection TRansformer or<br>DETR, are a set-based global loss that forces unique predictions via bi-<br>partite matching, and a transformer encoder-decoder architecture. Given<br>a fixed small set of learned object queries, DETR reasons about the re-<br>lations of the objects and the global image context to directly output<br>the final set of predictions in parallel. The new model is conceptually<br>simple and does not require a specialized library, unlike many other<br>modern detectors. DETR demonstrates accuracy and run-time perfor-<br>mance on par with the well-established and highly-optimized Faster R-<br>CNN baseline on the challenging COCO object detection dataset. More-<br>over, DETR can be easily generalized to produce panoptic segmentation<br>in a unified manner. We show that it significantly outperforms com-<br>petitive baselines. Training code and pretrained models are available at<br>https: / /github. com/facebookresearch/ detr.</p>",
            "id": 3,
            "page": 1,
            "text": "Abstract. We present a new method that views object detection as a\ndirect set prediction problem. Our approach streamlines the detection\npipeline, effectively removing the need for many hand-designed compo-\nnents like a non-maximum suppression procedure or anchor generation\nthat explicitly encode our prior knowledge about the task. The main\ningredients of the new framework, called DEtection TRansformer or\nDETR, are a set-based global loss that forces unique predictions via bi-\npartite matching, and a transformer encoder-decoder architecture. Given\na fixed small set of learned object queries, DETR reasons about the re-\nlations of the objects and the global image context to directly output\nthe final set of predictions in parallel. The new model is conceptually\nsimple and does not require a specialized library, unlike many other\nmodern detectors. DETR demonstrates accuracy and run-time perfor-\nmance on par with the well-established and highly-optimized Faster R-\nCNN baseline on the challenging COCO object detection dataset. More-\nover, DETR can be easily generalized to produce panoptic segmentation\nin a unified manner. We show that it significantly outperforms com-\npetitive baselines. Training code and pretrained models are available at\nhttps: / /github. com/facebookresearch/ detr."
        },
        {
            "bounding_box": [
                {
                    "x": 57,
                    "y": 145
                },
                {
                    "x": 163,
                    "y": 145
                },
                {
                    "x": 163,
                    "y": 1586
                },
                {
                    "x": 57,
                    "y": 1586
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='4' style='font-size:14px'>2020<br>May<br>28<br>[cs.CV]<br>arXiv:2005.12872v3</p>",
            "id": 4,
            "page": 1,
            "text": "2020\nMay\n28\n[cs.CV]\narXiv:2005.12872v3"
        },
        {
            "bounding_box": [
                {
                    "x": 132,
                    "y": 1586
                },
                {
                    "x": 538,
                    "y": 1586
                },
                {
                    "x": 538,
                    "y": 1636
                },
                {
                    "x": 132,
                    "y": 1636
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='5' style='font-size:18px'>1 Introduction</p>",
            "id": 5,
            "page": 1,
            "text": "1 Introduction"
        },
        {
            "bounding_box": [
                {
                    "x": 137,
                    "y": 1696
                },
                {
                    "x": 1591,
                    "y": 1696
                },
                {
                    "x": 1591,
                    "y": 2349
                },
                {
                    "x": 137,
                    "y": 2349
                }
            ],
            "category": "paragraph",
            "html": "<p id='6' style='font-size:18px'>The goal of object detection is to predict a set of bounding boxes and category<br>labels for each object of interest. Modern detectors address this set prediction<br>task in an indirect way, by defining surrogate regression and classification prob-<br>lems on a large set of proposals [37,5], anchors [23], or window centers [53,46].<br>Their performances are significantly influenced by postprocessing steps to col-<br>lapse near-duplicate predictions, by the design of the anchor sets and by the<br>heuristics that assign target boxes to anchors [52]. To simplify these pipelines,<br>we propose a direct set prediction approach to bypass the surrogate tasks. This<br>end-to-end philosophy has led to significant advances in complex structured pre-<br>diction tasks such as machine translation or speech recognition, but not yet in<br>object detection: previous attempts 43,16,4,39 either add other forms of prior<br>knowledge, or have not proven to be competitive with strong baselines on chal-<br>lenging benchmarks. This paper aims to bridge this gap.</p>",
            "id": 6,
            "page": 1,
            "text": "The goal of object detection is to predict a set of bounding boxes and category\nlabels for each object of interest. Modern detectors address this set prediction\ntask in an indirect way, by defining surrogate regression and classification prob-\nlems on a large set of proposals [37,5], anchors [23], or window centers [53,46].\nTheir performances are significantly influenced by postprocessing steps to col-\nlapse near-duplicate predictions, by the design of the anchor sets and by the\nheuristics that assign target boxes to anchors [52]. To simplify these pipelines,\nwe propose a direct set prediction approach to bypass the surrogate tasks. This\nend-to-end philosophy has led to significant advances in complex structured pre-\ndiction tasks such as machine translation or speech recognition, but not yet in\nobject detection: previous attempts 43,16,4,39 either add other forms of prior\nknowledge, or have not proven to be competitive with strong baselines on chal-\nlenging benchmarks. This paper aims to bridge this gap."
        },
        {
            "bounding_box": [
                {
                    "x": 149,
                    "y": 2388
                },
                {
                    "x": 503,
                    "y": 2388
                },
                {
                    "x": 503,
                    "y": 2430
                },
                {
                    "x": 149,
                    "y": 2430
                }
            ],
            "category": "paragraph",
            "html": "<p id='7' style='font-size:14px'>* Equal contribution</p>",
            "id": 7,
            "page": 1,
            "text": "* Equal contribution"
        },
        {
            "bounding_box": [
                {
                    "x": 138,
                    "y": 46
                },
                {
                    "x": 164,
                    "y": 46
                },
                {
                    "x": 164,
                    "y": 80
                },
                {
                    "x": 138,
                    "y": 80
                }
            ],
            "category": "header",
            "html": "<header id='8' style='font-size:14px'>2</header>",
            "id": 8,
            "page": 2,
            "text": "2"
        },
        {
            "bounding_box": [
                {
                    "x": 276,
                    "y": 44
                },
                {
                    "x": 491,
                    "y": 44
                },
                {
                    "x": 491,
                    "y": 81
                },
                {
                    "x": 276,
                    "y": 81
                }
            ],
            "category": "header",
            "html": "<br><header id='9' style='font-size:16px'>Carion et al.</header>",
            "id": 9,
            "page": 2,
            "text": "Carion et al."
        },
        {
            "bounding_box": [
                {
                    "x": 129,
                    "y": 155
                },
                {
                    "x": 1583,
                    "y": 155
                },
                {
                    "x": 1583,
                    "y": 405
                },
                {
                    "x": 129,
                    "y": 405
                }
            ],
            "category": "figure",
            "html": "<figure><img id='10' style='font-size:14px' alt=\"no object (⌀) no object (⌀)\ntransformer\nCNN encoder-\ndecoder\nset of image features set of box predictions bipartite matching loss\" data-coord=\"top-left:(129,155); bottom-right:(1583,405)\" /></figure>",
            "id": 10,
            "page": 2,
            "text": "no object (⌀) no object (⌀)\ntransformer\nCNN encoder-\ndecoder\nset of image features set of box predictions bipartite matching loss"
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 449
                },
                {
                    "x": 1591,
                    "y": 449
                },
                {
                    "x": 1591,
                    "y": 635
                },
                {
                    "x": 134,
                    "y": 635
                }
            ],
            "category": "caption",
            "html": "<caption id='11' style='font-size:18px'>Fig. 1: DETR directly predicts (in parallel) the final set of detections by combining<br>a common CNN with a transformer architecture. During training, bipartite matching<br>uniquely assigns predictions with ground truth boxes. Prediction with no match should<br>yield a \"no object\" (⌀) class prediction.</caption>",
            "id": 11,
            "page": 2,
            "text": "Fig. 1: DETR directly predicts (in parallel) the final set of detections by combining\na common CNN with a transformer architecture. During training, bipartite matching\nuniquely assigns predictions with ground truth boxes. Prediction with no match should\nyield a \"no object\" (⌀) class prediction."
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 741
                },
                {
                    "x": 1590,
                    "y": 741
                },
                {
                    "x": 1590,
                    "y": 1039
                },
                {
                    "x": 134,
                    "y": 1039
                }
            ],
            "category": "paragraph",
            "html": "<p id='12' style='font-size:20px'>We streamline the training pipeline by viewing object detection as a direct set<br>prediction problem. We adopt an encoder-decoder architecture based on trans-<br>formers [47], a popular architecture for sequence prediction. The self-attention<br>mechanisms of transformers, which explicitly model all pairwise interactions be-<br>tween elements in a sequence, make these architectures particularly suitable for<br>specific constraints of set prediction such as removing duplicate predictions.</p>",
            "id": 12,
            "page": 2,
            "text": "We streamline the training pipeline by viewing object detection as a direct set\nprediction problem. We adopt an encoder-decoder architecture based on trans-\nformers [47], a popular architecture for sequence prediction. The self-attention\nmechanisms of transformers, which explicitly model all pairwise interactions be-\ntween elements in a sequence, make these architectures particularly suitable for\nspecific constraints of set prediction such as removing duplicate predictions."
        },
        {
            "bounding_box": [
                {
                    "x": 133,
                    "y": 1045
                },
                {
                    "x": 1589,
                    "y": 1045
                },
                {
                    "x": 1589,
                    "y": 1440
                },
                {
                    "x": 133,
                    "y": 1440
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='13' style='font-size:20px'>Our DEtection TRansformer (DETR, see Figure 1) predicts all objects at<br>once, and is trained end-to-end with a set loss function which performs bipar-<br>tite matching between predicted and ground-truth objects. DETR simplifies the<br>detection pipeline by dropping multiple hand-designed components that encode<br>prior knowledge, like spatial anchors or non-maximal suppression. Unlike most<br>existing detection methods, DETR doesn't require any customized layers, and<br>thus can be reproduced easily in any framework that contains standard CNN<br>and transformer classes. 1<br>·</p>",
            "id": 13,
            "page": 2,
            "text": "Our DEtection TRansformer (DETR, see Figure 1) predicts all objects at\nonce, and is trained end-to-end with a set loss function which performs bipar-\ntite matching between predicted and ground-truth objects. DETR simplifies the\ndetection pipeline by dropping multiple hand-designed components that encode\nprior knowledge, like spatial anchors or non-maximal suppression. Unlike most\nexisting detection methods, DETR doesn't require any customized layers, and\nthus can be reproduced easily in any framework that contains standard CNN\nand transformer classes. 1\n·"
        },
        {
            "bounding_box": [
                {
                    "x": 135,
                    "y": 1447
                },
                {
                    "x": 1589,
                    "y": 1447
                },
                {
                    "x": 1589,
                    "y": 1742
                },
                {
                    "x": 135,
                    "y": 1742
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='14' style='font-size:20px'>Compared to most previous work on direct set prediction, the main features of<br>DETR are the conjunction of the bipartite matching loss and transformers with<br>(non-autoregressive) parallel decoding [29,12,10,8]. In contrast, previous work<br>focused on autoregressive decoding with RNNs 43,41,30,36,42 Our matching<br>loss function uniquely assigns a prediction to a ground truth object, and is<br>invariant to a permutation of predicted objects, SO we can emit them in parallel.</p>",
            "id": 14,
            "page": 2,
            "text": "Compared to most previous work on direct set prediction, the main features of\nDETR are the conjunction of the bipartite matching loss and transformers with\n(non-autoregressive) parallel decoding [29,12,10,8]. In contrast, previous work\nfocused on autoregressive decoding with RNNs 43,41,30,36,42 Our matching\nloss function uniquely assigns a prediction to a ground truth object, and is\ninvariant to a permutation of predicted objects, SO we can emit them in parallel."
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 1749
                },
                {
                    "x": 1589,
                    "y": 1749
                },
                {
                    "x": 1589,
                    "y": 2194
                },
                {
                    "x": 134,
                    "y": 2194
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='15' style='font-size:20px'>We evaluate DETR on one of the most popular object detection datasets,<br>COCO [24], against a very competitive Faster R-CNN baseline [37]. Faster R-<br>CNN has undergone many design iterations and its performance was greatly<br>improved since the original publication. Our experiments show that our new<br>model achieves comparable performances. More precisely, DETR demonstrates<br>significantly better performance on large objects, a result likely enabled by the<br>non-local computations of the transformer. It obtains, however, lower perfor-<br>mances on small objects. We expect that future work will improve this aspect<br>in the same way the development of FPN [22] did for Faster R-CNN.</p>",
            "id": 15,
            "page": 2,
            "text": "We evaluate DETR on one of the most popular object detection datasets,\nCOCO [24], against a very competitive Faster R-CNN baseline [37]. Faster R-\nCNN has undergone many design iterations and its performance was greatly\nimproved since the original publication. Our experiments show that our new\nmodel achieves comparable performances. More precisely, DETR demonstrates\nsignificantly better performance on large objects, a result likely enabled by the\nnon-local computations of the transformer. It obtains, however, lower perfor-\nmances on small objects. We expect that future work will improve this aspect\nin the same way the development of FPN [22] did for Faster R-CNN."
        },
        {
            "bounding_box": [
                {
                    "x": 135,
                    "y": 2202
                },
                {
                    "x": 1588,
                    "y": 2202
                },
                {
                    "x": 1588,
                    "y": 2298
                },
                {
                    "x": 135,
                    "y": 2298
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='16' style='font-size:22px'>Training settings for DETR differ from standard object detectors in mul-<br>tiple ways. The new model requires extra-long training schedule and benefits</p>",
            "id": 16,
            "page": 2,
            "text": "Training settings for DETR differ from standard object detectors in mul-\ntiple ways. The new model requires extra-long training schedule and benefits"
        },
        {
            "bounding_box": [
                {
                    "x": 144,
                    "y": 2341
                },
                {
                    "x": 1584,
                    "y": 2341
                },
                {
                    "x": 1584,
                    "y": 2432
                },
                {
                    "x": 144,
                    "y": 2432
                }
            ],
            "category": "paragraph",
            "html": "<p id='17' style='font-size:18px'>1 In our work we use standard implementations of Transformers [47] and ResNet [15]<br>backbones from standard deep learning libraries.</p>",
            "id": 17,
            "page": 2,
            "text": "1 In our work we use standard implementations of Transformers [47] and ResNet [15]\nbackbones from standard deep learning libraries."
        },
        {
            "bounding_box": [
                {
                    "x": 618,
                    "y": 41
                },
                {
                    "x": 1590,
                    "y": 41
                },
                {
                    "x": 1590,
                    "y": 87
                },
                {
                    "x": 618,
                    "y": 87
                }
            ],
            "category": "header",
            "html": "<header id='18' style='font-size:14px'>End-to-End Object Detection with Transformers 3</header>",
            "id": 18,
            "page": 3,
            "text": "End-to-End Object Detection with Transformers 3"
        },
        {
            "bounding_box": [
                {
                    "x": 132,
                    "y": 146
                },
                {
                    "x": 1588,
                    "y": 146
                },
                {
                    "x": 1588,
                    "y": 244
                },
                {
                    "x": 132,
                    "y": 244
                }
            ],
            "category": "paragraph",
            "html": "<p id='19' style='font-size:18px'>from auxiliary decoding losses in the transformer. We thoroughly explore what<br>components are crucial for the demonstrated performance.</p>",
            "id": 19,
            "page": 3,
            "text": "from auxiliary decoding losses in the transformer. We thoroughly explore what\ncomponents are crucial for the demonstrated performance."
        },
        {
            "bounding_box": [
                {
                    "x": 132,
                    "y": 248
                },
                {
                    "x": 1590,
                    "y": 248
                },
                {
                    "x": 1590,
                    "y": 451
                },
                {
                    "x": 132,
                    "y": 451
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='20' style='font-size:18px'>The design ethos of DETR easily extend to more complex tasks. In our<br>experiments, we show that a simple segmentation head trained on top of a pre-<br>trained DETR outperfoms competitive baselines on Panoptic Segmentation [19],<br>a challenging pixel-level recognition task that has recently gained popularity.</p>",
            "id": 20,
            "page": 3,
            "text": "The design ethos of DETR easily extend to more complex tasks. In our\nexperiments, we show that a simple segmentation head trained on top of a pre-\ntrained DETR outperfoms competitive baselines on Panoptic Segmentation [19],\na challenging pixel-level recognition task that has recently gained popularity."
        },
        {
            "bounding_box": [
                {
                    "x": 136,
                    "y": 531
                },
                {
                    "x": 557,
                    "y": 531
                },
                {
                    "x": 557,
                    "y": 585
                },
                {
                    "x": 136,
                    "y": 585
                }
            ],
            "category": "paragraph",
            "html": "<p id='21' style='font-size:22px'>2 Related work</p>",
            "id": 21,
            "page": 3,
            "text": "2 Related work"
        },
        {
            "bounding_box": [
                {
                    "x": 133,
                    "y": 642
                },
                {
                    "x": 1589,
                    "y": 642
                },
                {
                    "x": 1589,
                    "y": 795
                },
                {
                    "x": 133,
                    "y": 795
                }
            ],
            "category": "paragraph",
            "html": "<p id='22' style='font-size:18px'>Our work build on prior work in several domains: bipartite matching losses for<br>set prediction, encoder-decoder architectures based on the transformer, parallel<br>decoding, and object detection methods.</p>",
            "id": 22,
            "page": 3,
            "text": "Our work build on prior work in several domains: bipartite matching losses for\nset prediction, encoder-decoder architectures based on the transformer, parallel\ndecoding, and object detection methods."
        },
        {
            "bounding_box": [
                {
                    "x": 135,
                    "y": 876
                },
                {
                    "x": 555,
                    "y": 876
                },
                {
                    "x": 555,
                    "y": 925
                },
                {
                    "x": 135,
                    "y": 925
                }
            ],
            "category": "paragraph",
            "html": "<p id='23' style='font-size:16px'>2.1 Set Prediction</p>",
            "id": 23,
            "page": 3,
            "text": "2.1 Set Prediction"
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 964
                },
                {
                    "x": 1588,
                    "y": 964
                },
                {
                    "x": 1588,
                    "y": 1868
                },
                {
                    "x": 134,
                    "y": 1868
                }
            ],
            "category": "paragraph",
            "html": "<p id='24' style='font-size:18px'>There is no canonical deep learning model to directly predict sets. The basic set<br>prediction task is multilabel classification (see e.g., [40,33] for references in the<br>context of computer vision) for which the baseline approach, one-vs-rest, does<br>not apply to problems such as detection where there is an underlying structure<br>between elements (i.e., near-identical boxes). The first difficulty in these tasks<br>is to avoid near-duplicates. Most current detectors use postprocessings such as<br>non-maximal suppression to address this issue, but direct set prediction are<br>postprocessing-free. They need global inference schemes that model interactions<br>between all predicted elements to avoid redundancy. For constant-size set pre-<br>diction, dense fully connected networks [9] are sufficient but costly. A general<br>approach is to use auto-regressive sequence models such as recurrent neural net-<br>works [48]. In all cases, the loss function should be invariant by a permutation of<br>the predictions. The usual solution is to design a loss based on the Hungarian al-<br>gorithm [20], to find a bipartite matching between ground-truth and prediction.<br>This enforces permutation-invariance, and guarantees that each target element<br>has a unique match. We follow the bipartite matching loss approach. In contrast<br>to most prior work however, we step away from autoregressive models and use<br>transformers with parallel decoding, which we describe below.</p>",
            "id": 24,
            "page": 3,
            "text": "There is no canonical deep learning model to directly predict sets. The basic set\nprediction task is multilabel classification (see e.g., [40,33] for references in the\ncontext of computer vision) for which the baseline approach, one-vs-rest, does\nnot apply to problems such as detection where there is an underlying structure\nbetween elements (i.e., near-identical boxes). The first difficulty in these tasks\nis to avoid near-duplicates. Most current detectors use postprocessings such as\nnon-maximal suppression to address this issue, but direct set prediction are\npostprocessing-free. They need global inference schemes that model interactions\nbetween all predicted elements to avoid redundancy. For constant-size set pre-\ndiction, dense fully connected networks [9] are sufficient but costly. A general\napproach is to use auto-regressive sequence models such as recurrent neural net-\nworks [48]. In all cases, the loss function should be invariant by a permutation of\nthe predictions. The usual solution is to design a loss based on the Hungarian al-\ngorithm [20], to find a bipartite matching between ground-truth and prediction.\nThis enforces permutation-invariance, and guarantees that each target element\nhas a unique match. We follow the bipartite matching loss approach. In contrast\nto most prior work however, we step away from autoregressive models and use\ntransformers with parallel decoding, which we describe below."
        },
        {
            "bounding_box": [
                {
                    "x": 135,
                    "y": 1945
                },
                {
                    "x": 1011,
                    "y": 1945
                },
                {
                    "x": 1011,
                    "y": 1997
                },
                {
                    "x": 135,
                    "y": 1997
                }
            ],
            "category": "paragraph",
            "html": "<p id='25' style='font-size:20px'>2.2 Transformers and Parallel Decoding</p>",
            "id": 25,
            "page": 3,
            "text": "2.2 Transformers and Parallel Decoding"
        },
        {
            "bounding_box": [
                {
                    "x": 133,
                    "y": 2034
                },
                {
                    "x": 1590,
                    "y": 2034
                },
                {
                    "x": 1590,
                    "y": 2435
                },
                {
                    "x": 133,
                    "y": 2435
                }
            ],
            "category": "paragraph",
            "html": "<p id='26' style='font-size:18px'>Transformers were introduced by Vaswani et al. [47] as a new attention-based<br>building block for machine translation. Attention mechanisms [2] are neural net-<br>work layers that aggregate information from the entire input sequence. Trans-<br>formers introduced self-attention layers, which, similarly to Non-Local Neural<br>Networks [49], scan through each element of a sequence and update it by ag-<br>gregating information from the whole sequence. One of the main advantages of<br>attention-based models is their global computations and perfect memory, which<br>makes them more suitable than RNNs on long sequences. Transformers are now</p>",
            "id": 26,
            "page": 3,
            "text": "Transformers were introduced by Vaswani et al. [47] as a new attention-based\nbuilding block for machine translation. Attention mechanisms [2] are neural net-\nwork layers that aggregate information from the entire input sequence. Trans-\nformers introduced self-attention layers, which, similarly to Non-Local Neural\nNetworks [49], scan through each element of a sequence and update it by ag-\ngregating information from the whole sequence. One of the main advantages of\nattention-based models is their global computations and perfect memory, which\nmakes them more suitable than RNNs on long sequences. Transformers are now"
        },
        {
            "bounding_box": [
                {
                    "x": 137,
                    "y": 46
                },
                {
                    "x": 163,
                    "y": 46
                },
                {
                    "x": 163,
                    "y": 79
                },
                {
                    "x": 137,
                    "y": 79
                }
            ],
            "category": "header",
            "html": "<header id='27' style='font-size:14px'>4</header>",
            "id": 27,
            "page": 4,
            "text": "4"
        },
        {
            "bounding_box": [
                {
                    "x": 269,
                    "y": 46
                },
                {
                    "x": 490,
                    "y": 46
                },
                {
                    "x": 490,
                    "y": 82
                },
                {
                    "x": 269,
                    "y": 82
                }
            ],
            "category": "header",
            "html": "<br><header id='28' style='font-size:14px'>Carion et al.</header>",
            "id": 28,
            "page": 4,
            "text": "Carion et al."
        },
        {
            "bounding_box": [
                {
                    "x": 133,
                    "y": 148
                },
                {
                    "x": 1589,
                    "y": 148
                },
                {
                    "x": 1589,
                    "y": 242
                },
                {
                    "x": 133,
                    "y": 242
                }
            ],
            "category": "paragraph",
            "html": "<p id='29' style='font-size:18px'>replacing RNNs in many problems in natural language processing, speech pro-<br>cessing and computer vision 8,27,45,34,31</p>",
            "id": 29,
            "page": 4,
            "text": "replacing RNNs in many problems in natural language processing, speech pro-\ncessing and computer vision 8,27,45,34,31"
        },
        {
            "bounding_box": [
                {
                    "x": 133,
                    "y": 249
                },
                {
                    "x": 1625,
                    "y": 249
                },
                {
                    "x": 1625,
                    "y": 645
                },
                {
                    "x": 133,
                    "y": 645
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='30' style='font-size:18px'>Transformers were first used in auto-regressive models, following early sequence-<br>to-sequence models [44], generating output tokens one by one. However, the pro-<br>hibitive inference cost (proportional to output length, and hard to batch) lead<br>to the development of parallel sequence generation, in the domains of audio [29],<br>machine translation [12,10], word representation learning [8], and more recently<br>speech recognition [6]. We also combine transformers and parallel decoding for<br>their suitable trade-off between computational cost and the ability to perform<br>the global computations required for set prediction.</p>",
            "id": 30,
            "page": 4,
            "text": "Transformers were first used in auto-regressive models, following early sequence-\nto-sequence models [44], generating output tokens one by one. However, the pro-\nhibitive inference cost (proportional to output length, and hard to batch) lead\nto the development of parallel sequence generation, in the domains of audio [29],\nmachine translation [12,10], word representation learning [8], and more recently\nspeech recognition [6]. We also combine transformers and parallel decoding for\ntheir suitable trade-off between computational cost and the ability to perform\nthe global computations required for set prediction."
        },
        {
            "bounding_box": [
                {
                    "x": 135,
                    "y": 711
                },
                {
                    "x": 604,
                    "y": 711
                },
                {
                    "x": 604,
                    "y": 757
                },
                {
                    "x": 135,
                    "y": 757
                }
            ],
            "category": "paragraph",
            "html": "<p id='31' style='font-size:20px'>2.3 Object detection</p>",
            "id": 31,
            "page": 4,
            "text": "2.3 Object detection"
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 784
                },
                {
                    "x": 1591,
                    "y": 784
                },
                {
                    "x": 1591,
                    "y": 1183
                },
                {
                    "x": 134,
                    "y": 1183
                }
            ],
            "category": "paragraph",
            "html": "<p id='32' style='font-size:14px'>Most modern object detection methods make predictions relative to some ini-<br>tial guesses. Two-stage detectors [37,5] predict boxes w.r.t. proposals, whereas<br>single-stage methods make predictions w.r.t. anchors [23] or a grid of possible<br>object centers [53,46]. Recent work [52] demonstrate that the final performance<br>of these systems heavily depends on the exact way these initial guesses are set.<br>In our model we are able to remove this hand-crafted process and streamline the<br>detection process by directly predicting the set of detections with absolute box<br>prediction w.r.t. the input image rather than an anchor.</p>",
            "id": 32,
            "page": 4,
            "text": "Most modern object detection methods make predictions relative to some ini-\ntial guesses. Two-stage detectors [37,5] predict boxes w.r.t. proposals, whereas\nsingle-stage methods make predictions w.r.t. anchors [23] or a grid of possible\nobject centers [53,46]. Recent work [52] demonstrate that the final performance\nof these systems heavily depends on the exact way these initial guesses are set.\nIn our model we are able to remove this hand-crafted process and streamline the\ndetection process by directly predicting the set of detections with absolute box\nprediction w.r.t. the input image rather than an anchor."
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 1202
                },
                {
                    "x": 1589,
                    "y": 1202
                },
                {
                    "x": 1589,
                    "y": 1499
                },
                {
                    "x": 134,
                    "y": 1499
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='33' style='font-size:18px'>Set-based loss. Several object detectors [9,25,35] used the bipartite matching<br>loss. However, in these early deep learning models, the relation between different<br>prediction was modeled with convolutional or fully-connected layers only and a<br>hand-designed NMS post-processing can improve their performance. More recent<br>detectors [37,23,53] use non-unique assignment rules between ground truth and<br>predictions together with an NMS.</p>",
            "id": 33,
            "page": 4,
            "text": "Set-based loss. Several object detectors [9,25,35] used the bipartite matching\nloss. However, in these early deep learning models, the relation between different\nprediction was modeled with convolutional or fully-connected layers only and a\nhand-designed NMS post-processing can improve their performance. More recent\ndetectors [37,23,53] use non-unique assignment rules between ground truth and\npredictions together with an NMS."
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 1503
                },
                {
                    "x": 1591,
                    "y": 1503
                },
                {
                    "x": 1591,
                    "y": 1799
                },
                {
                    "x": 134,
                    "y": 1799
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='34' style='font-size:18px'>Learnable NMS methods [16,4] and relation networks [17] explicitly model<br>relations between different predictions with attention. Using direct set losses,<br>they do not require any post-processing steps. However, these methods employ<br>additional hand-crafted context features like proposal box coordinates to model<br>relations between detections efficiently, while we look for solutions that reduce<br>the prior knowledge encoded in the model.</p>",
            "id": 34,
            "page": 4,
            "text": "Learnable NMS methods [16,4] and relation networks [17] explicitly model\nrelations between different predictions with attention. Using direct set losses,\nthey do not require any post-processing steps. However, these methods employ\nadditional hand-crafted context features like proposal box coordinates to model\nrelations between detections efficiently, while we look for solutions that reduce\nthe prior knowledge encoded in the model."
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 1819
                },
                {
                    "x": 1592,
                    "y": 1819
                },
                {
                    "x": 1592,
                    "y": 2171
                },
                {
                    "x": 134,
                    "y": 2171
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='35' style='font-size:16px'>Recurrent detectors. Closest to our approach are end-to-end set predictions<br>for object detection [43] and instance segmentation 41,30,36,42 Similarly to us,<br>they use bipartite-matching losses with encoder-decoder architectures based on<br>CNN activations to directly produce a set of bounding boxes. These approaches,<br>however, were only evaluated on small datasets and not against modern baselines.<br>In particular, they are based on autoregressive models (more precisely RNNs),<br>SO they do not leverage the recent transformers with parallel decoding.</p>",
            "id": 35,
            "page": 4,
            "text": "Recurrent detectors. Closest to our approach are end-to-end set predictions\nfor object detection [43] and instance segmentation 41,30,36,42 Similarly to us,\nthey use bipartite-matching losses with encoder-decoder architectures based on\nCNN activations to directly produce a set of bounding boxes. These approaches,\nhowever, were only evaluated on small datasets and not against modern baselines.\nIn particular, they are based on autoregressive models (more precisely RNNs),\nSO they do not leverage the recent transformers with parallel decoding."
        },
        {
            "bounding_box": [
                {
                    "x": 136,
                    "y": 2239
                },
                {
                    "x": 673,
                    "y": 2239
                },
                {
                    "x": 673,
                    "y": 2290
                },
                {
                    "x": 136,
                    "y": 2290
                }
            ],
            "category": "paragraph",
            "html": "<p id='36' style='font-size:22px'>3 The DETR model</p>",
            "id": 36,
            "page": 4,
            "text": "3 The DETR model"
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 2332
                },
                {
                    "x": 1590,
                    "y": 2332
                },
                {
                    "x": 1590,
                    "y": 2438
                },
                {
                    "x": 134,
                    "y": 2438
                }
            ],
            "category": "paragraph",
            "html": "<p id='37' style='font-size:16px'>Two ingredients are essential for direct set predictions in detection: (1) a set<br>prediction loss that forces unique matching between predicted and ground truth</p>",
            "id": 37,
            "page": 4,
            "text": "Two ingredients are essential for direct set predictions in detection: (1) a set\nprediction loss that forces unique matching between predicted and ground truth"
        },
        {
            "bounding_box": [
                {
                    "x": 619,
                    "y": 42
                },
                {
                    "x": 1591,
                    "y": 42
                },
                {
                    "x": 1591,
                    "y": 86
                },
                {
                    "x": 619,
                    "y": 86
                }
            ],
            "category": "header",
            "html": "<header id='38' style='font-size:14px'>End-to-End Object Detection with Transformers 5</header>",
            "id": 38,
            "page": 5,
            "text": "End-to-End Object Detection with Transformers 5"
        },
        {
            "bounding_box": [
                {
                    "x": 133,
                    "y": 145
                },
                {
                    "x": 1589,
                    "y": 145
                },
                {
                    "x": 1589,
                    "y": 250
                },
                {
                    "x": 133,
                    "y": 250
                }
            ],
            "category": "paragraph",
            "html": "<p id='39' style='font-size:14px'>boxes; (2) an architecture that predicts (in a single pass) a set of objects and<br>models their relation. We describe our architecture in detail in Figure 2.</p>",
            "id": 39,
            "page": 5,
            "text": "boxes; (2) an architecture that predicts (in a single pass) a set of objects and\nmodels their relation. We describe our architecture in detail in Figure 2."
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 321
                },
                {
                    "x": 999,
                    "y": 321
                },
                {
                    "x": 999,
                    "y": 370
                },
                {
                    "x": 134,
                    "y": 370
                }
            ],
            "category": "paragraph",
            "html": "<p id='40' style='font-size:18px'>3.1 Object detection set prediction loss</p>",
            "id": 40,
            "page": 5,
            "text": "3.1 Object detection set prediction loss"
        },
        {
            "bounding_box": [
                {
                    "x": 133,
                    "y": 403
                },
                {
                    "x": 1589,
                    "y": 403
                },
                {
                    "x": 1589,
                    "y": 702
                },
                {
                    "x": 133,
                    "y": 702
                }
            ],
            "category": "paragraph",
            "html": "<p id='41' style='font-size:18px'>DETR infers a fixed-size set of N predictions, in a single pass through the<br>decoder, where N is set to be significantly larger than the typical number of<br>objects in an image. One of the main difficulties of training is to score predicted<br>objects (class, position, size) with respect to the ground truth. Our loss produces<br>an optimal bipartite matching between predicted and ground truth objects, and<br>then optimize object-specific (bounding box) losses.</p>",
            "id": 41,
            "page": 5,
            "text": "DETR infers a fixed-size set of N predictions, in a single pass through the\ndecoder, where N is set to be significantly larger than the typical number of\nobjects in an image. One of the main difficulties of training is to score predicted\nobjects (class, position, size) with respect to the ground truth. Our loss produces\nan optimal bipartite matching between predicted and ground truth objects, and\nthen optimize object-specific (bounding box) losses."
        },
        {
            "bounding_box": [
                {
                    "x": 132,
                    "y": 704
                },
                {
                    "x": 1589,
                    "y": 704
                },
                {
                    "x": 1589,
                    "y": 951
                },
                {
                    "x": 132,
                    "y": 951
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='42' style='font-size:14px'>Let us denote by y the ground truth set of objects, and y = {yi}i=1 the<br>set of N predictions. Assuming N is larger than the number of objects in the<br>image, we consider y also as a set of size N padded with ⌀ (no object). To find<br>a bipartite matching between these two sets we search for a permutation of N<br>elements 0 E GN with the lowest cost:</p>",
            "id": 42,
            "page": 5,
            "text": "Let us denote by y the ground truth set of objects, and y = {yi}i=1 the\nset of N predictions. Assuming N is larger than the number of objects in the\nimage, we consider y also as a set of size N padded with ⌀ (no object). To find\na bipartite matching between these two sets we search for a permutation of N\nelements 0 E GN with the lowest cost:"
        },
        {
            "bounding_box": [
                {
                    "x": 132,
                    "y": 1160
                },
                {
                    "x": 1589,
                    "y": 1160
                },
                {
                    "x": 1589,
                    "y": 1309
                },
                {
                    "x": 132,
                    "y": 1309
                }
            ],
            "category": "paragraph",
            "html": "<p id='43' style='font-size:18px'>where Lmatch (yi, Yo(i)) is a pair-wise matching cost between ground truth Yi and<br>a prediction with index o(i). This optimal assignment is computed efficiently<br>with the Hungarian algorithm, following prior work (e.g. [43]).</p>",
            "id": 43,
            "page": 5,
            "text": "where Lmatch (yi, Yo(i)) is a pair-wise matching cost between ground truth Yi and\na prediction with index o(i). This optimal assignment is computed efficiently\nwith the Hungarian algorithm, following prior work (e.g. [43])."
        },
        {
            "bounding_box": [
                {
                    "x": 133,
                    "y": 1313
                },
                {
                    "x": 1589,
                    "y": 1313
                },
                {
                    "x": 1589,
                    "y": 1671
                },
                {
                    "x": 133,
                    "y": 1671
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='44' style='font-size:14px'>The matching cost takes into account both the class prediction and the sim-<br>ilarity of predicted and ground truth boxes. Each element i of the ground truth<br>set can be seen as a Yi = (ci, bi) where Ci is the target class label (which<br>may be ⌀) and bi E [0, 1]4 is a vector that defines ground truth box cen-<br>ter coordinates and its height and width relative to the image size. For the<br>prediction with index o(i) we define probability of class Ci as Po(i)(Ci) and<br>the predicted box as bo(i). With these notations we define Lmatch (yi, Yo(i)) as</p>",
            "id": 44,
            "page": 5,
            "text": "The matching cost takes into account both the class prediction and the sim-\nilarity of predicted and ground truth boxes. Each element i of the ground truth\nset can be seen as a Yi = (ci, bi) where Ci is the target class label (which\nmay be ⌀) and bi E [0, 1]4 is a vector that defines ground truth box cen-\nter coordinates and its height and width relative to the image size. For the\nprediction with index o(i) we define probability of class Ci as Po(i)(Ci) and\nthe predicted box as bo(i). With these notations we define Lmatch (yi, Yo(i)) as"
        },
        {
            "bounding_box": [
                {
                    "x": 132,
                    "y": 1672
                },
                {
                    "x": 912,
                    "y": 1672
                },
                {
                    "x": 912,
                    "y": 1732
                },
                {
                    "x": 132,
                    "y": 1732
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='45' style='font-size:22px'>-1{ci≠⌀}��(i)(Ci) + 1{ci≠⌀} Lbox (bi, bo(i)).</p>",
            "id": 45,
            "page": 5,
            "text": "-1{ci≠⌀}��(i)(Ci) + 1{ci≠⌀} Lbox (bi, bo(i))."
        },
        {
            "bounding_box": [
                {
                    "x": 132,
                    "y": 1731
                },
                {
                    "x": 1591,
                    "y": 1731
                },
                {
                    "x": 1591,
                    "y": 1925
                },
                {
                    "x": 132,
                    "y": 1925
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='46' style='font-size:16px'>This procedure of finding matching plays the same role as the heuristic assign-<br>ment rules used to match proposal [37] or anchors [22] to ground truth objects<br>in modern detectors. The main difference is that we need to find one-to-one<br>matching for direct set prediction without duplicates.</p>",
            "id": 46,
            "page": 5,
            "text": "This procedure of finding matching plays the same role as the heuristic assign-\nment rules used to match proposal [37] or anchors [22] to ground truth objects\nin modern detectors. The main difference is that we need to find one-to-one\nmatching for direct set prediction without duplicates."
        },
        {
            "bounding_box": [
                {
                    "x": 133,
                    "y": 1928
                },
                {
                    "x": 1590,
                    "y": 1928
                },
                {
                    "x": 1590,
                    "y": 2126
                },
                {
                    "x": 133,
                    "y": 2126
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='47' style='font-size:16px'>The second step is to compute the loss function, the Hungarian loss for all<br>pairs matched in the previous step. We define the loss similarly to the losses of<br>common object detectors, i.e. a linear combination of a negative log-likelihood<br>for class prediction and a box loss defined later:</p>",
            "id": 47,
            "page": 5,
            "text": "The second step is to compute the loss function, the Hungarian loss for all\npairs matched in the previous step. We define the loss similarly to the losses of\ncommon object detectors, i.e. a linear combination of a negative log-likelihood\nfor class prediction and a box loss defined later:"
        },
        {
            "bounding_box": [
                {
                    "x": 132,
                    "y": 2334
                },
                {
                    "x": 1589,
                    "y": 2334
                },
                {
                    "x": 1589,
                    "y": 2438
                },
                {
                    "x": 132,
                    "y": 2438
                }
            ],
            "category": "paragraph",
            "html": "<p id='48' style='font-size:14px'>where 0 is the optimal assignment computed in the first step (1). In practice, we<br>down-weight the log-probability term when Ci = ⌀ by a factor 10 to account for</p>",
            "id": 48,
            "page": 5,
            "text": "where 0 is the optimal assignment computed in the first step (1). In practice, we\ndown-weight the log-probability term when Ci = ⌀ by a factor 10 to account for"
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 45
                },
                {
                    "x": 493,
                    "y": 45
                },
                {
                    "x": 493,
                    "y": 82
                },
                {
                    "x": 134,
                    "y": 82
                }
            ],
            "category": "header",
            "html": "<header id='49' style='font-size:14px'>6 Carion et al.</header>",
            "id": 49,
            "page": 6,
            "text": "6 Carion et al."
        },
        {
            "bounding_box": [
                {
                    "x": 133,
                    "y": 146
                },
                {
                    "x": 1590,
                    "y": 146
                },
                {
                    "x": 1590,
                    "y": 494
                },
                {
                    "x": 133,
                    "y": 494
                }
            ],
            "category": "paragraph",
            "html": "<p id='50' style='font-size:22px'>class imbalance. This is analogous to how Faster R-CNN training procedure bal-<br>ances positive/ negative proposals by subsampling [37]. Notice that the matching<br>cost between an object and ⌀ doesn't depend on the prediction, which means<br>that in that case the cost is a constant. In the matching cost we use probabil-<br>ities Po(i) (ci) instead of log-probabilities. This makes the class prediction term<br>commensurable to Lbox (·,·) (described below), and we observed better empirical<br>performances.</p>",
            "id": 50,
            "page": 6,
            "text": "class imbalance. This is analogous to how Faster R-CNN training procedure bal-\nances positive/ negative proposals by subsampling [37]. Notice that the matching\ncost between an object and ⌀ doesn't depend on the prediction, which means\nthat in that case the cost is a constant. In the matching cost we use probabil-\nities Po(i) (ci) instead of log-probabilities. This makes the class prediction term\ncommensurable to Lbox (·,·) (described below), and we observed better empirical\nperformances."
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 513
                },
                {
                    "x": 1592,
                    "y": 513
                },
                {
                    "x": 1592,
                    "y": 1027
                },
                {
                    "x": 134,
                    "y": 1027
                }
            ],
            "category": "paragraph",
            "html": "<p id='51' style='font-size:22px'>Bounding box loss. The second part of the matching cost and the Hungarian<br>loss is Lbox (·) that scores the bounding boxes. Unlike many detectors that do box<br>predictions as a △ w.r.t. some initial guesses, we make box predictions directly.<br>While such approach simplify the implementation it poses an issue with relative<br>scaling of the loss. The most commonly-used l1 loss will have different scales for<br>small and large boxes even if their relative errors are similar. To mitigate this<br>issue we use a linear combination of the l1 loss and the generalized IoU loss [38]<br>Liou (·, ·) that is scale-invariant. Overall, our box loss is Lbox (bi, bo(i)) defined as<br>入iou Liou (bi, bo(i)) + 入L1||bi - bo(i)ll1 where 入iou, 入L1 E R are hyperparameters.<br>These two losses are normalized by the number of objects inside the batch.</p>",
            "id": 51,
            "page": 6,
            "text": "Bounding box loss. The second part of the matching cost and the Hungarian\nloss is Lbox (·) that scores the bounding boxes. Unlike many detectors that do box\npredictions as a △ w.r.t. some initial guesses, we make box predictions directly.\nWhile such approach simplify the implementation it poses an issue with relative\nscaling of the loss. The most commonly-used l1 loss will have different scales for\nsmall and large boxes even if their relative errors are similar. To mitigate this\nissue we use a linear combination of the l1 loss and the generalized IoU loss [38]\nLiou (·, ·) that is scale-invariant. Overall, our box loss is Lbox (bi, bo(i)) defined as\n入iou Liou (bi, bo(i)) + 入L1||bi - bo(i)ll1 where 入iou, 入L1 E R are hyperparameters.\nThese two losses are normalized by the number of objects inside the batch."
        },
        {
            "bounding_box": [
                {
                    "x": 135,
                    "y": 1097
                },
                {
                    "x": 660,
                    "y": 1097
                },
                {
                    "x": 660,
                    "y": 1143
                },
                {
                    "x": 135,
                    "y": 1143
                }
            ],
            "category": "paragraph",
            "html": "<p id='52' style='font-size:22px'>3.2 DETR architecture</p>",
            "id": 52,
            "page": 6,
            "text": "3.2 DETR architecture"
        },
        {
            "bounding_box": [
                {
                    "x": 135,
                    "y": 1175
                },
                {
                    "x": 1589,
                    "y": 1175
                },
                {
                    "x": 1589,
                    "y": 1372
                },
                {
                    "x": 135,
                    "y": 1372
                }
            ],
            "category": "paragraph",
            "html": "<p id='53' style='font-size:22px'>The overall DETR architecture is surprisingly simple and depicted in Figure 2. It<br>contains three main components, which we describe below: a CNN backbone to<br>extract a compact feature representation, an encoder-decoder transformer, and<br>a simple feed forward network (FFN) that makes the final detection prediction.</p>",
            "id": 53,
            "page": 6,
            "text": "The overall DETR architecture is surprisingly simple and depicted in Figure 2. It\ncontains three main components, which we describe below: a CNN backbone to\nextract a compact feature representation, an encoder-decoder transformer, and\na simple feed forward network (FFN) that makes the final detection prediction."
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 1378
                },
                {
                    "x": 1588,
                    "y": 1378
                },
                {
                    "x": 1588,
                    "y": 1624
                },
                {
                    "x": 134,
                    "y": 1624
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='54' style='font-size:20px'>Unlike many modern detectors, DETR can be implemented in any deep learn-<br>ing framework that provides a common CNN backbone and a transformer archi-<br>tecture implementation with just a few hundred lines. Inference code for DETR<br>can be implemented in less than 50 lines in PyTorch [32]. We hope that the sim-<br>plicity of our method will attract new researchers to the detection community.</p>",
            "id": 54,
            "page": 6,
            "text": "Unlike many modern detectors, DETR can be implemented in any deep learn-\ning framework that provides a common CNN backbone and a transformer archi-\ntecture implementation with just a few hundred lines. Inference code for DETR\ncan be implemented in less than 50 lines in PyTorch [32]. We hope that the sim-\nplicity of our method will attract new researchers to the detection community."
        },
        {
            "bounding_box": [
                {
                    "x": 133,
                    "y": 1640
                },
                {
                    "x": 1589,
                    "y": 1640
                },
                {
                    "x": 1589,
                    "y": 1801
                },
                {
                    "x": 133,
                    "y": 1801
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='55' style='font-size:16px'>Backbone. Starting from the initial image Ximg E R3xHoxWo 3 color<br>(with<br>channels'), a conventional CNN backbone generates a lower-resolution activation<br>Typical values we use are C = 2048 and H, W =<br>map f E RCxHxW Ho Wo<br>32 , 32</p>",
            "id": 55,
            "page": 6,
            "text": "Backbone. Starting from the initial image Ximg E R3xHoxWo 3 color\n(with\nchannels'), a conventional CNN backbone generates a lower-resolution activation\nTypical values we use are C = 2048 and H, W =\nmap f E RCxHxW Ho Wo\n32 , 32"
        },
        {
            "bounding_box": [
                {
                    "x": 133,
                    "y": 1812
                },
                {
                    "x": 1590,
                    "y": 1812
                },
                {
                    "x": 1590,
                    "y": 2313
                },
                {
                    "x": 133,
                    "y": 2313
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='56' style='font-size:20px'>Transformer encoder. First, a 1x1 convolution reduces the channel dimension<br>of the high-level activation map f from C to a smaller dimension d. creating a<br>The encoder expects a sequence as input, hence<br>new feature map zo E RdxHxW<br>we collapse the spatial dimensions of zo into one dimension, resulting in a dx HW<br>feature map. Each encoder layer has a standard architecture and consists of a<br>multi-head self-attention module and a feed forward network (FFN). Since the<br>transformer architecture is permutation-invariant, we supplement it with fixed<br>positional encodings [31,3] that are added to the input of each attention layer. We<br>defer to the supplementary material the detailed definition of the architecture,<br>which follows the one described in [47].</p>",
            "id": 56,
            "page": 6,
            "text": "Transformer encoder. First, a 1x1 convolution reduces the channel dimension\nof the high-level activation map f from C to a smaller dimension d. creating a\nThe encoder expects a sequence as input, hence\nnew feature map zo E RdxHxW\nwe collapse the spatial dimensions of zo into one dimension, resulting in a dx HW\nfeature map. Each encoder layer has a standard architecture and consists of a\nmulti-head self-attention module and a feed forward network (FFN). Since the\ntransformer architecture is permutation-invariant, we supplement it with fixed\npositional encodings [31,3] that are added to the input of each attention layer. We\ndefer to the supplementary material the detailed definition of the architecture,\nwhich follows the one described in [47]."
        },
        {
            "bounding_box": [
                {
                    "x": 141,
                    "y": 2341
                },
                {
                    "x": 1588,
                    "y": 2341
                },
                {
                    "x": 1588,
                    "y": 2434
                },
                {
                    "x": 141,
                    "y": 2434
                }
            ],
            "category": "paragraph",
            "html": "<p id='57' style='font-size:18px'>2 The input images are batched together, applying 0-padding adequately to ensure<br>they all have the same dimensions (Ho, Wo) as the largest image of the batch.</p>",
            "id": 57,
            "page": 6,
            "text": "2 The input images are batched together, applying 0-padding adequately to ensure\nthey all have the same dimensions (Ho, Wo) as the largest image of the batch."
        },
        {
            "bounding_box": [
                {
                    "x": 619,
                    "y": 41
                },
                {
                    "x": 1479,
                    "y": 41
                },
                {
                    "x": 1479,
                    "y": 88
                },
                {
                    "x": 619,
                    "y": 88
                }
            ],
            "category": "header",
            "html": "<header id='58' style='font-size:18px'>End-to-End Object Detection with Transformers</header>",
            "id": 58,
            "page": 7,
            "text": "End-to-End Object Detection with Transformers"
        },
        {
            "bounding_box": [
                {
                    "x": 1558,
                    "y": 45
                },
                {
                    "x": 1586,
                    "y": 45
                },
                {
                    "x": 1586,
                    "y": 80
                },
                {
                    "x": 1558,
                    "y": 80
                }
            ],
            "category": "header",
            "html": "<br><header id='59' style='font-size:14px'>7</header>",
            "id": 59,
            "page": 7,
            "text": "7"
        },
        {
            "bounding_box": [
                {
                    "x": 133,
                    "y": 139
                },
                {
                    "x": 1587,
                    "y": 139
                },
                {
                    "x": 1587,
                    "y": 506
                },
                {
                    "x": 133,
                    "y": 506
                }
            ],
            "category": "figure",
            "html": "<figure><img id='60' style='font-size:14px' alt=\"backbone encoder decoder prediction heads\nset of image features\nclass,\nFFN\nbox\nCNN\nno\nFFN\ntransformer transformer object\nencoder decoder class,\nFFN\nbox\nno\nFFN\nobject\nobject queries\npositional encoding\" data-coord=\"top-left:(133,139); bottom-right:(1587,506)\" /></figure>",
            "id": 60,
            "page": 7,
            "text": "backbone encoder decoder prediction heads\nset of image features\nclass,\nFFN\nbox\nCNN\nno\nFFN\ntransformer transformer object\nencoder decoder class,\nFFN\nbox\nno\nFFN\nobject\nobject queries\npositional encoding"
        },
        {
            "bounding_box": [
                {
                    "x": 135,
                    "y": 543
                },
                {
                    "x": 1592,
                    "y": 543
                },
                {
                    "x": 1592,
                    "y": 868
                },
                {
                    "x": 135,
                    "y": 868
                }
            ],
            "category": "caption",
            "html": "<caption id='61' style='font-size:16px'>Fig. 2: DETR uses a conventional CNN backbone to learn a 2D representation of an<br>input image. The model flattens it and supplements it with a positional encoding before<br>passing it into a transformer encoder. A transformer decoder then takes as input a<br>small fixed number of learned positional embeddings, which we call object queries, and<br>additionally attends to the encoder output. We pass each output embedding of the<br>decoder to a shared feed forward network (FFN) that predicts either a detection (class<br>and bounding box) or a \"no object\" class.</caption>",
            "id": 61,
            "page": 7,
            "text": "Fig. 2: DETR uses a conventional CNN backbone to learn a 2D representation of an\ninput image. The model flattens it and supplements it with a positional encoding before\npassing it into a transformer encoder. A transformer decoder then takes as input a\nsmall fixed number of learned positional embeddings, which we call object queries, and\nadditionally attends to the encoder output. We pass each output embedding of the\ndecoder to a shared feed forward network (FFN) that predicts either a detection (class\nand bounding box) or a \"no object\" class."
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 966
                },
                {
                    "x": 1589,
                    "y": 966
                },
                {
                    "x": 1589,
                    "y": 1770
                },
                {
                    "x": 134,
                    "y": 1770
                }
            ],
            "category": "paragraph",
            "html": "<p id='62' style='font-size:20px'>Transformer decoder. The decoder follows the standard architecture of the<br>transformer, transforming N embeddings of size d using multi-headed self- and<br>encoder-decoder attention mechanisms. The difference with the original trans-<br>former is that our model decodes the N objects in parallel at each decoder layer,<br>while Vaswani et al. [47] use an autoregressive model that predicts the output<br>sequence one element at a time. We refer the reader unfamiliar with the concepts<br>to the supplementary material. Since the decoder is also permutation-invariant,<br>the N input embeddings must be different to produce different results. These in-<br>put embeddings are learnt positional encodings that we refer to as object queries,<br>and similarly to the encoder, we add them to the input of each attention layer.<br>The N object queries are transformed into an output embedding by the decoder.<br>They are then independently decoded into box coordinates and class labels by<br>a feed forward network (described in the next subsection), resulting N final<br>predictions. Using self- and encoder-decoder attention over these embeddings,<br>the model globally reasons about all objects together using pair-wise relations<br>between them, while being able to use the whole image as context.</p>",
            "id": 62,
            "page": 7,
            "text": "Transformer decoder. The decoder follows the standard architecture of the\ntransformer, transforming N embeddings of size d using multi-headed self- and\nencoder-decoder attention mechanisms. The difference with the original trans-\nformer is that our model decodes the N objects in parallel at each decoder layer,\nwhile Vaswani et al. [47] use an autoregressive model that predicts the output\nsequence one element at a time. We refer the reader unfamiliar with the concepts\nto the supplementary material. Since the decoder is also permutation-invariant,\nthe N input embeddings must be different to produce different results. These in-\nput embeddings are learnt positional encodings that we refer to as object queries,\nand similarly to the encoder, we add them to the input of each attention layer.\nThe N object queries are transformed into an output embedding by the decoder.\nThey are then independently decoded into box coordinates and class labels by\na feed forward network (described in the next subsection), resulting N final\npredictions. Using self- and encoder-decoder attention over these embeddings,\nthe model globally reasons about all objects together using pair-wise relations\nbetween them, while being able to use the whole image as context."
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 1802
                },
                {
                    "x": 1591,
                    "y": 1802
                },
                {
                    "x": 1591,
                    "y": 2301
                },
                {
                    "x": 134,
                    "y": 2301
                }
            ],
            "category": "paragraph",
            "html": "<p id='63' style='font-size:20px'>Prediction feed-forward networks (FFNs). The final prediction is com-<br>puted by a 3-layer perceptron with ReLU activation function and hidden dimen-<br>sion d, and a linear projection layer. The FFN predicts the normalized center<br>coordinates, height and width of the box w.r.t. the input image, and the lin-<br>ear layer predicts the class label using a softmax function. Since we predict a<br>fixed-size set of N bounding boxes, where N is usually much larger than the<br>actual number of objects of interest in an image, an additional special class la-<br>bel ⌀ is used to represent that no object is detected within a slot. This class<br>plays a similar role to the \"background\" class in the standard object detection<br>approaches.</p>",
            "id": 63,
            "page": 7,
            "text": "Prediction feed-forward networks (FFNs). The final prediction is com-\nputed by a 3-layer perceptron with ReLU activation function and hidden dimen-\nsion d, and a linear projection layer. The FFN predicts the normalized center\ncoordinates, height and width of the box w.r.t. the input image, and the lin-\near layer predicts the class label using a softmax function. Since we predict a\nfixed-size set of N bounding boxes, where N is usually much larger than the\nactual number of objects of interest in an image, an additional special class la-\nbel ⌀ is used to represent that no object is detected within a slot. This class\nplays a similar role to the \"background\" class in the standard object detection\napproaches."
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 2333
                },
                {
                    "x": 1590,
                    "y": 2333
                },
                {
                    "x": 1590,
                    "y": 2433
                },
                {
                    "x": 134,
                    "y": 2433
                }
            ],
            "category": "paragraph",
            "html": "<p id='64' style='font-size:22px'>Auxiliary decoding losses. We found helpful to use auxiliary losses [1] in<br>decoder during training, especially to help the model output the correct number</p>",
            "id": 64,
            "page": 7,
            "text": "Auxiliary decoding losses. We found helpful to use auxiliary losses [1] in\ndecoder during training, especially to help the model output the correct number"
        },
        {
            "bounding_box": [
                {
                    "x": 137,
                    "y": 45
                },
                {
                    "x": 165,
                    "y": 45
                },
                {
                    "x": 165,
                    "y": 80
                },
                {
                    "x": 137,
                    "y": 80
                }
            ],
            "category": "header",
            "html": "<header id='65' style='font-size:14px'>8</header>",
            "id": 65,
            "page": 8,
            "text": "8"
        },
        {
            "bounding_box": [
                {
                    "x": 276,
                    "y": 44
                },
                {
                    "x": 492,
                    "y": 44
                },
                {
                    "x": 492,
                    "y": 82
                },
                {
                    "x": 276,
                    "y": 82
                }
            ],
            "category": "header",
            "html": "<br><header id='66' style='font-size:14px'>Carion et al.</header>",
            "id": 66,
            "page": 8,
            "text": "Carion et al."
        },
        {
            "bounding_box": [
                {
                    "x": 132,
                    "y": 146
                },
                {
                    "x": 1590,
                    "y": 146
                },
                {
                    "x": 1590,
                    "y": 348
                },
                {
                    "x": 132,
                    "y": 348
                }
            ],
            "category": "paragraph",
            "html": "<p id='67' style='font-size:18px'>of objects of each class. We add prediction FFNs and Hungarian loss after each<br>decoder layer. All predictions FFNs share their parameters. We use an additional<br>shared layer-norm to normalize the input to the prediction FFNs from different<br>decoder layers.</p>",
            "id": 67,
            "page": 8,
            "text": "of objects of each class. We add prediction FFNs and Hungarian loss after each\ndecoder layer. All predictions FFNs share their parameters. We use an additional\nshared layer-norm to normalize the input to the prediction FFNs from different\ndecoder layers."
        },
        {
            "bounding_box": [
                {
                    "x": 138,
                    "y": 428
                },
                {
                    "x": 540,
                    "y": 428
                },
                {
                    "x": 540,
                    "y": 481
                },
                {
                    "x": 138,
                    "y": 481
                }
            ],
            "category": "paragraph",
            "html": "<p id='68' style='font-size:22px'>4 Experiments</p>",
            "id": 68,
            "page": 8,
            "text": "4 Experiments"
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 536
                },
                {
                    "x": 1590,
                    "y": 536
                },
                {
                    "x": 1590,
                    "y": 887
                },
                {
                    "x": 134,
                    "y": 887
                }
            ],
            "category": "paragraph",
            "html": "<p id='69' style='font-size:16px'>We show that DETR achieves competitive results compared to Faster R-CNN<br>in quantitative evaluation on COCO. Then, we provide a detailed ablation<br>study of the architecture and loss, with insights and qualitative results. Fi-<br>nally, to show that DETR is a versatile and extensible model, we present results<br>on panoptic segmentation, training only a small extension on a fixed DETR<br>model. We provide code and pretrained models to reproduce our experiments at<br>https : / /github . com/ facebookresear ch/ detr.</p>",
            "id": 69,
            "page": 8,
            "text": "We show that DETR achieves competitive results compared to Faster R-CNN\nin quantitative evaluation on COCO. Then, we provide a detailed ablation\nstudy of the architecture and loss, with insights and qualitative results. Fi-\nnally, to show that DETR is a versatile and extensible model, we present results\non panoptic segmentation, training only a small extension on a fixed DETR\nmodel. We provide code and pretrained models to reproduce our experiments at\nhttps : / /github . com/ facebookresear ch/ detr."
        },
        {
            "bounding_box": [
                {
                    "x": 133,
                    "y": 911
                },
                {
                    "x": 1590,
                    "y": 911
                },
                {
                    "x": 1590,
                    "y": 1359
                },
                {
                    "x": 133,
                    "y": 1359
                }
            ],
            "category": "paragraph",
            "html": "<p id='70' style='font-size:18px'>Dataset. We perform experiments on COCO 2017 detection and panoptic seg-<br>mentation datasets [24, 18], containing 118k training images and 5k validation<br>images. Each image is annotated with bounding boxes and panoptic segmenta-<br>tion. There are 7 instances per image on average, up to 63 instances in a single<br>image in training set, ranging from small to large on the same images. If not<br>specified, we report AP as bbox AP, the integral metric over multiple thresholds.<br>For comparison with Faster R-CNN we report validation AP at the last training<br>epoch, for ablations we report median over validation results from the last 10<br>epochs.</p>",
            "id": 70,
            "page": 8,
            "text": "Dataset. We perform experiments on COCO 2017 detection and panoptic seg-\nmentation datasets [24, 18], containing 118k training images and 5k validation\nimages. Each image is annotated with bounding boxes and panoptic segmenta-\ntion. There are 7 instances per image on average, up to 63 instances in a single\nimage in training set, ranging from small to large on the same images. If not\nspecified, we report AP as bbox AP, the integral metric over multiple thresholds.\nFor comparison with Faster R-CNN we report validation AP at the last training\nepoch, for ablations we report median over validation results from the last 10\nepochs."
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 1384
                },
                {
                    "x": 1590,
                    "y": 1384
                },
                {
                    "x": 1590,
                    "y": 2084
                },
                {
                    "x": 134,
                    "y": 2084
                }
            ],
            "category": "paragraph",
            "html": "<p id='71' style='font-size:18px'>Technical details. We train DETR with Adam W [26] setting the initial trans-<br>former's learning rate to 10-4, the backbone's to 10-5 and weight decay to 10-4.<br>,<br>All transformer weights are initialized with Xavier init [11], and the backbone<br>is with ImageNet-pretrained ResNet model [15] from TORCHVISION with frozen<br>batchnorm layers. We report results with two different backbones: a ResNet-<br>50 and a ResNet-101. The corresponding models are called respectively DETR<br>and DETR-R101. Following [21], we also increase the feature resolution by<br>adding a dilation to the last stage of the backbone and removing a stride from<br>the first convolution of this stage. The corresponding models are called respec-<br>tively DETR-DC5 and DETR-DC5-R101 (dilated C5 stage). This modification<br>increases the resolution by a factor of two, thus improving performance for small<br>objects, at the cost of a 16x higher cost in the self-attentions of the encoder,<br>leading to an overall 2x increase in computational cost. A full comparison of<br>FLOPs of these models and Faster R-CNN is given in Table 1.</p>",
            "id": 71,
            "page": 8,
            "text": "Technical details. We train DETR with Adam W [26] setting the initial trans-\nformer's learning rate to 10-4, the backbone's to 10-5 and weight decay to 10-4.\n,\nAll transformer weights are initialized with Xavier init [11], and the backbone\nis with ImageNet-pretrained ResNet model [15] from TORCHVISION with frozen\nbatchnorm layers. We report results with two different backbones: a ResNet-\n50 and a ResNet-101. The corresponding models are called respectively DETR\nand DETR-R101. Following [21], we also increase the feature resolution by\nadding a dilation to the last stage of the backbone and removing a stride from\nthe first convolution of this stage. The corresponding models are called respec-\ntively DETR-DC5 and DETR-DC5-R101 (dilated C5 stage). This modification\nincreases the resolution by a factor of two, thus improving performance for small\nobjects, at the cost of a 16x higher cost in the self-attentions of the encoder,\nleading to an overall 2x increase in computational cost. A full comparison of\nFLOPs of these models and Faster R-CNN is given in Table 1."
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 2088
                },
                {
                    "x": 1591,
                    "y": 2088
                },
                {
                    "x": 1591,
                    "y": 2434
                },
                {
                    "x": 134,
                    "y": 2434
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='72' style='font-size:20px'>We use scale augmentation, resizing the input images such that the shortest<br>side is at least 480 and at most 800 pixels while the longest at most 1333 [50].<br>To help learning global relationships through the self-attention of the encoder,<br>we also apply random crop augmentations during training, improving the per-<br>formance by approximately 1 AP. Specifically, a train image is cropped with<br>probability 0.5 to a random rectangular patch which is then resized again to<br>800-1333. The transformer is trained with default dropout of 0.1. At inference</p>",
            "id": 72,
            "page": 8,
            "text": "We use scale augmentation, resizing the input images such that the shortest\nside is at least 480 and at most 800 pixels while the longest at most 1333 [50].\nTo help learning global relationships through the self-attention of the encoder,\nwe also apply random crop augmentations during training, improving the per-\nformance by approximately 1 AP. Specifically, a train image is cropped with\nprobability 0.5 to a random rectangular patch which is then resized again to\n800-1333. The transformer is trained with default dropout of 0.1. At inference"
        },
        {
            "bounding_box": [
                {
                    "x": 620,
                    "y": 42
                },
                {
                    "x": 1578,
                    "y": 42
                },
                {
                    "x": 1578,
                    "y": 86
                },
                {
                    "x": 620,
                    "y": 86
                }
            ],
            "category": "header",
            "html": "<header id='73' style='font-size:14px'>End-to-End Object Detection with Transformers 9</header>",
            "id": 73,
            "page": 9,
            "text": "End-to-End Object Detection with Transformers 9"
        },
        {
            "bounding_box": [
                {
                    "x": 136,
                    "y": 179
                },
                {
                    "x": 1591,
                    "y": 179
                },
                {
                    "x": 1591,
                    "y": 544
                },
                {
                    "x": 136,
                    "y": 544
                }
            ],
            "category": "paragraph",
            "html": "<p id='74' style='font-size:14px'>Table 1: Comparison with Faster R-CNN with a ResNet-50 and ResNet-101 backbones<br>on the COCO validation set. The top section shows results for Faster R-CNN models<br>in Detectron2 [50], the middle section shows results for Faster R-CNN models with<br>GIoU [38], random crops train-time augmentation, and the long 9x training schedule.<br>DETR models achieve comparable results to heavily tuned Faster R-CNN baselines,<br>having lower APs but greatly improved APL. We use torchscript Faster R-CNN and<br>DETR models to measure FLOPS and FPS. Results without R101 in the name corre-<br>spond to ResNet-50.</p>",
            "id": 74,
            "page": 9,
            "text": "Table 1: Comparison with Faster R-CNN with a ResNet-50 and ResNet-101 backbones\non the COCO validation set. The top section shows results for Faster R-CNN models\nin Detectron2 [50], the middle section shows results for Faster R-CNN models with\nGIoU [38], random crops train-time augmentation, and the long 9x training schedule.\nDETR models achieve comparable results to heavily tuned Faster R-CNN baselines,\nhaving lower APs but greatly improved APL. We use torchscript Faster R-CNN and\nDETR models to measure FLOPS and FPS. Results without R101 in the name corre-\nspond to ResNet-50."
        },
        {
            "bounding_box": [
                {
                    "x": 136,
                    "y": 548
                },
                {
                    "x": 1598,
                    "y": 548
                },
                {
                    "x": 1598,
                    "y": 1141
                },
                {
                    "x": 136,
                    "y": 1141
                }
            ],
            "category": "table",
            "html": "<br><table id='75' style='font-size:16px'><tr><td>Model</td><td>GFLOPS/FPS</td><td>#params</td><td>AP</td><td>AP50</td><td>AP75</td><td>APs</td><td>APM</td><td>APL</td></tr><tr><td>Faster RCNN-DC5</td><td>320 /16</td><td>166M</td><td>39.0</td><td>60.5</td><td>42.3</td><td>21.4</td><td>43.5</td><td>52.5</td></tr><tr><td>Faster RCNN-FPN</td><td>180 / 26</td><td>42M</td><td>40.2</td><td>61.0</td><td>43.8</td><td>24.2</td><td>43.5</td><td>52.0</td></tr><tr><td>Faster RCNN-R101-FPN</td><td>246 /20</td><td>60M</td><td>42.0</td><td>62.5</td><td>45.9</td><td>25.2</td><td>45.6</td><td>54.6</td></tr><tr><td>Faster RCNN-DC5+</td><td>320 /16</td><td>166M</td><td>41.1</td><td>61.4</td><td>44.3</td><td>22.9</td><td>45.9</td><td>55.0</td></tr><tr><td>Faster RCNN-FPN+</td><td>180 / 26</td><td>42M</td><td>42.0</td><td>62.1</td><td>45.5</td><td>26.6</td><td>45.4</td><td>53.4</td></tr><tr><td>Faster RCNN-R101-FPN+</td><td>246 /20</td><td>60M</td><td>44.0</td><td>63.9</td><td>47.8</td><td>27.2</td><td>48.1</td><td>56.0</td></tr><tr><td>DETR</td><td>86/28</td><td>41M</td><td>42.0</td><td>62.4</td><td>44.2</td><td>20.5</td><td>45.8</td><td>61.1</td></tr><tr><td>DETR-DC5</td><td>187 /12</td><td>41M</td><td>43.3</td><td>63.1</td><td>45.9</td><td>22.5</td><td>47.3</td><td>61.1</td></tr><tr><td>DETR-R101</td><td>152 /20</td><td>60M</td><td>43.5</td><td>63.8</td><td>46.4</td><td>21.9</td><td>48.0</td><td>61.8</td></tr><tr><td>DETR-DC5-R101</td><td>253 /10</td><td>60M</td><td>44.9</td><td>64.7</td><td>47.7</td><td>23.7</td><td>49.5</td><td>62.3</td></tr></table>",
            "id": 75,
            "page": 9,
            "text": "Model GFLOPS/FPS #params AP AP50 AP75 APs APM APL\n Faster RCNN-DC5 320 /16 166M 39.0 60.5 42.3 21.4 43.5 52.5\n Faster RCNN-FPN 180 / 26 42M 40.2 61.0 43.8 24.2 43.5 52.0\n Faster RCNN-R101-FPN 246 /20 60M 42.0 62.5 45.9 25.2 45.6 54.6\n Faster RCNN-DC5+ 320 /16 166M 41.1 61.4 44.3 22.9 45.9 55.0\n Faster RCNN-FPN+ 180 / 26 42M 42.0 62.1 45.5 26.6 45.4 53.4\n Faster RCNN-R101-FPN+ 246 /20 60M 44.0 63.9 47.8 27.2 48.1 56.0\n DETR 86/28 41M 42.0 62.4 44.2 20.5 45.8 61.1\n DETR-DC5 187 /12 41M 43.3 63.1 45.9 22.5 47.3 61.1\n DETR-R101 152 /20 60M 43.5 63.8 46.4 21.9 48.0 61.8\n DETR-DC5-R101 253 /10 60M 44.9 64.7 47.7 23.7 49.5"
        },
        {
            "bounding_box": [
                {
                    "x": 135,
                    "y": 1240
                },
                {
                    "x": 1593,
                    "y": 1240
                },
                {
                    "x": 1593,
                    "y": 1785
                },
                {
                    "x": 135,
                    "y": 1785
                }
            ],
            "category": "paragraph",
            "html": "<p id='76' style='font-size:18px'>time, some slots predict empty class. To optimize for AP, we override the predic-<br>tion of these slots with the second highest scoring class, using the corresponding<br>confidence. This improves AP by 2 points compared to filtering out empty slots.<br>Other training hyperparameters can be found in section A.4. For our ablation<br>experiments we use training schedule of 300 epochs with a learning rate drop<br>by a factor of 10 after 200 epochs, where a single epoch is a pass over all train-<br>ing images once. Training the baseline model for 300 epochs on 16 V100 GPUs<br>takes 3 days, with 4 images per GPU (hence a total batch size of 64). For the<br>longer schedule used to compare with Faster R-CNN we train for 500 epochs<br>with learning rate drop after 400 epochs. This schedule adds 1.5 AP compared<br>to the shorter schedule.</p>",
            "id": 76,
            "page": 9,
            "text": "time, some slots predict empty class. To optimize for AP, we override the predic-\ntion of these slots with the second highest scoring class, using the corresponding\nconfidence. This improves AP by 2 points compared to filtering out empty slots.\nOther training hyperparameters can be found in section A.4. For our ablation\nexperiments we use training schedule of 300 epochs with a learning rate drop\nby a factor of 10 after 200 epochs, where a single epoch is a pass over all train-\ning images once. Training the baseline model for 300 epochs on 16 V100 GPUs\ntakes 3 days, with 4 images per GPU (hence a total batch size of 64). For the\nlonger schedule used to compare with Faster R-CNN we train for 500 epochs\nwith learning rate drop after 400 epochs. This schedule adds 1.5 AP compared\nto the shorter schedule."
        },
        {
            "bounding_box": [
                {
                    "x": 135,
                    "y": 1857
                },
                {
                    "x": 931,
                    "y": 1857
                },
                {
                    "x": 931,
                    "y": 1905
                },
                {
                    "x": 135,
                    "y": 1905
                }
            ],
            "category": "paragraph",
            "html": "<p id='77' style='font-size:20px'>4.1 Comparison with Faster R-CNN</p>",
            "id": 77,
            "page": 9,
            "text": "4.1 Comparison with Faster R-CNN"
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 1936
                },
                {
                    "x": 1592,
                    "y": 1936
                },
                {
                    "x": 1592,
                    "y": 2435
                },
                {
                    "x": 134,
                    "y": 2435
                }
            ],
            "category": "paragraph",
            "html": "<p id='78' style='font-size:16px'>Transformers are typically trained with Adam or Adagrad optimizers with very<br>long training schedules and dropout, and this is true for DETR as well. Faster<br>R-CNN, however, is trained with SGD with minimal data augmentation and<br>we are not aware of successful applications of Adam or dropout. Despite these<br>differences we attempt to make a Faster R-CNN baseline stronger. To align it<br>with DETR, we add generalized IoU [38] to the box loss, the same random<br>crop augmentation and long training known to improve results [13]. Results<br>are presented in Table 1. In the top section we show Faster R-CNN results<br>from Detectron2 Model Zoo [50] for models trained with the 3x schedule. In the<br>middle section we show results (with a \"+\") for the same models but trained</p>",
            "id": 78,
            "page": 9,
            "text": "Transformers are typically trained with Adam or Adagrad optimizers with very\nlong training schedules and dropout, and this is true for DETR as well. Faster\nR-CNN, however, is trained with SGD with minimal data augmentation and\nwe are not aware of successful applications of Adam or dropout. Despite these\ndifferences we attempt to make a Faster R-CNN baseline stronger. To align it\nwith DETR, we add generalized IoU [38] to the box loss, the same random\ncrop augmentation and long training known to improve results [13]. Results\nare presented in Table 1. In the top section we show Faster R-CNN results\nfrom Detectron2 Model Zoo [50] for models trained with the 3x schedule. In the\nmiddle section we show results (with a \"+\") for the same models but trained"
        },
        {
            "bounding_box": [
                {
                    "x": 137,
                    "y": 45
                },
                {
                    "x": 494,
                    "y": 45
                },
                {
                    "x": 494,
                    "y": 81
                },
                {
                    "x": 137,
                    "y": 81
                }
            ],
            "category": "header",
            "html": "<header id='79' style='font-size:14px'>10 Carion et al.</header>",
            "id": 79,
            "page": 10,
            "text": "10 Carion et al."
        },
        {
            "bounding_box": [
                {
                    "x": 135,
                    "y": 180
                },
                {
                    "x": 1590,
                    "y": 180
                },
                {
                    "x": 1590,
                    "y": 317
                },
                {
                    "x": 135,
                    "y": 317
                }
            ],
            "category": "caption",
            "html": "<caption id='80' style='font-size:14px'>Table 2: Effect of encoder size. Each row corresponds to a model with varied number<br>of encoder layers and fixed number of decoder layers. Performance gradually improves<br>with more encoder layers.</caption>",
            "id": 80,
            "page": 10,
            "text": "Table 2: Effect of encoder size. Each row corresponds to a model with varied number\nof encoder layers and fixed number of decoder layers. Performance gradually improves\nwith more encoder layers."
        },
        {
            "bounding_box": [
                {
                    "x": 137,
                    "y": 319
                },
                {
                    "x": 1584,
                    "y": 319
                },
                {
                    "x": 1584,
                    "y": 598
                },
                {
                    "x": 137,
                    "y": 598
                }
            ],
            "category": "table",
            "html": "<br><table id='81' style='font-size:16px'><tr><td>#layers</td><td>GFLOPS/ FPS</td><td>#params</td><td>AP</td><td>AP50</td><td>APs</td><td>APM</td><td>APL</td></tr><tr><td>0</td><td>76/28</td><td>33.4M</td><td>36.7</td><td>57.4</td><td>16.8</td><td>39.6</td><td>54.2</td></tr><tr><td>3</td><td>81 /25</td><td>37.4M</td><td>40.1</td><td>60.6</td><td>18.5</td><td>43.8</td><td>58.6</td></tr><tr><td>6</td><td>86 / 23</td><td>41.3M</td><td>40.6</td><td>61.6</td><td>19.9</td><td>44.3</td><td>60.2</td></tr><tr><td>12</td><td>95 / 20</td><td>49.2M</td><td>41.6</td><td>62.1</td><td>19.8</td><td>44.9</td><td>61.9</td></tr></table>",
            "id": 81,
            "page": 10,
            "text": "#layers GFLOPS/ FPS #params AP AP50 APs APM APL\n 0 76/28 33.4M 36.7 57.4 16.8 39.6 54.2\n 3 81 /25 37.4M 40.1 60.6 18.5 43.8 58.6\n 6 86 / 23 41.3M 40.6 61.6 19.9 44.3 60.2\n 12 95 / 20 49.2M 41.6 62.1 19.8 44.9"
        },
        {
            "bounding_box": [
                {
                    "x": 137,
                    "y": 695
                },
                {
                    "x": 1589,
                    "y": 695
                },
                {
                    "x": 1589,
                    "y": 1393
                },
                {
                    "x": 137,
                    "y": 1393
                }
            ],
            "category": "paragraph",
            "html": "<p id='82' style='font-size:18px'>with the 9x schedule (109 epochs) and the described enhancements, which in<br>total adds 1-2 AP. In the last section of Table 1 we show the results for multiple<br>DETR models. To be comparable in the number of parameters we choose a<br>model with 6 transformer and 6 decoder layers of width 256 with 8 attention<br>heads. Like Faster R-CNN with FPN this model has 41.3M parameters, out of<br>which 23.5M are in ResNet-50, and 17.8M are in the transformer. Even though<br>both Faster R-CNN and DETR are still likely to further improve with longer<br>training, we can conclude that DETR can be competitive with Faster R-CNN<br>with the same number of parameters, achieving 42 AP on the COCO val subset.<br>The way DETR achieves this is by improving APL (+7.8), however note that the<br>model is still lagging behind in APs (-5.5). DETR-DC5 with the same number<br>of parameters and similar FLOP count has higher AP, but is still significantly<br>behind in APs too. Faster R-CNN and DETR with ResNet-101 backbone show<br>comparable results as well.</p>",
            "id": 82,
            "page": 10,
            "text": "with the 9x schedule (109 epochs) and the described enhancements, which in\ntotal adds 1-2 AP. In the last section of Table 1 we show the results for multiple\nDETR models. To be comparable in the number of parameters we choose a\nmodel with 6 transformer and 6 decoder layers of width 256 with 8 attention\nheads. Like Faster R-CNN with FPN this model has 41.3M parameters, out of\nwhich 23.5M are in ResNet-50, and 17.8M are in the transformer. Even though\nboth Faster R-CNN and DETR are still likely to further improve with longer\ntraining, we can conclude that DETR can be competitive with Faster R-CNN\nwith the same number of parameters, achieving 42 AP on the COCO val subset.\nThe way DETR achieves this is by improving APL (+7.8), however note that the\nmodel is still lagging behind in APs (-5.5). DETR-DC5 with the same number\nof parameters and similar FLOP count has higher AP, but is still significantly\nbehind in APs too. Faster R-CNN and DETR with ResNet-101 backbone show\ncomparable results as well."
        },
        {
            "bounding_box": [
                {
                    "x": 136,
                    "y": 1466
                },
                {
                    "x": 455,
                    "y": 1466
                },
                {
                    "x": 455,
                    "y": 1511
                },
                {
                    "x": 136,
                    "y": 1511
                }
            ],
            "category": "paragraph",
            "html": "<p id='83' style='font-size:18px'>4.2 Ablations</p>",
            "id": 83,
            "page": 10,
            "text": "4.2 Ablations"
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 1546
                },
                {
                    "x": 1590,
                    "y": 1546
                },
                {
                    "x": 1590,
                    "y": 1895
                },
                {
                    "x": 134,
                    "y": 1895
                }
            ],
            "category": "paragraph",
            "html": "<p id='84' style='font-size:16px'>Attention mechanisms in the transformer decoder are the key components which<br>model relations between feature representations of different detections. In our<br>ablation analysis, we explore how other components of our architecture and loss<br>influence the final performance. For the study we choose ResNet-50-based DETR<br>model with 6 encoder, 6 decoder layers and width 256. The model has 41.3M<br>parameters, achieves 40.6 and 42.0 AP on short and long schedules respectively,<br>and runs at 28 FPS, similarly to Faster R-CNN-FPN with the same backbone.</p>",
            "id": 84,
            "page": 10,
            "text": "Attention mechanisms in the transformer decoder are the key components which\nmodel relations between feature representations of different detections. In our\nablation analysis, we explore how other components of our architecture and loss\ninfluence the final performance. For the study we choose ResNet-50-based DETR\nmodel with 6 encoder, 6 decoder layers and width 256. The model has 41.3M\nparameters, achieves 40.6 and 42.0 AP on short and long schedules respectively,\nand runs at 28 FPS, similarly to Faster R-CNN-FPN with the same backbone."
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 1914
                },
                {
                    "x": 1590,
                    "y": 1914
                },
                {
                    "x": 1590,
                    "y": 2315
                },
                {
                    "x": 134,
                    "y": 2315
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='85' style='font-size:20px'>Number of encoder layers. We evaluate the importance of global image-<br>level self-attention by changing the number of encoder layers (Table 2). Without<br>encoder layers, overall AP drops by 3.9 points, with a more significant drop of<br>6.0 AP on large objects. We hypothesize that, by using global scene reasoning,<br>the encoder is important for disentangling objects. In Figure 3, we visualize the<br>attention maps of the last encoder layer of a trained model, focusing on a few<br>points in the image. The encoder seems to separate instances already, which<br>likely simplifies object extraction and localization for the decoder.</p>",
            "id": 85,
            "page": 10,
            "text": "Number of encoder layers. We evaluate the importance of global image-\nlevel self-attention by changing the number of encoder layers (Table 2). Without\nencoder layers, overall AP drops by 3.9 points, with a more significant drop of\n6.0 AP on large objects. We hypothesize that, by using global scene reasoning,\nthe encoder is important for disentangling objects. In Figure 3, we visualize the\nattention maps of the last encoder layer of a trained model, focusing on a few\npoints in the image. The encoder seems to separate instances already, which\nlikely simplifies object extraction and localization for the decoder."
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 2334
                },
                {
                    "x": 1589,
                    "y": 2334
                },
                {
                    "x": 1589,
                    "y": 2433
                },
                {
                    "x": 134,
                    "y": 2433
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='86' style='font-size:22px'>Number of decoder layers. We apply auxiliary losses after each decoding<br>layer (see Section 3.2), hence, the prediction FFNs are trained by design to pre-</p>",
            "id": 86,
            "page": 10,
            "text": "Number of decoder layers. We apply auxiliary losses after each decoding\nlayer (see Section 3.2), hence, the prediction FFNs are trained by design to pre-"
        },
        {
            "bounding_box": [
                {
                    "x": 619,
                    "y": 41
                },
                {
                    "x": 1575,
                    "y": 41
                },
                {
                    "x": 1575,
                    "y": 87
                },
                {
                    "x": 619,
                    "y": 87
                }
            ],
            "category": "header",
            "html": "<header id='87' style='font-size:18px'>End-to-End Object Detection with Transformers 11</header>",
            "id": 87,
            "page": 11,
            "text": "End-to-End Object Detection with Transformers 11"
        },
        {
            "bounding_box": [
                {
                    "x": 145,
                    "y": 136
                },
                {
                    "x": 1580,
                    "y": 136
                },
                {
                    "x": 1580,
                    "y": 641
                },
                {
                    "x": 145,
                    "y": 641
                }
            ],
            "category": "figure",
            "html": "<figure><img id='88' style='font-size:14px' alt=\"self-attention(430, 600) self-attention(450, 830)\nself-attention(520, 450) self-attention(440, 1200)\" data-coord=\"top-left:(145,136); bottom-right:(1580,641)\" /></figure>",
            "id": 88,
            "page": 11,
            "text": "self-attention(430, 600) self-attention(450, 830)\nself-attention(520, 450) self-attention(440, 1200)"
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 681
                },
                {
                    "x": 1590,
                    "y": 681
                },
                {
                    "x": 1590,
                    "y": 823
                },
                {
                    "x": 134,
                    "y": 823
                }
            ],
            "category": "caption",
            "html": "<caption id='89' style='font-size:16px'>Fig. 3: Encoder self-attention for a set of reference points. The encoder is able to sep-<br>arate individual instances. Predictions are made with baseline DETR model on a vali-<br>dation set image.</caption>",
            "id": 89,
            "page": 11,
            "text": "Fig. 3: Encoder self-attention for a set of reference points. The encoder is able to sep-\narate individual instances. Predictions are made with baseline DETR model on a vali-\ndation set image."
        },
        {
            "bounding_box": [
                {
                    "x": 135,
                    "y": 923
                },
                {
                    "x": 1589,
                    "y": 923
                },
                {
                    "x": 1589,
                    "y": 1673
                },
                {
                    "x": 135,
                    "y": 1673
                }
            ],
            "category": "paragraph",
            "html": "<p id='90' style='font-size:20px'>dict objects out of the outputs of every decoder layer. We analyze the importance<br>of each decoder layer by evaluating the objects that would be predicted at each<br>stage of the decoding (Fig. 4). Both AP and AP50 improve after every layer,<br>totalling into a very significant +8.2/9.5 AP improvement between the first and<br>the last layer. With its set-based loss, DETR does not need NMS by design. To<br>verify this we run a standard NMS procedure with default parameters [50] for<br>the outputs after each decoder. NMS improves performance for the predictions<br>from the first decoder. This can be explained by the fact that a single decoding<br>layer of the transformer is not able to compute any cross-correlations between<br>the output elements, and thus it is prone to making multiple predictions for the<br>same object. In the second and subsequent layers, the self-attention mechanism<br>over the activations allows the model to inhibit duplicate predictions. We ob-<br>serve that the improvement brought by NMS diminishes as depth increases. At<br>the last layers, we observe a small loss in AP as NMS incorrectly removes true<br>positive predictions.</p>",
            "id": 90,
            "page": 11,
            "text": "dict objects out of the outputs of every decoder layer. We analyze the importance\nof each decoder layer by evaluating the objects that would be predicted at each\nstage of the decoding (Fig. 4). Both AP and AP50 improve after every layer,\ntotalling into a very significant +8.2/9.5 AP improvement between the first and\nthe last layer. With its set-based loss, DETR does not need NMS by design. To\nverify this we run a standard NMS procedure with default parameters [50] for\nthe outputs after each decoder. NMS improves performance for the predictions\nfrom the first decoder. This can be explained by the fact that a single decoding\nlayer of the transformer is not able to compute any cross-correlations between\nthe output elements, and thus it is prone to making multiple predictions for the\nsame object. In the second and subsequent layers, the self-attention mechanism\nover the activations allows the model to inhibit duplicate predictions. We ob-\nserve that the improvement brought by NMS diminishes as depth increases. At\nthe last layers, we observe a small loss in AP as NMS incorrectly removes true\npositive predictions."
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 1675
                },
                {
                    "x": 1591,
                    "y": 1675
                },
                {
                    "x": 1591,
                    "y": 1972
                },
                {
                    "x": 134,
                    "y": 1972
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='91' style='font-size:22px'>Similarly to visualizing encoder attention, we visualize decoder attentions in<br>Fig. 6, coloring attention maps for each predicted object in different colors. We<br>observe that decoder attention is fairly local, meaning that it mostly attends to<br>object extremities such as heads or legs. We hypothesise that after the encoder<br>has separated instances via global attention, the decoder only needs to attend<br>to the extremities to extract the class and object boundaries.</p>",
            "id": 91,
            "page": 11,
            "text": "Similarly to visualizing encoder attention, we visualize decoder attentions in\nFig. 6, coloring attention maps for each predicted object in different colors. We\nobserve that decoder attention is fairly local, meaning that it mostly attends to\nobject extremities such as heads or legs. We hypothesise that after the encoder\nhas separated instances via global attention, the decoder only needs to attend\nto the extremities to extract the class and object boundaries."
        },
        {
            "bounding_box": [
                {
                    "x": 133,
                    "y": 2002
                },
                {
                    "x": 1590,
                    "y": 2002
                },
                {
                    "x": 1590,
                    "y": 2306
                },
                {
                    "x": 133,
                    "y": 2306
                }
            ],
            "category": "paragraph",
            "html": "<p id='92' style='font-size:20px'>Importance of FFN. FFN inside tranformers can be seen as 1 x 1 convo-<br>lutional layers, making encoder similar to attention augmented convolutional<br>networks [3]. We attempt to remove it completely leaving only attention in the<br>transformer layers. By reducing the number of network parameters from 41.3M<br>to 28.7M, leaving only 10.8M in the transformer, performance drops by 2.3 AP,<br>we thus conclude that FFN are important for achieving good results.</p>",
            "id": 92,
            "page": 11,
            "text": "Importance of FFN. FFN inside tranformers can be seen as 1 x 1 convo-\nlutional layers, making encoder similar to attention augmented convolutional\nnetworks [3]. We attempt to remove it completely leaving only attention in the\ntransformer layers. By reducing the number of network parameters from 41.3M\nto 28.7M, leaving only 10.8M in the transformer, performance drops by 2.3 AP,\nwe thus conclude that FFN are important for achieving good results."
        },
        {
            "bounding_box": [
                {
                    "x": 133,
                    "y": 2333
                },
                {
                    "x": 1589,
                    "y": 2333
                },
                {
                    "x": 1589,
                    "y": 2433
                },
                {
                    "x": 133,
                    "y": 2433
                }
            ],
            "category": "paragraph",
            "html": "<p id='93' style='font-size:20px'>Importance of positional encodings. There are two kinds of positional en-<br>codings in our model: spatial positional encodings and output positional encod-</p>",
            "id": 93,
            "page": 11,
            "text": "Importance of positional encodings. There are two kinds of positional en-\ncodings in our model: spatial positional encodings and output positional encod-"
        },
        {
            "bounding_box": [
                {
                    "x": 135,
                    "y": 44
                },
                {
                    "x": 494,
                    "y": 44
                },
                {
                    "x": 494,
                    "y": 82
                },
                {
                    "x": 135,
                    "y": 82
                }
            ],
            "category": "header",
            "html": "<header id='94' style='font-size:16px'>12 Carion et al.</header>",
            "id": 94,
            "page": 12,
            "text": "12 Carion et al."
        },
        {
            "bounding_box": [
                {
                    "x": 248,
                    "y": 128
                },
                {
                    "x": 941,
                    "y": 128
                },
                {
                    "x": 941,
                    "y": 636
                },
                {
                    "x": 248,
                    "y": 636
                }
            ],
            "category": "figure",
            "html": "<figure><img id='95' style='font-size:14px' alt=\"62\n42 60\n58\n40\n0g\nAP 56 AP\n38\n54\nAP No NMS\n36 - ◆- AP NMS=0.7\nAP50 No NMS\nAP50 NMS=0.7\n34\n1 2 3 4 5 6\ndecoder layer\" data-coord=\"top-left:(248,128); bottom-right:(941,636)\" /></figure>",
            "id": 95,
            "page": 12,
            "text": "62\n42 60\n58\n40\n0g\nAP 56 AP\n38\n54\nAP No NMS\n36 - ◆- AP NMS=0.7\nAP50 No NMS\nAP50 NMS=0.7\n34\n1 2 3 4 5 6\ndecoder layer"
        },
        {
            "bounding_box": [
                {
                    "x": 166,
                    "y": 678
                },
                {
                    "x": 1005,
                    "y": 678
                },
                {
                    "x": 1005,
                    "y": 1094
                },
                {
                    "x": 166,
                    "y": 1094
                }
            ],
            "category": "caption",
            "html": "<caption id='96' style='font-size:18px'>Fig. 4: AP and AP50 performance after each de-<br>coder layer. A single long schedule baseline model<br>is evaluated. DETR does not need NMS by de-<br>sign, which is validated by this figure. NMS lowers<br>AP in the final layers, removing TP predictions,<br>but improves AP in the first decoder layers, re-<br>moving double predictions, as there is no commu-<br>nication in the first layer, and slightly improves<br>AP50.</caption>",
            "id": 96,
            "page": 12,
            "text": "Fig. 4: AP and AP50 performance after each de-\ncoder layer. A single long schedule baseline model\nis evaluated. DETR does not need NMS by de-\nsign, which is validated by this figure. NMS lowers\nAP in the final layers, removing TP predictions,\nbut improves AP in the first decoder layers, re-\nmoving double predictions, as there is no commu-\nnication in the first layer, and slightly improves\nAP50."
        },
        {
            "bounding_box": [
                {
                    "x": 1043,
                    "y": 185
                },
                {
                    "x": 1555,
                    "y": 185
                },
                {
                    "x": 1555,
                    "y": 637
                },
                {
                    "x": 1043,
                    "y": 637
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='97' style='font-size:14px' alt=\"giraffe99% giraffe99%\ngiraffe 99% giraffe 100% giraffe 100% giraffe 100% giraffe99%\ngiraffe 95% � 98% giraffe 100% giraffe 99%\ngiraffe.94% giraffe 98% giraffe98% giraffe 96% giratte\" data-coord=\"top-left:(1043,185); bottom-right:(1555,637)\" /></figure>",
            "id": 97,
            "page": 12,
            "text": "giraffe99% giraffe99%\ngiraffe 99% giraffe 100% giraffe 100% giraffe 100% giraffe99%\ngiraffe 95% � 98% giraffe 100% giraffe 99%\ngiraffe.94% giraffe 98% giraffe98% giraffe 96% giratte"
        },
        {
            "bounding_box": [
                {
                    "x": 1040,
                    "y": 704
                },
                {
                    "x": 1556,
                    "y": 704
                },
                {
                    "x": 1556,
                    "y": 1067
                },
                {
                    "x": 1040,
                    "y": 1067
                }
            ],
            "category": "paragraph",
            "html": "<p id='98' style='font-size:16px'>Fig. 5: Out of distribution gen-<br>eralization for rare classes.<br>Even though no image in the<br>training set has more than 13<br>giraffes, DETR has no diffi-<br>culty generalizing to 24 and<br>more instances of the same<br>class.</p>",
            "id": 98,
            "page": 12,
            "text": "Fig. 5: Out of distribution gen-\neralization for rare classes.\nEven though no image in the\ntraining set has more than 13\ngiraffes, DETR has no diffi-\nculty generalizing to 24 and\nmore instances of the same\nclass."
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 1196
                },
                {
                    "x": 1590,
                    "y": 1196
                },
                {
                    "x": 1590,
                    "y": 1897
                },
                {
                    "x": 134,
                    "y": 1897
                }
            ],
            "category": "paragraph",
            "html": "<p id='99' style='font-size:20px'>ings (object queries). We experiment with various combinations of fixed and<br>learned encodings, results can be found in table 3. Output positional encodings<br>are required and cannot be removed, SO we experiment with either passing them<br>once at decoder input or adding to queries at every decoder attention layer. In<br>the first experiment we completely remove spatial positional encodings and pass<br>output positional encodings at input and, interestingly, the model still achieves<br>more than 32 AP, losing 7.8 AP to the baseline. Then, we pass fixed sine spatial<br>positional encodings and the output encodings at input once, as in the original<br>transformer [47], and find that this leads to 1.4 AP drop compared to passing<br>the positional encodings directly in attention. Learned spatial encodings passed<br>to the attentions give similar results. Surprisingly, we find that not passing any<br>spatial encodings in the encoder only leads to a minor AP drop of 1.3 AP. When<br>we pass the encodings to the attentions, they are shared across all layers, and<br>the output encodings (object queries) are always learned.</p>",
            "id": 99,
            "page": 12,
            "text": "ings (object queries). We experiment with various combinations of fixed and\nlearned encodings, results can be found in table 3. Output positional encodings\nare required and cannot be removed, SO we experiment with either passing them\nonce at decoder input or adding to queries at every decoder attention layer. In\nthe first experiment we completely remove spatial positional encodings and pass\noutput positional encodings at input and, interestingly, the model still achieves\nmore than 32 AP, losing 7.8 AP to the baseline. Then, we pass fixed sine spatial\npositional encodings and the output encodings at input once, as in the original\ntransformer [47], and find that this leads to 1.4 AP drop compared to passing\nthe positional encodings directly in attention. Learned spatial encodings passed\nto the attentions give similar results. Surprisingly, we find that not passing any\nspatial encodings in the encoder only leads to a minor AP drop of 1.3 AP. When\nwe pass the encodings to the attentions, they are shared across all layers, and\nthe output encodings (object queries) are always learned."
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 1902
                },
                {
                    "x": 1588,
                    "y": 1902
                },
                {
                    "x": 1588,
                    "y": 2049
                },
                {
                    "x": 134,
                    "y": 2049
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='100' style='font-size:22px'>Given these ablations, we conclude that transformer components: the global<br>self-attention in encoder, FFN, multiple decoder layers, and positional encodings,<br>all significantly contribute to the final object detection performance.</p>",
            "id": 100,
            "page": 12,
            "text": "Given these ablations, we conclude that transformer components: the global\nself-attention in encoder, FFN, multiple decoder layers, and positional encodings,\nall significantly contribute to the final object detection performance."
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 2083
                },
                {
                    "x": 1590,
                    "y": 2083
                },
                {
                    "x": 1590,
                    "y": 2434
                },
                {
                    "x": 134,
                    "y": 2434
                }
            ],
            "category": "paragraph",
            "html": "<p id='101' style='font-size:18px'>Loss ablations. To evaluate the importance of different components of the<br>matching cost and the loss, we train several models turning them on and off.<br>There are three components to the loss: classification loss, l1 bounding box<br>distance loss, and GIoU [38] loss. The classification loss is essential for training<br>and cannot be turned off, SO we train a model without bounding box distance<br>loss, and a model without the GIoU loss, and compare with baseline, trained with<br>all three losses. Results are presented in table 4. GIoU loss on its own accounts</p>",
            "id": 101,
            "page": 12,
            "text": "Loss ablations. To evaluate the importance of different components of the\nmatching cost and the loss, we train several models turning them on and off.\nThere are three components to the loss: classification loss, l1 bounding box\ndistance loss, and GIoU [38] loss. The classification loss is essential for training\nand cannot be turned off, SO we train a model without bounding box distance\nloss, and a model without the GIoU loss, and compare with baseline, trained with\nall three losses. Results are presented in table 4. GIoU loss on its own accounts"
        },
        {
            "bounding_box": [
                {
                    "x": 620,
                    "y": 41
                },
                {
                    "x": 1586,
                    "y": 41
                },
                {
                    "x": 1586,
                    "y": 86
                },
                {
                    "x": 620,
                    "y": 86
                }
            ],
            "category": "header",
            "html": "<header id='102' style='font-size:20px'>End-to-End Object Detection with Transformers 13</header>",
            "id": 102,
            "page": 13,
            "text": "End-to-End Object Detection with Transformers 13"
        },
        {
            "bounding_box": [
                {
                    "x": 194,
                    "y": 134
                },
                {
                    "x": 1532,
                    "y": 134
                },
                {
                    "x": 1532,
                    "y": 767
                },
                {
                    "x": 194,
                    "y": 767
                }
            ],
            "category": "figure",
            "html": "<figure><img id='103' style='font-size:14px' alt=\"zebra 99%\nzebra\nelephant 100% zebra\nelephant 100%\" data-coord=\"top-left:(194,134); bottom-right:(1532,767)\" /></figure>",
            "id": 103,
            "page": 13,
            "text": "zebra 99%\nzebra\nelephant 100% zebra\nelephant 100%"
        },
        {
            "bounding_box": [
                {
                    "x": 136,
                    "y": 798
                },
                {
                    "x": 1591,
                    "y": 798
                },
                {
                    "x": 1591,
                    "y": 987
                },
                {
                    "x": 136,
                    "y": 987
                }
            ],
            "category": "caption",
            "html": "<caption id='104' style='font-size:18px'>Fig. 6: Visualizing decoder attention for every predicted object (images from COCO<br>val set). Predictions are made with DETR-DC5 model. Attention scores are coded with<br>different colors for different objects. Decoder typically attends to object extremities,<br>such as legs and heads. Best viewed in color.</caption>",
            "id": 104,
            "page": 13,
            "text": "Fig. 6: Visualizing decoder attention for every predicted object (images from COCO\nval set). Predictions are made with DETR-DC5 model. Attention scores are coded with\ndifferent colors for different objects. Decoder typically attends to object extremities,\nsuch as legs and heads. Best viewed in color."
        },
        {
            "bounding_box": [
                {
                    "x": 135,
                    "y": 1095
                },
                {
                    "x": 1589,
                    "y": 1095
                },
                {
                    "x": 1589,
                    "y": 1368
                },
                {
                    "x": 135,
                    "y": 1368
                }
            ],
            "category": "paragraph",
            "html": "<p id='105' style='font-size:20px'>Table 3: Results for different positional encodings compared to the baseline (last row),<br>which has fixed sine pos. encodings passed at every attention layer in both the encoder<br>and the decoder. Learned embeddings are shared between all layers. Not using spatial<br>positional encodings leads to a significant drop in AP. Interestingly, passing them in<br>decoder only leads to a minor AP drop. All these models use learned output positional<br>encodings.</p>",
            "id": 105,
            "page": 13,
            "text": "Table 3: Results for different positional encodings compared to the baseline (last row),\nwhich has fixed sine pos. encodings passed at every attention layer in both the encoder\nand the decoder. Learned embeddings are shared between all layers. Not using spatial\npositional encodings leads to a significant drop in AP. Interestingly, passing them in\ndecoder only leads to a minor AP drop. All these models use learned output positional\nencodings."
        },
        {
            "bounding_box": [
                {
                    "x": 139,
                    "y": 1373
                },
                {
                    "x": 1584,
                    "y": 1373
                },
                {
                    "x": 1584,
                    "y": 1739
                },
                {
                    "x": 139,
                    "y": 1739
                }
            ],
            "category": "table",
            "html": "<br><table id='106' style='font-size:14px'><tr><td>spatial pos. enc. encoder</td><td>decoder</td><td>output pos. enc. decoder</td><td>AP</td><td>AP50</td><td></td></tr><tr><td>none</td><td>none</td><td>learned at input</td><td>32.8 -7.8</td><td>55.2</td><td>-6.5</td></tr><tr><td>sine at input</td><td>sine at input</td><td>learned at input</td><td>39.2 -1.4</td><td>60.0</td><td>-1.6</td></tr><tr><td>learned at attn.</td><td>learned at attn.</td><td>learned at attn.</td><td>39.6 -1.0</td><td>60.7</td><td>-0.9</td></tr><tr><td>none</td><td>sine at attn.</td><td>learned at attn.</td><td>39.3 -1.3</td><td>60.3</td><td>-1.4</td></tr><tr><td>sine at attn.</td><td>sine at attn.</td><td>learned at attn.</td><td>40.6 -</td><td>61.6</td><td>-</td></tr></table>",
            "id": 106,
            "page": 13,
            "text": "spatial pos. enc. encoder decoder output pos. enc. decoder AP AP50 \n none none learned at input 32.8 -7.8 55.2 -6.5\n sine at input sine at input learned at input 39.2 -1.4 60.0 -1.6\n learned at attn. learned at attn. learned at attn. 39.6 -1.0 60.7 -0.9\n none sine at attn. learned at attn. 39.3 -1.3 60.3 -1.4\n sine at attn. sine at attn. learned at attn. 40.6 - 61.6"
        },
        {
            "bounding_box": [
                {
                    "x": 135,
                    "y": 1843
                },
                {
                    "x": 1589,
                    "y": 1843
                },
                {
                    "x": 1589,
                    "y": 1984
                },
                {
                    "x": 135,
                    "y": 1984
                }
            ],
            "category": "caption",
            "html": "<caption id='107' style='font-size:18px'>Table 4: Effect of loss components on AP. We train two models turning off l1 loss, and<br>GIoU loss, and observe that l1 gives poor results on its own, but when combined with<br>GIoU improves APM and APL. Our baseline (last row) combines both losses.</caption>",
            "id": 107,
            "page": 13,
            "text": "Table 4: Effect of loss components on AP. We train two models turning off l1 loss, and\nGIoU loss, and observe that l1 gives poor results on its own, but when combined with\nGIoU improves APM and APL. Our baseline (last row) combines both losses."
        },
        {
            "bounding_box": [
                {
                    "x": 152,
                    "y": 1985
                },
                {
                    "x": 1554,
                    "y": 1985
                },
                {
                    "x": 1554,
                    "y": 2219
                },
                {
                    "x": 152,
                    "y": 2219
                }
            ],
            "category": "table",
            "html": "<br><table id='108' style='font-size:16px'><tr><td>class</td><td>l1</td><td>GIoU</td><td>AP</td><td>△</td><td>AP50</td><td>△</td><td>I APs</td><td>APM</td><td>APL</td></tr><tr><td></td><td></td><td></td><td>35.8</td><td>-4.8</td><td>57.3</td><td>-4.4</td><td>13.7</td><td>39.8</td><td>57.9</td></tr><tr><td></td><td></td><td></td><td>39.9</td><td>-0.7</td><td>61.6</td><td>0</td><td>19.9</td><td>43.2</td><td>57.9</td></tr><tr><td></td><td></td><td></td><td>40.6</td><td>-</td><td>61.6</td><td>-</td><td>19.9</td><td>44.3</td><td>60.2</td></tr></table>",
            "id": 108,
            "page": 13,
            "text": "class l1 GIoU AP △ AP50 △ I APs APM APL\n    35.8 -4.8 57.3 -4.4 13.7 39.8 57.9\n    39.9 -0.7 61.6 0 19.9 43.2 57.9\n    40.6 - 61.6 - 19.9 44.3"
        },
        {
            "bounding_box": [
                {
                    "x": 135,
                    "y": 2334
                },
                {
                    "x": 1589,
                    "y": 2334
                },
                {
                    "x": 1589,
                    "y": 2434
                },
                {
                    "x": 135,
                    "y": 2434
                }
            ],
            "category": "paragraph",
            "html": "<p id='109' style='font-size:22px'>for most of the model performance, losing only 0.7 AP to the baseline with<br>combined losses. Using l1 without GIoU shows poor results. We only studied</p>",
            "id": 109,
            "page": 13,
            "text": "for most of the model performance, losing only 0.7 AP to the baseline with\ncombined losses. Using l1 without GIoU shows poor results. We only studied"
        },
        {
            "bounding_box": [
                {
                    "x": 136,
                    "y": 43
                },
                {
                    "x": 495,
                    "y": 43
                },
                {
                    "x": 495,
                    "y": 83
                },
                {
                    "x": 136,
                    "y": 83
                }
            ],
            "category": "header",
            "html": "<header id='110' style='font-size:14px'>14 Carion et al.</header>",
            "id": 110,
            "page": 14,
            "text": "14 Carion et al."
        },
        {
            "bounding_box": [
                {
                    "x": 142,
                    "y": 139
                },
                {
                    "x": 1585,
                    "y": 139
                },
                {
                    "x": 1585,
                    "y": 418
                },
                {
                    "x": 142,
                    "y": 418
                }
            ],
            "category": "figure",
            "html": "<figure><img id='111' alt=\"\" data-coord=\"top-left:(142,139); bottom-right:(1585,418)\" /></figure>",
            "id": 111,
            "page": 14,
            "text": ""
        },
        {
            "bounding_box": [
                {
                    "x": 135,
                    "y": 460
                },
                {
                    "x": 1590,
                    "y": 460
                },
                {
                    "x": 1590,
                    "y": 830
                },
                {
                    "x": 135,
                    "y": 830
                }
            ],
            "category": "paragraph",
            "html": "<p id='112' style='font-size:14px'>Fig. 7: Visualization of all box predictions on all images from COCO 2017 val set<br>for 20 out of total N = 100 prediction slots in DETR decoder. Each box prediction is<br>represented as a point with the coordinates of its center in the 1-by-1 square normalized<br>by each image size. The points are color-coded SO that green color corresponds to small<br>boxes, red to large horizontal boxes and blue to large vertical boxes. We observe that<br>each slot learns to specialize on certain areas and box sizes with several operating<br>modes. We note that almost all slots have a mode of predicting large image-wide boxes<br>that are common in COCO dataset.</p>",
            "id": 112,
            "page": 14,
            "text": "Fig. 7: Visualization of all box predictions on all images from COCO 2017 val set\nfor 20 out of total N = 100 prediction slots in DETR decoder. Each box prediction is\nrepresented as a point with the coordinates of its center in the 1-by-1 square normalized\nby each image size. The points are color-coded SO that green color corresponds to small\nboxes, red to large horizontal boxes and blue to large vertical boxes. We observe that\neach slot learns to specialize on certain areas and box sizes with several operating\nmodes. We note that almost all slots have a mode of predicting large image-wide boxes\nthat are common in COCO dataset."
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 934
                },
                {
                    "x": 1587,
                    "y": 934
                },
                {
                    "x": 1587,
                    "y": 1031
                },
                {
                    "x": 134,
                    "y": 1031
                }
            ],
            "category": "paragraph",
            "html": "<p id='113' style='font-size:18px'>simple ablations of different losses (using the same weighting every time), but<br>other means of combining them may achieve different results.</p>",
            "id": 113,
            "page": 14,
            "text": "simple ablations of different losses (using the same weighting every time), but\nother means of combining them may achieve different results."
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 1114
                },
                {
                    "x": 431,
                    "y": 1114
                },
                {
                    "x": 431,
                    "y": 1161
                },
                {
                    "x": 134,
                    "y": 1161
                }
            ],
            "category": "paragraph",
            "html": "<p id='114' style='font-size:20px'>4.3 Analysis</p>",
            "id": 114,
            "page": 14,
            "text": "4.3 Analysis"
        },
        {
            "bounding_box": [
                {
                    "x": 135,
                    "y": 1201
                },
                {
                    "x": 1590,
                    "y": 1201
                },
                {
                    "x": 1590,
                    "y": 1552
                },
                {
                    "x": 135,
                    "y": 1552
                }
            ],
            "category": "paragraph",
            "html": "<p id='115' style='font-size:18px'>Decoder output slot analysis In Fig. 7 we visualize the boxes predicted<br>by different slots for all images in COCO 2017 val set. DETR learns different<br>specialization for each query slot. We observe that each slot has several modes of<br>operation focusing on different areas and box sizes. In particular, all slots have<br>the mode for predicting image-wide boxes (visible as the red dots aligned in the<br>middle of the plot). We hypothesize that this is related to the distribution of<br>objects in COCO.</p>",
            "id": 115,
            "page": 14,
            "text": "Decoder output slot analysis In Fig. 7 we visualize the boxes predicted\nby different slots for all images in COCO 2017 val set. DETR learns different\nspecialization for each query slot. We observe that each slot has several modes of\noperation focusing on different areas and box sizes. In particular, all slots have\nthe mode for predicting image-wide boxes (visible as the red dots aligned in the\nmiddle of the plot). We hypothesize that this is related to the distribution of\nobjects in COCO."
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 1579
                },
                {
                    "x": 1591,
                    "y": 1579
                },
                {
                    "x": 1591,
                    "y": 1931
                },
                {
                    "x": 134,
                    "y": 1931
                }
            ],
            "category": "paragraph",
            "html": "<p id='116' style='font-size:16px'>Generalization to unseen numbers of instances. Some classes in COCO<br>are not well represented with many instances of the same class in the same<br>image. For example, there is no image with more than 13 giraffes in the training<br>set. We create a synthetic image3 to verify the generalization ability of DETR<br>(see Figure 5. Our model is able to find all 24 giraffes on the image which<br>is clearly out of distribution. This experiment confirms that there is no strong<br>class-specialization in each object query.</p>",
            "id": 116,
            "page": 14,
            "text": "Generalization to unseen numbers of instances. Some classes in COCO\nare not well represented with many instances of the same class in the same\nimage. For example, there is no image with more than 13 giraffes in the training\nset. We create a synthetic image3 to verify the generalization ability of DETR\n(see Figure 5. Our model is able to find all 24 giraffes on the image which\nis clearly out of distribution. This experiment confirms that there is no strong\nclass-specialization in each object query."
        },
        {
            "bounding_box": [
                {
                    "x": 136,
                    "y": 2011
                },
                {
                    "x": 954,
                    "y": 2011
                },
                {
                    "x": 954,
                    "y": 2060
                },
                {
                    "x": 136,
                    "y": 2060
                }
            ],
            "category": "paragraph",
            "html": "<p id='117' style='font-size:18px'>4.4 DETR for panoptic segmentation</p>",
            "id": 117,
            "page": 14,
            "text": "4.4 DETR for panoptic segmentation"
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 2099
                },
                {
                    "x": 1591,
                    "y": 2099
                },
                {
                    "x": 1591,
                    "y": 2350
                },
                {
                    "x": 134,
                    "y": 2350
                }
            ],
            "category": "paragraph",
            "html": "<p id='118' style='font-size:18px'>Panoptic segmentation [19] has recently attracted a lot of attention from the<br>computer vision community. Similarly to the extension of Faster R-CNN [37] to<br>Mask R-CNN [14], DETR can be naturally extended by adding a mask head on<br>top of the decoder outputs. In this section we demonstrate that such a head can<br>be used to produce panoptic segmentation [19] by treating stuff and thing classes</p>",
            "id": 118,
            "page": 14,
            "text": "Panoptic segmentation [19] has recently attracted a lot of attention from the\ncomputer vision community. Similarly to the extension of Faster R-CNN [37] to\nMask R-CNN [14], DETR can be naturally extended by adding a mask head on\ntop of the decoder outputs. In this section we demonstrate that such a head can\nbe used to produce panoptic segmentation [19] by treating stuff and thing classes"
        },
        {
            "bounding_box": [
                {
                    "x": 143,
                    "y": 2385
                },
                {
                    "x": 1477,
                    "y": 2385
                },
                {
                    "x": 1477,
                    "y": 2434
                },
                {
                    "x": 143,
                    "y": 2434
                }
            ],
            "category": "paragraph",
            "html": "<p id='119' style='font-size:16px'>3 www.pipzelk.com/en/priclo-domain-in-photo-jphotom<br>Base picture credit: https: / /</p>",
            "id": 119,
            "page": 14,
            "text": "3 www.pipzelk.com/en/priclo-domain-in-photo-jphotom\nBase picture credit: https: / /"
        },
        {
            "bounding_box": [
                {
                    "x": 620,
                    "y": 41
                },
                {
                    "x": 1583,
                    "y": 41
                },
                {
                    "x": 1583,
                    "y": 86
                },
                {
                    "x": 620,
                    "y": 86
                }
            ],
            "category": "header",
            "html": "<header id='120' style='font-size:18px'>End-to-End Object Detection with Transformers 15</header>",
            "id": 120,
            "page": 15,
            "text": "End-to-End Object Detection with Transformers 15"
        },
        {
            "bounding_box": [
                {
                    "x": 138,
                    "y": 132
                },
                {
                    "x": 1586,
                    "y": 132
                },
                {
                    "x": 1586,
                    "y": 590
                },
                {
                    "x": 138,
                    "y": 590
                }
            ],
            "category": "figure",
            "html": "<figure><img id='121' style='font-size:16px' alt=\"Encoded image Resnet features\n(d x H/32 x W/32) Res5 Res4 Res3 Res2\n2 Pixel-wise\nsky Multi x Conv\ntree\nCOW (Conv Conv Conv 3x3 sky\n+\nhead\n2x 3x3 2x 3x3 2x tree\nGN\n3x3 up + up + up +\ngrass Concatenate + + GN + GN + ReLU argmax\nattention\nGN add + add + add\nCOW 98%\nX grass\n+ ReLU ReLU\nConv\nReLU)\nInput image\n(3 xH x W) 3x3\nFPN-style CNN\nBox embeddings Attention maps Masks logits\n(d x N) (N x M x H/32 x W/32) (N x H/4 x W/4)\" data-coord=\"top-left:(138,132); bottom-right:(1586,590)\" /></figure>",
            "id": 121,
            "page": 15,
            "text": "Encoded image Resnet features\n(d x H/32 x W/32) Res5 Res4 Res3 Res2\n2 Pixel-wise\nsky Multi x Conv\ntree\nCOW (Conv Conv Conv 3x3 sky\n+\nhead\n2x 3x3 2x 3x3 2x tree\nGN\n3x3 up + up + up +\ngrass Concatenate + + GN + GN + ReLU argmax\nattention\nGN add + add + add\nCOW 98%\nX grass\n+ ReLU ReLU\nConv\nReLU)\nInput image\n(3 xH x W) 3x3\nFPN-style CNN\nBox embeddings Attention maps Masks logits\n(d x N) (N x M x H/32 x W/32) (N x H/4 x W/4)"
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 625
                },
                {
                    "x": 1587,
                    "y": 625
                },
                {
                    "x": 1587,
                    "y": 720
                },
                {
                    "x": 134,
                    "y": 720
                }
            ],
            "category": "caption",
            "html": "<caption id='122' style='font-size:18px'>Fig. 8: Illustration of the panoptic head. A binary mask is generated in parallel for each<br>detected object, then the masks are merged using pixel-wise argmax.</caption>",
            "id": 122,
            "page": 15,
            "text": "Fig. 8: Illustration of the panoptic head. A binary mask is generated in parallel for each\ndetected object, then the masks are merged using pixel-wise argmax."
        },
        {
            "bounding_box": [
                {
                    "x": 144,
                    "y": 796
                },
                {
                    "x": 1579,
                    "y": 796
                },
                {
                    "x": 1579,
                    "y": 1196
                },
                {
                    "x": 144,
                    "y": 1196
                }
            ],
            "category": "figure",
            "html": "<figure><img id='123' style='font-size:14px' alt=\"light\ncabinet sky\nsky\nwall-stonedoor-stuff microwave\npotted planti\nshelf\nvase\nbook bowl building\nbus giraffe\ntruck tree\ncounter oven\nsink\ngiraffe\ngrass\nfloor pavement\" data-coord=\"top-left:(144,796); bottom-right:(1579,1196)\" /></figure>",
            "id": 123,
            "page": 15,
            "text": "light\ncabinet sky\nsky\nwall-stonedoor-stuff microwave\npotted planti\nshelf\nvase\nbook bowl building\nbus giraffe\ntruck tree\ncounter oven\nsink\ngiraffe\ngrass\nfloor pavement"
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 1230
                },
                {
                    "x": 1587,
                    "y": 1230
                },
                {
                    "x": 1587,
                    "y": 1325
                },
                {
                    "x": 134,
                    "y": 1325
                }
            ],
            "category": "caption",
            "html": "<caption id='124' style='font-size:18px'>Fig. 9: Qualitative results for panoptic segmentation generated by DETR-R101. DETR<br>produces aligned mask predictions in a unified manner for things and stuff.</caption>",
            "id": 124,
            "page": 15,
            "text": "Fig. 9: Qualitative results for panoptic segmentation generated by DETR-R101. DETR\nproduces aligned mask predictions in a unified manner for things and stuff."
        },
        {
            "bounding_box": [
                {
                    "x": 135,
                    "y": 1455
                },
                {
                    "x": 1590,
                    "y": 1455
                },
                {
                    "x": 1590,
                    "y": 1555
                },
                {
                    "x": 135,
                    "y": 1555
                }
            ],
            "category": "paragraph",
            "html": "<p id='125' style='font-size:20px'>in a unified way. We perform our experiments on the panoptic annotations of the<br>COCO dataset that has 53 stuff categories in addition to 80 things categories.</p>",
            "id": 125,
            "page": 15,
            "text": "in a unified way. We perform our experiments on the panoptic annotations of the\nCOCO dataset that has 53 stuff categories in addition to 80 things categories."
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 1570
                },
                {
                    "x": 1591,
                    "y": 1570
                },
                {
                    "x": 1591,
                    "y": 2171
                },
                {
                    "x": 134,
                    "y": 2171
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='126' style='font-size:22px'>We train DETR to predict boxes around both stuff and things classes on<br>COCO, using the same recipe. Predicting boxes is required for the training to<br>be possible, since the Hungarian matching is computed using distances between<br>boxes. We also add a mask head which predicts a binary mask for each of the<br>predicted boxes, see Figure 8. It takes as input the output of transformer decoder<br>for each object and computes multi-head (with M heads) attention scores of this<br>embedding over the output of the encoder, generating M attention heatmaps<br>per object in a small resolution. To make the final prediction and increase the<br>resolution, an FPN-like architecture is used. We describe the architecture in<br>more details in the supplement. The final resolution of the masks has stride 4<br>and each mask is supervised independently using the DICE /F-1 loss [28] and<br>Focal loss [23].</p>",
            "id": 126,
            "page": 15,
            "text": "We train DETR to predict boxes around both stuff and things classes on\nCOCO, using the same recipe. Predicting boxes is required for the training to\nbe possible, since the Hungarian matching is computed using distances between\nboxes. We also add a mask head which predicts a binary mask for each of the\npredicted boxes, see Figure 8. It takes as input the output of transformer decoder\nfor each object and computes multi-head (with M heads) attention scores of this\nembedding over the output of the encoder, generating M attention heatmaps\nper object in a small resolution. To make the final prediction and increase the\nresolution, an FPN-like architecture is used. We describe the architecture in\nmore details in the supplement. The final resolution of the masks has stride 4\nand each mask is supervised independently using the DICE /F-1 loss [28] and\nFocal loss [23]."
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 2185
                },
                {
                    "x": 1590,
                    "y": 2185
                },
                {
                    "x": 1590,
                    "y": 2434
                },
                {
                    "x": 134,
                    "y": 2434
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='127' style='font-size:22px'>The mask head can be trained either jointly, or in a two steps process, where<br>we train DETR for boxes only, then freeze all the weights and train only the mask<br>head for 25 epochs. Experimentally, these two approaches give similar results, we<br>report results using the latter method since it results in a shorter total wall-clock<br>time training.</p>",
            "id": 127,
            "page": 15,
            "text": "The mask head can be trained either jointly, or in a two steps process, where\nwe train DETR for boxes only, then freeze all the weights and train only the mask\nhead for 25 epochs. Experimentally, these two approaches give similar results, we\nreport results using the latter method since it results in a shorter total wall-clock\ntime training."
        },
        {
            "bounding_box": [
                {
                    "x": 137,
                    "y": 45
                },
                {
                    "x": 494,
                    "y": 45
                },
                {
                    "x": 494,
                    "y": 82
                },
                {
                    "x": 137,
                    "y": 82
                }
            ],
            "category": "header",
            "html": "<header id='128' style='font-size:16px'>16 Carion et al.</header>",
            "id": 128,
            "page": 16,
            "text": "16 Carion et al."
        },
        {
            "bounding_box": [
                {
                    "x": 135,
                    "y": 180
                },
                {
                    "x": 1589,
                    "y": 180
                },
                {
                    "x": 1589,
                    "y": 362
                },
                {
                    "x": 135,
                    "y": 362
                }
            ],
            "category": "paragraph",
            "html": "<p id='129' style='font-size:18px'>Table 5: Comparison with the state-of-the-art methods UPSNet [51] and Panoptic<br>FPN [18] on the COCO val dataset We retrained PanopticFPN with the same data-<br>augmentation as DETR, on a 18x schedule for fair comparison. UPSNet uses the 1x<br>schedule, UPSNet-M is the version with multiscale test-time augmentations.</p>",
            "id": 129,
            "page": 16,
            "text": "Table 5: Comparison with the state-of-the-art methods UPSNet [51] and Panoptic\nFPN [18] on the COCO val dataset We retrained PanopticFPN with the same data-\naugmentation as DETR, on a 18x schedule for fair comparison. UPSNet uses the 1x\nschedule, UPSNet-M is the version with multiscale test-time augmentations."
        },
        {
            "bounding_box": [
                {
                    "x": 144,
                    "y": 365
                },
                {
                    "x": 1574,
                    "y": 365
                },
                {
                    "x": 1574,
                    "y": 740
                },
                {
                    "x": 144,
                    "y": 740
                }
            ],
            "category": "table",
            "html": "<br><table id='130' style='font-size:14px'><tr><td>Model</td><td>Backbone</td><td>PQ</td><td>SQ</td><td>RQ</td><td>PQth</td><td>SQth</td><td>RQth</td><td>PQst</td><td>SQst</td><td>RQst</td><td>AP</td></tr><tr><td>PanopticFPN++</td><td>R50</td><td>42.4</td><td>79.3</td><td>51.6</td><td>49.2</td><td>82.4</td><td>58.8</td><td>32.3</td><td>74.8</td><td>40.6</td><td>37.7</td></tr><tr><td>UPSnet</td><td>R50</td><td>42.5</td><td>78.0</td><td>52.5</td><td>48.6</td><td>79.4</td><td>59.6</td><td>33.4</td><td>75.9</td><td>41.7</td><td>34.3</td></tr><tr><td>UPSnet-M</td><td>R50</td><td>43.0</td><td>79.1</td><td>52.8</td><td>48.9</td><td>79.7</td><td>59.7</td><td>34.1</td><td>78.2</td><td>42.3</td><td>34.3</td></tr><tr><td>PanopticFPN++</td><td>R101</td><td>44.1</td><td>79.5</td><td>53.3</td><td>51.0</td><td>83.2</td><td>60.6</td><td>33.6</td><td>74.0</td><td>42.1</td><td>39.7</td></tr><tr><td>DETR</td><td>R50</td><td>43.4</td><td>79.3</td><td>53.8</td><td>48.2</td><td>79.8</td><td>59.5</td><td>36.3</td><td>78.5</td><td>45.3</td><td>31.1</td></tr><tr><td>DETR-DC5</td><td>R50</td><td>44.6</td><td>79.8</td><td>55.0</td><td>49.4</td><td>80.5</td><td>60.6</td><td>37.3</td><td>78.7</td><td>46.5</td><td>31.9</td></tr><tr><td>DETR-R101</td><td>R101</td><td>45.1</td><td>79.9</td><td>55.5</td><td>50.5</td><td>80.9</td><td>61.7</td><td>37.0</td><td>78.5</td><td>46.0</td><td>33.0</td></tr></table>",
            "id": 130,
            "page": 16,
            "text": "Model Backbone PQ SQ RQ PQth SQth RQth PQst SQst RQst AP\n PanopticFPN++ R50 42.4 79.3 51.6 49.2 82.4 58.8 32.3 74.8 40.6 37.7\n UPSnet R50 42.5 78.0 52.5 48.6 79.4 59.6 33.4 75.9 41.7 34.3\n UPSnet-M R50 43.0 79.1 52.8 48.9 79.7 59.7 34.1 78.2 42.3 34.3\n PanopticFPN++ R101 44.1 79.5 53.3 51.0 83.2 60.6 33.6 74.0 42.1 39.7\n DETR R50 43.4 79.3 53.8 48.2 79.8 59.5 36.3 78.5 45.3 31.1\n DETR-DC5 R50 44.6 79.8 55.0 49.4 80.5 60.6 37.3 78.7 46.5 31.9\n DETR-R101 R101 45.1 79.9 55.5 50.5 80.9 61.7 37.0 78.5 46.0"
        },
        {
            "bounding_box": [
                {
                    "x": 135,
                    "y": 875
                },
                {
                    "x": 1591,
                    "y": 875
                },
                {
                    "x": 1591,
                    "y": 1123
                },
                {
                    "x": 135,
                    "y": 1123
                }
            ],
            "category": "paragraph",
            "html": "<p id='131' style='font-size:20px'>To predict the final panoptic segmentation we simply use an argmax over<br>the mask scores at each pixel, and assign the corresponding categories to the<br>resulting masks. This procedure guarantees that the final masks have no overlaps<br>and, therefore, DETR does not require a heuristic [19] that is often used to align<br>different masks.</p>",
            "id": 131,
            "page": 16,
            "text": "To predict the final panoptic segmentation we simply use an argmax over\nthe mask scores at each pixel, and assign the corresponding categories to the\nresulting masks. This procedure guarantees that the final masks have no overlaps\nand, therefore, DETR does not require a heuristic [19] that is often used to align\ndifferent masks."
        },
        {
            "bounding_box": [
                {
                    "x": 136,
                    "y": 1231
                },
                {
                    "x": 1590,
                    "y": 1231
                },
                {
                    "x": 1590,
                    "y": 1581
                },
                {
                    "x": 136,
                    "y": 1581
                }
            ],
            "category": "paragraph",
            "html": "<p id='132' style='font-size:22px'>Training details. We train DETR, DETR-DC5 and DETR-R101 models fol-<br>lowing the recipe for bounding box detection to predict boxes around stuff and<br>things classes in COCO dataset. The new mask head is trained for 25 epochs<br>(see supplementary for details). During inference we first filter out the detection<br>with a confidence below 85%, then compute the per-pixel argmax to determine<br>in which mask each pixel belongs. We then collapse different mask predictions<br>of the same stuff category in one, and filter the empty ones (less than 4 pixels).</p>",
            "id": 132,
            "page": 16,
            "text": "Training details. We train DETR, DETR-DC5 and DETR-R101 models fol-\nlowing the recipe for bounding box detection to predict boxes around stuff and\nthings classes in COCO dataset. The new mask head is trained for 25 epochs\n(see supplementary for details). During inference we first filter out the detection\nwith a confidence below 85%, then compute the per-pixel argmax to determine\nin which mask each pixel belongs. We then collapse different mask predictions\nof the same stuff category in one, and filter the empty ones (less than 4 pixels)."
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 1684
                },
                {
                    "x": 1590,
                    "y": 1684
                },
                {
                    "x": 1590,
                    "y": 2438
                },
                {
                    "x": 134,
                    "y": 2438
                }
            ],
            "category": "paragraph",
            "html": "<p id='133' style='font-size:22px'>Main results. Qualitative results are shown in Figure 9. In table 5 we compare<br>our unified panoptic segmenation approach with several established methods<br>that treat things and stuff differently. We report the Panoptic Quality (PQ) and<br>the break-down on things (PQth) and stuff (PQst). We also report the mask<br>AP (computed on the things classes), before any panoptic post-treatment (in<br>our case, before taking the pixel-wise argmax). We show that DETR outper-<br>forms published results on COCO-val 2017, as well as our strong PanopticFPN<br>baseline (trained with same data-augmentation as DETR, for fair comparison).<br>The result break-down shows that DETR is especially dominant on stuff classes,<br>and we hypothesize that the global reasoning allowed by the encoder attention<br>is the key element to this result. For things class, despite a severe deficit of<br>up to 8 mAP compared to the baselines on the mask AP computation, DETR<br>obtains competitive PQth. We also evaluated our method on the test set of the<br>COCO dataset, and obtained 46 PQ. We hope that our approach will inspire the<br>exploration of fully unified models for panoptic segmentation in future work.</p>",
            "id": 133,
            "page": 16,
            "text": "Main results. Qualitative results are shown in Figure 9. In table 5 we compare\nour unified panoptic segmenation approach with several established methods\nthat treat things and stuff differently. We report the Panoptic Quality (PQ) and\nthe break-down on things (PQth) and stuff (PQst). We also report the mask\nAP (computed on the things classes), before any panoptic post-treatment (in\nour case, before taking the pixel-wise argmax). We show that DETR outper-\nforms published results on COCO-val 2017, as well as our strong PanopticFPN\nbaseline (trained with same data-augmentation as DETR, for fair comparison).\nThe result break-down shows that DETR is especially dominant on stuff classes,\nand we hypothesize that the global reasoning allowed by the encoder attention\nis the key element to this result. For things class, despite a severe deficit of\nup to 8 mAP compared to the baselines on the mask AP computation, DETR\nobtains competitive PQth. We also evaluated our method on the test set of the\nCOCO dataset, and obtained 46 PQ. We hope that our approach will inspire the\nexploration of fully unified models for panoptic segmentation in future work."
        },
        {
            "bounding_box": [
                {
                    "x": 620,
                    "y": 41
                },
                {
                    "x": 1584,
                    "y": 41
                },
                {
                    "x": 1584,
                    "y": 86
                },
                {
                    "x": 620,
                    "y": 86
                }
            ],
            "category": "header",
            "html": "<header id='134' style='font-size:14px'>End-to-End Object Detection with Transformers 17</header>",
            "id": 134,
            "page": 17,
            "text": "End-to-End Object Detection with Transformers 17"
        },
        {
            "bounding_box": [
                {
                    "x": 136,
                    "y": 141
                },
                {
                    "x": 498,
                    "y": 141
                },
                {
                    "x": 498,
                    "y": 194
                },
                {
                    "x": 136,
                    "y": 194
                }
            ],
            "category": "paragraph",
            "html": "<p id='135' style='font-size:18px'>5 Conclusion</p>",
            "id": 135,
            "page": 17,
            "text": "5 Conclusion"
        },
        {
            "bounding_box": [
                {
                    "x": 133,
                    "y": 243
                },
                {
                    "x": 1591,
                    "y": 243
                },
                {
                    "x": 1591,
                    "y": 640
                },
                {
                    "x": 133,
                    "y": 640
                }
            ],
            "category": "paragraph",
            "html": "<p id='136' style='font-size:16px'>We presented DETR, a new design for object detection systems based on trans-<br>formers and bipartite matching loss for direct set prediction. The approach<br>achieves comparable results to an optimized Faster R-CNN baseline on the chal-<br>lenging COCO dataset. DETR is straightforward to implement and has a flexible<br>architecture that is easily extensible to panoptic segmentation, with competitive<br>results. In addition, it achieves significantly better performance on large objects<br>than Faster R-CNN, likely thanks to the processing of global information per-<br>formed by the self-attention.</p>",
            "id": 136,
            "page": 17,
            "text": "We presented DETR, a new design for object detection systems based on trans-\nformers and bipartite matching loss for direct set prediction. The approach\nachieves comparable results to an optimized Faster R-CNN baseline on the chal-\nlenging COCO dataset. DETR is straightforward to implement and has a flexible\narchitecture that is easily extensible to panoptic segmentation, with competitive\nresults. In addition, it achieves significantly better performance on large objects\nthan Faster R-CNN, likely thanks to the processing of global information per-\nformed by the self-attention."
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 643
                },
                {
                    "x": 1590,
                    "y": 643
                },
                {
                    "x": 1590,
                    "y": 841
                },
                {
                    "x": 134,
                    "y": 841
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='137' style='font-size:14px'>This new design for detectors also comes with new challenges, in particular<br>regarding training, optimization and performances on small objects. Current<br>detectors required several years of improvements to cope with similar issues,<br>and we expect future work to successfully address them for DETR.</p>",
            "id": 137,
            "page": 17,
            "text": "This new design for detectors also comes with new challenges, in particular\nregarding training, optimization and performances on small objects. Current\ndetectors required several years of improvements to cope with similar issues,\nand we expect future work to successfully address them for DETR."
        },
        {
            "bounding_box": [
                {
                    "x": 135,
                    "y": 913
                },
                {
                    "x": 696,
                    "y": 913
                },
                {
                    "x": 696,
                    "y": 972
                },
                {
                    "x": 135,
                    "y": 972
                }
            ],
            "category": "paragraph",
            "html": "<p id='138' style='font-size:22px'>6 Acknowledgements</p>",
            "id": 138,
            "page": 17,
            "text": "6 Acknowledgements"
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 1015
                },
                {
                    "x": 1592,
                    "y": 1015
                },
                {
                    "x": 1592,
                    "y": 1216
                },
                {
                    "x": 134,
                    "y": 1216
                }
            ],
            "category": "paragraph",
            "html": "<p id='139' style='font-size:18px'>We thank Sainbayar Sukhbaatar, Piotr Bojanowski, Natalia Neverova, David<br>Lopez-Paz, Guillaume Lample, Danielle Rothermel, Kaiming He, Ross Girshick,<br>Xinlei Chen and the whole Facebook AI Research Paris team for discussions and<br>advices without which this work would not be possible.</p>",
            "id": 139,
            "page": 17,
            "text": "We thank Sainbayar Sukhbaatar, Piotr Bojanowski, Natalia Neverova, David\nLopez-Paz, Guillaume Lample, Danielle Rothermel, Kaiming He, Ross Girshick,\nXinlei Chen and the whole Facebook AI Research Paris team for discussions and\nadvices without which this work would not be possible."
        },
        {
            "bounding_box": [
                {
                    "x": 136,
                    "y": 1289
                },
                {
                    "x": 410,
                    "y": 1289
                },
                {
                    "x": 410,
                    "y": 1345
                },
                {
                    "x": 136,
                    "y": 1345
                }
            ],
            "category": "paragraph",
            "html": "<p id='140' style='font-size:20px'>References</p>",
            "id": 140,
            "page": 17,
            "text": "References"
        },
        {
            "bounding_box": [
                {
                    "x": 146,
                    "y": 1383
                },
                {
                    "x": 1595,
                    "y": 1383
                },
                {
                    "x": 1595,
                    "y": 2439
                },
                {
                    "x": 146,
                    "y": 2439
                }
            ],
            "category": "paragraph",
            "html": "<p id='141' style='font-size:16px'>1. Al-Rfou, R., Choe, D., Constant, N., Guo, M., Jones, L.: Character-level language<br>modeling with deeper self-attention. In: AAAI Conference on Artificial Intelligence<br>(2019)<br>2. Bahdanau, D., Cho, K., Bengio, Y.: Neural machine translation by jointly learning<br>to align and translate. In: ICLR (2015)<br>3. Bello, I., Zoph, B., Vaswani, A., Shlens, J., Le, Q.V.: Attention augmented convo-<br>lutional networks. In: ICCV (2019)<br>4. Bodla, N., Singh, B., Chellappa, R., Davis, L.S.: Soft-NMS improving object<br>detection with one line of code. In: ICCV (2017)<br>5. Cai, Z., Vasconcelos, N.: Cascade R-CNN: High quality object detection and in-<br>stance segmentation. PAMI (2019)<br>6. Chan, W., Saharia, C., Hinton, G., Norouzi, M., Jaitly, N.: Imputer: Sequence<br>modelling via imputation and dynamic programming. arXiv:2002.08926 (2020)<br>7. Cordonnier, J.B., Loukas, A., Jaggi, M.: On the relationship between self-attention<br>and convolutional layers. In: ICLR (2020)<br>8. Devlin, J., Chang, M. W., Lee, K., Toutanova, K.: BERT: Pre-training of deep<br>bidirectional transformers for language understanding. In: NAACL-HLT (2019)<br>9. Erhan, D., Szegedy, C., Toshev, A., Anguelov, D. : Scalable object detection using<br>deep neural networks. In: CVPR (2014)<br>10. Ghazvininejad, M., Levy, O., Liu, Y., Zettlemoyer, L.: Mask-predict: Parallel de-<br>coding of conditional masked language models. arXiv:1904.09324 (2019)<br>11. Glorot, X., Bengio, Y.: Understanding the difficulty of training deep feedforward<br>neural networks. In: AISTATS (2010)</p>",
            "id": 141,
            "page": 17,
            "text": "1. Al-Rfou, R., Choe, D., Constant, N., Guo, M., Jones, L.: Character-level language\nmodeling with deeper self-attention. In: AAAI Conference on Artificial Intelligence\n(2019)\n2. Bahdanau, D., Cho, K., Bengio, Y.: Neural machine translation by jointly learning\nto align and translate. In: ICLR (2015)\n3. Bello, I., Zoph, B., Vaswani, A., Shlens, J., Le, Q.V.: Attention augmented convo-\nlutional networks. In: ICCV (2019)\n4. Bodla, N., Singh, B., Chellappa, R., Davis, L.S.: Soft-NMS improving object\ndetection with one line of code. In: ICCV (2017)\n5. Cai, Z., Vasconcelos, N.: Cascade R-CNN: High quality object detection and in-\nstance segmentation. PAMI (2019)\n6. Chan, W., Saharia, C., Hinton, G., Norouzi, M., Jaitly, N.: Imputer: Sequence\nmodelling via imputation and dynamic programming. arXiv:2002.08926 (2020)\n7. Cordonnier, J.B., Loukas, A., Jaggi, M.: On the relationship between self-attention\nand convolutional layers. In: ICLR (2020)\n8. Devlin, J., Chang, M. W., Lee, K., Toutanova, K.: BERT: Pre-training of deep\nbidirectional transformers for language understanding. In: NAACL-HLT (2019)\n9. Erhan, D., Szegedy, C., Toshev, A., Anguelov, D. : Scalable object detection using\ndeep neural networks. In: CVPR (2014)\n10. Ghazvininejad, M., Levy, O., Liu, Y., Zettlemoyer, L.: Mask-predict: Parallel de-\ncoding of conditional masked language models. arXiv:1904.09324 (2019)\n11. Glorot, X., Bengio, Y.: Understanding the difficulty of training deep feedforward\nneural networks. In: AISTATS (2010)"
        },
        {
            "bounding_box": [
                {
                    "x": 138,
                    "y": 45
                },
                {
                    "x": 494,
                    "y": 45
                },
                {
                    "x": 494,
                    "y": 82
                },
                {
                    "x": 138,
                    "y": 82
                }
            ],
            "category": "header",
            "html": "<header id='142' style='font-size:14px'>18 Carion et al.</header>",
            "id": 142,
            "page": 18,
            "text": "18 Carion et al."
        },
        {
            "bounding_box": [
                {
                    "x": 132,
                    "y": 134
                },
                {
                    "x": 1596,
                    "y": 134
                },
                {
                    "x": 1596,
                    "y": 2440
                },
                {
                    "x": 132,
                    "y": 2440
                }
            ],
            "category": "paragraph",
            "html": "<p id='143' style='font-size:18px'>12. Gu, J., Bradbury, J., Xiong, C., Li, V.O., Socher, R.: Non-autoregressive neural<br>machine translation. In: ICLR (2018)<br>13. He, K., Girshick, R., Dollar, P. : Rethinking imagenet pre-training. In: ICCV (2019)<br>14. He, K., Gkioxari, G., Dollar, P., Girshick, R.B. : Mask R-CNN. In: ICCV (2017)<br>15. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.<br>In: CVPR (2016)<br>16. Hosang, J.H., Benenson, R., Schiele, B.: Learning non-maximum suppression. In:<br>CVPR (2017)<br>17. Hu, H., Gu, J., Zhang, Z., Dai, J., Wei, Y.: Relation networks for object detection.<br>In: CVPR (2018)<br>18. Kirillov, A., Girshick, R., He, K., Dollar, P.: Panoptic feature pyramid networks.<br>In: CVPR (2019)<br>19. Kirillov, A., He, K., Girshick, R., Rother, C., Dollar, P.: Panoptic segmentation.<br>In: CVPR (2019)<br>20. Kuhn, H.W. : The hungarian method for the assignment problem (1955)<br>21. Li, Y., Qi, H., Dai, J., Ji, X., Wei, Y.: Fully convolutional instance-aware semantic<br>segmentation. In: CVPR (2017)<br>22. Lin, T.Y., Dollar, P., Girshick, R., He, K., Hariharan, B., Belongie, S.: Feature<br>pyramid networks for object detection. In: CVPR (2017)<br>23. Lin, T.Y., Goyal, P., Girshick, R.B., He, K., Dollar, P.: Focal loss for dense object<br>detection. In: ICCV (2017)<br>24. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P.,<br>Zitnick, C.L.: Microsoft COCO: Common objects in context. In: ECCV (2014)<br>25. Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S.E., Fu, C.Y., Berg, A.C.:<br>Ssd: Single shot multibox detector. In: ECCV (2016)<br>26. Loshchilov, I., Hutter, F. : Decoupled weight decay regularization. In: ICLR (2017)<br>27. Luscher, C., Beck, E., Irie, K., Kitza, M., Michel, W., Zeyer, A., Schluter, R., Ney,<br>H. : Rwth asr systems for librispeech: Hybrid VS attention 、 w/o data augmentation.<br>arXiv:1905.03072 (2019)<br>28. Milletari, F., Navab, N., Ahmadi, S.A.: V-net: Fully convolutional neural networks<br>for volumetric medical image segmentation. In: 3DV (2016)<br>29. Oord, A.v.d., Li, Y., Babuschkin, I., Simonyan, K., Vinyals, O., Kavukcuoglu, K.,<br>Driessche, G.v.d., Lockhart, E., Cobo, L.C., Stimberg, F., et al.: Parallel wavenet:<br>Fast high-fidelity speech synthesis. arXiv:1711.10433 (2017)<br>30. Park, E., Berg, A.C.: Learning to decompose for object detection and instance<br>segmentation. arXiv:1511.06449 (2015)<br>31. Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Shazeer, N., Ku, A., Tran, D.:<br>Image transformer. In: ICML (2018)<br>32. Paszke, A., Gross, S., Massa, F. , Lerer, A., Bradbury, J., Chanan, G., Killeen, T.,<br>Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z.,<br>Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., Chintala, S.:<br>Pytorch: An imperative style, high-performance deep learning library. In: NeurIPS<br>(2019)<br>33. Pineda, L., Salvador, A., Drozdzal, M., Romero, A.: Elucidating image-to-set pre-<br>diction: An analysis of models, losses and datasets. arXiv:1904.05709 (2019)<br>34. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I.: Language<br>models are unsupervised multitask learners (2019)<br>35. Redmon, J., Divvala, S., Girshick, R., Farhadi, A.: You only look once: Unified,<br>real-time object detection. In: CVPR (2016)<br>36. Ren, M., Zemel, R.S.: End-to-end instance segmentation with recurrent attention.<br>In: CVPR (2017)</p>",
            "id": 143,
            "page": 18,
            "text": "12. Gu, J., Bradbury, J., Xiong, C., Li, V.O., Socher, R.: Non-autoregressive neural\nmachine translation. In: ICLR (2018)\n13. He, K., Girshick, R., Dollar, P. : Rethinking imagenet pre-training. In: ICCV (2019)\n14. He, K., Gkioxari, G., Dollar, P., Girshick, R.B. : Mask R-CNN. In: ICCV (2017)\n15. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.\nIn: CVPR (2016)\n16. Hosang, J.H., Benenson, R., Schiele, B.: Learning non-maximum suppression. In:\nCVPR (2017)\n17. Hu, H., Gu, J., Zhang, Z., Dai, J., Wei, Y.: Relation networks for object detection.\nIn: CVPR (2018)\n18. Kirillov, A., Girshick, R., He, K., Dollar, P.: Panoptic feature pyramid networks.\nIn: CVPR (2019)\n19. Kirillov, A., He, K., Girshick, R., Rother, C., Dollar, P.: Panoptic segmentation.\nIn: CVPR (2019)\n20. Kuhn, H.W. : The hungarian method for the assignment problem (1955)\n21. Li, Y., Qi, H., Dai, J., Ji, X., Wei, Y.: Fully convolutional instance-aware semantic\nsegmentation. In: CVPR (2017)\n22. Lin, T.Y., Dollar, P., Girshick, R., He, K., Hariharan, B., Belongie, S.: Feature\npyramid networks for object detection. In: CVPR (2017)\n23. Lin, T.Y., Goyal, P., Girshick, R.B., He, K., Dollar, P.: Focal loss for dense object\ndetection. In: ICCV (2017)\n24. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar, P.,\nZitnick, C.L.: Microsoft COCO: Common objects in context. In: ECCV (2014)\n25. Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S.E., Fu, C.Y., Berg, A.C.:\nSsd: Single shot multibox detector. In: ECCV (2016)\n26. Loshchilov, I., Hutter, F. : Decoupled weight decay regularization. In: ICLR (2017)\n27. Luscher, C., Beck, E., Irie, K., Kitza, M., Michel, W., Zeyer, A., Schluter, R., Ney,\nH. : Rwth asr systems for librispeech: Hybrid VS attention 、 w/o data augmentation.\narXiv:1905.03072 (2019)\n28. Milletari, F., Navab, N., Ahmadi, S.A.: V-net: Fully convolutional neural networks\nfor volumetric medical image segmentation. In: 3DV (2016)\n29. Oord, A.v.d., Li, Y., Babuschkin, I., Simonyan, K., Vinyals, O., Kavukcuoglu, K.,\nDriessche, G.v.d., Lockhart, E., Cobo, L.C., Stimberg, F., et al.: Parallel wavenet:\nFast high-fidelity speech synthesis. arXiv:1711.10433 (2017)\n30. Park, E., Berg, A.C.: Learning to decompose for object detection and instance\nsegmentation. arXiv:1511.06449 (2015)\n31. Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Shazeer, N., Ku, A., Tran, D.:\nImage transformer. In: ICML (2018)\n32. Paszke, A., Gross, S., Massa, F. , Lerer, A., Bradbury, J., Chanan, G., Killeen, T.,\nLin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z.,\nRaison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., Chintala, S.:\nPytorch: An imperative style, high-performance deep learning library. In: NeurIPS\n(2019)\n33. Pineda, L., Salvador, A., Drozdzal, M., Romero, A.: Elucidating image-to-set pre-\ndiction: An analysis of models, losses and datasets. arXiv:1904.05709 (2019)\n34. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I.: Language\nmodels are unsupervised multitask learners (2019)\n35. Redmon, J., Divvala, S., Girshick, R., Farhadi, A.: You only look once: Unified,\nreal-time object detection. In: CVPR (2016)\n36. Ren, M., Zemel, R.S.: End-to-end instance segmentation with recurrent attention.\nIn: CVPR (2017)"
        },
        {
            "bounding_box": [
                {
                    "x": 620,
                    "y": 42
                },
                {
                    "x": 1582,
                    "y": 42
                },
                {
                    "x": 1582,
                    "y": 84
                },
                {
                    "x": 620,
                    "y": 84
                }
            ],
            "category": "header",
            "html": "<header id='144' style='font-size:14px'>End-to-End Object Detection with Transformers 19</header>",
            "id": 144,
            "page": 19,
            "text": "End-to-End Object Detection with Transformers 19"
        },
        {
            "bounding_box": [
                {
                    "x": 131,
                    "y": 138
                },
                {
                    "x": 1592,
                    "y": 138
                },
                {
                    "x": 1592,
                    "y": 1849
                },
                {
                    "x": 131,
                    "y": 1849
                }
            ],
            "category": "paragraph",
            "html": "<p id='145' style='font-size:18px'>37. Ren, S., He, K., Girshick, R.B., Sun, J.: Faster R-CNN: Towards real-time object<br>detection with region proposal networks. PAMI (2015)<br>38. Rezatofighi, H., Tsoi, N., Gwak, J., Sadeghian, A., Reid, I., Savarese, S.: General-<br>ized intersection over union. In: CVPR (2019)<br>39. Rezatofighi, S.H., Kaskman, R., Motlagh, F.T., Shi, Q., Cremers, D., Leal-Taixe,<br>L., Reid, I.: Deep perm-set net: Learn to predict sets with unknown permutation<br>and cardinality using deep neural networks. arXiv:1805.00613 (2018)<br>40. Rezatofighi, S.H., Milan, A., Abbasnejad, E., Dick, A., Reid, I., Kaskman, R.,<br>Cremers, D., Leal- Taix, 1.: Deepsetnet: Predicting sets with deep neural networks.<br>In: ICCV (2017)<br>41. Romera-Paredes, B., Torr, P.H.S.: Recurrent instance segmentation. In: ECCV<br>(2015)<br>42. Salvador, A., Bellver, M., Baradad, M., Marques, F., Torres, J., Giro, X.: Recurrent<br>neural networks for semantic instance segmentation. arXiv:1712.00617 (2017)<br>43. Stewart, R.J., Andriluka, M., Ng, A.Y.: End-to-end people detection in crowded<br>scenes. In: CVPR (2015)<br>44. Sutskever, I., Vinyals, O., Le, Q.V.: Sequence to sequence learning with neural<br>networks. In: NeurIPS (2014)<br>45. Synnaeve, G., Xu, Q., Kahn, J., Grave, E., Likhomanenko, T., Pratap, V., Sri-<br>ram, A., Liptchinsky, V., Collobert, R.: End-to-end ASR: from supervised to semi-<br>supervised learning with modern architectures. arXiv:1911.08460 (2019)<br>46. Tian, Z., Shen, C., Chen, H., He, T.: FCOS: Fully convolutional one-stage object<br>detection. In: ICCV (2019)<br>47. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,<br>L., Polosukhin, I.: Attention is all you need. In: NeurIPS (2017)<br>48. Vinyals, 0., Bengio, S., Kudlur, M.: Order matters: Sequence to sequence for sets.<br>In: ICLR (2016)<br>49. Wang, X., Girshick, R.B., Gupta, A., He, K.: Non-local neural networks. In: CVPR<br>(2018)<br>50. Wu, Y., Kirillov, A., Massa, F., Lo, W.Y., Girshick, R.: Detectron2. https: //<br>github · com/facebookresearch/detectron2 (2019)<br>51. Xiong, Y. , Liao, R., Zhao, H., Hu, R., Bai, M., Yumer, E., Urtasun, R.: Upsnet: A<br>unified panoptic segmentation network. In: CVPR (2019)<br>52. Zhang, S., Chi, C., Yao, Y., Lei, Z., Li, S.Z.: Bridging the gap between anchor-based<br>and anchor-free detection via adaptive training sample selection. arXiv:1912.02424<br>(2019)<br>53. Zhou, X., Wang, D., Kr�henbuhl, P.: Objects as points. arXiv:1904.07850 (2019)</p>",
            "id": 145,
            "page": 19,
            "text": "37. Ren, S., He, K., Girshick, R.B., Sun, J.: Faster R-CNN: Towards real-time object\ndetection with region proposal networks. PAMI (2015)\n38. Rezatofighi, H., Tsoi, N., Gwak, J., Sadeghian, A., Reid, I., Savarese, S.: General-\nized intersection over union. In: CVPR (2019)\n39. Rezatofighi, S.H., Kaskman, R., Motlagh, F.T., Shi, Q., Cremers, D., Leal-Taixe,\nL., Reid, I.: Deep perm-set net: Learn to predict sets with unknown permutation\nand cardinality using deep neural networks. arXiv:1805.00613 (2018)\n40. Rezatofighi, S.H., Milan, A., Abbasnejad, E., Dick, A., Reid, I., Kaskman, R.,\nCremers, D., Leal- Taix, 1.: Deepsetnet: Predicting sets with deep neural networks.\nIn: ICCV (2017)\n41. Romera-Paredes, B., Torr, P.H.S.: Recurrent instance segmentation. In: ECCV\n(2015)\n42. Salvador, A., Bellver, M., Baradad, M., Marques, F., Torres, J., Giro, X.: Recurrent\nneural networks for semantic instance segmentation. arXiv:1712.00617 (2017)\n43. Stewart, R.J., Andriluka, M., Ng, A.Y.: End-to-end people detection in crowded\nscenes. In: CVPR (2015)\n44. Sutskever, I., Vinyals, O., Le, Q.V.: Sequence to sequence learning with neural\nnetworks. In: NeurIPS (2014)\n45. Synnaeve, G., Xu, Q., Kahn, J., Grave, E., Likhomanenko, T., Pratap, V., Sri-\nram, A., Liptchinsky, V., Collobert, R.: End-to-end ASR: from supervised to semi-\nsupervised learning with modern architectures. arXiv:1911.08460 (2019)\n46. Tian, Z., Shen, C., Chen, H., He, T.: FCOS: Fully convolutional one-stage object\ndetection. In: ICCV (2019)\n47. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\nL., Polosukhin, I.: Attention is all you need. In: NeurIPS (2017)\n48. Vinyals, 0., Bengio, S., Kudlur, M.: Order matters: Sequence to sequence for sets.\nIn: ICLR (2016)\n49. Wang, X., Girshick, R.B., Gupta, A., He, K.: Non-local neural networks. In: CVPR\n(2018)\n50. Wu, Y., Kirillov, A., Massa, F., Lo, W.Y., Girshick, R.: Detectron2. https: //\ngithub · com/facebookresearch/detectron2 (2019)\n51. Xiong, Y. , Liao, R., Zhao, H., Hu, R., Bai, M., Yumer, E., Urtasun, R.: Upsnet: A\nunified panoptic segmentation network. In: CVPR (2019)\n52. Zhang, S., Chi, C., Yao, Y., Lei, Z., Li, S.Z.: Bridging the gap between anchor-based\nand anchor-free detection via adaptive training sample selection. arXiv:1912.02424\n(2019)\n53. Zhou, X., Wang, D., Kr�henbuhl, P.: Objects as points. arXiv:1904.07850 (2019)"
        },
        {
            "bounding_box": [
                {
                    "x": 133,
                    "y": 43
                },
                {
                    "x": 494,
                    "y": 43
                },
                {
                    "x": 494,
                    "y": 82
                },
                {
                    "x": 133,
                    "y": 82
                }
            ],
            "category": "header",
            "html": "<header id='146' style='font-size:14px'>20 Carion et al.</header>",
            "id": 146,
            "page": 20,
            "text": "20 Carion et al."
        },
        {
            "bounding_box": [
                {
                    "x": 137,
                    "y": 141
                },
                {
                    "x": 484,
                    "y": 141
                },
                {
                    "x": 484,
                    "y": 198
                },
                {
                    "x": 137,
                    "y": 198
                }
            ],
            "category": "paragraph",
            "html": "<p id='147' style='font-size:22px'>A Appendix</p>",
            "id": 147,
            "page": 20,
            "text": "A Appendix"
        },
        {
            "bounding_box": [
                {
                    "x": 137,
                    "y": 241
                },
                {
                    "x": 1145,
                    "y": 241
                },
                {
                    "x": 1145,
                    "y": 288
                },
                {
                    "x": 137,
                    "y": 288
                }
            ],
            "category": "paragraph",
            "html": "<p id='148' style='font-size:16px'>A.1 Preliminaries: Multi-head attention layers</p>",
            "id": 148,
            "page": 20,
            "text": "A.1 Preliminaries: Multi-head attention layers"
        },
        {
            "bounding_box": [
                {
                    "x": 133,
                    "y": 317
                },
                {
                    "x": 1590,
                    "y": 317
                },
                {
                    "x": 1590,
                    "y": 517
                },
                {
                    "x": 133,
                    "y": 517
                }
            ],
            "category": "paragraph",
            "html": "<p id='149' style='font-size:16px'>Since our model is based on the Transformer architecture, we remind here the<br>general form of attention mechanisms we use for exhaustivity. The attention<br>mechanism follows [47], except for the details of positional encodings (see Equa-<br>tion 8) that follows [ ].</p>",
            "id": 149,
            "page": 20,
            "text": "Since our model is based on the Transformer architecture, we remind here the\ngeneral form of attention mechanisms we use for exhaustivity. The attention\nmechanism follows [47], except for the details of positional encodings (see Equa-\ntion 8) that follows [ ]."
        },
        {
            "bounding_box": [
                {
                    "x": 133,
                    "y": 536
                },
                {
                    "x": 1590,
                    "y": 536
                },
                {
                    "x": 1590,
                    "y": 686
                },
                {
                    "x": 133,
                    "y": 686
                }
            ],
            "category": "paragraph",
            "html": "<p id='150' style='font-size:16px'>Multi-head The general form of multi-head attention with M heads of dimen-<br>sion d is a function with the following signature (using d' = 음, and giving<br>matrix /tensors sizes in underbrace)</p>",
            "id": 150,
            "page": 20,
            "text": "Multi-head The general form of multi-head attention with M heads of dimen-\nsion d is a function with the following signature (using d' = 음, and giving\nmatrix /tensors sizes in underbrace)"
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 869
                },
                {
                    "x": 1589,
                    "y": 869
                },
                {
                    "x": 1589,
                    "y": 1169
                },
                {
                    "x": 134,
                    "y": 1169
                }
            ],
            "category": "paragraph",
            "html": "<p id='151' style='font-size:16px'>where Xq is the query sequence of length Nq, Xkv is the key-value sequence of<br>length Nkv (with the same number of channels d for simplicity of exposition), T<br>is the weight tensor to compute the so-called query, key and value embeddings,<br>and L is a projection matrix. The output is the same size as the query sequence.<br>To fix the vocabulary before giving details, multi-head self-attention (mh-s-attn)<br>is the special case Xq = Xkv, i.e.</p>",
            "id": 151,
            "page": 20,
            "text": "where Xq is the query sequence of length Nq, Xkv is the key-value sequence of\nlength Nkv (with the same number of channels d for simplicity of exposition), T\nis the weight tensor to compute the so-called query, key and value embeddings,\nand L is a projection matrix. The output is the same size as the query sequence.\nTo fix the vocabulary before giving details, multi-head self-attention (mh-s-attn)\nis the special case Xq = Xkv, i.e."
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 1286
                },
                {
                    "x": 1589,
                    "y": 1286
                },
                {
                    "x": 1589,
                    "y": 1498
                },
                {
                    "x": 134,
                    "y": 1498
                }
            ],
            "category": "paragraph",
            "html": "<p id='152' style='font-size:16px'>The multi-head attention is simply the concatenation of M single attention<br>heads followed by a projection with L. The common practice [47] is to use residual<br>connections, dropout and layer normalization. In other words, denoting Xq =<br>mh-attn(Xq, Xkv, T, L) and X(q) the concatenation of attention heads, we have</p>",
            "id": 152,
            "page": 20,
            "text": "The multi-head attention is simply the concatenation of M single attention\nheads followed by a projection with L. The common practice [47] is to use residual\nconnections, dropout and layer normalization. In other words, denoting Xq =\nmh-attn(Xq, Xkv, T, L) and X(q) the concatenation of attention heads, we have"
        },
        {
            "bounding_box": [
                {
                    "x": 133,
                    "y": 1680
                },
                {
                    "x": 1093,
                    "y": 1680
                },
                {
                    "x": 1093,
                    "y": 1736
                },
                {
                    "x": 133,
                    "y": 1736
                }
            ],
            "category": "paragraph",
            "html": "<p id='153' style='font-size:14px'>where [;] denotes concatenation on the channel axis.</p>",
            "id": 153,
            "page": 20,
            "text": "where [;] denotes concatenation on the channel axis."
        },
        {
            "bounding_box": [
                {
                    "x": 133,
                    "y": 1748
                },
                {
                    "x": 1590,
                    "y": 1748
                },
                {
                    "x": 1590,
                    "y": 1955
                },
                {
                    "x": 133,
                    "y": 1955
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='154' style='font-size:18px'>Single head An attention head with weight tensor T' E R3xd'xd, denoted by<br>attn(Xq, Xkv, T'), depends on additional positional encoding Pq E RdxNq and<br>Pkv E RdxNkv. It starts by computing so-called query, key and value embeddings<br>after adding the query and key positional encodings [7]:</p>",
            "id": 154,
            "page": 20,
            "text": "Single head An attention head with weight tensor T' E R3xd'xd, denoted by\nattn(Xq, Xkv, T'), depends on additional positional encoding Pq E RdxNq and\nPkv E RdxNkv. It starts by computing so-called query, key and value embeddings\nafter adding the query and key positional encodings [7]:"
        },
        {
            "bounding_box": [
                {
                    "x": 133,
                    "y": 2070
                },
                {
                    "x": 1589,
                    "y": 2070
                },
                {
                    "x": 1589,
                    "y": 2273
                },
                {
                    "x": 133,
                    "y": 2273
                }
            ],
            "category": "paragraph",
            "html": "<p id='155' style='font-size:16px'>where T' is the concatenation of T1, T2, T3. The attention weights a are then<br>computed based on the softmax of dot products between queries and keys, SO<br>that each element of the query sequence attends to all elements of the key-value<br>sequence (i is a query index and j a key-value index):</p>",
            "id": 155,
            "page": 20,
            "text": "where T' is the concatenation of T1, T2, T3. The attention weights a are then\ncomputed based on the softmax of dot products between queries and keys, SO\nthat each element of the query sequence attends to all elements of the key-value\nsequence (i is a query index and j a key-value index):"
        },
        {
            "bounding_box": [
                {
                    "x": 620,
                    "y": 42
                },
                {
                    "x": 1580,
                    "y": 42
                },
                {
                    "x": 1580,
                    "y": 86
                },
                {
                    "x": 620,
                    "y": 86
                }
            ],
            "category": "header",
            "html": "<header id='156' style='font-size:14px'>End-to-End Object Detection with Transformers 21</header>",
            "id": 156,
            "page": 21,
            "text": "End-to-End Object Detection with Transformers 21"
        },
        {
            "bounding_box": [
                {
                    "x": 133,
                    "y": 146
                },
                {
                    "x": 1589,
                    "y": 146
                },
                {
                    "x": 1589,
                    "y": 454
                },
                {
                    "x": 133,
                    "y": 454
                }
            ],
            "category": "paragraph",
            "html": "<p id='157' style='font-size:16px'>In our case, the positional encodings may be learnt or fixed, but are shared<br>across all attention layers for a given query /key-value sequence, SO we do not<br>explicitly write them as parameters of the attention. We give more details on<br>their exact value when describing the encoder and the decoder. The final output<br>is the aggregation of values weighted by attention weights: The i-th row is given<br>by attni (Xq, Xkv, T') = Ej=1 �i,jVj.</p>",
            "id": 157,
            "page": 21,
            "text": "In our case, the positional encodings may be learnt or fixed, but are shared\nacross all attention layers for a given query /key-value sequence, SO we do not\nexplicitly write them as parameters of the attention. We give more details on\ntheir exact value when describing the encoder and the decoder. The final output\nis the aggregation of values weighted by attention weights: The i-th row is given\nby attni (Xq, Xkv, T') = Ej=1 �i,jVj."
        },
        {
            "bounding_box": [
                {
                    "x": 133,
                    "y": 465
                },
                {
                    "x": 1590,
                    "y": 465
                },
                {
                    "x": 1590,
                    "y": 765
                },
                {
                    "x": 133,
                    "y": 765
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='158' style='font-size:16px'>Feed-forward network (FFN) layers The original transformer alternates<br>multi-head attention and so-called FFN layers [47], which are effectively multi-<br>layer 1x1 convolutions, which have Md input and output channels in our case.<br>The FFN we consider is composed of two-layers of 1x1 convolutions with ReLU<br>activations. There is also a residual connection/dropout/layernorm after the two<br>layers, similarly to equation 6.</p>",
            "id": 158,
            "page": 21,
            "text": "Feed-forward network (FFN) layers The original transformer alternates\nmulti-head attention and so-called FFN layers [47], which are effectively multi-\nlayer 1x1 convolutions, which have Md input and output channels in our case.\nThe FFN we consider is composed of two-layers of 1x1 convolutions with ReLU\nactivations. There is also a residual connection/dropout/layernorm after the two\nlayers, similarly to equation 6."
        },
        {
            "bounding_box": [
                {
                    "x": 135,
                    "y": 838
                },
                {
                    "x": 400,
                    "y": 838
                },
                {
                    "x": 400,
                    "y": 886
                },
                {
                    "x": 135,
                    "y": 886
                }
            ],
            "category": "paragraph",
            "html": "<p id='159' style='font-size:16px'>A.2 Losses</p>",
            "id": 159,
            "page": 21,
            "text": "A.2 Losses"
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 918
                },
                {
                    "x": 1590,
                    "y": 918
                },
                {
                    "x": 1590,
                    "y": 1218
                },
                {
                    "x": 134,
                    "y": 1218
                }
            ],
            "category": "paragraph",
            "html": "<p id='160' style='font-size:14px'>For completeness, we present in detail the losses used in our approach. All losses<br>are normalized by the number of objects inside the batch. Extra care must be<br>taken for distributed training: since each GPU receives a sub-batch, it is not<br>sufficient to normalize by the number of objects in the local batch, since in<br>general the sub-batches are not balanced across GPUs. Instead, it is important<br>to normalize by the total number of objects in all sub-batches.</p>",
            "id": 160,
            "page": 21,
            "text": "For completeness, we present in detail the losses used in our approach. All losses\nare normalized by the number of objects inside the batch. Extra care must be\ntaken for distributed training: since each GPU receives a sub-batch, it is not\nsufficient to normalize by the number of objects in the local batch, since in\ngeneral the sub-batches are not balanced across GPUs. Instead, it is important\nto normalize by the total number of objects in all sub-batches."
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 1238
                },
                {
                    "x": 1588,
                    "y": 1238
                },
                {
                    "x": 1588,
                    "y": 1342
                },
                {
                    "x": 134,
                    "y": 1342
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='161' style='font-size:14px'>Box loss Similarly to [41,36], we use a soft version of Intersection over Union<br>in our loss, together with a l1 loss on 6:</p>",
            "id": 161,
            "page": 21,
            "text": "Box loss Similarly to [41,36], we use a soft version of Intersection over Union\nin our loss, together with a l1 loss on 6:"
        },
        {
            "bounding_box": [
                {
                    "x": 132,
                    "y": 1473
                },
                {
                    "x": 1588,
                    "y": 1473
                },
                {
                    "x": 1588,
                    "y": 1527
                },
                {
                    "x": 132,
                    "y": 1527
                }
            ],
            "category": "paragraph",
            "html": "<p id='162' style='font-size:18px'>where 入iou, 入L1 E R are hyperparameters and Liou (·) is the generalized IoU [38]:</p>",
            "id": 162,
            "page": 21,
            "text": "where 入iou, 入L1 E R are hyperparameters and Liou (·) is the generalized IoU [38]:"
        },
        {
            "bounding_box": [
                {
                    "x": 133,
                    "y": 1722
                },
                {
                    "x": 1590,
                    "y": 1722
                },
                {
                    "x": 1590,
                    "y": 2043
                },
                {
                    "x": 133,
                    "y": 2043
                }
            ],
            "category": "paragraph",
            "html": "<p id='163' style='font-size:16px'>1.1 means \"area\" , and the union and intersection of box coordinates are used<br>as shorthands for the boxes themselves. The areas of unions or intersections<br>are computed by min / max of the linear functions of bo(i) and bi, which makes<br>the loss sufficiently well-behaved for stochastic gradients. B(bo(i), bi) means the<br>largest box containing bo(i), bi (the areas involving B are also computed based<br>on min / max of linear functions of the box coordinates).</p>",
            "id": 163,
            "page": 21,
            "text": "1.1 means \"area\" , and the union and intersection of box coordinates are used\nas shorthands for the boxes themselves. The areas of unions or intersections\nare computed by min / max of the linear functions of bo(i) and bi, which makes\nthe loss sufficiently well-behaved for stochastic gradients. B(bo(i), bi) means the\nlargest box containing bo(i), bi (the areas involving B are also computed based\non min / max of linear functions of the box coordinates)."
        },
        {
            "bounding_box": [
                {
                    "x": 132,
                    "y": 2057
                },
                {
                    "x": 1589,
                    "y": 2057
                },
                {
                    "x": 1589,
                    "y": 2211
                },
                {
                    "x": 132,
                    "y": 2211
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='164' style='font-size:16px'>DICE /F-1 loss [28] The DICE coefficient is closely related to the Intersection<br>over Union. If we denote by m the raw mask logits prediction of the model, and<br>m the binary target mask, the loss is defined as:</p>",
            "id": 164,
            "page": 21,
            "text": "DICE /F-1 loss [28] The DICE coefficient is closely related to the Intersection\nover Union. If we denote by m the raw mask logits prediction of the model, and\nm the binary target mask, the loss is defined as:"
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 2384
                },
                {
                    "x": 1578,
                    "y": 2384
                },
                {
                    "x": 1578,
                    "y": 2435
                },
                {
                    "x": 134,
                    "y": 2435
                }
            ],
            "category": "paragraph",
            "html": "<p id='165' style='font-size:16px'>where 0 is the sigmoid function. This loss is normalized by the number of objects.</p>",
            "id": 165,
            "page": 21,
            "text": "where 0 is the sigmoid function. This loss is normalized by the number of objects."
        },
        {
            "bounding_box": [
                {
                    "x": 133,
                    "y": 44
                },
                {
                    "x": 494,
                    "y": 44
                },
                {
                    "x": 494,
                    "y": 82
                },
                {
                    "x": 133,
                    "y": 82
                }
            ],
            "category": "header",
            "html": "<header id='166' style='font-size:16px'>22 Carion et al.</header>",
            "id": 166,
            "page": 22,
            "text": "22 Carion et al."
        },
        {
            "bounding_box": [
                {
                    "x": 135,
                    "y": 147
                },
                {
                    "x": 710,
                    "y": 147
                },
                {
                    "x": 710,
                    "y": 193
                },
                {
                    "x": 135,
                    "y": 193
                }
            ],
            "category": "paragraph",
            "html": "<p id='167' style='font-size:20px'>A.3 Detailed architecture</p>",
            "id": 167,
            "page": 22,
            "text": "A.3 Detailed architecture"
        },
        {
            "bounding_box": [
                {
                    "x": 135,
                    "y": 242
                },
                {
                    "x": 1592,
                    "y": 242
                },
                {
                    "x": 1592,
                    "y": 690
                },
                {
                    "x": 135,
                    "y": 690
                }
            ],
            "category": "paragraph",
            "html": "<p id='168' style='font-size:20px'>The detailed description of the transformer used in DETR, with positional en-<br>codings passed at every attention layer, is given in Fig. 10. Image features from<br>the CNN backbone are passed through the transformer encoder, together with<br>spatial positional encoding that are added to queries and keys at every multi-<br>head self-attention layer. Then, the decoder receives queries (initially set to zero),<br>output positional encoding (object queries), and encoder memory, and produces<br>the final set of predicted class labels and bounding boxes through multiple multi-<br>head self-attention and decoder-encoder attention. The first self-attention layer<br>in the first decoder layer can be skipped.</p>",
            "id": 168,
            "page": 22,
            "text": "The detailed description of the transformer used in DETR, with positional en-\ncodings passed at every attention layer, is given in Fig. 10. Image features from\nthe CNN backbone are passed through the transformer encoder, together with\nspatial positional encoding that are added to queries and keys at every multi-\nhead self-attention layer. Then, the decoder receives queries (initially set to zero),\noutput positional encoding (object queries), and encoder memory, and produces\nthe final set of predicted class labels and bounding boxes through multiple multi-\nhead self-attention and decoder-encoder attention. The first self-attention layer\nin the first decoder layer can be skipped."
        },
        {
            "bounding_box": [
                {
                    "x": 410,
                    "y": 771
                },
                {
                    "x": 1417,
                    "y": 771
                },
                {
                    "x": 1417,
                    "y": 1877
                },
                {
                    "x": 410,
                    "y": 1877
                }
            ],
            "category": "figure",
            "html": "<figure><img id='169' style='font-size:14px' alt=\"Class Bounding Box\nFFN FFN\nDecoder\nM x\nAdd & Norm\nFFN\nEncoder\nAdd & Norm\nAdd & Norm\nMulti-Head Attention\nV K Q\nFFN\n+ +\nAdd & Norm Add & Norm\nMulti-Head Self-Attention Multi-Head Self-Attention\nV K Q V K Q\n+ + + +\nImage features\nSpatial positional\nObject queries\nencoding\" data-coord=\"top-left:(410,771); bottom-right:(1417,1877)\" /></figure>",
            "id": 169,
            "page": 22,
            "text": "Class Bounding Box\nFFN FFN\nDecoder\nM x\nAdd & Norm\nFFN\nEncoder\nAdd & Norm\nAdd & Norm\nMulti-Head Attention\nV K Q\nFFN\n+ +\nAdd & Norm Add & Norm\nMulti-Head Self-Attention Multi-Head Self-Attention\nV K Q V K Q\n+ + + +\nImage features\nSpatial positional\nObject queries\nencoding"
        },
        {
            "bounding_box": [
                {
                    "x": 182,
                    "y": 1901
                },
                {
                    "x": 1534,
                    "y": 1901
                },
                {
                    "x": 1534,
                    "y": 1948
                },
                {
                    "x": 182,
                    "y": 1948
                }
            ],
            "category": "caption",
            "html": "<caption id='170' style='font-size:18px'>Fig. 10: Architecture of DETR's transformer. Please, see Section A.3 for details.</caption>",
            "id": 170,
            "page": 22,
            "text": "Fig. 10: Architecture of DETR's transformer. Please, see Section A.3 for details."
        },
        {
            "bounding_box": [
                {
                    "x": 132,
                    "y": 2082
                },
                {
                    "x": 1601,
                    "y": 2082
                },
                {
                    "x": 1601,
                    "y": 2434
                },
                {
                    "x": 132,
                    "y": 2434
                }
            ],
            "category": "paragraph",
            "html": "<p id='171' style='font-size:22px'>Computational complexity Every self-attention in the encoder has complex-<br>ity O(d2 HW +d(HW)2): O(d' d) is the cost of computing a single query /key / value<br>embeddings (and Md' = d), while O(d' (HW)2) is the cost of computing the at-<br>tention weights for one head. Other computations are negligible. In the decoder,<br>each self-attention is in O(d2N +dN2), and cross-attention between encoder and<br>decoder is in O(d2 (N + HW) + dNHW), which is much lower than the encoder<br>since N 《 HW in practice.</p>",
            "id": 171,
            "page": 22,
            "text": "Computational complexity Every self-attention in the encoder has complex-\nity O(d2 HW +d(HW)2): O(d' d) is the cost of computing a single query /key / value\nembeddings (and Md' = d), while O(d' (HW)2) is the cost of computing the at-\ntention weights for one head. Other computations are negligible. In the decoder,\neach self-attention is in O(d2N +dN2), and cross-attention between encoder and\ndecoder is in O(d2 (N + HW) + dNHW), which is much lower than the encoder\nsince N 《 HW in practice."
        },
        {
            "bounding_box": [
                {
                    "x": 620,
                    "y": 42
                },
                {
                    "x": 1585,
                    "y": 42
                },
                {
                    "x": 1585,
                    "y": 86
                },
                {
                    "x": 620,
                    "y": 86
                }
            ],
            "category": "header",
            "html": "<header id='172' style='font-size:14px'>End-to-End Object Detection with Transformers 23</header>",
            "id": 172,
            "page": 23,
            "text": "End-to-End Object Detection with Transformers 23"
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 144
                },
                {
                    "x": 1591,
                    "y": 144
                },
                {
                    "x": 1591,
                    "y": 446
                },
                {
                    "x": 134,
                    "y": 446
                }
            ],
            "category": "paragraph",
            "html": "<p id='173' style='font-size:18px'>FLOPS computation Given that the FLOPS for Faster R-CNN depends on<br>the number of proposals in the image, we report the average number of FLOPS<br>for the first 100 images in the COCO 2017 validation set. We compute the<br>FLOPS with the tool flop_count_operators from Detectron2 [50]. We use it<br>without modifications for Detectron2 models, and extend it to take batch matrix<br>multiply (bmm) into account for DETR models.</p>",
            "id": 173,
            "page": 23,
            "text": "FLOPS computation Given that the FLOPS for Faster R-CNN depends on\nthe number of proposals in the image, we report the average number of FLOPS\nfor the first 100 images in the COCO 2017 validation set. We compute the\nFLOPS with the tool flop_count_operators from Detectron2 [50]. We use it\nwithout modifications for Detectron2 models, and extend it to take batch matrix\nmultiply (bmm) into account for DETR models."
        },
        {
            "bounding_box": [
                {
                    "x": 135,
                    "y": 504
                },
                {
                    "x": 813,
                    "y": 504
                },
                {
                    "x": 813,
                    "y": 554
                },
                {
                    "x": 135,
                    "y": 554
                }
            ],
            "category": "paragraph",
            "html": "<p id='174' style='font-size:22px'>A.4 Training hyperparameters</p>",
            "id": 174,
            "page": 23,
            "text": "A.4 Training hyperparameters"
        },
        {
            "bounding_box": [
                {
                    "x": 135,
                    "y": 568
                },
                {
                    "x": 1591,
                    "y": 568
                },
                {
                    "x": 1591,
                    "y": 768
                },
                {
                    "x": 135,
                    "y": 768
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='175' style='font-size:18px'>We train DETR using Adam W [26] with improved weight decay handling, set to<br>10-4. We also apply gradient clipping, with a maximal gradient norm of 0.1. The<br>backbone and the transformers are treated slightly differently, we now discuss<br>the details for both.</p>",
            "id": 175,
            "page": 23,
            "text": "We train DETR using Adam W [26] with improved weight decay handling, set to\n10-4. We also apply gradient clipping, with a maximal gradient norm of 0.1. The\nbackbone and the transformers are treated slightly differently, we now discuss\nthe details for both."
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 785
                },
                {
                    "x": 1607,
                    "y": 785
                },
                {
                    "x": 1607,
                    "y": 1137
                },
                {
                    "x": 134,
                    "y": 1137
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='176' style='font-size:20px'>Backbone ImageNet pretrained backbone ResNet-50 is imported from Torchvi-<br>sion, discarding the last classification layer. Backbone batch normalization weights<br>and statistics are frozen during training, following widely adopted practice in ob-<br>ject detection. We fine-tune the backbone using learning rate of 10-5. We observe<br>that having the backbone learning rate roughly an order of magnitude smaller<br>than the rest of the network is important to stabilize training, especially in the<br>first few epochs.</p>",
            "id": 176,
            "page": 23,
            "text": "Backbone ImageNet pretrained backbone ResNet-50 is imported from Torchvi-\nsion, discarding the last classification layer. Backbone batch normalization weights\nand statistics are frozen during training, following widely adopted practice in ob-\nject detection. We fine-tune the backbone using learning rate of 10-5. We observe\nthat having the backbone learning rate roughly an order of magnitude smaller\nthan the rest of the network is important to stabilize training, especially in the\nfirst few epochs."
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 1155
                },
                {
                    "x": 1591,
                    "y": 1155
                },
                {
                    "x": 1591,
                    "y": 1306
                },
                {
                    "x": 134,
                    "y": 1306
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='177' style='font-size:18px'>Transformer We train the transformer with a learning rate of 10-4. Additive<br>dropout of 0.1 is applied after every multi-head attention and FFN before layer<br>normalization. The weights are randomly initialized with Xavier initialization.</p>",
            "id": 177,
            "page": 23,
            "text": "Transformer We train the transformer with a learning rate of 10-4. Additive\ndropout of 0.1 is applied after every multi-head attention and FFN before layer\nnormalization. The weights are randomly initialized with Xavier initialization."
        },
        {
            "bounding_box": [
                {
                    "x": 132,
                    "y": 1323
                },
                {
                    "x": 1589,
                    "y": 1323
                },
                {
                    "x": 1589,
                    "y": 1474
                },
                {
                    "x": 132,
                    "y": 1474
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='178' style='font-size:16px'>Losses We use linear combination of l1 and GIoU losses for bounding box re-<br>gression with 入L1 = 5 and 入iou = 2 weights respectively. All models were trained<br>with N = 100 decoder query slots.</p>",
            "id": 178,
            "page": 23,
            "text": "Losses We use linear combination of l1 and GIoU losses for bounding box re-\ngression with 入L1 = 5 and 入iou = 2 weights respectively. All models were trained\nwith N = 100 decoder query slots."
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 1491
                },
                {
                    "x": 1591,
                    "y": 1491
                },
                {
                    "x": 1591,
                    "y": 1843
                },
                {
                    "x": 134,
                    "y": 1843
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='179' style='font-size:18px'>Baseline Our enhanced Faster-RCNN+ baselines use GIoU [38] loss along with<br>the standard l1 loss for bounding box regression. We performed a grid search<br>to find the best weights for the losses and the final models use only GIoU loss<br>with weights 20 and 1 for box and proposal regression tasks respectively. For the<br>baselines we adopt the same data augmentation as used in DETR and train it<br>with 9x schedule (approximately 109 epochs). All other settings are identical to<br>the same models in the Detectron2 model ZOO [50].</p>",
            "id": 179,
            "page": 23,
            "text": "Baseline Our enhanced Faster-RCNN+ baselines use GIoU [38] loss along with\nthe standard l1 loss for bounding box regression. We performed a grid search\nto find the best weights for the losses and the final models use only GIoU loss\nwith weights 20 and 1 for box and proposal regression tasks respectively. For the\nbaselines we adopt the same data augmentation as used in DETR and train it\nwith 9x schedule (approximately 109 epochs). All other settings are identical to\nthe same models in the Detectron2 model ZOO [50]."
        },
        {
            "bounding_box": [
                {
                    "x": 133,
                    "y": 1859
                },
                {
                    "x": 1590,
                    "y": 1859
                },
                {
                    "x": 1590,
                    "y": 2212
                },
                {
                    "x": 133,
                    "y": 2212
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='180' style='font-size:18px'>Spatial positional encoding Encoder activations are associated with corre-<br>sponding spatial positions of image features. In our model we use a fixed absolute<br>encoding to represent these spatial positions. We adopt a generalization of the<br>original Transformer [47] encoding to the 2D case [31]. Specifically, for both<br>spatial coordinates of each embedding we independently use S2 sine and cosine<br>functions with different frequencies. We then concatenate them to get the final<br>d channel positional encoding.</p>",
            "id": 180,
            "page": 23,
            "text": "Spatial positional encoding Encoder activations are associated with corre-\nsponding spatial positions of image features. In our model we use a fixed absolute\nencoding to represent these spatial positions. We adopt a generalization of the\noriginal Transformer [47] encoding to the 2D case [31]. Specifically, for both\nspatial coordinates of each embedding we independently use S2 sine and cosine\nfunctions with different frequencies. We then concatenate them to get the final\nd channel positional encoding."
        },
        {
            "bounding_box": [
                {
                    "x": 137,
                    "y": 2269
                },
                {
                    "x": 643,
                    "y": 2269
                },
                {
                    "x": 643,
                    "y": 2317
                },
                {
                    "x": 137,
                    "y": 2317
                }
            ],
            "category": "paragraph",
            "html": "<p id='181' style='font-size:20px'>A.5 Additional results</p>",
            "id": 181,
            "page": 23,
            "text": "A.5 Additional results"
        },
        {
            "bounding_box": [
                {
                    "x": 133,
                    "y": 2334
                },
                {
                    "x": 1584,
                    "y": 2334
                },
                {
                    "x": 1584,
                    "y": 2433
                },
                {
                    "x": 133,
                    "y": 2433
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='182' style='font-size:18px'>Some extra qualitative results for the panoptic prediction of the DETR-R101<br>model are shown in Fig.11.</p>",
            "id": 182,
            "page": 23,
            "text": "Some extra qualitative results for the panoptic prediction of the DETR-R101\nmodel are shown in Fig.11."
        },
        {
            "bounding_box": [
                {
                    "x": 133,
                    "y": 44
                },
                {
                    "x": 494,
                    "y": 44
                },
                {
                    "x": 494,
                    "y": 83
                },
                {
                    "x": 133,
                    "y": 83
                }
            ],
            "category": "header",
            "html": "<header id='183' style='font-size:16px'>24 Carion et al.</header>",
            "id": 183,
            "page": 24,
            "text": "24 Carion et al."
        },
        {
            "bounding_box": [
                {
                    "x": 152,
                    "y": 138
                },
                {
                    "x": 1572,
                    "y": 138
                },
                {
                    "x": 1572,
                    "y": 452
                },
                {
                    "x": 152,
                    "y": 452
                }
            ],
            "category": "figure",
            "html": "<figure><img id='184' style='font-size:14px' alt=\"sky\nFairFairplanelane\nairplane\" data-coord=\"top-left:(152,138); bottom-right:(1572,452)\" /></figure>",
            "id": 184,
            "page": 24,
            "text": "sky\nFairFairplanelane\nairplane"
        },
        {
            "bounding_box": [
                {
                    "x": 137,
                    "y": 471
                },
                {
                    "x": 1588,
                    "y": 471
                },
                {
                    "x": 1588,
                    "y": 563
                },
                {
                    "x": 137,
                    "y": 563
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='185' style='font-size:18px'>(a) Failure case with overlapping objects. PanopticFPN misses one plane entirely, while<br>DETR fails to accurately segment 3 of them.</p>",
            "id": 185,
            "page": 24,
            "text": "(a) Failure case with overlapping objects. PanopticFPN misses one plane entirely, while\nDETR fails to accurately segment 3 of them."
        },
        {
            "bounding_box": [
                {
                    "x": 151,
                    "y": 566
                },
                {
                    "x": 1572,
                    "y": 566
                },
                {
                    "x": 1572,
                    "y": 879
                },
                {
                    "x": 151,
                    "y": 879
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='186' style='font-size:14px' alt=\"sky SBV sky\nairplane airplane\nroad\ngrass\ngrass\" data-coord=\"top-left:(151,566); bottom-right:(1572,879)\" /></figure>",
            "id": 186,
            "page": 24,
            "text": "sky SBV sky\nairplane airplane\nroad\ngrass\ngrass"
        },
        {
            "bounding_box": [
                {
                    "x": 138,
                    "y": 898
                },
                {
                    "x": 1587,
                    "y": 898
                },
                {
                    "x": 1587,
                    "y": 991
                },
                {
                    "x": 138,
                    "y": 991
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='187' style='font-size:18px'>(b) Things masks are predicted at full resolution, which allows sharper boundaries than<br>PanopticFPN</p>",
            "id": 187,
            "page": 24,
            "text": "(b) Things masks are predicted at full resolution, which allows sharper boundaries than\nPanopticFPN"
        },
        {
            "bounding_box": [
                {
                    "x": 135,
                    "y": 1031
                },
                {
                    "x": 1587,
                    "y": 1031
                },
                {
                    "x": 1587,
                    "y": 1123
                },
                {
                    "x": 135,
                    "y": 1123
                }
            ],
            "category": "paragraph",
            "html": "<p id='188' style='font-size:20px'>Fig. 11: Comparison of panoptic predictions. From left to right: Ground truth, Panop-<br>ticFPN with ResNet 101, DETR with ResNet 101</p>",
            "id": 188,
            "page": 24,
            "text": "Fig. 11: Comparison of panoptic predictions. From left to right: Ground truth, Panop-\nticFPN with ResNet 101, DETR with ResNet 101"
        },
        {
            "bounding_box": [
                {
                    "x": 135,
                    "y": 1229
                },
                {
                    "x": 1590,
                    "y": 1229
                },
                {
                    "x": 1590,
                    "y": 1981
                },
                {
                    "x": 135,
                    "y": 1981
                }
            ],
            "category": "paragraph",
            "html": "<p id='189' style='font-size:20px'>Increasing the number of instances By design, DETR cannot predict more<br>objects than it has query slots, i.e. 100 in our experiments. In this section,<br>we analyze the behavior of DETR when approaching this limit. We select a<br>canonical square image of a given class, repeat it on a 10 x 10 grid, and compute<br>the percentage of instances that are missed by the model. To test the model with<br>less than 100 instances, we randomly mask some of the cells. This ensures that<br>the absolute size of the objects is the same no matter how many are visible. To<br>account for the randomness in the masking, we repeat the experiment 100 times<br>with different masks. The results are shown in Fig.12. The behavior is similar<br>across classes, and while the model detects all instances when up to 50 are<br>visible, it then starts saturating and misses more and more instances. Notably,<br>when the image contains all 100 instances, the model only detects 30 on average,<br>which is less than if the image contains only 50 instances that are all detected.<br>The counter-intuitive behavior of the model is likely because the images and the<br>detections are far from the training distribution.</p>",
            "id": 189,
            "page": 24,
            "text": "Increasing the number of instances By design, DETR cannot predict more\nobjects than it has query slots, i.e. 100 in our experiments. In this section,\nwe analyze the behavior of DETR when approaching this limit. We select a\ncanonical square image of a given class, repeat it on a 10 x 10 grid, and compute\nthe percentage of instances that are missed by the model. To test the model with\nless than 100 instances, we randomly mask some of the cells. This ensures that\nthe absolute size of the objects is the same no matter how many are visible. To\naccount for the randomness in the masking, we repeat the experiment 100 times\nwith different masks. The results are shown in Fig.12. The behavior is similar\nacross classes, and while the model detects all instances when up to 50 are\nvisible, it then starts saturating and misses more and more instances. Notably,\nwhen the image contains all 100 instances, the model only detects 30 on average,\nwhich is less than if the image contains only 50 instances that are all detected.\nThe counter-intuitive behavior of the model is likely because the images and the\ndetections are far from the training distribution."
        },
        {
            "bounding_box": [
                {
                    "x": 133,
                    "y": 1987
                },
                {
                    "x": 1592,
                    "y": 1987
                },
                {
                    "x": 1592,
                    "y": 2433
                },
                {
                    "x": 133,
                    "y": 2433
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='190' style='font-size:22px'>Note that this test is a test of generalization out-of-distribution by design,<br>since there are very few example images with a lot of instances of a single class.<br>It is difficult to disentangle, from the experiment, two types of out-of-domain<br>generalization: the image itself VS the number of object per class. But since few<br>to no COCO images contain only a lot of objects of the same class, this type<br>of experiment represents our best effort to understand whether query objects<br>overfit the label and position distribution of the dataset. Overall, the experiments<br>suggests that the model does not overfit on these distributions since it yields<br>near-perfect detections up to 50 objects.</p>",
            "id": 190,
            "page": 24,
            "text": "Note that this test is a test of generalization out-of-distribution by design,\nsince there are very few example images with a lot of instances of a single class.\nIt is difficult to disentangle, from the experiment, two types of out-of-domain\ngeneralization: the image itself VS the number of object per class. But since few\nto no COCO images contain only a lot of objects of the same class, this type\nof experiment represents our best effort to understand whether query objects\noverfit the label and position distribution of the dataset. Overall, the experiments\nsuggests that the model does not overfit on these distributions since it yields\nnear-perfect detections up to 50 objects."
        },
        {
            "bounding_box": [
                {
                    "x": 620,
                    "y": 42
                },
                {
                    "x": 1585,
                    "y": 42
                },
                {
                    "x": 1585,
                    "y": 86
                },
                {
                    "x": 620,
                    "y": 86
                }
            ],
            "category": "header",
            "html": "<header id='191' style='font-size:16px'>End-to-End Object Detection with Transformers 25</header>",
            "id": 191,
            "page": 25,
            "text": "End-to-End Object Detection with Transformers 25"
        },
        {
            "bounding_box": [
                {
                    "x": 464,
                    "y": 130
                },
                {
                    "x": 1302,
                    "y": 130
                },
                {
                    "x": 1302,
                    "y": 835
                },
                {
                    "x": 464,
                    "y": 835
                }
            ],
            "category": "figure",
            "html": "<figure><img id='192' style='font-size:14px' alt=\"instances dog\n70\nperson\n60 apple\n50\n40\nmissed\n30\n20\nof\n10\n%\n0\n20 40 60 80 100\nNumber of visible instances\" data-coord=\"top-left:(464,130); bottom-right:(1302,835)\" /></figure>",
            "id": 192,
            "page": 25,
            "text": "instances dog\n70\nperson\n60 apple\n50\n40\nmissed\n30\n20\nof\n10\n%\n0\n20 40 60 80 100\nNumber of visible instances"
        },
        {
            "bounding_box": [
                {
                    "x": 135,
                    "y": 871
                },
                {
                    "x": 1589,
                    "y": 871
                },
                {
                    "x": 1589,
                    "y": 1060
                },
                {
                    "x": 135,
                    "y": 1060
                }
            ],
            "category": "caption",
            "html": "<caption id='193' style='font-size:16px'>Fig. 12: Analysis of the number of instances of various classes missed by DETR de-<br>pending on how many are present in the image. We report the mean and the standard<br>deviation. As the number of instances gets close to 100, DETR starts saturating and<br>misses more and more objects</caption>",
            "id": 193,
            "page": 25,
            "text": "Fig. 12: Analysis of the number of instances of various classes missed by DETR de-\npending on how many are present in the image. We report the mean and the standard\ndeviation. As the number of instances gets close to 100, DETR starts saturating and\nmisses more and more objects"
        },
        {
            "bounding_box": [
                {
                    "x": 136,
                    "y": 1157
                },
                {
                    "x": 758,
                    "y": 1157
                },
                {
                    "x": 758,
                    "y": 1208
                },
                {
                    "x": 136,
                    "y": 1208
                }
            ],
            "category": "paragraph",
            "html": "<p id='194' style='font-size:20px'>A.6 PyTorch inference code</p>",
            "id": 194,
            "page": 25,
            "text": "A.6 PyTorch inference code"
        },
        {
            "bounding_box": [
                {
                    "x": 134,
                    "y": 1239
                },
                {
                    "x": 1591,
                    "y": 1239
                },
                {
                    "x": 1591,
                    "y": 1739
                },
                {
                    "x": 134,
                    "y": 1739
                }
            ],
            "category": "paragraph",
            "html": "<p id='195' style='font-size:18px'>To demonstrate the simplicity of the approach, we include inference code with<br>PyTorch and Torchvision libraries in Listing 1. The code runs with Python 3.6+,<br>PyTorch 1.4 and Torchvision 0.5. Note that it does not support batching, hence<br>it is suitable only for inference or training with DistributedDataParallel with<br>one image per GPU. Also note that for clarity, this code uses learnt positional<br>encodings in the encoder instead of fixed, and positional encodings are added<br>to the input only instead of at each transformer layer. Making these changes<br>requires going beyond PyTorch implementation of transformers, which hampers<br>readability. The entire code to reproduce the experiments will be made available<br>before the conference.</p>",
            "id": 195,
            "page": 25,
            "text": "To demonstrate the simplicity of the approach, we include inference code with\nPyTorch and Torchvision libraries in Listing 1. The code runs with Python 3.6+,\nPyTorch 1.4 and Torchvision 0.5. Note that it does not support batching, hence\nit is suitable only for inference or training with DistributedDataParallel with\none image per GPU. Also note that for clarity, this code uses learnt positional\nencodings in the encoder instead of fixed, and positional encodings are added\nto the input only instead of at each transformer layer. Making these changes\nrequires going beyond PyTorch implementation of transformers, which hampers\nreadability. The entire code to reproduce the experiments will be made available\nbefore the conference."
        },
        {
            "bounding_box": [
                {
                    "x": 137,
                    "y": 45
                },
                {
                    "x": 492,
                    "y": 45
                },
                {
                    "x": 492,
                    "y": 81
                },
                {
                    "x": 137,
                    "y": 81
                }
            ],
            "category": "header",
            "html": "<header id='196' style='font-size:22px'>26 Carion et al.</header>",
            "id": 196,
            "page": 26,
            "text": "26 Carion et al."
        },
        {
            "bounding_box": [
                {
                    "x": 77,
                    "y": 534
                },
                {
                    "x": 93,
                    "y": 534
                },
                {
                    "x": 93,
                    "y": 553
                },
                {
                    "x": 77,
                    "y": 553
                }
            ],
            "category": "paragraph",
            "html": "<p id='197' style='font-size:14px'>1</p>",
            "id": 197,
            "page": 26,
            "text": "1"
        },
        {
            "bounding_box": [
                {
                    "x": 139,
                    "y": 531
                },
                {
                    "x": 330,
                    "y": 531
                },
                {
                    "x": 330,
                    "y": 549
                },
                {
                    "x": 139,
                    "y": 549
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='198' style='font-size:20px'>import torch</p>",
            "id": 198,
            "page": 26,
            "text": "import torch"
        },
        {
            "bounding_box": [
                {
                    "x": 138,
                    "y": 564
                },
                {
                    "x": 745,
                    "y": 564
                },
                {
                    "x": 745,
                    "y": 625
                },
                {
                    "x": 138,
                    "y": 625
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='199' style='font-size:18px'>from torch import nn<br>from torchvision.models import resnet50</p>",
            "id": 199,
            "page": 26,
            "text": "from torch import nn\nfrom torchvision.models import resnet50"
        },
        {
            "bounding_box": [
                {
                    "x": 139,
                    "y": 660
                },
                {
                    "x": 478,
                    "y": 660
                },
                {
                    "x": 478,
                    "y": 689
                },
                {
                    "x": 139,
                    "y": 689
                }
            ],
            "category": "paragraph",
            "html": "<p id='200' style='font-size:18px'>class DETR (nn.Module) :</p>",
            "id": 200,
            "page": 26,
            "text": "class DETR (nn.Module) :"
        },
        {
            "bounding_box": [
                {
                    "x": 198,
                    "y": 726
                },
                {
                    "x": 1022,
                    "y": 726
                },
                {
                    "x": 1022,
                    "y": 790
                },
                {
                    "x": 198,
                    "y": 790
                }
            ],
            "category": "paragraph",
            "html": "<p id='201' style='font-size:20px'>def init__ (self , num_classes, hidden_dim, nheads,<br>num_encoder_layers, num_decoder_layers) :</p>",
            "id": 201,
            "page": 26,
            "text": "def init__ (self , num_classes, hidden_dim, nheads,\nnum_encoder_layers, num_decoder_layers) :"
        },
        {
            "bounding_box": [
                {
                    "x": 261,
                    "y": 794
                },
                {
                    "x": 544,
                    "y": 794
                },
                {
                    "x": 544,
                    "y": 821
                },
                {
                    "x": 261,
                    "y": 821
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='202' style='font-size:16px'>super() · init ()</p>",
            "id": 202,
            "page": 26,
            "text": "super() · init ()"
        },
        {
            "bounding_box": [
                {
                    "x": 263,
                    "y": 827
                },
                {
                    "x": 1121,
                    "y": 827
                },
                {
                    "x": 1121,
                    "y": 854
                },
                {
                    "x": 263,
                    "y": 854
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='203' style='font-size:20px'># We take only convolutional layers from ResNet-50 model</p>",
            "id": 203,
            "page": 26,
            "text": "# We take only convolutional layers from ResNet-50 model"
        },
        {
            "bounding_box": [
                {
                    "x": 261,
                    "y": 859
                },
                {
                    "x": 1484,
                    "y": 859
                },
                {
                    "x": 1484,
                    "y": 956
                },
                {
                    "x": 261,
                    "y": 956
                }
            ],
            "category": "paragraph",
            "html": "<p id='204' style='font-size:18px'>self backbone = nn. Sequential (*list (resnet50(pretrained=True) · children ()) [:-2])<br>self · conv = nn. Conv2d(2048, hidden_dim, 1)<br>self · transformer = nn. Transformer (hidden_dim, nheads,</p>",
            "id": 204,
            "page": 26,
            "text": "self backbone = nn. Sequential (*list (resnet50(pretrained=True) · children ()) [:-2])\nself · conv = nn. Conv2d(2048, hidden_dim, 1)\nself · transformer = nn. Transformer (hidden_dim, nheads,"
        },
        {
            "bounding_box": [
                {
                    "x": 785,
                    "y": 961
                },
                {
                    "x": 1391,
                    "y": 961
                },
                {
                    "x": 1391,
                    "y": 989
                },
                {
                    "x": 785,
                    "y": 989
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='205' style='font-size:20px'>num_encoder_layers, num_decoder_layers)</p>",
            "id": 205,
            "page": 26,
            "text": "num_encoder_layers, num_decoder_layers)"
        },
        {
            "bounding_box": [
                {
                    "x": 261,
                    "y": 999
                },
                {
                    "x": 1230,
                    "y": 999
                },
                {
                    "x": 1230,
                    "y": 1156
                },
                {
                    "x": 261,
                    "y": 1156
                }
            ],
            "category": "paragraph",
            "html": "<p id='206' style='font-size:16px'>self linear_class = nn . Linear (hidden_dim, num_classes + 1)<br>self · linear_bbox = nn. Linear (hidden_dim, 4)<br>self · query_pos = nn · Parameter (torch. rand (100, hidden_dim))<br>self · row_embed = nn · Parameter(torch. rand (50, hidden_dim // 2))<br>self · col_embed = nn · Parameter (torch. rand(50, hidden_dim // 2))</p>",
            "id": 206,
            "page": 26,
            "text": "self linear_class = nn . Linear (hidden_dim, num_classes + 1)\nself · linear_bbox = nn. Linear (hidden_dim, 4)\nself · query_pos = nn · Parameter (torch. rand (100, hidden_dim))\nself · row_embed = nn · Parameter(torch. rand (50, hidden_dim // 2))\nself · col_embed = nn · Parameter (torch. rand(50, hidden_dim // 2))"
        },
        {
            "bounding_box": [
                {
                    "x": 196,
                    "y": 1190
                },
                {
                    "x": 605,
                    "y": 1190
                },
                {
                    "x": 605,
                    "y": 1220
                },
                {
                    "x": 196,
                    "y": 1220
                }
            ],
            "category": "paragraph",
            "html": "<p id='207' style='font-size:20px'>def forward (self , inputs) :</p>",
            "id": 207,
            "page": 26,
            "text": "def forward (self , inputs) :"
        },
        {
            "bounding_box": [
                {
                    "x": 259,
                    "y": 1225
                },
                {
                    "x": 651,
                    "y": 1225
                },
                {
                    "x": 651,
                    "y": 1353
                },
                {
                    "x": 259,
                    "y": 1353
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='208' style='font-size:16px'>x = self · backbone (inputs)<br>h = self · conv(x)<br>H, W = h. shape [-2:]<br>pos = torch · cat([</p>",
            "id": 208,
            "page": 26,
            "text": "x = self · backbone (inputs)\nh = self · conv(x)\nH, W = h. shape [-2:]\npos = torch · cat(["
        },
        {
            "bounding_box": [
                {
                    "x": 261,
                    "y": 1356
                },
                {
                    "x": 1155,
                    "y": 1356
                },
                {
                    "x": 1155,
                    "y": 1555
                },
                {
                    "x": 261,
                    "y": 1555
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='209' style='font-size:16px'>self · col_embed[:W] · unsqueeze (0) · repeat (H, 1, 1) ,<br>self · row_embed[:H] · unsqueeze (1) · repeat(1, W, 1) ,<br>] , dim=-1) · flatten(0, 1) · unsqueeze(1)<br>h = self · transformer (pos + h. flatten(2) · permute(2, 0, 1) ,<br>self · query_pos. unsqueeze(1))<br>return self linear_class(h) , self linear_bbox(h) · sigmoid()</p>",
            "id": 209,
            "page": 26,
            "text": "self · col_embed[:W] · unsqueeze (0) · repeat (H, 1, 1) ,\nself · row_embed[:H] · unsqueeze (1) · repeat(1, W, 1) ,\n] , dim=-1) · flatten(0, 1) · unsqueeze(1)\nh = self · transformer (pos + h. flatten(2) · permute(2, 0, 1) ,\nself · query_pos. unsqueeze(1))\nreturn self linear_class(h) , self linear_bbox(h) · sigmoid()"
        },
        {
            "bounding_box": [
                {
                    "x": 133,
                    "y": 1590
                },
                {
                    "x": 1639,
                    "y": 1590
                },
                {
                    "x": 1639,
                    "y": 1653
                },
                {
                    "x": 133,
                    "y": 1653
                }
            ],
            "category": "paragraph",
            "html": "<p id='210' style='font-size:18px'>detr = DETR (num_classes=91, hidden_dim=256, nheads=8, num_encoder_layers=6, num_decoder_layers=6)<br>detr . eval ()</p>",
            "id": 210,
            "page": 26,
            "text": "detr = DETR (num_classes=91, hidden_dim=256, nheads=8, num_encoder_layers=6, num_decoder_layers=6)\ndetr . eval ()"
        },
        {
            "bounding_box": [
                {
                    "x": 136,
                    "y": 1657
                },
                {
                    "x": 713,
                    "y": 1657
                },
                {
                    "x": 713,
                    "y": 1719
                },
                {
                    "x": 136,
                    "y": 1719
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='211' style='font-size:20px'>inputs = torch. randn (1, 3, 800, 1200)<br>logits, bboxes = detr (inputs)</p>",
            "id": 211,
            "page": 26,
            "text": "inputs = torch. randn (1, 3, 800, 1200)\nlogits, bboxes = detr (inputs)"
        },
        {
            "bounding_box": [
                {
                    "x": 137,
                    "y": 1799
                },
                {
                    "x": 1592,
                    "y": 1799
                },
                {
                    "x": 1592,
                    "y": 2027
                },
                {
                    "x": 137,
                    "y": 2027
                }
            ],
            "category": "paragraph",
            "html": "<p id='212' style='font-size:22px'>Listing 1: DETR PyTorch inference code. For clarity it uses learnt positional encod-<br>ings in the encoder instead of fixed, and positional encodings are added to the input<br>only instead of at each transformer layer. Making these changes requires going beyond<br>PyTorch implementation of transformers, which hampers readability. The entire code<br>to reproduce the experiments will be made available before the conference.</p>",
            "id": 212,
            "page": 26,
            "text": "Listing 1: DETR PyTorch inference code. For clarity it uses learnt positional encod-\nings in the encoder instead of fixed, and positional encodings are added to the input\nonly instead of at each transformer layer. Making these changes requires going beyond\nPyTorch implementation of transformers, which hampers readability. The entire code\nto reproduce the experiments will be made available before the conference."
        },
        {
            "bounding_box": [
                {
                    "x": 56,
                    "y": 570
                },
                {
                    "x": 100,
                    "y": 570
                },
                {
                    "x": 100,
                    "y": 1712
                },
                {
                    "x": 56,
                    "y": 1712
                }
            ],
            "category": "footer",
            "html": "<br><footer id='213' style='font-size:14px'>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36</footer>",
            "id": 213,
            "page": 26,
            "text": "2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36"
        }
    ]
}