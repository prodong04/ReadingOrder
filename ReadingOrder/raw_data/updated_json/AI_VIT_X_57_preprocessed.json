{
    "id": "32b27fa2-0f94-11ef-8828-426932df3dcf",
    "pdf_path": "/root/data/pdf/2112.10752v2.pdf",
    "elements": [
        {
            "bounding_box": [
                {
                    "x": 430,
                    "y": 291
                },
                {
                    "x": 2049,
                    "y": 291
                },
                {
                    "x": 2049,
                    "y": 356
                },
                {
                    "x": 430,
                    "y": 356
                }
            ],
            "category": "paragraph",
            "html": "<p id='0' style='font-size:22px'>High-Resolution Image Synthesis with Latent Diffusion Models</p>",
            "id": 0,
            "page": 1,
            "text": "High-Resolution Image Synthesis with Latent Diffusion Models"
        },
        {
            "bounding_box": [
                {
                    "x": 223,
                    "y": 448
                },
                {
                    "x": 2407,
                    "y": 448
                },
                {
                    "x": 2407,
                    "y": 510
                },
                {
                    "x": 223,
                    "y": 510
                }
            ],
            "category": "paragraph",
            "html": "<p id='1' style='font-size:20px'>Robin Rombach1 * Andreas Blattmann1 * Dominik Lorenz1 Patrick Esser�B Bjorn Ommer1</p>",
            "id": 1,
            "page": 1,
            "text": "Robin Rombach1 * Andreas Blattmann1 * Dominik Lorenz1 Patrick Esser�B Bjorn Ommer1"
        },
        {
            "bounding_box": [
                {
                    "x": 532,
                    "y": 520
                },
                {
                    "x": 2112,
                    "y": 520
                },
                {
                    "x": 2112,
                    "y": 624
                },
                {
                    "x": 532,
                    "y": 624
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='2' style='font-size:20px'>R Runway ML<br>1 Ludwig Maximilian University of Munich & IWR, Heidelberg University, Germany<br>https : / \\ gi thub · com / CompVis / latent -di ffusion</p>",
            "id": 2,
            "page": 1,
            "text": "R Runway ML 1 Ludwig Maximilian University of Munich & IWR, Heidelberg University, Germany https : / \\ gi thub · com / CompVis / latent -di ffusion"
        },
        {
            "bounding_box": [
                {
                    "x": 604,
                    "y": 743
                },
                {
                    "x": 796,
                    "y": 743
                },
                {
                    "x": 796,
                    "y": 793
                },
                {
                    "x": 604,
                    "y": 793
                }
            ],
            "category": "paragraph",
            "html": "<p id='3' style='font-size:22px'>Abstract</p>",
            "id": 3,
            "page": 1,
            "text": "Abstract"
        },
        {
            "bounding_box": [
                {
                    "x": 1375,
                    "y": 761
                },
                {
                    "x": 1446,
                    "y": 761
                },
                {
                    "x": 1446,
                    "y": 791
                },
                {
                    "x": 1375,
                    "y": 791
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='4' style='font-size:14px'>Input</p>",
            "id": 4,
            "page": 1,
            "text": "Input"
        },
        {
            "bounding_box": [
                {
                    "x": 1547,
                    "y": 738
                },
                {
                    "x": 2256,
                    "y": 738
                },
                {
                    "x": 2256,
                    "y": 790
                },
                {
                    "x": 1547,
                    "y": 790
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='5' style='font-size:14px'>ours (f =4) DALL-E (f =8) VQGAN (f = 16)<br>PSNR: 27.4 R-FID: 0.58 PSNR: 22.8 R-FID: 32.01 PSNR: 19.9 R-FID: 4.98</p>",
            "id": 5,
            "page": 1,
            "text": "ours (f =4) DALL-E (f =8) VQGAN (f = 16) PSNR: 27.4 R-FID: 0.58 PSNR: 22.8 R-FID: 32.01 PSNR: 19.9 R-FID: 4.98"
        },
        {
            "bounding_box": [
                {
                    "x": 198,
                    "y": 847
                },
                {
                    "x": 1199,
                    "y": 847
                },
                {
                    "x": 1199,
                    "y": 2197
                },
                {
                    "x": 198,
                    "y": 2197
                }
            ],
            "category": "paragraph",
            "html": "<p id='6' style='font-size:18px'>By decomposing the image formation process into a se-<br>quential application of denoising autoencoders, diffusion<br>models (DMs) achieve state-of-the-art synthesis results on<br>image data and beyond. Additionally, their formulation al-<br>lows for a guiding mechanism to control the image gen-<br>eration process without retraining. However, since these<br>models typically operate directly in pixel space, optimiza-<br>tion of powerful DMs often consumes hundreds of GPU<br>days and inference is expensive due to sequential evalu-<br>ations. To enable DM training on limited computational<br>resources while retaining their quality and flexibility, we<br>apply them in the latent space of powerful pretrained au-<br>toencoders. In contrast to previous work, training diffusion<br>models on such a representation allows for the first time<br>to reach a near-optimal point between complexity reduc-<br>tion and detail preservation, greatly boosting visual fidelity.<br>By introducing cross-attention layers into the model archi-<br>tecture, we turn diffusion models into powerful and flexi-<br>ble generators for general conditioning inputs such as text<br>or bounding boxes and high-resolution synthesis becomes<br>possible in a convolutional manner. Our latent diffusion<br>models (LDMs) achieve new state-of-the-art scores for im-<br>age inpainting and class-conditional image synthesis and<br>highly competitive performance on various tasks, includ-<br>ing text-to-image synthesis, unconditional image generation<br>and super-resolution, while significantly reducing computa-<br>tional requirements compared to pixel-based DMs.</p>",
            "id": 6,
            "page": 1,
            "text": "By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state-of-the-art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including text-to-image synthesis, unconditional image generation and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs."
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 2232
                },
                {
                    "x": 531,
                    "y": 2232
                },
                {
                    "x": 531,
                    "y": 2284
                },
                {
                    "x": 204,
                    "y": 2284
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='7' style='font-size:20px'>1. Introduction</p>",
            "id": 7,
            "page": 1,
            "text": "1. Introduction"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2300
                },
                {
                    "x": 1199,
                    "y": 2300
                },
                {
                    "x": 1199,
                    "y": 2945
                },
                {
                    "x": 201,
                    "y": 2945
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='8' style='font-size:18px'>Image synthesis is one of the computer vision fields with<br>the most spectacular recent development, but also among<br>those with the greatest computational demands. Espe-<br>cially high-resolution synthesis of complex, natural scenes<br>is presently dominated by scaling up likelihood-based mod-<br>els, potentially containing billions of parameters in autore-<br>gressive (AR) transformers [66,67]. In contrast, the promis-<br>ing results of GANs [3, 27, 40] have been revealed to be<br>mostly confined to data with comparably limited variability<br>as their adversarial learning procedure does not easily scale<br>to modeling complex, multi-modal distributions. Recently,<br>diffusion models [82], which are built from a hierarchy of<br>denoising autoencoders, have shown to achieve impressive</p>",
            "id": 8,
            "page": 1,
            "text": "Image synthesis is one of the computer vision fields with the most spectacular recent development, but also among those with the greatest computational demands. Especially high-resolution synthesis of complex, natural scenes is presently dominated by scaling up likelihood-based models, potentially containing billions of parameters in autoregressive (AR) transformers . In contrast, the promising results of GANs  have been revealed to be mostly confined to data with comparably limited variability as their adversarial learning procedure does not easily scale to modeling complex, multi-modal distributions. Recently, diffusion models , which are built from a hierarchy of denoising autoencoders, have shown to achieve impressive"
        },
        {
            "bounding_box": [
                {
                    "x": 1284,
                    "y": 785
                },
                {
                    "x": 2275,
                    "y": 785
                },
                {
                    "x": 2275,
                    "y": 1291
                },
                {
                    "x": 1284,
                    "y": 1291
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='9' alt=\"\" data-coord=\"top-left:(1284,785); bottom-right:(2275,1291)\" /></figure>",
            "id": 9,
            "page": 1,
            "text": ""
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 1331
                },
                {
                    "x": 2276,
                    "y": 1331
                },
                {
                    "x": 2276,
                    "y": 1742
                },
                {
                    "x": 1279,
                    "y": 1742
                }
            ],
            "category": "caption",
            "html": "<caption id='10' style='font-size:16px'>Figure 1. Boosting the upper bound on achievable quality with<br>less agressive downsampling. Since diffusion models offer excel-<br>lent inductive biases for spatial data, we do not need the heavy spa-<br>tial downsampling of related generative models in latent space, but<br>can still greatly reduce the dimensionality of the data via suitable<br>autoencoding models, see Sec. 3. Images are from the DIV2K [1]<br>validation set, evaluated at 5122 We denote the spatial down-<br>px.<br>sampling factor by f. Reconstruction FIDs [29] and PSNR are<br>calculated on ImageNet-val. [12]; see also Tab. 8.</caption>",
            "id": 10,
            "page": 1,
            "text": "Figure 1. Boosting the upper bound on achievable quality with less agressive downsampling. Since diffusion models offer excellent inductive biases for spatial data, we do not need the heavy spatial downsampling of related generative models in latent space, but can still greatly reduce the dimensionality of the data via suitable autoencoding models, see Sec. 3. Images are from the DIV2K  validation set, evaluated at 5122 We denote the spatial downpx. sampling factor by f. Reconstruction FIDs  and PSNR are calculated on ImageNet-val. ; see also Tab. 8."
        },
        {
            "bounding_box": [
                {
                    "x": 255,
                    "y": 2982
                },
                {
                    "x": 982,
                    "y": 2982
                },
                {
                    "x": 982,
                    "y": 3022
                },
                {
                    "x": 255,
                    "y": 3022
                }
            ],
            "category": "paragraph",
            "html": "<p id='11' style='font-size:16px'>*The first two authors contributed equally to this work.</p>",
            "id": 11,
            "page": 1,
            "text": "*The first two authors contributed equally to this work."
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 1783
                },
                {
                    "x": 2275,
                    "y": 1783
                },
                {
                    "x": 2275,
                    "y": 3027
                },
                {
                    "x": 1278,
                    "y": 3027
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='12' style='font-size:18px'>results in image synthesis [30,85] and beyond [7,45,48,57],<br>and define the state-of-the-art in class-conditional image<br>synthesis [15,31] and super-resolution [72]. Moreover, even<br>unconditional DMs can readily be applied to tasks such<br>as inpainting and colorization [85] or stroke-based syn-<br>thesis [53], in contrast to other types of generative mod-<br>els [19, 46, 69]. Being likelihood-based models, they do not<br>exhibit mode-collapse and training instabilities as GANs<br>and, by heavily exploiting parameter sharing, they can<br>model highly complex distributions of natural images with-<br>out involving billions of parameters as in AR models [67].<br>Democratizing High-Resolution Image Synthesis DMs<br>belong to the class of likelihood-based models, whose<br>mode-covering behavior makes them prone to spend ex-<br>cessive amounts of capacity (and thus compute resources)<br>on modeling imperceptible details of the data [16, 73]. Al-<br>though the reweighted variational objective [30] aims to ad-<br>dress this by undersampling the initial denoising steps, DMs<br>are still computationally demanding, since training and<br>evaluating such a model requires repeated function evalu-<br>ations (and gradient computations) in the high-dimensional<br>space of RGB images. As an example, training the most<br>powerful DMs often takes hundreds of GPU days (e.g. 150 -<br>1000 V100 days in [15]) and repeated evaluations on a noisy<br>version of the input space render also inference expensive,</p>",
            "id": 12,
            "page": 1,
            "text": "results in image synthesis  and beyond , and define the state-of-the-art in class-conditional image synthesis  and super-resolution . Moreover, even unconditional DMs can readily be applied to tasks such as inpainting and colorization  or stroke-based synthesis , in contrast to other types of generative models . Being likelihood-based models, they do not exhibit mode-collapse and training instabilities as GANs and, by heavily exploiting parameter sharing, they can model highly complex distributions of natural images without involving billions of parameters as in AR models . Democratizing High-Resolution Image Synthesis DMs belong to the class of likelihood-based models, whose mode-covering behavior makes them prone to spend excessive amounts of capacity (and thus compute resources) on modeling imperceptible details of the data . Although the reweighted variational objective  aims to address this by undersampling the initial denoising steps, DMs are still computationally demanding, since training and evaluating such a model requires repeated function evaluations (and gradient computations) in the high-dimensional space of RGB images. As an example, training the most powerful DMs often takes hundreds of GPU days (e.g. 150 1000 V100 days in ) and repeated evaluations on a noisy version of the input space render also inference expensive,"
        },
        {
            "bounding_box": [
                {
                    "x": 60,
                    "y": 869
                },
                {
                    "x": 150,
                    "y": 869
                },
                {
                    "x": 150,
                    "y": 2328
                },
                {
                    "x": 60,
                    "y": 2328
                }
            ],
            "category": "footer",
            "html": "<br><footer id='13' style='font-size:14px'>2022<br>Apr<br>13<br>[cs.CV]<br>arXiv:2112.10752v2</footer>",
            "id": 13,
            "page": 1,
            "text": "2022 Apr 13 [cs.CV] arXiv:2112.10752v2"
        },
        {
            "bounding_box": [
                {
                    "x": 1226,
                    "y": 3057
                },
                {
                    "x": 1250,
                    "y": 3057
                },
                {
                    "x": 1250,
                    "y": 3091
                },
                {
                    "x": 1226,
                    "y": 3091
                }
            ],
            "category": "footer",
            "html": "<footer id='14' style='font-size:16px'>1</footer>",
            "id": 14,
            "page": 1,
            "text": "1"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 309
                },
                {
                    "x": 1199,
                    "y": 309
                },
                {
                    "x": 1199,
                    "y": 753
                },
                {
                    "x": 201,
                    "y": 753
                }
            ],
            "category": "paragraph",
            "html": "<p id='15' style='font-size:16px'>SO that producing 50k samples takes approximately 5 days<br>[15] on a single A100 GPU. This has two consequences for<br>the research community and users in general: Firstly, train-<br>ing such a model requires massive computational resources<br>only available to a small fraction of the field, and leaves a<br>huge carbon footprint [65, 86]. Secondly, evaluating an al-<br>ready trained model is also expensive in time and memory,<br>since the same model architecture must run sequentially for<br>a large number of steps (e.g. 25 - 1000 steps in [15]).</p>",
            "id": 15,
            "page": 2,
            "text": "SO that producing 50k samples takes approximately 5 days  on a single A100 GPU. This has two consequences for the research community and users in general: Firstly, training such a model requires massive computational resources only available to a small fraction of the field, and leaves a huge carbon footprint . Secondly, evaluating an already trained model is also expensive in time and memory, since the same model architecture must run sequentially for a large number of steps (e.g. 25 - 1000 steps in )."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 761
                },
                {
                    "x": 1200,
                    "y": 761
                },
                {
                    "x": 1200,
                    "y": 1052
                },
                {
                    "x": 202,
                    "y": 1052
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='16' style='font-size:18px'>To increase the accessibility of this powerful model class<br>and at the same time reduce its significant resource con-<br>sumption, a method is needed that reduces the computa-<br>tional complexity for both training and sampling. Reducing<br>the computational demands of DMs without impairing their<br>performance is, therefore, key to enhance their accessibility.</p>",
            "id": 16,
            "page": 2,
            "text": "To increase the accessibility of this powerful model class and at the same time reduce its significant resource consumption, a method is needed that reduces the computational complexity for both training and sampling. Reducing the computational demands of DMs without impairing their performance is, therefore, key to enhance their accessibility."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 1069
                },
                {
                    "x": 1198,
                    "y": 1069
                },
                {
                    "x": 1198,
                    "y": 1667
                },
                {
                    "x": 200,
                    "y": 1667
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='17' style='font-size:16px'>Departure to Latent Space Our approach starts with<br>the analysis of already trained diffusion models in pixel<br>space: Fig. 2 shows the rate-distortion trade-off of a trained<br>model. As with any likelihood-based model, learning can<br>be roughly divided into two stages: First is a perceptual<br>compression stage which removes high-frequency details<br>but still learns little semantic variation. In the second stage,<br>the actual generative model learns the semantic and concep-<br>tual composition of the data (semantic compression). We<br>thus aim to first find a perceptually equivalent, but compu-<br>tationally more suitable space, in which we will train diffu-<br>sion models for high-resolution image synthesis.</p>",
            "id": 17,
            "page": 2,
            "text": "Departure to Latent Space Our approach starts with the analysis of already trained diffusion models in pixel space: Fig. 2 shows the rate-distortion trade-off of a trained model. As with any likelihood-based model, learning can be roughly divided into two stages: First is a perceptual compression stage which removes high-frequency details but still learns little semantic variation. In the second stage, the actual generative model learns the semantic and conceptual composition of the data (semantic compression). We thus aim to first find a perceptually equivalent, but computationally more suitable space, in which we will train diffusion models for high-resolution image synthesis."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1674
                },
                {
                    "x": 1199,
                    "y": 1674
                },
                {
                    "x": 1199,
                    "y": 2267
                },
                {
                    "x": 201,
                    "y": 2267
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='18' style='font-size:16px'>Following common practice [11, 23, 66, 67, 96], we sep-<br>arate training into two distinct phases: First, we train<br>an autoencoder which provides a lower-dimensional (and<br>thereby efficient) representational space which is perceptu-<br>ally equivalent to the data space. Importantly, and in con-<br>trast to previous work [23,66], we do not need to rely on ex-<br>cessive spatial compression, as we train DMs in the learned<br>latent space, which exhibits better scaling properties with<br>respect to the spatial dimensionality. The reduced complex-<br>ity also provides efficient image generation from the latent<br>space with a single network pass. We dub the resulting<br>model class Latent Diffusion Models (LDMs).</p>",
            "id": 18,
            "page": 2,
            "text": "Following common practice , we separate training into two distinct phases: First, we train an autoencoder which provides a lower-dimensional (and thereby efficient) representational space which is perceptually equivalent to the data space. Importantly, and in contrast to previous work , we do not need to rely on excessive spatial compression, as we train DMs in the learned latent space, which exhibits better scaling properties with respect to the spatial dimensionality. The reduced complexity also provides efficient image generation from the latent space with a single network pass. We dub the resulting model class Latent Diffusion Models (LDMs)."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2275
                },
                {
                    "x": 1199,
                    "y": 2275
                },
                {
                    "x": 1199,
                    "y": 2718
                },
                {
                    "x": 202,
                    "y": 2718
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='19' style='font-size:18px'>A notable advantage of this approach is that we need to<br>train the universal autoencoding stage only once and can<br>therefore reuse it for multiple DM trainings or to explore<br>possibly completely different tasks [81]. This enables effi-<br>cient exploration of a large number of diffusion models for<br>various image-to-image and text-to-image tasks. For the lat-<br>ter, we design an architecture that connects transformers to<br>the DM's UNet backbone [71] and enables arbitrary types<br>of token-based conditioning mechanisms, see Sec. 3.3.</p>",
            "id": 19,
            "page": 2,
            "text": "A notable advantage of this approach is that we need to train the universal autoencoding stage only once and can therefore reuse it for multiple DM trainings or to explore possibly completely different tasks . This enables efficient exploration of a large number of diffusion models for various image-to-image and text-to-image tasks. For the latter, we design an architecture that connects transformers to the DM's UNet backbone  and enables arbitrary types of token-based conditioning mechanisms, see Sec. 3.3."
        },
        {
            "bounding_box": [
                {
                    "x": 252,
                    "y": 2728
                },
                {
                    "x": 1152,
                    "y": 2728
                },
                {
                    "x": 1152,
                    "y": 2770
                },
                {
                    "x": 252,
                    "y": 2770
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='20' style='font-size:16px'>In sum, our work makes the following contributions:</p>",
            "id": 20,
            "page": 2,
            "text": "In sum, our work makes the following contributions:"
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 2780
                },
                {
                    "x": 1199,
                    "y": 2780
                },
                {
                    "x": 1199,
                    "y": 3027
                },
                {
                    "x": 203,
                    "y": 3027
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='21' style='font-size:16px'>(i) In contrast to purely transformer-based approaches<br>[23, 66], our method scales more graceful to higher dimen-<br>sional data and can thus (a) work on a compression level<br>which provides more faithful and detailed reconstructions<br>than previous work (see Fig. 1) and (b) can be efficiently</p>",
            "id": 21,
            "page": 2,
            "text": "(i) In contrast to purely transformer-based approaches , our method scales more graceful to higher dimensional data and can thus (a) work on a compression level which provides more faithful and detailed reconstructions than previous work (see Fig. 1) and (b) can be efficiently"
        },
        {
            "bounding_box": [
                {
                    "x": 1379,
                    "y": 287
                },
                {
                    "x": 2186,
                    "y": 287
                },
                {
                    "x": 2186,
                    "y": 925
                },
                {
                    "x": 1379,
                    "y": 925
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='22' style='font-size:14px' alt=\"Semantic Compression\n80\n(RMSE) 60 Latent Diffusion Model (LDM)\nGenerative Model:\nDistortion\n40\nPerceptual Compression\n20 - → Autoencoder+GAN\n0\n0 0.5 1 1.5\nRate (bits/dim)\" data-coord=\"top-left:(1379,287); bottom-right:(2186,925)\" /></figure>",
            "id": 22,
            "page": 2,
            "text": "Semantic Compression 80 (RMSE) 60 Latent Diffusion Model (LDM) Generative Model: Distortion 40 Perceptual Compression 20 - → Autoencoder+GAN 0 0 0.5 1 1.5 Rate (bits/dim)"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 948
                },
                {
                    "x": 2277,
                    "y": 948
                },
                {
                    "x": 2277,
                    "y": 1408
                },
                {
                    "x": 1279,
                    "y": 1408
                }
            ],
            "category": "caption",
            "html": "<caption id='23' style='font-size:14px'>Figure 2. Illustrating perceptual and semantic compression: Most<br>bits of a digital image correspond to imperceptible details. While<br>DMs allow to suppress this semantically meaningless information<br>by minimizing the responsible loss term, gradients (during train-<br>ing) and the neural network backbone (training and inference) still<br>need to be evaluated on all pixels, leading to superfluous compu-<br>tations and unnecessarily expensive optimization and inference.<br>We propose latent diffusion models (LDMs) as an effective gener-<br>ative model and a separate mild compression stage that only elim-<br>inates imperceptible details. Data and images from [30].</caption>",
            "id": 23,
            "page": 2,
            "text": "Figure 2. Illustrating perceptual and semantic compression: Most bits of a digital image correspond to imperceptible details. While DMs allow to suppress this semantically meaningless information by minimizing the responsible loss term, gradients (during training) and the neural network backbone (training and inference) still need to be evaluated on all pixels, leading to superfluous computations and unnecessarily expensive optimization and inference. We propose latent diffusion models (LDMs) as an effective generative model and a separate mild compression stage that only eliminates imperceptible details. Data and images from ."
        },
        {
            "bounding_box": [
                {
                    "x": 1282,
                    "y": 1460
                },
                {
                    "x": 2243,
                    "y": 1460
                },
                {
                    "x": 2243,
                    "y": 1505
                },
                {
                    "x": 1282,
                    "y": 1505
                }
            ],
            "category": "paragraph",
            "html": "<p id='24' style='font-size:20px'>applied to high-resolution synthesis of megapixel images.</p>",
            "id": 24,
            "page": 2,
            "text": "applied to high-resolution synthesis of megapixel images."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1514
                },
                {
                    "x": 2276,
                    "y": 1514
                },
                {
                    "x": 2276,
                    "y": 1756
                },
                {
                    "x": 1280,
                    "y": 1756
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='25' style='font-size:20px'>(ii) We achieve competitive performance on multiple<br>tasks (unconditional image synthesis, inpainting, stochastic<br>super-resolution) and datasets while significantly lowering<br>computational costs. Compared to pixel-based diffusion ap-<br>proaches, we also significantly decrease inference costs.</p>",
            "id": 25,
            "page": 2,
            "text": "(ii) We achieve competitive performance on multiple tasks (unconditional image synthesis, inpainting, stochastic super-resolution) and datasets while significantly lowering computational costs. Compared to pixel-based diffusion approaches, we also significantly decrease inference costs."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 1764
                },
                {
                    "x": 2276,
                    "y": 1764
                },
                {
                    "x": 2276,
                    "y": 2059
                },
                {
                    "x": 1281,
                    "y": 2059
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='26' style='font-size:16px'>(iii) We show that, in contrast to previous work [93]<br>which learns both an encoder/decoder architecture and a<br>score-based prior simultaneously, our approach does not re-<br>quire a delicate weighting of reconstruction and generative<br>abilities. This ensures extremely faithful reconstructions<br>and requires very little regularization of the latent space.</p>",
            "id": 26,
            "page": 2,
            "text": "(iii) We show that, in contrast to previous work  which learns both an encoder/decoder architecture and a score-based prior simultaneously, our approach does not require a delicate weighting of reconstruction and generative abilities. This ensures extremely faithful reconstructions and requires very little regularization of the latent space."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2065
                },
                {
                    "x": 2277,
                    "y": 2065
                },
                {
                    "x": 2277,
                    "y": 2262
                },
                {
                    "x": 1280,
                    "y": 2262
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='27' style='font-size:16px'>(iv) We find that for densely conditioned tasks such<br>as super-resolution, inpainting and semantic synthesis, our<br>model can be applied in a convolutional fashion and render<br>large, consistent images of ~ 10242 px.</p>",
            "id": 27,
            "page": 2,
            "text": "(iv) We find that for densely conditioned tasks such as super-resolution, inpainting and semantic synthesis, our model can be applied in a convolutional fashion and render large, consistent images of ~ 10242 px."
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 2268
                },
                {
                    "x": 2276,
                    "y": 2268
                },
                {
                    "x": 2276,
                    "y": 2463
                },
                {
                    "x": 1279,
                    "y": 2463
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='28' style='font-size:20px'>(v) Moreover, we design a general-purpose conditioning<br>mechanism based on cross-attention, enabling multi-modal<br>training. We use it to train class-conditional, text-to-image<br>and layout-to-image models.</p>",
            "id": 28,
            "page": 2,
            "text": "(v) Moreover, we design a general-purpose conditioning mechanism based on cross-attention, enabling multi-modal training. We use it to train class-conditional, text-to-image and layout-to-image models."
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 2469
                },
                {
                    "x": 2278,
                    "y": 2469
                },
                {
                    "x": 2278,
                    "y": 2668
                },
                {
                    "x": 1279,
                    "y": 2668
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='29' style='font-size:16px'>(vi) Finally, we release pretrained latent diffusion<br>and autoencoding models at https : / / github ●<br>com / CompVi s / latent -di ffusion which might be<br>reusable for a various tasks besides training of DMs [81].</p>",
            "id": 29,
            "page": 2,
            "text": "(vi) Finally, we release pretrained latent diffusion and autoencoding models at https : / / github ● com / CompVi s / latent -di ffusion which might be reusable for a various tasks besides training of DMs ."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 2721
                },
                {
                    "x": 1632,
                    "y": 2721
                },
                {
                    "x": 1632,
                    "y": 2769
                },
                {
                    "x": 1281,
                    "y": 2769
                }
            ],
            "category": "paragraph",
            "html": "<p id='30' style='font-size:22px'>2. Related Work</p>",
            "id": 30,
            "page": 2,
            "text": "2. Related Work"
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 2778
                },
                {
                    "x": 2277,
                    "y": 2778
                },
                {
                    "x": 2277,
                    "y": 3027
                },
                {
                    "x": 1281,
                    "y": 3027
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='31' style='font-size:20px'>Generative Models for Image Synthesis The high di-<br>mensional nature of images presents distinct challenges<br>to generative modeling. Generative Adversarial Networks<br>(GAN) [27] allow for efficient sampling of high resolution<br>images with good perceptual quality [3, 42], but are diffi-</p>",
            "id": 31,
            "page": 2,
            "text": "Generative Models for Image Synthesis The high dimensional nature of images presents distinct challenges to generative modeling. Generative Adversarial Networks (GAN)  allow for efficient sampling of high resolution images with good perceptual quality , but are diffi-"
        },
        {
            "bounding_box": [
                {
                    "x": 1223,
                    "y": 3053
                },
                {
                    "x": 1252,
                    "y": 3053
                },
                {
                    "x": 1252,
                    "y": 3092
                },
                {
                    "x": 1223,
                    "y": 3092
                }
            ],
            "category": "footer",
            "html": "<br><footer id='32' style='font-size:16px'>2</footer>",
            "id": 32,
            "page": 2,
            "text": "2"
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 307
                },
                {
                    "x": 1199,
                    "y": 307
                },
                {
                    "x": 1199,
                    "y": 1153
                },
                {
                    "x": 199,
                    "y": 1153
                }
            ],
            "category": "paragraph",
            "html": "<p id='33' style='font-size:18px'>cult to optimize [2, 28, 54] and struggle to capture the full<br>data distribution [55]. In contrast, likelihood-based meth-<br>ods emphasize good density estimation which renders op-<br>timization more well-behaved. Variational autoencoders<br>(VAE) [46] and flow-based models [18, 19] enable efficient<br>synthesis of high resolution images [9, 44, 92], but sam-<br>ple quality is not on par with GANs. While autoregressive<br>models (ARM) [6, 10, 94, 95] achieve strong performance<br>in density estimation, computationally demanding architec-<br>tures [97] and a sequential sampling process limit them to<br>low resolution images. Because pixel based representations<br>of images contain barely perceptible, high-frequency de-<br>tails [16, 73], maximum-likelihood training spends a dispro-<br>portionate amount of capacity on modeling them, resulting<br>in long training times. To scale to higher resolutions, several<br>two-stage approaches [23, 67 , 101 , 103] use ARMs to model<br>a compressed latent image space instead of raw pixels.</p>",
            "id": 33,
            "page": 3,
            "text": "cult to optimize  and struggle to capture the full data distribution . In contrast, likelihood-based methods emphasize good density estimation which renders optimization more well-behaved. Variational autoencoders (VAE)  and flow-based models  enable efficient synthesis of high resolution images , but sample quality is not on par with GANs. While autoregressive models (ARM)  achieve strong performance in density estimation, computationally demanding architectures  and a sequential sampling process limit them to low resolution images. Because pixel based representations of images contain barely perceptible, high-frequency details , maximum-likelihood training spends a disproportionate amount of capacity on modeling them, resulting in long training times. To scale to higher resolutions, several two-stage approaches [23, 67 , 101 , 103] use ARMs to model a compressed latent image space instead of raw pixels."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 1166
                },
                {
                    "x": 1199,
                    "y": 1166
                },
                {
                    "x": 1199,
                    "y": 2163
                },
                {
                    "x": 200,
                    "y": 2163
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='34' style='font-size:18px'>Recently, Diffusion Probabilistic Models (DM) [82],<br>have achieved state-of-the-art results in density estimation<br>[45] as well as in sample quality [15]. The generative power<br>of these models stems from a natural fit to the inductive bi-<br>ases of image-like data when their underlying neural back-<br>bone is implemented as a UNet [15, 30, 71 , 85]. The best<br>synthesis quality is usually achieved when a reweighted ob-<br>jective [30] is used for training. In this case, the DM corre-<br>sponds to a lossy compressor and allow to trade image qual-<br>ity for compression capabilities. Evaluating and optimizing<br>these models in pixel space, however, has the downside of<br>low inference speed and very high training costs. While<br>the former can be partially adressed by advanced sampling<br>strategies [47, 75, 84] and hierarchical approaches [31, 93],<br>training on high-resolution image data always requires to<br>calculate expensive gradients. We adress both drawbacks<br>with our proposed LDMs, which work on a compressed la-<br>tent space of lower dimensionality. This renders training<br>computationally cheaper and speeds up inference with al-<br>most no reduction in synthesis quality (see Fig. 1).</p>",
            "id": 34,
            "page": 3,
            "text": "Recently, Diffusion Probabilistic Models (DM) , have achieved state-of-the-art results in density estimation  as well as in sample quality . The generative power of these models stems from a natural fit to the inductive biases of image-like data when their underlying neural backbone is implemented as a UNet [15, 30, 71 , 85]. The best synthesis quality is usually achieved when a reweighted objective  is used for training. In this case, the DM corresponds to a lossy compressor and allow to trade image quality for compression capabilities. Evaluating and optimizing these models in pixel space, however, has the downside of low inference speed and very high training costs. While the former can be partially adressed by advanced sampling strategies  and hierarchical approaches , training on high-resolution image data always requires to calculate expensive gradients. We adress both drawbacks with our proposed LDMs, which work on a compressed latent space of lower dimensionality. This renders training computationally cheaper and speeds up inference with almost no reduction in synthesis quality (see Fig. 1)."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 2178
                },
                {
                    "x": 1200,
                    "y": 2178
                },
                {
                    "x": 1200,
                    "y": 3028
                },
                {
                    "x": 200,
                    "y": 3028
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='35' style='font-size:18px'>Two-Stage Image Synthesis To mitigate the shortcom-<br>ings of individual generative approaches, a lot of research<br>[11, 23, 67, 70, 101, 103] has gone into combining the<br>strengths of different methods into more efficient and per-<br>formant models via a two stage approach. VQ- VAEs [67,<br>101] use autoregressive models to learn an expressive prior<br>over a discretized latent space. [66] extend this approach to<br>text-to-image generation by learning a joint distributation<br>over discretized image and text representations. More gen-<br>erally, [70] uses conditionally invertible networks to pro-<br>vide a generic transfer between latent spaces of diverse do-<br>mains. Different from VQ-VAEs, VQGANs [23, 103] em-<br>ploy a first stage with an adversarial and perceptual objec-<br>tive to scale autoregressive transformers to larger images.<br>However, the high compression rates required for feasible<br>ARM training, which introduces billions of trainable pa-<br>rameters [23, 66], limit the overall performance of such ap-</p>",
            "id": 35,
            "page": 3,
            "text": "Two-Stage Image Synthesis To mitigate the shortcomings of individual generative approaches, a lot of research  has gone into combining the strengths of different methods into more efficient and performant models via a two stage approach. VQ- VAEs  use autoregressive models to learn an expressive prior over a discretized latent space.  extend this approach to text-to-image generation by learning a joint distributation over discretized image and text representations. More generally,  uses conditionally invertible networks to provide a generic transfer between latent spaces of diverse domains. Different from VQ-VAEs, VQGANs  employ a first stage with an adversarial and perceptual objective to scale autoregressive transformers to larger images. However, the high compression rates required for feasible ARM training, which introduces billions of trainable parameters , limit the overall performance of such ap-"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 309
                },
                {
                    "x": 2278,
                    "y": 309
                },
                {
                    "x": 2278,
                    "y": 753
                },
                {
                    "x": 1279,
                    "y": 753
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='36' style='font-size:16px'>proaches and less compression comes at the price of high<br>computational cost [23, 66]. Our work prevents such trade-<br>offs, as our proposed LDMs scale more gently to higher<br>dimensional latent spaces due to their convolutional back-<br>bone. Thus, we are free to choose the level of compression<br>which optimally mediates between learning a powerful first<br>stage, without leaving too much perceptual compression up<br>to the generative diffusion model while guaranteeing high-<br>fidelity reconstructions (see Fig. 1).</p>",
            "id": 36,
            "page": 3,
            "text": "proaches and less compression comes at the price of high computational cost . Our work prevents such tradeoffs, as our proposed LDMs scale more gently to higher dimensional latent spaces due to their convolutional backbone. Thus, we are free to choose the level of compression which optimally mediates between learning a powerful first stage, without leaving too much perceptual compression up to the generative diffusion model while guaranteeing highfidelity reconstructions (see Fig. 1)."
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 758
                },
                {
                    "x": 2277,
                    "y": 758
                },
                {
                    "x": 2277,
                    "y": 1056
                },
                {
                    "x": 1279,
                    "y": 1056
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='37' style='font-size:18px'>While approaches to jointly [93] or separately [80] learn<br>an encoding/decoding model together with a score-based<br>prior exist, the former still require a difficult weighting be-<br>tween reconstruction and generative capabilities [11] and<br>are outperformed by our approach (Sec. 4), and the latter<br>focus on highly structured images such as human faces.</p>",
            "id": 37,
            "page": 3,
            "text": "While approaches to jointly  or separately  learn an encoding/decoding model together with a score-based prior exist, the former still require a difficult weighting between reconstruction and generative capabilities  and are outperformed by our approach (Sec. 4), and the latter focus on highly structured images such as human faces."
        },
        {
            "bounding_box": [
                {
                    "x": 1282,
                    "y": 1110
                },
                {
                    "x": 1505,
                    "y": 1110
                },
                {
                    "x": 1505,
                    "y": 1158
                },
                {
                    "x": 1282,
                    "y": 1158
                }
            ],
            "category": "paragraph",
            "html": "<p id='38' style='font-size:20px'>3. Method</p>",
            "id": 38,
            "page": 3,
            "text": "3. Method"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 1176
                },
                {
                    "x": 2278,
                    "y": 1176
                },
                {
                    "x": 2278,
                    "y": 1522
                },
                {
                    "x": 1279,
                    "y": 1522
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='39' style='font-size:18px'>To lower the computational demands of training diffu-<br>sion models towards high-resolution image synthesis, we<br>observe that although diffusion models allow to ignore<br>perceptually irrelevant details by undersampling the corre-<br>sponding loss terms [30], they still require costly function<br>evaluations in pixel space, which causes huge demands in<br>computation time and energy resources.</p>",
            "id": 39,
            "page": 3,
            "text": "To lower the computational demands of training diffusion models towards high-resolution image synthesis, we observe that although diffusion models allow to ignore perceptually irrelevant details by undersampling the corresponding loss terms , they still require costly function evaluations in pixel space, which causes huge demands in computation time and energy resources."
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 1529
                },
                {
                    "x": 2278,
                    "y": 1529
                },
                {
                    "x": 2278,
                    "y": 1824
                },
                {
                    "x": 1279,
                    "y": 1824
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='40' style='font-size:18px'>We propose to circumvent this drawback by introducing<br>an explicit separation of the compressive from the genera-<br>tive learning phase (see Fig. 2). To achieve this, we utilize<br>an autoencoding model which learns a space that is percep-<br>tually equivalent to the image space, but offers significantly<br>reduced computational complexity.</p>",
            "id": 40,
            "page": 3,
            "text": "We propose to circumvent this drawback by introducing an explicit separation of the compressive from the generative learning phase (see Fig. 2). To achieve this, we utilize an autoencoding model which learns a space that is perceptually equivalent to the image space, but offers significantly reduced computational complexity."
        },
        {
            "bounding_box": [
                {
                    "x": 1277,
                    "y": 1832
                },
                {
                    "x": 2277,
                    "y": 1832
                },
                {
                    "x": 2277,
                    "y": 2478
                },
                {
                    "x": 1277,
                    "y": 2478
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='41' style='font-size:18px'>Such an approach offers several advantages: (i) By leav-<br>ing the high-dimensional image space, we obtain DMs<br>which are computationally much more efficient because<br>sampling is performed on a low-dimensional space. (ii) We<br>exploit the inductive bias of DMs inherited from their UNet<br>architecture [71], which makes them particularly effective<br>for data with spatial structure and therefore alleviates the<br>need for aggressive, quality-reducing compression levels as<br>required by previous approaches [23, 66]. (iii) Finally, we<br>obtain general-purpose compression models whose latent<br>space can be used to train multiple generative models and<br>which can also be utilized for other downstream applica-<br>tions such as single-image CLIP-guided synthesis [25].</p>",
            "id": 41,
            "page": 3,
            "text": "Such an approach offers several advantages: (i) By leaving the high-dimensional image space, we obtain DMs which are computationally much more efficient because sampling is performed on a low-dimensional space. (ii) We exploit the inductive bias of DMs inherited from their UNet architecture , which makes them particularly effective for data with spatial structure and therefore alleviates the need for aggressive, quality-reducing compression levels as required by previous approaches . (iii) Finally, we obtain general-purpose compression models whose latent space can be used to train multiple generative models and which can also be utilized for other downstream applications such as single-image CLIP-guided synthesis ."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2515
                },
                {
                    "x": 1978,
                    "y": 2515
                },
                {
                    "x": 1978,
                    "y": 2566
                },
                {
                    "x": 1280,
                    "y": 2566
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='42' style='font-size:22px'>3.1. Perceptual Image Compression</p>",
            "id": 42,
            "page": 3,
            "text": "3.1. Perceptual Image Compression"
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2577
                },
                {
                    "x": 2277,
                    "y": 2577
                },
                {
                    "x": 2277,
                    "y": 2920
                },
                {
                    "x": 1280,
                    "y": 2920
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='43' style='font-size:18px'>Our perceptual compression model is based on previous<br>work [23] and consists of an autoencoder trained by com-<br>bination of a perceptual loss [106] and a patch-based [33]<br>adversarial objective [20, 23, 103]. This ensures that the re-<br>constructions are confined to the image manifold by enforc-<br>ing local realism and avoids bluriness introduced by relying<br>solely on pixel-space losses such as L2 or L1 objectives.</p>",
            "id": 43,
            "page": 3,
            "text": "Our perceptual compression model is based on previous work  and consists of an autoencoder trained by combination of a perceptual loss  and a patch-based  adversarial objective . This ensures that the reconstructions are confined to the image manifold by enforcing local realism and avoids bluriness introduced by relying solely on pixel-space losses such as L2 or L1 objectives."
        },
        {
            "bounding_box": [
                {
                    "x": 1282,
                    "y": 2926
                },
                {
                    "x": 2275,
                    "y": 2926
                },
                {
                    "x": 2275,
                    "y": 3025
                },
                {
                    "x": 1282,
                    "y": 3025
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='44' style='font-size:14px'>in RGB<br>More precisely, given an image x E RHxW x3<br>space, the encoder 3 encodes x into a latent representa-</p>",
            "id": 44,
            "page": 3,
            "text": "in RGB More precisely, given an image x E RHxW x3 space, the encoder 3 encodes x into a latent representa-"
        },
        {
            "bounding_box": [
                {
                    "x": 1225,
                    "y": 3054
                },
                {
                    "x": 1252,
                    "y": 3054
                },
                {
                    "x": 1252,
                    "y": 3092
                },
                {
                    "x": 1225,
                    "y": 3092
                }
            ],
            "category": "footer",
            "html": "<br><footer id='45' style='font-size:14px'>3</footer>",
            "id": 45,
            "page": 3,
            "text": "3"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 306
                },
                {
                    "x": 1199,
                    "y": 306
                },
                {
                    "x": 1199,
                    "y": 552
                },
                {
                    "x": 201,
                    "y": 552
                }
            ],
            "category": "paragraph",
            "html": "<p id='46' style='font-size:16px'>tion 2 = E(x), and the decoder D reconstructs the im-<br>age from the latent, giving x = D(z) = D(E(x)), where<br>Importantly, the encoder downsamples the<br>2 E Rhxwxc<br>image by a factor f = H/h = W/w, and we investigate<br>different downsampling factors f = 2m with m E N.<br>,</p>",
            "id": 46,
            "page": 4,
            "text": "tion 2 = E(x), and the decoder D reconstructs the image from the latent, giving x = D(z) = D(E(x)), where Importantly, the encoder downsamples the 2 E Rhxwxc image by a factor f = H/h = W/w, and we investigate different downsampling factors f = 2m with m E N. ,"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 560
                },
                {
                    "x": 1199,
                    "y": 560
                },
                {
                    "x": 1199,
                    "y": 1405
                },
                {
                    "x": 201,
                    "y": 1405
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='47' style='font-size:18px'>In order to avoid arbitrarily high-variance latent spaces,<br>we experiment with two different kinds of regularizations.<br>The first variant, KL-reg., imposes a slight KL-penalty to-<br>wards a standard normal on the learned latent, similar to a<br>VAE [46, 69], whereas VQ-reg. uses a vector quantization<br>layer [96] within the decoder. This model can be interpreted<br>as a VQGAN [23] but with the quantization layer absorbed<br>by the decoder. Because our subsequent DM is designed<br>to work with the two-dimensional structure of our learned<br>latent space 2 = E(x), we can use relatively mild compres-<br>sion rates and achieve very good reconstructions. This is<br>in contrast to previous works [23, 66], which relied on an<br>arbitrary 1D ordering of the learned space 2 to model its<br>distribution autoregressively and thereby ignored much of<br>the inherent structure of z. Hence, our compression model<br>preserves details of x better (see Tab. 8). The full objective<br>and training details can be found in the supplement.</p>",
            "id": 47,
            "page": 4,
            "text": "In order to avoid arbitrarily high-variance latent spaces, we experiment with two different kinds of regularizations. The first variant, KL-reg., imposes a slight KL-penalty towards a standard normal on the learned latent, similar to a VAE , whereas VQ-reg. uses a vector quantization layer  within the decoder. This model can be interpreted as a VQGAN  but with the quantization layer absorbed by the decoder. Because our subsequent DM is designed to work with the two-dimensional structure of our learned latent space 2 = E(x), we can use relatively mild compression rates and achieve very good reconstructions. This is in contrast to previous works , which relied on an arbitrary 1D ordering of the learned space 2 to model its distribution autoregressively and thereby ignored much of the inherent structure of z. Hence, our compression model preserves details of x better (see Tab. 8). The full objective and training details can be found in the supplement."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 1437
                },
                {
                    "x": 767,
                    "y": 1437
                },
                {
                    "x": 767,
                    "y": 1487
                },
                {
                    "x": 202,
                    "y": 1487
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='48' style='font-size:20px'>3.2. Latent Diffusion Models</p>",
            "id": 48,
            "page": 4,
            "text": "3.2. Latent Diffusion Models"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1515
                },
                {
                    "x": 1199,
                    "y": 1515
                },
                {
                    "x": 1199,
                    "y": 2113
                },
                {
                    "x": 201,
                    "y": 2113
                }
            ],
            "category": "paragraph",
            "html": "<p id='49' style='font-size:18px'>Diffusion Models [82] are probabilistic models designed to<br>learn a data distribution p(x) by gradually denoising a nor-<br>mally distributed variable, which corresponds to learning<br>the reverse process of a fixed Markov Chain of length T.<br>For image synthesis, the most successful models [15,30, 72]<br>rely on a reweighted variant of the variational lower bound<br>on p(x), which mirrors denoising score-matching [85].<br>These models can be interpreted as an equally weighted<br>sequence of denoising autoencoders E0(xt, t); t = 1 · · · T,<br>which are trained to predict a denoised variant of their input<br>Xt, where Xt is a noisy version of the input x. The corre-<br>sponding objective can be simplified to (Sec. B)</p>",
            "id": 49,
            "page": 4,
            "text": "Diffusion Models  are probabilistic models designed to learn a data distribution p(x) by gradually denoising a normally distributed variable, which corresponds to learning the reverse process of a fixed Markov Chain of length T. For image synthesis, the most successful models  rely on a reweighted variant of the variational lower bound on p(x), which mirrors denoising score-matching . These models can be interpreted as an equally weighted sequence of denoising autoencoders E0(xt, t); t = 1 · · · T, which are trained to predict a denoised variant of their input Xt, where Xt is a noisy version of the input x. The corresponding objective can be simplified to (Sec. B)"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2276
                },
                {
                    "x": 933,
                    "y": 2276
                },
                {
                    "x": 933,
                    "y": 2322
                },
                {
                    "x": 202,
                    "y": 2322
                }
            ],
            "category": "paragraph",
            "html": "<p id='50' style='font-size:14px'>with t uniformly sampled from {1, · · · , T}.</p>",
            "id": 50,
            "page": 4,
            "text": "with t uniformly sampled from {1, · · · , T}."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2325
                },
                {
                    "x": 1197,
                    "y": 2325
                },
                {
                    "x": 1197,
                    "y": 2774
                },
                {
                    "x": 201,
                    "y": 2774
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='51' style='font-size:18px'>Generative Modeling of Latent Representations With<br>our trained perceptual compression models consisting of 3<br>and D, we now have access to an efficient, low-dimensional<br>latent space in which high-frequency, imperceptible details<br>are abstracted away. Compared to the high-dimensional<br>pixel space, this space is more suitable for likelihood-based<br>generative models, as they can now (i) focus on the impor-<br>tant, semantic bits of the data and (ii) train in a lower di-<br>mensional, computationally much more efficient space.</p>",
            "id": 51,
            "page": 4,
            "text": "Generative Modeling of Latent Representations With our trained perceptual compression models consisting of 3 and D, we now have access to an efficient, low-dimensional latent space in which high-frequency, imperceptible details are abstracted away. Compared to the high-dimensional pixel space, this space is more suitable for likelihood-based generative models, as they can now (i) focus on the important, semantic bits of the data and (ii) train in a lower dimensional, computationally much more efficient space."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2780
                },
                {
                    "x": 1199,
                    "y": 2780
                },
                {
                    "x": 1199,
                    "y": 2976
                },
                {
                    "x": 202,
                    "y": 2976
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='52' style='font-size:18px'>Unlike previous work that relied on autoregressive,<br>attention-based transformer models in a highly compressed,<br>discrete latent space [23, 66, 103], we can take advantage of<br>image-specific inductive biases that our model offers. This</p>",
            "id": 52,
            "page": 4,
            "text": "Unlike previous work that relied on autoregressive, attention-based transformer models in a highly compressed, discrete latent space , we can take advantage of image-specific inductive biases that our model offers. This"
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 295
                },
                {
                    "x": 2283,
                    "y": 295
                },
                {
                    "x": 2283,
                    "y": 778
                },
                {
                    "x": 1291,
                    "y": 778
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='53' style='font-size:14px' alt=\"Latent Space Conditioning\nx 3 Diffusion Process Semantic\nMap\nText\nz Denoising U-Net EA ZT\nx(T-1) Repres\nentations\n0 Q Images\nx D ㉤\nV KV KV V\nz ZT-1 ZT\nPixel Space\n��\nQ\nKV\ndenoising step crossattention switch skip connection concat\" data-coord=\"top-left:(1291,295); bottom-right:(2283,778)\" /></figure>",
            "id": 53,
            "page": 4,
            "text": "Latent Space Conditioning x 3 Diffusion Process Semantic Map Text z Denoising U-Net EA ZT x(T-1) Repres entations 0 Q Images x D ㉤ V KV KV V z ZT-1 ZT Pixel Space �� Q KV denoising step crossattention switch skip connection concat"
        },
        {
            "bounding_box": [
                {
                    "x": 1282,
                    "y": 822
                },
                {
                    "x": 2274,
                    "y": 822
                },
                {
                    "x": 2274,
                    "y": 911
                },
                {
                    "x": 1282,
                    "y": 911
                }
            ],
            "category": "caption",
            "html": "<caption id='54' style='font-size:14px'>Figure 3. We condition LDMs either via concatenation or by a<br>more general cross-attention mechanism. See Sec. 3.3</caption>",
            "id": 54,
            "page": 4,
            "text": "Figure 3. We condition LDMs either via concatenation or by a more general cross-attention mechanism. See Sec. 3.3"
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 948
                },
                {
                    "x": 2276,
                    "y": 948
                },
                {
                    "x": 2276,
                    "y": 1145
                },
                {
                    "x": 1281,
                    "y": 1145
                }
            ],
            "category": "paragraph",
            "html": "<p id='55' style='font-size:18px'>includes the ability to build the underlying UNet primar-<br>ily from 2D convolutional layers, and further focusing the<br>objective on the perceptually most relevant bits using the<br>reweighted bound, which now reads</p>",
            "id": 55,
            "page": 4,
            "text": "includes the ability to build the underlying UNet primarily from 2D convolutional layers, and further focusing the objective on the perceptually most relevant bits using the reweighted bound, which now reads"
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 1302
                },
                {
                    "x": 2276,
                    "y": 1302
                },
                {
                    "x": 2276,
                    "y": 1551
                },
                {
                    "x": 1281,
                    "y": 1551
                }
            ],
            "category": "paragraph",
            "html": "<p id='56' style='font-size:18px'>The neural backbone EA(O, t) of our model is realized as a<br>time-conditional UNet [71]. Since the forward process is<br>fixed, Zt can be efficiently obtained from 3 during training,<br>and samples from p(z) can be decoded to image space with<br>a single pass through D.</p>",
            "id": 56,
            "page": 4,
            "text": "The neural backbone EA(O, t) of our model is realized as a time-conditional UNet . Since the forward process is fixed, Zt can be efficiently obtained from 3 during training, and samples from p(z) can be decoded to image space with a single pass through D."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1584
                },
                {
                    "x": 1881,
                    "y": 1584
                },
                {
                    "x": 1881,
                    "y": 1633
                },
                {
                    "x": 1280,
                    "y": 1633
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='57' style='font-size:22px'>3.3. Conditioning Mechanisms</p>",
            "id": 57,
            "page": 4,
            "text": "3.3. Conditioning Mechanisms"
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1642
                },
                {
                    "x": 2278,
                    "y": 1642
                },
                {
                    "x": 2278,
                    "y": 1986
                },
                {
                    "x": 1280,
                    "y": 1986
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='58' style='font-size:18px'>Similar to other types of generative models [56, 83],<br>diffusion models are in principle capable of modeling<br>conditional distributions of the form p(z|y). This can<br>be implemented with a conditional denoising autoencoder<br>ea(zt, t, y) and paves the way to controlling the synthesis<br>process through inputs y such as text [68], semantic maps<br>[33, 61] or other image-to-image translation tasks [34].</p>",
            "id": 58,
            "page": 4,
            "text": "Similar to other types of generative models , diffusion models are in principle capable of modeling conditional distributions of the form p(z|y). This can be implemented with a conditional denoising autoencoder ea(zt, t, y) and paves the way to controlling the synthesis process through inputs y such as text , semantic maps  or other image-to-image translation tasks ."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1993
                },
                {
                    "x": 2276,
                    "y": 1993
                },
                {
                    "x": 2276,
                    "y": 2186
                },
                {
                    "x": 1280,
                    "y": 2186
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='59' style='font-size:18px'>In the context of image synthesis, however, combining<br>the generative power of DMs with other types of condition-<br>ings beyond class-labels [15] or blurred variants of the input<br>image [72] is SO far an under-explored area of research.</p>",
            "id": 59,
            "page": 4,
            "text": "In the context of image synthesis, however, combining the generative power of DMs with other types of conditionings beyond class-labels  or blurred variants of the input image  is SO far an under-explored area of research."
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 2188
                },
                {
                    "x": 2278,
                    "y": 2188
                },
                {
                    "x": 2278,
                    "y": 2713
                },
                {
                    "x": 1278,
                    "y": 2713
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='60' style='font-size:18px'>We turn DMs into more flexible conditional image gener-<br>ators by augmenting their underlying UNet backbone with<br>the cross-attention mechanism [97], which is effective for<br>learning attention-based models of various input modali-<br>ties [35,36]. To pre-process y from various modalities (such<br>as language prompts) we introduce a domain specific en-<br>coder �� that projects y to an intermediate representation<br>To(y) E R MxdT which is then mapped to the intermediate<br>,<br>layers of the UNet via a cross-attention layer implementing<br>Attention(Q, K, V) = softmax ( QKT ) · V, with<br>Vd</p>",
            "id": 60,
            "page": 4,
            "text": "We turn DMs into more flexible conditional image generators by augmenting their underlying UNet backbone with the cross-attention mechanism , which is effective for learning attention-based models of various input modalities . To pre-process y from various modalities (such as language prompts) we introduce a domain specific encoder �� that projects y to an intermediate representation To(y) E R MxdT which is then mapped to the intermediate , layers of the UNet via a cross-attention layer implementing Attention(Q, K, V) = softmax ( QKT ) · V, with Vd"
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2865
                },
                {
                    "x": 2278,
                    "y": 2865
                },
                {
                    "x": 2278,
                    "y": 2982
                },
                {
                    "x": 1280,
                    "y": 2982
                }
            ],
            "category": "paragraph",
            "html": "<p id='61' style='font-size:18px'>Here, 4i(zt) E RNxdie denotes a (flattened) intermediate<br>representation of the UNet implementing EA and W(i) E</p>",
            "id": 61,
            "page": 4,
            "text": "Here, 4i(zt) E RNxdie denotes a (flattened) intermediate representation of the UNet implementing EA and W(i) E"
        },
        {
            "bounding_box": [
                {
                    "x": 1225,
                    "y": 3055
                },
                {
                    "x": 1250,
                    "y": 3055
                },
                {
                    "x": 1250,
                    "y": 3089
                },
                {
                    "x": 1225,
                    "y": 3089
                }
            ],
            "category": "footer",
            "html": "<footer id='62' style='font-size:16px'>4</footer>",
            "id": 62,
            "page": 4,
            "text": "4"
        },
        {
            "bounding_box": [
                {
                    "x": 206,
                    "y": 301
                },
                {
                    "x": 2268,
                    "y": 301
                },
                {
                    "x": 2268,
                    "y": 792
                },
                {
                    "x": 206,
                    "y": 792
                }
            ],
            "category": "figure",
            "html": "<figure><img id='63' style='font-size:14px' alt=\"CelebAHQ FFHQ LSUN-Churches LSUN-Beds ImageNet\n\" data-coord=\"top-left:(206,301); bottom-right:(2268,792)\" /></figure>",
            "id": 63,
            "page": 5,
            "text": "CelebAHQ FFHQ LSUN-Churches LSUN-Beds ImageNet "
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 827
                },
                {
                    "x": 2275,
                    "y": 827
                },
                {
                    "x": 2275,
                    "y": 922
                },
                {
                    "x": 203,
                    "y": 922
                }
            ],
            "category": "caption",
            "html": "<caption id='64' style='font-size:14px'>Figure 4. Samples from LDMs trained on CelebAHQ [39], FFHQ [41], LSUN-Churches [102], LSUN-Bedrooms [102] and class-<br>conditional ImageNet [12], each with a resolution of 256 x 256. Best viewed when zoomed in. For more samples cf. the supplement.</caption>",
            "id": 64,
            "page": 5,
            "text": "Figure 4. Samples from LDMs trained on CelebAHQ , FFHQ , LSUN-Churches , LSUN-Bedrooms  and classconditional ImageNet , each with a resolution of 256 x 256. Best viewed when zoomed in. For more samples cf. the supplement."
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 991
                },
                {
                    "x": 1197,
                    "y": 991
                },
                {
                    "x": 1197,
                    "y": 1099
                },
                {
                    "x": 203,
                    "y": 1099
                }
            ],
            "category": "paragraph",
            "html": "<p id='65' style='font-size:18px'>Rdxd'�, W(i) E RdxdT & W(i) E RdxdT are learnable pro-<br>jection matrices [36,97]. See Fig. 3 for a visual depiction.</p>",
            "id": 65,
            "page": 5,
            "text": "Rdxd'�, W(i) E RdxdT & W(i) E RdxdT are learnable projection matrices . See Fig. 3 for a visual depiction."
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 1104
                },
                {
                    "x": 1199,
                    "y": 1104
                },
                {
                    "x": 1199,
                    "y": 1199
                },
                {
                    "x": 204,
                    "y": 1199
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='66' style='font-size:14px'>Based on image-conditioning pairs, we then learn the<br>conditional LDM via</p>",
            "id": 66,
            "page": 5,
            "text": "Based on image-conditioning pairs, we then learn the conditional LDM via"
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 1351
                },
                {
                    "x": 1198,
                    "y": 1351
                },
                {
                    "x": 1198,
                    "y": 1549
                },
                {
                    "x": 203,
                    "y": 1549
                }
            ],
            "category": "paragraph",
            "html": "<p id='67' style='font-size:16px'>where both �� and EA are jointly optimized via Eq. 3. This<br>conditioning mechanism is flexible as TO can be parameter-<br>ized with domain-specific experts, e.g. (unmasked) trans-<br>formers [97] when y are text prompts (see Sec. 4.3.1)</p>",
            "id": 67,
            "page": 5,
            "text": "where both �� and EA are jointly optimized via Eq. 3. This conditioning mechanism is flexible as TO can be parameterized with domain-specific experts, e.g. (unmasked) transformers  when y are text prompts (see Sec. 4.3.1)"
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 1601
                },
                {
                    "x": 533,
                    "y": 1601
                },
                {
                    "x": 533,
                    "y": 1653
                },
                {
                    "x": 204,
                    "y": 1653
                }
            ],
            "category": "paragraph",
            "html": "<p id='68' style='font-size:22px'>4. Experiments</p>",
            "id": 68,
            "page": 5,
            "text": "4. Experiments"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1663
                },
                {
                    "x": 1199,
                    "y": 1663
                },
                {
                    "x": 1199,
                    "y": 2412
                },
                {
                    "x": 201,
                    "y": 2412
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='69' style='font-size:16px'>LDMs provide means to flexible and computationally<br>tractable diffusion based image synthesis of various image<br>modalities, which we empirically show in the following.<br>Firstly, however, we analyze the gains of our models com-<br>pared to pixel-based diffusion models in both training and<br>inference. Interestingly, we find that LDMs trained in VQ-<br>regularized latent spaces sometimes achieve better sample<br>quality, even though the reconstruction capabilities of VQ-<br>regularized first stage models slightly fall behind those of<br>their continuous counterparts, cf. Tab. 8. A visual compari-<br>son between the effects of first stage regularization schemes<br>on LDM training and their generalization abilities to resolu-<br>tions > 2562 can be found in Appendix D.1. In E.2 we list<br>details on architecture, implementation, training and evalu-<br>ation for all results presented in this section.</p>",
            "id": 69,
            "page": 5,
            "text": "LDMs provide means to flexible and computationally tractable diffusion based image synthesis of various image modalities, which we empirically show in the following. Firstly, however, we analyze the gains of our models compared to pixel-based diffusion models in both training and inference. Interestingly, we find that LDMs trained in VQregularized latent spaces sometimes achieve better sample quality, even though the reconstruction capabilities of VQregularized first stage models slightly fall behind those of their continuous counterparts, cf. Tab. 8. A visual comparison between the effects of first stage regularization schemes on LDM training and their generalization abilities to resolutions > 2562 can be found in Appendix D.1. In E.2 we list details on architecture, implementation, training and evaluation for all results presented in this section."
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 2448
                },
                {
                    "x": 1038,
                    "y": 2448
                },
                {
                    "x": 1038,
                    "y": 2497
                },
                {
                    "x": 203,
                    "y": 2497
                }
            ],
            "category": "paragraph",
            "html": "<p id='70' style='font-size:20px'>4.1. On Perceptual Compression Tradeoffs</p>",
            "id": 70,
            "page": 5,
            "text": "4.1. On Perceptual Compression Tradeoffs"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2527
                },
                {
                    "x": 1199,
                    "y": 2527
                },
                {
                    "x": 1199,
                    "y": 2874
                },
                {
                    "x": 202,
                    "y": 2874
                }
            ],
            "category": "paragraph",
            "html": "<p id='71' style='font-size:16px'>This section analyzes the behavior of our LDMs with dif-<br>ferent downsampling factors f E {1,2,4,8, 16, 32} (abbre-<br>viated as LDM-f, where LDM-1 corresponds to pixel-based<br>DMs). To obtain a comparable test-field, we fix the com-<br>putational resources to a single NVIDIA A100 for all ex-<br>periments in this section and train all models for the same<br>number of steps and with the same number of parameters.</p>",
            "id": 71,
            "page": 5,
            "text": "This section analyzes the behavior of our LDMs with different downsampling factors f E {1,2,4,8, 16, 32} (abbreviated as LDM-f, where LDM-1 corresponds to pixel-based DMs). To obtain a comparable test-field, we fix the computational resources to a single NVIDIA A100 for all experiments in this section and train all models for the same number of steps and with the same number of parameters."
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 2879
                },
                {
                    "x": 1198,
                    "y": 2879
                },
                {
                    "x": 1198,
                    "y": 2975
                },
                {
                    "x": 203,
                    "y": 2975
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='72' style='font-size:16px'>Tab. 8 shows hyperparameters and reconstruction perfor-<br>mance of the first stage models used for the LDMs com-</p>",
            "id": 72,
            "page": 5,
            "text": "Tab. 8 shows hyperparameters and reconstruction performance of the first stage models used for the LDMs com-"
        },
        {
            "bounding_box": [
                {
                    "x": 1277,
                    "y": 998
                },
                {
                    "x": 2276,
                    "y": 998
                },
                {
                    "x": 2276,
                    "y": 1695
                },
                {
                    "x": 1277,
                    "y": 1695
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='73' style='font-size:18px'>pared in this section. Fig. 6 shows sample quality as a func-<br>tion of training progress for 2M steps of class-conditional<br>models on the ImageNet [12] dataset. We see that, i) small<br>downsampling factors for LDM-{1,2} result in slow train-<br>ing progress, whereas ii) overly large values of f cause stag-<br>nating fidelity after comparably few training steps. Revis-<br>iting the analysis above (Fig. 1 and 2) we attribute this to<br>i) leaving most of perceptual compression to the diffusion<br>model and ii) too strong first stage compression resulting<br>in information loss and thus limiting the achievable qual-<br>ity. LDM-{4-16} strike a good balance between efficiency<br>and perceptually faithful results, which manifests in a sig-<br>nificant FID [29] gap of 38 between pixel-based diffusion<br>(LDM-I) and LDM-8 after 2M training steps.</p>",
            "id": 73,
            "page": 5,
            "text": "pared in this section. Fig. 6 shows sample quality as a function of training progress for 2M steps of class-conditional models on the ImageNet  dataset. We see that, i) small downsampling factors for LDM-{1,2} result in slow training progress, whereas ii) overly large values of f cause stagnating fidelity after comparably few training steps. Revisiting the analysis above (Fig. 1 and 2) we attribute this to i) leaving most of perceptual compression to the diffusion model and ii) too strong first stage compression resulting in information loss and thus limiting the achievable quality. LDM-{4-16} strike a good balance between efficiency and perceptually faithful results, which manifests in a significant FID  gap of 38 between pixel-based diffusion (LDM-I) and LDM-8 after 2M training steps."
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 1700
                },
                {
                    "x": 2276,
                    "y": 1700
                },
                {
                    "x": 2276,
                    "y": 2295
                },
                {
                    "x": 1278,
                    "y": 2295
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='74' style='font-size:18px'>In Fig. 7, we compare models trained on CelebA-<br>HQ [39] and ImageNet in terms sampling speed for differ-<br>ent numbers of denoising steps with the DDIM sampler [84]<br>and plot it against FID-scores [29]. LDM-{4-8} outper-<br>form models with unsuitable ratios of perceptual and con-<br>ceptual compression. Especially compared to pixel-based<br>LDM-1, they achieve much lower FID scores while simulta-<br>neously significantly increasing sample throughput. Com-<br>plex datasets such as ImageNet require reduced compres-<br>sion rates to avoid reducing quality. In summary, LDM-4<br>and -8 offer the best conditions for achieving high-quality<br>synthesis results.</p>",
            "id": 74,
            "page": 5,
            "text": "In Fig. 7, we compare models trained on CelebAHQ  and ImageNet in terms sampling speed for different numbers of denoising steps with the DDIM sampler  and plot it against FID-scores . LDM-{4-8} outperform models with unsuitable ratios of perceptual and conceptual compression. Especially compared to pixel-based LDM-1, they achieve much lower FID scores while simultaneously significantly increasing sample throughput. Complex datasets such as ImageNet require reduced compression rates to avoid reducing quality. In summary, LDM-4 and -8 offer the best conditions for achieving high-quality synthesis results."
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 2420
                },
                {
                    "x": 2155,
                    "y": 2420
                },
                {
                    "x": 2155,
                    "y": 2469
                },
                {
                    "x": 1279,
                    "y": 2469
                }
            ],
            "category": "paragraph",
            "html": "<p id='75' style='font-size:20px'>4.2. Image Generation with Latent Diffusion</p>",
            "id": 75,
            "page": 5,
            "text": "4.2. Image Generation with Latent Diffusion"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 2479
                },
                {
                    "x": 2277,
                    "y": 2479
                },
                {
                    "x": 2277,
                    "y": 2977
                },
                {
                    "x": 1279,
                    "y": 2977
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='76' style='font-size:16px'>We train unconditional models of 2562 images on<br>CelebA-HQ [39], FFHQ [41], LSUN-Churches and<br>-Bedrooms [102] and evaluate the i) sample quality and ii)<br>their coverage of the data manifold using ii) FID [29] and<br>ii) Precision-and-Recall [50]. Tab. 1 summarizes our re-<br>sults. On CelebA-HQ, we report a new state-of-the-art FID<br>of 5.11, outperforming previous likelihood-based models as<br>well as GANs. We also outperform LSGM [93] where a la-<br>tent diffusion model is trained jointly together with the first<br>stage. In contrast, we train diffusion models in a fixed space</p>",
            "id": 76,
            "page": 5,
            "text": "We train unconditional models of 2562 images on CelebA-HQ , FFHQ , LSUN-Churches and -Bedrooms  and evaluate the i) sample quality and ii) their coverage of the data manifold using ii) FID  and ii) Precision-and-Recall . Tab. 1 summarizes our results. On CelebA-HQ, we report a new state-of-the-art FID of 5.11, outperforming previous likelihood-based models as well as GANs. We also outperform LSGM  where a latent diffusion model is trained jointly together with the first stage. In contrast, we train diffusion models in a fixed space"
        },
        {
            "bounding_box": [
                {
                    "x": 1226,
                    "y": 3055
                },
                {
                    "x": 1251,
                    "y": 3055
                },
                {
                    "x": 1251,
                    "y": 3090
                },
                {
                    "x": 1226,
                    "y": 3090
                }
            ],
            "category": "footer",
            "html": "<footer id='77' style='font-size:16px'>5</footer>",
            "id": 77,
            "page": 5,
            "text": "5"
        },
        {
            "bounding_box": [
                {
                    "x": 862,
                    "y": 318
                },
                {
                    "x": 1628,
                    "y": 318
                },
                {
                    "x": 1628,
                    "y": 358
                },
                {
                    "x": 862,
                    "y": 358
                }
            ],
            "category": "caption",
            "html": "<caption id='78' style='font-size:20px'>Text-to-Image Synthesis on LAION. 1.45B Model.</caption>",
            "id": 78,
            "page": 6,
            "text": "Text-to-Image Synthesis on LAION. 1.45B Model."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 370
                },
                {
                    "x": 2295,
                    "y": 370
                },
                {
                    "x": 2295,
                    "y": 1053
                },
                {
                    "x": 201,
                    "y": 1053
                }
            ],
            "category": "figure",
            "html": "<figure><img id='79' style='font-size:16px' alt=\"'A street sign that reads 'A zombie in the 'An image of an animal 'An illustration of a slightly 'A painting of a 'A watercolor painting of a 'A shirt with the inscription:\n'Latent Diffusion' style of Picasso' half mouse half octopus' conscious neural network' squirrel eating a burger' chair that looks like an octopus 'I love generative models!' ,\nLATENT\nDIFFUSION I\nGonoractive Moode!!\nLATETEN\nI\nDIFFUSION\nGenerative\nModels!\nachinata\" data-coord=\"top-left:(201,370); bottom-right:(2295,1053)\" /></figure>",
            "id": 79,
            "page": 6,
            "text": "'A street sign that reads 'A zombie in the 'An image of an animal 'An illustration of a slightly 'A painting of a 'A watercolor painting of a 'A shirt with the inscription: \"Latent Diffusion\" style of Picasso' half mouse half octopus' conscious neural network' squirrel eating a burger' chair that looks like an octopus \"I love generative models!\" , LATENT DIFFUSION I Gonoractive Moode!! LATETEN I DIFFUSION Generative Models! achinata"
        },
        {
            "bounding_box": [
                {
                    "x": 205,
                    "y": 1096
                },
                {
                    "x": 2273,
                    "y": 1096
                },
                {
                    "x": 2273,
                    "y": 1186
                },
                {
                    "x": 205,
                    "y": 1186
                }
            ],
            "category": "caption",
            "html": "<caption id='80' style='font-size:18px'>Figure 5. Samples for user-defined text prompts from our model for text-to-image synthesis, LDM-8 (KL), which was trained on the<br>LAION [78] database. Samples generated with 200 DDIM steps and 7 = 1.0. We use unconditional guidance [32] with s = 10.0.</caption>",
            "id": 80,
            "page": 6,
            "text": "Figure 5. Samples for user-defined text prompts from our model for text-to-image synthesis, LDM-8 (KL), which was trained on the LAION  database. Samples generated with 200 DDIM steps and 7 = 1.0. We use unconditional guidance  with s = 10.0."
        },
        {
            "bounding_box": [
                {
                    "x": 208,
                    "y": 1272
                },
                {
                    "x": 1198,
                    "y": 1272
                },
                {
                    "x": 1198,
                    "y": 1573
                },
                {
                    "x": 208,
                    "y": 1573
                }
            ],
            "category": "figure",
            "html": "<figure><img id='81' style='font-size:14px' alt=\"FID VS. training progress Inception Score VS. training progress\n200\nLDM-1 40\nLDM-2\n150\nLDM-4\n30\nLDM-8 Score\nFID 100\nLDM-16\n20\nLDM-32\n10\n50 Inception\n0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0\ntrain step 1e6 train step 1e6\" data-coord=\"top-left:(208,1272); bottom-right:(1198,1573)\" /></figure>",
            "id": 81,
            "page": 6,
            "text": "FID VS. training progress Inception Score VS. training progress 200 LDM-1 40 LDM-2 150 LDM-4 30 LDM-8 Score FID 100 LDM-16 20 LDM-32 10 50 Inception 0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0 train step 1e6 train step 1e6"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 1617
                },
                {
                    "x": 1197,
                    "y": 1617
                },
                {
                    "x": 1197,
                    "y": 1977
                },
                {
                    "x": 202,
                    "y": 1977
                }
            ],
            "category": "paragraph",
            "html": "<p id='82' style='font-size:18px'>Figure 6. Analyzing the training of class-conditional LDMs with<br>different downsampling factors f over 2M train steps on the Im-<br>ageNet dataset. Pixel-based LDM-1 requires substantially larger<br>train times compared to models with larger downsampling factors<br>(LDM-{4-16}). Too much perceptual compression as in LDM-32<br>limits the overall sample quality. All models are trained on a sin-<br>gle NVIDIA A100 with the same computational budget. Results<br>obtained with 100 DDIM steps [84] and K = 0.</p>",
            "id": 82,
            "page": 6,
            "text": "Figure 6. Analyzing the training of class-conditional LDMs with different downsampling factors f over 2M train steps on the ImageNet dataset. Pixel-based LDM-1 requires substantially larger train times compared to models with larger downsampling factors (LDM-{4-16}). Too much perceptual compression as in LDM-32 limits the overall sample quality. All models are trained on a single NVIDIA A100 with the same computational budget. Results obtained with 100 DDIM steps  and K = 0."
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 1991
                },
                {
                    "x": 1196,
                    "y": 1991
                },
                {
                    "x": 1196,
                    "y": 2301
                },
                {
                    "x": 203,
                    "y": 2301
                }
            ],
            "category": "figure",
            "html": "<figure><img id='83' style='font-size:16px' alt=\"FID VS sample throughput FID VS sample throughput\n4.4\n3.5 LDM-1\nLDM-2\n4.2\nFID 3.0 FID\nLDM-4\n4.0\nLDM-8 log\nlog\nLDM-16\n3.8\n2.5 LDM-32\n3.6\n2.0 3.4\n0 20 40 60 0 20 40 60\nthroughput [samples / s] throughput [samples / s]\" data-coord=\"top-left:(203,1991); bottom-right:(1196,2301)\" /></figure>",
            "id": 83,
            "page": 6,
            "text": "FID VS sample throughput FID VS sample throughput 4.4 3.5 LDM-1 LDM-2 4.2 FID 3.0 FID LDM-4 4.0 LDM-8 log log LDM-16 3.8 2.5 LDM-32 3.6 2.0 3.4 0 20 40 60 0 20 40 60 throughput [samples / s] throughput [samples / s]"
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 2341
                },
                {
                    "x": 1197,
                    "y": 2341
                },
                {
                    "x": 1197,
                    "y": 2659
                },
                {
                    "x": 203,
                    "y": 2659
                }
            ],
            "category": "paragraph",
            "html": "<p id='84' style='font-size:18px'>Figure 7. Comparing LDMs with varying compression on the<br>CelebA-HQ (left) and ImageNet (right) datasets. Different mark-<br>ers indicate {10, 20, 50, 100, 200} sampling steps using DDIM,<br>from right to left along each line. The dashed line shows the FID<br>scores for 200 steps, indicating the strong performance of LDM-<br>{4-8}. FID scores assessed on 5000 samples. All models were<br>trained for 500k (CelebA) / 2M (ImageNet) steps on an A100.</p>",
            "id": 84,
            "page": 6,
            "text": "Figure 7. Comparing LDMs with varying compression on the CelebA-HQ (left) and ImageNet (right) datasets. Different markers indicate {10, 20, 50, 100, 200} sampling steps using DDIM, from right to left along each line. The dashed line shows the FID scores for 200 steps, indicating the strong performance of LDM{4-8}. FID scores assessed on 5000 samples. All models were trained for 500k (CelebA) / 2M (ImageNet) steps on an A100."
        },
        {
            "bounding_box": [
                {
                    "x": 205,
                    "y": 2681
                },
                {
                    "x": 1194,
                    "y": 2681
                },
                {
                    "x": 1194,
                    "y": 2774
                },
                {
                    "x": 205,
                    "y": 2774
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='85' style='font-size:22px'>and avoid the difficulty of weighing reconstruction quality<br>against learning the prior over the latent space, see Fig. 1-2.</p>",
            "id": 85,
            "page": 6,
            "text": "and avoid the difficulty of weighing reconstruction quality against learning the prior over the latent space, see Fig. 1-2."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2776
                },
                {
                    "x": 1197,
                    "y": 2776
                },
                {
                    "x": 1197,
                    "y": 2975
                },
                {
                    "x": 202,
                    "y": 2975
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='86' style='font-size:20px'>We outperform prior diffusion based approaches on all<br>but the LSUN-Bedrooms dataset, where our score is close<br>to ADM [15], despite utilizing half its parameters and re-<br>quiring 4-times less train resources (see Appendix E.3.5).</p>",
            "id": 86,
            "page": 6,
            "text": "We outperform prior diffusion based approaches on all but the LSUN-Bedrooms dataset, where our score is close to ADM , despite utilizing half its parameters and requiring 4-times less train resources (see Appendix E.3.5)."
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 1261
                },
                {
                    "x": 2278,
                    "y": 1261
                },
                {
                    "x": 2278,
                    "y": 1785
                },
                {
                    "x": 1278,
                    "y": 1785
                }
            ],
            "category": "table",
            "html": "<br><table id='87' style='font-size:14px'><tr><td colspan=\"4\">CelebA-HQ 256 x 256</td><td colspan=\"4\">FFHQ 256 x 256</td></tr><tr><td>Method</td><td>FID ↓</td><td>Prec. ↑</td><td>Recall ↑</td><td>Method</td><td>FID ↓</td><td>Prec. ↑</td><td>Recall ↑</td></tr><tr><td>DC-VAE [63]</td><td>15.8</td><td></td><td></td><td>ImageBART [21]</td><td>9.57</td><td>-</td><td></td></tr><tr><td>VQGAN+T. [23] (k=400)</td><td>10.2</td><td></td><td></td><td>U-Net GAN (+aug) [77]</td><td>10.9 (7.6)</td><td>-</td><td>-</td></tr><tr><td>PGGAN [39]</td><td>8.0</td><td>、</td><td>-</td><td>UDM [43]</td><td>5.54</td><td>-</td><td></td></tr><tr><td>LSGM [93]</td><td>7.22</td><td>-</td><td>-</td><td>StyleGAN [41]</td><td>4.16</td><td>0.71</td><td>0.46</td></tr><tr><td>UDM [43]</td><td>7.16</td><td>、</td><td>-</td><td>ProjectedGAN [76]</td><td>3.08</td><td>0.65</td><td>0.46</td></tr><tr><td>LDM-4 (ours, 500-st)</td><td>5.11</td><td>0.72</td><td>0.49</td><td>LDM-4 (ours, 200-s)</td><td>4.98</td><td>0.73</td><td>0.50</td></tr><tr><td colspan=\"4\">LSUN-Churches 256 x 256</td><td colspan=\"4\">LSUN-Bedrooms 256 x 256</td></tr><tr><td>Method</td><td>FID ↓</td><td>Prec. ↑</td><td>Recall ↑</td><td>Method</td><td>FID ↓</td><td>Prec. ↑</td><td>Recall ↑</td></tr><tr><td>DDPM [30]</td><td>7.89</td><td>-</td><td></td><td>ImageBART [21]</td><td>5.51</td><td>-</td><td></td></tr><tr><td>ImageBART [21]</td><td>7.32</td><td>-</td><td>-</td><td>DDPM [30]</td><td>4.9</td><td>-</td><td>-</td></tr><tr><td>PGGAN [39]</td><td>6.42</td><td>-</td><td>-</td><td>UDM [43]</td><td>4.57</td><td>-</td><td>-</td></tr><tr><td>StyleGAN [41]</td><td>4.21</td><td>-</td><td>-</td><td>StyleGAN [41]</td><td>2.35</td><td>0.59</td><td>0.48</td></tr><tr><td>StyleGAN2 [42]</td><td>3.86</td><td>-</td><td>-</td><td>ADM [15]</td><td>1.90</td><td>0.66</td><td>0.51</td></tr><tr><td>ProjectedGAN [76]</td><td>1.59</td><td>0.61</td><td>0.44</td><td>ProjectedGAN [76]</td><td>1.52</td><td>0.61</td><td>0.34</td></tr><tr><td>LDM-8* (ours, 200-s)</td><td>4.02</td><td>0.64</td><td>0.52</td><td>LDM-4 (ours, 200-s)</td><td>2.95</td><td>0.66</td><td>0.48</td></tr></table>",
            "id": 87,
            "page": 6,
            "text": "CelebA-HQ 256 x 256 FFHQ 256 x 256  Method FID ↓ Prec. ↑ Recall ↑ Method FID ↓ Prec. ↑ Recall ↑  DC-VAE  15.8   ImageBART  9.57 -   VQGAN+T.  (k=400) 10.2   U-Net GAN (+aug)  10.9 (7.6) -  PGGAN  8.0 、 - UDM  5.54 -   LSGM  7.22 - - StyleGAN  4.16 0.71 0.46  UDM  7.16 、 - ProjectedGAN  3.08 0.65 0.46  LDM-4 (ours, 500-st) 5.11 0.72 0.49 LDM-4 (ours, 200-s) 4.98 0.73 0.50  LSUN-Churches 256 x 256 LSUN-Bedrooms 256 x 256  Method FID ↓ Prec. ↑ Recall ↑ Method FID ↓ Prec. ↑ Recall ↑  DDPM  7.89 -  ImageBART  5.51 -   ImageBART  7.32 - - DDPM  4.9 -  PGGAN  6.42 - - UDM  4.57 -  StyleGAN  4.21 - - StyleGAN  2.35 0.59 0.48  StyleGAN2  3.86 - - ADM  1.90 0.66 0.51  ProjectedGAN  1.59 0.61 0.44 ProjectedGAN  1.52 0.61 0.34  LDM-8* (ours, 200-s) 4.02 0.64 0.52 LDM-4 (ours, 200-s) 2.95 0.66"
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 1802
                },
                {
                    "x": 2274,
                    "y": 1802
                },
                {
                    "x": 2274,
                    "y": 2028
                },
                {
                    "x": 1281,
                    "y": 2028
                }
            ],
            "category": "caption",
            "html": "<caption id='88' style='font-size:18px'>Table 1. Evaluation metrics for unconditional image synthesis.<br>CelebA-HQ results reproduced from [43, 63, 100], FFHQ from<br>[42, 43]. 1: N-s refers to N sampling steps with the DDIM [84]<br>sampler. * trained in KL-regularized latent space. Additional re-<br>:<br>sults can be found in the supplementary.</caption>",
            "id": 88,
            "page": 6,
            "text": "Table 1. Evaluation metrics for unconditional image synthesis. CelebA-HQ results reproduced from , FFHQ from . 1: N-s refers to N sampling steps with the DDIM  sampler. * trained in KL-regularized latent space. Additional re: sults can be found in the supplementary."
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 2065
                },
                {
                    "x": 2278,
                    "y": 2065
                },
                {
                    "x": 2278,
                    "y": 2371
                },
                {
                    "x": 1279,
                    "y": 2371
                }
            ],
            "category": "table",
            "html": "<table id='89' style='font-size:16px'><tr><td colspan=\"5\">Text-Conditional Image Synthesis</td></tr><tr><td>Method</td><td>FID ↓</td><td>IS↑</td><td>Nparams</td><td></td></tr><tr><td>CogView† [17]</td><td>27.10</td><td>18.20</td><td>4B</td><td>self-ranking, rejection rate 0.017</td></tr><tr><td>LAFITE+ [109]</td><td>26.94</td><td>26.02</td><td>75M</td><td></td></tr><tr><td>GLIDE* [59]</td><td>12.24</td><td>-</td><td>6B</td><td>277 DDIM steps, c.f.g. [32] s = 3</td></tr><tr><td>Make-A-Scene* [26]</td><td>11.84</td><td>-</td><td>4B</td><td>c.f.g for AR models [98] s = 5</td></tr><tr><td>LDM-KL-8</td><td>23.31</td><td>20.03±0.33</td><td>1.45B</td><td>250 DDIM steps</td></tr><tr><td>LDM-KL-8-G*</td><td>12.63</td><td>30.29 ±0.42</td><td>1.45B</td><td>250 DDIM steps, c.f.g. [32] s = 1.5</td></tr></table>",
            "id": 89,
            "page": 6,
            "text": "Text-Conditional Image Synthesis  Method FID ↓ IS↑ Nparams   CogView†  27.10 18.20 4B self-ranking, rejection rate 0.017  LAFITE+  26.94 26.02 75M   GLIDE*  12.24 - 6B 277 DDIM steps, c.f.g.  s = 3  Make-A-Scene*  11.84 - 4B c.f.g for AR models  s = 5  LDM-KL-8 23.31 20.03±0.33 1.45B 250 DDIM steps  LDM-KL-8-G* 12.63 30.29 ±0.42 1.45B"
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2393
                },
                {
                    "x": 2275,
                    "y": 2393
                },
                {
                    "x": 2275,
                    "y": 2620
                },
                {
                    "x": 1280,
                    "y": 2620
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='90' style='font-size:18px'>Table 2. Evaluation of text-conditional image synthesis on the<br>256 x 256-sized MS-COCO [51] dataset: with 250 DDIM [84]<br>steps our model is on par with the most recent diffusion [59] and<br>autoregressive [26] methods despite using significantly less pa-<br>rameters. T/*:Numbers from [109]/ [26]</p>",
            "id": 90,
            "page": 6,
            "text": "Table 2. Evaluation of text-conditional image synthesis on the 256 x 256-sized MS-COCO  dataset: with 250 DDIM  steps our model is on par with the most recent diffusion  and autoregressive  methods despite using significantly less parameters. T/*:Numbers from / "
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2728
                },
                {
                    "x": 2276,
                    "y": 2728
                },
                {
                    "x": 2276,
                    "y": 2973
                },
                {
                    "x": 1280,
                    "y": 2973
                }
            ],
            "category": "paragraph",
            "html": "<p id='91' style='font-size:22px'>Moreover, LDMs consistently improve upon GAN-based<br>methods in Precision and Recall, thus confirming the ad-<br>vantages of their mode-covering likelihood-based training<br>objective over adversarial approaches. In Fig. 4 we also<br>show qualitative results on each dataset.</p>",
            "id": 91,
            "page": 6,
            "text": "Moreover, LDMs consistently improve upon GAN-based methods in Precision and Recall, thus confirming the advantages of their mode-covering likelihood-based training objective over adversarial approaches. In Fig. 4 we also show qualitative results on each dataset."
        },
        {
            "bounding_box": [
                {
                    "x": 1225,
                    "y": 3057
                },
                {
                    "x": 1254,
                    "y": 3057
                },
                {
                    "x": 1254,
                    "y": 3092
                },
                {
                    "x": 1225,
                    "y": 3092
                }
            ],
            "category": "footer",
            "html": "<footer id='92' style='font-size:18px'>6</footer>",
            "id": 92,
            "page": 6,
            "text": "6"
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 297
                },
                {
                    "x": 1150,
                    "y": 297
                },
                {
                    "x": 1150,
                    "y": 906
                },
                {
                    "x": 199,
                    "y": 906
                }
            ],
            "category": "figure",
            "html": "<figure><img id='93' style='font-size:14px' alt=\"light\ncomelight\nTrAsischente\nwall-coocrete\" data-coord=\"top-left:(199,297); bottom-right:(1150,906)\" /></figure>",
            "id": 93,
            "page": 7,
            "text": "light comelight TrAsischente wall-coocrete"
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 927
                },
                {
                    "x": 1193,
                    "y": 927
                },
                {
                    "x": 1193,
                    "y": 1014
                },
                {
                    "x": 204,
                    "y": 1014
                }
            ],
            "category": "caption",
            "html": "<br><caption id='94' style='font-size:16px'>Figure 8. Layout-to-image synthesis with an LDM on COCO [4],<br>see Sec. 4.3.1. Quantitative evaluation in the supplement D.3.</caption>",
            "id": 94,
            "page": 7,
            "text": "Figure 8. Layout-to-image synthesis with an LDM on COCO , see Sec. 4.3.1. Quantitative evaluation in the supplement D.3."
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 1069
                },
                {
                    "x": 855,
                    "y": 1069
                },
                {
                    "x": 855,
                    "y": 1116
                },
                {
                    "x": 204,
                    "y": 1116
                }
            ],
            "category": "paragraph",
            "html": "<p id='95' style='font-size:20px'>4.3. Conditional Latent Diffusion</p>",
            "id": 95,
            "page": 7,
            "text": "4.3. Conditional Latent Diffusion"
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 1148
                },
                {
                    "x": 926,
                    "y": 1148
                },
                {
                    "x": 926,
                    "y": 1192
                },
                {
                    "x": 203,
                    "y": 1192
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='96' style='font-size:18px'>4.3.1 Transformer Encoders for LDMs</p>",
            "id": 96,
            "page": 7,
            "text": "4.3.1 Transformer Encoders for LDMs"
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 1206
                },
                {
                    "x": 1199,
                    "y": 1206
                },
                {
                    "x": 1199,
                    "y": 2500
                },
                {
                    "x": 200,
                    "y": 2500
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='97' style='font-size:18px'>By introducing cross-attention based conditioning into<br>LDMs we open them up for various conditioning modali-<br>ties previously unexplored for diffusion models. For text-<br>to-image image modeling, we train a 1.45B parameter<br>KL-regularized LDM conditioned on language prompts on<br>LAION-400M [78]. We employ the BERT-tokenizer [14]<br>and implement �� as a transformer [97] to infer a latent<br>code which is mapped into the UNet via (multi-head) cross-<br>attention (Sec. 3.3). This combination of domain specific<br>experts for learning a language representation and visual<br>synthesis results in a powerful model, which generalizes<br>well to complex, user-defined text prompts, cf. Fig. 8 and 5.<br>For quantitative analysis, we follow prior work and evaluate<br>text-to-image generation on the MS-COCO [51] validation<br>set, where our model improves upon powerful AR [17, 66]<br>and GAN-based [109] methods, cf. Tab. 2. We note that ap-<br>plying classifier-free diffusion guidance [32] greatly boosts<br>sample quality, such that the guided LDM-KL-8-G is on par<br>with the recent state-of-the-art AR [26] and diffusion mod-<br>els [59] for text-to-image synthesis, while substantially re-<br>ducing parameter count. To further analyze the flexibility of<br>the cross-attention based conditioning mechanism we also<br>train models to synthesize images based on semantic lay-<br>outs on OpenImages [49], and finetune on COCO [4], see<br>Fig. 8. See Sec. D.3 for the quantitative evaluation and im-<br>plementation details.</p>",
            "id": 97,
            "page": 7,
            "text": "By introducing cross-attention based conditioning into LDMs we open them up for various conditioning modalities previously unexplored for diffusion models. For textto-image image modeling, we train a 1.45B parameter KL-regularized LDM conditioned on language prompts on LAION-400M . We employ the BERT-tokenizer  and implement �� as a transformer  to infer a latent code which is mapped into the UNet via (multi-head) crossattention (Sec. 3.3). This combination of domain specific experts for learning a language representation and visual synthesis results in a powerful model, which generalizes well to complex, user-defined text prompts, cf. Fig. 8 and 5. For quantitative analysis, we follow prior work and evaluate text-to-image generation on the MS-COCO  validation set, where our model improves upon powerful AR  and GAN-based  methods, cf. Tab. 2. We note that applying classifier-free diffusion guidance  greatly boosts sample quality, such that the guided LDM-KL-8-G is on par with the recent state-of-the-art AR  and diffusion models  for text-to-image synthesis, while substantially reducing parameter count. To further analyze the flexibility of the cross-attention based conditioning mechanism we also train models to synthesize images based on semantic layouts on OpenImages , and finetune on COCO , see Fig. 8. See Sec. D.3 for the quantitative evaluation and implementation details."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2505
                },
                {
                    "x": 1198,
                    "y": 2505
                },
                {
                    "x": 1198,
                    "y": 2801
                },
                {
                    "x": 201,
                    "y": 2801
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='98' style='font-size:18px'>Lastly, following prior work [3, 15, 21, 23], we evalu-<br>ate our best-performing class-conditional ImageNet mod-<br>els with f E {4,8} from Sec. 4.1 in Tab. 3, Fig. 4 and<br>Sec. D.4. Here we outperform the state of the art diffu-<br>sion model ADM [15] while significantly reducing compu-<br>tational requirements and parameter count, cf. Tab 18.</p>",
            "id": 98,
            "page": 7,
            "text": "Lastly, following prior work , we evaluate our best-performing class-conditional ImageNet models with f E {4,8} from Sec. 4.1 in Tab. 3, Fig. 4 and Sec. D.4. Here we outperform the state of the art diffusion model ADM  while significantly reducing computational requirements and parameter count, cf. Tab 18."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2829
                },
                {
                    "x": 992,
                    "y": 2829
                },
                {
                    "x": 992,
                    "y": 2874
                },
                {
                    "x": 201,
                    "y": 2874
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='99' style='font-size:20px'>4.3.2 Convolutional Sampling Beyond 2562</p>",
            "id": 99,
            "page": 7,
            "text": "4.3.2 Convolutional Sampling Beyond 2562"
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 2881
                },
                {
                    "x": 1195,
                    "y": 2881
                },
                {
                    "x": 1195,
                    "y": 2974
                },
                {
                    "x": 204,
                    "y": 2974
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='100' style='font-size:18px'>By concatenating spatially aligned conditioning informa-<br>tion to the input of EO, LDMs can serve as efficient general-</p>",
            "id": 100,
            "page": 7,
            "text": "By concatenating spatially aligned conditioning information to the input of EO, LDMs can serve as efficient general-"
        },
        {
            "bounding_box": [
                {
                    "x": 1284,
                    "y": 298
                },
                {
                    "x": 2275,
                    "y": 298
                },
                {
                    "x": 2275,
                    "y": 496
                },
                {
                    "x": 1284,
                    "y": 496
                }
            ],
            "category": "table",
            "html": "<br><table id='101' style='font-size:14px'><tr><td>Method</td><td>FID↓</td><td>IS↑</td><td>Precision↑</td><td>Recall↑</td><td>Nparams</td><td></td></tr><tr><td>BigGan-deep [3]</td><td>6.95</td><td>203.6±2.6</td><td>0.87</td><td>0.28</td><td>340M</td><td></td></tr><tr><td>ADM [15]</td><td>10.94</td><td>100.98</td><td>0.69</td><td>0.63</td><td>554M</td><td>250 DDIM steps</td></tr><tr><td>ADM-G [15]</td><td>4.59</td><td>186.7</td><td>0.82</td><td>0.52</td><td>608M</td><td>250 DDIM steps</td></tr><tr><td>LDM-4 (ours)</td><td>10.56</td><td>103.49±1.24</td><td>0.71</td><td>0.62</td><td>400M</td><td>250 DDIM steps</td></tr><tr><td>LDM-4-G (ours)</td><td>3.60</td><td>247.67±5.59</td><td>0.87</td><td>0.48</td><td>400M</td><td>250 steps, c.f.g [32], s = 1.5</td></tr></table>",
            "id": 101,
            "page": 7,
            "text": "Method FID↓ IS↑ Precision↑ Recall↑ Nparams   BigGan-deep  6.95 203.6±2.6 0.87 0.28 340M   ADM  10.94 100.98 0.69 0.63 554M 250 DDIM steps  ADM-G  4.59 186.7 0.82 0.52 608M 250 DDIM steps  LDM-4 (ours) 10.56 103.49±1.24 0.71 0.62 400M 250 DDIM steps  LDM-4-G (ours) 3.60 247.67±5.59 0.87 0.48 400M"
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 539
                },
                {
                    "x": 2276,
                    "y": 539
                },
                {
                    "x": 2276,
                    "y": 765
                },
                {
                    "x": 1281,
                    "y": 765
                }
            ],
            "category": "caption",
            "html": "<caption id='102' style='font-size:16px'>Table 3. Comparison of a class-conditional ImageNet LDM with<br>recent state-of-the-art methods for class-conditional image gener-<br>ation on ImageNet [12]. A more detailed comparison with addi-<br>tional baselines can be found in D.4, Tab. 10 and F. c.f.g. denotes<br>classifier-free guidance with a scale s as proposed in [32].</caption>",
            "id": 102,
            "page": 7,
            "text": "Table 3. Comparison of a class-conditional ImageNet LDM with recent state-of-the-art methods for class-conditional image generation on ImageNet . A more detailed comparison with additional baselines can be found in D.4, Tab. 10 and F. c.f.g. denotes classifier-free guidance with a scale s as proposed in ."
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 862
                },
                {
                    "x": 2277,
                    "y": 862
                },
                {
                    "x": 2277,
                    "y": 1802
                },
                {
                    "x": 1278,
                    "y": 1802
                }
            ],
            "category": "paragraph",
            "html": "<p id='103' style='font-size:18px'>purpose image-to-image translation models. We use this<br>to train models for semantic synthesis, super-resolution<br>(Sec. 4.4) and inpainting (Sec. 4.5). For semantic synthe-<br>sis, we use images of landscapes paired with semantic maps<br>[23, 61] and concatenate downsampled versions of the se-<br>mantic maps with the latent image representation of a f = 4<br>model (VQ-reg., see Tab. 8). We train on an input resolution<br>of 2562 (crops from 3842) but find that our model general-<br>izes to larger resolutions and can generate images up to the<br>megapixel regime when evaluated in a convolutional man-<br>ner (see Fig. 9). We exploit this behavior to also apply the<br>super-resolution models in Sec. 4.4 and the inpainting mod-<br>els in Sec. 4.5 to generate large images between 5122 and<br>10242. For this application, the signal-to-noise ratio (in-<br>duced by the scale of the latent space) significantly affects<br>the results. In Sec. D.1 we illustrate this when learning an<br>LDM on (i) the latent space as provided by a f = 4 model<br>(KL-reg., see Tab. 8), and (ii) a rescaled version, scaled by<br>the component-wise standard deviation.</p>",
            "id": 103,
            "page": 7,
            "text": "purpose image-to-image translation models. We use this to train models for semantic synthesis, super-resolution (Sec. 4.4) and inpainting (Sec. 4.5). For semantic synthesis, we use images of landscapes paired with semantic maps  and concatenate downsampled versions of the semantic maps with the latent image representation of a f = 4 model (VQ-reg., see Tab. 8). We train on an input resolution of 2562 (crops from 3842) but find that our model generalizes to larger resolutions and can generate images up to the megapixel regime when evaluated in a convolutional manner (see Fig. 9). We exploit this behavior to also apply the super-resolution models in Sec. 4.4 and the inpainting models in Sec. 4.5 to generate large images between 5122 and 10242. For this application, the signal-to-noise ratio (induced by the scale of the latent space) significantly affects the results. In Sec. D.1 we illustrate this when learning an LDM on (i) the latent space as provided by a f = 4 model (KL-reg., see Tab. 8), and (ii) a rescaled version, scaled by the component-wise standard deviation."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 1808
                },
                {
                    "x": 2275,
                    "y": 1808
                },
                {
                    "x": 2275,
                    "y": 1954
                },
                {
                    "x": 1281,
                    "y": 1954
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='104' style='font-size:18px'>The latter, in combination with classifier-free guid-<br>ance [32], also enables the direct synthesis of > 2562 im-<br>ages for the text-conditional LDM-KL-8-G as in Fig. 13.</p>",
            "id": 104,
            "page": 7,
            "text": "The latter, in combination with classifier-free guidance , also enables the direct synthesis of > 2562 images for the text-conditional LDM-KL-8-G as in Fig. 13."
        },
        {
            "bounding_box": [
                {
                    "x": 1282,
                    "y": 1997
                },
                {
                    "x": 2321,
                    "y": 1997
                },
                {
                    "x": 2321,
                    "y": 2585
                },
                {
                    "x": 1282,
                    "y": 2585
                }
            ],
            "category": "figure",
            "html": "<figure><img id='105' alt=\"\" data-coord=\"top-left:(1282,1997); bottom-right:(2321,2585)\" /></figure>",
            "id": 105,
            "page": 7,
            "text": ""
        },
        {
            "bounding_box": [
                {
                    "x": 1282,
                    "y": 2598
                },
                {
                    "x": 2278,
                    "y": 2598
                },
                {
                    "x": 2278,
                    "y": 2732
                },
                {
                    "x": 1282,
                    "y": 2732
                }
            ],
            "category": "caption",
            "html": "<br><caption id='106' style='font-size:16px'>Figure 9. A LDM trained on 2562 resolution can generalize to<br>larger resolution (here: 512 x 1024) for spatially conditioned tasks<br>such as semantic synthesis of landscape images. See Sec. 4.3.2.</caption>",
            "id": 106,
            "page": 7,
            "text": "Figure 9. A LDM trained on 2562 resolution can generalize to larger resolution (here: 512 x 1024) for spatially conditioned tasks such as semantic synthesis of landscape images. See Sec. 4.3.2."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 2770
                },
                {
                    "x": 2136,
                    "y": 2770
                },
                {
                    "x": 2136,
                    "y": 2817
                },
                {
                    "x": 1281,
                    "y": 2817
                }
            ],
            "category": "paragraph",
            "html": "<p id='107' style='font-size:22px'>4.4. Super-Resolution with Latent Diffusion</p>",
            "id": 107,
            "page": 7,
            "text": "4.4. Super-Resolution with Latent Diffusion"
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 2830
                },
                {
                    "x": 2275,
                    "y": 2830
                },
                {
                    "x": 2275,
                    "y": 2974
                },
                {
                    "x": 1281,
                    "y": 2974
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='108' style='font-size:18px'>LDMs can be efficiently trained for super-resolution by<br>diretly conditioning on low-resolution images via concate-<br>nation (cf. Sec. 3.3). In a first experiment, we follow SR3</p>",
            "id": 108,
            "page": 7,
            "text": "LDMs can be efficiently trained for super-resolution by diretly conditioning on low-resolution images via concatenation (cf. Sec. 3.3). In a first experiment, we follow SR3"
        },
        {
            "bounding_box": [
                {
                    "x": 1225,
                    "y": 3054
                },
                {
                    "x": 1252,
                    "y": 3054
                },
                {
                    "x": 1252,
                    "y": 3091
                },
                {
                    "x": 1225,
                    "y": 3091
                }
            ],
            "category": "footer",
            "html": "<footer id='109' style='font-size:16px'>7</footer>",
            "id": 109,
            "page": 7,
            "text": "7"
        },
        {
            "bounding_box": [
                {
                    "x": 205,
                    "y": 295
                },
                {
                    "x": 1183,
                    "y": 295
                },
                {
                    "x": 1183,
                    "y": 988
                },
                {
                    "x": 205,
                    "y": 988
                }
            ],
            "category": "figure",
            "html": "<figure><img id='110' style='font-size:20px' alt=\"bicubic LDM-SR SR3\" data-coord=\"top-left:(205,295); bottom-right:(1183,988)\" /></figure>",
            "id": 110,
            "page": 8,
            "text": "bicubic LDM-SR SR3"
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 998
                },
                {
                    "x": 1197,
                    "y": 998
                },
                {
                    "x": 1197,
                    "y": 1179
                },
                {
                    "x": 203,
                    "y": 1179
                }
            ],
            "category": "caption",
            "html": "<br><caption id='111' style='font-size:16px'>Figure 10. ImageNet 64→256 super-resolution on ImageNet- Val.<br>LDM-SR has advantages at rendering realistic textures but SR3<br>can synthesize more coherent fine structures. See appendix for<br>additional samples and cropouts. SR3 results from [72].</caption>",
            "id": 111,
            "page": 8,
            "text": "Figure 10. ImageNet 64→256 super-resolution on ImageNet- Val. LDM-SR has advantages at rendering realistic textures but SR3 can synthesize more coherent fine structures. See appendix for additional samples and cropouts. SR3 results from ."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1284
                },
                {
                    "x": 1199,
                    "y": 1284
                },
                {
                    "x": 1199,
                    "y": 2281
                },
                {
                    "x": 201,
                    "y": 2281
                }
            ],
            "category": "paragraph",
            "html": "<p id='112' style='font-size:20px'>[72] and fix the image degradation to a bicubic interpola-<br>tion with 4x-downsampling and train on ImageNet follow-<br>ing SR3's data processing pipeline. We use the f = 4 au-<br>toencoding model pretrained on OpenImages (VQ-reg., cf.<br>Tab. 8) and concatenate the low-resolution conditioning y<br>and the inputs to the UNet, i.e. TO is the identity. Our quali-<br>tative and quantitative results (see Fig. 10 and Tab. 5) show<br>competitive performance and LDM-SR outperforms SR3<br>in FID while SR3 has a better IS. A simple image regres-<br>sion model achieves the highest PSNR and SSIM scores;<br>however these metrics do not align well with human per-<br>ception [106] and favor blurriness over imperfectly aligned<br>high frequency details [72]. Further, we conduct a user<br>study comparing the pixel-baseline with LDM-SR. We fol-<br>low SR3 [72] where human subjects were shown a low-res<br>image in between two high-res images and asked for pref-<br>erence. The results in Tab. 4 affirm the good performance<br>of LDM-SR. PSNR and SSIM can be pushed by using a<br>post-hoc guiding mechanism [15] and we implement this<br>image-based guider via a perceptual loss, see Sec. D.6.</p>",
            "id": 112,
            "page": 8,
            "text": " and fix the image degradation to a bicubic interpolation with 4x-downsampling and train on ImageNet following SR3's data processing pipeline. We use the f = 4 autoencoding model pretrained on OpenImages (VQ-reg., cf. Tab. 8) and concatenate the low-resolution conditioning y and the inputs to the UNet, i.e. TO is the identity. Our qualitative and quantitative results (see Fig. 10 and Tab. 5) show competitive performance and LDM-SR outperforms SR3 in FID while SR3 has a better IS. A simple image regression model achieves the highest PSNR and SSIM scores; however these metrics do not align well with human perception  and favor blurriness over imperfectly aligned high frequency details . Further, we conduct a user study comparing the pixel-baseline with LDM-SR. We follow SR3  where human subjects were shown a low-res image in between two high-res images and asked for preference. The results in Tab. 4 affirm the good performance of LDM-SR. PSNR and SSIM can be pushed by using a post-hoc guiding mechanism  and we implement this image-based guider via a perceptual loss, see Sec. D.6."
        },
        {
            "bounding_box": [
                {
                    "x": 208,
                    "y": 2377
                },
                {
                    "x": 1201,
                    "y": 2377
                },
                {
                    "x": 1201,
                    "y": 2560
                },
                {
                    "x": 208,
                    "y": 2560
                }
            ],
            "category": "table",
            "html": "<table id='113' style='font-size:14px'><tr><td></td><td colspan=\"2\">SR on ImageNet</td><td colspan=\"2\">Inpainting on Places</td></tr><tr><td>User Study</td><td>Pixel-DM (f1)</td><td>LDM-4</td><td>LAMA [88]</td><td>LDM-4</td></tr><tr><td>Task 1: Preference VS GT ↑</td><td>16.0%</td><td>30.4%</td><td>13.6%</td><td>21.0%</td></tr><tr><td>Task 2: Preference Score ↑</td><td>29.4%</td><td>70.6%</td><td>31.9%</td><td>68.1%</td></tr></table>",
            "id": 113,
            "page": 8,
            "text": "SR on ImageNet Inpainting on Places  User Study Pixel-DM (f1) LDM-4 LAMA  LDM-4  Task 1: Preference VS GT ↑ 16.0% 30.4% 13.6% 21.0%  Task 2: Preference Score ↑ 29.4% 70.6% 31.9%"
        },
        {
            "bounding_box": [
                {
                    "x": 205,
                    "y": 2573
                },
                {
                    "x": 1197,
                    "y": 2573
                },
                {
                    "x": 1197,
                    "y": 2702
                },
                {
                    "x": 205,
                    "y": 2702
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='114' style='font-size:16px'>Table 4. Task 1: Subjects were shown ground truth and generated<br>image and asked for preference. Task 2: Subjects had to decide<br>between two generated images. More details in E.3.6</p>",
            "id": 114,
            "page": 8,
            "text": "Table 4. Task 1: Subjects were shown ground truth and generated image and asked for preference. Task 2: Subjects had to decide between two generated images. More details in E.3.6"
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 2779
                },
                {
                    "x": 1198,
                    "y": 2779
                },
                {
                    "x": 1198,
                    "y": 2974
                },
                {
                    "x": 203,
                    "y": 2974
                }
            ],
            "category": "paragraph",
            "html": "<p id='115' style='font-size:20px'>Since the bicubic degradation process does not generalize<br>well to images which do not follow this pre-processing, we<br>also train a generic model, LDM-BSR, by using more di-<br>verse degradation. The results are shown in Sec. D.6.1.</p>",
            "id": 115,
            "page": 8,
            "text": "Since the bicubic degradation process does not generalize well to images which do not follow this pre-processing, we also train a generic model, LDM-BSR, by using more diverse degradation. The results are shown in Sec. D.6.1."
        },
        {
            "bounding_box": [
                {
                    "x": 1284,
                    "y": 253
                },
                {
                    "x": 2275,
                    "y": 253
                },
                {
                    "x": 2275,
                    "y": 465
                },
                {
                    "x": 1284,
                    "y": 465
                }
            ],
            "category": "table",
            "html": "<br><table id='116' style='font-size:14px'><tr><td>Method</td><td>FID ↓</td><td>IS ↑</td><td>PSNR ↑</td><td>SSIM ↑</td><td>Nparams</td><td>[ samples ](*)</td></tr><tr><td>Image Regression [72]</td><td>15.2</td><td>121.1</td><td>27.9</td><td>0.801</td><td>625M</td><td>N/A</td></tr><tr><td>SR3 [72]</td><td>5.2</td><td>180.1</td><td>26.4</td><td>0.762</td><td>625M</td><td>N/A</td></tr><tr><td>LDM-4 (ours, 100 steps)</td><td>2.8±/4.8±</td><td>166.3</td><td>24.4±3.8</td><td>0.69±0.14</td><td>169M</td><td>4.62</td></tr><tr><td>emphLDM-4 (ours, big, 100 steps)</td><td>2.4±/4.3±</td><td>174.9</td><td>24.7±4.1</td><td>0.71±0.15</td><td>552M</td><td>4.5</td></tr><tr><td>LDM-4 (ours, 50 steps, guiding)</td><td>4.4±/6.4±</td><td>153.7</td><td>25.8±3.7</td><td>0.74±0.12</td><td>184M</td><td>0.38</td></tr></table>",
            "id": 116,
            "page": 8,
            "text": "Method FID ↓ IS ↑ PSNR ↑ SSIM ↑ Nparams [ samples ](*)  Image Regression  15.2 121.1 27.9 0.801 625M N/A  SR3  5.2 180.1 26.4 0.762 625M N/A  LDM-4 (ours, 100 steps) 2.8±/4.8± 166.3 24.4±3.8 0.69±0.14 169M 4.62  emphLDM-4 (ours, big, 100 steps) 2.4±/4.3± 174.9 24.7±4.1 0.71±0.15 552M 4.5  LDM-4 (ours, 50 steps, guiding) 4.4±/6.4± 153.7 25.8±3.7 0.74±0.12 184M"
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 475
                },
                {
                    "x": 2275,
                    "y": 475
                },
                {
                    "x": 2275,
                    "y": 605
                },
                {
                    "x": 1281,
                    "y": 605
                }
            ],
            "category": "caption",
            "html": "<br><caption id='117' style='font-size:16px'>Table 5. x4 upscaling results on ImageNet-Val. (2562); 1: FID<br>features computed on validation split, ‡: FID features computed<br>: Assessed on a NVIDIA A100<br>on train split; *</caption>",
            "id": 117,
            "page": 8,
            "text": "Table 5. x4 upscaling results on ImageNet-Val. (2562); 1: FID features computed on validation split, ‡: FID features computed : Assessed on a NVIDIA A100 on train split; *"
        },
        {
            "bounding_box": [
                {
                    "x": 1286,
                    "y": 659
                },
                {
                    "x": 2273,
                    "y": 659
                },
                {
                    "x": 2273,
                    "y": 877
                },
                {
                    "x": 1286,
                    "y": 877
                }
            ],
            "category": "table",
            "html": "<table id='118' style='font-size:14px'><tr><td>Model (reg.-type)</td><td>train throughput samples/sec.</td><td>sampling throughput� @256</td><td>@512</td><td>train+val hours/epoch</td><td>FID@2k epoch 6</td></tr><tr><td>LDM-1 (no first stage)</td><td>0.11</td><td>0.26</td><td>0.07</td><td>20.66</td><td>24.74</td></tr><tr><td>LDM-4 (KL, w/ attn)</td><td>0.32</td><td>0.97</td><td>0.34</td><td>7.66</td><td>15.21</td></tr><tr><td>LDM-4 (VQ, w/ attn)</td><td>0.33</td><td>0.97</td><td>0.34</td><td>7.04</td><td>14.99</td></tr><tr><td>LDM-4 (VQ, w/o attn)</td><td>0.35</td><td>0.99</td><td>0.36</td><td>6.66</td><td>15.95</td></tr></table>",
            "id": 118,
            "page": 8,
            "text": "Model (reg.-type) train throughput samples/sec. sampling throughput� @256 @512 train+val hours/epoch FID@2k epoch 6  LDM-1 (no first stage) 0.11 0.26 0.07 20.66 24.74  LDM-4 (KL, w/ attn) 0.32 0.97 0.34 7.66 15.21  LDM-4 (VQ, w/ attn) 0.33 0.97 0.34 7.04 14.99  LDM-4 (VQ, w/o attn) 0.35 0.99 0.36 6.66"
        },
        {
            "bounding_box": [
                {
                    "x": 1283,
                    "y": 885
                },
                {
                    "x": 2268,
                    "y": 885
                },
                {
                    "x": 2268,
                    "y": 968
                },
                {
                    "x": 1283,
                    "y": 968
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='119' style='font-size:18px'>Table 6. Assessing inpainting efficiency. 1: Deviations from Fig. 7<br>due to varying GPU settings/batch sizes cf. the supplement.</p>",
            "id": 119,
            "page": 8,
            "text": "Table 6. Assessing inpainting efficiency. 1: Deviations from Fig. 7 due to varying GPU settings/batch sizes cf. the supplement."
        },
        {
            "bounding_box": [
                {
                    "x": 1283,
                    "y": 1008
                },
                {
                    "x": 2004,
                    "y": 1008
                },
                {
                    "x": 2004,
                    "y": 1054
                },
                {
                    "x": 1283,
                    "y": 1054
                }
            ],
            "category": "paragraph",
            "html": "<p id='120' style='font-size:22px'>4.5. Inpainting with Latent Diffusion</p>",
            "id": 120,
            "page": 8,
            "text": "4.5. Inpainting with Latent Diffusion"
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 1071
                },
                {
                    "x": 2275,
                    "y": 1071
                },
                {
                    "x": 2275,
                    "y": 1562
                },
                {
                    "x": 1281,
                    "y": 1562
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='121' style='font-size:20px'>Inpainting is the task of filling masked regions of an im-<br>age with new content either because parts of the image are<br>are corrupted or to replace existing but undesired content<br>within the image. We evaluate how our general approach<br>for conditional image generation compares to more special-<br>ized, state-of-the-art approaches for this task. Our evalua-<br>tion follows the protocol of LaMa [88], a recent inpainting<br>model that introduces a specialized architecture relying on<br>Fast Fourier Convolutions [8]. The exact training & evalua-<br>tion protocol on Places [108] is described in Sec. E.2.2.</p>",
            "id": 121,
            "page": 8,
            "text": "Inpainting is the task of filling masked regions of an image with new content either because parts of the image are are corrupted or to replace existing but undesired content within the image. We evaluate how our general approach for conditional image generation compares to more specialized, state-of-the-art approaches for this task. Our evaluation follows the protocol of LaMa , a recent inpainting model that introduces a specialized architecture relying on Fast Fourier Convolutions . The exact training & evaluation protocol on Places  is described in Sec. E.2.2."
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 1571
                },
                {
                    "x": 2276,
                    "y": 1571
                },
                {
                    "x": 2276,
                    "y": 2215
                },
                {
                    "x": 1279,
                    "y": 2215
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='122' style='font-size:20px'>We first analyze the effect of different design choices for<br>the first stage. In particular, we compare the inpainting ef-<br>ficiency of LDM-1 (i.e. a pixel-based conditional DM) with<br>LDM-4, for both KL and VQ regularizations, as well as VQ-<br>LDM-4 without any attention in the first stage (see Tab. 8),<br>where the latter reduces GPU memory for decoding at high<br>resolutions. For comparability, we fix the number of param-<br>eters for all models. Tab. 6 reports the training and sampling<br>throughput at resolution 2562 and 5122. , the total training<br>time in hours per epoch and the FID score on the validation<br>split after six epochs. Overall, we observe a speed-up of at<br>least 2.7x between pixel- and latent-based diffusion models<br>while improving FID scores by a factor of at least 1.6x.</p>",
            "id": 122,
            "page": 8,
            "text": "We first analyze the effect of different design choices for the first stage. In particular, we compare the inpainting efficiency of LDM-1 (i.e. a pixel-based conditional DM) with LDM-4, for both KL and VQ regularizations, as well as VQLDM-4 without any attention in the first stage (see Tab. 8), where the latter reduces GPU memory for decoding at high resolutions. For comparability, we fix the number of parameters for all models. Tab. 6 reports the training and sampling throughput at resolution 2562 and 5122. , the total training time in hours per epoch and the FID score on the validation split after six epochs. Overall, we observe a speed-up of at least 2.7x between pixel- and latent-based diffusion models while improving FID scores by a factor of at least 1.6x."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2227
                },
                {
                    "x": 2275,
                    "y": 2227
                },
                {
                    "x": 2275,
                    "y": 2671
                },
                {
                    "x": 1280,
                    "y": 2671
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='123' style='font-size:20px'>The comparison with other inpainting approaches in<br>Tab. 7 shows that our model with attention improves the<br>overall image quality as measured by FID over that of [88].<br>LPIPS between the unmasked images and our samples is<br>slightly higher than that of [88]. We attribute this to [88]<br>only producing a single result which tends to recover more<br>of an average image compared to the diverse results pro-<br>duced by our LDM cf. Fig. 21. Additionally in a user study<br>(Tab. 4) human subjects favor our results over those of [88].</p>",
            "id": 123,
            "page": 8,
            "text": "The comparison with other inpainting approaches in Tab. 7 shows that our model with attention improves the overall image quality as measured by FID over that of . LPIPS between the unmasked images and our samples is slightly higher than that of . We attribute this to  only producing a single result which tends to recover more of an average image compared to the diverse results produced by our LDM cf. Fig. 21. Additionally in a user study (Tab. 4) human subjects favor our results over those of ."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 2678
                },
                {
                    "x": 2276,
                    "y": 2678
                },
                {
                    "x": 2276,
                    "y": 2976
                },
                {
                    "x": 1281,
                    "y": 2976
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='124' style='font-size:20px'>Based on these initial results, we also trained a larger dif-<br>fusion model (big in Tab. 7) in the latent space of the VQ-<br>regularized first stage without attention. Following [15],<br>the UNet of this diffusion model uses attention layers on<br>three levels ofits feature hierarchy, the BigGAN [3] residual<br>block for up- and downsampling and has 387M parameters</p>",
            "id": 124,
            "page": 8,
            "text": "Based on these initial results, we also trained a larger diffusion model (big in Tab. 7) in the latent space of the VQregularized first stage without attention. Following , the UNet of this diffusion model uses attention layers on three levels ofits feature hierarchy, the BigGAN  residual block for up- and downsampling and has 387M parameters"
        },
        {
            "bounding_box": [
                {
                    "x": 1226,
                    "y": 3057
                },
                {
                    "x": 1252,
                    "y": 3057
                },
                {
                    "x": 1252,
                    "y": 3090
                },
                {
                    "x": 1226,
                    "y": 3090
                }
            ],
            "category": "footer",
            "html": "<footer id='125' style='font-size:20px'>8</footer>",
            "id": 125,
            "page": 8,
            "text": "8"
        },
        {
            "bounding_box": [
                {
                    "x": 214,
                    "y": 305
                },
                {
                    "x": 1189,
                    "y": 305
                },
                {
                    "x": 1189,
                    "y": 1805
                },
                {
                    "x": 214,
                    "y": 1805
                }
            ],
            "category": "figure",
            "html": "<figure><img id='126' style='font-size:14px' alt=\"input result\" data-coord=\"top-left:(214,305); bottom-right:(1189,1805)\" /></figure>",
            "id": 126,
            "page": 9,
            "text": "input result"
        },
        {
            "bounding_box": [
                {
                    "x": 205,
                    "y": 1819
                },
                {
                    "x": 1194,
                    "y": 1819
                },
                {
                    "x": 1194,
                    "y": 1902
                },
                {
                    "x": 205,
                    "y": 1902
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='127' style='font-size:18px'>Figure 11. Qualitative results on object removal with our big, w/<br>ft inpainting model. For more results, see Fig. 22.</p>",
            "id": 127,
            "page": 9,
            "text": "Figure 11. Qualitative results on object removal with our big, w/ ft inpainting model. For more results, see Fig. 22."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 1930
                },
                {
                    "x": 1198,
                    "y": 1930
                },
                {
                    "x": 1198,
                    "y": 2276
                },
                {
                    "x": 202,
                    "y": 2276
                }
            ],
            "category": "paragraph",
            "html": "<p id='128' style='font-size:20px'>instead of 215M. After training, we noticed a discrepancy<br>in the quality of samples produced at resolutions 2562 and<br>5122, which we hypothesize to be caused by the additional<br>attention modules. However, fine-tuning the model for half<br>an epoch at resolution 5122 allows the model to adjust to<br>the new feature statistics and sets a new state of the art FID<br>on image inpainting (big, w/o attn, w/ft in Tab. 7, Fig. 11.).</p>",
            "id": 128,
            "page": 9,
            "text": "instead of 215M. After training, we noticed a discrepancy in the quality of samples produced at resolutions 2562 and 5122, which we hypothesize to be caused by the additional attention modules. However, fine-tuning the model for half an epoch at resolution 5122 allows the model to adjust to the new feature statistics and sets a new state of the art FID on image inpainting (big, w/o attn, w/ft in Tab. 7, Fig. 11.)."
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 2309
                },
                {
                    "x": 902,
                    "y": 2309
                },
                {
                    "x": 902,
                    "y": 2361
                },
                {
                    "x": 203,
                    "y": 2361
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='129' style='font-size:22px'>5. Limitations & Societal Impact</p>",
            "id": 129,
            "page": 9,
            "text": "5. Limitations & Societal Impact"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2394
                },
                {
                    "x": 1199,
                    "y": 2394
                },
                {
                    "x": 1199,
                    "y": 2890
                },
                {
                    "x": 201,
                    "y": 2890
                }
            ],
            "category": "paragraph",
            "html": "<p id='130' style='font-size:20px'>Limitations While LDMs significantly reduce computa-<br>tional requirements compared to pixel-based approaches,<br>their sequential sampling process is still slower than that<br>of GANs. Moreover, the use of LDMs can be question-<br>able when high precision is required: although the loss of<br>image quality is very small in our f = 4 autoencoding mod-<br>els (see Fig. 1), their reconstruction capability can become<br>a bottleneck for tasks that require fine-grained accuracy in<br>pixel space. We assume that our superresolution models<br>(Sec. 4.4) are already somewhat limited in this respect.</p>",
            "id": 130,
            "page": 9,
            "text": "Limitations While LDMs significantly reduce computational requirements compared to pixel-based approaches, their sequential sampling process is still slower than that of GANs. Moreover, the use of LDMs can be questionable when high precision is required: although the loss of image quality is very small in our f = 4 autoencoding models (see Fig. 1), their reconstruction capability can become a bottleneck for tasks that require fine-grained accuracy in pixel space. We assume that our superresolution models (Sec. 4.4) are already somewhat limited in this respect."
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 2929
                },
                {
                    "x": 1195,
                    "y": 2929
                },
                {
                    "x": 1195,
                    "y": 3024
                },
                {
                    "x": 203,
                    "y": 3024
                }
            ],
            "category": "paragraph",
            "html": "<p id='131' style='font-size:18px'>Societal Impact Generative models for media like im-<br>agery are a double-edged sword: On the one hand, they</p>",
            "id": 131,
            "page": 9,
            "text": "Societal Impact Generative models for media like imagery are a double-edged sword: On the one hand, they"
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 283
                },
                {
                    "x": 2274,
                    "y": 283
                },
                {
                    "x": 2274,
                    "y": 821
                },
                {
                    "x": 1280,
                    "y": 821
                }
            ],
            "category": "table",
            "html": "<br><table id='132' style='font-size:14px'><tr><td></td><td colspan=\"2\">40-50% masked</td><td colspan=\"2\">All samples</td></tr><tr><td>Method</td><td>FID ↓</td><td>LPIPS ↓</td><td>FID ↓</td><td>LPIPS ↓</td></tr><tr><td>LDM-4 (ours, big, w/ ft)</td><td>9.39</td><td>0.246± 0.042</td><td>1.50</td><td>0.137± 0.080</td></tr><tr><td>LDM-4 (ours, big, w/o ft)</td><td>12.89</td><td>0.257± 0.047</td><td>2.40</td><td>0.142± 0.085</td></tr><tr><td>LDM-4 (ours, w/ attn)</td><td>11.87</td><td>0.257± 0.042</td><td>2.15</td><td>0.144± 0.084</td></tr><tr><td>LDM-4 (ours, w/o attn)</td><td>12.60</td><td>0.259± 0.041</td><td>2.37</td><td>0.145± 0.084</td></tr><tr><td>LaMa [88]†</td><td>12.31</td><td>0.243± 0.038</td><td>2.23</td><td>0.134± 0.080</td></tr><tr><td>LaMa [88]</td><td>12.0</td><td>0.24</td><td>2.21</td><td>0.14</td></tr><tr><td>CoModGAN [107]</td><td>10.4</td><td>0.26</td><td>1.82</td><td>0.15</td></tr><tr><td>RegionWise [52]</td><td>21.3</td><td>0.27</td><td>4.75</td><td>0.15</td></tr><tr><td>DeepFill v2 [104]</td><td>22.1</td><td>0.28</td><td>5.20</td><td>0.16</td></tr><tr><td>EdgeConnect [58]</td><td>30.5</td><td>0.28</td><td>8.37</td><td>0.16</td></tr></table>",
            "id": 132,
            "page": 9,
            "text": "40-50% masked All samples  Method FID ↓ LPIPS ↓ FID ↓ LPIPS ↓  LDM-4 (ours, big, w/ ft) 9.39 0.246± 0.042 1.50 0.137± 0.080  LDM-4 (ours, big, w/o ft) 12.89 0.257± 0.047 2.40 0.142± 0.085  LDM-4 (ours, w/ attn) 11.87 0.257± 0.042 2.15 0.144± 0.084  LDM-4 (ours, w/o attn) 12.60 0.259± 0.041 2.37 0.145± 0.084  LaMa † 12.31 0.243± 0.038 2.23 0.134± 0.080  LaMa  12.0 0.24 2.21 0.14  CoModGAN  10.4 0.26 1.82 0.15  RegionWise  21.3 0.27 4.75 0.15  DeepFill v2  22.1 0.28 5.20 0.16  EdgeConnect  30.5 0.28 8.37"
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 834
                },
                {
                    "x": 2275,
                    "y": 834
                },
                {
                    "x": 2275,
                    "y": 1060
                },
                {
                    "x": 1280,
                    "y": 1060
                }
            ],
            "category": "caption",
            "html": "<br><caption id='133' style='font-size:16px'>Table 7. Comparison of inpainting performance on 30k crops of<br>size 512 x 512 from test images of Places [108]. The column 40-<br>50% reports metrics computed over hard examples where 40-50%<br>of the image region have to be inpainted. Trecomputed on our test<br>set, since the original test set used in [88] was not available.</caption>",
            "id": 133,
            "page": 9,
            "text": "Table 7. Comparison of inpainting performance on 30k crops of size 512 x 512 from test images of Places . The column 4050% reports metrics computed over hard examples where 40-50% of the image region have to be inpainted. Trecomputed on our test set, since the original test set used in  was not available."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1112
                },
                {
                    "x": 2275,
                    "y": 1112
                },
                {
                    "x": 2275,
                    "y": 1554
                },
                {
                    "x": 1280,
                    "y": 1554
                }
            ],
            "category": "paragraph",
            "html": "<p id='134' style='font-size:18px'>enable various creative applications, and in particular ap-<br>proaches like ours that reduce the cost of training and in-<br>ference have the potential to facilitate access to this tech-<br>nology and democratize its exploration. On the other hand,<br>it also means that it becomes easier to create and dissemi-<br>nate manipulated data or spread misinformation and spam.<br>In particular, the deliberate manipulation of images (\"deep<br>fakes\") is a common problem in this context, and women in<br>particular are disproportionately affected by it [13, 24].</p>",
            "id": 134,
            "page": 9,
            "text": "enable various creative applications, and in particular approaches like ours that reduce the cost of training and inference have the potential to facilitate access to this technology and democratize its exploration. On the other hand, it also means that it becomes easier to create and disseminate manipulated data or spread misinformation and spam. In particular, the deliberate manipulation of images (\"deep fakes\") is a common problem in this context, and women in particular are disproportionately affected by it ."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 1561
                },
                {
                    "x": 2276,
                    "y": 1561
                },
                {
                    "x": 2276,
                    "y": 1804
                },
                {
                    "x": 1281,
                    "y": 1804
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='135' style='font-size:18px'>Generative models can also reveal their training data<br>[5, 90], which is of great concern when the data contain<br>sensitive or personal information and were collected with-<br>out explicit consent. However, the extent to which this also<br>applies to DMs of images is not yet fully understood.</p>",
            "id": 135,
            "page": 9,
            "text": "Generative models can also reveal their training data , which is of great concern when the data contain sensitive or personal information and were collected without explicit consent. However, the extent to which this also applies to DMs of images is not yet fully understood."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1812
                },
                {
                    "x": 2275,
                    "y": 1812
                },
                {
                    "x": 2275,
                    "y": 2151
                },
                {
                    "x": 1280,
                    "y": 2151
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='136' style='font-size:18px'>Finally, deep learning modules tend to reproduce or ex-<br>acerbate biases that are already present in the data [22, 38,<br>91]. While diffusion models achieve better coverage of the<br>data distribution than e.g. GAN-based approaches, the ex-<br>tent to which our two-stage approach that combines adver-<br>sarial training and a likelihood-based objective misrepre-<br>sents the data remains an important research question.</p>",
            "id": 136,
            "page": 9,
            "text": "Finally, deep learning modules tend to reproduce or exacerbate biases that are already present in the data . While diffusion models achieve better coverage of the data distribution than e.g. GAN-based approaches, the extent to which our two-stage approach that combines adversarial training and a likelihood-based objective misrepresents the data remains an important research question."
        },
        {
            "bounding_box": [
                {
                    "x": 1282,
                    "y": 2159
                },
                {
                    "x": 2272,
                    "y": 2159
                },
                {
                    "x": 2272,
                    "y": 2253
                },
                {
                    "x": 1282,
                    "y": 2253
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='137' style='font-size:20px'>For a more general, detailed discussion of the ethical<br>considerations of deep generative models, see e.g. [13].</p>",
            "id": 137,
            "page": 9,
            "text": "For a more general, detailed discussion of the ethical considerations of deep generative models, see e.g. ."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 2282
                },
                {
                    "x": 1578,
                    "y": 2282
                },
                {
                    "x": 1578,
                    "y": 2329
                },
                {
                    "x": 1281,
                    "y": 2329
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='138' style='font-size:20px'>6. Conclusion</p>",
            "id": 138,
            "page": 9,
            "text": "6. Conclusion"
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2344
                },
                {
                    "x": 2277,
                    "y": 2344
                },
                {
                    "x": 2277,
                    "y": 2741
                },
                {
                    "x": 1280,
                    "y": 2741
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='139' style='font-size:20px'>We have presented latent diffusion models, a simple and<br>efficient way to significantly improve both the training and<br>sampling efficiency of denoising diffusion models with-<br>out degrading their quality. Based on this and our cross-<br>attention conditioning mechanism, our experiments could<br>demonstrate favorable results compared to state-of-the-art<br>methods across a wide range of conditional image synthesis<br>tasks without task-specific architectures.</p>",
            "id": 139,
            "page": 9,
            "text": "We have presented latent diffusion models, a simple and efficient way to significantly improve both the training and sampling efficiency of denoising diffusion models without degrading their quality. Based on this and our crossattention conditioning mechanism, our experiments could demonstrate favorable results compared to state-of-the-art methods across a wide range of conditional image synthesis tasks without task-specific architectures."
        },
        {
            "bounding_box": [
                {
                    "x": 1282,
                    "y": 2865
                },
                {
                    "x": 2277,
                    "y": 2865
                },
                {
                    "x": 2277,
                    "y": 3022
                },
                {
                    "x": 1282,
                    "y": 3022
                }
            ],
            "category": "paragraph",
            "html": "<p id='140' style='font-size:16px'>This work has been supported by the German Federal Ministry for<br>Economic Affairs and Energy within the project 'KI-Absicherung - Safe<br>AI for automated driving' and by the German Research Foundation (DFG)<br>project 421703927.</p>",
            "id": 140,
            "page": 9,
            "text": "This work has been supported by the German Federal Ministry for Economic Affairs and Energy within the project 'KI-Absicherung - Safe AI for automated driving' and by the German Research Foundation (DFG) project 421703927."
        },
        {
            "bounding_box": [
                {
                    "x": 1226,
                    "y": 3057
                },
                {
                    "x": 1252,
                    "y": 3057
                },
                {
                    "x": 1252,
                    "y": 3088
                },
                {
                    "x": 1226,
                    "y": 3088
                }
            ],
            "category": "footer",
            "html": "<footer id='141' style='font-size:20px'>9</footer>",
            "id": 141,
            "page": 9,
            "text": "9"
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 301
                },
                {
                    "x": 445,
                    "y": 301
                },
                {
                    "x": 445,
                    "y": 352
                },
                {
                    "x": 204,
                    "y": 352
                }
            ],
            "category": "paragraph",
            "html": "<p id='142' style='font-size:20px'>References</p>",
            "id": 142,
            "page": 10,
            "text": "References"
        },
        {
            "bounding_box": [
                {
                    "x": 236,
                    "y": 369
                },
                {
                    "x": 1201,
                    "y": 369
                },
                {
                    "x": 1201,
                    "y": 2970
                },
                {
                    "x": 236,
                    "y": 2970
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='143' style='font-size:14px'>[1] Eirikur Agustsson and Radu Timofte. NTIRE 2017 chal-<br>lenge on single image super-resolution: Dataset and study.<br>In 2017 IEEE Conference on Computer Vision and Pattern<br>Recognition Workshops, CVPR Workshops 2017, Honolulu,<br>HI, USA, July 21-26, 2017, pages 1122-1131. IEEE Com-<br>puter Society, 2017. 1<br>[2] Martin Arjovsky, Soumith Chintala, and Leon Bottou.<br>Wasserstein gan, 2017. 3<br>[3] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large<br>scale GAN training for high fidelity natural image synthe-<br>sis. In Int. Conf. Learn. Represent., 2019. 1, 2, 7, 8, 22,<br>28<br>[4] Holger Caesar, Jasper R. R. Uijlings, and Vittorio Ferrari.<br>Coco-stuff: Thing and stuff classes in context. In 2018<br>IEEE Conference on Computer Vision and Pattern Recog-<br>nition, CVPR 2018, Salt Lake City, UT, USA, June 18-<br>22, 2018, pages 1209-1218. Computer Vision Foundation /<br>IEEE Computer Society, 2018. 7, 20, 22<br>[5] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew<br>Jagielski, Ariel Herbert- Voss, Katherine Lee, Adam<br>Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al.<br>Extracting training data from large language models. In<br>30th USENIX Security Symposium (USENIX Security 21),<br>pages 2633-2650, 2021. 9<br>[6] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Hee-<br>WOO Jun, David Luan, and Ilya Sutskever. Generative pre-<br>training from pixels. In ICML, volume 119 of Proceedings<br>of Machine Learning Research, pages 1691-1703. PMLR,<br>2020. 3<br>[7] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J. Weiss, Mo-<br>hammad Norouzi, and William Chan. Wavegrad: Estimat-<br>ing gradients for waveform generation. In ICLR. OpenRe-<br>view.net, 2021. 1<br>[8] Lu Chi, Borui Jiang, and Yadong Mu. Fast fourier convolu-<br>tion. In NeurIPS, 2020. 8<br>[9] Rewon Child. Very deep vaes generalize autoregressive<br>models and can outperform them on images. CoRR,<br>abs/2011.10650, 2020. 3<br>[10] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.<br>Generating long sequences with sparse transformers.<br>CoRR, abs/1904. 10509, 2019. 3<br>[11] Bin Dai and David P. Wipf. Diagnosing and enhancing VAE<br>models. In ICLR (Poster). OpenReview.net, 2019. 2, 3<br>[12] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,<br>and Fei-Fei Li. Imagenet: A large-scale hierarchical im-<br>age database. In CVPR, pages 248-255. IEEE Computer<br>Society, 2009. 1, 5, 7, 22<br>[13] Emily Denton. Ethical considerations of generative ai. AI<br>for Content Creation Workshop, CVPR, 2021. 9<br>[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and<br>Kristina Toutanova. BERT: pre-training of deep bidirec-<br>tional transformers for language understanding. CoRR,<br>abs/1810.04805, 2018. 7<br>[15] Prafulla Dhariwal and Alex Nichol. Diffusion models beat<br>gans on image synthesis. CoRR, abs/2105.05233, 2021. 1,<br>2, 3, 4, 6, 7, 8, 18, 22, 25, 26, 28</p>",
            "id": 143,
            "page": 10,
            "text": " Eirikur Agustsson and Radu Timofte. NTIRE 2017 challenge on single image super-resolution: Dataset and study. In 2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2017, Honolulu, HI, USA, July 21-26, 2017, pages 1122-1131. IEEE Computer Society, 2017. 1  Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein gan, 2017. 3  Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity natural image synthesis. In Int. Conf. Learn. Represent., 2019. 1, 2, 7, 8, 22, 28  Holger Caesar, Jasper R. R. Uijlings, and Vittorio Ferrari. Coco-stuff: Thing and stuff classes in context. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 1822, 2018, pages 1209-1218. Computer Vision Foundation / IEEE Computer Society, 2018. 7, 20, 22  Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert- Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson,  Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21), pages 2633-2650, 2021. 9  Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, HeeWOO Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In ICML, volume 119 of Proceedings of Machine Learning Research, pages 1691-1703. PMLR, 2020. 3  Nanxin Chen, Yu Zhang, Heiga Zen, Ron J. Weiss, Mohammad Norouzi, and William Chan. Wavegrad: Estimating gradients for waveform generation. In ICLR. OpenReview.net, 2021. 1  Lu Chi, Borui Jiang, and Yadong Mu. Fast fourier convolution. In NeurIPS, 2020. 8  Rewon Child. Very deep vaes generalize autoregressive models and can outperform them on images. CoRR, abs/2011.10650, 2020. 3  Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. CoRR, abs/1904. 10509, 2019. 3  Bin Dai and David P. Wipf. Diagnosing and enhancing VAE models. In ICLR (Poster). OpenReview.net, 2019. 2, 3  Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. Imagenet: A large-scale hierarchical image database. In CVPR, pages 248-255. IEEE Computer Society, 2009. 1, 5, 7, 22  Emily Denton. Ethical considerations of generative ai. AI for Content Creation Workshop, CVPR, 2021. 9  Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018. 7  Prafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image synthesis. CoRR, abs/2105.05233, 2021. 1, 2, 3, 4, 6, 7, 8, 18, 22, 25, 26, 28"
        },
        {
            "bounding_box": [
                {
                    "x": 1298,
                    "y": 301
                },
                {
                    "x": 2290,
                    "y": 301
                },
                {
                    "x": 2290,
                    "y": 2973
                },
                {
                    "x": 1298,
                    "y": 2973
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='144' style='font-size:14px'>[16] Sander Dieleman. Musings on typicality, 2020. 1, 3<br>[17] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng,<br>Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao,<br>Hongxia Yang, and Jie Tang. Cogview: Mastering text-to-<br>image generation via transformers. CoRR, abs/2105.13290,<br>2021. 6, 7<br>[18] Laurent Dinh, David Krueger, and Yoshua Bengio. Nice:<br>Non-linear independent components estimation, 2015. 3<br>[19] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Ben-<br>gio. Density estimation using real NVP. In 5th Inter-<br>national Conference on Learning Representations, ICLR<br>2017, Toulon, France, April 24-26, 2017, Conference Track<br>Proceedings. OpenReview.net, 2017. 1, 3<br>[20] Alexey Dosovitskiy and Thomas Brox. Generating images<br>with perceptual similarity metrics based on deep networks.<br>In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg,<br>Isabelle Guyon, and Roman Garnett, editors, Adv. Neural<br>Inform. Process. Syst., pages 658-666, 2016. 3<br>[21] Patrick Esser, Robin Rombach, Andreas Blattmann, and<br>Bjorn Ommer. Imagebart: Bidirectional context with multi-<br>nomial diffusion for autoregressive image synthesis. CoRR,<br>abs/2108.08827, 2021. 6, 7, 22<br>[22] Patrick Esser, Robin Rombach, and Bjorn Ommer. A<br>note on data biases in generative models. arXiv preprint<br>arXiv:2012.02516, 2020. 9<br>[23] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming<br>transformers for high-resolution image synthesis. CoRR,<br>abs/2012.09841, 2020. 2, 3, 4, 6, 7, 21, 22, 29, 34, 36<br>[24] Mary Anne Franks and Ari Ezra Waldman. Sex, lies, and<br>videotape: Deep fakes and free speech delusions. Md. L.<br>Rev., 78:892, 2018. 9<br>[25] Kevin Frans, Lisa B. Soros, and Olaf Witkowski. Clipdraw:<br>Exploring text-to-drawing synthesis through language-<br>image encoders. ArXiv, abs/2106.14843, 2021. 3<br>[26] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin,<br>Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene-<br>based text-to-image generation with human priors. CoRR,<br>abs/2203.13131, 2022. 6, 7, 16<br>[27] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing<br>Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville,<br>and Yoshua Bengio. Generative adversarial networks.<br>CoRR, 2014. 1, 2<br>[28] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent<br>Dumoulin, and Aaron Courville. Improved training of<br>wasserstein gans, 2017. 3<br>[29] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,<br>Bernhard Nessler, and Sepp Hochreiter. Gans trained by<br>a two time-scale update rule converge to a local nash equi-<br>librium. In Adv. Neural Inform. Process. Syst., pages 6626-<br>6637, 2017. 1, 5, 26<br>[30] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-<br>fusion probabilistic models. In NeurIPS, 2020. 1, 2, 3, 4,<br>6, 17<br>[31] Jonathan Ho, Chitwan Saharia, William Chan, David J.<br>Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded<br>diffusion models for high fidelity image generation. CoRR,<br>abs/2106.15282, 2021. 1, 3, 22</p>",
            "id": 144,
            "page": 10,
            "text": " Sander Dieleman. Musings on typicality, 2020. 1, 3  Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, and Jie Tang. Cogview: Mastering text-toimage generation via transformers. CoRR, abs/2105.13290, 2021. 6, 7  Laurent Dinh, David Krueger, and Yoshua Bengio. Nice: Non-linear independent components estimation, 2015. 3  Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. 1, 3  Alexey Dosovitskiy and Thomas Brox. Generating images with perceptual similarity metrics based on deep networks. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, editors, Adv. Neural Inform. Process. Syst., pages 658-666, 2016. 3  Patrick Esser, Robin Rombach, Andreas Blattmann, and Bjorn Ommer. Imagebart: Bidirectional context with multinomial diffusion for autoregressive image synthesis. CoRR, abs/2108.08827, 2021. 6, 7, 22  Patrick Esser, Robin Rombach, and Bjorn Ommer. A note on data biases in generative models. arXiv preprint arXiv:2012.02516, 2020. 9  Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. CoRR, abs/2012.09841, 2020. 2, 3, 4, 6, 7, 21, 22, 29, 34, 36  Mary Anne Franks and Ari Ezra Waldman. Sex, lies, and videotape: Deep fakes and free speech delusions. Md. L. Rev., 78:892, 2018. 9  Kevin Frans, Lisa B. Soros, and Olaf Witkowski. Clipdraw: Exploring text-to-drawing synthesis through languageimage encoders. ArXiv, abs/2106.14843, 2021. 3  Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene: Scenebased text-to-image generation with human priors. CoRR, abs/2203.13131, 2022. 6, 7, 16  Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial networks. CoRR, 2014. 1, 2  Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved training of wasserstein gans, 2017. 3  Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Adv. Neural Inform. Process. Syst., pages 66266637, 2017. 1, 5, 26  Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. 1, 2, 3, 4, 6, 17  Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. CoRR, abs/2106.15282, 2021. 1, 3, 22"
        },
        {
            "bounding_box": [
                {
                    "x": 1220,
                    "y": 3054
                },
                {
                    "x": 1263,
                    "y": 3054
                },
                {
                    "x": 1263,
                    "y": 3091
                },
                {
                    "x": 1220,
                    "y": 3091
                }
            ],
            "category": "footer",
            "html": "<footer id='145' style='font-size:16px'>10</footer>",
            "id": 145,
            "page": 10,
            "text": "10"
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 295
                },
                {
                    "x": 1204,
                    "y": 295
                },
                {
                    "x": 1204,
                    "y": 2970
                },
                {
                    "x": 225,
                    "y": 2970
                }
            ],
            "category": "paragraph",
            "html": "<p id='146' style='font-size:14px'>[32] Jonathan Ho and Tim Salimans. Classifier-free diffusion<br>guidance. In NeurIPS 2021 Workshop on Deep Generative<br>Models and Downstream Applications, 2021. 6, 7, 16, 22,<br>28, 37, 38<br>[33] Phillip Isola, Jun- Yan Zhu, Tinghui Zhou, and Alexei A.<br>Efros. Image-to-image translation with conditional adver-<br>sarial networks. In CVPR, pages 5967-5976. IEEE Com-<br>puter Society, 2017. 3, 4<br>[34] Phillip Isola, Jun- Yan Zhu, Tinghui Zhou, and Alexei A.<br>Efros. Image-to-image translation with conditional adver-<br>sarial networks. 2017 IEEE Conference on Computer Vi-<br>sion and Pattern Recognition (CVPR), pages 5967-5976,<br>2017. 4<br>[35] Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste<br>Alayrac, Carl Doersch, Catalin Ionescu, David Ding,<br>Skanda Koppula, Daniel Zoran, Andrew Brock, Evan<br>Shelhamer, Olivier J. Henaff, Matthew M. Botvinick,<br>Andrew Zisserman, Oriol Vinyals, and Joao Carreira.<br>Perceiver IO: A general architecture for structured inputs<br>&outputs. CoRR, abs/2107.14795, 2021. 4<br>[36] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals,<br>Andrew Zisserman, and Joao Carreira. Perceiver: General<br>perception with iterative attention. In Marina Meila and<br>Tong Zhang, editors, Proceedings of the 38th International<br>Conference on Machine Learning, ICML 2021, 18-24 July<br>2021, Virtual Event, volume 139 of Proceedings ofMachine<br>Learning Research, pages 4651-4664. PMLR, 2021. 4, 5<br>[37] Manuel Jahn, Robin Rombach, and Bjorn Ommer. High-<br>resolution complex scene synthesis with transformers.<br>CoRR, abs/2105.06458, 2021. 20, 22, 27<br>[38] Niharika Jain, Alberto Olmo, Sailik Sengupta, Lydia<br>Manikonda, and Subbarao Kambhampati. Imperfect ima-<br>ganation: Implications of gans exacerbating biases on fa-<br>cial data augmentation and snapchat selfie lenses. arXiv<br>preprint arXiv:2001.09528, 2020. 9<br>[39] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehti-<br>nen. Progressive growing of gans for improved quality, sta-<br>bility, and variation. CoRR, abs/1710.10196, 2017. 5, 6<br>[40] Tero Karras, Samuli Laine, and Timo Aila. A style-based<br>generator architecture for generative adversarial networks.<br>In IEEE Conf. Comput. Vis. Pattern Recog., pages 4401-<br>4410, 2019. 1<br>[41] T. Karras, S. Laine, and T. Aila. A style-based gener-<br>ator architecture for generative adversarial networks. In<br>2019 IEEE/CVF Conference on Computer Vision and Pat-<br>tern Recognition (CVPR), 2019. 5, 6<br>[42] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,<br>Jaakko Lehtinen, and Timo Aila. Analyzing and improv-<br>ing the image quality of stylegan. CoRR, abs/1912.04958,<br>2019. 2, 6, 28<br>[43] Dongjun Kim, Seungjae Shin, Kyungwoo Song, Wanmo<br>Kang, and Il-Chul Moon. Score matching model for un-<br>bounded data score. CoRR, abs/2106.05527, 2021. 6<br>[44] Durk P Kingma and Prafulla Dhariwal. Glow: Generative<br>flow with invertible 1x1 convolutions. In S. Bengio, H. Wal-<br>lach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R.<br>Garnett, editors, Advances in Neural Information Process-<br>ing Systems, 2018. 3</p>",
            "id": 146,
            "page": 11,
            "text": " Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021. 6, 7, 16, 22, 28, 37, 38  Phillip Isola, Jun- Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with conditional adversarial networks. In CVPR, pages 5967-5976. IEEE Computer Society, 2017. 3, 4  Phillip Isola, Jun- Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with conditional adversarial networks. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5967-5976, 2017. 4  Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, Olivier J. Henaff, Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, and Joao Carreira. Perceiver IO: A general architecture for structured inputs &outputs. CoRR, abs/2107.14795, 2021. 4  Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira. Perceiver: General perception with iterative attention. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings ofMachine Learning Research, pages 4651-4664. PMLR, 2021. 4, 5  Manuel Jahn, Robin Rombach, and Bjorn Ommer. Highresolution complex scene synthesis with transformers. CoRR, abs/2105.06458, 2021. 20, 22, 27  Niharika Jain, Alberto Olmo, Sailik Sengupta, Lydia Manikonda, and Subbarao Kambhampati. Imperfect imaganation: Implications of gans exacerbating biases on facial data augmentation and snapchat selfie lenses. arXiv preprint arXiv:2001.09528, 2020. 9  Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. CoRR, abs/1710.10196, 2017. 5, 6  Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In IEEE Conf. Comput. Vis. Pattern Recog., pages 44014410, 2019. 1  T. Karras, S. Laine, and T. Aila. A style-based generator architecture for generative adversarial networks. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 5, 6  Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. CoRR, abs/1912.04958, 2019. 2, 6, 28  Dongjun Kim, Seungjae Shin, Kyungwoo Song, Wanmo Kang, and Il-Chul Moon. Score matching model for unbounded data score. CoRR, abs/2106.05527, 2021. 6  Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, 2018. 3"
        },
        {
            "bounding_box": [
                {
                    "x": 1299,
                    "y": 304
                },
                {
                    "x": 2291,
                    "y": 304
                },
                {
                    "x": 2291,
                    "y": 2970
                },
                {
                    "x": 1299,
                    "y": 2970
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='147' style='font-size:14px'>[45] Diederik P. Kingma, Tim Salimans, Ben Poole, and<br>Jonathan Ho. Variational diffusion models. CoRR,<br>abs/2107.00630, 2021. 1, 3, 16<br>[46] Diederik P. Kingma and Max Welling. Auto-Encoding Vari-<br>ational Bayes. In 2nd International Conference on Learn-<br>ing Representations, ICLR, 2014. 1, 3, 4, 29<br>[47] Zhifeng Kong and Wei Ping. On fast sampling of diffusion<br>probabilistic models. CoRR, abs/2106.00132, 2021. 3<br>[48] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and<br>Bryan Catanzaro. Diffwave: A versatile diffusion model<br>for audio synthesis. In ICLR. OpenReview.net, 2021. 1<br>[49] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper R. R.<br>Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali,<br>Stefan Popov, Matteo Malloci, Tom Duerig, and Vittorio<br>Ferrari. The open images dataset V4: unified image classi-<br>fication, object detection, and visual relationship detection<br>at scale. CoRR, abs/1811.00982, 2018. 7, 20, 22<br>[50] Tuomas Kynk��nniemi, Tero Karras, Samuli Laine, Jaakko<br>Lehtinen, and Timo Aila. Improved precision and re-<br>call metric for assessing generative models. CoRR,<br>abs/1904.06991, 2019. 5, 26<br>[51] Tsung-Yi Lin, Michael Maire, Serge J. Belongie,<br>Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro<br>Perona, Deva Ramanan, Piotr Dollar, and C. Lawrence Zit-<br>nick. Microsoft COCO: common objects in context. CoRR,<br>abs/1405.0312, 2014. 6, 7, 27<br>[52] Yuqing Ma, Xianglong Liu, Shihao Bai, Le-Yi Wang, Ais-<br>han Liu, Dacheng Tao, and Edwin Hancock. Region-wise<br>generative adversarial imageinpainting for large missing ar-<br>eas. ArXiv, abs/1909. 12507, 2019. 9<br>[53] Chenlin Meng, Yang Song, Jiaming Song, Jiajun Wu, Jun-<br>Yan Zhu, and Stefano Ermon. Sdedit: Image synthesis<br>and editing with stochastic differential equations. CoRR,<br>abs/2108.01073, 2021. 1<br>[54] Lars M. Mescheder. On the convergence properties of GAN<br>training. CoRR, abs/1801.04406, 2018. 3<br>[55] Luke Metz, Ben Poole, David Pfau, and Jascha Sohl-<br>Dickstein. Unrolled generative adversarial networks. In<br>5th International Conference on Learning Representations,<br>ICLR 2017, Toulon, France, April 24-26, 2017, Conference<br>Track Proceedings. OpenReview.net, 2017. 3<br>[56] Mehdi Mirza and Simon Osindero. Conditional generative<br>adversarial nets. CoRR, abs/1411.1784, 2014. 4<br>[57] Gautam Mittal, Jesse H. Engel, Curtis Hawthorne, and Ian<br>Simon. Symbolic music generation with diffusion models.<br>CoRR, abs/2103.16091, 2021. 1<br>[58] Kamyar Nazeri, Eric Ng, Tony Joseph, Faisal Z. Qureshi,<br>and Mehran Ebrahimi. Edgeconnect: Generative im-<br>age inpainting with adversarial edge learning. ArXiv,<br>abs/1901.00212, 2019. 9<br>[59] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav<br>Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and<br>Mark Chen. GLIDE: towards photorealistic image genera-<br>tion and editing with text-guided diffusion models. CoRR,<br>abs/2112.10741, 2021. 6, 7, 16<br>[60] Anton Obukhov, Maximilian Seitzer, Po-Wei Wu, Se-<br>men Zhydenko, Jonathan Kyl, and Elvis Yu-Jing Lin.</p>",
            "id": 147,
            "page": 11,
            "text": " Diederik P. Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. CoRR, abs/2107.00630, 2021. 1, 3, 16  Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In 2nd International Conference on Learning Representations, ICLR, 2014. 1, 3, 4, 29  Zhifeng Kong and Wei Ping. On fast sampling of diffusion probabilistic models. CoRR, abs/2106.00132, 2021. 3  Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion model for audio synthesis. In ICLR. OpenReview.net, 2021. 1  Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper R. R. Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Tom Duerig, and Vittorio Ferrari. The open images dataset V4: unified image classification, object detection, and visual relationship detection at scale. CoRR, abs/1811.00982, 2018. 7, 20, 22  Tuomas Kynk��nniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. CoRR, abs/1904.06991, 2019. 5, 26  Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D. Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. CoRR, abs/1405.0312, 2014. 6, 7, 27  Yuqing Ma, Xianglong Liu, Shihao Bai, Le-Yi Wang, Aishan Liu, Dacheng Tao, and Edwin Hancock. Region-wise generative adversarial imageinpainting for large missing areas. ArXiv, abs/1909. 12507, 2019. 9  Chenlin Meng, Yang Song, Jiaming Song, Jiajun Wu, JunYan Zhu, and Stefano Ermon. Sdedit: Image synthesis and editing with stochastic differential equations. CoRR, abs/2108.01073, 2021. 1  Lars M. Mescheder. On the convergence properties of GAN training. CoRR, abs/1801.04406, 2018. 3  Luke Metz, Ben Poole, David Pfau, and Jascha SohlDickstein. Unrolled generative adversarial networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. 3  Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. CoRR, abs/1411.1784, 2014. 4  Gautam Mittal, Jesse H. Engel, Curtis Hawthorne, and Ian Simon. Symbolic music generation with diffusion models. CoRR, abs/2103.16091, 2021. 1  Kamyar Nazeri, Eric Ng, Tony Joseph, Faisal Z. Qureshi, and Mehran Ebrahimi. Edgeconnect: Generative image inpainting with adversarial edge learning. ArXiv, abs/1901.00212, 2019. 9  Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. GLIDE: towards photorealistic image generation and editing with text-guided diffusion models. CoRR, abs/2112.10741, 2021. 6, 7, 16  Anton Obukhov, Maximilian Seitzer, Po-Wei Wu, Semen Zhydenko, Jonathan Kyl, and Elvis Yu-Jing Lin."
        },
        {
            "bounding_box": [
                {
                    "x": 1219,
                    "y": 3054
                },
                {
                    "x": 1260,
                    "y": 3054
                },
                {
                    "x": 1260,
                    "y": 3093
                },
                {
                    "x": 1219,
                    "y": 3093
                }
            ],
            "category": "footer",
            "html": "<footer id='148' style='font-size:18px'>11</footer>",
            "id": 148,
            "page": 11,
            "text": "11"
        },
        {
            "bounding_box": [
                {
                    "x": 303,
                    "y": 311
                },
                {
                    "x": 1199,
                    "y": 311
                },
                {
                    "x": 1199,
                    "y": 441
                },
                {
                    "x": 303,
                    "y": 441
                }
            ],
            "category": "paragraph",
            "html": "<p id='149' style='font-size:16px'>High-fidelity performance metrics for generative models<br>in pytorch, 2020. Version: 0.3.0, DOI: 10.5281/zen-<br>odo.4957738. 26, 27</p>",
            "id": 149,
            "page": 12,
            "text": "High-fidelity performance metrics for generative models in pytorch, 2020. Version: 0.3.0, DOI: 10.5281/zenodo.4957738. 26, 27"
        },
        {
            "bounding_box": [
                {
                    "x": 1384,
                    "y": 312
                },
                {
                    "x": 2271,
                    "y": 312
                },
                {
                    "x": 2271,
                    "y": 393
                },
                {
                    "x": 1384,
                    "y": 393
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='150' style='font-size:14px'>immediate - speedups - wi th - a1 00 -tf32, 2020.<br>28</p>",
            "id": 150,
            "page": 12,
            "text": "immediate - speedups - wi th - a1 00 -tf32, 2020. 28"
        },
        {
            "bounding_box": [
                {
                    "x": 227,
                    "y": 431
                },
                {
                    "x": 1201,
                    "y": 431
                },
                {
                    "x": 1201,
                    "y": 2975
                },
                {
                    "x": 227,
                    "y": 2975
                }
            ],
            "category": "paragraph",
            "html": "<p id='151' style='font-size:16px'>[61] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-<br>Yan Zhu. Semantic image synthesis with spatially-adaptive<br>normalization. In Proceedings of the IEEE Conference on<br>Computer Vision and Pattern Recognition, 2019. 4, 7<br>[62] Taesung Park, Ming- Yu Liu, Ting-Chun Wang, and Jun-<br>Yan Zhu. Semantic image synthesis with spatially-adaptive<br>normalization. In Proceedings of the IEEE/CVF Confer-<br>ence on Computer Vision and Pattern Recognition (CVPR),<br>June 2019. 22<br>[63] Gaurav Parmar, Dacheng Li, Kwonjoon Lee, and Zhuowen<br>Tu. Dual contradistinctive generative autoencoder. In IEEE<br>Conference on Computer Vision and Pattern Recognition,<br>CVPR 2021, virtual, June 19-25, 2021, pages 823-832.<br>Computer Vision Foundation / IEEE, 2021. 6<br>[64] Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu. On<br>buggy resizing libraries and surprising subtleties in fid cal-<br>culation. arXiv preprint arXiv:2104.11222, 2021. 26<br>[65] David A. Patterson, Joseph Gonzalez, Quoc V. Le, Chen<br>Liang, Lluis-Miquel Munguia, Daniel Rothchild, David R.<br>So, Maud Texier, and Jeff Dean. Carbon emissions and<br>large neural network training. CoRR, abs/2104.10350,<br>2021. 2<br>[66] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott<br>Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya<br>Sutskever. Zero-shot text-to-image generation. CoRR,<br>abs/2102.12092, 2021. 1, 2, 3, 4, 7, 21, 27<br>[67] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Gen-<br>erating diverse high-fidelity images with VQ-VAE-2. In<br>NeurIPS, pages 14837-14847, 2019. 1, 2, 3, 22<br>[68] Scott E. Reed, Zeynep Akata, Xinchen Yan, Lajanugen Lo-<br>geswaran, Bernt Schiele, and Honglak Lee. Generative ad-<br>versarial text to image synthesis. In ICML, 2016. 4<br>[69] Danilo Jimenez Rezende, Shakir Mohamed, and Daan<br>Wierstra. Stochastic backpropagation and approximate in-<br>ference in deep generative models. In Proceedings of the<br>31st International Conference on International Conference<br>on Machine Learning, ICML, 2014. 1, 4, 29<br>[70] Robin Rombach, Patrick Esser, and Bjorn Ommer.<br>Network-to-network translation with conditional invertible<br>neural networks. In NeurIPS, 2020. 3<br>[71] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-<br>net: Convolutional networks for biomedical image segmen-<br>tation. In MICCAI (3), volume 9351 of Lecture Notes in<br>Computer Science, pages 234-241. Springer, 2015. 2, 3, 4<br>[72] Chitwan Saharia, Jonathan Ho, William Chan, Tim Sal-<br>imans, David J. Fleet, and Mohammad Norouzi. Im-<br>age super-resolution via iterative refinement. CoRR,<br>abs/2104.07636, 2021. 1, 4, 8, 16, 22, 23, 27<br>[73] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P.<br>Kingma. Pixelcnn++: Improving the pixelcnn with dis-<br>cretized logistic mixture likelihood and other modifications.<br>CoRR, abs/1701.05517, 2017. 1, 3<br>[74] Dave Salvator. NVIDIA Developer Blog. https :<br>/ / developer · nvidia · com / blog / getting -</p>",
            "id": 151,
            "page": 12,
            "text": " Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and JunYan Zhu. Semantic image synthesis with spatially-adaptive normalization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019. 4, 7  Taesung Park, Ming- Yu Liu, Ting-Chun Wang, and JunYan Zhu. Semantic image synthesis with spatially-adaptive normalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019. 22  Gaurav Parmar, Dacheng Li, Kwonjoon Lee, and Zhuowen Tu. Dual contradistinctive generative autoencoder. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pages 823-832. Computer Vision Foundation / IEEE, 2021. 6  Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu. On buggy resizing libraries and surprising subtleties in fid calculation. arXiv preprint arXiv:2104.11222, 2021. 26  David A. Patterson, Joseph Gonzalez, Quoc V. Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David R. So, Maud Texier, and Jeff Dean. Carbon emissions and large neural network training. CoRR, abs/2104.10350, 2021. 2  Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. CoRR, abs/2102.12092, 2021. 1, 2, 3, 4, 7, 21, 27  Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with VQ-VAE-2. In NeurIPS, pages 14837-14847, 2019. 1, 2, 3, 22  Scott E. Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. Generative adversarial text to image synthesis. In ICML, 2016. 4  Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In Proceedings of the 31st International Conference on International Conference on Machine Learning, ICML, 2014. 1, 4, 29  Robin Rombach, Patrick Esser, and Bjorn Ommer. Network-to-network translation with conditional invertible neural networks. In NeurIPS, 2020. 3  Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In MICCAI (3), volume 9351 of Lecture Notes in Computer Science, pages 234-241. Springer, 2015. 2, 3, 4  Chitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J. Fleet, and Mohammad Norouzi. Image super-resolution via iterative refinement. CoRR, abs/2104.07636, 2021. 1, 4, 8, 16, 22, 23, 27  Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P. Kingma. Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications. CoRR, abs/1701.05517, 2017. 1, 3  Dave Salvator. NVIDIA Developer Blog. https : / / developer · nvidia · com / blog / getting -"
        },
        {
            "bounding_box": [
                {
                    "x": 1296,
                    "y": 396
                },
                {
                    "x": 2289,
                    "y": 396
                },
                {
                    "x": 2289,
                    "y": 2970
                },
                {
                    "x": 1296,
                    "y": 2970
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='152' style='font-size:16px'>[75] Robin San-Roman, Eliya Nachmani, and Lior Wolf.<br>Noise estimation for generative diffusion models. CoRR,<br>abs/2104.02600, 2021. 3<br>[76] Axel Sauer, Kashyap Chitta, Jens M�ller, and An-<br>dreas Geiger. Projected gans converge faster. CoRR,<br>abs/2111.01007, 2021. 6<br>[77] Edgar Schonfeld, Bernt Schiele, and Anna Khoreva. A u-<br>net based discriminator for generative adversarial networks.<br>In 2020 IEEE/CVF Conference on Computer Vision and<br>Pattern Recognition, CVPR 2020, Seattle, WA, USA, June<br>13-19, 2020, pages 8204-8213. Computer Vision Founda-<br>tion / IEEE, 2020. 6<br>[78] Christoph Schuhmann, Richard Vencu, Romain Beaumont,<br>Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo<br>Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-<br>400m: Open dataset of clip-filtered 400 million image-text<br>pairs, 2021. 6, 7<br>[79] Karen Simonyan and Andrew Zisserman. Very deep con-<br>volutional networks for large-scale image recognition. In<br>Yoshua Bengio and Yann LeCun, editors, Int. Conf. Learn.<br>Represent., 2015. 29, 43, 44, 45<br>[80] Abhishek Sinha, Jiaming Song, Chenlin Meng, and Stefano<br>Ermon. D2C: diffusion-denoising models for few-shot con-<br>ditional generation. CoRR, abs/2106.06819, 2021. 3<br>[81] Charlie Snell. Alien Dreams: An Emerging Art Scene.<br>https : / / ml · berkeley · edu / blog / posts /<br>clip-art/, 2021. [Online; accessed November-2021].<br>2<br>[82] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Mah-<br>eswaranathan, and Surya Ganguli. Deep unsupervised<br>learning using nonequilibrium thermodynamics. CoRR,<br>abs/1503.03585, 2015. 1, 3, 4, 18<br>[83] Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learn-<br>ing structured output representation using deep conditional<br>generative models. In C. Cortes, N. Lawrence, D. Lee,<br>M. Sugiyama, and R. Garnett, editors, Advances in Neural<br>Information Processing Systems, volume 28. Curran Asso-<br>ciates, Inc., 2015. 4<br>[84] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-<br>ing diffusion implicit models. In ICLR. OpenReview.net,<br>2021. 3, 5, 6, 22<br>[85] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma,<br>Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-<br>based generative modeling through stochastic differential<br>equations. CoRR, abs/2011.13456, 2020. 1, 3, 4, 18<br>[86] Emma Strubell, Ananya Ganesh, and Andrew McCallum.<br>Energy and policy considerations for modern deep learn-<br>ing research. In The Thirty-Fourth AAAI Conference on<br>Artificial Intelligence, AAAI 2020, The Thirty-Second In-<br>novative Applications of Artificial Intelligence Conference,<br>IAAI 2020, The Tenth AAAI Symposium on Educational<br>Advances in Artificial Intelligence, EAAI 2020, New York,<br>NY, USA, February 7-12, 2020, pages 13693-13696. AAAI<br>Press, 2020. 2</p>",
            "id": 152,
            "page": 12,
            "text": " Robin San-Roman, Eliya Nachmani, and Lior Wolf. Noise estimation for generative diffusion models. CoRR, abs/2104.02600, 2021. 3  Axel Sauer, Kashyap Chitta, Jens M�ller, and Andreas Geiger. Projected gans converge faster. CoRR, abs/2111.01007, 2021. 6  Edgar Schonfeld, Bernt Schiele, and Anna Khoreva. A unet based discriminator for generative adversarial networks. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 8204-8213. Computer Vision Foundation / IEEE, 2020. 6  Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion400m: Open dataset of clip-filtered 400 million image-text pairs, 2021. 6, 7  Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In Yoshua Bengio and Yann LeCun, editors, Int. Conf. Learn. Represent., 2015. 29, 43, 44, 45  Abhishek Sinha, Jiaming Song, Chenlin Meng, and Stefano Ermon. D2C: diffusion-denoising models for few-shot conditional generation. CoRR, abs/2106.06819, 2021. 3  Charlie Snell. Alien Dreams: An Emerging Art Scene. https : / / ml · berkeley · edu / blog / posts / clip-art/, 2021. [Online; accessed November-2021]. 2  Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. CoRR, abs/1503.03585, 2015. 1, 3, 4, 18  Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep conditional generative models. In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc., 2015. 4  Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR. OpenReview.net, 2021. 3, 5, 6, 22  Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Scorebased generative modeling through stochastic differential equations. CoRR, abs/2011.13456, 2020. 1, 3, 4, 18  Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for modern deep learning research. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 13693-13696. AAAI Press, 2020. 2"
        },
        {
            "bounding_box": [
                {
                    "x": 1218,
                    "y": 3052
                },
                {
                    "x": 1263,
                    "y": 3052
                },
                {
                    "x": 1263,
                    "y": 3093
                },
                {
                    "x": 1218,
                    "y": 3093
                }
            ],
            "category": "footer",
            "html": "<footer id='153' style='font-size:20px'>12</footer>",
            "id": 153,
            "page": 12,
            "text": "12"
        },
        {
            "bounding_box": [
                {
                    "x": 225,
                    "y": 287
                },
                {
                    "x": 1203,
                    "y": 287
                },
                {
                    "x": 1203,
                    "y": 2969
                },
                {
                    "x": 225,
                    "y": 2969
                }
            ],
            "category": "paragraph",
            "html": "<p id='154' style='font-size:14px'>[87] Wei Sun and Tianfu Wu. Learning layout and style re-<br>configurable gans for controllable image synthesis. CoRR,<br>abs/2003.11571, 2020. 22, 27<br>[88] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin,<br>Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov,<br>Naejin Kong, Harshith Goka, Kiwoong Park, and Victor S.<br>Lempitsky. Resolution-robust large mask inpainting with<br>fourier convolutions. ArXiv, abs/2109.07161, 2021. 8, 9,<br>26, 32<br>[89] Tristan Sylvain, Pengchuan Zhang, Yoshua Bengio, R. De-<br>von Hjelm, and Shikhar Sharma. Object-centric image gen-<br>eration from layouts. In Thirty-Fifth AAAI Conference on<br>Artificial Intelligence, AAAI 2021, Thirty-Third Conference<br>on Innovative Applications of Artificial Intelligence, IAAI<br>2021, The Eleventh Symposium on Educational Advances<br>in Artificial Intelligence, EAAI 2021, Virtual Event, Febru-<br>ary 2-9, 2021, pages 2647-2655. AAAI Press, 2021. 20,<br>22, 27<br>[90] Patrick Tinsley, Adam Czajka, and Patrick Flynn. This face<br>does not exist... but it might be yours! identity leakage in<br>generative models. In Proceedings of the IEEE/CVF Win-<br>ter Conference on Applications of Computer Vision, pages<br>1320-1328, 2021. 9<br>[91] Antonio Torralba and Alexei A Efros. Unbiased look at<br>dataset bias. In CVPR 2011, pages 1521-1528. IEEE, 2011.<br>9<br>[92] Arash Vahdat and Jan Kautz. NVAE: A deep hierarchical<br>variational autoencoder. In NeurIPS, 2020. 3<br>[93] Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-<br>based generative modeling in latent space. CoRR,<br>abs/2106.05931, 2021. 2, 3, 5, 6<br>[94] Aaron van den Oord, Nal Kalchbrenner, Lasse Espeholt,<br>koray kavukcuoglu, Oriol Vinyals, and Alex Graves. Con-<br>ditional image generation with pixelcnn decoders. In Ad-<br>vances in Neural Information Processing Systems, 2016. 3<br>[95] A�ron van den Oord, Nal Kalchbrenner, and Koray<br>Kavukcuoglu. Pixel recurrent neural networks. CoRR,<br>abs/1601.06759, 2016. 3<br>[96] A�ron van den Oord, Oriol Vinyals, and Koray<br>Kavukcuoglu. Neural discrete representation learning. In<br>NIPS, pages 6306-6315, 2017. 2, 4, 29<br>[97] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob<br>Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser,<br>and Illia Polosukhin. Attention is all you need. In NIPS,<br>pages 5998-6008, 2017. 3, 4, 5, 7<br>[98] Rivers Have Wings. Tweet on Classifier-free<br>guidance for autoregressive models. https :<br>/ / twitter · com / RiversHaveWings / status /<br>147809365871 6966912, 2022. 6<br>[99] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chau-<br>mond, Clement Delangue, Anthony Moi, Pierric Cistac,<br>Tim Rault, Remi Louf, Morgan Funtowicz, and Jamie<br>Brew. Huggingface's transformers: State-of-the-art natural<br>language processing. CoRR, abs/1910.03771, 2019. 26<br>[100] Zhisheng Xiao, Karsten Kreis, Jan Kautz, and Arash Vah-<br>dat. VAEBM: A symbiosis between variational autoen-<br>coders and energy-based models. In 9th International Con-</p>",
            "id": 154,
            "page": 13,
            "text": " Wei Sun and Tianfu Wu. Learning layout and style reconfigurable gans for controllable image synthesis. CoRR, abs/2003.11571, 2020. 22, 27  Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor S. Lempitsky. Resolution-robust large mask inpainting with fourier convolutions. ArXiv, abs/2109.07161, 2021. 8, 9, 26, 32  Tristan Sylvain, Pengchuan Zhang, Yoshua Bengio, R. Devon Hjelm, and Shikhar Sharma. Object-centric image generation from layouts. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, pages 2647-2655. AAAI Press, 2021. 20, 22, 27  Patrick Tinsley, Adam Czajka, and Patrick Flynn. This face does not exist... but it might be yours! identity leakage in generative models. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 1320-1328, 2021. 9  Antonio Torralba and Alexei A Efros. Unbiased look at dataset bias. In CVPR 2011, pages 1521-1528. IEEE, 2011. 9  Arash Vahdat and Jan Kautz. NVAE: A deep hierarchical variational autoencoder. In NeurIPS, 2020. 3  Arash Vahdat, Karsten Kreis, and Jan Kautz. Scorebased generative modeling in latent space. CoRR, abs/2106.05931, 2021. 2, 3, 5, 6  Aaron van den Oord, Nal Kalchbrenner, Lasse Espeholt, koray kavukcuoglu, Oriol Vinyals, and Alex Graves. Conditional image generation with pixelcnn decoders. In Advances in Neural Information Processing Systems, 2016. 3  A�ron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. CoRR, abs/1601.06759, 2016. 3  A�ron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. In NIPS, pages 6306-6315, 2017. 2, 4, 29  Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, pages 5998-6008, 2017. 3, 4, 5, 7  Rivers Have Wings. Tweet on Classifier-free guidance for autoregressive models. https : / / twitter · com / RiversHaveWings / status / 147809365871 6966912, 2022. 6  Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, and Jamie Brew. Huggingface's transformers: State-of-the-art natural language processing. CoRR, abs/1910.03771, 2019. 26  Zhisheng Xiao, Karsten Kreis, Jan Kautz, and Arash Vahdat. VAEBM: A symbiosis between variational autoencoders and energy-based models. In 9th International Con-"
        },
        {
            "bounding_box": [
                {
                    "x": 1382,
                    "y": 311
                },
                {
                    "x": 2277,
                    "y": 311
                },
                {
                    "x": 2277,
                    "y": 395
                },
                {
                    "x": 1382,
                    "y": 395
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='155' style='font-size:18px'>ference on Learning Representations, ICLR 2021, Virtual<br>Event, Austria, May 3-7, 2021. OpenReview.net, 2021. 6</p>",
            "id": 155,
            "page": 13,
            "text": "ference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. 6"
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 408
                },
                {
                    "x": 2280,
                    "y": 408
                },
                {
                    "x": 2280,
                    "y": 2173
                },
                {
                    "x": 1281,
                    "y": 2173
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='156' style='font-size:14px'>[101] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind<br>Srinivas. Videogpt: Video generation using VQ-VAE and<br>transformers. CoRR, abs/2104.10157, 2021. 3<br>[102] Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianx-<br>iong Xiao. LSUN: construction of a large-scale image<br>dataset using deep learning with humans in the loop. CoRR,<br>abs/1506.03365, 2015. 5<br>[103] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang,<br>James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge,<br>and Yonghui Wu. Vector-quantized image modeling with<br>improved vqgan, 2021. 3, 4<br>[104] Jiahui Yu, Zhe L. Lin, Jimei Yang, Xiaohui Shen, Xin Lu,<br>and Thomas S. Huang. Free-form image inpainting with<br>gated convolution. 2019 IEEE/CVF International Confer-<br>ence on Computer Vision (ICCV), pages 4470-4479, 2019.<br>9<br>[105] K. Zhang, Jingyun Liang, Luc Van Gool, and Radu Timo-<br>fte. Designing a practical degradation model for deep blind<br>image super-resolution. ArXiv, abs/2103.14006, 2021. 23<br>[106] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shecht-<br>man, and Oliver Wang. The unreasonable effectiveness of<br>deep features as a perceptual metric. In Proceedings of the<br>IEEE Conference on Computer Vision and Pattern Recog-<br>nition (CVPR), June 2018. 3, 8, 19<br>[107] Shengyu Zhao, Jianwei Cui, Yilun Sheng, Yue Dong, Xiao<br>Liang, Eric I-Chao Chang, and Yan Xu. Large scale image<br>completion via co-modulated generative adversarial net-<br>works. ArXiv, abs/2103.10428, 2021. 9<br>[108] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva,<br>and Antonio Torralba. Places: A 10 million image database<br>for scene recognition. IEEE Transactions on Pattern Anal-<br>ysis and Machine Intelligence, 40:1452-1464, 2018. 8, 9,<br>26<br>[109] Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li,<br>Chris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, and<br>Tong Sun. LAFITE: towards language-free training for<br>text-to-image generation. CoRR, abs/2111.13792, 2021. 6,<br>7, 16</p>",
            "id": 156,
            "page": 13,
            "text": " Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using VQ-VAE and transformers. CoRR, abs/2104.10157, 2021. 3  Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. LSUN: construction of a large-scale image dataset using deep learning with humans in the loop. CoRR, abs/1506.03365, 2015. 5  Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan, 2021. 3, 4  Jiahui Yu, Zhe L. Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S. Huang. Free-form image inpainting with gated convolution. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 4470-4479, 2019. 9  K. Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte. Designing a practical degradation model for deep blind image super-resolution. ArXiv, abs/2103.14006, 2021. 23  Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018. 3, 8, 19  Shengyu Zhao, Jianwei Cui, Yilun Sheng, Yue Dong, Xiao Liang, Eric I-Chao Chang, and Yan Xu. Large scale image completion via co-modulated generative adversarial networks. ArXiv, abs/2103.10428, 2021. 9  Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40:1452-1464, 2018. 8, 9, 26  Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li, Chris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, and Tong Sun. LAFITE: towards language-free training for text-to-image generation. CoRR, abs/2111.13792, 2021. 6, 7, 16"
        },
        {
            "bounding_box": [
                {
                    "x": 1218,
                    "y": 3051
                },
                {
                    "x": 1263,
                    "y": 3051
                },
                {
                    "x": 1263,
                    "y": 3094
                },
                {
                    "x": 1218,
                    "y": 3094
                }
            ],
            "category": "footer",
            "html": "<footer id='157' style='font-size:18px'>13</footer>",
            "id": 157,
            "page": 13,
            "text": "13"
        },
        {
            "bounding_box": [
                {
                    "x": 1084,
                    "y": 293
                },
                {
                    "x": 1396,
                    "y": 293
                },
                {
                    "x": 1396,
                    "y": 371
                },
                {
                    "x": 1084,
                    "y": 371
                }
            ],
            "category": "paragraph",
            "html": "<p id='158' style='font-size:20px'>Appendix</p>",
            "id": 158,
            "page": 14,
            "text": "Appendix"
        },
        {
            "bounding_box": [
                {
                    "x": 368,
                    "y": 439
                },
                {
                    "x": 2108,
                    "y": 439
                },
                {
                    "x": 2108,
                    "y": 2899
                },
                {
                    "x": 368,
                    "y": 2899
                }
            ],
            "category": "figure",
            "html": "<figure><img id='159' alt=\"\" data-coord=\"top-left:(368,439); bottom-right:(2108,2899)\" /></figure>",
            "id": 159,
            "page": 14,
            "text": ""
        },
        {
            "bounding_box": [
                {
                    "x": 373,
                    "y": 2921
                },
                {
                    "x": 2102,
                    "y": 2921
                },
                {
                    "x": 2102,
                    "y": 2975
                },
                {
                    "x": 373,
                    "y": 2975
                }
            ],
            "category": "caption",
            "html": "<br><caption id='160' style='font-size:14px'>Figure 12. Convolutional samples from the semantic landscapes model as in Sec. 4.3.2, finetuned on 5122 images.</caption>",
            "id": 160,
            "page": 14,
            "text": "Figure 12. Convolutional samples from the semantic landscapes model as in Sec. 4.3.2, finetuned on 5122 images."
        },
        {
            "bounding_box": [
                {
                    "x": 1219,
                    "y": 3053
                },
                {
                    "x": 1262,
                    "y": 3053
                },
                {
                    "x": 1262,
                    "y": 3093
                },
                {
                    "x": 1219,
                    "y": 3093
                }
            ],
            "category": "footer",
            "html": "<footer id='161' style='font-size:16px'>14</footer>",
            "id": 161,
            "page": 14,
            "text": "14"
        },
        {
            "bounding_box": [
                {
                    "x": 955,
                    "y": 345
                },
                {
                    "x": 1466,
                    "y": 345
                },
                {
                    "x": 1466,
                    "y": 382
                },
                {
                    "x": 955,
                    "y": 382
                }
            ],
            "category": "caption",
            "html": "<caption id='162' style='font-size:16px'>'A painting of the last supper by Picasso.'</caption>",
            "id": 162,
            "page": 15,
            "text": "'A painting of the last supper by Picasso.'"
        },
        {
            "bounding_box": [
                {
                    "x": 233,
                    "y": 358
                },
                {
                    "x": 2185,
                    "y": 358
                },
                {
                    "x": 2185,
                    "y": 2805
                },
                {
                    "x": 233,
                    "y": 2805
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='163' style='font-size:14px' alt=\"VO\n'An epic painting of Gandalf the Black\n'An oil painting of a latent space.' summoning thunder and lightning in the mountains.\n'A sunset over a mountain range, vector image.'\" data-coord=\"top-left:(233,358); bottom-right:(2185,2805)\" /></figure>",
            "id": 163,
            "page": 15,
            "text": "VO 'An epic painting of Gandalf the Black 'An oil painting of a latent space.' summoning thunder and lightning in the mountains. 'A sunset over a mountain range, vector image.'"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2841
                },
                {
                    "x": 2274,
                    "y": 2841
                },
                {
                    "x": 2274,
                    "y": 2935
                },
                {
                    "x": 201,
                    "y": 2935
                }
            ],
            "category": "caption",
            "html": "<caption id='164' style='font-size:18px'>Figure 13. Combining classifier free diffusion guidance with the convolutional sampling strategy from Sec. 4.3.2, our 1.45B parameter<br>text-to-image model can be used for rendering images larger than the native 2562 resolution the model was trained on.</caption>",
            "id": 164,
            "page": 15,
            "text": "Figure 13. Combining classifier free diffusion guidance with the convolutional sampling strategy from Sec. 4.3.2, our 1.45B parameter text-to-image model can be used for rendering images larger than the native 2562 resolution the model was trained on."
        },
        {
            "bounding_box": [
                {
                    "x": 1217,
                    "y": 3051
                },
                {
                    "x": 1264,
                    "y": 3051
                },
                {
                    "x": 1264,
                    "y": 3095
                },
                {
                    "x": 1217,
                    "y": 3095
                }
            ],
            "category": "footer",
            "html": "<footer id='165' style='font-size:20px'>15</footer>",
            "id": 165,
            "page": 15,
            "text": "15"
        },
        {
            "bounding_box": [
                {
                    "x": 205,
                    "y": 303
                },
                {
                    "x": 503,
                    "y": 303
                },
                {
                    "x": 503,
                    "y": 356
                },
                {
                    "x": 205,
                    "y": 356
                }
            ],
            "category": "paragraph",
            "html": "<p id='166' style='font-size:20px'>A. Changelog</p>",
            "id": 166,
            "page": 16,
            "text": "A. Changelog"
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 384
                },
                {
                    "x": 2276,
                    "y": 384
                },
                {
                    "x": 2276,
                    "y": 484
                },
                {
                    "x": 200,
                    "y": 484
                }
            ],
            "category": "paragraph",
            "html": "<p id='167' style='font-size:16px'>Here we list changes between this version (https : / / arxiv. org / abs / 2112 · 10752v2) of the paper and the<br>previous version, i.e. https : / / arxiv.org/abs/2112, 10752v1.</p>",
            "id": 167,
            "page": 16,
            "text": "Here we list changes between this version (https : / / arxiv. org / abs / 2112 · 10752v2) of the paper and the previous version, i.e. https : / / arxiv.org/abs/2112, 10752v1."
        },
        {
            "bounding_box": [
                {
                    "x": 247,
                    "y": 515
                },
                {
                    "x": 2279,
                    "y": 515
                },
                {
                    "x": 2279,
                    "y": 1121
                },
                {
                    "x": 247,
                    "y": 1121
                }
            ],
            "category": "paragraph",
            "html": "<p id='168' style='font-size:16px'>· We updated the results on text-to-image synthesis in Sec. 4.3 which were obtained by training a new, larger model (1.45B<br>parameters). This also includes a new comparison to very recent competing methods on this task that were published on<br>arXiv at the same time as ( [59, 109]) or after ( [26]) the publication of our work.<br>· We updated results on class-conditional synthesis on ImageNet in Sec. 4.1, Tab. 3 (see also Sec. D.4) obtained by<br>retraining the model with a larger batch size. The corresponding qualitative results in Fig. 26 and Fig. 27 were also<br>updated. Both the updated text-to-image and the class-conditional model now use classifier-free guidance [32] as a<br>measure to increase visual fidelity.<br>· We conducted a user study (following the scheme suggested by Saharia et al [72]) which provides additional evaluation<br>for our inpainting (Sec. 4.5) and superresolution models (Sec. 4.4).<br>· Added Fig. 5 to the main paper, moved Fig. 18 to the appendix, added Fig. 13 to the appendix.</p>",
            "id": 168,
            "page": 16,
            "text": "· We updated the results on text-to-image synthesis in Sec. 4.3 which were obtained by training a new, larger model (1.45B parameters). This also includes a new comparison to very recent competing methods on this task that were published on arXiv at the same time as ( ) or after ( ) the publication of our work. · We updated results on class-conditional synthesis on ImageNet in Sec. 4.1, Tab. 3 (see also Sec. D.4) obtained by retraining the model with a larger batch size. The corresponding qualitative results in Fig. 26 and Fig. 27 were also updated. Both the updated text-to-image and the class-conditional model now use classifier-free guidance  as a measure to increase visual fidelity. · We conducted a user study (following the scheme suggested by Saharia et al ) which provides additional evaluation for our inpainting (Sec. 4.5) and superresolution models (Sec. 4.4). · Added Fig. 5 to the main paper, moved Fig. 18 to the appendix, added Fig. 13 to the appendix."
        },
        {
            "bounding_box": [
                {
                    "x": 205,
                    "y": 1149
                },
                {
                    "x": 1387,
                    "y": 1149
                },
                {
                    "x": 1387,
                    "y": 1200
                },
                {
                    "x": 205,
                    "y": 1200
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='169' style='font-size:18px'>B. Detailed Information on Denoising Diffusion Models</p>",
            "id": 169,
            "page": 16,
            "text": "B. Detailed Information on Denoising Diffusion Models"
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 1240
                },
                {
                    "x": 2275,
                    "y": 1240
                },
                {
                    "x": 2275,
                    "y": 1356
                },
                {
                    "x": 203,
                    "y": 1356
                }
            ],
            "category": "paragraph",
            "html": "<p id='170' style='font-size:18px'>Diffusion models can be specified in terms of a signal-to-noise ratio SNR(t) = 03호 consisting of sequences (at)t=1 and<br>(ot)t=1 which, starting from a data sample xo, define a forward diffusion process q as</p>",
            "id": 170,
            "page": 16,
            "text": "Diffusion models can be specified in terms of a signal-to-noise ratio SNR(t) = 03호 consisting of sequences (at)t=1 and (ot)t=1 which, starting from a data sample xo, define a forward diffusion process q as"
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 1477
                },
                {
                    "x": 813,
                    "y": 1477
                },
                {
                    "x": 813,
                    "y": 1522
                },
                {
                    "x": 204,
                    "y": 1522
                }
            ],
            "category": "paragraph",
            "html": "<p id='171' style='font-size:14px'>with the Markov structure for s < t:</p>",
            "id": 171,
            "page": 16,
            "text": "with the Markov structure for s < t:"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 1814
                },
                {
                    "x": 2272,
                    "y": 1814
                },
                {
                    "x": 2272,
                    "y": 1911
                },
                {
                    "x": 202,
                    "y": 1911
                }
            ],
            "category": "paragraph",
            "html": "<p id='172' style='font-size:16px'>Denoising diffusion models are generative models p(xo) which revert this process with a similar Markov structure running<br>backward in time, i.e. they are specified as</p>",
            "id": 172,
            "page": 16,
            "text": "Denoising diffusion models are generative models p(xo) which revert this process with a similar Markov structure running backward in time, i.e. they are specified as"
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 2108
                },
                {
                    "x": 2048,
                    "y": 2108
                },
                {
                    "x": 2048,
                    "y": 2156
                },
                {
                    "x": 203,
                    "y": 2156
                }
            ],
            "category": "paragraph",
            "html": "<p id='173' style='font-size:16px'>The evidence lower bound (ELBO) associated with this model then decomposes over the discrete time steps as</p>",
            "id": 173,
            "page": 16,
            "text": "The evidence lower bound (ELBO) associated with this model then decomposes over the discrete time steps as"
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 2353
                },
                {
                    "x": 2277,
                    "y": 2353
                },
                {
                    "x": 2277,
                    "y": 2554
                },
                {
                    "x": 200,
                    "y": 2554
                }
            ],
            "category": "paragraph",
            "html": "<p id='174' style='font-size:16px'>The prior p(xT) is typically choosen as a standard normal distribution and the first term of the ELBO then depends only on<br>the final signal-to-noise ratio SNR(T). To minimize the remaining terms, a common choice to parameterize p(xt-1|xt) is to<br>specify it in terms of the true posterior q(xt-1|xt, xo) but with the unknown xo replaced by an estimate x0(xt,t) based on<br>the current step Xt. This gives [45]</p>",
            "id": 174,
            "page": 16,
            "text": "The prior p(xT) is typically choosen as a standard normal distribution and the first term of the ELBO then depends only on the final signal-to-noise ratio SNR(T). To minimize the remaining terms, a common choice to parameterize p(xt-1|xt) is to specify it in terms of the true posterior q(xt-1|xt, xo) but with the unknown xo replaced by an estimate x0(xt,t) based on the current step Xt. This gives "
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 2793
                },
                {
                    "x": 813,
                    "y": 2793
                },
                {
                    "x": 813,
                    "y": 2837
                },
                {
                    "x": 203,
                    "y": 2837
                }
            ],
            "category": "paragraph",
            "html": "<p id='175' style='font-size:14px'>where the mean can be expressed as</p>",
            "id": 175,
            "page": 16,
            "text": "where the mean can be expressed as"
        },
        {
            "bounding_box": [
                {
                    "x": 1218,
                    "y": 3054
                },
                {
                    "x": 1264,
                    "y": 3054
                },
                {
                    "x": 1264,
                    "y": 3093
                },
                {
                    "x": 1218,
                    "y": 3093
                }
            ],
            "category": "footer",
            "html": "<footer id='176' style='font-size:16px'>16</footer>",
            "id": 176,
            "page": 16,
            "text": "16"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 308
                },
                {
                    "x": 969,
                    "y": 308
                },
                {
                    "x": 969,
                    "y": 353
                },
                {
                    "x": 202,
                    "y": 353
                }
            ],
            "category": "paragraph",
            "html": "<p id='177' style='font-size:14px'>In this case, the sum of the ELBO simplify to</p>",
            "id": 177,
            "page": 17,
            "text": "In this case, the sum of the ELBO simplify to"
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 563
                },
                {
                    "x": 978,
                    "y": 563
                },
                {
                    "x": 978,
                    "y": 612
                },
                {
                    "x": 203,
                    "y": 612
                }
            ],
            "category": "paragraph",
            "html": "<p id='178' style='font-size:14px'>Following [30], we use the reparameterization</p>",
            "id": 178,
            "page": 17,
            "text": "Following , we use the reparameterization"
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 748
                },
                {
                    "x": 1192,
                    "y": 748
                },
                {
                    "x": 1192,
                    "y": 793
                },
                {
                    "x": 200,
                    "y": 793
                }
            ],
            "category": "paragraph",
            "html": "<p id='179' style='font-size:14px'>to express the reconstruction term as a denoising objective,</p>",
            "id": 179,
            "page": 17,
            "text": "to express the reconstruction term as a denoising objective,"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 975
                },
                {
                    "x": 1734,
                    "y": 975
                },
                {
                    "x": 1734,
                    "y": 1024
                },
                {
                    "x": 201,
                    "y": 1024
                }
            ],
            "category": "paragraph",
            "html": "<p id='180' style='font-size:16px'>and the reweighting, which assigns each of the terms the same weight and results in Eq. (1).</p>",
            "id": 180,
            "page": 17,
            "text": "and the reweighting, which assigns each of the terms the same weight and results in Eq. (1)."
        },
        {
            "bounding_box": [
                {
                    "x": 1218,
                    "y": 3053
                },
                {
                    "x": 1265,
                    "y": 3053
                },
                {
                    "x": 1265,
                    "y": 3093
                },
                {
                    "x": 1218,
                    "y": 3093
                }
            ],
            "category": "footer",
            "html": "<footer id='181' style='font-size:16px'>17</footer>",
            "id": 181,
            "page": 17,
            "text": "17"
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 299
                },
                {
                    "x": 872,
                    "y": 299
                },
                {
                    "x": 872,
                    "y": 354
                },
                {
                    "x": 203,
                    "y": 354
                }
            ],
            "category": "paragraph",
            "html": "<p id='182' style='font-size:22px'>C. Image Guiding Mechanisms</p>",
            "id": 182,
            "page": 18,
            "text": "C. Image Guiding Mechanisms"
        },
        {
            "bounding_box": [
                {
                    "x": 385,
                    "y": 439
                },
                {
                    "x": 2080,
                    "y": 439
                },
                {
                    "x": 2080,
                    "y": 2106
                },
                {
                    "x": 385,
                    "y": 2106
                }
            ],
            "category": "figure",
            "html": "<figure><img id='183' style='font-size:20px' alt=\"Samples 2562 Guided Convolutional Samples 5122 Convolutional Samples 5122\" data-coord=\"top-left:(385,439); bottom-right:(2080,2106)\" /></figure>",
            "id": 183,
            "page": 18,
            "text": "Samples 2562 Guided Convolutional Samples 5122 Convolutional Samples 5122"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2113
                },
                {
                    "x": 2276,
                    "y": 2113
                },
                {
                    "x": 2276,
                    "y": 2207
                },
                {
                    "x": 202,
                    "y": 2207
                }
            ],
            "category": "caption",
            "html": "<br><caption id='184' style='font-size:14px'>Figure 14. On landscapes, convolutional sampling with unconditional models can lead to homogeneous and incoherent global structures<br>(see column 2). L2-guiding with a low resolution image can help to reestablish coherent global structures.</caption>",
            "id": 184,
            "page": 18,
            "text": "Figure 14. On landscapes, convolutional sampling with unconditional models can lead to homogeneous and incoherent global structures (see column 2). L2-guiding with a low resolution image can help to reestablish coherent global structures."
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 2270
                },
                {
                    "x": 2280,
                    "y": 2270
                },
                {
                    "x": 2280,
                    "y": 2475
                },
                {
                    "x": 199,
                    "y": 2475
                }
            ],
            "category": "paragraph",
            "html": "<p id='185' style='font-size:18px'>An intriguing feature of diffusion models is that unconditional models can be conditioned at test-time [15, 82, 85]. In<br>particular, [15] presented an algorithm to guide both unconditional and conditional models trained on the ImageNet dataset<br>with a classifier log OF (y|xt), trained on each Xt of the diffusion process. We directly build on this formulation and introduce<br>post-hoc image-guiding:</p>",
            "id": 185,
            "page": 18,
            "text": "An intriguing feature of diffusion models is that unconditional models can be conditioned at test-time . In particular,  presented an algorithm to guide both unconditional and conditional models trained on the ImageNet dataset with a classifier log OF (y|xt), trained on each Xt of the diffusion process. We directly build on this formulation and introduce post-hoc image-guiding:"
        },
        {
            "bounding_box": [
                {
                    "x": 252,
                    "y": 2478
                },
                {
                    "x": 2034,
                    "y": 2478
                },
                {
                    "x": 2034,
                    "y": 2529
                },
                {
                    "x": 252,
                    "y": 2529
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='186' style='font-size:18px'>For an epsilon-parameterized model with fixed variance, the guiding algorithm as introduced in [15] reads:</p>",
            "id": 186,
            "page": 18,
            "text": "For an epsilon-parameterized model with fixed variance, the guiding algorithm as introduced in  reads:"
        },
        {
            "bounding_box": [
                {
                    "x": 249,
                    "y": 2715
                },
                {
                    "x": 2022,
                    "y": 2715
                },
                {
                    "x": 2022,
                    "y": 2772
                },
                {
                    "x": 249,
                    "y": 2772
                }
            ],
            "category": "paragraph",
            "html": "<p id='187' style='font-size:16px'>This can be interpreted as an update correcting the \"score\" EA with a conditional distribution log PF (y|zt).</p>",
            "id": 187,
            "page": 18,
            "text": "This can be interpreted as an update correcting the \"score\" EA with a conditional distribution log PF (y|zt)."
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 2776
                },
                {
                    "x": 2281,
                    "y": 2776
                },
                {
                    "x": 2281,
                    "y": 2976
                },
                {
                    "x": 199,
                    "y": 2976
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='188' style='font-size:18px'>So far, this scenario has only been applied to single-class classification models. We re-interpret the guiding distribution<br>2F (yT(D(zo(zt)))) as a general purpose image-to-image translation task given a target image y, where T can be any<br>differentiable transformation adopted to the image-to-image translation task at hand, such as the identity, a downsampling<br>operation or similar.</p>",
            "id": 188,
            "page": 18,
            "text": "So far, this scenario has only been applied to single-class classification models. We re-interpret the guiding distribution 2F (yT(D(zo(zt)))) as a general purpose image-to-image translation task given a target image y, where T can be any differentiable transformation adopted to the image-to-image translation task at hand, such as the identity, a downsampling operation or similar."
        },
        {
            "bounding_box": [
                {
                    "x": 1217,
                    "y": 3053
                },
                {
                    "x": 1264,
                    "y": 3053
                },
                {
                    "x": 1264,
                    "y": 3093
                },
                {
                    "x": 1217,
                    "y": 3093
                }
            ],
            "category": "footer",
            "html": "<footer id='189' style='font-size:16px'>18</footer>",
            "id": 189,
            "page": 18,
            "text": "18"
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 301
                },
                {
                    "x": 1664,
                    "y": 301
                },
                {
                    "x": 1664,
                    "y": 358
                },
                {
                    "x": 203,
                    "y": 358
                }
            ],
            "category": "paragraph",
            "html": "<p id='190' style='font-size:14px'>As an example, we can assume a Gaussian guider with fixed variance o2 = 1, such that</p>",
            "id": 190,
            "page": 19,
            "text": "As an example, we can assume a Gaussian guider with fixed variance o2 = 1, such that"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 521
                },
                {
                    "x": 794,
                    "y": 521
                },
                {
                    "x": 794,
                    "y": 563
                },
                {
                    "x": 202,
                    "y": 563
                }
            ],
            "category": "paragraph",
            "html": "<p id='191' style='font-size:20px'>becomes a L2 regression objective.</p>",
            "id": 191,
            "page": 19,
            "text": "becomes a L2 regression objective."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 569
                },
                {
                    "x": 2279,
                    "y": 569
                },
                {
                    "x": 2279,
                    "y": 766
                },
                {
                    "x": 200,
                    "y": 766
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='192' style='font-size:18px'>Fig. 14 demonstrates how this formulation can serve as an upsampling mechanism of an unconditional model trained on<br>2562 images, where unconditional samples of size 2562 guide the convolutional synthesis of 5122 images and T is a 2x<br>bicubic downsampling. Following this motivation, we also experiment with a perceptual similarity guiding and replace the<br>L2 objective with the LPIPS [106] metric, see Sec. 4.4.</p>",
            "id": 192,
            "page": 19,
            "text": "Fig. 14 demonstrates how this formulation can serve as an upsampling mechanism of an unconditional model trained on 2562 images, where unconditional samples of size 2562 guide the convolutional synthesis of 5122 images and T is a 2x bicubic downsampling. Following this motivation, we also experiment with a perceptual similarity guiding and replace the L2 objective with the LPIPS  metric, see Sec. 4.4."
        },
        {
            "bounding_box": [
                {
                    "x": 1219,
                    "y": 3054
                },
                {
                    "x": 1263,
                    "y": 3054
                },
                {
                    "x": 1263,
                    "y": 3091
                },
                {
                    "x": 1219,
                    "y": 3091
                }
            ],
            "category": "footer",
            "html": "<footer id='193' style='font-size:16px'>19</footer>",
            "id": 193,
            "page": 19,
            "text": "19"
        },
        {
            "bounding_box": [
                {
                    "x": 205,
                    "y": 300
                },
                {
                    "x": 667,
                    "y": 300
                },
                {
                    "x": 667,
                    "y": 351
                },
                {
                    "x": 205,
                    "y": 351
                }
            ],
            "category": "paragraph",
            "html": "<p id='194' style='font-size:20px'>D. Additional Results</p>",
            "id": 194,
            "page": 20,
            "text": "D. Additional Results"
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 383
                },
                {
                    "x": 1577,
                    "y": 383
                },
                {
                    "x": 1577,
                    "y": 434
                },
                {
                    "x": 204,
                    "y": 434
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='195' style='font-size:20px'>D.1. Choosing the Signal-to-Noise Ratio for High-Resolution Synthesis</p>",
            "id": 195,
            "page": 20,
            "text": "D.1. Choosing the Signal-to-Noise Ratio for High-Resolution Synthesis"
        },
        {
            "bounding_box": [
                {
                    "x": 208,
                    "y": 517
                },
                {
                    "x": 2353,
                    "y": 517
                },
                {
                    "x": 2353,
                    "y": 1683
                },
                {
                    "x": 208,
                    "y": 1683
                }
            ],
            "category": "figure",
            "html": "<figure><img id='196' style='font-size:18px' alt=\"KL-reg, w/o rescaling KL-reg, w/ rescaling VQ-reg, w/o rescaling\" data-coord=\"top-left:(208,517); bottom-right:(2353,1683)\" /></figure>",
            "id": 196,
            "page": 20,
            "text": "KL-reg, w/o rescaling KL-reg, w/ rescaling VQ-reg, w/o rescaling"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 1688
                },
                {
                    "x": 2280,
                    "y": 1688
                },
                {
                    "x": 2280,
                    "y": 1778
                },
                {
                    "x": 202,
                    "y": 1778
                }
            ],
            "category": "caption",
            "html": "<br><caption id='197' style='font-size:14px'>Figure 15. Illustrating the effect of latent space rescaling on convolutional sampling, here for semantic image synthesis on landscapes. See<br>Sec. 4.3.2 and Sec. D.1.</caption>",
            "id": 197,
            "page": 20,
            "text": "Figure 15. Illustrating the effect of latent space rescaling on convolutional sampling, here for semantic image synthesis on landscapes. See Sec. 4.3.2 and Sec. D.1."
        },
        {
            "bounding_box": [
                {
                    "x": 198,
                    "y": 1837
                },
                {
                    "x": 2280,
                    "y": 1837
                },
                {
                    "x": 2280,
                    "y": 2141
                },
                {
                    "x": 198,
                    "y": 2141
                }
            ],
            "category": "paragraph",
            "html": "<p id='198' style='font-size:14px'>As discussed in Sec. 4.3.2, the signal-to-noise ratio induced by the variance of the latent space (i.e. Var(z)/o2) significantly<br>affects the results for convolutional sampling. For example, when training a LDM directly in the latent space of a KL-<br>regularized model (see Tab. 8), this ratio is very high, such that the model allocates a lot of semantic detail early on in the<br>reverse denoising process. In contrast, when rescaling the latent space by the component-wise standard deviation of the<br>latents as described in Sec. G, the SNR is descreased. We illustrate the effect on convolutional sampling for semantic image<br>synthesis in Fig. 15. Note that the VQ-regularized space has a variance close to 1, such that it does not have to be rescaled.</p>",
            "id": 198,
            "page": 20,
            "text": "As discussed in Sec. 4.3.2, the signal-to-noise ratio induced by the variance of the latent space (i.e. Var(z)/o2) significantly affects the results for convolutional sampling. For example, when training a LDM directly in the latent space of a KLregularized model (see Tab. 8), this ratio is very high, such that the model allocates a lot of semantic detail early on in the reverse denoising process. In contrast, when rescaling the latent space by the component-wise standard deviation of the latents as described in Sec. G, the SNR is descreased. We illustrate the effect on convolutional sampling for semantic image synthesis in Fig. 15. Note that the VQ-regularized space has a variance close to 1, such that it does not have to be rescaled."
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 2171
                },
                {
                    "x": 954,
                    "y": 2171
                },
                {
                    "x": 954,
                    "y": 2221
                },
                {
                    "x": 203,
                    "y": 2221
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='199' style='font-size:18px'>D.2. Full List of all First Stage Models</p>",
            "id": 199,
            "page": 20,
            "text": "D.2. Full List of all First Stage Models"
        },
        {
            "bounding_box": [
                {
                    "x": 253,
                    "y": 2250
                },
                {
                    "x": 2018,
                    "y": 2250
                },
                {
                    "x": 2018,
                    "y": 2299
                },
                {
                    "x": 253,
                    "y": 2299
                }
            ],
            "category": "paragraph",
            "html": "<p id='200' style='font-size:14px'>We provide a complete list of various autoenconding models trained on the OpenImages dataset in Tab. 8.</p>",
            "id": 200,
            "page": 20,
            "text": "We provide a complete list of various autoenconding models trained on the OpenImages dataset in Tab. 8."
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 2334
                },
                {
                    "x": 825,
                    "y": 2334
                },
                {
                    "x": 825,
                    "y": 2386
                },
                {
                    "x": 203,
                    "y": 2386
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='201' style='font-size:22px'>D.3. Layout-to-Image Synthesis</p>",
            "id": 201,
            "page": 20,
            "text": "D.3. Layout-to-Image Synthesis"
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 2410
                },
                {
                    "x": 2279,
                    "y": 2410
                },
                {
                    "x": 2279,
                    "y": 2713
                },
                {
                    "x": 199,
                    "y": 2713
                }
            ],
            "category": "paragraph",
            "html": "<p id='202' style='font-size:16px'>Here we provide the quantitative evaluation and additional samples for our layout-to-image models from Sec. 4.3.1. We<br>train a model on the COCO [4] and one on the OpenImages [49] dataset, which we subsequently additionally finetune on<br>COCO. Tab 9 shows the result. Our COCO model reaches the performance of recent state-of-the art models in layout-to-<br>image synthesis, when following their training and evaluation protocol [89]. When finetuning from the OpenImages model,<br>we surpass these works. Our OpenImages model surpasses the results of Jahn et al [37] by a margin of nearly 11 in terms of<br>FID. In Fig. 16 we show additional samples of the model finetuned on COCO.</p>",
            "id": 202,
            "page": 20,
            "text": "Here we provide the quantitative evaluation and additional samples for our layout-to-image models from Sec. 4.3.1. We train a model on the COCO  and one on the OpenImages  dataset, which we subsequently additionally finetune on COCO. Tab 9 shows the result. Our COCO model reaches the performance of recent state-of-the art models in layout-toimage synthesis, when following their training and evaluation protocol . When finetuning from the OpenImages model, we surpass these works. Our OpenImages model surpasses the results of Jahn et al  by a margin of nearly 11 in terms of FID. In Fig. 16 we show additional samples of the model finetuned on COCO."
        },
        {
            "bounding_box": [
                {
                    "x": 205,
                    "y": 2749
                },
                {
                    "x": 1240,
                    "y": 2749
                },
                {
                    "x": 1240,
                    "y": 2800
                },
                {
                    "x": 205,
                    "y": 2800
                }
            ],
            "category": "paragraph",
            "html": "<p id='203' style='font-size:18px'>D.4. Class-Conditional Image Synthesis on ImageNet</p>",
            "id": 203,
            "page": 20,
            "text": "D.4. Class-Conditional Image Synthesis on ImageNet"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2825
                },
                {
                    "x": 2280,
                    "y": 2825
                },
                {
                    "x": 2280,
                    "y": 2977
                },
                {
                    "x": 201,
                    "y": 2977
                }
            ],
            "category": "paragraph",
            "html": "<p id='204' style='font-size:14px'>Tab. 10 contains the results for our class-conditional LDM measured in FID and Inception score (IS). LDM-8 requires<br>significantly fewer parameters and compute requirements (see Tab. 18) to achieve very competitive performance. Similar<br>to previous work, we can further boost the performance by training a classifier on each noise scale and guiding with it,</p>",
            "id": 204,
            "page": 20,
            "text": "Tab. 10 contains the results for our class-conditional LDM measured in FID and Inception score (IS). LDM-8 requires significantly fewer parameters and compute requirements (see Tab. 18) to achieve very competitive performance. Similar to previous work, we can further boost the performance by training a classifier on each noise scale and guiding with it,"
        },
        {
            "bounding_box": [
                {
                    "x": 1215,
                    "y": 3054
                },
                {
                    "x": 1264,
                    "y": 3054
                },
                {
                    "x": 1264,
                    "y": 3093
                },
                {
                    "x": 1215,
                    "y": 3093
                }
            ],
            "category": "footer",
            "html": "<footer id='205' style='font-size:14px'>20</footer>",
            "id": 205,
            "page": 20,
            "text": "20"
        },
        {
            "bounding_box": [
                {
                    "x": 490,
                    "y": 290
                },
                {
                    "x": 1981,
                    "y": 290
                },
                {
                    "x": 1981,
                    "y": 1180
                },
                {
                    "x": 490,
                    "y": 1180
                }
            ],
            "category": "table",
            "html": "<table id='206' style='font-size:16px'><tr><td>f</td><td>②</td><td>c</td><td>R-FID ↓</td><td>R-IS ↑</td><td>PSNR ↑</td><td>PSIM ↓</td><td>SSIM ↑</td></tr><tr><td>16 VQGAN [23]</td><td>16384</td><td>256</td><td>4.98</td><td></td><td>19.9 ±3.4</td><td>1.83 ±0.42</td><td>0.51 ±0.18</td></tr><tr><td>16 VQGAN [23]</td><td>1024</td><td>256</td><td>7.94</td><td></td><td>19.4 ±3.3</td><td>1.98 ±0.43</td><td>0.50 ±0.18</td></tr><tr><td>8 DALL-E [66]</td><td>8192</td><td>-</td><td>32.01</td><td>-</td><td>22.8 ±2.1</td><td>1.95 ±0.51</td><td>0.73 ±0.13</td></tr><tr><td>32</td><td>16384</td><td>16</td><td>31.83</td><td>40.40 ±1.07</td><td>17.45 ±2.90</td><td>2.58 ±0.48</td><td>0.41 ±0.18</td></tr><tr><td>16</td><td>16384</td><td>8</td><td>5.15</td><td>144.55 ±3.74</td><td>20.83 ±3.61</td><td>1.73 ±0.43</td><td>0.54 ±0.18</td></tr><tr><td>8</td><td>16384</td><td>4</td><td>1.14</td><td>201.92 ±3.97</td><td>23.07 ±3.99</td><td>1.17 ±0.36</td><td>0.65 ±0.16</td></tr><tr><td>8</td><td>256</td><td>4</td><td>1.49</td><td>194.20 ±3.87</td><td>22.35 ±3.81</td><td>1.26 ±0.37</td><td>0.62 ±0.16</td></tr><tr><td>4</td><td>8192</td><td>3</td><td>0.58</td><td>224.78 ±5.35</td><td>27.43 ±4.26</td><td>0.53 ±0.21</td><td>0.82 ±0.10</td></tr><tr><td>4t</td><td>8192</td><td>3</td><td>1.06</td><td>221.94 ±4.58</td><td>25.21 ±4.17</td><td>0.72 ±0.26</td><td>0.76 ±0.12</td></tr><tr><td>4</td><td>256</td><td>3</td><td>0.47</td><td>223.81 ±4.58</td><td>26.43 ±4. 22</td><td>0.62 ±0.24</td><td>0.80 ±0.11</td></tr><tr><td>2</td><td>2048</td><td>2</td><td>0.16</td><td>232.75 ±5.09</td><td>30.85 ±4.12</td><td>0.27 ±0.12</td><td>0.91 ±0.05</td></tr><tr><td>2</td><td>64</td><td>2</td><td>0.40</td><td>226.62 ±4.83</td><td>29.13 ±3.46</td><td>0.38 ±0.13</td><td>0.90 ±0.05</td></tr><tr><td>32</td><td>KL</td><td>64</td><td>2.04</td><td>189.53 ±3.68</td><td>22.27 ±3.93</td><td>1.41 ±0.40</td><td>0.61 ±0.17</td></tr><tr><td>32</td><td>KL</td><td>16</td><td>7.3</td><td>132.75 ±2.71</td><td>20.38 ±3.56</td><td>1.88 ±0.45</td><td>0.53 ±0.18</td></tr><tr><td>16</td><td>KL</td><td>16</td><td>0.87</td><td>210.31 ±3.97</td><td>24.08 ±4.22</td><td>1.07 ±0.36</td><td>0.68 ±0.15</td></tr><tr><td>16</td><td>KL</td><td>8</td><td>2.63</td><td>178.68 ±4.08</td><td>21.94 ±3.92</td><td>1.49 ±0.42</td><td>0.59 ±0.17</td></tr><tr><td>8</td><td>KL</td><td>4</td><td>0.90</td><td>209.90 ±4.92</td><td>24.19 ±4.19</td><td>1.02 ±0.35</td><td>0.69 ±0.15</td></tr><tr><td>4</td><td>KL</td><td>3</td><td>0.27</td><td>227.57 ±4.89</td><td>27.53 ±4.54</td><td>0.55 ±0.24</td><td>0.82 ±0.11</td></tr><tr><td>2</td><td>KL</td><td>2</td><td>0.086</td><td>232.66 ±5.16</td><td>32.47 ±4.19</td><td>0.20 ±0.09</td><td>0.93 ±0.04</td></tr></table>",
            "id": 206,
            "page": 21,
            "text": "f ② c R-FID ↓ R-IS ↑ PSNR ↑ PSIM ↓ SSIM ↑  16 VQGAN  16384 256 4.98  19.9 ±3.4 1.83 ±0.42 0.51 ±0.18  16 VQGAN  1024 256 7.94  19.4 ±3.3 1.98 ±0.43 0.50 ±0.18  8 DALL-E  8192 - 32.01 - 22.8 ±2.1 1.95 ±0.51 0.73 ±0.13  32 16384 16 31.83 40.40 ±1.07 17.45 ±2.90 2.58 ±0.48 0.41 ±0.18  16 16384 8 5.15 144.55 ±3.74 20.83 ±3.61 1.73 ±0.43 0.54 ±0.18  8 16384 4 1.14 201.92 ±3.97 23.07 ±3.99 1.17 ±0.36 0.65 ±0.16  8 256 4 1.49 194.20 ±3.87 22.35 ±3.81 1.26 ±0.37 0.62 ±0.16  4 8192 3 0.58 224.78 ±5.35 27.43 ±4.26 0.53 ±0.21 0.82 ±0.10  4t 8192 3 1.06 221.94 ±4.58 25.21 ±4.17 0.72 ±0.26 0.76 ±0.12  4 256 3 0.47 223.81 ±4.58 26.43 ±4. 22 0.62 ±0.24 0.80 ±0.11  2 2048 2 0.16 232.75 ±5.09 30.85 ±4.12 0.27 ±0.12 0.91 ±0.05  2 64 2 0.40 226.62 ±4.83 29.13 ±3.46 0.38 ±0.13 0.90 ±0.05  32 KL 64 2.04 189.53 ±3.68 22.27 ±3.93 1.41 ±0.40 0.61 ±0.17  32 KL 16 7.3 132.75 ±2.71 20.38 ±3.56 1.88 ±0.45 0.53 ±0.18  16 KL 16 0.87 210.31 ±3.97 24.08 ±4.22 1.07 ±0.36 0.68 ±0.15  16 KL 8 2.63 178.68 ±4.08 21.94 ±3.92 1.49 ±0.42 0.59 ±0.17  8 KL 4 0.90 209.90 ±4.92 24.19 ±4.19 1.02 ±0.35 0.69 ±0.15  4 KL 3 0.27 227.57 ±4.89 27.53 ±4.54 0.55 ±0.24 0.82 ±0.11  2 KL 2 0.086 232.66 ±5.16 32.47 ±4.19 0.20 ±0.09"
        },
        {
            "bounding_box": [
                {
                    "x": 263,
                    "y": 1220
                },
                {
                    "x": 2207,
                    "y": 1220
                },
                {
                    "x": 2207,
                    "y": 1264
                },
                {
                    "x": 263,
                    "y": 1264
                }
            ],
            "category": "caption",
            "html": "<caption id='207' style='font-size:18px'>Table 8. Complete autoencoder ZOO trained on OpenImages, evaluated on ImageNet-Val. 1 denotes an attention-free autoencoder.</caption>",
            "id": 207,
            "page": 21,
            "text": "Table 8. Complete autoencoder ZOO trained on OpenImages, evaluated on ImageNet-Val. 1 denotes an attention-free autoencoder."
        },
        {
            "bounding_box": [
                {
                    "x": 838,
                    "y": 1339
                },
                {
                    "x": 1640,
                    "y": 1339
                },
                {
                    "x": 1640,
                    "y": 1380
                },
                {
                    "x": 838,
                    "y": 1380
                }
            ],
            "category": "caption",
            "html": "<caption id='208' style='font-size:22px'>layout-to-image synthesis on the COCO dataset</caption>",
            "id": 208,
            "page": 21,
            "text": "layout-to-image synthesis on the COCO dataset"
        },
        {
            "bounding_box": [
                {
                    "x": 208,
                    "y": 1367
                },
                {
                    "x": 2269,
                    "y": 1367
                },
                {
                    "x": 2269,
                    "y": 2664
                },
                {
                    "x": 208,
                    "y": 2664
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='209' style='font-size:14px' alt=\"wall-concr teiling-other tree whallitilecrete abjiditigeother wwwalywaffeconcrete wall-stone\ngiraffe\ncabinet\ntraffic ight girafte person\n77\nnorse\nclothes\nshelf\nelephant\nence\npush\nfloor-wood grass\naffic light\ntrafficlighteragightte grass\npotte\nVQ2\nPHILIMAD\" data-coord=\"top-left:(208,1367); bottom-right:(2269,2664)\" /></figure>",
            "id": 209,
            "page": 21,
            "text": "wall-concr teiling-other tree whallitilecrete abjiditigeother wwwalywaffeconcrete wall-stone giraffe cabinet traffic ight girafte person 77 norse clothes shelf elephant ence push floor-wood grass affic light trafficlighteragightte grass potte VQ2 PHILIMAD"
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 2693
                },
                {
                    "x": 2274,
                    "y": 2693
                },
                {
                    "x": 2274,
                    "y": 2786
                },
                {
                    "x": 204,
                    "y": 2786
                }
            ],
            "category": "caption",
            "html": "<caption id='210' style='font-size:18px'>Figure 16. More samples from our best model for layout-to-image synthesis, LDM-4, which was trained on the OpenImages dataset and<br>finetuned on the COCO dataset. Samples generated with 100 DDIM steps and 7 = 0. Layouts are from the COCO validation set.</caption>",
            "id": 210,
            "page": 21,
            "text": "Figure 16. More samples from our best model for layout-to-image synthesis, LDM-4, which was trained on the OpenImages dataset and finetuned on the COCO dataset. Samples generated with 100 DDIM steps and 7 = 0. Layouts are from the COCO validation set."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2877
                },
                {
                    "x": 2276,
                    "y": 2877
                },
                {
                    "x": 2276,
                    "y": 2977
                },
                {
                    "x": 201,
                    "y": 2977
                }
            ],
            "category": "paragraph",
            "html": "<p id='211' style='font-size:22px'>see Sec. C. Unlike the pixel-based methods, this classifier is trained very cheaply in latent space. For additional qualitative<br>results, see Fig. 26 and Fig. 27.</p>",
            "id": 211,
            "page": 21,
            "text": "see Sec. C. Unlike the pixel-based methods, this classifier is trained very cheaply in latent space. For additional qualitative results, see Fig. 26 and Fig. 27."
        },
        {
            "bounding_box": [
                {
                    "x": 1214,
                    "y": 3053
                },
                {
                    "x": 1260,
                    "y": 3053
                },
                {
                    "x": 1260,
                    "y": 3093
                },
                {
                    "x": 1214,
                    "y": 3093
                }
            ],
            "category": "footer",
            "html": "<footer id='212' style='font-size:20px'>21</footer>",
            "id": 212,
            "page": 21,
            "text": "21"
        },
        {
            "bounding_box": [
                {
                    "x": 522,
                    "y": 291
                },
                {
                    "x": 1960,
                    "y": 291
                },
                {
                    "x": 1960,
                    "y": 708
                },
                {
                    "x": 522,
                    "y": 708
                }
            ],
            "category": "table",
            "html": "<table id='213' style='font-size:14px'><tr><td></td><td>COCO256 x 256</td><td>OpenImages 256 x 256</td><td>OpenImages 512 x 512</td></tr><tr><td>Method</td><td>FID↓</td><td>FID↓</td><td>FID↓</td></tr><tr><td>LostGAN-V2 [87]</td><td>42.55</td><td>-</td><td>-</td></tr><tr><td>OC-GAN [89]</td><td>41.65</td><td>-</td><td>-</td></tr><tr><td>SPADE [62]</td><td>41.11</td><td>-</td><td>-</td></tr><tr><td>VQGAN+T [37]</td><td>56.58</td><td>45.33</td><td>48.11</td></tr><tr><td>LDM-8 (100 steps, ours)</td><td>42.06t</td><td>-</td><td>-</td></tr><tr><td>LDM-4 (200 steps, ours)</td><td>40.91*</td><td>32.02</td><td>35.80</td></tr></table>",
            "id": 213,
            "page": 22,
            "text": "COCO256 x 256 OpenImages 256 x 256 OpenImages 512 x 512  Method FID↓ FID↓ FID↓  LostGAN-V2  42.55 -  OC-GAN  41.65 -  SPADE  41.11 -  VQGAN+T  56.58 45.33 48.11  LDM-8 (100 steps, ours) 42.06t -  LDM-4 (200 steps, ours) 40.91* 32.02"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 744
                },
                {
                    "x": 2276,
                    "y": 744
                },
                {
                    "x": 2276,
                    "y": 836
                },
                {
                    "x": 201,
                    "y": 836
                }
            ],
            "category": "caption",
            "html": "<caption id='214' style='font-size:18px'>Table 9. Quantitative comparison of our layout-to-image models on the COCO [4] and OpenImages [49] datasets. t: Training from scratch<br>on COCO; * : Finetuning from OpenImages.</caption>",
            "id": 214,
            "page": 22,
            "text": "Table 9. Quantitative comparison of our layout-to-image models on the COCO  and OpenImages  datasets. t: Training from scratch on COCO; * : Finetuning from OpenImages."
        },
        {
            "bounding_box": [
                {
                    "x": 205,
                    "y": 906
                },
                {
                    "x": 2274,
                    "y": 906
                },
                {
                    "x": 2274,
                    "y": 1573
                },
                {
                    "x": 205,
                    "y": 1573
                }
            ],
            "category": "table",
            "html": "<table id='215' style='font-size:14px'><tr><td>Method</td><td>FID↓</td><td>IS↑</td><td>Precision↑</td><td>Recall↑</td><td>Nparams</td><td></td></tr><tr><td>SR3 [72]</td><td>11.30</td><td>-</td><td>-</td><td>-</td><td>625M</td><td>-</td></tr><tr><td>ImageBART [21]</td><td>21.19</td><td>-</td><td>-</td><td>-</td><td>3.5B</td><td>-</td></tr><tr><td>ImageBART [21]</td><td>7.44</td><td>-</td><td>-</td><td>-</td><td>3.5B</td><td>0.05 acc. rate*</td></tr><tr><td>VQGAN+T [23]</td><td>17.04</td><td>70.6±1.8</td><td>-</td><td>-</td><td>1.3B</td><td>-</td></tr><tr><td>VQGAN+T [23]</td><td>5.88</td><td>304.8±3.6</td><td>-</td><td>-</td><td>1.3B</td><td>0.05 acc. rate*</td></tr><tr><td>BigGan-deep [3]</td><td>6.95</td><td>203.6±2.6</td><td>0.87</td><td>0.28</td><td>340M</td><td>-</td></tr><tr><td>ADM [15]</td><td>10.94</td><td>100.98</td><td>0.69</td><td>0.63</td><td>554M</td><td>250 DDIM steps</td></tr><tr><td>ADM-G [15]</td><td>4.59</td><td>186.7</td><td>0.82</td><td>0.52</td><td>608M</td><td>250 DDIM steps</td></tr><tr><td>ADM-G,ADM-U [15]</td><td>3.85</td><td>221.72</td><td>0.84</td><td>0.53</td><td>n/a</td><td>2 x 250 DDIM steps</td></tr><tr><td>CDM [31]</td><td>4.88</td><td>158.71 ±2.26</td><td>-</td><td>-</td><td>n/a</td><td>2 x 100 DDIM steps</td></tr><tr><td>LDM-8 (ours)</td><td>17.41</td><td>72.92±2.6</td><td>0.65</td><td>0.62</td><td>395M</td><td>200 DDIM steps, 2.9M train steps, batch size 64</td></tr><tr><td>LDM-8-G (ours)</td><td>8.11</td><td>190.43±2.60</td><td>0.83</td><td>0.36</td><td>506M</td><td>200 DDIM steps, classifier scale 10, 2.9M train steps, batch size 64</td></tr><tr><td>LDM-8 (ours)</td><td>15.51</td><td>79.03±1.03</td><td>0.65</td><td>0.63</td><td>395M</td><td>200 DDIM steps, 4.8M train steps, batch size 64</td></tr><tr><td>LDM-8-G (ours)</td><td>7.76</td><td>209.52±4.24</td><td>0.84</td><td>0.35</td><td>506M</td><td>200 DDIM steps, classifier scale 10, 4.8M train steps, batch size 64</td></tr><tr><td>LDM-4 (ours)</td><td>10.56</td><td>103.49±1.24</td><td>0.71</td><td>0.62</td><td>400M</td><td>250 DDIM steps, 178K train steps, batch size 1200</td></tr><tr><td>LDM-4-G (ours)</td><td>3.95</td><td>178.22±2.43</td><td>0.81</td><td>0.55</td><td>400M</td><td>250 DDIM steps, unconditional guidance [32] scale 1.25, 178K train steps, batch size 1200</td></tr><tr><td>LDM-4-G (ours)</td><td>3.60</td><td>247.67 ±5.59</td><td>0.87</td><td>0.48</td><td>400M</td><td>250 DDIM steps, unconditional guidance [32] scale 1.5, 178K train steps, batch size 1200</td></tr></table>",
            "id": 215,
            "page": 22,
            "text": "Method FID↓ IS↑ Precision↑ Recall↑ Nparams   SR3  11.30 - - - 625M  ImageBART  21.19 - - - 3.5B  ImageBART  7.44 - - - 3.5B 0.05 acc. rate*  VQGAN+T  17.04 70.6±1.8 - - 1.3B  VQGAN+T  5.88 304.8±3.6 - - 1.3B 0.05 acc. rate*  BigGan-deep  6.95 203.6±2.6 0.87 0.28 340M  ADM  10.94 100.98 0.69 0.63 554M 250 DDIM steps  ADM-G  4.59 186.7 0.82 0.52 608M 250 DDIM steps  ADM-G,ADM-U  3.85 221.72 0.84 0.53 n/a 2 x 250 DDIM steps  CDM  4.88 158.71 ±2.26 - - n/a 2 x 100 DDIM steps  LDM-8 (ours) 17.41 72.92±2.6 0.65 0.62 395M 200 DDIM steps, 2.9M train steps, batch size 64  LDM-8-G (ours) 8.11 190.43±2.60 0.83 0.36 506M 200 DDIM steps, classifier scale 10, 2.9M train steps, batch size 64  LDM-8 (ours) 15.51 79.03±1.03 0.65 0.63 395M 200 DDIM steps, 4.8M train steps, batch size 64  LDM-8-G (ours) 7.76 209.52±4.24 0.84 0.35 506M 200 DDIM steps, classifier scale 10, 4.8M train steps, batch size 64  LDM-4 (ours) 10.56 103.49±1.24 0.71 0.62 400M 250 DDIM steps, 178K train steps, batch size 1200  LDM-4-G (ours) 3.95 178.22±2.43 0.81 0.55 400M 250 DDIM steps, unconditional guidance  scale 1.25, 178K train steps, batch size 1200  LDM-4-G (ours) 3.60 247.67 ±5.59 0.87 0.48 400M"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 1607
                },
                {
                    "x": 2276,
                    "y": 1607
                },
                {
                    "x": 2276,
                    "y": 1700
                },
                {
                    "x": 202,
                    "y": 1700
                }
            ],
            "category": "caption",
            "html": "<caption id='216' style='font-size:16px'>Table 10. Comparison of a class-conditional ImageNet LDM with recent state-of-the-art methods for class-conditional image generation<br>on the ImageNet [12] dataset. : Classifier rejection sampling with the given rejection rate as proposed in [67].</caption>",
            "id": 216,
            "page": 22,
            "text": "Table 10. Comparison of a class-conditional ImageNet LDM with recent state-of-the-art methods for class-conditional image generation on the ImageNet  dataset. : Classifier rejection sampling with the given rejection rate as proposed in ."
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 1812
                },
                {
                    "x": 1406,
                    "y": 1812
                },
                {
                    "x": 1406,
                    "y": 1863
                },
                {
                    "x": 204,
                    "y": 1863
                }
            ],
            "category": "caption",
            "html": "<caption id='217' style='font-size:22px'>D.5. Sample Quality VS. V100 Days (Continued from Sec. 4.1)</caption>",
            "id": 217,
            "page": 22,
            "text": "D.5. Sample Quality VS. V100 Days (Continued from Sec. 4.1)"
        },
        {
            "bounding_box": [
                {
                    "x": 208,
                    "y": 1962
                },
                {
                    "x": 2276,
                    "y": 1962
                },
                {
                    "x": 2276,
                    "y": 2608
                },
                {
                    "x": 208,
                    "y": 2608
                }
            ],
            "category": "figure",
            "html": "<figure><img id='218' style='font-size:14px' alt=\"FID VS. V100 days\n200 Inception Score VS. V100 days\nLDM-1\nLDM-2\n30\n150\nLDM-4\nLDM-8 Score\nInception\n문 LDM-16 20\n100\nLDM-32\n10\n50\n0 10 20 30\n0 10 20 30\nV100 days\nV100 days\" data-coord=\"top-left:(208,1962); bottom-right:(2276,2608)\" /></figure>",
            "id": 218,
            "page": 22,
            "text": "FID VS. V100 days 200 Inception Score VS. V100 days LDM-1 LDM-2 30 150 LDM-4 LDM-8 Score Inception 문 LDM-16 20 100 LDM-32 10 50 0 10 20 30 0 10 20 30 V100 days V100 days"
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 2651
                },
                {
                    "x": 2277,
                    "y": 2651
                },
                {
                    "x": 2277,
                    "y": 2746
                },
                {
                    "x": 199,
                    "y": 2746
                }
            ],
            "category": "caption",
            "html": "<caption id='219' style='font-size:16px'>Figure 17. For completeness we also report the training progress of class-conditional LDMs on the ImageNet dataset for a fixed number<br>of 35 V100 days. Results obtained with 100 DDIM steps [84] and K = 0. FIDs computed on 5000 samples for efficiency reasons.</caption>",
            "id": 219,
            "page": 22,
            "text": "Figure 17. For completeness we also report the training progress of class-conditional LDMs on the ImageNet dataset for a fixed number of 35 V100 days. Results obtained with 100 DDIM steps  and K = 0. FIDs computed on 5000 samples for efficiency reasons."
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 2825
                },
                {
                    "x": 2278,
                    "y": 2825
                },
                {
                    "x": 2278,
                    "y": 2977
                },
                {
                    "x": 199,
                    "y": 2977
                }
            ],
            "category": "paragraph",
            "html": "<p id='220' style='font-size:20px'>For the assessment of sample quality over the training progress in Sec. 4.1, we reported FID and IS scores as a function<br>of train steps. Another possibility is to report these metrics over the used resources in V100 days. Such an analysis is<br>additionally provided in Fig. 17, showing qualitatively similar results.</p>",
            "id": 220,
            "page": 22,
            "text": "For the assessment of sample quality over the training progress in Sec. 4.1, we reported FID and IS scores as a function of train steps. Another possibility is to report these metrics over the used resources in V100 days. Such an analysis is additionally provided in Fig. 17, showing qualitatively similar results."
        },
        {
            "bounding_box": [
                {
                    "x": 1215,
                    "y": 3053
                },
                {
                    "x": 1263,
                    "y": 3053
                },
                {
                    "x": 1263,
                    "y": 3092
                },
                {
                    "x": 1215,
                    "y": 3092
                }
            ],
            "category": "footer",
            "html": "<footer id='221' style='font-size:20px'>22</footer>",
            "id": 221,
            "page": 22,
            "text": "22"
        },
        {
            "bounding_box": [
                {
                    "x": 626,
                    "y": 291
                },
                {
                    "x": 1850,
                    "y": 291
                },
                {
                    "x": 1850,
                    "y": 714
                },
                {
                    "x": 626,
                    "y": 714
                }
            ],
            "category": "table",
            "html": "<table id='222' style='font-size:14px'><tr><td>Method</td><td>FID ↓</td><td>IS ↑</td><td>PSNR ↑</td><td>SSIM ↑</td></tr><tr><td>Image Regression [72]</td><td>15.2</td><td>121.1</td><td>27.9</td><td>0.801</td></tr><tr><td>SR3 [72]</td><td>5.2</td><td>180.1</td><td>26.4</td><td>0.762</td></tr><tr><td>LDM-4 (ours, 100 steps)</td><td>2.8�/4.8±</td><td>166.3</td><td>24.4±3.8</td><td>0.69±0.14</td></tr><tr><td>LDM-4 (ours, 50 steps, guiding)</td><td>4.4±/6.4±</td><td>153.7</td><td>25.8±3.7</td><td>0.74±0.12</td></tr><tr><td>LDM-4 (ours, 100 steps, guiding)</td><td>4.4±/6.4±</td><td>154.1</td><td>25.7±3.7</td><td>0.73±0.12</td></tr><tr><td>LDM-4 (ours, 100 steps, +15 ep.)</td><td>2.6t / 4.6±</td><td>169.76±5.03</td><td>24.4±3.8</td><td>0.69±0.14</td></tr><tr><td>Pixel-DM (100 steps, +15 ep.)</td><td>5.1t / 7.1‡</td><td>163.06±4.67</td><td>24.1±3.3</td><td>0.59±0.12</td></tr></table>",
            "id": 222,
            "page": 23,
            "text": "Method FID ↓ IS ↑ PSNR ↑ SSIM ↑  Image Regression  15.2 121.1 27.9 0.801  SR3  5.2 180.1 26.4 0.762  LDM-4 (ours, 100 steps) 2.8�/4.8± 166.3 24.4±3.8 0.69±0.14  LDM-4 (ours, 50 steps, guiding) 4.4±/6.4± 153.7 25.8±3.7 0.74±0.12  LDM-4 (ours, 100 steps, guiding) 4.4±/6.4± 154.1 25.7±3.7 0.73±0.12  LDM-4 (ours, 100 steps, +15 ep.) 2.6t / 4.6± 169.76±5.03 24.4±3.8 0.69±0.14  Pixel-DM (100 steps, +15 ep.) 5.1t / 7.1‡ 163.06±4.67 24.1±3.3"
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 717
                },
                {
                    "x": 2279,
                    "y": 717
                },
                {
                    "x": 2279,
                    "y": 854
                },
                {
                    "x": 200,
                    "y": 854
                }
            ],
            "category": "caption",
            "html": "<br><caption id='223' style='font-size:14px'>Table 11. x4 upscaling results on ImageNet-Val. (2562); t: FID features computed on validation split, ‡: FID features computed on train<br>split. We also include a pixel-space baseline that receives the same amount of compute as LDM-4. The last two rows received 15 epochs<br>of additional training compared to the former results.</caption>",
            "id": 223,
            "page": 23,
            "text": "Table 11. x4 upscaling results on ImageNet-Val. (2562); t: FID features computed on validation split, ‡: FID features computed on train split. We also include a pixel-space baseline that receives the same amount of compute as LDM-4. The last two rows received 15 epochs of additional training compared to the former results."
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 939
                },
                {
                    "x": 640,
                    "y": 939
                },
                {
                    "x": 640,
                    "y": 989
                },
                {
                    "x": 203,
                    "y": 989
                }
            ],
            "category": "paragraph",
            "html": "<p id='224' style='font-size:22px'>D.6. Super-Resolution</p>",
            "id": 224,
            "page": 23,
            "text": "D.6. Super-Resolution"
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 1015
                },
                {
                    "x": 2277,
                    "y": 1015
                },
                {
                    "x": 2277,
                    "y": 1266
                },
                {
                    "x": 199,
                    "y": 1266
                }
            ],
            "category": "paragraph",
            "html": "<p id='225' style='font-size:18px'>For better comparability between LDMs and diffusion models in pixel space, we extend our analysis from Tab. 5 by<br>comparing a diffusion model trained for the same number of steps and with a comparable number 1 of parameters to our<br>LDM. The results of this comparison are shown in the last two rows of Tab. 11 and demonstrate that LDM achieves better<br>performance while allowing for significantly faster sampling. A qualitative comparison is given in Fig. 20 which shows<br>random samples from both LDM and the diffusion model in pixel space.</p>",
            "id": 225,
            "page": 23,
            "text": "For better comparability between LDMs and diffusion models in pixel space, we extend our analysis from Tab. 5 by comparing a diffusion model trained for the same number of steps and with a comparable number 1 of parameters to our LDM. The results of this comparison are shown in the last two rows of Tab. 11 and demonstrate that LDM achieves better performance while allowing for significantly faster sampling. A qualitative comparison is given in Fig. 20 which shows random samples from both LDM and the diffusion model in pixel space."
        },
        {
            "bounding_box": [
                {
                    "x": 205,
                    "y": 1326
                },
                {
                    "x": 1605,
                    "y": 1326
                },
                {
                    "x": 1605,
                    "y": 1373
                },
                {
                    "x": 205,
                    "y": 1373
                }
            ],
            "category": "caption",
            "html": "<caption id='226' style='font-size:20px'>D.6.1 LDM-BSR: General Purpose SR Model via Diverse Image Degradation</caption>",
            "id": 226,
            "page": 23,
            "text": "D.6.1 LDM-BSR: General Purpose SR Model via Diverse Image Degradation"
        },
        {
            "bounding_box": [
                {
                    "x": 207,
                    "y": 1438
                },
                {
                    "x": 2274,
                    "y": 1438
                },
                {
                    "x": 2274,
                    "y": 2142
                },
                {
                    "x": 207,
                    "y": 2142
                }
            ],
            "category": "figure",
            "html": "<figure><img id='227' style='font-size:18px' alt=\"bicubic LDM-SR LDM-BSR\" data-coord=\"top-left:(207,1438); bottom-right:(2274,2142)\" /></figure>",
            "id": 227,
            "page": 23,
            "text": "bicubic LDM-SR LDM-BSR"
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 2154
                },
                {
                    "x": 2275,
                    "y": 2154
                },
                {
                    "x": 2275,
                    "y": 2244
                },
                {
                    "x": 200,
                    "y": 2244
                }
            ],
            "category": "caption",
            "html": "<br><caption id='228' style='font-size:16px'>Figure 18. LDM-BSR generalizes to arbitrary inputs and can be used as a general-purpose upsampler, upscaling samples from a class-<br>conditional LDM (image cf. Fig. 4) to 10242 resolution. In contrast, using a fixed degradation process (see Sec. 4.4) hinders generalization.</caption>",
            "id": 228,
            "page": 23,
            "text": "Figure 18. LDM-BSR generalizes to arbitrary inputs and can be used as a general-purpose upsampler, upscaling samples from a classconditional LDM (image cf. Fig. 4) to 10242 resolution. In contrast, using a fixed degradation process (see Sec. 4.4) hinders generalization."
        },
        {
            "bounding_box": [
                {
                    "x": 198,
                    "y": 2263
                },
                {
                    "x": 2278,
                    "y": 2263
                },
                {
                    "x": 2278,
                    "y": 2913
                },
                {
                    "x": 198,
                    "y": 2913
                }
            ],
            "category": "paragraph",
            "html": "<p id='229' style='font-size:18px'>To evaluate generalization of our LDM-SR, we apply it both on synthetic LDM samples from a class-conditional ImageNet<br>model (Sec. 4.1) and images crawled from the internet. Interestingly, we observe that LDM-SR, trained only with a bicubicly<br>downsampled conditioning as in [72], does not generalize well to images which do not follow this pre-processing. Hence, to<br>obtain a superresolution model for a wide range of real world images, which can contain complex superpositions of camera<br>noise, compression artifacts, blurr and interpolations, we replace the bicubic downsampling operation in LDM-SR with the<br>degration pipeline from [105]. The BSR-degradation process is a degradation pipline which applies JPEG compressions<br>noise, camera sensor noise, different image interpolations for downsampling, Gaussian blur kernels and Gaussian noise in a<br>random order to an image. We found that using the bsr-degredation process with the original parameters as in [105] leads to<br>a very strong degradation process. Since a more moderate degradation process seemed apppropiate for our application, we<br>adapted the parameters of the bsr-degradation (our adapted degradation process can be found in our code base at https :<br>/ / github . com/ CompVi S / latent - diffusion). Fig. 18 illustrates the effectiveness of this approach by directly<br>comparing LDM-SR with LDM-BSR. The latter produces images much sharper than the models confined to a fixed pre-<br>processing, making it suitable for real-world applications. Further results of LDM-BSR are shown on LSUN-cows in Fig. 19.</p>",
            "id": 229,
            "page": 23,
            "text": "To evaluate generalization of our LDM-SR, we apply it both on synthetic LDM samples from a class-conditional ImageNet model (Sec. 4.1) and images crawled from the internet. Interestingly, we observe that LDM-SR, trained only with a bicubicly downsampled conditioning as in , does not generalize well to images which do not follow this pre-processing. Hence, to obtain a superresolution model for a wide range of real world images, which can contain complex superpositions of camera noise, compression artifacts, blurr and interpolations, we replace the bicubic downsampling operation in LDM-SR with the degration pipeline from . The BSR-degradation process is a degradation pipline which applies JPEG compressions noise, camera sensor noise, different image interpolations for downsampling, Gaussian blur kernels and Gaussian noise in a random order to an image. We found that using the bsr-degredation process with the original parameters as in  leads to a very strong degradation process. Since a more moderate degradation process seemed apppropiate for our application, we adapted the parameters of the bsr-degradation (our adapted degradation process can be found in our code base at https : / / github . com/ CompVi S / latent - diffusion). Fig. 18 illustrates the effectiveness of this approach by directly comparing LDM-SR with LDM-BSR. The latter produces images much sharper than the models confined to a fixed preprocessing, making it suitable for real-world applications. Further results of LDM-BSR are shown on LSUN-cows in Fig. 19."
        },
        {
            "bounding_box": [
                {
                    "x": 257,
                    "y": 2932
                },
                {
                    "x": 1668,
                    "y": 2932
                },
                {
                    "x": 1668,
                    "y": 2974
                },
                {
                    "x": 257,
                    "y": 2974
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='230' style='font-size:14px'>1Itis not possible to exactly match both architectures since the diffusion model operates in the pixel space</p>",
            "id": 230,
            "page": 23,
            "text": "1Itis not possible to exactly match both architectures since the diffusion model operates in the pixel space"
        },
        {
            "bounding_box": [
                {
                    "x": 1215,
                    "y": 3054
                },
                {
                    "x": 1262,
                    "y": 3054
                },
                {
                    "x": 1262,
                    "y": 3091
                },
                {
                    "x": 1215,
                    "y": 3091
                }
            ],
            "category": "footer",
            "html": "<footer id='231' style='font-size:18px'>23</footer>",
            "id": 231,
            "page": 23,
            "text": "23"
        },
        {
            "bounding_box": [
                {
                    "x": 205,
                    "y": 299
                },
                {
                    "x": 1255,
                    "y": 299
                },
                {
                    "x": 1255,
                    "y": 357
                },
                {
                    "x": 205,
                    "y": 357
                }
            ],
            "category": "paragraph",
            "html": "<p id='232' style='font-size:22px'>E. Implementation Details and Hyperparameters</p>",
            "id": 232,
            "page": 24,
            "text": "E. Implementation Details and Hyperparameters"
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 386
                },
                {
                    "x": 649,
                    "y": 386
                },
                {
                    "x": 649,
                    "y": 440
                },
                {
                    "x": 204,
                    "y": 440
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='233' style='font-size:20px'>E.1. Hyperparameters</p>",
            "id": 233,
            "page": 24,
            "text": "E.1. Hyperparameters"
        },
        {
            "bounding_box": [
                {
                    "x": 254,
                    "y": 471
                },
                {
                    "x": 2184,
                    "y": 471
                },
                {
                    "x": 2184,
                    "y": 520
                },
                {
                    "x": 254,
                    "y": 520
                }
            ],
            "category": "caption",
            "html": "<caption id='234' style='font-size:16px'>We provide an overview of the hyperparameters of all trained LDM models in Tab. 12, Tab. 13, Tab. 14 and Tab. 15.</caption>",
            "id": 234,
            "page": 24,
            "text": "We provide an overview of the hyperparameters of all trained LDM models in Tab. 12, Tab. 13, Tab. 14 and Tab. 15."
        },
        {
            "bounding_box": [
                {
                    "x": 416,
                    "y": 570
                },
                {
                    "x": 2073,
                    "y": 570
                },
                {
                    "x": 2073,
                    "y": 1173
                },
                {
                    "x": 416,
                    "y": 1173
                }
            ],
            "category": "table",
            "html": "<table id='235' style='font-size:14px'><tr><td></td><td>CelebA-HQ 256 x 256</td><td>FFHQ 256 x 256</td><td>LSUN-Churches 256 x 256</td><td>LSUN-Bedrooms 256 x 256</td></tr><tr><td>f</td><td>4</td><td>4</td><td>8</td><td>4</td></tr><tr><td>z-shape</td><td>64 x 64 x 3</td><td>64 x 64 x 3</td><td>-</td><td>64 x 64 x 3</td></tr><tr><td>121</td><td>8192</td><td>8192</td><td>-</td><td>8192</td></tr><tr><td>Diffusion steps</td><td>1000</td><td>1000</td><td>1000</td><td>1000</td></tr><tr><td>Noise Schedule</td><td>linear</td><td>linear</td><td>linear</td><td>linear</td></tr><tr><td>Nparams</td><td>274M</td><td>274M</td><td>294M</td><td>274M</td></tr><tr><td>Channels</td><td>224</td><td>224</td><td>192</td><td>224</td></tr><tr><td>Depth</td><td>2</td><td>2</td><td>2</td><td>2</td></tr><tr><td>Channel Multiplier</td><td>1,2,3,4</td><td>1,2,3,4</td><td>1,2,2,4,4</td><td>1,2,3,4</td></tr><tr><td>Attention resolutions</td><td>32, 16, 8</td><td>32, 16, 8</td><td>32, 16, 8, 4</td><td>32, 16, 8</td></tr><tr><td>Head Channels</td><td>32</td><td>32</td><td>24</td><td>32</td></tr><tr><td>Batch Size</td><td>48</td><td>42</td><td>96</td><td>48</td></tr><tr><td>Iterations*</td><td>410k</td><td>635k</td><td>500k</td><td>1.9M</td></tr><tr><td>Learning Rate</td><td>9.6e-5</td><td>8.4e-5</td><td>5.e-5</td><td>9.6e-5</td></tr></table>",
            "id": 235,
            "page": 24,
            "text": "CelebA-HQ 256 x 256 FFHQ 256 x 256 LSUN-Churches 256 x 256 LSUN-Bedrooms 256 x 256  f 4 4 8 4  z-shape 64 x 64 x 3 64 x 64 x 3 - 64 x 64 x 3  121 8192 8192 - 8192  Diffusion steps 1000 1000 1000 1000  Noise Schedule linear linear linear linear  Nparams 274M 274M 294M 274M  Channels 224 224 192 224  Depth 2 2 2 2  Channel Multiplier 1,2,3,4 1,2,3,4 1,2,2,4,4 1,2,3,4  Attention resolutions 32, 16, 8 32, 16, 8 32, 16, 8, 4 32, 16, 8  Head Channels 32 32 24 32  Batch Size 48 42 96 48  Iterations* 410k 635k 500k 1.9M  Learning Rate 9.6e-5 8.4e-5 5.e-5"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 1251
                },
                {
                    "x": 2278,
                    "y": 1251
                },
                {
                    "x": 2278,
                    "y": 1342
                },
                {
                    "x": 202,
                    "y": 1342
                }
            ],
            "category": "caption",
            "html": "<caption id='236' style='font-size:16px'>Table 12. Hyperparameters for the unconditional LDMs producing the numbers shown in Tab. 1. All models trained on a single NVIDIA<br>A100.</caption>",
            "id": 236,
            "page": 24,
            "text": "Table 12. Hyperparameters for the unconditional LDMs producing the numbers shown in Tab. 1. All models trained on a single NVIDIA A100."
        },
        {
            "bounding_box": [
                {
                    "x": 413,
                    "y": 1464
                },
                {
                    "x": 2073,
                    "y": 1464
                },
                {
                    "x": 2073,
                    "y": 2211
                },
                {
                    "x": 413,
                    "y": 2211
                }
            ],
            "category": "table",
            "html": "<table id='237' style='font-size:14px'><tr><td></td><td>LDM-1</td><td>LDM-2</td><td>LDM-4</td><td>LDM-8</td><td>LDM-16</td><td>LDM-32</td></tr><tr><td>z-shape</td><td>256 x 256 x 3</td><td>128 x 128 x 2</td><td>64 x 64 x 3</td><td>32 x 32 x 4</td><td>16 x 16 x 8</td><td>88 x 8 x 32</td></tr><tr><td>121</td><td>-</td><td>2048</td><td>8192</td><td>16384</td><td>16384</td><td>16384</td></tr><tr><td>Diffusion steps</td><td>1000</td><td>1000</td><td>1000</td><td>1000</td><td>1000</td><td>1000</td></tr><tr><td>Noise Schedule</td><td>linear</td><td>linear</td><td>linear</td><td>linear</td><td>linear</td><td>linear</td></tr><tr><td>Model Size</td><td>396M</td><td>391M</td><td>391M</td><td>395M</td><td>395M</td><td>395M</td></tr><tr><td>Channels</td><td>192</td><td>192</td><td>192</td><td>256</td><td>256</td><td>256</td></tr><tr><td>Depth</td><td>2</td><td>2</td><td>2</td><td>2</td><td>2</td><td>2</td></tr><tr><td>Channel Multiplier</td><td>1,1,2,2,4,4</td><td>1,2,2,4,4</td><td>1,2,3,5</td><td>1,2,4</td><td>1,2,4</td><td>1,2,4</td></tr><tr><td>Number of Heads</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td></tr><tr><td>Batch Size</td><td>7</td><td>9</td><td>40</td><td>64</td><td>112</td><td>112</td></tr><tr><td>Iterations</td><td>2M</td><td>2M</td><td>2M</td><td>2M</td><td>2M</td><td>2M</td></tr><tr><td>Learning Rate</td><td>4.9e-5</td><td>6.3e-5</td><td>8e-5</td><td>6.4e-5</td><td>4.5e-5</td><td>4.5e-5</td></tr><tr><td>Conditioning</td><td>CA</td><td>CA</td><td>CA</td><td>CA</td><td>CA</td><td>CA</td></tr><tr><td>CA-resolutions</td><td>32, 16, 8</td><td>32, 16, 8</td><td>32, 16, 8</td><td>32, 16, 8</td><td>16, 8, 4</td><td>8, 4, 2</td></tr><tr><td>Embedding Dimension</td><td>512</td><td>512</td><td>512</td><td>512</td><td>512</td><td>512</td></tr><tr><td>Transformers Depth</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td></tr></table>",
            "id": 237,
            "page": 24,
            "text": "LDM-1 LDM-2 LDM-4 LDM-8 LDM-16 LDM-32  z-shape 256 x 256 x 3 128 x 128 x 2 64 x 64 x 3 32 x 32 x 4 16 x 16 x 8 88 x 8 x 32  121 - 2048 8192 16384 16384 16384  Diffusion steps 1000 1000 1000 1000 1000 1000  Noise Schedule linear linear linear linear linear linear  Model Size 396M 391M 391M 395M 395M 395M  Channels 192 192 192 256 256 256  Depth 2 2 2 2 2 2  Channel Multiplier 1,1,2,2,4,4 1,2,2,4,4 1,2,3,5 1,2,4 1,2,4 1,2,4  Number of Heads 1 1 1 1 1 1  Batch Size 7 9 40 64 112 112  Iterations 2M 2M 2M 2M 2M 2M  Learning Rate 4.9e-5 6.3e-5 8e-5 6.4e-5 4.5e-5 4.5e-5  Conditioning CA CA CA CA CA CA  CA-resolutions 32, 16, 8 32, 16, 8 32, 16, 8 32, 16, 8 16, 8, 4 8, 4, 2  Embedding Dimension 512 512 512 512 512 512  Transformers Depth 1 1 1 1 1"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2288
                },
                {
                    "x": 2278,
                    "y": 2288
                },
                {
                    "x": 2278,
                    "y": 2383
                },
                {
                    "x": 202,
                    "y": 2383
                }
            ],
            "category": "paragraph",
            "html": "<p id='238' style='font-size:16px'>Table 13. Hyperparameters for the conditional LDMs trained on the ImageNet dataset for the analysis in Sec. 4.1 All models trained on a<br>single NVIDIA A100.</p>",
            "id": 238,
            "page": 24,
            "text": "Table 13. Hyperparameters for the conditional LDMs trained on the ImageNet dataset for the analysis in Sec. 4.1 All models trained on a single NVIDIA A100."
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 2497
                },
                {
                    "x": 760,
                    "y": 2497
                },
                {
                    "x": 760,
                    "y": 2551
                },
                {
                    "x": 203,
                    "y": 2551
                }
            ],
            "category": "paragraph",
            "html": "<p id='239' style='font-size:20px'>E.2. Implementation Details</p>",
            "id": 239,
            "page": 24,
            "text": "E.2. Implementation Details"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2583
                },
                {
                    "x": 1125,
                    "y": 2583
                },
                {
                    "x": 1125,
                    "y": 2634
                },
                {
                    "x": 202,
                    "y": 2634
                }
            ],
            "category": "paragraph",
            "html": "<p id='240' style='font-size:18px'>E.2.1 Implementations of �� for conditional LDMs</p>",
            "id": 240,
            "page": 24,
            "text": "E.2.1 Implementations of �� for conditional LDMs"
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 2672
                },
                {
                    "x": 2282,
                    "y": 2672
                },
                {
                    "x": 2282,
                    "y": 2876
                },
                {
                    "x": 200,
                    "y": 2876
                }
            ],
            "category": "paragraph",
            "html": "<p id='241' style='font-size:18px'>For the experiments on text-to-image and layout-to-image (Sec. 4.3.1) synthesis, we implement the conditioner �� as an<br>unmasked transformer which processes a tokenized version of the input y and produces an output 5 := TO(y), where 5 E<br>RMxdT More specifically, the transformer is implemented from N transformer blocks consisting of global self-attention<br>layers, layer-normalization and position-wise MLPs as follows2:</p>",
            "id": 241,
            "page": 24,
            "text": "For the experiments on text-to-image and layout-to-image (Sec. 4.3.1) synthesis, we implement the conditioner �� as an unmasked transformer which processes a tokenized version of the input y and produces an output 5 := TO(y), where 5 E RMxdT More specifically, the transformer is implemented from N transformer blocks consisting of global self-attention layers, layer-normalization and position-wise MLPs as follows2:"
        },
        {
            "bounding_box": [
                {
                    "x": 250,
                    "y": 2927
                },
                {
                    "x": 1337,
                    "y": 2927
                },
                {
                    "x": 1337,
                    "y": 2974
                },
                {
                    "x": 250,
                    "y": 2974
                }
            ],
            "category": "paragraph",
            "html": "<p id='242' style='font-size:18px'>2adapted from https://gittubb.com/lundraains/s-tuansforens</p>",
            "id": 242,
            "page": 24,
            "text": "2adapted from https://gittubb.com/lundraains/s-tuansforens"
        },
        {
            "bounding_box": [
                {
                    "x": 1215,
                    "y": 3052
                },
                {
                    "x": 1263,
                    "y": 3052
                },
                {
                    "x": 1263,
                    "y": 3093
                },
                {
                    "x": 1215,
                    "y": 3093
                }
            ],
            "category": "footer",
            "html": "<footer id='243' style='font-size:16px'>24</footer>",
            "id": 243,
            "page": 24,
            "text": "24"
        },
        {
            "bounding_box": [
                {
                    "x": 411,
                    "y": 291
                },
                {
                    "x": 2074,
                    "y": 291
                },
                {
                    "x": 2074,
                    "y": 917
                },
                {
                    "x": 411,
                    "y": 917
                }
            ],
            "category": "table",
            "html": "<table id='244' style='font-size:16px'><tr><td></td><td>LDM-1</td><td>LDM-2</td><td>LDM-4</td><td>LDM-8</td><td>LDM-16</td><td>LDM-32</td></tr><tr><td>z-shape</td><td>256 x 256 x 3</td><td>128 x 128 x 2</td><td>64 x 64 x 3</td><td>32 x 32 x 4</td><td>16 x 16 x 8</td><td>88 x 8 x 32</td></tr><tr><td>�</td><td>-</td><td>2048</td><td>8192</td><td>16384</td><td>16384</td><td>16384</td></tr><tr><td>Diffusion steps</td><td>1000</td><td>1000</td><td>1000</td><td>1000</td><td>1000</td><td>1000</td></tr><tr><td>Noise Schedule</td><td>linear</td><td>linear</td><td>linear</td><td>linear</td><td>linear</td><td>linear</td></tr><tr><td>Model Size</td><td>270M</td><td>265M</td><td>274M</td><td>258M</td><td>260M</td><td>258M</td></tr><tr><td>Channels</td><td>192</td><td>192</td><td>224</td><td>256</td><td>256</td><td>256</td></tr><tr><td>Depth</td><td>2</td><td>2</td><td>2</td><td>2</td><td>2</td><td>2</td></tr><tr><td>Channel Multiplier</td><td>1,1,2,2,4,4</td><td>1,2,2,4,4</td><td>1,2,3,4</td><td>1,2,4</td><td>1,2,4</td><td>1,2,4</td></tr><tr><td>Attention resolutions</td><td>32, 16, 8</td><td>32, 16, 8</td><td>32, 16, 8</td><td>32, 16, 8</td><td>16, 8, 4</td><td>8, 4, 2</td></tr><tr><td>Head Channels</td><td>32</td><td>32</td><td>32</td><td>32</td><td>32</td><td>32</td></tr><tr><td>Batch Size</td><td>9</td><td>11</td><td>48</td><td>96</td><td>128</td><td>128</td></tr><tr><td>Iterations*</td><td>500k</td><td>500k</td><td>500k</td><td>500k</td><td>500k</td><td>500k</td></tr><tr><td>Learning Rate</td><td>9e-5</td><td>1.1e-4</td><td>9.6e-5</td><td>9.6e-5</td><td>1.3e-4</td><td>1.3e-4</td></tr></table>",
            "id": 244,
            "page": 25,
            "text": "LDM-1 LDM-2 LDM-4 LDM-8 LDM-16 LDM-32  z-shape 256 x 256 x 3 128 x 128 x 2 64 x 64 x 3 32 x 32 x 4 16 x 16 x 8 88 x 8 x 32  � - 2048 8192 16384 16384 16384  Diffusion steps 1000 1000 1000 1000 1000 1000  Noise Schedule linear linear linear linear linear linear  Model Size 270M 265M 274M 258M 260M 258M  Channels 192 192 224 256 256 256  Depth 2 2 2 2 2 2  Channel Multiplier 1,1,2,2,4,4 1,2,2,4,4 1,2,3,4 1,2,4 1,2,4 1,2,4  Attention resolutions 32, 16, 8 32, 16, 8 32, 16, 8 32, 16, 8 16, 8, 4 8, 4, 2  Head Channels 32 32 32 32 32 32  Batch Size 9 11 48 96 128 128  Iterations* 500k 500k 500k 500k 500k 500k  Learning Rate 9e-5 1.1e-4 9.6e-5 9.6e-5 1.3e-4"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 990
                },
                {
                    "x": 2279,
                    "y": 990
                },
                {
                    "x": 2279,
                    "y": 1130
                },
                {
                    "x": 201,
                    "y": 1130
                }
            ],
            "category": "caption",
            "html": "<caption id='245' style='font-size:16px'>Table 14. Hyperparameters for the unconditional LDMs trained on the CelebA dataset for the analysis in Fig. 7. All models trained on a<br>single NVIDIA A100. * All models are trained for 500k iterations. If converging earlier, we used the best checkpoint for assessing the<br>:<br>provided FID scores.</caption>",
            "id": 245,
            "page": 25,
            "text": "Table 14. Hyperparameters for the unconditional LDMs trained on the CelebA dataset for the analysis in Fig. 7. All models trained on a single NVIDIA A100. * All models are trained for 500k iterations. If converging earlier, we used the best checkpoint for assessing the : provided FID scores."
        },
        {
            "bounding_box": [
                {
                    "x": 261,
                    "y": 1170
                },
                {
                    "x": 2232,
                    "y": 1170
                },
                {
                    "x": 2232,
                    "y": 1972
                },
                {
                    "x": 261,
                    "y": 1972
                }
            ],
            "category": "table",
            "html": "<table id='246' style='font-size:14px'><tr><td>Task</td><td>Text-to-Image</td><td colspan=\"2\">Layout-to-Image</td><td>Class-Label-to-Image</td><td>Super Resolution</td><td>Inpainting</td><td>Semantic-Map-to-Image</td></tr><tr><td>Dataset</td><td>LAION</td><td>OpenImages</td><td>COCO</td><td>ImageNet</td><td>ImageNet</td><td>Places</td><td>Landscapes</td></tr><tr><td>f</td><td>8</td><td>4</td><td>8</td><td>4</td><td>4</td><td>4</td><td>8</td></tr><tr><td>z-shape</td><td>32 x 32 x 4</td><td>64 x 64 x 3</td><td>32 x 32 x 4</td><td>64 x 64 x 3</td><td>64 x 64 x 3</td><td>64 x 64 x 3</td><td>32 x 32 x 4</td></tr><tr><td>121</td><td>-</td><td>8192</td><td>16384</td><td>8192</td><td>8192</td><td>8192</td><td>16384</td></tr><tr><td>Diffusion steps</td><td>1000</td><td>1000</td><td>1000</td><td>1000</td><td>1000</td><td>1000</td><td>1000</td></tr><tr><td>Noise Schedule</td><td>linear</td><td>linear</td><td>linear</td><td>linear</td><td>linear</td><td>linear</td><td>linear</td></tr><tr><td>Model Size</td><td>1.45B</td><td>306M</td><td>345M</td><td>395M</td><td>169M</td><td>215M</td><td>215M</td></tr><tr><td>Channels</td><td>320</td><td>128</td><td>192</td><td>192</td><td>160</td><td>128</td><td>128</td></tr><tr><td>Depth</td><td>2</td><td>2</td><td>2</td><td>2</td><td>2</td><td>2</td><td>2</td></tr><tr><td>Channel Multiplier</td><td>1,2,4,4</td><td>1,2,3,4</td><td>1,2,4</td><td>1,2,3,5</td><td>1,2,2,4</td><td>1,4,8</td><td>1,4,8</td></tr><tr><td>Number of Heads</td><td>8</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td></tr><tr><td>Dropout</td><td>-</td><td>-</td><td>0.1</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>Batch Size</td><td>680</td><td>24</td><td>48</td><td>1200</td><td>64</td><td>128</td><td>48</td></tr><tr><td>Iterations</td><td>390K</td><td>4.4M</td><td>170K</td><td>178K</td><td>860K</td><td>360K</td><td>360K</td></tr><tr><td>Learning Rate</td><td>1.0e-4</td><td>4.8e-5</td><td>4.8e-5</td><td>1.0e-4</td><td>6.4e-5</td><td>1.0e-6</td><td>4.8e-5</td></tr><tr><td>Conditioning</td><td>CA</td><td>CA</td><td>CA</td><td>CA</td><td>concat</td><td>concat</td><td>concat</td></tr><tr><td>(C)A-resolutions</td><td>32, 16, 8</td><td>32, 16, 8</td><td>32, 16, 8</td><td>32, 16, 8</td><td></td><td>-</td><td></td></tr><tr><td>Embedding Dimension</td><td>1280</td><td>512</td><td>512</td><td>512</td><td>-</td><td>-</td><td></td></tr><tr><td>Transformer Depth</td><td>1</td><td>3</td><td>2</td><td>1</td><td></td><td>-</td><td></td></tr></table>",
            "id": 246,
            "page": 25,
            "text": "Task Text-to-Image Layout-to-Image Class-Label-to-Image Super Resolution Inpainting Semantic-Map-to-Image  Dataset LAION OpenImages COCO ImageNet ImageNet Places Landscapes  f 8 4 8 4 4 4 8  z-shape 32 x 32 x 4 64 x 64 x 3 32 x 32 x 4 64 x 64 x 3 64 x 64 x 3 64 x 64 x 3 32 x 32 x 4  121 - 8192 16384 8192 8192 8192 16384  Diffusion steps 1000 1000 1000 1000 1000 1000 1000  Noise Schedule linear linear linear linear linear linear linear  Model Size 1.45B 306M 345M 395M 169M 215M 215M  Channels 320 128 192 192 160 128 128  Depth 2 2 2 2 2 2 2  Channel Multiplier 1,2,4,4 1,2,3,4 1,2,4 1,2,3,5 1,2,2,4 1,4,8 1,4,8  Number of Heads 8 1 1 1 1 1 1  Dropout - - 0.1 - - -  Batch Size 680 24 48 1200 64 128 48  Iterations 390K 4.4M 170K 178K 860K 360K 360K  Learning Rate 1.0e-4 4.8e-5 4.8e-5 1.0e-4 6.4e-5 1.0e-6 4.8e-5  Conditioning CA CA CA CA concat concat concat  (C)A-resolutions 32, 16, 8 32, 16, 8 32, 16, 8 32, 16, 8  -   Embedding Dimension 1280 512 512 512 - -   Transformer Depth 1 3 2 1  -"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2044
                },
                {
                    "x": 2277,
                    "y": 2044
                },
                {
                    "x": 2277,
                    "y": 2138
                },
                {
                    "x": 201,
                    "y": 2138
                }
            ],
            "category": "caption",
            "html": "<caption id='247' style='font-size:16px'>Table 15. Hyperparameters for the conditional LDMs from Sec. 4. All models trained on a single NVIDIA A100 except for the inpainting<br>model which was trained on eight V100.</caption>",
            "id": 247,
            "page": 25,
            "text": "Table 15. Hyperparameters for the conditional LDMs from Sec. 4. All models trained on a single NVIDIA A100 except for the inpainting model which was trained on eight V100."
        },
        {
            "bounding_box": [
                {
                    "x": 2195,
                    "y": 2742
                },
                {
                    "x": 2278,
                    "y": 2742
                },
                {
                    "x": 2278,
                    "y": 2790
                },
                {
                    "x": 2195,
                    "y": 2790
                }
            ],
            "category": "paragraph",
            "html": "<p id='248' style='font-size:22px'>(24)</p>",
            "id": 248,
            "page": 25,
            "text": "(24)"
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 2825
                },
                {
                    "x": 2279,
                    "y": 2825
                },
                {
                    "x": 2279,
                    "y": 2977
                },
                {
                    "x": 199,
                    "y": 2977
                }
            ],
            "category": "paragraph",
            "html": "<p id='249' style='font-size:18px'>With 5 available, the conditioning is mapped into the UNet via the cross-attention mechanism as depicted in Fig. 3. We<br>modify the \"ablated UNet\" [15] architecture and replace the self-attention layer with a shallow (unmasked) transformer<br>consisting of T blocks with alternating layers of (i) self-attention, (ii) a position-wise MLP and (iii) a cross-attention layer;</p>",
            "id": 249,
            "page": 25,
            "text": "With 5 available, the conditioning is mapped into the UNet via the cross-attention mechanism as depicted in Fig. 3. We modify the \"ablated UNet\"  architecture and replace the self-attention layer with a shallow (unmasked) transformer consisting of T blocks with alternating layers of (i) self-attention, (ii) a position-wise MLP and (iii) a cross-attention layer;"
        },
        {
            "bounding_box": [
                {
                    "x": 1216,
                    "y": 3053
                },
                {
                    "x": 1263,
                    "y": 3053
                },
                {
                    "x": 1263,
                    "y": 3092
                },
                {
                    "x": 1216,
                    "y": 3092
                }
            ],
            "category": "footer",
            "html": "<footer id='250' style='font-size:18px'>25</footer>",
            "id": 250,
            "page": 25,
            "text": "25"
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 307
                },
                {
                    "x": 1802,
                    "y": 307
                },
                {
                    "x": 1802,
                    "y": 352
                },
                {
                    "x": 203,
                    "y": 352
                }
            ],
            "category": "paragraph",
            "html": "<p id='251' style='font-size:16px'>see Tab. 16. Note that without (ii) and (iii), this architecture is equivalent to the \"ablated UNet\".</p>",
            "id": 251,
            "page": 26,
            "text": "see Tab. 16. Note that without (ii) and (iii), this architecture is equivalent to the \"ablated UNet\"."
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 357
                },
                {
                    "x": 2280,
                    "y": 357
                },
                {
                    "x": 2280,
                    "y": 501
                },
                {
                    "x": 199,
                    "y": 501
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='252' style='font-size:16px'>While it would be possible to increase the representational power of �� by additionally conditioning on the time step t, we<br>do not pursue this choice as it reduces the speed of inference. We leave a more detailed analysis of this modification to future<br>work.</p>",
            "id": 252,
            "page": 26,
            "text": "While it would be possible to increase the representational power of �� by additionally conditioning on the time step t, we do not pursue this choice as it reduces the speed of inference. We leave a more detailed analysis of this modification to future work."
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 507
                },
                {
                    "x": 2278,
                    "y": 507
                },
                {
                    "x": 2278,
                    "y": 702
                },
                {
                    "x": 199,
                    "y": 702
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='253' style='font-size:16px'>For the text-to-image model, we rely on a publicly available3 tokenizer [99]. The layout-to-image model discretizes the<br>spatial locations of the bounding boxes and encodes each box as a (1, b, c)-tuple, where l denotes the (discrete) top-left and 6<br>the bottom-right position. Class information is contained in c.<br>See Tab. 17 for the hyperparameters of �� and Tab. 13 for those of the UNet for both of the above tasks.</p>",
            "id": 253,
            "page": 26,
            "text": "For the text-to-image model, we rely on a publicly available3 tokenizer . The layout-to-image model discretizes the spatial locations of the bounding boxes and encodes each box as a (1, b, c)-tuple, where l denotes the (discrete) top-left and 6 the bottom-right position. Class information is contained in c. See Tab. 17 for the hyperparameters of �� and Tab. 13 for those of the UNet for both of the above tasks."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 753
                },
                {
                    "x": 2276,
                    "y": 753
                },
                {
                    "x": 2276,
                    "y": 853
                },
                {
                    "x": 200,
                    "y": 853
                }
            ],
            "category": "paragraph",
            "html": "<p id='254' style='font-size:16px'>Note that the class-conditional model as described in Sec. 4.1 is also implemented via cross-attention, where �� is a single<br>learnable embedding layer with a dimensionality of 512, mapping classes y to 5 E R1x512<br>·</p>",
            "id": 254,
            "page": 26,
            "text": "Note that the class-conditional model as described in Sec. 4.1 is also implemented via cross-attention, where �� is a single learnable embedding layer with a dimensionality of 512, mapping classes y to 5 E R1x512 ·"
        },
        {
            "bounding_box": [
                {
                    "x": 936,
                    "y": 890
                },
                {
                    "x": 1536,
                    "y": 890
                },
                {
                    "x": 1536,
                    "y": 1334
                },
                {
                    "x": 936,
                    "y": 1334
                }
            ],
            "category": "table",
            "html": "<table id='255' style='font-size:14px'><tr><td>input</td><td>Rhxwxc</td></tr><tr><td>LayerNorm</td><td>Rhxwxc</td></tr><tr><td>Convlx1</td><td>Rhxwx d·nh</td></tr><tr><td>Reshape</td><td>Rh·wx d.nh</td></tr><tr><td>SelfAttention</td><td>Rh·wx d·nh</td></tr><tr><td>xT MLP</td><td>Rh·w x d.nh Rh·w x d·nh</td></tr><tr><td>CrossAttention</td><td></td></tr><tr><td>Reshape</td><td>Rhxwx d.nh</td></tr><tr><td>Convlx1</td><td>R hxwxc</td></tr></table>",
            "id": 255,
            "page": 26,
            "text": "input Rhxwxc  LayerNorm Rhxwxc  Convlx1 Rhxwx d·nh  Reshape Rh·wx d.nh  SelfAttention Rh·wx d·nh  xT MLP Rh·w x d.nh Rh·w x d·nh  CrossAttention   Reshape Rhxwx d.nh  Convlx1"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1408
                },
                {
                    "x": 2274,
                    "y": 1408
                },
                {
                    "x": 2274,
                    "y": 1501
                },
                {
                    "x": 201,
                    "y": 1501
                }
            ],
            "category": "caption",
            "html": "<caption id='256' style='font-size:14px'>Table 16. Architecture of a transformer block as described in Sec. E.2.1, replacing the self-attention layer of the standard \"ablated UNet\"<br>architecture [15]. Here, nh denotes the number of attention heads and d the dimensionality per head.</caption>",
            "id": 256,
            "page": 26,
            "text": "Table 16. Architecture of a transformer block as described in Sec. E.2.1, replacing the self-attention layer of the standard \"ablated UNet\" architecture . Here, nh denotes the number of attention heads and d the dimensionality per head."
        },
        {
            "bounding_box": [
                {
                    "x": 796,
                    "y": 1593
                },
                {
                    "x": 1676,
                    "y": 1593
                },
                {
                    "x": 1676,
                    "y": 1848
                },
                {
                    "x": 796,
                    "y": 1848
                }
            ],
            "category": "table",
            "html": "<table id='257' style='font-size:20px'><tr><td></td><td>Text-to-Image</td><td>Layout-to-Image</td></tr><tr><td>seq-length</td><td>77</td><td>92</td></tr><tr><td>depth N</td><td>32</td><td>16</td></tr><tr><td>dim</td><td>1280</td><td>512</td></tr></table>",
            "id": 257,
            "page": 26,
            "text": "Text-to-Image Layout-to-Image  seq-length 77 92  depth N 32 16  dim 1280"
        },
        {
            "bounding_box": [
                {
                    "x": 587,
                    "y": 1923
                },
                {
                    "x": 1889,
                    "y": 1923
                },
                {
                    "x": 1889,
                    "y": 1970
                },
                {
                    "x": 587,
                    "y": 1970
                }
            ],
            "category": "caption",
            "html": "<caption id='258' style='font-size:14px'>Table 17. Hyperparameters for the experiments with transformer encoders in Sec. 4.3.</caption>",
            "id": 258,
            "page": 26,
            "text": "Table 17. Hyperparameters for the experiments with transformer encoders in Sec. 4.3."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2082
                },
                {
                    "x": 533,
                    "y": 2082
                },
                {
                    "x": 533,
                    "y": 2133
                },
                {
                    "x": 202,
                    "y": 2133
                }
            ],
            "category": "paragraph",
            "html": "<p id='259' style='font-size:20px'>E.2.2 Inpainting</p>",
            "id": 259,
            "page": 26,
            "text": "E.2.2 Inpainting"
        },
        {
            "bounding_box": [
                {
                    "x": 198,
                    "y": 2160
                },
                {
                    "x": 2277,
                    "y": 2160
                },
                {
                    "x": 2277,
                    "y": 2410
                },
                {
                    "x": 198,
                    "y": 2410
                }
            ],
            "category": "paragraph",
            "html": "<p id='260' style='font-size:16px'>For our experiments on image-inpainting in Sec. 4.5, we used the code of [88] to generate synthetic masks. We use a fixed<br>set of 2k validation and 30k testing samples from Places [108]. During training, we use random crops of size 256 x 256<br>and evaluate on crops of size 512 x 512. This follows the training and testing protocol in [88] and reproduces their reported<br>metrics (see 1 in Tab. 7). We include additional qualitative results of LDM-4, w/ attn in Fig. 21 and of LDM-4, w/o attn, big,<br>w/ft in Fig. 22.</p>",
            "id": 260,
            "page": 26,
            "text": "For our experiments on image-inpainting in Sec. 4.5, we used the code of  to generate synthetic masks. We use a fixed set of 2k validation and 30k testing samples from Places . During training, we use random crops of size 256 x 256 and evaluate on crops of size 512 x 512. This follows the training and testing protocol in  and reproduces their reported metrics (see 1 in Tab. 7). We include additional qualitative results of LDM-4, w/ attn in Fig. 21 and of LDM-4, w/o attn, big, w/ft in Fig. 22."
        },
        {
            "bounding_box": [
                {
                    "x": 205,
                    "y": 2437
                },
                {
                    "x": 660,
                    "y": 2437
                },
                {
                    "x": 660,
                    "y": 2489
                },
                {
                    "x": 205,
                    "y": 2489
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='261' style='font-size:22px'>E.3. Evaluation Details</p>",
            "id": 261,
            "page": 26,
            "text": "E.3. Evaluation Details"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2516
                },
                {
                    "x": 1709,
                    "y": 2516
                },
                {
                    "x": 1709,
                    "y": 2566
                },
                {
                    "x": 201,
                    "y": 2566
                }
            ],
            "category": "paragraph",
            "html": "<p id='262' style='font-size:16px'>This section provides additional details on evaluation for the experiments shown in Sec. 4.</p>",
            "id": 262,
            "page": 26,
            "text": "This section provides additional details on evaluation for the experiments shown in Sec. 4."
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 2625
                },
                {
                    "x": 1717,
                    "y": 2625
                },
                {
                    "x": 1717,
                    "y": 2674
                },
                {
                    "x": 203,
                    "y": 2674
                }
            ],
            "category": "paragraph",
            "html": "<p id='263' style='font-size:20px'>E.3.1 Quantitative Results in Unconditional and Class-Conditional Image Synthesis</p>",
            "id": 263,
            "page": 26,
            "text": "E.3.1 Quantitative Results in Unconditional and Class-Conditional Image Synthesis"
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 2702
                },
                {
                    "x": 2280,
                    "y": 2702
                },
                {
                    "x": 2280,
                    "y": 2904
                },
                {
                    "x": 199,
                    "y": 2904
                }
            ],
            "category": "paragraph",
            "html": "<p id='264' style='font-size:18px'>We follow common practice and estimate the statistics for calculating the FID-, Precision- and Recall-scores [29,50] shown in<br>Tab. 1 and 10 based on 50k samples from our models and the entire training set of each of the shown datasets. For calculating<br>FID scores we use the torch-fidelity package [60]. However, since different data processing pipelines might lead to<br>different results [64], we also evaluate our models with the script provided by Dhariwal and Nichol [15]. We find that results</p>",
            "id": 264,
            "page": 26,
            "text": "We follow common practice and estimate the statistics for calculating the FID-, Precision- and Recall-scores  shown in Tab. 1 and 10 based on 50k samples from our models and the entire training set of each of the shown datasets. For calculating FID scores we use the torch-fidelity package . However, since different data processing pipelines might lead to different results , we also evaluate our models with the script provided by Dhariwal and Nichol . We find that results"
        },
        {
            "bounding_box": [
                {
                    "x": 249,
                    "y": 2927
                },
                {
                    "x": 1724,
                    "y": 2927
                },
                {
                    "x": 1724,
                    "y": 2976
                },
                {
                    "x": 249,
                    "y": 2976
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='265' style='font-size:16px'>3https: //huppingface.co/tranaformers/indel_doc/dent html #berttokenizerfast</p>",
            "id": 265,
            "page": 26,
            "text": "3https: //huppingface.co/tranaformers/indel_doc/dent html #berttokenizerfast"
        },
        {
            "bounding_box": [
                {
                    "x": 1215,
                    "y": 3053
                },
                {
                    "x": 1265,
                    "y": 3053
                },
                {
                    "x": 1265,
                    "y": 3093
                },
                {
                    "x": 1215,
                    "y": 3093
                }
            ],
            "category": "footer",
            "html": "<footer id='266' style='font-size:16px'>26</footer>",
            "id": 266,
            "page": 26,
            "text": "26"
        },
        {
            "bounding_box": [
                {
                    "x": 197,
                    "y": 305
                },
                {
                    "x": 2281,
                    "y": 305
                },
                {
                    "x": 2281,
                    "y": 505
                },
                {
                    "x": 197,
                    "y": 505
                }
            ],
            "category": "paragraph",
            "html": "<p id='267' style='font-size:16px'>mainly coincide, except for the ImageNet and LSUN-Bedrooms datasets, where we notice slightly varying scores of 7.76<br>(torch-fidelity) VS. 7.77 (Nichol and Dhariwal) and 2.95 VS 3.0. For the future we emphasize the importance of a<br>unified procedure for sample quality assessment. Precision and Recall are also computed by using the script provided by<br>Nichol and Dhariwal.</p>",
            "id": 267,
            "page": 27,
            "text": "mainly coincide, except for the ImageNet and LSUN-Bedrooms datasets, where we notice slightly varying scores of 7.76 (torch-fidelity) VS. 7.77 (Nichol and Dhariwal) and 2.95 VS 3.0. For the future we emphasize the importance of a unified procedure for sample quality assessment. Precision and Recall are also computed by using the script provided by Nichol and Dhariwal."
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 562
                },
                {
                    "x": 772,
                    "y": 562
                },
                {
                    "x": 772,
                    "y": 614
                },
                {
                    "x": 204,
                    "y": 614
                }
            ],
            "category": "paragraph",
            "html": "<p id='268' style='font-size:20px'>E.3.2 Text-to-Image Synthesis</p>",
            "id": 268,
            "page": 27,
            "text": "E.3.2 Text-to-Image Synthesis"
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 640
                },
                {
                    "x": 2278,
                    "y": 640
                },
                {
                    "x": 2278,
                    "y": 794
                },
                {
                    "x": 199,
                    "y": 794
                }
            ],
            "category": "paragraph",
            "html": "<p id='269' style='font-size:16px'>Following the evaluation protocol of [66] we compute FID and Inception Score for the Text-to-Image models from Tab. 2 by<br>comparing generated samples with 30000 samples from the validation set of the MS-COCO dataset [51]. FID and Inception<br>Scores are computed with torch-fidelity.</p>",
            "id": 269,
            "page": 27,
            "text": "Following the evaluation protocol of  we compute FID and Inception Score for the Text-to-Image models from Tab. 2 by comparing generated samples with 30000 samples from the validation set of the MS-COCO dataset . FID and Inception Scores are computed with torch-fidelity."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 850
                },
                {
                    "x": 820,
                    "y": 850
                },
                {
                    "x": 820,
                    "y": 902
                },
                {
                    "x": 201,
                    "y": 902
                }
            ],
            "category": "paragraph",
            "html": "<p id='270' style='font-size:20px'>E.3.3 Layout-to-Image Synthesis</p>",
            "id": 270,
            "page": 27,
            "text": "E.3.3 Layout-to-Image Synthesis"
        },
        {
            "bounding_box": [
                {
                    "x": 198,
                    "y": 928
                },
                {
                    "x": 2280,
                    "y": 928
                },
                {
                    "x": 2280,
                    "y": 1133
                },
                {
                    "x": 198,
                    "y": 1133
                }
            ],
            "category": "paragraph",
            "html": "<p id='271' style='font-size:14px'>For assessing the sample quality of our Layout-to-Image models from Tab. 9 on the COCO dataset, we follow common<br>practice [37, 87, 89] and compute FID scores the 2048 unaugmented examples of the COCO Segmentation Challenge split.<br>To obtain better comparability, we use the exact same samples as in [37]. For the OpenImages dataset we similarly follow<br>their protocol and use 2048 center-cropped test images from the validation set.</p>",
            "id": 271,
            "page": 27,
            "text": "For assessing the sample quality of our Layout-to-Image models from Tab. 9 on the COCO dataset, we follow common practice  and compute FID scores the 2048 unaugmented examples of the COCO Segmentation Challenge split. To obtain better comparability, we use the exact same samples as in . For the OpenImages dataset we similarly follow their protocol and use 2048 center-cropped test images from the validation set."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1189
                },
                {
                    "x": 652,
                    "y": 1189
                },
                {
                    "x": 652,
                    "y": 1240
                },
                {
                    "x": 201,
                    "y": 1240
                }
            ],
            "category": "paragraph",
            "html": "<p id='272' style='font-size:18px'>E.3.4 Super Resolution</p>",
            "id": 272,
            "page": 27,
            "text": "E.3.4 Super Resolution"
        },
        {
            "bounding_box": [
                {
                    "x": 197,
                    "y": 1266
                },
                {
                    "x": 2281,
                    "y": 1266
                },
                {
                    "x": 2281,
                    "y": 1517
                },
                {
                    "x": 197,
                    "y": 1517
                }
            ],
            "category": "paragraph",
            "html": "<p id='273' style='font-size:16px'>We evaluate the super-resolution models on ImageNet following the pipeline suggested in [72], i.e. images with a shorter<br>size less than 256 px are removed (both for training and evaluation). On ImageNet, the low-resolution images are produced<br>using bicubic interpolation with anti-aliasing. FIDs are evaluated using torch-fidelity [60], and we produce samples<br>on the validation split. For FID scores, we additionally compare to reference features computed on the train split, see Tab. 5<br>and Tab. 11.</p>",
            "id": 273,
            "page": 27,
            "text": "We evaluate the super-resolution models on ImageNet following the pipeline suggested in , i.e. images with a shorter size less than 256 px are removed (both for training and evaluation). On ImageNet, the low-resolution images are produced using bicubic interpolation with anti-aliasing. FIDs are evaluated using torch-fidelity , and we produce samples on the validation split. For FID scores, we additionally compare to reference features computed on the train split, see Tab. 5 and Tab. 11."
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 1576
                },
                {
                    "x": 682,
                    "y": 1576
                },
                {
                    "x": 682,
                    "y": 1628
                },
                {
                    "x": 203,
                    "y": 1628
                }
            ],
            "category": "paragraph",
            "html": "<p id='274' style='font-size:20px'>E.3.5 Efficiency Analysis</p>",
            "id": 274,
            "page": 27,
            "text": "E.3.5 Efficiency Analysis"
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 1653
                },
                {
                    "x": 2280,
                    "y": 1653
                },
                {
                    "x": 2280,
                    "y": 1859
                },
                {
                    "x": 199,
                    "y": 1859
                }
            ],
            "category": "paragraph",
            "html": "<p id='275' style='font-size:14px'>For efficiency reasons we compute the sample quality metrics plotted in Fig. 6, 17 and 7 based on 5k samples. Therefore,<br>the results might vary from those shown in Tab. 1 and 10. All models have a comparable number of parameters as provided<br>in Tab. 13 and 14. We maximize the learning rates of the individual models such that they still train stably. Therefore, the<br>learning rates slightly vary between different runs cf. Tab. 13 and 14.</p>",
            "id": 275,
            "page": 27,
            "text": "For efficiency reasons we compute the sample quality metrics plotted in Fig. 6, 17 and 7 based on 5k samples. Therefore, the results might vary from those shown in Tab. 1 and 10. All models have a comparable number of parameters as provided in Tab. 13 and 14. We maximize the learning rates of the individual models such that they still train stably. Therefore, the learning rates slightly vary between different runs cf. Tab. 13 and 14."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 1914
                },
                {
                    "x": 539,
                    "y": 1914
                },
                {
                    "x": 539,
                    "y": 1965
                },
                {
                    "x": 202,
                    "y": 1965
                }
            ],
            "category": "paragraph",
            "html": "<p id='276' style='font-size:18px'>E.3.6 User Study</p>",
            "id": 276,
            "page": 27,
            "text": "E.3.6 User Study"
        },
        {
            "bounding_box": [
                {
                    "x": 197,
                    "y": 1991
                },
                {
                    "x": 2279,
                    "y": 1991
                },
                {
                    "x": 2279,
                    "y": 2395
                },
                {
                    "x": 197,
                    "y": 2395
                }
            ],
            "category": "paragraph",
            "html": "<p id='277' style='font-size:16px'>For the results of the user study presented in Tab. 4 we followed the protocoll of [72] and and use the 2-alternative force-choice<br>paradigm to assess human preference scores for two distinct tasks. In Task-1 subjects were shown a low resolution/masked<br>image between the corresponding ground truth high resolution/unmasked version and a synthesized image, which was gen-<br>erated by using the middle image as conditioning. For SuperResolution subjects were asked: 'Which of the two images is a<br>better high quality version of the low resolution image in the middle?' For Inpainting we asked 'Which of the two images<br>contains more realistic inpainted regions of the image in the middle?'. In Task-2, humans were similarly shown the low-<br>res/masked version and asked for preference between two corresponding images generated by the two competing methods.<br>As in [72] humans viewed the images for 3 seconds before responding.</p>",
            "id": 277,
            "page": 27,
            "text": "For the results of the user study presented in Tab. 4 we followed the protocoll of  and and use the 2-alternative force-choice paradigm to assess human preference scores for two distinct tasks. In Task-1 subjects were shown a low resolution/masked image between the corresponding ground truth high resolution/unmasked version and a synthesized image, which was generated by using the middle image as conditioning. For SuperResolution subjects were asked: 'Which of the two images is a better high quality version of the low resolution image in the middle?' For Inpainting we asked 'Which of the two images contains more realistic inpainted regions of the image in the middle?'. In Task-2, humans were similarly shown the lowres/masked version and asked for preference between two corresponding images generated by the two competing methods. As in  humans viewed the images for 3 seconds before responding."
        },
        {
            "bounding_box": [
                {
                    "x": 1214,
                    "y": 3052
                },
                {
                    "x": 1264,
                    "y": 3052
                },
                {
                    "x": 1264,
                    "y": 3093
                },
                {
                    "x": 1214,
                    "y": 3093
                }
            ],
            "category": "footer",
            "html": "<footer id='278' style='font-size:14px'>27</footer>",
            "id": 278,
            "page": 27,
            "text": "27"
        },
        {
            "bounding_box": [
                {
                    "x": 205,
                    "y": 299
                },
                {
                    "x": 899,
                    "y": 299
                },
                {
                    "x": 899,
                    "y": 355
                },
                {
                    "x": 205,
                    "y": 355
                }
            ],
            "category": "caption",
            "html": "<caption id='279' style='font-size:22px'>F. Computational Requirements</caption>",
            "id": 279,
            "page": 28,
            "text": "F. Computational Requirements"
        },
        {
            "bounding_box": [
                {
                    "x": 258,
                    "y": 416
                },
                {
                    "x": 2230,
                    "y": 416
                },
                {
                    "x": 2230,
                    "y": 1392
                },
                {
                    "x": 258,
                    "y": 1392
                }
            ],
            "category": "table",
            "html": "<table id='280' style='font-size:14px'><tr><td>Method</td><td>Generator Compute</td><td>Classifier Compute</td><td>Overall Compute</td><td>Inference Throughput*</td><td>Nparams</td><td>FID↓</td><td>IS↑</td><td>Precision↑</td><td>Recall↑</td></tr><tr><td>LSUN Churches 2562</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>StyleGAN2 [42]†</td><td>64</td><td>-</td><td>64</td><td>-</td><td>59M</td><td>3.86</td><td>-</td><td>-</td><td>-</td></tr><tr><td>LDM-8 (ours, 100 steps, 410K)</td><td>18</td><td>-</td><td>18</td><td>6.80</td><td>256M</td><td>4.02</td><td>-</td><td>0.64</td><td>0.52</td></tr><tr><td>LSUN Bedrooms 2562</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>ADM [15]† (1000 steps)</td><td>232</td><td>-</td><td>232</td><td>0.03</td><td>552M</td><td>1.9</td><td>-</td><td>0.66</td><td>0.51</td></tr><tr><td>LDM-4 (ours, 200 steps, 1.9M)</td><td>60</td><td>-</td><td>55</td><td>1.07</td><td>274M</td><td>2.95</td><td>、</td><td>0.66</td><td>0.48</td></tr><tr><td>CelebA-HQ 2562</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>LDM-4 (ours, 500 steps, 410K)</td><td>14.4</td><td>-</td><td>14.4</td><td>0.43</td><td>274M</td><td>5.11</td><td>-</td><td>0.72</td><td>0.49</td></tr><tr><td>FFHQ 2562</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>StyleGAN2 [42]</td><td>32.13‡</td><td>-</td><td>32.13t</td><td>-</td><td>59M</td><td>3.8</td><td>-</td><td>-</td><td>-</td></tr><tr><td>LDM-4 (ours, 200 steps, 635K)</td><td>26</td><td>-</td><td>26</td><td>1.07</td><td>274M</td><td>4.98</td><td>-</td><td>0.73</td><td>0.50</td></tr><tr><td>ImageNet 2562</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>VQGAN-f-4 (ours, first stage)</td><td>29</td><td>-</td><td>29</td><td>-</td><td>55M</td><td>0.58ft</td><td>-</td><td>-</td><td>-</td></tr><tr><td>VQGAN-f-8 (ours, first stage)</td><td>66</td><td>-</td><td>66</td><td>-</td><td>68M</td><td>1.14ft</td><td>-</td><td>-</td><td>-</td></tr><tr><td>BigGAN-deep [3]t</td><td>128-256</td><td></td><td>128-256</td><td>-</td><td>340M</td><td>6.95</td><td>203.6±2.6</td><td>0.87</td><td>0.28</td></tr><tr><td>ADM [15] (250 steps) +</td><td>916</td><td>-</td><td>916</td><td>0.12</td><td>554M</td><td>10.94</td><td>100.98</td><td>0.69</td><td>0.63</td></tr><tr><td>ADM-G [15] (25 steps) +</td><td>916</td><td>46</td><td>962</td><td>0.7</td><td>608M</td><td>5.58</td><td>-</td><td>0.81</td><td>0.49</td></tr><tr><td>ADM-G [15] (250 steps)†</td><td>916</td><td>46</td><td>962</td><td>0.07</td><td>608M</td><td>4.59</td><td>186.7</td><td>0.82</td><td>0.52</td></tr><tr><td>ADM-G,ADM-U [15] (250 steps)†</td><td>329</td><td>30</td><td>349</td><td>n/a</td><td>n/a</td><td>3.85</td><td>221.72</td><td>0.84</td><td>0.53</td></tr><tr><td>LDM-8-G (ours, 100, 2.9M)</td><td>79</td><td>12</td><td>91</td><td>1.93</td><td>506M</td><td>8.11</td><td>190.4±2.6</td><td>0.83</td><td>0.36</td></tr><tr><td>LDM-8 (ours, 200 ddim steps 2.9M, batch size 64)</td><td>79</td><td>-</td><td>79</td><td>1.9</td><td>395M</td><td>17.41</td><td>72.92</td><td>0.65</td><td>0.62</td></tr><tr><td>LDM-4 (ours, 250 ddim steps 178K, batch size 1200)</td><td>271</td><td>-</td><td>271</td><td>0.7</td><td>400M</td><td>10.56</td><td>103.49±1.24</td><td>0.71</td><td>0.62</td></tr><tr><td>LDM-4-G (ours, 250 ddim steps 178K, batch size 1200, classifier-free guidance [32] scale 1.25)</td><td>271</td><td>-</td><td>271</td><td>0.4</td><td>400M</td><td>3.95</td><td>178.22±2.43</td><td>0.81</td><td>0.55</td></tr><tr><td>LDM-4-G (ours, 250 ddim steps 178K, batch size 1200, classifier-free guidance [32] scale 1.5)</td><td>271</td><td>-</td><td>271</td><td>0.4</td><td>400M</td><td>3.60</td><td>247.67 ±5.59</td><td>0.87</td><td>0.48</td></tr></table>",
            "id": 280,
            "page": 28,
            "text": "Method Generator Compute Classifier Compute Overall Compute Inference Throughput* Nparams FID↓ IS↑ Precision↑ Recall↑  LSUN Churches 2562           StyleGAN2 † 64 - 64 - 59M 3.86 - -  LDM-8 (ours, 100 steps, 410K) 18 - 18 6.80 256M 4.02 - 0.64 0.52  LSUN Bedrooms 2562           ADM † (1000 steps) 232 - 232 0.03 552M 1.9 - 0.66 0.51  LDM-4 (ours, 200 steps, 1.9M) 60 - 55 1.07 274M 2.95 、 0.66 0.48  CelebA-HQ 2562           LDM-4 (ours, 500 steps, 410K) 14.4 - 14.4 0.43 274M 5.11 - 0.72 0.49  FFHQ 2562           StyleGAN2  32.13‡ - 32.13t - 59M 3.8 - -  LDM-4 (ours, 200 steps, 635K) 26 - 26 1.07 274M 4.98 - 0.73 0.50  ImageNet 2562           VQGAN-f-4 (ours, first stage) 29 - 29 - 55M 0.58ft - -  VQGAN-f-8 (ours, first stage) 66 - 66 - 68M 1.14ft - -  BigGAN-deep t 128-256  128-256 - 340M 6.95 203.6±2.6 0.87 0.28  ADM  (250 steps) + 916 - 916 0.12 554M 10.94 100.98 0.69 0.63  ADM-G  (25 steps) + 916 46 962 0.7 608M 5.58 - 0.81 0.49  ADM-G  (250 steps)† 916 46 962 0.07 608M 4.59 186.7 0.82 0.52  ADM-G,ADM-U  (250 steps)† 329 30 349 n/a n/a 3.85 221.72 0.84 0.53  LDM-8-G (ours, 100, 2.9M) 79 12 91 1.93 506M 8.11 190.4±2.6 0.83 0.36  LDM-8 (ours, 200 ddim steps 2.9M, batch size 64) 79 - 79 1.9 395M 17.41 72.92 0.65 0.62  LDM-4 (ours, 250 ddim steps 178K, batch size 1200) 271 - 271 0.7 400M 10.56 103.49±1.24 0.71 0.62  LDM-4-G (ours, 250 ddim steps 178K, batch size 1200, classifier-free guidance  scale 1.25) 271 - 271 0.4 400M 3.95 178.22±2.43 0.81 0.55  LDM-4-G (ours, 250 ddim steps 178K, batch size 1200, classifier-free guidance  scale 1.5) 271 - 271 0.4 400M 3.60 247.67 ±5.59 0.87"
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 1465
                },
                {
                    "x": 2280,
                    "y": 1465
                },
                {
                    "x": 2280,
                    "y": 1650
                },
                {
                    "x": 199,
                    "y": 1650
                }
            ],
            "category": "paragraph",
            "html": "<p id='281' style='font-size:16px'>Table 18. Comparing compute requirements during training and inference throughput with state-of-the-art generative models. Compute<br>during training in V100-days, numbers of competing methods taken from [15] unless stated differently;*: Throughput measured in sam-<br>ples/sec on a single NVIDIA A100;t: Numbers taken from [15] ;‡: Assumed to be trained on 25M train examples; tt : R-FID VS. ImageNet<br>validation set</p>",
            "id": 281,
            "page": 28,
            "text": "Table 18. Comparing compute requirements during training and inference throughput with state-of-the-art generative models. Compute during training in V100-days, numbers of competing methods taken from  unless stated differently;*: Throughput measured in samples/sec on a single NVIDIA A100;t: Numbers taken from  ;‡: Assumed to be trained on 25M train examples; tt : R-FID VS. ImageNet validation set"
        },
        {
            "bounding_box": [
                {
                    "x": 198,
                    "y": 1701
                },
                {
                    "x": 2280,
                    "y": 1701
                },
                {
                    "x": 2280,
                    "y": 2005
                },
                {
                    "x": 198,
                    "y": 2005
                }
            ],
            "category": "paragraph",
            "html": "<p id='282' style='font-size:20px'>In Tab 18 we provide a more detailed analysis on our used compute ressources and compare our best performing models<br>on the CelebA-HQ, FFHQ, LSUN and ImageNet datasets with the recent state of the art models by using their provided<br>numbers, cf. [15]. As they report their used compute in V100 days and we train all our models on a single NVIDIA A100<br>GPU, we convert the A100 days to V100 days by assuming a x2.2 speedup of A100 VS V100 [74]4. To assess sample quality,<br>we additionally report FID scores on the reported datasets. We closely reach the performance of state of the art methods as<br>StyleGAN2 [42] and ADM [15] while significantly reducing the required compute resources.</p>",
            "id": 282,
            "page": 28,
            "text": "In Tab 18 we provide a more detailed analysis on our used compute ressources and compare our best performing models on the CelebA-HQ, FFHQ, LSUN and ImageNet datasets with the recent state of the art models by using their provided numbers, cf. . As they report their used compute in V100 days and we train all our models on a single NVIDIA A100 GPU, we convert the A100 days to V100 days by assuming a x2.2 speedup of A100 VS V100 4. To assess sample quality, we additionally report FID scores on the reported datasets. We closely reach the performance of state of the art methods as StyleGAN2  and ADM  while significantly reducing the required compute resources."
        },
        {
            "bounding_box": [
                {
                    "x": 246,
                    "y": 2929
                },
                {
                    "x": 1694,
                    "y": 2929
                },
                {
                    "x": 1694,
                    "y": 2974
                },
                {
                    "x": 246,
                    "y": 2974
                }
            ],
            "category": "paragraph",
            "html": "<p id='283' style='font-size:14px'>4This factor corresponds to the speedup of the A100 over the V100 for a U-Net, as defined in Fig. 1 in [74]</p>",
            "id": 283,
            "page": 28,
            "text": "4This factor corresponds to the speedup of the A100 over the V100 for a U-Net, as defined in Fig. 1 in "
        },
        {
            "bounding_box": [
                {
                    "x": 1216,
                    "y": 3054
                },
                {
                    "x": 1262,
                    "y": 3054
                },
                {
                    "x": 1262,
                    "y": 3091
                },
                {
                    "x": 1216,
                    "y": 3091
                }
            ],
            "category": "footer",
            "html": "<footer id='284' style='font-size:18px'>28</footer>",
            "id": 284,
            "page": 28,
            "text": "28"
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 302
                },
                {
                    "x": 940,
                    "y": 302
                },
                {
                    "x": 940,
                    "y": 353
                },
                {
                    "x": 203,
                    "y": 353
                }
            ],
            "category": "paragraph",
            "html": "<p id='285' style='font-size:18px'>G. Details on Autoencoder Models</p>",
            "id": 285,
            "page": 29,
            "text": "G. Details on Autoencoder Models"
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 381
                },
                {
                    "x": 2279,
                    "y": 381
                },
                {
                    "x": 2279,
                    "y": 791
                },
                {
                    "x": 199,
                    "y": 791
                }
            ],
            "category": "paragraph",
            "html": "<p id='286' style='font-size:14px'>We train all our autoencoder models in an adversarial manner following [23], such that a patch-based discriminator D⌀<br>is optimized to differentiate original images from reconstructions D(E(x)). To avoid arbitrarily scaled latent spaces, we<br>regularize the latent 2 to be zero centered and obtain small variance by introducing an regularizing loss term Lreg.<br>We investigate two different regularization methods: (i) a low-weighted Kullback-Leibler-term between 36 (2|x) =<br>N(z; ��, Eo2) and a standard normal distribution N(z; 0, 1) as in a standard variational autoencoder [46, 69], and, (ii) regu-<br>larizing the latent space with a vector quantization layer by learning a codebook of 121 different exemplars [96].<br>To obtain high-fidelity reconstructions we only use a very small regularization for both scenarios, i.e. we either weight the<br>KL term by a factor ~ 10-6 or choose a high codebook dimensionality 121.</p>",
            "id": 286,
            "page": 29,
            "text": "We train all our autoencoder models in an adversarial manner following , such that a patch-based discriminator D⌀ is optimized to differentiate original images from reconstructions D(E(x)). To avoid arbitrarily scaled latent spaces, we regularize the latent 2 to be zero centered and obtain small variance by introducing an regularizing loss term Lreg. We investigate two different regularization methods: (i) a low-weighted Kullback-Leibler-term between 36 (2|x) = N(z; ��, Eo2) and a standard normal distribution N(z; 0, 1) as in a standard variational autoencoder , and, (ii) regularizing the latent space with a vector quantization layer by learning a codebook of 121 different exemplars . To obtain high-fidelity reconstructions we only use a very small regularization for both scenarios, i.e. we either weight the KL term by a factor ~ 10-6 or choose a high codebook dimensionality 121."
        },
        {
            "bounding_box": [
                {
                    "x": 254,
                    "y": 783
                },
                {
                    "x": 1323,
                    "y": 783
                },
                {
                    "x": 1323,
                    "y": 831
                },
                {
                    "x": 254,
                    "y": 831
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='287' style='font-size:16px'>The full objective to train the autoencoding model (3, D) reads:</p>",
            "id": 287,
            "page": 29,
            "text": "The full objective to train the autoencoding model (3, D) reads:"
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 1015
                },
                {
                    "x": 2279,
                    "y": 1015
                },
                {
                    "x": 2279,
                    "y": 1168
                },
                {
                    "x": 199,
                    "y": 1168
                }
            ],
            "category": "paragraph",
            "html": "<p id='288' style='font-size:14px'>DM Training in Latent Space Note that for training diffusion models on the learned latent space, we again distinguish two<br>cases when learning p(z) or p(z|y) (Sec. 4.3): (i) For a KL-regularized latent space, we sample 2 = ��(x)+��(x)·E =: E(x),<br>where E ~ N(0, 1). When rescaling the latent, we estimate the component-wise variance</p>",
            "id": 288,
            "page": 29,
            "text": "DM Training in Latent Space Note that for training diffusion models on the learned latent space, we again distinguish two cases when learning p(z) or p(z|y) (Sec. 4.3): (i) For a KL-regularized latent space, we sample 2 = ��(x)+��(x)·E =: E(x), where E ~ N(0, 1). When rescaling the latent, we estimate the component-wise variance"
        },
        {
            "bounding_box": [
                {
                    "x": 198,
                    "y": 1361
                },
                {
                    "x": 2279,
                    "y": 1361
                },
                {
                    "x": 2279,
                    "y": 1534
                },
                {
                    "x": 198,
                    "y": 1534
                }
            ],
            "category": "paragraph",
            "html": "<p id='289' style='font-size:14px'>1 Eb,c,h,w zb,c,h,w. The output of 3 is scaled such that the rescaled latent has<br>from the first batch in the data, where u =<br>bchw<br>E(x). (ii) For a VQ-regularized latent space, we extract 2 before the quantization layer<br>unit standard deviation, i.e. 2 ← 등 =<br>and absorb the quantization operation into the decoder, i.e. it can be interpreted as the first layer of D.</p>",
            "id": 289,
            "page": 29,
            "text": "1 Eb,c,h,w zb,c,h,w. The output of 3 is scaled such that the rescaled latent has from the first batch in the data, where u = bchw E(x). (ii) For a VQ-regularized latent space, we extract 2 before the quantization layer unit standard deviation, i.e. 2 ← 등 = and absorb the quantization operation into the decoder, i.e. it can be interpreted as the first layer of D."
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 1577
                },
                {
                    "x": 921,
                    "y": 1577
                },
                {
                    "x": 921,
                    "y": 1631
                },
                {
                    "x": 203,
                    "y": 1631
                }
            ],
            "category": "paragraph",
            "html": "<p id='290' style='font-size:20px'>H. Additional Qualitative Results</p>",
            "id": 290,
            "page": 29,
            "text": "H. Additional Qualitative Results"
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 1657
                },
                {
                    "x": 2279,
                    "y": 1657
                },
                {
                    "x": 2279,
                    "y": 1914
                },
                {
                    "x": 199,
                    "y": 1914
                }
            ],
            "category": "paragraph",
            "html": "<p id='291' style='font-size:14px'>Finally, we provide additional qualitative results for our landscapes model (Fig. 12, 23, 24 and 25), our class-conditional<br>ImageNet model (Fig. 26 - 27) and our unconditional models for the CelebA-HQ, FFHQ and LSUN datasets (Fig. 28 - 31).<br>Similar as for the inpainting model in Sec. 4.5 we also fine-tuned the semantic landscapes model from Sec. 4.3.2 directly on<br>5122 images and depict qualitative results in Fig. 12 and Fig. 23. For our those models trained on comparably small datasets,<br>we additionally show nearest neighbors in VGG [79] feature space for samples from our models in Fig. 32 - 34.</p>",
            "id": 291,
            "page": 29,
            "text": "Finally, we provide additional qualitative results for our landscapes model (Fig. 12, 23, 24 and 25), our class-conditional ImageNet model (Fig. 26 - 27) and our unconditional models for the CelebA-HQ, FFHQ and LSUN datasets (Fig. 28 - 31). Similar as for the inpainting model in Sec. 4.5 we also fine-tuned the semantic landscapes model from Sec. 4.3.2 directly on 5122 images and depict qualitative results in Fig. 12 and Fig. 23. For our those models trained on comparably small datasets, we additionally show nearest neighbors in VGG  feature space for samples from our models in Fig. 32 - 34."
        },
        {
            "bounding_box": [
                {
                    "x": 1215,
                    "y": 3053
                },
                {
                    "x": 1263,
                    "y": 3053
                },
                {
                    "x": 1263,
                    "y": 3092
                },
                {
                    "x": 1215,
                    "y": 3092
                }
            ],
            "category": "footer",
            "html": "<footer id='292' style='font-size:14px'>29</footer>",
            "id": 292,
            "page": 29,
            "text": "29"
        },
        {
            "bounding_box": [
                {
                    "x": 386,
                    "y": 337
                },
                {
                    "x": 2075,
                    "y": 337
                },
                {
                    "x": 2075,
                    "y": 2829
                },
                {
                    "x": 386,
                    "y": 2829
                }
            ],
            "category": "figure",
            "html": "<figure><img id='293' style='font-size:20px' alt=\"bicubic LDM-BSR\" data-coord=\"top-left:(386,337); bottom-right:(2075,2829)\" /></figure>",
            "id": 293,
            "page": 30,
            "text": "bicubic LDM-BSR"
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 2829
                },
                {
                    "x": 2267,
                    "y": 2829
                },
                {
                    "x": 2267,
                    "y": 2919
                },
                {
                    "x": 204,
                    "y": 2919
                }
            ],
            "category": "caption",
            "html": "<br><caption id='294' style='font-size:14px'>Figure 19. LDM-BSR generalizes to arbitrary inputs and can be used as a general-purpose upsampler, upscaling samples from the LSUN-<br>Cows dataset to 10242 resolution.</caption>",
            "id": 294,
            "page": 30,
            "text": "Figure 19. LDM-BSR generalizes to arbitrary inputs and can be used as a general-purpose upsampler, upscaling samples from the LSUNCows dataset to 10242 resolution."
        },
        {
            "bounding_box": [
                {
                    "x": 1216,
                    "y": 3054
                },
                {
                    "x": 1264,
                    "y": 3054
                },
                {
                    "x": 1264,
                    "y": 3092
                },
                {
                    "x": 1216,
                    "y": 3092
                }
            ],
            "category": "footer",
            "html": "<footer id='295' style='font-size:16px'>30</footer>",
            "id": 295,
            "page": 30,
            "text": "30"
        },
        {
            "bounding_box": [
                {
                    "x": 276,
                    "y": 298
                },
                {
                    "x": 2203,
                    "y": 298
                },
                {
                    "x": 2203,
                    "y": 2897
                },
                {
                    "x": 276,
                    "y": 2897
                }
            ],
            "category": "figure",
            "html": "<figure><img id='296' style='font-size:14px' alt=\"input GT Pixel Baseline #1 Pixel Baseline #2 LDM #1 LDM #2\nSoda\" data-coord=\"top-left:(276,298); bottom-right:(2203,2897)\" /></figure>",
            "id": 296,
            "page": 31,
            "text": "input GT Pixel Baseline #1 Pixel Baseline #2 LDM #1 LDM #2 Soda"
        },
        {
            "bounding_box": [
                {
                    "x": 205,
                    "y": 2937
                },
                {
                    "x": 2270,
                    "y": 2937
                },
                {
                    "x": 2270,
                    "y": 3026
                },
                {
                    "x": 205,
                    "y": 3026
                }
            ],
            "category": "caption",
            "html": "<caption id='297' style='font-size:16px'>Figure 20. Qualitative superresolution comparison of two random samples between LDM-SR and baseline-diffusionmodel in Pixelspace.<br>Evaluated on imagenet validation-set after same amount of training steps.</caption>",
            "id": 297,
            "page": 31,
            "text": "Figure 20. Qualitative superresolution comparison of two random samples between LDM-SR and baseline-diffusionmodel in Pixelspace. Evaluated on imagenet validation-set after same amount of training steps."
        },
        {
            "bounding_box": [
                {
                    "x": 1216,
                    "y": 3055
                },
                {
                    "x": 1259,
                    "y": 3055
                },
                {
                    "x": 1259,
                    "y": 3092
                },
                {
                    "x": 1216,
                    "y": 3092
                }
            ],
            "category": "footer",
            "html": "<br><footer id='298' style='font-size:20px'>31</footer>",
            "id": 298,
            "page": 31,
            "text": "31"
        },
        {
            "bounding_box": [
                {
                    "x": 277,
                    "y": 290
                },
                {
                    "x": 2201,
                    "y": 290
                },
                {
                    "x": 2201,
                    "y": 2903
                },
                {
                    "x": 277,
                    "y": 2903
                }
            ],
            "category": "figure",
            "html": "<figure><img id='299' style='font-size:14px' alt=\"input GT LaMa [88] LDM #1 LDM #2 LDM #3\nA Lacquered Affair Affair Affair\nA Lacquered\nA Lacquered\nA Lacquered Affair Affair\nA Lacquered\nA Lacquered Affair\nGA\" data-coord=\"top-left:(277,290); bottom-right:(2201,2903)\" /></figure>",
            "id": 299,
            "page": 32,
            "text": "input GT LaMa  LDM #1 LDM #2 LDM #3 A Lacquered Affair Affair Affair A Lacquered A Lacquered A Lacquered Affair Affair A Lacquered A Lacquered Affair GA"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2936
                },
                {
                    "x": 2274,
                    "y": 2936
                },
                {
                    "x": 2274,
                    "y": 3028
                },
                {
                    "x": 202,
                    "y": 3028
                }
            ],
            "category": "caption",
            "html": "<caption id='300' style='font-size:16px'>Figure 21. Qualitative results on image inpainting. In contrast to [88], our generative approach enables generation of multiple diverse<br>samples for a given input.</caption>",
            "id": 300,
            "page": 32,
            "text": "Figure 21. Qualitative results on image inpainting. In contrast to , our generative approach enables generation of multiple diverse samples for a given input."
        },
        {
            "bounding_box": [
                {
                    "x": 1215,
                    "y": 3052
                },
                {
                    "x": 1263,
                    "y": 3052
                },
                {
                    "x": 1263,
                    "y": 3093
                },
                {
                    "x": 1215,
                    "y": 3093
                }
            ],
            "category": "footer",
            "html": "<br><footer id='301' style='font-size:20px'>32</footer>",
            "id": 301,
            "page": 32,
            "text": "32"
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 309
                },
                {
                    "x": 2315,
                    "y": 309
                },
                {
                    "x": 2315,
                    "y": 2954
                },
                {
                    "x": 199,
                    "y": 2954
                }
            ],
            "category": "figure",
            "html": "<figure><img id='302' style='font-size:14px' alt=\"input result input result\nR GOODS R GOODS\nSHIRT SHOE S SHIRT18 & SHOE C\n3 E KS 88 3 BADKS\nLUGG EYEV EAR LUGG/LE CEYE\nS\nRANGE\nbavelli bavelli\n6 aue\n  !ルハ a\n········\n0000000 COOOOOS\n0000000 0000000 REDAL\n'California 'California\n7ZWK177 7ZWK177\" data-coord=\"top-left:(199,309); bottom-right:(2315,2954)\" /></figure>",
            "id": 302,
            "page": 33,
            "text": "input result input result R GOODS R GOODS SHIRT SHOE S SHIRT18 & SHOE C 3 E KS 88 3 BADKS LUGG EYEV EAR LUGG/LE CEYE S RANGE bavelli bavelli 6 aue   !ルハ a ········ 0000000 COOOOOS 0000000 0000000 REDAL \"California \"California 7ZWK177 7ZWK177"
        },
        {
            "bounding_box": [
                {
                    "x": 731,
                    "y": 3002
                },
                {
                    "x": 1746,
                    "y": 3002
                },
                {
                    "x": 1746,
                    "y": 3041
                },
                {
                    "x": 731,
                    "y": 3041
                }
            ],
            "category": "caption",
            "html": "<caption id='303' style='font-size:18px'>Figure 22. More qualitative results on object removal as in Fig. 11.</caption>",
            "id": 303,
            "page": 33,
            "text": "Figure 22. More qualitative results on object removal as in Fig. 11."
        },
        {
            "bounding_box": [
                {
                    "x": 1215,
                    "y": 3055
                },
                {
                    "x": 1261,
                    "y": 3055
                },
                {
                    "x": 1261,
                    "y": 3092
                },
                {
                    "x": 1215,
                    "y": 3092
                }
            ],
            "category": "footer",
            "html": "<br><footer id='304' style='font-size:18px'>33</footer>",
            "id": 304,
            "page": 33,
            "text": "33"
        },
        {
            "bounding_box": [
                {
                    "x": 479,
                    "y": 398
                },
                {
                    "x": 1994,
                    "y": 398
                },
                {
                    "x": 1994,
                    "y": 2758
                },
                {
                    "x": 479,
                    "y": 2758
                }
            ],
            "category": "figure",
            "html": "<figure><img id='305' style='font-size:20px' alt=\"Semantic Synthesis on Flickr-Landscapes [23] (5122 finetuning)\" data-coord=\"top-left:(479,398); bottom-right:(1994,2758)\" /></figure>",
            "id": 305,
            "page": 34,
            "text": "Semantic Synthesis on Flickr-Landscapes  (5122 finetuning)"
        },
        {
            "bounding_box": [
                {
                    "x": 375,
                    "y": 2798
                },
                {
                    "x": 2104,
                    "y": 2798
                },
                {
                    "x": 2104,
                    "y": 2853
                },
                {
                    "x": 375,
                    "y": 2853
                }
            ],
            "category": "caption",
            "html": "<caption id='306' style='font-size:14px'>Figure 23. Convolutional samples from the semantic landscapes model as in Sec. 4.3.2, finetuned on 5122 images.</caption>",
            "id": 306,
            "page": 34,
            "text": "Figure 23. Convolutional samples from the semantic landscapes model as in Sec. 4.3.2, finetuned on 5122 images."
        },
        {
            "bounding_box": [
                {
                    "x": 1214,
                    "y": 3053
                },
                {
                    "x": 1263,
                    "y": 3053
                },
                {
                    "x": 1263,
                    "y": 3093
                },
                {
                    "x": 1214,
                    "y": 3093
                }
            ],
            "category": "footer",
            "html": "<footer id='307' style='font-size:16px'>34</footer>",
            "id": 307,
            "page": 34,
            "text": "34"
        },
        {
            "bounding_box": [
                {
                    "x": 197,
                    "y": 555
                },
                {
                    "x": 2278,
                    "y": 555
                },
                {
                    "x": 2278,
                    "y": 2633
                },
                {
                    "x": 197,
                    "y": 2633
                }
            ],
            "category": "figure",
            "html": "<figure><img id='308' alt=\"\" data-coord=\"top-left:(197,555); bottom-right:(2278,2633)\" /></figure>",
            "id": 308,
            "page": 35,
            "text": ""
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 2642
                },
                {
                    "x": 2275,
                    "y": 2642
                },
                {
                    "x": 2275,
                    "y": 2731
                },
                {
                    "x": 203,
                    "y": 2731
                }
            ],
            "category": "caption",
            "html": "<br><caption id='309' style='font-size:14px'>Figure 24. A LDM trained on 2562 resolution can generalize to larger resolution for spatially conditioned tasks such as semantic synthesis<br>of landscape images. See Sec. 4.3.2.</caption>",
            "id": 309,
            "page": 35,
            "text": "Figure 24. A LDM trained on 2562 resolution can generalize to larger resolution for spatially conditioned tasks such as semantic synthesis of landscape images. See Sec. 4.3.2."
        },
        {
            "bounding_box": [
                {
                    "x": 1215,
                    "y": 3053
                },
                {
                    "x": 1263,
                    "y": 3053
                },
                {
                    "x": 1263,
                    "y": 3092
                },
                {
                    "x": 1215,
                    "y": 3092
                }
            ],
            "category": "footer",
            "html": "<footer id='310' style='font-size:18px'>35</footer>",
            "id": 310,
            "page": 35,
            "text": "35"
        },
        {
            "bounding_box": [
                {
                    "x": 847,
                    "y": 312
                },
                {
                    "x": 1627,
                    "y": 312
                },
                {
                    "x": 1627,
                    "y": 361
                },
                {
                    "x": 847,
                    "y": 361
                }
            ],
            "category": "caption",
            "html": "<caption id='311' style='font-size:18px'>Semantic Synthesis on Flickr-Landscapes [23]</caption>",
            "id": 311,
            "page": 36,
            "text": "Semantic Synthesis on Flickr-Landscapes "
        },
        {
            "bounding_box": [
                {
                    "x": 415,
                    "y": 347
                },
                {
                    "x": 2071,
                    "y": 347
                },
                {
                    "x": 2071,
                    "y": 2910
                },
                {
                    "x": 415,
                    "y": 2910
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='312' alt=\"\" data-coord=\"top-left:(415,347); bottom-right:(2071,2910)\" /></figure>",
            "id": 312,
            "page": 36,
            "text": ""
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2951
                },
                {
                    "x": 2282,
                    "y": 2951
                },
                {
                    "x": 2282,
                    "y": 3093
                },
                {
                    "x": 201,
                    "y": 3093
                }
            ],
            "category": "caption",
            "html": "<caption id='313' style='font-size:14px'>Figure 25. When provided a semantic map as conditioning, our LDMs generalize to substantially larger resolutions than those seen during<br>training. Although this model was trained on inputs of size 2562 it can be used to create high-resolution samples as the ones shown here,<br>which are of resolution 1024 x 384. 36</caption>",
            "id": 313,
            "page": 36,
            "text": "Figure 25. When provided a semantic map as conditioning, our LDMs generalize to substantially larger resolutions than those seen during training. Although this model was trained on inputs of size 2562 it can be used to create high-resolution samples as the ones shown here, which are of resolution 1024 x 384. 36"
        },
        {
            "bounding_box": [
                {
                    "x": 742,
                    "y": 315
                },
                {
                    "x": 1739,
                    "y": 315
                },
                {
                    "x": 1739,
                    "y": 359
                },
                {
                    "x": 742,
                    "y": 359
                }
            ],
            "category": "caption",
            "html": "<caption id='314' style='font-size:18px'>Random class conditional samples on the ImageNet dataset</caption>",
            "id": 314,
            "page": 37,
            "text": "Random class conditional samples on the ImageNet dataset"
        },
        {
            "bounding_box": [
                {
                    "x": 303,
                    "y": 362
                },
                {
                    "x": 2176,
                    "y": 362
                },
                {
                    "x": 2176,
                    "y": 2872
                },
                {
                    "x": 303,
                    "y": 2872
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='315' style='font-size:14px' alt=\"III\nCardens\nIntionalth PN\no anontenstrand'a\" data-coord=\"top-left:(303,362); bottom-right:(2176,2872)\" /></figure>",
            "id": 315,
            "page": 37,
            "text": "III Cardens Intionalth PN o anontenstrand'a"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2910
                },
                {
                    "x": 2276,
                    "y": 2910
                },
                {
                    "x": 2276,
                    "y": 3000
                },
                {
                    "x": 202,
                    "y": 3000
                }
            ],
            "category": "caption",
            "html": "<caption id='316' style='font-size:16px'>Figure 26. Random samples from LDM-4 trained on the ImageNet dataset. Sampled with classifier-free guidance [32] scale s = 5.0 and<br>200 DDIM steps with 7 = 1.0.</caption>",
            "id": 316,
            "page": 37,
            "text": "Figure 26. Random samples from LDM-4 trained on the ImageNet dataset. Sampled with classifier-free guidance  scale s = 5.0 and 200 DDIM steps with 7 = 1.0."
        },
        {
            "bounding_box": [
                {
                    "x": 1214,
                    "y": 3053
                },
                {
                    "x": 1264,
                    "y": 3053
                },
                {
                    "x": 1264,
                    "y": 3093
                },
                {
                    "x": 1214,
                    "y": 3093
                }
            ],
            "category": "footer",
            "html": "<footer id='317' style='font-size:20px'>37</footer>",
            "id": 317,
            "page": 37,
            "text": "37"
        },
        {
            "bounding_box": [
                {
                    "x": 743,
                    "y": 316
                },
                {
                    "x": 1737,
                    "y": 316
                },
                {
                    "x": 1737,
                    "y": 359
                },
                {
                    "x": 743,
                    "y": 359
                }
            ],
            "category": "caption",
            "html": "<caption id='318' style='font-size:20px'>Random class conditional samples on the ImageNet dataset</caption>",
            "id": 318,
            "page": 38,
            "text": "Random class conditional samples on the ImageNet dataset"
        },
        {
            "bounding_box": [
                {
                    "x": 303,
                    "y": 360
                },
                {
                    "x": 2175,
                    "y": 360
                },
                {
                    "x": 2175,
                    "y": 2874
                },
                {
                    "x": 303,
                    "y": 2874
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='319' style='font-size:16px' alt=\"LEMGCARION\" data-coord=\"top-left:(303,360); bottom-right:(2175,2874)\" /></figure>",
            "id": 319,
            "page": 38,
            "text": "LEMGCARION"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2910
                },
                {
                    "x": 2276,
                    "y": 2910
                },
                {
                    "x": 2276,
                    "y": 3001
                },
                {
                    "x": 202,
                    "y": 3001
                }
            ],
            "category": "caption",
            "html": "<caption id='320' style='font-size:14px'>Figure 27. Random samples from LDM-4 trained on the ImageNet dataset. Sampled with classifier-free guidance [32] scale s = 3.0 and<br>200 DDIM steps with 7 = 1.0.</caption>",
            "id": 320,
            "page": 38,
            "text": "Figure 27. Random samples from LDM-4 trained on the ImageNet dataset. Sampled with classifier-free guidance  scale s = 3.0 and 200 DDIM steps with 7 = 1.0."
        },
        {
            "bounding_box": [
                {
                    "x": 1214,
                    "y": 3053
                },
                {
                    "x": 1263,
                    "y": 3053
                },
                {
                    "x": 1263,
                    "y": 3093
                },
                {
                    "x": 1214,
                    "y": 3093
                }
            ],
            "category": "footer",
            "html": "<footer id='321' style='font-size:16px'>38</footer>",
            "id": 321,
            "page": 38,
            "text": "38"
        },
        {
            "bounding_box": [
                {
                    "x": 869,
                    "y": 372
                },
                {
                    "x": 1611,
                    "y": 372
                },
                {
                    "x": 1611,
                    "y": 417
                },
                {
                    "x": 869,
                    "y": 417
                }
            ],
            "category": "caption",
            "html": "<caption id='322' style='font-size:18px'>Random samples on the CelebA-HQ dataset</caption>",
            "id": 322,
            "page": 39,
            "text": "Random samples on the CelebA-HQ dataset"
        },
        {
            "bounding_box": [
                {
                    "x": 303,
                    "y": 421
                },
                {
                    "x": 2175,
                    "y": 421
                },
                {
                    "x": 2175,
                    "y": 2777
                },
                {
                    "x": 303,
                    "y": 2777
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='323' style='font-size:14px' alt=\"て\nDE\n��\" data-coord=\"top-left:(303,421); bottom-right:(2175,2777)\" /></figure>",
            "id": 323,
            "page": 39,
            "text": "て DE ��"
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 2812
                },
                {
                    "x": 2275,
                    "y": 2812
                },
                {
                    "x": 2275,
                    "y": 2904
                },
                {
                    "x": 199,
                    "y": 2904
                }
            ],
            "category": "caption",
            "html": "<caption id='324' style='font-size:16px'>Figure 28. Random samples of our best performing model LDM-4 on the CelebA-HQ dataset. Sampled with 500 DDIM steps and 7 = 0<br>(FID = 5.15).</caption>",
            "id": 324,
            "page": 39,
            "text": "Figure 28. Random samples of our best performing model LDM-4 on the CelebA-HQ dataset. Sampled with 500 DDIM steps and 7 = 0 (FID = 5.15)."
        },
        {
            "bounding_box": [
                {
                    "x": 1215,
                    "y": 3053
                },
                {
                    "x": 1263,
                    "y": 3053
                },
                {
                    "x": 1263,
                    "y": 3093
                },
                {
                    "x": 1215,
                    "y": 3093
                }
            ],
            "category": "footer",
            "html": "<footer id='325' style='font-size:20px'>39</footer>",
            "id": 325,
            "page": 39,
            "text": "39"
        },
        {
            "bounding_box": [
                {
                    "x": 914,
                    "y": 374
                },
                {
                    "x": 1564,
                    "y": 374
                },
                {
                    "x": 1564,
                    "y": 418
                },
                {
                    "x": 914,
                    "y": 418
                }
            ],
            "category": "caption",
            "html": "<caption id='326' style='font-size:16px'>Random samples on the FFHQ dataset</caption>",
            "id": 326,
            "page": 40,
            "text": "Random samples on the FFHQ dataset"
        },
        {
            "bounding_box": [
                {
                    "x": 301,
                    "y": 399
                },
                {
                    "x": 2176,
                    "y": 399
                },
                {
                    "x": 2176,
                    "y": 2780
                },
                {
                    "x": 301,
                    "y": 2780
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='327' alt=\"\" data-coord=\"top-left:(301,399); bottom-right:(2176,2780)\" /></figure>",
            "id": 327,
            "page": 40,
            "text": ""
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 2811
                },
                {
                    "x": 2276,
                    "y": 2811
                },
                {
                    "x": 2276,
                    "y": 2903
                },
                {
                    "x": 200,
                    "y": 2903
                }
            ],
            "category": "caption",
            "html": "<caption id='328' style='font-size:14px'>Figure 29. Random samples of our best performing model LDM-4 on the FFHQ dataset. Sampled with 200 DDIM steps and 7 = 1 (FID<br>= 4.98).</caption>",
            "id": 328,
            "page": 40,
            "text": "Figure 29. Random samples of our best performing model LDM-4 on the FFHQ dataset. Sampled with 200 DDIM steps and 7 = 1 (FID = 4.98)."
        },
        {
            "bounding_box": [
                {
                    "x": 1214,
                    "y": 3054
                },
                {
                    "x": 1264,
                    "y": 3054
                },
                {
                    "x": 1264,
                    "y": 3092
                },
                {
                    "x": 1214,
                    "y": 3092
                }
            ],
            "category": "footer",
            "html": "<footer id='329' style='font-size:20px'>40</footer>",
            "id": 329,
            "page": 40,
            "text": "40"
        },
        {
            "bounding_box": [
                {
                    "x": 828,
                    "y": 374
                },
                {
                    "x": 1651,
                    "y": 374
                },
                {
                    "x": 1651,
                    "y": 416
                },
                {
                    "x": 828,
                    "y": 416
                }
            ],
            "category": "caption",
            "html": "<caption id='330' style='font-size:20px'>Random samples on the LSUN-Churches dataset</caption>",
            "id": 330,
            "page": 41,
            "text": "Random samples on the LSUN-Churches dataset"
        },
        {
            "bounding_box": [
                {
                    "x": 302,
                    "y": 414
                },
                {
                    "x": 2175,
                    "y": 414
                },
                {
                    "x": 2175,
                    "y": 2780
                },
                {
                    "x": 302,
                    "y": 2780
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='331' style='font-size:14px' alt=\"209028001 1003 zootatestook\n675\nstonferslock.com 158628865 COOD in\nsteck\ntterstock\nPAS6863S\nhunterstsk\" data-coord=\"top-left:(302,414); bottom-right:(2175,2780)\" /></figure>",
            "id": 331,
            "page": 41,
            "text": "209028001 1003 zootatestook 675 stonferslock.com 158628865 COOD in steck tterstock PAS6863S hunterstsk"
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 2813
                },
                {
                    "x": 2275,
                    "y": 2813
                },
                {
                    "x": 2275,
                    "y": 2903
                },
                {
                    "x": 204,
                    "y": 2903
                }
            ],
            "category": "caption",
            "html": "<caption id='332' style='font-size:16px'>Figure 30. Random samples of our best performing model LDM-8 on the LSUN-Churches dataset. Sampled with 200 DDIM steps and<br>7 = 0 (FID = 4.48).</caption>",
            "id": 332,
            "page": 41,
            "text": "Figure 30. Random samples of our best performing model LDM-8 on the LSUN-Churches dataset. Sampled with 200 DDIM steps and 7 = 0 (FID = 4.48)."
        },
        {
            "bounding_box": [
                {
                    "x": 1214,
                    "y": 3054
                },
                {
                    "x": 1260,
                    "y": 3054
                },
                {
                    "x": 1260,
                    "y": 3093
                },
                {
                    "x": 1214,
                    "y": 3093
                }
            ],
            "category": "footer",
            "html": "<footer id='333' style='font-size:18px'>41</footer>",
            "id": 333,
            "page": 41,
            "text": "41"
        },
        {
            "bounding_box": [
                {
                    "x": 821,
                    "y": 372
                },
                {
                    "x": 1658,
                    "y": 372
                },
                {
                    "x": 1658,
                    "y": 417
                },
                {
                    "x": 821,
                    "y": 417
                }
            ],
            "category": "caption",
            "html": "<caption id='334' style='font-size:20px'>Random samples on the LSUN-Bedrooms dataset</caption>",
            "id": 334,
            "page": 42,
            "text": "Random samples on the LSUN-Bedrooms dataset"
        },
        {
            "bounding_box": [
                {
                    "x": 303,
                    "y": 417
                },
                {
                    "x": 2175,
                    "y": 417
                },
                {
                    "x": 2175,
                    "y": 2780
                },
                {
                    "x": 303,
                    "y": 2780
                }
            ],
            "category": "figure",
            "html": "<figure><img id='335' alt=\"\" data-coord=\"top-left:(303,417); bottom-right:(2175,2780)\" /></figure>",
            "id": 335,
            "page": 42,
            "text": ""
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2811
                },
                {
                    "x": 2277,
                    "y": 2811
                },
                {
                    "x": 2277,
                    "y": 2904
                },
                {
                    "x": 202,
                    "y": 2904
                }
            ],
            "category": "caption",
            "html": "<caption id='336' style='font-size:14px'>Figure 31. Random samples of our best performing model LDM-4 on the LSUN-Bedrooms dataset. Sampled with 200 DDIM steps and<br>7 = 1 (FID = 2.95).</caption>",
            "id": 336,
            "page": 42,
            "text": "Figure 31. Random samples of our best performing model LDM-4 on the LSUN-Bedrooms dataset. Sampled with 200 DDIM steps and 7 = 1 (FID = 2.95)."
        },
        {
            "bounding_box": [
                {
                    "x": 1213,
                    "y": 3053
                },
                {
                    "x": 1264,
                    "y": 3053
                },
                {
                    "x": 1264,
                    "y": 3093
                },
                {
                    "x": 1213,
                    "y": 3093
                }
            ],
            "category": "footer",
            "html": "<footer id='337' style='font-size:16px'>42</footer>",
            "id": 337,
            "page": 42,
            "text": "42"
        },
        {
            "bounding_box": [
                {
                    "x": 854,
                    "y": 686
                },
                {
                    "x": 1625,
                    "y": 686
                },
                {
                    "x": 1625,
                    "y": 733
                },
                {
                    "x": 854,
                    "y": 733
                }
            ],
            "category": "caption",
            "html": "<caption id='338' style='font-size:20px'>Nearest Neighbors on the CelebA-HQ dataset</caption>",
            "id": 338,
            "page": 43,
            "text": "Nearest Neighbors on the CelebA-HQ dataset"
        },
        {
            "bounding_box": [
                {
                    "x": 301,
                    "y": 740
                },
                {
                    "x": 2179,
                    "y": 740
                },
                {
                    "x": 2179,
                    "y": 2462
                },
                {
                    "x": 301,
                    "y": 2462
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='339' style='font-size:14px' alt=\"U\nU t\nna\nobnR\" data-coord=\"top-left:(301,740); bottom-right:(2179,2462)\" /></figure>",
            "id": 339,
            "page": 43,
            "text": "U U t na obnR"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2495
                },
                {
                    "x": 2276,
                    "y": 2495
                },
                {
                    "x": 2276,
                    "y": 2589
                },
                {
                    "x": 202,
                    "y": 2589
                }
            ],
            "category": "caption",
            "html": "<caption id='340' style='font-size:16px'>Figure 32. Nearest neighbors of our best CelebA-HQ model, computed in the feature space of a VGG-16 [79]. The leftmost sample is<br>from our model. The remaining samples in each row are its 10 nearest neighbors.</caption>",
            "id": 340,
            "page": 43,
            "text": "Figure 32. Nearest neighbors of our best CelebA-HQ model, computed in the feature space of a VGG-16 . The leftmost sample is from our model. The remaining samples in each row are its 10 nearest neighbors."
        },
        {
            "bounding_box": [
                {
                    "x": 1213,
                    "y": 3053
                },
                {
                    "x": 1263,
                    "y": 3053
                },
                {
                    "x": 1263,
                    "y": 3092
                },
                {
                    "x": 1213,
                    "y": 3092
                }
            ],
            "category": "footer",
            "html": "<footer id='341' style='font-size:18px'>43</footer>",
            "id": 341,
            "page": 43,
            "text": "43"
        },
        {
            "bounding_box": [
                {
                    "x": 901,
                    "y": 687
                },
                {
                    "x": 1576,
                    "y": 687
                },
                {
                    "x": 1576,
                    "y": 732
                },
                {
                    "x": 901,
                    "y": 732
                }
            ],
            "category": "caption",
            "html": "<caption id='342' style='font-size:16px'>Nearest Neighbors on the FFHQ dataset</caption>",
            "id": 342,
            "page": 44,
            "text": "Nearest Neighbors on the FFHQ dataset"
        },
        {
            "bounding_box": [
                {
                    "x": 304,
                    "y": 745
                },
                {
                    "x": 2176,
                    "y": 745
                },
                {
                    "x": 2176,
                    "y": 2459
                },
                {
                    "x": 304,
                    "y": 2459
                }
            ],
            "category": "figure",
            "html": "<figure><img id='343' alt=\"\" data-coord=\"top-left:(304,745); bottom-right:(2176,2459)\" /></figure>",
            "id": 343,
            "page": 44,
            "text": ""
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2495
                },
                {
                    "x": 2275,
                    "y": 2495
                },
                {
                    "x": 2275,
                    "y": 2588
                },
                {
                    "x": 201,
                    "y": 2588
                }
            ],
            "category": "caption",
            "html": "<caption id='344' style='font-size:14px'>Figure 33. Nearest neighbors of our best FFHQ model, computed in the feature space of a VGG-16 [79]. The leftmost sample is from our<br>model. The remaining samples in each row are its 10 nearest neighbors.</caption>",
            "id": 344,
            "page": 44,
            "text": "Figure 33. Nearest neighbors of our best FFHQ model, computed in the feature space of a VGG-16 . The leftmost sample is from our model. The remaining samples in each row are its 10 nearest neighbors."
        },
        {
            "bounding_box": [
                {
                    "x": 1214,
                    "y": 3054
                },
                {
                    "x": 1263,
                    "y": 3054
                },
                {
                    "x": 1263,
                    "y": 3091
                },
                {
                    "x": 1214,
                    "y": 3091
                }
            ],
            "category": "footer",
            "html": "<footer id='345' style='font-size:20px'>44</footer>",
            "id": 345,
            "page": 44,
            "text": "44"
        },
        {
            "bounding_box": [
                {
                    "x": 300,
                    "y": 677
                },
                {
                    "x": 2181,
                    "y": 677
                },
                {
                    "x": 2181,
                    "y": 2460
                },
                {
                    "x": 300,
                    "y": 2460
                }
            ],
            "category": "figure",
            "html": "<figure><img id='346' style='font-size:16px' alt=\"Nearest Neighbors on the LSUN-Churches dataset\" data-coord=\"top-left:(300,677); bottom-right:(2181,2460)\" /></figure>",
            "id": 346,
            "page": 45,
            "text": "Nearest Neighbors on the LSUN-Churches dataset"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2494
                },
                {
                    "x": 2277,
                    "y": 2494
                },
                {
                    "x": 2277,
                    "y": 2589
                },
                {
                    "x": 201,
                    "y": 2589
                }
            ],
            "category": "caption",
            "html": "<caption id='347' style='font-size:14px'>Figure 34. Nearest neighbors of our best LSUN-Churches model, computed in the feature space of a VGG-16 [79]. The leftmost sample<br>is from our model. The remaining samples in each row are its 10 nearest neighbors.</caption>",
            "id": 347,
            "page": 45,
            "text": "Figure 34. Nearest neighbors of our best LSUN-Churches model, computed in the feature space of a VGG-16 . The leftmost sample is from our model. The remaining samples in each row are its 10 nearest neighbors."
        },
        {
            "bounding_box": [
                {
                    "x": 1214,
                    "y": 3053
                },
                {
                    "x": 1264,
                    "y": 3053
                },
                {
                    "x": 1264,
                    "y": 3092
                },
                {
                    "x": 1214,
                    "y": 3092
                }
            ],
            "category": "footer",
            "html": "<footer id='348' style='font-size:20px'>45</footer>",
            "id": 348,
            "page": 45,
            "text": "45"
        }
    ]
}