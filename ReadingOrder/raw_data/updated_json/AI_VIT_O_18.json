{
    "id": "629b4e1c-0f92-11ef-8230-426932df3dcf",
    "pdf_path": "/root/data/pdf/1805.00932v1.pdf",
    "elements": [
        {
            "bounding_box": [
                {
                    "x": 809,
                    "y": 499
                },
                {
                    "x": 1740,
                    "y": 499
                },
                {
                    "x": 1740,
                    "y": 645
                },
                {
                    "x": 809,
                    "y": 645
                }
            ],
            "category": "paragraph",
            "html": "<p id='0' style='font-size:22px'>Exploring the Limits of<br>Weakly Supervised Pretraining</p>",
            "id": 0,
            "page": 1,
            "text": "Exploring the Limits of\nWeakly Supervised Pretraining"
        },
        {
            "bounding_box": [
                {
                    "x": 552,
                    "y": 732
                },
                {
                    "x": 2006,
                    "y": 732
                },
                {
                    "x": 2006,
                    "y": 834
                },
                {
                    "x": 552,
                    "y": 834
                }
            ],
            "category": "paragraph",
            "html": "<p id='1' style='font-size:16px'>Dhruv Mahajan Ross Girshick Vignesh Ramanathan Kaiming He<br>Manohar Paluri Yixuan Li Ashwin Bharambe Laurens van der Maaten</p>",
            "id": 1,
            "page": 1,
            "text": "Dhruv Mahajan Ross Girshick Vignesh Ramanathan Kaiming He\nManohar Paluri Yixuan Li Ashwin Bharambe Laurens van der Maaten"
        },
        {
            "bounding_box": [
                {
                    "x": 1191,
                    "y": 874
                },
                {
                    "x": 1358,
                    "y": 874
                },
                {
                    "x": 1358,
                    "y": 917
                },
                {
                    "x": 1191,
                    "y": 917
                }
            ],
            "category": "paragraph",
            "html": "<p id='2' style='font-size:14px'>Facebook</p>",
            "id": 2,
            "page": 1,
            "text": "Facebook"
        },
        {
            "bounding_box": [
                {
                    "x": 664,
                    "y": 988
                },
                {
                    "x": 1889,
                    "y": 988
                },
                {
                    "x": 1889,
                    "y": 1680
                },
                {
                    "x": 664,
                    "y": 1680
                }
            ],
            "category": "paragraph",
            "html": "<p id='3' style='font-size:14px'>Abstract. State-of-the-art visual perception models for a wide range<br>of tasks rely on supervised pretraining. ImageNet classification is the de<br>facto pretraining task for these models. Yet, ImageNet is now nearly ten<br>years old and is by modern standards \"small\" Even SO, relatively little is<br>known about the behavior of pretraining with datasets that are multiple<br>orders of magnitude larger. The reasons are obvious: such datasets are<br>difficult to collect and annotate. In this paper, we present a unique study<br>of transfer learning with large convolutional networks trained to predict<br>hashtags on billions of social media images. Our experiments demon-<br>strate that training for large-scale hashtag prediction leads to excellent<br>results. We show improvements on several image classification and object<br>detection tasks, and report the highest ImageNet-1k single-crop, top-1<br>accuracy to date: 85.4% (97.6% top-5). We also perform extensive ex-<br>periments that provide novel empirical data on the relationship between<br>large-scale pretraining and transfer learning performance.</p>",
            "id": 3,
            "page": 1,
            "text": "Abstract. State-of-the-art visual perception models for a wide range\nof tasks rely on supervised pretraining. ImageNet classification is the de\nfacto pretraining task for these models. Yet, ImageNet is now nearly ten\nyears old and is by modern standards \"small\" Even SO, relatively little is\nknown about the behavior of pretraining with datasets that are multiple\norders of magnitude larger. The reasons are obvious: such datasets are\ndifficult to collect and annotate. In this paper, we present a unique study\nof transfer learning with large convolutional networks trained to predict\nhashtags on billions of social media images. Our experiments demon-\nstrate that training for large-scale hashtag prediction leads to excellent\nresults. We show improvements on several image classification and object\ndetection tasks, and report the highest ImageNet-1k single-crop, top-1\naccuracy to date: 85.4% (97.6% top-5). We also perform extensive ex-\nperiments that provide novel empirical data on the relationship between\nlarge-scale pretraining and transfer learning performance."
        },
        {
            "bounding_box": [
                {
                    "x": 551,
                    "y": 1755
                },
                {
                    "x": 952,
                    "y": 1755
                },
                {
                    "x": 952,
                    "y": 1811
                },
                {
                    "x": 551,
                    "y": 1811
                }
            ],
            "category": "paragraph",
            "html": "<p id='4' style='font-size:20px'>1 Introduction</p>",
            "id": 4,
            "page": 1,
            "text": "1 Introduction"
        },
        {
            "bounding_box": [
                {
                    "x": 547,
                    "y": 1855
                },
                {
                    "x": 2005,
                    "y": 1855
                },
                {
                    "x": 2005,
                    "y": 2254
                },
                {
                    "x": 547,
                    "y": 2254
                }
            ],
            "category": "paragraph",
            "html": "<p id='5' style='font-size:16px'>Nearly all state-of-the-art visual perception algorithms rely on the same formula:<br>(1) pretrain a convolutional network on a large, manually annotated image clas-<br>sification dataset and (2) finetune the network on a smaller, task-specific dataset.<br>This formula [1,2,3] has been in wide use for several years and led to impres-<br>sive improvements on numerous tasks. Examples include: object detection [1,4],<br>semantic segmentation [5,6], human pose estimation [7,8], video recognition [9],<br>monocular depth estimation [10], and SO on. In fact, it is SO effective that it<br>would now be considered foolhardy not to use supervised pretraining.</p>",
            "id": 5,
            "page": 1,
            "text": "Nearly all state-of-the-art visual perception algorithms rely on the same formula:\n(1) pretrain a convolutional network on a large, manually annotated image clas-\nsification dataset and (2) finetune the network on a smaller, task-specific dataset.\nThis formula [1,2,3] has been in wide use for several years and led to impres-\nsive improvements on numerous tasks. Examples include: object detection [1,4],\nsemantic segmentation [5,6], human pose estimation [7,8], video recognition [9],\nmonocular depth estimation [10], and SO on. In fact, it is SO effective that it\nwould now be considered foolhardy not to use supervised pretraining."
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 2256
                },
                {
                    "x": 2006,
                    "y": 2256
                },
                {
                    "x": 2006,
                    "y": 2705
                },
                {
                    "x": 545,
                    "y": 2705
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='6' style='font-size:18px'>The ImageNet dataset [11] is the de facto pretraining dataset. While there are<br>studies analyzing the effects of various ImageNet pretraining factors on transfer<br>learning (e.g., [12,13]) or the use of different datasets that are of the same size<br>magnitude as ImageNet (e.g., [14,15]), relatively little is known about pretraining<br>on datasets that are multiple orders of magnitude larger ([16,17] are the largest<br>studies to date). The reasons for this are numerous: few such datasets exist,<br>building new datasets is labor intensive, and large computational resources are<br>needed to conduct experiments. Yet, given the central role of pretraining it is<br>important to expand our scientific knowledge in this domain.</p>",
            "id": 6,
            "page": 1,
            "text": "The ImageNet dataset [11] is the de facto pretraining dataset. While there are\nstudies analyzing the effects of various ImageNet pretraining factors on transfer\nlearning (e.g., [12,13]) or the use of different datasets that are of the same size\nmagnitude as ImageNet (e.g., [14,15]), relatively little is known about pretraining\non datasets that are multiple orders of magnitude larger ([16,17] are the largest\nstudies to date). The reasons for this are numerous: few such datasets exist,\nbuilding new datasets is labor intensive, and large computational resources are\nneeded to conduct experiments. Yet, given the central role of pretraining it is\nimportant to expand our scientific knowledge in this domain."
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 2705
                },
                {
                    "x": 2005,
                    "y": 2705
                },
                {
                    "x": 2005,
                    "y": 2804
                },
                {
                    "x": 545,
                    "y": 2804
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='7' style='font-size:18px'>This paper tries to address this complex issue by studying an unexplored data<br>regime: billions of images \"labeled\" in the wild with social media hashtags. This</p>",
            "id": 7,
            "page": 1,
            "text": "This paper tries to address this complex issue by studying an unexplored data\nregime: billions of images \"labeled\" in the wild with social media hashtags. This"
        },
        {
            "bounding_box": [
                {
                    "x": 64,
                    "y": 786
                },
                {
                    "x": 150,
                    "y": 786
                },
                {
                    "x": 150,
                    "y": 2241
                },
                {
                    "x": 64,
                    "y": 2241
                }
            ],
            "category": "footer",
            "html": "<br><footer id='8' style='font-size:14px'>2018<br>May<br>2<br>[cs.CV]<br>arXiv:1805.00932v1</footer>",
            "id": 8,
            "page": 1,
            "text": "2018\nMay\n2\n[cs.CV]\narXiv:1805.00932v1"
        },
        {
            "bounding_box": [
                {
                    "x": 546,
                    "y": 409
                },
                {
                    "x": 943,
                    "y": 409
                },
                {
                    "x": 943,
                    "y": 456
                },
                {
                    "x": 546,
                    "y": 456
                }
            ],
            "category": "header",
            "html": "<header id='9' style='font-size:14px'>2 Mahajan et al.</header>",
            "id": 9,
            "page": 2,
            "text": "2 Mahajan et al."
        },
        {
            "bounding_box": [
                {
                    "x": 544,
                    "y": 514
                },
                {
                    "x": 2005,
                    "y": 514
                },
                {
                    "x": 2005,
                    "y": 813
                },
                {
                    "x": 544,
                    "y": 813
                }
            ],
            "category": "paragraph",
            "html": "<p id='10' style='font-size:18px'>data source has the advantage of being large and continuously growing, as well<br>as \"free\" from an annotation perspective since no manual labeling is required.<br>However, the data source also has potential disadvantages: hashtags may be<br>too noisy to serve as an effective supervisory signal and the image distribution<br>might be biased in ways that harm transfer learning. It is not a priori obvious<br>that training on this data will yield good transfer learning results.</p>",
            "id": 10,
            "page": 2,
            "text": "data source has the advantage of being large and continuously growing, as well\nas \"free\" from an annotation perspective since no manual labeling is required.\nHowever, the data source also has potential disadvantages: hashtags may be\ntoo noisy to serve as an effective supervisory signal and the image distribution\nmight be biased in ways that harm transfer learning. It is not a priori obvious\nthat training on this data will yield good transfer learning results."
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 819
                },
                {
                    "x": 2005,
                    "y": 819
                },
                {
                    "x": 2005,
                    "y": 1615
                },
                {
                    "x": 545,
                    "y": 1615
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='11' style='font-size:20px'>The main result of this paper is that without manual dataset curation or SO-<br>phisticated data cleaning, models trained on billions of Instagram images using<br>thousands of distinct hashtags as labels exhibit excellent transfer learning per-<br>formance. For example, we observe improvements over the state-of-the-art for<br>image classification and object detection, where we obtain a single-crop, top-1<br>accuracy of 85.4% on the ImageNet-1k image-classification dataset and 45.2%<br>AP on the COCO object-detection dataset [18], compared to 79.8% and 43. 7%,<br>respectively, when training (or pretraining) the same models on ImageNet-1k.<br>Our primary goal, however, is to contribute novel experimental data about this<br>previously unexplored regime. To that end, we conduct numerous experiments<br>that reveal interesting trends. For example, we find that \"hashtag engineering\"<br>(i.e., collecting images tagged with a specific subset of hashtags) is a promising<br>new direction for improving transfer learning results, that training on large-scale<br>hashtag data is unexpectedly robust to label noise, and that the features learned<br>allow a simple linear classifier to achieve state-of-the-art ImageNet-1k top-1 ac-<br>curacy of 83.6% without any finetuning (compared to 84.2% with finetuning).</p>",
            "id": 11,
            "page": 2,
            "text": "The main result of this paper is that without manual dataset curation or SO-\nphisticated data cleaning, models trained on billions of Instagram images using\nthousands of distinct hashtags as labels exhibit excellent transfer learning per-\nformance. For example, we observe improvements over the state-of-the-art for\nimage classification and object detection, where we obtain a single-crop, top-1\naccuracy of 85.4% on the ImageNet-1k image-classification dataset and 45.2%\nAP on the COCO object-detection dataset [18], compared to 79.8% and 43. 7%,\nrespectively, when training (or pretraining) the same models on ImageNet-1k.\nOur primary goal, however, is to contribute novel experimental data about this\npreviously unexplored regime. To that end, we conduct numerous experiments\nthat reveal interesting trends. For example, we find that \"hashtag engineering\"\n(i.e., collecting images tagged with a specific subset of hashtags) is a promising\nnew direction for improving transfer learning results, that training on large-scale\nhashtag data is unexpectedly robust to label noise, and that the features learned\nallow a simple linear classifier to achieve state-of-the-art ImageNet-1k top-1 ac-\ncuracy of 83.6% without any finetuning (compared to 84.2% with finetuning)."
        },
        {
            "bounding_box": [
                {
                    "x": 544,
                    "y": 1700
                },
                {
                    "x": 1483,
                    "y": 1700
                },
                {
                    "x": 1483,
                    "y": 1761
                },
                {
                    "x": 544,
                    "y": 1761
                }
            ],
            "category": "paragraph",
            "html": "<p id='12' style='font-size:22px'>2 Scaling up Supervised Pretraining</p>",
            "id": 12,
            "page": 2,
            "text": "2 Scaling up Supervised Pretraining"
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 1819
                },
                {
                    "x": 2005,
                    "y": 1819
                },
                {
                    "x": 2005,
                    "y": 2072
                },
                {
                    "x": 545,
                    "y": 2072
                }
            ],
            "category": "paragraph",
            "html": "<p id='13' style='font-size:18px'>In our experiments, we train standard convolutional network architectures to<br>predict hashtags on up to 3.5 billion public Instagram images. To make training<br>at this scale practical, we adopt a distributed synchronous implementation of<br>stochastic gradient descent with large (8k image) minibatches, following Goyal<br>et al. [19]. We experiment on a variety of datasets, which we describe next.</p>",
            "id": 13,
            "page": 2,
            "text": "In our experiments, we train standard convolutional network architectures to\npredict hashtags on up to 3.5 billion public Instagram images. To make training\nat this scale practical, we adopt a distributed synchronous implementation of\nstochastic gradient descent with large (8k image) minibatches, following Goyal\net al. [19]. We experiment on a variety of datasets, which we describe next."
        },
        {
            "bounding_box": [
                {
                    "x": 546,
                    "y": 2155
                },
                {
                    "x": 1075,
                    "y": 2155
                },
                {
                    "x": 1075,
                    "y": 2206
                },
                {
                    "x": 546,
                    "y": 2206
                }
            ],
            "category": "paragraph",
            "html": "<p id='14' style='font-size:20px'>2.1 Instagram Datasets</p>",
            "id": 14,
            "page": 2,
            "text": "2.1 Instagram Datasets"
        },
        {
            "bounding_box": [
                {
                    "x": 546,
                    "y": 2250
                },
                {
                    "x": 2004,
                    "y": 2250
                },
                {
                    "x": 2004,
                    "y": 2651
                },
                {
                    "x": 546,
                    "y": 2651
                }
            ],
            "category": "paragraph",
            "html": "<p id='15' style='font-size:18px'>We use a simple data collection pipeline: (1) We select a set of hashtags. (2) We<br>download images that are tagged with at least one of these hashtags. (3) Then,<br>because multiple hashtags may refer to the same underlying concept, we apply<br>a simple process that utilizes WordNet [20] synsets to merge some hashtags into<br>a single canonical form (e.g., #brownbear and #ursusarctos are merged). (4)<br>Finally, for each downloaded image, we replace each hashtag with its canonical<br>form and discard any hashtags that were not in the selected set. The canonical<br>hashtags are used as labels for training and evaluation.</p>",
            "id": 15,
            "page": 2,
            "text": "We use a simple data collection pipeline: (1) We select a set of hashtags. (2) We\ndownload images that are tagged with at least one of these hashtags. (3) Then,\nbecause multiple hashtags may refer to the same underlying concept, we apply\na simple process that utilizes WordNet [20] synsets to merge some hashtags into\na single canonical form (e.g., #brownbear and #ursusarctos are merged). (4)\nFinally, for each downloaded image, we replace each hashtag with its canonical\nform and discard any hashtags that were not in the selected set. The canonical\nhashtags are used as labels for training and evaluation."
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 2654
                },
                {
                    "x": 2004,
                    "y": 2654
                },
                {
                    "x": 2004,
                    "y": 2805
                },
                {
                    "x": 545,
                    "y": 2805
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='16' style='font-size:16px'>By varying the selected hashtags and the number of images to sample, we<br>can construct a variety of datasets of different sizes and visual distributions. Ta-<br>ble 1 summarizes the datasets used in our experiments. Each dataset is named</p>",
            "id": 16,
            "page": 2,
            "text": "By varying the selected hashtags and the number of images to sample, we\ncan construct a variety of datasets of different sizes and visual distributions. Ta-\nble 1 summarizes the datasets used in our experiments. Each dataset is named"
        },
        {
            "bounding_box": [
                {
                    "x": 922,
                    "y": 410
                },
                {
                    "x": 1911,
                    "y": 410
                },
                {
                    "x": 1911,
                    "y": 456
                },
                {
                    "x": 922,
                    "y": 456
                }
            ],
            "category": "header",
            "html": "<header id='17' style='font-size:22px'>Exploring the Limits of Weakly Supervised Pretraining</header>",
            "id": 17,
            "page": 3,
            "text": "Exploring the Limits of Weakly Supervised Pretraining"
        },
        {
            "bounding_box": [
                {
                    "x": 1971,
                    "y": 414
                },
                {
                    "x": 2000,
                    "y": 414
                },
                {
                    "x": 2000,
                    "y": 449
                },
                {
                    "x": 1971,
                    "y": 449
                }
            ],
            "category": "header",
            "html": "<br><header id='18' style='font-size:16px'>3</header>",
            "id": 18,
            "page": 3,
            "text": "3"
        },
        {
            "bounding_box": [
                {
                    "x": 626,
                    "y": 504
                },
                {
                    "x": 1922,
                    "y": 504
                },
                {
                    "x": 1922,
                    "y": 1128
                },
                {
                    "x": 626,
                    "y": 1128
                }
            ],
            "category": "table",
            "html": "<table id='19' style='font-size:14px'><tr><td>Name template</td><td>Description</td></tr><tr><td>train-IG-I-1.5k train-IG-I-8.5k train-IG-I-17k</td><td>Instagram training set of I images and ~1.5k hashtags from ImageNet-1k. Instagram training set of I images and ~8.5k hashtags from WordNet. Instagram training set of I images and ~17k hashtags from WordNet.</td></tr><tr><td>train-IN-1M-1k val-IN-50k-1k</td><td>The standard ImageNet-1k ILSVRC training set with 1.28M images. The standard ImageNet-1k ILSVRC validation set with 50k images.</td></tr><tr><td>train-IN-I-L val-IN-I-L</td><td>Extended ImageNet training set of I images and L E {5k,9k} labels. Extended ImageNet validation set of I images and L E {5k, 9k} labels.</td></tr><tr><td>train-CUB-6k-200 val-CUB-6k-200</td><td>The Caltech-UCSD Birds-200-2011 training set. The Caltech-UCSD Birds-200-2011 validation set.</td></tr><tr><td>train-Places-1.8M-365 val-Places-37k-365</td><td>The Places365-Standard training set (high-resolution version). The Places365-Standard validation set (high-resolution version).</td></tr><tr><td>train-COCO-135k-80 val-COCO-5k-80 test-COCO-20k-80</td><td>The standard COCO detection training set (2017 version). The standard COCO detection validation set (2017 version). The standard COCO detection test-dev set (2017 version).</td></tr></table>",
            "id": 19,
            "page": 3,
            "text": "Name template Description\n train-IG-I-1.5k train-IG-I-8.5k train-IG-I-17k Instagram training set of I images and ~1.5k hashtags from ImageNet-1k. Instagram training set of I images and ~8.5k hashtags from WordNet. Instagram training set of I images and ~17k hashtags from WordNet.\n train-IN-1M-1k val-IN-50k-1k The standard ImageNet-1k ILSVRC training set with 1.28M images. The standard ImageNet-1k ILSVRC validation set with 50k images.\n train-IN-I-L val-IN-I-L Extended ImageNet training set of I images and L E {5k,9k} labels. Extended ImageNet validation set of I images and L E {5k, 9k} labels.\n train-CUB-6k-200 val-CUB-6k-200 The Caltech-UCSD Birds-200-2011 training set. The Caltech-UCSD Birds-200-2011 validation set.\n train-Places-1.8M-365 val-Places-37k-365 The Places365-Standard training set (high-resolution version). The Places365-Standard validation set (high-resolution version).\n train-COCO-135k-80 val-COCO-5k-80 test-COCO-20k-80"
        },
        {
            "bounding_box": [
                {
                    "x": 548,
                    "y": 1150
                },
                {
                    "x": 2005,
                    "y": 1150
                },
                {
                    "x": 2005,
                    "y": 1288
                },
                {
                    "x": 548,
                    "y": 1288
                }
            ],
            "category": "caption",
            "html": "<br><caption id='20' style='font-size:18px'>Table 1: Summary of image classification datasets. Each dataset is named with a<br>template, role-source-I-L, that indicates its role (training, validation, testing), source,<br>number of images I, and number of labels L.</caption>",
            "id": 20,
            "page": 3,
            "text": "Table 1: Summary of image classification datasets. Each dataset is named with a\ntemplate, role-source-I-L, that indicates its role (training, validation, testing), source,\nnumber of images I, and number of labels L."
        },
        {
            "bounding_box": [
                {
                    "x": 546,
                    "y": 1388
                },
                {
                    "x": 2005,
                    "y": 1388
                },
                {
                    "x": 2005,
                    "y": 1684
                },
                {
                    "x": 546,
                    "y": 1684
                }
            ],
            "category": "paragraph",
            "html": "<p id='21' style='font-size:22px'>by completing a template, role-source-I-L, that indicates its role (training, val-<br>idation, testing), source (IG for Instagram, IN for ImageNet, etc.), number of<br>images I, and number of labels L. We use approximate image and label counts<br>for convenience, for example \"train-IG-940M-1.5k\" is an Instagram dataset for<br>training with ~940e6 images and ~1,500 labels. We omit the role and image<br>count when it is clear from context or not useful to present.</p>",
            "id": 21,
            "page": 3,
            "text": "by completing a template, role-source-I-L, that indicates its role (training, val-\nidation, testing), source (IG for Instagram, IN for ImageNet, etc.), number of\nimages I, and number of labels L. We use approximate image and label counts\nfor convenience, for example \"train-IG-940M-1.5k\" is an Instagram dataset for\ntraining with ~940e6 images and ~1,500 labels. We omit the role and image\ncount when it is clear from context or not useful to present."
        },
        {
            "bounding_box": [
                {
                    "x": 546,
                    "y": 1688
                },
                {
                    "x": 2005,
                    "y": 1688
                },
                {
                    "x": 2005,
                    "y": 2184
                },
                {
                    "x": 546,
                    "y": 2184
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='22' style='font-size:20px'>We design three hashtag sets for the Instagram data: (1) A ~1.5k set with<br>hashtags from the standard 1,000 IN-1k synsets (each synset contains at least<br>one synonym, hence there are more hashtags than synsets). (2) A ~17k set with<br>hashtags that are synonyms in any of the noun synsets in WordNet. And (3)<br>an ~8.5k set with the most frequent hashtags from the 17k set. The hashtag set<br>sizes are measured after merging the hashtags into their canonical forms. We<br>hypothesize that the first set has a visual distribution similar to IN-1k, while<br>the other two represent more general visual distributions covering fine-grained<br>visual categories. Details of how these hashtags are selected and how the merging<br>process works are given in supplemental material.</p>",
            "id": 22,
            "page": 3,
            "text": "We design three hashtag sets for the Instagram data: (1) A ~1.5k set with\nhashtags from the standard 1,000 IN-1k synsets (each synset contains at least\none synonym, hence there are more hashtags than synsets). (2) A ~17k set with\nhashtags that are synonyms in any of the noun synsets in WordNet. And (3)\nan ~8.5k set with the most frequent hashtags from the 17k set. The hashtag set\nsizes are measured after merging the hashtags into their canonical forms. We\nhypothesize that the first set has a visual distribution similar to IN-1k, while\nthe other two represent more general visual distributions covering fine-grained\nvisual categories. Details of how these hashtags are selected and how the merging\nprocess works are given in supplemental material."
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 2204
                },
                {
                    "x": 2006,
                    "y": 2204
                },
                {
                    "x": 2006,
                    "y": 2805
                },
                {
                    "x": 545,
                    "y": 2805
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='23' style='font-size:22px'>Image deduplication. When performing transfer learning, it is essential to<br>understand and properly address overlap between training and test sets. Overlap<br>can exists because images may come from the same underlying sources (e.g.,<br>Wikipedia, Flickr, Google). For instance, ~5% of the images in the val-CUB-6k-<br>200 set [21] also appear in train-IN-1M-1k, and 1.78% of images in val-IN-50k-1k<br>set are in the JFT-300M training set [17]. To address this issue, we performed the<br>following deduplication procedure: we compute R-MAC features [22,23] for all<br>candidate images using a ResNet-50 model, and use these features to find the k=<br>21 nearest neighbors for each of the images in our test sets (additional details are<br>in the supplemental material). Subsequently, we manually inspected all images<br>and their nearest neighbors to identify duplicates. This procedure uncovered<br>150 val-IN-50k-1k (0.30%), 10 val-CUB-6k-200 (0.17%), 151 val-Places-37k-365</p>",
            "id": 23,
            "page": 3,
            "text": "Image deduplication. When performing transfer learning, it is essential to\nunderstand and properly address overlap between training and test sets. Overlap\ncan exists because images may come from the same underlying sources (e.g.,\nWikipedia, Flickr, Google). For instance, ~5% of the images in the val-CUB-6k-\n200 set [21] also appear in train-IN-1M-1k, and 1.78% of images in val-IN-50k-1k\nset are in the JFT-300M training set [17]. To address this issue, we performed the\nfollowing deduplication procedure: we compute R-MAC features [22,23] for all\ncandidate images using a ResNet-50 model, and use these features to find the k=\n21 nearest neighbors for each of the images in our test sets (additional details are\nin the supplemental material). Subsequently, we manually inspected all images\nand their nearest neighbors to identify duplicates. This procedure uncovered\n150 val-IN-50k-1k (0.30%), 10 val-CUB-6k-200 (0.17%), 151 val-Places-37k-365"
        },
        {
            "bounding_box": [
                {
                    "x": 547,
                    "y": 409
                },
                {
                    "x": 943,
                    "y": 409
                },
                {
                    "x": 943,
                    "y": 456
                },
                {
                    "x": 547,
                    "y": 456
                }
            ],
            "category": "header",
            "html": "<header id='24' style='font-size:14px'>4 Mahajan et al.</header>",
            "id": 24,
            "page": 4,
            "text": "4 Mahajan et al."
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 513
                },
                {
                    "x": 2005,
                    "y": 513
                },
                {
                    "x": 2005,
                    "y": 715
                },
                {
                    "x": 545,
                    "y": 715
                }
            ],
            "category": "paragraph",
            "html": "<p id='25' style='font-size:18px'>(0.41%), and 6 val-COCO-5k-80 (0.12%) duplicates. In our results, we report<br>the observed accuracy of our models; in the supplemental material, we report<br>a conservative lower bound on accuracy by marking all duplicates as incorrect.<br>Given the small percentage of duplicates, they do not impact our findings.</p>",
            "id": 25,
            "page": 4,
            "text": "(0.41%), and 6 val-COCO-5k-80 (0.12%) duplicates. In our results, we report\nthe observed accuracy of our models; in the supplemental material, we report\na conservative lower bound on accuracy by marking all duplicates as incorrect.\nGiven the small percentage of duplicates, they do not impact our findings."
        },
        {
            "bounding_box": [
                {
                    "x": 546,
                    "y": 764
                },
                {
                    "x": 2004,
                    "y": 764
                },
                {
                    "x": 2004,
                    "y": 1164
                },
                {
                    "x": 546,
                    "y": 1164
                }
            ],
            "category": "paragraph",
            "html": "<p id='26' style='font-size:18px'>Discussion. Our datasets have two nice properties: public visibility and sim-<br>plicity. By using publicly accessible images, the data used in our experiments is<br>visible to everyone. To see what it looks like, the images are browsable by hashtag<br>at https : 1 / www · instagram · com/ explore/tags/ followed by a specific hashtag;<br>for example https : / /www.instagram.com/ explore/tags /brownbear shows im-<br>ages tagged with #brownbear. Our data is also taken from the \"wild\" , essentially<br>as-is, with minimal effort to sanitize it. This makes the dataset construction pro-<br>cess particularly simple and transparent.</p>",
            "id": 26,
            "page": 4,
            "text": "Discussion. Our datasets have two nice properties: public visibility and sim-\nplicity. By using publicly accessible images, the data used in our experiments is\nvisible to everyone. To see what it looks like, the images are browsable by hashtag\nat https : 1 / www · instagram · com/ explore/tags/ followed by a specific hashtag;\nfor example https : / /www.instagram.com/ explore/tags /brownbear shows im-\nages tagged with #brownbear. Our data is also taken from the \"wild\" , essentially\nas-is, with minimal effort to sanitize it. This makes the dataset construction pro-\ncess particularly simple and transparent."
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 1171
                },
                {
                    "x": 2004,
                    "y": 1171
                },
                {
                    "x": 2004,
                    "y": 1418
                },
                {
                    "x": 545,
                    "y": 1418
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='27' style='font-size:20px'>We contrast these properties with the JFT-300M dataset [17], which is not<br>publicly visible and is the result of a proprietary collection process ( \"The [JFT-<br>300M] images are labeled using an algorithm that uses a complex mixture of raw<br>web signals, connections between web-pages and user feedback. \"). Additional<br>details describing the collection of JFT-300M have not been publicly disclosed.</p>",
            "id": 27,
            "page": 4,
            "text": "We contrast these properties with the JFT-300M dataset [17], which is not\npublicly visible and is the result of a proprietary collection process ( \"The [JFT-\n300M] images are labeled using an algorithm that uses a complex mixture of raw\nweb signals, connections between web-pages and user feedback. \"). Additional\ndetails describing the collection of JFT-300M have not been publicly disclosed."
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 1427
                },
                {
                    "x": 2005,
                    "y": 1427
                },
                {
                    "x": 2005,
                    "y": 1725
                },
                {
                    "x": 545,
                    "y": 1725
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='28' style='font-size:16px'>Despite our efforts to make the dataset content and collection process trans-<br>parent, we acknowledge that, similar to JFT-300M, it is not possible for other<br>research groups to know exactly which images we used nor to download them<br>en masse. Hence it is not possible for others to replicate our results at this time.<br>However, we believe that it is better if we undertake this study and share the<br>results with the community than to not publish the results.</p>",
            "id": 28,
            "page": 4,
            "text": "Despite our efforts to make the dataset content and collection process trans-\nparent, we acknowledge that, similar to JFT-300M, it is not possible for other\nresearch groups to know exactly which images we used nor to download them\nen masse. Hence it is not possible for others to replicate our results at this time.\nHowever, we believe that it is better if we undertake this study and share the\nresults with the community than to not publish the results."
        },
        {
            "bounding_box": [
                {
                    "x": 547,
                    "y": 1827
                },
                {
                    "x": 1069,
                    "y": 1827
                },
                {
                    "x": 1069,
                    "y": 1878
                },
                {
                    "x": 547,
                    "y": 1878
                }
            ],
            "category": "paragraph",
            "html": "<p id='29' style='font-size:20px'>2.2 ImageNet Datasets</p>",
            "id": 29,
            "page": 4,
            "text": "2.2 ImageNet Datasets"
        },
        {
            "bounding_box": [
                {
                    "x": 546,
                    "y": 1940
                },
                {
                    "x": 2004,
                    "y": 1940
                },
                {
                    "x": 2004,
                    "y": 2290
                },
                {
                    "x": 546,
                    "y": 2290
                }
            ],
            "category": "paragraph",
            "html": "<p id='30' style='font-size:18px'>In addition to the standard IN-1k dataset, we experiment with larger subsets<br>of the full ImageNet 2011 release that contains 14.2M images and 22k labels.<br>We construct training and validation sets that include 5k and 9k labels. For the<br>5k set, we use the now standard IN-5k proposed in [15] (6.6M training images).<br>For the 9k label set, we follow the same protocol used to construct IN-5k, which<br>involves taking the next most frequent 4k labels and all of the associated images<br>(10.5M training images). In all cases, we use 50 images per class for validation.</p>",
            "id": 30,
            "page": 4,
            "text": "In addition to the standard IN-1k dataset, we experiment with larger subsets\nof the full ImageNet 2011 release that contains 14.2M images and 22k labels.\nWe construct training and validation sets that include 5k and 9k labels. For the\n5k set, we use the now standard IN-5k proposed in [15] (6.6M training images).\nFor the 9k label set, we follow the same protocol used to construct IN-5k, which\ninvolves taking the next most frequent 4k labels and all of the associated images\n(10.5M training images). In all cases, we use 50 images per class for validation."
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 2392
                },
                {
                    "x": 820,
                    "y": 2392
                },
                {
                    "x": 820,
                    "y": 2439
                },
                {
                    "x": 549,
                    "y": 2439
                }
            ],
            "category": "paragraph",
            "html": "<p id='31' style='font-size:18px'>2.3 Models</p>",
            "id": 31,
            "page": 4,
            "text": "2.3 Models"
        },
        {
            "bounding_box": [
                {
                    "x": 546,
                    "y": 2502
                },
                {
                    "x": 2005,
                    "y": 2502
                },
                {
                    "x": 2005,
                    "y": 2803
                },
                {
                    "x": 546,
                    "y": 2803
                }
            ],
            "category": "paragraph",
            "html": "<p id='32' style='font-size:22px'>We use residual networks with grouped convolutional layers, called ResNeXt<br>[15]. Our experiments use ResNeXt-101 32x Cd, which has 101 layers, 32 groups,<br>and group widths C of: 4 (8B multiply-add FLOPs, 43M parameters), 8 (16B,<br>88M), 16 (36B, 193M), 32 (87B, 466M), and 48 (153B, 829M). Our implemen-<br>tation matches [19]. We believe our results will generalize to other architectures<br>[24,25,26].</p>",
            "id": 32,
            "page": 4,
            "text": "We use residual networks with grouped convolutional layers, called ResNeXt\n[15]. Our experiments use ResNeXt-101 32x Cd, which has 101 layers, 32 groups,\nand group widths C of: 4 (8B multiply-add FLOPs, 43M parameters), 8 (16B,\n88M), 16 (36B, 193M), 32 (87B, 466M), and 48 (153B, 829M). Our implemen-\ntation matches [19]. We believe our results will generalize to other architectures\n[24,25,26]."
        },
        {
            "bounding_box": [
                {
                    "x": 920,
                    "y": 408
                },
                {
                    "x": 1993,
                    "y": 408
                },
                {
                    "x": 1993,
                    "y": 457
                },
                {
                    "x": 920,
                    "y": 457
                }
            ],
            "category": "header",
            "html": "<header id='33' style='font-size:16px'>Exploring the Limits of Weakly Supervised Pretraining 5</header>",
            "id": 33,
            "page": 5,
            "text": "Exploring the Limits of Weakly Supervised Pretraining 5"
        },
        {
            "bounding_box": [
                {
                    "x": 546,
                    "y": 512
                },
                {
                    "x": 2005,
                    "y": 512
                },
                {
                    "x": 2005,
                    "y": 914
                },
                {
                    "x": 546,
                    "y": 914
                }
            ],
            "category": "paragraph",
            "html": "<p id='34' style='font-size:16px'>Loss function. In contrast to ImageNet, our Instagram datasets may contain<br>multiple labels per image (because a user specified multiple hashtags). The aver-<br>age number of hashtags per image varies depending on the dataset; for instance,<br>train-IG-1B-17k contains ~2 hashtags per image. Our model computes probabil-<br>ities over all hashtags in the vocabulary using a softmax activation and is trained<br>to minimize the cross-entropy between the predicted softmax distribution and<br>the target distribution of each image. The target is a vector with k non-zero<br>entries each set to 1/k corresponding to the k ≥ 1 hashtags for the image.</p>",
            "id": 34,
            "page": 5,
            "text": "Loss function. In contrast to ImageNet, our Instagram datasets may contain\nmultiple labels per image (because a user specified multiple hashtags). The aver-\nage number of hashtags per image varies depending on the dataset; for instance,\ntrain-IG-1B-17k contains ~2 hashtags per image. Our model computes probabil-\nities over all hashtags in the vocabulary using a softmax activation and is trained\nto minimize the cross-entropy between the predicted softmax distribution and\nthe target distribution of each image. The target is a vector with k non-zero\nentries each set to 1/k corresponding to the k ≥ 1 hashtags for the image."
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 915
                },
                {
                    "x": 2007,
                    "y": 915
                },
                {
                    "x": 2007,
                    "y": 1165
                },
                {
                    "x": 545,
                    "y": 1165
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='35' style='font-size:18px'>We have also experimented with per-hashtag sigmoid outputs and binary<br>logistic loss, but obtained significantly worse results. While counter-intuitive<br>given the multi-label data, these findings match similar observations in [16].<br>The successful application of sigmoid activations and logistic loss may require<br>sophisticated label completion techniques [17] and more hyper-parameter search.</p>",
            "id": 35,
            "page": 5,
            "text": "We have also experimented with per-hashtag sigmoid outputs and binary\nlogistic loss, but obtained significantly worse results. While counter-intuitive\ngiven the multi-label data, these findings match similar observations in [16].\nThe successful application of sigmoid activations and logistic loss may require\nsophisticated label completion techniques [17] and more hyper-parameter search."
        },
        {
            "bounding_box": [
                {
                    "x": 546,
                    "y": 1237
                },
                {
                    "x": 1071,
                    "y": 1237
                },
                {
                    "x": 1071,
                    "y": 1288
                },
                {
                    "x": 546,
                    "y": 1288
                }
            ],
            "category": "paragraph",
            "html": "<p id='36' style='font-size:18px'>2.4 Pretraining Details</p>",
            "id": 36,
            "page": 5,
            "text": "2.4 Pretraining Details"
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 1322
                },
                {
                    "x": 2004,
                    "y": 1322
                },
                {
                    "x": 2004,
                    "y": 1820
                },
                {
                    "x": 545,
                    "y": 1820
                }
            ],
            "category": "paragraph",
            "html": "<p id='37' style='font-size:14px'>Our models are trained by synchronous stochastic gradient descent (SGD) on 336<br>GPUs across 42 machines with minibatches of 8,064 images. Each GPU processes<br>24 images at a time and batch normalization (BN) [27] statistics are computed<br>on these 24 image sets. The length of the training schedule, measured in units<br>of number-of-images-processed (i.e., minibatch size x total SGD updates), is<br>determined by a heuristic: we choose two training extremes (for instance, 120<br>epochs on 1.2e6 images and 2 epochs on 3.5e9 images) and linearly interpolate<br>the schedule between them to set the number-of-images-processed for each ex-<br>periment. Schedules for each experiment are in the supplemental material. Our<br>ResNeXt-101 32x16d networks took ~22 days to train on 3.5B images.</p>",
            "id": 37,
            "page": 5,
            "text": "Our models are trained by synchronous stochastic gradient descent (SGD) on 336\nGPUs across 42 machines with minibatches of 8,064 images. Each GPU processes\n24 images at a time and batch normalization (BN) [27] statistics are computed\non these 24 image sets. The length of the training schedule, measured in units\nof number-of-images-processed (i.e., minibatch size x total SGD updates), is\ndetermined by a heuristic: we choose two training extremes (for instance, 120\nepochs on 1.2e6 images and 2 epochs on 3.5e9 images) and linearly interpolate\nthe schedule between them to set the number-of-images-processed for each ex-\nperiment. Schedules for each experiment are in the supplemental material. Our\nResNeXt-101 32x16d networks took ~22 days to train on 3.5B images."
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 1825
                },
                {
                    "x": 2004,
                    "y": 1825
                },
                {
                    "x": 2004,
                    "y": 2369
                },
                {
                    "x": 545,
                    "y": 2369
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='38' style='font-size:14px'>To set the learning rate, we follow the linear scaling rule with gradual warm-<br>up described in [19]. We use a warm-up from 0.1 up to 0.1 /256 x 8064, where 0.1<br>and 256 are canonical learning rate and minibatch sizes [28]. After the warm-up,<br>the learning rate is multiplied by 0.5 at equally spaced steps, such that the total<br>number of learning rate reductions is 20 over the course of training. The same<br>settings are used when training on ImageNet and Instagram data, except that<br>when training on ImageNet we use 128 GPUs in 16 machines (for a minibatch<br>size of 3,072) due to the smaller dataset size and we use the standard learning<br>rate schedule that involves three equally spaced reductions by a factor of 0.1.<br>All other initialization and training details match [19] and are summarized in<br>the supplemental material.</p>",
            "id": 38,
            "page": 5,
            "text": "To set the learning rate, we follow the linear scaling rule with gradual warm-\nup described in [19]. We use a warm-up from 0.1 up to 0.1 /256 x 8064, where 0.1\nand 256 are canonical learning rate and minibatch sizes [28]. After the warm-up,\nthe learning rate is multiplied by 0.5 at equally spaced steps, such that the total\nnumber of learning rate reductions is 20 over the course of training. The same\nsettings are used when training on ImageNet and Instagram data, except that\nwhen training on ImageNet we use 128 GPUs in 16 machines (for a minibatch\nsize of 3,072) due to the smaller dataset size and we use the standard learning\nrate schedule that involves three equally spaced reductions by a factor of 0.1.\nAll other initialization and training details match [19] and are summarized in\nthe supplemental material."
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 2447
                },
                {
                    "x": 952,
                    "y": 2447
                },
                {
                    "x": 952,
                    "y": 2504
                },
                {
                    "x": 549,
                    "y": 2504
                }
            ],
            "category": "paragraph",
            "html": "<p id='39' style='font-size:20px'>3 Experiments</p>",
            "id": 39,
            "page": 5,
            "text": "3 Experiments"
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 2554
                },
                {
                    "x": 2005,
                    "y": 2554
                },
                {
                    "x": 2005,
                    "y": 2804
                },
                {
                    "x": 545,
                    "y": 2804
                }
            ],
            "category": "paragraph",
            "html": "<p id='40' style='font-size:14px'>In our experiments, we pretrain convolutional networks for hashtag prediction<br>and transfer those networks to a variety of tasks. There are two established pro-<br>tocols for judging the quality of a pretrained model (see [29] §3 for a discussion).<br>Both analyze how pretraining on a source task, e.g. IN-1k classification, leads to<br>gains (or losses) on a target task, e.g. bird recognition or object detection.</p>",
            "id": 40,
            "page": 5,
            "text": "In our experiments, we pretrain convolutional networks for hashtag prediction\nand transfer those networks to a variety of tasks. There are two established pro-\ntocols for judging the quality of a pretrained model (see [29] §3 for a discussion).\nBoth analyze how pretraining on a source task, e.g. IN-1k classification, leads to\ngains (or losses) on a target task, e.g. bird recognition or object detection."
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 409
                },
                {
                    "x": 944,
                    "y": 409
                },
                {
                    "x": 944,
                    "y": 456
                },
                {
                    "x": 545,
                    "y": 456
                }
            ],
            "category": "header",
            "html": "<header id='41' style='font-size:14px'>6 Mahajan et al.</header>",
            "id": 41,
            "page": 6,
            "text": "6 Mahajan et al."
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 511
                },
                {
                    "x": 2005,
                    "y": 511
                },
                {
                    "x": 2005,
                    "y": 964
                },
                {
                    "x": 545,
                    "y": 964
                }
            ],
            "category": "paragraph",
            "html": "<p id='42' style='font-size:16px'>Full network finetuning views pretraining as sophisticated weight initializa-<br>tion: the success of pretraining is judged by its impact on the target task after<br>further training the network weights in a task-specific manner (i.e. finetuning).<br>By contrast, feature transfer uses the pretrained network as a feature extractor:<br>it judges the quality of the network by how effective its features are on other<br>tasks, without updating any of the network parameters. These protocols are two<br>extremes of a spectrum along which the proportion of pretrained weights that are<br>finetuned varies from all to none. We employ both protocols in our experiments;<br>at times one is more appropriate than the other.</p>",
            "id": 42,
            "page": 6,
            "text": "Full network finetuning views pretraining as sophisticated weight initializa-\ntion: the success of pretraining is judged by its impact on the target task after\nfurther training the network weights in a task-specific manner (i.e. finetuning).\nBy contrast, feature transfer uses the pretrained network as a feature extractor:\nit judges the quality of the network by how effective its features are on other\ntasks, without updating any of the network parameters. These protocols are two\nextremes of a spectrum along which the proportion of pretrained weights that are\nfinetuned varies from all to none. We employ both protocols in our experiments;\nat times one is more appropriate than the other."
        },
        {
            "bounding_box": [
                {
                    "x": 546,
                    "y": 986
                },
                {
                    "x": 2005,
                    "y": 986
                },
                {
                    "x": 2005,
                    "y": 1386
                },
                {
                    "x": 546,
                    "y": 1386
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='43' style='font-size:16px'>Full network finetuning is performed by removing the hashtag-specific fully<br>connected classification layer from the network and replacing it with a randomly<br>initialized classification layer with one output per class in the target task. This<br>modified network is then trained using SGD with momentum. We select the<br>finetuning learning rate and schedule by grid search on a proper validation set<br>for each target task. To do this, we randomly hold out a small portion of the<br>training set (see supplemental material). This practice ensures that our results<br>on the standard validation sets are clean.</p>",
            "id": 43,
            "page": 6,
            "text": "Full network finetuning is performed by removing the hashtag-specific fully\nconnected classification layer from the network and replacing it with a randomly\ninitialized classification layer with one output per class in the target task. This\nmodified network is then trained using SGD with momentum. We select the\nfinetuning learning rate and schedule by grid search on a proper validation set\nfor each target task. To do this, we randomly hold out a small portion of the\ntraining set (see supplemental material). This practice ensures that our results\non the standard validation sets are clean."
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 1412
                },
                {
                    "x": 2005,
                    "y": 1412
                },
                {
                    "x": 2005,
                    "y": 1613
                },
                {
                    "x": 545,
                    "y": 1613
                }
            ],
            "category": "paragraph",
            "html": "<p id='44' style='font-size:16px'>Feature transfer is performed by training an L2-regularized linear logistic<br>regressor on the training data for the target task using SGD. The features pro-<br>duced by the pretrained network are used as input into the classifier. We train<br>the classifier until convergence to the global optimum.</p>",
            "id": 44,
            "page": 6,
            "text": "Feature transfer is performed by training an L2-regularized linear logistic\nregressor on the training data for the target task using SGD. The features pro-\nduced by the pretrained network are used as input into the classifier. We train\nthe classifier until convergence to the global optimum."
        },
        {
            "bounding_box": [
                {
                    "x": 543,
                    "y": 1691
                },
                {
                    "x": 1366,
                    "y": 1691
                },
                {
                    "x": 1366,
                    "y": 1744
                },
                {
                    "x": 543,
                    "y": 1744
                }
            ],
            "category": "paragraph",
            "html": "<p id='45' style='font-size:20px'>3.1 Image Classification Experiments</p>",
            "id": 45,
            "page": 6,
            "text": "3.1 Image Classification Experiments"
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 1778
                },
                {
                    "x": 2004,
                    "y": 1778
                },
                {
                    "x": 2004,
                    "y": 2029
                },
                {
                    "x": 545,
                    "y": 2029
                }
            ],
            "category": "paragraph",
            "html": "<p id='46' style='font-size:18px'>We evaluate Instagram pretraining by measuring classification accuracies on<br>three classification target tasks: ImageNet [30], CUB2011 [21], and Places365<br>[14]. We perform inference on 224x224 center-cropped images, and study the<br>effects of (1) the hashtag vocabulary size, (2) the training set size, (3) the amount<br>of noise in the hashtag targets, and (4) the hashtag sampling strategy.</p>",
            "id": 46,
            "page": 6,
            "text": "We evaluate Instagram pretraining by measuring classification accuracies on\nthree classification target tasks: ImageNet [30], CUB2011 [21], and Places365\n[14]. We perform inference on 224x224 center-cropped images, and study the\neffects of (1) the hashtag vocabulary size, (2) the training set size, (3) the amount\nof noise in the hashtag targets, and (4) the hashtag sampling strategy."
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 2053
                },
                {
                    "x": 2004,
                    "y": 2053
                },
                {
                    "x": 2004,
                    "y": 2604
                },
                {
                    "x": 545,
                    "y": 2604
                }
            ],
            "category": "paragraph",
            "html": "<p id='47' style='font-size:16px'>3.1.1 How does the Instagram hashtag set impact accuracy? Our first<br>experiment varies the Instagram hashtag sets used in pretraining (1.5k, 8.5k,<br>VS. 17k) whilst keeping other factors constant. We compute transfer learning re-<br>sults as top-1 classification accuracy on five target datasets: val-IN-1k, val-IN-5k,<br>val-IN-9k, val-CUB-200, val-Places-365. For baseline models, we use ImageNet<br>classification as a source task: we train networks on train-IN-1k, train-IN-5k,<br>and train-IN-9k, and evaluate them on the corresponding validation sets (fine-<br>tuning is not needed in these cases). For val-CUB-200 and val-Places-365, we<br>use train-IN-1k as the baseline source task and finetune on train-CUB-200 and<br>train-Places-365. Full network finetuning of ResNeXt-101 32x16d is used for all<br>source-target pairs in which source and target are not the same.</p>",
            "id": 47,
            "page": 6,
            "text": "3.1.1 How does the Instagram hashtag set impact accuracy? Our first\nexperiment varies the Instagram hashtag sets used in pretraining (1.5k, 8.5k,\nVS. 17k) whilst keeping other factors constant. We compute transfer learning re-\nsults as top-1 classification accuracy on five target datasets: val-IN-1k, val-IN-5k,\nval-IN-9k, val-CUB-200, val-Places-365. For baseline models, we use ImageNet\nclassification as a source task: we train networks on train-IN-1k, train-IN-5k,\nand train-IN-9k, and evaluate them on the corresponding validation sets (fine-\ntuning is not needed in these cases). For val-CUB-200 and val-Places-365, we\nuse train-IN-1k as the baseline source task and finetune on train-CUB-200 and\ntrain-Places-365. Full network finetuning of ResNeXt-101 32x16d is used for all\nsource-target pairs in which source and target are not the same."
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 2605
                },
                {
                    "x": 2005,
                    "y": 2605
                },
                {
                    "x": 2005,
                    "y": 2804
                },
                {
                    "x": 545,
                    "y": 2804
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='48' style='font-size:16px'>Figure 1 shows that pretraining for hashtag prediction substantially improves<br>target task accuracy: on the standard IN-1k benchmark set, a network pretrained<br>on nearly 1B Instagram images with 1.5k hashtags achieves a state-of-the-art<br>accuracy of 84.2% an improvement of 4.6% over the same model architecture</p>",
            "id": 48,
            "page": 6,
            "text": "Figure 1 shows that pretraining for hashtag prediction substantially improves\ntarget task accuracy: on the standard IN-1k benchmark set, a network pretrained\non nearly 1B Instagram images with 1.5k hashtags achieves a state-of-the-art\naccuracy of 84.2% an improvement of 4.6% over the same model architecture"
        },
        {
            "bounding_box": [
                {
                    "x": 923,
                    "y": 409
                },
                {
                    "x": 1897,
                    "y": 409
                },
                {
                    "x": 1897,
                    "y": 457
                },
                {
                    "x": 923,
                    "y": 457
                }
            ],
            "category": "header",
            "html": "<header id='49' style='font-size:22px'>Exploring the Limits of Weakly Supervised Pretraining</header>",
            "id": 49,
            "page": 7,
            "text": "Exploring the Limits of Weakly Supervised Pretraining"
        },
        {
            "bounding_box": [
                {
                    "x": 1971,
                    "y": 414
                },
                {
                    "x": 2000,
                    "y": 414
                },
                {
                    "x": 2000,
                    "y": 447
                },
                {
                    "x": 1971,
                    "y": 447
                }
            ],
            "category": "header",
            "html": "<br><header id='50' style='font-size:16px'>7</header>",
            "id": 50,
            "page": 7,
            "text": "7"
        },
        {
            "bounding_box": [
                {
                    "x": 554,
                    "y": 550
                },
                {
                    "x": 1994,
                    "y": 550
                },
                {
                    "x": 1994,
                    "y": 1151
                },
                {
                    "x": 554,
                    "y": 1151
                }
            ],
            "category": "figure",
            "html": "<figure><img id='51' style='font-size:14px' alt=\"Target task: ImageNet Target task: CUB & Places\nSource task Source task\n90 90 89.2\nImageNet (target = source) 87.9 87.5 88.1 ImageNet (1.3M, 1k labels)\n%\n84.2 83.483.684.2 Instagram (940M, 1.5k tags) 84.0 Instagram (940M, 1.5k tags)\n80 79.6 Instagram (1B, 8.5k tags) (% 80 Instagram (1B, 8.5k tags)\n(in\n(in\nInstagram (1B, 17k tags) Instagram (1B, 17k tags)\naccuracy\nInstagram (3.5B, 17k tags) Instagram (3.5B, 17k tags)\n70 70\ntop-1\n60\n60 accuracy\n55.255.655.856.0 56.2 56.9 57.3 57.5 58.0\n53.5\nImageNet\n50 50\n48.448.248.549.0\n46.0\n40 Classification\n40\n30 30\n1,000 5,000 9,000 CUB2011 Places365\nNumber of classes in target task (ImageNet) Target task\" data-coord=\"top-left:(554,550); bottom-right:(1994,1151)\" /></figure>",
            "id": 51,
            "page": 7,
            "text": "Target task: ImageNet Target task: CUB & Places\nSource task Source task\n90 90 89.2\nImageNet (target = source) 87.9 87.5 88.1 ImageNet (1.3M, 1k labels)\n%\n84.2 83.483.684.2 Instagram (940M, 1.5k tags) 84.0 Instagram (940M, 1.5k tags)\n80 79.6 Instagram (1B, 8.5k tags) (% 80 Instagram (1B, 8.5k tags)\n(in\n(in\nInstagram (1B, 17k tags) Instagram (1B, 17k tags)\naccuracy\nInstagram (3.5B, 17k tags) Instagram (3.5B, 17k tags)\n70 70\ntop-1\n60\n60 accuracy\n55.255.655.856.0 56.2 56.9 57.3 57.5 58.0\n53.5\nImageNet\n50 50\n48.448.248.549.0\n46.0\n40 Classification\n40\n30 30\n1,000 5,000 9,000 CUB2011 Places365\nNumber of classes in target task (ImageNet) Target task"
        },
        {
            "bounding_box": [
                {
                    "x": 546,
                    "y": 1180
                },
                {
                    "x": 2004,
                    "y": 1180
                },
                {
                    "x": 2004,
                    "y": 1368
                },
                {
                    "x": 546,
                    "y": 1368
                }
            ],
            "category": "caption",
            "html": "<caption id='52' style='font-size:18px'>Fig. 1: Classification accuracy of ResNeXt-101 32x16d pretrained on IG-1B with dif-<br>ferent hashtag vocabularies (purple bars) on IN-{1k, 5k, 9k} (left) and CUB2011,<br>Places365 (right). Baseline models (gray bars) are trained on IN-{1k, 5k, 9k} (left)<br>and IN-1k (right), respectively. Full network finetuning is used. Higher is better.</caption>",
            "id": 52,
            "page": 7,
            "text": "Fig. 1: Classification accuracy of ResNeXt-101 32x16d pretrained on IG-1B with dif-\nferent hashtag vocabularies (purple bars) on IN-{1k, 5k, 9k} (left) and CUB2011,\nPlaces365 (right). Baseline models (gray bars) are trained on IN-{1k, 5k, 9k} (left)\nand IN-1k (right), respectively. Full network finetuning is used. Higher is better."
        },
        {
            "bounding_box": [
                {
                    "x": 546,
                    "y": 1469
                },
                {
                    "x": 2003,
                    "y": 1469
                },
                {
                    "x": 2003,
                    "y": 2019
                },
                {
                    "x": 546,
                    "y": 2019
                }
            ],
            "category": "paragraph",
            "html": "<p id='53' style='font-size:20px'>trained on IN-1k alone and a 1.5% boost over the prior state-of-the-art [31],<br>which uses an optimized network architecture. The performance improvements<br>due to Instagram pretraining vary between ImageNet tasks: on the 1k class task,<br>the model pretrained with the IN-1k-aligned 1.5k hashtag set outperforms source<br>networks trained on larger hashtag sets. This trend reverses as the number of<br>target ImageNet classes increases: on 9k ImageNet target classes, the model<br>pretrained with 17k hashtags strongly outperforms the 1.5k hashtag model. On<br>the CUB2011 and Places365 target tasks, source models trained with the largest<br>hashtag sets perform the best, likely, because the 17k hashtags span more objects,<br>scenes, and fine-grained categories. These patterns are intuitive and suggest that<br>alignment between the source and target label sets is an important factor.</p>",
            "id": 53,
            "page": 7,
            "text": "trained on IN-1k alone and a 1.5% boost over the prior state-of-the-art [31],\nwhich uses an optimized network architecture. The performance improvements\ndue to Instagram pretraining vary between ImageNet tasks: on the 1k class task,\nthe model pretrained with the IN-1k-aligned 1.5k hashtag set outperforms source\nnetworks trained on larger hashtag sets. This trend reverses as the number of\ntarget ImageNet classes increases: on 9k ImageNet target classes, the model\npretrained with 17k hashtags strongly outperforms the 1.5k hashtag model. On\nthe CUB2011 and Places365 target tasks, source models trained with the largest\nhashtag sets perform the best, likely, because the 17k hashtags span more objects,\nscenes, and fine-grained categories. These patterns are intuitive and suggest that\nalignment between the source and target label sets is an important factor."
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 2021
                },
                {
                    "x": 2004,
                    "y": 2021
                },
                {
                    "x": 2004,
                    "y": 2420
                },
                {
                    "x": 545,
                    "y": 2420
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='54' style='font-size:18px'>We also show results in Figure 1 using a larger 3.5B image set with 17k<br>hashtags (dark purple bars), which performs best across all target tasks. Fur-<br>thermore, following [32], we measure the rectified classification accuracy of this<br>model on val-IN-1k. We present all incorrect classifications to five human anno-<br>tators, asking whether or not the prediction is correct: if at least four annotators<br>answer this question affirmatively the model's prediction is considered correct.<br>Whereas the IN-1M-1k model obtained a rectified top-1 accuracy of 87.5% on<br>val-IN-1k, our IG-3.5B-17k pretrained model achieved 90.4%.</p>",
            "id": 54,
            "page": 7,
            "text": "We also show results in Figure 1 using a larger 3.5B image set with 17k\nhashtags (dark purple bars), which performs best across all target tasks. Fur-\nthermore, following [32], we measure the rectified classification accuracy of this\nmodel on val-IN-1k. We present all incorrect classifications to five human anno-\ntators, asking whether or not the prediction is correct: if at least four annotators\nanswer this question affirmatively the model's prediction is considered correct.\nWhereas the IN-1M-1k model obtained a rectified top-1 accuracy of 87.5% on\nval-IN-1k, our IG-3.5B-17k pretrained model achieved 90.4%."
        },
        {
            "bounding_box": [
                {
                    "x": 544,
                    "y": 2452
                },
                {
                    "x": 2003,
                    "y": 2452
                },
                {
                    "x": 2003,
                    "y": 2808
                },
                {
                    "x": 544,
                    "y": 2808
                }
            ],
            "category": "paragraph",
            "html": "<p id='55' style='font-size:20px'>3.1.2 How does the pretraining image set size impact accuracy? This<br>experiment studies the relationship between the number of images used in In-<br>stagram pretraining and classification accuracy on the target task. For these<br>experiments, when transferring to the target task we keep the pretrained net-<br>work weights fixed and only train a linear classifier for the target task. We make<br>this choice because when the number of pretraining images is small relative to<br>the number of target task images (e.g., 1M VS. 7M), the effect of pretraining is</p>",
            "id": 55,
            "page": 7,
            "text": "3.1.2 How does the pretraining image set size impact accuracy? This\nexperiment studies the relationship between the number of images used in In-\nstagram pretraining and classification accuracy on the target task. For these\nexperiments, when transferring to the target task we keep the pretrained net-\nwork weights fixed and only train a linear classifier for the target task. We make\nthis choice because when the number of pretraining images is small relative to\nthe number of target task images (e.g., 1M VS. 7M), the effect of pretraining is"
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 410
                },
                {
                    "x": 943,
                    "y": 410
                },
                {
                    "x": 943,
                    "y": 456
                },
                {
                    "x": 545,
                    "y": 456
                }
            ],
            "category": "header",
            "html": "<header id='56' style='font-size:16px'>8 Mahajan et al.</header>",
            "id": 56,
            "page": 8,
            "text": "8 Mahajan et al."
        },
        {
            "bounding_box": [
                {
                    "x": 555,
                    "y": 543
                },
                {
                    "x": 1997,
                    "y": 543
                },
                {
                    "x": 1997,
                    "y": 1821
                },
                {
                    "x": 555,
                    "y": 1821
                }
            ],
            "category": "figure",
            "html": "<figure><img id='57' style='font-size:14px' alt=\"Target task: ImageNet-1k Target task: ImageNet-5k\n90 60\n85\n%)\n%)\n55\n(in\n(in\n80\naccuracy\n50\n75\n70 accuracy\n45\ntop-1\n65 Source task / ResNext-101 capacity top-1\nImageNet\nInstagram (1.5k tags) / 32x4d 40\nInstagram (1.5k tags) / 32x8d\n60\nInstagram (1.5k tags) / 32x16d\nInstagram (17k tags) / 32x4d 35\n55 ImageNet\nInstagram (17k tags) / 32x8d\nInstagram (17k tags) / 32x16d\n50 30\n10' 108 109 107 108 109\nNumber of training images in source task (Instagram) Number of training images in source task (Instagram)\nTarget task: ImageNet-9k Target task: CUB2011\n50 90\n%)\n45\n80\n(in\naccuracy\n40\n%)\n70\n(in\nAccuracy\n35\ntop-1\n60\nImageNet\n30\n50\n25\n20 40\n10 108 109 107 108 109\nNumber of training images in source task (Instagram) Number of training images in source task (Instagram)\" data-coord=\"top-left:(555,543); bottom-right:(1997,1821)\" /></figure>",
            "id": 57,
            "page": 8,
            "text": "Target task: ImageNet-1k Target task: ImageNet-5k\n90 60\n85\n%)\n%)\n55\n(in\n(in\n80\naccuracy\n50\n75\n70 accuracy\n45\ntop-1\n65 Source task / ResNext-101 capacity top-1\nImageNet\nInstagram (1.5k tags) / 32x4d 40\nInstagram (1.5k tags) / 32x8d\n60\nInstagram (1.5k tags) / 32x16d\nInstagram (17k tags) / 32x4d 35\n55 ImageNet\nInstagram (17k tags) / 32x8d\nInstagram (17k tags) / 32x16d\n50 30\n10' 108 109 107 108 109\nNumber of training images in source task (Instagram) Number of training images in source task (Instagram)\nTarget task: ImageNet-9k Target task: CUB2011\n50 90\n%)\n45\n80\n(in\naccuracy\n40\n%)\n70\n(in\nAccuracy\n35\ntop-1\n60\nImageNet\n30\n50\n25\n20 40\n10 108 109 107 108 109\nNumber of training images in source task (Instagram) Number of training images in source task (Instagram)"
        },
        {
            "bounding_box": [
                {
                    "x": 547,
                    "y": 1843
                },
                {
                    "x": 2004,
                    "y": 1843
                },
                {
                    "x": 2004,
                    "y": 2027
                },
                {
                    "x": 547,
                    "y": 2027
                }
            ],
            "category": "caption",
            "html": "<caption id='58' style='font-size:18px'>Fig. 2: Classification accuracies on IN-{1k, 5k, 9k} and CUB2011 target tasks as a<br>function of the number of Instagram images used for pretraining for three network<br>architectures (colors) and two hashtag vocabularies (dashed / solid lines). Only the<br>linear classifier is trained on the target task. Higher is better.</caption>",
            "id": 58,
            "page": 8,
            "text": "Fig. 2: Classification accuracies on IN-{1k, 5k, 9k} and CUB2011 target tasks as a\nfunction of the number of Instagram images used for pretraining for three network\narchitectures (colors) and two hashtag vocabularies (dashed / solid lines). Only the\nlinear classifier is trained on the target task. Higher is better."
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 2130
                },
                {
                    "x": 2002,
                    "y": 2130
                },
                {
                    "x": 2002,
                    "y": 2227
                },
                {
                    "x": 545,
                    "y": 2227
                }
            ],
            "category": "paragraph",
            "html": "<p id='59' style='font-size:20px'>masked by the large amount of finetuning data (this was not the case in the pre-<br>vious experiment where the source task had orders of magnitude more images).</p>",
            "id": 59,
            "page": 8,
            "text": "masked by the large amount of finetuning data (this was not the case in the pre-\nvious experiment where the source task had orders of magnitude more images)."
        },
        {
            "bounding_box": [
                {
                    "x": 546,
                    "y": 2229
                },
                {
                    "x": 2003,
                    "y": 2229
                },
                {
                    "x": 2003,
                    "y": 2574
                },
                {
                    "x": 546,
                    "y": 2574
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='60' style='font-size:22px'>Figure 2 shows the classification accuracy on ImageNet validation sets (y-<br>axis) as a function of the number of Instagram training images (x-axis; note<br>the log scale) ranging from 3.5M to 3.5B images. The figure shows results for<br>models pretrained to predict 1.5k hashtags (dashed lines) or 17k hashtags (solid<br>lines) for ResNeXt-101 models with three different capacities (represented by<br>different colors). 1 The four panels correspond to ImageNet target tasks with<br>three different number of classes (1k, 5k, 9k) and CUB2011.</p>",
            "id": 60,
            "page": 8,
            "text": "Figure 2 shows the classification accuracy on ImageNet validation sets (y-\naxis) as a function of the number of Instagram training images (x-axis; note\nthe log scale) ranging from 3.5M to 3.5B images. The figure shows results for\nmodels pretrained to predict 1.5k hashtags (dashed lines) or 17k hashtags (solid\nlines) for ResNeXt-101 models with three different capacities (represented by\ndifferent colors). 1 The four panels correspond to ImageNet target tasks with\nthree different number of classes (1k, 5k, 9k) and CUB2011."
        },
        {
            "bounding_box": [
                {
                    "x": 547,
                    "y": 2577
                },
                {
                    "x": 2005,
                    "y": 2577
                },
                {
                    "x": 2005,
                    "y": 2726
                },
                {
                    "x": 547,
                    "y": 2726
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='61' style='font-size:20px'>In line with prior results [16,17], we observe near log-linear behavior: each<br>time we multiply the amount of training data by a factor of x, we observe a<br>fixed increase y in classification accuracy. While the scaling behavior is con-</p>",
            "id": 61,
            "page": 8,
            "text": "In line with prior results [16,17], we observe near log-linear behavior: each\ntime we multiply the amount of training data by a factor of x, we observe a\nfixed increase y in classification accuracy. While the scaling behavior is con-"
        },
        {
            "bounding_box": [
                {
                    "x": 565,
                    "y": 2752
                },
                {
                    "x": 1875,
                    "y": 2752
                },
                {
                    "x": 1875,
                    "y": 2803
                },
                {
                    "x": 565,
                    "y": 2803
                }
            ],
            "category": "paragraph",
            "html": "<p id='62' style='font-size:16px'>1 of images available for the 1.5k hashtag set is 940M.<br>The maximum number</p>",
            "id": 62,
            "page": 8,
            "text": "1 of images available for the 1.5k hashtag set is 940M.\nThe maximum number"
        },
        {
            "bounding_box": [
                {
                    "x": 921,
                    "y": 409
                },
                {
                    "x": 1957,
                    "y": 409
                },
                {
                    "x": 1957,
                    "y": 456
                },
                {
                    "x": 921,
                    "y": 456
                }
            ],
            "category": "header",
            "html": "<header id='63' style='font-size:20px'>Exploring the Limits of Weakly Supervised Pretraining</header>",
            "id": 63,
            "page": 9,
            "text": "Exploring the Limits of Weakly Supervised Pretraining"
        },
        {
            "bounding_box": [
                {
                    "x": 1970,
                    "y": 414
                },
                {
                    "x": 2001,
                    "y": 414
                },
                {
                    "x": 2001,
                    "y": 449
                },
                {
                    "x": 1970,
                    "y": 449
                }
            ],
            "category": "header",
            "html": "<br><header id='64' style='font-size:14px'>9</header>",
            "id": 64,
            "page": 9,
            "text": "9"
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 516
                },
                {
                    "x": 2004,
                    "y": 516
                },
                {
                    "x": 2004,
                    "y": 1012
                },
                {
                    "x": 545,
                    "y": 1012
                }
            ],
            "category": "paragraph",
            "html": "<p id='65' style='font-size:18px'>sistent across hashtag vocabulary sizes and models, the accuracy increase y is<br>larger for higher-capacity networks: across all figures, the lines corresponding to<br>ResNeXt-101 32x 16d networks (purple) are steeper than those corresponding to<br>32x8d and 32x4d models. This result suggests that when training convolutional<br>networks on billions of training images, current network architectures are prone<br>to underfitting. We also observe log-linear scaling break down in two regimes:<br>(1) because accuracy is bounded, endless log-linear scaling is not possible. On<br>datasets like IN-1k and CUB2011 the ceiling effect necessarily creates sub-log-<br>linear scaling. (2) We observe a deviation from log-linear scaling in the 1B to<br>3.5B image regime even without apparent ceiling effects on IN-{5k, 9k}.</p>",
            "id": 65,
            "page": 9,
            "text": "sistent across hashtag vocabulary sizes and models, the accuracy increase y is\nlarger for higher-capacity networks: across all figures, the lines corresponding to\nResNeXt-101 32x 16d networks (purple) are steeper than those corresponding to\n32x8d and 32x4d models. This result suggests that when training convolutional\nnetworks on billions of training images, current network architectures are prone\nto underfitting. We also observe log-linear scaling break down in two regimes:\n(1) because accuracy is bounded, endless log-linear scaling is not possible. On\ndatasets like IN-1k and CUB2011 the ceiling effect necessarily creates sub-log-\nlinear scaling. (2) We observe a deviation from log-linear scaling in the 1B to\n3.5B image regime even without apparent ceiling effects on IN-{5k, 9k}."
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 1018
                },
                {
                    "x": 2004,
                    "y": 1018
                },
                {
                    "x": 2004,
                    "y": 1461
                },
                {
                    "x": 545,
                    "y": 1461
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='66' style='font-size:18px'>These plots also illustrate an interesting effect of the hashtag vocabulary on<br>the transfer task accuracy. On IN-1k, networks pretrained on the target-task-<br>aligned 1.5k hashtags outperform those trained using a larger hashtag vocabu-<br>lary, because the 1.5k hashtags were selected to match the ImageNet synsets.<br>However, as the matching between hashtag vocabulary and target classes disap-<br>pears and the visual variety in the transfer task increases, networks pretrained<br>to recognize a larger number of hashtags increasingly outperform networks pre-<br>trained on fewer hashtags: on the IN-9k transfer task, the difference in accuracy<br>between networks trained on 1.5k and those trained on 17k hashtags is ~7%.</p>",
            "id": 66,
            "page": 9,
            "text": "These plots also illustrate an interesting effect of the hashtag vocabulary on\nthe transfer task accuracy. On IN-1k, networks pretrained on the target-task-\naligned 1.5k hashtags outperform those trained using a larger hashtag vocabu-\nlary, because the 1.5k hashtags were selected to match the ImageNet synsets.\nHowever, as the matching between hashtag vocabulary and target classes disap-\npears and the visual variety in the transfer task increases, networks pretrained\nto recognize a larger number of hashtags increasingly outperform networks pre-\ntrained on fewer hashtags: on the IN-9k transfer task, the difference in accuracy\nbetween networks trained on 1.5k and those trained on 17k hashtags is ~7%."
        },
        {
            "bounding_box": [
                {
                    "x": 546,
                    "y": 1467
                },
                {
                    "x": 2003,
                    "y": 1467
                },
                {
                    "x": 2003,
                    "y": 1864
                },
                {
                    "x": 546,
                    "y": 1864
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='67' style='font-size:18px'>The highest accuracies on val-IN-1k are 83.3% (source: IG-940M-1k) and<br>83.6% (source: IG-3.5B-17k), both with ResNeXt-101 32x16d. These results are<br>obtained by training a linear classifier on fixed features and yet are nearly as good<br>as full network finetuning, demonstrating the effectiveness of the feature repre-<br>sentation learned from hashtag prediction. These results also have low variance:<br>we pretrained the ResNeXt-101 32x 16d architecture of two different random<br>samples of 1B images and then trained linear classifiers on IN-{1k, 5k, 9k} find-<br>ing a difference in top-1 accuracy of less than 0.1% in all cases.</p>",
            "id": 67,
            "page": 9,
            "text": "The highest accuracies on val-IN-1k are 83.3% (source: IG-940M-1k) and\n83.6% (source: IG-3.5B-17k), both with ResNeXt-101 32x16d. These results are\nobtained by training a linear classifier on fixed features and yet are nearly as good\nas full network finetuning, demonstrating the effectiveness of the feature repre-\nsentation learned from hashtag prediction. These results also have low variance:\nwe pretrained the ResNeXt-101 32x 16d architecture of two different random\nsamples of 1B images and then trained linear classifiers on IN-{1k, 5k, 9k} find-\ning a difference in top-1 accuracy of less than 0.1% in all cases."
        },
        {
            "bounding_box": [
                {
                    "x": 546,
                    "y": 1869
                },
                {
                    "x": 2004,
                    "y": 1869
                },
                {
                    "x": 2004,
                    "y": 2168
                },
                {
                    "x": 546,
                    "y": 2168
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='68' style='font-size:16px'>To test whether the above observations generalize to fine-grained classifica-<br>tion, we repeated the experiments on the CUB2011 dataset, and show the results<br>in Figure 2, bottom right. The curves reveal that when training data is limited,<br>the 1.5k hashtag dataset is better, but once the number of training images sur-<br>passes ~100M, the larger 17k hashtag dataset prevails, presumably because it<br>represents a more diverse visual distribution with more fine-grained concepts.</p>",
            "id": 68,
            "page": 9,
            "text": "To test whether the above observations generalize to fine-grained classifica-\ntion, we repeated the experiments on the CUB2011 dataset, and show the results\nin Figure 2, bottom right. The curves reveal that when training data is limited,\nthe 1.5k hashtag dataset is better, but once the number of training images sur-\npasses ~100M, the larger 17k hashtag dataset prevails, presumably because it\nrepresents a more diverse visual distribution with more fine-grained concepts."
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 2200
                },
                {
                    "x": 2004,
                    "y": 2200
                },
                {
                    "x": 2004,
                    "y": 2703
                },
                {
                    "x": 545,
                    "y": 2703
                }
            ],
            "category": "paragraph",
            "html": "<p id='69' style='font-size:16px'>3.1.3 What is the effect of hashtag label noise on model accuracy? A<br>major difference between hashtag supervision and the labels provided in datasets<br>such as ImageNet is that hashtag supervision is inherently noisy: users may apply<br>hashtags that are irrelevant to the visual content of the image, or they may have<br>left out hashtags that would have been visually relevant [33]. Because an exact<br>characterization of this label noise is difficult, instead, we investigate the effect<br>of injecting additional label noise on the accuracy of our networks. To do SO, we<br>pretrain ResNeXt-101 32x16d networks on a version of IG-1B-17k in which we<br>randomly replaced p% of the hashtags by hashtags obtained by sampling from<br>the marginal distribution over hashtags (excluding the tag to be replaced).</p>",
            "id": 69,
            "page": 9,
            "text": "3.1.3 What is the effect of hashtag label noise on model accuracy? A\nmajor difference between hashtag supervision and the labels provided in datasets\nsuch as ImageNet is that hashtag supervision is inherently noisy: users may apply\nhashtags that are irrelevant to the visual content of the image, or they may have\nleft out hashtags that would have been visually relevant [33]. Because an exact\ncharacterization of this label noise is difficult, instead, we investigate the effect\nof injecting additional label noise on the accuracy of our networks. To do SO, we\npretrain ResNeXt-101 32x16d networks on a version of IG-1B-17k in which we\nrandomly replaced p% of the hashtags by hashtags obtained by sampling from\nthe marginal distribution over hashtags (excluding the tag to be replaced)."
        },
        {
            "bounding_box": [
                {
                    "x": 546,
                    "y": 2704
                },
                {
                    "x": 2004,
                    "y": 2704
                },
                {
                    "x": 2004,
                    "y": 2803
                },
                {
                    "x": 546,
                    "y": 2803
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='70' style='font-size:16px'>Figure 3 shows the ImageNet classification accuracy of the resulting networks<br>for different numbers of classes at three levels, p, of artificial label noise as well</p>",
            "id": 70,
            "page": 9,
            "text": "Figure 3 shows the ImageNet classification accuracy of the resulting networks\nfor different numbers of classes at three levels, p, of artificial label noise as well"
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 408
                },
                {
                    "x": 943,
                    "y": 408
                },
                {
                    "x": 943,
                    "y": 457
                },
                {
                    "x": 549,
                    "y": 457
                }
            ],
            "category": "header",
            "html": "<header id='71' style='font-size:16px'>10 Mahajan et al.</header>",
            "id": 71,
            "page": 10,
            "text": "10 Mahajan et al."
        },
        {
            "bounding_box": [
                {
                    "x": 553,
                    "y": 512
                },
                {
                    "x": 1246,
                    "y": 512
                },
                {
                    "x": 1246,
                    "y": 1067
                },
                {
                    "x": 553,
                    "y": 1067
                }
            ],
            "category": "figure",
            "html": "<figure><img id='72' style='font-size:14px' alt=\"90\nSource task\n82.1 81.5 80.2\nNo label noise\n%) 80\nLabel noise: 10%\n76.1\nLabel noise: 25%\n(in\n70 Label noise: 50%\naccuracy\n60\n52.651.7\ntop-1\n50.3\n50\n46.1\nImageNet\n42.7 41.9\n40.6\n40\n36.6\n30\n20\n1,000 5,000 9,000\nNumber of classes in target task (ImageNet)\" data-coord=\"top-left:(553,512); bottom-right:(1246,1067)\" /></figure>",
            "id": 72,
            "page": 10,
            "text": "90\nSource task\n82.1 81.5 80.2\nNo label noise\n%) 80\nLabel noise: 10%\n76.1\nLabel noise: 25%\n(in\n70 Label noise: 50%\naccuracy\n60\n52.651.7\ntop-1\n50.3\n50\n46.1\nImageNet\n42.7 41.9\n40.6\n40\n36.6\n30\n20\n1,000 5,000 9,000\nNumber of classes in target task (ImageNet)"
        },
        {
            "bounding_box": [
                {
                    "x": 547,
                    "y": 1106
                },
                {
                    "x": 1263,
                    "y": 1106
                },
                {
                    "x": 1263,
                    "y": 1432
                },
                {
                    "x": 547,
                    "y": 1432
                }
            ],
            "category": "caption",
            "html": "<caption id='73' style='font-size:18px'>Fig. 3: Classification accuracy of<br>ResNeXt-101 32x 16d, pretrained on<br>IG-1B-17k, on val-IN-{1k, 5k, 9k} at<br>three levels of injected label noise. The<br>no-label-noise baseline is trained on<br>the original hashtags. Only the linear<br>classifier is trained on the target task.</caption>",
            "id": 73,
            "page": 10,
            "text": "Fig. 3: Classification accuracy of\nResNeXt-101 32x 16d, pretrained on\nIG-1B-17k, on val-IN-{1k, 5k, 9k} at\nthree levels of injected label noise. The\nno-label-noise baseline is trained on\nthe original hashtags. Only the linear\nclassifier is trained on the target task."
        },
        {
            "bounding_box": [
                {
                    "x": 1290,
                    "y": 512
                },
                {
                    "x": 1993,
                    "y": 512
                },
                {
                    "x": 1993,
                    "y": 1069
                },
                {
                    "x": 1290,
                    "y": 1069
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='74' style='font-size:14px' alt=\"90\nClass sampling\nNatural\n%) 80 78.2 Square root\n76.7\nUniform\n(in\n72.9\n70\naccuracy\n60\ntop-1\n50 48.9 48.5\n43.6\nImageNet\n40 39.0 38.8\n33.3\n30\n20\n1,000 5,000 9,000\nNumber of classes in target task (ImageNet)\" data-coord=\"top-left:(1290,512); bottom-right:(1993,1069)\" /></figure>",
            "id": 74,
            "page": 10,
            "text": "90\nClass sampling\nNatural\n%) 80 78.2 Square root\n76.7\nUniform\n(in\n72.9\n70\naccuracy\n60\ntop-1\n50 48.9 48.5\n43.6\nImageNet\n40 39.0 38.8\n33.3\n30\n20\n1,000 5,000 9,000\nNumber of classes in target task (ImageNet)"
        },
        {
            "bounding_box": [
                {
                    "x": 1288,
                    "y": 1109
                },
                {
                    "x": 2004,
                    "y": 1109
                },
                {
                    "x": 2004,
                    "y": 1432
                },
                {
                    "x": 1288,
                    "y": 1432
                }
            ],
            "category": "caption",
            "html": "<caption id='75' style='font-size:18px'>Fig. 4: Classification accuracy of<br>ResNeXt-101 32x4d, pretrained on<br>IG-1B-17k, on val-IN-{1k, 5k, 9k} for<br>three different hashtag sampling strate-<br>gies: natural sampling, uniform sampling,<br>and square-root sampling. Only the linear<br>classifier is trained on the target task.</caption>",
            "id": 75,
            "page": 10,
            "text": "Fig. 4: Classification accuracy of\nResNeXt-101 32x4d, pretrained on\nIG-1B-17k, on val-IN-{1k, 5k, 9k} for\nthree different hashtag sampling strate-\ngies: natural sampling, uniform sampling,\nand square-root sampling. Only the linear\nclassifier is trained on the target task."
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 1535
                },
                {
                    "x": 2004,
                    "y": 1535
                },
                {
                    "x": 2004,
                    "y": 1883
                },
                {
                    "x": 545,
                    "y": 1883
                }
            ],
            "category": "paragraph",
            "html": "<p id='76' style='font-size:20px'>as for a baseline in which no artificial label noise was added during pretraining.<br>We only train the final linear classifier on the target task, because full finetuning<br>may mask the damage caused by pretraining noise. The results suggest that the<br>networks are remarkably resilient against label noise: a noise level of p = 10%<br>leads to a loss of less than 1% in classification accuracy, and at p = 25% label<br>noise, the reduction in accuracy is around 2%. These results suggest that label<br>noise may be a limited issue if networks are trained on billions of images.</p>",
            "id": 76,
            "page": 10,
            "text": "as for a baseline in which no artificial label noise was added during pretraining.\nWe only train the final linear classifier on the target task, because full finetuning\nmay mask the damage caused by pretraining noise. The results suggest that the\nnetworks are remarkably resilient against label noise: a noise level of p = 10%\nleads to a loss of less than 1% in classification accuracy, and at p = 25% label\nnoise, the reduction in accuracy is around 2%. These results suggest that label\nnoise may be a limited issue if networks are trained on billions of images."
        },
        {
            "bounding_box": [
                {
                    "x": 544,
                    "y": 1903
                },
                {
                    "x": 2004,
                    "y": 1903
                },
                {
                    "x": 2004,
                    "y": 2704
                },
                {
                    "x": 544,
                    "y": 2704
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='77' style='font-size:22px'>3.1.4 How does the sampling of pretraining data impact accuracy?<br>Another difference between hashtag and ImageNet supervision is that, like in<br>language modeling, hashtags are governed by a Zipfian distribution. Prior stud-<br>ies in language modeling found that resampling Zipfian distributions reduces<br>the impact of the head of the word distribution on the overall training loss [34].<br>Motivated by this work, we perform experiments in which we evaluate three<br>different types of data sampling in the Instagram pretraining: (1) a natural sam-<br>pling in which we sample images and hashtags according to the distribution by<br>which they appear on Instagram; (2) square-root sampling [34] in which we take<br>the square-root of the head of the hashtag distribution, renormalize, and sample<br>according to the resulting distribution (due to practical considerations, our im-<br>plementation is slightly different; see supplemental material); and (3) uniform<br>sampling in which we sample a hashtag uniformly at random, and then sample<br>an image that has this hashtag associated to it uniformly at random [16]. (Aside<br>from this experiment, we always pretrain on Instagram data using square-root<br>sampling.) As before, we only train the final linear classifier on the target task.</p>",
            "id": 77,
            "page": 10,
            "text": "3.1.4 How does the sampling of pretraining data impact accuracy?\nAnother difference between hashtag and ImageNet supervision is that, like in\nlanguage modeling, hashtags are governed by a Zipfian distribution. Prior stud-\nies in language modeling found that resampling Zipfian distributions reduces\nthe impact of the head of the word distribution on the overall training loss [34].\nMotivated by this work, we perform experiments in which we evaluate three\ndifferent types of data sampling in the Instagram pretraining: (1) a natural sam-\npling in which we sample images and hashtags according to the distribution by\nwhich they appear on Instagram; (2) square-root sampling [34] in which we take\nthe square-root of the head of the hashtag distribution, renormalize, and sample\naccording to the resulting distribution (due to practical considerations, our im-\nplementation is slightly different; see supplemental material); and (3) uniform\nsampling in which we sample a hashtag uniformly at random, and then sample\nan image that has this hashtag associated to it uniformly at random [16]. (Aside\nfrom this experiment, we always pretrain on Instagram data using square-root\nsampling.) As before, we only train the final linear classifier on the target task."
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 2702
                },
                {
                    "x": 2005,
                    "y": 2702
                },
                {
                    "x": 2005,
                    "y": 2803
                },
                {
                    "x": 545,
                    "y": 2803
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='78' style='font-size:20px'>Figure 4 displays classification accuracy as a function of the number of Im-<br>ageNet classes for networks that were pretrained on IG-1B-17k using the three</p>",
            "id": 78,
            "page": 10,
            "text": "Figure 4 displays classification accuracy as a function of the number of Im-\nageNet classes for networks that were pretrained on IG-1B-17k using the three"
        },
        {
            "bounding_box": [
                {
                    "x": 920,
                    "y": 409
                },
                {
                    "x": 1991,
                    "y": 409
                },
                {
                    "x": 1991,
                    "y": 456
                },
                {
                    "x": 920,
                    "y": 456
                }
            ],
            "category": "header",
            "html": "<header id='79' style='font-size:20px'>Exploring the Limits of Weakly Supervised Pretraining 11</header>",
            "id": 79,
            "page": 11,
            "text": "Exploring the Limits of Weakly Supervised Pretraining 11"
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 515
                },
                {
                    "x": 2003,
                    "y": 515
                },
                {
                    "x": 2003,
                    "y": 814
                },
                {
                    "x": 545,
                    "y": 814
                }
            ],
            "category": "paragraph",
            "html": "<p id='80' style='font-size:20px'>sampling strategies. The results show that resampling of the hashtag distribution<br>is important in order to obtain good transfer to ImageNet image-classification<br>tasks: using uniform or square-root sampling leads to an accuracy improvement<br>of 5 to 6% irrespective of the number of ImageNet classes in the transfer task.<br>In line with prior results, the figure also shows that larger hashtag vocabularies<br>lead to increasing accuracy improvements as the number of target classes grows.</p>",
            "id": 80,
            "page": 11,
            "text": "sampling strategies. The results show that resampling of the hashtag distribution\nis important in order to obtain good transfer to ImageNet image-classification\ntasks: using uniform or square-root sampling leads to an accuracy improvement\nof 5 to 6% irrespective of the number of ImageNet classes in the transfer task.\nIn line with prior results, the figure also shows that larger hashtag vocabularies\nlead to increasing accuracy improvements as the number of target classes grows."
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 834
                },
                {
                    "x": 1242,
                    "y": 834
                },
                {
                    "x": 1242,
                    "y": 1708
                },
                {
                    "x": 545,
                    "y": 1708
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='81' style='font-size:18px'>3.1.5 With billions of images, is<br>transfer learning model-capacity<br>bound? Now, we look at what hap-<br>pens when we train convolutional net-<br>works that are substantially larger<br>than those typically used in recent<br>studies (and our experiments SO far).<br>In particular, we use IG-940M-1.5k<br>to pretrain ResNeXt-101 32x32d and<br>ResNeXt-101 32x 48d, which have<br>2.4x and 4.3x more add-mult FLOPs<br>than ResNeXt-101 32x16d, respec-<br>tively. Using these \"super-sized\" mod-<br>els improves val-IN-1k results over the<br>32x16d model from 84.2% top-1 accu-<br>racy to 85.1% and 85.4%, respectively<br>(top-5 accuracy: from 97.2% to 97.5%<br>and when</p>",
            "id": 81,
            "page": 11,
            "text": "3.1.5 With billions of images, is\ntransfer learning model-capacity\nbound? Now, we look at what hap-\npens when we train convolutional net-\nworks that are substantially larger\nthan those typically used in recent\nstudies (and our experiments SO far).\nIn particular, we use IG-940M-1.5k\nto pretrain ResNeXt-101 32x32d and\nResNeXt-101 32x 48d, which have\n2.4x and 4.3x more add-mult FLOPs\nthan ResNeXt-101 32x16d, respec-\ntively. Using these \"super-sized\" mod-\nels improves val-IN-1k results over the\n32x16d model from 84.2% top-1 accu-\nracy to 85.1% and 85.4%, respectively\n(top-5 accuracy: from 97.2% to 97.5%\nand when"
        },
        {
            "bounding_box": [
                {
                    "x": 1274,
                    "y": 856
                },
                {
                    "x": 1993,
                    "y": 856
                },
                {
                    "x": 1993,
                    "y": 1430
                },
                {
                    "x": 1274,
                    "y": 1430
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='82' style='font-size:14px' alt=\"88\nSource task\n。 。 ImageNet (target = source)\n1% 86\nInstagram (940M, 1.5k tags)\n(in\naccuracy 32x48d\n32x32d\n84\n32x16d\ntop-1\n32x8d\n82\nImageNet\n32x4d\n80\n-●\n32x16d 32x32d 32x48d\n· 32x8d\n32x4d\n78\n233\n2 235 236 237\nModel capacity (number of mult-add operations)\" data-coord=\"top-left:(1274,856); bottom-right:(1993,1430)\" /></figure>",
            "id": 82,
            "page": 11,
            "text": "88\nSource task\n。 。 ImageNet (target = source)\n1% 86\nInstagram (940M, 1.5k tags)\n(in\naccuracy 32x48d\n32x32d\n84\n32x16d\ntop-1\n32x8d\n82\nImageNet\n32x4d\n80\n-●\n32x16d 32x32d 32x48d\n· 32x8d\n32x4d\n78\n233\n2 235 236 237\nModel capacity (number of mult-add operations)"
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 1457
                },
                {
                    "x": 2003,
                    "y": 1457
                },
                {
                    "x": 2003,
                    "y": 1645
                },
                {
                    "x": 1267,
                    "y": 1645
                }
            ],
            "category": "caption",
            "html": "<caption id='83' style='font-size:16px'>Fig. 5: Classification accuracy on val-IN-1k<br>using ResNeXt-101 32x{4, 8 16, 32, 48}d<br>with and without pretraining on the IG-<br>940M-1.5k dataset.</caption>",
            "id": 83,
            "page": 11,
            "text": "Fig. 5: Classification accuracy on val-IN-1k\nusing ResNeXt-101 32x{4, 8 16, 32, 48}d\nwith and without pretraining on the IG-\n940M-1.5k dataset."
        },
        {
            "bounding_box": [
                {
                    "x": 547,
                    "y": 1688
                },
                {
                    "x": 2001,
                    "y": 1688
                },
                {
                    "x": 2001,
                    "y": 1931
                },
                {
                    "x": 547,
                    "y": 1931
                }
            ],
            "category": "paragraph",
            "html": "<p id='84' style='font-size:22px'>97.6%). By comparison, training from scratch on IN-1k, top-1 accuracy<br>saturates at around 79.6% with the 32x 16d model and does not meaningfully<br>increase by using larger models. These results, plotted in Figure 5, indicate that<br>with large-scale Instagram hashtag training, transfer-learning performance ap-<br>pears bottlenecked by model capacity.</p>",
            "id": 84,
            "page": 11,
            "text": "97.6%). By comparison, training from scratch on IN-1k, top-1 accuracy\nsaturates at around 79.6% with the 32x 16d model and does not meaningfully\nincrease by using larger models. These results, plotted in Figure 5, indicate that\nwith large-scale Instagram hashtag training, transfer-learning performance ap-\npears bottlenecked by model capacity."
        },
        {
            "bounding_box": [
                {
                    "x": 546,
                    "y": 1956
                },
                {
                    "x": 2003,
                    "y": 1956
                },
                {
                    "x": 2003,
                    "y": 2603
                },
                {
                    "x": 546,
                    "y": 2603
                }
            ],
            "category": "paragraph",
            "html": "<p id='85' style='font-size:18px'>3.1.6 On what visual classes is Instagram pretraining most helpful?<br>Our results with different hashtag vocabularies suggest that choosing the right<br>hashtag vocabulary may be at least as important as scaling model training to<br>billions of images. Specifically, we expect that some hashtags are easier to predict<br>because they are more \"visually concrete\" than others: whereas #eiffeltower<br>corresponds to a very specific visual scene, #party may correspond to a large<br>variety of visual scenes. We matched the 17k Instagram hashtags with a list of<br>40k \"concreteness\" values of nouns [35] to obtain 5,730 hashtag concreteness<br>values. Figure 6 displays these hashtag concreteness values and the accuracy<br>of predicting the hashtags correctly (measured in terms of AUC on balanced<br>validation sets) in a scatter plot. The figure suggests a clear relation between<br>the concreteness of a noun and the model's ability to predict the corresponding<br>hashtag: the Pearson correlation, P, between both variables is 0.43.</p>",
            "id": 85,
            "page": 11,
            "text": "3.1.6 On what visual classes is Instagram pretraining most helpful?\nOur results with different hashtag vocabularies suggest that choosing the right\nhashtag vocabulary may be at least as important as scaling model training to\nbillions of images. Specifically, we expect that some hashtags are easier to predict\nbecause they are more \"visually concrete\" than others: whereas #eiffeltower\ncorresponds to a very specific visual scene, #party may correspond to a large\nvariety of visual scenes. We matched the 17k Instagram hashtags with a list of\n40k \"concreteness\" values of nouns [35] to obtain 5,730 hashtag concreteness\nvalues. Figure 6 displays these hashtag concreteness values and the accuracy\nof predicting the hashtags correctly (measured in terms of AUC on balanced\nvalidation sets) in a scatter plot. The figure suggests a clear relation between\nthe concreteness of a noun and the model's ability to predict the corresponding\nhashtag: the Pearson correlation, P, between both variables is 0.43."
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 2605
                },
                {
                    "x": 2004,
                    "y": 2605
                },
                {
                    "x": 2004,
                    "y": 2805
                },
                {
                    "x": 545,
                    "y": 2805
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='86' style='font-size:18px'>We also analyze the effect of Instagram pretraining on recognizing individual<br>IN-1k classes. Figure 7 shows the cumulative distribution function (CDF) of<br>the absolute accuracy improvement per class of a ResNeXt-101 32x16d network<br>pretrained on IG-3.5B-17k compared to the same network trained on IN-1k.</p>",
            "id": 86,
            "page": 11,
            "text": "We also analyze the effect of Instagram pretraining on recognizing individual\nIN-1k classes. Figure 7 shows the cumulative distribution function (CDF) of\nthe absolute accuracy improvement per class of a ResNeXt-101 32x16d network\npretrained on IG-3.5B-17k compared to the same network trained on IN-1k."
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 408
                },
                {
                    "x": 943,
                    "y": 408
                },
                {
                    "x": 943,
                    "y": 457
                },
                {
                    "x": 549,
                    "y": 457
                }
            ],
            "category": "header",
            "html": "<header id='87' style='font-size:16px'>12 Mahajan et al.</header>",
            "id": 87,
            "page": 12,
            "text": "12 Mahajan et al."
        },
        {
            "bounding_box": [
                {
                    "x": 568,
                    "y": 534
                },
                {
                    "x": 1206,
                    "y": 534
                },
                {
                    "x": 1206,
                    "y": 1024
                },
                {
                    "x": 568,
                    "y": 1024
                }
            ],
            "category": "figure",
            "html": "<figure><img id='88' style='font-size:14px' alt=\"1.00\n0.95\n(AUC)\n0.90\ncurve\n0.85\nROC\nunder\n0.80\nArea\n0.75\n0.70\n0.65.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nConcreteness\" data-coord=\"top-left:(568,534); bottom-right:(1206,1024)\" /></figure>",
            "id": 88,
            "page": 12,
            "text": "1.00\n0.95\n(AUC)\n0.90\ncurve\n0.85\nROC\nunder\n0.80\nArea\n0.75\n0.70\n0.65.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\nConcreteness"
        },
        {
            "bounding_box": [
                {
                    "x": 546,
                    "y": 1053
                },
                {
                    "x": 1262,
                    "y": 1053
                },
                {
                    "x": 1262,
                    "y": 1243
                },
                {
                    "x": 546,
                    "y": 1243
                }
            ],
            "category": "caption",
            "html": "<caption id='89' style='font-size:18px'>Fig. 6: Area under ROC curve (AUC) for<br>hashtag prediction as a function of the<br>hashtag concreteness [35], and correspond-<br>ing least-squares fit (p=0.43).</caption>",
            "id": 89,
            "page": 12,
            "text": "Fig. 6: Area under ROC curve (AUC) for\nhashtag prediction as a function of the\nhashtag concreteness [35], and correspond-\ning least-squares fit (p=0.43)."
        },
        {
            "bounding_box": [
                {
                    "x": 1312,
                    "y": 515
                },
                {
                    "x": 1982,
                    "y": 515
                },
                {
                    "x": 1982,
                    "y": 1032
                },
                {
                    "x": 1312,
                    "y": 1032
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='90' alt=\"1000\nCDF on IN-1k\nclasses\n800\nImageNet\n600\nof\nnumber\n400\nCumulative\n200\n0\n-10 0 10 20 30 40 50\nAbsolute accuracy improvement (in %)\" data-coord=\"top-left:(1312,515); bottom-right:(1982,1032)\" /></figure>",
            "id": 90,
            "page": 12,
            "text": "1000\nCDF on IN-1k\nclasses\n800\nImageNet\n600\nof\nnumber\n400\nCumulative\n200\n0\n-10 0 10 20 30 40 50\nAbsolute accuracy improvement (in %)"
        },
        {
            "bounding_box": [
                {
                    "x": 1289,
                    "y": 1055
                },
                {
                    "x": 2002,
                    "y": 1055
                },
                {
                    "x": 2002,
                    "y": 1243
                },
                {
                    "x": 1289,
                    "y": 1243
                }
            ],
            "category": "caption",
            "html": "<caption id='91' style='font-size:16px'>Fig. 7: CDF of absolute per-class accu-<br>racy improvements on IN-1k validation set<br>of an Instagram-pretrained network com-<br>pared to ImageNet baseline.</caption>",
            "id": 91,
            "page": 12,
            "text": "Fig. 7: CDF of absolute per-class accu-\nracy improvements on IN-1k validation set\nof an Instagram-pretrained network com-\npared to ImageNet baseline."
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 1342
                },
                {
                    "x": 2003,
                    "y": 1342
                },
                {
                    "x": 2003,
                    "y": 1444
                },
                {
                    "x": 545,
                    "y": 1444
                }
            ],
            "category": "paragraph",
            "html": "<p id='92' style='font-size:20px'>Accuracy improves on more than 80% of the IN-1k classes, with 20% of classes<br>gaining at least 10 percentage points.</p>",
            "id": 92,
            "page": 12,
            "text": "Accuracy improves on more than 80% of the IN-1k classes, with 20% of classes\ngaining at least 10 percentage points."
        },
        {
            "bounding_box": [
                {
                    "x": 548,
                    "y": 1520
                },
                {
                    "x": 1029,
                    "y": 1520
                },
                {
                    "x": 1029,
                    "y": 1569
                },
                {
                    "x": 548,
                    "y": 1569
                }
            ],
            "category": "paragraph",
            "html": "<p id='93' style='font-size:22px'>3.2 Object Detection</p>",
            "id": 93,
            "page": 12,
            "text": "3.2 Object Detection"
        },
        {
            "bounding_box": [
                {
                    "x": 546,
                    "y": 1604
                },
                {
                    "x": 2004,
                    "y": 1604
                },
                {
                    "x": 2004,
                    "y": 1853
                },
                {
                    "x": 546,
                    "y": 1853
                }
            ],
            "category": "paragraph",
            "html": "<p id='94' style='font-size:22px'>We have looked at target tasks that require image classification, but we are also<br>interested in observing if pretraining on Instagram hashtag data can improve<br>object detection and instance segmentation tasks by finetuning networks on the<br>COCO dataset [18]. We use Mask R-CNN [4,36] and experiment with ResNeXt-<br>101 FPN [37] backbones of three different capacities (see Figure 8).</p>",
            "id": 94,
            "page": 12,
            "text": "We have looked at target tasks that require image classification, but we are also\ninterested in observing if pretraining on Instagram hashtag data can improve\nobject detection and instance segmentation tasks by finetuning networks on the\nCOCO dataset [18]. We use Mask R-CNN [4,36] and experiment with ResNeXt-\n101 FPN [37] backbones of three different capacities (see Figure 8)."
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 1857
                },
                {
                    "x": 2004,
                    "y": 1857
                },
                {
                    "x": 2004,
                    "y": 2453
                },
                {
                    "x": 545,
                    "y": 2453
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='95' style='font-size:20px'>We compare performance on the 2017 test-dev set using several different pre-<br>trained networks. As baselines, we use IN-{1k, 5k} pretraining (IN-9k performs<br>no better than IN-5k) and compare them to IG-940M-1k and IG-1B-17k. For the<br>largest model (32x16d) we also include results for IG-3.5B-17k. We use standard<br>settings [36] for end-to-end Mask R-CNN training with one exception: for the<br>Instagram pretrained models we found it necessary to perform grid search for the<br>finetuning learning rate on the validation set. We found that models pretrained<br>on the Instagram data require finetuning learning rates that are ~4-10x lower<br>than ImageNet pretrained models (see supplemental material). This finding il-<br>lustrates that finetuning recipes developed for ImageNet pretrained models do<br>not transfer to new pretraining sets: a larger amount of pretraining data implies<br>the need for lower finetuning learning rates.</p>",
            "id": 95,
            "page": 12,
            "text": "We compare performance on the 2017 test-dev set using several different pre-\ntrained networks. As baselines, we use IN-{1k, 5k} pretraining (IN-9k performs\nno better than IN-5k) and compare them to IG-940M-1k and IG-1B-17k. For the\nlargest model (32x16d) we also include results for IG-3.5B-17k. We use standard\nsettings [36] for end-to-end Mask R-CNN training with one exception: for the\nInstagram pretrained models we found it necessary to perform grid search for the\nfinetuning learning rate on the validation set. We found that models pretrained\non the Instagram data require finetuning learning rates that are ~4-10x lower\nthan ImageNet pretrained models (see supplemental material). This finding il-\nlustrates that finetuning recipes developed for ImageNet pretrained models do\nnot transfer to new pretraining sets: a larger amount of pretraining data implies\nthe need for lower finetuning learning rates."
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 2456
                },
                {
                    "x": 2004,
                    "y": 2456
                },
                {
                    "x": 2004,
                    "y": 2805
                },
                {
                    "x": 545,
                    "y": 2805
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='96' style='font-size:20px'>Figure 8 shows two interesting trends. First, we observe that when using<br>large amounts of pretraining data, detection is model capacity bound: with the<br>lowest capacity model (32x 4d), the gains from larger datasets are small or even<br>negative, but as model capacity increases the larger pretraining datasets yield<br>consistent improvements. We need even larger models to take advantage of the<br>large-scale pretraining data. The second trend we observe comes from comparing<br>COCO's default AP metric (average precision averaged over intersection-over-</p>",
            "id": 96,
            "page": 12,
            "text": "Figure 8 shows two interesting trends. First, we observe that when using\nlarge amounts of pretraining data, detection is model capacity bound: with the\nlowest capacity model (32x 4d), the gains from larger datasets are small or even\nnegative, but as model capacity increases the larger pretraining datasets yield\nconsistent improvements. We need even larger models to take advantage of the\nlarge-scale pretraining data. The second trend we observe comes from comparing\nCOCO's default AP metric (average precision averaged over intersection-over-"
        },
        {
            "bounding_box": [
                {
                    "x": 920,
                    "y": 411
                },
                {
                    "x": 1991,
                    "y": 411
                },
                {
                    "x": 1991,
                    "y": 455
                },
                {
                    "x": 920,
                    "y": 455
                }
            ],
            "category": "header",
            "html": "<header id='97' style='font-size:18px'>Exploring the Limits of Weakly Supervised Pretraining 13</header>",
            "id": 97,
            "page": 13,
            "text": "Exploring the Limits of Weakly Supervised Pretraining 13"
        },
        {
            "bounding_box": [
                {
                    "x": 555,
                    "y": 552
                },
                {
                    "x": 2002,
                    "y": 552
                },
                {
                    "x": 2002,
                    "y": 1810
                },
                {
                    "x": 555,
                    "y": 1810
                }
            ],
            "category": "figure",
            "html": "<figure><img id='98' style='font-size:14px' alt=\"Target task: COCO detection (box AP) Target task: COCO detection (box AP@50)\n55 70\n68.3\nSource task 67.9\n67.2 67.2 67.1\nIN-1k (1.3M, 1k labels) IG (1B, 17k tags) 66.5\n65.5 65.6\n50 IN-5k (6.6M, 5k labels) IG (3.5B, 17k tags) 1% 65 64.4 64.8 64.9\n%)\n63.8\n(in\nIG (940M, 1.5k tags) 63.2\n(in\n45 45.0 45.2 44.8 45.2 60\nAP\n44.2\n43.7 43.7\n42.7 42.9\nbox\n42.3 42.0 AP@50\n41.6\n40.9\nbox\n40 55\ntest-dev\ntest-dev\n35 50\nCOCO\nCOCO\n30 45\n25 40\n32x4d 32x8d 32x16d 32x4d 32x8d 32x16d\nResNeXt-101 capacity (in Mask R-CNN) ResNeXt-101 capacity (in Mask R-CNN)\nTarget task: COCO detection (mask AP) Target task: COCO detection (mask AP@50)\n55 70\n65 64.3 64.4\n50 %)\n%)\n(in\n63.7 63.7\n63.0 62.7\n(in\n62.3\nAP@50\n61.5 61.7\n61.0 61.1\n60.3\nAP\n45 60 59.6\nmask\nmask\n40 39.6 39.7 39.4 55\n39.2 39.3\ntest-dev\n38.6 38.6\n38.0 37.8\n37.5 37.2\ntest-dev\n37.0\n36.3\n35 50\ncoco\nCOCO\n30 45\n25 40\n32x4d 32x8d 32x16d 32x4d 32x8d 32x16d\nResNeXt-101 capacity (in Mask R-CNN) ResNeXt-101 capacity (in Mask R-CNN)\" data-coord=\"top-left:(555,552); bottom-right:(2002,1810)\" /></figure>",
            "id": 98,
            "page": 13,
            "text": "Target task: COCO detection (box AP) Target task: COCO detection (box AP@50)\n55 70\n68.3\nSource task 67.9\n67.2 67.2 67.1\nIN-1k (1.3M, 1k labels) IG (1B, 17k tags) 66.5\n65.5 65.6\n50 IN-5k (6.6M, 5k labels) IG (3.5B, 17k tags) 1% 65 64.4 64.8 64.9\n%)\n63.8\n(in\nIG (940M, 1.5k tags) 63.2\n(in\n45 45.0 45.2 44.8 45.2 60\nAP\n44.2\n43.7 43.7\n42.7 42.9\nbox\n42.3 42.0 AP@50\n41.6\n40.9\nbox\n40 55\ntest-dev\ntest-dev\n35 50\nCOCO\nCOCO\n30 45\n25 40\n32x4d 32x8d 32x16d 32x4d 32x8d 32x16d\nResNeXt-101 capacity (in Mask R-CNN) ResNeXt-101 capacity (in Mask R-CNN)\nTarget task: COCO detection (mask AP) Target task: COCO detection (mask AP@50)\n55 70\n65 64.3 64.4\n50 %)\n%)\n(in\n63.7 63.7\n63.0 62.7\n(in\n62.3\nAP@50\n61.5 61.7\n61.0 61.1\n60.3\nAP\n45 60 59.6\nmask\nmask\n40 39.6 39.7 39.4 55\n39.2 39.3\ntest-dev\n38.6 38.6\n38.0 37.8\n37.5 37.2\ntest-dev\n37.0\n36.3\n35 50\ncoco\nCOCO\n30 45\n25 40\n32x4d 32x8d 32x16d 32x4d 32x8d 32x16d\nResNeXt-101 capacity (in Mask R-CNN) ResNeXt-101 capacity (in Mask R-CNN)"
        },
        {
            "bounding_box": [
                {
                    "x": 547,
                    "y": 1832
                },
                {
                    "x": 2005,
                    "y": 1832
                },
                {
                    "x": 2005,
                    "y": 1971
                },
                {
                    "x": 547,
                    "y": 1971
                }
            ],
            "category": "caption",
            "html": "<br><caption id='99' style='font-size:16px'>Fig. 8: Transfer to object detection and instance segmentation with Mask R-CNN. We<br>compare ResNeXt-101 FPN backbones of three different capacities using a variety of<br>source pretraining tasks. Higher is better.</caption>",
            "id": 99,
            "page": 13,
            "text": "Fig. 8: Transfer to object detection and instance segmentation with Mask R-CNN. We\ncompare ResNeXt-101 FPN backbones of three different capacities using a variety of\nsource pretraining tasks. Higher is better."
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 2255
                },
                {
                    "x": 2003,
                    "y": 2255
                },
                {
                    "x": 2003,
                    "y": 2805
                },
                {
                    "x": 545,
                    "y": 2805
                }
            ],
            "category": "paragraph",
            "html": "<p id='100' style='font-size:20px'>union (IoU) overlap thresholds 0.5:0.95) to AP@50 (average precision computed<br>at IoU threshold 0.5 only). The former emphasizes precise localization while the<br>later allows for looser localization. We observe that the improvement over IN-<br>{1k, 5k} pretraining from IG-1B-1k is much larger in terms of AP@50. Thus,<br>the gains from Instagram pretraining may be primarily due to improved object<br>classification performance, rather than spatial localization performance. Further<br>evidence comes from experiments with keypoint detection using Mask R-CNN,<br>where we found that compared to IN-1k pretraining, IG-1B-1k pretraining leads<br>to worse results (65.3% VS. 67.0% keypoint AP). These two findings suggest that<br>pretraining for Instagram hashtag classification may reduce spatial localization<br>performance while improving classification.</p>",
            "id": 100,
            "page": 13,
            "text": "union (IoU) overlap thresholds 0.5:0.95) to AP@50 (average precision computed\nat IoU threshold 0.5 only). The former emphasizes precise localization while the\nlater allows for looser localization. We observe that the improvement over IN-\n{1k, 5k} pretraining from IG-1B-1k is much larger in terms of AP@50. Thus,\nthe gains from Instagram pretraining may be primarily due to improved object\nclassification performance, rather than spatial localization performance. Further\nevidence comes from experiments with keypoint detection using Mask R-CNN,\nwhere we found that compared to IN-1k pretraining, IG-1B-1k pretraining leads\nto worse results (65.3% VS. 67.0% keypoint AP). These two findings suggest that\npretraining for Instagram hashtag classification may reduce spatial localization\nperformance while improving classification."
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 408
                },
                {
                    "x": 944,
                    "y": 408
                },
                {
                    "x": 944,
                    "y": 455
                },
                {
                    "x": 549,
                    "y": 455
                }
            ],
            "category": "header",
            "html": "<header id='101' style='font-size:14px'>14 Mahajan et al.</header>",
            "id": 101,
            "page": 14,
            "text": "14 Mahajan et al."
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 508
                },
                {
                    "x": 987,
                    "y": 508
                },
                {
                    "x": 987,
                    "y": 562
                },
                {
                    "x": 549,
                    "y": 562
                }
            ],
            "category": "paragraph",
            "html": "<p id='102' style='font-size:22px'>4 Related Work</p>",
            "id": 102,
            "page": 14,
            "text": "4 Related Work"
        },
        {
            "bounding_box": [
                {
                    "x": 544,
                    "y": 618
                },
                {
                    "x": 2005,
                    "y": 618
                },
                {
                    "x": 2005,
                    "y": 1018
                },
                {
                    "x": 544,
                    "y": 1018
                }
            ],
            "category": "paragraph",
            "html": "<p id='103' style='font-size:18px'>Our study is part of a larger body of work on training convolutional networks<br>on large, weakly supervised image datasets. Sun et al. [17] train convolutional<br>networks on the JFT-300M dataset of 300 million weakly supervised images.<br>Our Instagram datasets are an order of magnitude larger than JFT-300M, and<br>collecting them required much less manual annotation work (see Section 2.1).<br>Due to the larger training set size and the use of better network architectures,<br>we obtain substantially higher accuracies on transfer tasks: e.g., we obtain 85.4%<br>top-1 accuracy on ImageNet-1k, compared to 79.2% reported in [17].</p>",
            "id": 103,
            "page": 14,
            "text": "Our study is part of a larger body of work on training convolutional networks\non large, weakly supervised image datasets. Sun et al. [17] train convolutional\nnetworks on the JFT-300M dataset of 300 million weakly supervised images.\nOur Instagram datasets are an order of magnitude larger than JFT-300M, and\ncollecting them required much less manual annotation work (see Section 2.1).\nDue to the larger training set size and the use of better network architectures,\nwe obtain substantially higher accuracies on transfer tasks: e.g., we obtain 85.4%\ntop-1 accuracy on ImageNet-1k, compared to 79.2% reported in [17]."
        },
        {
            "bounding_box": [
                {
                    "x": 544,
                    "y": 1021
                },
                {
                    "x": 2005,
                    "y": 1021
                },
                {
                    "x": 2005,
                    "y": 1768
                },
                {
                    "x": 544,
                    "y": 1768
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='104' style='font-size:16px'>Other prior studies [16,38] trained convolutional networks to predict words<br>or n-grams in comments on a collection of 100 million Flickr photos and corre-<br>sponding comments [39]. Word or n-gram supervision is weaker than hashtag su-<br>pervision because it is less structured, as reflected in the relatively poor transfer<br>of features to ImageNet reported in [16]. Other work [40] also trained networks to<br>predict hashtags on the Flickr dataset but, unlike our study, does not investigate<br>transfer of the resulting networks to other tasks. In addition to Flickr hashtags,<br>[41] trained hard mixture of expert models on food-related Instagram hashtags;<br>our focus is on standard recognition networks and general hashtags. Other stud-<br>ies on hashtag prediction [42] do not train convolutional networks from scratch,<br>but train linear classifiers to predict relevant hashtags from pre-defined image<br>features. Several other works have trained models on web-scale image data for<br>other purposes, such as face recognition [43,44] and similarity search [45,46], but<br>to the best of our knowledge, we are the first to report the results of experiments<br>that involve training convolutional networks from scratch on billions of images.</p>",
            "id": 104,
            "page": 14,
            "text": "Other prior studies [16,38] trained convolutional networks to predict words\nor n-grams in comments on a collection of 100 million Flickr photos and corre-\nsponding comments [39]. Word or n-gram supervision is weaker than hashtag su-\npervision because it is less structured, as reflected in the relatively poor transfer\nof features to ImageNet reported in [16]. Other work [40] also trained networks to\npredict hashtags on the Flickr dataset but, unlike our study, does not investigate\ntransfer of the resulting networks to other tasks. In addition to Flickr hashtags,\n[41] trained hard mixture of expert models on food-related Instagram hashtags;\nour focus is on standard recognition networks and general hashtags. Other stud-\nies on hashtag prediction [42] do not train convolutional networks from scratch,\nbut train linear classifiers to predict relevant hashtags from pre-defined image\nfeatures. Several other works have trained models on web-scale image data for\nother purposes, such as face recognition [43,44] and similarity search [45,46], but\nto the best of our knowledge, we are the first to report the results of experiments\nthat involve training convolutional networks from scratch on billions of images."
        },
        {
            "bounding_box": [
                {
                    "x": 550,
                    "y": 1847
                },
                {
                    "x": 898,
                    "y": 1847
                },
                {
                    "x": 898,
                    "y": 1905
                },
                {
                    "x": 550,
                    "y": 1905
                }
            ],
            "category": "paragraph",
            "html": "<p id='105' style='font-size:20px'>5 Discussion</p>",
            "id": 105,
            "page": 14,
            "text": "5 Discussion"
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 1955
                },
                {
                    "x": 2005,
                    "y": 1955
                },
                {
                    "x": 2005,
                    "y": 2159
                },
                {
                    "x": 545,
                    "y": 2159
                }
            ],
            "category": "paragraph",
            "html": "<p id='106' style='font-size:18px'>We have attempted to explore the limits of supervised pretraining. In addition<br>to producing state-of-the-art results on the ImageNet-1k benchmark task (85.4%<br>single-crop, top-1 accuracy; 97.6% single-crop, top-5 accuracy) and several other<br>vision tasks, our study has led to four important observations:</p>",
            "id": 106,
            "page": 14,
            "text": "We have attempted to explore the limits of supervised pretraining. In addition\nto producing state-of-the-art results on the ImageNet-1k benchmark task (85.4%\nsingle-crop, top-1 accuracy; 97.6% single-crop, top-5 accuracy) and several other\nvision tasks, our study has led to four important observations:"
        },
        {
            "bounding_box": [
                {
                    "x": 556,
                    "y": 2202
                },
                {
                    "x": 2009,
                    "y": 2202
                },
                {
                    "x": 2009,
                    "y": 2810
                },
                {
                    "x": 556,
                    "y": 2810
                }
            ],
            "category": "paragraph",
            "html": "<p id='107' style='font-size:16px'>1. Our results suggests that, whilst increasing the size of the pretraining dataset<br>may be worthwhile, it may be at least as important to select a label space<br>for the source task to match that of the target task. We found that networks<br>trained on a hashtag vocabulary that was designed to match the classes in the<br>ImageNet-1k dataset outperformed those trained on twice as many images<br>without such careful selection of hashtags (Figure 2, top left). This obser-<br>vation paves the way for the design of \"label-space engineering\" approaches<br>that aim to optimally select (weakly supervised) label sets for a particular<br>target task. Such label-space engineering may be much more fruitful than<br>further increasing the scale of the data on which models are trained.<br>2. In line with prior work [16,17], we observe that current network architectures<br>are underfitting when trained on billions of images. Whilst such underfitting</p>",
            "id": 107,
            "page": 14,
            "text": "1. Our results suggests that, whilst increasing the size of the pretraining dataset\nmay be worthwhile, it may be at least as important to select a label space\nfor the source task to match that of the target task. We found that networks\ntrained on a hashtag vocabulary that was designed to match the classes in the\nImageNet-1k dataset outperformed those trained on twice as many images\nwithout such careful selection of hashtags (Figure 2, top left). This obser-\nvation paves the way for the design of \"label-space engineering\" approaches\nthat aim to optimally select (weakly supervised) label sets for a particular\ntarget task. Such label-space engineering may be much more fruitful than\nfurther increasing the scale of the data on which models are trained.\n2. In line with prior work [16,17], we observe that current network architectures\nare underfitting when trained on billions of images. Whilst such underfitting"
        },
        {
            "bounding_box": [
                {
                    "x": 922,
                    "y": 409
                },
                {
                    "x": 1995,
                    "y": 409
                },
                {
                    "x": 1995,
                    "y": 457
                },
                {
                    "x": 922,
                    "y": 457
                }
            ],
            "category": "header",
            "html": "<header id='108' style='font-size:16px'>Exploring the Limits of Weakly Supervised Pretraining 15</header>",
            "id": 108,
            "page": 15,
            "text": "Exploring the Limits of Weakly Supervised Pretraining 15"
        },
        {
            "bounding_box": [
                {
                    "x": 569,
                    "y": 511
                },
                {
                    "x": 2007,
                    "y": 511
                },
                {
                    "x": 2007,
                    "y": 1810
                },
                {
                    "x": 569,
                    "y": 1810
                }
            ],
            "category": "paragraph",
            "html": "<p id='109' style='font-size:14px'>does lead to very high robustness to noise in our hashtag targets (Figure 3),<br>our results do suggest that accuracy improvements on target tasks may be<br>obtained by further increases of the capacity of our networks (Figure 2).<br>Capacity may be increased, for instance, by increasing the number of layers<br>and the number of filters per layer of existing architectures or by mixtures-<br>of-experts [41] (using model parallelization across GPUs). However, it is not<br>unthinkable that some of the design choices that were made in current net-<br>work architectures are too tailored to ImageNet-1k classification, and need<br>to be revisited when training on billions of images with hashtag supervision.<br>3. Our results also underline the importance of increasing the visual variety<br>that we consider in our benchmark tasks. They show that the differences in<br>the quality of visual features become much more pronounced if these features<br>are evaluated on tasks with a larger visual variety. For instance, we find<br>that the accuracy difference between models pretrained using two different<br>vocabularies increases as the number of target classes increases (Figure 2):<br>if we would have only evaluated our models on ImageNet-1k, we would have<br>concluded they learned visual features of similar quality, whereas results on<br>ImageNet-9k show that one model learns substantially better features than<br>the other. We believe evaluation on more ImageNet classes is a good step<br>towards a more comprehensive assessment of visual recognition models.<br>4. Results from transferring our models to object detection, instance segmen-<br>tation, and keypoint detection tasks suggestion that training for large-scale<br>hashtag prediction improves classification while at the same time possibly<br>harming localization performance. This opens a future direction of modifying<br>large-scale, weakly supervised pretraining tasks to better suit the localization<br>needs of important target tasks like detection and human pose estimation.</p>",
            "id": 109,
            "page": 15,
            "text": "does lead to very high robustness to noise in our hashtag targets (Figure 3),\nour results do suggest that accuracy improvements on target tasks may be\nobtained by further increases of the capacity of our networks (Figure 2).\nCapacity may be increased, for instance, by increasing the number of layers\nand the number of filters per layer of existing architectures or by mixtures-\nof-experts [41] (using model parallelization across GPUs). However, it is not\nunthinkable that some of the design choices that were made in current net-\nwork architectures are too tailored to ImageNet-1k classification, and need\nto be revisited when training on billions of images with hashtag supervision.\n3. Our results also underline the importance of increasing the visual variety\nthat we consider in our benchmark tasks. They show that the differences in\nthe quality of visual features become much more pronounced if these features\nare evaluated on tasks with a larger visual variety. For instance, we find\nthat the accuracy difference between models pretrained using two different\nvocabularies increases as the number of target classes increases (Figure 2):\nif we would have only evaluated our models on ImageNet-1k, we would have\nconcluded they learned visual features of similar quality, whereas results on\nImageNet-9k show that one model learns substantially better features than\nthe other. We believe evaluation on more ImageNet classes is a good step\ntowards a more comprehensive assessment of visual recognition models.\n4. Results from transferring our models to object detection, instance segmen-\ntation, and keypoint detection tasks suggestion that training for large-scale\nhashtag prediction improves classification while at the same time possibly\nharming localization performance. This opens a future direction of modifying\nlarge-scale, weakly supervised pretraining tasks to better suit the localization\nneeds of important target tasks like detection and human pose estimation."
        },
        {
            "bounding_box": [
                {
                    "x": 546,
                    "y": 1841
                },
                {
                    "x": 2003,
                    "y": 1841
                },
                {
                    "x": 2003,
                    "y": 2044
                },
                {
                    "x": 546,
                    "y": 2044
                }
            ],
            "category": "paragraph",
            "html": "<p id='110' style='font-size:14px'>In closing, we reflect on the remarkable fact that training for hashtag prediction,<br>without the need for additional manual annotation or data cleaning, works at all.<br>We believe our study illustrates the potential of natural or \"wild\" data compared<br>to the traditional approach of manually designing and annotating datasets.</p>",
            "id": 110,
            "page": 15,
            "text": "In closing, we reflect on the remarkable fact that training for hashtag prediction,\nwithout the need for additional manual annotation or data cleaning, works at all.\nWe believe our study illustrates the potential of natural or \"wild\" data compared\nto the traditional approach of manually designing and annotating datasets."
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 2114
                },
                {
                    "x": 1024,
                    "y": 2114
                },
                {
                    "x": 1024,
                    "y": 2173
                },
                {
                    "x": 549,
                    "y": 2173
                }
            ],
            "category": "paragraph",
            "html": "<p id='111' style='font-size:22px'>Acknowledgements</p>",
            "id": 111,
            "page": 15,
            "text": "Acknowledgements"
        },
        {
            "bounding_box": [
                {
                    "x": 547,
                    "y": 2214
                },
                {
                    "x": 1999,
                    "y": 2214
                },
                {
                    "x": 1999,
                    "y": 2316
                },
                {
                    "x": 547,
                    "y": 2316
                }
            ],
            "category": "paragraph",
            "html": "<p id='112' style='font-size:18px'>We would like to thank Matthijs Douze, Aapo Kyrola, Andrew Dye, Jerry Pan,<br>Kevin Wilfong, and Martin Englund for helpful discussions and support.</p>",
            "id": 112,
            "page": 15,
            "text": "We would like to thank Matthijs Douze, Aapo Kyrola, Andrew Dye, Jerry Pan,\nKevin Wilfong, and Martin Englund for helpful discussions and support."
        },
        {
            "bounding_box": [
                {
                    "x": 548,
                    "y": 2385
                },
                {
                    "x": 823,
                    "y": 2385
                },
                {
                    "x": 823,
                    "y": 2443
                },
                {
                    "x": 548,
                    "y": 2443
                }
            ],
            "category": "paragraph",
            "html": "<p id='113' style='font-size:20px'>References</p>",
            "id": 113,
            "page": 15,
            "text": "References"
        },
        {
            "bounding_box": [
                {
                    "x": 563,
                    "y": 2483
                },
                {
                    "x": 2005,
                    "y": 2483
                },
                {
                    "x": 2005,
                    "y": 2803
                },
                {
                    "x": 563,
                    "y": 2803
                }
            ],
            "category": "paragraph",
            "html": "<p id='114' style='font-size:14px'>1. Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for accu-<br>rate object detection and semantic segmentation. In: CVPR. (2014)<br>2. Donahue, J., Jia, Y., Vinyals, 0., Hoffman, J., Zhang, N., Tzeng, E., Darrell, T.:<br>DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition.<br>arXiv:1310.1531 (2013)<br>3. Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional neural net-<br>works. In: ECCV. (2014)</p>",
            "id": 114,
            "page": 15,
            "text": "1. Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for accu-\nrate object detection and semantic segmentation. In: CVPR. (2014)\n2. Donahue, J., Jia, Y., Vinyals, 0., Hoffman, J., Zhang, N., Tzeng, E., Darrell, T.:\nDeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition.\narXiv:1310.1531 (2013)\n3. Zeiler, M.D., Fergus, R.: Visualizing and understanding convolutional neural net-\nworks. In: ECCV. (2014)"
        },
        {
            "bounding_box": [
                {
                    "x": 550,
                    "y": 410
                },
                {
                    "x": 944,
                    "y": 410
                },
                {
                    "x": 944,
                    "y": 455
                },
                {
                    "x": 550,
                    "y": 455
                }
            ],
            "category": "header",
            "html": "<header id='115' style='font-size:14px'>16 Mahajan et al.</header>",
            "id": 115,
            "page": 16,
            "text": "16 Mahajan et al."
        },
        {
            "bounding_box": [
                {
                    "x": 550,
                    "y": 525
                },
                {
                    "x": 2006,
                    "y": 525
                },
                {
                    "x": 2006,
                    "y": 2810
                },
                {
                    "x": 550,
                    "y": 2810
                }
            ],
            "category": "paragraph",
            "html": "<p id='116' style='font-size:18px'>4. He, K., Gkioxari, G., Dollar, P., Girshick, R.: Mask R-CNN. In: ICCV. (2017)<br>5. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic<br>segmentation. In: CVPR. (2015)<br>6. Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J.: Pyramid scene parsing network. In:<br>CVPR. (2017)<br>7. Cao, Z., Simon, T., Wei, S.E., Sheikh, Y.: Realtime multi-person 2d pose estimation<br>using part affinity fields. In: CVPR. (2017)<br>8. Papandreou, G., Zhu, T., Kanazawa, N., Toshev, A., Tompson, J., Bregler, C.,<br>Murphy, K.: Towards accurate multi-person pose estimation in the wild. In:<br>CVPR. (2017)<br>9. Carreira, J., Zisserman, A.: Quo vadis, action recognition? a new model and the<br>kinetics dataset. In: CVPR. (2017)<br>10. Eigen, D., Fergus, R.: Predicting depth, surface normals and semantic labels with<br>a common multi-scale convolutional architecture. In: ICCV. (2015)<br>11. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,<br>Karpathy, A., Khosla, A., Bernstein, M., Berg, A.C., Fei-Fei, L.: ImageNet Large<br>Scale Visual Recognition Challenge. IJCV (2015)<br>12. Agrawal, P., Girshick, R., Malik, J.: Analyzing the performance of multilayer<br>neural networks for object recognition. In: ECCV. (2014)<br>13. Huh, M., Agrawal, P., Efros, A.: What makes ImageNet good for transfer learning?<br>arXiv:1608.08614 (2016)<br>14. Zhou, B., Lapedriza, A., Khosla, A., Oliva, A., Torralba, A.: Places: A 10 million<br>image database for scene recognition. PAMI (2017)<br>15. Xie, S., Girshick, R., Dollar, P., Tu, Z., He, K.: Aggregated residual transformations<br>for deep neural networks. In: CVPR. (2017)<br>16. Joulin, A., van der Maaten, L., Jabri, A., Vasilache, N.: Learning visual features<br>from large weakly supervised data. In: Proceedings of the European Conference<br>on Computer Vision (ECCV), Springer (2016) 67-84<br>17. Sun, C., Shrivastava, A., Singh, S., Gupta, A.: Revisiting unreasonable effectiveness<br>of data in deep learning era. In: Proc. ICCV. (2017)<br>18. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar,<br>P., Zitnick, C.L.: Microsoft COCO: Common objects in context. In: European<br>conference on computer vision, Springer (2014) 740-755<br>19. Goyal, P., Dollar, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., Tul-<br>loch, A., Jia, Y., He, K.: Accurate, large minibatch SGD: Training ImageNet in 1<br>hour. In: arXiv:1706.02677. (2017)<br>20. WordNet: About WordNet. http : / /wordnet.princeton.edu (2010)<br>21. Welinder, P., Branson, S., Mita, T., Wah, C., Schroff, F., Belongie, S., Perona, P.:<br>Caltech-UCSD Birds 200. Technical report, Caltech (2010)<br>22. Gordo, A., Almazan, J., Revaud, J., Larlus, D.: Deep image retrieval: Learning<br>global representations for image search. In: arXiv:1604.01325. (2016)<br>23. Tolias, G., Sicre, R., , Jegou, H.: Particular object retrieval with integral max-<br>pooling of cnn activations. In: ICLR. (2016)<br>24. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.<br>In: CVPR. (2016)<br>25. Huang, G., Liu, Z., Weinberger, K., van der Maaten, L.: Densely connected con-<br>volutional networks. In: CVPR. (2017)<br>26. Szegedy, C., Ioffe, S., Vanhoucke, V., Alemi, A.: Inception-v4, inception-resnet and<br>the impact of residual connections on learning. In: arXiv:1602.07261. (2016)<br>27. Ioffe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by<br>reducing internal covariate shift. In: ICML. (2015)</p>",
            "id": 116,
            "page": 16,
            "text": "4. He, K., Gkioxari, G., Dollar, P., Girshick, R.: Mask R-CNN. In: ICCV. (2017)\n5. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic\nsegmentation. In: CVPR. (2015)\n6. Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J.: Pyramid scene parsing network. In:\nCVPR. (2017)\n7. Cao, Z., Simon, T., Wei, S.E., Sheikh, Y.: Realtime multi-person 2d pose estimation\nusing part affinity fields. In: CVPR. (2017)\n8. Papandreou, G., Zhu, T., Kanazawa, N., Toshev, A., Tompson, J., Bregler, C.,\nMurphy, K.: Towards accurate multi-person pose estimation in the wild. In:\nCVPR. (2017)\n9. Carreira, J., Zisserman, A.: Quo vadis, action recognition? a new model and the\nkinetics dataset. In: CVPR. (2017)\n10. Eigen, D., Fergus, R.: Predicting depth, surface normals and semantic labels with\na common multi-scale convolutional architecture. In: ICCV. (2015)\n11. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,\nKarpathy, A., Khosla, A., Bernstein, M., Berg, A.C., Fei-Fei, L.: ImageNet Large\nScale Visual Recognition Challenge. IJCV (2015)\n12. Agrawal, P., Girshick, R., Malik, J.: Analyzing the performance of multilayer\nneural networks for object recognition. In: ECCV. (2014)\n13. Huh, M., Agrawal, P., Efros, A.: What makes ImageNet good for transfer learning?\narXiv:1608.08614 (2016)\n14. Zhou, B., Lapedriza, A., Khosla, A., Oliva, A., Torralba, A.: Places: A 10 million\nimage database for scene recognition. PAMI (2017)\n15. Xie, S., Girshick, R., Dollar, P., Tu, Z., He, K.: Aggregated residual transformations\nfor deep neural networks. In: CVPR. (2017)\n16. Joulin, A., van der Maaten, L., Jabri, A., Vasilache, N.: Learning visual features\nfrom large weakly supervised data. In: Proceedings of the European Conference\non Computer Vision (ECCV), Springer (2016) 67-84\n17. Sun, C., Shrivastava, A., Singh, S., Gupta, A.: Revisiting unreasonable effectiveness\nof data in deep learning era. In: Proc. ICCV. (2017)\n18. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollar,\nP., Zitnick, C.L.: Microsoft COCO: Common objects in context. In: European\nconference on computer vision, Springer (2014) 740-755\n19. Goyal, P., Dollar, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., Tul-\nloch, A., Jia, Y., He, K.: Accurate, large minibatch SGD: Training ImageNet in 1\nhour. In: arXiv:1706.02677. (2017)\n20. WordNet: About WordNet. http : / /wordnet.princeton.edu (2010)\n21. Welinder, P., Branson, S., Mita, T., Wah, C., Schroff, F., Belongie, S., Perona, P.:\nCaltech-UCSD Birds 200. Technical report, Caltech (2010)\n22. Gordo, A., Almazan, J., Revaud, J., Larlus, D.: Deep image retrieval: Learning\nglobal representations for image search. In: arXiv:1604.01325. (2016)\n23. Tolias, G., Sicre, R., , Jegou, H.: Particular object retrieval with integral max-\npooling of cnn activations. In: ICLR. (2016)\n24. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.\nIn: CVPR. (2016)\n25. Huang, G., Liu, Z., Weinberger, K., van der Maaten, L.: Densely connected con-\nvolutional networks. In: CVPR. (2017)\n26. Szegedy, C., Ioffe, S., Vanhoucke, V., Alemi, A.: Inception-v4, inception-resnet and\nthe impact of residual connections on learning. In: arXiv:1602.07261. (2016)\n27. Ioffe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. In: ICML. (2015)"
        },
        {
            "bounding_box": [
                {
                    "x": 922,
                    "y": 409
                },
                {
                    "x": 1996,
                    "y": 409
                },
                {
                    "x": 1996,
                    "y": 456
                },
                {
                    "x": 922,
                    "y": 456
                }
            ],
            "category": "header",
            "html": "<header id='117' style='font-size:18px'>Exploring the Limits of Weakly Supervised Pretraining 17</header>",
            "id": 117,
            "page": 17,
            "text": "Exploring the Limits of Weakly Supervised Pretraining 17"
        },
        {
            "bounding_box": [
                {
                    "x": 544,
                    "y": 495
                },
                {
                    "x": 2008,
                    "y": 495
                },
                {
                    "x": 2008,
                    "y": 2817
                },
                {
                    "x": 544,
                    "y": 2817
                }
            ],
            "category": "paragraph",
            "html": "<p id='118' style='font-size:14px'>28. He, K., Zhang, X., Ren, S., Sun, J.: Delving deep into rectifiers: Surpassing human-<br>level performance on imagenet classification. In: ICCV. (2015)<br>29. Pathak, D., Girshick, R., Dollar, P., Darrell, T., Hariharan, B.: Learning features<br>by watching objects move. In: CVPR. (2017)<br>30. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale<br>hierarchical image database. In: CVPR. (2009)<br>31. Zoph, B., Vasudevan, V., Shlens, J., Le, Q.: Learning transferable architectures<br>for scalable image recognition. In: arXiv:1707.07012. (2017)<br>32. Storck, P., Cisse, M.: Convnets and imagenet beyond accuracy: Explanations, bias<br>detection, adversarial examples and model criticism. In: arXiv:1711.11443. (2017)<br>33. Misra, I., Zitnick, C.L., Mitchell, M., Girshick, R.: Seeing through the human<br>reporting bias: Visual classifiers from noisy human-centric labels. In: CVPR. (2016)<br>34. Mikolov, T., Sutskever, I., Chen, K., Corrado, G., Dean, J.: Distributed represen-<br>tations of words and phrases and their compositionality. In: NIPS. (2013)<br>35. Brysbaert, M., Warriner, A.B., Kuperman, V.: Concreteness ratings for 40 thou-<br>sand generally known english word lemmas. Behavior Research Methods (2014)<br>36. Girshick, R., Radosavovic, I., Gkioxari, G., Dollar, P., He, K.: Detectron.<br>https .. //github.com/facebookmeseandh/detectron (2018)<br>37. Lin, T.Y., Dollar, P., Girshick, R., He, K., Hariharan, B., Belongie, S.: Feature<br>pyramid networks for object detection. In: CVPR. (2017)<br>38. Li, A., Jabri, A., Joulin, A., van der Maaten, L.: Learning visual n-grams from<br>web data. In: Proc. ICCV. (2017)<br>39. Thomee, B., Shamma, D.A., Friedland, G., Elizalde, B., Ni, K., Poland, D., Borth,<br>D., Li, L.J.: Yfcc100m: The new data in multimedia research. Communications of<br>the ACM 59(2) (2016) 64-73<br>40. Veit, A., Nickel, M., Belongie, S., van der Maaten, L.: Separating self-expression<br>and visual content in hashtag supervision. In: arXiv 1711.09825. (2017)<br>41. Gross, S., Ranzato, M., Szlam, A.: Hard mixtures of experts for large scale weakly<br>supervised vision. In: CVPR. (2017)<br>42. Denton, E., Weston, J., Paluri, M., Bourdev, L., Fergus, R.: User conditional<br>hashtag prediction for images. In: Proc. KDD. (2015) 1731-1740<br>43. Schroff, F., Kalenichenko, D., Philbin, J.: Facenet: A unified embedding for face<br>recognition and clustering. In: CVPR. (2015)<br>44. Taigman, Y., Yang, M., Ranzato, M., Wolf, L.: Web-scale training for face identi-<br>fication. In: CVPR. (2015)<br>45. Johnson, J., Douze, M., Jegou, H.: Billion-scale similarity search with GPUs. arXiv<br>preprint arXiv:1702.08734 (2017)<br>46. Stewenius, H., Gunderson, S., Pilet, J.: Size matters: Exhaustive geometric verifica-<br>tion for image retrieval. In: Proceedings of the European Conference on Computer<br>Vision (ECCV), Springer (2012)<br>47. Jegou, H., Douze, M., Schmid, C.: Hamming embedding and weak geometry con-<br>sistency for large scale image search. In: Proceedings of the European Conference<br>on Computer Vision (ECCV). (2008)<br>48. Ge, T., He, K., Ke, Q., Sun, J.: Optimized product quantization. PAMI 36(4)<br>(2013) 744-755<br>49. Jegou, H., Douze, M., Schmid, C.: Product quantization for nearest neighbor<br>search. PAMI 33(1) (2011) 117-128<br>50. Nesterov, Y.: Introductory lectures on convex optimization: A basic course.<br>Springer (2004)</p>",
            "id": 118,
            "page": 17,
            "text": "28. He, K., Zhang, X., Ren, S., Sun, J.: Delving deep into rectifiers: Surpassing human-\nlevel performance on imagenet classification. In: ICCV. (2015)\n29. Pathak, D., Girshick, R., Dollar, P., Darrell, T., Hariharan, B.: Learning features\nby watching objects move. In: CVPR. (2017)\n30. Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale\nhierarchical image database. In: CVPR. (2009)\n31. Zoph, B., Vasudevan, V., Shlens, J., Le, Q.: Learning transferable architectures\nfor scalable image recognition. In: arXiv:1707.07012. (2017)\n32. Storck, P., Cisse, M.: Convnets and imagenet beyond accuracy: Explanations, bias\ndetection, adversarial examples and model criticism. In: arXiv:1711.11443. (2017)\n33. Misra, I., Zitnick, C.L., Mitchell, M., Girshick, R.: Seeing through the human\nreporting bias: Visual classifiers from noisy human-centric labels. In: CVPR. (2016)\n34. Mikolov, T., Sutskever, I., Chen, K., Corrado, G., Dean, J.: Distributed represen-\ntations of words and phrases and their compositionality. In: NIPS. (2013)\n35. Brysbaert, M., Warriner, A.B., Kuperman, V.: Concreteness ratings for 40 thou-\nsand generally known english word lemmas. Behavior Research Methods (2014)\n36. Girshick, R., Radosavovic, I., Gkioxari, G., Dollar, P., He, K.: Detectron.\nhttps .. //github.com/facebookmeseandh/detectron (2018)\n37. Lin, T.Y., Dollar, P., Girshick, R., He, K., Hariharan, B., Belongie, S.: Feature\npyramid networks for object detection. In: CVPR. (2017)\n38. Li, A., Jabri, A., Joulin, A., van der Maaten, L.: Learning visual n-grams from\nweb data. In: Proc. ICCV. (2017)\n39. Thomee, B., Shamma, D.A., Friedland, G., Elizalde, B., Ni, K., Poland, D., Borth,\nD., Li, L.J.: Yfcc100m: The new data in multimedia research. Communications of\nthe ACM 59(2) (2016) 64-73\n40. Veit, A., Nickel, M., Belongie, S., van der Maaten, L.: Separating self-expression\nand visual content in hashtag supervision. In: arXiv 1711.09825. (2017)\n41. Gross, S., Ranzato, M., Szlam, A.: Hard mixtures of experts for large scale weakly\nsupervised vision. In: CVPR. (2017)\n42. Denton, E., Weston, J., Paluri, M., Bourdev, L., Fergus, R.: User conditional\nhashtag prediction for images. In: Proc. KDD. (2015) 1731-1740\n43. Schroff, F., Kalenichenko, D., Philbin, J.: Facenet: A unified embedding for face\nrecognition and clustering. In: CVPR. (2015)\n44. Taigman, Y., Yang, M., Ranzato, M., Wolf, L.: Web-scale training for face identi-\nfication. In: CVPR. (2015)\n45. Johnson, J., Douze, M., Jegou, H.: Billion-scale similarity search with GPUs. arXiv\npreprint arXiv:1702.08734 (2017)\n46. Stewenius, H., Gunderson, S., Pilet, J.: Size matters: Exhaustive geometric verifica-\ntion for image retrieval. In: Proceedings of the European Conference on Computer\nVision (ECCV), Springer (2012)\n47. Jegou, H., Douze, M., Schmid, C.: Hamming embedding and weak geometry con-\nsistency for large scale image search. In: Proceedings of the European Conference\non Computer Vision (ECCV). (2008)\n48. Ge, T., He, K., Ke, Q., Sun, J.: Optimized product quantization. PAMI 36(4)\n(2013) 744-755\n49. Jegou, H., Douze, M., Schmid, C.: Product quantization for nearest neighbor\nsearch. PAMI 33(1) (2011) 117-128\n50. Nesterov, Y.: Introductory lectures on convex optimization: A basic course.\nSpringer (2004)"
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 408
                },
                {
                    "x": 944,
                    "y": 408
                },
                {
                    "x": 944,
                    "y": 456
                },
                {
                    "x": 549,
                    "y": 456
                }
            ],
            "category": "header",
            "html": "<header id='119' style='font-size:14px'>18 Mahajan et al.</header>",
            "id": 119,
            "page": 18,
            "text": "18 Mahajan et al."
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 514
                },
                {
                    "x": 2006,
                    "y": 514
                },
                {
                    "x": 2006,
                    "y": 1167
                },
                {
                    "x": 545,
                    "y": 1167
                }
            ],
            "category": "paragraph",
            "html": "<p id='120' style='font-size:16px'>51. Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z.: Rethinking the incep-<br>tion architecture for computer vision. In: Proceedings of the IEEE Conference on<br>Computer Vision and Pattern Recognition (CVPR). (2016)<br>52. Chollet, F. : Xception: Deep learning with depthwise separable convolutions. In:<br>arXiv:1610.02357. (2016)<br>53. Szegedy, C., Ioffe, S., Vanhoucke, V., Alemi, A.: Inceptionv4, inception-resnet and<br>the impact of residual connections on learning. In: Proceedings of the International<br>Conference on Learning Representations (ICLR) Workshop. (2016)<br>54. Zhang, X., Li, Z., Loy, C., Lin, D.: Polynet: A pursuit of structural diversity in<br>very deep networks. In: arXiv:1611.05725. (2016)<br>55. Chen, Y., Li, J., Xiao, H., Jin, X., Yan, S., Feng, J.: Dual path networks. In:<br>arXiv:1707.01629. (2017)<br>56. Hu, J., Shen, L., Sun, G.: Squeeze-and-excitation networks. In: arXiv:1709.01507.<br>(2017)</p>",
            "id": 120,
            "page": 18,
            "text": "51. Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z.: Rethinking the incep-\ntion architecture for computer vision. In: Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR). (2016)\n52. Chollet, F. : Xception: Deep learning with depthwise separable convolutions. In:\narXiv:1610.02357. (2016)\n53. Szegedy, C., Ioffe, S., Vanhoucke, V., Alemi, A.: Inceptionv4, inception-resnet and\nthe impact of residual connections on learning. In: Proceedings of the International\nConference on Learning Representations (ICLR) Workshop. (2016)\n54. Zhang, X., Li, Z., Loy, C., Lin, D.: Polynet: A pursuit of structural diversity in\nvery deep networks. In: arXiv:1611.05725. (2016)\n55. Chen, Y., Li, J., Xiao, H., Jin, X., Yan, S., Feng, J.: Dual path networks. In:\narXiv:1707.01629. (2017)\n56. Hu, J., Shen, L., Sun, G.: Squeeze-and-excitation networks. In: arXiv:1709.01507.\n(2017)"
        },
        {
            "bounding_box": [
                {
                    "x": 547,
                    "y": 1248
                },
                {
                    "x": 1218,
                    "y": 1248
                },
                {
                    "x": 1218,
                    "y": 1306
                },
                {
                    "x": 547,
                    "y": 1306
                }
            ],
            "category": "paragraph",
            "html": "<p id='121' style='font-size:22px'>A Supplemental Material</p>",
            "id": 121,
            "page": 18,
            "text": "A Supplemental Material"
        },
        {
            "bounding_box": [
                {
                    "x": 550,
                    "y": 1360
                },
                {
                    "x": 1054,
                    "y": 1360
                },
                {
                    "x": 1054,
                    "y": 1411
                },
                {
                    "x": 550,
                    "y": 1411
                }
            ],
            "category": "paragraph",
            "html": "<p id='122' style='font-size:20px'>A.1 Hashtag Selection</p>",
            "id": 122,
            "page": 18,
            "text": "A.1 Hashtag Selection"
        },
        {
            "bounding_box": [
                {
                    "x": 546,
                    "y": 1449
                },
                {
                    "x": 2005,
                    "y": 1449
                },
                {
                    "x": 2005,
                    "y": 2100
                },
                {
                    "x": 546,
                    "y": 2100
                }
            ],
            "category": "paragraph",
            "html": "<p id='123' style='font-size:16px'>All of our Instagram datasets are subsets of I, where each image in I is from<br>a public Instagram post that has at least one hashtag in its caption (we do not<br>consider hashtags from other sources, such as comments). Let H be the set of all<br>unique hashtags associated with the images in I. To construct a dataset, we use<br>a simple data collection pipeline: (1) We select a set of hashtags that is a subset<br>of H. (2) We randomly samples images from I that are tagged with at least<br>one of these selected hashtags. (3) Then, because multiple hashtags may refer to<br>the same underlying concept, we apply a simple process (described below) that<br>utilizes WordNet [20] synsets to merge some hashtags into a single canonical form<br>(e.g., #brownbear and #ursusarctos are merged). (4) Finally, for each sampled<br>image, we replace each of its hashtags with its canonical form and discard any<br>hashtags that were not in the selected set. The canonical hashtags are used as<br>labels for training and evaluation.</p>",
            "id": 123,
            "page": 18,
            "text": "All of our Instagram datasets are subsets of I, where each image in I is from\na public Instagram post that has at least one hashtag in its caption (we do not\nconsider hashtags from other sources, such as comments). Let H be the set of all\nunique hashtags associated with the images in I. To construct a dataset, we use\na simple data collection pipeline: (1) We select a set of hashtags that is a subset\nof H. (2) We randomly samples images from I that are tagged with at least\none of these selected hashtags. (3) Then, because multiple hashtags may refer to\nthe same underlying concept, we apply a simple process (described below) that\nutilizes WordNet [20] synsets to merge some hashtags into a single canonical form\n(e.g., #brownbear and #ursusarctos are merged). (4) Finally, for each sampled\nimage, we replace each of its hashtags with its canonical form and discard any\nhashtags that were not in the selected set. The canonical hashtags are used as\nlabels for training and evaluation."
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 2124
                },
                {
                    "x": 2004,
                    "y": 2124
                },
                {
                    "x": 2004,
                    "y": 2627
                },
                {
                    "x": 545,
                    "y": 2627
                }
            ],
            "category": "paragraph",
            "html": "<p id='124' style='font-size:16px'>Hashtag-to-synset matching. Rather than taking a random subset of hash-<br>tags from H in step (1) above, we start with a set of WordNet synsets S and filter<br>the hashtags in H by accepting only the ones that match to any synset in S. To<br>determine if a match exists, we define a function s(S, h) that returns the (possibly<br>empty) subset of S that matches hashtag h E H. The function s is implemented<br>using the nltk interface to WordNet. Specifically, s returns the union of synsets<br>found by querying WordNet using calls to nltk. corpus . wordnet · synsets (x)<br>for different values of x, which are query strings derived from h. As values of x,<br>we use the original hashtag h as well as all bigrams formed by inserting a space<br>character at each position in h.</p>",
            "id": 124,
            "page": 18,
            "text": "Hashtag-to-synset matching. Rather than taking a random subset of hash-\ntags from H in step (1) above, we start with a set of WordNet synsets S and filter\nthe hashtags in H by accepting only the ones that match to any synset in S. To\ndetermine if a match exists, we define a function s(S, h) that returns the (possibly\nempty) subset of S that matches hashtag h E H. The function s is implemented\nusing the nltk interface to WordNet. Specifically, s returns the union of synsets\nfound by querying WordNet using calls to nltk. corpus . wordnet · synsets (x)\nfor different values of x, which are query strings derived from h. As values of x,\nwe use the original hashtag h as well as all bigrams formed by inserting a space\ncharacter at each position in h."
        },
        {
            "bounding_box": [
                {
                    "x": 546,
                    "y": 2653
                },
                {
                    "x": 2004,
                    "y": 2653
                },
                {
                    "x": 2004,
                    "y": 2805
                },
                {
                    "x": 546,
                    "y": 2805
                }
            ],
            "category": "paragraph",
            "html": "<p id='125' style='font-size:18px'>Canonical hashtag merging. Our hashtag merging process in step (3) is also<br>based on WordNet synsets. We consider two hashtags h E H and h' E H du-<br>plicates if and only if s(S,h) = s(S,h'). Herein, s(S,h) is the function defined</p>",
            "id": 125,
            "page": 18,
            "text": "Canonical hashtag merging. Our hashtag merging process in step (3) is also\nbased on WordNet synsets. We consider two hashtags h E H and h' E H du-\nplicates if and only if s(S,h) = s(S,h'). Herein, s(S,h) is the function defined"
        },
        {
            "bounding_box": [
                {
                    "x": 922,
                    "y": 409
                },
                {
                    "x": 1994,
                    "y": 409
                },
                {
                    "x": 1994,
                    "y": 456
                },
                {
                    "x": 922,
                    "y": 456
                }
            ],
            "category": "header",
            "html": "<header id='126' style='font-size:18px'>Exploring the Limits of Weakly Supervised Pretraining 19</header>",
            "id": 126,
            "page": 19,
            "text": "Exploring the Limits of Weakly Supervised Pretraining 19"
        },
        {
            "bounding_box": [
                {
                    "x": 546,
                    "y": 515
                },
                {
                    "x": 2003,
                    "y": 515
                },
                {
                    "x": 2003,
                    "y": 666
                },
                {
                    "x": 546,
                    "y": 666
                }
            ],
            "category": "paragraph",
            "html": "<p id='127' style='font-size:16px'>above that returns the set of all synsets that h matches in S. For hashtag merg-<br>ing, we set S to all WordNet synsets. This conservative deduplication strategy<br>merges two hashtags whenever they exactly coincide in all possible word senses.</p>",
            "id": 127,
            "page": 19,
            "text": "above that returns the set of all synsets that h matches in S. For hashtag merg-\ning, we set S to all WordNet synsets. This conservative deduplication strategy\nmerges two hashtags whenever they exactly coincide in all possible word senses."
        },
        {
            "bounding_box": [
                {
                    "x": 550,
                    "y": 744
                },
                {
                    "x": 1114,
                    "y": 744
                },
                {
                    "x": 1114,
                    "y": 794
                },
                {
                    "x": 550,
                    "y": 794
                }
            ],
            "category": "paragraph",
            "html": "<p id='128' style='font-size:20px'>A.2 Image Deduplication</p>",
            "id": 128,
            "page": 19,
            "text": "A.2 Image Deduplication"
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 832
                },
                {
                    "x": 2004,
                    "y": 832
                },
                {
                    "x": 2004,
                    "y": 1230
                },
                {
                    "x": 545,
                    "y": 1230
                }
            ],
            "category": "paragraph",
            "html": "<p id='129' style='font-size:18px'>We implemented a two-stage deduplication procedure. In the first stage, we<br>search the set of 3.5 billion Instagram images2 using an approximate nearest<br>neighbor search algorithm to identify 128 potential duplicates for each query<br>image (e.g., an image from val-IN-1k). In the second stage, we compute high-<br>quality image features for these duplicate candidates, compute pairwise distances<br>between the candidates and the query image, and apply a conservative distance<br>threshold to generate potential pairs of duplicates for annotation by human<br>annotators. We describe both stages separately below.</p>",
            "id": 129,
            "page": 19,
            "text": "We implemented a two-stage deduplication procedure. In the first stage, we\nsearch the set of 3.5 billion Instagram images2 using an approximate nearest\nneighbor search algorithm to identify 128 potential duplicates for each query\nimage (e.g., an image from val-IN-1k). In the second stage, we compute high-\nquality image features for these duplicate candidates, compute pairwise distances\nbetween the candidates and the query image, and apply a conservative distance\nthreshold to generate potential pairs of duplicates for annotation by human\nannotators. We describe both stages separately below."
        },
        {
            "bounding_box": [
                {
                    "x": 544,
                    "y": 1234
                },
                {
                    "x": 2004,
                    "y": 1234
                },
                {
                    "x": 2004,
                    "y": 1780
                },
                {
                    "x": 544,
                    "y": 1780
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='130' style='font-size:18px'>In the first stage, we remove the last five3 convolutional layers from a<br>ResNet-50 model. We resize each of the Instagram images such that its height or<br>width (whichever is larger) is 400 pixels, and perform a forward-pass through the<br>truncated convolutional network. We compute regional maximum activations of<br>convolutions (R-MAC) features [22,23] from the resulting feature maps: i.e., we<br>perform max-pooling on fixed sub-regions of the feature maps. R-MAC features<br>substantially improve the performance of the deduplication method: they add<br>invariance to cropping transformations that may have been applied on the im-<br>ages. We whiten the 2048-dimensional R-MAC features using PCA to obtain<br>a 512-dimensional descriptor for each image. The resulting image descriptor is<br>scalar-quantized to 8 bits per dimension to facilitate storage on disk.</p>",
            "id": 130,
            "page": 19,
            "text": "In the first stage, we remove the last five3 convolutional layers from a\nResNet-50 model. We resize each of the Instagram images such that its height or\nwidth (whichever is larger) is 400 pixels, and perform a forward-pass through the\ntruncated convolutional network. We compute regional maximum activations of\nconvolutions (R-MAC) features [22,23] from the resulting feature maps: i.e., we\nperform max-pooling on fixed sub-regions of the feature maps. R-MAC features\nsubstantially improve the performance of the deduplication method: they add\ninvariance to cropping transformations that may have been applied on the im-\nages. We whiten the 2048-dimensional R-MAC features using PCA to obtain\na 512-dimensional descriptor for each image. The resulting image descriptor is\nscalar-quantized to 8 bits per dimension to facilitate storage on disk."
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 1783
                },
                {
                    "x": 2003,
                    "y": 1783
                },
                {
                    "x": 2003,
                    "y": 2425
                },
                {
                    "x": 545,
                    "y": 2425
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='131' style='font-size:16px'>To construct a searchable index of 3.5 billion images, we undo the scalar<br>quantization, L2-normalize the resulting vector, and apply optimized product<br>quantization (OPQ; [48]) to further reduce the feature representation to 256 di-<br>mensions. Next, we apply a coarse product quantizer [49] with 2 sub-quantizers<br>that each operate on 128 of the 256 dimensions. Each sub-quantizer uses k-means<br>clustering to quantize the 128 dimensions into 14 bits. The resulting 28-bit im-<br>age representation is used to construct an inverted list of the 3.5 billion images.<br>Finally, we apply a product quantizer with 32 sub-quantizers on the residual<br>feature vector; each of the sub-quantizers uses k-means to quantize 256/32 = 8<br>dimensions of the image representation into 8 bits. The resulting 32-byte rep-<br>resentation is stored with the image in the corresponding inverted list. All pre-<br>processors and quantizers were trained on 2 million images; we implemented the<br>image index using Faiss [45].</p>",
            "id": 131,
            "page": 19,
            "text": "To construct a searchable index of 3.5 billion images, we undo the scalar\nquantization, L2-normalize the resulting vector, and apply optimized product\nquantization (OPQ; [48]) to further reduce the feature representation to 256 di-\nmensions. Next, we apply a coarse product quantizer [49] with 2 sub-quantizers\nthat each operate on 128 of the 256 dimensions. Each sub-quantizer uses k-means\nclustering to quantize the 128 dimensions into 14 bits. The resulting 28-bit im-\nage representation is used to construct an inverted list of the 3.5 billion images.\nFinally, we apply a product quantizer with 32 sub-quantizers on the residual\nfeature vector; each of the sub-quantizers uses k-means to quantize 256/32 = 8\ndimensions of the image representation into 8 bits. The resulting 32-byte rep-\nresentation is stored with the image in the corresponding inverted list. All pre-\nprocessors and quantizers were trained on 2 million images; we implemented the\nimage index using Faiss [45]."
        },
        {
            "bounding_box": [
                {
                    "x": 546,
                    "y": 2430
                },
                {
                    "x": 2004,
                    "y": 2430
                },
                {
                    "x": 2004,
                    "y": 2630
                },
                {
                    "x": 546,
                    "y": 2630
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='132' style='font-size:16px'>Given a query image from the validation set of a target task, we search the<br>resulting index as described in [49]. First, we compute R-MAC features for the<br>query image, and preprocess the features using PCA, scalar quantization, and<br>OPQ. Next, we apply the coarse quantizer on the resulting descriptor and find</p>",
            "id": 132,
            "page": 19,
            "text": "Given a query image from the validation set of a target task, we search the\nresulting index as described in [49]. First, we compute R-MAC features for the\nquery image, and preprocess the features using PCA, scalar quantization, and\nOPQ. Next, we apply the coarse quantizer on the resulting descriptor and find"
        },
        {
            "bounding_box": [
                {
                    "x": 556,
                    "y": 2661
                },
                {
                    "x": 2003,
                    "y": 2661
                },
                {
                    "x": 2003,
                    "y": 2803
                },
                {
                    "x": 556,
                    "y": 2803
                }
            ],
            "category": "paragraph",
            "html": "<p id='133' style='font-size:14px'>2 This is the largest data set we used and all other image sets are subsets of this one.<br>3 The hyperparameters of our deduplication pipeline were manually tuned using pre-<br>liminary experiments on the Holidays [47] dataset.</p>",
            "id": 133,
            "page": 19,
            "text": "2 This is the largest data set we used and all other image sets are subsets of this one.\n3 The hyperparameters of our deduplication pipeline were manually tuned using pre-\nliminary experiments on the Holidays [47] dataset."
        },
        {
            "bounding_box": [
                {
                    "x": 547,
                    "y": 409
                },
                {
                    "x": 943,
                    "y": 409
                },
                {
                    "x": 943,
                    "y": 456
                },
                {
                    "x": 547,
                    "y": 456
                }
            ],
            "category": "header",
            "html": "<header id='134' style='font-size:16px'>20 Mahajan et al.</header>",
            "id": 134,
            "page": 20,
            "text": "20 Mahajan et al."
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 514
                },
                {
                    "x": 2007,
                    "y": 514
                },
                {
                    "x": 2007,
                    "y": 811
                },
                {
                    "x": 545,
                    "y": 811
                }
            ],
            "category": "paragraph",
            "html": "<p id='135' style='font-size:20px'>the 256 nearest sub-quantizers (in terms of Hamming distance). We compute<br>the residual for each of these sub-quantizers, and compute the squared distances<br>between this residual and the residuals stored in the corresponding entries in the<br>inverted list. This produces distance estimates, which we use to select the 128<br>nearest neighbors (in terms of the distance map) efficiently using a max-heap<br>implementation.</p>",
            "id": 135,
            "page": 20,
            "text": "the 256 nearest sub-quantizers (in terms of Hamming distance). We compute\nthe residual for each of these sub-quantizers, and compute the squared distances\nbetween this residual and the residuals stored in the corresponding entries in the\ninverted list. This produces distance estimates, which we use to select the 128\nnearest neighbors (in terms of the distance map) efficiently using a max-heap\nimplementation."
        },
        {
            "bounding_box": [
                {
                    "x": 544,
                    "y": 816
                },
                {
                    "x": 2004,
                    "y": 816
                },
                {
                    "x": 2004,
                    "y": 1310
                },
                {
                    "x": 544,
                    "y": 1310
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='136' style='font-size:18px'>In the second stage, we compute R-MAC features for the query image and<br>each of the 128 identified neighbors in the same way as in the first stage, however,<br>in this stage we do not compress the 2048-dimensional R-MAC features in any<br>way. We compute exact squared Euclidean distances between the features of the<br>query and neighbor images, and apply a (very conservative) distance threshold<br>of 0.6 on each of the distances. For each image in the query set (e.g., an image<br>from val-IN-1k) that has at least one neighbor in IG-3.5B for which the R-MAC<br>feature distance is smaller than this threshold, we manually annotated the 21<br>nearest neighbors (in terms of the R-MAC feature distance) to assess whether<br>or not the query image has duplicates in the Instagram dataset.</p>",
            "id": 136,
            "page": 20,
            "text": "In the second stage, we compute R-MAC features for the query image and\neach of the 128 identified neighbors in the same way as in the first stage, however,\nin this stage we do not compress the 2048-dimensional R-MAC features in any\nway. We compute exact squared Euclidean distances between the features of the\nquery and neighbor images, and apply a (very conservative) distance threshold\nof 0.6 on each of the distances. For each image in the query set (e.g., an image\nfrom val-IN-1k) that has at least one neighbor in IG-3.5B for which the R-MAC\nfeature distance is smaller than this threshold, we manually annotated the 21\nnearest neighbors (in terms of the R-MAC feature distance) to assess whether\nor not the query image has duplicates in the Instagram dataset."
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 1315
                },
                {
                    "x": 2006,
                    "y": 1315
                },
                {
                    "x": 2006,
                    "y": 1612
                },
                {
                    "x": 545,
                    "y": 1612
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='137' style='font-size:18px'>This procedure led us to 150 val-IN-50k-1k (0.30%), 10 val-CUB-6k-200<br>(0.17%), 151 val-Places-37k-365 (0.41%), and 6 val-COCO-5k-80 (0.12%) images<br>as duplicates. In the results in the main paper, we included these duplicates in<br>the reported accuracies. In Table 2, we also report a conservative lower bound<br>on the accuracy of our best models that treats all images that have duplicates<br>in the training set as being classified incorrectly4.</p>",
            "id": 137,
            "page": 20,
            "text": "This procedure led us to 150 val-IN-50k-1k (0.30%), 10 val-CUB-6k-200\n(0.17%), 151 val-Places-37k-365 (0.41%), and 6 val-COCO-5k-80 (0.12%) images\nas duplicates. In the results in the main paper, we included these duplicates in\nthe reported accuracies. In Table 2, we also report a conservative lower bound\non the accuracy of our best models that treats all images that have duplicates\nin the training set as being classified incorrectly4."
        },
        {
            "bounding_box": [
                {
                    "x": 852,
                    "y": 1698
                },
                {
                    "x": 1693,
                    "y": 1698
                },
                {
                    "x": 1693,
                    "y": 1947
                },
                {
                    "x": 852,
                    "y": 1947
                }
            ],
            "category": "table",
            "html": "<table id='138' style='font-size:14px'><tr><td>Dataset</td><td>Measured Accuracy (%)</td><td>Lower-bound Accuracy (%)</td></tr><tr><td>val-IN-50k-1k</td><td>84.2</td><td>83.9</td></tr><tr><td>val-CUB-6k-200</td><td>89.2</td><td>89.0</td></tr><tr><td>val-Places-37k-365</td><td>58.0</td><td>57.6</td></tr></table>",
            "id": 138,
            "page": 20,
            "text": "Dataset Measured Accuracy (%) Lower-bound Accuracy (%)\n val-IN-50k-1k 84.2 83.9\n val-CUB-6k-200 89.2 89.0\n val-Places-37k-365 58.0"
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 1980
                },
                {
                    "x": 2005,
                    "y": 1980
                },
                {
                    "x": 2005,
                    "y": 2168
                },
                {
                    "x": 545,
                    "y": 2168
                }
            ],
            "category": "caption",
            "html": "<caption id='139' style='font-size:16px'>Table 2: Lower bounds on reported accuracies. Measured top-1 accuracies (left<br>column) and conservative lower bounds on those accuracies obtained by considering<br>all test images with duplicates in the training set as being classified incorrectly (right<br>column). See text in Section A.2 for details.</caption>",
            "id": 139,
            "page": 20,
            "text": "Table 2: Lower bounds on reported accuracies. Measured top-1 accuracies (left\ncolumn) and conservative lower bounds on those accuracies obtained by considering\nall test images with duplicates in the training set as being classified incorrectly (right\ncolumn). See text in Section A.2 for details."
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 2345
                },
                {
                    "x": 1019,
                    "y": 2345
                },
                {
                    "x": 1019,
                    "y": 2398
                },
                {
                    "x": 549,
                    "y": 2398
                }
            ],
            "category": "paragraph",
            "html": "<p id='140' style='font-size:22px'>A.3 Training Details</p>",
            "id": 140,
            "page": 20,
            "text": "A.3 Training Details"
        },
        {
            "bounding_box": [
                {
                    "x": 547,
                    "y": 2433
                },
                {
                    "x": 2005,
                    "y": 2433
                },
                {
                    "x": 2005,
                    "y": 2581
                },
                {
                    "x": 547,
                    "y": 2581
                }
            ],
            "category": "paragraph",
            "html": "<p id='141' style='font-size:22px'>Classification hyperparameters. Our training hyperparameters match those<br>in [19]. We briefly summarize them here for completeness. For SGD, we use<br>Nesterov momentum [50] of 0.9 and weight decay of 0.0001. Weight decay is</p>",
            "id": 141,
            "page": 20,
            "text": "Classification hyperparameters. Our training hyperparameters match those\nin [19]. We briefly summarize them here for completeness. For SGD, we use\nNesterov momentum [50] of 0.9 and weight decay of 0.0001. Weight decay is"
        },
        {
            "bounding_box": [
                {
                    "x": 553,
                    "y": 2616
                },
                {
                    "x": 2004,
                    "y": 2616
                },
                {
                    "x": 2004,
                    "y": 2803
                },
                {
                    "x": 553,
                    "y": 2803
                }
            ],
            "category": "paragraph",
            "html": "<p id='142' style='font-size:14px'>4<br>We note that similar lower bounds should also be computed for networks that are<br>pretrained on ImageNet and then transferred to other datasets (for instance, it is<br>estimated that approximately 5% of the test images in val-CUB-6k-200 are also in<br>train-IN-1M-1k), but we did not perform the required duplicate analyses.</p>",
            "id": 142,
            "page": 20,
            "text": "4\nWe note that similar lower bounds should also be computed for networks that are\npretrained on ImageNet and then transferred to other datasets (for instance, it is\nestimated that approximately 5% of the test images in val-CUB-6k-200 are also in\ntrain-IN-1M-1k), but we did not perform the required duplicate analyses."
        },
        {
            "bounding_box": [
                {
                    "x": 922,
                    "y": 410
                },
                {
                    "x": 1989,
                    "y": 410
                },
                {
                    "x": 1989,
                    "y": 456
                },
                {
                    "x": 922,
                    "y": 456
                }
            ],
            "category": "header",
            "html": "<header id='143' style='font-size:20px'>Exploring the Limits of Weakly Supervised Pretraining 21</header>",
            "id": 143,
            "page": 21,
            "text": "Exploring the Limits of Weakly Supervised Pretraining 21"
        },
        {
            "bounding_box": [
                {
                    "x": 546,
                    "y": 515
                },
                {
                    "x": 2003,
                    "y": 515
                },
                {
                    "x": 2003,
                    "y": 863
                },
                {
                    "x": 546,
                    "y": 863
                }
            ],
            "category": "paragraph",
            "html": "<p id='144' style='font-size:20px'>not applied to the batch normalization (BN) scale () and bias (B) parameters.<br>All convolutional filters are initialized according to [28]. Weights of the final<br>fully connected layer are sampled from a zero-mean Gaussian with standard<br>deviation 0.01. Following [19], the final BN scale parameter 2 of each residual<br>block is initialized to 0 (instead of 1). We use standard image rescaling and data<br>augmentation as described in [19]. These settings are consistent across ImageNet<br>and Instagram pretraining.</p>",
            "id": 144,
            "page": 21,
            "text": "not applied to the batch normalization (BN) scale () and bias (B) parameters.\nAll convolutional filters are initialized according to [28]. Weights of the final\nfully connected layer are sampled from a zero-mean Gaussian with standard\ndeviation 0.01. Following [19], the final BN scale parameter 2 of each residual\nblock is initialized to 0 (instead of 1). We use standard image rescaling and data\naugmentation as described in [19]. These settings are consistent across ImageNet\nand Instagram pretraining."
        },
        {
            "bounding_box": [
                {
                    "x": 546,
                    "y": 884
                },
                {
                    "x": 2006,
                    "y": 884
                },
                {
                    "x": 2006,
                    "y": 1182
                },
                {
                    "x": 546,
                    "y": 1182
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='145' style='font-size:20px'>Our feature transfer experiments require training an L2-regularized logistic<br>regressor. Since the objective function is convex, optimization is straightforward<br>and we only need to perform a grid search for the L2 penalty. For train-CUB-200,<br>the optimal penalty was 0.001; for all other datasets it was 0.0001. When training<br>from scratch or performing full network finetuning, the training hyperparameters<br>are more complex. We give details in Tables 3 and 4.</p>",
            "id": 145,
            "page": 21,
            "text": "Our feature transfer experiments require training an L2-regularized logistic\nregressor. Since the objective function is convex, optimization is straightforward\nand we only need to perform a grid search for the L2 penalty. For train-CUB-200,\nthe optimal penalty was 0.001; for all other datasets it was 0.0001. When training\nfrom scratch or performing full network finetuning, the training hyperparameters\nare more complex. We give details in Tables 3 and 4."
        },
        {
            "bounding_box": [
                {
                    "x": 575,
                    "y": 1307
                },
                {
                    "x": 1972,
                    "y": 1307
                },
                {
                    "x": 1972,
                    "y": 1667
                },
                {
                    "x": 575,
                    "y": 1667
                }
            ],
            "category": "table",
            "html": "<table id='146' style='font-size:14px'><tr><td>Dataset</td><td>Total length</td><td>LR steps</td><td>Initial LR</td><td>LR decay</td><td>Weight decay</td></tr><tr><td rowspan=\"3\">train-IN-1k train-IN-5k train-IN-9k</td><td>100 epochs</td><td>[30, 30, 30, 10]</td><td>0.1/256</td><td>0.1</td><td>1e-4</td></tr><tr><td>40 epochs</td><td>[15, 15, 6, 2]</td><td>0.1/256</td><td>0.1</td><td>1e-4</td></tr><tr><td>25 epochs</td><td>[9.35, 9.35, 3.75, 1.25]</td><td>0.1/256</td><td>0.1</td><td>1e-4</td></tr><tr><td rowspan=\"4\">train-IG-940M-1.5k train-IG-940k-1.5k train-IG-3.5B-17k train-IG-3.5M-17k</td><td>1925e6 images</td><td>20</td><td>0.1/256</td><td>0.5</td><td>1e-4</td></tr><tr><td>300e6 images</td><td>20</td><td>0.1/256</td><td>0.5</td><td>1e-4</td></tr><tr><td>7000e6 images</td><td>40</td><td>0.1/256</td><td>V0.5</td><td>1e-4</td></tr><tr><td>300e6 images</td><td>20</td><td>0.1/256</td><td>0.5</td><td>1e-4</td></tr></table>",
            "id": 146,
            "page": 21,
            "text": "Dataset Total length LR steps Initial LR LR decay Weight decay\n train-IN-1k train-IN-5k train-IN-9k 100 epochs [30, 30, 30, 10] 0.1/256 0.1 1e-4\n 40 epochs [15, 15, 6, 2] 0.1/256 0.1 1e-4\n 25 epochs [9.35, 9.35, 3.75, 1.25] 0.1/256 0.1 1e-4\n train-IG-940M-1.5k train-IG-940k-1.5k train-IG-3.5B-17k train-IG-3.5M-17k 1925e6 images 20 0.1/256 0.5 1e-4\n 300e6 images 20 0.1/256 0.5 1e-4\n 7000e6 images 40 0.1/256 V0.5 1e-4\n 300e6 images 20 0.1/256 0.5"
        },
        {
            "bounding_box": [
                {
                    "x": 547,
                    "y": 1703
                },
                {
                    "x": 2001,
                    "y": 1703
                },
                {
                    "x": 2001,
                    "y": 2207
                },
                {
                    "x": 547,
                    "y": 2207
                }
            ],
            "category": "paragraph",
            "html": "<p id='147' style='font-size:16px'>Table 3: Training schedules for pretraining of models. For ImageNet datasets,<br>the total training length is given in terms of dataset epochs. The learning rate (LR)<br>is set to the initial LR at the start of training (the same for all cases, given as a<br>minibatch size normalized reference value [19]) and decayed by the specified LR decay<br>factor according to the steps, also given in epochs for ImageNet datasets. For Instagram<br>datasets, the total training length is given in terms of the number of images processed.<br>The LR is decayed from the initial value by the specified factor at equally spaced<br>steps; the total number LR decay steps is given in the LR steps column. We specify<br>the schedules for the two dataset extremes for the cases of 1.5k and 17k hashtags. When<br>training on a number of images between the two extremes, we linearly interpolate the<br>training schedule.</p>",
            "id": 147,
            "page": 21,
            "text": "Table 3: Training schedules for pretraining of models. For ImageNet datasets,\nthe total training length is given in terms of dataset epochs. The learning rate (LR)\nis set to the initial LR at the start of training (the same for all cases, given as a\nminibatch size normalized reference value [19]) and decayed by the specified LR decay\nfactor according to the steps, also given in epochs for ImageNet datasets. For Instagram\ndatasets, the total training length is given in terms of the number of images processed.\nThe LR is decayed from the initial value by the specified factor at equally spaced\nsteps; the total number LR decay steps is given in the LR steps column. We specify\nthe schedules for the two dataset extremes for the cases of 1.5k and 17k hashtags. When\ntraining on a number of images between the two extremes, we linearly interpolate the\ntraining schedule."
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 2451
                },
                {
                    "x": 2003,
                    "y": 2451
                },
                {
                    "x": 2003,
                    "y": 2804
                },
                {
                    "x": 545,
                    "y": 2804
                }
            ],
            "category": "paragraph",
            "html": "<p id='148' style='font-size:18px'>Detection hyperparameters. For our object detection experiments, we found<br>it necessary to perform grid search for the initial learning rate; using defaults<br>that work well when finetuning from ImageNet-pretrained models worked poorly<br>when finetuning with Instagram-pretrained models. The grid search was per-<br>formed on the COCO val2017 set, which results in the paper are reported on<br>the test-dev2017 set, which requires submitting results to the COCO evaluation<br>server. The optimal initial learning rates are shown in Table 5.</p>",
            "id": 148,
            "page": 21,
            "text": "Detection hyperparameters. For our object detection experiments, we found\nit necessary to perform grid search for the initial learning rate; using defaults\nthat work well when finetuning from ImageNet-pretrained models worked poorly\nwhen finetuning with Instagram-pretrained models. The grid search was per-\nformed on the COCO val2017 set, which results in the paper are reported on\nthe test-dev2017 set, which requires submitting results to the COCO evaluation\nserver. The optimal initial learning rates are shown in Table 5."
        },
        {
            "bounding_box": [
                {
                    "x": 547,
                    "y": 409
                },
                {
                    "x": 945,
                    "y": 409
                },
                {
                    "x": 945,
                    "y": 457
                },
                {
                    "x": 547,
                    "y": 457
                }
            ],
            "category": "header",
            "html": "<header id='149' style='font-size:18px'>22 Mahajan et al.</header>",
            "id": 149,
            "page": 22,
            "text": "22 Mahajan et al."
        },
        {
            "bounding_box": [
                {
                    "x": 569,
                    "y": 502
                },
                {
                    "x": 1977,
                    "y": 502
                },
                {
                    "x": 1977,
                    "y": 1196
                },
                {
                    "x": 569,
                    "y": 1196
                }
            ],
            "category": "table",
            "html": "<table id='150' style='font-size:14px'><tr><td>Source dataset</td><td>Target dataset</td><td>Total length</td><td>LR steps</td><td>Initial LR</td><td>LR decay</td><td>Weight decay</td></tr><tr><td rowspan=\"3\">train-IG-940M-1.5k train-IG-1B-17k train-IG-3.5B-17k</td><td>train-IN-1k</td><td>30 epochs</td><td>[10, 10, 10]</td><td>0.00025/256</td><td>0.1</td><td>1e-4</td></tr><tr><td>train-IN-1k</td><td>30 epochs</td><td>[10, 10, 10]</td><td>0.00025/256</td><td>0.1</td><td>1e-4</td></tr><tr><td>train-IN-1k</td><td>30 epochs</td><td>[10, 10, 10]</td><td>0.00025 /256</td><td>0.1</td><td>1e-4</td></tr><tr><td rowspan=\"3\">train-IG-940M-1.5k train-IG-1B-17k train-IG-3.5B-17k</td><td>train-IN-5k</td><td>30 epochs</td><td>[10, 10, 10]</td><td>0.0025, / 256</td><td>0.1</td><td>1e-4</td></tr><tr><td>train-IN-5k</td><td>30 epochs</td><td>[10, 10, 10]</td><td>0.00025/256</td><td>0.1</td><td>1e-4</td></tr><tr><td>train-IN-5k</td><td>30 epochs</td><td>10,10, 10]</td><td>0.00025, /256</td><td>0.1</td><td>1e-4</td></tr><tr><td rowspan=\"3\">train-IG-940M-1.5k train-IG-1B-17k train-IG-3.5B-17k</td><td>train-IN-9k</td><td>24 epochs</td><td>[8, 8,8]</td><td>0.0025 /256</td><td>0.1</td><td>1e-4</td></tr><tr><td>train-IN-9k</td><td>24 epochs</td><td>[8, 8,8]</td><td>0.00025/256</td><td>0.1</td><td>1e-4</td></tr><tr><td>train-IN-9k</td><td>24 epochs</td><td>[8, 8,8]</td><td>0.00025 /256</td><td>0.1</td><td>1e-4</td></tr><tr><td rowspan=\"4\">train-IG-940M-1.5k train-IG-1B-17k train-IG-3.5B-7k train-IN-1.3M-1k</td><td>train-CUB-200</td><td>300 epochs</td><td>[100, 100, 100]</td><td>0.00025 /256</td><td>0.1</td><td>1e-3</td></tr><tr><td>train-CUB-200</td><td>300 epochs</td><td>[100, 100, 100]</td><td>0.00025/256</td><td>0.1</td><td>1e-3</td></tr><tr><td>train-CUB-200</td><td>300 epochs</td><td>[100, 100, 100]</td><td>0.00025/256</td><td>0.1</td><td>1e-3</td></tr><tr><td>train-CUB-200</td><td>300 epochs</td><td>[100, 100, 100]</td><td>0.0025 /256</td><td>0.1</td><td>1e-2</td></tr><tr><td rowspan=\"4\">train-IG-940M-1.5k train-IG-1B-17k train-IG-3.5B-7k train-IN-1.3M-1k</td><td>train-Places-365</td><td>8 epochs</td><td>[4, 2, 2]</td><td>0.00025/256</td><td>0.1</td><td>1e-4</td></tr><tr><td>train-Places-365</td><td>8 epochs</td><td>[4,2, 2]</td><td>0.00025/256</td><td>0.1</td><td>1e-4</td></tr><tr><td>train-Places-365</td><td>8 epochs</td><td>[4, 2,2]</td><td>0.00025/256</td><td>0.1</td><td>1e-4</td></tr><tr><td>train-Places-365</td><td>18 epochs</td><td>[8, 6, 2, 2]</td><td>0.0025/256</td><td>0.1</td><td>1e-4</td></tr></table>",
            "id": 150,
            "page": 22,
            "text": "Source dataset Target dataset Total length LR steps Initial LR LR decay Weight decay\n train-IG-940M-1.5k train-IG-1B-17k train-IG-3.5B-17k train-IN-1k 30 epochs [10, 10, 10] 0.00025/256 0.1 1e-4\n train-IN-1k 30 epochs [10, 10, 10] 0.00025/256 0.1 1e-4\n train-IN-1k 30 epochs [10, 10, 10] 0.00025 /256 0.1 1e-4\n train-IG-940M-1.5k train-IG-1B-17k train-IG-3.5B-17k train-IN-5k 30 epochs [10, 10, 10] 0.0025, / 256 0.1 1e-4\n train-IN-5k 30 epochs [10, 10, 10] 0.00025/256 0.1 1e-4\n train-IN-5k 30 epochs 10,10, 10] 0.00025, /256 0.1 1e-4\n train-IG-940M-1.5k train-IG-1B-17k train-IG-3.5B-17k train-IN-9k 24 epochs [8, 8,8] 0.0025 /256 0.1 1e-4\n train-IN-9k 24 epochs [8, 8,8] 0.00025/256 0.1 1e-4\n train-IN-9k 24 epochs [8, 8,8] 0.00025 /256 0.1 1e-4\n train-IG-940M-1.5k train-IG-1B-17k train-IG-3.5B-7k train-IN-1.3M-1k train-CUB-200 300 epochs [100, 100, 100] 0.00025 /256 0.1 1e-3\n train-CUB-200 300 epochs [100, 100, 100] 0.00025/256 0.1 1e-3\n train-CUB-200 300 epochs [100, 100, 100] 0.00025/256 0.1 1e-3\n train-CUB-200 300 epochs [100, 100, 100] 0.0025 /256 0.1 1e-2\n train-IG-940M-1.5k train-IG-1B-17k train-IG-3.5B-7k train-IN-1.3M-1k train-Places-365 8 epochs [4, 2, 2] 0.00025/256 0.1 1e-4\n train-Places-365 8 epochs [4,2, 2] 0.00025/256 0.1 1e-4\n train-Places-365 8 epochs [4, 2,2] 0.00025/256 0.1 1e-4\n train-Places-365 18 epochs [8, 6, 2, 2] 0.0025/256 0.1"
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 1229
                },
                {
                    "x": 2005,
                    "y": 1229
                },
                {
                    "x": 2005,
                    "y": 1511
                },
                {
                    "x": 545,
                    "y": 1511
                }
            ],
            "category": "paragraph",
            "html": "<p id='151' style='font-size:18px'>Table 4: Training schedules for finetuning of models. For transfer learning with<br>full network finetuning, we used a proper validation set held out from the training<br>set of the target task. Using this validation set, we did a coarse grid search to find<br>the initial LR (chosen from 0.0025 /256, 0.00025 /256, or 0.000025 /256) and the weight<br>decay (chosen from 0.01, 0.001, or 0.0001). In all cases, we fixed the length of the<br>finetuning schedule based on some preliminary experiments.</p>",
            "id": 151,
            "page": 22,
            "text": "Table 4: Training schedules for finetuning of models. For transfer learning with\nfull network finetuning, we used a proper validation set held out from the training\nset of the target task. Using this validation set, we did a coarse grid search to find\nthe initial LR (chosen from 0.0025 /256, 0.00025 /256, or 0.000025 /256) and the weight\ndecay (chosen from 0.01, 0.001, or 0.0001). In all cases, we fixed the length of the\nfinetuning schedule based on some preliminary experiments."
        },
        {
            "bounding_box": [
                {
                    "x": 925,
                    "y": 1559
                },
                {
                    "x": 1624,
                    "y": 1559
                },
                {
                    "x": 1624,
                    "y": 2130
                },
                {
                    "x": 925,
                    "y": 2130
                }
            ],
            "category": "table",
            "html": "<table id='152' style='font-size:14px'><tr><td>Backbone</td><td>model</td><td>Source dataset</td><td>Initial LR</td></tr><tr><td>ResNeXt-101</td><td>32x4d</td><td>IN-1k</td><td>0.01</td></tr><tr><td>ResNeXt-101</td><td>32x4d</td><td>IN-5k</td><td>0.01</td></tr><tr><td>ResNeXt-101</td><td>32x4d</td><td>IG-940M-1.5k</td><td>0.0025</td></tr><tr><td>ResNeXt-101</td><td>32x4d</td><td>IG-1B-17k</td><td>0.0025</td></tr><tr><td>ResNeXt-101</td><td>32x8d</td><td>IN-1k</td><td>0.01</td></tr><tr><td>ResNeXt-101</td><td>32x8d</td><td>IN-5k</td><td>0.01</td></tr><tr><td>ResNeXt-101</td><td>32x8d</td><td>IG-940M-1.5k</td><td>0.0025</td></tr><tr><td>ResNeXt-101</td><td>32x8d</td><td>IG-1B-17k</td><td>0.0025</td></tr><tr><td>ResNeXt-101</td><td>32x16d</td><td>IN-1k</td><td>0.01</td></tr><tr><td>ResNeXt-101</td><td>32x 16d</td><td>IN-5k</td><td>0.01</td></tr><tr><td>ResNeXt-101</td><td>32x16d</td><td>IG-940M-1.5k</td><td>0.0025</td></tr><tr><td>ResNeXt-101</td><td>32x16d</td><td>IG-1B-17k</td><td>0.0025</td></tr><tr><td>ResNeXt-101</td><td>32x16d</td><td>IG-3.5B-17k</td><td>0.00075</td></tr></table>",
            "id": 152,
            "page": 22,
            "text": "Backbone model Source dataset Initial LR\n ResNeXt-101 32x4d IN-1k 0.01\n ResNeXt-101 32x4d IN-5k 0.01\n ResNeXt-101 32x4d IG-940M-1.5k 0.0025\n ResNeXt-101 32x4d IG-1B-17k 0.0025\n ResNeXt-101 32x8d IN-1k 0.01\n ResNeXt-101 32x8d IN-5k 0.01\n ResNeXt-101 32x8d IG-940M-1.5k 0.0025\n ResNeXt-101 32x8d IG-1B-17k 0.0025\n ResNeXt-101 32x16d IN-1k 0.01\n ResNeXt-101 32x 16d IN-5k 0.01\n ResNeXt-101 32x16d IG-940M-1.5k 0.0025\n ResNeXt-101 32x16d IG-1B-17k 0.0025\n ResNeXt-101 32x16d IG-3.5B-17k"
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 2162
                },
                {
                    "x": 2004,
                    "y": 2162
                },
                {
                    "x": 2004,
                    "y": 2304
                },
                {
                    "x": 545,
                    "y": 2304
                }
            ],
            "category": "caption",
            "html": "<caption id='153' style='font-size:16px'>Table 5: Training schedules for finetuning of Mask R-CNN for detection on<br>COCO. All models are trained with a minibatch size 8 (images) for a total of 180k<br>iterations. The learning rate is decreased by a factor of 0.1 at 120k and 160k iterations.</caption>",
            "id": 153,
            "page": 22,
            "text": "Table 5: Training schedules for finetuning of Mask R-CNN for detection on\nCOCO. All models are trained with a minibatch size 8 (images) for a total of 180k\niterations. The learning rate is decreased by a factor of 0.1 at 120k and 160k iterations."
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 2407
                },
                {
                    "x": 1045,
                    "y": 2407
                },
                {
                    "x": 1045,
                    "y": 2463
                },
                {
                    "x": 549,
                    "y": 2463
                }
            ],
            "category": "paragraph",
            "html": "<p id='154' style='font-size:22px'>A.4 Data Resampling</p>",
            "id": 154,
            "page": 22,
            "text": "A.4 Data Resampling"
        },
        {
            "bounding_box": [
                {
                    "x": 546,
                    "y": 2502
                },
                {
                    "x": 2006,
                    "y": 2502
                },
                {
                    "x": 2006,
                    "y": 2806
                },
                {
                    "x": 546,
                    "y": 2806
                }
            ],
            "category": "paragraph",
            "html": "<p id='155' style='font-size:20px'>Hashtag frequencies follow a Zipfian distribution. For example, in the 17k hash-<br>tag vocabulary, the most frequent hashtag (#fineart) appears more than 1<br>million times as often as the least frequent hashtag (#shirtfront). Training<br>on the natural distribution may not be optimal and therefore we consider two<br>alternative ways to sample: uniform and square root. In both cases, we compute<br>a replication factor r(h) for each hashtag h: r(h) = max(1, �(t/f(h))), where</p>",
            "id": 155,
            "page": 22,
            "text": "Hashtag frequencies follow a Zipfian distribution. For example, in the 17k hash-\ntag vocabulary, the most frequent hashtag (#fineart) appears more than 1\nmillion times as often as the least frequent hashtag (#shirtfront). Training\non the natural distribution may not be optimal and therefore we consider two\nalternative ways to sample: uniform and square root. In both cases, we compute\na replication factor r(h) for each hashtag h: r(h) = max(1, �(t/f(h))), where"
        },
        {
            "bounding_box": [
                {
                    "x": 921,
                    "y": 408
                },
                {
                    "x": 2000,
                    "y": 408
                },
                {
                    "x": 2000,
                    "y": 457
                },
                {
                    "x": 921,
                    "y": 457
                }
            ],
            "category": "header",
            "html": "<header id='156' style='font-size:20px'>Exploring the Limits of Weakly Supervised Pretraining 23</header>",
            "id": 156,
            "page": 23,
            "text": "Exploring the Limits of Weakly Supervised Pretraining 23"
        },
        {
            "bounding_box": [
                {
                    "x": 545,
                    "y": 513
                },
                {
                    "x": 2006,
                    "y": 513
                },
                {
                    "x": 2006,
                    "y": 916
                },
                {
                    "x": 545,
                    "y": 916
                }
            ],
            "category": "paragraph",
            "html": "<p id='157' style='font-size:20px'>⌀(x) = x in the case of uniform sampling and ⌀(x) = Vx in the case of square<br>root sampling. Given an image I with (possibly multiple) hashtags {hi}, the<br>image-level replication factor for I is computed as r(I) = maxi r(hi). For a set<br>of n unique images, a list of training images is constructed by computing the<br>replication factor for each image, duplicating the image the prescribed number of<br>times, and then randomly permuting the list. 5 The threshold t is selected such<br>that the final list has a target length matching the desired training schedule<br>length (e.g., processing 2 billion images during training).</p>",
            "id": 157,
            "page": 23,
            "text": "⌀(x) = x in the case of uniform sampling and ⌀(x) = Vx in the case of square\nroot sampling. Given an image I with (possibly multiple) hashtags {hi}, the\nimage-level replication factor for I is computed as r(I) = maxi r(hi). For a set\nof n unique images, a list of training images is constructed by computing the\nreplication factor for each image, duplicating the image the prescribed number of\ntimes, and then randomly permuting the list. 5 The threshold t is selected such\nthat the final list has a target length matching the desired training schedule\nlength (e.g., processing 2 billion images during training)."
        },
        {
            "bounding_box": [
                {
                    "x": 546,
                    "y": 985
                },
                {
                    "x": 1828,
                    "y": 985
                },
                {
                    "x": 1828,
                    "y": 1040
                },
                {
                    "x": 546,
                    "y": 1040
                }
            ],
            "category": "caption",
            "html": "<caption id='158' style='font-size:18px'>A.5 Comparison with the State of the Art on ImageNet-1k</caption>",
            "id": 158,
            "page": 23,
            "text": "A.5 Comparison with the State of the Art on ImageNet-1k"
        },
        {
            "bounding_box": [
                {
                    "x": 565,
                    "y": 1152
                },
                {
                    "x": 1979,
                    "y": 1152
                },
                {
                    "x": 1979,
                    "y": 1726
                },
                {
                    "x": 565,
                    "y": 1726
                }
            ],
            "category": "table",
            "html": "<table id='159' style='font-size:14px'><tr><td>Model</td><td>Image size</td><td>Parameters</td><td>Mult-adds</td><td>Top-1 Acc. (%)</td><td>Top-5 Acc. (%)</td></tr><tr><td>Inception V2 [27]</td><td>224</td><td>11.2M</td><td>1.94B</td><td>74.8</td><td>92.2</td></tr><tr><td>NASNet-A (5 @ 1538) [31]</td><td>299</td><td>10.9M</td><td>2.35B</td><td>78.6</td><td>94.2</td></tr><tr><td>Inception V3 [51]</td><td>299</td><td>23.8M</td><td>5.72B</td><td>78.0</td><td>93.9</td></tr><tr><td>Xception [52]</td><td>299</td><td>22.8M</td><td>8.38B</td><td>79.0</td><td>94.5</td></tr><tr><td>Inception ResNet V2 [53]</td><td>299</td><td>55.8M</td><td>13.2B</td><td>80.4</td><td>95.3</td></tr><tr><td>NASNet-A (7 @ 1920) [31]</td><td>299</td><td>22.6M</td><td>4.93B</td><td>80.8</td><td>95.3</td></tr><tr><td>ResNeXt-101 64x4 [15]</td><td>320</td><td>83.6M</td><td>31.5B</td><td>80.9</td><td>95.6</td></tr><tr><td>PolyNet [54]</td><td>331</td><td>92M</td><td>34.7B</td><td>81.3</td><td>95.8</td></tr><tr><td>DPN-131 [55]</td><td>320</td><td>79.5M</td><td>32.0B</td><td>81.5</td><td>95.8</td></tr><tr><td>SENet [56]</td><td>320</td><td>145.8M</td><td>42.3B</td><td>82.7</td><td>96.2</td></tr><tr><td>NASNet-A (6 @ 4032) [31]</td><td>331</td><td>88.9M</td><td>23.8B</td><td>82.7</td><td>96.2</td></tr><tr><td>Our models:</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>IG-3.5B-17k ResNeXt-101 32x16d</td><td>224</td><td>194M</td><td>36B</td><td>84.2</td><td>97.2</td></tr><tr><td>IG-940M-1.5k ResNeXt-101 32x32d</td><td>224</td><td>466M</td><td>87B</td><td>85.1</td><td>97.5</td></tr><tr><td>IG-940M-1.5k ResNeXt-101 32x48d</td><td>224</td><td>829M</td><td>153B</td><td>85.4</td><td>97.6</td></tr></table>",
            "id": 159,
            "page": 23,
            "text": "Model Image size Parameters Mult-adds Top-1 Acc. (%) Top-5 Acc. (%)\n Inception V2 [27] 224 11.2M 1.94B 74.8 92.2\n NASNet-A (5 @ 1538) [31] 299 10.9M 2.35B 78.6 94.2\n Inception V3 [51] 299 23.8M 5.72B 78.0 93.9\n Xception [52] 299 22.8M 8.38B 79.0 94.5\n Inception ResNet V2 [53] 299 55.8M 13.2B 80.4 95.3\n NASNet-A (7 @ 1920) [31] 299 22.6M 4.93B 80.8 95.3\n ResNeXt-101 64x4 [15] 320 83.6M 31.5B 80.9 95.6\n PolyNet [54] 331 92M 34.7B 81.3 95.8\n DPN-131 [55] 320 79.5M 32.0B 81.5 95.8\n SENet [56] 320 145.8M 42.3B 82.7 96.2\n NASNet-A (6 @ 4032) [31] 331 88.9M 23.8B 82.7 96.2\n Our models:     \n IG-3.5B-17k ResNeXt-101 32x16d 224 194M 36B 84.2 97.2\n IG-940M-1.5k ResNeXt-101 32x32d 224 466M 87B 85.1 97.5\n IG-940M-1.5k ResNeXt-101 32x48d 224 829M 153B 85.4"
        },
        {
            "bounding_box": [
                {
                    "x": 546,
                    "y": 1760
                },
                {
                    "x": 2004,
                    "y": 1760
                },
                {
                    "x": 2004,
                    "y": 2084
                },
                {
                    "x": 546,
                    "y": 2084
                }
            ],
            "category": "paragraph",
            "html": "<p id='160' style='font-size:16px'>Table 6: Comparison with the state of the art on the ImageNet-1k validation<br>set. Result table adopted from Zoph et al. [31], to which we append the result of<br>ResNeXt-101 32xCd, with C E {16, 32, 48} pretrained on Instagram hashtag data and<br>finetuned on train-IN-1k. All results are based on a single image crop of the specified size<br>(squared). Our results demonstrate that pretraining on billions of images using their<br>hashtags as labels significantly improve on the state-of-the-art ImageNet-1k results,<br>particularly in the case of top-5 accuracy.</p>",
            "id": 160,
            "page": 23,
            "text": "Table 6: Comparison with the state of the art on the ImageNet-1k validation\nset. Result table adopted from Zoph et al. [31], to which we append the result of\nResNeXt-101 32xCd, with C E {16, 32, 48} pretrained on Instagram hashtag data and\nfinetuned on train-IN-1k. All results are based on a single image crop of the specified size\n(squared). Our results demonstrate that pretraining on billions of images using their\nhashtags as labels significantly improve on the state-of-the-art ImageNet-1k results,\nparticularly in the case of top-5 accuracy."
        },
        {
            "bounding_box": [
                {
                    "x": 552,
                    "y": 2704
                },
                {
                    "x": 2004,
                    "y": 2704
                },
                {
                    "x": 2004,
                    "y": 2804
                },
                {
                    "x": 552,
                    "y": 2804
                }
            ],
            "category": "paragraph",
            "html": "<p id='161' style='font-size:16px'>5 When an image with multiple hashtags is replicated r(I) times, each individual<br>hashtag hi is removed as needed such that hi is only replicated r(hi) times.</p>",
            "id": 161,
            "page": 23,
            "text": "5 When an image with multiple hashtags is replicated r(I) times, each individual\nhashtag hi is removed as needed such that hi is only replicated r(hi) times."
        }
    ]
}