{
    "id": "6298b6a2-0f92-11ef-8230-426932df3dcf",
    "pdf_path": "/root/data/pdf/2103.15808v1.pdf",
    "elements": [
        {
            "bounding_box": [
                {
                    "x": 531,
                    "y": 438
                },
                {
                    "x": 1949,
                    "y": 438
                },
                {
                    "x": 1949,
                    "y": 502
                },
                {
                    "x": 531,
                    "y": 502
                }
            ],
            "category": "paragraph",
            "html": "<p id='0' style='font-size:22px'>CvT: Introducing Convolutions to Vision Transformers</p>",
            "id": 0,
            "page": 1,
            "text": "CvT: Introducing Convolutions to Vision Transformers"
        },
        {
            "bounding_box": [
                {
                    "x": 321,
                    "y": 595
                },
                {
                    "x": 2154,
                    "y": 595
                },
                {
                    "x": 2154,
                    "y": 778
                },
                {
                    "x": 321,
                    "y": 778
                }
            ],
            "category": "paragraph",
            "html": "<p id='1' style='font-size:20px'>Haiping Wu1,2* Bin Xiao2† Noel Codella2 Mengchen Liu2 Xiyang Dai2<br>Lu Yuan2 Lei Zhang2<br>1McGill University 2Microsoft Cloud + AI</p>",
            "id": 1,
            "page": 1,
            "text": "Haiping Wu1,2* Bin Xiao2† Noel Codella2 Mengchen Liu2 Xiyang Dai2\nLu Yuan2 Lei Zhang2\n1McGill University 2Microsoft Cloud + AI"
        },
        {
            "bounding_box": [
                {
                    "x": 238,
                    "y": 787
                },
                {
                    "x": 2332,
                    "y": 787
                },
                {
                    "x": 2332,
                    "y": 834
                },
                {
                    "x": 238,
                    "y": 834
                }
            ],
            "category": "paragraph",
            "html": "<p id='2' style='font-size:18px'>haiping · wu2@mail · mcgill · ca, {bixi, ncodella, mengcliu, xidai, luyuan, leizhang}@microsoft. com</p>",
            "id": 2,
            "page": 1,
            "text": "haiping · wu2@mail · mcgill · ca, {bixi, ncodella, mengcliu, xidai, luyuan, leizhang}@microsoft. com"
        },
        {
            "bounding_box": [
                {
                    "x": 602,
                    "y": 950
                },
                {
                    "x": 798,
                    "y": 950
                },
                {
                    "x": 798,
                    "y": 1001
                },
                {
                    "x": 602,
                    "y": 1001
                }
            ],
            "category": "paragraph",
            "html": "<p id='3' style='font-size:22px'>Abstract</p>",
            "id": 3,
            "page": 1,
            "text": "Abstract"
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 1056
                },
                {
                    "x": 1199,
                    "y": 1056
                },
                {
                    "x": 1199,
                    "y": 2304
                },
                {
                    "x": 200,
                    "y": 2304
                }
            ],
            "category": "paragraph",
            "html": "<p id='4' style='font-size:20px'>We present in this paper a new architecture, named Con-<br>volutional vision Transformer (CvT), that improves Vision<br>Transformer (ViT) in performance and efficiency by intro-<br>ducing convolutions into ViT to yield the best of both de-<br>signs. This is accomplished through two primary modifica-<br>tions: a hierarchy of Transformers containing a new convo-<br>lutional token embedding, and a convolutional Transformer<br>block leveraging a convolutional projection. These changes<br>introduce desirable properties of convolutional neural net-<br>works (CNNs) to the ViT architecture (i.e. shift, scale,<br>and distortion invariance) while maintaining the merits of<br>Transformers (i.e. dynamic attention, global context, and<br>better generalization). We validate CvT by conducting ex-<br>tensive experiments, showing that this approach achieves<br>state-of-the-art performance over other Vision Transform-<br>ers and ResNets on ImageNet-1k, with fewer parame-<br>ters and lower FLOPs. In addition, performance gains<br>are maintained when pretrained on larger datasets (e.g.<br>ImageNet-22k) and fine-tuned to downstream tasks. Pre-<br>trained on ImageNet-22k, our CvT-W24 obtains a top-1 ac-<br>curacy of 87.7% on the ImageNet-1k val set. Finally, our<br>results show that the positional encoding, a crucial com-<br>ponent in existing Vision Transformers, can be safely re-<br>moved in our model, simplifying the design for higher res-<br>olution vision tasks. Code will be released at https :</p>",
            "id": 4,
            "page": 1,
            "text": "We present in this paper a new architecture, named Con-\nvolutional vision Transformer (CvT), that improves Vision\nTransformer (ViT) in performance and efficiency by intro-\nducing convolutions into ViT to yield the best of both de-\nsigns. This is accomplished through two primary modifica-\ntions: a hierarchy of Transformers containing a new convo-\nlutional token embedding, and a convolutional Transformer\nblock leveraging a convolutional projection. These changes\nintroduce desirable properties of convolutional neural net-\nworks (CNNs) to the ViT architecture (i.e. shift, scale,\nand distortion invariance) while maintaining the merits of\nTransformers (i.e. dynamic attention, global context, and\nbetter generalization). We validate CvT by conducting ex-\ntensive experiments, showing that this approach achieves\nstate-of-the-art performance over other Vision Transform-\ners and ResNets on ImageNet-1k, with fewer parame-\nters and lower FLOPs. In addition, performance gains\nare maintained when pretrained on larger datasets (e.g.\nImageNet-22k) and fine-tuned to downstream tasks. Pre-\ntrained on ImageNet-22k, our CvT-W24 obtains a top-1 ac-\ncuracy of 87.7% on the ImageNet-1k val set. Finally, our\nresults show that the positional encoding, a crucial com-\nponent in existing Vision Transformers, can be safely re-\nmoved in our model, simplifying the design for higher res-\nolution vision tasks. Code will be released at https :"
        },
        {
            "bounding_box": [
                {
                    "x": 205,
                    "y": 2306
                },
                {
                    "x": 899,
                    "y": 2306
                },
                {
                    "x": 899,
                    "y": 2351
                },
                {
                    "x": 205,
                    "y": 2351
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='5' style='font-size:18px'>\\ / github · com/ leoxiaobin/ CvT.</p>",
            "id": 5,
            "page": 1,
            "text": "\\ / github · com/ leoxiaobin/ CvT."
        },
        {
            "bounding_box": [
                {
                    "x": 205,
                    "y": 2460
                },
                {
                    "x": 531,
                    "y": 2460
                },
                {
                    "x": 531,
                    "y": 2511
                },
                {
                    "x": 205,
                    "y": 2511
                }
            ],
            "category": "paragraph",
            "html": "<p id='6' style='font-size:20px'>1. Introduction</p>",
            "id": 6,
            "page": 1,
            "text": "1. Introduction"
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 2545
                },
                {
                    "x": 1199,
                    "y": 2545
                },
                {
                    "x": 1199,
                    "y": 2846
                },
                {
                    "x": 203,
                    "y": 2846
                }
            ],
            "category": "paragraph",
            "html": "<p id='7' style='font-size:20px'>Transformers [31, 10] have recently dominated a wide<br>range of tasks in natural language processing (NLP) [32].<br>The Vision Transformer (ViT) [11] is the first computer vi-<br>sion model to rely exclusively on the Transformer archi-<br>tecture to obtain competitive image classification perfor-<br>mance at large scale. The ViT design adapts Transformer</p>",
            "id": 7,
            "page": 1,
            "text": "Transformers [31, 10] have recently dominated a wide\nrange of tasks in natural language processing (NLP) [32].\nThe Vision Transformer (ViT) [11] is the first computer vi-\nsion model to rely exclusively on the Transformer archi-\ntecture to obtain competitive image classification perfor-\nmance at large scale. The ViT design adapts Transformer"
        },
        {
            "bounding_box": [
                {
                    "x": 251,
                    "y": 2893
                },
                {
                    "x": 1123,
                    "y": 2893
                },
                {
                    "x": 1123,
                    "y": 2970
                },
                {
                    "x": 251,
                    "y": 2970
                }
            ],
            "category": "paragraph",
            "html": "<p id='8' style='font-size:16px'>*This work is done when Haiping Wu was an intern at Microsoft.<br>1 Corresponding author</p>",
            "id": 8,
            "page": 1,
            "text": "*This work is done when Haiping Wu was an intern at Microsoft.\n1 Corresponding author"
        },
        {
            "bounding_box": [
                {
                    "x": 1283,
                    "y": 944
                },
                {
                    "x": 2277,
                    "y": 944
                },
                {
                    "x": 2277,
                    "y": 1612
                },
                {
                    "x": 1283,
                    "y": 1612
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='9' style='font-size:14px' alt=\"CvT T2T TNT\nCvT ViT BiT\nDeiT PVT\n88 277M\n82.5\n(%) (%)\n928M\naccuracy\n82.0\n307M\n32M\n86M 81.5\n84 accuracy\ntop-1\n20M top-1 81.0\nImageNet\nImageNet\n82\n80.5\n25M\n80\n80.0\n78\nCvT ViT BiT 20 40 60 80\nModel Paramters (M)\n(a) (b)\" data-coord=\"top-left:(1283,944); bottom-right:(2277,1612)\" /></figure>",
            "id": 9,
            "page": 1,
            "text": "CvT T2T TNT\nCvT ViT BiT\nDeiT PVT\n88 277M\n82.5\n(%) (%)\n928M\naccuracy\n82.0\n307M\n32M\n86M 81.5\n84 accuracy\ntop-1\n20M top-1 81.0\nImageNet\nImageNet\n82\n80.5\n25M\n80\n80.0\n78\nCvT ViT BiT 20 40 60 80\nModel Paramters (M)\n(a) (b)"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 1645
                },
                {
                    "x": 2277,
                    "y": 1645
                },
                {
                    "x": 2277,
                    "y": 2047
                },
                {
                    "x": 1279,
                    "y": 2047
                }
            ],
            "category": "caption",
            "html": "<caption id='10' style='font-size:20px'>Figure 1: Top-1 Accuracy on ImageNet validation com-<br>pared to other methods with respect to model parame-<br>ters. (a) Comparison to CNN-based model BiT [18] and<br>Transformer-based model ViT [11], when pretrained on<br>ImageNet-22k. Larger marker size indicates larger archi-<br>tectures. (b) Comparison to concurrent works: DeiT [30],<br>T2T [41], PVT [34], TNT [14] when pretrained on<br>ImageNet-1k.</caption>",
            "id": 10,
            "page": 1,
            "text": "Figure 1: Top-1 Accuracy on ImageNet validation com-\npared to other methods with respect to model parame-\nters. (a) Comparison to CNN-based model BiT [18] and\nTransformer-based model ViT [11], when pretrained on\nImageNet-22k. Larger marker size indicates larger archi-\ntectures. (b) Comparison to concurrent works: DeiT [30],\nT2T [41], PVT [34], TNT [14] when pretrained on\nImageNet-1k."
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 2166
                },
                {
                    "x": 2275,
                    "y": 2166
                },
                {
                    "x": 2275,
                    "y": 2512
                },
                {
                    "x": 1279,
                    "y": 2512
                }
            ],
            "category": "paragraph",
            "html": "<p id='11' style='font-size:20px'>architectures [10] from language understanding with mini-<br>mal modifications. First, images are split into discrete non-<br>overlapping patches (e.g. 16 x 16). Then, these patches are<br>treated as tokens (analogous to tokens in NLP), summed<br>with a special positional encoding to represent coarse spa-<br>tial information, and input into repeated standard Trans-<br>former layers to model global relations for classification.</p>",
            "id": 11,
            "page": 1,
            "text": "architectures [10] from language understanding with mini-\nmal modifications. First, images are split into discrete non-\noverlapping patches (e.g. 16 x 16). Then, these patches are\ntreated as tokens (analogous to tokens in NLP), summed\nwith a special positional encoding to represent coarse spa-\ntial information, and input into repeated standard Trans-\nformer layers to model global relations for classification."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2529
                },
                {
                    "x": 2277,
                    "y": 2529
                },
                {
                    "x": 2277,
                    "y": 2978
                },
                {
                    "x": 1280,
                    "y": 2978
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='12' style='font-size:20px'>Despite the success of vision Transformers at large scale,<br>the performance is still below similarly sized convolutional<br>neural network (CNN) counterparts (e.g., ResNets [15])<br>when trained on smaller amounts of data. One possible rea-<br>son may be that ViT lacks certain desirable properties in-<br>herently built into the CNN architecture that make CNNs<br>uniquely suited to solve vision tasks. For example, im-<br>ages have a strong 2D local structure: spatially neighbor-<br>ing pixels are usually highly correlated. The CNN archi-</p>",
            "id": 12,
            "page": 1,
            "text": "Despite the success of vision Transformers at large scale,\nthe performance is still below similarly sized convolutional\nneural network (CNN) counterparts (e.g., ResNets [15])\nwhen trained on smaller amounts of data. One possible rea-\nson may be that ViT lacks certain desirable properties in-\nherently built into the CNN architecture that make CNNs\nuniquely suited to solve vision tasks. For example, im-\nages have a strong 2D local structure: spatially neighbor-\ning pixels are usually highly correlated. The CNN archi-"
        },
        {
            "bounding_box": [
                {
                    "x": 64,
                    "y": 868
                },
                {
                    "x": 150,
                    "y": 868
                },
                {
                    "x": 150,
                    "y": 2340
                },
                {
                    "x": 64,
                    "y": 2340
                }
            ],
            "category": "footer",
            "html": "<br><footer id='13' style='font-size:14px'>2021<br>Mar<br>29<br>[cs.CV]<br>arXiv:2103.15808v1</footer>",
            "id": 13,
            "page": 1,
            "text": "2021\nMar\n29\n[cs.CV]\narXiv:2103.15808v1"
        },
        {
            "bounding_box": [
                {
                    "x": 1226,
                    "y": 3055
                },
                {
                    "x": 1250,
                    "y": 3055
                },
                {
                    "x": 1250,
                    "y": 3090
                },
                {
                    "x": 1226,
                    "y": 3090
                }
            ],
            "category": "footer",
            "html": "<footer id='14' style='font-size:16px'>1</footer>",
            "id": 14,
            "page": 1,
            "text": "1"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 299
                },
                {
                    "x": 2295,
                    "y": 299
                },
                {
                    "x": 2295,
                    "y": 668
                },
                {
                    "x": 201,
                    "y": 668
                }
            ],
            "category": "table",
            "html": "<table id='15' style='font-size:14px'><tr><td>Method</td><td>Needs Position Encoding (PE)</td><td>Token Embedding</td><td>Projection for Attention</td><td>Hierarchical Transformers</td></tr><tr><td>ViT [11], DeiT [30]</td><td>yes</td><td>non-overlapping</td><td>linear</td><td>no</td></tr><tr><td>CPVT [6]</td><td>no (w/ PE Generator)</td><td>non-overlapping</td><td>linear</td><td>no</td></tr><tr><td>TNT[14]</td><td>yes</td><td>non-overlapping (patch+pixel)</td><td>linear</td><td>no</td></tr><tr><td>T2T [41]</td><td>yes</td><td>overlapping (concatenate)</td><td>linear</td><td>partial (tokenization)</td></tr><tr><td>PVT [34]</td><td>yes</td><td>non-overlapping</td><td>spatial reduction</td><td>yes</td></tr><tr><td>CvT (ours)</td><td>no</td><td>overlapping (convolution)</td><td>convolution</td><td>yes</td></tr></table>",
            "id": 15,
            "page": 2,
            "text": "Method Needs Position Encoding (PE) Token Embedding Projection for Attention Hierarchical Transformers\n ViT [11], DeiT [30] yes non-overlapping linear no\n CPVT [6] no (w/ PE Generator) non-overlapping linear no\n TNT[14] yes non-overlapping (patch+pixel) linear no\n T2T [41] yes overlapping (concatenate) linear partial (tokenization)\n PVT [34] yes non-overlapping spatial reduction yes\n CvT (ours) no overlapping (convolution) convolution"
        },
        {
            "bounding_box": [
                {
                    "x": 782,
                    "y": 708
                },
                {
                    "x": 1694,
                    "y": 708
                },
                {
                    "x": 1694,
                    "y": 753
                },
                {
                    "x": 782,
                    "y": 753
                }
            ],
            "category": "caption",
            "html": "<caption id='16' style='font-size:18px'>Table 1: Representative works of vision Transformers.</caption>",
            "id": 16,
            "page": 2,
            "text": "Table 1: Representative works of vision Transformers."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 848
                },
                {
                    "x": 1198,
                    "y": 848
                },
                {
                    "x": 1198,
                    "y": 1243
                },
                {
                    "x": 201,
                    "y": 1243
                }
            ],
            "category": "paragraph",
            "html": "<p id='17' style='font-size:18px'>tecture forces the capture of this local structure by using<br>local receptive fields, shared weights, and spatial subsam-<br>pling [20], and thus also achieves some degree of shift,<br>scale, and distortion invariance. In addition, the hierarchi-<br>cal structure of convolutional kernels learns visual patterns<br>that take into account local spatial context at varying levels<br>of complexity, from simple low-level edges and textures to<br>higher order semantic patterns.</p>",
            "id": 17,
            "page": 2,
            "text": "tecture forces the capture of this local structure by using\nlocal receptive fields, shared weights, and spatial subsam-\npling [20], and thus also achieves some degree of shift,\nscale, and distortion invariance. In addition, the hierarchi-\ncal structure of convolutional kernels learns visual patterns\nthat take into account local spatial context at varying levels\nof complexity, from simple low-level edges and textures to\nhigher order semantic patterns."
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 1257
                },
                {
                    "x": 1199,
                    "y": 1257
                },
                {
                    "x": 1199,
                    "y": 1704
                },
                {
                    "x": 203,
                    "y": 1704
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='18' style='font-size:18px'>In this paper, we hypothesize that convolutions can be<br>strategically introduced to the ViT structure to improve<br>performance and robustness, while concurrently maintain-<br>ing a high degree of computational and memory efficiency.<br>To verify our hypothesises, we present a new architecture,<br>called the Convolutional vision Transformer (CvT), which<br>incorporates convolutions into the Transformer that is in-<br>herently efficient, both in terms of floating point operations<br>(FLOPs) and parameters.</p>",
            "id": 18,
            "page": 2,
            "text": "In this paper, we hypothesize that convolutions can be\nstrategically introduced to the ViT structure to improve\nperformance and robustness, while concurrently maintain-\ning a high degree of computational and memory efficiency.\nTo verify our hypothesises, we present a new architecture,\ncalled the Convolutional vision Transformer (CvT), which\nincorporates convolutions into the Transformer that is in-\nherently efficient, both in terms of floating point operations\n(FLOPs) and parameters."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 1718
                },
                {
                    "x": 1199,
                    "y": 1718
                },
                {
                    "x": 1199,
                    "y": 2913
                },
                {
                    "x": 200,
                    "y": 2913
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='19' style='font-size:18px'>The CvT design introduces convolutions to two core sec-<br>tions of the ViT architecture. First, we partition the Trans-<br>formers into multiple stages that form a hierarchical struc-<br>ture of Transformers. The beginning of each stage consists<br>of a convolutional token embedding that performs an over-<br>lapping convolution operation with stride on a 2D-reshaped<br>token map (i.e., reshaping flattened token sequences back<br>to the spatial grid), followed by layer normalization. This<br>allows the model to not only capture local information, but<br>also progressively decrease the sequence length while si-<br>multaneously increasing the dimension of token features<br>across stages, achieving spatial downsampling while con-<br>currently increasing the number of feature maps, as is per-<br>formed in CNNs [20]. Second, the linear projection prior<br>to every self-attention block in the Transformer module is<br>replaced with our proposed convolutional projection, which<br>employs a S x s depth-wise separable convolution [5] oper-<br>ation on an 2D-reshaped token map. This allows the model<br>to further capture local spatial context and reduce seman-<br>tic ambiguity in the attention mechanism. It also permits<br>management of computational complexity, as the stride of<br>convolution can be used to subsample the key and value ma-<br>trices to improve efficiency by 4x or more, with minimal<br>degradation of performance.</p>",
            "id": 19,
            "page": 2,
            "text": "The CvT design introduces convolutions to two core sec-\ntions of the ViT architecture. First, we partition the Trans-\nformers into multiple stages that form a hierarchical struc-\nture of Transformers. The beginning of each stage consists\nof a convolutional token embedding that performs an over-\nlapping convolution operation with stride on a 2D-reshaped\ntoken map (i.e., reshaping flattened token sequences back\nto the spatial grid), followed by layer normalization. This\nallows the model to not only capture local information, but\nalso progressively decrease the sequence length while si-\nmultaneously increasing the dimension of token features\nacross stages, achieving spatial downsampling while con-\ncurrently increasing the number of feature maps, as is per-\nformed in CNNs [20]. Second, the linear projection prior\nto every self-attention block in the Transformer module is\nreplaced with our proposed convolutional projection, which\nemploys a S x s depth-wise separable convolution [5] oper-\nation on an 2D-reshaped token map. This allows the model\nto further capture local spatial context and reduce seman-\ntic ambiguity in the attention mechanism. It also permits\nmanagement of computational complexity, as the stride of\nconvolution can be used to subsample the key and value ma-\ntrices to improve efficiency by 4x or more, with minimal\ndegradation of performance."
        },
        {
            "bounding_box": [
                {
                    "x": 250,
                    "y": 2929
                },
                {
                    "x": 1195,
                    "y": 2929
                },
                {
                    "x": 1195,
                    "y": 2973
                },
                {
                    "x": 250,
                    "y": 2973
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='20' style='font-size:14px'>In summary, our proposed Convolutional vision Trans-</p>",
            "id": 20,
            "page": 2,
            "text": "In summary, our proposed Convolutional vision Trans-"
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 846
                },
                {
                    "x": 2278,
                    "y": 846
                },
                {
                    "x": 2278,
                    "y": 1740
                },
                {
                    "x": 1278,
                    "y": 1740
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='21' style='font-size:20px'>former (CvT) employs all the benefits of CNNs: local re-<br>ceptive fields, shared weights, and spatial subsampling,<br>while keeping all the advantages of Transformers: dynamic<br>attention, global context fusion, and better generalization.<br>Our results demonstrate that this approach attains state-of-<br>art performance when CvT is pre-trained with ImageNet-<br>1k, while being lightweight and efficient: CvT improves the<br>performance compared to CNN-based models (e.g. ResNet)<br>and prior Transformer-based models (e.g. ViT, DeiT) while<br>utilizing fewer FLOPS and parameters. In addition, CvT<br>achieves state-of-the-art performance when evaluated at<br>larger scale pretraining (e.g. on the public ImageNet-22k<br>dataset). Finally, we demonstrate thatin this new design, we<br>can drop the positional embedding for tokens without any<br>degradation to model performance. This not only simplifies<br>the architecture design, but also makes it readily capable of<br>accommodating variable resolutions of input images that is<br>critical to many vision tasks.</p>",
            "id": 21,
            "page": 2,
            "text": "former (CvT) employs all the benefits of CNNs: local re-\nceptive fields, shared weights, and spatial subsampling,\nwhile keeping all the advantages of Transformers: dynamic\nattention, global context fusion, and better generalization.\nOur results demonstrate that this approach attains state-of-\nart performance when CvT is pre-trained with ImageNet-\n1k, while being lightweight and efficient: CvT improves the\nperformance compared to CNN-based models (e.g. ResNet)\nand prior Transformer-based models (e.g. ViT, DeiT) while\nutilizing fewer FLOPS and parameters. In addition, CvT\nachieves state-of-the-art performance when evaluated at\nlarger scale pretraining (e.g. on the public ImageNet-22k\ndataset). Finally, we demonstrate thatin this new design, we\ncan drop the positional embedding for tokens without any\ndegradation to model performance. This not only simplifies\nthe architecture design, but also makes it readily capable of\naccommodating variable resolutions of input images that is\ncritical to many vision tasks."
        },
        {
            "bounding_box": [
                {
                    "x": 1282,
                    "y": 1794
                },
                {
                    "x": 1636,
                    "y": 1794
                },
                {
                    "x": 1636,
                    "y": 1843
                },
                {
                    "x": 1282,
                    "y": 1843
                }
            ],
            "category": "paragraph",
            "html": "<p id='22' style='font-size:22px'>2. Related Work</p>",
            "id": 22,
            "page": 2,
            "text": "2. Related Work"
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 1878
                },
                {
                    "x": 2278,
                    "y": 1878
                },
                {
                    "x": 2278,
                    "y": 2327
                },
                {
                    "x": 1281,
                    "y": 2327
                }
            ],
            "category": "paragraph",
            "html": "<p id='23' style='font-size:20px'>Transformers that exclusively rely on the self-attention<br>mechanism to capture global dependencies have dominated<br>in natural language modelling [31, 10, 25]. Recently, the<br>Transformer based architecture has been viewed as a viable<br>alternative to the convolutional neural networks (CNNs) in<br>visual recognition tasks, such as classification [11, 30], ob-<br>ject detection [3, 45, 43, 8, 28], segmentation [33, 36], im-<br>age enhancement [4, 40], image generation [24], video pro-<br>cessing [42, 44] and 3D point cloud processing [12].</p>",
            "id": 23,
            "page": 2,
            "text": "Transformers that exclusively rely on the self-attention\nmechanism to capture global dependencies have dominated\nin natural language modelling [31, 10, 25]. Recently, the\nTransformer based architecture has been viewed as a viable\nalternative to the convolutional neural networks (CNNs) in\nvisual recognition tasks, such as classification [11, 30], ob-\nject detection [3, 45, 43, 8, 28], segmentation [33, 36], im-\nage enhancement [4, 40], image generation [24], video pro-\ncessing [42, 44] and 3D point cloud processing [12]."
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 2378
                },
                {
                    "x": 2276,
                    "y": 2378
                },
                {
                    "x": 2276,
                    "y": 2976
                },
                {
                    "x": 1278,
                    "y": 2976
                }
            ],
            "category": "paragraph",
            "html": "<p id='24' style='font-size:18px'>Vision Transformers. The Vision Transformer (ViT) is<br>the first to prove that a pure Transformer architecture can<br>attain state-of-the-art performance (e.g. ResNets [15], Ef-<br>ficientNet [29]) on image classification when the data is<br>large enough (i.e. on ImageNet-22k, JFT-300M). Specifi-<br>cally, ViT decomposes each image into a sequence of tokens<br>(i.e. non-overlapping patches) with fixed length, and then<br>applies multiple standard Transformer layers, consisting of<br>Multi-Head Self-Attention module (MHSA) and Position-<br>wise Feed-forward module (FFN), to model these tokens.<br>DeiT [30] further explores the data-efficient training and<br>distillation for ViT. In this work, we study how to combine</p>",
            "id": 24,
            "page": 2,
            "text": "Vision Transformers. The Vision Transformer (ViT) is\nthe first to prove that a pure Transformer architecture can\nattain state-of-the-art performance (e.g. ResNets [15], Ef-\nficientNet [29]) on image classification when the data is\nlarge enough (i.e. on ImageNet-22k, JFT-300M). Specifi-\ncally, ViT decomposes each image into a sequence of tokens\n(i.e. non-overlapping patches) with fixed length, and then\napplies multiple standard Transformer layers, consisting of\nMulti-Head Self-Attention module (MHSA) and Position-\nwise Feed-forward module (FFN), to model these tokens.\nDeiT [30] further explores the data-efficient training and\ndistillation for ViT. In this work, we study how to combine"
        },
        {
            "bounding_box": [
                {
                    "x": 1225,
                    "y": 3055
                },
                {
                    "x": 1251,
                    "y": 3055
                },
                {
                    "x": 1251,
                    "y": 3091
                },
                {
                    "x": 1225,
                    "y": 3091
                }
            ],
            "category": "footer",
            "html": "<footer id='25' style='font-size:16px'>2</footer>",
            "id": 25,
            "page": 2,
            "text": "2"
        },
        {
            "bounding_box": [
                {
                    "x": 197,
                    "y": 283
                },
                {
                    "x": 2275,
                    "y": 283
                },
                {
                    "x": 2275,
                    "y": 1043
                },
                {
                    "x": 197,
                    "y": 1043
                }
            ],
            "category": "figure",
            "html": "<figure><img id='26' style='font-size:22px' alt=\"class\nMLP\nHead\ncls token MLP\nNorm\nConvolutional\nConvolutional Transformer\nConvolutional\nTransformer\nTransformer\nConvolutional Embedding\nConvolutional Token :\nConvolutional Convolutional\n: Embedding\nEmbedding\nMulti-Head Attention\n↑ ↑\nQ K V\nBlock\nBlock\nBlock\nToken\nToken\n:\n: Token map x2\nProjection\nToken map x1\nxN2 X N3\nx N1\nInput image xo\nStage 3\nStage 2\nStage 1 (a) (b)\" data-coord=\"top-left:(197,283); bottom-right:(2275,1043)\" /></figure>",
            "id": 26,
            "page": 3,
            "text": "class\nMLP\nHead\ncls token MLP\nNorm\nConvolutional\nConvolutional Transformer\nConvolutional\nTransformer\nTransformer\nConvolutional Embedding\nConvolutional Token :\nConvolutional Convolutional\n: Embedding\nEmbedding\nMulti-Head Attention\n↑ ↑\nQ K V\nBlock\nBlock\nBlock\nToken\nToken\n:\n: Token map x2\nProjection\nToken map x1\nxN2 X N3\nx N1\nInput image xo\nStage 3\nStage 2\nStage 1 (a) (b)"
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 1066
                },
                {
                    "x": 2275,
                    "y": 1066
                },
                {
                    "x": 2275,
                    "y": 1218
                },
                {
                    "x": 200,
                    "y": 1218
                }
            ],
            "category": "caption",
            "html": "<br><caption id='27' style='font-size:20px'>Figure 2: The pipeline of the proposed CvT architecture. (a) Overall architecture, showing the hierarchical multi-stage<br>structure facilitated by the Convolutional Token Embedding layer. (b) Details of the Convolutional Transformer Block,<br>which contains the convolution projection as the first layer.</caption>",
            "id": 27,
            "page": 3,
            "text": "Figure 2: The pipeline of the proposed CvT architecture. (a) Overall architecture, showing the hierarchical multi-stage\nstructure facilitated by the Convolutional Token Embedding layer. (b) Details of the Convolutional Transformer Block,\nwhich contains the convolution projection as the first layer."
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 1309
                },
                {
                    "x": 1196,
                    "y": 1309
                },
                {
                    "x": 1196,
                    "y": 1403
                },
                {
                    "x": 203,
                    "y": 1403
                }
            ],
            "category": "paragraph",
            "html": "<p id='28' style='font-size:14px'>CNNs and Transformers to model both local and global de-<br>pendencies for image classification in an efficient way.</p>",
            "id": 28,
            "page": 3,
            "text": "CNNs and Transformers to model both local and global de-\npendencies for image classification in an efficient way."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 1411
                },
                {
                    "x": 1199,
                    "y": 1411
                },
                {
                    "x": 1199,
                    "y": 2407
                },
                {
                    "x": 200,
                    "y": 2407
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='29' style='font-size:18px'>In order to better model local context in vision Trans-<br>formers, some concurrent works have introduced design<br>changes. For example, the Conditional Position encod-<br>ings Visual Transformer (CPVT) [6] replaces the prede-<br>fined positional embedding used in ViT with conditional<br>position encodings (CPE), enabling Transformers to pro-<br>cess input images of arbitrary size without interpolation.<br>Transformer-iN- Transformer (TNT) [14] utilizes both an<br>outer Transformer block that processes the patch embed-<br>dings, and an inner Transformer block that models the re-<br>lation among pixel embeddings, to model both patch-level<br>and pixel-level representation. Tokens-to-Token (T2T) [41]<br>mainly improves tokenization in ViT by concatenating mul-<br>tiple tokens within a sliding window into one token. How-<br>ever, this operation fundamentally differs from convolutions<br>especially in normalization details, and the concatenation<br>of multiple tokens greatly increases complexity in compu-<br>tation and memory. PVT [34] incorporates a multi-stage<br>design (without convolutions) for Transformer similar to<br>multi-scales in CNNs, favoring dense prediction tasks.</p>",
            "id": 29,
            "page": 3,
            "text": "In order to better model local context in vision Trans-\nformers, some concurrent works have introduced design\nchanges. For example, the Conditional Position encod-\nings Visual Transformer (CPVT) [6] replaces the prede-\nfined positional embedding used in ViT with conditional\nposition encodings (CPE), enabling Transformers to pro-\ncess input images of arbitrary size without interpolation.\nTransformer-iN- Transformer (TNT) [14] utilizes both an\nouter Transformer block that processes the patch embed-\ndings, and an inner Transformer block that models the re-\nlation among pixel embeddings, to model both patch-level\nand pixel-level representation. Tokens-to-Token (T2T) [41]\nmainly improves tokenization in ViT by concatenating mul-\ntiple tokens within a sliding window into one token. How-\never, this operation fundamentally differs from convolutions\nespecially in normalization details, and the concatenation\nof multiple tokens greatly increases complexity in compu-\ntation and memory. PVT [34] incorporates a multi-stage\ndesign (without convolutions) for Transformer similar to\nmulti-scales in CNNs, favoring dense prediction tasks."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2412
                },
                {
                    "x": 1199,
                    "y": 2412
                },
                {
                    "x": 1199,
                    "y": 2810
                },
                {
                    "x": 201,
                    "y": 2810
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='30' style='font-size:16px'>In contrast to these concurrent works, this work aims<br>to achieve the best of both worlds by introducing convolu-<br>tions, with image domain specific inductive biases, into the<br>Transformer architecture. Table 1 shows the key differences<br>in terms of necessity of positional encodings, type of token<br>embedding, type of projection, and Transformer structure in<br>the backbone, between the above representative concurrent<br>works and ours.</p>",
            "id": 30,
            "page": 3,
            "text": "In contrast to these concurrent works, this work aims\nto achieve the best of both worlds by introducing convolu-\ntions, with image domain specific inductive biases, into the\nTransformer architecture. Table 1 shows the key differences\nin terms of necessity of positional encodings, type of token\nembedding, type of projection, and Transformer structure in\nthe backbone, between the above representative concurrent\nworks and ours."
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 2878
                },
                {
                    "x": 1197,
                    "y": 2878
                },
                {
                    "x": 1197,
                    "y": 2975
                },
                {
                    "x": 204,
                    "y": 2975
                }
            ],
            "category": "paragraph",
            "html": "<p id='31' style='font-size:18px'>Introducing Self-attentions to CNNs. Self-attention<br>mechanisms have been widely applied to CNNs in vision</p>",
            "id": 31,
            "page": 3,
            "text": "Introducing Self-attentions to CNNs. Self-attention\nmechanisms have been widely applied to CNNs in vision"
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 1307
                },
                {
                    "x": 2277,
                    "y": 1307
                },
                {
                    "x": 2277,
                    "y": 2102
                },
                {
                    "x": 1278,
                    "y": 2102
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='32' style='font-size:18px'>tasks. Among these works, the non-local networks [35] are<br>designed for capturing long range dependencies via global<br>attention. The local relation networks [17] adapts its weight<br>aggregation based on the compositional relations (similar-<br>ity) between pixels/features within a local window, in con-<br>trast to convolution layers which employ fixed aggrega-<br>tion weights over spatially neighboring input feature. Such<br>an adaptive weight aggregation introduces geometric pri-<br>ors into the network which are important for the recogni-<br>tion tasks. Recently, BoTNet [27] proposes a simple yet<br>powerful backbone architecture that just replaces the spa-<br>tial convolutions with global self-attention in the final three<br>bottleneck blocks of a ResNet and achieves a strong per-<br>formance in image recognition. Instead, our work performs<br>an opposite research direction: introducing convolutions to<br>Transformers.</p>",
            "id": 32,
            "page": 3,
            "text": "tasks. Among these works, the non-local networks [35] are\ndesigned for capturing long range dependencies via global\nattention. The local relation networks [17] adapts its weight\naggregation based on the compositional relations (similar-\nity) between pixels/features within a local window, in con-\ntrast to convolution layers which employ fixed aggrega-\ntion weights over spatially neighboring input feature. Such\nan adaptive weight aggregation introduces geometric pri-\nors into the network which are important for the recogni-\ntion tasks. Recently, BoTNet [27] proposes a simple yet\npowerful backbone architecture that just replaces the spa-\ntial convolutions with global self-attention in the final three\nbottleneck blocks of a ResNet and achieves a strong per-\nformance in image recognition. Instead, our work performs\nan opposite research direction: introducing convolutions to\nTransformers."
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 2177
                },
                {
                    "x": 2278,
                    "y": 2177
                },
                {
                    "x": 2278,
                    "y": 2979
                },
                {
                    "x": 1278,
                    "y": 2979
                }
            ],
            "category": "paragraph",
            "html": "<p id='33' style='font-size:18px'>Introducing Convolutions to Transformers. In NLP<br>and speech recognition, convolutions have been used to<br>modify the Transformer block, either by replacing multi-<br>head attentions with convolution layers [38], or adding<br>additional convolution layers in parallel [39] or sequen-<br>tially [13], to capture local relationships. Other prior work<br>[37] proposes to propagate attention maps to succeeding<br>layers via a residual connection, which is first transformed<br>by convolutions. Different from these works, we propose<br>to introduce convolutions to two primary parts of the vi-<br>sion Transformer: first, to replace the existing Position-wise<br>Linear Projection for the attention operation with our Con-<br>volutional Projection, and second, to use our hierarchical<br>multi-stage structure to enable varied resolution of 2D re-<br>shaped token maps, similar to CNNs. Our unique design<br>affords significant performance and efficiency benefits over</p>",
            "id": 33,
            "page": 3,
            "text": "Introducing Convolutions to Transformers. In NLP\nand speech recognition, convolutions have been used to\nmodify the Transformer block, either by replacing multi-\nhead attentions with convolution layers [38], or adding\nadditional convolution layers in parallel [39] or sequen-\ntially [13], to capture local relationships. Other prior work\n[37] proposes to propagate attention maps to succeeding\nlayers via a residual connection, which is first transformed\nby convolutions. Different from these works, we propose\nto introduce convolutions to two primary parts of the vi-\nsion Transformer: first, to replace the existing Position-wise\nLinear Projection for the attention operation with our Con-\nvolutional Projection, and second, to use our hierarchical\nmulti-stage structure to enable varied resolution of 2D re-\nshaped token maps, similar to CNNs. Our unique design\naffords significant performance and efficiency benefits over"
        },
        {
            "bounding_box": [
                {
                    "x": 1226,
                    "y": 3056
                },
                {
                    "x": 1250,
                    "y": 3056
                },
                {
                    "x": 1250,
                    "y": 3091
                },
                {
                    "x": 1226,
                    "y": 3091
                }
            ],
            "category": "footer",
            "html": "<footer id='34' style='font-size:14px'>3</footer>",
            "id": 34,
            "page": 3,
            "text": "3"
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 311
                },
                {
                    "x": 413,
                    "y": 311
                },
                {
                    "x": 413,
                    "y": 351
                },
                {
                    "x": 203,
                    "y": 351
                }
            ],
            "category": "paragraph",
            "html": "<p id='35' style='font-size:20px'>prior works.</p>",
            "id": 35,
            "page": 4,
            "text": "prior works."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 412
                },
                {
                    "x": 983,
                    "y": 412
                },
                {
                    "x": 983,
                    "y": 465
                },
                {
                    "x": 201,
                    "y": 465
                }
            ],
            "category": "paragraph",
            "html": "<p id='36' style='font-size:22px'>3. Convolutional vision Transformer</p>",
            "id": 36,
            "page": 4,
            "text": "3. Convolutional vision Transformer"
        },
        {
            "bounding_box": [
                {
                    "x": 199,
                    "y": 498
                },
                {
                    "x": 1199,
                    "y": 498
                },
                {
                    "x": 1199,
                    "y": 2094
                },
                {
                    "x": 199,
                    "y": 2094
                }
            ],
            "category": "paragraph",
            "html": "<p id='37' style='font-size:18px'>The overall pipeline of the Convolutional vision Trans-<br>former (CvT) is shown in Figure 2. We introduce two<br>convolution-based operations into the Vision Transformer<br>architecture, namely the Convolutional Token Embedding<br>and Convolutional Projection. As shown in Figure 2 (a), a<br>multi-stage hierarchy design borrowed from CNNs [20, 15]<br>is employed, where three stages in total are used in this<br>work. Each stage has two parts. First, the input image<br>(or 2D reshaped token maps) are subjected to the Convo-<br>lutional Token Embedding layer, which is implemented as a<br>convolution with overlapping patches with tokens reshaped<br>to the 2D spatial grid as the input (the degree of overlap<br>can be controlled via the stride length). An additional layer<br>normalization is applied to the tokens. This allows each<br>stage to progressively reduce the number of tokens (i.e. fea-<br>ture resolution) while simultaneously increasing the width<br>of the tokens (i.e. feature dimension), thus achieving spa-<br>tial downsampling and increased richness of representation,<br>similar to the design of CNNs. Different from other prior<br>Transformer-based architectures [11, 30, 41, 34], we do not<br>sum the ad-hod position embedding to the tokens. Next,<br>a stack of the proposed Convolutional Transformer Blocks<br>comprise the remainder of each stage. Figure 2 (b) shows<br>the architecture of the Convolutional Transformer Block,<br>where a depth-wise separable convolution operation [5],<br>referred as Convolutional Projection, is applied for query,<br>key, and value embeddings respectively, instead of the stan-<br>dard position-wise linear projection in ViT [11]. Addition-<br>ally, the classification token is added only in the last stage.<br>Finally, an MLP (i.e. fully connected) Head is utilized upon<br>the classification token of the final stage output to predict<br>the class.</p>",
            "id": 37,
            "page": 4,
            "text": "The overall pipeline of the Convolutional vision Trans-\nformer (CvT) is shown in Figure 2. We introduce two\nconvolution-based operations into the Vision Transformer\narchitecture, namely the Convolutional Token Embedding\nand Convolutional Projection. As shown in Figure 2 (a), a\nmulti-stage hierarchy design borrowed from CNNs [20, 15]\nis employed, where three stages in total are used in this\nwork. Each stage has two parts. First, the input image\n(or 2D reshaped token maps) are subjected to the Convo-\nlutional Token Embedding layer, which is implemented as a\nconvolution with overlapping patches with tokens reshaped\nto the 2D spatial grid as the input (the degree of overlap\ncan be controlled via the stride length). An additional layer\nnormalization is applied to the tokens. This allows each\nstage to progressively reduce the number of tokens (i.e. fea-\nture resolution) while simultaneously increasing the width\nof the tokens (i.e. feature dimension), thus achieving spa-\ntial downsampling and increased richness of representation,\nsimilar to the design of CNNs. Different from other prior\nTransformer-based architectures [11, 30, 41, 34], we do not\nsum the ad-hod position embedding to the tokens. Next,\na stack of the proposed Convolutional Transformer Blocks\ncomprise the remainder of each stage. Figure 2 (b) shows\nthe architecture of the Convolutional Transformer Block,\nwhere a depth-wise separable convolution operation [5],\nreferred as Convolutional Projection, is applied for query,\nkey, and value embeddings respectively, instead of the stan-\ndard position-wise linear projection in ViT [11]. Addition-\nally, the classification token is added only in the last stage.\nFinally, an MLP (i.e. fully connected) Head is utilized upon\nthe classification token of the final stage output to predict\nthe class."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2101
                },
                {
                    "x": 1198,
                    "y": 2101
                },
                {
                    "x": 1198,
                    "y": 2300
                },
                {
                    "x": 202,
                    "y": 2300
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='38' style='font-size:18px'>We first elaborate on the proposed Convolutional Token<br>Embedding layer. Next we show how to perform Convolu-<br>tional Projection for the Multi-Head Self-Attention module,<br>and its efficient design for managing computational cost.</p>",
            "id": 38,
            "page": 4,
            "text": "We first elaborate on the proposed Convolutional Token\nEmbedding layer. Next we show how to perform Convolu-\ntional Projection for the Multi-Head Self-Attention module,\nand its efficient design for managing computational cost."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2342
                },
                {
                    "x": 936,
                    "y": 2342
                },
                {
                    "x": 936,
                    "y": 2393
                },
                {
                    "x": 202,
                    "y": 2393
                }
            ],
            "category": "paragraph",
            "html": "<p id='39' style='font-size:20px'>3.1. Convolutional Token Embedding</p>",
            "id": 39,
            "page": 4,
            "text": "3.1. Convolutional Token Embedding"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2425
                },
                {
                    "x": 1198,
                    "y": 2425
                },
                {
                    "x": 1198,
                    "y": 2618
                },
                {
                    "x": 202,
                    "y": 2618
                }
            ],
            "category": "paragraph",
            "html": "<p id='40' style='font-size:18px'>This convolution operation in CvT aims to model local<br>spatial contexts, from low-level edges to higher order se-<br>mantic primitives, over a multi-stage hierarchy approach,<br>similar to CNNs.</p>",
            "id": 40,
            "page": 4,
            "text": "This convolution operation in CvT aims to model local\nspatial contexts, from low-level edges to higher order se-\nmantic primitives, over a multi-stage hierarchy approach,\nsimilar to CNNs."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2630
                },
                {
                    "x": 1199,
                    "y": 2630
                },
                {
                    "x": 1199,
                    "y": 2977
                },
                {
                    "x": 202,
                    "y": 2977
                }
            ],
            "category": "paragraph",
            "html": "<p id='41' style='font-size:16px'>Formally, given a 2D image or a 2D-reshaped output to-<br>ken map from a previous stage Xi-1 E RHi-1 x Wi-1xCi-1<br>as the input to stage i, we learn a function f(·) that maps<br>Xi-1 into new tokens f(xi-1) with a channel size Ci, where<br>f(·) is 2D convolution operation of kernel size s x s, stride<br>s - 0 and p padding (to deal with boundary conditions).<br>The new token map f(xi-1) E RHixWixCi has height and</p>",
            "id": 41,
            "page": 4,
            "text": "Formally, given a 2D image or a 2D-reshaped output to-\nken map from a previous stage Xi-1 E RHi-1 x Wi-1xCi-1\nas the input to stage i, we learn a function f(·) that maps\nXi-1 into new tokens f(xi-1) with a channel size Ci, where\nf(·) is 2D convolution operation of kernel size s x s, stride\ns - 0 and p padding (to deal with boundary conditions).\nThe new token map f(xi-1) E RHixWixCi has height and"
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 309
                },
                {
                    "x": 1385,
                    "y": 309
                },
                {
                    "x": 1385,
                    "y": 350
                },
                {
                    "x": 1281,
                    "y": 350
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='42' style='font-size:18px'>width</p>",
            "id": 42,
            "page": 4,
            "text": "width"
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 517
                },
                {
                    "x": 2276,
                    "y": 517
                },
                {
                    "x": 2276,
                    "y": 665
                },
                {
                    "x": 1281,
                    "y": 665
                }
            ],
            "category": "paragraph",
            "html": "<p id='43' style='font-size:18px'>f(xi-1) is then flattened into size HiWi x Ci and normal-<br>ized by layer normalization [1] for input into the subsequent<br>Transformer blocks of stage 2.</p>",
            "id": 43,
            "page": 4,
            "text": "f(xi-1) is then flattened into size HiWi x Ci and normal-\nized by layer normalization [1] for input into the subsequent\nTransformer blocks of stage 2."
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 668
                },
                {
                    "x": 2277,
                    "y": 668
                },
                {
                    "x": 2277,
                    "y": 1112
                },
                {
                    "x": 1279,
                    "y": 1112
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='44' style='font-size:18px'>The Convolutional Token Embedding layer allows us to<br>adjust the token feature dimension and the number of to-<br>kens at each stage by varying parameters of the convolution<br>operation. In this manner, in each stage we progressively<br>decrease the token sequence length, while increasing the<br>token feature dimension. This gives the tokens the ability<br>to represent increasingly complex visual patterns over in-<br>creasingly larger spatial footprints, similar to feature layers<br>of CNNs.</p>",
            "id": 44,
            "page": 4,
            "text": "The Convolutional Token Embedding layer allows us to\nadjust the token feature dimension and the number of to-\nkens at each stage by varying parameters of the convolution\noperation. In this manner, in each stage we progressively\ndecrease the token sequence length, while increasing the\ntoken feature dimension. This gives the tokens the ability\nto represent increasingly complex visual patterns over in-\ncreasingly larger spatial footprints, similar to feature layers\nof CNNs."
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 1140
                },
                {
                    "x": 2123,
                    "y": 1140
                },
                {
                    "x": 2123,
                    "y": 1189
                },
                {
                    "x": 1279,
                    "y": 1189
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='45' style='font-size:22px'>3.2. Convolutional Projection for Attention</p>",
            "id": 45,
            "page": 4,
            "text": "3.2. Convolutional Projection for Attention"
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 1216
                },
                {
                    "x": 2277,
                    "y": 1216
                },
                {
                    "x": 2277,
                    "y": 1412
                },
                {
                    "x": 1281,
                    "y": 1412
                }
            ],
            "category": "paragraph",
            "html": "<p id='46' style='font-size:18px'>The goal of the proposed Convolutional Projection layer<br>is to achieve additional modeling of local spatial context,<br>and to provide efficiency benefits by permitting the under-<br>sampling of K and V matrices.</p>",
            "id": 46,
            "page": 4,
            "text": "The goal of the proposed Convolutional Projection layer\nis to achieve additional modeling of local spatial context,\nand to provide efficiency benefits by permitting the under-\nsampling of K and V matrices."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 1418
                },
                {
                    "x": 2276,
                    "y": 1418
                },
                {
                    "x": 2276,
                    "y": 1913
                },
                {
                    "x": 1281,
                    "y": 1913
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='47' style='font-size:18px'>Fundamentally, the proposed Transformer block with<br>Convolutional Projection is a generalization of the origi-<br>nal Transformer block. While previous works [13, 39] try<br>to add additional convolution modules to the Transformer<br>Block for speech recognition and natural language process-<br>ing, they result in a more complicated design and addi-<br>tional computational cost. Instead, we propose to replace<br>the original position-wise linear projection for Multi-Head<br>Self-Attention (MHSA) with depth-wise separable convo-<br>lutions, forming the Convolutional Projection layer.</p>",
            "id": 47,
            "page": 4,
            "text": "Fundamentally, the proposed Transformer block with\nConvolutional Projection is a generalization of the origi-\nnal Transformer block. While previous works [13, 39] try\nto add additional convolution modules to the Transformer\nBlock for speech recognition and natural language process-\ning, they result in a more complicated design and addi-\ntional computational cost. Instead, we propose to replace\nthe original position-wise linear projection for Multi-Head\nSelf-Attention (MHSA) with depth-wise separable convo-\nlutions, forming the Convolutional Projection layer."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1971
                },
                {
                    "x": 1831,
                    "y": 1971
                },
                {
                    "x": 1831,
                    "y": 2019
                },
                {
                    "x": 1280,
                    "y": 2019
                }
            ],
            "category": "paragraph",
            "html": "<p id='48' style='font-size:20px'>3.2.1 Implementation Details</p>",
            "id": 48,
            "page": 4,
            "text": "3.2.1 Implementation Details"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 2049
                },
                {
                    "x": 2277,
                    "y": 2049
                },
                {
                    "x": 2277,
                    "y": 2444
                },
                {
                    "x": 1279,
                    "y": 2444
                }
            ],
            "category": "paragraph",
            "html": "<p id='49' style='font-size:18px'>Figure 3 (a) shows the original position-wise linear projec-<br>tion used in ViT [11] and Figure 3 (b) shows our proposed<br>s x s Convolutional Projection. As shown in Figure 3 (b),<br>tokens are first reshaped into a 2D token map. Next, a Con-<br>volutional Projection is implemented using a depth-wise<br>separable convolution layer with kernel size s. Finally, the<br>projected tokens are flattened into 1D for subsequent pro-<br>cess. This can be formulated as:</p>",
            "id": 49,
            "page": 4,
            "text": "Figure 3 (a) shows the original position-wise linear projec-\ntion used in ViT [11] and Figure 3 (b) shows our proposed\ns x s Convolutional Projection. As shown in Figure 3 (b),\ntokens are first reshaped into a 2D token map. Next, a Con-\nvolutional Projection is implemented using a depth-wise\nseparable convolution layer with kernel size s. Finally, the\nprojected tokens are flattened into 1D for subsequent pro-\ncess. This can be formulated as:"
        },
        {
            "bounding_box": [
                {
                    "x": 1279,
                    "y": 2572
                },
                {
                    "x": 2277,
                    "y": 2572
                },
                {
                    "x": 2277,
                    "y": 2873
                },
                {
                    "x": 1279,
                    "y": 2873
                }
            ],
            "category": "paragraph",
            "html": "<p id='50' style='font-size:16px'>is the token input for Q/K/V matrices at<br>where xq/k/v<br>layer 2, Xi is the unperturbed token prior to the Convolu-<br>tional Projection, Conv2d is a depth-wise separable con-<br>volution [5] implemented by: Depth-wise Conv2d →<br>Bat chNorm2d → Point-wise Conv2d, and s refers<br>to the convolution kernel size.</p>",
            "id": 50,
            "page": 4,
            "text": "is the token input for Q/K/V matrices at\nwhere xq/k/v\nlayer 2, Xi is the unperturbed token prior to the Convolu-\ntional Projection, Conv2d is a depth-wise separable con-\nvolution [5] implemented by: Depth-wise Conv2d →\nBat chNorm2d → Point-wise Conv2d, and s refers\nto the convolution kernel size."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 2879
                },
                {
                    "x": 2277,
                    "y": 2879
                },
                {
                    "x": 2277,
                    "y": 2976
                },
                {
                    "x": 1281,
                    "y": 2976
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='51' style='font-size:18px'>The resulting new Transformer Block with the Convo-<br>lutional Projection layer is a generalization of the original</p>",
            "id": 51,
            "page": 4,
            "text": "The resulting new Transformer Block with the Convo-\nlutional Projection layer is a generalization of the original"
        },
        {
            "bounding_box": [
                {
                    "x": 1225,
                    "y": 3055
                },
                {
                    "x": 1250,
                    "y": 3055
                },
                {
                    "x": 1250,
                    "y": 3088
                },
                {
                    "x": 1225,
                    "y": 3088
                }
            ],
            "category": "footer",
            "html": "<footer id='52' style='font-size:14px'>4</footer>",
            "id": 52,
            "page": 4,
            "text": "4"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 285
                },
                {
                    "x": 2286,
                    "y": 285
                },
                {
                    "x": 2286,
                    "y": 766
                },
                {
                    "x": 202,
                    "y": 766
                }
            ],
            "category": "figure",
            "html": "<figure><img id='53' style='font-size:14px' alt=\"Convolutional Projection\n1 flatten\nquery S Projection\nConvolutional\nkey for one position: S x\nfor one position: S X s → 1 flatten\nConvolutional Projection query\nslide with stride\nvalue sxs\nwith stride = 1 slide with stride Projection query\nsxs\nwith stride = 1\nwindow Convolutional\nwindow\nreshape Convolutional Projection flatten reshape Convolutional Projection flatten locally squeezed\nkey\npad with stride = 1 key\npad with stride = 2\nConvolutional Projection 1 = 2 flatten locally squeezed\nConvolutional Projection\n□ : with stride =\nwith stride\n□\nflatten\nvalue\ntokens\nvalue\ntokens\n(a) (b) (c)\" data-coord=\"top-left:(202,285); bottom-right:(2286,766)\" /></figure>",
            "id": 53,
            "page": 5,
            "text": "Convolutional Projection\n1 flatten\nquery S Projection\nConvolutional\nkey for one position: S x\nfor one position: S X s → 1 flatten\nConvolutional Projection query\nslide with stride\nvalue sxs\nwith stride = 1 slide with stride Projection query\nsxs\nwith stride = 1\nwindow Convolutional\nwindow\nreshape Convolutional Projection flatten reshape Convolutional Projection flatten locally squeezed\nkey\npad with stride = 1 key\npad with stride = 2\nConvolutional Projection 1 = 2 flatten locally squeezed\nConvolutional Projection\n□ : with stride =\nwith stride\n□\nflatten\nvalue\ntokens\nvalue\ntokens\n(a) (b) (c)"
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 788
                },
                {
                    "x": 2273,
                    "y": 788
                },
                {
                    "x": 2273,
                    "y": 885
                },
                {
                    "x": 203,
                    "y": 885
                }
            ],
            "category": "caption",
            "html": "<br><caption id='54' style='font-size:18px'>Figure 3: (a) Linear projection in ViT [11]. (b) Convolutional projection. (c) Squeezed convolutional projection. Unless<br>otherwise stated, we use (c) Squeezed convolutional projection by default.</caption>",
            "id": 54,
            "page": 5,
            "text": "Figure 3: (a) Linear projection in ViT [11]. (b) Convolutional projection. (c) Squeezed convolutional projection. Unless\notherwise stated, we use (c) Squeezed convolutional projection by default."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 974
                },
                {
                    "x": 1198,
                    "y": 974
                },
                {
                    "x": 1198,
                    "y": 1125
                },
                {
                    "x": 201,
                    "y": 1125
                }
            ],
            "category": "paragraph",
            "html": "<p id='55' style='font-size:16px'>Transformer Block design. The original position-wise lin-<br>ear projection layer could be trivially implemented using a<br>convolution layer with kernel size of 1 x 1.</p>",
            "id": 55,
            "page": 5,
            "text": "Transformer Block design. The original position-wise lin-\near projection layer could be trivially implemented using a\nconvolution layer with kernel size of 1 x 1."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 1191
                },
                {
                    "x": 790,
                    "y": 1191
                },
                {
                    "x": 790,
                    "y": 1237
                },
                {
                    "x": 202,
                    "y": 1237
                }
            ],
            "category": "paragraph",
            "html": "<p id='56' style='font-size:20px'>3.2.2 Efficiency Considerations</p>",
            "id": 56,
            "page": 5,
            "text": "3.2.2 Efficiency Considerations"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 1269
                },
                {
                    "x": 1197,
                    "y": 1269
                },
                {
                    "x": 1197,
                    "y": 1365
                },
                {
                    "x": 202,
                    "y": 1365
                }
            ],
            "category": "paragraph",
            "html": "<p id='57' style='font-size:18px'>There are two primary efficiency benefits from the design<br>of our Convolutional Projection layer.</p>",
            "id": 57,
            "page": 5,
            "text": "There are two primary efficiency benefits from the design\nof our Convolutional Projection layer."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1368
                },
                {
                    "x": 1198,
                    "y": 1368
                },
                {
                    "x": 1198,
                    "y": 1915
                },
                {
                    "x": 201,
                    "y": 1915
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='58' style='font-size:14px'>First, we utilize efficient convolutions. Directly using<br>standard s x s convolutions for the Convolutional Projection<br>would require s2C2 parameters and O(s2C2T) FLOPs,<br>where C is the token channel dimension, and T is the num-<br>ber of tokens for processing. Instead, we split the standard<br>s x s convolution into a depth-wise separable convolution<br>[16]. In this way, each of the proposed Convolutional Pro-<br>jection would only introduce an extra of s2C parameters<br>and O(s2CT) FLOPs compared to the original position-<br>wise linear projection, which are negligible with respect to<br>the total parameters and FLOPs of the models.</p>",
            "id": 58,
            "page": 5,
            "text": "First, we utilize efficient convolutions. Directly using\nstandard s x s convolutions for the Convolutional Projection\nwould require s2C2 parameters and O(s2C2T) FLOPs,\nwhere C is the token channel dimension, and T is the num-\nber of tokens for processing. Instead, we split the standard\ns x s convolution into a depth-wise separable convolution\n[16]. In this way, each of the proposed Convolutional Pro-\njection would only introduce an extra of s2C parameters\nand O(s2CT) FLOPs compared to the original position-\nwise linear projection, which are negligible with respect to\nthe total parameters and FLOPs of the models."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 1921
                },
                {
                    "x": 1199,
                    "y": 1921
                },
                {
                    "x": 1199,
                    "y": 2717
                },
                {
                    "x": 200,
                    "y": 2717
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='59' style='font-size:14px'>Second, we leverage the proposed Convolutional Projec-<br>tion to reduce the computation cost for the MHSA opera-<br>tion. The s x s Convolutional Projection permits reducing<br>the number of tokens by using a stride larger than 1. Fig-<br>ure 3 (c) shows the Convolutional Projection, where the key<br>and value projection are subsampled by using a convolu-<br>tion with stride larger than 1. We use a stride of 2 for key<br>and value projection, leaving the stride of 1 for query un-<br>changed. In this way, the number of tokens for key and<br>value is reduced 4 times, and the computational cost is re-<br>duced by 4 times for the later MHSA operation. This comes<br>with a minimal performance penalty, as neighboring pix-<br>els/patches in images tend to have redundancy in appear-<br>ance/semantics. In addition, the local context modeling of<br>the proposed Convolutional Projection compensates for the<br>loss of information incurred by resolution reduction.</p>",
            "id": 59,
            "page": 5,
            "text": "Second, we leverage the proposed Convolutional Projec-\ntion to reduce the computation cost for the MHSA opera-\ntion. The s x s Convolutional Projection permits reducing\nthe number of tokens by using a stride larger than 1. Fig-\nure 3 (c) shows the Convolutional Projection, where the key\nand value projection are subsampled by using a convolu-\ntion with stride larger than 1. We use a stride of 2 for key\nand value projection, leaving the stride of 1 for query un-\nchanged. In this way, the number of tokens for key and\nvalue is reduced 4 times, and the computational cost is re-\nduced by 4 times for the later MHSA operation. This comes\nwith a minimal performance penalty, as neighboring pix-\nels/patches in images tend to have redundancy in appear-\nance/semantics. In addition, the local context modeling of\nthe proposed Convolutional Projection compensates for the\nloss of information incurred by resolution reduction."
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 2750
                },
                {
                    "x": 828,
                    "y": 2750
                },
                {
                    "x": 828,
                    "y": 2798
                },
                {
                    "x": 204,
                    "y": 2798
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='60' style='font-size:22px'>3.3. Methodological Discussions</p>",
            "id": 60,
            "page": 5,
            "text": "3.3. Methodological Discussions"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2827
                },
                {
                    "x": 1197,
                    "y": 2827
                },
                {
                    "x": 1197,
                    "y": 2976
                },
                {
                    "x": 202,
                    "y": 2976
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='61' style='font-size:18px'>Removing Positional Embeddings: The introduction of<br>Convolutional Projections for every Transformer block,<br>combined with the Convolutional Token Embedding, gives</p>",
            "id": 61,
            "page": 5,
            "text": "Removing Positional Embeddings: The introduction of\nConvolutional Projections for every Transformer block,\ncombined with the Convolutional Token Embedding, gives"
        },
        {
            "bounding_box": [
                {
                    "x": 1277,
                    "y": 975
                },
                {
                    "x": 2277,
                    "y": 975
                },
                {
                    "x": 2277,
                    "y": 1224
                },
                {
                    "x": 1277,
                    "y": 1224
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='62' style='font-size:18px'>us the ability to model local spatial relationships through the<br>network. This built-in property allows dropping the position<br>embedding from the network without hurting performance,<br>as evidenced by our experiments (Section 4.4), simplifying<br>design for vision tasks with variable input resolution.</p>",
            "id": 62,
            "page": 5,
            "text": "us the ability to model local spatial relationships through the\nnetwork. This built-in property allows dropping the position\nembedding from the network without hurting performance,\nas evidenced by our experiments (Section 4.4), simplifying\ndesign for vision tasks with variable input resolution."
        },
        {
            "bounding_box": [
                {
                    "x": 1277,
                    "y": 1279
                },
                {
                    "x": 2277,
                    "y": 1279
                },
                {
                    "x": 2277,
                    "y": 2426
                },
                {
                    "x": 1277,
                    "y": 2426
                }
            ],
            "category": "paragraph",
            "html": "<p id='63' style='font-size:14px'>Relations to Concurrent Work: Recently, two more re-<br>lated concurrent works also propose to improve ViT by in-<br>corporating elements of CNNs to Transformers. Tokens-<br>to-Token ViT [41] implements a progressive tokenization,<br>and then uses a Transformer-based backbone in which the<br>length of tokens is fixed. By contrast, our CvT implements<br>a progressive tokenization by a multi-stage process - con-<br>taining both convolutional token embeddings and convolu-<br>tional Transformer blocks in each stage. As the length of<br>tokens are decreased in each stage, the width of the tokens<br>(dimension of feature) can be increased, allowing increased<br>richness of representations at each feature spatial resolu-<br>tion. Additionally, whereas T2T concatenates neighboring<br>tokens into one new token, leading to increasing the com-<br>plexity of memory and computation, our usage of convolu-<br>tional token embedding directly performs contextual learn-<br>ing without concatenation, while providing the flexibility<br>of controlling stride and feature dimension. To manage the<br>complexity, T2T has to consider a deep-narrow architecture<br>design with smaller hidden dimensions and MLP size than<br>ViT in the subsequent backbone. Instead, we changed pre-<br>vious Transformer modules by replacing the position-wise<br>linear projection with our convolutional projection</p>",
            "id": 63,
            "page": 5,
            "text": "Relations to Concurrent Work: Recently, two more re-\nlated concurrent works also propose to improve ViT by in-\ncorporating elements of CNNs to Transformers. Tokens-\nto-Token ViT [41] implements a progressive tokenization,\nand then uses a Transformer-based backbone in which the\nlength of tokens is fixed. By contrast, our CvT implements\na progressive tokenization by a multi-stage process - con-\ntaining both convolutional token embeddings and convolu-\ntional Transformer blocks in each stage. As the length of\ntokens are decreased in each stage, the width of the tokens\n(dimension of feature) can be increased, allowing increased\nrichness of representations at each feature spatial resolu-\ntion. Additionally, whereas T2T concatenates neighboring\ntokens into one new token, leading to increasing the com-\nplexity of memory and computation, our usage of convolu-\ntional token embedding directly performs contextual learn-\ning without concatenation, while providing the flexibility\nof controlling stride and feature dimension. To manage the\ncomplexity, T2T has to consider a deep-narrow architecture\ndesign with smaller hidden dimensions and MLP size than\nViT in the subsequent backbone. Instead, we changed pre-\nvious Transformer modules by replacing the position-wise\nlinear projection with our convolutional projection"
        },
        {
            "bounding_box": [
                {
                    "x": 1277,
                    "y": 2429
                },
                {
                    "x": 2277,
                    "y": 2429
                },
                {
                    "x": 2277,
                    "y": 2977
                },
                {
                    "x": 1277,
                    "y": 2977
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='64' style='font-size:16px'>Pyramid Vision Transformer (PVT) [34] overcomes the<br>difficulties of porting ViT to various dense prediction tasks.<br>In ViT, the output feature map has only a single scale with<br>low resolution. In addition, computations and memory cost<br>are relatively high, even for common input image sizes. To<br>address this problem, both PVT and our CvT incorporate<br>pyramid structures from CNNs to the Transformers struc-<br>ture. Compared with PVT, which only spatially subsam-<br>ples the feature map or key/value matrices in projection, our<br>CvT instead employs convolutions with stride to achieve<br>this goal. Our experiments (shown in Section 4.4) demon-</p>",
            "id": 64,
            "page": 5,
            "text": "Pyramid Vision Transformer (PVT) [34] overcomes the\ndifficulties of porting ViT to various dense prediction tasks.\nIn ViT, the output feature map has only a single scale with\nlow resolution. In addition, computations and memory cost\nare relatively high, even for common input image sizes. To\naddress this problem, both PVT and our CvT incorporate\npyramid structures from CNNs to the Transformers struc-\nture. Compared with PVT, which only spatially subsam-\nples the feature map or key/value matrices in projection, our\nCvT instead employs convolutions with stride to achieve\nthis goal. Our experiments (shown in Section 4.4) demon-"
        },
        {
            "bounding_box": [
                {
                    "x": 1226,
                    "y": 3055
                },
                {
                    "x": 1251,
                    "y": 3055
                },
                {
                    "x": 1251,
                    "y": 3091
                },
                {
                    "x": 1226,
                    "y": 3091
                }
            ],
            "category": "footer",
            "html": "<footer id='65' style='font-size:14px'>5</footer>",
            "id": 65,
            "page": 5,
            "text": "5"
        },
        {
            "bounding_box": [
                {
                    "x": 208,
                    "y": 290
                },
                {
                    "x": 2294,
                    "y": 290
                },
                {
                    "x": 2294,
                    "y": 991
                },
                {
                    "x": 208,
                    "y": 991
                }
            ],
            "category": "table",
            "html": "<table id='66' style='font-size:14px'><tr><td></td><td>Output Size</td><td>Layer Name</td><td colspan=\"2\">CvT-13</td><td colspan=\"2\">CvT-21</td><td colspan=\"2\">CvT-W24</td></tr><tr><td rowspan=\"2\">Stagel</td><td>56 x 56</td><td>Conv. Embed.</td><td colspan=\"4\">7x 7, 64, stride 4</td><td colspan=\"2\">7 x 7, 192, stride 4</td></tr><tr><td>56 x 56</td><td>Conv. Proj. MHSA MLP</td><td>3 x 3, 64 H1 = 1, D1 = 64 R1 =4</td><td>x 1</td><td>3 x 3, 64 H1 = 1, D1 = 64 R1 =4</td><td>x 1</td><td>3 x 3, 192 H1 = 3,D1 = 192 R1 =4</td><td>x 2</td></tr><tr><td rowspan=\"2\">Stage2</td><td>28 x 28</td><td>Conv. Embed.</td><td colspan=\"4\">3 x 3, 192, stride 2</td><td colspan=\"2\">3 x 3, 768, stride 2</td></tr><tr><td>28 x 28</td><td>Conv. Proj. MHSA MLP</td><td>3 x 3, 192 H2 = 3, D2 = 192 R2 = 4</td><td>x 2</td><td>3 x 3, 192 H2 = 3, D2 = 192 R2 = 4</td><td>x 4</td><td>3 x3, 768 H2 = 12, D2 = 768 R2 = 4</td><td>x 2</td></tr><tr><td rowspan=\"2\">Stage3</td><td>14x 14</td><td>Conv. Embed.</td><td colspan=\"4\">3 x 3, 384, stride 2</td><td colspan=\"2\">3 x 3, 1024, stride 2</td></tr><tr><td>14 x 14</td><td>Conv. Proj. MHSA MLP</td><td>3 x 3, 384 H3 = 6, D3 = 384 R3 = 4</td><td>x 10</td><td>3 x 3, 384 H3 = 6, D3 = 384 R3 = 4</td><td>x 16</td><td>3 x 3, 1024 H3 = 16, D3 = 1024 R3 = 4</td><td>x 20</td></tr><tr><td>Head</td><td>1 x 1</td><td>Linear</td><td colspan=\"6\">1000</td></tr><tr><td colspan=\"3\">Params</td><td colspan=\"2\">19.98 M</td><td colspan=\"2\">31.54 M</td><td colspan=\"2\">276.7 M</td></tr><tr><td colspan=\"3\">FLOPs</td><td colspan=\"2\">4.53 G</td><td colspan=\"2\">7.13 G</td><td colspan=\"2\">60.86 G</td></tr></table>",
            "id": 66,
            "page": 6,
            "text": "Output Size Layer Name CvT-13 CvT-21 CvT-W24\n Stagel 56 x 56 Conv. Embed. 7x 7, 64, stride 4 7 x 7, 192, stride 4\n 56 x 56 Conv. Proj. MHSA MLP 3 x 3, 64 H1 = 1, D1 = 64 R1 =4 x 1 3 x 3, 64 H1 = 1, D1 = 64 R1 =4 x 1 3 x 3, 192 H1 = 3,D1 = 192 R1 =4 x 2\n Stage2 28 x 28 Conv. Embed. 3 x 3, 192, stride 2 3 x 3, 768, stride 2\n 28 x 28 Conv. Proj. MHSA MLP 3 x 3, 192 H2 = 3, D2 = 192 R2 = 4 x 2 3 x 3, 192 H2 = 3, D2 = 192 R2 = 4 x 4 3 x3, 768 H2 = 12, D2 = 768 R2 = 4 x 2\n Stage3 14x 14 Conv. Embed. 3 x 3, 384, stride 2 3 x 3, 1024, stride 2\n 14 x 14 Conv. Proj. MHSA MLP 3 x 3, 384 H3 = 6, D3 = 384 R3 = 4 x 10 3 x 3, 384 H3 = 6, D3 = 384 R3 = 4 x 16 3 x 3, 1024 H3 = 16, D3 = 1024 R3 = 4 x 20\n Head 1 x 1 Linear 1000\n Params 19.98 M 31.54 M 276.7 M\n FLOPs 4.53 G 7.13 G"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 1012
                },
                {
                    "x": 2278,
                    "y": 1012
                },
                {
                    "x": 2278,
                    "y": 1163
                },
                {
                    "x": 202,
                    "y": 1163
                }
            ],
            "category": "caption",
            "html": "<br><caption id='67' style='font-size:18px'>Table 2: Architectures for ImageNet classification. Input image size is 224 x 224 by default. Conv. Embed.: Convolutional<br>Token Embedding. Conv. Proj.: Convolutional Projection. Hi and Di is the number of heads and embedding feature<br>dimension in the ith MHSA module. Ri is the feature dimension expansion ratio in the ith MLP layer.</caption>",
            "id": 67,
            "page": 6,
            "text": "Table 2: Architectures for ImageNet classification. Input image size is 224 x 224 by default. Conv. Embed.: Convolutional\nToken Embedding. Conv. Proj.: Convolutional Projection. Hi and Di is the number of heads and embedding feature\ndimension in the ith MHSA module. Ri is the feature dimension expansion ratio in the ith MLP layer."
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 1258
                },
                {
                    "x": 1198,
                    "y": 1258
                },
                {
                    "x": 1198,
                    "y": 1348
                },
                {
                    "x": 204,
                    "y": 1348
                }
            ],
            "category": "paragraph",
            "html": "<p id='68' style='font-size:18px'>strate that the fusion of local neighboring information plays<br>an important role on the performance.</p>",
            "id": 68,
            "page": 6,
            "text": "strate that the fusion of local neighboring information plays\nan important role on the performance."
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 1401
                },
                {
                    "x": 532,
                    "y": 1401
                },
                {
                    "x": 532,
                    "y": 1451
                },
                {
                    "x": 203,
                    "y": 1451
                }
            ],
            "category": "paragraph",
            "html": "<p id='69' style='font-size:22px'>4. Experiments</p>",
            "id": 69,
            "page": 6,
            "text": "4. Experiments"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 1485
                },
                {
                    "x": 1199,
                    "y": 1485
                },
                {
                    "x": 1199,
                    "y": 1683
                },
                {
                    "x": 202,
                    "y": 1683
                }
            ],
            "category": "paragraph",
            "html": "<p id='70' style='font-size:16px'>In this section, we evaluate the CvT model on large-scale<br>image classification datasets and transfer to various down-<br>stream datasets. In addition, we perform through ablation<br>studies to validate the design of the proposed architecture.</p>",
            "id": 70,
            "page": 6,
            "text": "In this section, we evaluate the CvT model on large-scale\nimage classification datasets and transfer to various down-\nstream datasets. In addition, we perform through ablation\nstudies to validate the design of the proposed architecture."
        },
        {
            "bounding_box": [
                {
                    "x": 205,
                    "y": 1718
                },
                {
                    "x": 404,
                    "y": 1718
                },
                {
                    "x": 404,
                    "y": 1767
                },
                {
                    "x": 205,
                    "y": 1767
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='71' style='font-size:20px'>4.1. Setup</p>",
            "id": 71,
            "page": 6,
            "text": "4.1. Setup"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 1795
                },
                {
                    "x": 1198,
                    "y": 1795
                },
                {
                    "x": 1198,
                    "y": 2092
                },
                {
                    "x": 202,
                    "y": 2092
                }
            ],
            "category": "paragraph",
            "html": "<p id='72' style='font-size:18px'>For evaluation, we use the ImageNet dataset, with 1.3M<br>images and 1k classes, as well as its superset ImageNet-22k<br>with 22k classes and 14M images [9]. We further trans-<br>fer the models pretrained on ImageNet-22k to downstream<br>tasks, including CIFAR-10/100 [19], Oxford-IIIT-Pet [23],<br>Oxford-IIIT-Flower [22], following [18, 11].</p>",
            "id": 72,
            "page": 6,
            "text": "For evaluation, we use the ImageNet dataset, with 1.3M\nimages and 1k classes, as well as its superset ImageNet-22k\nwith 22k classes and 14M images [9]. We further trans-\nfer the models pretrained on ImageNet-22k to downstream\ntasks, including CIFAR-10/100 [19], Oxford-IIIT-Pet [23],\nOxford-IIIT-Flower [22], following [18, 11]."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 2136
                },
                {
                    "x": 1198,
                    "y": 2136
                },
                {
                    "x": 1198,
                    "y": 2683
                },
                {
                    "x": 201,
                    "y": 2683
                }
            ],
            "category": "paragraph",
            "html": "<p id='73' style='font-size:16px'>Model Variants We instantiate models with different pa-<br>rameters and FLOPs by varying the number of Transformer<br>blocks of each stage and the hidden feature dimension used,<br>as shown in Table 2. Three stages are adapted. We de-<br>fine CvT-13 and CvT-21 as basic models, with 19.98M and<br>31.54M paramters. CvT-X stands for Convolutional vision<br>Transformer with X Transformer Blocks in total. Addition-<br>ally, we experiment with a wider model with a larger token<br>dimension for each stage, namely CvT-W24 (W stands for<br>Wide), resulting 298.3M parameters, to validate the scaling<br>ability of the proposed architecture.</p>",
            "id": 73,
            "page": 6,
            "text": "Model Variants We instantiate models with different pa-\nrameters and FLOPs by varying the number of Transformer\nblocks of each stage and the hidden feature dimension used,\nas shown in Table 2. Three stages are adapted. We de-\nfine CvT-13 and CvT-21 as basic models, with 19.98M and\n31.54M paramters. CvT-X stands for Convolutional vision\nTransformer with X Transformer Blocks in total. Addition-\nally, we experiment with a wider model with a larger token\ndimension for each stage, namely CvT-W24 (W stands for\nWide), resulting 298.3M parameters, to validate the scaling\nability of the proposed architecture."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2726
                },
                {
                    "x": 1199,
                    "y": 2726
                },
                {
                    "x": 1199,
                    "y": 2977
                },
                {
                    "x": 202,
                    "y": 2977
                }
            ],
            "category": "paragraph",
            "html": "<p id='74' style='font-size:16px'>Training AdamW [21] optimizer is used with the weight<br>decay of 0.05 for our CvT-13, and 0.1 for our CvT-21 and<br>CvT-W24. We train our models with an initial learning<br>rate of 0.02 and a total batch size of 2048 for 300 epochs,<br>with a cosine learning rate decay scheduler. We adopt the</p>",
            "id": 74,
            "page": 6,
            "text": "Training AdamW [21] optimizer is used with the weight\ndecay of 0.05 for our CvT-13, and 0.1 for our CvT-21 and\nCvT-W24. We train our models with an initial learning\nrate of 0.02 and a total batch size of 2048 for 300 epochs,\nwith a cosine learning rate decay scheduler. We adopt the"
        },
        {
            "bounding_box": [
                {
                    "x": 1282,
                    "y": 1256
                },
                {
                    "x": 2276,
                    "y": 1256
                },
                {
                    "x": 2276,
                    "y": 1399
                },
                {
                    "x": 1282,
                    "y": 1399
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='75' style='font-size:16px'>same data augmentation and regularization methods as in<br>ViT [30]. Unless otherwise stated, all ImageNet models are<br>trained with an 224 x 224 input size.</p>",
            "id": 75,
            "page": 6,
            "text": "same data augmentation and regularization methods as in\nViT [30]. Unless otherwise stated, all ImageNet models are\ntrained with an 224 x 224 input size."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 1457
                },
                {
                    "x": 2276,
                    "y": 1457
                },
                {
                    "x": 2276,
                    "y": 1854
                },
                {
                    "x": 1281,
                    "y": 1854
                }
            ],
            "category": "paragraph",
            "html": "<p id='76' style='font-size:16px'>Fine-tuning We adopt fine-tuning strategy from ViT [30].<br>SGD optimizor with a momentum of 0.9 is used for fine-<br>tuning. As in ViT [30], we pre-train our models at resolu-<br>tion 224 x 224, and fine-tune at resolution of 384 x 384.<br>We fine-tune each model with a total batch size of 512,<br>for 20,000 steps on ImageNet-1k, 10,000 steps on CIFAR-<br>10 and CIFAR-100, and 500 steps on Oxford-IIIT Pets and<br>Oxford-IIIT Flowers-102.</p>",
            "id": 76,
            "page": 6,
            "text": "Fine-tuning We adopt fine-tuning strategy from ViT [30].\nSGD optimizor with a momentum of 0.9 is used for fine-\ntuning. As in ViT [30], we pre-train our models at resolu-\ntion 224 x 224, and fine-tune at resolution of 384 x 384.\nWe fine-tune each model with a total batch size of 512,\nfor 20,000 steps on ImageNet-1k, 10,000 steps on CIFAR-\n10 and CIFAR-100, and 500 steps on Oxford-IIIT Pets and\nOxford-IIIT Flowers-102."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 1897
                },
                {
                    "x": 1954,
                    "y": 1897
                },
                {
                    "x": 1954,
                    "y": 1944
                },
                {
                    "x": 1281,
                    "y": 1944
                }
            ],
            "category": "paragraph",
            "html": "<p id='77' style='font-size:20px'>4.2. Comparison to state of the art</p>",
            "id": 77,
            "page": 6,
            "text": "4.2. Comparison to state of the art"
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 1977
                },
                {
                    "x": 2275,
                    "y": 1977
                },
                {
                    "x": 2275,
                    "y": 2171
                },
                {
                    "x": 1281,
                    "y": 2171
                }
            ],
            "category": "paragraph",
            "html": "<p id='78' style='font-size:18px'>We compare our method with state-of-the-art classifica-<br>tion methods including Transformer-based models and rep-<br>resentative CNN-based models on ImageNet [9], ImageNet<br>Real [2] and ImageNet V2 [26] datasets in Table 3.</p>",
            "id": 78,
            "page": 6,
            "text": "We compare our method with state-of-the-art classifica-\ntion methods including Transformer-based models and rep-\nresentative CNN-based models on ImageNet [9], ImageNet\nReal [2] and ImageNet V2 [26] datasets in Table 3."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2179
                },
                {
                    "x": 2276,
                    "y": 2179
                },
                {
                    "x": 2276,
                    "y": 2621
                },
                {
                    "x": 1280,
                    "y": 2621
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='79' style='font-size:18px'>Compared to Transformer based models, CvT achieves<br>a much higher accuracy with fewer parameters and FLOPs.<br>CvT-21 obtains a 82.5% ImageNet Top-1 accuracy, which<br>is 0.5% higher than DeiT-B with the reduction of 63% pa-<br>rameters and 60% FLOPs. When comparing to concurrent<br>works, CvT still shows superior advantages. With fewer<br>paramerters, CvT-13 achieves a 81.6% ImageNet Top-1 ac-<br>curacy, outperforming PVT-Small [34], T2T-ViTt-14 [41],<br>TNT-S [14] by 1.7%, 0.8%, 0.2% respectively.</p>",
            "id": 79,
            "page": 6,
            "text": "Compared to Transformer based models, CvT achieves\na much higher accuracy with fewer parameters and FLOPs.\nCvT-21 obtains a 82.5% ImageNet Top-1 accuracy, which\nis 0.5% higher than DeiT-B with the reduction of 63% pa-\nrameters and 60% FLOPs. When comparing to concurrent\nworks, CvT still shows superior advantages. With fewer\nparamerters, CvT-13 achieves a 81.6% ImageNet Top-1 ac-\ncuracy, outperforming PVT-Small [34], T2T-ViTt-14 [41],\nTNT-S [14] by 1.7%, 0.8%, 0.2% respectively."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2630
                },
                {
                    "x": 2277,
                    "y": 2630
                },
                {
                    "x": 2277,
                    "y": 2975
                },
                {
                    "x": 1280,
                    "y": 2975
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='80' style='font-size:16px'>Our architecture designing can be further improved in<br>terms of model parameters and FLOPs by neural architec-<br>ture search (NAS) [7]. In particular, we search the proper<br>stride for each convolution projection of key and value<br>(stride = 1,2) and the expansion ratio for each MLP<br>layer (ratioMLP = 2, 4). Such architecture candidates<br>with FLOPs ranging from 2.59G to 4.03G and the num-</p>",
            "id": 80,
            "page": 6,
            "text": "Our architecture designing can be further improved in\nterms of model parameters and FLOPs by neural architec-\nture search (NAS) [7]. In particular, we search the proper\nstride for each convolution projection of key and value\n(stride = 1,2) and the expansion ratio for each MLP\nlayer (ratioMLP = 2, 4). Such architecture candidates\nwith FLOPs ranging from 2.59G to 4.03G and the num-"
        },
        {
            "bounding_box": [
                {
                    "x": 1226,
                    "y": 3057
                },
                {
                    "x": 1252,
                    "y": 3057
                },
                {
                    "x": 1252,
                    "y": 3092
                },
                {
                    "x": 1226,
                    "y": 3092
                }
            ],
            "category": "footer",
            "html": "<footer id='81' style='font-size:14px'>6</footer>",
            "id": 81,
            "page": 6,
            "text": "6"
        },
        {
            "bounding_box": [
                {
                    "x": 210,
                    "y": 295
                },
                {
                    "x": 2265,
                    "y": 295
                },
                {
                    "x": 2265,
                    "y": 1870
                },
                {
                    "x": 210,
                    "y": 1870
                }
            ],
            "category": "table",
            "html": "<table id='82' style='font-size:14px'><tr><td>Method Type</td><td>Network</td><td>#Param. (M)</td><td>image FLOPs size</td><td>(G)</td><td>ImageNet top-1 (%)</td><td>Real top-1 (%)</td><td>V2 top-1 (%)</td></tr><tr><td rowspan=\"3\">Convolutional Networks</td><td>ResNet-50 [15]</td><td>25</td><td>2242</td><td>4.1</td><td>76.2</td><td>82.5</td><td>63.3</td></tr><tr><td>ResNet-101 [15]</td><td>45</td><td>2242</td><td>7.9</td><td>77.4</td><td>83.7</td><td>65.7</td></tr><tr><td>ResNet-152 [15]</td><td>60</td><td>2242</td><td>11</td><td>78.3</td><td>84.1</td><td>67.0</td></tr><tr><td rowspan=\"11\">Transformers</td><td>ViT-B/16 [11]</td><td>86</td><td>3842</td><td>55.5</td><td>77.9</td><td>83.6</td><td></td></tr><tr><td>ViT-L/16 [11]</td><td>307</td><td>3842</td><td>191.1</td><td>76.5</td><td>82.2</td><td>-</td></tr><tr><td>DeiT-S [30][arxiv 2020] DeiT-B [30][arxiv 2020]</td><td>22</td><td>2242</td><td>4.6</td><td>79.8</td><td>85.7</td><td>68.5</td></tr><tr><td></td><td>86</td><td>2242</td><td>17.6</td><td>81.8</td><td>86.7</td><td>71.5</td></tr><tr><td>PVT-Small [34][arxiv 2021] PVT-Medium [34][arxiv 2021] PVT-Large</td><td>25</td><td>2242</td><td>3.8</td><td>79.8</td><td>-</td><td></td></tr><tr><td>[34][arxiv 2021]</td><td>44</td><td>2242</td><td>6.7</td><td>81.2</td><td></td><td></td></tr><tr><td></td><td>61</td><td>2242</td><td>9.8</td><td>81.7</td><td>-</td><td></td></tr><tr><td>T2T-ViTt-14 [41][arxiv 2021] T2T-ViTt-19 [41][arxiv 2021]</td><td>22 39</td><td>2242</td><td>6.1 9.8</td><td>80.7 81.4</td><td>-</td><td></td></tr><tr><td>T2T-ViTt-24 [41][arxiv 2021]</td><td>64</td><td>2242 2242</td><td>15.0</td><td>82.2</td><td></td><td></td></tr><tr><td>TNT-S [14][arxiv 2021]</td><td>24</td><td>2242</td><td>5.2</td><td>81.3</td><td>-</td><td></td></tr><tr><td>TNT-B [14][arxiv 2021]</td><td>66</td><td>2242</td><td>14.1</td><td>82.8</td><td>-</td><td>-</td></tr><tr><td rowspan=\"5\">Convolutional Transformers</td><td>Ours: CvT-13</td><td>20</td><td>2242</td><td>4.5</td><td>81.6</td><td>86.7</td><td>70.4</td></tr><tr><td>Ours: CvT-21</td><td>32</td><td>2242</td><td>7.1</td><td>82.5</td><td>87.2</td><td>71.3</td></tr><tr><td>Ours: CvT-13↑384</td><td>20</td><td>3842</td><td>16.3</td><td>83.0</td><td>87.9</td><td>71.9</td></tr><tr><td>Ours: CvT-21↑384</td><td>32</td><td>3842</td><td>24.9</td><td>83.3</td><td>87.7</td><td>71.9</td></tr><tr><td>Ours: CvT-13-NAS</td><td>18</td><td>2242</td><td>4.1</td><td>82.2</td><td>87.5</td><td>71.3</td></tr><tr><td>Convolution Networks22k</td><td>BiT-M↑480 [18]</td><td>928</td><td>4802</td><td>837</td><td>85.4</td><td>-</td><td>-</td></tr><tr><td rowspan=\"3\">Transformers22k</td><td>ViT-B/16↑384 I I</td><td>86</td><td>3842</td><td>55.5</td><td>84.0</td><td>88.4</td><td>-</td></tr><tr><td>ViT-L/16↑384 1 ]</td><td>307</td><td>3842</td><td>191.1</td><td>85.2</td><td>88.4</td><td>-</td></tr><tr><td>ViT-H/16↑384 I 1 1 ]</td><td>632</td><td>3842</td><td></td><td>85.1</td><td>88.7</td><td></td></tr><tr><td rowspan=\"3\">Convolutional Transformers22k</td><td>Ours: CvT-13↑384</td><td>20</td><td>3842</td><td>16</td><td>83.3</td><td>88.7</td><td>72.9</td></tr><tr><td>Ours: CvT-21↑384</td><td>32</td><td>3842</td><td>25</td><td>84.9</td><td>89.8</td><td>75.6</td></tr><tr><td>Ours: CvT-W24↑384</td><td>277</td><td>3842</td><td>193.2</td><td>87.7</td><td>90.6</td><td>78.8</td></tr></table>",
            "id": 82,
            "page": 7,
            "text": "Method Type Network #Param. (M) image FLOPs size (G) ImageNet top-1 (%) Real top-1 (%) V2 top-1 (%)\n Convolutional Networks ResNet-50 [15] 25 2242 4.1 76.2 82.5 63.3\n ResNet-101 [15] 45 2242 7.9 77.4 83.7 65.7\n ResNet-152 [15] 60 2242 11 78.3 84.1 67.0\n Transformers ViT-B/16 [11] 86 3842 55.5 77.9 83.6 \n ViT-L/16 [11] 307 3842 191.1 76.5 82.2 -\n DeiT-S [30][arxiv 2020] DeiT-B [30][arxiv 2020] 22 2242 4.6 79.8 85.7 68.5\n  86 2242 17.6 81.8 86.7 71.5\n PVT-Small [34][arxiv 2021] PVT-Medium [34][arxiv 2021] PVT-Large 25 2242 3.8 79.8 - \n [34][arxiv 2021] 44 2242 6.7 81.2  \n  61 2242 9.8 81.7 - \n T2T-ViTt-14 [41][arxiv 2021] T2T-ViTt-19 [41][arxiv 2021] 22 39 2242 6.1 9.8 80.7 81.4 - \n T2T-ViTt-24 [41][arxiv 2021] 64 2242 2242 15.0 82.2  \n TNT-S [14][arxiv 2021] 24 2242 5.2 81.3 - \n TNT-B [14][arxiv 2021] 66 2242 14.1 82.8 - -\n Convolutional Transformers Ours: CvT-13 20 2242 4.5 81.6 86.7 70.4\n Ours: CvT-21 32 2242 7.1 82.5 87.2 71.3\n Ours: CvT-13↑384 20 3842 16.3 83.0 87.9 71.9\n Ours: CvT-21↑384 32 3842 24.9 83.3 87.7 71.9\n Ours: CvT-13-NAS 18 2242 4.1 82.2 87.5 71.3\n Convolution Networks22k BiT-M↑480 [18] 928 4802 837 85.4 - -\n Transformers22k ViT-B/16↑384 I I 86 3842 55.5 84.0 88.4 -\n ViT-L/16↑384 1 ] 307 3842 191.1 85.2 88.4 -\n ViT-H/16↑384 I 1 1 ] 632 3842  85.1 88.7 \n Convolutional Transformers22k Ours: CvT-13↑384 20 3842 16 83.3 88.7 72.9\n Ours: CvT-21↑384 32 3842 25 84.9 89.8 75.6\n Ours: CvT-W24↑384 277 3842 193.2 87.7 90.6"
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 1891
                },
                {
                    "x": 2278,
                    "y": 1891
                },
                {
                    "x": 2278,
                    "y": 2043
                },
                {
                    "x": 200,
                    "y": 2043
                }
            ],
            "category": "caption",
            "html": "<br><caption id='83' style='font-size:18px'>Table 3: Accuracy of manual designed architecture on ImageNet [9], ImageNet Real [2] and ImageNet V2 matched fre-<br>quency [26]. Subscript 22k indicates the model pre-trained on ImageNet22k [9], and finetuned on ImageNet1k with the input<br>size of 384 x 384, except BiT-M [18] finetuned with input size of 480 x 480.</caption>",
            "id": 83,
            "page": 7,
            "text": "Table 3: Accuracy of manual designed architecture on ImageNet [9], ImageNet Real [2] and ImageNet V2 matched fre-\nquency [26]. Subscript 22k indicates the model pre-trained on ImageNet22k [9], and finetuned on ImageNet1k with the input\nsize of 384 x 384, except BiT-M [18] finetuned with input size of 480 x 480."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 2117
                },
                {
                    "x": 1200,
                    "y": 2117
                },
                {
                    "x": 1200,
                    "y": 2465
                },
                {
                    "x": 200,
                    "y": 2465
                }
            ],
            "category": "paragraph",
            "html": "<p id='84' style='font-size:14px'>ber of model parameters ranging from 13.66M to 19.88M<br>construct the search space. The NAS is evaluated directly<br>on ImageNet-1k. The searched CvT-13-NAS, a bottleneck-<br>like architecture with stride = 2, ratioMLP = 2 at the first<br>and last stages, and stride = 1, ratioMLP = 4 at most lay-<br>ers of the middle stage, reaches to a 82.2% ImageNet Top-1<br>accuracy with fewer model parameters than CvT-13.</p>",
            "id": 84,
            "page": 7,
            "text": "ber of model parameters ranging from 13.66M to 19.88M\nconstruct the search space. The NAS is evaluated directly\non ImageNet-1k. The searched CvT-13-NAS, a bottleneck-\nlike architecture with stride = 2, ratioMLP = 2 at the first\nand last stages, and stride = 1, ratioMLP = 4 at most lay-\ners of the middle stage, reaches to a 82.2% ImageNet Top-1\naccuracy with fewer model parameters than CvT-13."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 2475
                },
                {
                    "x": 1201,
                    "y": 2475
                },
                {
                    "x": 1201,
                    "y": 2770
                },
                {
                    "x": 200,
                    "y": 2770
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='85' style='font-size:16px'>Compared to CNN-based models, CvT further closes the<br>performance gap of Transformer-based models. Our small-<br>est model CvT-13 with 20M parameters and 4.5G FLOPs<br>surpasses the large ResNet-152 model by 3.2% on Ima-<br>geNet Top-1 accuracy, while ResNet-151 has 3 times the<br>parameters of CvT-13.</p>",
            "id": 85,
            "page": 7,
            "text": "Compared to CNN-based models, CvT further closes the\nperformance gap of Transformer-based models. Our small-\nest model CvT-13 with 20M parameters and 4.5G FLOPs\nsurpasses the large ResNet-152 model by 3.2% on Ima-\ngeNet Top-1 accuracy, while ResNet-151 has 3 times the\nparameters of CvT-13."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 2780
                },
                {
                    "x": 1200,
                    "y": 2780
                },
                {
                    "x": 1200,
                    "y": 2975
                },
                {
                    "x": 202,
                    "y": 2975
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='86' style='font-size:16px'>Furthermore, when more data are involved, our wide<br>model CvT-W24* pretrained on ImageNet-22k reaches to<br>87.7% Top-1 Accuracy on ImageNet without extra data<br>(e.g. JFT-300M), surpassing the previous best Transformer</p>",
            "id": 86,
            "page": 7,
            "text": "Furthermore, when more data are involved, our wide\nmodel CvT-W24* pretrained on ImageNet-22k reaches to\n87.7% Top-1 Accuracy on ImageNet without extra data\n(e.g. JFT-300M), surpassing the previous best Transformer"
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2119
                },
                {
                    "x": 2274,
                    "y": 2119
                },
                {
                    "x": 2274,
                    "y": 2213
                },
                {
                    "x": 1280,
                    "y": 2213
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='87' style='font-size:16px'>based models ViT-L/16 by 2.5% with similar number of<br>model parameters and FLOPs.</p>",
            "id": 87,
            "page": 7,
            "text": "based models ViT-L/16 by 2.5% with similar number of\nmodel parameters and FLOPs."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2247
                },
                {
                    "x": 1878,
                    "y": 2247
                },
                {
                    "x": 1878,
                    "y": 2295
                },
                {
                    "x": 1280,
                    "y": 2295
                }
            ],
            "category": "paragraph",
            "html": "<p id='88' style='font-size:20px'>4.3. Downstream task transfer</p>",
            "id": 88,
            "page": 7,
            "text": "4.3. Downstream task transfer"
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2323
                },
                {
                    "x": 2278,
                    "y": 2323
                },
                {
                    "x": 2278,
                    "y": 2672
                },
                {
                    "x": 1280,
                    "y": 2672
                }
            ],
            "category": "paragraph",
            "html": "<p id='89' style='font-size:14px'>We further investigate the ability of our models to trans-<br>fer by fine-tuning models on various tasks, with all models<br>being pre-trained on ImageNet-22k. Table 4 shows the re-<br>sults. Our CvT-W24 model is able to obtain the best per-<br>formance across all the downstream tasks considered, even<br>when compared to the large BiT-R152x4 [18] model, which<br>has more than 3x the number of parameters as CvT-W24.</p>",
            "id": 89,
            "page": 7,
            "text": "We further investigate the ability of our models to trans-\nfer by fine-tuning models on various tasks, with all models\nbeing pre-trained on ImageNet-22k. Table 4 shows the re-\nsults. Our CvT-W24 model is able to obtain the best per-\nformance across all the downstream tasks considered, even\nwhen compared to the large BiT-R152x4 [18] model, which\nhas more than 3x the number of parameters as CvT-W24."
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 2701
                },
                {
                    "x": 1663,
                    "y": 2701
                },
                {
                    "x": 1663,
                    "y": 2748
                },
                {
                    "x": 1280,
                    "y": 2748
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='90' style='font-size:22px'>4.4. Ablation Study</p>",
            "id": 90,
            "page": 7,
            "text": "4.4. Ablation Study"
        },
        {
            "bounding_box": [
                {
                    "x": 1282,
                    "y": 2779
                },
                {
                    "x": 2278,
                    "y": 2779
                },
                {
                    "x": 2278,
                    "y": 2975
                },
                {
                    "x": 1282,
                    "y": 2975
                }
            ],
            "category": "paragraph",
            "html": "<p id='91' style='font-size:16px'>We design various ablation experiments to investigate<br>the effectiveness of the proposed components of our archi-<br>tecture. First, we show that with our introduction of con-<br>volutions, position embeddings can be removed from the</p>",
            "id": 91,
            "page": 7,
            "text": "We design various ablation experiments to investigate\nthe effectiveness of the proposed components of our archi-\ntecture. First, we show that with our introduction of con-\nvolutions, position embeddings can be removed from the"
        },
        {
            "bounding_box": [
                {
                    "x": 1226,
                    "y": 3055
                },
                {
                    "x": 1251,
                    "y": 3055
                },
                {
                    "x": 1251,
                    "y": 3090
                },
                {
                    "x": 1226,
                    "y": 3090
                }
            ],
            "category": "footer",
            "html": "<footer id='92' style='font-size:14px'>7</footer>",
            "id": 92,
            "page": 7,
            "text": "7"
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 302
                },
                {
                    "x": 1197,
                    "y": 302
                },
                {
                    "x": 1197,
                    "y": 785
                },
                {
                    "x": 204,
                    "y": 785
                }
            ],
            "category": "table",
            "html": "<table id='93' style='font-size:14px'><tr><td>Model</td><td>Param (M)</td><td>CIFAR 10</td><td>CIFAR 100</td><td>Pets</td><td>Flowers 102</td></tr><tr><td>BiT-M [18]</td><td>928</td><td>98.91</td><td>92.17</td><td>94.46</td><td>99.30</td></tr><tr><td>ViT-B/16[11]</td><td>86</td><td>98.95</td><td>91.67</td><td>94.43</td><td>99.38</td></tr><tr><td>ViT-L/16 [11]</td><td>307</td><td>99.16</td><td>93.44</td><td>94.73</td><td>99.61</td></tr><tr><td>ViT-H/16 [11]</td><td>632</td><td>99.27</td><td>93.82</td><td>94.82</td><td>99.51</td></tr><tr><td>Ours: CvT-13</td><td>20</td><td>98.83</td><td>91.11</td><td>93.25</td><td>99.50</td></tr><tr><td>Ours: CvT-21</td><td>32</td><td>99.16</td><td>92.88</td><td>94.03</td><td>99.62</td></tr><tr><td>Ours: CvT-W24</td><td>277</td><td>99.39</td><td>94.09</td><td>94.73</td><td>99.72</td></tr></table>",
            "id": 93,
            "page": 8,
            "text": "Model Param (M) CIFAR 10 CIFAR 100 Pets Flowers 102\n BiT-M [18] 928 98.91 92.17 94.46 99.30\n ViT-B/16[11] 86 98.95 91.67 94.43 99.38\n ViT-L/16 [11] 307 99.16 93.44 94.73 99.61\n ViT-H/16 [11] 632 99.27 93.82 94.82 99.51\n Ours: CvT-13 20 98.83 91.11 93.25 99.50\n Ours: CvT-21 32 99.16 92.88 94.03 99.62\n Ours: CvT-W24 277 99.39 94.09 94.73"
        },
        {
            "bounding_box": [
                {
                    "x": 206,
                    "y": 810
                },
                {
                    "x": 1192,
                    "y": 810
                },
                {
                    "x": 1192,
                    "y": 902
                },
                {
                    "x": 206,
                    "y": 902
                }
            ],
            "category": "caption",
            "html": "<caption id='94' style='font-size:18px'>Table 4: Top-1 accuracy on downstream tasks. All the mod-<br>els are pre-trained on ImageNet-22k data</caption>",
            "id": 94,
            "page": 8,
            "text": "Table 4: Top-1 accuracy on downstream tasks. All the mod-\nels are pre-trained on ImageNet-22k data"
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 972
                },
                {
                    "x": 1197,
                    "y": 972
                },
                {
                    "x": 1197,
                    "y": 1450
                },
                {
                    "x": 203,
                    "y": 1450
                }
            ],
            "category": "table",
            "html": "<table id='95' style='font-size:14px'><tr><td>Method</td><td>Model</td><td>Param (M)</td><td>Pos. Emb.</td><td>ImageNet Top-1 (%)</td></tr><tr><td>a</td><td>DeiT-S</td><td>22</td><td>Default</td><td>79.8</td></tr><tr><td>b</td><td>DeiT-S</td><td>22</td><td>N/A</td><td>78.0</td></tr><tr><td>c</td><td>CvT-13</td><td>20</td><td>Every stage</td><td>81.5</td></tr><tr><td>d</td><td>CvT-13</td><td>20</td><td>First stage</td><td>81.4</td></tr><tr><td>e</td><td>CvT-13</td><td>20</td><td>Last stage</td><td>81.4</td></tr><tr><td>f</td><td>CvT-13</td><td>20</td><td>N/A</td><td>81.6</td></tr></table>",
            "id": 95,
            "page": 8,
            "text": "Method Model Param (M) Pos. Emb. ImageNet Top-1 (%)\n a DeiT-S 22 Default 79.8\n b DeiT-S 22 N/A 78.0\n c CvT-13 20 Every stage 81.5\n d CvT-13 20 First stage 81.4\n e CvT-13 20 Last stage 81.4\n f CvT-13 20 N/A"
        },
        {
            "bounding_box": [
                {
                    "x": 344,
                    "y": 1473
                },
                {
                    "x": 1054,
                    "y": 1473
                },
                {
                    "x": 1054,
                    "y": 1516
                },
                {
                    "x": 344,
                    "y": 1516
                }
            ],
            "category": "caption",
            "html": "<br><caption id='96' style='font-size:22px'>Table 5: Ablations on position embedding.</caption>",
            "id": 96,
            "page": 8,
            "text": "Table 5: Ablations on position embedding."
        },
        {
            "bounding_box": [
                {
                    "x": 203,
                    "y": 1629
                },
                {
                    "x": 1197,
                    "y": 1629
                },
                {
                    "x": 1197,
                    "y": 1772
                },
                {
                    "x": 203,
                    "y": 1772
                }
            ],
            "category": "paragraph",
            "html": "<p id='97' style='font-size:20px'>model. Then, we study the impact of each of the proposed<br>Convolutional Token Embedding and Convolutional Projec-<br>tion components.</p>",
            "id": 97,
            "page": 8,
            "text": "model. Then, we study the impact of each of the proposed\nConvolutional Token Embedding and Convolutional Projec-\ntion components."
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1879
                },
                {
                    "x": 1199,
                    "y": 1879
                },
                {
                    "x": 1199,
                    "y": 2979
                },
                {
                    "x": 201,
                    "y": 2979
                }
            ],
            "category": "paragraph",
            "html": "<p id='98' style='font-size:22px'>Removing Position Embedding Given that we have in-<br>troduced convolutions into the model, allowing local con-<br>text to be captured, we study whether position embed-<br>ding is still needed for CvT. The results are shown in Ta-<br>ble 5, and demonstrate that removing position embedding<br>of our model does not degrade the performance. There-<br>fore, position embeddings have been removed from CvT<br>by default. As a comparison, removing the position em-<br>bedding of DeiT-S would lead to 1.8% drop of ImageNet<br>Top-1 accuracy, as it does not model image spatial relation-<br>ships other than by adding the position embedding. This<br>further shows the effectiveness of our introduced convolu-<br>tions. Position Embedding is often realized by fixed-length<br>learn-able vectors, limiting the trained model adaptation of<br>variable-length input. However, a wide range of vision ap-<br>plications take variable image resolutions. Recent work<br>CPVT [6] tries to replace explicit position embedding of<br>Vision Transformers with a conditional position encodings<br>module to model position information on-the-fly. CvT is<br>able to completely remove the positional embedding, pro-<br>viding the possibility of simplifying adaption to more vision<br>tasks without requiring a re-designing of the embedding.</p>",
            "id": 98,
            "page": 8,
            "text": "Removing Position Embedding Given that we have in-\ntroduced convolutions into the model, allowing local con-\ntext to be captured, we study whether position embed-\nding is still needed for CvT. The results are shown in Ta-\nble 5, and demonstrate that removing position embedding\nof our model does not degrade the performance. There-\nfore, position embeddings have been removed from CvT\nby default. As a comparison, removing the position em-\nbedding of DeiT-S would lead to 1.8% drop of ImageNet\nTop-1 accuracy, as it does not model image spatial relation-\nships other than by adding the position embedding. This\nfurther shows the effectiveness of our introduced convolu-\ntions. Position Embedding is often realized by fixed-length\nlearn-able vectors, limiting the trained model adaptation of\nvariable-length input. However, a wide range of vision ap-\nplications take variable image resolutions. Recent work\nCPVT [6] tries to replace explicit position embedding of\nVision Transformers with a conditional position encodings\nmodule to model position information on-the-fly. CvT is\nable to completely remove the positional embedding, pro-\nviding the possibility of simplifying adaption to more vision\ntasks without requiring a re-designing of the embedding."
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 302
                },
                {
                    "x": 2288,
                    "y": 302
                },
                {
                    "x": 2288,
                    "y": 655
                },
                {
                    "x": 1278,
                    "y": 655
                }
            ],
            "category": "table",
            "html": "<br><table id='99' style='font-size:14px'><tr><td>Method</td><td>Conv. Embed.</td><td>Pos. Embed.</td><td>#Param (M)</td><td>ImageNet top-1 (%)</td></tr><tr><td>a</td><td rowspan=\"4\"></td><td rowspan=\"4\"></td><td>19.5</td><td>80.7</td></tr><tr><td>b</td><td>19.9</td><td>81.1</td></tr><tr><td>c</td><td>20.3</td><td>81.4</td></tr><tr><td>d</td><td>20.0</td><td>81.6</td></tr></table>",
            "id": 99,
            "page": 8,
            "text": "Method Conv. Embed. Pos. Embed. #Param (M) ImageNet top-1 (%)\n a   19.5 80.7\n b 19.9 81.1\n c 20.3 81.4\n d 20.0"
        },
        {
            "bounding_box": [
                {
                    "x": 1309,
                    "y": 679
                },
                {
                    "x": 2241,
                    "y": 679
                },
                {
                    "x": 2241,
                    "y": 722
                },
                {
                    "x": 1309,
                    "y": 722
                }
            ],
            "category": "caption",
            "html": "<br><caption id='100' style='font-size:20px'>Table 6: Ablations on Convolutional Token Embedding.</caption>",
            "id": 100,
            "page": 8,
            "text": "Table 6: Ablations on Convolutional Token Embedding."
        },
        {
            "bounding_box": [
                {
                    "x": 1282,
                    "y": 780
                },
                {
                    "x": 2277,
                    "y": 780
                },
                {
                    "x": 2277,
                    "y": 1034
                },
                {
                    "x": 1282,
                    "y": 1034
                }
            ],
            "category": "table",
            "html": "<table id='101' style='font-size:16px'><tr><td>Method</td><td>Conv. Proj. KV. stride</td><td>Params (M)</td><td>FLOPs (G)</td><td>ImageNet top-1 (%)</td></tr><tr><td>a</td><td>1</td><td>20</td><td>6.55</td><td>82.3</td></tr><tr><td>b</td><td>2</td><td>20</td><td>4.53</td><td>81.6</td></tr></table>",
            "id": 101,
            "page": 8,
            "text": "Method Conv. Proj. KV. stride Params (M) FLOPs (G) ImageNet top-1 (%)\n a 1 20 6.55 82.3\n b 2 20 4.53"
        },
        {
            "bounding_box": [
                {
                    "x": 1280,
                    "y": 1056
                },
                {
                    "x": 2276,
                    "y": 1056
                },
                {
                    "x": 2276,
                    "y": 1253
                },
                {
                    "x": 1280,
                    "y": 1253
                }
            ],
            "category": "caption",
            "html": "<br><caption id='102' style='font-size:22px'>Table 7: Ablations on Convolutional Projection with differ-<br>ent strides for key and value projection. Conv. Proj. KV.:<br>Convolutional Projection for key and value. We apply Con-<br>volutional Projection in all Transformer blocks.</caption>",
            "id": 102,
            "page": 8,
            "text": "Table 7: Ablations on Convolutional Projection with differ-\nent strides for key and value projection. Conv. Proj. KV.:\nConvolutional Projection for key and value. We apply Con-\nvolutional Projection in all Transformer blocks."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 1307
                },
                {
                    "x": 2279,
                    "y": 1307
                },
                {
                    "x": 2279,
                    "y": 1739
                },
                {
                    "x": 1281,
                    "y": 1739
                }
            ],
            "category": "table",
            "html": "<table id='103' style='font-size:16px'><tr><td rowspan=\"2\">Method</td><td colspan=\"3\">Conv. Projection</td><td rowspan=\"2\">Imagenet top-1 (%)</td></tr><tr><td>Stage 1</td><td>Stage 2</td><td>Stage 3</td></tr><tr><td>a</td><td></td><td></td><td></td><td>80.6</td></tr><tr><td>b</td><td></td><td></td><td></td><td>80.8</td></tr><tr><td>c</td><td></td><td></td><td></td><td>81.0</td></tr><tr><td>d</td><td></td><td></td><td></td><td>81.6</td></tr><tr><td>#Blocks</td><td>1</td><td>2</td><td>10</td><td></td></tr></table>",
            "id": 103,
            "page": 8,
            "text": "Method Conv. Projection Imagenet top-1 (%)\n Stage 1 Stage 2 Stage 3\n a    80.6\n b    80.8\n c    81.0\n d    81.6\n #Blocks 1 2 10"
        },
        {
            "bounding_box": [
                {
                    "x": 1282,
                    "y": 1757
                },
                {
                    "x": 2275,
                    "y": 1757
                },
                {
                    "x": 2275,
                    "y": 1953
                },
                {
                    "x": 1282,
                    "y": 1953
                }
            ],
            "category": "caption",
            "html": "<br><caption id='104' style='font-size:18px'>Table 8: Ablations on Convolutional Projection V.S.<br>Position-wise Linear Projection. V indicates the use of<br>Convolutional Projection, otherwise use Position-wise Lin-<br>ear Projection.</caption>",
            "id": 104,
            "page": 8,
            "text": "Table 8: Ablations on Convolutional Projection V.S.\nPosition-wise Linear Projection. V indicates the use of\nConvolutional Projection, otherwise use Position-wise Lin-\near Projection."
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 2049
                },
                {
                    "x": 2275,
                    "y": 2049
                },
                {
                    "x": 2275,
                    "y": 2746
                },
                {
                    "x": 1278,
                    "y": 2746
                }
            ],
            "category": "paragraph",
            "html": "<p id='105' style='font-size:22px'>Convolutional Token Embedding We study the effec-<br>tiveness of the proposed Convolutional Token Embedding,<br>and Table 6 shows the results. Table 6d is the CvT-13<br>model. When we replace the Convolutional Token Embed-<br>ding with non-overlapping Patch Embedding [11], the per-<br>formance drops 0.8% (Table 6a V.S. Table 6d). When po-<br>sition embedding is used, the introduction of Convolutional<br>Token Embedding still obtains 0.3% improvement (Table 6b<br>V.S. Table 6c). Further, when using both Convolutional<br>Token Embedding and position embedding as Table 6d, it<br>slightly drops 0.1% accuracy. These results validate the in-<br>troduction of Convolutional Token Embedding not only im-<br>proves the performance, but also helps CvT model spatial<br>relationships without position embedding.</p>",
            "id": 105,
            "page": 8,
            "text": "Convolutional Token Embedding We study the effec-\ntiveness of the proposed Convolutional Token Embedding,\nand Table 6 shows the results. Table 6d is the CvT-13\nmodel. When we replace the Convolutional Token Embed-\nding with non-overlapping Patch Embedding [11], the per-\nformance drops 0.8% (Table 6a V.S. Table 6d). When po-\nsition embedding is used, the introduction of Convolutional\nToken Embedding still obtains 0.3% improvement (Table 6b\nV.S. Table 6c). Further, when using both Convolutional\nToken Embedding and position embedding as Table 6d, it\nslightly drops 0.1% accuracy. These results validate the in-\ntroduction of Convolutional Token Embedding not only im-\nproves the performance, but also helps CvT model spatial\nrelationships without position embedding."
        },
        {
            "bounding_box": [
                {
                    "x": 1281,
                    "y": 2777
                },
                {
                    "x": 2275,
                    "y": 2777
                },
                {
                    "x": 2275,
                    "y": 2978
                },
                {
                    "x": 1281,
                    "y": 2978
                }
            ],
            "category": "paragraph",
            "html": "<p id='106' style='font-size:20px'>Convolutional Projection First, we compare the pro-<br>posed Convolutional Projection with different strides in Ta-<br>ble 7. By using a stride of 2 for key and value projection,<br>we observe a 0.3% drop in ImageNet Top-1 accuracy, but</p>",
            "id": 106,
            "page": 8,
            "text": "Convolutional Projection First, we compare the pro-\nposed Convolutional Projection with different strides in Ta-\nble 7. By using a stride of 2 for key and value projection,\nwe observe a 0.3% drop in ImageNet Top-1 accuracy, but"
        },
        {
            "bounding_box": [
                {
                    "x": 1226,
                    "y": 3057
                },
                {
                    "x": 1251,
                    "y": 3057
                },
                {
                    "x": 1251,
                    "y": 3091
                },
                {
                    "x": 1226,
                    "y": 3091
                }
            ],
            "category": "footer",
            "html": "<footer id='107' style='font-size:18px'>8</footer>",
            "id": 107,
            "page": 8,
            "text": "8"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 308
                },
                {
                    "x": 1200,
                    "y": 308
                },
                {
                    "x": 1200,
                    "y": 452
                },
                {
                    "x": 202,
                    "y": 452
                }
            ],
            "category": "paragraph",
            "html": "<p id='108' style='font-size:16px'>with 30% fewer FLOPs. We choose to use Convolutional<br>Projection with stride 2 for key and value as default for less<br>computational cost and memory usage.</p>",
            "id": 108,
            "page": 9,
            "text": "with 30% fewer FLOPs. We choose to use Convolutional\nProjection with stride 2 for key and value as default for less\ncomputational cost and memory usage."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 458
                },
                {
                    "x": 1199,
                    "y": 458
                },
                {
                    "x": 1199,
                    "y": 953
                },
                {
                    "x": 202,
                    "y": 953
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='109' style='font-size:18px'>Then, we study how the proposed Convolutional Pro-<br>jection affects the performance by choosing whether to use<br>Convolutional Projection or the regular Position-wise Lin-<br>ear Projection for each stage. The results are shown in Ta-<br>ble 8. We observe that replacing the original Position-wise<br>Linear Projection with the proposed Convolutional Projec-<br>tion improves the Top-1 Accuracy on ImageNet from 80.6%<br>to 81.5%. In addition, performance continually improves as<br>more stages use the design, validating this approach as an<br>effective modeling strategy.</p>",
            "id": 109,
            "page": 9,
            "text": "Then, we study how the proposed Convolutional Pro-\njection affects the performance by choosing whether to use\nConvolutional Projection or the regular Position-wise Lin-\near Projection for each stage. The results are shown in Ta-\nble 8. We observe that replacing the original Position-wise\nLinear Projection with the proposed Convolutional Projec-\ntion improves the Top-1 Accuracy on ImageNet from 80.6%\nto 81.5%. In addition, performance continually improves as\nmore stages use the design, validating this approach as an\neffective modeling strategy."
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 997
                },
                {
                    "x": 499,
                    "y": 997
                },
                {
                    "x": 499,
                    "y": 1048
                },
                {
                    "x": 202,
                    "y": 1048
                }
            ],
            "category": "paragraph",
            "html": "<p id='110' style='font-size:20px'>5. Conclusion</p>",
            "id": 110,
            "page": 9,
            "text": "5. Conclusion"
        },
        {
            "bounding_box": [
                {
                    "x": 201,
                    "y": 1082
                },
                {
                    "x": 1199,
                    "y": 1082
                },
                {
                    "x": 1199,
                    "y": 1730
                },
                {
                    "x": 201,
                    "y": 1730
                }
            ],
            "category": "paragraph",
            "html": "<p id='111' style='font-size:18px'>In this work, we have presented a detailed study of in-<br>troducing convolutions into the Vision Transformer archi-<br>tecture to merge the benefits of Transformers with the ben-<br>efits of CNNs for image recognition tasks. Extensive ex-<br>periments demonstrate that the introduced convolutional to-<br>ken embedding and convolutional projection, along with the<br>multi-stage design of the network enabled by convolutions,<br>make our CvT architecture achieve superior performance<br>while maintaining computational efficiency. Furthermore,<br>due to the built-in local context structure introduced by con-<br>volutions, CvT no longer requires a position embedding,<br>giving it a potential advantage for adaption to a wide range<br>of vision tasks requiring variable input resolution.</p>",
            "id": 111,
            "page": 9,
            "text": "In this work, we have presented a detailed study of in-\ntroducing convolutions into the Vision Transformer archi-\ntecture to merge the benefits of Transformers with the ben-\nefits of CNNs for image recognition tasks. Extensive ex-\nperiments demonstrate that the introduced convolutional to-\nken embedding and convolutional projection, along with the\nmulti-stage design of the network enabled by convolutions,\nmake our CvT architecture achieve superior performance\nwhile maintaining computational efficiency. Furthermore,\ndue to the built-in local context structure introduced by con-\nvolutions, CvT no longer requires a position embedding,\ngiving it a potential advantage for adaption to a wide range\nof vision tasks requiring variable input resolution."
        },
        {
            "bounding_box": [
                {
                    "x": 204,
                    "y": 1775
                },
                {
                    "x": 445,
                    "y": 1775
                },
                {
                    "x": 445,
                    "y": 1822
                },
                {
                    "x": 204,
                    "y": 1822
                }
            ],
            "category": "paragraph",
            "html": "<p id='112' style='font-size:22px'>References</p>",
            "id": 112,
            "page": 9,
            "text": "References"
        },
        {
            "bounding_box": [
                {
                    "x": 219,
                    "y": 1852
                },
                {
                    "x": 1201,
                    "y": 1852
                },
                {
                    "x": 1201,
                    "y": 2975
                },
                {
                    "x": 219,
                    "y": 2975
                }
            ],
            "category": "paragraph",
            "html": "<p id='113' style='font-size:14px'>[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton.<br>Layer normalization, 2016. 4<br>[2] Lucas Beyer, Olivier J Henaff, Alexander Kolesnikov, Xi-<br>aohua Zhai, and A�ron van den Oord. Are we done with<br>imagenet? arXiv preprint arXiv:2006.07159, 2020. 6, 7<br>[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas<br>Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-<br>end object detection with transformers. In European Confer-<br>ence on Computer Vision, pages 213-229. Springer, 2020.<br>2<br>[4] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping<br>Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, and<br>Wen Gao. Pre-trained image processing transformer. arXiv<br>preprint arXiv:2012.00364, 2020. 2<br>[5] Fran�ois Chollet. Xception: Deep learning with depthwise<br>separable convolutions. In Proceedings of the IEEE con-<br>ference on computer vision and pattern recognition, pages<br>1251-1258, 2017. 2, 4<br>[6] Xiangxiang Chu, Bo Zhang, Zhi Tian, Xiaolin Wei, and<br>Huaxia Xia. Do we really need explicit position encodings<br>for vision transformers? arXiv preprint arXiv:2102.10882,<br>2021. 3, 8<br>[7] Xiyang Dai, Dongdong Chen, Mengchen Liu, Yinpeng<br>Chen, and Lu YUan. Da-nas: Data adapted pruning for effi-</p>",
            "id": 113,
            "page": 9,
            "text": "[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton.\nLayer normalization, 2016. 4\n[2] Lucas Beyer, Olivier J Henaff, Alexander Kolesnikov, Xi-\naohua Zhai, and A�ron van den Oord. Are we done with\nimagenet? arXiv preprint arXiv:2006.07159, 2020. 6, 7\n[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In European Confer-\nence on Computer Vision, pages 213-229. Springer, 2020.\n2\n[4] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping\nDeng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, and\nWen Gao. Pre-trained image processing transformer. arXiv\npreprint arXiv:2012.00364, 2020. 2\n[5] Fran�ois Chollet. Xception: Deep learning with depthwise\nseparable convolutions. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition, pages\n1251-1258, 2017. 2, 4\n[6] Xiangxiang Chu, Bo Zhang, Zhi Tian, Xiaolin Wei, and\nHuaxia Xia. Do we really need explicit position encodings\nfor vision transformers? arXiv preprint arXiv:2102.10882,\n2021. 3, 8\n[7] Xiyang Dai, Dongdong Chen, Mengchen Liu, Yinpeng\nChen, and Lu YUan. Da-nas: Data adapted pruning for effi-"
        },
        {
            "bounding_box": [
                {
                    "x": 1282,
                    "y": 306
                },
                {
                    "x": 2288,
                    "y": 306
                },
                {
                    "x": 2288,
                    "y": 2977
                },
                {
                    "x": 1282,
                    "y": 2977
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='114' style='font-size:14px'>cient neural architecture search. In European Conference on<br>Computer Vision, 2020. 6<br>[8] Zhigang Dai, Bolun Cai, Yugeng Lin, and Junying Chen.<br>Up-detr: Unsupervised pre-training for object detection with<br>transformers. arXiv preprint arXiv:2011.09094, 2020. 2<br>[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,<br>and Li Fei-Fei. Imagenet: A large-scale hierarchical image<br>database. In 2009 IEEE conference on computer vision and<br>pattern recognition, pages 248-255. Ieee, 2009. 6, 7<br>[10] Jacob Devlin, Ming- Wei Chang, Kenton Lee, and Kristina<br>Toutanova. BERT: Pre-training of deep bidirectional trans-<br>formers for language understanding. In Proceedings of the<br>2019 Conference of the North American Chapter of the As-<br>sociation for Computational Linguistics: Human Language<br>Technologies, Volume 1 (Long and Short Papers), pages<br>4171-4186, Minneapolis, Minnesota, 2019. Association for<br>Computational Linguistics. 1, 2<br>[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,<br>Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,<br>Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-<br>vain Gelly, et al. An image is worth 16x16 words: Trans-<br>formers for image recognition at scale. arXiv preprint<br>arXiv:2010.11929, 2020. 1 2, 4, 5, 6, 7, 8<br>[12] Nico Engel, Vasileios Belagiannis, and Klaus Dietmayer.<br>Point transformer. arXiv preprint arXiv:011.00931, 2020.<br>2<br>[13] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Par-<br>mar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zheng-<br>dong Zhang, Yonghui Wu, et al. Conformer: Convolution-<br>augmented transformer for speech recognition. arXiv<br>preprint arXiv:2005.08100, 2020. 3, 4<br>[14] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu,<br>and Yunhe Wang. Transformer in transformer. arXiv preprint<br>arXiv:2103.00112, 2021. 1, 3, 6, 7<br>[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.<br>Deep residual learning for image recognition. In Proceed-<br>ings of the IEEE conference on computer vision and pattern<br>recognition, pages 770-778, 2016. 1, 2, 4, 7<br>[16] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry<br>Kalenichenko, Weijun Wang, Tobias Weyand, Marco An-<br>dreetto, and Hartwig Adam. Mobilenets: Efficient convolu-<br>tional neural networks for mobile vision applications. arXiv<br>preprint arXiv:1704. 04861, 2017. 5<br>[17] Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Lo-<br>cal relation networks for image recognition. arXiv preprint<br>arXiv:1904.11491, 2019. 3<br>[18] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan<br>Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby.<br>Big transfer (bit): General visual representation learning.<br>arXiv preprint arXiv:1912.11370, 6(2):8, 2019. 1, 6, 7<br>[19] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple<br>layers of features from tiny images. 2009. 6<br>[20] Yann Lecun, Patrick Haffner, Leon Bottou, and Yoshua Ben-<br>gio. Object recognition with gradient-based learning. In<br>Contour and Grouping in Computer Vision. Springer, 1999.<br>2, 4<br>[21] Ilya Loshchilov and Frank Hutter. Decoupled weight decay<br>regularization. arXiv preprint arXiv:1711.05101, 2017. 6</p>",
            "id": 114,
            "page": 9,
            "text": "cient neural architecture search. In European Conference on\nComputer Vision, 2020. 6\n[8] Zhigang Dai, Bolun Cai, Yugeng Lin, and Junying Chen.\nUp-detr: Unsupervised pre-training for object detection with\ntransformers. arXiv preprint arXiv:2011.09094, 2020. 2\n[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, pages 248-255. Ieee, 2009. 6, 7\n[10] Jacob Devlin, Ming- Wei Chang, Kenton Lee, and Kristina\nToutanova. BERT: Pre-training of deep bidirectional trans-\nformers for language understanding. In Proceedings of the\n2019 Conference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), pages\n4171-4186, Minneapolis, Minnesota, 2019. Association for\nComputational Linguistics. 1, 2\n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 1 2, 4, 5, 6, 7, 8\n[12] Nico Engel, Vasileios Belagiannis, and Klaus Dietmayer.\nPoint transformer. arXiv preprint arXiv:011.00931, 2020.\n2\n[13] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Par-\nmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zheng-\ndong Zhang, Yonghui Wu, et al. Conformer: Convolution-\naugmented transformer for speech recognition. arXiv\npreprint arXiv:2005.08100, 2020. 3, 4\n[14] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu,\nand Yunhe Wang. Transformer in transformer. arXiv preprint\narXiv:2103.00112, 2021. 1, 3, 6, 7\n[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770-778, 2016. 1, 2, 4, 7\n[16] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry\nKalenichenko, Weijun Wang, Tobias Weyand, Marco An-\ndreetto, and Hartwig Adam. Mobilenets: Efficient convolu-\ntional neural networks for mobile vision applications. arXiv\npreprint arXiv:1704. 04861, 2017. 5\n[17] Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Lo-\ncal relation networks for image recognition. arXiv preprint\narXiv:1904.11491, 2019. 3\n[18] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan\nPuigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby.\nBig transfer (bit): General visual representation learning.\narXiv preprint arXiv:1912.11370, 6(2):8, 2019. 1, 6, 7\n[19] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple\nlayers of features from tiny images. 2009. 6\n[20] Yann Lecun, Patrick Haffner, Leon Bottou, and Yoshua Ben-\ngio. Object recognition with gradient-based learning. In\nContour and Grouping in Computer Vision. Springer, 1999.\n2, 4\n[21] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 6"
        },
        {
            "bounding_box": [
                {
                    "x": 1226,
                    "y": 3056
                },
                {
                    "x": 1251,
                    "y": 3056
                },
                {
                    "x": 1251,
                    "y": 3090
                },
                {
                    "x": 1226,
                    "y": 3090
                }
            ],
            "category": "footer",
            "html": "<footer id='115' style='font-size:14px'>9</footer>",
            "id": 115,
            "page": 9,
            "text": "9"
        },
        {
            "bounding_box": [
                {
                    "x": 206,
                    "y": 297
                },
                {
                    "x": 1202,
                    "y": 297
                },
                {
                    "x": 1202,
                    "y": 2970
                },
                {
                    "x": 206,
                    "y": 2970
                }
            ],
            "category": "paragraph",
            "html": "<p id='116' style='font-size:14px'>[22] Maria-Elena Nilsback and Andrew Zisserman. Automated<br>flower classification over a large number of classes. In In-<br>dian Conference on Computer Vision, Graphics and Image<br>Processing, Dec 2008. 6<br>[23] Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and<br>C. V. Jawahar. Cats and dogs. In IEEE Conference on Com-<br>puter Vision and Pattern Recognition, 2012. 6<br>[24] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz<br>Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Im-<br>age transformer. In International Conference on Machine<br>Learning, pages 4055-4064. PMLR, 2018. 2<br>[25] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya<br>Sutskever. Improving language understanding by generative<br>pre-training. 2018. 2<br>[26] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and<br>Vaishaal Shankar. Do imagenet classifiers generalize to im-<br>agenet? In International Conference on Machine Learning,<br>pages 5389-5400. PMLR, 2019. 6, 7<br>[27] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon<br>Shlens, Pieter Abbeel, and Ashish Vaswani. Bottle-<br>neck transformers for visual recognition. arXiv preprint<br>arXiv:2101.11605, 2021. 3<br>[28] Zhiqing Sun, Shengcao Cao, Yiming Yang, and Kris Kitani.<br>Rethinking transformer-based set prediction for object detec-<br>tion. arXiv preprint arXiv:2011.10881, 2020. 2<br>[29] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model<br>scaling for convolutional neural networks. In International<br>Conference on Machine Learning, pages 6105-6114. PMLR,<br>2019. 2<br>[30] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco<br>Massa, Alexandre Sablayrolles, and Herve Jegou. Training<br>data-efficient image transformers & distillation through at-<br>tention. arXiv preprint arXiv:2012.12877, 2020. 1, 2, 4, 6,<br>7<br>[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-<br>reit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia<br>Polosukhin. Attention is all you need. In Isabelle Guyon,<br>Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob<br>Fergus, S. V. N. Vishwanathan, and Roman Garnett, editors,<br>Advances in Neural Information Processing Systems 30: An-<br>nual Conference on Neural Information Processing Systems<br>2017, December 4-9, 2017, Long Beach, CA, USA, pages<br>5998-6008, 2017. 1, 2<br>[32] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,<br>Omer Levy, and Samuel R. Bowman. GLUE: A multi-task<br>benchmark and analysis platform for natural language un-<br>derstanding. In 7th International Conference on Learning<br>Representations, ICLR 2019, New Orleans, LA, USA, May<br>6-9, 2019. OpenReview.net, 2019. 1<br>[33] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille,<br>and Liang-Chieh Chen. Max-deeplab: End-to-end panop-<br>tic segmentation with mask transformers. arXiv preprint<br>arXiv:2012.00759, 2020. 2<br>[34] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao<br>Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.<br>Pyramid vision transformer: A versatile backbone for<br>dense prediction without convolutions. arXiv preprint<br>arXiv:2102.12122, 2021. 1, 3, 4, 5, 6, 7</p>",
            "id": 116,
            "page": 10,
            "text": "[22] Maria-Elena Nilsback and Andrew Zisserman. Automated\nflower classification over a large number of classes. In In-\ndian Conference on Computer Vision, Graphics and Image\nProcessing, Dec 2008. 6\n[23] Omkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and\nC. V. Jawahar. Cats and dogs. In IEEE Conference on Com-\nputer Vision and Pattern Recognition, 2012. 6\n[24] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz\nKaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Im-\nage transformer. In International Conference on Machine\nLearning, pages 4055-4064. PMLR, 2018. 2\n[25] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya\nSutskever. Improving language understanding by generative\npre-training. 2018. 2\n[26] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and\nVaishaal Shankar. Do imagenet classifiers generalize to im-\nagenet? In International Conference on Machine Learning,\npages 5389-5400. PMLR, 2019. 6, 7\n[27] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon\nShlens, Pieter Abbeel, and Ashish Vaswani. Bottle-\nneck transformers for visual recognition. arXiv preprint\narXiv:2101.11605, 2021. 3\n[28] Zhiqing Sun, Shengcao Cao, Yiming Yang, and Kris Kitani.\nRethinking transformer-based set prediction for object detec-\ntion. arXiv preprint arXiv:2011.10881, 2020. 2\n[29] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model\nscaling for convolutional neural networks. In International\nConference on Machine Learning, pages 6105-6114. PMLR,\n2019. 2\n[30] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herve Jegou. Training\ndata-efficient image transformers & distillation through at-\ntention. arXiv preprint arXiv:2012.12877, 2020. 1, 2, 4, 6,\n7\n[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Isabelle Guyon,\nUlrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob\nFergus, S. V. N. Vishwanathan, and Roman Garnett, editors,\nAdvances in Neural Information Processing Systems 30: An-\nnual Conference on Neural Information Processing Systems\n2017, December 4-9, 2017, Long Beach, CA, USA, pages\n5998-6008, 2017. 1, 2\n[32] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill,\nOmer Levy, and Samuel R. Bowman. GLUE: A multi-task\nbenchmark and analysis platform for natural language un-\nderstanding. In 7th International Conference on Learning\nRepresentations, ICLR 2019, New Orleans, LA, USA, May\n6-9, 2019. OpenReview.net, 2019. 1\n[33] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille,\nand Liang-Chieh Chen. Max-deeplab: End-to-end panop-\ntic segmentation with mask transformers. arXiv preprint\narXiv:2012.00759, 2020. 2\n[34] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao.\nPyramid vision transformer: A versatile backbone for\ndense prediction without convolutions. arXiv preprint\narXiv:2102.12122, 2021. 1, 3, 4, 5, 6, 7"
        },
        {
            "bounding_box": [
                {
                    "x": 1278,
                    "y": 303
                },
                {
                    "x": 2286,
                    "y": 303
                },
                {
                    "x": 2286,
                    "y": 2454
                },
                {
                    "x": 1278,
                    "y": 2454
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='117' style='font-size:18px'>[35] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-<br>ing He. Non-local neural networks. In Proceedings of the<br>IEEE conference on computer vision and pattern recogni-<br>tion, pages 7794-7803, 2018. 3<br>[36] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen,<br>Baoshan Cheng, Hao Shen, and Huaxia Xia. End-to-<br>end video instance segmentation with transformers. arXiv<br>preprint arXiv:2011.14503, 2020. 2<br>[37] Yujing Wang, Yaming Yang, Jiangang Bai, Mingliang<br>Zhang, Jing Bai, Jing Yu, Ce Zhang, Gao Huang, and Yunhai<br>Tong. Evolving attention with residual convolutions. arXiv<br>preprint arXiv:2102.12895, 2021. 3<br>[38] Felix Wu, Angela Fan, Alexei Baevski, Yann N Dauphin,<br>and Michael Auli. Pay less attention with lightweight and dy-<br>namic convolutions. arXiv preprint arXiv: 1901.10430, 2019.<br>3<br>[39] Zhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, and Song<br>Han. Lite transformer with long-short range attention. arXiv<br>preprint arXiv:2004. 11886, 2020. 3, 4<br>[40] Fuzhi Yang, Huan Yang, Jianlong Fu, Hongtao Lu, and Bain-<br>ing Guo. Learning texture transformer network for image<br>super-resolution. In Proceedings of the IEEE/CVF Confer-<br>ence on Computer Vision and Pattern Recognition, pages<br>5791-5800, 2020. 2<br>[41] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,<br>Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-<br>to-token vit: Training vision transformers from scratch on<br>imagenet. arXiv preprint arXiv:2101.11986, 2021. 1, 3, 4,<br>5, 6, 7<br>[42] Yanhong Zeng, Jianlong Fu, and Hongyang Chao. Learning<br>joint spatial-temporal transformations for video inpainting.<br>In European Conference on Computer Vision, pages 528-<br>543. Springer, 2020. 2<br>[43] Minghang Zheng, Peng Gao, Xiaogang Wang, Hongsheng<br>Li, and Hao Dong. End-to-end object detection with adaptive<br>clustering transformer. arXiv preprint arXiv:2011.09315,<br>2020. 2<br>[44] Luowei Zhou, Yingbo Zhou, Jason J. Corso, Richard Socher,<br>and Caiming Xiong. End-to-end dense video captioning with<br>masked transformer. In Proceedings of the IEEE Conference<br>on Computer Vision and Pattern Recognition (CVPR), June<br>2018. 2<br>[45] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang<br>Wang, and Jifeng Dai. Deformable detr: Deformable trans-<br>formers for end-to-end object detection. arXiv preprint<br>arXiv:2010.04159, 2020. 2</p>",
            "id": 117,
            "page": 10,
            "text": "[35] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-\ning He. Non-local neural networks. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 7794-7803, 2018. 3\n[36] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen,\nBaoshan Cheng, Hao Shen, and Huaxia Xia. End-to-\nend video instance segmentation with transformers. arXiv\npreprint arXiv:2011.14503, 2020. 2\n[37] Yujing Wang, Yaming Yang, Jiangang Bai, Mingliang\nZhang, Jing Bai, Jing Yu, Ce Zhang, Gao Huang, and Yunhai\nTong. Evolving attention with residual convolutions. arXiv\npreprint arXiv:2102.12895, 2021. 3\n[38] Felix Wu, Angela Fan, Alexei Baevski, Yann N Dauphin,\nand Michael Auli. Pay less attention with lightweight and dy-\nnamic convolutions. arXiv preprint arXiv: 1901.10430, 2019.\n3\n[39] Zhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, and Song\nHan. Lite transformer with long-short range attention. arXiv\npreprint arXiv:2004. 11886, 2020. 3, 4\n[40] Fuzhi Yang, Huan Yang, Jianlong Fu, Hongtao Lu, and Bain-\ning Guo. Learning texture transformer network for image\nsuper-resolution. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pages\n5791-5800, 2020. 2\n[41] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\nFrancis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-\nto-token vit: Training vision transformers from scratch on\nimagenet. arXiv preprint arXiv:2101.11986, 2021. 1, 3, 4,\n5, 6, 7\n[42] Yanhong Zeng, Jianlong Fu, and Hongyang Chao. Learning\njoint spatial-temporal transformations for video inpainting.\nIn European Conference on Computer Vision, pages 528-\n543. Springer, 2020. 2\n[43] Minghang Zheng, Peng Gao, Xiaogang Wang, Hongsheng\nLi, and Hao Dong. End-to-end object detection with adaptive\nclustering transformer. arXiv preprint arXiv:2011.09315,\n2020. 2\n[44] Luowei Zhou, Yingbo Zhou, Jason J. Corso, Richard Socher,\nand Caiming Xiong. End-to-end dense video captioning with\nmasked transformer. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), June\n2018. 2\n[45] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang\nWang, and Jifeng Dai. Deformable detr: Deformable trans-\nformers for end-to-end object detection. arXiv preprint\narXiv:2010.04159, 2020. 2"
        },
        {
            "bounding_box": [
                {
                    "x": 1220,
                    "y": 3054
                },
                {
                    "x": 1264,
                    "y": 3054
                },
                {
                    "x": 1264,
                    "y": 3091
                },
                {
                    "x": 1220,
                    "y": 3091
                }
            ],
            "category": "footer",
            "html": "<footer id='118' style='font-size:18px'>10</footer>",
            "id": 118,
            "page": 10,
            "text": "10"
        }
    ]
}