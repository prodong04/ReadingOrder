{
    "id": "32a1bf1e-0f94-11ef-8828-426932df3dcf",
    "pdf_path": "/root/data/pdf/1804.07461v3.pdf",
    "elements": [
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 109
                },
                {
                    "x": 1224,
                    "y": 109
                },
                {
                    "x": 1224,
                    "y": 158
                },
                {
                    "x": 444,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='0' style='font-size:16px'>Published as a conference paper at ICLR 2019</header>",
            "id": 0,
            "page": 1,
            "text": "Published as a conference paper at ICLR 2019"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 328
                },
                {
                    "x": 2108,
                    "y": 328
                },
                {
                    "x": 2108,
                    "y": 568
                },
                {
                    "x": 442,
                    "y": 568
                }
            ],
            "category": "paragraph",
            "html": "<p id='1' style='font-size:22px'>GLUE: A MULTI-TASK BENCHMARK AND ANALYSIS<br>PLATFORM FOR NATURAL LANGUAGE UNDERSTAND-<br>ING</p>",
            "id": 1,
            "page": 1,
            "text": "GLUE: A MULTI-TASK BENCHMARK AND ANALYSIS PLATFORM FOR NATURAL LANGUAGE UNDERSTANDING"
        },
        {
            "bounding_box": [
                {
                    "x": 468,
                    "y": 648
                },
                {
                    "x": 1563,
                    "y": 648
                },
                {
                    "x": 1563,
                    "y": 742
                },
                {
                    "x": 468,
                    "y": 742
                }
            ],
            "category": "paragraph",
            "html": "<p id='2' style='font-size:18px'>Alex Wang1, Amanpreet Singh1, Julian Michael2, Felix Hill3 ·<br>Omer Levy2 & Samuel R. Bowman1</p>",
            "id": 2,
            "page": 1,
            "text": "Alex Wang1, Amanpreet Singh1, Julian Michael2, Felix Hill3 · Omer Levy2 & Samuel R. Bowman1"
        },
        {
            "bounding_box": [
                {
                    "x": 474,
                    "y": 744
                },
                {
                    "x": 1914,
                    "y": 744
                },
                {
                    "x": 1914,
                    "y": 884
                },
                {
                    "x": 474,
                    "y": 884
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='3' style='font-size:18px'>1Courant Institute of Mathematical Sciences, New York University<br>2Paul G. Allen School of Computer Science & Engineering, University of Washington<br>3DeepMind</p>",
            "id": 3,
            "page": 1,
            "text": "1Courant Institute of Mathematical Sciences, New York University 2Paul G. Allen School of Computer Science & Engineering, University of Washington 3DeepMind"
        },
        {
            "bounding_box": [
                {
                    "x": 477,
                    "y": 890
                },
                {
                    "x": 1391,
                    "y": 890
                },
                {
                    "x": 1391,
                    "y": 1024
                },
                {
                    "x": 477,
                    "y": 1024
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='4' style='font-size:16px'>{alexwang, amanpreet, bowman}@nyu · edu<br>{julian jm, omerlevy}@cs · washington · edu<br>felixhill@google · com</p>",
            "id": 4,
            "page": 1,
            "text": "{alexwang, amanpreet, bowman}@nyu · edu {julian jm, omerlevy}@cs · washington · edu felixhill@google · com"
        },
        {
            "bounding_box": [
                {
                    "x": 1154,
                    "y": 1143
                },
                {
                    "x": 1396,
                    "y": 1143
                },
                {
                    "x": 1396,
                    "y": 1194
                },
                {
                    "x": 1154,
                    "y": 1194
                }
            ],
            "category": "paragraph",
            "html": "<p id='5' style='font-size:20px'>ABSTRACT</p>",
            "id": 5,
            "page": 1,
            "text": "ABSTRACT"
        },
        {
            "bounding_box": [
                {
                    "x": 590,
                    "y": 1237
                },
                {
                    "x": 1961,
                    "y": 1237
                },
                {
                    "x": 1961,
                    "y": 1795
                },
                {
                    "x": 590,
                    "y": 1795
                }
            ],
            "category": "paragraph",
            "html": "<p id='6' style='font-size:16px'>For natural language understanding (NLU) technology to be maximally useful, it<br>must be able to process language in a way that is not exclusive to a single task,<br>genre, or dataset. In pursuit of this objective, we introduce the General Language<br>Understanding Evaluation (GLUE) benchmark, a collection of tools for evaluat-<br>ing the performance of models across a diverse set of existing NLU tasks. By<br>including tasks with limited training data, GLUE is designed to favor and encour-<br>age models that share general linguistic knowledge across tasks. GLUE also in-<br>cludes a hand-crafted diagnostic test suite that enables detailed linguistic analysis<br>of models. We evaluate baselines based on current methods for transfer and rep-<br>resentation learning and find that multi-task training on all tasks performs better<br>than training a separate model per task. However, the low absolute performance<br>of our best model indicates the need for improved general NLU systems.</p>",
            "id": 6,
            "page": 1,
            "text": "For natural language understanding (NLU) technology to be maximally useful, it must be able to process language in a way that is not exclusive to a single task, genre, or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation (GLUE) benchmark, a collection of tools for evaluating the performance of models across a diverse set of existing NLU tasks. By including tasks with limited training data, GLUE is designed to favor and encourage models that share general linguistic knowledge across tasks. GLUE also includes a hand-crafted diagnostic test suite that enables detailed linguistic analysis of models. We evaluate baselines based on current methods for transfer and representation learning and find that multi-task training on all tasks performs better than training a separate model per task. However, the low absolute performance of our best model indicates the need for improved general NLU systems."
        },
        {
            "bounding_box": [
                {
                    "x": 449,
                    "y": 1867
                },
                {
                    "x": 864,
                    "y": 1867
                },
                {
                    "x": 864,
                    "y": 1920
                },
                {
                    "x": 449,
                    "y": 1920
                }
            ],
            "category": "paragraph",
            "html": "<p id='7' style='font-size:20px'>1 INTRODUCTION</p>",
            "id": 7,
            "page": 1,
            "text": "1 INTRODUCTION"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1968
                },
                {
                    "x": 2109,
                    "y": 1968
                },
                {
                    "x": 2109,
                    "y": 2200
                },
                {
                    "x": 442,
                    "y": 2200
                }
            ],
            "category": "paragraph",
            "html": "<p id='8' style='font-size:16px'>The human ability to understand language is general, flexible, and robust. In contrast, most NLU<br>models above the word level are designed for a specific task and struggle with out-of-domain data. If<br>we aspire to develop models with understanding beyond the detection of superficial correspondences<br>between inputs and outputs, then it is critical to develop a more unified model that can learn to<br>execute a range of different linguistic tasks in different domains.</p>",
            "id": 8,
            "page": 1,
            "text": "The human ability to understand language is general, flexible, and robust. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a more unified model that can learn to execute a range of different linguistic tasks in different domains."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2222
                },
                {
                    "x": 2108,
                    "y": 2222
                },
                {
                    "x": 2108,
                    "y": 2773
                },
                {
                    "x": 441,
                    "y": 2773
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='9' style='font-size:16px'>To facilitate research in this direction, we present the General Language Understanding Evaluation<br>(GLUE) benchmark: a collection of NLU tasks including question answering, sentiment analysis,<br>and textual entailment, and an associated online platform for model evaluation, comparison, and<br>analysis. GLUE does not place any constraints on model architecture beyond the ability to process<br>single-sentence and sentence-pair inputs and to make corresponding predictions. For some GLUE<br>tasks, training data is plentiful, but for others it is limited or fails to match the genre of the test set.<br>GLUE therefore favors models that can learn to represent linguistic knowledge in a way that facil-<br>itates sample-efficient learning and effective knowledge-transfer across tasks. None of the datasets<br>in GLUE were created from scratch for the benchmark; we rely on preexisting datasets because they<br>have been implicitly agreed upon by the NLP community as challenging and interesting. Four of<br>the datasets feature privately-held test data, which will be used to ensure that the benchmark is used<br>fairly. 1</p>",
            "id": 9,
            "page": 1,
            "text": "To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE) benchmark: a collection of NLU tasks including question answering, sentiment analysis, and textual entailment, and an associated online platform for model evaluation, comparison, and analysis. GLUE does not place any constraints on model architecture beyond the ability to process single-sentence and sentence-pair inputs and to make corresponding predictions. For some GLUE tasks, training data is plentiful, but for others it is limited or fails to match the genre of the test set. GLUE therefore favors models that can learn to represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. None of the datasets in GLUE were created from scratch for the benchmark; we rely on preexisting datasets because they have been implicitly agreed upon by the NLP community as challenging and interesting. Four of the datasets feature privately-held test data, which will be used to ensure that the benchmark is used fairly. 1"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2793
                },
                {
                    "x": 2108,
                    "y": 2793
                },
                {
                    "x": 2108,
                    "y": 2983
                },
                {
                    "x": 442,
                    "y": 2983
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='10' style='font-size:16px'>To understand the types of knowledge learned by models and to encourage linguistic-meaningful<br>solution strategies, GLUE also includes a set of hand-crafted analysis examples for probing trained<br>models. This dataset is designed to highlight common challenges, such as the use of world knowl-<br>edge and logical operators, that we expect models must handle to robustly solve the tasks.</p>",
            "id": 10,
            "page": 1,
            "text": "To understand the types of knowledge learned by models and to encourage linguistic-meaningful solution strategies, GLUE also includes a set of hand-crafted analysis examples for probing trained models. This dataset is designed to highlight common challenges, such as the use of world knowledge and logical operators, that we expect models must handle to robustly solve the tasks."
        },
        {
            "bounding_box": [
                {
                    "x": 501,
                    "y": 3006
                },
                {
                    "x": 2029,
                    "y": 3006
                },
                {
                    "x": 2029,
                    "y": 3053
                },
                {
                    "x": 501,
                    "y": 3053
                }
            ],
            "category": "paragraph",
            "html": "<p id='11' style='font-size:14px'>1To evaluate on the private test data, users of the benchmark must submit to gluebenchmark · com</p>",
            "id": 11,
            "page": 1,
            "text": "1To evaluate on the private test data, users of the benchmark must submit to gluebenchmark · com"
        },
        {
            "bounding_box": [
                {
                    "x": 60,
                    "y": 887
                },
                {
                    "x": 146,
                    "y": 887
                },
                {
                    "x": 146,
                    "y": 2328
                },
                {
                    "x": 60,
                    "y": 2328
                }
            ],
            "category": "footer",
            "html": "<br><footer id='12' style='font-size:14px'>2019<br>Feb<br>22<br>[cs.CL]<br>arXiv:1804.07461v3</footer>",
            "id": 12,
            "page": 1,
            "text": "2019 Feb 22 [cs.CL] arXiv:1804.07461v3"
        },
        {
            "bounding_box": [
                {
                    "x": 1261,
                    "y": 3133
                },
                {
                    "x": 1290,
                    "y": 3133
                },
                {
                    "x": 1290,
                    "y": 3173
                },
                {
                    "x": 1261,
                    "y": 3173
                }
            ],
            "category": "footer",
            "html": "<footer id='13' style='font-size:14px'>1</footer>",
            "id": 13,
            "page": 1,
            "text": "1"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 111
                },
                {
                    "x": 1225,
                    "y": 111
                },
                {
                    "x": 1225,
                    "y": 156
                },
                {
                    "x": 445,
                    "y": 156
                }
            ],
            "category": "header",
            "html": "<header id='14' style='font-size:14px'>Published as a conference paper at ICLR 2019</header>",
            "id": 14,
            "page": 2,
            "text": "Published as a conference paper at ICLR 2019"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 335
                },
                {
                    "x": 2107,
                    "y": 335
                },
                {
                    "x": 2107,
                    "y": 1032
                },
                {
                    "x": 445,
                    "y": 1032
                }
            ],
            "category": "table",
            "html": "<table id='15' style='font-size:14px'><tr><td>Corpus</td><td>Train</td><td>|Test|</td><td>Task</td><td>Metrics</td><td>Domain</td></tr><tr><td colspan=\"6\">Single-Sentence Tasks</td></tr><tr><td>CoLA</td><td>8.5k</td><td>1k</td><td>acceptability</td><td>Matthews corr.</td><td>misc.</td></tr><tr><td>SST-2</td><td>67k</td><td>1.8k</td><td>sentiment</td><td>acc.</td><td>movie reviews</td></tr><tr><td colspan=\"6\">Similarity and Paraphrase Tasks</td></tr><tr><td>MRPC</td><td>3.7k</td><td>1.7k</td><td>paraphrase</td><td>acc./F1</td><td>news</td></tr><tr><td>STS-B</td><td>7k</td><td>1.4k</td><td>sentence similarity</td><td>Pearson/Spearman corr.</td><td>misc.</td></tr><tr><td>QQP</td><td>364k</td><td>391k</td><td>paraphrase</td><td>acc./F1</td><td>social QA questions</td></tr><tr><td colspan=\"6\">Inference Tasks</td></tr><tr><td>MNLI</td><td>393k</td><td>20k</td><td>NLI</td><td>matched acc./mismatched acc.</td><td>misc.</td></tr><tr><td>QNLI</td><td>105k</td><td>5.4k</td><td>QA/NLI</td><td>acc.</td><td>Wikipedia</td></tr><tr><td>RTE</td><td>2.5k</td><td>3k</td><td>NLI</td><td>acc.</td><td>news, Wikipedia</td></tr><tr><td>WNLI</td><td>634</td><td>146</td><td>coreference/NLI</td><td>acc.</td><td>fiction books</td></tr></table>",
            "id": 15,
            "page": 2,
            "text": "Corpus Train |Test| Task Metrics Domain  Single-Sentence Tasks  CoLA 8.5k 1k acceptability Matthews corr. misc.  SST-2 67k 1.8k sentiment acc. movie reviews  Similarity and Paraphrase Tasks  MRPC 3.7k 1.7k paraphrase acc./F1 news  STS-B 7k 1.4k sentence similarity Pearson/Spearman corr. misc.  QQP 364k 391k paraphrase acc./F1 social QA questions  Inference Tasks  MNLI 393k 20k NLI matched acc./mismatched acc. misc.  QNLI 105k 5.4k QA/NLI acc. Wikipedia  RTE 2.5k 3k NLI acc. news, Wikipedia  WNLI 634 146 coreference/NLI acc."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1068
                },
                {
                    "x": 2106,
                    "y": 1068
                },
                {
                    "x": 2106,
                    "y": 1209
                },
                {
                    "x": 442,
                    "y": 1209
                }
            ],
            "category": "paragraph",
            "html": "<p id='16' style='font-size:14px'>Table 1: Task descriptions and statistics. All tasks are single sentence or sentence pair classification,<br>except STS-B, which is a regression task. MNLI has three classes; all other classification tasks have<br>two. Test sets shown in bold use labels that have never been made public in any form.</p>",
            "id": 16,
            "page": 2,
            "text": "Table 1: Task descriptions and statistics. All tasks are single sentence or sentence pair classification, except STS-B, which is a regression task. MNLI has three classes; all other classification tasks have two. Test sets shown in bold use labels that have never been made public in any form."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1295
                },
                {
                    "x": 2107,
                    "y": 1295
                },
                {
                    "x": 2107,
                    "y": 1571
                },
                {
                    "x": 441,
                    "y": 1571
                }
            ],
            "category": "paragraph",
            "html": "<p id='17' style='font-size:18px'>To better understand the challenged posed by GLUE, we conduct experiments with simple baselines<br>and state-of-the-art sentence representation models. We find that unified multi-task trained models<br>slightly outperform comparable models trained on each task separately. Our best multi-task model<br>makes use of ELMo (Peters et al., 2018), a recently proposed pre-training technique. However, this<br>model still achieves a fairly low absolute score. Analysis with our diagnostic dataset reveals that our<br>baseline models deal well with strong lexical signals but struggle with deeper logical structure.</p>",
            "id": 17,
            "page": 2,
            "text": "To better understand the challenged posed by GLUE, we conduct experiments with simple baselines and state-of-the-art sentence representation models. We find that unified multi-task trained models slightly outperform comparable models trained on each task separately. Our best multi-task model makes use of ELMo (Peters , 2018), a recently proposed pre-training technique. However, this model still achieves a fairly low absolute score. Analysis with our diagnostic dataset reveals that our baseline models deal well with strong lexical signals but struggle with deeper logical structure."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1595
                },
                {
                    "x": 2108,
                    "y": 1595
                },
                {
                    "x": 2108,
                    "y": 1872
                },
                {
                    "x": 441,
                    "y": 1872
                }
            ],
            "category": "paragraph",
            "html": "<p id='18' style='font-size:16px'>In summary, we offer: (i) A suite of nine sentence or sentence-pair NLU tasks, built on established<br>annotated datasets and selected to cover a diverse range of text genres, dataset sizes, and degrees<br>of difficulty. (ii) An online evaluation platform and leaderboard, based primarily on privately-held<br>test data. The platform is model-agnostic, and can evaluate any method capable of producing results<br>on all nine tasks. (iii) An expert-constructed diagnostic evaluation dataset. (iv) Baseline results for<br>several major existing approaches to sentence representation learning.</p>",
            "id": 18,
            "page": 2,
            "text": "In summary, we offer: (i) A suite of nine sentence or sentence-pair NLU tasks, built on established annotated datasets and selected to cover a diverse range of text genres, dataset sizes, and degrees of difficulty. (ii) An online evaluation platform and leaderboard, based primarily on privately-held test data. The platform is model-agnostic, and can evaluate any method capable of producing results on all nine tasks. (iii) An expert-constructed diagnostic evaluation dataset. (iv) Baseline results for several major existing approaches to sentence representation learning."
        },
        {
            "bounding_box": [
                {
                    "x": 446,
                    "y": 1939
                },
                {
                    "x": 883,
                    "y": 1939
                },
                {
                    "x": 883,
                    "y": 1988
                },
                {
                    "x": 446,
                    "y": 1988
                }
            ],
            "category": "paragraph",
            "html": "<p id='19' style='font-size:22px'>2 RELATED WORK</p>",
            "id": 19,
            "page": 2,
            "text": "2 RELATED WORK"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2043
                },
                {
                    "x": 2107,
                    "y": 2043
                },
                {
                    "x": 2107,
                    "y": 2273
                },
                {
                    "x": 441,
                    "y": 2273
                }
            ],
            "category": "paragraph",
            "html": "<p id='20' style='font-size:20px'>Collobert et al. (2011) used a multi-task model with a shared sentence understanding component to<br>jointly learn POS tagging, chunking, named entity recognition, and semantic role labeling. More<br>recent work has explored using labels from core NLP tasks to supervise training of lower levels<br>of deep neural networks (S⌀gaard & Goldberg, 2016; Hashimoto et al., 2017) and automatically<br>learning cross-task sharing mechanisms for multi-task learning (Ruder et al., 2017).</p>",
            "id": 20,
            "page": 2,
            "text": "Collobert  (2011) used a multi-task model with a shared sentence understanding component to jointly learn POS tagging, chunking, named entity recognition, and semantic role labeling. More recent work has explored using labels from core NLP tasks to supervise training of lower levels of deep neural networks (S⌀gaard & Goldberg, 2016; Hashimoto , 2017) and automatically learning cross-task sharing mechanisms for multi-task learning (Ruder , 2017)."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2296
                },
                {
                    "x": 2108,
                    "y": 2296
                },
                {
                    "x": 2108,
                    "y": 2937
                },
                {
                    "x": 441,
                    "y": 2937
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='21' style='font-size:18px'>Beyond multi-task learning, much work in developing general NLU systems has focused on<br>sentence-to-vector encoders (Le & Mikolov, 2014; Kiros et al., 2015, i.a.), leveraging unlabeled<br>data (Hill et al., 2016; Peters et al., 2018), labeled data (Conneau & Kiela, 2018; McCann et al.,<br>2017), and combinations of these (Collobert et al., 2011; Subramanian et al., 2018). In this line<br>of work, a standard evaluation practice has emerged, recently codified as SentEval (Conneau et al.,<br>2017; Conneau & Kiela, 2018). Like GLUE, SentEval relies on a set of existing classification tasks<br>involving either one or two sentences as inputs. Unlike GLUE, SentEval only evaluates sentence-<br>to-vector encoders, making it well-suited for evaluating models on tasks involving sentences in<br>isolation. However, cross-sentence contextualization and alignment are instrumental in achieving<br>state-of-the-art performance on tasks such as machine translation (Bahdanau et al., 2015; Vaswani<br>et al., 2017), question answering (Seo et al., 2017), and natural language inference (Rockt�schel<br>et al., 2016). GLUE is designed to facilitate the development of these methods: Itis model-agnostic,<br>allowing for any kind of representation or contextualization, including models that use no explicit<br>vector or symbolic representations for sentences whatsoever.</p>",
            "id": 21,
            "page": 2,
            "text": "Beyond multi-task learning, much work in developing general NLU systems has focused on sentence-to-vector encoders (Le & Mikolov, 2014; Kiros , 2015, i.a.), leveraging unlabeled data (Hill , 2016; Peters , 2018), labeled data (Conneau & Kiela, 2018; McCann , 2017), and combinations of these (Collobert , 2011; Subramanian , 2018). In this line of work, a standard evaluation practice has emerged, recently codified as SentEval (Conneau , 2017; Conneau & Kiela, 2018). Like GLUE, SentEval relies on a set of existing classification tasks involving either one or two sentences as inputs. Unlike GLUE, SentEval only evaluates sentenceto-vector encoders, making it well-suited for evaluating models on tasks involving sentences in isolation. However, cross-sentence contextualization and alignment are instrumental in achieving state-of-the-art performance on tasks such as machine translation (Bahdanau , 2015; Vaswani , 2017), question answering (Seo , 2017), and natural language inference (Rockt�schel , 2016). GLUE is designed to facilitate the development of these methods: Itis model-agnostic, allowing for any kind of representation or contextualization, including models that use no explicit vector or symbolic representations for sentences whatsoever."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2958
                },
                {
                    "x": 2106,
                    "y": 2958
                },
                {
                    "x": 2106,
                    "y": 3054
                },
                {
                    "x": 442,
                    "y": 3054
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='22' style='font-size:16px'>GLUE also diverges from SentEval in the selection of evaluation tasks that are included in the suite.<br>Many of the SentEval tasks are closely related to sentiment analysis, such as MR (Pang & Lee,</p>",
            "id": 22,
            "page": 2,
            "text": "GLUE also diverges from SentEval in the selection of evaluation tasks that are included in the suite. Many of the SentEval tasks are closely related to sentiment analysis, such as MR (Pang & Lee,"
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3135
                },
                {
                    "x": 1289,
                    "y": 3135
                },
                {
                    "x": 1289,
                    "y": 3170
                },
                {
                    "x": 1260,
                    "y": 3170
                }
            ],
            "category": "footer",
            "html": "<footer id='23' style='font-size:16px'>2</footer>",
            "id": 23,
            "page": 2,
            "text": "2"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 110
                },
                {
                    "x": 1224,
                    "y": 110
                },
                {
                    "x": 1224,
                    "y": 157
                },
                {
                    "x": 444,
                    "y": 157
                }
            ],
            "category": "header",
            "html": "<header id='24' style='font-size:14px'>Published as a conference paper at ICLR 2019</header>",
            "id": 24,
            "page": 3,
            "text": "Published as a conference paper at ICLR 2019"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 345
                },
                {
                    "x": 2108,
                    "y": 345
                },
                {
                    "x": 2108,
                    "y": 530
                },
                {
                    "x": 441,
                    "y": 530
                }
            ],
            "category": "paragraph",
            "html": "<p id='25' style='font-size:18px'>2005), SST (Socher et al., 2013), CR (Hu & Liu, 2004), and SUBJ (Pang & Lee, 2004). Other<br>tasks are SO close to being solved that evaluation on them is relatively uninformative, such as MPQA<br>(Wiebe et al., 2005) and TREC question classification (Voorhees et al., 1999). In GLUE, we attempt<br>to construct a benchmark that is both diverse and difficult.</p>",
            "id": 25,
            "page": 3,
            "text": "2005), SST (Socher , 2013), CR (Hu & Liu, 2004), and SUBJ (Pang & Lee, 2004). Other tasks are SO close to being solved that evaluation on them is relatively uninformative, such as MPQA (Wiebe , 2005) and TREC question classification (Voorhees , 1999). In GLUE, we attempt to construct a benchmark that is both diverse and difficult."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 553
                },
                {
                    "x": 2108,
                    "y": 553
                },
                {
                    "x": 2108,
                    "y": 925
                },
                {
                    "x": 441,
                    "y": 925
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='26' style='font-size:18px'>McCann et al. (2018) introduce decaNLP, which also scores NLP systems based on their perfor-<br>mance on multiple datasets. Their benchmark recasts the ten evaluation tasks as question answer-<br>ing, converting tasks like summarization and text-to-SQL semantic parsing into question answering<br>using automatic transformations. That benchmark lacks the leaderboard and error analysis toolkit of<br>GLUE, but more importantly, we see it as pursuing a more ambitious but less immediately practical<br>goal: While GLUE rewards methods that yield good performance on a circumscribed set of tasks<br>using methods like those that are currently used for those tasks, their benchmark rewards systems<br>that make progress toward their goal of unifying all of NLU under the rubric of question answering.</p>",
            "id": 26,
            "page": 3,
            "text": "McCann  (2018) introduce decaNLP, which also scores NLP systems based on their performance on multiple datasets. Their benchmark recasts the ten evaluation tasks as question answering, converting tasks like summarization and text-to-SQL semantic parsing into question answering using automatic transformations. That benchmark lacks the leaderboard and error analysis toolkit of GLUE, but more importantly, we see it as pursuing a more ambitious but less immediately practical goal: While GLUE rewards methods that yield good performance on a circumscribed set of tasks using methods like those that are currently used for those tasks, their benchmark rewards systems that make progress toward their goal of unifying all of NLU under the rubric of question answering."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 991
                },
                {
                    "x": 671,
                    "y": 991
                },
                {
                    "x": 671,
                    "y": 1042
                },
                {
                    "x": 444,
                    "y": 1042
                }
            ],
            "category": "paragraph",
            "html": "<p id='27' style='font-size:22px'>3 TASKS</p>",
            "id": 27,
            "page": 3,
            "text": "3 TASKS"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1093
                },
                {
                    "x": 2109,
                    "y": 1093
                },
                {
                    "x": 2109,
                    "y": 1510
                },
                {
                    "x": 441,
                    "y": 1510
                }
            ],
            "category": "paragraph",
            "html": "<p id='28' style='font-size:18px'>GLUE is centered on nine English sentence understanding tasks, which cover a broad range of do-<br>mains, data quantities, and difficulties. As the goal of GLUE is to spur development of generalizable<br>NLU systems, we design the benchmark such that good performance should require a model to share<br>substantial knowledge (e.g., trained parameters) across all tasks, while still maintaining some task-<br>specific components. Though it is possible to train a single model for each task with no pretraining<br>or other outside sources of knowledge and evaluate the resulting set of models on this benchmark,<br>we expect that our inclusion of several data-scarce tasks will ultimately render this approach un-<br>competitive. We describe the tasks below and in Table 1. Appendix A includes additional details.<br>Unless otherwise mentioned, tasks are evaluated on accuracy and are balanced across classes.</p>",
            "id": 28,
            "page": 3,
            "text": "GLUE is centered on nine English sentence understanding tasks, which cover a broad range of domains, data quantities, and difficulties. As the goal of GLUE is to spur development of generalizable NLU systems, we design the benchmark such that good performance should require a model to share substantial knowledge (e.g., trained parameters) across all tasks, while still maintaining some taskspecific components. Though it is possible to train a single model for each task with no pretraining or other outside sources of knowledge and evaluate the resulting set of models on this benchmark, we expect that our inclusion of several data-scarce tasks will ultimately render this approach uncompetitive. We describe the tasks below and in Table 1. Appendix A includes additional details. Unless otherwise mentioned, tasks are evaluated on accuracy and are balanced across classes."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 1566
                },
                {
                    "x": 1024,
                    "y": 1566
                },
                {
                    "x": 1024,
                    "y": 1614
                },
                {
                    "x": 444,
                    "y": 1614
                }
            ],
            "category": "paragraph",
            "html": "<p id='29' style='font-size:20px'>3.1 SINGLE-SENTENCE TASKS</p>",
            "id": 29,
            "page": 3,
            "text": "3.1 SINGLE-SENTENCE TASKS"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1651
                },
                {
                    "x": 2107,
                    "y": 1651
                },
                {
                    "x": 2107,
                    "y": 2021
                },
                {
                    "x": 443,
                    "y": 2021
                }
            ],
            "category": "paragraph",
            "html": "<p id='30' style='font-size:18px'>CoLA The Corpus of Linguistic Acceptability (Warstadt et al., 2018) consists of English accept-<br>ability judgments drawn from books and journal articles on linguistic theory. Each example is a<br>sequence of words annotated with whether it is a grammatical English sentence. Following the au-<br>thors, we use Matthews correlation coefficient (Matthews, 1975) as the evaluation metric, which<br>evaluates performance on unbalanced binary classification and ranges from -1 to 1, with 0 being the<br>performance of uninformed guessing. We use the standard test set, for which we obtained private<br>labels from the authors. We report a single performance number on the combination of the in- and<br>out-of-domain sections of the test set.</p>",
            "id": 30,
            "page": 3,
            "text": "CoLA The Corpus of Linguistic Acceptability (Warstadt , 2018) consists of English acceptability judgments drawn from books and journal articles on linguistic theory. Each example is a sequence of words annotated with whether it is a grammatical English sentence. Following the authors, we use Matthews correlation coefficient (Matthews, 1975) as the evaluation metric, which evaluates performance on unbalanced binary classification and ranges from -1 to 1, with 0 being the performance of uninformed guessing. We use the standard test set, for which we obtained private labels from the authors. We report a single performance number on the combination of the in- and out-of-domain sections of the test set."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2070
                },
                {
                    "x": 2108,
                    "y": 2070
                },
                {
                    "x": 2108,
                    "y": 2214
                },
                {
                    "x": 441,
                    "y": 2214
                }
            ],
            "category": "paragraph",
            "html": "<p id='31' style='font-size:18px'>SST-2 The Stanford Sentiment Treebank (Socher et al., 2013) consists of sentences from movie<br>reviews and human annotations of their sentiment. The task is to predict the sentiment of a given<br>sentence. We use the two-way (positivelnegative) class split, and use only sentence-level labels.</p>",
            "id": 31,
            "page": 3,
            "text": "SST-2 The Stanford Sentiment Treebank (Socher , 2013) consists of sentences from movie reviews and human annotations of their sentiment. The task is to predict the sentiment of a given sentence. We use the two-way (positivelnegative) class split, and use only sentence-level labels."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 2268
                },
                {
                    "x": 1236,
                    "y": 2268
                },
                {
                    "x": 1236,
                    "y": 2318
                },
                {
                    "x": 445,
                    "y": 2318
                }
            ],
            "category": "paragraph",
            "html": "<p id='32' style='font-size:16px'>3.2 SIMILARITY AND PARAPHRASE TASKS</p>",
            "id": 32,
            "page": 3,
            "text": "3.2 SIMILARITY AND PARAPHRASE TASKS"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2356
                },
                {
                    "x": 2107,
                    "y": 2356
                },
                {
                    "x": 2107,
                    "y": 2543
                },
                {
                    "x": 441,
                    "y": 2543
                }
            ],
            "category": "paragraph",
            "html": "<p id='33' style='font-size:18px'>MRPC The Microsoft Research Paraphrase Corpus (Dolan & Brockett, 2005) is a corpus of sen-<br>tence pairs automatically extracted from online news sources, with human annotations for whether<br>the sentences in the pair are semantically equivalent. Because the classes are imbalanced (68%<br>positive), we follow common practice and report both accuracy and F1 score.</p>",
            "id": 33,
            "page": 3,
            "text": "MRPC The Microsoft Research Paraphrase Corpus (Dolan & Brockett, 2005) is a corpus of sentence pairs automatically extracted from online news sources, with human annotations for whether the sentences in the pair are semantically equivalent. Because the classes are imbalanced (68% positive), we follow common practice and report both accuracy and F1 score."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2590
                },
                {
                    "x": 2107,
                    "y": 2590
                },
                {
                    "x": 2107,
                    "y": 2827
                },
                {
                    "x": 441,
                    "y": 2827
                }
            ],
            "category": "paragraph",
            "html": "<p id='34' style='font-size:18px'>QQP The Quora Question Pairs2 dataset is a collection of question pairs from the community<br>question-answering website Quora. The task is to determine whether a pair of questions are seman-<br>tically equivalent. As in MRPC, the class distribution in QQP is unbalanced (63% negative), SO we<br>report both accuracy and F1 score. We use the standard test set, for which we obtained private labels<br>from the authors. We observe that the test set has a different label distribution than the training set.</p>",
            "id": 34,
            "page": 3,
            "text": "QQP The Quora Question Pairs2 dataset is a collection of question pairs from the community question-answering website Quora. The task is to determine whether a pair of questions are semantically equivalent. As in MRPC, the class distribution in QQP is unbalanced (63% negative), SO we report both accuracy and F1 score. We use the standard test set, for which we obtained private labels from the authors. We observe that the test set has a different label distribution than the training set."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2875
                },
                {
                    "x": 2107,
                    "y": 2875
                },
                {
                    "x": 2107,
                    "y": 2973
                },
                {
                    "x": 442,
                    "y": 2973
                }
            ],
            "category": "paragraph",
            "html": "<p id='35' style='font-size:18px'>STS-B The Semantic Textual Similarity Benchmark (Cer et al., 2017) is a collection of sentence<br>pairs drawn from news headlines, video and image captions, and natural language inference data.</p>",
            "id": 35,
            "page": 3,
            "text": "STS-B The Semantic Textual Similarity Benchmark (Cer , 2017) is a collection of sentence pairs drawn from news headlines, video and image captions, and natural language inference data."
        },
        {
            "bounding_box": [
                {
                    "x": 497,
                    "y": 3008
                },
                {
                    "x": 1809,
                    "y": 3008
                },
                {
                    "x": 1809,
                    "y": 3053
                },
                {
                    "x": 497,
                    "y": 3053
                }
            ],
            "category": "paragraph",
            "html": "<p id='36' style='font-size:14px'>2<br>data · quora · com/First-Quora-Dataset-Re lease-Question-Pairs</p>",
            "id": 36,
            "page": 3,
            "text": "2 data · quora · com/First-Quora-Dataset-Re lease-Question-Pairs"
        },
        {
            "bounding_box": [
                {
                    "x": 1261,
                    "y": 3132
                },
                {
                    "x": 1288,
                    "y": 3132
                },
                {
                    "x": 1288,
                    "y": 3171
                },
                {
                    "x": 1261,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='37' style='font-size:18px'>3</footer>",
            "id": 37,
            "page": 3,
            "text": "3"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 111
                },
                {
                    "x": 1224,
                    "y": 111
                },
                {
                    "x": 1224,
                    "y": 157
                },
                {
                    "x": 445,
                    "y": 157
                }
            ],
            "category": "header",
            "html": "<header id='38' style='font-size:16px'>Published as a conference paper at ICLR 2019</header>",
            "id": 38,
            "page": 4,
            "text": "Published as a conference paper at ICLR 2019"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 337
                },
                {
                    "x": 2115,
                    "y": 337
                },
                {
                    "x": 2115,
                    "y": 909
                },
                {
                    "x": 442,
                    "y": 909
                }
            ],
            "category": "table",
            "html": "<table id='39' style='font-size:22px'><tr><td>Coarse-Grained Categories</td><td>Fine-Grained Categories</td></tr><tr><td>Lexical Semantics</td><td>Lexical Entailment, Morphological Negation, Factivity, Symmetry/Collectivity, Redundancy, Named Entities, Quantifiers</td></tr><tr><td>Predicate-Argument Structure</td><td>Core Arguments, Prepositional Phrases, Ellipsis/Implicits, Anaphora/Coreference Active/Passive, Nominalization, Genitives/Partitives, Datives, Relative Clauses, Coordination Scope, Intersectivity, Restrictivity</td></tr><tr><td>Logic</td><td>Negation, Double Negation, Intervals/Numbers, Conjunction, Disjunction, Conditionals, Universal, Existential, Temporal, Upward Monotone, Downward Monotone, Non-Monotone</td></tr><tr><td>Knowledge</td><td>Common Sense, World Knowledge</td></tr></table>",
            "id": 39,
            "page": 4,
            "text": "Coarse-Grained Categories Fine-Grained Categories  Lexical Semantics Lexical Entailment, Morphological Negation, Factivity, Symmetry/Collectivity, Redundancy, Named Entities, Quantifiers  Predicate-Argument Structure Core Arguments, Prepositional Phrases, Ellipsis/Implicits, Anaphora/Coreference Active/Passive, Nominalization, Genitives/Partitives, Datives, Relative Clauses, Coordination Scope, Intersectivity, Restrictivity  Logic Negation, Double Negation, Intervals/Numbers, Conjunction, Disjunction, Conditionals, Universal, Existential, Temporal, Upward Monotone, Downward Monotone, Non-Monotone  Knowledge"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 943
                },
                {
                    "x": 2105,
                    "y": 943
                },
                {
                    "x": 2105,
                    "y": 1039
                },
                {
                    "x": 442,
                    "y": 1039
                }
            ],
            "category": "paragraph",
            "html": "<p id='40' style='font-size:22px'>Table 2: The types of linguistic phenomena annotated in the diagnostic dataset, organized under four<br>major categories. For a description of each phenomenon, see Appendix E.</p>",
            "id": 40,
            "page": 4,
            "text": "Table 2: The types of linguistic phenomena annotated in the diagnostic dataset, organized under four major categories. For a description of each phenomenon, see Appendix E."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1122
                },
                {
                    "x": 2104,
                    "y": 1122
                },
                {
                    "x": 2104,
                    "y": 1217
                },
                {
                    "x": 442,
                    "y": 1217
                }
            ],
            "category": "paragraph",
            "html": "<p id='41' style='font-size:18px'>Each pair is human-annotated with a similarity score from 1 to 5; the task is to predict these scores.<br>Follow common practice, we evaluate using Pearson and Spearman correlation coefficients.</p>",
            "id": 41,
            "page": 4,
            "text": "Each pair is human-annotated with a similarity score from 1 to 5; the task is to predict these scores. Follow common practice, we evaluate using Pearson and Spearman correlation coefficients."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 1272
                },
                {
                    "x": 885,
                    "y": 1272
                },
                {
                    "x": 885,
                    "y": 1318
                },
                {
                    "x": 445,
                    "y": 1318
                }
            ],
            "category": "paragraph",
            "html": "<p id='42' style='font-size:20px'>3.3 INFERENCE TASKS</p>",
            "id": 42,
            "page": 4,
            "text": "3.3 INFERENCE TASKS"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 1355
                },
                {
                    "x": 2107,
                    "y": 1355
                },
                {
                    "x": 2107,
                    "y": 1727
                },
                {
                    "x": 444,
                    "y": 1727
                }
            ],
            "category": "paragraph",
            "html": "<p id='43' style='font-size:20px'>MNLI The Multi-Genre Natural Language Inference Corpus (Williams et al., 2018) is a crowd-<br>sourced collection of sentence pairs with textual entailment annotations. Given a premise sentence<br>and a hypothesis sentence, the task is to predict whether the premise entails the hypothesis (entail-<br>ment), contradicts the hypothesis (contradiction), or neither (neutral). The premise sentences are<br>gathered from ten different sources, including transcribed speech, fiction, and government reports.<br>We use the standard test set, for which we obtained private labels from the authors, and evaluate<br>on both the matched (in-domain) and mismatched (cross-domain) sections. We also use and recom-<br>mend the SNLI corpus (Bowman et al., 2015) as 550k examples of auxiliary training data.</p>",
            "id": 43,
            "page": 4,
            "text": "MNLI The Multi-Genre Natural Language Inference Corpus (Williams , 2018) is a crowdsourced collection of sentence pairs with textual entailment annotations. Given a premise sentence and a hypothesis sentence, the task is to predict whether the premise entails the hypothesis (entailment), contradicts the hypothesis (contradiction), or neither (neutral). The premise sentences are gathered from ten different sources, including transcribed speech, fiction, and government reports. We use the standard test set, for which we obtained private labels from the authors, and evaluate on both the matched (in-domain) and mismatched (cross-domain) sections. We also use and recommend the SNLI corpus (Bowman , 2015) as 550k examples of auxiliary training data."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1773
                },
                {
                    "x": 2107,
                    "y": 1773
                },
                {
                    "x": 2107,
                    "y": 2283
                },
                {
                    "x": 442,
                    "y": 2283
                }
            ],
            "category": "paragraph",
            "html": "<p id='44' style='font-size:20px'>QNLI The Stanford Question Answering Dataset (Rajpurkar et al. 2016) is a question-answering<br>dataset consisting of question-paragraph pairs, where one of the sentences in the paragraph (drawn<br>from Wikipedia) contains the answer to the corresponding question (written by an annotator). We<br>convert the task into sentence pair classification by forming a pair between each question and each<br>sentence in the corresponding context, and filtering out pairs with low lexical overlap between the<br>question and the context sentence. The task is to determine whether the context sentence contains<br>the answer to the question. This modified version of the original task removes the requirement that<br>the model select the exact answer, but also removes the simplifying assumptions that the answer<br>is always present in the input and that lexical overlap is a reliable cue. This process of recasting<br>existing datasets into NLI is similar to methods introduced in White et al. (2017) and expanded<br>upon in Demszky et al. (2018). We call the converted dataset QNLI (Question-answering NLI).3</p>",
            "id": 44,
            "page": 4,
            "text": "QNLI The Stanford Question Answering Dataset (Rajpurkar  2016) is a question-answering dataset consisting of question-paragraph pairs, where one of the sentences in the paragraph (drawn from Wikipedia) contains the answer to the corresponding question (written by an annotator). We convert the task into sentence pair classification by forming a pair between each question and each sentence in the corresponding context, and filtering out pairs with low lexical overlap between the question and the context sentence. The task is to determine whether the context sentence contains the answer to the question. This modified version of the original task removes the requirement that the model select the exact answer, but also removes the simplifying assumptions that the answer is always present in the input and that lexical overlap is a reliable cue. This process of recasting existing datasets into NLI is similar to methods introduced in White  (2017) and expanded upon in Demszky  (2018). We call the converted dataset QNLI (Question-answering NLI).3"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2328
                },
                {
                    "x": 2107,
                    "y": 2328
                },
                {
                    "x": 2107,
                    "y": 2563
                },
                {
                    "x": 441,
                    "y": 2563
                }
            ],
            "category": "paragraph",
            "html": "<p id='45' style='font-size:20px'>RTE The Recognizing Textual Entailment (RTE) datasets come from a series of annual textual<br>entailment challenges. We combine the data from RTE1 (Dagan et al., 2006), RTE2 (Bar Haim<br>et al., 2006), RTE3 (Giampiccolo et al., 2007), and RTE5 (Bentivogli et al., 2009). 4 Examples are<br>constructed based on news and Wikipedia text. We convert all datasets to a two-class split, where<br>for three-class datasets we collapse neutral and contradiction into not_entailment, for consistency.</p>",
            "id": 45,
            "page": 4,
            "text": "RTE The Recognizing Textual Entailment (RTE) datasets come from a series of annual textual entailment challenges. We combine the data from RTE1 (Dagan , 2006), RTE2 (Bar Haim , 2006), RTE3 (Giampiccolo , 2007), and RTE5 (Bentivogli , 2009). 4 Examples are constructed based on news and Wikipedia text. We convert all datasets to a two-class split, where for three-class datasets we collapse neutral and contradiction into not_entailment, for consistency."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2609
                },
                {
                    "x": 2108,
                    "y": 2609
                },
                {
                    "x": 2108,
                    "y": 2889
                },
                {
                    "x": 442,
                    "y": 2889
                }
            ],
            "category": "paragraph",
            "html": "<p id='46' style='font-size:20px'>WNLI The Winograd Schema Challenge (Levesque et al., 2011) is a reading comprehension task<br>in which a system must read a sentence with a pronoun and select the referent of that pronoun from<br>a list of choices. The examples are manually constructed to foil simple statistical methods: Each<br>one is contingent on contextual information provided by a single word or phrase in the sentence.<br>To convert the problem into sentence pair classification, we construct sentence pairs by replacing<br>the ambiguous pronoun with each possible referent. The task is to predict if the sentence with the</p>",
            "id": 46,
            "page": 4,
            "text": "WNLI The Winograd Schema Challenge (Levesque , 2011) is a reading comprehension task in which a system must read a sentence with a pronoun and select the referent of that pronoun from a list of choices. The examples are manually constructed to foil simple statistical methods: Each one is contingent on contextual information provided by a single word or phrase in the sentence. To convert the problem into sentence pair classification, we construct sentence pairs by replacing the ambiguous pronoun with each possible referent. The task is to predict if the sentence with the"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2919
                },
                {
                    "x": 2105,
                    "y": 2919
                },
                {
                    "x": 2105,
                    "y": 2997
                },
                {
                    "x": 443,
                    "y": 2997
                }
            ],
            "category": "paragraph",
            "html": "<p id='47' style='font-size:14px'>3 An earlier release of QNLI had an artifact where the task could be modeled and solved as an easier task<br>than we describe here. We have since released an updated version of QNLI that removes this possibility.</p>",
            "id": 47,
            "page": 4,
            "text": "3 An earlier release of QNLI had an artifact where the task could be modeled and solved as an easier task than we describe here. We have since released an updated version of QNLI that removes this possibility."
        },
        {
            "bounding_box": [
                {
                    "x": 494,
                    "y": 3009
                },
                {
                    "x": 1818,
                    "y": 3009
                },
                {
                    "x": 1818,
                    "y": 3049
                },
                {
                    "x": 494,
                    "y": 3049
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='48' style='font-size:14px'>4RTE4 is not publicly available, while RTE6 and RTE7 do not fit the standard NLI task.</p>",
            "id": 48,
            "page": 4,
            "text": "4RTE4 is not publicly available, while RTE6 and RTE7 do not fit the standard NLI task."
        },
        {
            "bounding_box": [
                {
                    "x": 1258,
                    "y": 3135
                },
                {
                    "x": 1289,
                    "y": 3135
                },
                {
                    "x": 1289,
                    "y": 3168
                },
                {
                    "x": 1258,
                    "y": 3168
                }
            ],
            "category": "footer",
            "html": "<footer id='49' style='font-size:16px'>4</footer>",
            "id": 49,
            "page": 4,
            "text": "4"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 111
                },
                {
                    "x": 1225,
                    "y": 111
                },
                {
                    "x": 1225,
                    "y": 157
                },
                {
                    "x": 444,
                    "y": 157
                }
            ],
            "category": "header",
            "html": "<header id='50' style='font-size:14px'>Published as a conference paper at ICLR 2019</header>",
            "id": 50,
            "page": 5,
            "text": "Published as a conference paper at ICLR 2019"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 335
                },
                {
                    "x": 2107,
                    "y": 335
                },
                {
                    "x": 2107,
                    "y": 883
                },
                {
                    "x": 442,
                    "y": 883
                }
            ],
            "category": "table",
            "html": "<table id='51' style='font-size:14px'><tr><td>Tags</td><td>Sentence 1</td><td>Sentence 2</td><td>Fwd</td><td>Bwd</td></tr><tr><td>Lexical Entailment (Lexi- cal Semantics), Downward Monotone (Logic)</td><td>The timing of the meeting has not been set, according to a Starbucks spokesper- son.</td><td>The timing of the meet- ing has not been consid- ered, according to a Star- bucks spokesperson.</td><td>N</td><td>E</td></tr><tr><td>Universal Quantifiers (Logic)</td><td>Our deepest sympathies are with all those affected by this accident.</td><td>Our deepest sympathies are with a victim who was af- fected by this accident.</td><td>E</td><td>N</td></tr><tr><td>Quantifiers (Lexical Se- mantics), Double Negation (Logic)</td><td>I have never seen a hum- mingbird not flying.</td><td>I have never seen a hum- mingbird.</td><td>N</td><td>E</td></tr></table>",
            "id": 51,
            "page": 5,
            "text": "Tags Sentence 1 Sentence 2 Fwd Bwd  Lexical Entailment (Lexi- cal Semantics), Downward Monotone (Logic) The timing of the meeting has not been set, according to a Starbucks spokesper- son. The timing of the meet- ing has not been consid- ered, according to a Star- bucks spokesperson. N E  Universal Quantifiers (Logic) Our deepest sympathies are with all those affected by this accident. Our deepest sympathies are with a victim who was af- fected by this accident. E N  Quantifiers (Lexical Se- mantics), Double Negation (Logic) I have never seen a hum- mingbird not flying. I have never seen a hum- mingbird. N"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 921
                },
                {
                    "x": 2107,
                    "y": 921
                },
                {
                    "x": 2107,
                    "y": 1109
                },
                {
                    "x": 442,
                    "y": 1109
                }
            ],
            "category": "paragraph",
            "html": "<p id='52' style='font-size:20px'>Table 3: Examples from the diagnostic set. Fwd (resp. Bwd) denotes the label when sentence 1<br>(resp. sentence 2) is the premise. Labels are entailment (E), neutral (N), or contradiction (C).<br>Examples are tagged with the phenomena they demonstrate, and each phenomenon belongs to one<br>of four broad categories (in parentheses).</p>",
            "id": 52,
            "page": 5,
            "text": "Table 3: Examples from the diagnostic set. Fwd (resp. Bwd) denotes the label when sentence 1 (resp. sentence 2) is the premise. Labels are entailment (E), neutral (N), or contradiction (C). Examples are tagged with the phenomena they demonstrate, and each phenomenon belongs to one of four broad categories (in parentheses)."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1205
                },
                {
                    "x": 2108,
                    "y": 1205
                },
                {
                    "x": 2108,
                    "y": 1618
                },
                {
                    "x": 442,
                    "y": 1618
                }
            ],
            "category": "paragraph",
            "html": "<p id='53' style='font-size:18px'>pronoun substituted is entailed by the original sentence. We use a small evaluation set consisting of<br>new examples derived from fiction books5 that was shared privately by the authors of the original<br>corpus. While the included training set is balanced between two classes, the test set is imbalanced<br>between them (65% not entailment). Also, due to a data quirk, the development set is adversarial:<br>hypotheses are sometimes shared between training and development examples, so if a model mem-<br>orizes the training examples, they will predict the wrong label on corresponding development set<br>example. As with QNLI, each example is evaluated separately, SO there is not a systematic corre-<br>spondence between a model's score on this task and its score on the unconverted original task. We<br>call converted dataset WNLI (Winograd NLI).</p>",
            "id": 53,
            "page": 5,
            "text": "pronoun substituted is entailed by the original sentence. We use a small evaluation set consisting of new examples derived from fiction books5 that was shared privately by the authors of the original corpus. While the included training set is balanced between two classes, the test set is imbalanced between them (65% not entailment). Also, due to a data quirk, the development set is adversarial: hypotheses are sometimes shared between training and development examples, so if a model memorizes the training examples, they will predict the wrong label on corresponding development set example. As with QNLI, each example is evaluated separately, SO there is not a systematic correspondence between a model's score on this task and its score on the unconverted original task. We call converted dataset WNLI (Winograd NLI)."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 1687
                },
                {
                    "x": 786,
                    "y": 1687
                },
                {
                    "x": 786,
                    "y": 1734
                },
                {
                    "x": 444,
                    "y": 1734
                }
            ],
            "category": "paragraph",
            "html": "<p id='54' style='font-size:16px'>3.4 EVALUATION</p>",
            "id": 54,
            "page": 5,
            "text": "3.4 EVALUATION"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1777
                },
                {
                    "x": 2107,
                    "y": 1777
                },
                {
                    "x": 2107,
                    "y": 2101
                },
                {
                    "x": 442,
                    "y": 2101
                }
            ],
            "category": "paragraph",
            "html": "<p id='55' style='font-size:16px'>The GLUE benchmark follows the same evaluation model as SemEval and Kaggle. To evaluate<br>a system on the benchmark, one must run the system on the provided test data for the tasks, then<br>upload the results to the website gluebenchmark · com for scoring. The benchmark site shows<br>per-task scores and a macro-average of those scores to determine a system's position on the leader-<br>board. For tasks with multiple metrics (e.g., accuracy and F1), we use an unweighted average of<br>the metrics as the score for the task when computing the overall macro-average. The website also<br>provides fine- and coarse-grained results on the diagnostic dataset. See Appendix D for details.</p>",
            "id": 55,
            "page": 5,
            "text": "The GLUE benchmark follows the same evaluation model as SemEval and Kaggle. To evaluate a system on the benchmark, one must run the system on the provided test data for the tasks, then upload the results to the website gluebenchmark · com for scoring. The benchmark site shows per-task scores and a macro-average of those scores to determine a system's position on the leaderboard. For tasks with multiple metrics (e.g., accuracy and F1), we use an unweighted average of the metrics as the score for the task when computing the overall macro-average. The website also provides fine- and coarse-grained results on the diagnostic dataset. See Appendix D for details."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 2179
                },
                {
                    "x": 1014,
                    "y": 2179
                },
                {
                    "x": 1014,
                    "y": 2232
                },
                {
                    "x": 445,
                    "y": 2232
                }
            ],
            "category": "paragraph",
            "html": "<p id='56' style='font-size:22px'>4 DIAGNOSTIC DATASET</p>",
            "id": 56,
            "page": 5,
            "text": "4 DIAGNOSTIC DATASET"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2290
                },
                {
                    "x": 2108,
                    "y": 2290
                },
                {
                    "x": 2108,
                    "y": 2520
                },
                {
                    "x": 441,
                    "y": 2520
                }
            ],
            "category": "paragraph",
            "html": "<p id='57' style='font-size:20px'>Drawing inspiration from the FraCaS suite (Cooper et al., 1996) and the recent Build-It-Break-It<br>competition (Ettinger et al., 2017), we include a small, manually-curated test set for the analysis of<br>system performance. While the main benchmark mostly reflects an application-driven distribution<br>of examples, our diagnostic dataset highlights a pre-defined set of phenomena that we believe are<br>interesting and important for models to capture. We show the full set of phenomena in Table 2.</p>",
            "id": 57,
            "page": 5,
            "text": "Drawing inspiration from the FraCaS suite (Cooper , 1996) and the recent Build-It-Break-It competition (Ettinger , 2017), we include a small, manually-curated test set for the analysis of system performance. While the main benchmark mostly reflects an application-driven distribution of examples, our diagnostic dataset highlights a pre-defined set of phenomena that we believe are interesting and important for models to capture. We show the full set of phenomena in Table 2."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2543
                },
                {
                    "x": 2107,
                    "y": 2543
                },
                {
                    "x": 2107,
                    "y": 2910
                },
                {
                    "x": 443,
                    "y": 2910
                }
            ],
            "category": "paragraph",
            "html": "<p id='58' style='font-size:20px'>Each diagnostic example is an NLI sentence pair with tags for the phenomena demonstrated. The<br>NLI task is well-suited to this kind of analysis, as it can easily evaluate the full set of skills involved<br>in (ungrounded) sentence understanding, from resolution of syntactic ambiguity to pragmatic rea-<br>soning with world knowledge. We ensure the data is reasonably diverse by producing examples for<br>a variety of linguistic phenomena and basing our examples on naturally-occurring sentences from<br>several domains (news, Reddit, Wikipedia, academic papers). This approaches differs from that of<br>FraCaS, which was designed to test linguistic theories with a minimal and uniform set of examples.<br>A sample from our dataset is shown in Table 3.</p>",
            "id": 58,
            "page": 5,
            "text": "Each diagnostic example is an NLI sentence pair with tags for the phenomena demonstrated. The NLI task is well-suited to this kind of analysis, as it can easily evaluate the full set of skills involved in (ungrounded) sentence understanding, from resolution of syntactic ambiguity to pragmatic reasoning with world knowledge. We ensure the data is reasonably diverse by producing examples for a variety of linguistic phenomena and basing our examples on naturally-occurring sentences from several domains (news, Reddit, Wikipedia, academic papers). This approaches differs from that of FraCaS, which was designed to test linguistic theories with a minimal and uniform set of examples. A sample from our dataset is shown in Table 3."
        },
        {
            "bounding_box": [
                {
                    "x": 446,
                    "y": 2966
                },
                {
                    "x": 2102,
                    "y": 2966
                },
                {
                    "x": 2102,
                    "y": 3051
                },
                {
                    "x": 446,
                    "y": 3051
                }
            ],
            "category": "paragraph",
            "html": "<p id='59' style='font-size:14px'>5See similar examples at CS · nyu · edu / facul ty/ davise/papers/MinogradSchenas/<br>WS html</p>",
            "id": 59,
            "page": 5,
            "text": "5See similar examples at CS · nyu · edu / facul ty/ davise/papers/MinogradSchenas/ WS html"
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3134
                },
                {
                    "x": 1289,
                    "y": 3134
                },
                {
                    "x": 1289,
                    "y": 3171
                },
                {
                    "x": 1260,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='60' style='font-size:14px'>5</footer>",
            "id": 60,
            "page": 5,
            "text": "5"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 112
                },
                {
                    "x": 1224,
                    "y": 112
                },
                {
                    "x": 1224,
                    "y": 156
                },
                {
                    "x": 445,
                    "y": 156
                }
            ],
            "category": "header",
            "html": "<header id='61' style='font-size:14px'>Published as a conference paper at ICLR 2019</header>",
            "id": 61,
            "page": 6,
            "text": "Published as a conference paper at ICLR 2019"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 346
                },
                {
                    "x": 2107,
                    "y": 346
                },
                {
                    "x": 2107,
                    "y": 807
                },
                {
                    "x": 443,
                    "y": 807
                }
            ],
            "category": "paragraph",
            "html": "<p id='62' style='font-size:18px'>Annotation Process We begin with a target set of phenomena, based roughly on those used in the<br>FraCaS suite (Cooper et al., 1996). We construct each example by locating a sentence that can be<br>easily made to demonstrate a target phenomenon, and editing it in two ways to produce an appro-<br>priate sentence pair. We make minimal modifications SO as to maintain high lexical and structural<br>overlap within each sentence pair and limit superficial cues. We then label the inference relation-<br>ships between the sentences, considering each sentence alternatively as the premise, producing two<br>labeled examples for each pair (1 100 total). Where possible, we produce several pairs with differ-<br>ent labels for a single source sentence, to have minimal sets of sentence pairs that are lexically and<br>structurally very similar but correspond to different entailment relationships. The resulting labels<br>are 42% entailment, 35% neutral, and 23% contradiction.</p>",
            "id": 62,
            "page": 6,
            "text": "Annotation Process We begin with a target set of phenomena, based roughly on those used in the FraCaS suite (Cooper , 1996). We construct each example by locating a sentence that can be easily made to demonstrate a target phenomenon, and editing it in two ways to produce an appropriate sentence pair. We make minimal modifications SO as to maintain high lexical and structural overlap within each sentence pair and limit superficial cues. We then label the inference relationships between the sentences, considering each sentence alternatively as the premise, producing two labeled examples for each pair (1 100 total). Where possible, we produce several pairs with different labels for a single source sentence, to have minimal sets of sentence pairs that are lexically and structurally very similar but correspond to different entailment relationships. The resulting labels are 42% entailment, 35% neutral, and 23% contradiction."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 853
                },
                {
                    "x": 2104,
                    "y": 853
                },
                {
                    "x": 2104,
                    "y": 946
                },
                {
                    "x": 442,
                    "y": 946
                }
            ],
            "category": "paragraph",
            "html": "<p id='63' style='font-size:16px'>Evaluation Since the class distribution in the diagnostic set is not balanced, we use R3 (Gorodkin,<br>2004), a three-class generalization of the Matthews correlation coefficient, for evaluation.</p>",
            "id": 63,
            "page": 6,
            "text": "Evaluation Since the class distribution in the diagnostic set is not balanced, we use R3 (Gorodkin, 2004), a three-class generalization of the Matthews correlation coefficient, for evaluation."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 970
                },
                {
                    "x": 2106,
                    "y": 970
                },
                {
                    "x": 2106,
                    "y": 1288
                },
                {
                    "x": 443,
                    "y": 1288
                }
            ],
            "category": "paragraph",
            "html": "<p id='64' style='font-size:18px'>In light of recent work showing that crowdsourced data often contains artifacts which can be ex-<br>ploited to perform well without solving the intended task (Schwartz et al., 2017; Poliak et al., 2018;<br>Tsuchiya, 2018, i.a.), we audit the data for such artifacts. We reproduce the methodology of Guru-<br>rangan et al. (2018), training two fastText classifiers (Joulin et al., 2016) to predict entailment labels<br>on SNLI and MNLI using only the hypothesis as input. The models respectively get near-chance<br>accuracies of 32.7% and 36.4% on our diagnostic data, showing that the data does not suffer from<br>such artifacts.</p>",
            "id": 64,
            "page": 6,
            "text": "In light of recent work showing that crowdsourced data often contains artifacts which can be exploited to perform well without solving the intended task (Schwartz , 2017; Poliak , 2018; Tsuchiya, 2018, i.a.), we audit the data for such artifacts. We reproduce the methodology of Gururangan  (2018), training two fastText classifiers (Joulin , 2016) to predict entailment labels on SNLI and MNLI using only the hypothesis as input. The models respectively get near-chance accuracies of 32.7% and 36.4% on our diagnostic data, showing that the data does not suffer from such artifacts."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1314
                },
                {
                    "x": 2107,
                    "y": 1314
                },
                {
                    "x": 2107,
                    "y": 1500
                },
                {
                    "x": 441,
                    "y": 1500
                }
            ],
            "category": "paragraph",
            "html": "<p id='65' style='font-size:18px'>To establish human baseline performance on the diagnostic set, we have six NLP researchers anno-<br>tate 50 sentence pairs (100 entailment examples) randomly sampled from the diagnostic set. Inter-<br>annotator agreement is high, with a Fleiss's K of 0.73. The average R3 score among the annotators<br>is 0.80, much higher than any of the baseline systems described in Section 5.</p>",
            "id": 65,
            "page": 6,
            "text": "To establish human baseline performance on the diagnostic set, we have six NLP researchers annotate 50 sentence pairs (100 entailment examples) randomly sampled from the diagnostic set. Interannotator agreement is high, with a Fleiss's K of 0.73. The average R3 score among the annotators is 0.80, much higher than any of the baseline systems described in Section 5."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1547
                },
                {
                    "x": 2107,
                    "y": 1547
                },
                {
                    "x": 2107,
                    "y": 1824
                },
                {
                    "x": 441,
                    "y": 1824
                }
            ],
            "category": "paragraph",
            "html": "<p id='66' style='font-size:18px'>Intended Use The diagnostic examples are hand-picked to address certain phenomena, and NLI<br>is a task with no natural input distribution, SO we do not expect performance on the diagnostic set<br>to reflect overall performance or generalization in downstream applications. Performance on the<br>analysis set should be compared between models but not between categories. The set is provided<br>not as a benchmark, but as an analysis tool for error analysis, qualitative model comparison, and<br>development of adversarial examples.</p>",
            "id": 66,
            "page": 6,
            "text": "Intended Use The diagnostic examples are hand-picked to address certain phenomena, and NLI is a task with no natural input distribution, SO we do not expect performance on the diagnostic set to reflect overall performance or generalization in downstream applications. Performance on the analysis set should be compared between models but not between categories. The set is provided not as a benchmark, but as an analysis tool for error analysis, qualitative model comparison, and development of adversarial examples."
        },
        {
            "bounding_box": [
                {
                    "x": 447,
                    "y": 1888
                },
                {
                    "x": 772,
                    "y": 1888
                },
                {
                    "x": 772,
                    "y": 1938
                },
                {
                    "x": 447,
                    "y": 1938
                }
            ],
            "category": "paragraph",
            "html": "<p id='67' style='font-size:20px'>5 BASELINES</p>",
            "id": 67,
            "page": 6,
            "text": "5 BASELINES"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1988
                },
                {
                    "x": 2107,
                    "y": 1988
                },
                {
                    "x": 2107,
                    "y": 2219
                },
                {
                    "x": 442,
                    "y": 2219
                }
            ],
            "category": "paragraph",
            "html": "<p id='68' style='font-size:16px'>For baselines, we evaluate a multi-task learning model trained on the GLUE tasks, as well as several<br>variants based on recent pre-training methods. We briefly describe them here. See Appendix B for<br>details. We implement our models in the AllenNLP library (Gardner et al., 2017). Original code<br>for the baselines is available at https : / / github · com/ nyu-mll / GLUE-basel ines and a<br>newer version is available at https : / / github · com / jsalt18-sentence-repl / jiant.</p>",
            "id": 68,
            "page": 6,
            "text": "For baselines, we evaluate a multi-task learning model trained on the GLUE tasks, as well as several variants based on recent pre-training methods. We briefly describe them here. See Appendix B for details. We implement our models in the AllenNLP library (Gardner , 2017). Original code for the baselines is available at https : / / github · com/ nyu-mll / GLUE-basel ines and a newer version is available at https : / / github · com / jsalt18-sentence-repl / jiant."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2265
                },
                {
                    "x": 2106,
                    "y": 2265
                },
                {
                    "x": 2106,
                    "y": 2588
                },
                {
                    "x": 443,
                    "y": 2588
                }
            ],
            "category": "paragraph",
            "html": "<p id='69' style='font-size:18px'>Architecture Our simplest baseline architecture is based on sentence-to-vector encoders, and sets<br>aside GLUE's ability to evaluate models with more complex structures. Taking inspiration from<br>Conneau et al. (2017), the model uses a two-layer, 1500D (per direction) BiLSTM with max pooling<br>and 300D GloVe word embeddings (840B Common Crawl version; Pennington et al., 2014). For<br>single-sentence tasks, we encode the sentence and pass the resulting vector to a classifier. For<br>sentence-pair tasks, we encode sentences independently to produce vectors u, v, and pass [u; v; |u -<br>01; u * v] to a classifier. The classifier is an MLP with a 512D hidden layer.</p>",
            "id": 69,
            "page": 6,
            "text": "Architecture Our simplest baseline architecture is based on sentence-to-vector encoders, and sets aside GLUE's ability to evaluate models with more complex structures. Taking inspiration from Conneau  (2017), the model uses a two-layer, 1500D (per direction) BiLSTM with max pooling and 300D GloVe word embeddings (840B Common Crawl version; Pennington , 2014). For single-sentence tasks, we encode the sentence and pass the resulting vector to a classifier. For sentence-pair tasks, we encode sentences independently to produce vectors u, v, and pass [u; v; |u 01; u * v] to a classifier. The classifier is an MLP with a 512D hidden layer."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2613
                },
                {
                    "x": 2107,
                    "y": 2613
                },
                {
                    "x": 2107,
                    "y": 2795
                },
                {
                    "x": 443,
                    "y": 2795
                }
            ],
            "category": "paragraph",
            "html": "<p id='70' style='font-size:18px'>We also consider a variant of our model which for sentence pair tasks uses an attention mechanism<br>inspired by Seo et al. (2017) between all pairs of words, followed by a second BiLSTM with max<br>pooling. By explicitly modeling the interaction between sentences, these models fall outside the<br>sentence-to-vector paradigm.</p>",
            "id": 70,
            "page": 6,
            "text": "We also consider a variant of our model which for sentence pair tasks uses an attention mechanism inspired by Seo  (2017) between all pairs of words, followed by a second BiLSTM with max pooling. By explicitly modeling the interaction between sentences, these models fall outside the sentence-to-vector paradigm."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2843
                },
                {
                    "x": 2106,
                    "y": 2843
                },
                {
                    "x": 2106,
                    "y": 2935
                },
                {
                    "x": 442,
                    "y": 2935
                }
            ],
            "category": "paragraph",
            "html": "<p id='71' style='font-size:18px'>Pre-Training We augment our base model with two recent methods for pre-training: ELMo and<br>CoVe. We use existing trained models for both.</p>",
            "id": 71,
            "page": 6,
            "text": "Pre-Training We augment our base model with two recent methods for pre-training: ELMo and CoVe. We use existing trained models for both."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2960
                },
                {
                    "x": 2106,
                    "y": 2960
                },
                {
                    "x": 2106,
                    "y": 3053
                },
                {
                    "x": 443,
                    "y": 3053
                }
            ],
            "category": "paragraph",
            "html": "<p id='72' style='font-size:18px'>ELMo uses a pair of two-layer neural language models trained on the Billion Word Benchmark<br>(Chelba et al., 2013). Each word is represented by a contextual embedding, produced by taking a</p>",
            "id": 72,
            "page": 6,
            "text": "ELMo uses a pair of two-layer neural language models trained on the Billion Word Benchmark (Chelba , 2013). Each word is represented by a contextual embedding, produced by taking a"
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3136
                },
                {
                    "x": 1289,
                    "y": 3136
                },
                {
                    "x": 1289,
                    "y": 3172
                },
                {
                    "x": 1260,
                    "y": 3172
                }
            ],
            "category": "footer",
            "html": "<footer id='73' style='font-size:14px'>6</footer>",
            "id": 73,
            "page": 6,
            "text": "6"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 111
                },
                {
                    "x": 1224,
                    "y": 111
                },
                {
                    "x": 1224,
                    "y": 157
                },
                {
                    "x": 445,
                    "y": 157
                }
            ],
            "category": "header",
            "html": "<header id='74' style='font-size:14px'>Published as a conference paper at ICLR 2019</header>",
            "id": 74,
            "page": 7,
            "text": "Published as a conference paper at ICLR 2019"
        },
        {
            "bounding_box": [
                {
                    "x": 454,
                    "y": 339
                },
                {
                    "x": 2099,
                    "y": 339
                },
                {
                    "x": 2099,
                    "y": 1416
                },
                {
                    "x": 454,
                    "y": 1416
                }
            ],
            "category": "table",
            "html": "<table id='75' style='font-size:14px'><tr><td></td><td></td><td colspan=\"2\">Single Sentence</td><td colspan=\"3\">Similarity and Paraphrase</td><td colspan=\"4\">Natural Language Inference</td></tr><tr><td>Model</td><td>Avg</td><td>CoLA</td><td>SST-2</td><td>MRPC</td><td>QQP</td><td>STS-B</td><td>MNLI</td><td>QNLI</td><td>RTE</td><td>WNLI</td></tr><tr><td colspan=\"11\">Single-Task Training</td></tr><tr><td>BiLSTM</td><td>63.9</td><td>15.7</td><td>85.9</td><td>69.3/79.4</td><td>81.7/61.4</td><td>66.0/62.8</td><td>70.3/70.8</td><td>75.7</td><td>52.8</td><td>65.1</td></tr><tr><td>+ELMo</td><td>66.4</td><td>35.0</td><td>90.2</td><td>69.0/80.8</td><td>85.7/65.6</td><td>64.0/60.2</td><td>72.9/73.4</td><td>71.7</td><td>50.1</td><td>65.1</td></tr><tr><td>+CoVe</td><td>64.0</td><td>14.5</td><td>88.5</td><td>73.4/81.4</td><td>83.3/59.4</td><td>67.2/64.1</td><td>64.5/64.8</td><td>75.4</td><td>53.5</td><td>65.1</td></tr><tr><td>+Attn</td><td>63.9</td><td>15.7</td><td>85.9</td><td>68.5/80.3</td><td>83.5/62.9</td><td>59.3/55.8</td><td>74.2/73.8</td><td>77.2</td><td>51.9</td><td>65.1</td></tr><tr><td>+Attn, ELMo</td><td>66.5</td><td>35.0</td><td>90.2</td><td>68.8/80.2</td><td>86.5/66.1</td><td>55.5/52.5</td><td>76.9/76.7</td><td>76.7</td><td>50.4</td><td>65.1</td></tr><tr><td>+Attn, CoVe</td><td>63.2</td><td>14.5</td><td>88.5</td><td>68.6/79.7</td><td>84.1/60.1</td><td>57.2/53.6</td><td>71.6/71.5</td><td>74.5</td><td>52.7</td><td>65.1</td></tr><tr><td colspan=\"11\">Multi-Task Training</td></tr><tr><td>BiLSTM</td><td>64.2</td><td>11.6</td><td>82.8</td><td>74.3/81.8</td><td>84.2/62.5</td><td>70.3/67.8</td><td>65.4/66.1</td><td>74.6</td><td>57.4</td><td>65.1</td></tr><tr><td>+ELMo</td><td>67.7</td><td>32.1</td><td>89.3</td><td>78.0/84.7</td><td>82.6/61.1</td><td>67.2/67.9</td><td>70.3/67.8</td><td>75.5</td><td>57.4</td><td>65.1</td></tr><tr><td>+CoVe</td><td>62.9</td><td>18.5</td><td>81.9</td><td>71.5/78.7</td><td>84.9/60.6</td><td>64.4/62.7</td><td>65.4/65.7</td><td>70.8</td><td>52.7</td><td>65.1</td></tr><tr><td>+Attn</td><td>65.6</td><td>18.6</td><td>83.0</td><td>76.2/83.9</td><td>82.4/60.1</td><td>72.8/70.5</td><td>67.6/68.3</td><td>74.3</td><td>58.4</td><td>65.1</td></tr><tr><td>+Attn, ELMo</td><td>70.0</td><td>33.6</td><td>90.4</td><td>78.0/84.4</td><td>84.3/63.1</td><td>74.2/72.3</td><td>74.1/74.5</td><td>79.8</td><td>58.9</td><td>65.1</td></tr><tr><td>+Attn, CoVe</td><td>63.1</td><td>8.3</td><td>80.7</td><td>71.8/80.0</td><td>83.4/60.5</td><td>69.8/68.4</td><td>68.1/68.6</td><td>72.9</td><td>56.0</td><td>65.1</td></tr><tr><td colspan=\"11\">Pre-Trained Sentence Representation Models</td></tr><tr><td>CBoW</td><td>58.9</td><td>0.0</td><td>80.0</td><td>73.4/81.5</td><td>79.1/51.4</td><td>61.2/58.7</td><td>56.0/56.4</td><td>72.1</td><td>54.1</td><td>65.1</td></tr><tr><td>Skip-Thought</td><td>61.3</td><td>0.0</td><td>81.8</td><td>71.7/80.8</td><td>82.2/56.4</td><td>71.8/69.7</td><td>62.9/62.8</td><td>72.9</td><td>53.1</td><td>65.1</td></tr><tr><td>InferSent</td><td>63.9</td><td>4.5</td><td>85.1</td><td>74.1/81.2</td><td>81.7/59.1</td><td>75.9/75.3</td><td>66.1/65.7</td><td>72.7</td><td>58.0</td><td>65.1</td></tr><tr><td>DisSent</td><td>62.0</td><td>4.9</td><td>83.7</td><td>74.1/81.7</td><td>82.6/59.5</td><td>66.1/64.8</td><td>58.7/59.1</td><td>73.9</td><td>56.4</td><td>65.1</td></tr><tr><td>GenSen</td><td>66.2</td><td>7.7</td><td>83.1</td><td>76.6/83.0</td><td>82.9/59.8</td><td>79.3/79.2</td><td>71.4/71.3</td><td>78.6</td><td>59.2</td><td>65.1</td></tr></table>",
            "id": 75,
            "page": 7,
            "text": "Single Sentence Similarity and Paraphrase Natural Language Inference  Model Avg CoLA SST-2 MRPC QQP STS-B MNLI QNLI RTE WNLI  Single-Task Training  BiLSTM 63.9 15.7 85.9 69.3/79.4 81.7/61.4 66.0/62.8 70.3/70.8 75.7 52.8 65.1  +ELMo 66.4 35.0 90.2 69.0/80.8 85.7/65.6 64.0/60.2 72.9/73.4 71.7 50.1 65.1  +CoVe 64.0 14.5 88.5 73.4/81.4 83.3/59.4 67.2/64.1 64.5/64.8 75.4 53.5 65.1  +Attn 63.9 15.7 85.9 68.5/80.3 83.5/62.9 59.3/55.8 74.2/73.8 77.2 51.9 65.1  +Attn, ELMo 66.5 35.0 90.2 68.8/80.2 86.5/66.1 55.5/52.5 76.9/76.7 76.7 50.4 65.1  +Attn, CoVe 63.2 14.5 88.5 68.6/79.7 84.1/60.1 57.2/53.6 71.6/71.5 74.5 52.7 65.1  Multi-Task Training  BiLSTM 64.2 11.6 82.8 74.3/81.8 84.2/62.5 70.3/67.8 65.4/66.1 74.6 57.4 65.1  +ELMo 67.7 32.1 89.3 78.0/84.7 82.6/61.1 67.2/67.9 70.3/67.8 75.5 57.4 65.1  +CoVe 62.9 18.5 81.9 71.5/78.7 84.9/60.6 64.4/62.7 65.4/65.7 70.8 52.7 65.1  +Attn 65.6 18.6 83.0 76.2/83.9 82.4/60.1 72.8/70.5 67.6/68.3 74.3 58.4 65.1  +Attn, ELMo 70.0 33.6 90.4 78.0/84.4 84.3/63.1 74.2/72.3 74.1/74.5 79.8 58.9 65.1  +Attn, CoVe 63.1 8.3 80.7 71.8/80.0 83.4/60.5 69.8/68.4 68.1/68.6 72.9 56.0 65.1  Pre-Trained Sentence Representation Models  CBoW 58.9 0.0 80.0 73.4/81.5 79.1/51.4 61.2/58.7 56.0/56.4 72.1 54.1 65.1  Skip-Thought 61.3 0.0 81.8 71.7/80.8 82.2/56.4 71.8/69.7 62.9/62.8 72.9 53.1 65.1  InferSent 63.9 4.5 85.1 74.1/81.2 81.7/59.1 75.9/75.3 66.1/65.7 72.7 58.0 65.1  DisSent 62.0 4.9 83.7 74.1/81.7 82.6/59.5 66.1/64.8 58.7/59.1 73.9 56.4 65.1  GenSen 66.2 7.7 83.1 76.6/83.0 82.9/59.8 79.3/79.2 71.4/71.3 78.6 59.2"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1450
                },
                {
                    "x": 2107,
                    "y": 1450
                },
                {
                    "x": 2107,
                    "y": 1683
                },
                {
                    "x": 441,
                    "y": 1683
                }
            ],
            "category": "paragraph",
            "html": "<p id='76' style='font-size:16px'>Table 4: Baseline performance on the GLUE task test sets. For MNLI, we report accuracy on the<br>matched and mismatched test sets. For MRPC and Quora, we report accuracy and F1. For STS-B,<br>we report Pearson and Spearman correlation. For CoLA, we report Matthews correlation. For all<br>other tasks we report accuracy. All values are scaled by 100. A similar table is presented on the<br>online platform.</p>",
            "id": 76,
            "page": 7,
            "text": "Table 4: Baseline performance on the GLUE task test sets. For MNLI, we report accuracy on the matched and mismatched test sets. For MRPC and Quora, we report accuracy and F1. For STS-B, we report Pearson and Spearman correlation. For CoLA, we report Matthews correlation. For all other tasks we report accuracy. All values are scaled by 100. A similar table is presented on the online platform."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1793
                },
                {
                    "x": 2105,
                    "y": 1793
                },
                {
                    "x": 2105,
                    "y": 1888
                },
                {
                    "x": 441,
                    "y": 1888
                }
            ],
            "category": "paragraph",
            "html": "<p id='77' style='font-size:20px'>linear combination of the corresponding hidden states of each layer of the two models. We follow<br>the authors' recommendations6 and use ELMo embeddings in place of any other embeddings.</p>",
            "id": 77,
            "page": 7,
            "text": "linear combination of the corresponding hidden states of each layer of the two models. We follow the authors' recommendations6 and use ELMo embeddings in place of any other embeddings."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1910
                },
                {
                    "x": 2108,
                    "y": 1910
                },
                {
                    "x": 2108,
                    "y": 2050
                },
                {
                    "x": 441,
                    "y": 2050
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='78' style='font-size:18px'>CoVe (McCann et al., 2017) uses a two-layer BiLSTM encoder originally trained for English-to-<br>German translation. The CoVe vector of a word is the corresponding hidden state of the top-layer<br>LSTM. As in the original work, we concatenate the CoVe vectors to the GloVe word embeddings.</p>",
            "id": 78,
            "page": 7,
            "text": "CoVe (McCann , 2017) uses a two-layer BiLSTM encoder originally trained for English-toGerman translation. The CoVe vector of a word is the corresponding hidden state of the top-layer LSTM. As in the original work, we concatenate the CoVe vectors to the GloVe word embeddings."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2127
                },
                {
                    "x": 2109,
                    "y": 2127
                },
                {
                    "x": 2109,
                    "y": 2407
                },
                {
                    "x": 441,
                    "y": 2407
                }
            ],
            "category": "paragraph",
            "html": "<p id='79' style='font-size:18px'>Training We train our models with the BiLSTM sentence encoder and post-attention BiLSTMs<br>shared across tasks, and classifiers trained separately for each task. For each training update, we<br>sample a task to train with a probability proportional to the number of training examples for each<br>task. We train our models with Adam (Kingma & Ba, 2015) with initial learning rate 10-4 and<br>batch size 128. We use the macro-average score as the validation metric and stop training when the<br>learning rate drops below 10-5 or performance does not improve after 5 validation checks.</p>",
            "id": 79,
            "page": 7,
            "text": "Training We train our models with the BiLSTM sentence encoder and post-attention BiLSTMs shared across tasks, and classifiers trained separately for each task. For each training update, we sample a task to train with a probability proportional to the number of training examples for each task. We train our models with Adam (Kingma & Ba, 2015) with initial learning rate 10-4 and batch size 128. We use the macro-average score as the validation metric and stop training when the learning rate drops below 10-5 or performance does not improve after 5 validation checks."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2426
                },
                {
                    "x": 2108,
                    "y": 2426
                },
                {
                    "x": 2108,
                    "y": 2612
                },
                {
                    "x": 441,
                    "y": 2612
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='80' style='font-size:16px'>We also train a set of single-task models, which are configured and trained identically, but share no<br>parameters. To allow for fair comparisons with the multi-task analogs, we do not tune parameter or<br>training settings for each task, SO these single-task models do not generally represent the state of the<br>art for each task.</p>",
            "id": 80,
            "page": 7,
            "text": "We also train a set of single-task models, which are configured and trained identically, but share no parameters. To allow for fair comparisons with the multi-task analogs, we do not tune parameter or training settings for each task, SO these single-task models do not generally represent the state of the art for each task."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2690
                },
                {
                    "x": 2108,
                    "y": 2690
                },
                {
                    "x": 2108,
                    "y": 2924
                },
                {
                    "x": 441,
                    "y": 2924
                }
            ],
            "category": "paragraph",
            "html": "<p id='81' style='font-size:22px'>Sentence Representation Models Finally, we evaluate the following trained sentence-to-vector<br>encoder models using our benchmark: average bag-of-words using GloVe embeddings (CBoW),<br>Skip-Thought (Kiros et al., 2015), InferSent (Conneau et al., 2017), DisSent (Nie et al., 2017), and<br>GenSen (Subramanian et al., 2018). For these models, we only train task-specific classifiers on the<br>representations they produce.</p>",
            "id": 81,
            "page": 7,
            "text": "Sentence Representation Models Finally, we evaluate the following trained sentence-to-vector encoder models using our benchmark: average bag-of-words using GloVe embeddings (CBoW), Skip-Thought (Kiros , 2015), InferSent (Conneau , 2017), DisSent (Nie , 2017), and GenSen (Subramanian , 2018). For these models, we only train task-specific classifiers on the representations they produce."
        },
        {
            "bounding_box": [
                {
                    "x": 496,
                    "y": 3005
                },
                {
                    "x": 1950,
                    "y": 3005
                },
                {
                    "x": 1950,
                    "y": 3054
                },
                {
                    "x": 496,
                    "y": 3054
                }
            ],
            "category": "paragraph",
            "html": "<p id='82' style='font-size:20px'>'github. com/allean.klanding/hit/wastea/tractionstoveloveloa . md</p>",
            "id": 82,
            "page": 7,
            "text": "'github. com/allean.klanding/hit/wastea/tractionstoveloveloa . md"
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3134
                },
                {
                    "x": 1290,
                    "y": 3134
                },
                {
                    "x": 1290,
                    "y": 3170
                },
                {
                    "x": 1260,
                    "y": 3170
                }
            ],
            "category": "footer",
            "html": "<footer id='83' style='font-size:14px'>7</footer>",
            "id": 83,
            "page": 7,
            "text": "7"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 111
                },
                {
                    "x": 1225,
                    "y": 111
                },
                {
                    "x": 1225,
                    "y": 157
                },
                {
                    "x": 445,
                    "y": 157
                }
            ],
            "category": "header",
            "html": "<header id='84' style='font-size:14px'>Published as a conference paper at ICLR 2019</header>",
            "id": 84,
            "page": 8,
            "text": "Published as a conference paper at ICLR 2019"
        },
        {
            "bounding_box": [
                {
                    "x": 507,
                    "y": 337
                },
                {
                    "x": 2045,
                    "y": 337
                },
                {
                    "x": 2045,
                    "y": 1417
                },
                {
                    "x": 507,
                    "y": 1417
                }
            ],
            "category": "table",
            "html": "<table id='85' style='font-size:14px'><tr><td></td><td colspan=\"5\">Coarse-Grained</td><td colspan=\"6\">Fine-Grained</td></tr><tr><td>Model</td><td>All</td><td>LS</td><td>PAS</td><td>L</td><td>K</td><td>UQuant</td><td>MNeg</td><td>2Neg</td><td>Coref</td><td>Restr</td><td>Down</td></tr><tr><td colspan=\"12\">Single-Task Training</td></tr><tr><td>BiLSTM</td><td>21</td><td>25</td><td>24</td><td>16</td><td>16</td><td>70</td><td>53</td><td>4</td><td>21</td><td>-15</td><td>12</td></tr><tr><td>+ELMo</td><td>20</td><td>20</td><td>21</td><td>14</td><td>17</td><td>70</td><td>20</td><td>42</td><td>33</td><td>-26</td><td>-3</td></tr><tr><td>+CoVe</td><td>21</td><td>19</td><td>23</td><td>20</td><td>18</td><td>71</td><td>47</td><td>-1</td><td>33</td><td>-15</td><td>8</td></tr><tr><td>+Attn</td><td>25</td><td>24</td><td>30</td><td>20</td><td>14</td><td>50</td><td>47</td><td>21</td><td>38</td><td>-8</td><td>-3</td></tr><tr><td>+Attn, ELMo</td><td>28</td><td>30</td><td>35</td><td>23</td><td>14</td><td>85</td><td>20</td><td>42</td><td>33</td><td>-26</td><td>-3</td></tr><tr><td>+Attn, CoVe</td><td>24</td><td>29</td><td>29</td><td>18</td><td>12</td><td>77</td><td>50</td><td>1</td><td>18</td><td>-1</td><td>12</td></tr><tr><td colspan=\"12\">Multi-Task Training</td></tr><tr><td>BiLSTM</td><td>20</td><td>13</td><td>24</td><td>14</td><td>22</td><td>71</td><td>17</td><td>-8</td><td>31</td><td>-15</td><td>8</td></tr><tr><td>+ELMo</td><td>21</td><td>20</td><td>21</td><td>19</td><td>21</td><td>71</td><td>60</td><td>2</td><td>22</td><td>0</td><td>12</td></tr><tr><td>+CoVe</td><td>18</td><td>15</td><td>11</td><td>18</td><td>27</td><td>71</td><td>40</td><td>그</td><td>40</td><td>0</td><td>8</td></tr><tr><td>+Attn</td><td>18</td><td>13</td><td>24</td><td>11</td><td>16</td><td>71</td><td>1</td><td>-12</td><td>31</td><td>-15</td><td>8</td></tr><tr><td>+Attn, ELMo</td><td>22</td><td>18</td><td>26</td><td>13</td><td>19</td><td>70</td><td>27</td><td>5</td><td>31</td><td>-26</td><td>-3</td></tr><tr><td>+Attn, CoVe</td><td>18</td><td>16</td><td>25</td><td>16</td><td>13</td><td>71</td><td>26</td><td>-8</td><td>33</td><td>9</td><td>8</td></tr><tr><td colspan=\"12\">Pre-Trained Sentence Representation Models</td></tr><tr><td>CBoW</td><td>9</td><td>6</td><td>13</td><td>5</td><td>10</td><td>3</td><td>0</td><td>13</td><td>28</td><td>-15</td><td>-11</td></tr><tr><td>Skip-Thought</td><td>12</td><td>2</td><td>23</td><td>11</td><td>9</td><td>61</td><td>6</td><td>-2</td><td>30</td><td>-15</td><td>0</td></tr><tr><td>InferSent</td><td>18</td><td>20</td><td>20</td><td>15</td><td>14</td><td>77</td><td>50</td><td>-20</td><td>15</td><td>-15</td><td>-9</td></tr><tr><td>DisSent</td><td>16</td><td>16</td><td>19</td><td>13</td><td>15</td><td>70</td><td>43</td><td>-11</td><td>20</td><td>-36</td><td>-09</td></tr><tr><td>GenSen</td><td>20</td><td>28</td><td>26</td><td>14</td><td>12</td><td>78</td><td>57</td><td>2</td><td>21</td><td>-15</td><td>12</td></tr></table>",
            "id": 85,
            "page": 8,
            "text": "Coarse-Grained Fine-Grained  Model All LS PAS L K UQuant MNeg 2Neg Coref Restr Down  Single-Task Training  BiLSTM 21 25 24 16 16 70 53 4 21 -15 12  +ELMo 20 20 21 14 17 70 20 42 33 -26 -3  +CoVe 21 19 23 20 18 71 47 -1 33 -15 8  +Attn 25 24 30 20 14 50 47 21 38 -8 -3  +Attn, ELMo 28 30 35 23 14 85 20 42 33 -26 -3  +Attn, CoVe 24 29 29 18 12 77 50 1 18 -1 12  Multi-Task Training  BiLSTM 20 13 24 14 22 71 17 -8 31 -15 8  +ELMo 21 20 21 19 21 71 60 2 22 0 12  +CoVe 18 15 11 18 27 71 40 그 40 0 8  +Attn 18 13 24 11 16 71 1 -12 31 -15 8  +Attn, ELMo 22 18 26 13 19 70 27 5 31 -26 -3  +Attn, CoVe 18 16 25 16 13 71 26 -8 33 9 8  Pre-Trained Sentence Representation Models  CBoW 9 6 13 5 10 3 0 13 28 -15 -11  Skip-Thought 12 2 23 11 9 61 6 -2 30 -15 0  InferSent 18 20 20 15 14 77 50 -20 15 -15 -9  DisSent 16 16 19 13 15 70 43 -11 20 -36 -09  GenSen 20 28 26 14 12 78 57 2 21 -15"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1451
                },
                {
                    "x": 2108,
                    "y": 1451
                },
                {
                    "x": 2108,
                    "y": 1682
                },
                {
                    "x": 442,
                    "y": 1682
                }
            ],
            "category": "paragraph",
            "html": "<p id='86' style='font-size:20px'>Table 5: Results on the diagnostic set. We report R3 coefficients between gold and predicted la-<br>bels, scaled by 100. The coarse-grained categories are Lexical Semantics (LS), Predicate-Argument<br>Structure (PAS), Logic (L), and Knowledge and Common Sense (K). Our example fine-grained cate-<br>gories are Universal Quantification (UQuant), Morphological Negation (MNeg), Double Negation<br>(2Neg), Anaphora/Coreference (Coref), Restrictivity (Restr), and Downward Monotone (Down).</p>",
            "id": 86,
            "page": 8,
            "text": "Table 5: Results on the diagnostic set. We report R3 coefficients between gold and predicted labels, scaled by 100. The coarse-grained categories are Lexical Semantics (LS), Predicate-Argument Structure (PAS), Logic (L), and Knowledge and Common Sense (K). Our example fine-grained categories are Universal Quantification (UQuant), Morphological Negation (MNeg), Double Negation (2Neg), Anaphora/Coreference (Coref), Restrictivity (Restr), and Downward Monotone (Down)."
        },
        {
            "bounding_box": [
                {
                    "x": 446,
                    "y": 1879
                },
                {
                    "x": 1019,
                    "y": 1879
                },
                {
                    "x": 1019,
                    "y": 1931
                },
                {
                    "x": 446,
                    "y": 1931
                }
            ],
            "category": "paragraph",
            "html": "<p id='87' style='font-size:22px'>6 BENCHMARK RESULTS</p>",
            "id": 87,
            "page": 8,
            "text": "6 BENCHMARK RESULTS"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2062
                },
                {
                    "x": 2108,
                    "y": 2062
                },
                {
                    "x": 2108,
                    "y": 2247
                },
                {
                    "x": 442,
                    "y": 2247
                }
            ],
            "category": "paragraph",
            "html": "<p id='88' style='font-size:16px'>We train three runs of each model and evaluate the run with the best macro-average development set<br>performance (see Table 6 in Appendix C). For single-task and sentence representation models, we<br>evaluate the best run for each individual task. We present performance on the main benchmark tasks<br>in Table 4.</p>",
            "id": 88,
            "page": 8,
            "text": "We train three runs of each model and evaluate the run with the best macro-average development set performance (see Table 6 in Appendix C). For single-task and sentence representation models, we evaluate the best run for each individual task. We present performance on the main benchmark tasks in Table 4."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2270
                },
                {
                    "x": 2108,
                    "y": 2270
                },
                {
                    "x": 2108,
                    "y": 2501
                },
                {
                    "x": 442,
                    "y": 2501
                }
            ],
            "category": "paragraph",
            "html": "<p id='89' style='font-size:20px'>We find that multi-task training yields better overall scores over single-task training amongst models<br>using attention or ELMo. Attention generally has negligible or negative aggregate effect in single<br>task training, but helps in multi-task training. We see a consistent improvement in using ELMo<br>embeddings in place of GloVe or CoVe embeddings, particularly for single-sentence tasks. Using<br>CoVe has mixed effects over using only GloVe.</p>",
            "id": 89,
            "page": 8,
            "text": "We find that multi-task training yields better overall scores over single-task training amongst models using attention or ELMo. Attention generally has negligible or negative aggregate effect in single task training, but helps in multi-task training. We see a consistent improvement in using ELMo embeddings in place of GloVe or CoVe embeddings, particularly for single-sentence tasks. Using CoVe has mixed effects over using only GloVe."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2525
                },
                {
                    "x": 2107,
                    "y": 2525
                },
                {
                    "x": 2107,
                    "y": 2664
                },
                {
                    "x": 442,
                    "y": 2664
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='90' style='font-size:18px'>Among the pre-trained sentence representation models, we observe fairly consistent gains moving<br>from CBoW to Skip-Thought to Infersent and GenSen. Relative to the models trained directly on<br>the GLUE tasks, InferSent is competitive and GenSen outperforms all but the two best.</p>",
            "id": 90,
            "page": 8,
            "text": "Among the pre-trained sentence representation models, we observe fairly consistent gains moving from CBoW to Skip-Thought to Infersent and GenSen. Relative to the models trained directly on the GLUE tasks, InferSent is competitive and GenSen outperforms all but the two best."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2685
                },
                {
                    "x": 2108,
                    "y": 2685
                },
                {
                    "x": 2108,
                    "y": 3055
                },
                {
                    "x": 442,
                    "y": 3055
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='91' style='font-size:16px'>Looking at results per task, we find that the sentence representation models substantially underper-<br>form on CoLA compared to the models directly trained on the task. On the other hand, for STS-B,<br>models trained directly on the task lag significantly behind the performance of the best sentence<br>representation model. Finally, there are tasks for which no model does particularly well. On WNLI,<br>no model exceeds most-frequent-class guessing (65.1%) and we substitute the model predictions<br>for the most-frequent baseline. On RTE and in aggregate, even our best baselines leave room for<br>improvement. These early results indicate that solving GLUE is beyond the capabilities of current<br>models and methods.</p>",
            "id": 91,
            "page": 8,
            "text": "Looking at results per task, we find that the sentence representation models substantially underperform on CoLA compared to the models directly trained on the task. On the other hand, for STS-B, models trained directly on the task lag significantly behind the performance of the best sentence representation model. Finally, there are tasks for which no model does particularly well. On WNLI, no model exceeds most-frequent-class guessing (65.1%) and we substitute the model predictions for the most-frequent baseline. On RTE and in aggregate, even our best baselines leave room for improvement. These early results indicate that solving GLUE is beyond the capabilities of current models and methods."
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3133
                },
                {
                    "x": 1288,
                    "y": 3133
                },
                {
                    "x": 1288,
                    "y": 3171
                },
                {
                    "x": 1260,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='92' style='font-size:16px'>8</footer>",
            "id": 92,
            "page": 8,
            "text": "8"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 111
                },
                {
                    "x": 1224,
                    "y": 111
                },
                {
                    "x": 1224,
                    "y": 157
                },
                {
                    "x": 445,
                    "y": 157
                }
            ],
            "category": "header",
            "html": "<header id='93' style='font-size:14px'>Published as a conference paper at ICLR 2019</header>",
            "id": 93,
            "page": 9,
            "text": "Published as a conference paper at ICLR 2019"
        },
        {
            "bounding_box": [
                {
                    "x": 447,
                    "y": 340
                },
                {
                    "x": 749,
                    "y": 340
                },
                {
                    "x": 749,
                    "y": 393
                },
                {
                    "x": 447,
                    "y": 393
                }
            ],
            "category": "paragraph",
            "html": "<p id='94' style='font-size:22px'>7 ANALYSIS</p>",
            "id": 94,
            "page": 9,
            "text": "7 ANALYSIS"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 449
                },
                {
                    "x": 2106,
                    "y": 449
                },
                {
                    "x": 2106,
                    "y": 545
                },
                {
                    "x": 443,
                    "y": 545
                }
            ],
            "category": "paragraph",
            "html": "<p id='95' style='font-size:14px'>We analyze the baselines by evaluating each model's MNLI classifier on the diagnostic set to get a<br>better sense of their linguistic capabilities. Results are presented in Table 5.</p>",
            "id": 95,
            "page": 9,
            "text": "We analyze the baselines by evaluating each model's MNLI classifier on the diagnostic set to get a better sense of their linguistic capabilities. Results are presented in Table 5."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 601
                },
                {
                    "x": 2108,
                    "y": 601
                },
                {
                    "x": 2108,
                    "y": 1018
                },
                {
                    "x": 441,
                    "y": 1018
                }
            ],
            "category": "paragraph",
            "html": "<p id='96' style='font-size:16px'>Coarse Categories Overall performance is low for all models: The highest total score of 28 still<br>denotes poor absolute performance. Performance tends to be higher on Predicate-Argument Struc-<br>ture and lower on Logic, though numbers are not closely comparable across categories. Unlike on<br>the main benchmark, the multi-task models are almost always outperformed by their single-task<br>counterparts. This is perhaps unsurprising, since with our simple multi-task training regime, there is<br>likely some destructive interference between MNLI and the other tasks. The models trained on the<br>GLUE tasks largely outperform the pretrained sentence representation models, with the exception<br>of GenSen. Using attention has a greater influence on diagnostic scores than using ELMo or CoVe,<br>which we take to indicate that attention is especially important for generalization in NLI.</p>",
            "id": 96,
            "page": 9,
            "text": "Coarse Categories Overall performance is low for all models: The highest total score of 28 still denotes poor absolute performance. Performance tends to be higher on Predicate-Argument Structure and lower on Logic, though numbers are not closely comparable across categories. Unlike on the main benchmark, the multi-task models are almost always outperformed by their single-task counterparts. This is perhaps unsurprising, since with our simple multi-task training regime, there is likely some destructive interference between MNLI and the other tasks. The models trained on the GLUE tasks largely outperform the pretrained sentence representation models, with the exception of GenSen. Using attention has a greater influence on diagnostic scores than using ELMo or CoVe, which we take to indicate that attention is especially important for generalization in NLI."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1075
                },
                {
                    "x": 2107,
                    "y": 1075
                },
                {
                    "x": 2107,
                    "y": 1214
                },
                {
                    "x": 442,
                    "y": 1214
                }
            ],
            "category": "paragraph",
            "html": "<p id='97' style='font-size:16px'>Fine-Grained Subcategories Most models handle universal quantification relatively well. Look-<br>ing at relevant examples, it seems that relying on lexical cues such as \"all\" often suffices for good<br>performance. Similarly, lexical cues often provide good signal in morphological negation examples.</p>",
            "id": 97,
            "page": 9,
            "text": "Fine-Grained Subcategories Most models handle universal quantification relatively well. Looking at relevant examples, it seems that relying on lexical cues such as \"all\" often suffices for good performance. Similarly, lexical cues often provide good signal in morphological negation examples."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1238
                },
                {
                    "x": 2107,
                    "y": 1238
                },
                {
                    "x": 2107,
                    "y": 1694
                },
                {
                    "x": 441,
                    "y": 1694
                }
            ],
            "category": "paragraph",
            "html": "<p id='98' style='font-size:14px'>We observe varying weaknesses between models. Double negation is especially difficult for the<br>GLUE-trained models that only use GloVe embeddings. This is ameliorated by ELMo, and to some<br>degree CoVe. Also, attention has mixed effects on overall results, and models with attention tend<br>to struggle with downward monotonicity. Examining their predictions, we found that the models<br>are sensitive to hypernym/hyponym substitution and word deletion as a signal of entailment, but<br>predict it in the wrong direction (as if the substituted/deleted word were in an upward monotone<br>context). This is consistent with recent findings by McCoy & Linzen (2019) that these systems<br>use the subsequence relation between premise and hypothesis as a heuristic shortcut. Restrictivity<br>examples, which often depend on nuances of quantifier scope, are especially difficult for almost all<br>models.</p>",
            "id": 98,
            "page": 9,
            "text": "We observe varying weaknesses between models. Double negation is especially difficult for the GLUE-trained models that only use GloVe embeddings. This is ameliorated by ELMo, and to some degree CoVe. Also, attention has mixed effects on overall results, and models with attention tend to struggle with downward monotonicity. Examining their predictions, we found that the models are sensitive to hypernym/hyponym substitution and word deletion as a signal of entailment, but predict it in the wrong direction (as if the substituted/deleted word were in an upward monotone context). This is consistent with recent findings by McCoy & Linzen (2019) that these systems use the subsequence relation between premise and hypothesis as a heuristic shortcut. Restrictivity examples, which often depend on nuances of quantifier scope, are especially difficult for almost all models."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1717
                },
                {
                    "x": 2108,
                    "y": 1717
                },
                {
                    "x": 2108,
                    "y": 2041
                },
                {
                    "x": 441,
                    "y": 2041
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='99' style='font-size:16px'>Overall, there is evidence that going beyond sentence-to-vector representations, e.g. with an atten-<br>tion mechanism, might aid performance on out-of-domain data, and that transfer methods like ELMo<br>and CoVe encode linguistic information specific to their supervision signal. However, increased rep-<br>resentational capacity may lead to overfitting, such as the failure of attention models in downward<br>monotone contexts. We expect that our platform and diagnostic dataset will be useful for similar<br>analyses in the future, SO that model designers can better understand their models' generalization<br>behavior and implicit knowledge.</p>",
            "id": 99,
            "page": 9,
            "text": "Overall, there is evidence that going beyond sentence-to-vector representations, e.g. with an attention mechanism, might aid performance on out-of-domain data, and that transfer methods like ELMo and CoVe encode linguistic information specific to their supervision signal. However, increased representational capacity may lead to overfitting, such as the failure of attention models in downward monotone contexts. We expect that our platform and diagnostic dataset will be useful for similar analyses in the future, SO that model designers can better understand their models' generalization behavior and implicit knowledge."
        },
        {
            "bounding_box": [
                {
                    "x": 446,
                    "y": 2115
                },
                {
                    "x": 820,
                    "y": 2115
                },
                {
                    "x": 820,
                    "y": 2168
                },
                {
                    "x": 446,
                    "y": 2168
                }
            ],
            "category": "paragraph",
            "html": "<p id='100' style='font-size:20px'>8 CONCLUSION</p>",
            "id": 100,
            "page": 9,
            "text": "8 CONCLUSION"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2221
                },
                {
                    "x": 2107,
                    "y": 2221
                },
                {
                    "x": 2107,
                    "y": 2641
                },
                {
                    "x": 442,
                    "y": 2641
                }
            ],
            "category": "paragraph",
            "html": "<p id='101' style='font-size:14px'>We introduce GLUE, a platform and collection of resources for evaluating and analyzing natural<br>language understanding systems. We find that, in aggregate, models trained jointly on our tasks see<br>better performance than the combined performance of models trained for each task separately. We<br>confirm the utility of attention mechanisms and transfer learning methods such as ELMo in NLU<br>systems, which combine to outperform the best sentence representation models on the GLUE bench-<br>mark, but still leave room for improvement. When evaluating these models on our diagnostic dataset,<br>we find that they fail (often spectacularly) on many linguistic phenomena, suggesting possible av-<br>enues for future work. In sum, the question of how to design general-purpose NLU models remains<br>unanswered, and we believe that GLUE can provide fertile soil for addressing this challenge.</p>",
            "id": 101,
            "page": 9,
            "text": "We introduce GLUE, a platform and collection of resources for evaluating and analyzing natural language understanding systems. We find that, in aggregate, models trained jointly on our tasks see better performance than the combined performance of models trained for each task separately. We confirm the utility of attention mechanisms and transfer learning methods such as ELMo in NLU systems, which combine to outperform the best sentence representation models on the GLUE benchmark, but still leave room for improvement. When evaluating these models on our diagnostic dataset, we find that they fail (often spectacularly) on many linguistic phenomena, suggesting possible avenues for future work. In sum, the question of how to design general-purpose NLU models remains unanswered, and we believe that GLUE can provide fertile soil for addressing this challenge."
        },
        {
            "bounding_box": [
                {
                    "x": 446,
                    "y": 2716
                },
                {
                    "x": 915,
                    "y": 2716
                },
                {
                    "x": 915,
                    "y": 2766
                },
                {
                    "x": 446,
                    "y": 2766
                }
            ],
            "category": "paragraph",
            "html": "<p id='102' style='font-size:18px'>ACKNOWLEDGMENTS</p>",
            "id": 102,
            "page": 9,
            "text": "ACKNOWLEDGMENTS"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2820
                },
                {
                    "x": 2108,
                    "y": 2820
                },
                {
                    "x": 2108,
                    "y": 3053
                },
                {
                    "x": 442,
                    "y": 3053
                }
            ],
            "category": "paragraph",
            "html": "<p id='103' style='font-size:16px'>We thank Ellie Pavlick, Tal Linzen, Kyunghyun Cho, and Nikita Nangia for their comments on this<br>work at its early stages, and we thank Ernie Davis, Alex Warstadt, and Quora's Nikhil Dandekar<br>and Kornel Csernai for providing access to private evaluation data. This project has benefited from<br>financial support to SB by Google, Tencent Holdings, and Samsung Research, and to AW from<br>AdeptMind and an NSF Graduate Research Fellowship.</p>",
            "id": 103,
            "page": 9,
            "text": "We thank Ellie Pavlick, Tal Linzen, Kyunghyun Cho, and Nikita Nangia for their comments on this work at its early stages, and we thank Ernie Davis, Alex Warstadt, and Quora's Nikhil Dandekar and Kornel Csernai for providing access to private evaluation data. This project has benefited from financial support to SB by Google, Tencent Holdings, and Samsung Research, and to AW from AdeptMind and an NSF Graduate Research Fellowship."
        },
        {
            "bounding_box": [
                {
                    "x": 1259,
                    "y": 3132
                },
                {
                    "x": 1290,
                    "y": 3132
                },
                {
                    "x": 1290,
                    "y": 3169
                },
                {
                    "x": 1259,
                    "y": 3169
                }
            ],
            "category": "footer",
            "html": "<footer id='104' style='font-size:14px'>9</footer>",
            "id": 104,
            "page": 9,
            "text": "9"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 112
                },
                {
                    "x": 1224,
                    "y": 112
                },
                {
                    "x": 1224,
                    "y": 156
                },
                {
                    "x": 444,
                    "y": 156
                }
            ],
            "category": "header",
            "html": "<header id='105' style='font-size:14px'>Published as a conference paper at ICLR 2019</header>",
            "id": 105,
            "page": 10,
            "text": "Published as a conference paper at ICLR 2019"
        },
        {
            "bounding_box": [
                {
                    "x": 446,
                    "y": 342
                },
                {
                    "x": 734,
                    "y": 342
                },
                {
                    "x": 734,
                    "y": 392
                },
                {
                    "x": 446,
                    "y": 392
                }
            ],
            "category": "paragraph",
            "html": "<p id='106' style='font-size:22px'>REFERENCES</p>",
            "id": 106,
            "page": 10,
            "text": "REFERENCES"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 422
                },
                {
                    "x": 2107,
                    "y": 422
                },
                {
                    "x": 2107,
                    "y": 558
                },
                {
                    "x": 443,
                    "y": 558
                }
            ],
            "category": "paragraph",
            "html": "<p id='107' style='font-size:20px'>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly<br>learning to align and translate. In Proceedings of the International Conference on Learning Rep-<br>resentations, 2015.</p>",
            "id": 107,
            "page": 10,
            "text": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In Proceedings of the International Conference on Learning Representations, 2015."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 592
                },
                {
                    "x": 2106,
                    "y": 592
                },
                {
                    "x": 2106,
                    "y": 687
                },
                {
                    "x": 441,
                    "y": 687
                }
            ],
            "category": "paragraph",
            "html": "<p id='108' style='font-size:18px'>Roy Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and<br>Idan Szpektor. The second PASCAL recognising textual entailment challenge. 2006.</p>",
            "id": 108,
            "page": 10,
            "text": "Roy Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan Szpektor. The second PASCAL recognising textual entailment challenge. 2006."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 721
                },
                {
                    "x": 2107,
                    "y": 721
                },
                {
                    "x": 2107,
                    "y": 813
                },
                {
                    "x": 442,
                    "y": 813
                }
            ],
            "category": "paragraph",
            "html": "<p id='109' style='font-size:20px'>Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. The<br>fifth PASCAL recognizing textual entailment challenge. 2009.</p>",
            "id": 109,
            "page": 10,
            "text": "Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, Danilo Giampiccolo, and Bernardo Magnini. The fifth PASCAL recognizing textual entailment challenge. 2009."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 846
                },
                {
                    "x": 2108,
                    "y": 846
                },
                {
                    "x": 2108,
                    "y": 1029
                },
                {
                    "x": 443,
                    "y": 1029
                }
            ],
            "category": "paragraph",
            "html": "<p id='110' style='font-size:18px'>Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large anno-<br>tated corpus for learning natural language inference. In Proceedings of the Conference on Em-<br>pirical Methods in Natural Language Processing, pp. 632-642. Association for Computational<br>Linguistics, 2015.</p>",
            "id": 110,
            "page": 10,
            "text": "Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large annotated corpus for learning natural language inference. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pp. 632-642. Association for Computational Linguistics, 2015."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1063
                },
                {
                    "x": 2108,
                    "y": 1063
                },
                {
                    "x": 2108,
                    "y": 1202
                },
                {
                    "x": 442,
                    "y": 1202
                }
            ],
            "category": "paragraph",
            "html": "<p id='111' style='font-size:18px'>Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task<br>1: Semantic textual similarity-multilingual and cross-lingual focused evaluation. In Eleventh<br>International Workshop on Semantic Evaluations, 2017.</p>",
            "id": 111,
            "page": 10,
            "text": "Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation. In Eleventh International Workshop on Semantic Evaluations, 2017."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1237
                },
                {
                    "x": 2107,
                    "y": 1237
                },
                {
                    "x": 2107,
                    "y": 1374
                },
                {
                    "x": 442,
                    "y": 1374
                }
            ],
            "category": "paragraph",
            "html": "<p id='112' style='font-size:18px'>Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony<br>Robinson. One billion word benchmark for measuring progress in statistical language modeling.<br>arXiv preprint 1312.3005, 2013.</p>",
            "id": 112,
            "page": 10,
            "text": "Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson. One billion word benchmark for measuring progress in statistical language modeling. arXiv preprint 1312.3005, 2013."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1408
                },
                {
                    "x": 2106,
                    "y": 1408
                },
                {
                    "x": 2106,
                    "y": 1546
                },
                {
                    "x": 442,
                    "y": 1546
                }
            ],
            "category": "paragraph",
            "html": "<p id='113' style='font-size:18px'>Ronan Collobert, Jason Weston, Leon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel<br>Kuksa. Natural language processing (almost) from scratch. Journal of Machine Learning Re-<br>search, 12(Aug):2493-2537, 2011.</p>",
            "id": 113,
            "page": 10,
            "text": "Ronan Collobert, Jason Weston, Leon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. Natural language processing (almost) from scratch. Journal of Machine Learning Research, 12(Aug):2493-2537, 2011."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1581
                },
                {
                    "x": 2107,
                    "y": 1581
                },
                {
                    "x": 2107,
                    "y": 1720
                },
                {
                    "x": 443,
                    "y": 1720
                }
            ],
            "category": "paragraph",
            "html": "<p id='114' style='font-size:16px'>Alexis Conneau and Douwe Kiela. SentEval: An evaluation toolkit for universal sentence repre-<br>sentations. In Proceedings of the Eleventh International Conference on Language Resources and<br>Evaluation, 2018.</p>",
            "id": 114,
            "page": 10,
            "text": "Alexis Conneau and Douwe Kiela. SentEval: An evaluation toolkit for universal sentence representations. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation, 2018."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 1754
                },
                {
                    "x": 2106,
                    "y": 1754
                },
                {
                    "x": 2106,
                    "y": 1937
                },
                {
                    "x": 444,
                    "y": 1937
                }
            ],
            "category": "paragraph",
            "html": "<p id='115' style='font-size:18px'>Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine Bordes. Supervised<br>learning of universal sentence representations from natural language inference data. In Proceed-<br>ings of the Conference on Empirical Methods in Natural Language Processing, Copenhagen,<br>Denmark, September 9-11, 2017, pp. 681-691, 2017.</p>",
            "id": 115,
            "page": 10,
            "text": "Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine Bordes. Supervised learning of universal sentence representations from natural language inference data. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, Copenhagen, Denmark, September 9-11, 2017, pp. 681-691, 2017."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1971
                },
                {
                    "x": 2106,
                    "y": 1971
                },
                {
                    "x": 2106,
                    "y": 2110
                },
                {
                    "x": 442,
                    "y": 2110
                }
            ],
            "category": "paragraph",
            "html": "<p id='116' style='font-size:18px'>Robin Cooper, Dick Crouch, Jan Van Eijck, Chris Fox, Josef Van Genabith, Jan Jaspars, Hans Kamp,<br>David Milward, Manfred Pinkal, Massimo Poesio, Steve Pulman, Ted Briscoe, Holger Maier, and<br>Karsten Konrad. Using the framework. Technical report, The FraCaS Consortium, 1996.</p>",
            "id": 116,
            "page": 10,
            "text": "Robin Cooper, Dick Crouch, Jan Van Eijck, Chris Fox, Josef Van Genabith, Jan Jaspars, Hans Kamp, David Milward, Manfred Pinkal, Massimo Poesio, Steve Pulman, Ted Briscoe, Holger Maier, and Karsten Konrad. Using the framework. Technical report, The FraCaS Consortium, 1996."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2144
                },
                {
                    "x": 2107,
                    "y": 2144
                },
                {
                    "x": 2107,
                    "y": 2282
                },
                {
                    "x": 442,
                    "y": 2282
                }
            ],
            "category": "paragraph",
            "html": "<p id='117' style='font-size:18px'>Ido Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL recognising textual entailment<br>challenge. In Machine learning challenges. evaluating predictive uncertainty, visual object clas-<br>sification, and recognising tectual entailment, pp. 177-190. Springer, 2006.</p>",
            "id": 117,
            "page": 10,
            "text": "Ido Dagan, Oren Glickman, and Bernardo Magnini. The PASCAL recognising textual entailment challenge. In Machine learning challenges. evaluating predictive uncertainty, visual object classification, and recognising tectual entailment, pp. 177-190. Springer, 2006."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2317
                },
                {
                    "x": 2107,
                    "y": 2317
                },
                {
                    "x": 2107,
                    "y": 2410
                },
                {
                    "x": 442,
                    "y": 2410
                }
            ],
            "category": "paragraph",
            "html": "<p id='118' style='font-size:20px'>Dorottya Demszky, Kelvin Guu, and Percy Liang. Transforming question answering datasets into<br>natural language inference datasets. arXiv preprint 1809.02922, 2018.</p>",
            "id": 118,
            "page": 10,
            "text": "Dorottya Demszky, Kelvin Guu, and Percy Liang. Transforming question answering datasets into natural language inference datasets. arXiv preprint 1809.02922, 2018."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2443
                },
                {
                    "x": 2104,
                    "y": 2443
                },
                {
                    "x": 2104,
                    "y": 2535
                },
                {
                    "x": 443,
                    "y": 2535
                }
            ],
            "category": "paragraph",
            "html": "<p id='119' style='font-size:18px'>William B Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases.<br>In Proceedings of the International Workshop on Paraphrasing, 2005.</p>",
            "id": 119,
            "page": 10,
            "text": "William B Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the International Workshop on Paraphrasing, 2005."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2569
                },
                {
                    "x": 2107,
                    "y": 2569
                },
                {
                    "x": 2107,
                    "y": 2707
                },
                {
                    "x": 444,
                    "y": 2707
                }
            ],
            "category": "paragraph",
            "html": "<p id='120' style='font-size:18px'>Allyson Ettinger, Sudha Rao, Hal Daume III, and Emily M Bender. Towards linguistically general-<br>izable NLP systems: A workshop and shared task. In First Workshop on Building Linguistically<br>Generalizable NLP Systems, 2017.</p>",
            "id": 120,
            "page": 10,
            "text": "Allyson Ettinger, Sudha Rao, Hal Daume III, and Emily M Bender. Towards linguistically generalizable NLP systems: A workshop and shared task. In First Workshop on Building Linguistically Generalizable NLP Systems, 2017."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2740
                },
                {
                    "x": 2106,
                    "y": 2740
                },
                {
                    "x": 2106,
                    "y": 2880
                },
                {
                    "x": 442,
                    "y": 2880
                }
            ],
            "category": "paragraph",
            "html": "<p id='121' style='font-size:18px'>Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson F. Liu, Matthew<br>Peters, Michael Schmitz, and Luke S. Zettlemoyer. AllenNLP: A deep semantic natural language<br>processing platform. 2017.</p>",
            "id": 121,
            "page": 10,
            "text": "Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson F. Liu, Matthew Peters, Michael Schmitz, and Luke S. Zettlemoyer. AllenNLP: A deep semantic natural language processing platform. 2017."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2914
                },
                {
                    "x": 2108,
                    "y": 2914
                },
                {
                    "x": 2108,
                    "y": 3054
                },
                {
                    "x": 443,
                    "y": 3054
                }
            ],
            "category": "paragraph",
            "html": "<p id='122' style='font-size:18px'>Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third PASCAL recognizing<br>textual entailment challenge. In Proceedings of the ACL-PASCAL workshop on textual entailment<br>and paraphrasing, pp. 1-9. Association for Computational Linguistics, 2007.</p>",
            "id": 122,
            "page": 10,
            "text": "Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third PASCAL recognizing textual entailment challenge. In Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing, pp. 1-9. Association for Computational Linguistics, 2007."
        },
        {
            "bounding_box": [
                {
                    "x": 1253,
                    "y": 3133
                },
                {
                    "x": 1300,
                    "y": 3133
                },
                {
                    "x": 1300,
                    "y": 3172
                },
                {
                    "x": 1253,
                    "y": 3172
                }
            ],
            "category": "footer",
            "html": "<footer id='123' style='font-size:14px'>10</footer>",
            "id": 123,
            "page": 10,
            "text": "10"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 112
                },
                {
                    "x": 1224,
                    "y": 112
                },
                {
                    "x": 1224,
                    "y": 156
                },
                {
                    "x": 445,
                    "y": 156
                }
            ],
            "category": "header",
            "html": "<header id='124' style='font-size:14px'>Published as a conference paper at ICLR 2019</header>",
            "id": 124,
            "page": 11,
            "text": "Published as a conference paper at ICLR 2019"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 348
                },
                {
                    "x": 2105,
                    "y": 348
                },
                {
                    "x": 2105,
                    "y": 437
                },
                {
                    "x": 443,
                    "y": 437
                }
            ],
            "category": "paragraph",
            "html": "<p id='125' style='font-size:18px'>Jan Gorodkin. Comparing two k-category assignments by a k-category correlation coefficient. Com-<br>put. Biol. Chem., 28(5-6):367-374, December 2004. ISSN 1476-9271.</p>",
            "id": 125,
            "page": 11,
            "text": "Jan Gorodkin. Comparing two k-category assignments by a k-category correlation coefficient. Comput. Biol. Chem., 28(5-6):367-374, December 2004. ISSN 1476-9271."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 473
                },
                {
                    "x": 2108,
                    "y": 473
                },
                {
                    "x": 2108,
                    "y": 656
                },
                {
                    "x": 444,
                    "y": 656
                }
            ],
            "category": "paragraph",
            "html": "<p id='126' style='font-size:20px'>Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel R. Bowman, and<br>Noah A. Smith. Annotation artifacts in natural language inference data. In Proceedings of the<br>North American Chapter of the Association for Computational Linguistics: Human Language<br>Technologies, 2018.</p>",
            "id": 126,
            "page": 11,
            "text": "Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel R. Bowman, and Noah A. Smith. Annotation artifacts in natural language inference data. In Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2018."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 690
                },
                {
                    "x": 2108,
                    "y": 690
                },
                {
                    "x": 2108,
                    "y": 829
                },
                {
                    "x": 442,
                    "y": 829
                }
            ],
            "category": "paragraph",
            "html": "<p id='127' style='font-size:20px'>Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka, and Richard Socher. A joint many-task<br>model: Growing a neural network for multiple nlp tasks. In Proceedings of the Conference on<br>Empirical Methods in Natural Language Processing, 2017.</p>",
            "id": 127,
            "page": 11,
            "text": "Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka, and Richard Socher. A joint many-task model: Growing a neural network for multiple nlp tasks. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2017."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 862
                },
                {
                    "x": 2108,
                    "y": 862
                },
                {
                    "x": 2108,
                    "y": 1001
                },
                {
                    "x": 442,
                    "y": 1001
                }
            ],
            "category": "paragraph",
            "html": "<p id='128' style='font-size:20px'>Felix Hill, Kyunghyun Cho, and Anna Korhonen. Learning distributed representations of sentences<br>from unlabelled data. In Proceedings of the North American Chapter of the Association for<br>Computational Linguistics: Human Language Technologies, 2016.</p>",
            "id": 128,
            "page": 11,
            "text": "Felix Hill, Kyunghyun Cho, and Anna Korhonen. Learning distributed representations of sentences from unlabelled data. In Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2016."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1035
                },
                {
                    "x": 2108,
                    "y": 1035
                },
                {
                    "x": 2108,
                    "y": 1170
                },
                {
                    "x": 443,
                    "y": 1170
                }
            ],
            "category": "paragraph",
            "html": "<p id='129' style='font-size:20px'>Minqing Hu and Bing Liu. Mining and summarizing customer reviews. In Proceedings of the<br>tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp.<br>168-177. ACM, 2004.</p>",
            "id": 129,
            "page": 11,
            "text": "Minqing Hu and Bing Liu. Mining and summarizing customer reviews. In Proceedings of the tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 168-177. ACM, 2004."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 1206
                },
                {
                    "x": 2106,
                    "y": 1206
                },
                {
                    "x": 2106,
                    "y": 1299
                },
                {
                    "x": 444,
                    "y": 1299
                }
            ],
            "category": "paragraph",
            "html": "<p id='130' style='font-size:16px'>Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for efficient<br>text classification. arXiv preprint 1607.01759, 2016.</p>",
            "id": 130,
            "page": 11,
            "text": "Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for efficient text classification. arXiv preprint 1607.01759, 2016."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1332
                },
                {
                    "x": 2110,
                    "y": 1332
                },
                {
                    "x": 2110,
                    "y": 1425
                },
                {
                    "x": 442,
                    "y": 1425
                }
            ],
            "category": "paragraph",
            "html": "<p id='131' style='font-size:20px'>Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of<br>the International Conference on Learning Representations, 2015.</p>",
            "id": 131,
            "page": 11,
            "text": "Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings of the International Conference on Learning Representations, 2015."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1458
                },
                {
                    "x": 2106,
                    "y": 1458
                },
                {
                    "x": 2106,
                    "y": 1596
                },
                {
                    "x": 442,
                    "y": 1596
                }
            ],
            "category": "paragraph",
            "html": "<p id='132' style='font-size:20px'>Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Tor-<br>ralba, and Sanja Fidler. Skip-Thought vectors. In Advances in Neural Information Processing<br>Systems, pp. 3294-3302, 2015.</p>",
            "id": 132,
            "page": 11,
            "text": "Ryan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Skip-Thought vectors. In Advances in Neural Information Processing Systems, pp. 3294-3302, 2015."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1629
                },
                {
                    "x": 2106,
                    "y": 1629
                },
                {
                    "x": 2106,
                    "y": 1813
                },
                {
                    "x": 442,
                    "y": 1813
                }
            ],
            "category": "paragraph",
            "html": "<p id='133' style='font-size:18px'>Quoc Le and Tomas Mikolov. Distributed representations of sentences and documents. In Eric P.<br>Xing and Tony Jebara (eds.), Proceedings of the 31st International Conference on Machine Learn-<br>ing, volume 32 of Proceedings of Machine Learning Research, pp. 1188-1196, Bejing, China,<br>22-24 Jun 2014. PMLR.</p>",
            "id": 133,
            "page": 11,
            "text": "Quoc Le and Tomas Mikolov. Distributed representations of sentences and documents. In Eric P. Xing and Tony Jebara (eds.), Proceedings of the 31st International Conference on Machine Learning, volume 32 of Proceedings of Machine Learning Research, pp. 1188-1196, Bejing, China, 22-24 Jun 2014. PMLR."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1849
                },
                {
                    "x": 2107,
                    "y": 1849
                },
                {
                    "x": 2107,
                    "y": 1984
                },
                {
                    "x": 443,
                    "y": 1984
                }
            ],
            "category": "paragraph",
            "html": "<p id='134' style='font-size:20px'>Hector J Levesque, Ernest Davis, and Leora Morgenstern. The Winograd schema challenge. In<br>AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning, volume 46, pp.<br>47, 2011.</p>",
            "id": 134,
            "page": 11,
            "text": "Hector J Levesque, Ernest Davis, and Leora Morgenstern. The Winograd schema challenge. In AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning, volume 46, pp. 47, 2011."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2019
                },
                {
                    "x": 2107,
                    "y": 2019
                },
                {
                    "x": 2107,
                    "y": 2113
                },
                {
                    "x": 443,
                    "y": 2113
                }
            ],
            "category": "paragraph",
            "html": "<p id='135' style='font-size:18px'>Brian W Matthews. Comparison of the predicted and observed secondary structure of t4 phage<br>lysozyme. Biochimica et Biophysica Acta (BBA)-Protein Structure, 405(2):442-451, 1975.</p>",
            "id": 135,
            "page": 11,
            "text": "Brian W Matthews. Comparison of the predicted and observed secondary structure of t4 phage lysozyme. Biochimica et Biophysica Acta (BBA)-Protein Structure, 405(2):442-451, 1975."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2146
                },
                {
                    "x": 2107,
                    "y": 2146
                },
                {
                    "x": 2107,
                    "y": 2282
                },
                {
                    "x": 442,
                    "y": 2282
                }
            ],
            "category": "paragraph",
            "html": "<p id='136' style='font-size:20px'>Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in translation:<br>Contextualized word vectors. In Advances in Neural Information Processing Systems, pp. 6297-<br>6308, 2017.</p>",
            "id": 136,
            "page": 11,
            "text": "Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in translation: Contextualized word vectors. In Advances in Neural Information Processing Systems, pp. 62976308, 2017."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2318
                },
                {
                    "x": 2106,
                    "y": 2318
                },
                {
                    "x": 2106,
                    "y": 2411
                },
                {
                    "x": 442,
                    "y": 2411
                }
            ],
            "category": "paragraph",
            "html": "<p id='137' style='font-size:20px'>Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language<br>decathlon: Multitask learning as question answering. arXiv preprint 1806.08730, 2018.</p>",
            "id": 137,
            "page": 11,
            "text": "Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language decathlon: Multitask learning as question answering. arXiv preprint 1806.08730, 2018."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2443
                },
                {
                    "x": 2105,
                    "y": 2443
                },
                {
                    "x": 2105,
                    "y": 2579
                },
                {
                    "x": 443,
                    "y": 2579
                }
            ],
            "category": "paragraph",
            "html": "<p id='138' style='font-size:20px'>R. Thomas McCoy and Tal Linzen. Non-entailed subsequences as a challenge for natural language<br>inference. In Proceedings of the Society for Computation in Linguistics, volume 2, pp. 357-360,<br>2019.</p>",
            "id": 138,
            "page": 11,
            "text": "R. Thomas McCoy and Tal Linzen. Non-entailed subsequences as a challenge for natural language inference. In Proceedings of the Society for Computation in Linguistics, volume 2, pp. 357-360, 2019."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2616
                },
                {
                    "x": 2107,
                    "y": 2616
                },
                {
                    "x": 2107,
                    "y": 2708
                },
                {
                    "x": 444,
                    "y": 2708
                }
            ],
            "category": "paragraph",
            "html": "<p id='139' style='font-size:16px'>Allen Nie, Erin D Bennett, and Noah D Goodman. Dissent: Sentence representation learning from<br>explicit discourse relations. arXiv preprint 1710.04334, 2017.</p>",
            "id": 139,
            "page": 11,
            "text": "Allen Nie, Erin D Bennett, and Noah D Goodman. Dissent: Sentence representation learning from explicit discourse relations. arXiv preprint 1710.04334, 2017."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2742
                },
                {
                    "x": 2106,
                    "y": 2742
                },
                {
                    "x": 2106,
                    "y": 2879
                },
                {
                    "x": 442,
                    "y": 2879
                }
            ],
            "category": "paragraph",
            "html": "<p id='140' style='font-size:18px'>Bo Pang and Lillian Lee. A sentimental education: Sentiment analysis using subjectivity summa-<br>rization based on minimum cuts. In Proceedings of the 42nd Annual Meeting on Association for<br>Computational Linguistics, pp. 271. Association for Computational Linguistics, 2004.</p>",
            "id": 140,
            "page": 11,
            "text": "Bo Pang and Lillian Lee. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, pp. 271. Association for Computational Linguistics, 2004."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2916
                },
                {
                    "x": 2108,
                    "y": 2916
                },
                {
                    "x": 2108,
                    "y": 3054
                },
                {
                    "x": 443,
                    "y": 3054
                }
            ],
            "category": "paragraph",
            "html": "<p id='141' style='font-size:20px'>Bo Pang and Lillian Lee. Seeing stars: Exploiting class relationships for sentiment categorization<br>with respect to rating scales. In Proceedings of the 43rd Annual Meeting on Association for<br>Computational Linguistics, pp. 115-124. Association for Computational Linguistics, 2005.</p>",
            "id": 141,
            "page": 11,
            "text": "Bo Pang and Lillian Lee. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, pp. 115-124. Association for Computational Linguistics, 2005."
        },
        {
            "bounding_box": [
                {
                    "x": 1252,
                    "y": 3133
                },
                {
                    "x": 1297,
                    "y": 3133
                },
                {
                    "x": 1297,
                    "y": 3172
                },
                {
                    "x": 1252,
                    "y": 3172
                }
            ],
            "category": "footer",
            "html": "<footer id='142' style='font-size:14px'>11</footer>",
            "id": 142,
            "page": 11,
            "text": "11"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 112
                },
                {
                    "x": 1224,
                    "y": 112
                },
                {
                    "x": 1224,
                    "y": 156
                },
                {
                    "x": 444,
                    "y": 156
                }
            ],
            "category": "header",
            "html": "<header id='143' style='font-size:14px'>Published as a conference paper at ICLR 2019</header>",
            "id": 143,
            "page": 12,
            "text": "Published as a conference paper at ICLR 2019"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 346
                },
                {
                    "x": 2107,
                    "y": 346
                },
                {
                    "x": 2107,
                    "y": 485
                },
                {
                    "x": 442,
                    "y": 485
                }
            ],
            "category": "paragraph",
            "html": "<p id='144' style='font-size:18px'>Jeffrey Pennington, Richard Socher, and Christopher Manning. GloVe: Global vectors for word<br>representation. In Proceedings of the Conference on Empirical Methods in Natural Language<br>processing, pp. 1532-1543, 2014.</p>",
            "id": 144,
            "page": 12,
            "text": "Jeffrey Pennington, Richard Socher, and Christopher Manning. GloVe: Global vectors for word representation. In Proceedings of the Conference on Empirical Methods in Natural Language processing, pp. 1532-1543, 2014."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 519
                },
                {
                    "x": 2108,
                    "y": 519
                },
                {
                    "x": 2108,
                    "y": 700
                },
                {
                    "x": 443,
                    "y": 700
                }
            ],
            "category": "paragraph",
            "html": "<p id='145' style='font-size:20px'>Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and<br>Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of the North Amer-<br>ican Chapter of the Association for Computational Linguistics: Human Language Technologies,<br>2018.</p>",
            "id": 145,
            "page": 12,
            "text": "Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations. In Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2018."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 736
                },
                {
                    "x": 2103,
                    "y": 736
                },
                {
                    "x": 2103,
                    "y": 828
                },
                {
                    "x": 444,
                    "y": 828
                }
            ],
            "category": "paragraph",
            "html": "<p id='146' style='font-size:20px'>Adam Poliak, Jason Naradowsky, Aparajita Haldar, Rachel Rudinger, and Benjamin Van Durme.<br>Hypothesis only baselines in natural language inference. In *SEM@NAACL-HLT, 2018.</p>",
            "id": 146,
            "page": 12,
            "text": "Adam Poliak, Jason Naradowsky, Aparajita Haldar, Rachel Rudinger, and Benjamin Van Durme. Hypothesis only baselines in natural language inference. In *SEM@NAACL-HLT, 2018."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 860
                },
                {
                    "x": 2107,
                    "y": 860
                },
                {
                    "x": 2107,
                    "y": 998
                },
                {
                    "x": 441,
                    "y": 998
                }
            ],
            "category": "paragraph",
            "html": "<p id='147' style='font-size:20px'>Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions<br>for machine comprehension of text. In Proceedings of the Conference on Empirical Methods in<br>Natural Language Processing, pp. 2383-2392. Association for Computational Linguistics, 2016.</p>",
            "id": 147,
            "page": 12,
            "text": "Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pp. 2383-2392. Association for Computational Linguistics, 2016."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1029
                },
                {
                    "x": 2107,
                    "y": 1029
                },
                {
                    "x": 2107,
                    "y": 1170
                },
                {
                    "x": 443,
                    "y": 1170
                }
            ],
            "category": "paragraph",
            "html": "<p id='148' style='font-size:20px'>Tim Rockt�schel, Edward Grefenstette, Moritz Hermann, Karl, Tomas Kocisky, and Phil Blunsom.<br>Reasoning about entailment with neural attention. In Proceedings of the International Conference<br>on Learning Representations, 2016.</p>",
            "id": 148,
            "page": 12,
            "text": "Tim Rockt�schel, Edward Grefenstette, Moritz Hermann, Karl, Tomas Kocisky, and Phil Blunsom. Reasoning about entailment with neural attention. In Proceedings of the International Conference on Learning Representations, 2016."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1202
                },
                {
                    "x": 2105,
                    "y": 1202
                },
                {
                    "x": 2105,
                    "y": 1295
                },
                {
                    "x": 442,
                    "y": 1295
                }
            ],
            "category": "paragraph",
            "html": "<p id='149' style='font-size:18px'>Sebastian Ruder, Joachim Bingel, Isabelle Augenstein, and Anders S⌀gaard. Sluice networks:<br>Learning what to share between loosely related tasks. arXiv preprint 1705.08142, 2017.</p>",
            "id": 149,
            "page": 12,
            "text": "Sebastian Ruder, Joachim Bingel, Isabelle Augenstein, and Anders S⌀gaard. Sluice networks: Learning what to share between loosely related tasks. arXiv preprint 1705.08142, 2017."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1328
                },
                {
                    "x": 2108,
                    "y": 1328
                },
                {
                    "x": 2108,
                    "y": 1466
                },
                {
                    "x": 442,
                    "y": 1466
                }
            ],
            "category": "paragraph",
            "html": "<p id='150' style='font-size:18px'>Roy Schwartz, Maarten Sap, Ioannis Konstas, Li Zilles, Yejin Choi, and Noah A. Smith. The<br>effect of different writing tasks on linguistic style: A case study of the ROC story cloze task. In<br>Proceedings of CoNLL, 2017.</p>",
            "id": 150,
            "page": 12,
            "text": "Roy Schwartz, Maarten Sap, Ioannis Konstas, Li Zilles, Yejin Choi, and Noah A. Smith. The effect of different writing tasks on linguistic style: A case study of the ROC story cloze task. In Proceedings of CoNLL, 2017."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1499
                },
                {
                    "x": 2106,
                    "y": 1499
                },
                {
                    "x": 2106,
                    "y": 1637
                },
                {
                    "x": 443,
                    "y": 1637
                }
            ],
            "category": "paragraph",
            "html": "<p id='151' style='font-size:22px'>Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. Bidirectional attention<br>flow for machine comprehension. In Proceedings of the International Conference of Learning<br>Representations, 2017.</p>",
            "id": 151,
            "page": 12,
            "text": "Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. Bidirectional attention flow for machine comprehension. In Proceedings of the International Conference of Learning Representations, 2017."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1669
                },
                {
                    "x": 2107,
                    "y": 1669
                },
                {
                    "x": 2107,
                    "y": 1855
                },
                {
                    "x": 442,
                    "y": 1855
                }
            ],
            "category": "paragraph",
            "html": "<p id='152' style='font-size:20px'>Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and<br>Christopher Potts. Recursive deep models for semantic compositionality over a sentiment tree-<br>bank. In Proceedings of the Conference on Empirical Methods in Natural Language Processing,<br>pp. 1631-1642, 2013.</p>",
            "id": 152,
            "page": 12,
            "text": "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pp. 1631-1642, 2013."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1888
                },
                {
                    "x": 2107,
                    "y": 1888
                },
                {
                    "x": 2107,
                    "y": 2025
                },
                {
                    "x": 443,
                    "y": 2025
                }
            ],
            "category": "paragraph",
            "html": "<p id='153' style='font-size:20px'>Anders S⌀gaard and Yoav Goldberg. Deep multi-task learning with low level tasks supervised at<br>lower layers. In Proceedings of the 54th Annual Meeting of the Association for Computational<br>Linguistics (Volume 2: Short Papers), volume 2, pp. 231-235, 2016.</p>",
            "id": 153,
            "page": 12,
            "text": "Anders S⌀gaard and Yoav Goldberg. Deep multi-task learning with low level tasks supervised at lower layers. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), volume 2, pp. 231-235, 2016."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2058
                },
                {
                    "x": 2107,
                    "y": 2058
                },
                {
                    "x": 2107,
                    "y": 2197
                },
                {
                    "x": 442,
                    "y": 2197
                }
            ],
            "category": "paragraph",
            "html": "<p id='154' style='font-size:20px'>Sandeep Subramanian, Adam Trischler, Yoshua Bengio, and Christopher J. Pal. Learning general<br>purpose distributed sentence representations via large scale multi-task learning. In Proceedings<br>of the International Conference on Learning Representations, 2018.</p>",
            "id": 154,
            "page": 12,
            "text": "Sandeep Subramanian, Adam Trischler, Yoshua Bengio, and Christopher J. Pal. Learning general purpose distributed sentence representations via large scale multi-task learning. In Proceedings of the International Conference on Learning Representations, 2018."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2229
                },
                {
                    "x": 2107,
                    "y": 2229
                },
                {
                    "x": 2107,
                    "y": 2413
                },
                {
                    "x": 442,
                    "y": 2413
                }
            ],
            "category": "paragraph",
            "html": "<p id='155' style='font-size:20px'>Masatoshi Tsuchiya. Performance Impact Caused by Hidden Bias of Training Data for Recogniz-<br>ing Textual Entailment. In Proceedings of the Eleventh International Conference on Language<br>Resources and Evaluation (LREC 2018), Miyazaki, Japan, May 7-12, 2018 2018. European Lan-<br>guage Resources Association (ELRA).</p>",
            "id": 155,
            "page": 12,
            "text": "Masatoshi Tsuchiya. Performance Impact Caused by Hidden Bias of Training Data for Recognizing Textual Entailment. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan, May 7-12, 2018 2018. European Language Resources Association (ELRA)."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2446
                },
                {
                    "x": 2106,
                    "y": 2446
                },
                {
                    "x": 2106,
                    "y": 2584
                },
                {
                    "x": 443,
                    "y": 2584
                }
            ],
            "category": "paragraph",
            "html": "<p id='156' style='font-size:16px'>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,<br>Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-<br>mation Processing Systems, pp. 6000-6010, 2017.</p>",
            "id": 156,
            "page": 12,
            "text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pp. 6000-6010, 2017."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2618
                },
                {
                    "x": 2106,
                    "y": 2618
                },
                {
                    "x": 2106,
                    "y": 2708
                },
                {
                    "x": 443,
                    "y": 2708
                }
            ],
            "category": "paragraph",
            "html": "<p id='157' style='font-size:16px'>Ellen M Voorhees et al. The TREC-8 question answering track report. In TREC, volume 99, pp.<br>77-82, 1999.</p>",
            "id": 157,
            "page": 12,
            "text": "Ellen M Voorhees  The TREC-8 question answering track report. In TREC, volume 99, pp. 77-82, 1999."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2743
                },
                {
                    "x": 2105,
                    "y": 2743
                },
                {
                    "x": 2105,
                    "y": 2834
                },
                {
                    "x": 444,
                    "y": 2834
                }
            ],
            "category": "paragraph",
            "html": "<p id='158' style='font-size:20px'>Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments.<br>arXiv preprint 1805.12471, 2018.</p>",
            "id": 158,
            "page": 12,
            "text": "Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments. arXiv preprint 1805.12471, 2018."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 2869
                },
                {
                    "x": 2108,
                    "y": 2869
                },
                {
                    "x": 2108,
                    "y": 3054
                },
                {
                    "x": 445,
                    "y": 3054
                }
            ],
            "category": "paragraph",
            "html": "<p id='159' style='font-size:18px'>Aaron Steven White, Pushpendre Rastogi, Kevin Duh, and Benjamin Van Durme. Inference is<br>everything: Recasting semantic resources into a unified evaluation framework. In Proceedings<br>of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long<br>Papers), volume 1, pp. 996-1005, 2017.</p>",
            "id": 159,
            "page": 12,
            "text": "Aaron Steven White, Pushpendre Rastogi, Kevin Duh, and Benjamin Van Durme. Inference is everything: Recasting semantic resources into a unified evaluation framework. In Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers), volume 1, pp. 996-1005, 2017."
        },
        {
            "bounding_box": [
                {
                    "x": 1253,
                    "y": 3133
                },
                {
                    "x": 1300,
                    "y": 3133
                },
                {
                    "x": 1300,
                    "y": 3171
                },
                {
                    "x": 1253,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='160' style='font-size:14px'>12</footer>",
            "id": 160,
            "page": 12,
            "text": "12"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 111
                },
                {
                    "x": 1225,
                    "y": 111
                },
                {
                    "x": 1225,
                    "y": 156
                },
                {
                    "x": 444,
                    "y": 156
                }
            ],
            "category": "header",
            "html": "<header id='161' style='font-size:14px'>Published as a conference paper at ICLR 2019</header>",
            "id": 161,
            "page": 13,
            "text": "Published as a conference paper at ICLR 2019"
        },
        {
            "bounding_box": [
                {
                    "x": 472,
                    "y": 340
                },
                {
                    "x": 2082,
                    "y": 340
                },
                {
                    "x": 2082,
                    "y": 1415
                },
                {
                    "x": 472,
                    "y": 1415
                }
            ],
            "category": "table",
            "html": "<table id='162' style='font-size:14px'><tr><td></td><td></td><td colspan=\"2\">Single Sentence</td><td colspan=\"3\">Similarity and Paraphrase</td><td colspan=\"4\">Natural Language Inference</td></tr><tr><td>Model</td><td>Avg</td><td>CoLA</td><td>SST-2</td><td>MRPC</td><td>QQP</td><td>STS-B</td><td>MNLI</td><td>QNLI</td><td>RTE</td><td>WNLI</td></tr><tr><td colspan=\"11\">Single-Task Training</td></tr><tr><td>BiLSTM</td><td>66.7</td><td>17.6</td><td>87.5</td><td>77.9/85.1</td><td>85.3/82.0</td><td>71.6/72.0</td><td>66.7</td><td>77.0</td><td>58.5</td><td>56.3</td></tr><tr><td>+ELMo</td><td>68.7</td><td>44.1</td><td>91.5</td><td>70.8/82.3</td><td>88.0/84.3</td><td>70.3/70.5</td><td>68.6</td><td>71.2</td><td>53.4</td><td>56.3</td></tr><tr><td>+CoVe</td><td>66.8</td><td>25.1</td><td>89.2</td><td>76.5/83.4</td><td>86.2/81.8</td><td>70.7/70.8</td><td>62.4</td><td>74.4</td><td>59.6</td><td>54.9</td></tr><tr><td>+Attn</td><td>66.9</td><td>17.6</td><td>87.5</td><td>72.8/82.9</td><td>87.7/83.9</td><td>66.6/66.7</td><td>70.0</td><td>77.2</td><td>58.5</td><td>60.6</td></tr><tr><td>+Attn, ELMo</td><td>67.9</td><td>44.1</td><td>91.5</td><td>71.1/82.1</td><td>87.8/83.6</td><td>57.9/56.1</td><td>72.4</td><td>75.2</td><td>52.7</td><td>56.3</td></tr><tr><td>+Attn, CoVe</td><td>65.6</td><td>25.1</td><td>89.2</td><td>72.8/82.4</td><td>86.1/81.3</td><td>59.4/58.0</td><td>67.9</td><td>72.5</td><td>58.1</td><td>57.7</td></tr><tr><td colspan=\"11\">Multi-Task Training</td></tr><tr><td>BiLSTM</td><td>60.0</td><td>18.6</td><td>82.3</td><td>75.0/82.7</td><td>84.4/79.3</td><td>69.0/66.9</td><td>65.6</td><td>74.9</td><td>59.9</td><td>9.9</td></tr><tr><td>+ELMo</td><td>63.1</td><td>26.4</td><td>90.9</td><td>80.2/86.7</td><td>84.2/79.7</td><td>72.9/71.5</td><td>67.4</td><td>76.0</td><td>55.6</td><td>14.1</td></tr><tr><td>+CoVe</td><td>59.3</td><td>9.8</td><td>82.0</td><td>73.8/81.0</td><td>83.4/76.6</td><td>64.5/61.9</td><td>65.5</td><td>70.4</td><td>52.7</td><td>32.4</td></tr><tr><td>+Attn</td><td>60.5</td><td>15.2</td><td>83.1</td><td>77.5/85.1</td><td>82.6/77.2</td><td>72.4/70.5</td><td>68.0</td><td>73.7</td><td>61.7</td><td>9.9</td></tr><tr><td>+Attn, ELMo</td><td>67.3</td><td>36.7</td><td>91.1</td><td>80.6/86.6</td><td>84.6/79.6</td><td>74.4/72.9</td><td>74.6</td><td>80.4</td><td>61.4</td><td>22.5</td></tr><tr><td>+Attn, CoVe</td><td>61.4</td><td>17.4</td><td>82.1</td><td>71.3/80.1</td><td>83.4/77.7</td><td>68.6/66.7</td><td>68.2</td><td>73.2</td><td>58.5</td><td>29.6</td></tr><tr><td colspan=\"11\">Pre-Trained Sentence Representation Models</td></tr><tr><td>CBoW</td><td>61.4</td><td>4.6</td><td>79.5</td><td>75.0/83.7</td><td>75.0/65.5</td><td>70.6/71.1</td><td>57.1</td><td>62.5</td><td>71.9</td><td>56.3</td></tr><tr><td>Skip-Thought</td><td>61.8</td><td>0.0</td><td>82.0</td><td>76.2/84.3</td><td>78.9/70.7</td><td>74.8/74.8</td><td>63.4</td><td>58.5</td><td>73.4</td><td>49.3</td></tr><tr><td>InferSent</td><td>65.7</td><td>8.6</td><td>83.9</td><td>76.5/84.1</td><td>81.7/75.9</td><td>80.2/80.4</td><td>67.8</td><td>63.5</td><td>71.5</td><td>56.3</td></tr><tr><td>DisSent</td><td>63.8</td><td>11.7</td><td>82.5</td><td>77.0/84.4</td><td>81.8/75.6</td><td>68.9/69.0</td><td>61.2</td><td>59.9</td><td>73.9</td><td>56.3</td></tr><tr><td>GenSen</td><td>67.8</td><td>10.3</td><td>87.2</td><td>80.4/86.2</td><td>82.6/76.6</td><td>81.3/81.8</td><td>71.4</td><td>62.5</td><td>78.4</td><td>56.3</td></tr></table>",
            "id": 162,
            "page": 13,
            "text": "Single Sentence Similarity and Paraphrase Natural Language Inference  Model Avg CoLA SST-2 MRPC QQP STS-B MNLI QNLI RTE WNLI  Single-Task Training  BiLSTM 66.7 17.6 87.5 77.9/85.1 85.3/82.0 71.6/72.0 66.7 77.0 58.5 56.3  +ELMo 68.7 44.1 91.5 70.8/82.3 88.0/84.3 70.3/70.5 68.6 71.2 53.4 56.3  +CoVe 66.8 25.1 89.2 76.5/83.4 86.2/81.8 70.7/70.8 62.4 74.4 59.6 54.9  +Attn 66.9 17.6 87.5 72.8/82.9 87.7/83.9 66.6/66.7 70.0 77.2 58.5 60.6  +Attn, ELMo 67.9 44.1 91.5 71.1/82.1 87.8/83.6 57.9/56.1 72.4 75.2 52.7 56.3  +Attn, CoVe 65.6 25.1 89.2 72.8/82.4 86.1/81.3 59.4/58.0 67.9 72.5 58.1 57.7  Multi-Task Training  BiLSTM 60.0 18.6 82.3 75.0/82.7 84.4/79.3 69.0/66.9 65.6 74.9 59.9 9.9  +ELMo 63.1 26.4 90.9 80.2/86.7 84.2/79.7 72.9/71.5 67.4 76.0 55.6 14.1  +CoVe 59.3 9.8 82.0 73.8/81.0 83.4/76.6 64.5/61.9 65.5 70.4 52.7 32.4  +Attn 60.5 15.2 83.1 77.5/85.1 82.6/77.2 72.4/70.5 68.0 73.7 61.7 9.9  +Attn, ELMo 67.3 36.7 91.1 80.6/86.6 84.6/79.6 74.4/72.9 74.6 80.4 61.4 22.5  +Attn, CoVe 61.4 17.4 82.1 71.3/80.1 83.4/77.7 68.6/66.7 68.2 73.2 58.5 29.6  Pre-Trained Sentence Representation Models  CBoW 61.4 4.6 79.5 75.0/83.7 75.0/65.5 70.6/71.1 57.1 62.5 71.9 56.3  Skip-Thought 61.8 0.0 82.0 76.2/84.3 78.9/70.7 74.8/74.8 63.4 58.5 73.4 49.3  InferSent 65.7 8.6 83.9 76.5/84.1 81.7/75.9 80.2/80.4 67.8 63.5 71.5 56.3  DisSent 63.8 11.7 82.5 77.0/84.4 81.8/75.6 68.9/69.0 61.2 59.9 73.9 56.3  GenSen 67.8 10.3 87.2 80.4/86.2 82.6/76.6 81.3/81.8 71.4 62.5 78.4"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1450
                },
                {
                    "x": 2107,
                    "y": 1450
                },
                {
                    "x": 2107,
                    "y": 1639
                },
                {
                    "x": 441,
                    "y": 1639
                }
            ],
            "category": "paragraph",
            "html": "<p id='163' style='font-size:16px'>Table 6: Baseline performance on the GLUE tasks' development sets. For MNLI, we report accuracy<br>averaged over the matched and mismatched test sets. For MRPC and QQP, we report accuracy and<br>F1. For STS-B, we report Pearson and Spearman correlation. For CoLA, we report Matthews<br>correlation. For all other tasks we report accuracy. All values are scaled by 100.</p>",
            "id": 163,
            "page": 13,
            "text": "Table 6: Baseline performance on the GLUE tasks' development sets. For MNLI, we report accuracy averaged over the matched and mismatched test sets. For MRPC and QQP, we report accuracy and F1. For STS-B, we report Pearson and Spearman correlation. For CoLA, we report Matthews correlation. For all other tasks we report accuracy. All values are scaled by 100."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1768
                },
                {
                    "x": 2106,
                    "y": 1768
                },
                {
                    "x": 2106,
                    "y": 1909
                },
                {
                    "x": 443,
                    "y": 1909
                }
            ],
            "category": "paragraph",
            "html": "<p id='164' style='font-size:18px'>Janyce Wiebe, Theresa Wilson, and Claire Cardie. Annotating expressions of opinions and emo-<br>tions in language. In Proceedings of the International Conference on Language Resources and<br>Evaluation, volume 39, pp. 165-210. Springer, 2005.</p>",
            "id": 164,
            "page": 13,
            "text": "Janyce Wiebe, Theresa Wilson, and Claire Cardie. Annotating expressions of opinions and emotions in language. In Proceedings of the International Conference on Language Resources and Evaluation, volume 39, pp. 165-210. Springer, 2005."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2030
                },
                {
                    "x": 2107,
                    "y": 2030
                },
                {
                    "x": 2107,
                    "y": 2171
                },
                {
                    "x": 443,
                    "y": 2171
                }
            ],
            "category": "paragraph",
            "html": "<p id='165' style='font-size:20px'>Adina Williams, Nikita Nangia, and Samuel R. Bowman. A broad-coverage challenge corpus for<br>sentence understanding through inference. In Proceedings of the North American Chapter of the<br>Association for Computational Linguistics: Human Language Technologies, 2018.</p>",
            "id": 165,
            "page": 13,
            "text": "Adina Williams, Nikita Nangia, and Samuel R. Bowman. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2018."
        },
        {
            "bounding_box": [
                {
                    "x": 446,
                    "y": 2290
                },
                {
                    "x": 2107,
                    "y": 2290
                },
                {
                    "x": 2107,
                    "y": 2479
                },
                {
                    "x": 446,
                    "y": 2479
                }
            ],
            "category": "paragraph",
            "html": "<p id='166' style='font-size:18px'>Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and<br>Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching<br>movies and reading books. In Proceedings of the International Conference on Computer Vision,<br>pp. 19-27, 2015.</p>",
            "id": 166,
            "page": 13,
            "text": "Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the International Conference on Computer Vision, pp. 19-27, 2015."
        },
        {
            "bounding_box": [
                {
                    "x": 446,
                    "y": 2597
                },
                {
                    "x": 1312,
                    "y": 2597
                },
                {
                    "x": 1312,
                    "y": 2649
                },
                {
                    "x": 446,
                    "y": 2649
                }
            ],
            "category": "paragraph",
            "html": "<p id='167' style='font-size:22px'>A ADDITIONAL BENCHMARK DETAILS</p>",
            "id": 167,
            "page": 13,
            "text": "A ADDITIONAL BENCHMARK DETAILS"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2728
                },
                {
                    "x": 2109,
                    "y": 2728
                },
                {
                    "x": 2109,
                    "y": 3055
                },
                {
                    "x": 443,
                    "y": 3055
                }
            ],
            "category": "paragraph",
            "html": "<p id='168' style='font-size:14px'>QNLI To construct a balanced dataset, we select all pairs in which the most similar sentence to<br>the question was not the answer sentence, as well as an equal amount of cases in which the correct<br>sentence was the most similar to the question, but another distracting sentence was a close second.<br>Our similarity metric is based on CBoW representations with pre-trained GloVe embeddings. This<br>approach to converting pre-existing datasets into NLI format is closely related to recent work by<br>White et al. (2017), as well as to the original motivation for textual entailment presented by Dagan<br>et al. (2006). Both argue that many NLP tasks can be productively reduced to textual entailment.</p>",
            "id": 168,
            "page": 13,
            "text": "QNLI To construct a balanced dataset, we select all pairs in which the most similar sentence to the question was not the answer sentence, as well as an equal amount of cases in which the correct sentence was the most similar to the question, but another distracting sentence was a close second. Our similarity metric is based on CBoW representations with pre-trained GloVe embeddings. This approach to converting pre-existing datasets into NLI format is closely related to recent work by White  (2017), as well as to the original motivation for textual entailment presented by Dagan  (2006). Both argue that many NLP tasks can be productively reduced to textual entailment."
        },
        {
            "bounding_box": [
                {
                    "x": 1253,
                    "y": 3133
                },
                {
                    "x": 1299,
                    "y": 3133
                },
                {
                    "x": 1299,
                    "y": 3172
                },
                {
                    "x": 1253,
                    "y": 3172
                }
            ],
            "category": "footer",
            "html": "<footer id='169' style='font-size:18px'>13</footer>",
            "id": 169,
            "page": 13,
            "text": "13"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 110
                },
                {
                    "x": 1225,
                    "y": 110
                },
                {
                    "x": 1225,
                    "y": 157
                },
                {
                    "x": 444,
                    "y": 157
                }
            ],
            "category": "header",
            "html": "<header id='170' style='font-size:14px'>Published as a conference paper at ICLR 2019</header>",
            "id": 170,
            "page": 14,
            "text": "Published as a conference paper at ICLR 2019"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 340
                },
                {
                    "x": 1247,
                    "y": 340
                },
                {
                    "x": 1247,
                    "y": 394
                },
                {
                    "x": 444,
                    "y": 394
                }
            ],
            "category": "paragraph",
            "html": "<p id='171' style='font-size:22px'>B ADDITIONAL BASELINE DETAILS</p>",
            "id": 171,
            "page": 14,
            "text": "B ADDITIONAL BASELINE DETAILS"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 442
                },
                {
                    "x": 1009,
                    "y": 442
                },
                {
                    "x": 1009,
                    "y": 490
                },
                {
                    "x": 445,
                    "y": 490
                }
            ],
            "category": "paragraph",
            "html": "<p id='172' style='font-size:16px'>B.1 ATTENTION MECHANISM</p>",
            "id": 172,
            "page": 14,
            "text": "B.1 ATTENTION MECHANISM"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 526
                },
                {
                    "x": 2110,
                    "y": 526
                },
                {
                    "x": 2110,
                    "y": 822
                },
                {
                    "x": 441,
                    "y": 822
                }
            ],
            "category": "paragraph",
            "html": "<p id='173' style='font-size:14px'>We implement our attention mechanism as follows: given two sequences of hidden states<br>For each<br>u1, u2, · · · , uM and v1, V2, · · · , UN, we first compute matrix H where Hij = Ui · vj.<br>ui, we get attention weights ai by taking a softmax over the ith row of H, and get the correspond-<br>ing context vector Vi = �j aijuj by taking the attention-weighted sum of the vj. We pass a second<br>BiLSTM with max pooling over the sequence [u1; v1], · · [uM; �M] to produce u'. We process the<br>vj vectors analogously to obtain v'. Finally, we feed [u';v';|u' - 04; u' * v'] into a classifier.</p>",
            "id": 173,
            "page": 14,
            "text": "We implement our attention mechanism as follows: given two sequences of hidden states For each u1, u2, · · · , uM and v1, V2, · · · , UN, we first compute matrix H where Hij = Ui · vj. ui, we get attention weights ai by taking a softmax over the ith row of H, and get the corresponding context vector Vi = �j aijuj by taking the attention-weighted sum of the vj. We pass a second BiLSTM with max pooling over the sequence [u1; v1], · · [uM; �M] to produce u'. We process the vj vectors analogously to obtain v'. Finally, we feed [u';v';|u' - 04; u' * v'] into a classifier."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 871
                },
                {
                    "x": 743,
                    "y": 871
                },
                {
                    "x": 743,
                    "y": 919
                },
                {
                    "x": 444,
                    "y": 919
                }
            ],
            "category": "paragraph",
            "html": "<p id='174' style='font-size:18px'>B.2 TRAINING</p>",
            "id": 174,
            "page": 14,
            "text": "B.2 TRAINING"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 955
                },
                {
                    "x": 2109,
                    "y": 955
                },
                {
                    "x": 2109,
                    "y": 1372
                },
                {
                    "x": 442,
                    "y": 1372
                }
            ],
            "category": "paragraph",
            "html": "<p id='175' style='font-size:16px'>We train our models with the BiLSTM sentence encoder and post-attention BiLSTMs shared across<br>tasks, and classifiers trained separately for each task. For each training update, we sample a task<br>to train with a probability proportional to the number of training examples for each task. We scale<br>each task's loss inversely proportional to the number of examples for that task, which we found to<br>improve overall performance. We train our models with Adam (Kingma & Ba, 2015) with initial<br>learning rate 10-3 batch size 128, and gradient clipping. We use macro-average score over all tasks<br>,<br>as our validation metric, and perform a validation check every 10k updates. We divide the learning<br>rate by 5 whenever validation performance does not improve. We stop training when the learning<br>rate drops below 10-5 performance does not improve after 5 validation checks.<br>or</p>",
            "id": 175,
            "page": 14,
            "text": "We train our models with the BiLSTM sentence encoder and post-attention BiLSTMs shared across tasks, and classifiers trained separately for each task. For each training update, we sample a task to train with a probability proportional to the number of training examples for each task. We scale each task's loss inversely proportional to the number of examples for that task, which we found to improve overall performance. We train our models with Adam (Kingma & Ba, 2015) with initial learning rate 10-3 batch size 128, and gradient clipping. We use macro-average score over all tasks , as our validation metric, and perform a validation check every 10k updates. We divide the learning rate by 5 whenever validation performance does not improve. We stop training when the learning rate drops below 10-5 performance does not improve after 5 validation checks. or"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 1423
                },
                {
                    "x": 1247,
                    "y": 1423
                },
                {
                    "x": 1247,
                    "y": 1471
                },
                {
                    "x": 445,
                    "y": 1471
                }
            ],
            "category": "paragraph",
            "html": "<p id='176' style='font-size:16px'>B.3 SENTENCE REPRESENTATION MODELS</p>",
            "id": 176,
            "page": 14,
            "text": "B.3 SENTENCE REPRESENTATION MODELS"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 1510
                },
                {
                    "x": 1429,
                    "y": 1510
                },
                {
                    "x": 1429,
                    "y": 1557
                },
                {
                    "x": 445,
                    "y": 1557
                }
            ],
            "category": "paragraph",
            "html": "<p id='177' style='font-size:16px'>We evaluate the following sentence representation models:</p>",
            "id": 177,
            "page": 14,
            "text": "We evaluate the following sentence representation models:"
        },
        {
            "bounding_box": [
                {
                    "x": 537,
                    "y": 1586
                },
                {
                    "x": 2110,
                    "y": 1586
                },
                {
                    "x": 2110,
                    "y": 2241
                },
                {
                    "x": 537,
                    "y": 2241
                }
            ],
            "category": "paragraph",
            "html": "<p id='178' style='font-size:16px'>1. CBoW, the average of the GloVe embeddings of the tokens in the sentence.<br>2. Skip-Thought (Kiros et al., 2015), a sequence-to-sequence(s) model trained to generate the<br>previous and next sentences given the middle sentence. We use the original pre-trained<br>model7 trained on sequences of sentences from the Toronto Book Corpus (Zhu et al. 2015,<br>TBC).<br>3. InferSent (Conneau et al., 2017), a BiLSTM with max-pooling trained on MNLI and SNLI.<br>4. DisSent (Nie et al., 2017), a BiLSTM with max-pooling trained to predict the discourse<br>marker (because, so, etc.) relating two sentences on data derived from TBC. We use the<br>variant trained for eight-way classification.<br>5. GenSen (Subramanian et al., 2018), a sequence-to-sequence model trained on a variety of<br>supervised and unsupervised objectives. We use the variant of the model trained on both<br>MNLI and SNLI, the Skip-Thought objective on TBC, and a constituency parsing objective<br>on the Billion Word Benchmark.</p>",
            "id": 178,
            "page": 14,
            "text": "1. CBoW, the average of the GloVe embeddings of the tokens in the sentence. 2. Skip-Thought (Kiros , 2015), a sequence-to-sequence(s) model trained to generate the previous and next sentences given the middle sentence. We use the original pre-trained model7 trained on sequences of sentences from the Toronto Book Corpus (Zhu  2015, TBC). 3. InferSent (Conneau , 2017), a BiLSTM with max-pooling trained on MNLI and SNLI. 4. DisSent (Nie , 2017), a BiLSTM with max-pooling trained to predict the discourse marker (because, so, etc.) relating two sentences on data derived from TBC. We use the variant trained for eight-way classification. 5. GenSen (Subramanian , 2018), a sequence-to-sequence model trained on a variety of supervised and unsupervised objectives. We use the variant of the model trained on both MNLI and SNLI, the Skip-Thought objective on TBC, and a constituency parsing objective on the Billion Word Benchmark."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2274
                },
                {
                    "x": 2107,
                    "y": 2274
                },
                {
                    "x": 2107,
                    "y": 2371
                },
                {
                    "x": 441,
                    "y": 2371
                }
            ],
            "category": "paragraph",
            "html": "<p id='179' style='font-size:16px'>We train task-specific classifiers on top of frozen sentence encoders, using the default parameters<br>from SentEval. See https://gihub.com/nyu-mil/SentEval for details and code.</p>",
            "id": 179,
            "page": 14,
            "text": "We train task-specific classifiers on top of frozen sentence encoders, using the default parameters from SentEval. See https://gihub.com/nyu-mil/SentEval for details and code."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 2432
                },
                {
                    "x": 1171,
                    "y": 2432
                },
                {
                    "x": 1171,
                    "y": 2486
                },
                {
                    "x": 445,
                    "y": 2486
                }
            ],
            "category": "paragraph",
            "html": "<p id='180' style='font-size:22px'>C DEVELOPMENT SET RESULTS</p>",
            "id": 180,
            "page": 14,
            "text": "C DEVELOPMENT SET RESULTS"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2533
                },
                {
                    "x": 2107,
                    "y": 2533
                },
                {
                    "x": 2107,
                    "y": 2675
                },
                {
                    "x": 441,
                    "y": 2675
                }
            ],
            "category": "paragraph",
            "html": "<p id='181' style='font-size:14px'>The GLUE website limits users to two submissions per day in order to avoid overfitting to the<br>private test data. To provide a reference for future work on GLUE, we present the best development<br>set results achieved by our baselines in Table 6.</p>",
            "id": 181,
            "page": 14,
            "text": "The GLUE website limits users to two submissions per day in order to avoid overfitting to the private test data. To provide a reference for future work on GLUE, we present the best development set results achieved by our baselines in Table 6."
        },
        {
            "bounding_box": [
                {
                    "x": 446,
                    "y": 2738
                },
                {
                    "x": 1237,
                    "y": 2738
                },
                {
                    "x": 1237,
                    "y": 2791
                },
                {
                    "x": 446,
                    "y": 2791
                }
            ],
            "category": "paragraph",
            "html": "<p id='182' style='font-size:20px'>D BENCHMARK WEBSITE DETAILS</p>",
            "id": 182,
            "page": 14,
            "text": "D BENCHMARK WEBSITE DETAILS"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2840
                },
                {
                    "x": 2108,
                    "y": 2840
                },
                {
                    "x": 2108,
                    "y": 2980
                },
                {
                    "x": 442,
                    "y": 2980
                }
            ],
            "category": "paragraph",
            "html": "<p id='183' style='font-size:16px'>GLUE's online platform is built using React, Redux and TypeScript. We use Google Firebase for<br>data storage and Google Cloud Functions to host and run our grading script when a submission is<br>made. Figure 1 shows the visual presentation of our baselines on the leaderboard.</p>",
            "id": 183,
            "page": 14,
            "text": "GLUE's online platform is built using React, Redux and TypeScript. We use Google Firebase for data storage and Google Cloud Functions to host and run our grading script when a submission is made. Figure 1 shows the visual presentation of our baselines on the leaderboard."
        },
        {
            "bounding_box": [
                {
                    "x": 499,
                    "y": 3007
                },
                {
                    "x": 1283,
                    "y": 3007
                },
                {
                    "x": 1283,
                    "y": 3054
                },
                {
                    "x": 499,
                    "y": 3054
                }
            ],
            "category": "paragraph",
            "html": "<p id='184' style='font-size:20px'>7github . com/ ryankiros/ skip-thoughts</p>",
            "id": 184,
            "page": 14,
            "text": "7github . com/ ryankiros/ skip-thoughts"
        },
        {
            "bounding_box": [
                {
                    "x": 1252,
                    "y": 3132
                },
                {
                    "x": 1300,
                    "y": 3132
                },
                {
                    "x": 1300,
                    "y": 3171
                },
                {
                    "x": 1252,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='185' style='font-size:14px'>14</footer>",
            "id": 185,
            "page": 14,
            "text": "14"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 111
                },
                {
                    "x": 1225,
                    "y": 111
                },
                {
                    "x": 1225,
                    "y": 157
                },
                {
                    "x": 445,
                    "y": 157
                }
            ],
            "category": "header",
            "html": "<header id='186' style='font-size:14px'>Published as a conference paper at ICLR 2019</header>",
            "id": 186,
            "page": 15,
            "text": "Published as a conference paper at ICLR 2019"
        },
        {
            "bounding_box": [
                {
                    "x": 446,
                    "y": 336
                },
                {
                    "x": 2105,
                    "y": 336
                },
                {
                    "x": 2105,
                    "y": 1398
                },
                {
                    "x": 446,
                    "y": 1398
                }
            ],
            "category": "figure",
            "html": "<figure><img id='187' style='font-size:14px' alt=\"PRIMARY AUXILIARY\nRankName Model URL Score CoLA SST-2 MRPC STS-B QQP MNLI-m MNLI-mm QNLI RTE WNLI\n1 GLUE Baselines BiLSTM+ELMo+Attn 68.9 18.9 91.6 77.3/83.5 72.8/71.1 83.5/63.3 75.6 75.9 81.7 61.2 65.1\nGenSen  66.6 7.7 83.1 76.6/83.0 79.3/79.2 82.9/59.8 71.4 71.3 82.3 59.2 65.1\nSingle Task BiLSTM+ELMo V 66.2 35.0 90.2 69.0/80.8 64.0/60.2 85.7/65.6 72.9 73.4 69.4 50.1 65.1\nBILSTM+Attn 65.7 0.0 85.0 75.1/83.7 73.9/71.8 84.3/63.6 72.2 72.1 82.1 61.7 63.7\nBiLSTM+ELMo V 64.9 27.5 89.6 76.2/83.5 67.0/65.9 78.5/57.8 67.1 68.0 66.7 55.7 62.3\nSingle Task BiLSTM+ELMo+Attn V 64.8 35.0 90.2 68.8/80.2 55.5/52.5 86.5/66.1 76.9 76.7 61.1 50.3 65.1\nInferSent V 64.7 4.5 85.1 74.1/81.2 75.9/75.3 81.7/59.1 66.1 65.7 79.8 58.0 65.1\nBiLSTM+CoVe+Attn V 64.3 19.4 83.6 75.2/83.0 72.3/71.1 84.9/61.1 69.9 68.7 78.9 38.3 65.1\nBiLSTM 63.5 24.0 85.8 71.9/82.1 68.8/67.0 80.2/59.1 65.8 66.0 71.1 46.8 63.7\nSingle Task BiLSTM+CoVe V 62.4 14.5 88.5 73.4/81.4 67.2/64.1 83.3/59.4 64.5 64.8 64.8 53.5 61.6\nBiLSTM+CoVe V 62.2 16.2 84.3 71.8/80.0 68.0/67.1 82.0/59.1 65.3 65.9 70.4 44.2 65.1\nDisSent V 62.1 4.9 83.7 74.1/81.7 66.1/64.8 82.6/59.5 58.7 59.1 75.2 56.4 65.1\nSingle Task BiLSTM 62.0 15.7 85.9 69.3/79.4 66.0/62.8 81.7/61.4 70.3 70.8 60.8 52.8 62.3\nSkip-Thought V 61.5 0.0 81.8 71.7/80.8 71.8/69.7 82.2/56.4 62.9 62.8 74.7 53.1 65.1\nSingle Task BiLSTM+CoVe+Attn  60.8 14.5 88.5 68.6/79.7 57.2/53.6 84.1/60.1 71.6 71.5 53.8 52.7 64.4\nSingle Task BiLSTM+Attn 60.0 15.7 85.9 68.5/80.3 59.3/55.8 83.5/62.9 74.2 73.8 51.9 51.9 55.5\nCBOW 58.9 0.0 80.0 73.4/81.5 61.2/58.7 79.1/51.4 56.0 56.4 75.1 54.1 62.3\" data-coord=\"top-left:(446,336); bottom-right:(2105,1398)\" /></figure>",
            "id": 187,
            "page": 15,
            "text": "PRIMARY AUXILIARY RankName Model URL Score CoLA SST-2 MRPC STS-B QQP MNLI-m MNLI-mm QNLI RTE WNLI 1 GLUE Baselines BiLSTM+ELMo+Attn 68.9 18.9 91.6 77.3/83.5 72.8/71.1 83.5/63.3 75.6 75.9 81.7 61.2 65.1 GenSen  66.6 7.7 83.1 76.6/83.0 79.3/79.2 82.9/59.8 71.4 71.3 82.3 59.2 65.1 Single Task BiLSTM+ELMo V 66.2 35.0 90.2 69.0/80.8 64.0/60.2 85.7/65.6 72.9 73.4 69.4 50.1 65.1 BILSTM+Attn 65.7 0.0 85.0 75.1/83.7 73.9/71.8 84.3/63.6 72.2 72.1 82.1 61.7 63.7 BiLSTM+ELMo V 64.9 27.5 89.6 76.2/83.5 67.0/65.9 78.5/57.8 67.1 68.0 66.7 55.7 62.3 Single Task BiLSTM+ELMo+Attn V 64.8 35.0 90.2 68.8/80.2 55.5/52.5 86.5/66.1 76.9 76.7 61.1 50.3 65.1 InferSent V 64.7 4.5 85.1 74.1/81.2 75.9/75.3 81.7/59.1 66.1 65.7 79.8 58.0 65.1 BiLSTM+CoVe+Attn V 64.3 19.4 83.6 75.2/83.0 72.3/71.1 84.9/61.1 69.9 68.7 78.9 38.3 65.1 BiLSTM 63.5 24.0 85.8 71.9/82.1 68.8/67.0 80.2/59.1 65.8 66.0 71.1 46.8 63.7 Single Task BiLSTM+CoVe V 62.4 14.5 88.5 73.4/81.4 67.2/64.1 83.3/59.4 64.5 64.8 64.8 53.5 61.6 BiLSTM+CoVe V 62.2 16.2 84.3 71.8/80.0 68.0/67.1 82.0/59.1 65.3 65.9 70.4 44.2 65.1 DisSent V 62.1 4.9 83.7 74.1/81.7 66.1/64.8 82.6/59.5 58.7 59.1 75.2 56.4 65.1 Single Task BiLSTM 62.0 15.7 85.9 69.3/79.4 66.0/62.8 81.7/61.4 70.3 70.8 60.8 52.8 62.3 Skip-Thought V 61.5 0.0 81.8 71.7/80.8 71.8/69.7 82.2/56.4 62.9 62.8 74.7 53.1 65.1 Single Task BiLSTM+CoVe+Attn  60.8 14.5 88.5 68.6/79.7 57.2/53.6 84.1/60.1 71.6 71.5 53.8 52.7 64.4 Single Task BiLSTM+Attn 60.0 15.7 85.9 68.5/80.3 59.3/55.8 83.5/62.9 74.2 73.8 51.9 51.9 55.5 CBOW 58.9 0.0 80.0 73.4/81.5 61.2/58.7 79.1/51.4 56.0 56.4 75.1 54.1 62.3"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1450
                },
                {
                    "x": 2104,
                    "y": 1450
                },
                {
                    "x": 2104,
                    "y": 1544
                },
                {
                    "x": 442,
                    "y": 1544
                }
            ],
            "category": "caption",
            "html": "<caption id='188' style='font-size:18px'>Figure 1: The benchmark website leaderboard. An expanded view shows additional details about<br>each submission, including a brief prose description and parameter count.</caption>",
            "id": 188,
            "page": 15,
            "text": "Figure 1: The benchmark website leaderboard. An expanded view shows additional details about each submission, including a brief prose description and parameter count."
        },
        {
            "bounding_box": [
                {
                    "x": 537,
                    "y": 1594
                },
                {
                    "x": 2010,
                    "y": 1594
                },
                {
                    "x": 2010,
                    "y": 1853
                },
                {
                    "x": 537,
                    "y": 1853
                }
            ],
            "category": "table",
            "html": "<table id='189' style='font-size:14px'><tr><td>Category</td><td>Count</td><td>% Neutral</td><td>% Contradiction</td><td>% Entailment</td></tr><tr><td>Lexical Semantics</td><td>368</td><td>31.0</td><td>27.2</td><td>41.8</td></tr><tr><td>Predicate-Argument Structure</td><td>424</td><td>37.0</td><td>13.7</td><td>49.3</td></tr><tr><td>Logic</td><td>364</td><td>37.6</td><td>26.9</td><td>35.4</td></tr><tr><td>Knowledge</td><td>284</td><td>26.4</td><td>31.7</td><td>41.9</td></tr></table>",
            "id": 189,
            "page": 15,
            "text": "Category Count % Neutral % Contradiction % Entailment  Lexical Semantics 368 31.0 27.2 41.8  Predicate-Argument Structure 424 37.0 13.7 49.3  Logic 364 37.6 26.9 35.4  Knowledge 284 26.4 31.7"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1888
                },
                {
                    "x": 2107,
                    "y": 1888
                },
                {
                    "x": 2107,
                    "y": 1986
                },
                {
                    "x": 441,
                    "y": 1986
                }
            ],
            "category": "paragraph",
            "html": "<p id='190' style='font-size:20px'>Table 7: Diagnostic dataset statistics by coarse-grained category. Note that some examples may be<br>tagged with phenomena belonging to multiple categories.</p>",
            "id": 190,
            "page": 15,
            "text": "Table 7: Diagnostic dataset statistics by coarse-grained category. Note that some examples may be tagged with phenomena belonging to multiple categories."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 2070
                },
                {
                    "x": 1430,
                    "y": 2070
                },
                {
                    "x": 1430,
                    "y": 2124
                },
                {
                    "x": 445,
                    "y": 2124
                }
            ],
            "category": "paragraph",
            "html": "<p id='191' style='font-size:22px'>E ADDITIONAL DIAGNOSTIC DATA DETAILS</p>",
            "id": 191,
            "page": 15,
            "text": "E ADDITIONAL DIAGNOSTIC DATA DETAILS"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2178
                },
                {
                    "x": 2106,
                    "y": 2178
                },
                {
                    "x": 2106,
                    "y": 2638
                },
                {
                    "x": 443,
                    "y": 2638
                }
            ],
            "category": "paragraph",
            "html": "<p id='192' style='font-size:18px'>The dataset is designed to allow for analyzing many levels of natural language understanding, from<br>word meaning and sentence structure to high-level reasoning and application of world knowledge.<br>To make this kind of analysis feasible, we first identify four broad categories of phenomena: Lexical<br>Semantics, Predicate-Argument Structure, Logic, and Knowledge. However, since these categories<br>are vague, we divide each into a larger set of fine-grained subcategories. Descriptions of all of the<br>fine-grained categories are given in the remainder of this section. These categories are just one lens<br>that can be used to understand linguistic phenomena and entailment, and there is certainly room<br>to argue about how examples should be categorized, what the categories should be, etc. These<br>categories are not based on any particular linguistic theory, but broadly based on issues that linguists<br>have often identified and modeled in the study of syntax and semantics.</p>",
            "id": 192,
            "page": 15,
            "text": "The dataset is designed to allow for analyzing many levels of natural language understanding, from word meaning and sentence structure to high-level reasoning and application of world knowledge. To make this kind of analysis feasible, we first identify four broad categories of phenomena: Lexical Semantics, Predicate-Argument Structure, Logic, and Knowledge. However, since these categories are vague, we divide each into a larger set of fine-grained subcategories. Descriptions of all of the fine-grained categories are given in the remainder of this section. These categories are just one lens that can be used to understand linguistic phenomena and entailment, and there is certainly room to argue about how examples should be categorized, what the categories should be, etc. These categories are not based on any particular linguistic theory, but broadly based on issues that linguists have often identified and modeled in the study of syntax and semantics."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2660
                },
                {
                    "x": 2107,
                    "y": 2660
                },
                {
                    "x": 2107,
                    "y": 2984
                },
                {
                    "x": 442,
                    "y": 2984
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='193' style='font-size:16px'>The dataset is provided not as a benchmark, but as an analysis tool to paint in broad strokes the<br>kinds of phenomena a model may or may not capture, and to provide a set of examples that can<br>serve for error analysis, qualitative model comparison, and development of adversarial examples<br>that expose a model's weaknesses. Because the distribution of language is somewhat arbitrary, it<br>will not be helpful to compare performance of the same model on different categories. Rather, we<br>recommend comparing performance that different models score on the same category, or using the<br>reported scores as a guide for error analysis.</p>",
            "id": 193,
            "page": 15,
            "text": "The dataset is provided not as a benchmark, but as an analysis tool to paint in broad strokes the kinds of phenomena a model may or may not capture, and to provide a set of examples that can serve for error analysis, qualitative model comparison, and development of adversarial examples that expose a model's weaknesses. Because the distribution of language is somewhat arbitrary, it will not be helpful to compare performance of the same model on different categories. Rather, we recommend comparing performance that different models score on the same category, or using the reported scores as a guide for error analysis."
        },
        {
            "bounding_box": [
                {
                    "x": 446,
                    "y": 3005
                },
                {
                    "x": 1996,
                    "y": 3005
                },
                {
                    "x": 1996,
                    "y": 3053
                },
                {
                    "x": 446,
                    "y": 3053
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='194' style='font-size:18px'>We show coarse-grain category counts and label distributions of the diagnostic set in Table 7.</p>",
            "id": 194,
            "page": 15,
            "text": "We show coarse-grain category counts and label distributions of the diagnostic set in Table 7."
        },
        {
            "bounding_box": [
                {
                    "x": 1252,
                    "y": 3131
                },
                {
                    "x": 1300,
                    "y": 3131
                },
                {
                    "x": 1300,
                    "y": 3172
                },
                {
                    "x": 1252,
                    "y": 3172
                }
            ],
            "category": "footer",
            "html": "<footer id='195' style='font-size:16px'>15</footer>",
            "id": 195,
            "page": 15,
            "text": "15"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 111
                },
                {
                    "x": 1225,
                    "y": 111
                },
                {
                    "x": 1225,
                    "y": 157
                },
                {
                    "x": 445,
                    "y": 157
                }
            ],
            "category": "header",
            "html": "<header id='196' style='font-size:14px'>Published as a conference paper at ICLR 2019</header>",
            "id": 196,
            "page": 16,
            "text": "Published as a conference paper at ICLR 2019"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 346
                },
                {
                    "x": 942,
                    "y": 346
                },
                {
                    "x": 942,
                    "y": 392
                },
                {
                    "x": 445,
                    "y": 392
                }
            ],
            "category": "paragraph",
            "html": "<p id='197' style='font-size:16px'>E.1 LEXICAL SEMANTICS</p>",
            "id": 197,
            "page": 16,
            "text": "E.1 LEXICAL SEMANTICS"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 435
                },
                {
                    "x": 1351,
                    "y": 435
                },
                {
                    "x": 1351,
                    "y": 483
                },
                {
                    "x": 443,
                    "y": 483
                }
            ],
            "category": "paragraph",
            "html": "<p id='198' style='font-size:16px'>These phenomena center on aspects of word meaning.</p>",
            "id": 198,
            "page": 16,
            "text": "These phenomena center on aspects of word meaning."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 537
                },
                {
                    "x": 2108,
                    "y": 537
                },
                {
                    "x": 2108,
                    "y": 910
                },
                {
                    "x": 442,
                    "y": 910
                }
            ],
            "category": "paragraph",
            "html": "<p id='199' style='font-size:20px'>Lexical Entailment Entailment can be applied not only on the sentence level, but the word level.<br>For example, we say \"dog\" lexically entails \"animal\" because anything that is a dog is also an<br>animal, and \"dog\" lexically contradicts \"cat\" because it is impossible to be both at once. This rela-<br>tionship applies to many types of words (nouns, adjectives, verbs, many prepositions, etc.) and the<br>relationship between lexical and sentential entailment has been deeply explored, e.g., in systems of<br>natural logic. This connection often hinges on monotonicity in language, SO many Lexical Entail-<br>ment examples will also be tagged with one of the Monotone categories, though we do not do this<br>in every case (see Monotonicity, under Logic).</p>",
            "id": 199,
            "page": 16,
            "text": "Lexical Entailment Entailment can be applied not only on the sentence level, but the word level. For example, we say \"dog\" lexically entails \"animal\" because anything that is a dog is also an animal, and \"dog\" lexically contradicts \"cat\" because it is impossible to be both at once. This relationship applies to many types of words (nouns, adjectives, verbs, many prepositions, etc.) and the relationship between lexical and sentential entailment has been deeply explored, e.g., in systems of natural logic. This connection often hinges on monotonicity in language, SO many Lexical Entailment examples will also be tagged with one of the Monotone categories, though we do not do this in every case (see Monotonicity, under Logic)."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 964
                },
                {
                    "x": 2108,
                    "y": 964
                },
                {
                    "x": 2108,
                    "y": 1151
                },
                {
                    "x": 442,
                    "y": 1151
                }
            ],
            "category": "paragraph",
            "html": "<p id='200' style='font-size:18px'>Morphological Negation This is a special case of lexical contradiction where one word is derived<br>from the other: from \"affordable\" to \"unaffordable\", \"agree\" to \"disagree\", etc. We also include<br>examples like \"ever\" and \"never\". We also label these examples with Negation or Double Negation,<br>since they can be viewed as involving a word-level logical negation.</p>",
            "id": 200,
            "page": 16,
            "text": "Morphological Negation This is a special case of lexical contradiction where one word is derived from the other: from \"affordable\" to \"unaffordable\", \"agree\" to \"disagree\", etc. We also include examples like \"ever\" and \"never\". We also label these examples with Negation or Double Negation, since they can be viewed as involving a word-level logical negation."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1206
                },
                {
                    "x": 2106,
                    "y": 1206
                },
                {
                    "x": 2106,
                    "y": 1348
                },
                {
                    "x": 441,
                    "y": 1348
                }
            ],
            "category": "paragraph",
            "html": "<p id='201' style='font-size:18px'>Factivity Propositions appearing in a sentence may be in any entailment relation with the sentence<br>as a whole, depending on the context in which they appear. In many cases, this is determined by<br>lexical triggers (usually verbs or adverbs) in the sentence. For example,</p>",
            "id": 201,
            "page": 16,
            "text": "Factivity Propositions appearing in a sentence may be in any entailment relation with the sentence as a whole, depending on the context in which they appear. In many cases, this is determined by lexical triggers (usually verbs or adverbs) in the sentence. For example,"
        },
        {
            "bounding_box": [
                {
                    "x": 550,
                    "y": 1389
                },
                {
                    "x": 1668,
                    "y": 1389
                },
                {
                    "x": 1668,
                    "y": 1859
                },
                {
                    "x": 550,
                    "y": 1859
                }
            ],
            "category": "paragraph",
            "html": "<p id='202' style='font-size:14px'>● \"I recognize that X\" entails \"X\".<br>● \"I did not recognize that X\" entails \"X\".<br>● \"I believe that X\" does not entail \"X\".<br>● \"I am refusing to do X\" contradicts \"I am doing X\".<br>● \"I am not refusing to do X\" does not contradict \"I am doing X\".<br>● \"I almost finished X\" contradicts \"I finished X\".<br>● \"I barely finished X\" entails \"I finished X\".</p>",
            "id": 202,
            "page": 16,
            "text": "● \"I recognize that X\" entails \"X\". ● \"I did not recognize that X\" entails \"X\". ● \"I believe that X\" does not entail \"X\". ● \"I am refusing to do X\" contradicts \"I am doing X\". ● \"I am not refusing to do X\" does not contradict \"I am doing X\". ● \"I almost finished X\" contradicts \"I finished X\". ● \"I barely finished X\" entails \"I finished X\"."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1900
                },
                {
                    "x": 2108,
                    "y": 1900
                },
                {
                    "x": 2108,
                    "y": 2315
                },
                {
                    "x": 441,
                    "y": 2315
                }
            ],
            "category": "paragraph",
            "html": "<p id='203' style='font-size:16px'>Constructions like \"I recognize that X\" are often called factive, since the entailment (of X above,<br>regarded as a presupposition) persists even under negation. Constructions like \"I am refusing to do<br>X\" above are often called implicative, and are sensitive to negation. There are also cases where<br>a sentence (non-)entails the existence of an entity mentioned in it, for example \"I have found a<br>unicorn\" entails \"A unicorn exists\" while \"I am looking for a unicorn\" doesn't necessarily entail \"A<br>unicorn exists\" · Readings where the entity does not necessarily exist are often called intensional<br>readings, since they seem to deal with the properties denoted by a description (its intension) rather<br>than being reducible to the set of entities that match the description (its extension, which in cases of<br>non-existence will be empty).</p>",
            "id": 203,
            "page": 16,
            "text": "Constructions like \"I recognize that X\" are often called factive, since the entailment (of X above, regarded as a presupposition) persists even under negation. Constructions like \"I am refusing to do X\" above are often called implicative, and are sensitive to negation. There are also cases where a sentence (non-)entails the existence of an entity mentioned in it, for example \"I have found a unicorn\" entails \"A unicorn exists\" while \"I am looking for a unicorn\" doesn't necessarily entail \"A unicorn exists\" · Readings where the entity does not necessarily exist are often called intensional readings, since they seem to deal with the properties denoted by a description (its intension) rather than being reducible to the set of entities that match the description (its extension, which in cases of non-existence will be empty)."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2337
                },
                {
                    "x": 2107,
                    "y": 2337
                },
                {
                    "x": 2107,
                    "y": 2521
                },
                {
                    "x": 442,
                    "y": 2521
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='204' style='font-size:18px'>We place all examples involving these phenomena under the label of Factivity. While it often de-<br>pends on context to determine whether a nested proposition or existence of an entity is entailed by<br>the overall statement, very often it relies heavily on lexical triggers, SO we place the category under<br>Lexical Semantics.</p>",
            "id": 204,
            "page": 16,
            "text": "We place all examples involving these phenomena under the label of Factivity. While it often depends on context to determine whether a nested proposition or existence of an entity is entailed by the overall statement, very often it relies heavily on lexical triggers, SO we place the category under Lexical Semantics."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2579
                },
                {
                    "x": 2107,
                    "y": 2579
                },
                {
                    "x": 2107,
                    "y": 2858
                },
                {
                    "x": 441,
                    "y": 2858
                }
            ],
            "category": "paragraph",
            "html": "<p id='205' style='font-size:18px'>Symmetry/Collectivity Some propositions denote symmetric relations, while others do not. For<br>example, \"John married Gary\" entails \"Gary married John\" but \"John likes Gary\" does not entail<br>\"Gary likes John · Symmetric relations can often be rephrased by collecting both arguments into the<br>subject: \"John met Gary\" entails \"John and Gary met\". Whether a relation is symmetric, or admits<br>collecting its arguments into the subject, is often determined by its head word (e.g., \"like\" , \"marry\"<br>or \"meet\"), SO we classify it under Lexical Semantics.</p>",
            "id": 205,
            "page": 16,
            "text": "Symmetry/Collectivity Some propositions denote symmetric relations, while others do not. For example, \"John married Gary\" entails \"Gary married John\" but \"John likes Gary\" does not entail \"Gary likes John · Symmetric relations can often be rephrased by collecting both arguments into the subject: \"John met Gary\" entails \"John and Gary met\". Whether a relation is symmetric, or admits collecting its arguments into the subject, is often determined by its head word (e.g., \"like\" , \"marry\" or \"meet\"), SO we classify it under Lexical Semantics."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2913
                },
                {
                    "x": 2107,
                    "y": 2913
                },
                {
                    "x": 2107,
                    "y": 3054
                },
                {
                    "x": 441,
                    "y": 3054
                }
            ],
            "category": "paragraph",
            "html": "<p id='206' style='font-size:18px'>Redundancy If a word can be removed from a sentence without changing its meaning, that means<br>the word's meaning was more-or-less adequately expressed by the sentence; so, identifying these<br>cases reflects an understanding of both lexical and sentential semantics.</p>",
            "id": 206,
            "page": 16,
            "text": "Redundancy If a word can be removed from a sentence without changing its meaning, that means the word's meaning was more-or-less adequately expressed by the sentence; so, identifying these cases reflects an understanding of both lexical and sentential semantics."
        },
        {
            "bounding_box": [
                {
                    "x": 1252,
                    "y": 3133
                },
                {
                    "x": 1300,
                    "y": 3133
                },
                {
                    "x": 1300,
                    "y": 3173
                },
                {
                    "x": 1252,
                    "y": 3173
                }
            ],
            "category": "footer",
            "html": "<footer id='207' style='font-size:14px'>16</footer>",
            "id": 207,
            "page": 16,
            "text": "16"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 111
                },
                {
                    "x": 1223,
                    "y": 111
                },
                {
                    "x": 1223,
                    "y": 156
                },
                {
                    "x": 445,
                    "y": 156
                }
            ],
            "category": "header",
            "html": "<header id='208' style='font-size:14px'>Published as a conference paper at ICLR 2019</header>",
            "id": 208,
            "page": 17,
            "text": "Published as a conference paper at ICLR 2019"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 346
                },
                {
                    "x": 2107,
                    "y": 346
                },
                {
                    "x": 2107,
                    "y": 624
                },
                {
                    "x": 441,
                    "y": 624
                }
            ],
            "category": "paragraph",
            "html": "<p id='209' style='font-size:18px'>Named Entities Words often name entities that exist in the world. There are many different kinds<br>of understanding we might wish to understand about these names, including their compositional<br>structure (for example, the \"Baltimore Police\" is the same as the \"Police of the City of Baltimore\") or<br>their real-world referents and acronym expansions (for example, \"SNL\" is \"Saturday Night Live\").<br>This category is closely related to World Knowledge, but focuses on the semantics of names as<br>lexical items rather than background knowledge about their denoted entities.</p>",
            "id": 209,
            "page": 17,
            "text": "Named Entities Words often name entities that exist in the world. There are many different kinds of understanding we might wish to understand about these names, including their compositional structure (for example, the \"Baltimore Police\" is the same as the \"Police of the City of Baltimore\") or their real-world referents and acronym expansions (for example, \"SNL\" is \"Saturday Night Live\"). This category is closely related to World Knowledge, but focuses on the semantics of names as lexical items rather than background knowledge about their denoted entities."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 680
                },
                {
                    "x": 2108,
                    "y": 680
                },
                {
                    "x": 2108,
                    "y": 910
                },
                {
                    "x": 441,
                    "y": 910
                }
            ],
            "category": "paragraph",
            "html": "<p id='210' style='font-size:20px'>Quantifiers Logical quantification in natural language is often expressed through lexical triggers<br>such as \"every\", \"most\", \"some\", and \"no\". While we reserve the categories in Quantification<br>and Monotonicity for entailments involving operations on these quantifiers and their arguments, we<br>choose to regard the interchangeability of quantifiers (e.g., in many cases \"most\" entails \"many\") as<br>a question of lexical semantics.</p>",
            "id": 210,
            "page": 17,
            "text": "Quantifiers Logical quantification in natural language is often expressed through lexical triggers such as \"every\", \"most\", \"some\", and \"no\". While we reserve the categories in Quantification and Monotonicity for entailments involving operations on these quantifiers and their arguments, we choose to regard the interchangeability of quantifiers (e.g., in many cases \"most\" entails \"many\") as a question of lexical semantics."
        },
        {
            "bounding_box": [
                {
                    "x": 446,
                    "y": 975
                },
                {
                    "x": 1213,
                    "y": 975
                },
                {
                    "x": 1213,
                    "y": 1020
                },
                {
                    "x": 446,
                    "y": 1020
                }
            ],
            "category": "paragraph",
            "html": "<p id='211' style='font-size:22px'>E.2 PREDICATE-ARGUMENT STRUCTURE</p>",
            "id": 211,
            "page": 17,
            "text": "E.2 PREDICATE-ARGUMENT STRUCTURE"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1063
                },
                {
                    "x": 2106,
                    "y": 1063
                },
                {
                    "x": 2106,
                    "y": 1202
                },
                {
                    "x": 442,
                    "y": 1202
                }
            ],
            "category": "paragraph",
            "html": "<p id='212' style='font-size:18px'>An important component of understanding the meaning of a sentence is understanding how its parts<br>are composed together into a whole. In this category, we address issues across that spectrum, from<br>syntactic ambiguity to semantic roles and coreference.</p>",
            "id": 212,
            "page": 17,
            "text": "An important component of understanding the meaning of a sentence is understanding how its parts are composed together into a whole. In this category, we address issues across that spectrum, from syntactic ambiguity to semantic roles and coreference."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1258
                },
                {
                    "x": 2105,
                    "y": 1258
                },
                {
                    "x": 2105,
                    "y": 1399
                },
                {
                    "x": 441,
                    "y": 1399
                }
            ],
            "category": "paragraph",
            "html": "<p id='213' style='font-size:22px'>Syntactic Ambiguity: Relative Clauses, Coordination Scope These two categories deal purely<br>with resolving syntactic ambiguity. Relative clauses and coordination scope are both sources of a<br>great amount of ambiguity in English.</p>",
            "id": 213,
            "page": 17,
            "text": "Syntactic Ambiguity: Relative Clauses, Coordination Scope These two categories deal purely with resolving syntactic ambiguity. Relative clauses and coordination scope are both sources of a great amount of ambiguity in English."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1455
                },
                {
                    "x": 2106,
                    "y": 1455
                },
                {
                    "x": 2106,
                    "y": 1642
                },
                {
                    "x": 442,
                    "y": 1642
                }
            ],
            "category": "paragraph",
            "html": "<p id='214' style='font-size:20px'>Prepositional phrases Prepositional phrase attachment is a particularly difficult problem that syn-<br>tactic parsers in NLP systems continue to struggle with. We view it as a problem both of syntax and<br>semantics, since prepositional phrases can express a wide variety of semantic roles and often seman-<br>tically apply beyond their direct syntactic attachment.</p>",
            "id": 214,
            "page": 17,
            "text": "Prepositional phrases Prepositional phrase attachment is a particularly difficult problem that syntactic parsers in NLP systems continue to struggle with. We view it as a problem both of syntax and semantics, since prepositional phrases can express a wide variety of semantic roles and often semantically apply beyond their direct syntactic attachment."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1697
                },
                {
                    "x": 2107,
                    "y": 1697
                },
                {
                    "x": 2107,
                    "y": 1929
                },
                {
                    "x": 442,
                    "y": 1929
                }
            ],
            "category": "paragraph",
            "html": "<p id='215' style='font-size:18px'>Core Arguments Verbs select for particular arguments, particularly subjects and objects, which<br>might be interchangeable depending on the context or the surface form. One example is the ergative<br>alternation: \"Jake broke the vase\" entails \"the vase broke\" but \"Jake broke the vase\" does not entail<br>\"Jake broke\". Other rearrangements of core arguments, such as those seen in Symmetry/Collectivity,<br>also fall under the Core Arguments label.</p>",
            "id": 215,
            "page": 17,
            "text": "Core Arguments Verbs select for particular arguments, particularly subjects and objects, which might be interchangeable depending on the context or the surface form. One example is the ergative alternation: \"Jake broke the vase\" entails \"the vase broke\" but \"Jake broke the vase\" does not entail \"Jake broke\". Other rearrangements of core arguments, such as those seen in Symmetry/Collectivity, also fall under the Core Arguments label."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1987
                },
                {
                    "x": 2105,
                    "y": 1987
                },
                {
                    "x": 2105,
                    "y": 2081
                },
                {
                    "x": 443,
                    "y": 2081
                }
            ],
            "category": "paragraph",
            "html": "<p id='216' style='font-size:20px'>Alternations: Active/Passive, Genitives/Partitives, Nominalization, Datives All four of these<br>categories correspond to syntactic alternations that are known to follow specific patterns in English:</p>",
            "id": 216,
            "page": 17,
            "text": "Alternations: Active/Passive, Genitives/Partitives, Nominalization, Datives All four of these categories correspond to syntactic alternations that are known to follow specific patterns in English:"
        },
        {
            "bounding_box": [
                {
                    "x": 547,
                    "y": 2117
                },
                {
                    "x": 2107,
                    "y": 2117
                },
                {
                    "x": 2107,
                    "y": 2510
                },
                {
                    "x": 547,
                    "y": 2510
                }
            ],
            "category": "paragraph",
            "html": "<p id='217' style='font-size:14px'>● Active/Passive: \"I saw him\" is equivalent to \"He was seen by me\" and entails \"He was<br>seen\".<br>● Genitives/Partitives: \"the elephant's foot\" is the same thing as \"the foot of the elephant\".<br>● Nominalization: \"I caused him to submit his resignation\" entails \"I caused the submission<br>of his resignation\".<br>● Datives: \"I baked him a cake\" entails \"I baked a cake for him\" and \"I baked a cake\" but not<br>\"I baked him\".</p>",
            "id": 217,
            "page": 17,
            "text": "● Active/Passive: \"I saw him\" is equivalent to \"He was seen by me\" and entails \"He was seen\". ● Genitives/Partitives: \"the elephant's foot\" is the same thing as \"the foot of the elephant\". ● Nominalization: \"I caused him to submit his resignation\" entails \"I caused the submission of his resignation\". ● Datives: \"I baked him a cake\" entails \"I baked a cake for him\" and \"I baked a cake\" but not \"I baked him\"."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2568
                },
                {
                    "x": 2107,
                    "y": 2568
                },
                {
                    "x": 2107,
                    "y": 2891
                },
                {
                    "x": 442,
                    "y": 2891
                }
            ],
            "category": "paragraph",
            "html": "<p id='218' style='font-size:18px'>Ellipsis/Implicits Often, the argument of a verb or other predicate is omitted (elided) in the text,<br>with the reader filling in the gap. We can construct entailment examples by explicitly filling in<br>the gap with the correct or incorrect referents. For example, the premise \"Putin is SO entrenched<br>within Russias ruling system that many of its members can imagine no other leader\" entails \"Putin<br>is SO entrenched within Russias ruling system that many of its members can imagine no other leader<br>than Putin\" and contradicts \"Putin is SO entrenched within Russias ruling system that many of its<br>members can imagine no other leader than themselves.\"</p>",
            "id": 218,
            "page": 17,
            "text": "Ellipsis/Implicits Often, the argument of a verb or other predicate is omitted (elided) in the text, with the reader filling in the gap. We can construct entailment examples by explicitly filling in the gap with the correct or incorrect referents. For example, the premise \"Putin is SO entrenched within Russias ruling system that many of its members can imagine no other leader\" entails \"Putin is SO entrenched within Russias ruling system that many of its members can imagine no other leader than Putin\" and contradicts \"Putin is SO entrenched within Russias ruling system that many of its members can imagine no other leader than themselves.\""
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2915
                },
                {
                    "x": 2107,
                    "y": 2915
                },
                {
                    "x": 2107,
                    "y": 3053
                },
                {
                    "x": 442,
                    "y": 3053
                }
            ],
            "category": "paragraph",
            "html": "<p id='219' style='font-size:16px'>This is often regarded as a special case of anaphora, but we decided to split out these cases from<br>explicit anaphora, which is often also regarded as a case of coreference (and attempted to some<br>degree in modern coreference resolution systems).</p>",
            "id": 219,
            "page": 17,
            "text": "This is often regarded as a special case of anaphora, but we decided to split out these cases from explicit anaphora, which is often also regarded as a case of coreference (and attempted to some degree in modern coreference resolution systems)."
        },
        {
            "bounding_box": [
                {
                    "x": 1253,
                    "y": 3133
                },
                {
                    "x": 1299,
                    "y": 3133
                },
                {
                    "x": 1299,
                    "y": 3172
                },
                {
                    "x": 1253,
                    "y": 3172
                }
            ],
            "category": "footer",
            "html": "<footer id='220' style='font-size:18px'>17</footer>",
            "id": 220,
            "page": 17,
            "text": "17"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 112
                },
                {
                    "x": 1224,
                    "y": 112
                },
                {
                    "x": 1224,
                    "y": 156
                },
                {
                    "x": 445,
                    "y": 156
                }
            ],
            "category": "header",
            "html": "<header id='221' style='font-size:14px'>Published as a conference paper at ICLR 2019</header>",
            "id": 221,
            "page": 18,
            "text": "Published as a conference paper at ICLR 2019"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 347
                },
                {
                    "x": 2107,
                    "y": 347
                },
                {
                    "x": 2107,
                    "y": 763
                },
                {
                    "x": 442,
                    "y": 763
                }
            ],
            "category": "paragraph",
            "html": "<p id='222' style='font-size:18px'>Anaphora/Coreference Coreference refers to when multiple expressions refer to the same entity<br>or event. It is closely related to Anaphora, where the meaning of an expression depends on another<br>(antecedent) expression in context. These two phenomena have significant overlap; for example,<br>pronouns (\"she\" , \"we\" , \"it\") are anaphors that are co-referent with their antecedents. However, they<br>also may occur independently, such as coreference between two definite noun phrases (e.g., \"Theresa<br>May \"and the \"British Prime Minister\") that refer to the same entity, or anaphora from a word like<br>\"other\" which requires an antecedent to distinguish something from. In this category we only include<br>cases where there is an explicit phrase (anaphoric or not) that is co-referent with an antecedent or<br>other phrase. We construct examples for these in much the same way as for Ellipsis/Implicits.</p>",
            "id": 222,
            "page": 18,
            "text": "Anaphora/Coreference Coreference refers to when multiple expressions refer to the same entity or event. It is closely related to Anaphora, where the meaning of an expression depends on another (antecedent) expression in context. These two phenomena have significant overlap; for example, pronouns (\"she\" , \"we\" , \"it\") are anaphors that are co-referent with their antecedents. However, they also may occur independently, such as coreference between two definite noun phrases (e.g., \"Theresa May \"and the \"British Prime Minister\") that refer to the same entity, or anaphora from a word like \"other\" which requires an antecedent to distinguish something from. In this category we only include cases where there is an explicit phrase (anaphoric or not) that is co-referent with an antecedent or other phrase. We construct examples for these in much the same way as for Ellipsis/Implicits."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 816
                },
                {
                    "x": 2104,
                    "y": 816
                },
                {
                    "x": 2104,
                    "y": 909
                },
                {
                    "x": 443,
                    "y": 909
                }
            ],
            "category": "paragraph",
            "html": "<p id='223' style='font-size:22px'>Intersectivity Many modifiers, especially adjectives, allow non-intersective uses, which affect<br>their entailment behavior. For example:</p>",
            "id": 223,
            "page": 18,
            "text": "Intersectivity Many modifiers, especially adjectives, allow non-intersective uses, which affect their entailment behavior. For example:"
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 950
                },
                {
                    "x": 2107,
                    "y": 950
                },
                {
                    "x": 2107,
                    "y": 1223
                },
                {
                    "x": 549,
                    "y": 1223
                }
            ],
            "category": "paragraph",
            "html": "<p id='224' style='font-size:14px'>● Intersective: \"He is a violinist and an old surgeon\" entails \"He is an old violinist\" and \"He<br>is a surgeon\".<br>● Non-intersective: \"He is a violinist and a skilled surgeon\" does not entail \"He is a skilled<br>violinist\".<br>● Non-intersective: \"He is a fake surgeon\" does not entail \"He is a surgeon\".</p>",
            "id": 224,
            "page": 18,
            "text": "● Intersective: \"He is a violinist and an old surgeon\" entails \"He is an old violinist\" and \"He is a surgeon\". ● Non-intersective: \"He is a violinist and a skilled surgeon\" does not entail \"He is a skilled violinist\". ● Non-intersective: \"He is a fake surgeon\" does not entail \"He is a surgeon\"."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1264
                },
                {
                    "x": 2106,
                    "y": 1264
                },
                {
                    "x": 2106,
                    "y": 1401
                },
                {
                    "x": 441,
                    "y": 1401
                }
            ],
            "category": "paragraph",
            "html": "<p id='225' style='font-size:18px'>Generally, an intersective use of a modifier, like \"old\" in \"old men\", is one which may be interpreted<br>as referring to the set of entities with both properties (they are old and they are men). Linguists often<br>formalize this using set intersection, hence the name.</p>",
            "id": 225,
            "page": 18,
            "text": "Generally, an intersective use of a modifier, like \"old\" in \"old men\", is one which may be interpreted as referring to the set of entities with both properties (they are old and they are men). Linguists often formalize this using set intersection, hence the name."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1426
                },
                {
                    "x": 2106,
                    "y": 1426
                },
                {
                    "x": 2106,
                    "y": 1655
                },
                {
                    "x": 442,
                    "y": 1655
                }
            ],
            "category": "paragraph",
            "html": "<p id='226' style='font-size:18px'>Intersectivity is related to Factivity. For example, \"fake\" may be regarded as a counter-implicative<br>modifier, and these examples will be labeled as such. However, we choose to categorize intersec-<br>tivity under predicate-argument structure rather than lexical semantics, because generally the same<br>word will admit both intersective and non-intersective uses, SO it may be regarded as an ambiguity<br>of argument structure.</p>",
            "id": 226,
            "page": 18,
            "text": "Intersectivity is related to Factivity. For example, \"fake\" may be regarded as a counter-implicative modifier, and these examples will be labeled as such. However, we choose to categorize intersectivity under predicate-argument structure rather than lexical semantics, because generally the same word will admit both intersective and non-intersective uses, SO it may be regarded as an ambiguity of argument structure."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1711
                },
                {
                    "x": 2107,
                    "y": 1711
                },
                {
                    "x": 2107,
                    "y": 1896
                },
                {
                    "x": 442,
                    "y": 1896
                }
            ],
            "category": "paragraph",
            "html": "<p id='227' style='font-size:16px'>Restrictivity Restrictivity is most often used to refer to a property of uses of noun modifiers. In<br>particular, a restrictive use of a modifier is one that serves to identify the entity or entities being<br>described, whereas a non-restrictive use adds extra details to the identified entity. The distinction<br>can often be highlighted by entailments:</p>",
            "id": 227,
            "page": 18,
            "text": "Restrictivity Restrictivity is most often used to refer to a property of uses of noun modifiers. In particular, a restrictive use of a modifier is one that serves to identify the entity or entities being described, whereas a non-restrictive use adds extra details to the identified entity. The distinction can often be highlighted by entailments:"
        },
        {
            "bounding_box": [
                {
                    "x": 552,
                    "y": 1938
                },
                {
                    "x": 2105,
                    "y": 1938
                },
                {
                    "x": 2105,
                    "y": 2142
                },
                {
                    "x": 552,
                    "y": 2142
                }
            ],
            "category": "paragraph",
            "html": "<p id='228' style='font-size:18px'>● Restrictive: \"I finished all of my homework due today\" does not entail \"I finished all of my<br>homework\".<br>● Non-restrictive: \"I got rid of all those pesky bedbugs\" entails \"I got rid of all those bed-<br>bugs\".</p>",
            "id": 228,
            "page": 18,
            "text": "● Restrictive: \"I finished all of my homework due today\" does not entail \"I finished all of my homework\". ● Non-restrictive: \"I got rid of all those pesky bedbugs\" entails \"I got rid of all those bedbugs\"."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2184
                },
                {
                    "x": 2103,
                    "y": 2184
                },
                {
                    "x": 2103,
                    "y": 2281
                },
                {
                    "x": 442,
                    "y": 2281
                }
            ],
            "category": "paragraph",
            "html": "<p id='229' style='font-size:20px'>Modifiers that are commonly used non-restrictively are appositives, relative clauses starting with<br>\"which\" or \"who\", and expletives (e.g. \"pesky\"). Non-restrictive uses can appear in many forms.</p>",
            "id": 229,
            "page": 18,
            "text": "Modifiers that are commonly used non-restrictively are appositives, relative clauses starting with \"which\" or \"who\", and expletives (e.g. \"pesky\"). Non-restrictive uses can appear in many forms."
        },
        {
            "bounding_box": [
                {
                    "x": 446,
                    "y": 2342
                },
                {
                    "x": 675,
                    "y": 2342
                },
                {
                    "x": 675,
                    "y": 2383
                },
                {
                    "x": 446,
                    "y": 2383
                }
            ],
            "category": "paragraph",
            "html": "<p id='230' style='font-size:20px'>E.3 LOGIC</p>",
            "id": 230,
            "page": 18,
            "text": "E.3 LOGIC"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2427
                },
                {
                    "x": 2106,
                    "y": 2427
                },
                {
                    "x": 2106,
                    "y": 2658
                },
                {
                    "x": 442,
                    "y": 2658
                }
            ],
            "category": "paragraph",
            "html": "<p id='231' style='font-size:20px'>With an understanding of the structure of a sentence, there is often a baseline set of shallow conclu-<br>sions that can be drawn using logical operators and often modeled using the mathematical tools of<br>logic. Indeed, the development of mathematical logic was initially guided by questions about natural<br>language meaning, from Aristotelian syllogisms to Fregean symbols. The notion of entailment is<br>also borrowed from mathematical logic.</p>",
            "id": 231,
            "page": 18,
            "text": "With an understanding of the structure of a sentence, there is often a baseline set of shallow conclusions that can be drawn using logical operators and often modeled using the mathematical tools of logic. Indeed, the development of mathematical logic was initially guided by questions about natural language meaning, from Aristotelian syllogisms to Fregean symbols. The notion of entailment is also borrowed from mathematical logic."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2729
                },
                {
                    "x": 2108,
                    "y": 2729
                },
                {
                    "x": 2108,
                    "y": 2851
                },
                {
                    "x": 443,
                    "y": 2851
                }
            ],
            "category": "paragraph",
            "html": "<p id='232' style='font-size:22px'>Propositional Structure: Negation, Double Negation, Conjunction, Disjunction, Conditionals<br>All of the basic operations of propositional logic appear in natural language, and we tag them where<br>they are relevant to our examples:</p>",
            "id": 232,
            "page": 18,
            "text": "Propositional Structure: Negation, Double Negation, Conjunction, Disjunction, Conditionals All of the basic operations of propositional logic appear in natural language, and we tag them where they are relevant to our examples:"
        },
        {
            "bounding_box": [
                {
                    "x": 551,
                    "y": 2895
                },
                {
                    "x": 2106,
                    "y": 2895
                },
                {
                    "x": 2106,
                    "y": 3053
                },
                {
                    "x": 551,
                    "y": 3053
                }
            ],
            "category": "paragraph",
            "html": "<p id='233' style='font-size:14px'>● Negation: \"The cat sat on the mat\" contradicts \"The cat did not sit on the mat\".<br>● Double negation: \"The market is not impossible to navigate\" entails \"The market is possi-<br>ble to navigate\".</p>",
            "id": 233,
            "page": 18,
            "text": "● Negation: \"The cat sat on the mat\" contradicts \"The cat did not sit on the mat\". ● Double negation: \"The market is not impossible to navigate\" entails \"The market is possible to navigate\"."
        },
        {
            "bounding_box": [
                {
                    "x": 1253,
                    "y": 3133
                },
                {
                    "x": 1299,
                    "y": 3133
                },
                {
                    "x": 1299,
                    "y": 3172
                },
                {
                    "x": 1253,
                    "y": 3172
                }
            ],
            "category": "footer",
            "html": "<footer id='234' style='font-size:16px'>18</footer>",
            "id": 234,
            "page": 18,
            "text": "18"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 113
                },
                {
                    "x": 1223,
                    "y": 113
                },
                {
                    "x": 1223,
                    "y": 156
                },
                {
                    "x": 445,
                    "y": 156
                }
            ],
            "category": "header",
            "html": "<header id='235' style='font-size:14px'>Published as a conference paper at ICLR 2019</header>",
            "id": 235,
            "page": 19,
            "text": "Published as a conference paper at ICLR 2019"
        },
        {
            "bounding_box": [
                {
                    "x": 550,
                    "y": 345
                },
                {
                    "x": 2109,
                    "y": 345
                },
                {
                    "x": 2109,
                    "y": 647
                },
                {
                    "x": 550,
                    "y": 647
                }
            ],
            "category": "paragraph",
            "html": "<p id='236' style='font-size:20px'>● Conjunction: \"Temperature and snow consistency must be just right\" entails \"Temperature<br>must be just right\".<br>● Disjunction: \"Life is either a daring adventure or nothing at all\" does not entail, but is<br>entailed by, \"Life is a daring adventure\".<br>● Conditionals: \"If both apply, they are essentially impossible\" does not entail \"They are<br>essentially impossible\".</p>",
            "id": 236,
            "page": 19,
            "text": "● Conjunction: \"Temperature and snow consistency must be just right\" entails \"Temperature must be just right\". ● Disjunction: \"Life is either a daring adventure or nothing at all\" does not entail, but is entailed by, \"Life is a daring adventure\". ● Conditionals: \"If both apply, they are essentially impossible\" does not entail \"They are essentially impossible\"."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 681
                },
                {
                    "x": 2106,
                    "y": 681
                },
                {
                    "x": 2106,
                    "y": 864
                },
                {
                    "x": 441,
                    "y": 864
                }
            ],
            "category": "paragraph",
            "html": "<p id='237' style='font-size:18px'>Conditionals are more complicated because their use in language does not always mirror their mean-<br>ing in logic. For example, they may be used at a higher level than the at-issue assertion: \"If you<br>think about it, it's the perfect reverse psychology tactic\" entails \"It's the perfect reverse psychology<br>tactic\".</p>",
            "id": 237,
            "page": 19,
            "text": "Conditionals are more complicated because their use in language does not always mirror their meaning in logic. For example, they may be used at a higher level than the at-issue assertion: \"If you think about it, it's the perfect reverse psychology tactic\" entails \"It's the perfect reverse psychology tactic\"."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 912
                },
                {
                    "x": 2107,
                    "y": 912
                },
                {
                    "x": 2107,
                    "y": 1097
                },
                {
                    "x": 442,
                    "y": 1097
                }
            ],
            "category": "paragraph",
            "html": "<p id='238' style='font-size:20px'>Quantification: Universal, Existential Quantifiers are often triggered by words such as \"all\",<br>\"some\", \"many\", and \"no\". There is a rich body of work modeling their meaning in mathematical<br>logic with generalized quantifiers. In these two categories, we focus on straightforward inferences<br>from the natural language analogs of universal and existential quantification:</p>",
            "id": 238,
            "page": 19,
            "text": "Quantification: Universal, Existential Quantifiers are often triggered by words such as \"all\", \"some\", \"many\", and \"no\". There is a rich body of work modeling their meaning in mathematical logic with generalized quantifiers. In these two categories, we focus on straightforward inferences from the natural language analogs of universal and existential quantification:"
        },
        {
            "bounding_box": [
                {
                    "x": 550,
                    "y": 1129
                },
                {
                    "x": 2106,
                    "y": 1129
                },
                {
                    "x": 2106,
                    "y": 1327
                },
                {
                    "x": 550,
                    "y": 1327
                }
            ],
            "category": "paragraph",
            "html": "<p id='239' style='font-size:20px'>● Universal: \"All parakeets have two wings\" entails, but is not entailed by, \"My parakeet has<br>two wings\".<br>● Existential: \"Some parakeets have two wings\" does not entail, but is entailed by, \"My<br>parakeet has two wings\".</p>",
            "id": 239,
            "page": 19,
            "text": "● Universal: \"All parakeets have two wings\" entails, but is not entailed by, \"My parakeet has two wings\". ● Existential: \"Some parakeets have two wings\" does not entail, but is entailed by, \"My parakeet has two wings\"."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1372
                },
                {
                    "x": 2107,
                    "y": 1372
                },
                {
                    "x": 2107,
                    "y": 1559
                },
                {
                    "x": 441,
                    "y": 1559
                }
            ],
            "category": "paragraph",
            "html": "<p id='240' style='font-size:22px'>Monotonicity: Upward Monotone, Downward Monotone, Non-Monotone Monotonicity is a<br>property of argument positions in certain logical systems. In general, it gives a way of deriving<br>entailment relations between expressions that differ on only one subexpression. In language, it can<br>explain how some entailments propagate through logical operators and quantifiers.</p>",
            "id": 240,
            "page": 19,
            "text": "Monotonicity: Upward Monotone, Downward Monotone, Non-Monotone Monotonicity is a property of argument positions in certain logical systems. In general, it gives a way of deriving entailment relations between expressions that differ on only one subexpression. In language, it can explain how some entailments propagate through logical operators and quantifiers."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1583
                },
                {
                    "x": 2104,
                    "y": 1583
                },
                {
                    "x": 2104,
                    "y": 1675
                },
                {
                    "x": 442,
                    "y": 1675
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='241' style='font-size:22px'>For example, \"pet\" entails \"pet squirrel\", which further entails \"happy pet squirrel\". We can demon-<br>strate how the quantifiers \"a\", \"no\" and \"exactly one\" differ with respect to monotonicity:</p>",
            "id": 241,
            "page": 19,
            "text": "For example, \"pet\" entails \"pet squirrel\", which further entails \"happy pet squirrel\". We can demonstrate how the quantifiers \"a\", \"no\" and \"exactly one\" differ with respect to monotonicity:"
        },
        {
            "bounding_box": [
                {
                    "x": 551,
                    "y": 1705
                },
                {
                    "x": 2106,
                    "y": 1705
                },
                {
                    "x": 2106,
                    "y": 1964
                },
                {
                    "x": 551,
                    "y": 1964
                }
            ],
            "category": "paragraph",
            "html": "<p id='242' style='font-size:16px'>● \"I have a pet squirrel\" entails \"I have a pet\", but not \"I have a happy pet squirrel\".<br>● \"I have no pet squirrels\" does not entail \"I have no pets\", but does entail \"I have no happy<br>pet squirrels\".<br>● \"I have exactly one pet squirrel\" entails neither \"I have exactly one pet\" nor \"I have exactly<br>one happy pet squirrel\".</p>",
            "id": 242,
            "page": 19,
            "text": "● \"I have a pet squirrel\" entails \"I have a pet\", but not \"I have a happy pet squirrel\". ● \"I have no pet squirrels\" does not entail \"I have no pets\", but does entail \"I have no happy pet squirrels\". ● \"I have exactly one pet squirrel\" entails neither \"I have exactly one pet\" nor \"I have exactly one happy pet squirrel\"."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1993
                },
                {
                    "x": 2104,
                    "y": 1993
                },
                {
                    "x": 2104,
                    "y": 2089
                },
                {
                    "x": 441,
                    "y": 2089
                }
            ],
            "category": "paragraph",
            "html": "<p id='243' style='font-size:20px'>In all of these examples, \"pet squirrel\" appears in what we call the restrictor position of the quantifier.<br>We say:</p>",
            "id": 243,
            "page": 19,
            "text": "In all of these examples, \"pet squirrel\" appears in what we call the restrictor position of the quantifier. We say:"
        },
        {
            "bounding_box": [
                {
                    "x": 550,
                    "y": 2118
                },
                {
                    "x": 2108,
                    "y": 2118
                },
                {
                    "x": 2108,
                    "y": 2422
                },
                {
                    "x": 550,
                    "y": 2422
                }
            ],
            "category": "paragraph",
            "html": "<p id='244' style='font-size:14px'>● \"a\" is upward monotone in its restrictor: an entailment in the restrictor yields an entailment<br>of the whole statement.<br>● \"no\" is downward monotone in its restrictor: an entailment in the restrictor yields an en-<br>tailment of the whole statement in the opposite direction.<br>● \"exactly one\" is non-monotone in its restrictor: entailments in the restrictor do not yield<br>entailments of the whole statement.</p>",
            "id": 244,
            "page": 19,
            "text": "● \"a\" is upward monotone in its restrictor: an entailment in the restrictor yields an entailment of the whole statement. ● \"no\" is downward monotone in its restrictor: an entailment in the restrictor yields an entailment of the whole statement in the opposite direction. ● \"exactly one\" is non-monotone in its restrictor: entailments in the restrictor do not yield entailments of the whole statement."
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 2453
                },
                {
                    "x": 2106,
                    "y": 2453
                },
                {
                    "x": 2106,
                    "y": 2638
                },
                {
                    "x": 440,
                    "y": 2638
                }
            ],
            "category": "paragraph",
            "html": "<p id='245' style='font-size:20px'>In this way, entailments between sentences that are built off of entailments of sub-phrases almost<br>always rely on monotonicity judgments; see, for example, Lexical Entailment. However, because<br>this is such a general class of sentence pairs, to keep the Logic category meaningful we do not<br>always tag these examples with monotonicity.</p>",
            "id": 245,
            "page": 19,
            "text": "In this way, entailments between sentences that are built off of entailments of sub-phrases almost always rely on monotonicity judgments; see, for example, Lexical Entailment. However, because this is such a general class of sentence pairs, to keep the Logic category meaningful we do not always tag these examples with monotonicity."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2684
                },
                {
                    "x": 2105,
                    "y": 2684
                },
                {
                    "x": 2105,
                    "y": 2867
                },
                {
                    "x": 443,
                    "y": 2867
                }
            ],
            "category": "paragraph",
            "html": "<p id='246' style='font-size:22px'>Richer Logical Structure: Intervals/Numbers, Temporal Some higher-level facets of reason-<br>ing have been traditionally modeled using logic, such as actual mathematical reasoning (entailments<br>based off of numbers) and temporal reasoning (which is often modeled as reasoning about a mathe-<br>matical timeline).</p>",
            "id": 246,
            "page": 19,
            "text": "Richer Logical Structure: Intervals/Numbers, Temporal Some higher-level facets of reasoning have been traditionally modeled using logic, such as actual mathematical reasoning (entailments based off of numbers) and temporal reasoning (which is often modeled as reasoning about a mathematical timeline)."
        },
        {
            "bounding_box": [
                {
                    "x": 550,
                    "y": 2904
                },
                {
                    "x": 2105,
                    "y": 2904
                },
                {
                    "x": 2105,
                    "y": 3052
                },
                {
                    "x": 550,
                    "y": 3052
                }
            ],
            "category": "paragraph",
            "html": "<p id='247' style='font-size:16px'>● Intervals/Numbers: \"I have had more than 2 drinks tonight\" entails \"I have had more than<br>1 drink tonight\".<br>● Temporal: \"Mary left before John entered\" entails \"John entered after Mary left\".</p>",
            "id": 247,
            "page": 19,
            "text": "● Intervals/Numbers: \"I have had more than 2 drinks tonight\" entails \"I have had more than 1 drink tonight\". ● Temporal: \"Mary left before John entered\" entails \"John entered after Mary left\"."
        },
        {
            "bounding_box": [
                {
                    "x": 1253,
                    "y": 3133
                },
                {
                    "x": 1299,
                    "y": 3133
                },
                {
                    "x": 1299,
                    "y": 3171
                },
                {
                    "x": 1253,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='248' style='font-size:18px'>19</footer>",
            "id": 248,
            "page": 19,
            "text": "19"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 110
                },
                {
                    "x": 1225,
                    "y": 110
                },
                {
                    "x": 1225,
                    "y": 157
                },
                {
                    "x": 444,
                    "y": 157
                }
            ],
            "category": "header",
            "html": "<header id='249' style='font-size:14px'>Published as a conference paper at ICLR 2019</header>",
            "id": 249,
            "page": 20,
            "text": "Published as a conference paper at ICLR 2019"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 345
                },
                {
                    "x": 795,
                    "y": 345
                },
                {
                    "x": 795,
                    "y": 394
                },
                {
                    "x": 443,
                    "y": 394
                }
            ],
            "category": "paragraph",
            "html": "<p id='250' style='font-size:20px'>E.4 KNOWLEDGE</p>",
            "id": 250,
            "page": 20,
            "text": "E.4 KNOWLEDGE"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 431
                },
                {
                    "x": 2109,
                    "y": 431
                },
                {
                    "x": 2109,
                    "y": 710
                },
                {
                    "x": 442,
                    "y": 710
                }
            ],
            "category": "paragraph",
            "html": "<p id='251' style='font-size:16px'>Strictly speaking, world knowledge and common sense are required on every level of language<br>understanding for disambiguating word senses, syntactic structures, anaphora, and more. So our<br>entire suite (and any test of entailment) does test these features to some degree. However, in these<br>categories, we gather examples where the entailment rests not only on correct disambiguation of the<br>sentences, but also application of extra knowledge, whether concrete knowledge about world affairs<br>or more common-sense knowledge about word meanings or social or physical dynamics.</p>",
            "id": 251,
            "page": 20,
            "text": "Strictly speaking, world knowledge and common sense are required on every level of language understanding for disambiguating word senses, syntactic structures, anaphora, and more. So our entire suite (and any test of entailment) does test these features to some degree. However, in these categories, we gather examples where the entailment rests not only on correct disambiguation of the sentences, but also application of extra knowledge, whether concrete knowledge about world affairs or more common-sense knowledge about word meanings or social or physical dynamics."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 758
                },
                {
                    "x": 2106,
                    "y": 758
                },
                {
                    "x": 2106,
                    "y": 898
                },
                {
                    "x": 443,
                    "y": 898
                }
            ],
            "category": "paragraph",
            "html": "<p id='252' style='font-size:20px'>World Knowledge In this category we focus on knowledge that can clearly be expressed as facts,<br>as well as broader and less common geographical, legal, political, technical, or cultural knowledge.<br>Examples:</p>",
            "id": 252,
            "page": 20,
            "text": "World Knowledge In this category we focus on knowledge that can clearly be expressed as facts, as well as broader and less common geographical, legal, political, technical, or cultural knowledge. Examples:"
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 935
                },
                {
                    "x": 2109,
                    "y": 935
                },
                {
                    "x": 2109,
                    "y": 1204
                },
                {
                    "x": 549,
                    "y": 1204
                }
            ],
            "category": "paragraph",
            "html": "<p id='253' style='font-size:14px'>● \"This is the most oniony article I've seen on the entire internet\" entails \"This article reads<br>like satire\".<br>● \"The reaction was strongly exothermic\" entails \"The reaction media got very hot\".<br>● \"There are amazing hikes around Mt. Fuji\" entails \"There are amazing hikes in Japan\" but<br>not \"There are amazing hikes in Nepal\".</p>",
            "id": 253,
            "page": 20,
            "text": "● \"This is the most oniony article I've seen on the entire internet\" entails \"This article reads like satire\". ● \"The reaction was strongly exothermic\" entails \"The reaction media got very hot\". ● \"There are amazing hikes around Mt. Fuji\" entails \"There are amazing hikes in Japan\" but not \"There are amazing hikes in Nepal\"."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1250
                },
                {
                    "x": 2108,
                    "y": 1250
                },
                {
                    "x": 2108,
                    "y": 1437
                },
                {
                    "x": 443,
                    "y": 1437
                }
            ],
            "category": "paragraph",
            "html": "<p id='254' style='font-size:16px'>Common Sense In this category we focus on knowledge that is more difficult to express as facts<br>and that we expect to be possessed by most people independent of cultural or educational back-<br>ground. This includes a basic understanding of physical and social dynamics as well as lexical<br>meaning (beyond simple lexical entailment or logical relations). Examples:</p>",
            "id": 254,
            "page": 20,
            "text": "Common Sense In this category we focus on knowledge that is more difficult to express as facts and that we expect to be possessed by most people independent of cultural or educational background. This includes a basic understanding of physical and social dynamics as well as lexical meaning (beyond simple lexical entailment or logical relations). Examples:"
        },
        {
            "bounding_box": [
                {
                    "x": 550,
                    "y": 1470
                },
                {
                    "x": 2109,
                    "y": 1470
                },
                {
                    "x": 2109,
                    "y": 1830
                },
                {
                    "x": 550,
                    "y": 1830
                }
            ],
            "category": "paragraph",
            "html": "<p id='255' style='font-size:14px'>● \"The announcement of Tillerson's departure sent shock waves across the globe\" contradicts<br>\"People across the globe were prepared for Tillerson's departure\".<br>● \"Marc Sims has been seeing his barber once a week, for several years\" entails \"Marc Sims<br>has been getting his hair cut once a week, for several years\".<br>● \"Hummingbirds are really attracted to bright orange and red (hence why the feeders are<br>usually these colours)\" entails \"The feeders are usually coloured SO as to attract humming-<br>birds\".</p>",
            "id": 255,
            "page": 20,
            "text": "● \"The announcement of Tillerson's departure sent shock waves across the globe\" contradicts \"People across the globe were prepared for Tillerson's departure\". ● \"Marc Sims has been seeing his barber once a week, for several years\" entails \"Marc Sims has been getting his hair cut once a week, for several years\". ● \"Hummingbirds are really attracted to bright orange and red (hence why the feeders are usually these colours)\" entails \"The feeders are usually coloured SO as to attract hummingbirds\"."
        },
        {
            "bounding_box": [
                {
                    "x": 1249,
                    "y": 3131
                },
                {
                    "x": 1300,
                    "y": 3131
                },
                {
                    "x": 1300,
                    "y": 3173
                },
                {
                    "x": 1249,
                    "y": 3173
                }
            ],
            "category": "footer",
            "html": "<footer id='256' style='font-size:16px'>20</footer>",
            "id": 256,
            "page": 20,
            "text": "20"
        }
    ]
}