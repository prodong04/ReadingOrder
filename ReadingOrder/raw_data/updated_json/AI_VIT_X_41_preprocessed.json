{
    "id": "32b4864e-0f94-11ef-8828-426932df3dcf",
    "pdf_path": "/root/data/pdf/2109.06835v1.pdf",
    "elements": [
        {
            "bounding_box": [
                {
                    "x": 699,
                    "y": 321
                },
                {
                    "x": 1779,
                    "y": 321
                },
                {
                    "x": 1779,
                    "y": 456
                },
                {
                    "x": 699,
                    "y": 456
                }
            ],
            "category": "paragraph",
            "html": "<p id='0' style='font-size:22px'>The Perils of Using Mechanical Turk<br>to Evaluate Open-Ended Text Generation</p>",
            "id": 0,
            "page": 1,
            "text": "The Perils of Using Mechanical Turk to Evaluate Open-Ended Text Generation"
        },
        {
            "bounding_box": [
                {
                    "x": 671,
                    "y": 547
                },
                {
                    "x": 1827,
                    "y": 547
                },
                {
                    "x": 1827,
                    "y": 608
                },
                {
                    "x": 671,
                    "y": 608
                }
            ],
            "category": "paragraph",
            "html": "<p id='1' style='font-size:22px'>Marzena Karpinska Nader Akoury Mohit Iyyer</p>",
            "id": 1,
            "page": 1,
            "text": "Marzena Karpinska Nader Akoury Mohit Iyyer"
        },
        {
            "bounding_box": [
                {
                    "x": 862,
                    "y": 608
                },
                {
                    "x": 1626,
                    "y": 608
                },
                {
                    "x": 1626,
                    "y": 662
                },
                {
                    "x": 862,
                    "y": 662
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='2' style='font-size:20px'>University of Massachusetts Amherst</p>",
            "id": 2,
            "page": 1,
            "text": "University of Massachusetts Amherst"
        },
        {
            "bounding_box": [
                {
                    "x": 708,
                    "y": 671
                },
                {
                    "x": 1789,
                    "y": 671
                },
                {
                    "x": 1789,
                    "y": 720
                },
                {
                    "x": 708,
                    "y": 720
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='3' style='font-size:14px'>{mkarpinska, nsa , mi yyer } @cs · umas S · edu</p>",
            "id": 3,
            "page": 1,
            "text": "{mkarpinska, nsa , mi yyer } @cs · umas S · edu"
        },
        {
            "bounding_box": [
                {
                    "x": 650,
                    "y": 888
                },
                {
                    "x": 849,
                    "y": 888
                },
                {
                    "x": 849,
                    "y": 943
                },
                {
                    "x": 650,
                    "y": 943
                }
            ],
            "category": "paragraph",
            "html": "<p id='4' style='font-size:20px'>Abstract</p>",
            "id": 4,
            "page": 1,
            "text": "Abstract"
        },
        {
            "bounding_box": [
                {
                    "x": 355,
                    "y": 972
                },
                {
                    "x": 1148,
                    "y": 972
                },
                {
                    "x": 1148,
                    "y": 2375
                },
                {
                    "x": 355,
                    "y": 2375
                }
            ],
            "category": "paragraph",
            "html": "<p id='5' style='font-size:16px'>Recent text generation research has increas-<br>ingly focused on open-ended domains such as<br>story and poetry generation. Because mod-<br>els built for such tasks are difficult to evaluate<br>automatically, most researchers in the space<br>justify their modeling choices by collecting<br>crowdsourced human judgments of text qual-<br>ity (e.g., Likert scores of coherence or gram-<br>maticality) from Amazon Mechanical Turk<br>(AMT). In this paper, we first conduct a sur-<br>vey of 45 open-ended text generation papers<br>and find that the vast majority of them fail to<br>report crucial details about their AMT tasks,<br>hindering reproducibility. We then run a se-<br>ries of story evaluation experiments with both<br>AMT workers and English teachers and dis-<br>cover that even with strict qualification fil-<br>ters, AMT workers (unlike teachers) fail to<br>distinguish between model-generated text and<br>human-generated references. We show that<br>AMT worker judgments improve when they<br>are shown model-generated output alongside<br>human-generated references, which enables<br>the workers to better calibrate their ratings.<br>Finally, interviews with the English teachers<br>provide deeper insights into the challenges of<br>the evaluation process, particularly when rat-<br>ing model-generated text.</p>",
            "id": 5,
            "page": 1,
            "text": "Recent text generation research has increasingly focused on open-ended domains such as story and poetry generation. Because models built for such tasks are difficult to evaluate automatically, most researchers in the space justify their modeling choices by collecting crowdsourced human judgments of text quality (e.g., Likert scores of coherence or grammaticality) from Amazon Mechanical Turk (AMT). In this paper, we first conduct a survey of 45 open-ended text generation papers and find that the vast majority of them fail to report crucial details about their AMT tasks, hindering reproducibility. We then run a series of story evaluation experiments with both AMT workers and English teachers and discover that even with strict qualification filters, AMT workers (unlike teachers) fail to distinguish between model-generated text and human-generated references. We show that AMT worker judgments improve when they are shown model-generated output alongside human-generated references, which enables the workers to better calibrate their ratings. Finally, interviews with the English teachers provide deeper insights into the challenges of the evaluation process, particularly when rating model-generated text."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 2402
                },
                {
                    "x": 647,
                    "y": 2402
                },
                {
                    "x": 647,
                    "y": 2460
                },
                {
                    "x": 290,
                    "y": 2460
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='6' style='font-size:18px'>1 Introduction</p>",
            "id": 6,
            "page": 1,
            "text": "1 Introduction"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 2491
                },
                {
                    "x": 1216,
                    "y": 2491
                },
                {
                    "x": 1216,
                    "y": 3112
                },
                {
                    "x": 287,
                    "y": 3112
                }
            ],
            "category": "paragraph",
            "html": "<p id='7' style='font-size:16px'>Recent advances in neural language modeling have<br>spurred research into open-ended text generation<br>tasks such as story generation (Peng et al., 2018a),<br>style transfer (Krishna et al., 2020), and pun gen-<br>eration (He et al., 2019). Since the space of pos-<br>sible outputs for these tasks is huge compared to<br>more constrained problems such as machine trans-<br>lation, automatic metrics such as BLEU (Papineni<br>et al., 2002) and ROUGE (Lin, 2004) that measure<br>similarity to reference texts are mostly uninforma-<br>tive (Akoury et al., 2020). 1 Human evaluation of</p>",
            "id": 7,
            "page": 1,
            "text": "Recent advances in neural language modeling have spurred research into open-ended text generation tasks such as story generation (Peng , 2018a), style transfer (Krishna , 2020), and pun generation (He , 2019). Since the space of possible outputs for these tasks is huge compared to more constrained problems such as machine translation, automatic metrics such as BLEU (Papineni , 2002) and ROUGE (Lin, 2004) that measure similarity to reference texts are mostly uninformative (Akoury , 2020). 1 Human evaluation of"
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 3138
                },
                {
                    "x": 1210,
                    "y": 3138
                },
                {
                    "x": 1210,
                    "y": 3226
                },
                {
                    "x": 289,
                    "y": 3226
                }
            ],
            "category": "paragraph",
            "html": "<p id='8' style='font-size:14px'>1 Nevertheless, such metrics are commonly reported in<br>research papers on open-ended text generation.</p>",
            "id": 8,
            "page": 1,
            "text": "1 Nevertheless, such metrics are commonly reported in research papers on open-ended text generation."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 893
                },
                {
                    "x": 2197,
                    "y": 893
                },
                {
                    "x": 2197,
                    "y": 1396
                },
                {
                    "x": 1267,
                    "y": 1396
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='9' style='font-size:18px'>model-generated text, which is critical for open-<br>ended tasks given the unreliability of automatic<br>metrics (Peng et al., 2017; Reiter, 2018; See et al.,<br>2019), is frequently conducted on Amazon's popu-<br>lar Mechanical Turk platform (AMT) to minimize<br>cost and time. Most existing AMT studies ask<br>crowdworkers to provide Likert scale ratings of<br>various properties of generated text, such as flu-<br>ency and likability.</p>",
            "id": 9,
            "page": 1,
            "text": "model-generated text, which is critical for openended tasks given the unreliability of automatic metrics (Peng , 2017; Reiter, 2018; See , 2019), is frequently conducted on Amazon's popular Mechanical Turk platform (AMT) to minimize cost and time. Most existing AMT studies ask crowdworkers to provide Likert scale ratings of various properties of generated text, such as fluency and likability."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 1405
                },
                {
                    "x": 2198,
                    "y": 1405
                },
                {
                    "x": 2198,
                    "y": 1966
                },
                {
                    "x": 1267,
                    "y": 1966
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='10' style='font-size:18px'>In this paper, we study the reliability and repro-<br>ducibility of AMT evaluations of open-ended text<br>generation. We first conduct a survey of papers<br>on open-ended text generation between 2018-2020<br>and find many critical details often go unreported<br>(e.g., worker qualifications, payment, task descrip-<br>tions, annotator agreement), a finding in line with<br>prior reproducibility studies outside open-ended<br>text generation (Card et al., 2020; Howcroft et al.,<br>2020; van der Lee et al., 2021).</p>",
            "id": 10,
            "page": 1,
            "text": "In this paper, we study the reliability and reproducibility of AMT evaluations of open-ended text generation. We first conduct a survey of papers on open-ended text generation between 2018-2020 and find many critical details often go unreported (e.g., worker qualifications, payment, task descriptions, annotator agreement), a finding in line with prior reproducibility studies outside open-ended text generation (Card , 2020; Howcroft , 2020; van der Lee , 2021)."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 1974
                },
                {
                    "x": 2198,
                    "y": 1974
                },
                {
                    "x": 2198,
                    "y": 2764
                },
                {
                    "x": 1267,
                    "y": 2764
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='11' style='font-size:16px'>Next, we perform a series of story generation<br>evaluations with both AMT workers and expert<br>raters (English teachers), applying a variant of the<br>most common task configuration that appeared in<br>our survey (5 point Likert scale ratings of 200<br>examples with three annotators per example) to<br>the paragraph-length WritingP rompts dataset<br>of Fan et al. (2018). Unlike prior work in this area,<br>we ask raters to evaluate both stories generated by<br>a fine-tuned GPT-2 language model (Radford et al.,<br>2019) and human-written reference stories on the<br>same scale, as we expect the latter to consistently<br>score higher on all evaluations. Our experiments<br>expose and quantify several troubling trends:</p>",
            "id": 11,
            "page": 1,
            "text": "Next, we perform a series of story generation evaluations with both AMT workers and expert raters (English teachers), applying a variant of the most common task configuration that appeared in our survey (5 point Likert scale ratings of 200 examples with three annotators per example) to the paragraph-length WritingP rompts dataset of Fan  (2018). Unlike prior work in this area, we ask raters to evaluate both stories generated by a fine-tuned GPT-2 language model (Radford , 2019) and human-written reference stories on the same scale, as we expect the latter to consistently score higher on all evaluations. Our experiments expose and quantify several troubling trends:"
        },
        {
            "bounding_box": [
                {
                    "x": 1307,
                    "y": 2825
                },
                {
                    "x": 2199,
                    "y": 2825
                },
                {
                    "x": 2199,
                    "y": 3229
                },
                {
                    "x": 1307,
                    "y": 3229
                }
            ],
            "category": "paragraph",
            "html": "<p id='12' style='font-size:16px'>1. AMT ratings do not reliably distinguish<br>model-generated text from human-generated<br>text unless workers are asked to rate both side-<br>by-side, which allows them to better calibrate<br>their ratings.<br>2. Running an identical task (same AMT param-<br>eters and input data) on different days of the</p>",
            "id": 12,
            "page": 1,
            "text": "1. AMT ratings do not reliably distinguish model-generated text from human-generated text unless workers are asked to rate both sideby-side, which allows them to better calibrate their ratings. 2. Running an identical task (same AMT parameters and input data) on different days of the"
        },
        {
            "bounding_box": [
                {
                    "x": 58,
                    "y": 1106
                },
                {
                    "x": 149,
                    "y": 1106
                },
                {
                    "x": 149,
                    "y": 2536
                },
                {
                    "x": 58,
                    "y": 2536
                }
            ],
            "category": "footer",
            "html": "<br><footer id='13' style='font-size:14px'>2021<br>Sep<br>14<br>[cs.CL]<br>arXiv:2109.06835v1</footer>",
            "id": 13,
            "page": 1,
            "text": "2021 Sep 14 [cs.CL] arXiv:2109.06835v1"
        },
        {
            "bounding_box": [
                {
                    "x": 300,
                    "y": 345
                },
                {
                    "x": 2195,
                    "y": 345
                },
                {
                    "x": 2195,
                    "y": 1207
                },
                {
                    "x": 300,
                    "y": 1207
                }
            ],
            "category": "figure",
            "html": "<figure><img id='14' style='font-size:14px' alt=\"(a) While teachers unanimously marked (b) Well written story was rated as 1 out of 5 (c) Gaps in world knowledge affected ratings\nthis story as irrelevant to the prompt, an for grammar by a worker on AMT. of English teachers. A teacher unfamiliar\nAMT worker spent ~19sec reading it with the show 'The Office' rated the story\nbefore rating it a 5 out of 5 on relevance. AMT 5 lower for coherence.\n1 5, 5,\nAMT5 1, 1, 1 Prompt: The taboo over necromancy is a thing of the past.\n5, 4, 2\nYou live in a world where it is normal to become a lich and\n'live' forever. Prompt: The Office's Michael Scott has gotten\nHUMAN REFERENCE wind of the show Undercover Boss and decides he\nwants to go undercover at Dunder-Mifflin Scranton.\nPrompt: The Aliens want to know what '\n``You`re getting old.\nthe hell we did with Atlantis. HUMAN REFERENCE\nIt was a gray and miserable afternoon but Jill didn't\nGPT-2 EXT. DUNDER MIFFLIN WAREHOUSE ENTRANCE\nreally seem to notice. Her cappuccino caught\nIt was a small village nestled in the raindrops.\nMICHAEL has his back to the wall that leads to the\nmountains of Colombia. There was a little\n'I know, I know, ' dismissed Max. His latte had a lid. OPEN WAREHOUSE GARAGE DOOR. He's trying to\nvillage here. All of it belonged to a little girl\nThe two sipped their coffee in silence. Only one of them stand like he's being very stealthy, palms against the\nnamed Kate. She was a greenhorn by the\nnoticed the barista added way too much caramel. wall.\nname of Jorge, but he wasn't surprised that\nhe, and a woman named Joan were the ones\n``I... ' began Jill. She fidgeted with her mug. ``I'm MICHAEL: I Just finished Season 1 of ``Undercover\nmaking the efforts to get her to a healer. And,\nscared of losing you. ' (...) Boss' on DVD... well it was only the first disc of the\nof course, they had provided her with healthy\nfirst season. (...)\nwater, safe food, and medical care. (...)\" data-coord=\"top-left:(300,345); bottom-right:(2195,1207)\" /></figure>",
            "id": 14,
            "page": 2,
            "text": "(a) While teachers unanimously marked (b) Well written story was rated as 1 out of 5 (c) Gaps in world knowledge affected ratings this story as irrelevant to the prompt, an for grammar by a worker on AMT. of English teachers. A teacher unfamiliar AMT worker spent ~19sec reading it with the show \"The Office\" rated the story before rating it a 5 out of 5 on relevance. AMT 5 lower for coherence. 1 5, 5, AMT5 1, 1, 1 Prompt: The taboo over necromancy is a thing of the past. 5, 4, 2 You live in a world where it is normal to become a lich and \"live\" forever. Prompt: The Office's Michael Scott has gotten HUMAN REFERENCE wind of the show Undercover Boss and decides he wants to go undercover at Dunder-Mifflin Scranton. Prompt: The Aliens want to know what \" ``You`re getting old. the hell we did with Atlantis. HUMAN REFERENCE It was a gray and miserable afternoon but Jill didn't GPT-2 EXT. DUNDER MIFFLIN WAREHOUSE ENTRANCE really seem to notice. Her cappuccino caught It was a small village nestled in the raindrops. MICHAEL has his back to the wall that leads to the mountains of Colombia. There was a little \"I know, I know, \" dismissed Max. His latte had a lid. OPEN WAREHOUSE GARAGE DOOR. He's trying to village here. All of it belonged to a little girl The two sipped their coffee in silence. Only one of them stand like he's being very stealthy, palms against the named Kate. She was a greenhorn by the noticed the barista added way too much caramel. wall. name of Jorge, but he wasn't surprised that he, and a woman named Joan were the ones ``I... \" began Jill. She fidgeted with her mug. ``I'm MICHAEL: I Just finished Season 1 of ``Undercover making the efforts to get her to a healer. And, scared of losing you. \" (...) Boss\" on DVD... well it was only the first disc of the of course, they had provided her with healthy first season. (...) water, safe food, and medical care. (...)"
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 1257
                },
                {
                    "x": 2188,
                    "y": 1257
                },
                {
                    "x": 2188,
                    "y": 1357
                },
                {
                    "x": 292,
                    "y": 1357
                }
            ],
            "category": "caption",
            "html": "<caption id='15' style='font-size:16px'>Figure 1: Three examples of prompt-story pairs along with ratings from AMT workers and expert teachers that<br>demonstrate major issues with human evaluation of open-ended text generation.</caption>",
            "id": 15,
            "page": 2,
            "text": "Figure 1: Three examples of prompt-story pairs along with ratings from AMT workers and expert teachers that demonstrate major issues with human evaluation of open-ended text generation."
        },
        {
            "bounding_box": [
                {
                    "x": 325,
                    "y": 1446
                },
                {
                    "x": 1214,
                    "y": 1446
                },
                {
                    "x": 1214,
                    "y": 2312
                },
                {
                    "x": 325,
                    "y": 2312
                }
            ],
            "category": "paragraph",
            "html": "<p id='16' style='font-size:20px'>week exhibits high variance and can lead to<br>dubious conclusions (e.g., that reference texts<br>are lower quality than GPT-2 generated text).<br>3. Many AMT workers do not carefully read the<br>text that they are evaluating. Even after en-<br>abling multiple qualifications to exclude low-<br>quality workers, 42% of workers on average<br>take fewer than 40 seconds to complete each<br>task. Filtering out these workers can make a<br>significant impact to the overall ratings, but<br>also notably reduces the number of datapoints.<br>4. Even expert raters struggle to read and judge<br>model-generated text. The time they spend per<br>example increases significantly compared to<br>that for references, and agreement also drops.</p>",
            "id": 16,
            "page": 2,
            "text": "week exhibits high variance and can lead to dubious conclusions (e.g., that reference texts are lower quality than GPT-2 generated text). 3. Many AMT workers do not carefully read the text that they are evaluating. Even after enabling multiple qualifications to exclude lowquality workers, 42% of workers on average take fewer than 40 seconds to complete each task. Filtering out these workers can make a significant impact to the overall ratings, but also notably reduces the number of datapoints. 4. Even expert raters struggle to read and judge model-generated text. The time they spend per example increases significantly compared to that for references, and agreement also drops."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 2355
                },
                {
                    "x": 1213,
                    "y": 2355
                },
                {
                    "x": 1213,
                    "y": 2865
                },
                {
                    "x": 288,
                    "y": 2865
                }
            ],
            "category": "paragraph",
            "html": "<p id='17' style='font-size:20px'>For future human evaluations of open-ended text<br>generation tasks, we urge researchers to obtain ex-<br>pert raters whenever possible. If AMT is the only<br>feasible option, we recommend that available ref-<br>erence outputs also be evaluated alongside model-<br>generated ones to improve rating calibration, and<br>also that heavy filtering of the worker population<br>(possibly through qualification tasks, or post-hoc<br>removal) is performed prior to reporting results.</p>",
            "id": 17,
            "page": 2,
            "text": "For future human evaluations of open-ended text generation tasks, we urge researchers to obtain expert raters whenever possible. If AMT is the only feasible option, we recommend that available reference outputs also be evaluated alongside modelgenerated ones to improve rating calibration, and also that heavy filtering of the worker population (possibly through qualification tasks, or post-hoc removal) is performed prior to reporting results."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 2910
                },
                {
                    "x": 1185,
                    "y": 2910
                },
                {
                    "x": 1185,
                    "y": 3023
                },
                {
                    "x": 288,
                    "y": 3023
                }
            ],
            "category": "paragraph",
            "html": "<p id='18' style='font-size:22px'>2 A survey of papers that evaluate<br>open-ended text generation with AMT</p>",
            "id": 18,
            "page": 2,
            "text": "2 A survey of papers that evaluate open-ended text generation with AMT"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 3061
                },
                {
                    "x": 1212,
                    "y": 3061
                },
                {
                    "x": 1212,
                    "y": 3227
                },
                {
                    "x": 288,
                    "y": 3227
                }
            ],
            "category": "paragraph",
            "html": "<p id='19' style='font-size:20px'>We begin with a survey of 45 papers that use<br>AMT to evaluate the output of open-ended English-<br>language text generation models, which includes</p>",
            "id": 19,
            "page": 2,
            "text": "We begin with a survey of 45 papers that use AMT to evaluate the output of open-ended Englishlanguage text generation models, which includes"
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 1451
                },
                {
                    "x": 2198,
                    "y": 1451
                },
                {
                    "x": 2198,
                    "y": 2235
                },
                {
                    "x": 1267,
                    "y": 2235
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='20' style='font-size:18px'>generated stories, metaphors, paraphrases, puns,<br>sarcasm, and sentences with transferred style or at-<br>tributes 2. Each paper was published between 2018<br>and 2020 at ACL, NAACL, or EMNLP, and we ex-<br>clude papers that use AMT to evaluate more well-<br>established generation tasks like machine transla-<br>tion, or summarization. 3 Unlike previous surveys<br>of evaluating generated text (Celikyilmaz et al.,<br>2020; van der Lee et al., 2021), we focus specifi-<br>cally on AMT evaluations of open-ended text gen-<br>eration. In this section, we provide an overview of<br>the different types of evaluation task setups present<br>in our survey; later, we experiment with several<br>variants of the most common setup.</p>",
            "id": 20,
            "page": 2,
            "text": "generated stories, metaphors, paraphrases, puns, sarcasm, and sentences with transferred style or attributes 2. Each paper was published between 2018 and 2020 at ACL, NAACL, or EMNLP, and we exclude papers that use AMT to evaluate more wellestablished generation tasks like machine translation, or summarization. 3 Unlike previous surveys of evaluating generated text (Celikyilmaz , 2020; van der Lee , 2021), we focus specifically on AMT evaluations of open-ended text generation. In this section, we provide an overview of the different types of evaluation task setups present in our survey; later, we experiment with several variants of the most common setup."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 2325
                },
                {
                    "x": 2194,
                    "y": 2325
                },
                {
                    "x": 2194,
                    "y": 2831
                },
                {
                    "x": 1268,
                    "y": 2831
                }
            ],
            "category": "paragraph",
            "html": "<p id='21' style='font-size:20px'>Evaluation criteria: As in the survey<br>of Howcroft et al. (2020), we observe a va-<br>riety of different evaluation criteria and definitions<br>of these criteria across the 45 papers. The most<br>common criteria include fluency and/or grammati-<br>cality (19), overall \"quality\" (12), and relevance<br>(10) to a corresponding prompt. Furthermore,<br>stories in particular tend to also be evaluated on<br>some notion of coherence (9) and likability (4).</p>",
            "id": 21,
            "page": 2,
            "text": "Evaluation criteria: As in the survey of Howcroft  (2020), we observe a variety of different evaluation criteria and definitions of these criteria across the 45 papers. The most common criteria include fluency and/or grammaticality (19), overall \"quality\" (12), and relevance (10) to a corresponding prompt. Furthermore, stories in particular tend to also be evaluated on some notion of coherence (9) and likability (4)."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 2925
                },
                {
                    "x": 2190,
                    "y": 2925
                },
                {
                    "x": 2190,
                    "y": 3011
                },
                {
                    "x": 1269,
                    "y": 3011
                }
            ],
            "category": "paragraph",
            "html": "<p id='22' style='font-size:16px'>2The details on the survey questions and surveyed papers<br>are provided in the Appendix A</p>",
            "id": 22,
            "page": 2,
            "text": "2The details on the survey questions and surveyed papers are provided in the Appendix A"
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 3005
                },
                {
                    "x": 2191,
                    "y": 3005
                },
                {
                    "x": 2191,
                    "y": 3224
                },
                {
                    "x": 1267,
                    "y": 3224
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='23' style='font-size:14px'>3For detailed numbers see Table A1; the specific papers in<br>the survey are in Table A2. Four papers conducted multiple<br>evaluations with different settings, and as such we count them<br>in multiple categories. Moreover, all but nine papers evaluated<br>their systems on more than one attribute.</p>",
            "id": 23,
            "page": 2,
            "text": "3For detailed numbers see Table A1; the specific papers in the survey are in Table A2. Four papers conducted multiple evaluations with different settings, and as such we count them in multiple categories. Moreover, all but nine papers evaluated their systems on more than one attribute."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 300
                },
                {
                    "x": 1213,
                    "y": 300
                },
                {
                    "x": 1213,
                    "y": 809
                },
                {
                    "x": 288,
                    "y": 809
                }
            ],
            "category": "paragraph",
            "html": "<p id='24' style='font-size:18px'>Rating scales: More than half of the papers (24)<br>employ a 5-point Likert scale to evaluate the above<br>criteria; of these, 19 provided labels for just the end<br>points of the scale (e.g., \"lowest\" VS. \"highest\"),<br>while 5 labeled all points on the scale. The next<br>most common evaluation type is ranking two or<br>more system outputs (23). Less common are other<br>Likert scales (3, 4, or 6-point), pass/fail tasks, and<br>output-prompt matching tasks.</p>",
            "id": 24,
            "page": 3,
            "text": "Rating scales: More than half of the papers (24) employ a 5-point Likert scale to evaluate the above criteria; of these, 19 provided labels for just the end points of the scale (e.g., \"lowest\" VS. \"highest\"), while 5 labeled all points on the scale. The next most common evaluation type is ranking two or more system outputs (23). Less common are other Likert scales (3, 4, or 6-point), pass/fail tasks, and output-prompt matching tasks."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 839
                },
                {
                    "x": 1213,
                    "y": 839
                },
                {
                    "x": 1213,
                    "y": 1348
                },
                {
                    "x": 288,
                    "y": 1348
                }
            ],
            "category": "paragraph",
            "html": "<p id='25' style='font-size:16px'>Number of raters and rated items: An alarm-<br>ing number of papers (14) do not even report the<br>number of raters and/or items (6) used for evalu-<br>ation. Of the remaining, most papers (16) obtain<br>ratings from 3 separate AMT workers per item.<br>The most common number of items per evaluation<br>is 100 (14). The number of raters per item in other<br>studies ranges from 2 to 11, while the number of<br>items ranges from 12 to 1,000.</p>",
            "id": 25,
            "page": 3,
            "text": "Number of raters and rated items: An alarming number of papers (14) do not even report the number of raters and/or items (6) used for evaluation. Of the remaining, most papers (16) obtain ratings from 3 separate AMT workers per item. The most common number of items per evaluation is 100 (14). The number of raters per item in other studies ranges from 2 to 11, while the number of items ranges from 12 to 1,000."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 1378
                },
                {
                    "x": 1213,
                    "y": 1378
                },
                {
                    "x": 1213,
                    "y": 2000
                },
                {
                    "x": 287,
                    "y": 2000
                }
            ],
            "category": "paragraph",
            "html": "<p id='26' style='font-size:18px'>Workers qualifications and compensation:<br>The vast majority of papers do not report AMT<br>worker qualifications (32) or worker compensation<br>(35), which adds to the reproducibility woes.<br>Among papers that report qualifications, the most<br>approval rate ≥ 90%-99%,<br>common were HIT4<br>and number of approved HITs between 500 to<br>5,000. Only 11 papers mentioned restricting<br>workers to those from English-speaking countries<br>or applying some kind of language test, despite all<br>evaluations being done on English text.</p>",
            "id": 26,
            "page": 3,
            "text": "Workers qualifications and compensation: The vast majority of papers do not report AMT worker qualifications (32) or worker compensation (35), which adds to the reproducibility woes. Among papers that report qualifications, the most approval rate ≥ 90%-99%, common were HIT4 and number of approved HITs between 500 to 5,000. Only 11 papers mentioned restricting workers to those from English-speaking countries or applying some kind of language test, despite all evaluations being done on English text."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 2031
                },
                {
                    "x": 1213,
                    "y": 2031
                },
                {
                    "x": 1213,
                    "y": 2425
                },
                {
                    "x": 288,
                    "y": 2425
                }
            ],
            "category": "paragraph",
            "html": "<p id='27' style='font-size:18px'>Length of the Rated Text: As open-ended text<br>generation encompasses an array of different tasks,<br>the length of the rated text differed greatly, ranging<br>from single sentences (28), sometimes presented<br>in a longer context, to short paragraphs (7), and<br>longer paragraphs (14). The latter setting is most<br>commonly used for story generation tasks.</p>",
            "id": 27,
            "page": 3,
            "text": "Length of the Rated Text: As open-ended text generation encompasses an array of different tasks, the length of the rated text differed greatly, ranging from single sentences (28), sometimes presented in a longer context, to short paragraphs (7), and longer paragraphs (14). The latter setting is most commonly used for story generation tasks."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 2466
                },
                {
                    "x": 1207,
                    "y": 2466
                },
                {
                    "x": 1207,
                    "y": 2524
                },
                {
                    "x": 288,
                    "y": 2524
                }
            ],
            "category": "paragraph",
            "html": "<p id='28' style='font-size:22px'>3 Evaluating story generation with AMT</p>",
            "id": 28,
            "page": 3,
            "text": "3 Evaluating story generation with AMT"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 2557
                },
                {
                    "x": 1213,
                    "y": 2557
                },
                {
                    "x": 1213,
                    "y": 3064
                },
                {
                    "x": 288,
                    "y": 3064
                }
            ],
            "category": "paragraph",
            "html": "<p id='29' style='font-size:16px'>Our survey reveals that the most popular Mechani-<br>cal Turk task design for open-ended text generation<br>asks AMT workers to rate various properties of<br>generated text on a 5-point Likert scale. In this<br>section, we conduct a series of AMT evaluations<br>for the open-ended problem of story generation by<br>varying different parameters within this standard<br>task design. Importantly, we evaluate both model-<br>generated stories as well as human-generated refer-</p>",
            "id": 29,
            "page": 3,
            "text": "Our survey reveals that the most popular Mechanical Turk task design for open-ended text generation asks AMT workers to rate various properties of generated text on a 5-point Likert scale. In this section, we conduct a series of AMT evaluations for the open-ended problem of story generation by varying different parameters within this standard task design. Importantly, we evaluate both modelgenerated stories as well as human-generated refer-"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 3096
                },
                {
                    "x": 1212,
                    "y": 3096
                },
                {
                    "x": 1212,
                    "y": 3224
                },
                {
                    "x": 288,
                    "y": 3224
                }
            ],
            "category": "paragraph",
            "html": "<p id='30' style='font-size:14px'>4In AMT parlance, a human intelligence task (HIT) refers<br>to a single item; in our case, each HIT corresponds to one<br>story, which workers rate on four different properties.</p>",
            "id": 30,
            "page": 3,
            "text": "4In AMT parlance, a human intelligence task (HIT) refers to a single item; in our case, each HIT corresponds to one story, which workers rate on four different properties."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 304
                },
                {
                    "x": 2196,
                    "y": 304
                },
                {
                    "x": 2196,
                    "y": 1031
                },
                {
                    "x": 1267,
                    "y": 1031
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='31' style='font-size:16px'>ence stories, which provides a pseudo upper bound<br>for the ratings. Our experiments reveal that worker<br>qualifications (e.g., HIT approval rate and number<br>of accepted HITs) do not notably impact judgments<br>or spam rate on reference stories, with the excep-<br>tion of country of origin. Furthermore, we uncover<br>an issue with rating calibration: when both ref-<br>erence and model-generated stories are included<br>for the same prompt, average reference scores are<br>significantly higher than those for model-generated<br>text; however, when workers only see one type of<br>text per HIT, they give similar average scores to<br>both types.</p>",
            "id": 31,
            "page": 3,
            "text": "ence stories, which provides a pseudo upper bound for the ratings. Our experiments reveal that worker qualifications (e.g., HIT approval rate and number of accepted HITs) do not notably impact judgments or spam rate on reference stories, with the exception of country of origin. Furthermore, we uncover an issue with rating calibration: when both reference and model-generated stories are included for the same prompt, average reference scores are significantly higher than those for model-generated text; however, when workers only see one type of text per HIT, they give similar average scores to both types."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 1069
                },
                {
                    "x": 1769,
                    "y": 1069
                },
                {
                    "x": 1769,
                    "y": 1123
                },
                {
                    "x": 1268,
                    "y": 1123
                }
            ],
            "category": "paragraph",
            "html": "<p id='32' style='font-size:20px'>3.1 Experimental Setup</p>",
            "id": 32,
            "page": 3,
            "text": "3.1 Experimental Setup"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 1141
                },
                {
                    "x": 2191,
                    "y": 1141
                },
                {
                    "x": 2191,
                    "y": 1257
                },
                {
                    "x": 1268,
                    "y": 1257
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='33' style='font-size:16px'>We first describe the parameters of our experiments<br>before later analyzing the results.</p>",
            "id": 33,
            "page": 3,
            "text": "We first describe the parameters of our experiments before later analyzing the results."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 1285
                },
                {
                    "x": 2195,
                    "y": 1285
                },
                {
                    "x": 2195,
                    "y": 2470
                },
                {
                    "x": 1268,
                    "y": 2470
                }
            ],
            "category": "paragraph",
            "html": "<p id='34' style='font-size:16px'>Dataset: We use the WritingP rompts<br>dataset collected by Fan et al. (2018), which is a<br>collection of 303,358 English language stories writ-<br>ten by Reddit users on the r /WritingP rompts<br>subreddit.5 This dataset, which consists of short<br>prompts paired with user-written stories (e.g.,<br>\"There are 10 legendary dentists who review every<br>toothpaste. You are the 10th... being hunted by<br>the other 9...\"), has been used in multiple previous<br>works on paragraph-length story generation (Fan<br>et al., 2019; See et al., 2019; Mao et al., 2019). We<br>randomly select 200 prompts from the test set for<br>all of our experiments. Since the human-written<br>stories in the dataset are already tokenized, we<br>first de-tokenized the stories, cleaned up artifacts<br>from lemmatization, and manually truncated each<br>story SO that it ends with a full sentence and is no<br>longer than 150 words in order to make the length<br>6<br>comparable with the machine-generated story.<br>We use the resulting stories for all experiments<br>with reference text.</p>",
            "id": 34,
            "page": 3,
            "text": "Dataset: We use the WritingP rompts dataset collected by Fan  (2018), which is a collection of 303,358 English language stories written by Reddit users on the r /WritingP rompts subreddit.5 This dataset, which consists of short prompts paired with user-written stories (e.g., \"There are 10 legendary dentists who review every toothpaste. You are the 10th... being hunted by the other 9...\"), has been used in multiple previous works on paragraph-length story generation (Fan , 2019; See , 2019; Mao , 2019). We randomly select 200 prompts from the test set for all of our experiments. Since the human-written stories in the dataset are already tokenized, we first de-tokenized the stories, cleaned up artifacts from lemmatization, and manually truncated each story SO that it ends with a full sentence and is no longer than 150 words in order to make the length 6 comparable with the machine-generated story. We use the resulting stories for all experiments with reference text."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 2497
                },
                {
                    "x": 2195,
                    "y": 2497
                },
                {
                    "x": 2195,
                    "y": 3066
                },
                {
                    "x": 1269,
                    "y": 3066
                }
            ],
            "category": "paragraph",
            "html": "<p id='35' style='font-size:18px'>Model-generated stories: We follow a simi-<br>lar modeling approach to prior story genera-<br>tion work (Mao et al., 2019; Guan et al., 2020)<br>by fine-tuning a pretrained GPT-2 medium-sized<br>model (Radford et al., 2019) on the training set<br>of the WritingPrompts dataset, using the Hugging-<br>Face Transformers library (Wolf et al., 2020). We<br>use a batch size of approximately 50k tokens, a<br>learning rate of 5e - 5 with a linear learning rate<br>schedule, and train for 3 epochs, stopping training</p>",
            "id": 35,
            "page": 3,
            "text": "Model-generated stories: We follow a similar modeling approach to prior story generation work (Mao , 2019; Guan , 2020) by fine-tuning a pretrained GPT-2 medium-sized model (Radford , 2019) on the training set of the WritingPrompts dataset, using the HuggingFace Transformers library (Wolf , 2020). We use a batch size of approximately 50k tokens, a learning rate of 5e - 5 with a linear learning rate schedule, and train for 3 epochs, stopping training"
        },
        {
            "bounding_box": [
                {
                    "x": 1325,
                    "y": 3096
                },
                {
                    "x": 2150,
                    "y": 3096
                },
                {
                    "x": 2150,
                    "y": 3134
                },
                {
                    "x": 1325,
                    "y": 3134
                }
            ],
            "category": "paragraph",
            "html": "<p id='36' style='font-size:14px'>5https : / / reddit · com/ r /WritingPrompts /</p>",
            "id": 36,
            "page": 3,
            "text": "5https : / / reddit · com/ r /WritingPrompts /"
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 3143
                },
                {
                    "x": 2186,
                    "y": 3143
                },
                {
                    "x": 2186,
                    "y": 3221
                },
                {
                    "x": 1271,
                    "y": 3221
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='37' style='font-size:14px'>6The mean length of the selected reference stories is 134.5<br>tokens, with a standard deviation of 8.81.</p>",
            "id": 37,
            "page": 3,
            "text": "6The mean length of the selected reference stories is 134.5 tokens, with a standard deviation of 8.81."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 300
                },
                {
                    "x": 1213,
                    "y": 300
                },
                {
                    "x": 1213,
                    "y": 976
                },
                {
                    "x": 287,
                    "y": 976
                }
            ],
            "category": "paragraph",
            "html": "<p id='38' style='font-size:18px'>after validation perplexity converges to ~ 19. Each<br>training example consists of a concatenation of a<br>prompt, separator token (new line character), and<br>reference story. At test-time, we feed the same<br>200 prompts selected above to our model for fair<br>comparison to the human-written stories, and we<br>generate three stories per prompt using nucleus<br>sampling (Holtzman et al., 2019) with p = 0.9. We<br>manually truncate each sample SO that it ends with<br>a full sentence and is no longer than 150 words. 7<br>These stories are used in all experiments evaluating<br>machine-generated stories.</p>",
            "id": 38,
            "page": 4,
            "text": "after validation perplexity converges to ~ 19. Each training example consists of a concatenation of a prompt, separator token (new line character), and reference story. At test-time, we feed the same 200 prompts selected above to our model for fair comparison to the human-written stories, and we generate three stories per prompt using nucleus sampling (Holtzman , 2019) with p = 0.9. We manually truncate each sample SO that it ends with a full sentence and is no longer than 150 words. 7 These stories are used in all experiments evaluating machine-generated stories."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 1012
                },
                {
                    "x": 1214,
                    "y": 1012
                },
                {
                    "x": 1214,
                    "y": 1351
                },
                {
                    "x": 287,
                    "y": 1351
                }
            ],
            "category": "paragraph",
            "html": "<p id='39' style='font-size:18px'>AMT task parameters: We conduct all exper-<br>iments using the default interface in Mechanical<br>Turk (see Figure A1 and Figure A2). Workers<br>were asked to rate human-written and/or machine-<br>generated stories on four attributes, with the fol-<br>lowing definitions provided to them:</p>",
            "id": 39,
            "page": 4,
            "text": "AMT task parameters: We conduct all experiments using the default interface in Mechanical Turk (see Figure A1 and Figure A2). Workers were asked to rate human-written and/or machinegenerated stories on four attributes, with the following definitions provided to them:"
        },
        {
            "bounding_box": [
                {
                    "x": 323,
                    "y": 1393
                },
                {
                    "x": 1216,
                    "y": 1393
                },
                {
                    "x": 1216,
                    "y": 1872
                },
                {
                    "x": 323,
                    "y": 1872
                }
            ],
            "category": "paragraph",
            "html": "<p id='40' style='font-size:18px'>1. Grammar: \"How grammatically correct is the<br>text of the story fragment?\"<br>2. Coherence: \"How well do the sentences in the<br>story fragment fit together?\"<br>3. Likability: \"How enjoyable do you find the<br>story fragment?\"<br>4. Relevance: \"How relevant is the story frag-<br>ment to the prompt?\"</p>",
            "id": 40,
            "page": 4,
            "text": "1. Grammar: \"How grammatically correct is the text of the story fragment?\" 2. Coherence: \"How well do the sentences in the story fragment fit together?\" 3. Likability: \"How enjoyable do you find the story fragment?\" 4. Relevance: \"How relevant is the story fragment to the prompt?\""
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 1916
                },
                {
                    "x": 1212,
                    "y": 1916
                },
                {
                    "x": 1212,
                    "y": 2306
                },
                {
                    "x": 288,
                    "y": 2306
                }
            ],
            "category": "paragraph",
            "html": "<p id='41' style='font-size:20px'>Their ratings fall on a 5-point Likert scale with<br>the corresponding endpoints labelled as \"lowest\" (1<br>point) and \"highest\" (5 points). Since our survey<br>did not find many previous papers that reported<br>using detailed descriptions for each point on the<br>scale, we chose to use minimal labels to mimic the<br>most popular setup (see Section 2 for details).</p>",
            "id": 41,
            "page": 4,
            "text": "Their ratings fall on a 5-point Likert scale with the corresponding endpoints labelled as \"lowest\" (1 point) and \"highest\" (5 points). Since our survey did not find many previous papers that reported using detailed descriptions for each point on the scale, we chose to use minimal labels to mimic the most popular setup (see Section 2 for details)."
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 2312
                },
                {
                    "x": 1213,
                    "y": 2312
                },
                {
                    "x": 1213,
                    "y": 3104
                },
                {
                    "x": 286,
                    "y": 3104
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='42' style='font-size:20px'>Each of our AMT experiments shows workers<br>the same 200 prompts paired with human and/or<br>machine-generated stories, and we solicit three<br>worker judgments per HIT. Workers were paid<br>$0.20 per HIT for tasks that showed one story, and<br>$0.35 per HIT for those that showed two stories;<br>in total, our AMT experiments cost roughly $1.5K.<br>Importantly, each experiment used a completely dif-<br>ferent set of workers (i.e., each worker could only<br>participate in one experiment, although they can<br>complete multiple HITs within that experiment),<br>which is an intentional choice to prevent workers<br>from judging the same story multiple times. Fi-<br>nally, to eliminate potential variations stemming</p>",
            "id": 42,
            "page": 4,
            "text": "Each of our AMT experiments shows workers the same 200 prompts paired with human and/or machine-generated stories, and we solicit three worker judgments per HIT. Workers were paid $0.20 per HIT for tasks that showed one story, and $0.35 per HIT for those that showed two stories; in total, our AMT experiments cost roughly $1.5K. Importantly, each experiment used a completely different set of workers (i.e., each worker could only participate in one experiment, although they can complete multiple HITs within that experiment), which is an intentional choice to prevent workers from judging the same story multiple times. Finally, to eliminate potential variations stemming"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 3137
                },
                {
                    "x": 1215,
                    "y": 3137
                },
                {
                    "x": 1215,
                    "y": 3222
                },
                {
                    "x": 288,
                    "y": 3222
                }
            ],
            "category": "paragraph",
            "html": "<p id='43' style='font-size:14px'>7The mean length of the generated stories is 137.4 tokens,<br>with a standard deviation of 8.36.</p>",
            "id": 43,
            "page": 4,
            "text": "7The mean length of the generated stories is 137.4 tokens, with a standard deviation of 8.36."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 303
                },
                {
                    "x": 2198,
                    "y": 303
                },
                {
                    "x": 2198,
                    "y": 468
                },
                {
                    "x": 1268,
                    "y": 468
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='44' style='font-size:18px'>from evaluation on different days (weekdays VS.<br>weekends) and time of day, we launch all experi-<br>ments on weekdays between 11:00-11:30AM PST.</p>",
            "id": 44,
            "page": 4,
            "text": "from evaluation on different days (weekdays VS. weekends) and time of day, we launch all experiments on weekdays between 11:00-11:30AM PST."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 504
                },
                {
                    "x": 2079,
                    "y": 504
                },
                {
                    "x": 2079,
                    "y": 555
                },
                {
                    "x": 1269,
                    "y": 555
                }
            ],
            "category": "paragraph",
            "html": "<p id='45' style='font-size:18px'>3.2 AMT Evaluations of Reference Text</p>",
            "id": 45,
            "page": 4,
            "text": "3.2 AMT Evaluations of Reference Text"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 578
                },
                {
                    "x": 2197,
                    "y": 578
                },
                {
                    "x": 2197,
                    "y": 1252
                },
                {
                    "x": 1268,
                    "y": 1252
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='46' style='font-size:18px'>Our first set of experiments concerns only human-<br>written reference stories; we move to machine-<br>generated text in the next subsection. One of our<br>assumptions with human-written stories, supported<br>by the expert teacher assessment in Section 4, is<br>that they should receive relatively high scores for<br>all four properties (except perhaps likability which<br>is highly subjective). We thus use reference texts<br>to evaluate various AMT parameters such as qual-<br>ifications or day of task launch, observing how<br>modifications to these parameters affect the aver-<br>age scores of reference text.</p>",
            "id": 46,
            "page": 4,
            "text": "Our first set of experiments concerns only humanwritten reference stories; we move to machinegenerated text in the next subsection. One of our assumptions with human-written stories, supported by the expert teacher assessment in Section 4, is that they should receive relatively high scores for all four properties (except perhaps likability which is highly subjective). We thus use reference texts to evaluate various AMT parameters such as qualifications or day of task launch, observing how modifications to these parameters affect the average scores of reference text."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 1280
                },
                {
                    "x": 2196,
                    "y": 1280
                },
                {
                    "x": 2196,
                    "y": 1842
                },
                {
                    "x": 1268,
                    "y": 1842
                }
            ],
            "category": "paragraph",
            "html": "<p id='47' style='font-size:20px'>Impact of worker qualifications: We run four<br>experiments evaluating the previously-described<br>set of 200 prompts with reference story fragments,<br>varying the worker qualifications as follows: (1)<br>no qualifications, (2) including only workers with<br>HIT approval rate > 90%, (3) including only work-<br>ers with approval rate > 90% and at least 1000<br>approved HITs, (4) including only workers with ap-<br>proval rate > 90% and at least 1000 approved HITs<br>who are located in English-speaking countries. 8</p>",
            "id": 47,
            "page": 4,
            "text": "Impact of worker qualifications: We run four experiments evaluating the previously-described set of 200 prompts with reference story fragments, varying the worker qualifications as follows: (1) no qualifications, (2) including only workers with HIT approval rate > 90%, (3) including only workers with approval rate > 90% and at least 1000 approved HITs, (4) including only workers with approval rate > 90% and at least 1000 approved HITs who are located in English-speaking countries. 8"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 1850
                },
                {
                    "x": 2197,
                    "y": 1850
                },
                {
                    "x": 2197,
                    "y": 2971
                },
                {
                    "x": 1268,
                    "y": 2971
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='48' style='font-size:20px'>The results in the top portion of Table 1 suggest<br>that applying all of the qualifications (i.e., workers<br>from English-speaking countries, approval rate ><br>90%, approved HITs ≥ 1000) has a positive effect<br>on the quality of workers, as this setting yielded the<br>highest scores out of the four experiments for co-<br>herence and relevance while ratings for grammar<br>were also considerably high. Ratings for likabil-<br>ity were lower than in the experiments with less<br>strict qualifications, but likability is a very sub-<br>jective measure which consistently shows a very<br>low agreement (Krippendorff's a of -0.04 to 0.11).<br>When all AMT worker qualifications are enabled,<br>the worker ratings more closely align to those made<br>by English teachers, although there are still substan-<br>tial deviations (Section 4). Additionally, with all<br>qualifications enabled, workers show higher agree-<br>ment for grammar, coherence, relevance and even<br>likability, although the agreement between raters<br>remains low.</p>",
            "id": 48,
            "page": 4,
            "text": "The results in the top portion of Table 1 suggest that applying all of the qualifications (i.e., workers from English-speaking countries, approval rate > 90%, approved HITs ≥ 1000) has a positive effect on the quality of workers, as this setting yielded the highest scores out of the four experiments for coherence and relevance while ratings for grammar were also considerably high. Ratings for likability were lower than in the experiments with less strict qualifications, but likability is a very subjective measure which consistently shows a very low agreement (Krippendorff's a of -0.04 to 0.11). When all AMT worker qualifications are enabled, the worker ratings more closely align to those made by English teachers, although there are still substantial deviations (Section 4). Additionally, with all qualifications enabled, workers show higher agreement for grammar, coherence, relevance and even likability, although the agreement between raters remains low."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 3001
                },
                {
                    "x": 2198,
                    "y": 3001
                },
                {
                    "x": 2198,
                    "y": 3112
                },
                {
                    "x": 1269,
                    "y": 3112
                }
            ],
            "category": "paragraph",
            "html": "<p id='49' style='font-size:20px'>High variance across different days: Con-<br>cerned by the low overall agreement, we decided</p>",
            "id": 49,
            "page": 4,
            "text": "High variance across different days: Concerned by the low overall agreement, we decided"
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 3137
                },
                {
                    "x": 2194,
                    "y": 3137
                },
                {
                    "x": 2194,
                    "y": 3224
                },
                {
                    "x": 1270,
                    "y": 3224
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='50' style='font-size:16px'>8We include workers from the US, Canada, the UK, Aus-<br>tralia, and New Zealand.</p>",
            "id": 50,
            "page": 4,
            "text": "8We include workers from the US, Canada, the UK, Australia, and New Zealand."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 291
                },
                {
                    "x": 2190,
                    "y": 291
                },
                {
                    "x": 2190,
                    "y": 1120
                },
                {
                    "x": 292,
                    "y": 1120
                }
            ],
            "category": "table",
            "html": "<table id='51' style='font-size:20px'><tr><td rowspan=\"2\">Experiment description</td><td colspan=\"2\">Grammar</td><td colspan=\"2\">Coherence</td><td colspan=\"2\">Relevance</td><td colspan=\"2\">Likability</td></tr><tr><td>MeansTD</td><td>IAA%</td><td>MeansTD</td><td>IAA%</td><td>MeansTD</td><td>IAA%</td><td>MeansTD</td><td>IAA%</td></tr><tr><td></td><td colspan=\"8\">Impact of Qualifications</td></tr><tr><td>No qualifications</td><td>4.050.90</td><td>0.0814.5</td><td>3.920.98</td><td>0.024.5</td><td>3.661.22</td><td>0.1311</td><td>3.641.16</td><td>0.027</td></tr><tr><td>+ > 90% HIT approval</td><td>4.160.86</td><td>0.0718</td><td>4.070.93</td><td>0.0610.5</td><td>3.671.14</td><td>0.07g</td><td>3.681.15</td><td>0.0810</td></tr><tr><td>+ at least 1000 HITs</td><td>3.910.85</td><td>0.0512</td><td>3.850.98</td><td>0.0811.5</td><td>3.601.15</td><td>0.188</td><td>3.631.13</td><td>0.0712.5</td></tr><tr><td>+ English-speaking countries</td><td>4.000.92</td><td>0.2115.5</td><td>4.110.96</td><td>0.1416.5</td><td>3.711.26</td><td>0.2710</td><td>3.371.18</td><td>0.117.5</td></tr><tr><td></td><td colspan=\"8\">Variance Across Days</td></tr><tr><td>Day 1 (all quals.)</td><td>4.000.92</td><td>0.2115.5</td><td>4.110.96</td><td>0.1416.5</td><td>3.711.26</td><td>0.2710</td><td>3.371.18</td><td>0.117.5</td></tr><tr><td>Day 2 (all quals.)</td><td>3.860.92</td><td>-0.0310.5</td><td>3.920.98</td><td>-0.036.5</td><td>3.711.08</td><td>0.0211</td><td>3.730.97</td><td>-0.048.5</td></tr><tr><td>Day 3 (all quals.)</td><td>3.980.96</td><td>0.1811</td><td>4.050.94</td><td>0.1310.5</td><td>3.461.29</td><td>0.26g</td><td>3.421.16</td><td>0.074.5</td></tr><tr><td></td><td colspan=\"8\">Impact of Country of Origin</td></tr><tr><td>- English-speaking countries</td><td>3.821.04</td><td>0.0311</td><td>3.451.19</td><td>-0.019</td><td>3.251.27</td><td>0.036.5</td><td>3.321.26</td><td>-0.093</td></tr><tr><td></td><td colspan=\"8\">Impact of Filtering by the Median Work Time</td></tr><tr><td>Day 2 (Median ≥ 40s)</td><td>4.040.94</td><td></td><td>4.330.92</td><td></td><td>3.741.34</td><td></td><td>3.671.06</td><td></td></tr></table>",
            "id": 51,
            "page": 5,
            "text": "Experiment description Grammar Coherence Relevance Likability  MeansTD IAA% MeansTD IAA% MeansTD IAA% MeansTD IAA%   Impact of Qualifications  No qualifications 4.050.90 0.0814.5 3.920.98 0.024.5 3.661.22 0.1311 3.641.16 0.027  + > 90% HIT approval 4.160.86 0.0718 4.070.93 0.0610.5 3.671.14 0.07g 3.681.15 0.0810  + at least 1000 HITs 3.910.85 0.0512 3.850.98 0.0811.5 3.601.15 0.188 3.631.13 0.0712.5  + English-speaking countries 4.000.92 0.2115.5 4.110.96 0.1416.5 3.711.26 0.2710 3.371.18 0.117.5   Variance Across Days  Day 1 (all quals.) 4.000.92 0.2115.5 4.110.96 0.1416.5 3.711.26 0.2710 3.371.18 0.117.5  Day 2 (all quals.) 3.860.92 -0.0310.5 3.920.98 -0.036.5 3.711.08 0.0211 3.730.97 -0.048.5  Day 3 (all quals.) 3.980.96 0.1811 4.050.94 0.1310.5 3.461.29 0.26g 3.421.16 0.074.5   Impact of Country of Origin  - English-speaking countries 3.821.04 0.0311 3.451.19 -0.019 3.251.27 0.036.5 3.321.26 -0.093   Impact of Filtering by the Median Work Time  Day 2 (Median ≥ 40s) 4.040.94  4.330.92  3.741.34  3.671.06"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 1148
                },
                {
                    "x": 2197,
                    "y": 1148
                },
                {
                    "x": 2197,
                    "y": 1354
                },
                {
                    "x": 287,
                    "y": 1354
                }
            ],
            "category": "caption",
            "html": "<caption id='52' style='font-size:18px'>Table 1: AMT experiments on human-written reference stories. Inter-annotator agreement (IAA) between the<br>three raters is measured with Krippendorff's a as well as the percentage of stories for which all three raters exactly<br>agreed on a rating (the latter is subscripted). Last section omits IAA due to the large number of missing datapoints.<br>Statistical significance for the relations between groups is provided in Appendix D.</caption>",
            "id": 52,
            "page": 5,
            "text": "Table 1: AMT experiments on human-written reference stories. Inter-annotator agreement (IAA) between the three raters is measured with Krippendorff's a as well as the percentage of stories for which all three raters exactly agreed on a rating (the latter is subscripted). Last section omits IAA due to the large number of missing datapoints. Statistical significance for the relations between groups is provided in Appendix D."
        },
        {
            "bounding_box": [
                {
                    "x": 297,
                    "y": 1441
                },
                {
                    "x": 1197,
                    "y": 1441
                },
                {
                    "x": 1197,
                    "y": 1801
                },
                {
                    "x": 297,
                    "y": 1801
                }
            ],
            "category": "figure",
            "html": "<figure><img id='53' style='font-size:14px' alt=\"Median Completion Time < 40s\nHITS 60\nMedian Completion Time >= 40s\nCompleted\n40\n20\n0\nIndividual Workers\" data-coord=\"top-left:(297,1441); bottom-right:(1197,1801)\" /></figure>",
            "id": 53,
            "page": 5,
            "text": "Median Completion Time < 40s HITS 60 Median Completion Time >= 40s Completed 40 20 0 Individual Workers"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 1851
                },
                {
                    "x": 1213,
                    "y": 1851
                },
                {
                    "x": 1213,
                    "y": 2004
                },
                {
                    "x": 287,
                    "y": 2004
                }
            ],
            "category": "caption",
            "html": "<caption id='54' style='font-size:18px'>Figure 2: This example from Day 2 shows that many<br>AMT workers complete multiple HITs in less time and<br>with lower agreement in comparison to our experts.</caption>",
            "id": 54,
            "page": 5,
            "text": "Figure 2: This example from Day 2 shows that many AMT workers complete multiple HITs in less time and with lower agreement in comparison to our experts."
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 2104
                },
                {
                    "x": 1213,
                    "y": 2104
                },
                {
                    "x": 1213,
                    "y": 3062
                },
                {
                    "x": 286,
                    "y": 3062
                }
            ],
            "category": "paragraph",
            "html": "<p id='55' style='font-size:22px'>to run another set of experiments that repeats the<br>same experiment (all qualifications enabled) across<br>three different days. Due to our constraint that<br>each worker can only participate in one experi-<br>ment, each of these experiments has a different<br>subset of qualified workers. As shown in the sec-<br>ond portion of Table 1, although the first and third<br>days yielded similar mean ratings/agreement in<br>terms of grammar (M=4.00, IAA=0.21 VS M=3.98,<br>IAA=0.18) and coherence (M=4.11, IAA=0.14 VS<br>M=4.05, IAA=0.13), the second day received lower<br>ratings across the board and had overall poor IAA<br>(see Table 1). Furthermore, ratings for relevance<br>in the third day (M=3.46) were significantly lower<br>than in the first two days (M=3.71), which indi-<br>cates that simply using all AMT qualifications is<br>not enough to achieve consistent results.</p>",
            "id": 55,
            "page": 5,
            "text": "to run another set of experiments that repeats the same experiment (all qualifications enabled) across three different days. Due to our constraint that each worker can only participate in one experiment, each of these experiments has a different subset of qualified workers. As shown in the second portion of Table 1, although the first and third days yielded similar mean ratings/agreement in terms of grammar (M=4.00, IAA=0.21 VS M=3.98, IAA=0.18) and coherence (M=4.11, IAA=0.14 VS M=4.05, IAA=0.13), the second day received lower ratings across the board and had overall poor IAA (see Table 1). Furthermore, ratings for relevance in the third day (M=3.46) were significantly lower than in the first two days (M=3.71), which indicates that simply using all AMT qualifications is not enough to achieve consistent results."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 3114
                },
                {
                    "x": 1214,
                    "y": 3114
                },
                {
                    "x": 1214,
                    "y": 3229
                },
                {
                    "x": 287,
                    "y": 3229
                }
            ],
            "category": "caption",
            "html": "<caption id='56' style='font-size:22px'>Many AMT workers do not spend enough time<br>reading the stories: The low overall agreement</caption>",
            "id": 56,
            "page": 5,
            "text": "Many AMT workers do not spend enough time reading the stories: The low overall agreement"
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 1443
                },
                {
                    "x": 2197,
                    "y": 1443
                },
                {
                    "x": 2197,
                    "y": 2678
                },
                {
                    "x": 1267,
                    "y": 2678
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='57' style='font-size:20px'>also motivated us to examine the average time each<br>worker spent per HIT. While AMT reports Work-<br>TimeInSeconds in the results file made available to<br>task requesters, we observe similar to Akoury et al.<br>(2020) that these times are artificially inflated due<br>to workers who accept multiple HITs at the same<br>time and work on them sequentially (e.g., in differ-<br>ent tabs). Such workers are also frequently among<br>the most prolific in terms of HITs completed per<br>experiment (see Figure 2), since there is no max-<br>imum number of HITs per worker. 9 We correct<br>for this by measuring the time between consecu-<br>tively submitted HITs by the same worker, which<br>can be derived by analyzing start and end times of<br>each HIT. This \"actual time\" differs considerably<br>from the AMT reported WorkTimeInSeconds: for<br>instance, a worker that AMT reports had a mean<br>work-time of 360 seconds had an actual mean work-<br>ing time10 of 22s and a median of 13s. To put these<br>numbers in perspective, this is about one-fourth of<br>the time that the fastest English teacher achieved<br>(see Section 4).</p>",
            "id": 57,
            "page": 5,
            "text": "also motivated us to examine the average time each worker spent per HIT. While AMT reports WorkTimeInSeconds in the results file made available to task requesters, we observe similar to Akoury  (2020) that these times are artificially inflated due to workers who accept multiple HITs at the same time and work on them sequentially (e.g., in different tabs). Such workers are also frequently among the most prolific in terms of HITs completed per experiment (see Figure 2), since there is no maximum number of HITs per worker. 9 We correct for this by measuring the time between consecutively submitted HITs by the same worker, which can be derived by analyzing start and end times of each HIT. This \"actual time\" differs considerably from the AMT reported WorkTimeInSeconds: for instance, a worker that AMT reports had a mean work-time of 360 seconds had an actual mean working time10 of 22s and a median of 13s. To put these numbers in perspective, this is about one-fourth of the time that the fastest English teacher achieved (see Section 4)."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 2687
                },
                {
                    "x": 2196,
                    "y": 2687
                },
                {
                    "x": 2196,
                    "y": 2851
                },
                {
                    "x": 1268,
                    "y": 2851
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='58' style='font-size:18px'>As it is impossible to carefully read a paragraph-<br>length story and assess all four properties in as little<br>as 13 seconds, we measure the impact on average</p>",
            "id": 58,
            "page": 5,
            "text": "As it is impossible to carefully read a paragraphlength story and assess all four properties in as little as 13 seconds, we measure the impact on average"
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 2883
                },
                {
                    "x": 2194,
                    "y": 2883
                },
                {
                    "x": 2194,
                    "y": 3092
                },
                {
                    "x": 1267,
                    "y": 3092
                }
            ],
            "category": "paragraph",
            "html": "<p id='59' style='font-size:14px'>9Like most AMT tasks (Fort et al., 2011; Robinson et al.,<br>2019), the majority of HITs for our evaluations are provided by<br>a small fraction of workers. The majority of workers provided<br>ratings for only one or two stories while a very few productive<br>workers rated over 50% of the stories (see Figure 2).</p>",
            "id": 59,
            "page": 5,
            "text": "9Like most AMT tasks (Fort , 2011; Robinson , 2019), the majority of HITs for our evaluations are provided by a small fraction of workers. The majority of workers provided ratings for only one or two stories while a very few productive workers rated over 50% of the stories (see Figure 2)."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 3100
                },
                {
                    "x": 2193,
                    "y": 3100
                },
                {
                    "x": 2193,
                    "y": 3223
                },
                {
                    "x": 1271,
                    "y": 3223
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='60' style='font-size:16px'>10The mean work time is also not very representative as<br>workers typically accept multiple HITs, wait a period of time,<br>then submit all accepted HITs in quick succession.</p>",
            "id": 60,
            "page": 5,
            "text": "10The mean work time is also not very representative as workers typically accept multiple HITs, wait a period of time, then submit all accepted HITs in quick succession."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 301
                },
                {
                    "x": 1213,
                    "y": 301
                },
                {
                    "x": 1213,
                    "y": 863
                },
                {
                    "x": 287,
                    "y": 863
                }
            ],
            "category": "paragraph",
            "html": "<p id='61' style='font-size:18px'>ratings when filtering out workers who spend too lit-<br>tle time per HIT (last row of Table 1). Specifically,<br>we remove judgments from workers whose median<br>time is below 40s (which is a low bar), and find that<br>on average about 42% of our ratings are filtered out<br>(ranging from 20%-72% across all experiments). 11<br>Of our surveyed papers, only Akoury et al. (2020)<br>report actual work time, demonstrating that this is<br>a major issue in modern AMT evaluations of text<br>quality that most researchers have overlooked.</p>",
            "id": 61,
            "page": 6,
            "text": "ratings when filtering out workers who spend too little time per HIT (last row of Table 1). Specifically, we remove judgments from workers whose median time is below 40s (which is a low bar), and find that on average about 42% of our ratings are filtered out (ranging from 20%-72% across all experiments). 11 Of our surveyed papers, only Akoury  (2020) report actual work time, demonstrating that this is a major issue in modern AMT evaluations of text quality that most researchers have overlooked."
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 897
                },
                {
                    "x": 1214,
                    "y": 897
                },
                {
                    "x": 1214,
                    "y": 2203
                },
                {
                    "x": 286,
                    "y": 2203
                }
            ],
            "category": "paragraph",
            "html": "<p id='62' style='font-size:20px'>Impact of worker country of origin: While all<br>of the surveyed papers evaluate only English text,<br>only 11 of them reported using some kind of filter-<br>ing to ensure that workers have sufficient knowl-<br>edge of English. The default AMT setting does not<br>filter workers by country of origin, which poten-<br>tially increases the variance of results depending<br>on the English proficiency of workers who accept<br>HITs. To measure this, we re-run our experiment<br>with all qualifications, except we restrict the task<br>to only workers from countries that do not primar-<br>ily speak English (i.e., we exclude workers from<br>the US, Canada, UK, Australia, New Zealand, Ire-<br>land, and Singapore). The third portion of Table 1<br>shows that workers from non-English speaking<br>countries rated coherence, relevance, and gram-<br>mar12 lower than identically-qualified<br>significantly<br>workers from English-speaking countries (Day 1-<br>3). Thus, researchers rating English text should<br>restrict their tasks to English-speaking countries, al-<br>though Kennedy et al. (2020) find that many work-<br>ers use Virtual Private Networks (VPNs) to take<br>part in tasks restricted to those in the US.</p>",
            "id": 62,
            "page": 6,
            "text": "Impact of worker country of origin: While all of the surveyed papers evaluate only English text, only 11 of them reported using some kind of filtering to ensure that workers have sufficient knowledge of English. The default AMT setting does not filter workers by country of origin, which potentially increases the variance of results depending on the English proficiency of workers who accept HITs. To measure this, we re-run our experiment with all qualifications, except we restrict the task to only workers from countries that do not primarily speak English (i.e., we exclude workers from the US, Canada, UK, Australia, New Zealand, Ireland, and Singapore). The third portion of Table 1 shows that workers from non-English speaking countries rated coherence, relevance, and grammar12 lower than identically-qualified significantly workers from English-speaking countries (Day 13). Thus, researchers rating English text should restrict their tasks to English-speaking countries, although Kennedy  (2020) find that many workers use Virtual Private Networks (VPNs) to take part in tasks restricted to those in the US."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 2248
                },
                {
                    "x": 1110,
                    "y": 2248
                },
                {
                    "x": 1110,
                    "y": 2301
                },
                {
                    "x": 288,
                    "y": 2301
                }
            ],
            "category": "paragraph",
            "html": "<p id='63' style='font-size:22px'>3.3 Evaluating Machine-Generated Text</p>",
            "id": 63,
            "page": 6,
            "text": "3.3 Evaluating Machine-Generated Text"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 2325
                },
                {
                    "x": 1213,
                    "y": 2325
                },
                {
                    "x": 1213,
                    "y": 2718
                },
                {
                    "x": 288,
                    "y": 2718
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='64' style='font-size:18px'>We now turn to AMT evaluation of machine-<br>generated stories produced by the GPT-2 model<br>described in subsection 3.1. Based on our previous<br>experiments with reference texts, we select the \"all<br>qualifications\" setting (i.e., workers from English-<br>speaking countries, approval rate > 90%, approved<br>HITs ≥ 1000) for all GPT-2 AMT tasks. We study</p>",
            "id": 64,
            "page": 6,
            "text": "We now turn to AMT evaluation of machinegenerated stories produced by the GPT-2 model described in subsection 3.1. Based on our previous experiments with reference texts, we select the \"all qualifications\" setting (i.e., workers from Englishspeaking countries, approval rate > 90%, approved HITs ≥ 1000) for all GPT-2 AMT tasks. We study"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 2760
                },
                {
                    "x": 1213,
                    "y": 2760
                },
                {
                    "x": 1213,
                    "y": 3092
                },
                {
                    "x": 287,
                    "y": 3092
                }
            ],
            "category": "paragraph",
            "html": "<p id='65' style='font-size:14px'>11We also ran experiments with even stricter qualification<br>filters (i.e., acceptance rate ≥ 99% and at least 10,000 ap-<br>proved tasks), but this made no notable difference to the per-<br>centage of data being filtered out (35%). This is most likely<br>due to the fact that most requesters are reluctant to reject HITs<br>regardless of quality, which results in an estimated 95% of<br>workers having an approval rate of 98% or above (Matherly,<br>2019; Wessling et al., 2017).</p>",
            "id": 65,
            "page": 6,
            "text": "11We also ran experiments with even stricter qualification filters (i.e., acceptance rate ≥ 99% and at least 10,000 approved tasks), but this made no notable difference to the percentage of data being filtered out (35%). This is most likely due to the fact that most requesters are reluctant to reject HITs regardless of quality, which results in an estimated 95% of workers having an approval rate of 98% or above (Matherly, 2019; Wessling , 2017)."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 3098
                },
                {
                    "x": 1213,
                    "y": 3098
                },
                {
                    "x": 1213,
                    "y": 3225
                },
                {
                    "x": 288,
                    "y": 3225
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='66' style='font-size:14px'>12There was no significant difference between grammar<br>ratings collected from raters from non-English speaking coun-<br>tries and ratings collected on Day 2.</p>",
            "id": 66,
            "page": 6,
            "text": "12There was no significant difference between grammar ratings collected from raters from non-English speaking countries and ratings collected on Day 2."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 300
                },
                {
                    "x": 2196,
                    "y": 300
                },
                {
                    "x": 2196,
                    "y": 975
                },
                {
                    "x": 1267,
                    "y": 975
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='67' style='font-size:16px'>two different conditions: (1) HITs contain a prompt<br>and a GPT-2 generated text, and (2) HITs contain a<br>prompt and both a human-written reference story<br>as well as a GPT-2 generated story. In the latter<br>case, we ask AMT workers to rate both texts on<br>each of the four properties. Overall, we observe<br>that workers cannot effectively distinguish between<br>reference and model-generated stories when they<br>are evaluated separately (in terms of average rat-<br>ings), but that this distinction emerges clearly when<br>they are presented with both types of stories in the<br>same HIT.</p>",
            "id": 67,
            "page": 6,
            "text": "two different conditions: (1) HITs contain a prompt and a GPT-2 generated text, and (2) HITs contain a prompt and both a human-written reference story as well as a GPT-2 generated story. In the latter case, we ask AMT workers to rate both texts on each of the four properties. Overall, we observe that workers cannot effectively distinguish between reference and model-generated stories when they are evaluated separately (in terms of average ratings), but that this distinction emerges clearly when they are presented with both types of stories in the same HIT."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 1025
                },
                {
                    "x": 2197,
                    "y": 1025
                },
                {
                    "x": 2197,
                    "y": 1759
                },
                {
                    "x": 1268,
                    "y": 1759
                }
            ],
            "category": "paragraph",
            "html": "<p id='68' style='font-size:18px'>When presented only GPT-2 generated text,<br>AMT worker ratings rate them similarly to ref-<br>erence texts, despite obviously worse quality:<br>In our first experiment, we follow the protocol<br>from our experiments with human-written refer-<br>ence stories, showing AMT workers a prompt and<br>a model-generated story and asking them to rate it<br>on the same attributes (grammar, coherence, rele-<br>vance, and likability). The results of this evaluation<br>are presented in the upper row of Table 2 along<br>with the three sets of ratings of reference stories<br>obtained with the same \"all qualifications\" setting<br>from before (Days 1-3 in Table 1).</p>",
            "id": 68,
            "page": 6,
            "text": "When presented only GPT-2 generated text, AMT worker ratings rate them similarly to reference texts, despite obviously worse quality: In our first experiment, we follow the protocol from our experiments with human-written reference stories, showing AMT workers a prompt and a model-generated story and asking them to rate it on the same attributes (grammar, coherence, relevance, and likability). The results of this evaluation are presented in the upper row of Table 2 along with the three sets of ratings of reference stories obtained with the same \"all qualifications\" setting from before (Days 1-3 in Table 1)."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 1768
                },
                {
                    "x": 2198,
                    "y": 1768
                },
                {
                    "x": 2198,
                    "y": 2616
                },
                {
                    "x": 1268,
                    "y": 2616
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='69' style='font-size:20px'>Surprisingly, GPT-2 output is not consistently<br>rated significantly lower than human-written text.<br>For instance, workers in Day 2 rated human-written<br>stories similarly to the GPT-2 generated stories in<br>terms of grammar (M=3.86 VS. M=3.94) and co-<br>herence (M=3.92 VS. M=3.82), while workers in<br>Day 3 rated human-written stories as similarly rel-<br>evant to the prompt as GPT-2 output (M=3.46 VS.<br>M=3.44). Depending on which reference day we<br>compare the GPT-2 output to, GPT-2 is rated simi-<br>larly to human-written stories in terms of all four<br>properties, which indicates that this evaluation is<br>uninformative; nevertheless, the majority of sur-<br>veyed papers use exactly this task design to obtain<br>ratings for model-generated output.</p>",
            "id": 69,
            "page": 6,
            "text": "Surprisingly, GPT-2 output is not consistently rated significantly lower than human-written text. For instance, workers in Day 2 rated human-written stories similarly to the GPT-2 generated stories in terms of grammar (M=3.86 VS. M=3.94) and coherence (M=3.92 VS. M=3.82), while workers in Day 3 rated human-written stories as similarly relevant to the prompt as GPT-2 output (M=3.46 VS. M=3.44). Depending on which reference day we compare the GPT-2 output to, GPT-2 is rated similarly to human-written stories in terms of all four properties, which indicates that this evaluation is uninformative; nevertheless, the majority of surveyed papers use exactly this task design to obtain ratings for model-generated output."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 2665
                },
                {
                    "x": 2198,
                    "y": 2665
                },
                {
                    "x": 2198,
                    "y": 3230
                },
                {
                    "x": 1268,
                    "y": 3230
                }
            ],
            "category": "paragraph",
            "html": "<p id='70' style='font-size:18px'>Asking workers to rate both human-written<br>and model-generated stories side-by-side im-<br>proves ratings: We hypothesize that the previ-<br>ous result is due to scale calibration differences be-<br>tween the two settings: when repeatedly confronted<br>with incoherent model-generated text, a worker<br>may be more generous with their ratings compared<br>to if they only see coherent human-written text.<br>Thus, we explore whether their ratings can be bet-<br>ter calibrated by asking them to rate both types of</p>",
            "id": 70,
            "page": 6,
            "text": "Asking workers to rate both human-written and model-generated stories side-by-side improves ratings: We hypothesize that the previous result is due to scale calibration differences between the two settings: when repeatedly confronted with incoherent model-generated text, a worker may be more generous with their ratings compared to if they only see coherent human-written text. Thus, we explore whether their ratings can be better calibrated by asking them to rate both types of"
        },
        {
            "bounding_box": [
                {
                    "x": 322,
                    "y": 286
                },
                {
                    "x": 2158,
                    "y": 286
                },
                {
                    "x": 2158,
                    "y": 1000
                },
                {
                    "x": 322,
                    "y": 1000
                }
            ],
            "category": "table",
            "html": "<table id='71' style='font-size:16px'><tr><td rowspan=\"2\">Raters</td><td rowspan=\"2\">Type of text</td><td colspan=\"2\">Grammar</td><td colspan=\"2\">Coherence</td><td colspan=\"2\">Relevance</td><td colspan=\"2\">Likability</td></tr><tr><td>MeanSTD</td><td>IAA%</td><td>MeansTD</td><td>IAA%</td><td>MeansTD</td><td>IAA%</td><td>MeanSTD</td><td>IAA%</td></tr><tr><td colspan=\"10\">AMT workers fail to effectively distinguish between human written and GPT-2 generated stories</td></tr><tr><td>AMT</td><td>Ref. (Day 1)</td><td>4.000.92</td><td>0.2115.5</td><td>4.110.96</td><td>0.1416.5</td><td>3.711.26</td><td>0.2710</td><td>3.371.18</td><td>0.117.5</td></tr><tr><td>AMT</td><td>Ref. (Day 2)</td><td>3.860.92</td><td>-0.0310.5</td><td>3.920.98</td><td>-0.036.5</td><td>3.711.08</td><td>0.0211</td><td>3.730.97</td><td>-0.048.5</td></tr><tr><td>AMT</td><td>Ref. (Day 3)</td><td>3.980.96</td><td>0.1811</td><td>4.050.94</td><td>0.1310.5</td><td>3.461.29</td><td>0.26g</td><td>3.421.16</td><td>0.074.5</td></tr><tr><td>AMT</td><td>GPT-2</td><td>3.940.93</td><td>0.1117.5</td><td>3.821.12</td><td>0.057.5</td><td>3.441.41</td><td>0.107</td><td>3.421.25</td><td>0.024.5</td></tr><tr><td colspan=\"10\">AMT workers score GPT-2 lower when also presented with reference text</td></tr><tr><td>AMT</td><td>Reference</td><td>3.830.99</td><td>0.1312.5</td><td>3.831.1</td><td>0.078</td><td>3.491.26</td><td>0.20g</td><td>3.481.08</td><td>0.036.5</td></tr><tr><td>AMT</td><td>GPT-2</td><td>3.820.90</td><td>0.1012</td><td>3.391.1</td><td>0.049.5</td><td>2.701.26</td><td>0.066.5</td><td>2.991.14</td><td>-0.044</td></tr><tr><td colspan=\"10\">Teachers rate GPT-2 generated stories lower than AMT workers</td></tr><tr><td>Teachers</td><td>Reference</td><td>4.500.83</td><td>0.1935.5</td><td>4.380.91</td><td>0.1425</td><td>3.821.38</td><td>0.2516</td><td>3.691.30</td><td>-0.015</td></tr><tr><td>Teachers</td><td>GPT-2</td><td>4.560.62</td><td>0.0024.5</td><td>3.731.19</td><td>0.1713</td><td>2.541.49</td><td>0.5425.5</td><td>2.961.46</td><td>-0.073</td></tr></table>",
            "id": 71,
            "page": 7,
            "text": "Raters Type of text Grammar Coherence Relevance Likability  MeanSTD IAA% MeansTD IAA% MeansTD IAA% MeanSTD IAA%  AMT workers fail to effectively distinguish between human written and GPT-2 generated stories  AMT Ref. (Day 1) 4.000.92 0.2115.5 4.110.96 0.1416.5 3.711.26 0.2710 3.371.18 0.117.5  AMT Ref. (Day 2) 3.860.92 -0.0310.5 3.920.98 -0.036.5 3.711.08 0.0211 3.730.97 -0.048.5  AMT Ref. (Day 3) 3.980.96 0.1811 4.050.94 0.1310.5 3.461.29 0.26g 3.421.16 0.074.5  AMT GPT-2 3.940.93 0.1117.5 3.821.12 0.057.5 3.441.41 0.107 3.421.25 0.024.5  AMT workers score GPT-2 lower when also presented with reference text  AMT Reference 3.830.99 0.1312.5 3.831.1 0.078 3.491.26 0.20g 3.481.08 0.036.5  AMT GPT-2 3.820.90 0.1012 3.391.1 0.049.5 2.701.26 0.066.5 2.991.14 -0.044  Teachers rate GPT-2 generated stories lower than AMT workers  Teachers Reference 4.500.83 0.1935.5 4.380.91 0.1425 3.821.38 0.2516 3.691.30 -0.015  Teachers GPT-2 4.560.62 0.0024.5 3.731.19 0.1713 2.541.49 0.5425.5 2.961.46"
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 1024
                },
                {
                    "x": 2198,
                    "y": 1024
                },
                {
                    "x": 2198,
                    "y": 1231
                },
                {
                    "x": 286,
                    "y": 1231
                }
            ],
            "category": "caption",
            "html": "<caption id='72' style='font-size:14px'>Table 2: Comparison of AMT workers and expert teachers on both human and machine-generated text. Inter-<br>annotator agreement (IAA) between the three raters is measured with Krippendorff's a as well as the percentage<br>of stories for which all three raters exactly agreed on a rating (the latter is subscripted). Statistical significance for<br>the relations between groups is provided in Appendix C and Appendix D.</caption>",
            "id": 72,
            "page": 7,
            "text": "Table 2: Comparison of AMT workers and expert teachers on both human and machine-generated text. Interannotator agreement (IAA) between the three raters is measured with Krippendorff's a as well as the percentage of stories for which all three raters exactly agreed on a rating (the latter is subscripted). Statistical significance for the relations between groups is provided in Appendix C and Appendix D."
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 1318
                },
                {
                    "x": 1214,
                    "y": 1318
                },
                {
                    "x": 1214,
                    "y": 1995
                },
                {
                    "x": 286,
                    "y": 1995
                }
            ],
            "category": "paragraph",
            "html": "<p id='73' style='font-size:18px'>stories side-by-side, using the same qualification<br>settings as for the other experiments. The results<br>of this experiment are presented in the middle row<br>of Table 2. Workers score GPT-2 generated stories<br>significantly lower than reference stories on coher-<br>ence (M=3.39 VS. M=3.83), relevance (M=2.70<br>VS. M=3.49), and likability (M=2.99 VS. M=3.48),<br>which is in line with our expectations. Their rat-<br>ings for grammar (M=3.82 VS. M=3.83) are similar<br>for both types of text, which we also observe with<br>expert teacher ratings in Section 4 and is expected<br>since GPT-2's output is generally grammatical.</p>",
            "id": 73,
            "page": 7,
            "text": "stories side-by-side, using the same qualification settings as for the other experiments. The results of this experiment are presented in the middle row of Table 2. Workers score GPT-2 generated stories significantly lower than reference stories on coherence (M=3.39 VS. M=3.83), relevance (M=2.70 VS. M=3.49), and likability (M=2.99 VS. M=3.48), which is in line with our expectations. Their ratings for grammar (M=3.82 VS. M=3.83) are similar for both types of text, which we also observe with expert teacher ratings in Section 4 and is expected since GPT-2's output is generally grammatical."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 2033
                },
                {
                    "x": 1011,
                    "y": 2033
                },
                {
                    "x": 1011,
                    "y": 2092
                },
                {
                    "x": 289,
                    "y": 2092
                }
            ],
            "category": "paragraph",
            "html": "<p id='74' style='font-size:22px'>4 Evaluation by expert teachers</p>",
            "id": 74,
            "page": 7,
            "text": "4 Evaluation by expert teachers"
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 2121
                },
                {
                    "x": 1214,
                    "y": 2121
                },
                {
                    "x": 1214,
                    "y": 3028
                },
                {
                    "x": 286,
                    "y": 3028
                }
            ],
            "category": "paragraph",
            "html": "<p id='75' style='font-size:20px'>The experiments in the previous section demon-<br>strate the unreliability of AMT ratings for open-<br>ended text generation, even when qualifications are<br>used to restrict the task to ostensibly reliable work-<br>ers. In this section, we compare the ratings pro-<br>duced by AMT workers to those of expert raters,<br>specifically a set of three English teachers, and<br>discover significant deviations between the two<br>groups. Though they rated both types of stories<br>separately, their ratings clearly distinguish between<br>human-written references and machine-generated<br>stories. We also conducted post-task interviews<br>with the teachers and organized a mediation session<br>to discuss stories with high disagreement, observ-<br>ing that they reach consensus after discussion in<br>about 80% of cases.</p>",
            "id": 75,
            "page": 7,
            "text": "The experiments in the previous section demonstrate the unreliability of AMT ratings for openended text generation, even when qualifications are used to restrict the task to ostensibly reliable workers. In this section, we compare the ratings produced by AMT workers to those of expert raters, specifically a set of three English teachers, and discover significant deviations between the two groups. Though they rated both types of stories separately, their ratings clearly distinguish between human-written references and machine-generated stories. We also conducted post-task interviews with the teachers and organized a mediation session to discuss stories with high disagreement, observing that they reach consensus after discussion in about 80% of cases."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 3059
                },
                {
                    "x": 1213,
                    "y": 3059
                },
                {
                    "x": 1213,
                    "y": 3230
                },
                {
                    "x": 288,
                    "y": 3230
                }
            ],
            "category": "paragraph",
            "html": "<p id='76' style='font-size:20px'>Recruiting English teachers: We choose En-<br>glish teachers as experts for our story generation<br>task because they regularly evaluate student-written</p>",
            "id": 76,
            "page": 7,
            "text": "Recruiting English teachers: We choose English teachers as experts for our story generation task because they regularly evaluate student-written"
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 1319
                },
                {
                    "x": 2197,
                    "y": 1319
                },
                {
                    "x": 2197,
                    "y": 1992
                },
                {
                    "x": 1267,
                    "y": 1992
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='77' style='font-size:16px'>papers and are experienced at detecting both low-<br>level grammatical mistakes as well as discourse-<br>level issues with logical coherence. The three<br>teachers were recruited from the authors' personal<br>networks, and each of them either has a degree<br>in teaching English as a Second Language or a<br>CELTA certificate. 13 They were paid $125 each for<br>participating in our experiments, which required<br>them to rate the same 200 human-written stories<br>and 200 GPT-2 generated stories on the same four<br>properties as that of the AMT workers, given an<br>identical task interface. 14</p>",
            "id": 77,
            "page": 7,
            "text": "papers and are experienced at detecting both lowlevel grammatical mistakes as well as discourselevel issues with logical coherence. The three teachers were recruited from the authors' personal networks, and each of them either has a degree in teaching English as a Second Language or a CELTA certificate. 13 They were paid $125 each for participating in our experiments, which required them to rate the same 200 human-written stories and 200 GPT-2 generated stories on the same four properties as that of the AMT workers, given an identical task interface. 14"
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 2023
                },
                {
                    "x": 2197,
                    "y": 2023
                },
                {
                    "x": 2197,
                    "y": 2640
                },
                {
                    "x": 1267,
                    "y": 2640
                }
            ],
            "category": "paragraph",
            "html": "<p id='78' style='font-size:18px'>Unlike AMT workers, teachers rate reference<br>stories higher than GPT-2 generated ones: We<br>asked teachers to first rate the 200 reference stories,<br>and then a week later to rate the GPT-2 generated<br>stories. Just like the AMT workers, they were not<br>told that the text in the second task was machine-<br>generated. Importantly, we used the same set of<br>teachers for both tasks, SO they already had sig-<br>nificant experience with the task when rating the<br>machine-generated text (as opposed to using new<br>AMT workers for each experiment).</p>",
            "id": 78,
            "page": 7,
            "text": "Unlike AMT workers, teachers rate reference stories higher than GPT-2 generated ones: We asked teachers to first rate the 200 reference stories, and then a week later to rate the GPT-2 generated stories. Just like the AMT workers, they were not told that the text in the second task was machinegenerated. Importantly, we used the same set of teachers for both tasks, SO they already had significant experience with the task when rating the machine-generated text (as opposed to using new AMT workers for each experiment)."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 2647
                },
                {
                    "x": 2198,
                    "y": 2647
                },
                {
                    "x": 2198,
                    "y": 2981
                },
                {
                    "x": 1267,
                    "y": 2981
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='79' style='font-size:18px'>The results of this evaluation are presented in<br>the last row of Table 2. Unsurprisingly, teach-<br>ers rated human-written stories significantly higher<br>than GPT-2 generated stories in terms of coher-<br>ence (M=4.38 VS. M=3.73), relevance (M=3.82 VS.<br>M=2.54), and likability (M=3.69 VS. M=2.96) (all</p>",
            "id": 79,
            "page": 7,
            "text": "The results of this evaluation are presented in the last row of Table 2. Unsurprisingly, teachers rated human-written stories significantly higher than GPT-2 generated stories in terms of coherence (M=4.38 VS. M=3.73), relevance (M=3.82 VS. M=2.54), and likability (M=3.69 VS. M=2.96) (all"
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 3008
                },
                {
                    "x": 2192,
                    "y": 3008
                },
                {
                    "x": 2192,
                    "y": 3095
                },
                {
                    "x": 1270,
                    "y": 3095
                }
            ],
            "category": "paragraph",
            "html": "<p id='80' style='font-size:14px'>13Certificate in Teaching English to Speakers of Other Lan-<br>guages.</p>",
            "id": 80,
            "page": 7,
            "text": "13Certificate in Teaching English to Speakers of Other Languages."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 3100
                },
                {
                    "x": 2194,
                    "y": 3100
                },
                {
                    "x": 2194,
                    "y": 3221
                },
                {
                    "x": 1271,
                    "y": 3221
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='81' style='font-size:14px'>14The teachers completed their ratings using an identical<br>version of our task deployed on the AMT sandbox environ-<br>ment.</p>",
            "id": 81,
            "page": 7,
            "text": "14The teachers completed their ratings using an identical version of our task deployed on the AMT sandbox environment."
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 300
                },
                {
                    "x": 1215,
                    "y": 300
                },
                {
                    "x": 1215,
                    "y": 1031
                },
                {
                    "x": 286,
                    "y": 1031
                }
            ],
            "category": "paragraph",
            "html": "<p id='82' style='font-size:20px'>p's<0.001). On the other hand, they rated human-<br>written stories and GPT-2 generated stories as sim-<br>ilar in terms of grammar (M=4.50 VS. M=4.56).<br>Moreover, teachers' ratings of human-written sto-<br>ries are considerably higher than AMT ratings for<br>all attributes except likability (M=3.69) which de-<br>pending on the day was rated lower (MDay1=3.37)<br>or higher (MDay2=3.73) by the AMT workers. Sim-<br>ilarly, teachers' ratings of GPT-2 stories are lower<br>than the ratings we obtained from AMT workers<br>for coherence (M=3.73 VS. M=4. 11), relevance<br>(M=2.54 VS. M=3.71), and likability (M=2.96 VS.<br>M=3.37).</p>",
            "id": 82,
            "page": 8,
            "text": "p's<0.001). On the other hand, they rated humanwritten stories and GPT-2 generated stories as similar in terms of grammar (M=4.50 VS. M=4.56). Moreover, teachers' ratings of human-written stories are considerably higher than AMT ratings for all attributes except likability (M=3.69) which depending on the day was rated lower (MDay1=3.37) or higher (MDay2=3.73) by the AMT workers. Similarly, teachers' ratings of GPT-2 stories are lower than the ratings we obtained from AMT workers for coherence (M=3.73 VS. M=4. 11), relevance (M=2.54 VS. M=3.71), and likability (M=2.96 VS. M=3.37)."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 1072
                },
                {
                    "x": 1215,
                    "y": 1072
                },
                {
                    "x": 1215,
                    "y": 1525
                },
                {
                    "x": 288,
                    "y": 1525
                }
            ],
            "category": "paragraph",
            "html": "<p id='83' style='font-size:18px'>Teachers need to see many examples to prop-<br>erly calibrate their ratings: In post-task inter-<br>views, all teachers reported that it took them 10-20<br>stories on average to calibrate their ratings. Since<br>most AMT workers complete only one to two HITs,<br>they do not have similar time to get acquainted with<br>the task; this may suggest that having a pre-task<br>training phase can improve worker calibration.</p>",
            "id": 83,
            "page": 8,
            "text": "Teachers need to see many examples to properly calibrate their ratings: In post-task interviews, all teachers reported that it took them 10-20 stories on average to calibrate their ratings. Since most AMT workers complete only one to two HITs, they do not have similar time to get acquainted with the task; this may suggest that having a pre-task training phase can improve worker calibration."
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 1561
                },
                {
                    "x": 1214,
                    "y": 1561
                },
                {
                    "x": 1214,
                    "y": 2413
                },
                {
                    "x": 286,
                    "y": 2413
                }
            ],
            "category": "paragraph",
            "html": "<p id='84' style='font-size:18px'>Coherence is difficult to rate for machine-<br>generated text: The teachers unanimously re-<br>port that while coherence is easy to rate for ref-<br>erence stories (since most of them are largely CO-<br>herent), it is the most difficult property to rate for<br>GPT-2 generated stories. Since they did not know<br>that they were rating machine-generated text, they<br>spent time trying to make sense of the author's<br>possible intent in producing many of the strange<br>artifacts and hallucinations common to output of<br>neural language models (Holtzman et al., 2019). In<br>contrast, relevance turned out to be the easiest prop-<br>erty of machine-generated text for teachers to rate,<br>which is expected as many of GPT-2's stories devi-<br>ate very quickly from the prompt (see Figure 1).</p>",
            "id": 84,
            "page": 8,
            "text": "Coherence is difficult to rate for machinegenerated text: The teachers unanimously report that while coherence is easy to rate for reference stories (since most of them are largely COherent), it is the most difficult property to rate for GPT-2 generated stories. Since they did not know that they were rating machine-generated text, they spent time trying to make sense of the author's possible intent in producing many of the strange artifacts and hallucinations common to output of neural language models (Holtzman , 2019). In contrast, relevance turned out to be the easiest property of machine-generated text for teachers to rate, which is expected as many of GPT-2's stories deviate very quickly from the prompt (see Figure 1)."
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 2447
                },
                {
                    "x": 1213,
                    "y": 2447
                },
                {
                    "x": 1213,
                    "y": 3016
                },
                {
                    "x": 286,
                    "y": 3016
                }
            ],
            "category": "paragraph",
            "html": "<p id='85' style='font-size:18px'>GPT-2 generated stories are much harder for<br>teachers to rate overall: All teachers reported<br>struggling more when rating GPT-2 stories, a fact<br>reflected in their average rating time per story in-<br>creasing significantly from 69.8 seconds to 87.3<br>seconds (p<0.05). In contrast, the average rating<br>time of AMT workers decreased from 135.3 sec-<br>onds for human-written text (Day 1) to 91.5 sec-<br>onds for GPT-2 text (p<0.05)15. Teachers also re-<br>ported having to recalibrate their scale when rating</p>",
            "id": 85,
            "page": 8,
            "text": "GPT-2 generated stories are much harder for teachers to rate overall: All teachers reported struggling more when rating GPT-2 stories, a fact reflected in their average rating time per story increasing significantly from 69.8 seconds to 87.3 seconds (p<0.05). In contrast, the average rating time of AMT workers decreased from 135.3 seconds for human-written text (Day 1) to 91.5 seconds for GPT-2 text (p<0.05)15. Teachers also reported having to recalibrate their scale when rating"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 3051
                },
                {
                    "x": 1212,
                    "y": 3051
                },
                {
                    "x": 1212,
                    "y": 3225
                },
                {
                    "x": 288,
                    "y": 3225
                }
            ],
            "category": "paragraph",
            "html": "<p id='86' style='font-size:16px'>15This time was computed by the researchers to account for<br>workers accepting multiple HITs at the same time, however,<br>the WorkTimeInSeconds reported in the AMT results shows<br>similar trends.</p>",
            "id": 86,
            "page": 8,
            "text": "15This time was computed by the researchers to account for workers accepting multiple HITs at the same time, however, the WorkTimeInSeconds reported in the AMT results shows similar trends."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 301
                },
                {
                    "x": 2197,
                    "y": 301
                },
                {
                    "x": 2197,
                    "y": 920
                },
                {
                    "x": 1267,
                    "y": 920
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='87' style='font-size:18px'>the GPT-2 generated stories, as the stories were sig-<br>nificantly worse than the human-written text. Con-<br>sequently, they suggested that it would be easier<br>to calibrate their scale had the GPT-2 output been<br>presented beside the human-written text, which<br>supports the results from our joint rating task with<br>AMT workers. Finally, the teachers suggested that<br>creating a standardized rubric would greatly facil-<br>itate the rating process. This step is even more<br>important as machine-generated text faces different<br>issues than human-written text.</p>",
            "id": 87,
            "page": 8,
            "text": "the GPT-2 generated stories, as the stories were significantly worse than the human-written text. Consequently, they suggested that it would be easier to calibrate their scale had the GPT-2 output been presented beside the human-written text, which supports the results from our joint rating task with AMT workers. Finally, the teachers suggested that creating a standardized rubric would greatly facilitate the rating process. This step is even more important as machine-generated text faces different issues than human-written text."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 957
                },
                {
                    "x": 2197,
                    "y": 957
                },
                {
                    "x": 2197,
                    "y": 2372
                },
                {
                    "x": 1267,
                    "y": 2372
                }
            ],
            "category": "paragraph",
            "html": "<p id='88' style='font-size:18px'>Resolving teacher disagreement: One advan-<br>tage of using human expert raters is that we can<br>easily have them discuss examples on which they<br>disagree. We arranged a mediation meeting be-<br>tween two of the three teachers to discuss 60 stories<br>on which they showed the highest disagreement (3<br>attributes x 10 stories x 2 types, we excluded lika-<br>bility due to its subjective nature). In this meeting,<br>they were first asked to rate the stories again, with-<br>out being provided their previous rating. In about<br>20% of cases, one of the teachers disagreed with<br>their own previous rating due to honest lapses of<br>judgment. Another common reason for disagree-<br>ment was missing world knowledge (see Figure 1,<br>right). One more reason for disagreement, a confu-<br>sion about how to rate slang in terms of grammati-<br>cality. While the text was not correct in the view<br>of the official grammar, it was appropriate for the<br>prompt, SO one teacher rated it high while the other<br>rated it low. Overall, after discussing examples that<br>they still disagreed on after re-rating, teachers were<br>able to come to a consensus on 80% of the stories;<br>the remaining disagreements persisted due to indi-<br>vidual differences in strictness. See Appendix C<br>for details on the mediation meeting.</p>",
            "id": 88,
            "page": 8,
            "text": "Resolving teacher disagreement: One advantage of using human expert raters is that we can easily have them discuss examples on which they disagree. We arranged a mediation meeting between two of the three teachers to discuss 60 stories on which they showed the highest disagreement (3 attributes x 10 stories x 2 types, we excluded likability due to its subjective nature). In this meeting, they were first asked to rate the stories again, without being provided their previous rating. In about 20% of cases, one of the teachers disagreed with their own previous rating due to honest lapses of judgment. Another common reason for disagreement was missing world knowledge (see Figure 1, right). One more reason for disagreement, a confusion about how to rate slang in terms of grammaticality. While the text was not correct in the view of the official grammar, it was appropriate for the prompt, SO one teacher rated it high while the other rated it low. Overall, after discussing examples that they still disagreed on after re-rating, teachers were able to come to a consensus on 80% of the stories; the remaining disagreements persisted due to individual differences in strictness. See Appendix C for details on the mediation meeting."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 2403
                },
                {
                    "x": 2197,
                    "y": 2403
                },
                {
                    "x": 2197,
                    "y": 3141
                },
                {
                    "x": 1268,
                    "y": 3141
                }
            ],
            "category": "paragraph",
            "html": "<p id='89' style='font-size:18px'>Replicating the study on Upwork: We recog-<br>nize that replicating our study is difficult without<br>access to a network of English teachers. As such,<br>we performed the same experiment using three cer-<br>tified teachers recruited on a freelance platform,<br>Upwork. 16 The teachers were paid $175 for evalu-<br>ating the same 200 human-written and 200 GPT-2<br>generated stories using the exact same setup as in<br>subsection 3.1. It took approximately one week<br>to collect the data (including break between rating<br>human-written and GPT-2 generated stories). The<br>results obtained via Upwork were comparable with<br>the results obtained from the English teachers de-</p>",
            "id": 89,
            "page": 8,
            "text": "Replicating the study on Upwork: We recognize that replicating our study is difficult without access to a network of English teachers. As such, we performed the same experiment using three certified teachers recruited on a freelance platform, Upwork. 16 The teachers were paid $175 for evaluating the same 200 human-written and 200 GPT-2 generated stories using the exact same setup as in subsection 3.1. It took approximately one week to collect the data (including break between rating human-written and GPT-2 generated stories). The results obtained via Upwork were comparable with the results obtained from the English teachers de-"
        },
        {
            "bounding_box": [
                {
                    "x": 1310,
                    "y": 3181
                },
                {
                    "x": 1841,
                    "y": 3181
                },
                {
                    "x": 1841,
                    "y": 3223
                },
                {
                    "x": 1310,
                    "y": 3223
                }
            ],
            "category": "footer",
            "html": "<footer id='90' style='font-size:14px'>16https : / / www · upwork. com</footer>",
            "id": 90,
            "page": 8,
            "text": "16https : / / www · upwork. com"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 303
                },
                {
                    "x": 1213,
                    "y": 303
                },
                {
                    "x": 1213,
                    "y": 696
                },
                {
                    "x": 287,
                    "y": 696
                }
            ],
            "category": "paragraph",
            "html": "<p id='91' style='font-size:16px'>scribed in this section, i.e. the Upwork teachers<br>rated human-written stories higher for coherence,<br>relevance, and likability than the GPT-2 generated<br>stories (all p's<0.001). Interestingly, their IAA was<br>higher than the English teachers recruited from the<br>authors' personal networks. The details of this<br>experiment are provided in the Appendix B.</p>",
            "id": 91,
            "page": 9,
            "text": "scribed in this section, i.e. the Upwork teachers rated human-written stories higher for coherence, relevance, and likability than the GPT-2 generated stories (all p's<0.001). Interestingly, their IAA was higher than the English teachers recruited from the authors' personal networks. The details of this experiment are provided in the Appendix B."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 757
                },
                {
                    "x": 671,
                    "y": 757
                },
                {
                    "x": 671,
                    "y": 812
                },
                {
                    "x": 289,
                    "y": 812
                }
            ],
            "category": "paragraph",
            "html": "<p id='92' style='font-size:20px'>5 Related Work</p>",
            "id": 92,
            "page": 9,
            "text": "5 Related Work"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 863
                },
                {
                    "x": 1214,
                    "y": 863
                },
                {
                    "x": 1214,
                    "y": 1028
                },
                {
                    "x": 288,
                    "y": 1028
                }
            ],
            "category": "paragraph",
            "html": "<p id='93' style='font-size:14px'>Our work is related to previous studies of human<br>evaluation of text quality as well as collecting judg-<br>ments using Amazon Mechanical Turk.</p>",
            "id": 93,
            "page": 9,
            "text": "Our work is related to previous studies of human evaluation of text quality as well as collecting judgments using Amazon Mechanical Turk."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 1083
                },
                {
                    "x": 1213,
                    "y": 1083
                },
                {
                    "x": 1213,
                    "y": 1588
                },
                {
                    "x": 288,
                    "y": 1588
                }
            ],
            "category": "paragraph",
            "html": "<p id='94' style='font-size:16px'>Human evaluation of text quality: Most previ-<br>ous studies on human evaluation concentrate on<br>constrained generation domains, such as machine<br>translation (Guzman et al., 2015; Graham et al.,<br>2017; Toral et al., 2018; Castilho, 2021) or sum-<br>marization (Gillick and Liu, 2010; Iskender et al.,<br>2020). Other studies evaluate very short, often one<br>sentence long, outputs (Grundkiewicz et al., 2015;<br>Mori et al., 2019; Khashabi et al., 2021).</p>",
            "id": 94,
            "page": 9,
            "text": "Human evaluation of text quality: Most previous studies on human evaluation concentrate on constrained generation domains, such as machine translation (Guzman , 2015; Graham , 2017; Toral , 2018; Castilho, 2021) or summarization (Gillick and Liu, 2010; Iskender , 2020). Other studies evaluate very short, often one sentence long, outputs (Grundkiewicz , 2015; Mori , 2019; Khashabi , 2021)."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 1599
                },
                {
                    "x": 1213,
                    "y": 1599
                },
                {
                    "x": 1213,
                    "y": 2215
                },
                {
                    "x": 287,
                    "y": 2215
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='95' style='font-size:16px'>Even professional translators struggle when eval-<br>uating longer machine translated texts (Castilho,<br>2021). Creative texts, such as stories, are less con-<br>strained than translated texts, but researchers con-<br>tinue to employ crowd workers to evaluate creative<br>texts, often without evaluating reference texts (see<br>Section 2). Previous studies have asked workers to<br>choose from (Mori et al., 2019) or distinguish be-<br>tween human-written and machine-generated texts<br>(Garbacea et al., 2019; Ippolito et al., 2020; Clark<br>et al., 2021).</p>",
            "id": 95,
            "page": 9,
            "text": "Even professional translators struggle when evaluating longer machine translated texts (Castilho, 2021). Creative texts, such as stories, are less constrained than translated texts, but researchers continue to employ crowd workers to evaluate creative texts, often without evaluating reference texts (see Section 2). Previous studies have asked workers to choose from (Mori , 2019) or distinguish between human-written and machine-generated texts (Garbacea , 2019; Ippolito , 2020; Clark , 2021)."
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 2267
                },
                {
                    "x": 1213,
                    "y": 2267
                },
                {
                    "x": 1213,
                    "y": 3231
                },
                {
                    "x": 286,
                    "y": 3231
                }
            ],
            "category": "paragraph",
            "html": "<p id='96' style='font-size:16px'>Data collection using AMT: Many previous<br>works raise concerns about the reliability of data<br>collected on AMT (Necka et al., 2016; Matherly,<br>2019; Ahler et al., 2020). Reluctance of requesters<br>to reject HITs leads to positive bias in workers'<br>qualifications (Matherly, 2019). Furthermore, a<br>large number of responses are provided by small<br>number of productive workers (Fort et al., 2011;<br>Robinson et al., 2019). Researchers also report an<br>increasing number of workers use VPNs to mask<br>their location (Bauer et al., 2020) and contribute<br>lower-quality data (Moss and Litman; Ahler et al.,<br>2020). Hence, simple quality control measures,<br>such as approval rate or the country of residence<br>as suggested in (Berinsky et al., 2012), may not<br>be sufficient to effectively filter workers who are<br>spamming a task.</p>",
            "id": 96,
            "page": 9,
            "text": "Data collection using AMT: Many previous works raise concerns about the reliability of data collected on AMT (Necka , 2016; Matherly, 2019; Ahler , 2020). Reluctance of requesters to reject HITs leads to positive bias in workers' qualifications (Matherly, 2019). Furthermore, a large number of responses are provided by small number of productive workers (Fort , 2011; Robinson , 2019). Researchers also report an increasing number of workers use VPNs to mask their location (Bauer , 2020) and contribute lower-quality data (Moss and Litman; Ahler , 2020). Hence, simple quality control measures, such as approval rate or the country of residence as suggested in (Berinsky , 2012), may not be sufficient to effectively filter workers who are spamming a task."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 299
                },
                {
                    "x": 2052,
                    "y": 299
                },
                {
                    "x": 2052,
                    "y": 353
                },
                {
                    "x": 1269,
                    "y": 353
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='97' style='font-size:20px'>6 Recommendations & Conclusion</p>",
            "id": 97,
            "page": 9,
            "text": "6 Recommendations & Conclusion"
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 387
                },
                {
                    "x": 2197,
                    "y": 387
                },
                {
                    "x": 2197,
                    "y": 1970
                },
                {
                    "x": 1267,
                    "y": 1970
                }
            ],
            "category": "paragraph",
            "html": "<p id='98' style='font-size:16px'>Our experiments show that evaluating open-ended<br>generated text is an incredibly challenging task<br>even for expert raters. While AMT is a conve-<br>nient and affordable solution, we observe that high<br>variance between workers, poor calibration, and<br>cognitively-demanding tasks can lead researchers<br>to draw misleading scientific conclusions (e.g., that<br>human-written text is \"worse\" than GPT-2's). Sim-<br>ple fixes such as adding strict worker qualifications<br>do not address the root of the problem. As such,<br>we recommend future AMT evaluations imple-<br>ment additional quality control mechanisms (some<br>of which require custom task setups on external<br>servers) such as (1) filtering workers by observed<br>time spent per HIT rather than WorkTimeInSec-<br>onds, (2) specifying a maximum number of items<br>per worker, (3) employing a pre-task language pro-<br>ficiency test, and (4) providing training HITs to<br>allow workers to calibrate their ratings. Further-<br>more, we show that researchers can improve rating<br>calibration by presenting machine-generated text<br>alongside human reference text. That said, expert<br>raters such as linguists or language teachers should<br>be used whenever possible as they have already<br>been trained to evaluate written text, and it is not<br>much more expensive (it cost us $144 to rate 200<br>stories with AMT VS. $187.50 with English teach-<br>ers VS. $262.5 with Upwork.</p>",
            "id": 98,
            "page": 9,
            "text": "Our experiments show that evaluating open-ended generated text is an incredibly challenging task even for expert raters. While AMT is a convenient and affordable solution, we observe that high variance between workers, poor calibration, and cognitively-demanding tasks can lead researchers to draw misleading scientific conclusions (e.g., that human-written text is \"worse\" than GPT-2's). Simple fixes such as adding strict worker qualifications do not address the root of the problem. As such, we recommend future AMT evaluations implement additional quality control mechanisms (some of which require custom task setups on external servers) such as (1) filtering workers by observed time spent per HIT rather than WorkTimeInSeconds, (2) specifying a maximum number of items per worker, (3) employing a pre-task language proficiency test, and (4) providing training HITs to allow workers to calibrate their ratings. Furthermore, we show that researchers can improve rating calibration by presenting machine-generated text alongside human reference text. That said, expert raters such as linguists or language teachers should be used whenever possible as they have already been trained to evaluate written text, and it is not much more expensive (it cost us $144 to rate 200 stories with AMT VS. $187.50 with English teachers VS. $262.5 with Upwork."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 2010
                },
                {
                    "x": 1840,
                    "y": 2010
                },
                {
                    "x": 1840,
                    "y": 2064
                },
                {
                    "x": 1270,
                    "y": 2064
                }
            ],
            "category": "paragraph",
            "html": "<p id='99' style='font-size:20px'>7 Ethical Considerations</p>",
            "id": 99,
            "page": 9,
            "text": "7 Ethical Considerations"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 2101
                },
                {
                    "x": 2196,
                    "y": 2101
                },
                {
                    "x": 2196,
                    "y": 2831
                },
                {
                    "x": 1268,
                    "y": 2831
                }
            ],
            "category": "paragraph",
            "html": "<p id='100' style='font-size:14px'>As with all research that makes use of human sub-<br>jects, we must carefully reflect on our methodol-<br>ogy to minimize the risk of harm to those we ask<br>to evaluate open-ended texts. Specifically, texts<br>from social media sites like Reddit may contain<br>racist, sexist, and other forms of vulgar content.<br>Additionally, neural language models like GPT-<br>2, which have been trained on open domain text<br>crawled from the web, have been shown to generate<br>similarly offensive content. As such, we advocate<br>adequately warning any humans who take part in<br>open-ended text evaluation of the potential for such<br>harms (as we did in our research).</p>",
            "id": 100,
            "page": 9,
            "text": "As with all research that makes use of human subjects, we must carefully reflect on our methodology to minimize the risk of harm to those we ask to evaluate open-ended texts. Specifically, texts from social media sites like Reddit may contain racist, sexist, and other forms of vulgar content. Additionally, neural language models like GPT2, which have been trained on open domain text crawled from the web, have been shown to generate similarly offensive content. As such, we advocate adequately warning any humans who take part in open-ended text evaluation of the potential for such harms (as we did in our research)."
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 2836
                },
                {
                    "x": 2198,
                    "y": 2836
                },
                {
                    "x": 2198,
                    "y": 3229
                },
                {
                    "x": 1267,
                    "y": 3229
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='101' style='font-size:14px'>Additionally, crowd workers are frequently un-<br>derpaid for their labor, which harms both the qual-<br>ity of the research, and more importantly, the ability<br>of these crowd workers to earn an adequate living.<br>As such, we report our hourly wage for both crowd<br>workers and experts. We ensure that crowd work-<br>ers earn at least $14 per hour by assuming 50-55</p>",
            "id": 101,
            "page": 9,
            "text": "Additionally, crowd workers are frequently underpaid for their labor, which harms both the quality of the research, and more importantly, the ability of these crowd workers to earn an adequate living. As such, we report our hourly wage for both crowd workers and experts. We ensure that crowd workers earn at least $14 per hour by assuming 50-55"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 301
                },
                {
                    "x": 1212,
                    "y": 301
                },
                {
                    "x": 1212,
                    "y": 581
                },
                {
                    "x": 287,
                    "y": 581
                }
            ],
            "category": "paragraph",
            "html": "<p id='102' style='font-size:18px'>seconds per HIT (though on average our crowd<br>workers were paid substantially higher due to the<br>low average time to completion on each HIT). Our<br>experts averaged around $20 per hour (not counting<br>mediation).</p>",
            "id": 102,
            "page": 10,
            "text": "seconds per HIT (though on average our crowd workers were paid substantially higher due to the low average time to completion on each HIT). Our experts averaged around $20 per hour (not counting mediation)."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 626
                },
                {
                    "x": 690,
                    "y": 626
                },
                {
                    "x": 690,
                    "y": 681
                },
                {
                    "x": 289,
                    "y": 681
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='103' style='font-size:22px'>Acknowledgments</p>",
            "id": 103,
            "page": 10,
            "text": "Acknowledgments"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 715
                },
                {
                    "x": 1211,
                    "y": 715
                },
                {
                    "x": 1211,
                    "y": 1336
                },
                {
                    "x": 287,
                    "y": 1336
                }
            ],
            "category": "paragraph",
            "html": "<p id='104' style='font-size:16px'>We thank the reviewers for their insightful com-<br>ments. We would also like to thank the UMass<br>NLP group for the great advice on the draft of<br>this paper. We are grateful to the English teach-<br>ers recruited from Upwork and from the authors'<br>personal network, as well as workers on AMT, for<br>their help in the story evaluation. MK is supported<br>by the Chan Zuckerberg Initiative under the project<br>Scientific Knowledge Base Construction. NA and<br>MI are supported by award IIS-1955567 from the<br>National Science Foundation (NSF).</p>",
            "id": 104,
            "page": 10,
            "text": "We thank the reviewers for their insightful comments. We would also like to thank the UMass NLP group for the great advice on the draft of this paper. We are grateful to the English teachers recruited from Upwork and from the authors' personal network, as well as workers on AMT, for their help in the story evaluation. MK is supported by the Chan Zuckerberg Initiative under the project Scientific Knowledge Base Construction. NA and MI are supported by award IIS-1955567 from the National Science Foundation (NSF)."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 1433
                },
                {
                    "x": 531,
                    "y": 1433
                },
                {
                    "x": 531,
                    "y": 1486
                },
                {
                    "x": 289,
                    "y": 1486
                }
            ],
            "category": "paragraph",
            "html": "<p id='105' style='font-size:20px'>References</p>",
            "id": 105,
            "page": 10,
            "text": "References"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 1513
                },
                {
                    "x": 1213,
                    "y": 1513
                },
                {
                    "x": 1213,
                    "y": 1697
                },
                {
                    "x": 291,
                    "y": 1697
                }
            ],
            "category": "paragraph",
            "html": "<p id='106' style='font-size:14px'>Douglas J. Ahler, Carolyn E. Roush, and Gaurav Sood.<br>2020. Replication data for: \"the micro-task market<br>for lemons: Data quality on Amazon's Mechanical<br>Turk\".</p>",
            "id": 106,
            "page": 10,
            "text": "Douglas J. Ahler, Carolyn E. Roush, and Gaurav Sood. 2020. Replication data for: \"the micro-task market for lemons: Data quality on Amazon's Mechanical Turk\"."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 1737
                },
                {
                    "x": 1212,
                    "y": 1737
                },
                {
                    "x": 1212,
                    "y": 2104
                },
                {
                    "x": 290,
                    "y": 2104
                }
            ],
            "category": "paragraph",
            "html": "<p id='107' style='font-size:16px'>Nader Akoury, Shufan Wang, Josh Whiting, Stephen<br>Hood, Nanyun Peng, and Mohit Iyyer. 2020. STO-<br>RIUM: A Dataset and Evaluation Platform for<br>Machine-in-the-Loop Story Generation. In Proceed-<br>ings of the 2020 Conference on Empirical Methods<br>in Natural Language Processing (EMNLP), pages<br>6470-6484, Online. Association for Computational<br>Linguistics.</p>",
            "id": 107,
            "page": 10,
            "text": "Nader Akoury, Shufan Wang, Josh Whiting, Stephen Hood, Nanyun Peng, and Mohit Iyyer. 2020. STORIUM: A Dataset and Evaluation Platform for Machine-in-the-Loop Story Generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6470-6484, Online. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 2141
                },
                {
                    "x": 1212,
                    "y": 2141
                },
                {
                    "x": 1212,
                    "y": 2420
                },
                {
                    "x": 290,
                    "y": 2420
                }
            ],
            "category": "paragraph",
            "html": "<p id='108' style='font-size:16px'>Milad Alshomary, Shahbaz Syed, Martin Potthast, and<br>Henning Wachsmuth. 2020. Target inference in<br>argument conclusion generation. In Proceedings<br>of the 58th Annual Meeting of the Association for<br>Computational Linguistics, pages 4334-4345, On-<br>line. Association for Computational Linguistics.</p>",
            "id": 108,
            "page": 10,
            "text": "Milad Alshomary, Shahbaz Syed, Martin Potthast, and Henning Wachsmuth. 2020. Target inference in argument conclusion generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4334-4345, Online. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 2455
                },
                {
                    "x": 1213,
                    "y": 2455
                },
                {
                    "x": 1213,
                    "y": 2686
                },
                {
                    "x": 290,
                    "y": 2686
                }
            ],
            "category": "paragraph",
            "html": "<p id='109' style='font-size:14px'>Brian Bauer, Kristy L. Larsen, Nicole Caulfield,<br>Domynic Elder, Sara Jordan, and Daniel Capron.<br>2020. Review of best practice recommendations for<br>ensuring high quality data with amazon's mechani-<br>cal turk.</p>",
            "id": 109,
            "page": 10,
            "text": "Brian Bauer, Kristy L. Larsen, Nicole Caulfield, Domynic Elder, Sara Jordan, and Daniel Capron. 2020. Review of best practice recommendations for ensuring high quality data with amazon's mechanical turk."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 2725
                },
                {
                    "x": 1213,
                    "y": 2725
                },
                {
                    "x": 1213,
                    "y": 2911
                },
                {
                    "x": 290,
                    "y": 2911
                }
            ],
            "category": "paragraph",
            "html": "<p id='110' style='font-size:14px'>Adam J. Berinsky, Gregory A. Huber, and Gabriel S.<br>Lenz. 2012. Evaluating online labor markets for<br>experimental research: Amazon.com's mechanical<br>turk. Political Analysis, 20(3):351-368.</p>",
            "id": 110,
            "page": 10,
            "text": "Adam J. Berinsky, Gregory A. Huber, and Gabriel S. Lenz. 2012. Evaluating online labor markets for experimental research: Amazon.com's mechanical turk. Political Analysis, 20(3):351-368."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 2949
                },
                {
                    "x": 1212,
                    "y": 2949
                },
                {
                    "x": 1212,
                    "y": 3227
                },
                {
                    "x": 291,
                    "y": 3227
                }
            ],
            "category": "paragraph",
            "html": "<p id='111' style='font-size:16px'>Antoine Bosselut, Asli Celikyilmaz, Xiaodong He,<br>Jianfeng Gao, Po-Sen Huang, and Yejin Choi. 2018.<br>Discourse-aware neural rewards for coherent text<br>generation. In Proceedings of the 2018 Conference<br>of the North American Chapter of the Association<br>for Computational Linguistics: Human Language</p>",
            "id": 111,
            "page": 10,
            "text": "Antoine Bosselut, Asli Celikyilmaz, Xiaodong He, Jianfeng Gao, Po-Sen Huang, and Yejin Choi. 2018. Discourse-aware neural rewards for coherent text generation. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language"
        },
        {
            "bounding_box": [
                {
                    "x": 1316,
                    "y": 304
                },
                {
                    "x": 2198,
                    "y": 304
                },
                {
                    "x": 2198,
                    "y": 444
                },
                {
                    "x": 1316,
                    "y": 444
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='112' style='font-size:14px'>Technologies, Volume 1 (Long Papers), pages 173-<br>184, New Orleans, Louisiana. Association for Com-<br>putational Linguistics.</p>",
            "id": 112,
            "page": 10,
            "text": "Technologies, Volume 1 (Long Papers), pages 173184, New Orleans, Louisiana. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 475
                },
                {
                    "x": 2198,
                    "y": 475
                },
                {
                    "x": 2198,
                    "y": 753
                },
                {
                    "x": 1268,
                    "y": 753
                }
            ],
            "category": "paragraph",
            "html": "<p id='113' style='font-size:16px'>Faeze Brahman and Snigdha Chaturvedi. 2020. Mod-<br>eling protagonist emotions for emotion-aware story-<br>telling. In Proceedings of the 2020 Conference on<br>Empirical Methods in Natural Language Process-<br>ing (EMNLP), pages 5277-5294, Online. Associa-<br>tion for Computational Linguistics.</p>",
            "id": 113,
            "page": 10,
            "text": "Faeze Brahman and Snigdha Chaturvedi. 2020. Modeling protagonist emotions for emotion-aware storytelling. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5277-5294, Online. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 784
                },
                {
                    "x": 2192,
                    "y": 784
                },
                {
                    "x": 2192,
                    "y": 924
                },
                {
                    "x": 1269,
                    "y": 924
                }
            ],
            "category": "paragraph",
            "html": "<p id='114' style='font-size:14px'>D. Card, Peter Henderson, Urvashi Khandelwal, Robin<br>Jia, Kyle Mahowald, and Dan Jurafsky. 2020. With<br>little power comes great responsibility. In EMNLP.</p>",
            "id": 114,
            "page": 10,
            "text": "D. Card, Peter Henderson, Urvashi Khandelwal, Robin Jia, Kyle Mahowald, and Dan Jurafsky. 2020. With little power comes great responsibility. In EMNLP."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 958
                },
                {
                    "x": 2196,
                    "y": 958
                },
                {
                    "x": 2196,
                    "y": 1234
                },
                {
                    "x": 1273,
                    "y": 1234
                }
            ],
            "category": "paragraph",
            "html": "<p id='115' style='font-size:14px'>Sheila Castilho. 2021. Towards document-level human<br>MT evaluation: On the issues of annotator agree-<br>ment, effort and misevaluation. In Proceedings of<br>the Workshop on Human Evaluation ofNLP Systems<br>(HumEval), pages 34-45, Online. Association for<br>Computational Linguistics.</p>",
            "id": 115,
            "page": 10,
            "text": "Sheila Castilho. 2021. Towards document-level human MT evaluation: On the issues of annotator agreement, effort and misevaluation. In Proceedings of the Workshop on Human Evaluation ofNLP Systems (HumEval), pages 34-45, Online. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1265
                },
                {
                    "x": 2198,
                    "y": 1265
                },
                {
                    "x": 2198,
                    "y": 1589
                },
                {
                    "x": 1272,
                    "y": 1589
                }
            ],
            "category": "paragraph",
            "html": "<p id='116' style='font-size:14px'>Tuhin Chakrabarty, Debanjan Ghosh, Smaranda Mure-<br>san, and Nanyun Peng. 2020a. R^3: Reverse, re-<br>trieve, and rank for sarcasm generation with com-<br>monsense knowledge. In Proceedings of the 58th<br>Annual Meeting of the Association for Computa-<br>tional Linguistics, pages 7976-7986, Online. Asso-<br>ciation for Computational Linguistics.</p>",
            "id": 116,
            "page": 10,
            "text": "Tuhin Chakrabarty, Debanjan Ghosh, Smaranda Muresan, and Nanyun Peng. 2020a. R^3: Reverse, retrieve, and rank for sarcasm generation with commonsense knowledge. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7976-7986, Online. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1620
                },
                {
                    "x": 2196,
                    "y": 1620
                },
                {
                    "x": 2196,
                    "y": 1942
                },
                {
                    "x": 1272,
                    "y": 1942
                }
            ],
            "category": "paragraph",
            "html": "<p id='117' style='font-size:16px'>Tuhin Chakrabarty, Smaranda Muresan, and Nanyun<br>Peng. 2020b. Generating similes effortlessly like a<br>pro: A style transfer approach for simile generation.<br>In Proceedings of the 2020 Conference on Empirical<br>Methods in Natural Language Processing (EMNLP),<br>pages 6455-6469, Online. Association for Computa-<br>tional Linguistics.</p>",
            "id": 117,
            "page": 10,
            "text": "Tuhin Chakrabarty, Smaranda Muresan, and Nanyun Peng. 2020b. Generating similes effortlessly like a pro: A style transfer approach for simile generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6455-6469, Online. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 1975
                },
                {
                    "x": 2196,
                    "y": 1975
                },
                {
                    "x": 2196,
                    "y": 2390
                },
                {
                    "x": 1270,
                    "y": 2390
                }
            ],
            "category": "paragraph",
            "html": "<p id='118' style='font-size:14px'>Elizabeth Clark, Tal August, Sofia Serrano, Nikita<br>Haduong, Suchin Gururangan, and Noah A. Smith.<br>2021. All that's 'human' is not gold: Evaluating<br>human evaluation of generated text. In Proceed-<br>ings of the 59th Annual Meeting of the Association<br>for Computational Linguistics and the 11th Interna-<br>tional Joint Conference on Natural Language Pro-<br>cessing (Volume 1: Long Papers), pages 7282-7296,<br>Online. Association for Computational Linguistics.</p>",
            "id": 118,
            "page": 10,
            "text": "Elizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, Suchin Gururangan, and Noah A. Smith. 2021. All that's 'human' is not gold: Evaluating human evaluation of generated text. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 7282-7296, Online. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 2420
                },
                {
                    "x": 2197,
                    "y": 2420
                },
                {
                    "x": 2197,
                    "y": 2790
                },
                {
                    "x": 1270,
                    "y": 2790
                }
            ],
            "category": "paragraph",
            "html": "<p id='119' style='font-size:16px'>Elizabeth Clark, Yangfeng Ji, and Noah A. Smith. 2018.<br>Neural text generation in stories using entity repre-<br>sentations as context. In Proceedings of the 2018<br>Conference of the North American Chapter of the<br>Association for Computational Linguistics: Human<br>Language Technologies, Volume 1 (Long Papers),<br>pages 2250-2260, New Orleans, Louisiana. Associ-<br>ation for Computational Linguistics.</p>",
            "id": 119,
            "page": 10,
            "text": "Elizabeth Clark, Yangfeng Ji, and Noah A. Smith. 2018. Neural text generation in stories using entity representations as context. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 2250-2260, New Orleans, Louisiana. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 2822
                },
                {
                    "x": 2191,
                    "y": 2822
                },
                {
                    "x": 2191,
                    "y": 2915
                },
                {
                    "x": 1270,
                    "y": 2915
                }
            ],
            "category": "paragraph",
            "html": "<p id='120' style='font-size:14px'>Jacob Cohen. 1988. Statistical power analysis for the<br>behavioral sciences. Routledge.</p>",
            "id": 120,
            "page": 10,
            "text": "Jacob Cohen. 1988. Statistical power analysis for the behavioral sciences. Routledge."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2948
                },
                {
                    "x": 2198,
                    "y": 2948
                },
                {
                    "x": 2198,
                    "y": 3225
                },
                {
                    "x": 1272,
                    "y": 3225
                }
            ],
            "category": "paragraph",
            "html": "<p id='121' style='font-size:14px'>Chris Donahue, Mina Lee, and Percy Liang. 2020. En-<br>abling language models to fill in the blanks. In Pro-<br>ceedings of the 58th Annual Meeting of the Asso-<br>ciation for Computational Linguistics, pages 2492-<br>2501, Online. Association for Computational Lin-<br>guistics.</p>",
            "id": 121,
            "page": 10,
            "text": "Chris Donahue, Mina Lee, and Percy Liang. 2020. Enabling language models to fill in the blanks. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 24922501, Online. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 305
                },
                {
                    "x": 1213,
                    "y": 305
                },
                {
                    "x": 1213,
                    "y": 445
                },
                {
                    "x": 289,
                    "y": 445
                }
            ],
            "category": "paragraph",
            "html": "<p id='122' style='font-size:18px'>Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hi-<br>erarchical neural story generation. In Association<br>for Computational Linguistics.</p>",
            "id": 122,
            "page": 11,
            "text": "Angela Fan, Mike Lewis, and Yann Dauphin. 2018. Hierarchical neural story generation. In Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 486
                },
                {
                    "x": 1215,
                    "y": 486
                },
                {
                    "x": 1215,
                    "y": 764
                },
                {
                    "x": 289,
                    "y": 764
                }
            ],
            "category": "paragraph",
            "html": "<p id='123' style='font-size:20px'>Angela Fan, Mike Lewis, and Yann Dauphin. 2019.<br>Strategies for structuring story generation. In Pro-<br>ceedings of the 57th Annual Meeting of the Asso-<br>ciation for Computational Linguistics, pages 2650-<br>2660, Florence, Italy. Association for Computa-<br>tional Linguistics.</p>",
            "id": 123,
            "page": 11,
            "text": "Angela Fan, Mike Lewis, and Yann Dauphin. 2019. Strategies for structuring story generation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 26502660, Florence, Italy. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 805
                },
                {
                    "x": 1214,
                    "y": 805
                },
                {
                    "x": 1214,
                    "y": 1175
                },
                {
                    "x": 287,
                    "y": 1175
                }
            ],
            "category": "paragraph",
            "html": "<p id='124' style='font-size:22px'>Zhiyuan Fang, Tejas Gokhale, Pratyay Baner-<br>jee, Chitta Baral, and Yezhou Yang. 2020.<br>Video2Commonsense: Generating commonsense<br>descriptions to enrich video captioning. In Proceed-<br>ings of the 2020 Conference on Empirical Methods<br>in Natural Language Processing (EMNLP), pages<br>840-860, Online. Association for Computational<br>Linguistics.</p>",
            "id": 124,
            "page": 11,
            "text": "Zhiyuan Fang, Tejas Gokhale, Pratyay Banerjee, Chitta Baral, and Yezhou Yang. 2020. Video2Commonsense: Generating commonsense descriptions to enrich video captioning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 840-860, Online. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 1214
                },
                {
                    "x": 1215,
                    "y": 1214
                },
                {
                    "x": 1215,
                    "y": 1356
                },
                {
                    "x": 290,
                    "y": 1356
                }
            ],
            "category": "paragraph",
            "html": "<p id='125' style='font-size:14px'>Karen Fort, Gilles Adda, and K. Bretonnel Cohen.<br>2011. Amazon mechanical turk: Gold mine or coal<br>mine? Computational Linguistics, 37(2):413-420.</p>",
            "id": 125,
            "page": 11,
            "text": "Karen Fort, Gilles Adda, and K. Bretonnel Cohen. 2011. Amazon mechanical turk: Gold mine or coal mine? Computational Linguistics, 37(2):413-420."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 1396
                },
                {
                    "x": 1214,
                    "y": 1396
                },
                {
                    "x": 1214,
                    "y": 1812
                },
                {
                    "x": 288,
                    "y": 1812
                }
            ],
            "category": "paragraph",
            "html": "<p id='126' style='font-size:18px'>Cristina Garbacea, Samuel Carton, Shiyan Yan, and<br>Qiaozhu Mei. 2019. Judge the judges: A large-<br>scale evaluation study of neural language models<br>for online review generation. In Proceedings of<br>the 2019 Conference on Empirical Methods in Nat-<br>ural Language Processing and the 9th International<br>Joint Conference on Natural Language Processing<br>(EMNLP-IJCNLP), pages 3968-3981, Hong Kong,<br>China. Association for Computational Linguistics.</p>",
            "id": 126,
            "page": 11,
            "text": "Cristina Garbacea, Samuel Carton, Shiyan Yan, and Qiaozhu Mei. 2019. Judge the judges: A largescale evaluation study of neural language models for online review generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3968-3981, Hong Kong, China. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 1853
                },
                {
                    "x": 1214,
                    "y": 1853
                },
                {
                    "x": 1214,
                    "y": 2131
                },
                {
                    "x": 291,
                    "y": 2131
                }
            ],
            "category": "paragraph",
            "html": "<p id='127' style='font-size:18px'>Dan Gillick and Yang Liu. 2010. Non-expert evalua-<br>tion of summarization systems is risky. In Proceed-<br>ings of the NAACL HLT 2010 Workshop on Creating<br>Speech and Language Data with Amazon 's Mechan-<br>ical Turk, pages 148-151, Los Angeles. Association<br>for Computational Linguistics.</p>",
            "id": 127,
            "page": 11,
            "text": "Dan Gillick and Yang Liu. 2010. Non-expert evaluation of summarization systems is risky. In Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon 's Mechanical Turk, pages 148-151, Los Angeles. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 2172
                },
                {
                    "x": 1213,
                    "y": 2172
                },
                {
                    "x": 1213,
                    "y": 2497
                },
                {
                    "x": 288,
                    "y": 2497
                }
            ],
            "category": "paragraph",
            "html": "<p id='128' style='font-size:20px'>Seraphina Goldfarb-Tarrant, Tuhin Chakrabarty, Ralph<br>Weischedel, and Nanyun Peng. 2020. Content plan-<br>ning for neural story generation with aristotelian<br>rescoring. In Proceedings of the 2020 Conference<br>on Empirical Methods in Natural Language Process-<br>ing (EMNLP), pages 4319-4338, Online. Associa-<br>tion for Computational Linguistics.</p>",
            "id": 128,
            "page": 11,
            "text": "Seraphina Goldfarb-Tarrant, Tuhin Chakrabarty, Ralph Weischedel, and Nanyun Peng. 2020. Content planning for neural story generation with aristotelian rescoring. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4319-4338, Online. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 2537
                },
                {
                    "x": 1214,
                    "y": 2537
                },
                {
                    "x": 1214,
                    "y": 2908
                },
                {
                    "x": 290,
                    "y": 2908
                }
            ],
            "category": "paragraph",
            "html": "<p id='129' style='font-size:18px'>Philip John Gorinski and Mirella Lapata. 2018. What's<br>this movie about? a joint neural network architec-<br>ture for movie content analysis. In Proceedings of<br>the 2018 Conference of the North American Chap-<br>ter of the Associationfor Computational Linguistics:<br>Human Language Technologies, Volume 1 (Long Pa-<br>pers), pages 1770-1781, New Orleans, Louisiana.<br>Association for Computational Linguistics.</p>",
            "id": 129,
            "page": 11,
            "text": "Philip John Gorinski and Mirella Lapata. 2018. What's this movie about? a joint neural network architecture for movie content analysis. In Proceedings of the 2018 Conference of the North American Chapter of the Associationfor Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1770-1781, New Orleans, Louisiana. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 2947
                },
                {
                    "x": 1215,
                    "y": 2947
                },
                {
                    "x": 1215,
                    "y": 3223
                },
                {
                    "x": 289,
                    "y": 3223
                }
            ],
            "category": "paragraph",
            "html": "<p id='130' style='font-size:20px'>Tanya Goyal and Greg Durrett. 2020. Neural syntactic<br>preordering for controlled paraphrase generation. In<br>Proceedings of the 58th Annual Meeting of the As-<br>sociation for Computational Linguistics, pages 238-<br>252, Online. Association for Computational Linguis-<br>tics.</p>",
            "id": 130,
            "page": 11,
            "text": "Tanya Goyal and Greg Durrett. 2020. Neural syntactic preordering for controlled paraphrase generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 238252, Online. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 305
                },
                {
                    "x": 2198,
                    "y": 305
                },
                {
                    "x": 2198,
                    "y": 490
                },
                {
                    "x": 1271,
                    "y": 490
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='131' style='font-size:16px'>Yvette Graham, Timothy Baldwin, Alistair Moffat, and<br>Justin Zobel. 2017. Can machine translation sys-<br>tems be evaluated by the crowd alone. Natural Lan-<br>guage Engineering, 23(1):3-30.</p>",
            "id": 131,
            "page": 11,
            "text": "Yvette Graham, Timothy Baldwin, Alistair Moffat, and Justin Zobel. 2017. Can machine translation systems be evaluated by the crowd alone. Natural Language Engineering, 23(1):3-30."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 527
                },
                {
                    "x": 2194,
                    "y": 527
                },
                {
                    "x": 2194,
                    "y": 666
                },
                {
                    "x": 1269,
                    "y": 666
                }
            ],
            "category": "paragraph",
            "html": "<p id='132' style='font-size:14px'>Roman Grundkiewicz, Marcin Junczys-Dowmunt, and<br>Edward Gillian. 2015. Human evaluation of gram-<br>matical error correction systems. In EMNLP.</p>",
            "id": 132,
            "page": 11,
            "text": "Roman Grundkiewicz, Marcin Junczys-Dowmunt, and Edward Gillian. 2015. Human evaluation of grammatical error correction systems. In EMNLP."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 704
                },
                {
                    "x": 2197,
                    "y": 704
                },
                {
                    "x": 2197,
                    "y": 933
                },
                {
                    "x": 1271,
                    "y": 933
                }
            ],
            "category": "paragraph",
            "html": "<p id='133' style='font-size:18px'>Jian Guan, Fei Huang, Zhihao Zhao, Xiaoyan Zhu, and<br>Minlie Huang. 2020. A knowledge-enhanced pre-<br>training model for commonsense story generation.<br>Transactions of the Association for Computational<br>Linguistics, 8:93-108.</p>",
            "id": 133,
            "page": 11,
            "text": "Jian Guan, Fei Huang, Zhihao Zhao, Xiaoyan Zhu, and Minlie Huang. 2020. A knowledge-enhanced pretraining model for commonsense story generation. Transactions of the Association for Computational Linguistics, 8:93-108."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 972
                },
                {
                    "x": 2197,
                    "y": 972
                },
                {
                    "x": 2197,
                    "y": 1247
                },
                {
                    "x": 1270,
                    "y": 1247
                }
            ],
            "category": "paragraph",
            "html": "<p id='134' style='font-size:20px'>Francisco Guzman, Ahmed Abdelali, Irina Temnikova,<br>Hassan Sajjad, and Stephan Vogel. 2015. How do<br>humans evaluate machine translation. In Proceed-<br>ings of the Tenth Workshop on Statistical Machine<br>Translation. Association for Computational Linguis-<br>tics.</p>",
            "id": 134,
            "page": 11,
            "text": "Francisco Guzman, Ahmed Abdelali, Irina Temnikova, Hassan Sajjad, and Stephan Vogel. 2015. How do humans evaluate machine translation. In Proceedings of the Tenth Workshop on Statistical Machine Translation. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 1285
                },
                {
                    "x": 2195,
                    "y": 1285
                },
                {
                    "x": 2195,
                    "y": 1610
                },
                {
                    "x": 1270,
                    "y": 1610
                }
            ],
            "category": "paragraph",
            "html": "<p id='135' style='font-size:22px'>He He, Nanyun Peng, and Percy Liang. 2019. Pun gen-<br>eration with surprise. In Proceedings of the 2019<br>Conference of the North American Chapter of the<br>Association for Computational Linguistics: Human<br>Language Technologies, Volume 1 (Long and Short<br>Papers), pages 1734-1744, Minneapolis, Minnesota.<br>Association for Computational Linguistics.</p>",
            "id": 135,
            "page": 11,
            "text": "He He, Nanyun Peng, and Percy Liang. 2019. Pun generation with surprise. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1734-1744, Minneapolis, Minnesota. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 1648
                },
                {
                    "x": 2198,
                    "y": 1648
                },
                {
                    "x": 2198,
                    "y": 1968
                },
                {
                    "x": 1272,
                    "y": 1968
                }
            ],
            "category": "paragraph",
            "html": "<p id='136' style='font-size:18px'>Allison Hegel, Sudha Rao, Asli Celikyilmaz, and Bill<br>Dolan. 2020. Substance over Style: Document-<br>Level Targeted Content Transfer. In Proceedings of<br>the 2020 Conference on Empirical Methods in Nat-<br>ural Language Processing (EMNLP), pages 6485-<br>6504, Online. Association for Computational Lin-<br>guistics.</p>",
            "id": 136,
            "page": 11,
            "text": "Allison Hegel, Sudha Rao, Asli Celikyilmaz, and Bill Dolan. 2020. Substance over Style: DocumentLevel Targeted Content Transfer. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 64856504, Online. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 1272,
                    "y": 2005
                },
                {
                    "x": 2197,
                    "y": 2005
                },
                {
                    "x": 2197,
                    "y": 2191
                },
                {
                    "x": 1272,
                    "y": 2191
                }
            ],
            "category": "paragraph",
            "html": "<p id='137' style='font-size:16px'>Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and<br>Yejin Choi. 2019. The curious case of neural text de-<br>generation. In International Conference on Learn-<br>ing Representations.</p>",
            "id": 137,
            "page": 11,
            "text": "Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degeneration. In International Conference on Learning Representations."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 2229
                },
                {
                    "x": 2196,
                    "y": 2229
                },
                {
                    "x": 2196,
                    "y": 2553
                },
                {
                    "x": 1270,
                    "y": 2553
                }
            ],
            "category": "paragraph",
            "html": "<p id='138' style='font-size:20px'>Ari Holtzman, Jan Buys, Maxwell Forbes, Antoine<br>Bosselut, David Golub, and Yejin Choi. 2018.<br>Learning to write with cooperative discriminators.<br>In Proceedings ofthe 56th Annual Meeting of the As-<br>sociation for Computational Linguistics (Volume 1:<br>Long Papers), pages 1638-1649, Melbourne, Aus-<br>tralia. Association for Computational Linguistics.</p>",
            "id": 138,
            "page": 11,
            "text": "Ari Holtzman, Jan Buys, Maxwell Forbes, Antoine Bosselut, David Golub, and Yejin Choi. 2018. Learning to write with cooperative discriminators. In Proceedings ofthe 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1638-1649, Melbourne, Australia. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 2588
                },
                {
                    "x": 2196,
                    "y": 2588
                },
                {
                    "x": 2196,
                    "y": 3047
                },
                {
                    "x": 1271,
                    "y": 3047
                }
            ],
            "category": "paragraph",
            "html": "<p id='139' style='font-size:18px'>David M. Howcroft, Anya Belz, Miruna-Adriana<br>Clinciu, Dimitra Gkatzia, Sadid A. Hasan, Saad<br>Mahamood, Simon Mille, Emiel van Miltenburg,<br>Sashank Santhanam, and Verena Rieser. 2020.<br>Twenty years of confusion in human evaluation:<br>NLG needs evaluation sheets and standardised def-<br>initions. In Proceedings of the 13th International<br>Conference on Natural Language Generation, pages<br>169-182, Dublin, Ireland. Association for Computa-<br>tional Linguistics.</p>",
            "id": 139,
            "page": 11,
            "text": "David M. Howcroft, Anya Belz, Miruna-Adriana Clinciu, Dimitra Gkatzia, Sadid A. Hasan, Saad Mahamood, Simon Mille, Emiel van Miltenburg, Sashank Santhanam, and Verena Rieser. 2020. Twenty years of confusion in human evaluation: NLG needs evaluation sheets and standardised definitions. In Proceedings of the 13th International Conference on Natural Language Generation, pages 169-182, Dublin, Ireland. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 3084
                },
                {
                    "x": 2196,
                    "y": 3084
                },
                {
                    "x": 2196,
                    "y": 3225
                },
                {
                    "x": 1271,
                    "y": 3225
                }
            ],
            "category": "paragraph",
            "html": "<p id='140' style='font-size:22px'>Ting- Yao Hsu, Chieh- Yang Huang, Yen-Chia Hsu, and<br>Ting-Hao Huang. 2019. Visual story post-editing.<br>In Proceedings of the 57th Annual Meeting of the</p>",
            "id": 140,
            "page": 11,
            "text": "Ting- Yao Hsu, Chieh- Yang Huang, Yen-Chia Hsu, and Ting-Hao Huang. 2019. Visual story post-editing. In Proceedings of the 57th Annual Meeting of the"
        },
        {
            "bounding_box": [
                {
                    "x": 330,
                    "y": 306
                },
                {
                    "x": 1212,
                    "y": 306
                },
                {
                    "x": 1212,
                    "y": 444
                },
                {
                    "x": 330,
                    "y": 444
                }
            ],
            "category": "paragraph",
            "html": "<p id='141' style='font-size:22px'>Association for Computational Linguistics, pages<br>6581-6586, Florence, Italy. Association for Compu-<br>tational Linguistics.</p>",
            "id": 141,
            "page": 12,
            "text": "Association for Computational Linguistics, pages 6581-6586, Florence, Italy. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 486
                },
                {
                    "x": 1215,
                    "y": 486
                },
                {
                    "x": 1215,
                    "y": 809
                },
                {
                    "x": 289,
                    "y": 809
                }
            ],
            "category": "paragraph",
            "html": "<p id='142' style='font-size:18px'>Daphne Ippolito, Daniel Duckworth, Chris Callison-<br>Burch, and Douglas Eck. 2020. Automatic detec-<br>tion of generated text is easiest when humans are<br>fooled. In Proceedings of the 58th Annual Meet-<br>ing of the Associationfor Computational Linguistics,<br>pages 1808-1822, Online. Association for Computa-<br>tional Linguistics.</p>",
            "id": 142,
            "page": 12,
            "text": "Daphne Ippolito, Daniel Duckworth, Chris CallisonBurch, and Douglas Eck. 2020. Automatic detection of generated text is easiest when humans are fooled. In Proceedings of the 58th Annual Meeting of the Associationfor Computational Linguistics, pages 1808-1822, Online. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 850
                },
                {
                    "x": 1215,
                    "y": 850
                },
                {
                    "x": 1215,
                    "y": 1129
                },
                {
                    "x": 289,
                    "y": 1129
                }
            ],
            "category": "paragraph",
            "html": "<p id='143' style='font-size:20px'>Daphne Ippolito, David Grangier, Chris Callison-<br>Burch, and Douglas Eck. 2019. Unsupervised hier-<br>archical story infilling. In Proceedings of the First<br>Workshop on Narrative Understanding, pages 37-<br>43, Minneapolis, Minnesota. Association for Com-<br>putational Linguistics.</p>",
            "id": 143,
            "page": 12,
            "text": "Daphne Ippolito, David Grangier, Chris CallisonBurch, and Douglas Eck. 2019. Unsupervised hierarchical story infilling. In Proceedings of the First Workshop on Narrative Understanding, pages 3743, Minneapolis, Minnesota. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 1170
                },
                {
                    "x": 1213,
                    "y": 1170
                },
                {
                    "x": 1213,
                    "y": 1494
                },
                {
                    "x": 289,
                    "y": 1494
                }
            ],
            "category": "paragraph",
            "html": "<p id='144' style='font-size:18px'>Neslihan Iskender, Tim Polzehl, and Sebastian M�ller.<br>2020. Best practices for crowd-based evaluation of<br>German summarization: Comparing crowd, expert<br>and automatic evaluation. In Proceedings of the<br>First Workshop on Evaluation and Comparison of<br>NLP Systems, pages 164-175, Online. Association<br>for Computational Linguistics.</p>",
            "id": 144,
            "page": 12,
            "text": "Neslihan Iskender, Tim Polzehl, and Sebastian M�ller. 2020. Best practices for crowd-based evaluation of German summarization: Comparing crowd, expert and automatic evaluation. In Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems, pages 164-175, Online. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 1533
                },
                {
                    "x": 1214,
                    "y": 1533
                },
                {
                    "x": 1214,
                    "y": 1857
                },
                {
                    "x": 289,
                    "y": 1857
                }
            ],
            "category": "paragraph",
            "html": "<p id='145' style='font-size:20px'>Chao Jiang, Mounica Maddela, Wuwei Lan, Yang<br>Zhong, and Wei Xu. 2020. Neural CRF model for<br>sentence alignment in text simplification. In Pro-<br>ceedings of the 58th Annual Meeting of the Asso-<br>ciation for Computational Linguistics, pages 7943-<br>7960, Online. Association for Computational Lin-<br>guistics.</p>",
            "id": 145,
            "page": 12,
            "text": "Chao Jiang, Mounica Maddela, Wuwei Lan, Yang Zhong, and Wei Xu. 2020. Neural CRF model for sentence alignment in text simplification. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 79437960, Online. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 1898
                },
                {
                    "x": 1214,
                    "y": 1898
                },
                {
                    "x": 1214,
                    "y": 2129
                },
                {
                    "x": 290,
                    "y": 2129
                }
            ],
            "category": "paragraph",
            "html": "<p id='146' style='font-size:20px'>Ryan Kennedy, Scott Clifford, Tyler Burleigh, Philip D.<br>Waggoner, Ryan Jewell, and Nicholas J. G. Winter.<br>2020. The shape of and solutions to the mturk qual-<br>ity crisis. Political Science Research and Methods,<br>8(4):614-629.</p>",
            "id": 146,
            "page": 12,
            "text": "Ryan Kennedy, Scott Clifford, Tyler Burleigh, Philip D. Waggoner, Ryan Jewell, and Nicholas J. G. Winter. 2020. The shape of and solutions to the mturk quality crisis. Political Science Research and Methods, 8(4):614-629."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 2171
                },
                {
                    "x": 1215,
                    "y": 2171
                },
                {
                    "x": 1215,
                    "y": 2403
                },
                {
                    "x": 289,
                    "y": 2403
                }
            ],
            "category": "paragraph",
            "html": "<p id='147' style='font-size:18px'>Daniel Khashabi, Gabriel Stanovsky, Jonathan Bragg,<br>Nicholas Lourie, Jungo Kasai, Yejin Choi, Noah A.<br>Smith, and Daniel S. Weld. 2021. Genie: A leader-<br>board for human-in-the-loop evaluation of text gen-<br>eration.</p>",
            "id": 147,
            "page": 12,
            "text": "Daniel Khashabi, Gabriel Stanovsky, Jonathan Bragg, Nicholas Lourie, Jungo Kasai, Yejin Choi, Noah A. Smith, and Daniel S. Weld. 2021. Genie: A leaderboard for human-in-the-loop evaluation of text generation."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 2446
                },
                {
                    "x": 1214,
                    "y": 2446
                },
                {
                    "x": 1214,
                    "y": 2724
                },
                {
                    "x": 291,
                    "y": 2724
                }
            ],
            "category": "paragraph",
            "html": "<p id='148' style='font-size:20px'>Kalpesh Krishna, John Wieting, and Mohit Iyyer. 2020.<br>Reformulating unsupervised style transfer as para-<br>phrase generation. In Proceedings of the 2020 Con-<br>ference on Empirical Methods in Natural Language<br>Processing (EMNLP), pages 737-762, Online. Asso-<br>ciation for Computational Linguistics.</p>",
            "id": 148,
            "page": 12,
            "text": "Kalpesh Krishna, John Wieting, and Mohit Iyyer. 2020. Reformulating unsupervised style transfer as paraphrase generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 737-762, Online. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 2764
                },
                {
                    "x": 1214,
                    "y": 2764
                },
                {
                    "x": 1214,
                    "y": 3227
                },
                {
                    "x": 288,
                    "y": 3227
                }
            ],
            "category": "paragraph",
            "html": "<p id='149' style='font-size:20px'>Reno Kriz, Joao Sedoc, Marianna Apidianaki, Car-<br>olina Zheng, Gaurav Kumar, Eleni Miltsakaki, and<br>Chris Callison-Burch. 2019. Complexity-weighted<br>loss and diverse reranking for sentence simplifica-<br>tion. In Proceedings of the 2019 Conference of<br>the North American Chapter of the Association for<br>Computational Linguistics: Human Language Tech-<br>nologies, Volume 1 (Long and Short Papers), pages<br>3137-3147, Minneapolis, Minnesota. Association<br>for Computational Linguistics.</p>",
            "id": 149,
            "page": 12,
            "text": "Reno Kriz, Joao Sedoc, Marianna Apidianaki, Carolina Zheng, Gaurav Kumar, Eleni Miltsakaki, and Chris Callison-Burch. 2019. Complexity-weighted loss and diverse reranking for sentence simplification. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3137-3147, Minneapolis, Minnesota. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 305
                },
                {
                    "x": 2197,
                    "y": 305
                },
                {
                    "x": 2197,
                    "y": 674
                },
                {
                    "x": 1269,
                    "y": 674
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='150' style='font-size:20px'>Juncen Li, Robin Jia, He He, and Percy Liang. 2018.<br>Delete, retrieve, generate: a simple approach to sen-<br>timent and style transfer. In Proceedings of the 2018<br>Conference of the North American Chapter of the<br>Association for Computational Linguistics: Human<br>Language Technologies, Volume 1 (Long Papers),<br>pages 1865-1874, New Orleans, Louisiana. Associ-<br>ation for Computational Linguistics.</p>",
            "id": 150,
            "page": 12,
            "text": "Juncen Li, Robin Jia, He He, and Percy Liang. 2018. Delete, retrieve, generate: a simple approach to sentiment and style transfer. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1865-1874, New Orleans, Louisiana. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 707
                },
                {
                    "x": 2191,
                    "y": 707
                },
                {
                    "x": 2191,
                    "y": 804
                },
                {
                    "x": 1270,
                    "y": 804
                }
            ],
            "category": "paragraph",
            "html": "<p id='151' style='font-size:16px'>Chin- Yew Lin. 2004. Rouge: A package for automatic<br>evaluation of summaries. In ACL.</p>",
            "id": 151,
            "page": 12,
            "text": "Chin- Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In ACL."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 841
                },
                {
                    "x": 2195,
                    "y": 841
                },
                {
                    "x": 2195,
                    "y": 1118
                },
                {
                    "x": 1271,
                    "y": 1118
                }
            ],
            "category": "paragraph",
            "html": "<p id='152' style='font-size:18px'>Kevin Lin, Ming- Yu Liu, Ming-Ting Sun, and Jan<br>Kautz. 2020. Learning to generate multiple style<br>transfer outputs for an input sentence. In Proceed-<br>ings of the Fourth Workshop on Neural Generation<br>and Translation, pages 10-23, Online. Association<br>for Computational Linguistics.</p>",
            "id": 152,
            "page": 12,
            "text": "Kevin Lin, Ming- Yu Liu, Ming-Ting Sun, and Jan Kautz. 2020. Learning to generate multiple style transfer outputs for an input sentence. In Proceedings of the Fourth Workshop on Neural Generation and Translation, pages 10-23, Online. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 1155
                },
                {
                    "x": 2197,
                    "y": 1155
                },
                {
                    "x": 2197,
                    "y": 1433
                },
                {
                    "x": 1270,
                    "y": 1433
                }
            ],
            "category": "paragraph",
            "html": "<p id='153' style='font-size:20px'>Hui Liu, Qingyu Yin, and William Yang Wang. 2019.<br>Towards explainable NLP: A generative explanation<br>framework for text classification. In Proceedings of<br>the 57th Annual Meeting of the Associationfor Com-<br>putational Linguistics, pages 5570-5581, Florence,<br>Italy. Association for Computational Linguistics.</p>",
            "id": 153,
            "page": 12,
            "text": "Hui Liu, Qingyu Yin, and William Yang Wang. 2019. Towards explainable NLP: A generative explanation framework for text classification. In Proceedings of the 57th Annual Meeting of the Associationfor Computational Linguistics, pages 5570-5581, Florence, Italy. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 1469
                },
                {
                    "x": 2198,
                    "y": 1469
                },
                {
                    "x": 2198,
                    "y": 1747
                },
                {
                    "x": 1270,
                    "y": 1747
                }
            ],
            "category": "paragraph",
            "html": "<p id='154' style='font-size:20px'>Jonathan Mallinson, Rico Sennrich, and Mirella Lap-<br>ata. 2020. Zero-shot crosslingual sentence simplifi-<br>cation. In Proceedings of the 2020 Conference on<br>Empirical Methods in Natural Language Process-<br>ing (EMNLP), pages 5109-5126, Online. Associa-<br>tion for Computational Linguistics.</p>",
            "id": 154,
            "page": 12,
            "text": "Jonathan Mallinson, Rico Sennrich, and Mirella Lapata. 2020. Zero-shot crosslingual sentence simplification. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5109-5126, Online. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 1782
                },
                {
                    "x": 2197,
                    "y": 1782
                },
                {
                    "x": 2197,
                    "y": 2198
                },
                {
                    "x": 1271,
                    "y": 2198
                }
            ],
            "category": "paragraph",
            "html": "<p id='155' style='font-size:18px'>Huanru Henry Mao, Bodhisattwa Prasad Majumder,<br>Julian McAuley, and Garrison Cottrell. 2019. Im-<br>proving neural story generation by targeted common<br>sense grounding. In Proceedings of the 2019 Con-<br>ference on Empirical Methods in Natural Language<br>Processing and the 9th International Joint Confer-<br>ence on Natural Language Processing (EMNLP-<br>IJCNLP), pages 5988-5993, Hong Kong, China. As-<br>sociation for Computational Linguistics.</p>",
            "id": 155,
            "page": 12,
            "text": "Huanru Henry Mao, Bodhisattwa Prasad Majumder, Julian McAuley, and Garrison Cottrell. 2019. Improving neural story generation by targeted common sense grounding. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLPIJCNLP), pages 5988-5993, Hong Kong, China. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 2233
                },
                {
                    "x": 2196,
                    "y": 2233
                },
                {
                    "x": 2196,
                    "y": 2510
                },
                {
                    "x": 1271,
                    "y": 2510
                }
            ],
            "category": "paragraph",
            "html": "<p id='156' style='font-size:18px'>Pedro Henrique Martins, Zita Marinho, and Andre F. T.<br>Martins. 2020. Sparse text generation. In Proceed-<br>ings of the 2020 Conference on Empirical Methods<br>in Natural Language Processing (EMNLP), pages<br>4252-4273, Online. Association for Computational<br>Linguistics.</p>",
            "id": 156,
            "page": 12,
            "text": "Pedro Henrique Martins, Zita Marinho, and Andre F. T. Martins. 2020. Sparse text generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4252-4273, Online. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 2548
                },
                {
                    "x": 2196,
                    "y": 2548
                },
                {
                    "x": 2196,
                    "y": 2687
                },
                {
                    "x": 1271,
                    "y": 2687
                }
            ],
            "category": "paragraph",
            "html": "<p id='157' style='font-size:18px'>Ted Matherly. 2019. A panel for lemons? positivity<br>bias, reputation systems and data quality on MTurk.<br>European Journal of Marketing, 53(2):195-223.</p>",
            "id": 157,
            "page": 12,
            "text": "Ted Matherly. 2019. A panel for lemons? positivity bias, reputation systems and data quality on MTurk. European Journal of Marketing, 53(2):195-223."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 2725
                },
                {
                    "x": 2196,
                    "y": 2725
                },
                {
                    "x": 2196,
                    "y": 3092
                },
                {
                    "x": 1271,
                    "y": 3092
                }
            ],
            "category": "paragraph",
            "html": "<p id='158' style='font-size:22px'>Remi Mir, Bjarke Felbo, Nick Obradovich, and Iyad<br>Rahwan. 2019. Evaluating style transfer for text. In<br>Proceedings of the 2019 Conference of the North<br>American Chapter of the Association for Computa-<br>tional Linguistics: Human Language Technologies,<br>Volume 1 (Long and Short Papers), pages 495-504,<br>Minneapolis, Minnesota. Association for Computa-<br>tional Linguistics.</p>",
            "id": 158,
            "page": 12,
            "text": "Remi Mir, Bjarke Felbo, Nick Obradovich, and Iyad Rahwan. 2019. Evaluating style transfer for text. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 495-504, Minneapolis, Minnesota. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 3129
                },
                {
                    "x": 2192,
                    "y": 3129
                },
                {
                    "x": 2192,
                    "y": 3225
                },
                {
                    "x": 1270,
                    "y": 3225
                }
            ],
            "category": "paragraph",
            "html": "<p id='159' style='font-size:14px'>Yatri Modi and Natalie Parde. 2019. The steep road<br>to happily ever after: an analysis of current visual</p>",
            "id": 159,
            "page": 12,
            "text": "Yatri Modi and Natalie Parde. 2019. The steep road to happily ever after: an analysis of current visual"
        },
        {
            "bounding_box": [
                {
                    "x": 331,
                    "y": 306
                },
                {
                    "x": 1213,
                    "y": 306
                },
                {
                    "x": 1213,
                    "y": 490
                },
                {
                    "x": 331,
                    "y": 490
                }
            ],
            "category": "paragraph",
            "html": "<p id='160' style='font-size:22px'>storytelling models. In Proceedings of the Second<br>Workshop on Shortcomings in Vision and Language,<br>pages 47-57, Minneapolis, Minnesota. Association<br>for Computational Linguistics.</p>",
            "id": 160,
            "page": 13,
            "text": "storytelling models. In Proceedings of the Second Workshop on Shortcomings in Vision and Language, pages 47-57, Minneapolis, Minnesota. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 526
                },
                {
                    "x": 1213,
                    "y": 526
                },
                {
                    "x": 1213,
                    "y": 851
                },
                {
                    "x": 291,
                    "y": 851
                }
            ],
            "category": "paragraph",
            "html": "<p id='161' style='font-size:18px'>Yusuke Mori, Hiroaki Yamane, Yusuke Mukuta, and<br>Tatsuya Harada. 2019. Toward a better story end:<br>Collecting human evaluation with reasons. In<br>Proceedings of the 12th International Conference<br>on Natural Language Generation, pages 383-390,<br>Tokyo, Japan. Association for Computational Lin-<br>guistics.</p>",
            "id": 161,
            "page": 13,
            "text": "Yusuke Mori, Hiroaki Yamane, Yusuke Mukuta, and Tatsuya Harada. 2019. Toward a better story end: Collecting human evaluation with reasons. In Proceedings of the 12th International Conference on Natural Language Generation, pages 383-390, Tokyo, Japan. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 887
                },
                {
                    "x": 1214,
                    "y": 887
                },
                {
                    "x": 1214,
                    "y": 1070
                },
                {
                    "x": 290,
                    "y": 1070
                }
            ],
            "category": "paragraph",
            "html": "<p id='162' style='font-size:14px'>Aaron Moss and Leib Litman. After the bot scare:<br>Understanding what's been happening with data col-<br>lection on mturk and how to stop it. Accessed on<br>15.04.2021.</p>",
            "id": 162,
            "page": 13,
            "text": "Aaron Moss and Leib Litman. After the bot scare: Understanding what's been happening with data collection on mturk and how to stop it. Accessed on 15.04.2021."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 1109
                },
                {
                    "x": 1214,
                    "y": 1109
                },
                {
                    "x": 1214,
                    "y": 1340
                },
                {
                    "x": 290,
                    "y": 1340
                }
            ],
            "category": "paragraph",
            "html": "<p id='163' style='font-size:20px'>Elizabeth A. Necka, Stephanie Cacioppo, Greg J. Nor-<br>man, and John T. Cacioppo. 2016. Measuring<br>the prevalence of problematic respondent behaviors<br>among MTurk, campus, and community participants.<br>PLOS ONE, 11(6):e0157732.</p>",
            "id": 163,
            "page": 13,
            "text": "Elizabeth A. Necka, Stephanie Cacioppo, Greg J. Norman, and John T. Cacioppo. 2016. Measuring the prevalence of problematic respondent behaviors among MTurk, campus, and community participants. PLOS ONE, 11(6):e0157732."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 1377
                },
                {
                    "x": 1213,
                    "y": 1377
                },
                {
                    "x": 1213,
                    "y": 1655
                },
                {
                    "x": 291,
                    "y": 1655
                }
            ],
            "category": "paragraph",
            "html": "<p id='164' style='font-size:16px'>Kishore Papineni, Salim Roukos, Todd Ward, and Wei-<br>Jing Zhu. 2002. Bleu: a method for automatic eval-<br>uation of machine translation. In Proceedings of<br>the 40th annual meeting on association for compu-<br>tational linguistics, pages 311-318. Association for<br>Computational Linguistics.</p>",
            "id": 164,
            "page": 13,
            "text": "Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 311-318. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 1691
                },
                {
                    "x": 1214,
                    "y": 1691
                },
                {
                    "x": 1214,
                    "y": 2015
                },
                {
                    "x": 289,
                    "y": 2015
                }
            ],
            "category": "paragraph",
            "html": "<p id='165' style='font-size:22px'>Baolin Peng, Xiujun Li, Lihong Li, Jianfeng Gao,<br>Asli Celikyilmaz, Sungjin Lee, and Kam-Fai Wong.<br>2017. Composite task-completion dialogue policy<br>learning via hierarchical deep reinforcement learn-<br>ing. In Proceedings of the 2017 Conference on<br>Empirical Methods in Natural Language Processing.<br>Association for Computational Linguistics.</p>",
            "id": 165,
            "page": 13,
            "text": "Baolin Peng, Xiujun Li, Lihong Li, Jianfeng Gao, Asli Celikyilmaz, Sungjin Lee, and Kam-Fai Wong. 2017. Composite task-completion dialogue policy learning via hierarchical deep reinforcement learning. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 2051
                },
                {
                    "x": 1213,
                    "y": 2051
                },
                {
                    "x": 1213,
                    "y": 2283
                },
                {
                    "x": 291,
                    "y": 2283
                }
            ],
            "category": "paragraph",
            "html": "<p id='166' style='font-size:22px'>Nanyun Peng, Marjan Ghazvininejad, Jonathan May,<br>and Kevin Knight. 2018a. Towards controllable<br>story generation. In Proceedings of the First Work-<br>shop on Storytelling. Association for Computational<br>Linguistics.</p>",
            "id": 166,
            "page": 13,
            "text": "Nanyun Peng, Marjan Ghazvininejad, Jonathan May, and Kevin Knight. 2018a. Towards controllable story generation. In Proceedings of the First Workshop on Storytelling. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 2319
                },
                {
                    "x": 1215,
                    "y": 2319
                },
                {
                    "x": 1215,
                    "y": 2595
                },
                {
                    "x": 289,
                    "y": 2595
                }
            ],
            "category": "paragraph",
            "html": "<p id='167' style='font-size:20px'>Nanyun Peng, Marjan Ghazvininejad, Jonathan May,<br>and Kevin Knight. 2018b. Towards controllable<br>story generation. In Proceedings of the First Work-<br>shop on Storytelling, pages 43-49, New Orleans,<br>Louisiana. Association for Computational Linguis-<br>tics.</p>",
            "id": 167,
            "page": 13,
            "text": "Nanyun Peng, Marjan Ghazvininejad, Jonathan May, and Kevin Knight. 2018b. Towards controllable story generation. In Proceedings of the First Workshop on Storytelling, pages 43-49, New Orleans, Louisiana. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 2633
                },
                {
                    "x": 1214,
                    "y": 2633
                },
                {
                    "x": 1214,
                    "y": 2956
                },
                {
                    "x": 289,
                    "y": 2956
                }
            ],
            "category": "paragraph",
            "html": "<p id='168' style='font-size:22px'>Pouya Pezeshkpour, Liyan Chen, and Sameer Singh.<br>2018. Embedding multimodal relational data for<br>knowledge base completion. In Proceedings of the<br>2018 Conference on Empirical Methods in Natu-<br>ral Language Processing, pages 3208-3218, Brus-<br>sels, Belgium. Association for Computational Lin-<br>guistics.</p>",
            "id": 168,
            "page": 13,
            "text": "Pouya Pezeshkpour, Liyan Chen, and Sameer Singh. 2018. Embedding multimodal relational data for knowledge base completion. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3208-3218, Brussels, Belgium. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 2993
                },
                {
                    "x": 1213,
                    "y": 2993
                },
                {
                    "x": 1213,
                    "y": 3226
                },
                {
                    "x": 290,
                    "y": 3226
                }
            ],
            "category": "paragraph",
            "html": "<p id='169' style='font-size:18px'>Lianhui Qin, Antoine Bosselut, Ari Holtzman, Chandra<br>Bhagavatula, Elizabeth Clark, and Yejin Choi. 2019.<br>Counterfactual story reasoning and generation. In<br>Proceedings of the 2019 Conference on Empirical<br>Methods in Natural Language Processing and the</p>",
            "id": 169,
            "page": 13,
            "text": "Lianhui Qin, Antoine Bosselut, Ari Holtzman, Chandra Bhagavatula, Elizabeth Clark, and Yejin Choi. 2019. Counterfactual story reasoning and generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the"
        },
        {
            "bounding_box": [
                {
                    "x": 1316,
                    "y": 305
                },
                {
                    "x": 2198,
                    "y": 305
                },
                {
                    "x": 2198,
                    "y": 490
                },
                {
                    "x": 1316,
                    "y": 490
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='170' style='font-size:20px'>9th International Joint Conference on Natural Lan-<br>guage Processing (EMNLP-IJCNLP), pages 5043-<br>5053, Hong Kong, China. Association for Computa-<br>tional Linguistics.</p>",
            "id": 170,
            "page": 13,
            "text": "9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 50435053, Hong Kong, China. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 527
                },
                {
                    "x": 2197,
                    "y": 527
                },
                {
                    "x": 2197,
                    "y": 940
                },
                {
                    "x": 1269,
                    "y": 940
                }
            ],
            "category": "paragraph",
            "html": "<p id='171' style='font-size:18px'>Lianhui Qin, Vered Shwartz, Peter West, Chandra Bha-<br>gavatula, Jena D. Hwang, Ronan Le Bras, Antoine<br>Bosselut, and Yejin Choi. 2020. Back to the future:<br>Unsupervised backprop-based decoding for counter-<br>factual and abductive commonsense reasoning. In<br>Proceedings of the 2020 Conference on Empirical<br>Methods in Natural Language Processing (EMNLP),<br>pages 794-805, Online. Association for Computa-<br>tional Linguistics.</p>",
            "id": 171,
            "page": 13,
            "text": "Lianhui Qin, Vered Shwartz, Peter West, Chandra Bhagavatula, Jena D. Hwang, Ronan Le Bras, Antoine Bosselut, and Yejin Choi. 2020. Back to the future: Unsupervised backprop-based decoding for counterfactual and abductive commonsense reasoning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 794-805, Online. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 977
                },
                {
                    "x": 2196,
                    "y": 977
                },
                {
                    "x": 2196,
                    "y": 1117
                },
                {
                    "x": 1270,
                    "y": 1117
                }
            ],
            "category": "paragraph",
            "html": "<p id='172' style='font-size:18px'>Alec Radford, Jeffrey Wu, Rewon Child, David Luan,<br>Dario Amodei, and Ilya Sutskever. 2019. Language<br>models are unsupervised multitask learners.</p>",
            "id": 172,
            "page": 13,
            "text": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 1153
                },
                {
                    "x": 2198,
                    "y": 1153
                },
                {
                    "x": 2198,
                    "y": 1569
                },
                {
                    "x": 1271,
                    "y": 1569
                }
            ],
            "category": "paragraph",
            "html": "<p id='173' style='font-size:18px'>Sudha Rao and Joel Tetreault. 2018. Dear sir or<br>madam, may I introduce the GYAFC dataset: Cor-<br>pus, benchmarks and metrics for formality style<br>transfer. In Proceedings of the 2018 Conference of<br>the North American Chapter of the Association for<br>Computational Linguistics: Human Language Tech-<br>nologies, Volume 1 (Long Papers), pages 129-140,<br>New Orleans, Louisiana. Association for Computa-<br>tional Linguistics.</p>",
            "id": 173,
            "page": 13,
            "text": "Sudha Rao and Joel Tetreault. 2018. Dear sir or madam, may I introduce the GYAFC dataset: Corpus, benchmarks and metrics for formality style transfer. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 129-140, New Orleans, Louisiana. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 1605
                },
                {
                    "x": 2197,
                    "y": 1605
                },
                {
                    "x": 2197,
                    "y": 1930
                },
                {
                    "x": 1271,
                    "y": 1930
                }
            ],
            "category": "paragraph",
            "html": "<p id='174' style='font-size:20px'>Hannah Rashkin, Asli Celikyilmaz, Yejin Choi, and<br>Jianfeng Gao. 2020. PlotMachines: Outline-<br>conditioned generation with dynamic plot state<br>tracking. In Proceedings of the 2020 Conference<br>on Empirical Methods in Natural Language Process-<br>ing (EMNLP), pages 4274-4295, Online. Associa-<br>tion for Computational Linguistics.</p>",
            "id": 174,
            "page": 13,
            "text": "Hannah Rashkin, Asli Celikyilmaz, Yejin Choi, and Jianfeng Gao. 2020. PlotMachines: Outlineconditioned generation with dynamic plot state tracking. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4274-4295, Online. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 1964
                },
                {
                    "x": 2195,
                    "y": 1964
                },
                {
                    "x": 2195,
                    "y": 2101
                },
                {
                    "x": 1270,
                    "y": 2101
                }
            ],
            "category": "paragraph",
            "html": "<p id='175' style='font-size:16px'>Ehud Reiter. 2018. A structured review of the validity<br>of BLEU. Computational Linguistics, 44(3):393-<br>401.</p>",
            "id": 175,
            "page": 13,
            "text": "Ehud Reiter. 2018. A structured review of the validity of BLEU. Computational Linguistics, 44(3):393401."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 2143
                },
                {
                    "x": 2197,
                    "y": 2143
                },
                {
                    "x": 2197,
                    "y": 2417
                },
                {
                    "x": 1271,
                    "y": 2417
                }
            ],
            "category": "paragraph",
            "html": "<p id='176' style='font-size:14px'>Jonathan Robinson, Cheskie Rosenzweig, Aaron J.<br>Moss, and Leib Litman. 2019. Tapped out or<br>barely tapped? recommendations for how to har-<br>ness the vast and largely unused potential of the<br>mechanical turk participant pool. PLOS ONE,<br>14(12):e0226394.</p>",
            "id": 176,
            "page": 13,
            "text": "Jonathan Robinson, Cheskie Rosenzweig, Aaron J. Moss, and Leib Litman. 2019. Tapped out or barely tapped? recommendations for how to harness the vast and largely unused potential of the mechanical turk participant pool. PLOS ONE, 14(12):e0226394."
        },
        {
            "bounding_box": [
                {
                    "x": 1273,
                    "y": 2457
                },
                {
                    "x": 2196,
                    "y": 2457
                },
                {
                    "x": 2196,
                    "y": 2689
                },
                {
                    "x": 1273,
                    "y": 2689
                }
            ],
            "category": "paragraph",
            "html": "<p id='177' style='font-size:22px'>Abigail See, Aneesh Pappu, Rohun Saxena, Akhila<br>Yerukola, and Christopher D Manning. 2019. Do<br>massively pretrained language models make better<br>storytellers? In Conference on Computational Natu-<br>ral Language Learning.</p>",
            "id": 177,
            "page": 13,
            "text": "Abigail See, Aneesh Pappu, Rohun Saxena, Akhila Yerukola, and Christopher D Manning. 2019. Do massively pretrained language models make better storytellers? In Conference on Computational Natural Language Learning."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 2725
                },
                {
                    "x": 2197,
                    "y": 2725
                },
                {
                    "x": 2197,
                    "y": 3092
                },
                {
                    "x": 1271,
                    "y": 3092
                }
            ],
            "category": "paragraph",
            "html": "<p id='178' style='font-size:20px'>Dinghan Shen, Asli Celikyilmaz, Yizhe Zhang, Liqun<br>Chen, Xin Wang, Jianfeng Gao, and Lawrence Carin.<br>2019. Towards generating long and coherent text<br>with multi-level latent variable models. In Proceed-<br>ings of the 57th Annual Meeting of the Association<br>for Computational Linguistics, pages 2079-2089,<br>Florence, Italy. Association for Computational Lin-<br>guistics.</p>",
            "id": 178,
            "page": 13,
            "text": "Dinghan Shen, Asli Celikyilmaz, Yizhe Zhang, Liqun Chen, Xin Wang, Jianfeng Gao, and Lawrence Carin. 2019. Towards generating long and coherent text with multi-level latent variable models. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2079-2089, Florence, Italy. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 1270,
                    "y": 3130
                },
                {
                    "x": 2197,
                    "y": 3130
                },
                {
                    "x": 2197,
                    "y": 3226
                },
                {
                    "x": 1270,
                    "y": 3226
                }
            ],
            "category": "paragraph",
            "html": "<p id='179' style='font-size:20px'>Akhilesh Sudhakar, Bhargav Upadhyay, and Arjun Ma-<br>heswaran. 2019. \"transforming\" delete, retrieve,</p>",
            "id": 179,
            "page": 13,
            "text": "Akhilesh Sudhakar, Bhargav Upadhyay, and Arjun Maheswaran. 2019. \"transforming\" delete, retrieve,"
        },
        {
            "bounding_box": [
                {
                    "x": 331,
                    "y": 306
                },
                {
                    "x": 1215,
                    "y": 306
                },
                {
                    "x": 1215,
                    "y": 627
                },
                {
                    "x": 331,
                    "y": 627
                }
            ],
            "category": "paragraph",
            "html": "<p id='180' style='font-size:16px'>generate approach for controlled text style transfer.<br>In Proceedings of the 2019 Conference on Empirical<br>Methods in Natural Language Processing and the<br>9th International Joint Conference on Natural Lan-<br>guage Processing (EMNLP-IJCNLP), pages 3269-<br>3279, Hong Kong, China. Association for Computa-<br>tional Linguistics.</p>",
            "id": 180,
            "page": 14,
            "text": "generate approach for controlled text style transfer. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 32693279, Hong Kong, China. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 664
                },
                {
                    "x": 1214,
                    "y": 664
                },
                {
                    "x": 1214,
                    "y": 847
                },
                {
                    "x": 291,
                    "y": 847
                }
            ],
            "category": "paragraph",
            "html": "<p id='181' style='font-size:14px'>Antonio Toral, Sheila Castilho, Ke Hu, and Andy<br>Way. 2018. Attaining the unattainable? reassess-<br>ing claims of human parity in neural machine trans-<br>lation.</p>",
            "id": 181,
            "page": 14,
            "text": "Antonio Toral, Sheila Castilho, Ke Hu, and Andy Way. 2018. Attaining the unattainable? reassessing claims of human parity in neural machine translation."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 885
                },
                {
                    "x": 1213,
                    "y": 885
                },
                {
                    "x": 1213,
                    "y": 1161
                },
                {
                    "x": 289,
                    "y": 1161
                }
            ],
            "category": "paragraph",
            "html": "<p id='182' style='font-size:18px'>Lifu Tu, Xiaoan Ding, Dong Yu, and Kevin Gimpel.<br>2019. Generating diverse story continuations with<br>controllable semantics. In Proceedings of the 3rd<br>Workshop on Neural Generation and Translation,<br>pages 44-58, Hong Kong. Association for Compu-<br>tational Linguistics.</p>",
            "id": 182,
            "page": 14,
            "text": "Lifu Tu, Xiaoan Ding, Dong Yu, and Kevin Gimpel. 2019. Generating diverse story continuations with controllable semantics. In Proceedings of the 3rd Workshop on Neural Generation and Translation, pages 44-58, Hong Kong. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 1198
                },
                {
                    "x": 1214,
                    "y": 1198
                },
                {
                    "x": 1214,
                    "y": 1428
                },
                {
                    "x": 288,
                    "y": 1428
                }
            ],
            "category": "paragraph",
            "html": "<p id='183' style='font-size:14px'>Chris van der Lee, Albert Gatt, Emiel van Miltenburg,<br>and Emiel Krahmer. 2021. Human evaluation of au-<br>tomatically generated text: Current trends and best<br>practice guidelines. Computer Speech & Language,<br>67:101151.</p>",
            "id": 183,
            "page": 14,
            "text": "Chris van der Lee, Albert Gatt, Emiel van Miltenburg, and Emiel Krahmer. 2021. Human evaluation of automatically generated text: Current trends and best practice guidelines. Computer Speech & Language, 67:101151."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 1466
                },
                {
                    "x": 1214,
                    "y": 1466
                },
                {
                    "x": 1214,
                    "y": 1605
                },
                {
                    "x": 290,
                    "y": 1605
                }
            ],
            "category": "paragraph",
            "html": "<p id='184' style='font-size:16px'>K. Wessling, J. Huber, and 0. Netzer. 2017. Mturk<br>character misrepresentation: Assessment and solu-<br>tions. Journal of Consumer Research, 44:211-230.</p>",
            "id": 184,
            "page": 14,
            "text": "K. Wessling, J. Huber, and 0. Netzer. 2017. Mturk character misrepresentation: Assessment and solutions. Journal of Consumer Research, 44:211-230."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 1641
                },
                {
                    "x": 1213,
                    "y": 1641
                },
                {
                    "x": 1213,
                    "y": 2194
                },
                {
                    "x": 288,
                    "y": 2194
                }
            ],
            "category": "paragraph",
            "html": "<p id='185' style='font-size:18px'>Thomas Wolf, Lysandre Debut, Victor Sanh, Julien<br>Chaumond, Clement Delangue, Anthony Moi, Pier-<br>ric Cistac, Tim Rault, Remi Louf, Morgan Funtow-<br>icz, Joe Davison, Sam Shleifer, Patrick von Platen,<br>Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,<br>Teven Le Scao, Sylvain Gugger, Mariama Drame,<br>Quentin Lhoest, and Alexander Rush. 2020. Trans-<br>formers: State-of-the-art natural language process-<br>ing. In Proceedings of the 2020 Conference on Em-<br>pirical Methods in Natural Language Processing:<br>System Demonstrations, pages 38-45, Online. Asso-<br>ciation for Computational Linguistics.</p>",
            "id": 185,
            "page": 14,
            "text": "Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38-45, Online. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 2228
                },
                {
                    "x": 1215,
                    "y": 2228
                },
                {
                    "x": 1215,
                    "y": 2645
                },
                {
                    "x": 289,
                    "y": 2645
                }
            ],
            "category": "paragraph",
            "html": "<p id='186' style='font-size:18px'>Peng Xu, Mostofa Patwary, Mohammad Shoeybi, Raul<br>Puri, Pascale Fung, Anima Anandkumar, and Bryan<br>Catanzaro. 2020. MEGATRON-CNTRL: Control-<br>lable story generation with external knowledge us-<br>ing large-scale language models. In Proceedings of<br>the 2020 Conference on Empirical Methods in Nat-<br>ural Language Processing (EMNLP), pages 2831-<br>2845, Online. Association for Computational Lin-<br>guistics.</p>",
            "id": 186,
            "page": 14,
            "text": "Peng Xu, Mostofa Patwary, Mohammad Shoeybi, Raul Puri, Pascale Fung, Anima Anandkumar, and Bryan Catanzaro. 2020. MEGATRON-CNTRL: Controllable story generation with external knowledge using large-scale language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 28312845, Online. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 290,
                    "y": 2679
                },
                {
                    "x": 1215,
                    "y": 2679
                },
                {
                    "x": 1215,
                    "y": 3048
                },
                {
                    "x": 290,
                    "y": 3048
                }
            ],
            "category": "paragraph",
            "html": "<p id='187' style='font-size:20px'>Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen<br>Tan, Kun Xiong, Ming Li, and Jimmy Lin. 2019.<br>End-to-end open-domain question answering with<br>BERTserini. In Proceedings of the 2019 Confer-<br>ence of the North American Chapter of the Asso-<br>ciation for Computational Linguistics (Demonstra-<br>tions), pages 72-77, Minneapolis, Minnesota. Asso-<br>ciation for Computational Linguistics.</p>",
            "id": 187,
            "page": 14,
            "text": "Wei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen Tan, Kun Xiong, Ming Li, and Jimmy Lin. 2019. End-to-end open-domain question answering with BERTserini. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pages 72-77, Minneapolis, Minnesota. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 3083
                },
                {
                    "x": 1212,
                    "y": 3083
                },
                {
                    "x": 1212,
                    "y": 3228
                },
                {
                    "x": 288,
                    "y": 3228
                }
            ],
            "category": "paragraph",
            "html": "<p id='188' style='font-size:16px'>Zhiwei Yu, Jiwei Tan, and Xiaojun Wan. 2018. A<br>neural approach to pun generation. In Proceedings<br>of the 56th Annual Meeting of the Association for</p>",
            "id": 188,
            "page": 14,
            "text": "Zhiwei Yu, Jiwei Tan, and Xiaojun Wan. 2018. A neural approach to pun generation. In Proceedings of the 56th Annual Meeting of the Association for"
        },
        {
            "bounding_box": [
                {
                    "x": 1316,
                    "y": 304
                },
                {
                    "x": 2199,
                    "y": 304
                },
                {
                    "x": 2199,
                    "y": 443
                },
                {
                    "x": 1316,
                    "y": 443
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='189' style='font-size:20px'>Computational Linguistics (Volume 1: Long Papers),<br>pages 1650-1660, Melbourne, Australia. Associa-<br>tion for Computational Linguistics.</p>",
            "id": 189,
            "page": 14,
            "text": "Computational Linguistics (Volume 1: Long Papers), pages 1650-1660, Melbourne, Australia. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 477
                },
                {
                    "x": 2199,
                    "y": 477
                },
                {
                    "x": 2199,
                    "y": 892
                },
                {
                    "x": 1268,
                    "y": 892
                }
            ],
            "category": "paragraph",
            "html": "<p id='190' style='font-size:16px'>Zhiwei Yu and Xiaojun Wan. 2019. How to avoid sen-<br>tences spelling boring? towards a neural approach<br>to unsupervised metaphor generation. In Proceed-<br>ings of the 2019 Conference of the North American<br>Chapter of the Association for Computational Lin-<br>guistics: Human Language Technologies, Volume 1<br>(Long and Short Papers), pages 861-871, Minneapo-<br>lis, Minnesota. Association for Computational Lin-<br>guistics.</p>",
            "id": 190,
            "page": 14,
            "text": "Zhiwei Yu and Xiaojun Wan. 2019. How to avoid sentences spelling boring? towards a neural approach to unsupervised metaphor generation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 861-871, Minneapolis, Minnesota. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 926
                },
                {
                    "x": 2198,
                    "y": 926
                },
                {
                    "x": 2198,
                    "y": 1204
                },
                {
                    "x": 1268,
                    "y": 1204
                }
            ],
            "category": "paragraph",
            "html": "<p id='191' style='font-size:18px'>Zhiwei Yu, Hongyu Zang, and Xiaojun Wan. 2020.<br>Routing enforced generative model for recipe gen-<br>eration. In Proceedings of the 2020 Conference on<br>Empirical Methods in Natural Language Process-<br>ing (EMNLP), pages 3797-3806, Online. Associa-<br>tion for Computational Linguistics.</p>",
            "id": 191,
            "page": 14,
            "text": "Zhiwei Yu, Hongyu Zang, and Xiaojun Wan. 2020. Routing enforced generative model for recipe generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3797-3806, Online. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 1238
                },
                {
                    "x": 2197,
                    "y": 1238
                },
                {
                    "x": 2197,
                    "y": 1515
                },
                {
                    "x": 1271,
                    "y": 1515
                }
            ],
            "category": "paragraph",
            "html": "<p id='192' style='font-size:18px'>Hongyu Zang, Zhiwei Yu, and Xiaojun Wan. 2019.<br>Automated chess commentator powered by neural<br>chess engine. In Proceedings of the 57th Annual<br>Meeting of the Association for Computational Lin-<br>guistics, pages 5952-5961, Florence, Italy. Associa-<br>tion for Computational Linguistics.</p>",
            "id": 192,
            "page": 14,
            "text": "Hongyu Zang, Zhiwei Yu, and Xiaojun Wan. 2019. Automated chess commentator powered by neural chess engine. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5952-5961, Florence, Italy. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 1548
                },
                {
                    "x": 2196,
                    "y": 1548
                },
                {
                    "x": 2196,
                    "y": 1828
                },
                {
                    "x": 1271,
                    "y": 1828
                }
            ],
            "category": "paragraph",
            "html": "<p id='193' style='font-size:16px'>Rui Zhang and Joel Tetreault. 2019. This email could<br>save your life: Introducing the task of email subject<br>line generation. In Proceedings of the 57th Annual<br>Meeting of the Association for Computational Lin-<br>guistics, pages 446-456, Florence, Italy. Association<br>for Computational Linguistics.</p>",
            "id": 193,
            "page": 14,
            "text": "Rui Zhang and Joel Tetreault. 2019. This email could save your life: Introducing the task of email subject line generation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 446-456, Florence, Italy. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 1271,
                    "y": 1863
                },
                {
                    "x": 2198,
                    "y": 1863
                },
                {
                    "x": 2198,
                    "y": 2000
                },
                {
                    "x": 1271,
                    "y": 2000
                }
            ],
            "category": "paragraph",
            "html": "<p id='194' style='font-size:16px'>A. Celikyilmaz, Elizabeth Clark, and Jianfeng Gao.<br>2020. Evaluation of text generation: A survey.<br>ArXiv, abs/2006.14799.</p>",
            "id": 194,
            "page": 14,
            "text": "A. Celikyilmaz, Elizabeth Clark, and Jianfeng Gao. 2020. Evaluation of text generation: A survey. ArXiv, abs/2006.14799."
        },
        {
            "bounding_box": [
                {
                    "x": 311,
                    "y": 293
                },
                {
                    "x": 597,
                    "y": 293
                },
                {
                    "x": 597,
                    "y": 349
                },
                {
                    "x": 311,
                    "y": 349
                }
            ],
            "category": "caption",
            "html": "<caption id='195' style='font-size:22px'>Qualifications</caption>",
            "id": 195,
            "page": 15,
            "text": "Qualifications"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 380
                },
                {
                    "x": 2122,
                    "y": 380
                },
                {
                    "x": 2122,
                    "y": 1454
                },
                {
                    "x": 287,
                    "y": 1454
                }
            ],
            "category": "table",
            "html": "<table id='196' style='font-size:18px'><tr><td>Approval Rate Accepted HITs English Other</td><td>90% (4) 500 (1) Resident (6) Complete all ratings (1)</td><td>95% (3) 1000 (5) Native Speaker (4)</td><td>97% (1) 5000 (1) Self-reported (1) Passed quality check (1)</td><td>99% (1)</td></tr><tr><td>Payment</td><td colspan=\"2\">$0.05 - $2 per HIT (8)</td><td colspan=\"2\">$12 - $20 per hour (2)</td></tr><tr><td>Number of Items</td><td>< 100 items (9)</td><td>100 items (14)</td><td>> 100 items (20)</td><td>not reported (5)</td></tr><tr><td>Number of Raters</td><td>< 3 raters (3)</td><td>3 raters (16)</td><td>5 raters (13)</td><td>> 5 raters (3)</td></tr><tr><td>Likert Scale</td><td>3-point (4)</td><td>4-point (3)</td><td>5-point (24)</td><td>6-point (1)</td></tr><tr><td>Ranking Task</td><td>two texts (19)</td><td>more texts (4)</td><td></td><td></td></tr><tr><td>Text Length</td><td>sentence (28)</td><td>paragraph (21)</td><td></td><td></td></tr></table>",
            "id": 196,
            "page": 15,
            "text": "Approval Rate Accepted HITs English Other 90% (4) 500 (1) Resident (6) Complete all ratings (1) 95% (3) 1000 (5) Native Speaker (4) 97% (1) 5000 (1) Self-reported (1) Passed quality check (1) 99% (1)  Payment $0.05 - $2 per HIT (8) $12 - $20 per hour (2)  Number of Items < 100 items (9) 100 items (14) > 100 items (20) not reported (5)  Number of Raters < 3 raters (3) 3 raters (16) 5 raters (13) > 5 raters (3)  Likert Scale 3-point (4) 4-point (3) 5-point (24) 6-point (1)  Ranking Task two texts (19) more texts (4)    Text Length sentence (28) paragraph (21)"
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 1489
                },
                {
                    "x": 2194,
                    "y": 1489
                },
                {
                    "x": 2194,
                    "y": 1590
                },
                {
                    "x": 286,
                    "y": 1590
                }
            ],
            "category": "paragraph",
            "html": "<p id='197' style='font-size:16px'>Table A1: Results of the survey from Section 2. Numbers in brackets refer to the number of papers/experiments<br>which employed the given measure.</p>",
            "id": 197,
            "page": 15,
            "text": "Table A1: Results of the survey from Section 2. Numbers in brackets refer to the number of papers/experiments which employed the given measure."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 1672
                },
                {
                    "x": 2006,
                    "y": 1672
                },
                {
                    "x": 2006,
                    "y": 1735
                },
                {
                    "x": 1269,
                    "y": 1735
                }
            ],
            "category": "paragraph",
            "html": "<p id='198' style='font-size:20px'>B Collecting Ratings on Upwork</p>",
            "id": 198,
            "page": 15,
            "text": "B Collecting Ratings on Upwork"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 1771
                },
                {
                    "x": 1027,
                    "y": 1771
                },
                {
                    "x": 1027,
                    "y": 1832
                },
                {
                    "x": 287,
                    "y": 1832
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='199' style='font-size:20px'>A Questions Used for the Survey</p>",
            "id": 199,
            "page": 15,
            "text": "A Questions Used for the Survey"
        },
        {
            "bounding_box": [
                {
                    "x": 285,
                    "y": 1861
                },
                {
                    "x": 1213,
                    "y": 1861
                },
                {
                    "x": 1213,
                    "y": 1971
                },
                {
                    "x": 285,
                    "y": 1971
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='200' style='font-size:18px'>The paper survey described in Section 2 included<br>the following questions:</p>",
            "id": 200,
            "page": 15,
            "text": "The paper survey described in Section 2 included the following questions:"
        },
        {
            "bounding_box": [
                {
                    "x": 338,
                    "y": 1995
                },
                {
                    "x": 1206,
                    "y": 1995
                },
                {
                    "x": 1206,
                    "y": 3209
                },
                {
                    "x": 338,
                    "y": 3209
                }
            ],
            "category": "paragraph",
            "html": "<p id='201' style='font-size:14px'>● type of the task<br>● length of rated text<br>● rated attributes<br>● rating scale<br>● labels used for ratings<br>● definitions of each attribute<br>● instructions provided to raters<br>● qualifications and quality control measures<br>employed<br>● number of rated items<br>● number of rated systems<br>● number of raters per item<br>· inclusion of ground truth<br>● monetary compensation</p>",
            "id": 201,
            "page": 15,
            "text": "● type of the task ● length of rated text ● rated attributes ● rating scale ● labels used for ratings ● definitions of each attribute ● instructions provided to raters ● qualifications and quality control measures employed ● number of rated items ● number of rated systems ● number of raters per item · inclusion of ground truth ● monetary compensation"
        },
        {
            "bounding_box": [
                {
                    "x": 1268,
                    "y": 1850
                },
                {
                    "x": 2198,
                    "y": 1850
                },
                {
                    "x": 2198,
                    "y": 2924
                },
                {
                    "x": 1268,
                    "y": 2924
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='202' style='font-size:16px'>We also hired three teachers using the freelancing<br>platform Upwork1. The teachers were paid $175 to<br>evaluate the same 200 human-written stories and<br>200 GPT-2 generated stories. They were asked to<br>perform the ratings on the AMT platform in order<br>to use the same interface as workers on AMT. Sim-<br>ilarly to the teachers recruited from the authors'<br>personal network, the teachers recruited on Up-<br>work were asked to rate the 200 human-written<br>stories first and then, after a few days break, pro-<br>vide the ratings for the GPT-2 generated stories.<br>Furthermore, Upwork teachers also held TEFL,2<br>TESOL,3 or CELTA 13 certificates. Table A3 shows<br>mean ratings and agreement for the data collected<br>on Upwork. Similarly to the results described in<br>Section 4 and summarized in Table A5, the average<br>scores for coherence, relevance, and likability are<br>higher for the human-written stories than for the<br>GPT-2 generated stories (see Table A4).</p>",
            "id": 202,
            "page": 15,
            "text": "We also hired three teachers using the freelancing platform Upwork1. The teachers were paid $175 to evaluate the same 200 human-written stories and 200 GPT-2 generated stories. They were asked to perform the ratings on the AMT platform in order to use the same interface as workers on AMT. Similarly to the teachers recruited from the authors' personal network, the teachers recruited on Upwork were asked to rate the 200 human-written stories first and then, after a few days break, provide the ratings for the GPT-2 generated stories. Furthermore, Upwork teachers also held TEFL,2 TESOL,3 or CELTA 13 certificates. Table A3 shows mean ratings and agreement for the data collected on Upwork. Similarly to the results described in Section 4 and summarized in Table A5, the average scores for coherence, relevance, and likability are higher for the human-written stories than for the GPT-2 generated stories (see Table A4)."
        },
        {
            "bounding_box": [
                {
                    "x": 1319,
                    "y": 3087
                },
                {
                    "x": 2099,
                    "y": 3087
                },
                {
                    "x": 2099,
                    "y": 3227
                },
                {
                    "x": 1319,
                    "y": 3227
                }
            ],
            "category": "paragraph",
            "html": "<p id='203' style='font-size:14px'>1https : / / www · upwork . com/<br>2Teaching English as a Foreign Language.<br>3Teaching English to Speakers of Other Languages</p>",
            "id": 203,
            "page": 15,
            "text": "1https : / / www · upwork . com/ 2Teaching English as a Foreign Language. 3Teaching English to Speakers of Other Languages"
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 886
                },
                {
                    "x": 2185,
                    "y": 886
                },
                {
                    "x": 2185,
                    "y": 2529
                },
                {
                    "x": 293,
                    "y": 2529
                }
            ],
            "category": "table",
            "html": "<table id='204' style='font-size:14px'><tr><td>Authors</td><td>Year</td><td>Title</td><td>Venue</td></tr><tr><td>Akoury et al.</td><td>2020</td><td>STORIUM: A Dataset and Evaluation Platform for Machine-in-the-Loop Story Generation</td><td>EMNLP</td></tr><tr><td>Alshomary et al.</td><td>2020</td><td>Target Inference in Argument Conclusion Generation</td><td>ACL</td></tr><tr><td>Bosselut et al.</td><td>2018</td><td>Discourse-Aware Neural Rewards for Coherent Text Generation</td><td>EMNLP</td></tr><tr><td>Brahman and Chaturvedi</td><td>2020</td><td>Modeling Protagonist Emotions for Emotion-Aware Storytelling</td><td>EMNLP</td></tr><tr><td>Chakrabarty et al.</td><td>2020b</td><td>Generating similes effortlessly like a Pro: A Style Transfer Approach for Simile Generation</td><td>EMNLP</td></tr><tr><td>Chakrabarty et al.</td><td>2020a</td><td>R3: Reverse, Retrieve, and Rank for Sarcasm Generation with Commonsense Knowledge</td><td>ACL</td></tr><tr><td>Clark et al.</td><td>2018</td><td>Neural Text Generation in Stories Using Entity Representations as Context</td><td>NAACL</td></tr><tr><td>Donahue et al.</td><td>2020</td><td>Enabling Language Models to Fill in the Blanks</td><td>ACL</td></tr><tr><td>Fan et al.</td><td>2018</td><td>Hierarchical Neural Story Generation</td><td>ACL</td></tr><tr><td>Fang et al.</td><td>2020</td><td>Video2Commonsense: Generating Commonsense Descriptions to Enrich Video Captioning</td><td>EMNLP</td></tr><tr><td>Goldfarb- Tarrant et al.</td><td>2020</td><td>Content Planning for Neural Story Generation with Aristotelian Rescoring</td><td>EMNLP</td></tr><tr><td>Gorinski and Lapata</td><td>2018</td><td>What's this Movie about? A Joint Neural Network Architecture for Movie Content Analysis</td><td>NAACL</td></tr><tr><td>Goyal and Durrett</td><td>2020</td><td>Neural Syntactic Preordering for Controlled Paraphrase Generation</td><td>ACL</td></tr><tr><td>He et al.</td><td>2019</td><td>Pun Generation with Surprise</td><td>NAACL</td></tr><tr><td>Hegel et al.</td><td>2020</td><td>Substance over Style: Document-Level Targeted Content Transfer</td><td>EMNLP</td></tr><tr><td>Holtzman et al.</td><td>2018</td><td>Learning to Write with Cooperative Discriminators</td><td>ACL</td></tr><tr><td>Hsu et al.</td><td>2019</td><td>Visual Story Post-Editing</td><td>ACL</td></tr><tr><td>Ippolito et al.</td><td>2019</td><td>Unsupervised Hierarchical Story Infilling</td><td>NAACL</td></tr><tr><td>Jiang et al.</td><td>2020</td><td>Neural CRF Model for Sentence Alignment in Text Simplification</td><td>ACL</td></tr><tr><td>Krishna et al.</td><td>2020</td><td>Reformulating Unsupervised Style Transfer as Paraphrase Generation</td><td>EMNLP</td></tr><tr><td>Kriz et al.</td><td>2019</td><td>Complexity-Weighted Loss and Diverse Reranking for Sentence Simplification</td><td>NAACL</td></tr><tr><td>Li et al.</td><td>2018</td><td>Delete, Retrieve, Generate: A Simple Approach to Sentiment and Style Transfer</td><td>NAACL</td></tr><tr><td>Lin et al.</td><td>2020</td><td>Learning to Generate Multiple Style Transfer Outputs for an Input Sentence</td><td>ACL</td></tr><tr><td>Liu et al.</td><td>2019</td><td>Towards Explainable NLP: A Generative Explanation Framework for Text Classification</td><td>ACL</td></tr><tr><td>Mallinson et al.</td><td>2020</td><td>Zero-Shot Crosslingual Sentence Simplification</td><td>EMNLP</td></tr><tr><td>Martins et al.</td><td>2020</td><td>Sparse Text Generation</td><td>EMNLP</td></tr><tr><td>Mir et al.</td><td>2019</td><td>Evaluating Style Transfer for Text</td><td>NAACL</td></tr><tr><td>Peng et al.</td><td>2018b</td><td>Towards Controllable Story Generation</td><td>NAACL</td></tr><tr><td>Pezeshkpour et al.</td><td>2018</td><td>Embedding Multimodal Relational Data for Knowledge Base Completion</td><td>EMNLP</td></tr><tr><td>Qin et al.</td><td>2020</td><td>Embedding Multimodal Relational Data for Knowledge Base Completion</td><td>EMNLP</td></tr><tr><td>Qin et al.</td><td>2019</td><td>Counterfactual Story Reasoning and Generation</td><td>EMNLP</td></tr><tr><td>Rao and Tetreault</td><td>2018</td><td>Dear Sir or Madam, May I Introduce the GYAFC Dataset: Corpus, Benchmarks and Metrics for Formality Style Transfer</td><td>NAACL</td></tr><tr><td>Rashkin et al.</td><td>2020</td><td>PLOT MACHINES: Outline-Conditioned Generation with Dynamic Plot State Tracking</td><td>EMNLP</td></tr><tr><td>Shen et al.</td><td>2019</td><td>Towards Generating Long and Coherent Text with Multi-Level Latent Variable Models</td><td>ACL</td></tr><tr><td>Sudhakar et al.</td><td>2019</td><td>\"Transforming\" Delete, Retrieve, Generate Approach for Controlled Text Style Transfer</td><td>EMNLP</td></tr><tr><td>Tu et al.</td><td>2019</td><td>Generating Diverse Story Continuations with Controllable Semantics</td><td>EMNLP</td></tr><tr><td>Tu et al.</td><td>2019</td><td>Can Humor Prediction Datasets be used for Humor Generation? Humorous Headline Generation via Style Transfer</td><td>ACL</td></tr><tr><td>Xu et al.</td><td>2020</td><td>MEGATRON - CNTRL : Controllable Story Generation with External Knowledge Using Large-Scale Language Models</td><td>EMNLP</td></tr><tr><td></td><td>2019 2019</td><td>An End-to-End Generative Architecture for Paraphrase Generation The Steep Road to Happily Ever After: An Analysis of Current Visual Storytelling Models</td><td>EMNLP</td></tr><tr><td></td><td></td><td></td><td>NAACL</td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td>Yang et al. Modi and Parde</td><td>2019 2018</td><td>How to Avoid Sentences Spelling Boring? Towards a Neural Approach to Unsupervised Metaphor Generation A Neural Approach to Pun Generation</td><td>NAACL ACL</td></tr><tr><td>Yu and Wan Yu et al.</td><td>2020</td><td>Routing Enforced Generative Model for Recipe Generation</td><td>EMNLP</td></tr><tr><td>Yu et al. Zhang and Tetreault</td><td>2019</td><td>This Email Could Save Your Life: Introducing the Task of Email Subject Line Generation</td><td>ACL</td></tr><tr><td>Zang et al.</td><td>2019</td><td>Automated Chess Commentator Powered by Neural Chess Engine</td><td>ACL</td></tr></table>",
            "id": 204,
            "page": 16,
            "text": "Authors Year Title Venue  Akoury  2020 STORIUM: A Dataset and Evaluation Platform for Machine-in-the-Loop Story Generation EMNLP  Alshomary  2020 Target Inference in Argument Conclusion Generation ACL  Bosselut  2018 Discourse-Aware Neural Rewards for Coherent Text Generation EMNLP  Brahman and Chaturvedi 2020 Modeling Protagonist Emotions for Emotion-Aware Storytelling EMNLP  Chakrabarty  2020b Generating similes effortlessly like a Pro: A Style Transfer Approach for Simile Generation EMNLP  Chakrabarty  2020a R3: Reverse, Retrieve, and Rank for Sarcasm Generation with Commonsense Knowledge ACL  Clark  2018 Neural Text Generation in Stories Using Entity Representations as Context NAACL  Donahue  2020 Enabling Language Models to Fill in the Blanks ACL  Fan  2018 Hierarchical Neural Story Generation ACL  Fang  2020 Video2Commonsense: Generating Commonsense Descriptions to Enrich Video Captioning EMNLP  Goldfarb- Tarrant  2020 Content Planning for Neural Story Generation with Aristotelian Rescoring EMNLP  Gorinski and Lapata 2018 What's this Movie about? A Joint Neural Network Architecture for Movie Content Analysis NAACL  Goyal and Durrett 2020 Neural Syntactic Preordering for Controlled Paraphrase Generation ACL  He  2019 Pun Generation with Surprise NAACL  Hegel  2020 Substance over Style: Document-Level Targeted Content Transfer EMNLP  Holtzman  2018 Learning to Write with Cooperative Discriminators ACL  Hsu  2019 Visual Story Post-Editing ACL  Ippolito  2019 Unsupervised Hierarchical Story Infilling NAACL  Jiang  2020 Neural CRF Model for Sentence Alignment in Text Simplification ACL  Krishna  2020 Reformulating Unsupervised Style Transfer as Paraphrase Generation EMNLP  Kriz  2019 Complexity-Weighted Loss and Diverse Reranking for Sentence Simplification NAACL  Li  2018 Delete, Retrieve, Generate: A Simple Approach to Sentiment and Style Transfer NAACL  Lin  2020 Learning to Generate Multiple Style Transfer Outputs for an Input Sentence ACL  Liu  2019 Towards Explainable NLP: A Generative Explanation Framework for Text Classification ACL  Mallinson  2020 Zero-Shot Crosslingual Sentence Simplification EMNLP  Martins  2020 Sparse Text Generation EMNLP  Mir  2019 Evaluating Style Transfer for Text NAACL  Peng  2018b Towards Controllable Story Generation NAACL  Pezeshkpour  2018 Embedding Multimodal Relational Data for Knowledge Base Completion EMNLP  Qin  2020 Embedding Multimodal Relational Data for Knowledge Base Completion EMNLP  Qin  2019 Counterfactual Story Reasoning and Generation EMNLP  Rao and Tetreault 2018 Dear Sir or Madam, May I Introduce the GYAFC Dataset: Corpus, Benchmarks and Metrics for Formality Style Transfer NAACL  Rashkin  2020 PLOT MACHINES: Outline-Conditioned Generation with Dynamic Plot State Tracking EMNLP  Shen  2019 Towards Generating Long and Coherent Text with Multi-Level Latent Variable Models ACL  Sudhakar  2019 \"Transforming\" Delete, Retrieve, Generate Approach for Controlled Text Style Transfer EMNLP  Tu  2019 Generating Diverse Story Continuations with Controllable Semantics EMNLP  Tu  2019 Can Humor Prediction Datasets be used for Humor Generation? Humorous Headline Generation via Style Transfer ACL  Xu  2020 MEGATRON - CNTRL : Controllable Story Generation with External Knowledge Using Large-Scale Language Models EMNLP   2019 2019 An End-to-End Generative Architecture for Paraphrase Generation The Steep Road to Happily Ever After: An Analysis of Current Visual Storytelling Models EMNLP     NAACL       Yang  Modi and Parde 2019 2018 How to Avoid Sentences Spelling Boring? Towards a Neural Approach to Unsupervised Metaphor Generation A Neural Approach to Pun Generation NAACL ACL  Yu and Wan Yu  2020 Routing Enforced Generative Model for Recipe Generation EMNLP  Yu  Zhang and Tetreault 2019 This Email Could Save Your Life: Introducing the Task of Email Subject Line Generation ACL  Zang  2019 Automated Chess Commentator Powered by Neural Chess Engine"
        },
        {
            "bounding_box": [
                {
                    "x": 943,
                    "y": 2554
                },
                {
                    "x": 1533,
                    "y": 2554
                },
                {
                    "x": 1533,
                    "y": 2608
                },
                {
                    "x": 943,
                    "y": 2608
                }
            ],
            "category": "caption",
            "html": "<br><caption id='205' style='font-size:18px'>Table A2: List of Surveyed Papers.</caption>",
            "id": 205,
            "page": 16,
            "text": "Table A2: List of Surveyed Papers."
        },
        {
            "bounding_box": [
                {
                    "x": 324,
                    "y": 292
                },
                {
                    "x": 2155,
                    "y": 292
                },
                {
                    "x": 2155,
                    "y": 521
                },
                {
                    "x": 324,
                    "y": 521
                }
            ],
            "category": "table",
            "html": "<table id='206' style='font-size:20px'><tr><td rowspan=\"2\">Experiment description</td><td colspan=\"2\">Grammar</td><td colspan=\"2\">Coherence</td><td colspan=\"2\">Relevance</td><td colspan=\"2\">Likability</td></tr><tr><td>MeansTD</td><td>IAA%</td><td>MeansTD</td><td>IAA%</td><td>MeansTD</td><td>IAA%</td><td>MeansTD</td><td>IAA%</td></tr><tr><td>Reference</td><td>4.280.78</td><td>0.2926</td><td>4.550.66</td><td>0.1130.5</td><td>4.250.88</td><td>0.4926.4</td><td>4.021.16</td><td>0.0114.5</td></tr><tr><td>GPT-2</td><td>4.250.82</td><td>0.1219</td><td>3.990.98</td><td>0.0612.5</td><td>3.021.53</td><td>0.255</td><td>3.681.15</td><td>0.0510.5</td></tr></table>",
            "id": 206,
            "page": 17,
            "text": "Experiment description Grammar Coherence Relevance Likability  MeansTD IAA% MeansTD IAA% MeansTD IAA% MeansTD IAA%  Reference 4.280.78 0.2926 4.550.66 0.1130.5 4.250.88 0.4926.4 4.021.16 0.0114.5  GPT-2 4.250.82 0.1219 3.990.98 0.0612.5 3.021.53 0.255 3.681.15"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 552
                },
                {
                    "x": 2195,
                    "y": 552
                },
                {
                    "x": 2195,
                    "y": 706
                },
                {
                    "x": 287,
                    "y": 706
                }
            ],
            "category": "caption",
            "html": "<caption id='207' style='font-size:14px'>Table A3: Ratings of human-written reference stories and GPT-2 generated stories collected on Upwork. Inter-<br>annotator agreement (IAA) between the three raters is measured with Krippendorff's a well as the percentage of<br>stories for which all three raters exactly agreed on a rating (the latter is subscripted)</caption>",
            "id": 207,
            "page": 17,
            "text": "Table A3: Ratings of human-written reference stories and GPT-2 generated stories collected on Upwork. Interannotator agreement (IAA) between the three raters is measured with Krippendorff's a well as the percentage of stories for which all three raters exactly agreed on a rating (the latter is subscripted)"
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 746
                },
                {
                    "x": 2190,
                    "y": 746
                },
                {
                    "x": 2190,
                    "y": 972
                },
                {
                    "x": 292,
                    "y": 972
                }
            ],
            "category": "table",
            "html": "<table id='208' style='font-size:14px'><tr><td></td><td>mean (human)</td><td>mean (GPT-2)</td><td>difference</td><td>95% CI lower</td><td>95% CI upper</td><td>t</td><td>df</td><td>p-val</td></tr><tr><td>grammar</td><td>4.28</td><td>4.25</td><td>0.03</td><td>-0.06</td><td>0.12</td><td>0.72</td><td>1194.9</td><td>0.47</td></tr><tr><td>coherence</td><td>4.55</td><td>3.99</td><td>0.56</td><td>0.46</td><td>0.65</td><td>11.46</td><td>1044.3</td><td><0.001</td></tr><tr><td>relevance</td><td>4.02</td><td>3.02</td><td>1.00</td><td>0.85</td><td>1.15</td><td>12.82</td><td>1103.8</td><td><0.001</td></tr><tr><td>likability</td><td>4.23</td><td>3.83</td><td>0.40</td><td>0.30</td><td>0.53</td><td>7.24</td><td>1143.4</td><td><0.001</td></tr></table>",
            "id": 208,
            "page": 17,
            "text": "mean (human) mean (GPT-2) difference 95% CI lower 95% CI upper t df p-val  grammar 4.28 4.25 0.03 -0.06 0.12 0.72 1194.9 0.47  coherence 4.55 3.99 0.56 0.46 0.65 11.46 1044.3 <0.001  relevance 4.02 3.02 1.00 0.85 1.15 12.82 1103.8 <0.001  likability 4.23 3.83 0.40 0.30 0.53 7.24 1143.4"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 1008
                },
                {
                    "x": 2196,
                    "y": 1008
                },
                {
                    "x": 2196,
                    "y": 1158
                },
                {
                    "x": 288,
                    "y": 1158
                }
            ],
            "category": "caption",
            "html": "<caption id='209' style='font-size:16px'>Table A4: Welch's t-test for ratings collected on Upwork (human-written stories VS GPT-2 generated stories).<br>Human-written stories were rated higher on coherence, relevance, and likability than GPT-2 generated stories.<br>These results are similar to the one obtained from English teachers described in Section 4.</caption>",
            "id": 209,
            "page": 17,
            "text": "Table A4: Welch's t-test for ratings collected on Upwork (human-written stories VS GPT-2 generated stories). Human-written stories were rated higher on coherence, relevance, and likability than GPT-2 generated stories. These results are similar to the one obtained from English teachers described in Section 4."
        },
        {
            "bounding_box": [
                {
                    "x": 289,
                    "y": 1244
                },
                {
                    "x": 1084,
                    "y": 1244
                },
                {
                    "x": 1084,
                    "y": 1301
                },
                {
                    "x": 289,
                    "y": 1301
                }
            ],
            "category": "paragraph",
            "html": "<p id='210' style='font-size:20px'>C Details on Post-rating Interviews</p>",
            "id": 210,
            "page": 17,
            "text": "C Details on Post-rating Interviews"
        },
        {
            "bounding_box": [
                {
                    "x": 285,
                    "y": 1333
                },
                {
                    "x": 1213,
                    "y": 1333
                },
                {
                    "x": 1213,
                    "y": 2858
                },
                {
                    "x": 285,
                    "y": 2858
                }
            ],
            "category": "paragraph",
            "html": "<p id='211' style='font-size:18px'>Two mediation meetings were organized with two<br>of the three teachers (due to availability) over<br>4 The teachers were asked to reevaluate<br>Zoom ·<br>60 stories on which they showed disagreement (3<br>attributes x 10 stories x 2 types; likability was ex-<br>cluded due to its subjective nature). Each meeting<br>took approximately 2h (including a short break)<br>and was led by one of the authors. The teachers<br>were shown one story at a time and were asked to<br>reevaluate it on the given attribute. In about 20% of<br>the cases, the teachers agreed with each other, sug-<br>gesting that the previous disagreement was due to<br>honest lapses of judgment. As for the cases where<br>disagreement occurred, each was asked to provide<br>a justification for their ratings. Often hearing the<br>other party's argument enabled them to see the<br>text from a different perspective and understand<br>the ratings of the other person. This process of-<br>ten resulted in them adjusting their own ratings.<br>Common reasons for disagreement which could be<br>resolved during the mediation meeting included:<br>world knowledge, difference in understanding of<br>the prompt and its relation to the text (e.g., prompt<br>enforcing specific style), difference in the way they<br>treated author's comments which were sometimes<br>present at the beginning of the story, and rationaliz-<br>ing connections between the sentences.</p>",
            "id": 211,
            "page": 17,
            "text": "Two mediation meetings were organized with two of the three teachers (due to availability) over 4 The teachers were asked to reevaluate Zoom · 60 stories on which they showed disagreement (3 attributes x 10 stories x 2 types; likability was excluded due to its subjective nature). Each meeting took approximately 2h (including a short break) and was led by one of the authors. The teachers were shown one story at a time and were asked to reevaluate it on the given attribute. In about 20% of the cases, the teachers agreed with each other, suggesting that the previous disagreement was due to honest lapses of judgment. As for the cases where disagreement occurred, each was asked to provide a justification for their ratings. Often hearing the other party's argument enabled them to see the text from a different perspective and understand the ratings of the other person. This process often resulted in them adjusting their own ratings. Common reasons for disagreement which could be resolved during the mediation meeting included: world knowledge, difference in understanding of the prompt and its relation to the text (e.g., prompt enforcing specific style), difference in the way they treated author's comments which were sometimes present at the beginning of the story, and rationalizing connections between the sentences."
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 2866
                },
                {
                    "x": 1216,
                    "y": 2866
                },
                {
                    "x": 1216,
                    "y": 3142
                },
                {
                    "x": 287,
                    "y": 3142
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='212' style='font-size:20px'>After each batch, consisting of ratings of both<br>human-written stories and GPT-2 generated stories,<br>each of the three teachers took part in a short one-<br>on-one interview (~ 10min each). They were asked<br>the following questions:</p>",
            "id": 212,
            "page": 17,
            "text": "After each batch, consisting of ratings of both human-written stories and GPT-2 generated stories, each of the three teachers took part in a short oneon-one interview (~ 10min each). They were asked the following questions:"
        },
        {
            "bounding_box": [
                {
                    "x": 1303,
                    "y": 1248
                },
                {
                    "x": 2199,
                    "y": 1248
                },
                {
                    "x": 2199,
                    "y": 2169
                },
                {
                    "x": 1303,
                    "y": 2169
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='213' style='font-size:16px'>1. How long did it take you to calibrate your<br>ratings?<br>2. Explain in more detail how you rated co-<br>herencelgrammarlilikabilty/relevance? What<br>was the process. Did you have to reread the<br>text?<br>3. How did you calibrate the ratings for co-<br>herence/grammarlikability/relevonce? What<br>constituted a 5? What about a 1?<br>4. How often did you take breaks?<br>5. Which attribute was the easiest to calibrate?<br>Which was the most difficult?<br>6. Any other comments or suggestions?</p>",
            "id": 213,
            "page": 17,
            "text": "1. How long did it take you to calibrate your ratings? 2. Explain in more detail how you rated coherencelgrammarlilikabilty/relevance? What was the process. Did you have to reread the text? 3. How did you calibrate the ratings for coherence/grammarlikability/relevonce? What constituted a 5? What about a 1? 4. How often did you take breaks? 5. Which attribute was the easiest to calibrate? Which was the most difficult? 6. Any other comments or suggestions?"
        },
        {
            "bounding_box": [
                {
                    "x": 1267,
                    "y": 2211
                },
                {
                    "x": 2196,
                    "y": 2211
                },
                {
                    "x": 2196,
                    "y": 2601
                },
                {
                    "x": 1267,
                    "y": 2601
                }
            ],
            "category": "paragraph",
            "html": "<p id='214' style='font-size:18px'>Additionally, after the second batch of GPT-2<br>stories, the teachers were also asked: (1) which<br>batch was better written, (2) which batch included<br>more computer generated stories and to what extent,<br>(3) whether they had to recalibrate their ratings, and<br>(4) whether they would prefer to see both batches<br>at the same time.</p>",
            "id": 214,
            "page": 17,
            "text": "Additionally, after the second batch of GPT-2 stories, the teachers were also asked: (1) which batch was better written, (2) which batch included more computer generated stories and to what extent, (3) whether they had to recalibrate their ratings, and (4) whether they would prefer to see both batches at the same time."
        },
        {
            "bounding_box": [
                {
                    "x": 1269,
                    "y": 2644
                },
                {
                    "x": 1770,
                    "y": 2644
                },
                {
                    "x": 1770,
                    "y": 2705
                },
                {
                    "x": 1269,
                    "y": 2705
                }
            ],
            "category": "paragraph",
            "html": "<p id='215' style='font-size:22px'>D Statistical Analysis</p>",
            "id": 215,
            "page": 17,
            "text": "D Statistical Analysis"
        },
        {
            "bounding_box": [
                {
                    "x": 338,
                    "y": 3175
                },
                {
                    "x": 725,
                    "y": 3175
                },
                {
                    "x": 725,
                    "y": 3228
                },
                {
                    "x": 338,
                    "y": 3228
                }
            ],
            "category": "footer",
            "html": "<footer id='216' style='font-size:20px'>4https: / / zoom.us/</footer>",
            "id": 216,
            "page": 17,
            "text": "4https: / / zoom.us/"
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 302
                },
                {
                    "x": 2189,
                    "y": 302
                },
                {
                    "x": 2189,
                    "y": 527
                },
                {
                    "x": 291,
                    "y": 527
                }
            ],
            "category": "table",
            "html": "<table id='217' style='font-size:18px'><tr><td></td><td>mean (human)</td><td>mean (GPT-2)</td><td>difference</td><td>95% CI lower</td><td>95% CI upper</td><td>t</td><td>df</td><td>p-val</td></tr><tr><td>grammar</td><td>4.50</td><td>4.55</td><td>0.05</td><td>-0.14</td><td>0.03</td><td>-1.27</td><td>1111.7</td><td>0.21</td></tr><tr><td>coherence</td><td>4.38</td><td>3.73</td><td>0.65</td><td>0.53</td><td>0.77</td><td>10.63</td><td>1119.6</td><td><0.001</td></tr><tr><td>relevance</td><td>3.82</td><td>2.54</td><td>1.28</td><td>1.12</td><td>1.44</td><td>15.45</td><td>1190.7</td><td><0.001</td></tr><tr><td>likability</td><td>3.69</td><td>2.96</td><td>0.73</td><td>0.57</td><td>0.89</td><td>9.16</td><td>1182</td><td><0.001</td></tr></table>",
            "id": 217,
            "page": 18,
            "text": "mean (human) mean (GPT-2) difference 95% CI lower 95% CI upper t df p-val  grammar 4.50 4.55 0.05 -0.14 0.03 -1.27 1111.7 0.21  coherence 4.38 3.73 0.65 0.53 0.77 10.63 1119.6 <0.001  relevance 3.82 2.54 1.28 1.12 1.44 15.45 1190.7 <0.001  likability 3.69 2.96 0.73 0.57 0.89 9.16 1182"
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 560
                },
                {
                    "x": 2194,
                    "y": 560
                },
                {
                    "x": 2194,
                    "y": 663
                },
                {
                    "x": 286,
                    "y": 663
                }
            ],
            "category": "paragraph",
            "html": "<p id='218' style='font-size:20px'>Table A5: Welch's t-test for ratings collected in the experiment described in Section 4 (teachers' ratings). Human-<br>written stories were rated higher for coherence, relevance, and likability than GPT-2 generated stories.</p>",
            "id": 218,
            "page": 18,
            "text": "Table A5: Welch's t-test for ratings collected in the experiment described in Section 4 (teachers' ratings). Humanwritten stories were rated higher for coherence, relevance, and likability than GPT-2 generated stories."
        },
        {
            "bounding_box": [
                {
                    "x": 300,
                    "y": 763
                },
                {
                    "x": 679,
                    "y": 763
                },
                {
                    "x": 679,
                    "y": 797
                },
                {
                    "x": 300,
                    "y": 797
                }
            ],
            "category": "paragraph",
            "html": "<p id='219' style='font-size:16px'>Please Rate the Story Fragment</p>",
            "id": 219,
            "page": 18,
            "text": "Please Rate the Story Fragment"
        },
        {
            "bounding_box": [
                {
                    "x": 303,
                    "y": 809
                },
                {
                    "x": 750,
                    "y": 809
                },
                {
                    "x": 750,
                    "y": 836
                },
                {
                    "x": 303,
                    "y": 836
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='220' style='font-size:14px'>The goal of this task is to rate story fragments on four criteria.</p>",
            "id": 220,
            "page": 18,
            "text": "The goal of this task is to rate story fragments on four criteria."
        },
        {
            "bounding_box": [
                {
                    "x": 303,
                    "y": 844
                },
                {
                    "x": 1417,
                    "y": 844
                },
                {
                    "x": 1417,
                    "y": 872
                },
                {
                    "x": 303,
                    "y": 872
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='221' style='font-size:14px'>NOTE: Please take the time to fully read and understand the story fragment. We will reject submissions from workers that are clearly spamming the task.</p>",
            "id": 221,
            "page": 18,
            "text": "NOTE: Please take the time to fully read and understand the story fragment. We will reject submissions from workers that are clearly spamming the task."
        },
        {
            "bounding_box": [
                {
                    "x": 1124,
                    "y": 940
                },
                {
                    "x": 1350,
                    "y": 940
                },
                {
                    "x": 1350,
                    "y": 979
                },
                {
                    "x": 1124,
                    "y": 979
                }
            ],
            "category": "paragraph",
            "html": "<p id='222' style='font-size:18px'>Story Fragment</p>",
            "id": 222,
            "page": 18,
            "text": "Story Fragment"
        },
        {
            "bounding_box": [
                {
                    "x": 338,
                    "y": 996
                },
                {
                    "x": 2112,
                    "y": 996
                },
                {
                    "x": 2112,
                    "y": 1098
                },
                {
                    "x": 338,
                    "y": 1098
                }
            ],
            "category": "paragraph",
            "html": "<p id='223' style='font-size:16px'>The night before came as a shock for Oren, he was always a conscientious child. It was a necessary skill of a new master, an inherent capability to make the world a better place. But no, today, the day he<br>brought his sister to his cooking school was the first time Oren had been shocked out of a small calm. He looked over at his sister in the small room, who was idly flipping through the magazine he had brought<br>with him, and then back to the breakfast. It took all his willpower to stay calm, he could tell from the way the noodles he was looking at were slathered in gherkin and he felt the freshness of the rice. He shook<br>his head in disbelief, his stomach began to churn and he was too exhausted to react, he was just preparing to go to bed.</p>",
            "id": 223,
            "page": 18,
            "text": "The night before came as a shock for Oren, he was always a conscientious child. It was a necessary skill of a new master, an inherent capability to make the world a better place. But no, today, the day he brought his sister to his cooking school was the first time Oren had been shocked out of a small calm. He looked over at his sister in the small room, who was idly flipping through the magazine he had brought with him, and then back to the breakfast. It took all his willpower to stay calm, he could tell from the way the noodles he was looking at were slathered in gherkin and he felt the freshness of the rice. He shook his head in disbelief, his stomach began to churn and he was too exhausted to react, he was just preparing to go to bed."
        },
        {
            "bounding_box": [
                {
                    "x": 328,
                    "y": 1137
                },
                {
                    "x": 1100,
                    "y": 1137
                },
                {
                    "x": 1100,
                    "y": 1170
                },
                {
                    "x": 328,
                    "y": 1170
                }
            ],
            "category": "paragraph",
            "html": "<p id='224' style='font-size:14px'>1. How grammatically correct is the text of the story fragment? (on a scale of 1-5, with 1 being the lowest)</p>",
            "id": 224,
            "page": 18,
            "text": "1. How grammatically correct is the text of the story fragment? (on a scale of 1-5, with 1 being the lowest)"
        },
        {
            "bounding_box": [
                {
                    "x": 337,
                    "y": 1172
                },
                {
                    "x": 775,
                    "y": 1172
                },
                {
                    "x": 775,
                    "y": 1204
                },
                {
                    "x": 337,
                    "y": 1204
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='225' style='font-size:14px'>(lowest) ○ 1 ○ 2 ○ 3 ○ 4 ○ 5 (highest)</p>",
            "id": 225,
            "page": 18,
            "text": "(lowest) ○ 1 ○ 2 ○ 3 ○ 4 ○ 5 (highest)"
        },
        {
            "bounding_box": [
                {
                    "x": 328,
                    "y": 1209
                },
                {
                    "x": 1109,
                    "y": 1209
                },
                {
                    "x": 1109,
                    "y": 1238
                },
                {
                    "x": 328,
                    "y": 1238
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='226' style='font-size:14px'>2. How well do the sentences in the story fragment fit together? (on a scale of 1-5, with 1 being the lowest)</p>",
            "id": 226,
            "page": 18,
            "text": "2. How well do the sentences in the story fragment fit together? (on a scale of 1-5, with 1 being the lowest)"
        },
        {
            "bounding_box": [
                {
                    "x": 342,
                    "y": 1244
                },
                {
                    "x": 766,
                    "y": 1244
                },
                {
                    "x": 766,
                    "y": 1275
                },
                {
                    "x": 342,
                    "y": 1275
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='227' style='font-size:14px'>(lowest) ○ 1 ○ 2 ○ 3 ○ 4 ○ 5 (highest)</p>",
            "id": 227,
            "page": 18,
            "text": "(lowest) ○ 1 ○ 2 ○ 3 ○ 4 ○ 5 (highest)"
        },
        {
            "bounding_box": [
                {
                    "x": 328,
                    "y": 1279
                },
                {
                    "x": 998,
                    "y": 1279
                },
                {
                    "x": 998,
                    "y": 1308
                },
                {
                    "x": 328,
                    "y": 1308
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='228' style='font-size:14px'>3. How enjoyable do you find the story fragment? (on a scale of 1-5, with 1 being the lowest)</p>",
            "id": 228,
            "page": 18,
            "text": "3. How enjoyable do you find the story fragment? (on a scale of 1-5, with 1 being the lowest)"
        },
        {
            "bounding_box": [
                {
                    "x": 341,
                    "y": 1315
                },
                {
                    "x": 766,
                    "y": 1315
                },
                {
                    "x": 766,
                    "y": 1343
                },
                {
                    "x": 341,
                    "y": 1343
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='229' style='font-size:14px'>(lowest) ○ 1 ○ 2 ○ 3 ○ 4 5 (highest)</p>",
            "id": 229,
            "page": 18,
            "text": "(lowest) ○ 1 ○ 2 ○ 3 ○ 4 5 (highest)"
        },
        {
            "bounding_box": [
                {
                    "x": 328,
                    "y": 1349
                },
                {
                    "x": 862,
                    "y": 1349
                },
                {
                    "x": 862,
                    "y": 1378
                },
                {
                    "x": 328,
                    "y": 1378
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='230' style='font-size:14px'>4. Now read the PROMPT based on which the story fragment was written.</p>",
            "id": 230,
            "page": 18,
            "text": "4. Now read the PROMPT based on which the story fragment was written."
        },
        {
            "bounding_box": [
                {
                    "x": 347,
                    "y": 1384
                },
                {
                    "x": 1871,
                    "y": 1384
                },
                {
                    "x": 1871,
                    "y": 1413
                },
                {
                    "x": 347,
                    "y": 1413
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='231' style='font-size:14px'>PROMPT: After brushing your teeth in the morning you go downstairs to fry an egg, but when you try the frying pan buzzes at you and text appears reading, ``level 18 cooking required to use object\".</p>",
            "id": 231,
            "page": 18,
            "text": "PROMPT: After brushing your teeth in the morning you go downstairs to fry an egg, but when you try the frying pan buzzes at you and text appears reading, ``level 18 cooking required to use object\"."
        },
        {
            "bounding_box": [
                {
                    "x": 348,
                    "y": 1418
                },
                {
                    "x": 1032,
                    "y": 1418
                },
                {
                    "x": 1032,
                    "y": 1446
                },
                {
                    "x": 348,
                    "y": 1446
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='232' style='font-size:14px'>How relevant is the story fragment to the prompt? (on a scale of 1-5, with 1 being the lowest)</p>",
            "id": 232,
            "page": 18,
            "text": "How relevant is the story fragment to the prompt? (on a scale of 1-5, with 1 being the lowest)"
        },
        {
            "bounding_box": [
                {
                    "x": 346,
                    "y": 1454
                },
                {
                    "x": 765,
                    "y": 1454
                },
                {
                    "x": 765,
                    "y": 1482
                },
                {
                    "x": 346,
                    "y": 1482
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='233' style='font-size:14px'>(lowest) ○ 1 ○ 2 ○ 3 4 5 (highest)</p>",
            "id": 233,
            "page": 18,
            "text": "(lowest) ○ 1 ○ 2 ○ 3 4 5 (highest)"
        },
        {
            "bounding_box": [
                {
                    "x": 330,
                    "y": 1497
                },
                {
                    "x": 397,
                    "y": 1497
                },
                {
                    "x": 397,
                    "y": 1521
                },
                {
                    "x": 330,
                    "y": 1521
                }
            ],
            "category": "paragraph",
            "html": "<p id='234' style='font-size:14px'>Submit</p>",
            "id": 234,
            "page": 18,
            "text": "Submit"
        },
        {
            "bounding_box": [
                {
                    "x": 785,
                    "y": 1650
                },
                {
                    "x": 1688,
                    "y": 1650
                },
                {
                    "x": 1688,
                    "y": 1699
                },
                {
                    "x": 785,
                    "y": 1699
                }
            ],
            "category": "caption",
            "html": "<caption id='235' style='font-size:20px'>Figure A1: AMT interface for evaluation of one story.</caption>",
            "id": 235,
            "page": 18,
            "text": "Figure A1: AMT interface for evaluation of one story."
        },
        {
            "bounding_box": [
                {
                    "x": 599,
                    "y": 1756
                },
                {
                    "x": 1874,
                    "y": 1756
                },
                {
                    "x": 1874,
                    "y": 1941
                },
                {
                    "x": 599,
                    "y": 1941
                }
            ],
            "category": "table",
            "html": "<table id='236' style='font-size:22px'><tr><td></td><td>Df</td><td>Sum Sq</td><td>Mean Sq</td><td>F value</td><td>Pr(>F)</td><td>72</td></tr><tr><td>Group</td><td>3</td><td>13.63</td><td>4.54</td><td>4.90</td><td>0.002</td><td>0.01</td></tr><tr><td>Residuals</td><td>2396</td><td>2223.51</td><td>0.93</td><td></td><td></td><td></td></tr></table>",
            "id": 236,
            "page": 18,
            "text": "Df Sum Sq Mean Sq F value Pr(>F) 72  Group 3 13.63 4.54 4.90 0.002 0.01  Residuals 2396 2223.51 0.93"
        },
        {
            "bounding_box": [
                {
                    "x": 285,
                    "y": 1977
                },
                {
                    "x": 2195,
                    "y": 1977
                },
                {
                    "x": 2195,
                    "y": 2178
                },
                {
                    "x": 285,
                    "y": 2178
                }
            ],
            "category": "paragraph",
            "html": "<p id='237' style='font-size:20px'>Table A6: One-way ANOVA investigating the effect of group (Day 1, Day 2, Day 3, and workers from non-English-<br>speaking countries) on the ratings of grammar of the reference texts. Partial eta squared (72) is provided for the<br>effect size (72 = 0.01 indicates small effect size; 72 = 0.06 indicates medium effect size; 72 = 0.14 indicates large<br>effect size (Cohen, 1988)).</p>",
            "id": 237,
            "page": 18,
            "text": "Table A6: One-way ANOVA investigating the effect of group (Day 1, Day 2, Day 3, and workers from non-Englishspeaking countries) on the ratings of grammar of the reference texts. Partial eta squared (72) is provided for the effect size (72 = 0.01 indicates small effect size; 72 = 0.06 indicates medium effect size; 72 = 0.14 indicates large effect size (Cohen, 1988))."
        },
        {
            "bounding_box": [
                {
                    "x": 909,
                    "y": 2241
                },
                {
                    "x": 1565,
                    "y": 2241
                },
                {
                    "x": 1565,
                    "y": 2480
                },
                {
                    "x": 909,
                    "y": 2480
                }
            ],
            "category": "table",
            "html": "<table id='238' style='font-size:22px'><tr><td></td><td>Day 1</td><td>Day 2</td><td>Day 3</td></tr><tr><td>Day 2</td><td>0.08</td><td></td><td></td></tr><tr><td>Day 3</td><td>1.00</td><td>0.20</td><td></td></tr><tr><td>NNS</td><td>0.01</td><td>1.00</td><td>0.03</td></tr></table>",
            "id": 238,
            "page": 18,
            "text": "Day 1 Day 2 Day 3  Day 2 0.08    Day 3 1.00 0.20   NNS 0.01 1.00"
        },
        {
            "bounding_box": [
                {
                    "x": 285,
                    "y": 2513
                },
                {
                    "x": 2194,
                    "y": 2513
                },
                {
                    "x": 2194,
                    "y": 2717
                },
                {
                    "x": 285,
                    "y": 2717
                }
            ],
            "category": "paragraph",
            "html": "<p id='239' style='font-size:20px'>Table A7: Pairwise post-hoc test with Bonferroni adjustment for the ratings of grammar between Day 1, Day<br>2, Day 3, and non-English speaking countries (NNS). The numbers provided in the table are p-values for the<br>given pairwise comparison. Grammar ratings provided by the workers from non-English speaking countries are<br>significantly different from ratings provided by the workers from English-speaking countries on Day 1 and Day 3 5</p>",
            "id": 239,
            "page": 18,
            "text": "Table A7: Pairwise post-hoc test with Bonferroni adjustment for the ratings of grammar between Day 1, Day 2, Day 3, and non-English speaking countries (NNS). The numbers provided in the table are p-values for the given pairwise comparison. Grammar ratings provided by the workers from non-English speaking countries are significantly different from ratings provided by the workers from English-speaking countries on Day 1 and Day 3 5"
        },
        {
            "bounding_box": [
                {
                    "x": 586,
                    "y": 2774
                },
                {
                    "x": 1894,
                    "y": 2774
                },
                {
                    "x": 1894,
                    "y": 2961
                },
                {
                    "x": 586,
                    "y": 2961
                }
            ],
            "category": "table",
            "html": "<table id='240' style='font-size:22px'><tr><td></td><td>Df</td><td>Sum Sq</td><td>Mean Sq</td><td>F value</td><td>Pr(>F) )</td><td>72</td></tr><tr><td>Group</td><td>3</td><td>161.28</td><td>53.76</td><td>51.35</td><td><0.001</td><td>0.06</td></tr><tr><td>Residuals</td><td>2396</td><td>2508.21</td><td>1.05</td><td></td><td></td><td></td></tr></table>",
            "id": 240,
            "page": 18,
            "text": "Df Sum Sq Mean Sq F value Pr(>F) ) 72  Group 3 161.28 53.76 51.35 <0.001 0.06  Residuals 2396 2508.21 1.05"
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 2995
                },
                {
                    "x": 2195,
                    "y": 2995
                },
                {
                    "x": 2195,
                    "y": 3195
                },
                {
                    "x": 286,
                    "y": 3195
                }
            ],
            "category": "paragraph",
            "html": "<p id='241' style='font-size:20px'>Table A8: One-way ANOVA investigating the effect of group (Day 1, Day 2, Day 3, and workers from non-English-<br>speaking countries) on the ratings of coherence of the reference texts. Partial eta squared (7p) is provided for the<br>effect size (72 = 0.01 indicates small effect size; 72 = 0.06 indicates medium effect size; 72 = 0.14 indicates large<br>effect size (Cohen, 1988)).</p>",
            "id": 241,
            "page": 18,
            "text": "Table A8: One-way ANOVA investigating the effect of group (Day 1, Day 2, Day 3, and workers from non-Englishspeaking countries) on the ratings of coherence of the reference texts. Partial eta squared (7p) is provided for the effect size (72 = 0.01 indicates small effect size; 72 = 0.06 indicates medium effect size; 72 = 0.14 indicates large effect size (Cohen, 1988))."
        },
        {
            "bounding_box": [
                {
                    "x": 318,
                    "y": 332
                },
                {
                    "x": 2169,
                    "y": 332
                },
                {
                    "x": 2169,
                    "y": 1986
                },
                {
                    "x": 318,
                    "y": 1986
                }
            ],
            "category": "figure",
            "html": "<figure><img id='242' style='font-size:14px' alt=\"Please Rate the Story Fragment 1\nThe goal of this task is to rate story fragments on four criteria.\nNOTE: Please take the time to fully read and understand the story fragment. We will reject submissions from workers that are clearly spamming the task.\nStory Fragment 1\n'Man, Clark? You've never been the greatest detective, but do | even need to tell you this? The Invaders might be all talk and no walk, but the Jackal Boy has the sense of an eagle to stick to his arse. ' Clark was\nabout to say something, but he didn't want to risk it. He didn't like it when people were talking about him in such a nice light. He wasn't about to go walking into someone's bar. He certainly wasn't about to try to\nconvince himself that the local kids were going crazy. A 'Psh, psh, this doesn't feel right. I don't want to stick my nose where you don't want to smell it, Gordon, You shouldn't say that about yourself. All of you. ·\nHe stared at Gordon, dumbfounded. This wasn't Gordon's first rodeo, this was something new.\n1. How grammatically correct is the text of the story fragment 1 ? (on a scale of 1-5, with 1 being the lowest)\n(lowest) ○ 1 ◇ 2 ◇ 3 ○ 4 ○ 5 (highest)\n2. How well do the sentences in the story fragment 1 fit together? (on a scale of 1-5, with 1 being the lowesf)\n(lowest) ○ 1 ○ 2 ○ 3 ○ 4 ○ 5 (highest)\n3. How enjoyable do you find the story fragment 1 ? (on a scale of 1-5. with 1 being the lowest)\n(lowest) ◇ 1 ◇ 2 ◇ 3 ○ 4 ○ 5 (highest)\n4. Now read the PROMPT based on which the story fragment 1 was written.\nPROMPT: With the earth currently under attack by aliens, Clark Kent searches for a place to change into his super suit. He decides to use the blue police public call box standing curiously in the middle of the street.\nHow relevant is the story fragment 1 to the prompt? (on a scale of 1-5, with 1 being the lowest)\n(lowest) ○ 1 2 3 5 (highest)\nPlease Rate the Story Fragment 2\nThe goal of this task is to rate story fragments on four crileria.\nNOTE: Please take the time to fully read and understand the story fragment. We will reject submissions trom workers that are clearly spamming the task.\nStory Fragment 2\nHe burst through the door like he owned the place. Silly, really, even I only *sorta* own her. I glanced across the console, and Clara caught my eye ``Oi, ' called Clara. I scowled at him (scowling has actually\nbecome rather easy lately) as he turned around. He was still wearing those stupid round spectacles. It was early in his career, then. He scanned the room quickly, processing his surroundings. He didn't seem\nsurprised by the interior dimensions. at least. He loosened his tie, dropping the coat he'd been wearing. What is this? Some sort of trap?' He looked tense, ready to strike. Oh, that's lovely, isn't it. ' | said with a\nsarcastic chuckle, smirking at Clara. You burst in here and then *you're* the one who's angry. How d'you think we feel, hmm?' Whatever he'd been expecting, it wasn't that.\n1. How grammatically correctis the text of the story fragment 2 ? (on a scale of 1-5, with 1 being the lowest)\n(lowest) ○ 1 ○ 2 ○ 3 ○ 4 ○ 5 (highest)\n2. How well do the sentences in the story fragment 2 fit together? (on a scale of 1-5, with 1 being the lowest)\n(lowest) ◇ 1 ○ 2 ◇ 3 ○ 4 ○ 5 (highest)\n3. How enjoyable do you find the story fragment 2 ? (on a scale of 1-5, with 1 being the lowest)\n(lowest) ○ 1 ○ 2 ○ 3 ○ 4 ○ 5 (highest)\n4. Now read the PROMPT based on which the story fragment 2 was written.\nPROMPT: With the earth currently under attack by aliens, Clark Kent searches for a place to change into his super suit. He decides to use the blue police public call box standing curiously in the middle of the street.\nHow relevant is the story fragment 2 to the prompt? (on a scale of 1-5, with 1 being the lowest)\n(lowest) ○ 1 2 3 4 ○ 5 (highest)\nSubmit\" data-coord=\"top-left:(318,332); bottom-right:(2169,1986)\" /></figure>",
            "id": 242,
            "page": 19,
            "text": "Please Rate the Story Fragment 1 The goal of this task is to rate story fragments on four criteria. NOTE: Please take the time to fully read and understand the story fragment. We will reject submissions from workers that are clearly spamming the task. Story Fragment 1 \"Man, Clark? You've never been the greatest detective, but do | even need to tell you this? The Invaders might be all talk and no walk, but the Jackal Boy has the sense of an eagle to stick to his arse. \" Clark was about to say something, but he didn't want to risk it. He didn't like it when people were talking about him in such a nice light. He wasn't about to go walking into someone's bar. He certainly wasn't about to try to convince himself that the local kids were going crazy. A \"Psh, psh, this doesn't feel right. I don't want to stick my nose where you don't want to smell it, Gordon, You shouldn't say that about yourself. All of you. · He stared at Gordon, dumbfounded. This wasn't Gordon's first rodeo, this was something new. 1. How grammatically correct is the text of the story fragment 1 ? (on a scale of 1-5, with 1 being the lowest) (lowest) ○ 1 ◇ 2 ◇ 3 ○ 4 ○ 5 (highest) 2. How well do the sentences in the story fragment 1 fit together? (on a scale of 1-5, with 1 being the lowesf) (lowest) ○ 1 ○ 2 ○ 3 ○ 4 ○ 5 (highest) 3. How enjoyable do you find the story fragment 1 ? (on a scale of 1-5. with 1 being the lowest) (lowest) ◇ 1 ◇ 2 ◇ 3 ○ 4 ○ 5 (highest) 4. Now read the PROMPT based on which the story fragment 1 was written. PROMPT: With the earth currently under attack by aliens, Clark Kent searches for a place to change into his super suit. He decides to use the blue police public call box standing curiously in the middle of the street. How relevant is the story fragment 1 to the prompt? (on a scale of 1-5, with 1 being the lowest) (lowest) ○ 1 2 3 5 (highest) Please Rate the Story Fragment 2 The goal of this task is to rate story fragments on four crileria. NOTE: Please take the time to fully read and understand the story fragment. We will reject submissions trom workers that are clearly spamming the task. Story Fragment 2 He burst through the door like he owned the place. Silly, really, even I only *sorta* own her. I glanced across the console, and Clara caught my eye ``Oi, \" called Clara. I scowled at him (scowling has actually become rather easy lately) as he turned around. He was still wearing those stupid round spectacles. It was early in his career, then. He scanned the room quickly, processing his surroundings. He didn't seem surprised by the interior dimensions. at least. He loosened his tie, dropping the coat he'd been wearing. What is this? Some sort of trap?\" He looked tense, ready to strike. Oh, that's lovely, isn't it. \" | said with a sarcastic chuckle, smirking at Clara. You burst in here and then *you're* the one who's angry. How d'you think we feel, hmm?\" Whatever he'd been expecting, it wasn't that. 1. How grammatically correctis the text of the story fragment 2 ? (on a scale of 1-5, with 1 being the lowest) (lowest) ○ 1 ○ 2 ○ 3 ○ 4 ○ 5 (highest) 2. How well do the sentences in the story fragment 2 fit together? (on a scale of 1-5, with 1 being the lowest) (lowest) ◇ 1 ○ 2 ◇ 3 ○ 4 ○ 5 (highest) 3. How enjoyable do you find the story fragment 2 ? (on a scale of 1-5, with 1 being the lowest) (lowest) ○ 1 ○ 2 ○ 3 ○ 4 ○ 5 (highest) 4. Now read the PROMPT based on which the story fragment 2 was written. PROMPT: With the earth currently under attack by aliens, Clark Kent searches for a place to change into his super suit. He decides to use the blue police public call box standing curiously in the middle of the street. How relevant is the story fragment 2 to the prompt? (on a scale of 1-5, with 1 being the lowest) (lowest) ○ 1 2 3 4 ○ 5 (highest) Submit"
        },
        {
            "bounding_box": [
                {
                    "x": 438,
                    "y": 2023
                },
                {
                    "x": 2036,
                    "y": 2023
                },
                {
                    "x": 2036,
                    "y": 2077
                },
                {
                    "x": 438,
                    "y": 2077
                }
            ],
            "category": "caption",
            "html": "<caption id='243' style='font-size:16px'>Figure A2: AMT interface for evaluation of both types of stories (GPT-2 and human reference).</caption>",
            "id": 243,
            "page": 19,
            "text": "Figure A2: AMT interface for evaluation of both types of stories (GPT-2 and human reference)."
        },
        {
            "bounding_box": [
                {
                    "x": 896,
                    "y": 2155
                },
                {
                    "x": 1576,
                    "y": 2155
                },
                {
                    "x": 1576,
                    "y": 2396
                },
                {
                    "x": 896,
                    "y": 2396
                }
            ],
            "category": "table",
            "html": "<table id='244' style='font-size:20px'><tr><td></td><td>Day 1</td><td>Day 2</td><td>Day 3</td></tr><tr><td>Day 2</td><td>1.00</td><td></td><td></td></tr><tr><td>Day 3</td><td><0.001</td><td><0.001</td><td></td></tr><tr><td>NNS</td><td><0.001</td><td><0.001</td><td>0.02</td></tr></table>",
            "id": 244,
            "page": 19,
            "text": "Day 1 Day 2 Day 3  Day 2 1.00    Day 3 <0.001 <0.001   NNS <0.001 <0.001"
        },
        {
            "bounding_box": [
                {
                    "x": 287,
                    "y": 2425
                },
                {
                    "x": 2197,
                    "y": 2425
                },
                {
                    "x": 2197,
                    "y": 2684
                },
                {
                    "x": 287,
                    "y": 2684
                }
            ],
            "category": "paragraph",
            "html": "<p id='245' style='font-size:18px'>Table A9: Pairwise post hoc test with Bonferroni adjustment for the ratings of coherence between Day 1, Day<br>2, Day 3, and non-English speaking countries (NNS). The numbers provided in the table are p-values for the<br>given pairwise comparison. Ratings of coherence provided by raters from non-English speaking countries are<br>significantly different from ratings of workers from English-speaking countries. Furthermore, there are some<br>difference between Day 1, Day 2, and Day 2.</p>",
            "id": 245,
            "page": 19,
            "text": "Table A9: Pairwise post hoc test with Bonferroni adjustment for the ratings of coherence between Day 1, Day 2, Day 3, and non-English speaking countries (NNS). The numbers provided in the table are p-values for the given pairwise comparison. Ratings of coherence provided by raters from non-English speaking countries are significantly different from ratings of workers from English-speaking countries. Furthermore, there are some difference between Day 1, Day 2, and Day 2."
        },
        {
            "bounding_box": [
                {
                    "x": 602,
                    "y": 2764
                },
                {
                    "x": 1873,
                    "y": 2764
                },
                {
                    "x": 1873,
                    "y": 2948
                },
                {
                    "x": 602,
                    "y": 2948
                }
            ],
            "category": "table",
            "html": "<table id='246' style='font-size:22px'><tr><td></td><td>Df</td><td>Sum Sq</td><td>Mean Sq</td><td>F value</td><td>Pr(>F)</td><td>72</td></tr><tr><td>Group</td><td>3</td><td>89.99</td><td>30.00</td><td>19.92</td><td><0.001</td><td>0.02</td></tr><tr><td>Residuals</td><td>2396</td><td>3607.85</td><td>1.51</td><td></td><td></td><td></td></tr></table>",
            "id": 246,
            "page": 19,
            "text": "Df Sum Sq Mean Sq F value Pr(>F) 72  Group 3 89.99 30.00 19.92 <0.001 0.02  Residuals 2396 3607.85 1.51"
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 2982
                },
                {
                    "x": 2198,
                    "y": 2982
                },
                {
                    "x": 2198,
                    "y": 3186
                },
                {
                    "x": 286,
                    "y": 3186
                }
            ],
            "category": "paragraph",
            "html": "<p id='247' style='font-size:18px'>Table A10: One-way ANOVA investigating the effect of group (Day 1, Day 2, Day 3, and workers from non-<br>English-speaking countries) on the ratings of relevance of the reference texts. Partial eta squared ( 72) is provided<br>for the effect size (72 = 0.01 indicates small effect size; 72 = 0.06 indicates medium effect size; 72 = 0.14 indicates<br>large effect size (Cohen, 1988)).</p>",
            "id": 247,
            "page": 19,
            "text": "Table A10: One-way ANOVA investigating the effect of group (Day 1, Day 2, Day 3, and workers from nonEnglish-speaking countries) on the ratings of relevance of the reference texts. Partial eta squared ( 72) is provided for the effect size (72 = 0.01 indicates small effect size; 72 = 0.06 indicates medium effect size; 72 = 0.14 indicates large effect size (Cohen, 1988))."
        },
        {
            "bounding_box": [
                {
                    "x": 892,
                    "y": 303
                },
                {
                    "x": 1587,
                    "y": 303
                },
                {
                    "x": 1587,
                    "y": 538
                },
                {
                    "x": 892,
                    "y": 538
                }
            ],
            "category": "table",
            "html": "<table id='248' style='font-size:20px'><tr><td></td><td>Day 1</td><td>Day 2</td><td>Day 3</td></tr><tr><td>Day 2</td><td>0.01</td><td></td><td></td></tr><tr><td>Day 3</td><td>1.00</td><td>0.14</td><td></td></tr><tr><td>NNS</td><td><0.001</td><td><0.001</td><td><0.001</td></tr></table>",
            "id": 248,
            "page": 20,
            "text": "Day 1 Day 2 Day 3  Day 2 0.01    Day 3 1.00 0.14   NNS <0.001 <0.001"
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 571
                },
                {
                    "x": 2197,
                    "y": 571
                },
                {
                    "x": 2197,
                    "y": 827
                },
                {
                    "x": 286,
                    "y": 827
                }
            ],
            "category": "caption",
            "html": "<caption id='249' style='font-size:18px'>Table A11: Pairwise post hoc test with Bonferroni adjustment for the ratings of relevance between Day 1, Day 2,<br>Day 3, and non-English speaking countries (NNS). The numbers provided in the table are p-values for the given<br>pairwise comparison. Ratings obtained from workers from non-English speaking countries differ significantly<br>from ratings obtained from workers from English-speaking countries on Day 1, Day 2, and Day 3. Furthermore,<br>there is a significant difference between ratings collected on Day 1 and Day 2.</caption>",
            "id": 249,
            "page": 20,
            "text": "Table A11: Pairwise post hoc test with Bonferroni adjustment for the ratings of relevance between Day 1, Day 2, Day 3, and non-English speaking countries (NNS). The numbers provided in the table are p-values for the given pairwise comparison. Ratings obtained from workers from non-English speaking countries differ significantly from ratings obtained from workers from English-speaking countries on Day 1, Day 2, and Day 3. Furthermore, there is a significant difference between ratings collected on Day 1 and Day 2."
        },
        {
            "bounding_box": [
                {
                    "x": 600,
                    "y": 875
                },
                {
                    "x": 1873,
                    "y": 875
                },
                {
                    "x": 1873,
                    "y": 1060
                },
                {
                    "x": 600,
                    "y": 1060
                }
            ],
            "category": "table",
            "html": "<table id='250' style='font-size:22px'><tr><td></td><td>Df</td><td>Sum Sq</td><td>Mean Sq</td><td>F value</td><td>Pr(>F)</td><td>72</td></tr><tr><td>Group</td><td>3</td><td>62.70</td><td>20.90</td><td>15.89</td><td><0.001</td><td>0.02</td></tr><tr><td>Residuals</td><td>2396</td><td>3151.22</td><td>1.32</td><td></td><td></td><td></td></tr></table>",
            "id": 250,
            "page": 20,
            "text": "Df Sum Sq Mean Sq F value Pr(>F) 72  Group 3 62.70 20.90 15.89 <0.001 0.02  Residuals 2396 3151.22 1.32"
        },
        {
            "bounding_box": [
                {
                    "x": 284,
                    "y": 1092
                },
                {
                    "x": 2197,
                    "y": 1092
                },
                {
                    "x": 2197,
                    "y": 1298
                },
                {
                    "x": 284,
                    "y": 1298
                }
            ],
            "category": "caption",
            "html": "<caption id='251' style='font-size:18px'>Table A12: One-way ANOVA investigating the effect of group (Day 1, Day 2, Day 3, and workers from non-<br>English-speaking countries) on the ratings of likability of the reference texts. Partial eta squared (72) is provided<br>for the effect size (72 = 0.01 indicates small effect size; 72 = 0.06 indicates medium effect size; 72 = 0.14 indicates<br>large effect size (Cohen, 1988)).</caption>",
            "id": 251,
            "page": 20,
            "text": "Table A12: One-way ANOVA investigating the effect of group (Day 1, Day 2, Day 3, and workers from nonEnglish-speaking countries) on the ratings of likability of the reference texts. Partial eta squared (72) is provided for the effect size (72 = 0.01 indicates small effect size; 72 = 0.06 indicates medium effect size; 72 = 0.14 indicates large effect size (Cohen, 1988))."
        },
        {
            "bounding_box": [
                {
                    "x": 899,
                    "y": 1349
                },
                {
                    "x": 1579,
                    "y": 1349
                },
                {
                    "x": 1579,
                    "y": 1590
                },
                {
                    "x": 899,
                    "y": 1590
                }
            ],
            "category": "table",
            "html": "<table id='252' style='font-size:22px'><tr><td></td><td>Day 1</td><td>Day 2</td><td>Day 3</td></tr><tr><td>Day 2</td><td><0.001</td><td></td><td></td></tr><tr><td>Day 3</td><td>1.00</td><td><0.001</td><td></td></tr><tr><td>NNS</td><td>1.00</td><td><0.001</td><td>0.75</td></tr></table>",
            "id": 252,
            "page": 20,
            "text": "Day 1 Day 2 Day 3  Day 2 <0.001    Day 3 1.00 <0.001   NNS 1.00 <0.001"
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 1621
                },
                {
                    "x": 2197,
                    "y": 1621
                },
                {
                    "x": 2197,
                    "y": 1921
                },
                {
                    "x": 286,
                    "y": 1921
                }
            ],
            "category": "caption",
            "html": "<caption id='253' style='font-size:18px'>Table A13: Pairwise post hoc test with Bonferroni adjustment for the ratings of likability between Day 1, Day<br>2, Day 3, and non-English speaking countries (NNS). The numbers provided in the table are p-values for the<br>given pairwise comparison. Ratings provided by workers from non-English speaking countries differ significantly<br>from ratings obtained from workers from English-speaking countries on Day 2. Furthermore, there are significant<br>differences between ratings obtained on Day 1 and Day 2, as well as between ratings obtained on Day 2 and Day<br>3.</caption>",
            "id": 253,
            "page": 20,
            "text": "Table A13: Pairwise post hoc test with Bonferroni adjustment for the ratings of likability between Day 1, Day 2, Day 3, and non-English speaking countries (NNS). The numbers provided in the table are p-values for the given pairwise comparison. Ratings provided by workers from non-English speaking countries differ significantly from ratings obtained from workers from English-speaking countries on Day 2. Furthermore, there are significant differences between ratings obtained on Day 1 and Day 2, as well as between ratings obtained on Day 2 and Day 3."
        },
        {
            "bounding_box": [
                {
                    "x": 292,
                    "y": 1972
                },
                {
                    "x": 2183,
                    "y": 1972
                },
                {
                    "x": 2183,
                    "y": 2197
                },
                {
                    "x": 292,
                    "y": 2197
                }
            ],
            "category": "table",
            "html": "<table id='254' style='font-size:14px'><tr><td></td><td>mean (human)</td><td>mean (GPT-2)</td><td>difference</td><td>95% CI lower</td><td>95% CI upper</td><td>t</td><td>df</td><td>p-val</td></tr><tr><td>grammar</td><td>4.00</td><td>3.94</td><td>0.06</td><td>-0.05</td><td>0.16</td><td>1.06</td><td>1197.9</td><td>0.29</td></tr><tr><td>coherence</td><td>4.11</td><td>3.82</td><td>0.29</td><td>0.18</td><td>0.42</td><td>4.97</td><td>1169.1</td><td><0.001</td></tr><tr><td>relevance</td><td>3.71</td><td>3.44</td><td>0.27</td><td>0.12</td><td>0.43</td><td>3.54</td><td>1184.7</td><td><0.001</td></tr><tr><td>likability</td><td>3.37</td><td>3.42</td><td>0.05</td><td>-0.18</td><td>0.09</td><td>-0.64</td><td>1194.5</td><td>0.52</td></tr></table>",
            "id": 254,
            "page": 20,
            "text": "mean (human) mean (GPT-2) difference 95% CI lower 95% CI upper t df p-val  grammar 4.00 3.94 0.06 -0.05 0.16 1.06 1197.9 0.29  coherence 4.11 3.82 0.29 0.18 0.42 4.97 1169.1 <0.001  relevance 3.71 3.44 0.27 0.12 0.43 3.54 1184.7 <0.001  likability 3.37 3.42 0.05 -0.18 0.09 -0.64 1194.5"
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 2229
                },
                {
                    "x": 2193,
                    "y": 2229
                },
                {
                    "x": 2193,
                    "y": 2381
                },
                {
                    "x": 286,
                    "y": 2381
                }
            ],
            "category": "caption",
            "html": "<caption id='255' style='font-size:16px'>Table A14: Welch's t-test on ratings collected on AMT for human-written stories (Day 1) and GPT-2 generated<br>stories. Human-written stories are being rated higher for coherence and more relevance than GPT-2 generated<br>stories (p<0.05).</caption>",
            "id": 255,
            "page": 20,
            "text": "Table A14: Welch's t-test on ratings collected on AMT for human-written stories (Day 1) and GPT-2 generated stories. Human-written stories are being rated higher for coherence and more relevance than GPT-2 generated stories (p<0.05)."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 2431
                },
                {
                    "x": 2181,
                    "y": 2431
                },
                {
                    "x": 2181,
                    "y": 2654
                },
                {
                    "x": 293,
                    "y": 2654
                }
            ],
            "category": "table",
            "html": "<table id='256' style='font-size:14px'><tr><td></td><td>mean (human)</td><td>mean (GPT-2)</td><td>difference</td><td>95% CIlower</td><td>95% CI upper</td><td>t</td><td>df</td><td>p-val</td></tr><tr><td>grammar</td><td>3.86</td><td>3.94</td><td>0.08</td><td>-0.19</td><td>0.03</td><td>-1.50</td><td>1197.9</td><td>0.14</td></tr><tr><td>coherence</td><td>3.92</td><td>3.82</td><td>0.10</td><td>-0.02</td><td>0.23</td><td>1.72</td><td>1176.5</td><td>0.09</td></tr><tr><td>relevance</td><td>3.71</td><td>3.44</td><td>0.27</td><td>0.13</td><td>0.41</td><td>3.69</td><td>1123.5</td><td><0.001</td></tr><tr><td>likability</td><td>3.73</td><td>3.42</td><td>0.31</td><td>0.19</td><td>0.44</td><td>4.89</td><td>1128.9</td><td><0.001</td></tr></table>",
            "id": 256,
            "page": 20,
            "text": "mean (human) mean (GPT-2) difference 95% CIlower 95% CI upper t df p-val  grammar 3.86 3.94 0.08 -0.19 0.03 -1.50 1197.9 0.14  coherence 3.92 3.82 0.10 -0.02 0.23 1.72 1176.5 0.09  relevance 3.71 3.44 0.27 0.13 0.41 3.69 1123.5 <0.001  likability 3.73 3.42 0.31 0.19 0.44 4.89 1128.9"
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 2690
                },
                {
                    "x": 2193,
                    "y": 2690
                },
                {
                    "x": 2193,
                    "y": 2793
                },
                {
                    "x": 286,
                    "y": 2793
                }
            ],
            "category": "caption",
            "html": "<caption id='257' style='font-size:16px'>Table A15: Welch's t-test on ratings collected on AMT for human-written stories (Day 2) and GPT-2 generated<br>stories. Human-written stories were rated higher for relevance and likability than GPT-2 generated stories (p<0.05).</caption>",
            "id": 257,
            "page": 20,
            "text": "Table A15: Welch's t-test on ratings collected on AMT for human-written stories (Day 2) and GPT-2 generated stories. Human-written stories were rated higher for relevance and likability than GPT-2 generated stories (p<0.05)."
        },
        {
            "bounding_box": [
                {
                    "x": 293,
                    "y": 2838
                },
                {
                    "x": 2182,
                    "y": 2838
                },
                {
                    "x": 2182,
                    "y": 3064
                },
                {
                    "x": 293,
                    "y": 3064
                }
            ],
            "category": "table",
            "html": "<table id='258' style='font-size:14px'><tr><td></td><td>mean (human)</td><td>mean (GPT-2)</td><td>difference</td><td>95% CI lower</td><td>95% CI upper</td><td>t</td><td>df</td><td>p-val</td></tr><tr><td>grammar</td><td>3.98</td><td>3.94</td><td>0.04</td><td>-0.07</td><td>0.15</td><td>0.70</td><td>1196.4</td><td>0.48</td></tr><tr><td>coherence</td><td>4.05</td><td>3.82</td><td>0.23</td><td>0.12</td><td>0.36</td><td>3.98</td><td>1163.5</td><td><0.001</td></tr><tr><td>relevance</td><td>3.46</td><td>3.44</td><td>0.02</td><td>-0.13</td><td>0.17</td><td>0.27</td><td>1188.9</td><td>0.80</td></tr><tr><td>likability</td><td>3.42</td><td>3.42</td><td>0.00</td><td>-0.14</td><td>0.14</td><td>0.00</td><td>1192.5</td><td>1</td></tr></table>",
            "id": 258,
            "page": 20,
            "text": "mean (human) mean (GPT-2) difference 95% CI lower 95% CI upper t df p-val  grammar 3.98 3.94 0.04 -0.07 0.15 0.70 1196.4 0.48  coherence 4.05 3.82 0.23 0.12 0.36 3.98 1163.5 <0.001  relevance 3.46 3.44 0.02 -0.13 0.17 0.27 1188.9 0.80  likability 3.42 3.42 0.00 -0.14 0.14 0.00 1192.5"
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 3099
                },
                {
                    "x": 2192,
                    "y": 3099
                },
                {
                    "x": 2192,
                    "y": 3200
                },
                {
                    "x": 286,
                    "y": 3200
                }
            ],
            "category": "paragraph",
            "html": "<p id='259' style='font-size:16px'>Table A16: Welch's t-test for ratings collected on AMT for human-written stories (Day 3) and GPT-2 generated<br>stories. Human-written stories were rated higher for coherence than GPT-2 generated stories (p<0.05).</p>",
            "id": 259,
            "page": 20,
            "text": "Table A16: Welch's t-test for ratings collected on AMT for human-written stories (Day 3) and GPT-2 generated stories. Human-written stories were rated higher for coherence than GPT-2 generated stories (p<0.05)."
        },
        {
            "bounding_box": [
                {
                    "x": 291,
                    "y": 806
                },
                {
                    "x": 2185,
                    "y": 806
                },
                {
                    "x": 2185,
                    "y": 1032
                },
                {
                    "x": 291,
                    "y": 1032
                }
            ],
            "category": "table",
            "html": "<table id='260' style='font-size:14px'><tr><td></td><td>mean (human)</td><td>mean (GPT-2)</td><td>difference</td><td>95% CIlower</td><td>95% CI upper</td><td>t</td><td>df</td><td>p-val</td></tr><tr><td>grammar</td><td>3.82</td><td>3.94</td><td>0.12</td><td>-0.23</td><td>-0.01</td><td>-2.11</td><td>1183.1</td><td>0.04</td></tr><tr><td>coherence</td><td>3.45</td><td>3.82</td><td>0.37</td><td>-0.49</td><td>-0.23</td><td>-5.42</td><td>1194.2</td><td><0.001</td></tr><tr><td>relevance</td><td>3.25</td><td>3.44</td><td>0.19</td><td>-0.35</td><td>-0.04</td><td>-2.50</td><td>1185</td><td>0.01</td></tr><tr><td>likability</td><td>3.32</td><td>3.42</td><td>0.10</td><td>-0.24</td><td>0.04</td><td>-1.41</td><td>1197.9</td><td>0.16</td></tr></table>",
            "id": 260,
            "page": 21,
            "text": "mean (human) mean (GPT-2) difference 95% CIlower 95% CI upper t df p-val  grammar 3.82 3.94 0.12 -0.23 -0.01 -2.11 1183.1 0.04  coherence 3.45 3.82 0.37 -0.49 -0.23 -5.42 1194.2 <0.001  relevance 3.25 3.44 0.19 -0.35 -0.04 -2.50 1185 0.01  likability 3.32 3.42 0.10 -0.24 0.04 -1.41 1197.9"
        },
        {
            "bounding_box": [
                {
                    "x": 286,
                    "y": 1064
                },
                {
                    "x": 2195,
                    "y": 1064
                },
                {
                    "x": 2195,
                    "y": 1221
                },
                {
                    "x": 286,
                    "y": 1221
                }
            ],
            "category": "caption",
            "html": "<caption id='261' style='font-size:18px'>Table A17: Welch 's t-test for ratings collected on AMT for human-written stories (non-English speaking countries)<br>and GPT-2 generated stories. GPT-2 generated stories were rated higher for grammar, coherence, and relevance<br>than human-written stories (p<0.05)</caption>",
            "id": 261,
            "page": 21,
            "text": "Table A17: Welch 's t-test for ratings collected on AMT for human-written stories (non-English speaking countries) and GPT-2 generated stories. GPT-2 generated stories were rated higher for grammar, coherence, and relevance than human-written stories (p<0.05)"
        },
        {
            "bounding_box": [
                {
                    "x": 288,
                    "y": 2283
                },
                {
                    "x": 2187,
                    "y": 2283
                },
                {
                    "x": 2187,
                    "y": 2506
                },
                {
                    "x": 288,
                    "y": 2506
                }
            ],
            "category": "table",
            "html": "<table id='262' style='font-size:14px'><tr><td></td><td>mean (human)</td><td>mean (GPT-2)</td><td>difference</td><td>95% CI lower</td><td>95% CI upper</td><td>t</td><td>df</td><td>p-val</td></tr><tr><td>grammar</td><td>3.83</td><td>3.82</td><td>0.01</td><td>-0.09</td><td>0.12</td><td>0.28</td><td>1188.1</td><td>0.78</td></tr><tr><td>coherence</td><td>3.83</td><td>3.39</td><td>0.44</td><td>0.32</td><td>0.57</td><td>6.92</td><td>1198</td><td><0.001</td></tr><tr><td>relevance</td><td>3.49</td><td>2.70</td><td>0.79</td><td>0.65</td><td>0.93</td><td>10.85</td><td>1198</td><td><0.001</td></tr><tr><td>likability</td><td>3.48</td><td>2.99</td><td>0.49</td><td>0.37</td><td>0.62</td><td>7.72</td><td>1195.2</td><td><0.001</td></tr></table>",
            "id": 262,
            "page": 21,
            "text": "mean (human) mean (GPT-2) difference 95% CI lower 95% CI upper t df p-val  grammar 3.83 3.82 0.01 -0.09 0.12 0.28 1188.1 0.78  coherence 3.83 3.39 0.44 0.32 0.57 6.92 1198 <0.001  relevance 3.49 2.70 0.79 0.65 0.93 10.85 1198 <0.001  likability 3.48 2.99 0.49 0.37 0.62 7.72 1195.2"
        },
        {
            "bounding_box": [
                {
                    "x": 285,
                    "y": 2538
                },
                {
                    "x": 2195,
                    "y": 2538
                },
                {
                    "x": 2195,
                    "y": 2696
                },
                {
                    "x": 285,
                    "y": 2696
                }
            ],
            "category": "caption",
            "html": "<caption id='263' style='font-size:18px'>Table A18: Welch's t-test for ratings collected on AMT for human-written stories and GPT-2 generated stories<br>(both stories shown in one HIT). GPT-2 generated stories were rated lower for coherence, relevance, and likability<br>than human-written stories (p<0.05) which is in line with the ratings provided by English teachers.</caption>",
            "id": 263,
            "page": 21,
            "text": "Table A18: Welch's t-test for ratings collected on AMT for human-written stories and GPT-2 generated stories (both stories shown in one HIT). GPT-2 generated stories were rated lower for coherence, relevance, and likability than human-written stories (p<0.05) which is in line with the ratings provided by English teachers."
        }
    ]
}