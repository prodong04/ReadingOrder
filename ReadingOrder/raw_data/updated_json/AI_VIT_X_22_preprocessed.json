{
    "id": "32a4d01e-0f94-11ef-8828-426932df3dcf",
    "pdf_path": "/root/data/pdf/1507.01526v3.pdf",
    "elements": [
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 110
                },
                {
                    "x": 1288,
                    "y": 110
                },
                {
                    "x": 1288,
                    "y": 157
                },
                {
                    "x": 445,
                    "y": 157
                }
            ],
            "category": "header",
            "html": "<header id='0' style='font-size:16px'>Under review as a conference paper at ICLR 2016</header>",
            "id": 0,
            "page": 1,
            "text": "Under review as a conference paper at ICLR 2016"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 334
                },
                {
                    "x": 1553,
                    "y": 334
                },
                {
                    "x": 1553,
                    "y": 401
                },
                {
                    "x": 443,
                    "y": 401
                }
            ],
            "category": "paragraph",
            "html": "<p id='1' style='font-size:22px'>GRID LONG SHORT-TERM MEMORY</p>",
            "id": 1,
            "page": 1,
            "text": "GRID LONG SHORT-TERM MEMORY"
        },
        {
            "bounding_box": [
                {
                    "x": 470,
                    "y": 482
                },
                {
                    "x": 1379,
                    "y": 482
                },
                {
                    "x": 1379,
                    "y": 529
                },
                {
                    "x": 470,
                    "y": 529
                }
            ],
            "category": "paragraph",
            "html": "<p id='2' style='font-size:18px'>Nal Kalchbrenner & Ivo Danihelka & Alex Graves</p>",
            "id": 2,
            "page": 1,
            "text": "Nal Kalchbrenner & Ivo Danihelka & Alex Graves"
        },
        {
            "bounding_box": [
                {
                    "x": 471,
                    "y": 531
                },
                {
                    "x": 907,
                    "y": 531
                },
                {
                    "x": 907,
                    "y": 620
                },
                {
                    "x": 471,
                    "y": 620
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='3' style='font-size:20px'>Google DeepMind<br>London, United Kingdom</p>",
            "id": 3,
            "page": 1,
            "text": "Google DeepMind London, United Kingdom"
        },
        {
            "bounding_box": [
                {
                    "x": 477,
                    "y": 624
                },
                {
                    "x": 1342,
                    "y": 624
                },
                {
                    "x": 1342,
                    "y": 666
                },
                {
                    "x": 477,
                    "y": 666
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='4' style='font-size:18px'>{nalk, danihelka, gravesa}@google · com</p>",
            "id": 4,
            "page": 1,
            "text": "{nalk, danihelka, gravesa}@google · com"
        },
        {
            "bounding_box": [
                {
                    "x": 1157,
                    "y": 789
                },
                {
                    "x": 1393,
                    "y": 789
                },
                {
                    "x": 1393,
                    "y": 837
                },
                {
                    "x": 1157,
                    "y": 837
                }
            ],
            "category": "paragraph",
            "html": "<p id='5' style='font-size:22px'>ABSTRACT</p>",
            "id": 5,
            "page": 1,
            "text": "ABSTRACT"
        },
        {
            "bounding_box": [
                {
                    "x": 589,
                    "y": 894
                },
                {
                    "x": 1961,
                    "y": 894
                },
                {
                    "x": 1961,
                    "y": 1538
                },
                {
                    "x": 589,
                    "y": 1538
                }
            ],
            "category": "paragraph",
            "html": "<p id='6' style='font-size:18px'>This paper introduces Grid Long Short-Term Memory, a network of LSTM cells<br>arranged in a multidimensional grid that can be applied to vectors, sequences or<br>higher dimensional data such as images. The network differs from existing deep<br>LSTM architectures in that the cells are connected between network layers as<br>well as along the spatiotemporal dimensions of the data. The network provides<br>a unified way of using LSTM for both deep and sequential computation. We ap-<br>ply the model to algorithmic tasks such as 15-digit integer addition and sequence<br>memorization, where itis able to significantly outperform the standard LSTM. We<br>then give results for two empirical tasks. We find that 2D Grid LSTM achieves<br>1.47 bits per character on the Wikipedia character prediction benchmark, which is<br>state-of-the-art among neural approaches. In addition, we use the Grid LSTM to<br>define a novel two-dimensional translation model, the Reencoder, and show that it<br>outperforms a phrase-based reference system on a Chinese-to-English translation<br>task.</p>",
            "id": 6,
            "page": 1,
            "text": "This paper introduces Grid Long Short-Term Memory, a network of LSTM cells arranged in a multidimensional grid that can be applied to vectors, sequences or higher dimensional data such as images. The network differs from existing deep LSTM architectures in that the cells are connected between network layers as well as along the spatiotemporal dimensions of the data. The network provides a unified way of using LSTM for both deep and sequential computation. We apply the model to algorithmic tasks such as 15-digit integer addition and sequence memorization, where itis able to significantly outperform the standard LSTM. We then give results for two empirical tasks. We find that 2D Grid LSTM achieves 1.47 bits per character on the Wikipedia character prediction benchmark, which is state-of-the-art among neural approaches. In addition, we use the Grid LSTM to define a novel two-dimensional translation model, the Reencoder, and show that it outperforms a phrase-based reference system on a Chinese-to-English translation task."
        },
        {
            "bounding_box": [
                {
                    "x": 450,
                    "y": 1637
                },
                {
                    "x": 862,
                    "y": 1637
                },
                {
                    "x": 862,
                    "y": 1687
                },
                {
                    "x": 450,
                    "y": 1687
                }
            ],
            "category": "paragraph",
            "html": "<p id='7' style='font-size:20px'>1 INTRODUCTION</p>",
            "id": 7,
            "page": 1,
            "text": "1 INTRODUCTION"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1742
                },
                {
                    "x": 2109,
                    "y": 1742
                },
                {
                    "x": 2109,
                    "y": 2248
                },
                {
                    "x": 441,
                    "y": 2248
                }
            ],
            "category": "paragraph",
            "html": "<p id='8' style='font-size:18px'>Long Short-Term Memory (LSTM) networks are recurrent neural networks equipped with a special<br>gating mechanism that controls access to memory cells (Hochreiter & Schmidhuber, 1997). Since<br>the gates can prevent the rest of the network from modifying the contents of the memory cells for<br>multiple time steps, LSTM networks preserve signals and propagate errors for much longer than<br>ordinary recurrent neural networks. By independently reading, writing and erasing content from<br>the memory cells, the gates can also learn to attend to specific parts of the input signals and ignore<br>other parts. These properties allow LSTM networks to process data with complex and separated<br>interdependencies and to excel in a range of sequence learning domains such as speech recognition<br>(Graves et al., 2013), offline hand-writing recognition (Graves & Schmidhuber, 2008), machine<br>translation (Sutskever et al., 2014) and image-to-caption generation (Vinyals et al., 2014; Kiros<br>et al., 2014).</p>",
            "id": 8,
            "page": 1,
            "text": "Long Short-Term Memory (LSTM) networks are recurrent neural networks equipped with a special gating mechanism that controls access to memory cells (Hochreiter & Schmidhuber, 1997). Since the gates can prevent the rest of the network from modifying the contents of the memory cells for multiple time steps, LSTM networks preserve signals and propagate errors for much longer than ordinary recurrent neural networks. By independently reading, writing and erasing content from the memory cells, the gates can also learn to attend to specific parts of the input signals and ignore other parts. These properties allow LSTM networks to process data with complex and separated interdependencies and to excel in a range of sequence learning domains such as speech recognition (Graves , 2013), offline hand-writing recognition (Graves & Schmidhuber, 2008), machine translation (Sutskever , 2014) and image-to-caption generation (Vinyals , 2014; Kiros , 2014)."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2269
                },
                {
                    "x": 2108,
                    "y": 2269
                },
                {
                    "x": 2108,
                    "y": 2547
                },
                {
                    "x": 441,
                    "y": 2547
                }
            ],
            "category": "paragraph",
            "html": "<p id='9' style='font-size:18px'>Even for non-sequential data, the recent success of deep networks has shown that long chains of<br>sequential computation are key to finding and exploiting complex patterns. Deep networks suffer<br>from exactly the same problems as recurrent networks applied to long sequences: namely that infor-<br>mation from past computations rapidly attenuates as it progresses through the chain - the vanishing<br>gradient problem (Hochreiter, 1991) - and that each layer cannot dynamically select or ignore its<br>inputs. It therefore seems attractive to generalise the advantages of LSTM to deep computation.</p>",
            "id": 9,
            "page": 1,
            "text": "Even for non-sequential data, the recent success of deep networks has shown that long chains of sequential computation are key to finding and exploiting complex patterns. Deep networks suffer from exactly the same problems as recurrent networks applied to long sequences: namely that information from past computations rapidly attenuates as it progresses through the chain - the vanishing gradient problem (Hochreiter, 1991) - and that each layer cannot dynamically select or ignore its inputs. It therefore seems attractive to generalise the advantages of LSTM to deep computation."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2569
                },
                {
                    "x": 2108,
                    "y": 2569
                },
                {
                    "x": 2108,
                    "y": 2846
                },
                {
                    "x": 441,
                    "y": 2846
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='10' style='font-size:16px'>We extend LSTM cells to deep networks within a unified architecture. We introduce Grid LSTM, a<br>network that is arranged in a grid of one or more dimensions. The network has LSTM cells along<br>any or all of the dimensions of the grid. The depth dimension is treated like the other dimensions<br>and also uses LSTM cells to communicate directly from one layer to the next. Since the number N<br>of dimensions in the grid can easily be 2 or more, we propose a novel, robust way for modulating<br>the N-way communication across the LSTM cells.</p>",
            "id": 10,
            "page": 1,
            "text": "We extend LSTM cells to deep networks within a unified architecture. We introduce Grid LSTM, a network that is arranged in a grid of one or more dimensions. The network has LSTM cells along any or all of the dimensions of the grid. The depth dimension is treated like the other dimensions and also uses LSTM cells to communicate directly from one layer to the next. Since the number N of dimensions in the grid can easily be 2 or more, we propose a novel, robust way for modulating the N-way communication across the LSTM cells."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2867
                },
                {
                    "x": 2109,
                    "y": 2867
                },
                {
                    "x": 2109,
                    "y": 3054
                },
                {
                    "x": 441,
                    "y": 3054
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='11' style='font-size:16px'>N-dimensional Grid LSTM (N-LSTM for short) can naturally be applied as feed-forward networks<br>as well as recurrent ones. One-dimensional Grid LSTM corresponds to a feed-forward network that<br>uses LSTM cells in place of transfer functions such as tanh and ReLU (Nair & Hinton, 2010). These<br>networks are related to Highway Networks (Srivastava et al., 2015) where a gated transfer function</p>",
            "id": 11,
            "page": 1,
            "text": "N-dimensional Grid LSTM (N-LSTM for short) can naturally be applied as feed-forward networks as well as recurrent ones. One-dimensional Grid LSTM corresponds to a feed-forward network that uses LSTM cells in place of transfer functions such as tanh and ReLU (Nair & Hinton, 2010). These networks are related to Highway Networks (Srivastava , 2015) where a gated transfer function"
        },
        {
            "bounding_box": [
                {
                    "x": 65,
                    "y": 913
                },
                {
                    "x": 148,
                    "y": 913
                },
                {
                    "x": 148,
                    "y": 2320
                },
                {
                    "x": 65,
                    "y": 2320
                }
            ],
            "category": "footer",
            "html": "<br><footer id='12' style='font-size:14px'>2016<br>Jan<br>L<br>[cs.NE]<br>arXiv:1507.01526v3</footer>",
            "id": 12,
            "page": 1,
            "text": "2016 Jan L [cs.NE] arXiv:1507.01526v3"
        },
        {
            "bounding_box": [
                {
                    "x": 1261,
                    "y": 3134
                },
                {
                    "x": 1289,
                    "y": 3134
                },
                {
                    "x": 1289,
                    "y": 3172
                },
                {
                    "x": 1261,
                    "y": 3172
                }
            ],
            "category": "footer",
            "html": "<footer id='13' style='font-size:14px'>1</footer>",
            "id": 13,
            "page": 1,
            "text": "1"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 110
                },
                {
                    "x": 1289,
                    "y": 110
                },
                {
                    "x": 1289,
                    "y": 158
                },
                {
                    "x": 444,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='14' style='font-size:14px'>Under review as a conference paper at ICLR 2016</header>",
            "id": 14,
            "page": 2,
            "text": "Under review as a conference paper at ICLR 2016"
        },
        {
            "bounding_box": [
                {
                    "x": 435,
                    "y": 332
                },
                {
                    "x": 2109,
                    "y": 332
                },
                {
                    "x": 2109,
                    "y": 793
                },
                {
                    "x": 435,
                    "y": 793
                }
            ],
            "category": "figure",
            "html": "<figure><img id='15' style='font-size:14px' alt=\"h/ h'1 m'\nm m m2 m2\nh h' h2 h'2\nI * xi h1 m1\nStandard LSTM block 1d Grid LSTM Block 2d Grid LSTM block 3d Grid LSTM Block\" data-coord=\"top-left:(435,332); bottom-right:(2109,793)\" /></figure>",
            "id": 15,
            "page": 2,
            "text": "h/ h'1 m' m m m2 m2 h h' h2 h'2 I * xi h1 m1 Standard LSTM block 1d Grid LSTM Block 2d Grid LSTM block 3d Grid LSTM Block"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 837
                },
                {
                    "x": 2109,
                    "y": 837
                },
                {
                    "x": 2109,
                    "y": 1028
                },
                {
                    "x": 440,
                    "y": 1028
                }
            ],
            "category": "caption",
            "html": "<caption id='16' style='font-size:16px'>Figure 1: Blocks form the standard LSTM and those that form Grid LSTM networks of N = 1, 2<br>and 3 dimensions. The dashed lines indicate identity transformations. The standard LSTM block<br>does not have a memory vector in the vertical dimension; by contrast, the 2d Grid LSTM block has<br>the memory vector m1 applied along the vertical dimension.</caption>",
            "id": 16,
            "page": 2,
            "text": "Figure 1: Blocks form the standard LSTM and those that form Grid LSTM networks of N = 1, 2 and 3 dimensions. The dashed lines indicate identity transformations. The standard LSTM block does not have a memory vector in the vertical dimension; by contrast, the 2d Grid LSTM block has the memory vector m1 applied along the vertical dimension."
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 1171
                },
                {
                    "x": 2108,
                    "y": 1171
                },
                {
                    "x": 2108,
                    "y": 1450
                },
                {
                    "x": 440,
                    "y": 1450
                }
            ],
            "category": "paragraph",
            "html": "<p id='17' style='font-size:20px'>is used to successfully train feed-forward networks with up to 900 layers of depth. Grid LSTM with<br>two dimensions is analogous to the Stacked LSTM, but it adds cells along the depth dimension too.<br>Grid LSTM with three or more dimensions is analogous to Multidimensional LSTM (Graves et al.,<br>2013; Sutskever et al., 2014; Graves et al., 2007; Graves, 2012), but differs from it not just by having<br>the cells along the depth dimension, but also by using the proposed mechanism for modulating the<br>N-way interaction that is not prone to the instability present in Multidimesional LSTM.</p>",
            "id": 17,
            "page": 2,
            "text": "is used to successfully train feed-forward networks with up to 900 layers of depth. Grid LSTM with two dimensions is analogous to the Stacked LSTM, but it adds cells along the depth dimension too. Grid LSTM with three or more dimensions is analogous to Multidimensional LSTM (Graves , 2013; Sutskever , 2014; Graves , 2007; Graves, 2012), but differs from it not just by having the cells along the depth dimension, but also by using the proposed mechanism for modulating the N-way interaction that is not prone to the instability present in Multidimesional LSTM."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1470
                },
                {
                    "x": 2108,
                    "y": 1470
                },
                {
                    "x": 2108,
                    "y": 1750
                },
                {
                    "x": 441,
                    "y": 1750
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='18' style='font-size:20px'>We study some of the learning properties of Grid LSTM in various algorithmic tasks. We compare<br>the performance of two-dimensional Grid LSTM to Stacked LSTM on computing the addition of two<br>15-digit integers without curriculum learning and on memorizing sequences of numbers (Zaremba<br>& Sutskever, 2014). We find that in these settings having cells along the depth dimension is more<br>effective than not having them; similarly, tying the weights across the layers is also more effective<br>than untying the weights, despite the reduced number of parameters.</p>",
            "id": 18,
            "page": 2,
            "text": "We study some of the learning properties of Grid LSTM in various algorithmic tasks. We compare the performance of two-dimensional Grid LSTM to Stacked LSTM on computing the addition of two 15-digit integers without curriculum learning and on memorizing sequences of numbers (Zaremba & Sutskever, 2014). We find that in these settings having cells along the depth dimension is more effective than not having them; similarly, tying the weights across the layers is also more effective than untying the weights, despite the reduced number of parameters."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1770
                },
                {
                    "x": 2109,
                    "y": 1770
                },
                {
                    "x": 2109,
                    "y": 2093
                },
                {
                    "x": 441,
                    "y": 2093
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='19' style='font-size:18px'>We also apply Grid LSTM to two empirical tasks. The architecture achieves 1.47 bits-per-character<br>in the 100M characters Wikipedia dataset (Hutter, 2012) outperforming other neural networks. Sec-<br>ondly, we use Grid LSTM to define a novel neural translation model that re-encodes the source sen-<br>tence based on the target words generated up to that point. The network outperforms the reference<br>phrase-based CDEC system (Dyer et al., 2010) on the IWSLT BTEC Chinese-to-Ensligh transla-<br>tion task. The appendix contains additional results for Grid LSTM on learning parity functions and<br>classifying MNIST images.</p>",
            "id": 19,
            "page": 2,
            "text": "We also apply Grid LSTM to two empirical tasks. The architecture achieves 1.47 bits-per-character in the 100M characters Wikipedia dataset (Hutter, 2012) outperforming other neural networks. Secondly, we use Grid LSTM to define a novel neural translation model that re-encodes the source sentence based on the target words generated up to that point. The network outperforms the reference phrase-based CDEC system (Dyer , 2010) on the IWSLT BTEC Chinese-to-Ensligh translation task. The appendix contains additional results for Grid LSTM on learning parity functions and classifying MNIST images."
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 2114
                },
                {
                    "x": 2108,
                    "y": 2114
                },
                {
                    "x": 2108,
                    "y": 2255
                },
                {
                    "x": 440,
                    "y": 2255
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='20' style='font-size:16px'>The outline of the paper is as follows. In Sect. 2 we describe standard LSTM networks that comprise<br>the background. In Sect. 3 we define the Grid LSTM architecture. In Sect. 4 we consider the six<br>experiments and we conclude in Sect. 5.</p>",
            "id": 20,
            "page": 2,
            "text": "The outline of the paper is as follows. In Sect. 2 we describe standard LSTM networks that comprise the background. In Sect. 3 we define the Grid LSTM architecture. In Sect. 4 we consider the six experiments and we conclude in Sect. 5."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 2391
                },
                {
                    "x": 838,
                    "y": 2391
                },
                {
                    "x": 838,
                    "y": 2443
                },
                {
                    "x": 445,
                    "y": 2443
                }
            ],
            "category": "paragraph",
            "html": "<p id='21' style='font-size:22px'>2 BACKGROUND</p>",
            "id": 21,
            "page": 2,
            "text": "2 BACKGROUND"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2535
                },
                {
                    "x": 2107,
                    "y": 2535
                },
                {
                    "x": 2107,
                    "y": 2630
                },
                {
                    "x": 442,
                    "y": 2630
                }
            ],
            "category": "paragraph",
            "html": "<p id='22' style='font-size:16px'>We begin by describing the standard LSTM recurrent neural network and the derived Stacked and<br>Multidimensional LSTM networks; some aspects of the networks motivate the Grid LSTM.</p>",
            "id": 22,
            "page": 2,
            "text": "We begin by describing the standard LSTM recurrent neural network and the derived Stacked and Multidimensional LSTM networks; some aspects of the networks motivate the Grid LSTM."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2754
                },
                {
                    "x": 1085,
                    "y": 2754
                },
                {
                    "x": 1085,
                    "y": 2802
                },
                {
                    "x": 444,
                    "y": 2802
                }
            ],
            "category": "paragraph",
            "html": "<p id='23' style='font-size:18px'>2.1 LONG SHORT- TERM MEMORY</p>",
            "id": 23,
            "page": 2,
            "text": "2.1 LONG SHORT- TERM MEMORY"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2866
                },
                {
                    "x": 2109,
                    "y": 2866
                },
                {
                    "x": 2109,
                    "y": 3055
                },
                {
                    "x": 441,
                    "y": 3055
                }
            ],
            "category": "paragraph",
            "html": "<p id='24' style='font-size:16px'>The LSTM network processes a sequence of input and target pairs (x1, y1) , · .., (xm, ym). For each<br>pair (xi, Yi) the LSTM network takes the new input Xi and produces an estimate for the target<br>Yi given all the previous inputs x1, · .. , xi. The past inputs X1, ..., Xi-1 determine the state of the<br>network that comprises a hidden vector h E Rd and a memory vector m E Rd. The computation at</p>",
            "id": 24,
            "page": 2,
            "text": "The LSTM network processes a sequence of input and target pairs (x1, y1) , · .., (xm, ym). For each pair (xi, Yi) the LSTM network takes the new input Xi and produces an estimate for the target Yi given all the previous inputs x1, · .. , xi. The past inputs X1, ..., Xi-1 determine the state of the network that comprises a hidden vector h E Rd and a memory vector m E Rd. The computation at"
        },
        {
            "bounding_box": [
                {
                    "x": 1259,
                    "y": 3132
                },
                {
                    "x": 1290,
                    "y": 3132
                },
                {
                    "x": 1290,
                    "y": 3172
                },
                {
                    "x": 1259,
                    "y": 3172
                }
            ],
            "category": "footer",
            "html": "<footer id='25' style='font-size:18px'>2</footer>",
            "id": 25,
            "page": 2,
            "text": "2"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 111
                },
                {
                    "x": 1287,
                    "y": 111
                },
                {
                    "x": 1287,
                    "y": 157
                },
                {
                    "x": 444,
                    "y": 157
                }
            ],
            "category": "header",
            "html": "<header id='26' style='font-size:16px'>Under review as a conference paper at ICLR 2016</header>",
            "id": 26,
            "page": 3,
            "text": "Under review as a conference paper at ICLR 2016"
        },
        {
            "bounding_box": [
                {
                    "x": 436,
                    "y": 325
                },
                {
                    "x": 2116,
                    "y": 325
                },
                {
                    "x": 2116,
                    "y": 1112
                },
                {
                    "x": 436,
                    "y": 1112
                }
            ],
            "category": "figure",
            "html": "<figure><img id='27' style='font-size:14px' alt=\"- R\n수\nH H\nStacked LSTM 2d Grid LSTM\" data-coord=\"top-left:(436,325); bottom-right:(2116,1112)\" /></figure>",
            "id": 27,
            "page": 3,
            "text": "- R 수 H H Stacked LSTM 2d Grid LSTM"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 1156
                },
                {
                    "x": 2108,
                    "y": 1156
                },
                {
                    "x": 2108,
                    "y": 1301
                },
                {
                    "x": 440,
                    "y": 1301
                }
            ],
            "category": "caption",
            "html": "<caption id='28' style='font-size:20px'>Figure 2: Stacked LSTM and 2d Grid LSTM applied to character prediction composed from the<br>respective blocks (Fig. 1). Note how in the Grid LSTM the signal flows through LSTM cells (shaded<br>rectangles) along both the time and the depth dimensions.</caption>",
            "id": 28,
            "page": 3,
            "text": "Figure 2: Stacked LSTM and 2d Grid LSTM applied to character prediction composed from the respective blocks (Fig. 1). Note how in the Grid LSTM the signal flows through LSTM cells (shaded rectangles) along both the time and the depth dimensions."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1398
                },
                {
                    "x": 1315,
                    "y": 1398
                },
                {
                    "x": 1315,
                    "y": 1442
                },
                {
                    "x": 443,
                    "y": 1442
                }
            ],
            "category": "paragraph",
            "html": "<p id='29' style='font-size:18px'>each step is defined as follows (Graves et al., 2013):</p>",
            "id": 29,
            "page": 3,
            "text": "each step is defined as follows (Graves , 2013):"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 1891
                },
                {
                    "x": 2107,
                    "y": 1891
                },
                {
                    "x": 2107,
                    "y": 2030
                },
                {
                    "x": 440,
                    "y": 2030
                }
            ],
            "category": "paragraph",
            "html": "<p id='30' style='font-size:18px'>where 0 is the logistic sigmoid function, Wu Wf W°, Wc in Rdx2d<br>are the recurrent weight<br>, ,<br>matrices of the network and H E R2d is the concatenation of the new input Xi, transformed by a<br>projection matrix I, and the previous hidden vector h:</p>",
            "id": 30,
            "page": 3,
            "text": "where 0 is the logistic sigmoid function, Wu Wf W°, Wc in Rdx2d are the recurrent weight , , matrices of the network and H E R2d is the concatenation of the new input Xi, transformed by a projection matrix I, and the previous hidden vector h:"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 2211
                },
                {
                    "x": 2106,
                    "y": 2211
                },
                {
                    "x": 2106,
                    "y": 2353
                },
                {
                    "x": 440,
                    "y": 2353
                }
            ],
            "category": "paragraph",
            "html": "<p id='31' style='font-size:18px'>The computation outputs new hidden and memory vectors h' and m` that comprise the next state of<br>the network. The estimate for the target is then computed in terms of the hidden vector h'. We use<br>the functional LSTM(·, ., ·) as shorthand for Eq. 1 as follows:</p>",
            "id": 31,
            "page": 3,
            "text": "The computation outputs new hidden and memory vectors h' and m` that comprise the next state of the network. The estimate for the target is then computed in terms of the hidden vector h'. We use the functional LSTM(·, ., ·) as shorthand for Eq. 1 as follows:"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2476
                },
                {
                    "x": 1604,
                    "y": 2476
                },
                {
                    "x": 1604,
                    "y": 2524
                },
                {
                    "x": 443,
                    "y": 2524
                }
            ],
            "category": "paragraph",
            "html": "<p id='32' style='font-size:14px'>where W concatenates the four weight matrices Wu Wf , W° Wc.<br>,<br>,</p>",
            "id": 32,
            "page": 3,
            "text": "where W concatenates the four weight matrices Wu Wf , W° Wc. , ,"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 2548
                },
                {
                    "x": 2107,
                    "y": 2548
                },
                {
                    "x": 2107,
                    "y": 3054
                },
                {
                    "x": 440,
                    "y": 3054
                }
            ],
            "category": "paragraph",
            "html": "<p id='33' style='font-size:18px'>One aspect of LSTM networks is the role of the gates gu , gf , g° and gc. The forget gate gf can<br>delete parts of the previous memory vector mi-1 whereas the gate gc can write new content to the<br>new memory mi modulated by the input gate gu. The output gate controls what is then read from the<br>new memory mi onto the hidden vector hi. The mechanism has two important learning properties.<br>Each memory vector is obtained by a linear transformation of the previous memory vector and the<br>gates; this ensures that the forward signals from one step to the other are not repeatedly squashed<br>by a non-linearity such as tanh and that the backward error signals do not decay sharply at each<br>step, an issue known as the vanishing gradient problem (Hochreiter et al., 2001). The mechanism<br>also acts as a memory and implicit attention system, whereby the signal from some input Xi can be<br>written to the memory vector and attended to in parts across multiple steps by being retrieved one<br>part at a time.</p>",
            "id": 33,
            "page": 3,
            "text": "One aspect of LSTM networks is the role of the gates gu , gf , g° and gc. The forget gate gf can delete parts of the previous memory vector mi-1 whereas the gate gc can write new content to the new memory mi modulated by the input gate gu. The output gate controls what is then read from the new memory mi onto the hidden vector hi. The mechanism has two important learning properties. Each memory vector is obtained by a linear transformation of the previous memory vector and the gates; this ensures that the forward signals from one step to the other are not repeatedly squashed by a non-linearity such as tanh and that the backward error signals do not decay sharply at each step, an issue known as the vanishing gradient problem (Hochreiter , 2001). The mechanism also acts as a memory and implicit attention system, whereby the signal from some input Xi can be written to the memory vector and attended to in parts across multiple steps by being retrieved one part at a time."
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3135
                },
                {
                    "x": 1288,
                    "y": 3135
                },
                {
                    "x": 1288,
                    "y": 3170
                },
                {
                    "x": 1260,
                    "y": 3170
                }
            ],
            "category": "footer",
            "html": "<footer id='34' style='font-size:16px'>3</footer>",
            "id": 34,
            "page": 3,
            "text": "3"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 110
                },
                {
                    "x": 1289,
                    "y": 110
                },
                {
                    "x": 1289,
                    "y": 158
                },
                {
                    "x": 444,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='35' style='font-size:16px'>Under review as a conference paper at ICLR 2016</header>",
            "id": 35,
            "page": 4,
            "text": "Under review as a conference paper at ICLR 2016"
        },
        {
            "bounding_box": [
                {
                    "x": 759,
                    "y": 329
                },
                {
                    "x": 1890,
                    "y": 329
                },
                {
                    "x": 1890,
                    "y": 881
                },
                {
                    "x": 759,
                    "y": 881
                }
            ],
            "category": "figure",
            "html": "<figure><img id='36' style='font-size:14px' alt=\"V\nV.\nV.\n1000110010\n1d Grid LSTM 3d Grid LSTM\" data-coord=\"top-left:(759,329); bottom-right:(1890,881)\" /></figure>",
            "id": 36,
            "page": 4,
            "text": "V V. V. 1000110010 1d Grid LSTM 3d Grid LSTM"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 916
                },
                {
                    "x": 2109,
                    "y": 916
                },
                {
                    "x": 2109,
                    "y": 1060
                },
                {
                    "x": 440,
                    "y": 1060
                }
            ],
            "category": "caption",
            "html": "<caption id='37' style='font-size:20px'>Figure 3: Instances of one-dimensional and three-dimensional Grid LSTM. The network to the left<br>is used for the parity results in the appendix. The translation and MNIST models below are specific<br>instances of the 3d Grid LSTM to the right.</caption>",
            "id": 37,
            "page": 4,
            "text": "Figure 3: Instances of one-dimensional and three-dimensional Grid LSTM. The network to the left is used for the parity results in the appendix. The translation and MNIST models below are specific instances of the 3d Grid LSTM to the right."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 1168
                },
                {
                    "x": 851,
                    "y": 1168
                },
                {
                    "x": 851,
                    "y": 1217
                },
                {
                    "x": 444,
                    "y": 1217
                }
            ],
            "category": "paragraph",
            "html": "<p id='38' style='font-size:18px'>2.2 STACKED LSTM</p>",
            "id": 38,
            "page": 4,
            "text": "2.2 STACKED LSTM"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1265
                },
                {
                    "x": 2108,
                    "y": 1265
                },
                {
                    "x": 2108,
                    "y": 1544
                },
                {
                    "x": 441,
                    "y": 1544
                }
            ],
            "category": "paragraph",
            "html": "<p id='39' style='font-size:20px'>A model that is closely related to the standard LSTM network is Stacked LSTM (Graves et al., 2013;<br>Sutskever et al., 2014). Stacked LSTM adds capacity by stacking LSTM layers on top of each other.<br>The output hidden vector hi in Eq. 1 from the LSTM below is taken as the input to the LSTM above<br>in place of I * Xi. The Stacked LSTM is depicted in Fig. 2. Note that although the LSTM cells are<br>present along the sequential computation of each LSTM network, they are not present in the vertical<br>computation from one layer to the next.</p>",
            "id": 39,
            "page": 4,
            "text": "A model that is closely related to the standard LSTM network is Stacked LSTM (Graves , 2013; Sutskever , 2014). Stacked LSTM adds capacity by stacking LSTM layers on top of each other. The output hidden vector hi in Eq. 1 from the LSTM below is taken as the input to the LSTM above in place of I * Xi. The Stacked LSTM is depicted in Fig. 2. Note that although the LSTM cells are present along the sequential computation of each LSTM network, they are not present in the vertical computation from one layer to the next."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1626
                },
                {
                    "x": 1054,
                    "y": 1626
                },
                {
                    "x": 1054,
                    "y": 1675
                },
                {
                    "x": 442,
                    "y": 1675
                }
            ],
            "category": "paragraph",
            "html": "<p id='40' style='font-size:18px'>2.3 MULTIDIMENSIONAL LSTM</p>",
            "id": 40,
            "page": 4,
            "text": "2.3 MULTIDIMENSIONAL LSTM"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1723
                },
                {
                    "x": 2108,
                    "y": 1723
                },
                {
                    "x": 2108,
                    "y": 2099
                },
                {
                    "x": 442,
                    "y": 2099
                }
            ],
            "category": "paragraph",
            "html": "<p id='41' style='font-size:16px'>Another related model is Multidimensional LSTM (Graves et al., 2007). Here the inputs are not<br>arranged in a sequence, but in a N-dimensional grid, such as the two-dimensional grid of pixels<br>in an image. At each input x in the array the network receives N hidden vectors h1, · · · , hN and<br>N memory vectors m1, ..., mN and computes a hidden vector h and a memory vector m that are<br>passed as the next state for each of the N dimensions. The network concatenates the transformed<br>input I * x and the N hidden vectors h1, ..., hN into a vector H and as in Eq. 1 computes gu, g°<br>and gc, as well as N forget gates g.f. These gates are then used to compute the memory vector as<br>follows:</p>",
            "id": 41,
            "page": 4,
            "text": "Another related model is Multidimensional LSTM (Graves , 2007). Here the inputs are not arranged in a sequence, but in a N-dimensional grid, such as the two-dimensional grid of pixels in an image. At each input x in the array the network receives N hidden vectors h1, · · · , hN and N memory vectors m1, ..., mN and computes a hidden vector h and a memory vector m that are passed as the next state for each of the N dimensions. The network concatenates the transformed input I * x and the N hidden vectors h1, ..., hN into a vector H and as in Eq. 1 computes gu, g° and gc, as well as N forget gates g.f. These gates are then used to compute the memory vector as follows:"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2328
                },
                {
                    "x": 2108,
                    "y": 2328
                },
                {
                    "x": 2108,
                    "y": 2561
                },
                {
                    "x": 441,
                    "y": 2561
                }
            ],
            "category": "paragraph",
            "html": "<p id='42' style='font-size:20px'>As the number of paths in a grid grows combinatorially with the size of each dimension and the<br>total number of dimensions N, the values in m can grow at the same rate due to the unconstrained<br>summation in Eq. 4. This can cause instability for large grids, and adding cells along the depth<br>dimension increases N and exacerbates the problem. This motivates the simple alternate way of<br>computing the output memory vectors in the Grid LSTM.</p>",
            "id": 42,
            "page": 4,
            "text": "As the number of paths in a grid grows combinatorially with the size of each dimension and the total number of dimensions N, the values in m can grow at the same rate due to the unconstrained summation in Eq. 4. This can cause instability for large grids, and adding cells along the depth dimension increases N and exacerbates the problem. This motivates the simple alternate way of computing the output memory vectors in the Grid LSTM."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 2656
                },
                {
                    "x": 870,
                    "y": 2656
                },
                {
                    "x": 870,
                    "y": 2707
                },
                {
                    "x": 445,
                    "y": 2707
                }
            ],
            "category": "paragraph",
            "html": "<p id='43' style='font-size:22px'>3 ARCHITECTURE</p>",
            "id": 43,
            "page": 4,
            "text": "3 ARCHITECTURE"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2776
                },
                {
                    "x": 2108,
                    "y": 2776
                },
                {
                    "x": 2108,
                    "y": 3053
                },
                {
                    "x": 441,
                    "y": 3053
                }
            ],
            "category": "paragraph",
            "html": "<p id='44' style='font-size:20px'>Grid LSTM deploys cells along any or all of the dimensions including the depth of the network. In<br>the context of predicting a sequence, the Grid LSTM has cells along two dimensions, the temporal<br>one of the sequence itself and the vertical one along the depth. To modulate the interaction of the<br>cells in the two dimensions, the Grid LSTM proposes a simple mechanism where the values in the<br>cells cannot grow combinatorially as in Eq. 4. In this section we describe the multidimensional<br>blocks and the way in which they are combined to form a Grid LSTM.</p>",
            "id": 44,
            "page": 4,
            "text": "Grid LSTM deploys cells along any or all of the dimensions including the depth of the network. In the context of predicting a sequence, the Grid LSTM has cells along two dimensions, the temporal one of the sequence itself and the vertical one along the depth. To modulate the interaction of the cells in the two dimensions, the Grid LSTM proposes a simple mechanism where the values in the cells cannot grow combinatorially as in Eq. 4. In this section we describe the multidimensional blocks and the way in which they are combined to form a Grid LSTM."
        },
        {
            "bounding_box": [
                {
                    "x": 1258,
                    "y": 3134
                },
                {
                    "x": 1289,
                    "y": 3134
                },
                {
                    "x": 1289,
                    "y": 3170
                },
                {
                    "x": 1258,
                    "y": 3170
                }
            ],
            "category": "footer",
            "html": "<footer id='45' style='font-size:14px'>4</footer>",
            "id": 45,
            "page": 4,
            "text": "4"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 111
                },
                {
                    "x": 1288,
                    "y": 111
                },
                {
                    "x": 1288,
                    "y": 157
                },
                {
                    "x": 444,
                    "y": 157
                }
            ],
            "category": "header",
            "html": "<header id='46' style='font-size:14px'>Under review as a conference paper at ICLR 2016</header>",
            "id": 46,
            "page": 5,
            "text": "Under review as a conference paper at ICLR 2016"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 347
                },
                {
                    "x": 938,
                    "y": 347
                },
                {
                    "x": 938,
                    "y": 392
                },
                {
                    "x": 445,
                    "y": 392
                }
            ],
            "category": "paragraph",
            "html": "<p id='47' style='font-size:16px'>3.1 GRID LSTM BLOCKS</p>",
            "id": 47,
            "page": 5,
            "text": "3.1 GRID LSTM BLOCKS"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 431
                },
                {
                    "x": 2109,
                    "y": 431
                },
                {
                    "x": 2109,
                    "y": 572
                },
                {
                    "x": 441,
                    "y": 572
                }
            ],
            "category": "paragraph",
            "html": "<p id='48' style='font-size:14px'>As in multidimensional LSTM, a N-dimensional block in a Grid LSTM receives as input N hidden<br>vectors h1, .., hN and N memory vectors m1, ..., mN. Unlike the multidimensional case, the block<br>outputs N hidden vectors h'1, · .., h'N and N memory vectors m'1 , · · , m'N that are all distinct.</p>",
            "id": 48,
            "page": 5,
            "text": "As in multidimensional LSTM, a N-dimensional block in a Grid LSTM receives as input N hidden vectors h1, .., hN and N memory vectors m1, ..., mN. Unlike the multidimensional case, the block outputs N hidden vectors h'1, · .., h'N and N memory vectors m'1 , · · , m'N that are all distinct."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 595
                },
                {
                    "x": 2106,
                    "y": 595
                },
                {
                    "x": 2106,
                    "y": 685
                },
                {
                    "x": 441,
                    "y": 685
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='49' style='font-size:16px'>The computation is simple and proceeds as follows. The model first concatenates the input hidden<br>vectors from the N dimensions:</p>",
            "id": 49,
            "page": 5,
            "text": "The computation is simple and proceeds as follows. The model first concatenates the input hidden vectors from the N dimensions:"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 859
                },
                {
                    "x": 2108,
                    "y": 859
                },
                {
                    "x": 2108,
                    "y": 956
                },
                {
                    "x": 440,
                    "y": 956
                }
            ],
            "category": "paragraph",
            "html": "<p id='50' style='font-size:16px'>Then the block computes N transforms LSTM(·,·,·), one for each dimension, obtaining the desired<br>output hidden and memory vectors:</p>",
            "id": 50,
            "page": 5,
            "text": "Then the block computes N transforms LSTM(·,·,·), one for each dimension, obtaining the desired output hidden and memory vectors:"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 1193
                },
                {
                    "x": 2108,
                    "y": 1193
                },
                {
                    "x": 2108,
                    "y": 1570
                },
                {
                    "x": 440,
                    "y": 1570
                }
            ],
            "category": "paragraph",
            "html": "<p id='51' style='font-size:16px'>Each transform has distinct weight matrices Wu W.f, Wi, Wc in RdxNd and applies the standard<br>LSTM mechanism across the respective dimension. Note how the vector H that contains all the<br>input hidden vectors is shared across the transforms, whereas the input memory vectors affect the<br>N-way interaction but are not directly combined. N-dimensional blocks can naturally be arranged<br>in a N-dimensional grid forming a Grid LSTM. As for a block, the grid has N sides with incoming<br>hidden and memory vectors and N sides with outgoing hidden and memory vectors. Note that a<br>block does not receive a separate data representation. A data point is projected into the network via<br>a pair of input hidden and memory vectors along one of the sides of the grid.</p>",
            "id": 51,
            "page": 5,
            "text": "Each transform has distinct weight matrices Wu W.f, Wi, Wc in RdxNd and applies the standard LSTM mechanism across the respective dimension. Note how the vector H that contains all the input hidden vectors is shared across the transforms, whereas the input memory vectors affect the N-way interaction but are not directly combined. N-dimensional blocks can naturally be arranged in a N-dimensional grid forming a Grid LSTM. As for a block, the grid has N sides with incoming hidden and memory vectors and N sides with outgoing hidden and memory vectors. Note that a block does not receive a separate data representation. A data point is projected into the network via a pair of input hidden and memory vectors along one of the sides of the grid."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 1620
                },
                {
                    "x": 971,
                    "y": 1620
                },
                {
                    "x": 971,
                    "y": 1668
                },
                {
                    "x": 444,
                    "y": 1668
                }
            ],
            "category": "paragraph",
            "html": "<p id='52' style='font-size:16px'>3.2 PRIORITY DIMENSIONS</p>",
            "id": 52,
            "page": 5,
            "text": "3.2 PRIORITY DIMENSIONS"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1704
                },
                {
                    "x": 2108,
                    "y": 1704
                },
                {
                    "x": 2108,
                    "y": 2023
                },
                {
                    "x": 441,
                    "y": 2023
                }
            ],
            "category": "paragraph",
            "html": "<p id='53' style='font-size:14px'>In a N-dimensional block the transforms for all dimensions are computed in parallel. But it can be<br>useful for a dimension to know the outputs of the transforms from the other dimensions, especially<br>if the outgoing vectors from that dimension will be used to estimate the target. For instance, to<br>prioritize the first dimension of the network, the block first computes the N - 1 transforms for the<br>other dimensions obtaining the output hidden vectors h'2, · .., h'N. Then the block concatenates these<br>output hidden vectors and the input hidden vector h1 for the first dimension into a new vector H' as<br>follows:</p>",
            "id": 53,
            "page": 5,
            "text": "In a N-dimensional block the transforms for all dimensions are computed in parallel. But it can be useful for a dimension to know the outputs of the transforms from the other dimensions, especially if the outgoing vectors from that dimension will be used to estimate the target. For instance, to prioritize the first dimension of the network, the block first computes the N - 1 transforms for the other dimensions obtaining the output hidden vectors h'2, · .., h'N. Then the block concatenates these output hidden vectors and the input hidden vector h1 for the first dimension into a new vector H' as follows:"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2238
                },
                {
                    "x": 2105,
                    "y": 2238
                },
                {
                    "x": 2105,
                    "y": 2334
                },
                {
                    "x": 441,
                    "y": 2334
                }
            ],
            "category": "paragraph",
            "html": "<p id='54' style='font-size:16px'>The vector is then used in the final transform to obtain the prioritized output hidden and memory<br>vectors h'1 and m'1.</p>",
            "id": 54,
            "page": 5,
            "text": "The vector is then used in the final transform to obtain the prioritized output hidden and memory vectors h'1 and m'1."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 2385
                },
                {
                    "x": 1014,
                    "y": 2385
                },
                {
                    "x": 1014,
                    "y": 2432
                },
                {
                    "x": 445,
                    "y": 2432
                }
            ],
            "category": "paragraph",
            "html": "<p id='55' style='font-size:14px'>3.3 NON-LSTM DIMENSIONS</p>",
            "id": 55,
            "page": 5,
            "text": "3.3 NON-LSTM DIMENSIONS"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2471
                },
                {
                    "x": 2108,
                    "y": 2471
                },
                {
                    "x": 2108,
                    "y": 2699
                },
                {
                    "x": 442,
                    "y": 2699
                }
            ],
            "category": "paragraph",
            "html": "<p id='56' style='font-size:16px'>In Grid LSTM networks that have only a few blocks along a given dimension in the grid, it can be<br>useful to just have regular connections along that dimension without the use of cells. This can be<br>naturally accomplished inside the block by using for that dimension in Eq. 6 a simple transformation<br>with a nonlinear activation function instead of the transform LSTM(·,·, .). Given a weight matrix<br>V E RdxNd, for the first dimension this looks as follows:</p>",
            "id": 56,
            "page": 5,
            "text": "In Grid LSTM networks that have only a few blocks along a given dimension in the grid, it can be useful to just have regular connections along that dimension without the use of cells. This can be naturally accomplished inside the block by using for that dimension in Eq. 6 a simple transformation with a nonlinear activation function instead of the transform LSTM(·,·, .). Given a weight matrix V E RdxNd, for the first dimension this looks as follows:"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2779
                },
                {
                    "x": 2108,
                    "y": 2779
                },
                {
                    "x": 2108,
                    "y": 3053
                },
                {
                    "x": 441,
                    "y": 3053
                }
            ],
            "category": "paragraph",
            "html": "<p id='57' style='font-size:14px'>where a is a standard nonlinear transfer function or simply the identity. This allows us to see<br>how, modulo the differences in the mechanism inside the blocks, Grid LSTM networks generalize<br>the models in Sect. 2. A 2d Grid LSTM applied to temporal sequences with cells in the temporal<br>dimension but not in the vertical depth dimension, corresponds to the Stacked LSTM. Likewise, the<br>3d Grid LSTM without cells along the depth corresponds to Multidimensional LSTM, stacked with<br>one or more layers.</p>",
            "id": 57,
            "page": 5,
            "text": "where a is a standard nonlinear transfer function or simply the identity. This allows us to see how, modulo the differences in the mechanism inside the blocks, Grid LSTM networks generalize the models in Sect. 2. A 2d Grid LSTM applied to temporal sequences with cells in the temporal dimension but not in the vertical depth dimension, corresponds to the Stacked LSTM. Likewise, the 3d Grid LSTM without cells along the depth corresponds to Multidimensional LSTM, stacked with one or more layers."
        },
        {
            "bounding_box": [
                {
                    "x": 1261,
                    "y": 3132
                },
                {
                    "x": 1288,
                    "y": 3132
                },
                {
                    "x": 1288,
                    "y": 3171
                },
                {
                    "x": 1261,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='58' style='font-size:14px'>5</footer>",
            "id": 58,
            "page": 5,
            "text": "5"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 110
                },
                {
                    "x": 1288,
                    "y": 110
                },
                {
                    "x": 1288,
                    "y": 158
                },
                {
                    "x": 444,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='59' style='font-size:16px'>Under review as a conference paper at ICLR 2016</header>",
            "id": 59,
            "page": 6,
            "text": "Under review as a conference paper at ICLR 2016"
        },
        {
            "bounding_box": [
                {
                    "x": 455,
                    "y": 331
                },
                {
                    "x": 1998,
                    "y": 331
                },
                {
                    "x": 1998,
                    "y": 794
                },
                {
                    "x": 455,
                    "y": 794
                }
            ],
            "category": "figure",
            "html": "<figure><img id='60' style='font-size:14px' alt=\"Accuracy Tied 2-LSTM\n1\nLayers Samples Accuracy 0.7\nStacked LSTM 1 5M 51%\nUntied 2-LSTM 5 5M 67%\n0.4\nTied 2-LSTM 18 0.55M > 99%\n0.1\n0 0.1 0.2 0.3 0.4 0.5 0.6\nSamples (millions)\" data-coord=\"top-left:(455,331); bottom-right:(1998,794)\" /></figure>",
            "id": 60,
            "page": 6,
            "text": "Accuracy Tied 2-LSTM 1 Layers Samples Accuracy 0.7 Stacked LSTM 1 5M 51% Untied 2-LSTM 5 5M 67% 0.4 Tied 2-LSTM 18 0.55M > 99% 0.1 0 0.1 0.2 0.3 0.4 0.5 0.6 Samples (millions)"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 826
                },
                {
                    "x": 2109,
                    "y": 826
                },
                {
                    "x": 2109,
                    "y": 1016
                },
                {
                    "x": 440,
                    "y": 1016
                }
            ],
            "category": "caption",
            "html": "<caption id='61' style='font-size:20px'>Figure 4: Results on 15-digit addition. The left table gives results for the best performing networks<br>of each type. The right graph depicts the learning curve of the 18-layer tied 2-LSTM that solves the<br>problem with less than 550K examples. The spike in the curve is likely due to the repetitions in the<br>steps of the addition algorithm.</caption>",
            "id": 61,
            "page": 6,
            "text": "Figure 4: Results on 15-digit addition. The left table gives results for the best performing networks of each type. The right graph depicts the learning curve of the 18-layer tied 2-LSTM that solves the problem with less than 550K examples. The spike in the curve is likely due to the repetitions in the steps of the addition algorithm."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 1094
                },
                {
                    "x": 1109,
                    "y": 1094
                },
                {
                    "x": 1109,
                    "y": 1141
                },
                {
                    "x": 444,
                    "y": 1141
                }
            ],
            "category": "paragraph",
            "html": "<p id='62' style='font-size:18px'>3.4 INPUTS FROM MULTIPLE SIDES</p>",
            "id": 62,
            "page": 6,
            "text": "3.4 INPUTS FROM MULTIPLE SIDES"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1178
                },
                {
                    "x": 2108,
                    "y": 1178
                },
                {
                    "x": 2108,
                    "y": 1552
                },
                {
                    "x": 441,
                    "y": 1552
                }
            ],
            "category": "paragraph",
            "html": "<p id='63' style='font-size:18px'>If we picture a N-dimensional block as in Fig. 1, we see that N of the sides of the block have input<br>vectors associated with them and the other N sides have output vectors. As the blocks are arranged in<br>a grid, this separation extends to the grid as a whole; each side of the grid has either input or output<br>vectors associated with it. In certain tasks that have inputs of different types, a model can exploit<br>this separation by projecting each type of input on a different side of the grid. The mechanism inside<br>the blocks ensures that the hidden and memory vectors from the different sides will interact closely<br>without being conflated. This is the case in the neural translation model introduced in Sect. 4 where<br>source words and target words are projected on two different sides of a Grid LSTM.</p>",
            "id": 63,
            "page": 6,
            "text": "If we picture a N-dimensional block as in Fig. 1, we see that N of the sides of the block have input vectors associated with them and the other N sides have output vectors. As the blocks are arranged in a grid, this separation extends to the grid as a whole; each side of the grid has either input or output vectors associated with it. In certain tasks that have inputs of different types, a model can exploit this separation by projecting each type of input on a different side of the grid. The mechanism inside the blocks ensures that the hidden and memory vectors from the different sides will interact closely without being conflated. This is the case in the neural translation model introduced in Sect. 4 where source words and target words are projected on two different sides of a Grid LSTM."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 1603
                },
                {
                    "x": 884,
                    "y": 1603
                },
                {
                    "x": 884,
                    "y": 1651
                },
                {
                    "x": 444,
                    "y": 1651
                }
            ],
            "category": "paragraph",
            "html": "<p id='64' style='font-size:18px'>3.5 WEIGHT SHARING</p>",
            "id": 64,
            "page": 6,
            "text": "3.5 WEIGHT SHARING"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1688
                },
                {
                    "x": 2109,
                    "y": 1688
                },
                {
                    "x": 2109,
                    "y": 1921
                },
                {
                    "x": 442,
                    "y": 1921
                }
            ],
            "category": "paragraph",
            "html": "<p id='65' style='font-size:18px'>Sharing of weight matrices can be specified along any dimension in a Grid LSTM and it can be<br>useful to induce invariance in the computation along that dimension. As in the translation and image<br>models, if multiple sides of a grid need to share weights, capacity can be added to the model by<br>introducing into the grid a new dimension without sharing of weights. If the weights are shared<br>along all dimensions including the depth, we refer to the model as a Tied N-LSTM.</p>",
            "id": 65,
            "page": 6,
            "text": "Sharing of weight matrices can be specified along any dimension in a Grid LSTM and it can be useful to induce invariance in the computation along that dimension. As in the translation and image models, if multiple sides of a grid need to share weights, capacity can be added to the model by introducing into the grid a new dimension without sharing of weights. If the weights are shared along all dimensions including the depth, we refer to the model as a Tied N-LSTM."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 1986
                },
                {
                    "x": 839,
                    "y": 1986
                },
                {
                    "x": 839,
                    "y": 2038
                },
                {
                    "x": 444,
                    "y": 2038
                }
            ],
            "category": "paragraph",
            "html": "<p id='66' style='font-size:22px'>4 EXPERIMENTS</p>",
            "id": 66,
            "page": 6,
            "text": "4 EXPERIMENTS"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2089
                },
                {
                    "x": 740,
                    "y": 2089
                },
                {
                    "x": 740,
                    "y": 2135
                },
                {
                    "x": 442,
                    "y": 2135
                }
            ],
            "category": "paragraph",
            "html": "<p id='67' style='font-size:18px'>4.1 ADDITION</p>",
            "id": 67,
            "page": 6,
            "text": "4.1 ADDITION"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2172
                },
                {
                    "x": 2109,
                    "y": 2172
                },
                {
                    "x": 2109,
                    "y": 2405
                },
                {
                    "x": 441,
                    "y": 2405
                }
            ],
            "category": "paragraph",
            "html": "<p id='68' style='font-size:18px'>We first experiment with 2-LSTM networks on learning to sum two 15-digit integers. The problem<br>formulation is similar to that in (Zaremba & Sutskever, 2014), where each number is given to the<br>network one digit at a time and the result is also predicted one digit at a time. The input numbers<br>are separated by delimiter symbols and an end-of-result symbol is predicted by the network; these<br>symbols as well as input and target padding are indicated by - An example is as follows:<br>·</p>",
            "id": 68,
            "page": 6,
            "text": "We first experiment with 2-LSTM networks on learning to sum two 15-digit integers. The problem formulation is similar to that in (Zaremba & Sutskever, 2014), where each number is given to the network one digit at a time and the result is also predicted one digit at a time. The input numbers are separated by delimiter symbols and an end-of-result symbol is predicted by the network; these symbols as well as input and target padding are indicated by - An example is as follows: ·"
        },
        {
            "bounding_box": [
                {
                    "x": 818,
                    "y": 2429
                },
                {
                    "x": 1731,
                    "y": 2429
                },
                {
                    "x": 1731,
                    "y": 2638
                },
                {
                    "x": 818,
                    "y": 2638
                }
            ],
            "category": "figure",
            "html": "<figure><img id='69' style='font-size:14px' alt=\"- 1 2 3 - 8 9 9 - - - -\n↓\n- - - - - - - 1 0 2 2 -\" data-coord=\"top-left:(818,2429); bottom-right:(1731,2638)\" /></figure>",
            "id": 69,
            "page": 6,
            "text": "- 1 2 3 - 8 9 9 - - - ↓ - - - - - - - 1 0 2 2 -"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2661
                },
                {
                    "x": 2109,
                    "y": 2661
                },
                {
                    "x": 2109,
                    "y": 2890
                },
                {
                    "x": 441,
                    "y": 2890
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='70' style='font-size:18px'>Contrary to the work in (Zaremba & Sutskever, 2014) that uses from 4 to 9 digits for the input<br>integers, we fix the number of digits to 15, we do not use curriculum learning strategies and we<br>do not put digits from the partially predicted output back into the network, forcing the network<br>to remember its partial predictions and making the task more challenging. The predicted output<br>numbers have either 15 or 16 digits.</p>",
            "id": 70,
            "page": 6,
            "text": "Contrary to the work in (Zaremba & Sutskever, 2014) that uses from 4 to 9 digits for the input integers, we fix the number of digits to 15, we do not use curriculum learning strategies and we do not put digits from the partially predicted output back into the network, forcing the network to remember its partial predictions and making the task more challenging. The predicted output numbers have either 15 or 16 digits."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2914
                },
                {
                    "x": 2109,
                    "y": 2914
                },
                {
                    "x": 2109,
                    "y": 3053
                },
                {
                    "x": 442,
                    "y": 3053
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='71' style='font-size:18px'>We compare the performance of 2-LSTM networks with that of standard Stacked LSTM (Fig. 2).<br>We train the two types of networks with either tied or untied weights, with 400 hidden units each<br>and with between 1 and 50 layers. We train the network with stochastic gradient descent using</p>",
            "id": 71,
            "page": 6,
            "text": "We compare the performance of 2-LSTM networks with that of standard Stacked LSTM (Fig. 2). We train the two types of networks with either tied or untied weights, with 400 hidden units each and with between 1 and 50 layers. We train the network with stochastic gradient descent using"
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3135
                },
                {
                    "x": 1289,
                    "y": 3135
                },
                {
                    "x": 1289,
                    "y": 3171
                },
                {
                    "x": 1260,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='72' style='font-size:16px'>6</footer>",
            "id": 72,
            "page": 6,
            "text": "6"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 110
                },
                {
                    "x": 1288,
                    "y": 110
                },
                {
                    "x": 1288,
                    "y": 158
                },
                {
                    "x": 444,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='73' style='font-size:16px'>Under review as a conference paper at ICLR 2016</header>",
            "id": 73,
            "page": 7,
            "text": "Under review as a conference paper at ICLR 2016"
        },
        {
            "bounding_box": [
                {
                    "x": 439,
                    "y": 338
                },
                {
                    "x": 2102,
                    "y": 338
                },
                {
                    "x": 2102,
                    "y": 806
                },
                {
                    "x": 439,
                    "y": 806
                }
            ],
            "category": "figure",
            "html": "<figure><img id='74' style='font-size:14px' alt=\"■ Untied Stacked LSTM\nSamples (millions) Tied 2-LSTM Untied 2-LSTM · Tied Stacked LSTM\n1 1.7\n3\n1.5\n0.7\n2\n1.3\n0.4\n1\n1.1\n0.1 0 0.9\n0 10 20 30 40 50 0 10 20 30 0 4 8 12 16\nLayers Layers Layers\nAccuracy > 99% Accuracy > 80% Accuracy > 50%\" data-coord=\"top-left:(439,338); bottom-right:(2102,806)\" /></figure>",
            "id": 74,
            "page": 7,
            "text": "■ Untied Stacked LSTM Samples (millions) Tied 2-LSTM Untied 2-LSTM · Tied Stacked LSTM 1 1.7 3 1.5 0.7 2 1.3 0.4 1 1.1 0.1 0 0.9 0 10 20 30 40 50 0 10 20 30 0 4 8 12 16 Layers Layers Layers Accuracy > 99% Accuracy > 80% Accuracy > 50%"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 847
                },
                {
                    "x": 2108,
                    "y": 847
                },
                {
                    "x": 2108,
                    "y": 1130
                },
                {
                    "x": 440,
                    "y": 1130
                }
            ],
            "category": "caption",
            "html": "<caption id='75' style='font-size:18px'>Figure 5: Each dot in the three plots corresponds to a neural network of the respective type that has<br>reached the accuracy of, respectively, > 99%, > 80% and > 50% at the memorization task. The<br>networks all have 100 hidden units and the number of layers are indicated on the horizontal axis.<br>The vertical axis indicates the number of samples needed to achieve the threshold accuracy. We see<br>that deeper networks tend to learn faster than shallower ones, and that 2-LSTM networks are more<br>effective than Stacked LSTM networks in both the tied and untied settings.</caption>",
            "id": 75,
            "page": 7,
            "text": "Figure 5: Each dot in the three plots corresponds to a neural network of the respective type that has reached the accuracy of, respectively, > 99%, > 80% and > 50% at the memorization task. The networks all have 100 hidden units and the number of layers are indicated on the horizontal axis. The vertical axis indicates the number of samples needed to achieve the threshold accuracy. We see that deeper networks tend to learn faster than shallower ones, and that 2-LSTM networks are more effective than Stacked LSTM networks in both the tied and untied settings."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1213
                },
                {
                    "x": 2107,
                    "y": 1213
                },
                {
                    "x": 2107,
                    "y": 1445
                },
                {
                    "x": 441,
                    "y": 1445
                }
            ],
            "category": "paragraph",
            "html": "<p id='76' style='font-size:18px'>mini-batches of size 15 and the Adam optimizer with a learning rate of 0.001 (Kingma & Ba, 2014).<br>We train the networks for up to 5 million samples or until they reach 100% accuracy on a random<br>sample of 100 unseen addition problems. Note that since during training all samples are randomly<br>generated, samples are seen only once and it is not possible for the network to overfit on training<br>data. The training and test accuracies agree closely.</p>",
            "id": 76,
            "page": 7,
            "text": "mini-batches of size 15 and the Adam optimizer with a learning rate of 0.001 (Kingma & Ba, 2014). We train the networks for up to 5 million samples or until they reach 100% accuracy on a random sample of 100 unseen addition problems. Note that since during training all samples are randomly generated, samples are seen only once and it is not possible for the network to overfit on training data. The training and test accuracies agree closely."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1465
                },
                {
                    "x": 2108,
                    "y": 1465
                },
                {
                    "x": 2108,
                    "y": 1883
                },
                {
                    "x": 441,
                    "y": 1883
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='77' style='font-size:20px'>Figure 4 relates the results of the experiments on the addition problem. The best performing tied<br>2-LSTM is 18 layers deep and learns to perfectly solve the task in less than 550K training samples.<br>We find that tied 2-LSTM networks generally perform better than untied 2-LSTM networks, which<br>is likely due to the repetitive nature of the steps involved in the addition algorithm. The best untied<br>2-LSTM network has 5 layers, learns more slowly and achieves a per-digit accuracy of 67% after 5<br>million examples. 2-LSTM networks in turn perform better than either tied or untied Stacked LSTM<br>networks, where more stacked layers do not improve over the single-layer models. We see that the<br>cells present a clear advantage for the deep 2-LSTM networks by helping to mitigate the vanishing<br>of gradients along the depth dimension.</p>",
            "id": 77,
            "page": 7,
            "text": "Figure 4 relates the results of the experiments on the addition problem. The best performing tied 2-LSTM is 18 layers deep and learns to perfectly solve the task in less than 550K training samples. We find that tied 2-LSTM networks generally perform better than untied 2-LSTM networks, which is likely due to the repetitive nature of the steps involved in the addition algorithm. The best untied 2-LSTM network has 5 layers, learns more slowly and achieves a per-digit accuracy of 67% after 5 million examples. 2-LSTM networks in turn perform better than either tied or untied Stacked LSTM networks, where more stacked layers do not improve over the single-layer models. We see that the cells present a clear advantage for the deep 2-LSTM networks by helping to mitigate the vanishing of gradients along the depth dimension."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 1937
                },
                {
                    "x": 844,
                    "y": 1937
                },
                {
                    "x": 844,
                    "y": 1984
                },
                {
                    "x": 445,
                    "y": 1984
                }
            ],
            "category": "paragraph",
            "html": "<p id='78' style='font-size:22px'>4.2 MEMORIZATION</p>",
            "id": 78,
            "page": 7,
            "text": "4.2 MEMORIZATION"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2022
                },
                {
                    "x": 2108,
                    "y": 2022
                },
                {
                    "x": 2108,
                    "y": 2255
                },
                {
                    "x": 441,
                    "y": 2255
                }
            ],
            "category": "paragraph",
            "html": "<p id='79' style='font-size:18px'>For our third algorithmic task, we analyze the performance of 2-LSTM networks on the task of<br>memorizing a random sequence of symbols. The sequences are 20 symbols long and we use a<br>vocabulary of 64 symbols encoded as one-hot vectors and given to the network one symbol per step.<br>The setup is similar to the one for addition above. The network is tasked with reading the input<br>sequence and outputting the same sequence unchanged:</p>",
            "id": 79,
            "page": 7,
            "text": "For our third algorithmic task, we analyze the performance of 2-LSTM networks on the task of memorizing a random sequence of symbols. The sequences are 20 symbols long and we use a vocabulary of 64 symbols encoded as one-hot vectors and given to the network one symbol per step. The setup is similar to the one for addition above. The network is tasked with reading the input sequence and outputting the same sequence unchanged:"
        },
        {
            "bounding_box": [
                {
                    "x": 986,
                    "y": 2278
                },
                {
                    "x": 1556,
                    "y": 2278
                },
                {
                    "x": 1556,
                    "y": 2517
                },
                {
                    "x": 986,
                    "y": 2517
                }
            ],
            "category": "figure",
            "html": "<figure><img id='80' style='font-size:14px' alt=\"- a B 2 - - - -\n↓\n- - - a B 2 -\" data-coord=\"top-left:(986,2278); bottom-right:(1556,2517)\" /></figure>",
            "id": 80,
            "page": 7,
            "text": "- a B 2 - - - ↓ - - - a B 2 -"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2544
                },
                {
                    "x": 2107,
                    "y": 2544
                },
                {
                    "x": 2107,
                    "y": 2637
                },
                {
                    "x": 441,
                    "y": 2637
                }
            ],
            "category": "paragraph",
            "html": "<p id='81' style='font-size:18px'>Since the sequences are randomly generated, there is no correlation between successive symbols and<br>the network must memorize the whole sequence without compression.</p>",
            "id": 81,
            "page": 7,
            "text": "Since the sequences are randomly generated, there is no correlation between successive symbols and the network must memorize the whole sequence without compression."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2660
                },
                {
                    "x": 2108,
                    "y": 2660
                },
                {
                    "x": 2108,
                    "y": 2937
                },
                {
                    "x": 442,
                    "y": 2937
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='82' style='font-size:18px'>We train 2-LSTM and Stacked LSTM with either tied or untied weights on the memorization task.<br>All networks have 100 hidden units and have between 1 and 50 layers. We use mini-batches of size<br>15 and optimize the network using Adam and a learning rate of 0.001. As above, we train each<br>network for up to 5 million samples or until they reach 100% accuracy on 100 unseen samples.<br>Accuracy is measured per individual symbol, not per sequence. We do not use curriculum learning<br>or other training strategies.</p>",
            "id": 82,
            "page": 7,
            "text": "We train 2-LSTM and Stacked LSTM with either tied or untied weights on the memorization task. All networks have 100 hidden units and have between 1 and 50 layers. We use mini-batches of size 15 and optimize the network using Adam and a learning rate of 0.001. As above, we train each network for up to 5 million samples or until they reach 100% accuracy on 100 unseen samples. Accuracy is measured per individual symbol, not per sequence. We do not use curriculum learning or other training strategies."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2959
                },
                {
                    "x": 2108,
                    "y": 2959
                },
                {
                    "x": 2108,
                    "y": 3054
                },
                {
                    "x": 441,
                    "y": 3054
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='83' style='font-size:18px'>Figure 5 reports the performance of the networks. The small number of hidden units contributes<br>to making the training of the networks difficult. But we see that tied 2-LSTM networks are most</p>",
            "id": 83,
            "page": 7,
            "text": "Figure 5 reports the performance of the networks. The small number of hidden units contributes to making the training of the networks difficult. But we see that tied 2-LSTM networks are most"
        },
        {
            "bounding_box": [
                {
                    "x": 1261,
                    "y": 3134
                },
                {
                    "x": 1289,
                    "y": 3134
                },
                {
                    "x": 1289,
                    "y": 3170
                },
                {
                    "x": 1261,
                    "y": 3170
                }
            ],
            "category": "footer",
            "html": "<footer id='84' style='font-size:16px'>7</footer>",
            "id": 84,
            "page": 7,
            "text": "7"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 111
                },
                {
                    "x": 1287,
                    "y": 111
                },
                {
                    "x": 1287,
                    "y": 157
                },
                {
                    "x": 444,
                    "y": 157
                }
            ],
            "category": "header",
            "html": "<header id='85' style='font-size:14px'>Under review as a conference paper at ICLR 2016</header>",
            "id": 85,
            "page": 8,
            "text": "Under review as a conference paper at ICLR 2016"
        },
        {
            "bounding_box": [
                {
                    "x": 613,
                    "y": 334
                },
                {
                    "x": 1941,
                    "y": 334
                },
                {
                    "x": 1941,
                    "y": 570
                },
                {
                    "x": 613,
                    "y": 570
                }
            ],
            "category": "table",
            "html": "<table id='86' style='font-size:14px'><tr><td></td><td>BPC</td><td>Parameters</td><td>Alphabet Size</td><td>Test data</td></tr><tr><td>Stacked LSTM (Graves, 2013)</td><td>1.67</td><td>27M</td><td>205</td><td>last 4MB</td></tr><tr><td>MRNN (Sutskever et al., 2011)</td><td>1.60</td><td>4.9M</td><td>86</td><td>last 10MB</td></tr><tr><td>GFRNN (Chung et al., 2015)</td><td>1.58</td><td>20M</td><td>205</td><td>last 5MB</td></tr><tr><td>Tied 2-LSTM</td><td>1.47</td><td>16.8M</td><td>205</td><td>last 5MB</td></tr></table>",
            "id": 86,
            "page": 8,
            "text": "BPC Parameters Alphabet Size Test data  Stacked LSTM (Graves, 2013) 1.67 27M 205 last 4MB  MRNN (Sutskever , 2011) 1.60 4.9M 86 last 10MB  GFRNN (Chung , 2015) 1.58 20M 205 last 5MB  Tied 2-LSTM 1.47 16.8M 205"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 622
                },
                {
                    "x": 2108,
                    "y": 622
                },
                {
                    "x": 2108,
                    "y": 764
                },
                {
                    "x": 440,
                    "y": 764
                }
            ],
            "category": "caption",
            "html": "<caption id='87' style='font-size:20px'>Figure 6: Bits-per-character results for various models measured on the Wikipedia dataset together<br>with the respective number of parameters and the size of the alphabet that was used. Note the slight<br>differences in test data and alphabet size.</caption>",
            "id": 87,
            "page": 8,
            "text": "Figure 6: Bits-per-character results for various models measured on the Wikipedia dataset together with the respective number of parameters and the size of the alphabet that was used. Note the slight differences in test data and alphabet size."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 857
                },
                {
                    "x": 2108,
                    "y": 857
                },
                {
                    "x": 2108,
                    "y": 1316
                },
                {
                    "x": 442,
                    "y": 1316
                }
            ],
            "category": "paragraph",
            "html": "<p id='88' style='font-size:18px'>successful and learn to solve the task with the smallest number of samples. The 43-layer tied 2-<br>LSTM network learns a solution with less than 150K samples. Although there is fairly high variance<br>amid the solving networks, deeper networks tend to learn faster. In addition, there is large difference<br>in the performance of tied 2-LSTM networks and tied Stacked LSTM networks. The latter perform<br>with much lower accuracy and Stacked LSTM networks with more than 16 layers do not reach an<br>accuracy of more than 50%. Here we see that the optimization property of the cells in the depth<br>dimension delivers a large gain. Similarly to the case of the addition problem, both the untied 2-<br>LSTM networks and the untied Stacked LSTM networks take significantly longer to learn than the<br>respective counterparts with tied weights, but the advantage of the cells in the depth direction clearly<br>emerges for untied 2-LSTM networks too.</p>",
            "id": 88,
            "page": 8,
            "text": "successful and learn to solve the task with the smallest number of samples. The 43-layer tied 2LSTM network learns a solution with less than 150K samples. Although there is fairly high variance amid the solving networks, deeper networks tend to learn faster. In addition, there is large difference in the performance of tied 2-LSTM networks and tied Stacked LSTM networks. The latter perform with much lower accuracy and Stacked LSTM networks with more than 16 layers do not reach an accuracy of more than 50%. Here we see that the optimization property of the cells in the depth dimension delivers a large gain. Similarly to the case of the addition problem, both the untied 2LSTM networks and the untied Stacked LSTM networks take significantly longer to learn than the respective counterparts with tied weights, but the advantage of the cells in the depth direction clearly emerges for untied 2-LSTM networks too."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1382
                },
                {
                    "x": 1361,
                    "y": 1382
                },
                {
                    "x": 1361,
                    "y": 1431
                },
                {
                    "x": 443,
                    "y": 1431
                }
            ],
            "category": "paragraph",
            "html": "<p id='89' style='font-size:20px'>4.3 CHARACTER-LEVEL LANGUAGE MODELLING</p>",
            "id": 89,
            "page": 8,
            "text": "4.3 CHARACTER-LEVEL LANGUAGE MODELLING"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1472
                },
                {
                    "x": 2107,
                    "y": 1472
                },
                {
                    "x": 2107,
                    "y": 1657
                },
                {
                    "x": 442,
                    "y": 1657
                }
            ],
            "category": "paragraph",
            "html": "<p id='90' style='font-size:18px'>We next test the 2-LSTM network on the Hutter challenge Wikipedia dataset (Hutter, 2012). The aim<br>is to successively predict the next character in the corpus. The dataset has 100 million characters.<br>We follow the splitting procedure of (Chung et al., 2015), where the last 5 million characters are<br>used for testing. The alphabet has 205 characters in total.</p>",
            "id": 90,
            "page": 8,
            "text": "We next test the 2-LSTM network on the Hutter challenge Wikipedia dataset (Hutter, 2012). The aim is to successively predict the next character in the corpus. The dataset has 100 million characters. We follow the splitting procedure of (Chung , 2015), where the last 5 million characters are used for testing. The alphabet has 205 characters in total."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1679
                },
                {
                    "x": 2107,
                    "y": 1679
                },
                {
                    "x": 2107,
                    "y": 2185
                },
                {
                    "x": 442,
                    "y": 2185
                }
            ],
            "category": "paragraph",
            "html": "<p id='91' style='font-size:18px'>We use a tied 2-LSTM with 1000 hidden units and 6 layers of depth. As in Fig. 2 and in the previous<br>tasks, the characters are projected both to form the initial input hidden and cell vectors and the top<br>softmax layer is connected to the topmost output hidden and cell vectors. The model has a total of<br>2000 x 4000 + 205 x 4 x 1000 = 8.82 x 106 parameters. As usual the objective is to minimize<br>the negative log-likelihood of the character sequence under the model. Training is performed by<br>sampling sequences of 10000 characters and processing them in order. We back propagate the errors<br>every 50 characters. The initial cell and hidden vectors in the temporal direction are initialized to<br>zero only at the beginning of each sequence; they maintain their forward propagated values after each<br>update in order to simulate full back propagation. We use mini-batches of 100, thereby processing<br>100 sequences of 10000 characters each in parallel. The network is trained with Adam with a<br>learning rate of 0.001 and training proceeds for approximately 20 epochs.</p>",
            "id": 91,
            "page": 8,
            "text": "We use a tied 2-LSTM with 1000 hidden units and 6 layers of depth. As in Fig. 2 and in the previous tasks, the characters are projected both to form the initial input hidden and cell vectors and the top softmax layer is connected to the topmost output hidden and cell vectors. The model has a total of 2000 x 4000 + 205 x 4 x 1000 = 8.82 x 106 parameters. As usual the objective is to minimize the negative log-likelihood of the character sequence under the model. Training is performed by sampling sequences of 10000 characters and processing them in order. We back propagate the errors every 50 characters. The initial cell and hidden vectors in the temporal direction are initialized to zero only at the beginning of each sequence; they maintain their forward propagated values after each update in order to simulate full back propagation. We use mini-batches of 100, thereby processing 100 sequences of 10000 characters each in parallel. The network is trained with Adam with a learning rate of 0.001 and training proceeds for approximately 20 epochs."
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 2208
                },
                {
                    "x": 2108,
                    "y": 2208
                },
                {
                    "x": 2108,
                    "y": 2393
                },
                {
                    "x": 440,
                    "y": 2393
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='92' style='font-size:22px'>Figure 6 reports the bits-per-character performance together with the number of parameters of var-<br>ious recently proposed models on the dataset. The tied 2-LSTM significantly outperforms other<br>models despite having fewer parameters. More layers of depth and adding capacity by untying some<br>of the weights are likely to further enhance the 2-LSTM.</p>",
            "id": 92,
            "page": 8,
            "text": "Figure 6 reports the bits-per-character performance together with the number of parameters of various recently proposed models on the dataset. The tied 2-LSTM significantly outperforms other models despite having fewer parameters. More layers of depth and adding capacity by untying some of the weights are likely to further enhance the 2-LSTM."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2460
                },
                {
                    "x": 810,
                    "y": 2460
                },
                {
                    "x": 810,
                    "y": 2505
                },
                {
                    "x": 444,
                    "y": 2505
                }
            ],
            "category": "paragraph",
            "html": "<p id='93' style='font-size:18px'>4.4 TRANSLATION</p>",
            "id": 93,
            "page": 8,
            "text": "4.4 TRANSLATION"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2547
                },
                {
                    "x": 2107,
                    "y": 2547
                },
                {
                    "x": 2107,
                    "y": 3054
                },
                {
                    "x": 441,
                    "y": 3054
                }
            ],
            "category": "paragraph",
            "html": "<p id='94' style='font-size:16px'>We next use the flexibility of Grid LSTM to define a novel neural translation model. In the neural<br>approach to machine translation one trains a neural network end-to-end to map the source sentence<br>to the target sentence (Kalchbrenner & Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014). The<br>mapping is usually performed within the encoder-decoder framework. A neural network, that can be<br>convolutional or recurrent, first encodes the source sentence and then the computed representation<br>of the source conditions a recurrent neural network to generate the target sentence. This approach<br>has yielded strong empirical results, but it can suffer from a bottleneck. The encoding of the source<br>sentence must contain information about all the words and their order; the decoder network in turn<br>cannot easily revisit the unencoded source sentence to make decisions based on partially produced<br>translations. This issue can be alleviated by a soft attention mechanism in the decoder neural network<br>that uses gates to focus on specific parts of the source sentence (Bahdanau et al., 2014).</p>",
            "id": 94,
            "page": 8,
            "text": "We next use the flexibility of Grid LSTM to define a novel neural translation model. In the neural approach to machine translation one trains a neural network end-to-end to map the source sentence to the target sentence (Kalchbrenner & Blunsom, 2013; Sutskever , 2014; Cho , 2014). The mapping is usually performed within the encoder-decoder framework. A neural network, that can be convolutional or recurrent, first encodes the source sentence and then the computed representation of the source conditions a recurrent neural network to generate the target sentence. This approach has yielded strong empirical results, but it can suffer from a bottleneck. The encoding of the source sentence must contain information about all the words and their order; the decoder network in turn cannot easily revisit the unencoded source sentence to make decisions based on partially produced translations. This issue can be alleviated by a soft attention mechanism in the decoder neural network that uses gates to focus on specific parts of the source sentence (Bahdanau , 2014)."
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3134
                },
                {
                    "x": 1289,
                    "y": 3134
                },
                {
                    "x": 1289,
                    "y": 3170
                },
                {
                    "x": 1260,
                    "y": 3170
                }
            ],
            "category": "footer",
            "html": "<footer id='95' style='font-size:18px'>8</footer>",
            "id": 95,
            "page": 8,
            "text": "8"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 111
                },
                {
                    "x": 1288,
                    "y": 111
                },
                {
                    "x": 1288,
                    "y": 157
                },
                {
                    "x": 444,
                    "y": 157
                }
            ],
            "category": "header",
            "html": "<header id='96' style='font-size:16px'>Under review as a conference paper at ICLR 2016</header>",
            "id": 96,
            "page": 9,
            "text": "Under review as a conference paper at ICLR 2016"
        },
        {
            "bounding_box": [
                {
                    "x": 850,
                    "y": 330
                },
                {
                    "x": 1700,
                    "y": 330
                },
                {
                    "x": 1700,
                    "y": 1326
                },
                {
                    "x": 850,
                    "y": 1326
                }
            ],
            "category": "figure",
            "html": "<figure><img id='97' style='font-size:14px' alt=\"<t> Le chat etait assis sur le tapis\n<S>\nLe chat etait assis sur le tapis </t>\nThe 4 ▶ 4\ncat\nsat\non\nthe\nmat\n</s>\n3d Grid LSTM\" data-coord=\"top-left:(850,330); bottom-right:(1700,1326)\" /></figure>",
            "id": 97,
            "page": 9,
            "text": "<t> Le chat etait assis sur le tapis <S> Le chat etait assis sur le tapis </t> The 4 ▶ 4 cat sat on the mat </s> 3d Grid LSTM"
        },
        {
            "bounding_box": [
                {
                    "x": 745,
                    "y": 1372
                },
                {
                    "x": 1783,
                    "y": 1372
                },
                {
                    "x": 1783,
                    "y": 1422
                },
                {
                    "x": 745,
                    "y": 1422
                }
            ],
            "category": "caption",
            "html": "<caption id='98' style='font-size:22px'>Figure 7: Illustration of the 3-LSTM neural translation model.</caption>",
            "id": 98,
            "page": 9,
            "text": "Figure 7: Illustration of the 3-LSTM neural translation model."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1538
                },
                {
                    "x": 2108,
                    "y": 1538
                },
                {
                    "x": 2108,
                    "y": 2136
                },
                {
                    "x": 441,
                    "y": 2136
                }
            ],
            "category": "paragraph",
            "html": "<p id='99' style='font-size:18px'>We use Grid LSTM to view translation in a novel fashion as a two-dimensional mapping. We<br>call this the Reencoder network. One dimension processes the source sentence whereas the other<br>dimension produces the target sentence. The resulting network repeatedly re-encodes the source<br>sentence conditioned on the part of the target sentence generated so far, thus functioning as an<br>implicit attention mechanism. The size of the representation of the source sentence varies with<br>length and the source sentence is repeatedly scanned based on each generated target word. As<br>represented in Fig. 9, for each target word, beginning with the start-of-target-sentence symbol, the<br>network scans the source sentence one way in the first layer and the other way in the second layer;<br>the scan depends on all the target words that have been generated SO far and at each block the two<br>layers communicate directly. Note that, like the attention-based model (Bahdanau et al., 2014),<br>the two-dimensional translation model has complexity O(nm), where n and m are respectively the<br>length of the source and target; by contrast the recurrent encoder-decoder model only has complexity<br>0(m + n). This gives additional computational capacity to the former models.</p>",
            "id": 99,
            "page": 9,
            "text": "We use Grid LSTM to view translation in a novel fashion as a two-dimensional mapping. We call this the Reencoder network. One dimension processes the source sentence whereas the other dimension produces the target sentence. The resulting network repeatedly re-encodes the source sentence conditioned on the part of the target sentence generated so far, thus functioning as an implicit attention mechanism. The size of the representation of the source sentence varies with length and the source sentence is repeatedly scanned based on each generated target word. As represented in Fig. 9, for each target word, beginning with the start-of-target-sentence symbol, the network scans the source sentence one way in the first layer and the other way in the second layer; the scan depends on all the target words that have been generated SO far and at each block the two layers communicate directly. Note that, like the attention-based model (Bahdanau , 2014), the two-dimensional translation model has complexity O(nm), where n and m are respectively the length of the source and target; by contrast the recurrent encoder-decoder model only has complexity 0(m + n). This gives additional computational capacity to the former models."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2157
                },
                {
                    "x": 2107,
                    "y": 2157
                },
                {
                    "x": 2107,
                    "y": 2663
                },
                {
                    "x": 441,
                    "y": 2663
                }
            ],
            "category": "paragraph",
            "html": "<p id='100' style='font-size:20px'>Besides addressing the bottleneck, the two-dimensional setup aims at explicitly capturing the invari-<br>ance present in translation. Translation patterns between two languages are invariant above all to<br>position and scale of the pattern. For instance, reordering patterns - such as the one that maps the<br>English \"do not <verb> to the French \"ne <verb> pas\" , or the one that sends a part of an English<br>verb to the end of a German sentence - should be detected and applied independently of where they<br>occur in the source sentence or of the number of words involved in that instance of the pattern. To<br>capture this, the Grid LSTM translation model shares the weights across the source and target di-<br>mensions. In addition, a hierarchy of stacked two-dimensional grids in opposite directions is used to<br>both increase capacity and help with learning longer scale translation patterns. The resulting model<br>is a three-dimensional Grid LSTM where hierarchy grows along the third dimension. The model is<br>depicted in Fig. 7.</p>",
            "id": 100,
            "page": 9,
            "text": "Besides addressing the bottleneck, the two-dimensional setup aims at explicitly capturing the invariance present in translation. Translation patterns between two languages are invariant above all to position and scale of the pattern. For instance, reordering patterns - such as the one that maps the English \"do not <verb> to the French \"ne <verb> pas\" , or the one that sends a part of an English verb to the end of a German sentence - should be detected and applied independently of where they occur in the source sentence or of the number of words involved in that instance of the pattern. To capture this, the Grid LSTM translation model shares the weights across the source and target dimensions. In addition, a hierarchy of stacked two-dimensional grids in opposite directions is used to both increase capacity and help with learning longer scale translation patterns. The resulting model is a three-dimensional Grid LSTM where hierarchy grows along the third dimension. The model is depicted in Fig. 7."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2683
                },
                {
                    "x": 2108,
                    "y": 2683
                },
                {
                    "x": 2108,
                    "y": 3055
                },
                {
                    "x": 441,
                    "y": 3055
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='101' style='font-size:20px'>We evaluate the Grid LSTM translation model on the IWSLT BTEC Chinese-to-English corpus that<br>consists of 44016 pairs of source and target sentences for training, 1006 for development and 503 for<br>testing. The corpus has about 0.5M words in each language, a source vocabulary of 7055 Chinese<br>words and a target vocabulary of 5646 English words (after replacing words that occur only once<br>with the UNK symbol). Target sentences are on average around 12 words long. The development<br>and test corpora come with 15 reference translations. The 3-LSTM uses two two-dimensional grids<br>of 3-LSTM blocks for the hierarchy. Since the network has just two layers in the third dimension,<br>we use regular identity connections without nonlinear transfer function along the third dimension,</p>",
            "id": 101,
            "page": 9,
            "text": "We evaluate the Grid LSTM translation model on the IWSLT BTEC Chinese-to-English corpus that consists of 44016 pairs of source and target sentences for training, 1006 for development and 503 for testing. The corpus has about 0.5M words in each language, a source vocabulary of 7055 Chinese words and a target vocabulary of 5646 English words (after replacing words that occur only once with the UNK symbol). Target sentences are on average around 12 words long. The development and test corpora come with 15 reference translations. The 3-LSTM uses two two-dimensional grids of 3-LSTM blocks for the hierarchy. Since the network has just two layers in the third dimension, we use regular identity connections without nonlinear transfer function along the third dimension,"
        },
        {
            "bounding_box": [
                {
                    "x": 1259,
                    "y": 3132
                },
                {
                    "x": 1290,
                    "y": 3132
                },
                {
                    "x": 1290,
                    "y": 3168
                },
                {
                    "x": 1259,
                    "y": 3168
                }
            ],
            "category": "footer",
            "html": "<footer id='102' style='font-size:16px'>9</footer>",
            "id": 102,
            "page": 9,
            "text": "9"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 113
                },
                {
                    "x": 1286,
                    "y": 113
                },
                {
                    "x": 1286,
                    "y": 156
                },
                {
                    "x": 445,
                    "y": 156
                }
            ],
            "category": "header",
            "html": "<header id='103' style='font-size:16px'>Under review as a conference paper at ICLR 2016</header>",
            "id": 103,
            "page": 10,
            "text": "Under review as a conference paper at ICLR 2016"
        },
        {
            "bounding_box": [
                {
                    "x": 496,
                    "y": 327
                },
                {
                    "x": 2048,
                    "y": 327
                },
                {
                    "x": 2048,
                    "y": 790
                },
                {
                    "x": 496,
                    "y": 790
                }
            ],
            "category": "paragraph",
            "html": "<p id='104' style='font-size:14px'>Valid-1 Test-1 Valid-15 Test-15<br>DGLSTM-Attention (Yao et al., 2015) - 34.5 - -<br>CDEC (Dyer et al., 2010) 30.1 41 50.1 58.9<br>3-LSTM (7 Models) 30.3 42.4 51.8 60.2<br>Reference thank you · please pay for this bill at the cashier ·<br>Generated thank you , ma , am · please give this bill to the cashier and pay there ·<br>Reference how about having lunch with me some day ? i found a good restaurant near my hotel<br>Generated how about lunch with me ? i found a good restaurant near my hotel ·</p>",
            "id": 104,
            "page": 10,
            "text": "Valid-1 Test-1 Valid-15 Test-15 DGLSTM-Attention (Yao , 2015) - 34.5 - CDEC (Dyer , 2010) 30.1 41 50.1 58.9 3-LSTM (7 Models) 30.3 42.4 51.8 60.2 Reference thank you · please pay for this bill at the cashier · Generated thank you , ma , am · please give this bill to the cashier and pay there · Reference how about having lunch with me some day ? i found a good restaurant near my hotel Generated how about lunch with me ? i found a good restaurant near my hotel ·"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 834
                },
                {
                    "x": 2107,
                    "y": 834
                },
                {
                    "x": 2107,
                    "y": 1066
                },
                {
                    "x": 441,
                    "y": 1066
                }
            ],
            "category": "paragraph",
            "html": "<p id='105' style='font-size:16px'>Figure 8: The first table contains BLEU-4 scores of the 3-LSTM neural translation model, the CDEC<br>system and the Depth-Gated LSTM (DGLSTM) with attention mechanism; the scores are calculated<br>against either the main reference translation or against the 15 available reference translations in the<br>BTEC corpus. CDEC is a state-of-the-art hierarchical phrase based system with many component<br>models. The second table contains examples of generated translations.</p>",
            "id": 105,
            "page": 10,
            "text": "Figure 8: The first table contains BLEU-4 scores of the 3-LSTM neural translation model, the CDEC system and the Depth-Gated LSTM (DGLSTM) with attention mechanism; the scores are calculated against either the main reference translation or against the 15 available reference translations in the BTEC corpus. CDEC is a state-of-the-art hierarchical phrase based system with many component models. The second table contains examples of generated translations."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1156
                },
                {
                    "x": 2108,
                    "y": 1156
                },
                {
                    "x": 2108,
                    "y": 1751
                },
                {
                    "x": 442,
                    "y": 1751
                }
            ],
            "category": "paragraph",
            "html": "<p id='106' style='font-size:16px'>as defined in Sect. 3.3; the source and target dimensions have tied weights and LSTM cells. The<br>processing is bidirectional, in that the first grid processes the source sentence from beginning to<br>end and the second one from end to beginning. This allows for the shortest distance that the signal<br>travels between input and output target words to be constant and independent of the length of the<br>source. Note that the second grid receives an input coming from the grid below at each 3-LSTM<br>block. We train seven models with vectors of size 450 and apply dropout with probability 0.5 to the<br>hidden vectors within the blocks. For the optimization we use Adam with a learning rate of 0.001.<br>At decoding the output probabilities are averaged across the models. The beam search has size 20<br>and we discard all candidates that are shorter than half of the length of the source sentence. The<br>results are shown in Fig. 8. Our best model reaches a perplexity of 4.54 on the test data. We use as<br>baseline the state-of-the-art hierarchical phrase-based system CDEC (Dyer et al., 2010). We see that<br>the Grid LSTM significantly outperforms the baseline system on both the validation and test data<br>sets.</p>",
            "id": 106,
            "page": 10,
            "text": "as defined in Sect. 3.3; the source and target dimensions have tied weights and LSTM cells. The processing is bidirectional, in that the first grid processes the source sentence from beginning to end and the second one from end to beginning. This allows for the shortest distance that the signal travels between input and output target words to be constant and independent of the length of the source. Note that the second grid receives an input coming from the grid below at each 3-LSTM block. We train seven models with vectors of size 450 and apply dropout with probability 0.5 to the hidden vectors within the blocks. For the optimization we use Adam with a learning rate of 0.001. At decoding the output probabilities are averaged across the models. The beam search has size 20 and we discard all candidates that are shorter than half of the length of the source sentence. The results are shown in Fig. 8. Our best model reaches a perplexity of 4.54 on the test data. We use as baseline the state-of-the-art hierarchical phrase-based system CDEC (Dyer , 2010). We see that the Grid LSTM significantly outperforms the baseline system on both the validation and test data sets."
        },
        {
            "bounding_box": [
                {
                    "x": 447,
                    "y": 1827
                },
                {
                    "x": 817,
                    "y": 1827
                },
                {
                    "x": 817,
                    "y": 1875
                },
                {
                    "x": 447,
                    "y": 1875
                }
            ],
            "category": "paragraph",
            "html": "<p id='107' style='font-size:22px'>5 CONCLUSION</p>",
            "id": 107,
            "page": 10,
            "text": "5 CONCLUSION"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1931
                },
                {
                    "x": 2107,
                    "y": 1931
                },
                {
                    "x": 2107,
                    "y": 2163
                },
                {
                    "x": 442,
                    "y": 2163
                }
            ],
            "category": "paragraph",
            "html": "<p id='108' style='font-size:16px'>We have introduced Grid LSTM, a network that uses LSTM cells along all of the dimensions and<br>modulates in a novel fashion the multi-way interaction. We have seen the advantages of the cells<br>compared to regular connections in solving tasks such as parity, addition and memorization. We<br>have described powerful and flexible ways of applying the model to character prediction, machine<br>translation and image classification, showing strong performance across the board.</p>",
            "id": 108,
            "page": 10,
            "text": "We have introduced Grid LSTM, a network that uses LSTM cells along all of the dimensions and modulates in a novel fashion the multi-way interaction. We have seen the advantages of the cells compared to regular connections in solving tasks such as parity, addition and memorization. We have described powerful and flexible ways of applying the model to character prediction, machine translation and image classification, showing strong performance across the board."
        },
        {
            "bounding_box": [
                {
                    "x": 446,
                    "y": 2226
                },
                {
                    "x": 859,
                    "y": 2226
                },
                {
                    "x": 859,
                    "y": 2269
                },
                {
                    "x": 446,
                    "y": 2269
                }
            ],
            "category": "paragraph",
            "html": "<p id='109' style='font-size:18px'>ACKNOWLEDGEMENTS</p>",
            "id": 109,
            "page": 10,
            "text": "ACKNOWLEDGEMENTS"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2310
                },
                {
                    "x": 2103,
                    "y": 2310
                },
                {
                    "x": 2103,
                    "y": 2403
                },
                {
                    "x": 443,
                    "y": 2403
                }
            ],
            "category": "paragraph",
            "html": "<p id='110' style='font-size:20px'>We thank Koray Kavukcuoglu, Razvan Pascanu, Ilya Sutskever and Oriol Vinyals for helpful com-<br>ments and discussions.</p>",
            "id": 110,
            "page": 10,
            "text": "We thank Koray Kavukcuoglu, Razvan Pascanu, Ilya Sutskever and Oriol Vinyals for helpful comments and discussions."
        },
        {
            "bounding_box": [
                {
                    "x": 446,
                    "y": 2472
                },
                {
                    "x": 734,
                    "y": 2472
                },
                {
                    "x": 734,
                    "y": 2522
                },
                {
                    "x": 446,
                    "y": 2522
                }
            ],
            "category": "paragraph",
            "html": "<p id='111' style='font-size:20px'>REFERENCES</p>",
            "id": 111,
            "page": 10,
            "text": "REFERENCES"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2549
                },
                {
                    "x": 2106,
                    "y": 2549
                },
                {
                    "x": 2106,
                    "y": 2635
                },
                {
                    "x": 444,
                    "y": 2635
                }
            ],
            "category": "paragraph",
            "html": "<p id='112' style='font-size:14px'>Bahdanau, Dzmitry, Cho, Kyunghyun, and Bengio, Yoshua. Neural machine translation by jointly learning to<br>align and translate. CoRR, abs/1409.0473, 2014. URL http : / / arxiv · org/ abs / 1409 · 0473.</p>",
            "id": 112,
            "page": 10,
            "text": "Bahdanau, Dzmitry, Cho, Kyunghyun, and Bengio, Yoshua. Neural machine translation by jointly learning to align and translate. CoRR, abs/1409.0473, 2014. URL http : / / arxiv · org/ abs / 1409 · 0473."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2673
                },
                {
                    "x": 2104,
                    "y": 2673
                },
                {
                    "x": 2104,
                    "y": 2801
                },
                {
                    "x": 443,
                    "y": 2801
                }
            ],
            "category": "paragraph",
            "html": "<p id='113' style='font-size:16px'>Cho, Kyunghyun, van Merrienboer, Bart, G�l�ehre, Caglar, Bougares, Fethi, Schwenk, Holger, and Bengio,<br>Yoshua. Learning phrase representations using RNN encoder-decoder for statistical machine translation.<br>CoRR, abs/1406.1078, 2014. URL http : / / arxiv · org/ abs / 1 406 · 1 078.</p>",
            "id": 113,
            "page": 10,
            "text": "Cho, Kyunghyun, van Merrienboer, Bart, G�l�ehre, Caglar, Bougares, Fethi, Schwenk, Holger, and Bengio, Yoshua. Learning phrase representations using RNN encoder-decoder for statistical machine translation. CoRR, abs/1406.1078, 2014. URL http : / / arxiv · org/ abs / 1 406 · 1 078."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2841
                },
                {
                    "x": 2104,
                    "y": 2841
                },
                {
                    "x": 2104,
                    "y": 2926
                },
                {
                    "x": 443,
                    "y": 2926
                }
            ],
            "category": "paragraph",
            "html": "<p id='114' style='font-size:14px'>Chung, Junyoung, G�l�ehre, Caglar, Cho, KyungHyun, and Bengio, Yoshua. Gated feedback recurrent neural<br>networks. CoRR, abs/1502.02367, 2015. URL http : / / arxiv · org/ abs/ 1502 · 02367.</p>",
            "id": 114,
            "page": 10,
            "text": "Chung, Junyoung, G�l�ehre, Caglar, Cho, KyungHyun, and Bengio, Yoshua. Gated feedback recurrent neural networks. CoRR, abs/1502.02367, 2015. URL http : / / arxiv · org/ abs/ 1502 · 02367."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2966
                },
                {
                    "x": 2107,
                    "y": 2966
                },
                {
                    "x": 2107,
                    "y": 3050
                },
                {
                    "x": 444,
                    "y": 3050
                }
            ],
            "category": "paragraph",
            "html": "<p id='115' style='font-size:16px'>Ciresan, Dan Claudiu, Meier, Ueli, and Schmidhuber, Jurgen. Multi-column deep neural networks for image<br>classification. In arXiv:1202.2745v1 [cs.CV], 2012.</p>",
            "id": 115,
            "page": 10,
            "text": "Ciresan, Dan Claudiu, Meier, Ueli, and Schmidhuber, Jurgen. Multi-column deep neural networks for image classification. In arXiv:1202.2745v1 [cs.CV], 2012."
        },
        {
            "bounding_box": [
                {
                    "x": 1253,
                    "y": 3133
                },
                {
                    "x": 1300,
                    "y": 3133
                },
                {
                    "x": 1300,
                    "y": 3172
                },
                {
                    "x": 1253,
                    "y": 3172
                }
            ],
            "category": "footer",
            "html": "<footer id='116' style='font-size:16px'>10</footer>",
            "id": 116,
            "page": 10,
            "text": "10"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 112
                },
                {
                    "x": 1286,
                    "y": 112
                },
                {
                    "x": 1286,
                    "y": 157
                },
                {
                    "x": 445,
                    "y": 157
                }
            ],
            "category": "header",
            "html": "<header id='117' style='font-size:20px'>Under review as a conference paper at ICLR 2016</header>",
            "id": 117,
            "page": 11,
            "text": "Under review as a conference paper at ICLR 2016"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 349
                },
                {
                    "x": 2108,
                    "y": 349
                },
                {
                    "x": 2108,
                    "y": 518
                },
                {
                    "x": 443,
                    "y": 518
                }
            ],
            "category": "paragraph",
            "html": "<p id='118' style='font-size:16px'>Duch, Wlodzislaw. K-separability. In Kollias, Stefanos, Stafylopatis, Andreas, Duch, Wlodzislaw, and Oja,<br>Erkki (eds.), Artificial Neural Networks, ICANN 2006, volume 4131 of Lecture Notes in Computer Science,<br>pp. 188-197. Springer Berlin Heidelberg, 2006. ISBN 978-3-540-38625-4. doi: 10.1007/11840817 _20.<br>URL http : / / dx · doi · org/1 0 · 1007/11840817_20.</p>",
            "id": 118,
            "page": 11,
            "text": "Duch, Wlodzislaw. K-separability. In Kollias, Stefanos, Stafylopatis, Andreas, Duch, Wlodzislaw, and Oja, Erkki (eds.), Artificial Neural Networks, ICANN 2006, volume 4131 of Lecture Notes in Computer Science, pp. 188-197. Springer Berlin Heidelberg, 2006. ISBN 978-3-540-38625-4. doi: 10.1007/11840817 _20. URL http : / / dx · doi · org/1 0 · 1007/11840817_20."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 552
                },
                {
                    "x": 2106,
                    "y": 552
                },
                {
                    "x": 2106,
                    "y": 719
                },
                {
                    "x": 443,
                    "y": 719
                }
            ],
            "category": "paragraph",
            "html": "<p id='119' style='font-size:16px'>Duchi, John, Hazan, Elad, and Singer, Yoram. Adaptive subgradient methods for online learning and<br>stochastic optimization. Technical Report UCB/EECS-2010-24, EECS Department, University of Cali-<br>fornia, Berkeley, Mar 2010. URL http : / / www eecs · berkeley · edu/Pubs /TechRpts/2010/<br>EECS-2010-24 html.</p>",
            "id": 119,
            "page": 11,
            "text": "Duchi, John, Hazan, Elad, and Singer, Yoram. Adaptive subgradient methods for online learning and stochastic optimization. Technical Report UCB/EECS-2010-24, EECS Department, University of California, Berkeley, Mar 2010. URL http : / / www eecs · berkeley · edu/Pubs /TechRpts/2010/ EECS-2010-24 html."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 754
                },
                {
                    "x": 2107,
                    "y": 754
                },
                {
                    "x": 2107,
                    "y": 924
                },
                {
                    "x": 442,
                    "y": 924
                }
            ],
            "category": "paragraph",
            "html": "<p id='120' style='font-size:22px'>Dyer, Chris, Lopez, Adam, Ganitkevitch, Juri, Weese, Johnathan, Ture, Ferhan, Blunsom, Phil, Setiawan,<br>Hendra, Eidelman, Vladimir, and Resnik, Philip. cdec: A decoder, alignment, and learning framework<br>for finite-state and context-free translation models. In Proceedings of the Association for Computational<br>Linguistics (ACL), 2010.</p>",
            "id": 120,
            "page": 11,
            "text": "Dyer, Chris, Lopez, Adam, Ganitkevitch, Juri, Weese, Johnathan, Ture, Ferhan, Blunsom, Phil, Setiawan, Hendra, Eidelman, Vladimir, and Resnik, Philip. cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models. In Proceedings of the Association for Computational Linguistics (ACL), 2010."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 958
                },
                {
                    "x": 2105,
                    "y": 958
                },
                {
                    "x": 2105,
                    "y": 1128
                },
                {
                    "x": 442,
                    "y": 1128
                }
            ],
            "category": "paragraph",
            "html": "<p id='121' style='font-size:18px'>Goodfellow, Ian J., Warde-Farley, David, Mirza, Mehdi, Courville, Aaron C., and Bengio, Yoshua. Maxout<br>networks. In Proceedings of the 30th International Conference on Machine Learning, ICML 2013, At-<br>lanta, GA, USA, 16-21 June 2013, pp. 1319-1327, 2013. URL http : / / jmlr · org/proceedings/<br>papers/v28/ goodfellow13 · html.</p>",
            "id": 121,
            "page": 11,
            "text": "Goodfellow, Ian J., Warde-Farley, David, Mirza, Mehdi, Courville, Aaron C., and Bengio, Yoshua. Maxout networks. In Proceedings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, pp. 1319-1327, 2013. URL http : / / jmlr · org/proceedings/ papers/v28/ goodfellow13 · html."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1163
                },
                {
                    "x": 2104,
                    "y": 1163
                },
                {
                    "x": 2104,
                    "y": 1247
                },
                {
                    "x": 441,
                    "y": 1247
                }
            ],
            "category": "paragraph",
            "html": "<p id='122' style='font-size:14px'>Graham, Benjamin. Spatially-sparse convolutional neural networks. CoRR, abs/1409.6070, 2014a. URL<br>http : / / arxiv · org/ abs/1409 · 6070.</p>",
            "id": 122,
            "page": 11,
            "text": "Graham, Benjamin. Spatially-sparse convolutional neural networks. CoRR, abs/1409.6070, 2014a. URL http : / / arxiv · org/ abs/1409 · 6070."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1283
                },
                {
                    "x": 2102,
                    "y": 1283
                },
                {
                    "x": 2102,
                    "y": 1365
                },
                {
                    "x": 442,
                    "y": 1365
                }
            ],
            "category": "paragraph",
            "html": "<p id='123' style='font-size:14px'>Graham, Benjamin. Fractional max-pooling. CoRR, abs/1412.6071, 2014b. URL http : / / arxiv · org/<br>abs/1412 · 6071.</p>",
            "id": 123,
            "page": 11,
            "text": "Graham, Benjamin. Fractional max-pooling. CoRR, abs/1412.6071, 2014b. URL http : / / arxiv · org/ abs/1412 · 6071."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1403
                },
                {
                    "x": 2002,
                    "y": 1403
                },
                {
                    "x": 2002,
                    "y": 1448
                },
                {
                    "x": 443,
                    "y": 1448
                }
            ],
            "category": "paragraph",
            "html": "<p id='124' style='font-size:20px'>Graves, A. Supervised sequence labelling with recurrent neural networks, volume 385. Springer, 2012.</p>",
            "id": 124,
            "page": 11,
            "text": "Graves, A. Supervised sequence labelling with recurrent neural networks, volume 385. Springer, 2012."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1482
                },
                {
                    "x": 2105,
                    "y": 1482
                },
                {
                    "x": 2105,
                    "y": 1566
                },
                {
                    "x": 443,
                    "y": 1566
                }
            ],
            "category": "paragraph",
            "html": "<p id='125' style='font-size:18px'>Graves, A. and Schmidhuber, J. Offline handwriting recognition with multidimensional recurrent neural net-<br>works. In Advances in Neural Information Processing Systems, volume 21, 2008.</p>",
            "id": 125,
            "page": 11,
            "text": "Graves, A. and Schmidhuber, J. Offline handwriting recognition with multidimensional recurrent neural networks. In Advances in Neural Information Processing Systems, volume 21, 2008."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1602
                },
                {
                    "x": 2106,
                    "y": 1602
                },
                {
                    "x": 2106,
                    "y": 1689
                },
                {
                    "x": 442,
                    "y": 1689
                }
            ],
            "category": "paragraph",
            "html": "<p id='126' style='font-size:22px'>Graves, A., Fernandez, S., and Schmidhuber, J. Multi-dimensional recurrent neural networks. In Proceedings<br>of the 2007 International Conference on Artificial Neural Networks, Porto, Portugal, September 2007.</p>",
            "id": 126,
            "page": 11,
            "text": "Graves, A., Fernandez, S., and Schmidhuber, J. Multi-dimensional recurrent neural networks. In Proceedings of the 2007 International Conference on Artificial Neural Networks, Porto, Portugal, September 2007."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1722
                },
                {
                    "x": 2104,
                    "y": 1722
                },
                {
                    "x": 2104,
                    "y": 1807
                },
                {
                    "x": 441,
                    "y": 1807
                }
            ],
            "category": "paragraph",
            "html": "<p id='127' style='font-size:20px'>Graves, A., Mohamed, A., and Hinton, G. Speech recognition with deep recurrent neural networks. In Proc<br>ICASSP 2013, Vancouver, Canada, May 2013.</p>",
            "id": 127,
            "page": 11,
            "text": "Graves, A., Mohamed, A., and Hinton, G. Speech recognition with deep recurrent neural networks. In Proc ICASSP 2013, Vancouver, Canada, May 2013."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1843
                },
                {
                    "x": 2105,
                    "y": 1843
                },
                {
                    "x": 2105,
                    "y": 1926
                },
                {
                    "x": 442,
                    "y": 1926
                }
            ],
            "category": "paragraph",
            "html": "<p id='128' style='font-size:14px'>Graves, Alex. Generating sequences with recurrent neural networks. CoRR, abs/1308.0850, 2013. URL<br>http : / / arxiv · org/ abs /1308 · 0850.</p>",
            "id": 128,
            "page": 11,
            "text": "Graves, Alex. Generating sequences with recurrent neural networks. CoRR, abs/1308.0850, 2013. URL http : / / arxiv · org/ abs /1308 · 0850."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1962
                },
                {
                    "x": 2103,
                    "y": 1962
                },
                {
                    "x": 2103,
                    "y": 2049
                },
                {
                    "x": 442,
                    "y": 2049
                }
            ],
            "category": "paragraph",
            "html": "<p id='129' style='font-size:20px'>Hochreiter, S. Untersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, Institut fur Informatik,<br>Lehrstuhl Prof. Brauer, Technische Universitat Munchen, 1991.</p>",
            "id": 129,
            "page": 11,
            "text": "Hochreiter, S. Untersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, Institut fur Informatik, Lehrstuhl Prof. Brauer, Technische Universitat Munchen, 1991."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2083
                },
                {
                    "x": 2074,
                    "y": 2083
                },
                {
                    "x": 2074,
                    "y": 2127
                },
                {
                    "x": 444,
                    "y": 2127
                }
            ],
            "category": "paragraph",
            "html": "<p id='130' style='font-size:22px'>Hochreiter, S. and Schmidhuber, J. Long Short-Term Memory. Neural Computation, 9(8):1735-1780, 1997.</p>",
            "id": 130,
            "page": 11,
            "text": "Hochreiter, S. and Schmidhuber, J. Long Short-Term Memory. Neural Computation, 9(8):1735-1780, 1997."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2162
                },
                {
                    "x": 2106,
                    "y": 2162
                },
                {
                    "x": 2106,
                    "y": 2288
                },
                {
                    "x": 443,
                    "y": 2288
                }
            ],
            "category": "paragraph",
            "html": "<p id='131' style='font-size:18px'>Hochreiter, S., Bengio, Y., Frasconi, P., and Schmidhuber, J. Gradient flow in recurrent nets: the difficulty<br>of learning long-term dependencies. In Kremer and Kolen (eds.), A Field Guide to Dynamical Recurrent<br>Neural Networks. IEEE Press, 2001.</p>",
            "id": 131,
            "page": 11,
            "text": "Hochreiter, S., Bengio, Y., Frasconi, P., and Schmidhuber, J. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies. In Kremer and Kolen (eds.), A Field Guide to Dynamical Recurrent Neural Networks. IEEE Press, 2001."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2323
                },
                {
                    "x": 2103,
                    "y": 2323
                },
                {
                    "x": 2103,
                    "y": 2451
                },
                {
                    "x": 442,
                    "y": 2451
                }
            ],
            "category": "paragraph",
            "html": "<p id='132' style='font-size:18px'>Hohil, Myron E., Liu, Derong, and Smith, Stanley H. Solving the n-bit parity problem using neural networks.<br>Neural Networks, 12(9):1321-1323, 1999. doi: 10.1016/80893-6080(99)00069-6. URL http : / / dx.<br>doi · org/10 · 1016/S0893-6080 (99) 00069-6.</p>",
            "id": 132,
            "page": 11,
            "text": "Hohil, Myron E., Liu, Derong, and Smith, Stanley H. Solving the n-bit parity problem using neural networks. Neural Networks, 12(9):1321-1323, 1999. doi: 10.1016/80893-6080(99)00069-6. URL http : / / dx. doi · org/10 · 1016/S0893-6080 (99) 00069-6."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2485
                },
                {
                    "x": 2105,
                    "y": 2485
                },
                {
                    "x": 2105,
                    "y": 2531
                },
                {
                    "x": 444,
                    "y": 2531
                }
            ],
            "category": "paragraph",
            "html": "<p id='133' style='font-size:14px'>Hutter, Marcus. The human knowledge compression context, 2012. URL http : / /prize · hutter1 · net.</p>",
            "id": 133,
            "page": 11,
            "text": "Hutter, Marcus. The human knowledge compression context, 2012. URL http : / /prize · hutter1 · net."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2563
                },
                {
                    "x": 2105,
                    "y": 2563
                },
                {
                    "x": 2105,
                    "y": 2649
                },
                {
                    "x": 443,
                    "y": 2649
                }
            ],
            "category": "paragraph",
            "html": "<p id='134' style='font-size:20px'>Kalchbrenner, Nal and Blunsom, Phil. Recurrent continuous translation models. Seattle, October 2013. Asso-<br>ciation for Computational Linguistics.</p>",
            "id": 134,
            "page": 11,
            "text": "Kalchbrenner, Nal and Blunsom, Phil. Recurrent continuous translation models. Seattle, October 2013. Association for Computational Linguistics."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2685
                },
                {
                    "x": 2104,
                    "y": 2685
                },
                {
                    "x": 2104,
                    "y": 2769
                },
                {
                    "x": 443,
                    "y": 2769
                }
            ],
            "category": "paragraph",
            "html": "<p id='135' style='font-size:16px'>Kingma, Diederik P. and Ba, Jimmy. Adam: A method for stochastic optimization. CoRR, abs/1412.6980,<br>2014. URL http: / / arxiv · org/ abs/1412 · 6980.</p>",
            "id": 135,
            "page": 11,
            "text": "Kingma, Diederik P. and Ba, Jimmy. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2014. URL http: / / arxiv · org/ abs/1412 · 6980."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2805
                },
                {
                    "x": 2103,
                    "y": 2805
                },
                {
                    "x": 2103,
                    "y": 2929
                },
                {
                    "x": 444,
                    "y": 2929
                }
            ],
            "category": "paragraph",
            "html": "<p id='136' style='font-size:18px'>Kiros, Ryan, Salakhutdinov, Ruslan, and Zemel, Richard S. Unifying visual-semantic embeddings with multi-<br>modal neural language models. CoRR, abs/1411.2539, 2014. URL http : / / arxiv · org/ abs / 1 411.<br>2539.</p>",
            "id": 136,
            "page": 11,
            "text": "Kiros, Ryan, Salakhutdinov, Ruslan, and Zemel, Richard S. Unifying visual-semantic embeddings with multimodal neural language models. CoRR, abs/1411.2539, 2014. URL http : / / arxiv · org/ abs / 1 411. 2539."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2968
                },
                {
                    "x": 2105,
                    "y": 2968
                },
                {
                    "x": 2105,
                    "y": 3050
                },
                {
                    "x": 442,
                    "y": 3050
                }
            ],
            "category": "paragraph",
            "html": "<p id='137' style='font-size:22px'>LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document recognition.<br>Proceedings of the IEEE, 86(11):2278-2324, 1998.</p>",
            "id": 137,
            "page": 11,
            "text": "LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998."
        },
        {
            "bounding_box": [
                {
                    "x": 1253,
                    "y": 3134
                },
                {
                    "x": 1296,
                    "y": 3134
                },
                {
                    "x": 1296,
                    "y": 3171
                },
                {
                    "x": 1253,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='138' style='font-size:20px'>11</footer>",
            "id": 138,
            "page": 11,
            "text": "11"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 113
                },
                {
                    "x": 1286,
                    "y": 113
                },
                {
                    "x": 1286,
                    "y": 156
                },
                {
                    "x": 445,
                    "y": 156
                }
            ],
            "category": "header",
            "html": "<header id='139' style='font-size:18px'>Under review as a conference paper at ICLR 2016</header>",
            "id": 139,
            "page": 12,
            "text": "Under review as a conference paper at ICLR 2016"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 349
                },
                {
                    "x": 2109,
                    "y": 349
                },
                {
                    "x": 2109,
                    "y": 517
                },
                {
                    "x": 443,
                    "y": 517
                }
            ],
            "category": "paragraph",
            "html": "<p id='140' style='font-size:18px'>Lee, Chen- Yu, Xie, Saining, Gallagher, Patrick, Zhang, Zhengyou, and Tu, Zhuowen. Deeply-supervised nets.<br>In Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics, AISTATS<br>2015, San Diego, California, USA, May 9-12, 2015, 2015. URL http : / / jml r · org/proceedings/<br>papers/v38/lee15a . html.</p>",
            "id": 140,
            "page": 12,
            "text": "Lee, Chen- Yu, Xie, Saining, Gallagher, Patrick, Zhang, Zhengyou, and Tu, Zhuowen. Deeply-supervised nets. In Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2015, San Diego, California, USA, May 9-12, 2015, 2015. URL http : / / jml r · org/proceedings/ papers/v38/lee15a . html."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 550
                },
                {
                    "x": 2100,
                    "y": 550
                },
                {
                    "x": 2100,
                    "y": 633
                },
                {
                    "x": 442,
                    "y": 633
                }
            ],
            "category": "paragraph",
            "html": "<p id='141' style='font-size:14px'>Lin, Min, Chen, Qiang, and Yan, Shuicheng. Network in network. CoRR, abs/1312.4400, 2013. URL http:<br>/ / arxiv · org/ abs / 1312 · 4400.</p>",
            "id": 141,
            "page": 12,
            "text": "Lin, Min, Chen, Qiang, and Yan, Shuicheng. Network in network. CoRR, abs/1312.4400, 2013. URL http: / / arxiv · org/ abs / 1312 · 4400."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 667
                },
                {
                    "x": 2106,
                    "y": 667
                },
                {
                    "x": 2106,
                    "y": 752
                },
                {
                    "x": 444,
                    "y": 752
                }
            ],
            "category": "paragraph",
            "html": "<p id='142' style='font-size:14px'>Mairal, Julien, Koniusz, Piotr, Harchaoui, Zaid, and Schmid, Cordelia. Convolutional kernel networks. Neural<br>Information Processing Systems, 2014. URL http: / / arxiv · org/ abs/1406 · 3332.</p>",
            "id": 142,
            "page": 12,
            "text": "Mairal, Julien, Koniusz, Piotr, Harchaoui, Zaid, and Schmid, Cordelia. Convolutional kernel networks. Neural Information Processing Systems, 2014. URL http: / / arxiv · org/ abs/1406 · 3332."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 785
                },
                {
                    "x": 2105,
                    "y": 785
                },
                {
                    "x": 2105,
                    "y": 869
                },
                {
                    "x": 444,
                    "y": 869
                }
            ],
            "category": "paragraph",
            "html": "<p id='143' style='font-size:20px'>Marvin Minsky, Seymour Papert. Perceptrons: An Introduction to Computational Geometry. MIT Press,<br>Cambridge MA, 1972.</p>",
            "id": 143,
            "page": 12,
            "text": "Marvin Minsky, Seymour Papert. Perceptrons: An Introduction to Computational Geometry. MIT Press, Cambridge MA, 1972."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 901
                },
                {
                    "x": 2104,
                    "y": 901
                },
                {
                    "x": 2104,
                    "y": 1029
                },
                {
                    "x": 442,
                    "y": 1029
                }
            ],
            "category": "paragraph",
            "html": "<p id='144' style='font-size:14px'>Nair, Vinod and Hinton, Geoffrey E. Rectified linear units improve restricted boltzmann machines. In Pro-<br>ceedings of the 27th International Conference on Machine Learning (ICML-10), June 21-24, 2010, Haifa,<br>Israel, pp. 807-814, 2010. URL http : / / www · icml2010 · org/ papers/ 432 · pdf.</p>",
            "id": 144,
            "page": 12,
            "text": "Nair, Vinod and Hinton, Geoffrey E. Rectified linear units improve restricted boltzmann machines. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), June 21-24, 2010, Haifa, Israel, pp. 807-814, 2010. URL http : / / www · icml2010 · org/ papers/ 432 · pdf."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1060
                },
                {
                    "x": 2107,
                    "y": 1060
                },
                {
                    "x": 2107,
                    "y": 1228
                },
                {
                    "x": 442,
                    "y": 1228
                }
            ],
            "category": "paragraph",
            "html": "<p id='145' style='font-size:14px'>Simard, Patrice Y., Steinkraus, David, and Platt, John C. Best practices for convolutional neural networks<br>applied to visual document analysis. In 7th International Conference on Document Analysis and Recognition<br>(ICDAR 2003), 2-Volume Set, 3-6 August 2003, Edinburgh, Scotland, UK, pp. 958-962, 2003. doi: 10.1109/<br>ICDAR.2003.1227801. URL http : / / dx · doi · org/10 · 1109 / ICDAR · 2003 。 1227801.</p>",
            "id": 145,
            "page": 12,
            "text": "Simard, Patrice Y., Steinkraus, David, and Platt, John C. Best practices for convolutional neural networks applied to visual document analysis. In 7th International Conference on Document Analysis and Recognition (ICDAR 2003), 2-Volume Set, 3-6 August 2003, Edinburgh, Scotland, UK, pp. 958-962, 2003. doi: 10.1109/ ICDAR.2003.1227801. URL http : / / dx · doi · org/10 · 1109 / ICDAR · 2003 。 1227801."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1262
                },
                {
                    "x": 2105,
                    "y": 1262
                },
                {
                    "x": 2105,
                    "y": 1345
                },
                {
                    "x": 443,
                    "y": 1345
                }
            ],
            "category": "paragraph",
            "html": "<p id='146' style='font-size:18px'>Srivastava, Rupesh Kumar, Greff, Klaus, and Schmidhuber, Jurgen. Highway networks. arXiv preprint<br>arXiv:1505.00387, 2015.</p>",
            "id": 146,
            "page": 12,
            "text": "Srivastava, Rupesh Kumar, Greff, Klaus, and Schmidhuber, Jurgen. Highway networks. arXiv preprint arXiv:1505.00387, 2015."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 1378
                },
                {
                    "x": 2029,
                    "y": 1378
                },
                {
                    "x": 2029,
                    "y": 1422
                },
                {
                    "x": 444,
                    "y": 1422
                }
            ],
            "category": "paragraph",
            "html": "<p id='147' style='font-size:16px'>Sutskever, I., Martens, J., and Hinton, G. Generating text with recurrent neural networks. In ICML, 2011.</p>",
            "id": 147,
            "page": 12,
            "text": "Sutskever, I., Martens, J., and Hinton, G. Generating text with recurrent neural networks. In ICML, 2011."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1453
                },
                {
                    "x": 2106,
                    "y": 1453
                },
                {
                    "x": 2106,
                    "y": 1540
                },
                {
                    "x": 443,
                    "y": 1540
                }
            ],
            "category": "paragraph",
            "html": "<p id='148' style='font-size:18px'>Sutskever, Ilya, Vinyals, Oriol, and Le, Quoc VV. Sequence to sequence learning with neural networks. In<br>Advances in Neural Information Processing Systems, pp. 3104-3112, 2014.</p>",
            "id": 148,
            "page": 12,
            "text": "Sutskever, Ilya, Vinyals, Oriol, and Le, Quoc VV. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pp. 3104-3112, 2014."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 1573
                },
                {
                    "x": 2102,
                    "y": 1573
                },
                {
                    "x": 2102,
                    "y": 1656
                },
                {
                    "x": 444,
                    "y": 1656
                }
            ],
            "category": "paragraph",
            "html": "<p id='149' style='font-size:18px'>Vinyals, Oriol, Toshev, Alexander, Bengio, Samy, and Erhan, Dumitru. Show and tell: A neural image caption<br>generator. arXiv preprint arXiv:1411.4555, 2014.</p>",
            "id": 149,
            "page": 12,
            "text": "Vinyals, Oriol, Toshev, Alexander, Bengio, Samy, and Erhan, Dumitru. Show and tell: A neural image caption generator. arXiv preprint arXiv:1411.4555, 2014."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 1690
                },
                {
                    "x": 2105,
                    "y": 1690
                },
                {
                    "x": 2105,
                    "y": 1815
                },
                {
                    "x": 444,
                    "y": 1815
                }
            ],
            "category": "paragraph",
            "html": "<p id='150' style='font-size:14px'>Visin, Francesco, Kastner, Kyle, Cho, Kyunghyun, Matteucci, Matteo, Courville, Aaron C., and Ben-<br>gio, Yoshua. Renet: A recurrent neural network based alternative to convolutional networks. CoRR,<br>abs/1505.00393, 2015. URL http : / / arxiv org/ abs/1505 · 00393.</p>",
            "id": 150,
            "page": 12,
            "text": "Visin, Francesco, Kastner, Kyle, Cho, Kyunghyun, Matteucci, Matteo, Courville, Aaron C., and Bengio, Yoshua. Renet: A recurrent neural network based alternative to convolutional networks. CoRR, abs/1505.00393, 2015. URL http : / / arxiv org/ abs/1505 · 00393."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1848
                },
                {
                    "x": 2106,
                    "y": 1848
                },
                {
                    "x": 2106,
                    "y": 1973
                },
                {
                    "x": 443,
                    "y": 1973
                }
            ],
            "category": "paragraph",
            "html": "<p id='151' style='font-size:14px'>Wan, Li, Zeiler, Matthew D., Zhang, Sixin, LeCun, Yann, and Fergus, Rob. Regularization of neural networks<br>using dropconnect. In ICML (3), volume 28 of JMLR Proceedings, pp. 1058-1066. JMLR.org, 2013. URL<br>http : / / dblp · uni-trier · de/ db / conf / icml / i cml2013 · html #WanZZLF13.</p>",
            "id": 151,
            "page": 12,
            "text": "Wan, Li, Zeiler, Matthew D., Zhang, Sixin, LeCun, Yann, and Fergus, Rob. Regularization of neural networks using dropconnect. In ICML (3), volume 28 of JMLR Proceedings, pp. 1058-1066. JMLR.org, 2013. URL http : / / dblp · uni-trier · de/ db / conf / icml / i cml2013 · html #WanZZLF13."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2006
                },
                {
                    "x": 2105,
                    "y": 2006
                },
                {
                    "x": 2105,
                    "y": 2091
                },
                {
                    "x": 444,
                    "y": 2091
                }
            ],
            "category": "paragraph",
            "html": "<p id='152' style='font-size:16px'>Yao, Kaisheng, Cohn, Trevor, Vylomova, Katerina, Duh, Kevin, and Dyer, Chris. Depth-gated LSTM. CoRR,<br>abs/1508.03790, 2015. URL http : / / arxiv · org/ abs/1508 · 03790.</p>",
            "id": 152,
            "page": 12,
            "text": "Yao, Kaisheng, Cohn, Trevor, Vylomova, Katerina, Duh, Kevin, and Dyer, Chris. Depth-gated LSTM. CoRR, abs/1508.03790, 2015. URL http : / / arxiv · org/ abs/1508 · 03790."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2123
                },
                {
                    "x": 2100,
                    "y": 2123
                },
                {
                    "x": 2100,
                    "y": 2209
                },
                {
                    "x": 443,
                    "y": 2209
                }
            ],
            "category": "paragraph",
            "html": "<p id='153' style='font-size:14px'>Zaremba, Wojciech and Sutskever, Ilya. Learning to execute. CoRR, abs/1410.4615, 2014. URL http:<br>/ / arxiv · org/ abs /1 410 · 4615.</p>",
            "id": 153,
            "page": 12,
            "text": "Zaremba, Wojciech and Sutskever, Ilya. Learning to execute. CoRR, abs/1410.4615, 2014. URL http: / / arxiv · org/ abs /1 410 · 4615."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 2304
                },
                {
                    "x": 677,
                    "y": 2304
                },
                {
                    "x": 677,
                    "y": 2350
                },
                {
                    "x": 445,
                    "y": 2350
                }
            ],
            "category": "paragraph",
            "html": "<p id='154' style='font-size:22px'>APPENDIX</p>",
            "id": 154,
            "page": 12,
            "text": "APPENDIX"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2406
                },
                {
                    "x": 2104,
                    "y": 2406
                },
                {
                    "x": 2104,
                    "y": 2542
                },
                {
                    "x": 442,
                    "y": 2542
                }
            ],
            "category": "paragraph",
            "html": "<p id='155' style='font-size:18px'>We here report on two additional results, one algorithmic and the other one empirical, where we see<br>that without special initialization or training tricks, a 1-LSTM network can learn to compute parity<br>for up to 250 input bits, and a 3-LSTM network applied to images obtains strong results on MNIST.</p>",
            "id": 155,
            "page": 12,
            "text": "We here report on two additional results, one algorithmic and the other one empirical, where we see that without special initialization or training tricks, a 1-LSTM network can learn to compute parity for up to 250 input bits, and a 3-LSTM network applied to images obtains strong results on MNIST."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2601
                },
                {
                    "x": 684,
                    "y": 2601
                },
                {
                    "x": 684,
                    "y": 2645
                },
                {
                    "x": 444,
                    "y": 2645
                }
            ],
            "category": "paragraph",
            "html": "<p id='156' style='font-size:20px'>5.1 PARITY</p>",
            "id": 156,
            "page": 12,
            "text": "5.1 PARITY"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2685
                },
                {
                    "x": 2107,
                    "y": 2685
                },
                {
                    "x": 2107,
                    "y": 3055
                },
                {
                    "x": 443,
                    "y": 3055
                }
            ],
            "category": "paragraph",
            "html": "<p id='157' style='font-size:18px'>We apply one-dimensional Grid LSTM to learning parity. Given a string b1, · · · , bk of k bits 0 or 1,<br>the parity or generalized XOR of the string is defined to be 1 if the sum of the bits is odd, and 0 if<br>the sum of the bits is even. Although manually crafted neural networks for the problem have been<br>devised (Hohil et al., 1999), training a generic neural network from a finite number of examples and<br>a generic random initialization of the weights to successfully learn to compute the parity of k-bit<br>strings for significant values of k is a longstanding problem (Marvin Minsky, 1972; Duch, 2006). It<br>is core to the problem that the k-bit string is given to the neural network as a whole through a single<br>projection; considering one bit at a time and remembering the previous partial result in a recurrent</p>",
            "id": 157,
            "page": 12,
            "text": "We apply one-dimensional Grid LSTM to learning parity. Given a string b1, · · · , bk of k bits 0 or 1, the parity or generalized XOR of the string is defined to be 1 if the sum of the bits is odd, and 0 if the sum of the bits is even. Although manually crafted neural networks for the problem have been devised (Hohil , 1999), training a generic neural network from a finite number of examples and a generic random initialization of the weights to successfully learn to compute the parity of k-bit strings for significant values of k is a longstanding problem (Marvin Minsky, 1972; Duch, 2006). It is core to the problem that the k-bit string is given to the neural network as a whole through a single projection; considering one bit at a time and remembering the previous partial result in a recurrent"
        },
        {
            "bounding_box": [
                {
                    "x": 1254,
                    "y": 3133
                },
                {
                    "x": 1299,
                    "y": 3133
                },
                {
                    "x": 1299,
                    "y": 3170
                },
                {
                    "x": 1254,
                    "y": 3170
                }
            ],
            "category": "footer",
            "html": "<footer id='158' style='font-size:18px'>12</footer>",
            "id": 158,
            "page": 12,
            "text": "12"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 112
                },
                {
                    "x": 1287,
                    "y": 112
                },
                {
                    "x": 1287,
                    "y": 157
                },
                {
                    "x": 445,
                    "y": 157
                }
            ],
            "category": "header",
            "html": "<header id='159' style='font-size:16px'>Under review as a conference paper at ICLR 2016</header>",
            "id": 159,
            "page": 13,
            "text": "Under review as a conference paper at ICLR 2016"
        },
        {
            "bounding_box": [
                {
                    "x": 483,
                    "y": 337
                },
                {
                    "x": 2067,
                    "y": 337
                },
                {
                    "x": 2067,
                    "y": 833
                },
                {
                    "x": 483,
                    "y": 833
                }
            ],
            "category": "figure",
            "html": "<figure><img id='160' style='font-size:14px' alt=\"160 160\n8\n140 140\n120 120\n100 100\nLayers 80 80\n60 60\n40 40\n20 20\n0 0\n0 50 100 150 200 250 0 50 100 150 200 250\nInput Bits Input Bits\" data-coord=\"top-left:(483,337); bottom-right:(2067,833)\" /></figure>",
            "id": 160,
            "page": 13,
            "text": "160 160 8 140 140 120 120 100 100 Layers 80 80 60 60 40 40 20 20 0 0 0 50 100 150 200 250 0 50 100 150 200 250 Input Bits Input Bits"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 874
                },
                {
                    "x": 2107,
                    "y": 874
                },
                {
                    "x": 2107,
                    "y": 1248
                },
                {
                    "x": 442,
                    "y": 1248
                }
            ],
            "category": "caption",
            "html": "<caption id='161' style='font-size:20px'>Figure 9: Results on training tied 1-LSTM networks to compute the k-bit parity of k input bits.<br>The left diagram contains solutions found with 1-LSTM networks with 500 hidden units, whereas<br>the right diagram shows solutions found with 1-LSTM networks with 1500 units. The horizontal<br>axis corresponds to the number k of input bits. The vertical axis corresponds to the number of<br>layers in the networks. Each point in the diagram corresponds to 100% classification accuracy of<br>the respective network on a sample of 100 unseen k-bit strings. The networks see up to 10 million<br>bit strings during training but often find solutions with many fewer strings. Missing points in the<br>diagram indicate failure to find a solution within the training set size or time constraints.</caption>",
            "id": 161,
            "page": 13,
            "text": "Figure 9: Results on training tied 1-LSTM networks to compute the k-bit parity of k input bits. The left diagram contains solutions found with 1-LSTM networks with 500 hidden units, whereas the right diagram shows solutions found with 1-LSTM networks with 1500 units. The horizontal axis corresponds to the number k of input bits. The vertical axis corresponds to the number of layers in the networks. Each point in the diagram corresponds to 100% classification accuracy of the respective network on a sample of 100 unseen k-bit strings. The networks see up to 10 million bit strings during training but often find solutions with many fewer strings. Missing points in the diagram indicate failure to find a solution within the training set size or time constraints."
        },
        {
            "bounding_box": [
                {
                    "x": 438,
                    "y": 1299
                },
                {
                    "x": 2105,
                    "y": 1299
                },
                {
                    "x": 2105,
                    "y": 1560
                },
                {
                    "x": 438,
                    "y": 1560
                }
            ],
            "category": "figure",
            "html": "<figure><img id='162' style='font-size:14px' alt=\"Layers Hidden Input Bits k\n20\nLayer\nTied Tanh FFN 5 1500 30 Projection\n5 10 15 20 25\nTied ReLU FFN 4 1500 30\n0\nTied 1-LSTM 72 1500 220\nTied 1-LSTM 148 500 250\n-20\" data-coord=\"top-left:(438,1299); bottom-right:(2105,1560)\" /></figure>",
            "id": 162,
            "page": 13,
            "text": "Layers Hidden Input Bits k 20 Layer Tied Tanh FFN 5 1500 30 Projection 5 10 15 20 25 Tied ReLU FFN 4 1500 30 0 Tied 1-LSTM 72 1500 220 Tied 1-LSTM 148 500 250 -20"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1593
                },
                {
                    "x": 2107,
                    "y": 1593
                },
                {
                    "x": 2107,
                    "y": 1783
                },
                {
                    "x": 441,
                    "y": 1783
                }
            ],
            "category": "paragraph",
            "html": "<p id='163' style='font-size:18px'>Figure 10: The left table reports the best performing networks on k-bit parity. The right figure is a<br>heat map of activation values of selected counter neurons in a 1-LSTM network that has 25 layers<br>and is trained on the parity of 50-bit strings. The specific values are obtained by a feed-forward pass<br>through the network using as input the bit string 010140 ; different bit strings gave similar results.</p>",
            "id": 163,
            "page": 13,
            "text": "Figure 10: The left table reports the best performing networks on k-bit parity. The right figure is a heat map of activation values of selected counter neurons in a 1-LSTM network that has 25 layers and is trained on the parity of 50-bit strings. The specific values are obtained by a feed-forward pass through the network using as input the bit string 010140 ; different bit strings gave similar results."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1881
                },
                {
                    "x": 2105,
                    "y": 1881
                },
                {
                    "x": 2105,
                    "y": 2018
                },
                {
                    "x": 441,
                    "y": 2018
                }
            ],
            "category": "paragraph",
            "html": "<p id='164' style='font-size:22px'>or multi-step architecture reduces the problem of learning k-bit parity to the simple one of learning<br>just 2-bit parity. Learning parity is difficult because a change in a single bit in the input changes the<br>target value and the decision boundaries in the resulting space are highly non-linear.</p>",
            "id": 164,
            "page": 13,
            "text": "or multi-step architecture reduces the problem of learning k-bit parity to the simple one of learning just 2-bit parity. Learning parity is difficult because a change in a single bit in the input changes the target value and the decision boundaries in the resulting space are highly non-linear."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2042
                },
                {
                    "x": 2106,
                    "y": 2042
                },
                {
                    "x": 2106,
                    "y": 2639
                },
                {
                    "x": 441,
                    "y": 2639
                }
            ],
            "category": "paragraph",
            "html": "<p id='165' style='font-size:18px'>We train 1-LSTM networks with tied weights and we compare them with fully-connected feed-<br>forward networks with ReLU or tanh activation functions and with either tied or untied weights.<br>We search the space of hyper-parameters as follows. The 1-LSTM networks are trained with either<br>500 or 1500 hidden units and having from 1 to 150 hidden layers. The 1-LSTM networks are trained<br>on input strings that have from k = 20 to k = 250 bits in increments of 10. The feed-forward ReLU<br>and tanh networks are trained with 500, 1500 or 3000 units and also having from 1 to 150 hidden<br>layers. The latter networks are trained on input bit strings that have between k = 20 and k = 60<br>bits in increments of 5. Each network is trained with a maximum of 10 million samples or four days<br>of computation on a Tesla K40m GPU. For the optimization we use mini-batches of size 20 and the<br>AdaGrad rule with a learning rate of 0.06 (Duchi et al., 2010). A network is considered to have<br>found the solution if the network correctly computes the parity of 100 randomly sampled unseen<br>k-bit strings. Due to the nature of the problem, during training the predicted accuracy is never better<br>than random guessing and when the network finds a solution the accuracy suddenly spikes to 100%.</p>",
            "id": 165,
            "page": 13,
            "text": "We train 1-LSTM networks with tied weights and we compare them with fully-connected feedforward networks with ReLU or tanh activation functions and with either tied or untied weights. We search the space of hyper-parameters as follows. The 1-LSTM networks are trained with either 500 or 1500 hidden units and having from 1 to 150 hidden layers. The 1-LSTM networks are trained on input strings that have from k = 20 to k = 250 bits in increments of 10. The feed-forward ReLU and tanh networks are trained with 500, 1500 or 3000 units and also having from 1 to 150 hidden layers. The latter networks are trained on input bit strings that have between k = 20 and k = 60 bits in increments of 5. Each network is trained with a maximum of 10 million samples or four days of computation on a Tesla K40m GPU. For the optimization we use mini-batches of size 20 and the AdaGrad rule with a learning rate of 0.06 (Duchi , 2010). A network is considered to have found the solution if the network correctly computes the parity of 100 randomly sampled unseen k-bit strings. Due to the nature of the problem, during training the predicted accuracy is never better than random guessing and when the network finds a solution the accuracy suddenly spikes to 100%."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2661
                },
                {
                    "x": 2106,
                    "y": 2661
                },
                {
                    "x": 2106,
                    "y": 2889
                },
                {
                    "x": 441,
                    "y": 2889
                }
            ],
            "category": "paragraph",
            "html": "<p id='166' style='font-size:18px'>Figure 9 depicts the results of the experiments with 1-LSTM networks and Figure 10 relates the best<br>performing networks of each type. For the feed-forward ReLU and tanh networks with either tied<br>or untied weights, we find that these networks fail to find solutions for k = 35 bits and beyond.<br>Some networks in the search space find solutions for k = 30 input bits. By contrast, as represented<br>in Fig. 9, tied 1-LSTM networks find solutions for up to k = 250 bits.</p>",
            "id": 166,
            "page": 13,
            "text": "Figure 9 depicts the results of the experiments with 1-LSTM networks and Figure 10 relates the best performing networks of each type. For the feed-forward ReLU and tanh networks with either tied or untied weights, we find that these networks fail to find solutions for k = 35 bits and beyond. Some networks in the search space find solutions for k = 30 input bits. By contrast, as represented in Fig. 9, tied 1-LSTM networks find solutions for up to k = 250 bits."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2915
                },
                {
                    "x": 2107,
                    "y": 2915
                },
                {
                    "x": 2107,
                    "y": 3055
                },
                {
                    "x": 441,
                    "y": 3055
                }
            ],
            "category": "paragraph",
            "html": "<p id='167' style='font-size:20px'>There appears to be a correlation between the length k of the input bit strings and the minimum<br>depth of the 1-LSTM networks. The minimum depth of the networks increases with k suggesting<br>that longer bit strings need more operations to be applied to them; however, the rate of growth is sub-</p>",
            "id": 167,
            "page": 13,
            "text": "There appears to be a correlation between the length k of the input bit strings and the minimum depth of the 1-LSTM networks. The minimum depth of the networks increases with k suggesting that longer bit strings need more operations to be applied to them; however, the rate of growth is sub-"
        },
        {
            "bounding_box": [
                {
                    "x": 1253,
                    "y": 3133
                },
                {
                    "x": 1299,
                    "y": 3133
                },
                {
                    "x": 1299,
                    "y": 3171
                },
                {
                    "x": 1253,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='168' style='font-size:18px'>13</footer>",
            "id": 168,
            "page": 13,
            "text": "13"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 110
                },
                {
                    "x": 1287,
                    "y": 110
                },
                {
                    "x": 1287,
                    "y": 158
                },
                {
                    "x": 445,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='169' style='font-size:14px'>Under review as a conference paper at ICLR 2016</header>",
            "id": 169,
            "page": 14,
            "text": "Under review as a conference paper at ICLR 2016"
        },
        {
            "bounding_box": [
                {
                    "x": 680,
                    "y": 329
                },
                {
                    "x": 1868,
                    "y": 329
                },
                {
                    "x": 1868,
                    "y": 953
                },
                {
                    "x": 680,
                    "y": 953
                }
            ],
            "category": "figure",
            "html": "<figure><img id='170' alt=\"\" data-coord=\"top-left:(680,329); bottom-right:(1868,953)\" /></figure>",
            "id": 170,
            "page": 14,
            "text": ""
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 996
                },
                {
                    "x": 2110,
                    "y": 996
                },
                {
                    "x": 2110,
                    "y": 1282
                },
                {
                    "x": 440,
                    "y": 1282
                }
            ],
            "category": "caption",
            "html": "<caption id='171' style='font-size:20px'>Figure 11: A 3-LSTM network applied to non-overlapping patches of an image. Each patch is<br>projected to form the input hidden and cell vectors of the depth dimension of the 3-LSTM blocks.<br>The arrows across the spatial dimensions indicate the flow of the computation for that layer. No<br>subsampling or pooling occurs in the networks as the topmost layer simply concatenates all the<br>output hidden and memory vectors of the depth dimension, passes them through a layer of ReLUs<br>and the final softmax layer.</caption>",
            "id": 171,
            "page": 14,
            "text": "Figure 11: A 3-LSTM network applied to non-overlapping patches of an image. Each patch is projected to form the input hidden and cell vectors of the depth dimension of the 3-LSTM blocks. The arrows across the spatial dimensions indicate the flow of the computation for that layer. No subsampling or pooling occurs in the networks as the topmost layer simply concatenates all the output hidden and memory vectors of the depth dimension, passes them through a layer of ReLUs and the final softmax layer."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1375
                },
                {
                    "x": 2107,
                    "y": 1375
                },
                {
                    "x": 2107,
                    "y": 1698
                },
                {
                    "x": 441,
                    "y": 1698
                }
            ],
            "category": "paragraph",
            "html": "<p id='172' style='font-size:16px'>linear suggesting that more than a single bit of the input is considered at every step. We visualized<br>the activations of the memory vectors obtained via a feed-forward pass through one of the 1-LSTM<br>networks using selected input bit strings (Fig. 10). This revealed the prominent presence of counting<br>neurons that keep a counter for the number of layers processed SO far. These two aspects seem to<br>suggest that the networks are using the cells to process the bit string sequentially by attending to<br>parts of it at each step in the computation, a seemingly crucial feature that is not available in ReLU<br>or tanh transfer functions.</p>",
            "id": 172,
            "page": 14,
            "text": "linear suggesting that more than a single bit of the input is considered at every step. We visualized the activations of the memory vectors obtained via a feed-forward pass through one of the 1-LSTM networks using selected input bit strings (Fig. 10). This revealed the prominent presence of counting neurons that keep a counter for the number of layers processed SO far. These two aspects seem to suggest that the networks are using the cells to process the bit string sequentially by attending to parts of it at each step in the computation, a seemingly crucial feature that is not available in ReLU or tanh transfer functions."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1766
                },
                {
                    "x": 1076,
                    "y": 1766
                },
                {
                    "x": 1076,
                    "y": 1816
                },
                {
                    "x": 443,
                    "y": 1816
                }
            ],
            "category": "paragraph",
            "html": "<p id='173' style='font-size:16px'>5.2 MNIST DIGIT RECOGNITION</p>",
            "id": 173,
            "page": 14,
            "text": "5.2 MNIST DIGIT RECOGNITION"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1856
                },
                {
                    "x": 2107,
                    "y": 1856
                },
                {
                    "x": 2107,
                    "y": 2364
                },
                {
                    "x": 442,
                    "y": 2364
                }
            ],
            "category": "paragraph",
            "html": "<p id='174' style='font-size:16px'>In our last experiment we apply a 3-LSTM network to images. We consider non-overlapping patches<br>of pixels in an image as forming a two-dimensional grid of inputs. The 3-LSTM performs compu-<br>tations with LSTM cells along three different dimensions. Two of the dimensions correspond to the<br>two spatial dimensions of the grid, whereas the remaining dimension is the depth of the network.<br>Like in a convolutional neural network (LeCun et al., 1998), the same three-way transform of the<br>3-LSTM is applied at all parts of the grid, ensuring that the same features can be extracted across<br>all parts of the input image. Due to the unbounded context size of the 3-LSTM, the computations of<br>features at one end of the image can be influenced by the features computed at the other end of the<br>image within the same layer. Due to the cells along the depth direction, features from the present<br>patch can be passed onto the next layer either unprocessed or as processed by the layer itself as a<br>function of neighboring patches.</p>",
            "id": 174,
            "page": 14,
            "text": "In our last experiment we apply a 3-LSTM network to images. We consider non-overlapping patches of pixels in an image as forming a two-dimensional grid of inputs. The 3-LSTM performs computations with LSTM cells along three different dimensions. Two of the dimensions correspond to the two spatial dimensions of the grid, whereas the remaining dimension is the depth of the network. Like in a convolutional neural network (LeCun , 1998), the same three-way transform of the 3-LSTM is applied at all parts of the grid, ensuring that the same features can be extracted across all parts of the input image. Due to the unbounded context size of the 3-LSTM, the computations of features at one end of the image can be influenced by the features computed at the other end of the image within the same layer. Due to the cells along the depth direction, features from the present patch can be passed onto the next layer either unprocessed or as processed by the layer itself as a function of neighboring patches."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2386
                },
                {
                    "x": 2106,
                    "y": 2386
                },
                {
                    "x": 2106,
                    "y": 2846
                },
                {
                    "x": 441,
                    "y": 2846
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='175' style='font-size:16px'>We construct the network as depicted in Fig. 11. We divide the 28 x 28 MNIST image into pxp pixel<br>patches, where p is a small number such as 2 or 4. The patches are then linearized and projected<br>into two vectors of the size of the hidden layer of the 3-LSTM; the projected vectors are the input<br>hidden and memory vectors at the first layer in the depth direction of the 3-LSTM. At each layer the<br>computation of the 3-LSTM starts from one corner of the image, follows the two spatial dimensions<br>and ends in the opposite corner of the image. The network has a few layers of depth, each layer<br>starting the computation at one of the corners of the image. In the current form there is no pooling<br>between successive layers of the 3-LSTM. The topmost layer concatenates all the output hidden and<br>memory vectors at all parts of the grid. These are then passed through a layer of ReLUs and a final<br>softmax layer.</p>",
            "id": 175,
            "page": 14,
            "text": "We construct the network as depicted in Fig. 11. We divide the 28 x 28 MNIST image into pxp pixel patches, where p is a small number such as 2 or 4. The patches are then linearized and projected into two vectors of the size of the hidden layer of the 3-LSTM; the projected vectors are the input hidden and memory vectors at the first layer in the depth direction of the 3-LSTM. At each layer the computation of the 3-LSTM starts from one corner of the image, follows the two spatial dimensions and ends in the opposite corner of the image. The network has a few layers of depth, each layer starting the computation at one of the corners of the image. In the current form there is no pooling between successive layers of the 3-LSTM. The topmost layer concatenates all the output hidden and memory vectors at all parts of the grid. These are then passed through a layer of ReLUs and a final softmax layer."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2867
                },
                {
                    "x": 2109,
                    "y": 2867
                },
                {
                    "x": 2109,
                    "y": 3056
                },
                {
                    "x": 441,
                    "y": 3056
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='176' style='font-size:20px'>The setup has some similarity with the original application of Multidimensional LSTM to images<br>(Graves, 2012) and with the recently described ReNet architecture (Visin et al., 2015). The differ-<br>ence with the former is that we apply multiple layers of depth to the image, use three-dimensional<br>blocks and concatenate the top output vectors before classification. The difference with the ReNet</p>",
            "id": 176,
            "page": 14,
            "text": "The setup has some similarity with the original application of Multidimensional LSTM to images (Graves, 2012) and with the recently described ReNet architecture (Visin , 2015). The difference with the former is that we apply multiple layers of depth to the image, use three-dimensional blocks and concatenate the top output vectors before classification. The difference with the ReNet"
        },
        {
            "bounding_box": [
                {
                    "x": 1252,
                    "y": 3133
                },
                {
                    "x": 1300,
                    "y": 3133
                },
                {
                    "x": 1300,
                    "y": 3171
                },
                {
                    "x": 1252,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='177' style='font-size:16px'>14</footer>",
            "id": 177,
            "page": 14,
            "text": "14"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 110
                },
                {
                    "x": 1287,
                    "y": 110
                },
                {
                    "x": 1287,
                    "y": 158
                },
                {
                    "x": 444,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='178' style='font-size:14px'>Under review as a conference paper at ICLR 2016</header>",
            "id": 178,
            "page": 15,
            "text": "Under review as a conference paper at ICLR 2016"
        },
        {
            "bounding_box": [
                {
                    "x": 734,
                    "y": 333
                },
                {
                    "x": 1815,
                    "y": 333
                },
                {
                    "x": 1815,
                    "y": 949
                },
                {
                    "x": 734,
                    "y": 949
                }
            ],
            "category": "table",
            "html": "<table id='179' style='font-size:20px'><tr><td></td><td>Test Error (%)</td></tr><tr><td>Wan et al. (Wan et al., 2013)</td><td>0.28</td></tr><tr><td>Graham (Graham, 2014a)</td><td>0.31</td></tr><tr><td>Untied 3-LSTM</td><td>0.32</td></tr><tr><td>Ciresan et al. (Ciresan et al., 2012)</td><td>0.35</td></tr><tr><td>Untied 3-LSTM with ReLU (*)</td><td>0.36</td></tr><tr><td>Mairar et al. (Mairal et al., 2014)</td><td>0.39</td></tr><tr><td>Lee et al. (Lee et al., 2015)</td><td>0.39</td></tr><tr><td>Simard et al. (Simard et al., 2003)</td><td>0.4</td></tr><tr><td>Graham (Graham, 2014b)</td><td>0.44</td></tr><tr><td>Goodfellow et al. (Goodfellow et al., 2013)</td><td>0.45</td></tr><tr><td>Visin et al. (Visin et al., 2015)</td><td>0.45</td></tr><tr><td>Lin et al. (Lin et al., 2013)</td><td>0.47</td></tr></table>",
            "id": 179,
            "page": 15,
            "text": "Test Error (%)  Wan  (Wan , 2013) 0.28  Graham (Graham, 2014a) 0.31  Untied 3-LSTM 0.32  Ciresan  (Ciresan , 2012) 0.35  Untied 3-LSTM with ReLU (*) 0.36  Mairar  (Mairal , 2014) 0.39  Lee  (Lee , 2015) 0.39  Simard  (Simard , 2003) 0.4  Graham (Graham, 2014b) 0.44  Goodfellow  (Goodfellow , 2013) 0.45  Visin  (Visin , 2015) 0.45  Lin  (Lin , 2013)"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 1002
                },
                {
                    "x": 2109,
                    "y": 1002
                },
                {
                    "x": 2109,
                    "y": 1147
                },
                {
                    "x": 440,
                    "y": 1147
                }
            ],
            "category": "caption",
            "html": "<caption id='180' style='font-size:18px'>Figure 12: Test error on the MNIST dataset. All approaches are convolutional networks except for<br>Visin et al. that uses a stack of single-direction recurrent neural networks. (*) This Grid LSTM has<br>non-LSTM connections along the depth only and uses the ReLU instead.</caption>",
            "id": 180,
            "page": 15,
            "text": "Figure 12: Test error on the MNIST dataset. All approaches are convolutional networks except for Visin  that uses a stack of single-direction recurrent neural networks. (*) This Grid LSTM has non-LSTM connections along the depth only and uses the ReLU instead."
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 1231
                },
                {
                    "x": 2106,
                    "y": 1231
                },
                {
                    "x": 2106,
                    "y": 1368
                },
                {
                    "x": 440,
                    "y": 1368
                }
            ],
            "category": "paragraph",
            "html": "<p id='181' style='font-size:20px'>architecture is that the 3-LSTM processes the image according to the two inherent spatial dimen-<br>sions; instead of stacking hidden layers as in the ReNet, the block also modulates directly what<br>information is passed along the depth dimension.</p>",
            "id": 181,
            "page": 15,
            "text": "architecture is that the 3-LSTM processes the image according to the two inherent spatial dimensions; instead of stacking hidden layers as in the ReNet, the block also modulates directly what information is passed along the depth dimension."
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 1390
                },
                {
                    "x": 2108,
                    "y": 1390
                },
                {
                    "x": 2108,
                    "y": 1943
                },
                {
                    "x": 440,
                    "y": 1943
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='182' style='font-size:20px'>The training details are as follows. The MNIST dataset consists of 50000 training images, 10000<br>validation images and 10000 test images. The pixel values are normalized by dividing them by 255.<br>Data augmentation is performed by shifting training images from 0 to 4 pixels in the horizontal and<br>vertical directions and padding with zero values. The shift in the two directions is chosen uniformly<br>at random. Validation samples are used for retraining the best model settings found during the grid<br>search. We train the 3-LSTM both with and without cells in the depth dimension. The 3-LSTM with<br>the cells uses patches of 2 x 2 pixels, has four LSTM layers with 100 hidden units and one ReLU<br>layer with 4096 units. The 3-LSTM without the cells in the depth dimension has input patches of<br>size 3 x 3 obtained by cropping the image to a size of 27 x 27, it also has four LSTM layers of 100<br>units and has a ReLU layer of 2048 units. For the latter model we use ReLU as transfer function for<br>the depth direction as in Eq. 6. We use mini-batches of size 128 and train the models using Adam<br>and a learning rate of 0.001.</p>",
            "id": 182,
            "page": 15,
            "text": "The training details are as follows. The MNIST dataset consists of 50000 training images, 10000 validation images and 10000 test images. The pixel values are normalized by dividing them by 255. Data augmentation is performed by shifting training images from 0 to 4 pixels in the horizontal and vertical directions and padding with zero values. The shift in the two directions is chosen uniformly at random. Validation samples are used for retraining the best model settings found during the grid search. We train the 3-LSTM both with and without cells in the depth dimension. The 3-LSTM with the cells uses patches of 2 x 2 pixels, has four LSTM layers with 100 hidden units and one ReLU layer with 4096 units. The 3-LSTM without the cells in the depth dimension has input patches of size 3 x 3 obtained by cropping the image to a size of 27 x 27, it also has four LSTM layers of 100 units and has a ReLU layer of 2048 units. For the latter model we use ReLU as transfer function for the depth direction as in Eq. 6. We use mini-batches of size 128 and train the models using Adam and a learning rate of 0.001."
        },
        {
            "bounding_box": [
                {
                    "x": 439,
                    "y": 1963
                },
                {
                    "x": 2109,
                    "y": 1963
                },
                {
                    "x": 2109,
                    "y": 2196
                },
                {
                    "x": 439,
                    "y": 2196
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='183' style='font-size:20px'>Figure 12 reports test set errors of our models and that of competing approaches. We can see that<br>even in the absence of pooling the 3-LSTM with the cells performs near the state-of-the-art. The<br>3-LSTM without the cells also performs quite well; the cells in the depth direction likely help with<br>the feature extraction at the higher layers. The other approaches, with the exception of ReNet, are<br>convolutional neural networks.</p>",
            "id": 183,
            "page": 15,
            "text": "Figure 12 reports test set errors of our models and that of competing approaches. We can see that even in the absence of pooling the 3-LSTM with the cells performs near the state-of-the-art. The 3-LSTM without the cells also performs quite well; the cells in the depth direction likely help with the feature extraction at the higher layers. The other approaches, with the exception of ReNet, are convolutional neural networks."
        },
        {
            "bounding_box": [
                {
                    "x": 1252,
                    "y": 3132
                },
                {
                    "x": 1300,
                    "y": 3132
                },
                {
                    "x": 1300,
                    "y": 3172
                },
                {
                    "x": 1252,
                    "y": 3172
                }
            ],
            "category": "footer",
            "html": "<footer id='184' style='font-size:16px'>15</footer>",
            "id": 184,
            "page": 15,
            "text": "15"
        }
    ]
}