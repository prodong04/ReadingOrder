{
    "id": "62af7482-0f92-11ef-8230-426932df3dcf",
    "pdf_path": "/root/data/pdf/1911.03584v2.pdf",
    "elements": [
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 109
                },
                {
                    "x": 1226,
                    "y": 109
                },
                {
                    "x": 1226,
                    "y": 158
                },
                {
                    "x": 444,
                    "y": 158
                }
            ],
            "category": "header",
            "html": "<header id='0' style='font-size:16px'>Published as a conference paper at ICLR 2020</header>",
            "id": 0,
            "page": 1,
            "text": "Published as a conference paper at ICLR 2020"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 329
                },
                {
                    "x": 2107,
                    "y": 329
                },
                {
                    "x": 2107,
                    "y": 485
                },
                {
                    "x": 441,
                    "y": 485
                }
            ],
            "category": "paragraph",
            "html": "<p id='1' style='font-size:22px'>ON THE RELATIONSHIP BETWEEN SELF-ATTENTION<br>AND CONVOLUTIONAL LAYERS</p>",
            "id": 1,
            "page": 1,
            "text": "ON THE RELATIONSHIP BETWEEN SELF-ATTENTION\nAND CONVOLUTIONAL LAYERS"
        },
        {
            "bounding_box": [
                {
                    "x": 467,
                    "y": 564
                },
                {
                    "x": 1532,
                    "y": 564
                },
                {
                    "x": 1532,
                    "y": 613
                },
                {
                    "x": 467,
                    "y": 613
                }
            ],
            "category": "paragraph",
            "html": "<p id='2' style='font-size:20px'>Jean-Baptiste Cordonnier, Andreas Loukas & Martin Jaggi</p>",
            "id": 2,
            "page": 1,
            "text": "Jean-Baptiste Cordonnier, Andreas Loukas & Martin Jaggi"
        },
        {
            "bounding_box": [
                {
                    "x": 473,
                    "y": 616
                },
                {
                    "x": 1330,
                    "y": 616
                },
                {
                    "x": 1330,
                    "y": 710
                },
                {
                    "x": 473,
                    "y": 710
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='3' style='font-size:18px'>Ecole Polytechnique Federale de Lausanne (EPFL)<br>{first · last}@epfl · ch</p>",
            "id": 3,
            "page": 1,
            "text": "Ecole Polytechnique Federale de Lausanne (EPFL)\n{first · last}@epfl · ch"
        },
        {
            "bounding_box": [
                {
                    "x": 1154,
                    "y": 831
                },
                {
                    "x": 1397,
                    "y": 831
                },
                {
                    "x": 1397,
                    "y": 882
                },
                {
                    "x": 1154,
                    "y": 882
                }
            ],
            "category": "paragraph",
            "html": "<p id='4' style='font-size:22px'>ABSTRACT</p>",
            "id": 4,
            "page": 1,
            "text": "ABSTRACT"
        },
        {
            "bounding_box": [
                {
                    "x": 591,
                    "y": 924
                },
                {
                    "x": 1963,
                    "y": 924
                },
                {
                    "x": 1963,
                    "y": 1479
                },
                {
                    "x": 591,
                    "y": 1479
                }
            ],
            "category": "paragraph",
            "html": "<p id='5' style='font-size:18px'>Recent trends of incorporating attention mechanisms in vision have led re-<br>searchers to reconsider the supremacy of convolutional layers as a primary build-<br>ing block. Beyond helping CNNs to handle long-range dependencies, Ramachan-<br>dran et al. (2019) showed that attention can completely replace convolution and<br>achieve state-of-the-art performance on vision tasks. This raises the question: do<br>learned attention layers operate similarly to convolutional layers? This work pro-<br>vides evidence that attention layers can perform convolution and, indeed, they of-<br>ten learn to do so in practice. Specifically, we prove that a multi-head self-attention<br>layer with sufficient number of heads is at least as expressive as any convolutional<br>layer. Our numerical experiments then show that self-attention layers attend to<br>pixel-grid patterns similarly to CNN layers, corroborating our analysis. Our code<br>is publicly available1</p>",
            "id": 5,
            "page": 1,
            "text": "Recent trends of incorporating attention mechanisms in vision have led re-\nsearchers to reconsider the supremacy of convolutional layers as a primary build-\ning block. Beyond helping CNNs to handle long-range dependencies, Ramachan-\ndran et al. (2019) showed that attention can completely replace convolution and\nachieve state-of-the-art performance on vision tasks. This raises the question: do\nlearned attention layers operate similarly to convolutional layers? This work pro-\nvides evidence that attention layers can perform convolution and, indeed, they of-\nten learn to do so in practice. Specifically, we prove that a multi-head self-attention\nlayer with sufficient number of heads is at least as expressive as any convolutional\nlayer. Our numerical experiments then show that self-attention layers attend to\npixel-grid patterns similarly to CNN layers, corroborating our analysis. Our code\nis publicly available1"
        },
        {
            "bounding_box": [
                {
                    "x": 449,
                    "y": 1551
                },
                {
                    "x": 863,
                    "y": 1551
                },
                {
                    "x": 863,
                    "y": 1605
                },
                {
                    "x": 449,
                    "y": 1605
                }
            ],
            "category": "paragraph",
            "html": "<p id='6' style='font-size:20px'>1 INTRODUCTION</p>",
            "id": 6,
            "page": 1,
            "text": "1 INTRODUCTION"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1646
                },
                {
                    "x": 2110,
                    "y": 1646
                },
                {
                    "x": 2110,
                    "y": 2202
                },
                {
                    "x": 441,
                    "y": 2202
                }
            ],
            "category": "paragraph",
            "html": "<p id='7' style='font-size:18px'>Recent advances in Natural Language Processing (NLP) are largely attributed to the rise of the trans-<br>former (Vaswani et al., 2017). Pre-trained to solve an unsupervised task on large corpora of text,<br>transformer-based architectures, such as GPT-2 (Radford et al., 2018), BERT (Devlin et al., 2018)<br>and Transformer-XL (Dai et al., 2019), seem to possess the capacity to learn the underlying structure<br>of text and, as a consequence, to learn representations that generalize across tasks. The key differ-<br>ence between transformers and previous methods, such as recurrent neural networks (Hochreiter &<br>Schmidhuber, 1997) and convolutional neural networks (CNN), is that the former can simultane-<br>ously attend to every word of their input sequence. This is made possible thanks to the attention<br>mechanism-originally introduced in Neural Machine Translation to better handle long-range de-<br>pendencies (Bahdanau et al., 2015). With self-attention in particular, the similarity of two words in<br>a sequence is captured by an attention score measuring the distance of their representations. The<br>representation of each word is then updated based on those words whose attention score is highest.</p>",
            "id": 7,
            "page": 1,
            "text": "Recent advances in Natural Language Processing (NLP) are largely attributed to the rise of the trans-\nformer (Vaswani et al., 2017). Pre-trained to solve an unsupervised task on large corpora of text,\ntransformer-based architectures, such as GPT-2 (Radford et al., 2018), BERT (Devlin et al., 2018)\nand Transformer-XL (Dai et al., 2019), seem to possess the capacity to learn the underlying structure\nof text and, as a consequence, to learn representations that generalize across tasks. The key differ-\nence between transformers and previous methods, such as recurrent neural networks (Hochreiter &\nSchmidhuber, 1997) and convolutional neural networks (CNN), is that the former can simultane-\nously attend to every word of their input sequence. This is made possible thanks to the attention\nmechanism-originally introduced in Neural Machine Translation to better handle long-range de-\npendencies (Bahdanau et al., 2015). With self-attention in particular, the similarity of two words in\na sequence is captured by an attention score measuring the distance of their representations. The\nrepresentation of each word is then updated based on those words whose attention score is highest."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2223
                },
                {
                    "x": 2110,
                    "y": 2223
                },
                {
                    "x": 2110,
                    "y": 2637
                },
                {
                    "x": 441,
                    "y": 2637
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='8' style='font-size:18px'>Inspired by its capacity to learn meaningful inter-dependencies between words, researchers have<br>recently considered utilizing self-attention in vision tasks. Self-attention was first added to CNN<br>by either using channel-based attention (Hu et al., 2018) or non-local relationships across the image<br>(Wang et al., 2018). More recently, Bello et al. (2019) augmented CNNs by replacing some convolu-<br>tional layers with self-attention layers, leading to improvements on image classification and object<br>detection tasks. Interestingly, Ramachandran et al. (2019) noticed that, even though state-of-the<br>art results are reached when attention and convolutional features are combined, under same com-<br>putation and model size constraints, self-attention-only architectures also reach competitive image<br>classification accuracy.</p>",
            "id": 8,
            "page": 1,
            "text": "Inspired by its capacity to learn meaningful inter-dependencies between words, researchers have\nrecently considered utilizing self-attention in vision tasks. Self-attention was first added to CNN\nby either using channel-based attention (Hu et al., 2018) or non-local relationships across the image\n(Wang et al., 2018). More recently, Bello et al. (2019) augmented CNNs by replacing some convolu-\ntional layers with self-attention layers, leading to improvements on image classification and object\ndetection tasks. Interestingly, Ramachandran et al. (2019) noticed that, even though state-of-the\nart results are reached when attention and convolutional features are combined, under same com-\nputation and model size constraints, self-attention-only architectures also reach competitive image\nclassification accuracy."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2657
                },
                {
                    "x": 2110,
                    "y": 2657
                },
                {
                    "x": 2110,
                    "y": 2983
                },
                {
                    "x": 442,
                    "y": 2983
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='9' style='font-size:18px'>These findings raise the question, do self-attention layers process images in a similar manner to<br>convolutional layers? From a theoretical perspective, one could argue that transfomers have the<br>capacity to simulate any function -including a CNN. Indeed, Perez et al. (2019) showed that a multi-<br>layer attention-based architecture with additive positional encodings is Turing complete under some<br>strong theoretical assumptions, such as unbounded precision arithmetic. Unfortunately, universality<br>results do not reveal how a machine solves a task, only that it has the capacity to do so. Thus, the<br>question of how self-attention layers actually process images remains open.</p>",
            "id": 9,
            "page": 1,
            "text": "These findings raise the question, do self-attention layers process images in a similar manner to\nconvolutional layers? From a theoretical perspective, one could argue that transfomers have the\ncapacity to simulate any function -including a CNN. Indeed, Perez et al. (2019) showed that a multi-\nlayer attention-based architecture with additive positional encodings is Turing complete under some\nstrong theoretical assumptions, such as unbounded precision arithmetic. Unfortunately, universality\nresults do not reveal how a machine solves a task, only that it has the capacity to do so. Thus, the\nquestion of how self-attention layers actually process images remains open."
        },
        {
            "bounding_box": [
                {
                    "x": 507,
                    "y": 3004
                },
                {
                    "x": 2125,
                    "y": 3004
                },
                {
                    "x": 2125,
                    "y": 3056
                },
                {
                    "x": 507,
                    "y": 3056
                }
            ],
            "category": "paragraph",
            "html": "<p id='10' style='font-size:16px'>1Code: github · com/ epfml / attention-cnn. Website: epfml · github · io/ attention-cnn.</p>",
            "id": 10,
            "page": 1,
            "text": "1Code: github · com/ epfml / attention-cnn. Website: epfml · github · io/ attention-cnn."
        },
        {
            "bounding_box": [
                {
                    "x": 60,
                    "y": 889
                },
                {
                    "x": 147,
                    "y": 889
                },
                {
                    "x": 147,
                    "y": 2327
                },
                {
                    "x": 60,
                    "y": 2327
                }
            ],
            "category": "footer",
            "html": "<br><footer id='11' style='font-size:14px'>2020<br>Jan<br>10<br>[cs.LG]<br>arXiv:1911.03584v2</footer>",
            "id": 11,
            "page": 1,
            "text": "2020\nJan\n10\n[cs.LG]\narXiv:1911.03584v2"
        },
        {
            "bounding_box": [
                {
                    "x": 1263,
                    "y": 3134
                },
                {
                    "x": 1287,
                    "y": 3134
                },
                {
                    "x": 1287,
                    "y": 3170
                },
                {
                    "x": 1263,
                    "y": 3170
                }
            ],
            "category": "footer",
            "html": "<footer id='12' style='font-size:14px'>1</footer>",
            "id": 12,
            "page": 1,
            "text": "1"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 112
                },
                {
                    "x": 1225,
                    "y": 112
                },
                {
                    "x": 1225,
                    "y": 156
                },
                {
                    "x": 445,
                    "y": 156
                }
            ],
            "category": "header",
            "html": "<header id='13' style='font-size:14px'>Published as a conference paper at ICLR 2020</header>",
            "id": 13,
            "page": 2,
            "text": "Published as a conference paper at ICLR 2020"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 348
                },
                {
                    "x": 2105,
                    "y": 348
                },
                {
                    "x": 2105,
                    "y": 438
                },
                {
                    "x": 443,
                    "y": 438
                }
            ],
            "category": "paragraph",
            "html": "<p id='14' style='font-size:16px'>Contributions. In this work, we put forth theoretical and empirical evidence that self-attention<br>layers can (and do) learn to behave similar to convolutional layers:</p>",
            "id": 14,
            "page": 2,
            "text": "Contributions. In this work, we put forth theoretical and empirical evidence that self-attention\nlayers can (and do) learn to behave similar to convolutional layers:"
        },
        {
            "bounding_box": [
                {
                    "x": 549,
                    "y": 453
                },
                {
                    "x": 2104,
                    "y": 453
                },
                {
                    "x": 2104,
                    "y": 543
                },
                {
                    "x": 549,
                    "y": 543
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='15' style='font-size:16px'>I. From a theoretical perspective, we provide a constructive proof showing that self-attention<br>layers can express any convolutional layers.</p>",
            "id": 15,
            "page": 2,
            "text": "I. From a theoretical perspective, we provide a constructive proof showing that self-attention\nlayers can express any convolutional layers."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 559
                },
                {
                    "x": 2105,
                    "y": 559
                },
                {
                    "x": 2105,
                    "y": 651
                },
                {
                    "x": 444,
                    "y": 651
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='16' style='font-size:18px'>Specifically, we show that a single multi-head self-attention layer using relative positional encoding<br>can be re-parametrized to express any convolutional layer.</p>",
            "id": 16,
            "page": 2,
            "text": "Specifically, we show that a single multi-head self-attention layer using relative positional encoding\ncan be re-parametrized to express any convolutional layer."
        },
        {
            "bounding_box": [
                {
                    "x": 535,
                    "y": 664
                },
                {
                    "x": 2105,
                    "y": 664
                },
                {
                    "x": 2105,
                    "y": 803
                },
                {
                    "x": 535,
                    "y": 803
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='17' style='font-size:16px'>II. Our experiments show that the first few layers of attention-only architectures (Ramachan-<br>dran et al., 2019) do learn to attend on grid-like pattern around each query pixel, similar to<br>our theoretical construction.</p>",
            "id": 17,
            "page": 2,
            "text": "II. Our experiments show that the first few layers of attention-only architectures (Ramachan-\ndran et al., 2019) do learn to attend on grid-like pattern around each query pixel, similar to\nour theoretical construction."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 838
                },
                {
                    "x": 2107,
                    "y": 838
                },
                {
                    "x": 2107,
                    "y": 1071
                },
                {
                    "x": 441,
                    "y": 1071
                }
            ],
            "category": "paragraph",
            "html": "<p id='18' style='font-size:16px'>Strikingly, this behavior is confirmed both for our quadratic encoding, but also for relative encoding<br>that is learned. Our results seem to suggest that localized convolution is the right inductive bias<br>for the first few layers of an image classifying network. We provide an interactive website2 to<br>explore how self-attention exploits localized position-based attention in lower layers and content-<br>based attention in deeper layers. For reproducibility purposes, our code is publicly available.</p>",
            "id": 18,
            "page": 2,
            "text": "Strikingly, this behavior is confirmed both for our quadratic encoding, but also for relative encoding\nthat is learned. Our results seem to suggest that localized convolution is the right inductive bias\nfor the first few layers of an image classifying network. We provide an interactive website2 to\nexplore how self-attention exploits localized position-based attention in lower layers and content-\nbased attention in deeper layers. For reproducibility purposes, our code is publicly available."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 1135
                },
                {
                    "x": 1753,
                    "y": 1135
                },
                {
                    "x": 1753,
                    "y": 1187
                },
                {
                    "x": 444,
                    "y": 1187
                }
            ],
            "category": "paragraph",
            "html": "<p id='19' style='font-size:18px'>2 BACKGROUND ON ATTENTION MECHANISMS FOR VISION</p>",
            "id": 19,
            "page": 2,
            "text": "2 BACKGROUND ON ATTENTION MECHANISMS FOR VISION"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1239
                },
                {
                    "x": 2103,
                    "y": 1239
                },
                {
                    "x": 2103,
                    "y": 1330
                },
                {
                    "x": 443,
                    "y": 1330
                }
            ],
            "category": "paragraph",
            "html": "<p id='20' style='font-size:18px'>We here recall the mathematical formulation of self-attention layers and emphasize the role of posi-<br>tional encodings.</p>",
            "id": 20,
            "page": 2,
            "text": "We here recall the mathematical formulation of self-attention layers and emphasize the role of posi-\ntional encodings."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 1385
                },
                {
                    "x": 1339,
                    "y": 1385
                },
                {
                    "x": 1339,
                    "y": 1432
                },
                {
                    "x": 444,
                    "y": 1432
                }
            ],
            "category": "paragraph",
            "html": "<p id='21' style='font-size:16px'>2.1 THE MULTI-HEAD SELF-ATTENTION LAYER</p>",
            "id": 21,
            "page": 2,
            "text": "2.1 THE MULTI-HEAD SELF-ATTENTION LAYER"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1466
                },
                {
                    "x": 2106,
                    "y": 1466
                },
                {
                    "x": 2106,
                    "y": 1656
                },
                {
                    "x": 442,
                    "y": 1656
                }
            ],
            "category": "paragraph",
            "html": "<p id='22' style='font-size:16px'>Let X E RTxDin be an input matrix consisting of T tokens in of Din dimensions each. While in<br>NLP each token corresponds to a word in a sentence, the same formalism can be applied to any<br>sequence of T discrete objects, e.g. pixels. A self-attention layer maps any query token t E [T]<br>from Din to Dout dimensions as follows:</p>",
            "id": 22,
            "page": 2,
            "text": "Let X E RTxDin be an input matrix consisting of T tokens in of Din dimensions each. While in\nNLP each token corresponds to a word in a sentence, the same formalism can be applied to any\nsequence of T discrete objects, e.g. pixels. A self-attention layer maps any query token t E [T]\nfrom Din to Dout dimensions as follows:"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 1739
                },
                {
                    "x": 1309,
                    "y": 1739
                },
                {
                    "x": 1309,
                    "y": 1786
                },
                {
                    "x": 444,
                    "y": 1786
                }
            ],
            "category": "paragraph",
            "html": "<p id='23' style='font-size:14px'>where we refer to the elements of the T x T matrix</p>",
            "id": 23,
            "page": 2,
            "text": "where we refer to the elements of the T x T matrix"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1882
                },
                {
                    "x": 2104,
                    "y": 1882
                },
                {
                    "x": 2104,
                    "y": 2068
                },
                {
                    "x": 442,
                    "y": 2068
                }
            ],
            "category": "paragraph",
            "html": "<p id='24' style='font-size:14px'>as attention scores and the softmax output3 as attention probabilities. The layer is parametrized<br>by a query matrix Wqry E RDinxDk, a key matrix Wkey E RDinxDk and a value matrix Wval E<br>RDinxDout<br>.For simplicity, we exclude any residual connections, batch normalization and constant<br>factors.</p>",
            "id": 24,
            "page": 2,
            "text": "as attention scores and the softmax output3 as attention probabilities. The layer is parametrized\nby a query matrix Wqry E RDinxDk, a key matrix Wkey E RDinxDk and a value matrix Wval E\nRDinxDout\n.For simplicity, we exclude any residual connections, batch normalization and constant\nfactors."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2095
                },
                {
                    "x": 2106,
                    "y": 2095
                },
                {
                    "x": 2106,
                    "y": 2323
                },
                {
                    "x": 442,
                    "y": 2323
                }
            ],
            "category": "paragraph",
            "html": "<p id='25' style='font-size:16px'>A key property of the self-attention model described above is that it is equivariant to reordering, that<br>is, it gives the same outputindependently ofhow the T input tokens are shuffled. This is problematic<br>for cases we expect the order of things to matter. To alleviate the limitation, a positional encoding<br>is learned for each token in the sequence (or pixel in an image), and added to the representation of<br>the token itself before applying self-attention</p>",
            "id": 25,
            "page": 2,
            "text": "A key property of the self-attention model described above is that it is equivariant to reordering, that\nis, it gives the same outputindependently ofhow the T input tokens are shuffled. This is problematic\nfor cases we expect the order of things to matter. To alleviate the limitation, a positional encoding\nis learned for each token in the sequence (or pixel in an image), and added to the representation of\nthe token itself before applying self-attention"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2419
                },
                {
                    "x": 2104,
                    "y": 2419
                },
                {
                    "x": 2104,
                    "y": 2516
                },
                {
                    "x": 442,
                    "y": 2516
                }
            ],
            "category": "paragraph",
            "html": "<p id='26' style='font-size:16px'>contains the embedding vectors for each position. More generally, P may be<br>where P E RTxDin<br>substituted by any function that returns a vector representation of the position.</p>",
            "id": 26,
            "page": 2,
            "text": "contains the embedding vectors for each position. More generally, P may be\nwhere P E RTxDin\nsubstituted by any function that returns a vector representation of the position."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2541
                },
                {
                    "x": 2105,
                    "y": 2541
                },
                {
                    "x": 2105,
                    "y": 2723
                },
                {
                    "x": 443,
                    "y": 2723
                }
            ],
            "category": "paragraph",
            "html": "<p id='27' style='font-size:16px'>Ithas been found beneficial in practice to replicate this self-attention mechanism into multiple heads,<br>each being able to focus on different parts of the input by using different query, key and value<br>matrices. In multi-head self-attention, the output of the Nh heads of output dimension Dh are<br>concatenated and projected to dimension D out as follows:</p>",
            "id": 27,
            "page": 2,
            "text": "Ithas been found beneficial in practice to replicate this self-attention mechanism into multiple heads,\neach being able to focus on different parts of the input by using different query, key and value\nmatrices. In multi-head self-attention, the output of the Nh heads of output dimension Dh are\nconcatenated and projected to dimension D out as follows:"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2841
                },
                {
                    "x": 2103,
                    "y": 2841
                },
                {
                    "x": 2103,
                    "y": 2936
                },
                {
                    "x": 442,
                    "y": 2936
                }
            ],
            "category": "paragraph",
            "html": "<p id='28' style='font-size:14px'>and two new parameters are introduced: the projection matrix Wout E RNhDhxDout bias term<br>and a<br>bout E RDout</p>",
            "id": 28,
            "page": 2,
            "text": "and two new parameters are introduced: the projection matrix Wout E RNhDhxDout bias term\nand a\nbout E RDout"
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3134
                },
                {
                    "x": 1289,
                    "y": 3134
                },
                {
                    "x": 1289,
                    "y": 3170
                },
                {
                    "x": 1260,
                    "y": 3170
                }
            ],
            "category": "footer",
            "html": "<footer id='29' style='font-size:16px'>2</footer>",
            "id": 29,
            "page": 2,
            "text": "2"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 112
                },
                {
                    "x": 1225,
                    "y": 112
                },
                {
                    "x": 1225,
                    "y": 156
                },
                {
                    "x": 445,
                    "y": 156
                }
            ],
            "category": "header",
            "html": "<header id='30' style='font-size:14px'>Published as a conference paper at ICLR 2020</header>",
            "id": 30,
            "page": 3,
            "text": "Published as a conference paper at ICLR 2020"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 346
                },
                {
                    "x": 993,
                    "y": 346
                },
                {
                    "x": 993,
                    "y": 393
                },
                {
                    "x": 444,
                    "y": 393
                }
            ],
            "category": "paragraph",
            "html": "<p id='31' style='font-size:14px'>2.2 ATTENTION FOR IMAGES</p>",
            "id": 31,
            "page": 3,
            "text": "2.2 ATTENTION FOR IMAGES"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 433
                },
                {
                    "x": 2106,
                    "y": 433
                },
                {
                    "x": 2106,
                    "y": 570
                },
                {
                    "x": 442,
                    "y": 570
                }
            ],
            "category": "paragraph",
            "html": "<p id='32' style='font-size:16px'>Convolutional layers are the de facto choice for building neural networks that operate on images.<br>of width W, height H and Din channels, the<br>We recall that, given an image tensor X E RWxHxDin<br>output of a convolutional layer for pixel (i,j) is given by</p>",
            "id": 32,
            "page": 3,
            "text": "Convolutional layers are the de facto choice for building neural networks that operate on images.\nof width W, height H and Din channels, the\nWe recall that, given an image tensor X E RWxHxDin\noutput of a convolutional layer for pixel (i,j) is given by"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 723
                },
                {
                    "x": 2000,
                    "y": 723
                },
                {
                    "x": 2000,
                    "y": 776
                },
                {
                    "x": 443,
                    "y": 776
                }
            ],
            "category": "paragraph",
            "html": "<p id='33' style='font-size:14px'>4 b E RDout is the bias vector and the set<br>where W is the K x K x Din x Dout weight tensor ,</p>",
            "id": 33,
            "page": 3,
            "text": "4 b E RDout is the bias vector and the set\nwhere W is the K x K x Din x Dout weight tensor ,"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 916
                },
                {
                    "x": 1906,
                    "y": 916
                },
                {
                    "x": 1906,
                    "y": 961
                },
                {
                    "x": 442,
                    "y": 961
                }
            ],
            "category": "paragraph",
            "html": "<p id='34' style='font-size:16px'>contains all possible shifts appearing when convolving the image with a K x K kernel.</p>",
            "id": 34,
            "page": 3,
            "text": "contains all possible shifts appearing when convolving the image with a K x K kernel."
        },
        {
            "bounding_box": [
                {
                    "x": 446,
                    "y": 986
                },
                {
                    "x": 1986,
                    "y": 986
                },
                {
                    "x": 1986,
                    "y": 1033
                },
                {
                    "x": 446,
                    "y": 1033
                }
            ],
            "category": "paragraph",
            "html": "<p id='35' style='font-size:16px'>In the following, we review how self-attention can be adapted from 1D sequences to images.</p>",
            "id": 35,
            "page": 3,
            "text": "In the following, we review how self-attention can be adapted from 1D sequences to images."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1056
                },
                {
                    "x": 2105,
                    "y": 1056
                },
                {
                    "x": 2105,
                    "y": 1191
                },
                {
                    "x": 442,
                    "y": 1191
                }
            ],
            "category": "paragraph",
            "html": "<p id='36' style='font-size:16px'>With images, rather than tokens, we have query and key pixels q, k E [W] x [H]. Accordingly, the<br>input is a tensor X of dimension W x H x Din and each attention score associates a query and a key<br>pixel.</p>",
            "id": 36,
            "page": 3,
            "text": "With images, rather than tokens, we have query and key pixels q, k E [W] x [H]. Accordingly, the\ninput is a tensor X of dimension W x H x Din and each attention score associates a query and a key\npixel."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1217
                },
                {
                    "x": 2106,
                    "y": 1217
                },
                {
                    "x": 2106,
                    "y": 1356
                },
                {
                    "x": 443,
                    "y": 1356
                }
            ],
            "category": "paragraph",
            "html": "<p id='37' style='font-size:16px'>To keep the formulas consistent with the 1D case, we abuse notation and slice tensors by using a 2D<br>index vector: if p = (i,j), we write Xp,: and Ap,: to mean Xi,j,: and Ai,j,:,:, respectively. With this<br>notation in place, the multi-head self attention layer output at pixel q can be expressed as follows:</p>",
            "id": 37,
            "page": 3,
            "text": "To keep the formulas consistent with the 1D case, we abuse notation and slice tensors by using a 2D\nindex vector: if p = (i,j), we write Xp,: and Ap,: to mean Xi,j,: and Ai,j,:,:, respectively. With this\nnotation in place, the multi-head self attention layer output at pixel q can be expressed as follows:"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1496
                },
                {
                    "x": 1123,
                    "y": 1496
                },
                {
                    "x": 1123,
                    "y": 1541
                },
                {
                    "x": 443,
                    "y": 1541
                }
            ],
            "category": "paragraph",
            "html": "<p id='38' style='font-size:16px'>and accordingly for the multi-head case.</p>",
            "id": 38,
            "page": 3,
            "text": "and accordingly for the multi-head case."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 1596
                },
                {
                    "x": 1211,
                    "y": 1596
                },
                {
                    "x": 1211,
                    "y": 1643
                },
                {
                    "x": 445,
                    "y": 1643
                }
            ],
            "category": "paragraph",
            "html": "<p id='39' style='font-size:16px'>2.3 POSITIONAL ENCODING FOR IMAGES</p>",
            "id": 39,
            "page": 3,
            "text": "2.3 POSITIONAL ENCODING FOR IMAGES"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1683
                },
                {
                    "x": 2103,
                    "y": 1683
                },
                {
                    "x": 2103,
                    "y": 1773
                },
                {
                    "x": 443,
                    "y": 1773
                }
            ],
            "category": "paragraph",
            "html": "<p id='40' style='font-size:16px'>There are two types of positional encoding that has been used in transformer-based architectures:<br>the absolute and relative encoding (see also Table 3 in the Appendix).</p>",
            "id": 40,
            "page": 3,
            "text": "There are two types of positional encoding that has been used in transformer-based architectures:\nthe absolute and relative encoding (see also Table 3 in the Appendix)."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 1800
                },
                {
                    "x": 2104,
                    "y": 1800
                },
                {
                    "x": 2104,
                    "y": 1893
                },
                {
                    "x": 444,
                    "y": 1893
                }
            ],
            "category": "paragraph",
            "html": "<p id='41' style='font-size:16px'>With absolute encodings, a (fixed or learned) vector Pp,: is assigned to each pixel p. The computa-<br>tion of the attention scores we saw in eq. (2) can then be decomposed as follows:</p>",
            "id": 41,
            "page": 3,
            "text": "With absolute encodings, a (fixed or learned) vector Pp,: is assigned to each pixel p. The computa-\ntion of the attention scores we saw in eq. (2) can then be decomposed as follows:"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 2062
                },
                {
                    "x": 1568,
                    "y": 2062
                },
                {
                    "x": 1568,
                    "y": 2107
                },
                {
                    "x": 445,
                    "y": 2107
                }
            ],
            "category": "paragraph",
            "html": "<p id='42' style='font-size:16px'>where q and k correspond to the query and key pixels, respectively.</p>",
            "id": 42,
            "page": 3,
            "text": "where q and k correspond to the query and key pixels, respectively."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2130
                },
                {
                    "x": 2105,
                    "y": 2130
                },
                {
                    "x": 2105,
                    "y": 2267
                },
                {
                    "x": 442,
                    "y": 2267
                }
            ],
            "category": "paragraph",
            "html": "<p id='43' style='font-size:16px'>The relative positional encoding was introduced by Dai et al. (2019). The main idea is to only<br>consider the position difference between the query pixel (pixel we compute the representation of)<br>and the key pixel (pixel we attend) instead of the absolute position of the key pixel:</p>",
            "id": 43,
            "page": 3,
            "text": "The relative positional encoding was introduced by Dai et al. (2019). The main idea is to only\nconsider the position difference between the query pixel (pixel we compute the representation of)\nand the key pixel (pixel we attend) instead of the absolute position of the key pixel:"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2365
                },
                {
                    "x": 2105,
                    "y": 2365
                },
                {
                    "x": 2105,
                    "y": 2560
                },
                {
                    "x": 442,
                    "y": 2560
                }
            ],
            "category": "paragraph",
            "html": "<p id='44' style='font-size:16px'>In this manner, the attention scores only depend on the shift 8 := k - q. Above, the learnable<br>vectors u and v are unique for each head, whereas for every shift 8 the relative positional encoding<br>rs E RDp is shared by all layers and heads. Moreover, now the key weights are split into two types:<br>Wkey pertain to the input and Wkey to the relative position of pixels.</p>",
            "id": 44,
            "page": 3,
            "text": "In this manner, the attention scores only depend on the shift 8 := k - q. Above, the learnable\nvectors u and v are unique for each head, whereas for every shift 8 the relative positional encoding\nrs E RDp is shared by all layers and heads. Moreover, now the key weights are split into two types:\nWkey pertain to the input and Wkey to the relative position of pixels."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 2622
                },
                {
                    "x": 1569,
                    "y": 2622
                },
                {
                    "x": 1569,
                    "y": 2676
                },
                {
                    "x": 445,
                    "y": 2676
                }
            ],
            "category": "paragraph",
            "html": "<p id='45' style='font-size:18px'>3 SELF-ATTENTION AS A CONVOLUTIONAL LAYER</p>",
            "id": 45,
            "page": 3,
            "text": "3 SELF-ATTENTION AS A CONVOLUTIONAL LAYER"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2728
                },
                {
                    "x": 2105,
                    "y": 2728
                },
                {
                    "x": 2105,
                    "y": 2816
                },
                {
                    "x": 443,
                    "y": 2816
                }
            ],
            "category": "paragraph",
            "html": "<p id='46' style='font-size:16px'>This section derives sufficient conditions such that a multi-head self-attention layer can simulate a<br>convolutional layer. Our main result is the following:</p>",
            "id": 46,
            "page": 3,
            "text": "This section derives sufficient conditions such that a multi-head self-attention layer can simulate a\nconvolutional layer. Our main result is the following:"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2831
                },
                {
                    "x": 2105,
                    "y": 2831
                },
                {
                    "x": 2105,
                    "y": 2978
                },
                {
                    "x": 443,
                    "y": 2978
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='47' style='font-size:18px'>Theorem 1. A multi-head self-attention layer with Nh heads of dimension Dh, output dimen-<br>sion Dout and a relative positional encoding of dimension Dp ≥ 3 can express any convolutional<br>layer of kernel size VNh x VNh and min(Dh, Dout) output channels.</p>",
            "id": 47,
            "page": 3,
            "text": "Theorem 1. A multi-head self-attention layer with Nh heads of dimension Dh, output dimen-\nsion Dout and a relative positional encoding of dimension Dp ≥ 3 can express any convolutional\nlayer of kernel size VNh x VNh and min(Dh, Dout) output channels."
        },
        {
            "bounding_box": [
                {
                    "x": 504,
                    "y": 3007
                },
                {
                    "x": 1930,
                    "y": 3007
                },
                {
                    "x": 1930,
                    "y": 3054
                },
                {
                    "x": 504,
                    "y": 3054
                }
            ],
            "category": "paragraph",
            "html": "<p id='48' style='font-size:14px'>4To simplify notation, we index the first two dimensions of the tensor from -[K/2] to [K/2].</p>",
            "id": 48,
            "page": 3,
            "text": "4To simplify notation, we index the first two dimensions of the tensor from -[K/2] to [K/2]."
        },
        {
            "bounding_box": [
                {
                    "x": 1261,
                    "y": 3132
                },
                {
                    "x": 1288,
                    "y": 3132
                },
                {
                    "x": 1288,
                    "y": 3171
                },
                {
                    "x": 1261,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='49' style='font-size:16px'>3</footer>",
            "id": 49,
            "page": 3,
            "text": "3"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 111
                },
                {
                    "x": 1225,
                    "y": 111
                },
                {
                    "x": 1225,
                    "y": 156
                },
                {
                    "x": 445,
                    "y": 156
                }
            ],
            "category": "header",
            "html": "<header id='50' style='font-size:14px'>Published as a conference paper at ICLR 2020</header>",
            "id": 50,
            "page": 4,
            "text": "Published as a conference paper at ICLR 2020"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 345
                },
                {
                    "x": 2108,
                    "y": 345
                },
                {
                    "x": 2108,
                    "y": 576
                },
                {
                    "x": 441,
                    "y": 576
                }
            ],
            "category": "paragraph",
            "html": "<p id='51' style='font-size:14px'>The theorem is proven constructively by selecting the parameters of the multi-head self-attention<br>layer SO that the latter acts like a convolutional layer. In the proposed construction, the attention<br>scores of each self-attention head should attend to a different relative shift within the set K =<br>{ - [K/2], · · · , [K/2]}2 of all pixel shifts in a K x K kernel. The exact condition can be found in<br>the statement of Lemma 1.</p>",
            "id": 51,
            "page": 4,
            "text": "The theorem is proven constructively by selecting the parameters of the multi-head self-attention\nlayer SO that the latter acts like a convolutional layer. In the proposed construction, the attention\nscores of each self-attention head should attend to a different relative shift within the set K =\n{ - [K/2], · · · , [K/2]}2 of all pixel shifts in a K x K kernel. The exact condition can be found in\nthe statement of Lemma 1."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 602
                },
                {
                    "x": 2103,
                    "y": 602
                },
                {
                    "x": 2103,
                    "y": 693
                },
                {
                    "x": 442,
                    "y": 693
                }
            ],
            "category": "paragraph",
            "html": "<p id='52' style='font-size:18px'>Then, Lemma 2 shows that the aforementioned condition is satisfied for the relative positional en-<br>coding that we refer to as the quadratic encoding:</p>",
            "id": 52,
            "page": 4,
            "text": "Then, Lemma 2 shows that the aforementioned condition is satisfied for the relative positional en-\ncoding that we refer to as the quadratic encoding:"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 833
                },
                {
                    "x": 2106,
                    "y": 833
                },
                {
                    "x": 2106,
                    "y": 984
                },
                {
                    "x": 441,
                    "y": 984
                }
            ],
            "category": "paragraph",
            "html": "<p id='53' style='font-size:20px'>The learned parameters △ (h) = (△(h) , △ (h) ) and a(h) determine the center and width of attention<br>of each head, respectively. On the other hand, 8 = (81, 82) is fixed and expresses the relative shift<br>between query and key pixels.</p>",
            "id": 53,
            "page": 4,
            "text": "The learned parameters △ (h) = (△(h) , △ (h) ) and a(h) determine the center and width of attention\nof each head, respectively. On the other hand, 8 = (81, 82) is fixed and expresses the relative shift\nbetween query and key pixels."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1007
                },
                {
                    "x": 2107,
                    "y": 1007
                },
                {
                    "x": 2107,
                    "y": 1283
                },
                {
                    "x": 441,
                    "y": 1283
                }
            ],
            "category": "paragraph",
            "html": "<p id='54' style='font-size:18px'>It is important to stress that the above encoding is not the only one for which the conditions of<br>Lemma 1 are satisfied. In fact, in our experiments, the relative encoding learned by the neural<br>network also matched the conditions of the lemma (despite being different from the quadratic en-<br>coding). Nevertheless, the encoding defined above is very efficient in terms of size, as only Dp = 3<br>dimensions suffice to encode the relative position of pixels, while also reaching similar or better<br>empirical performance (than the learned one).</p>",
            "id": 54,
            "page": 4,
            "text": "It is important to stress that the above encoding is not the only one for which the conditions of\nLemma 1 are satisfied. In fact, in our experiments, the relative encoding learned by the neural\nnetwork also matched the conditions of the lemma (despite being different from the quadratic en-\ncoding). Nevertheless, the encoding defined above is very efficient in terms of size, as only Dp = 3\ndimensions suffice to encode the relative position of pixels, while also reaching similar or better\nempirical performance (than the learned one)."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1306
                },
                {
                    "x": 2106,
                    "y": 1306
                },
                {
                    "x": 2106,
                    "y": 1446
                },
                {
                    "x": 441,
                    "y": 1446
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='55' style='font-size:20px'>The theorem covers the general convolution operator as defined in eq. (17). However, machine<br>learning practitioners using differential programming frameworks (Paszke et al., 2017; Abadi et al.,<br>2015) might question if the theorem holds for all hyper-parameters of 2D convolutional layers:</p>",
            "id": 55,
            "page": 4,
            "text": "The theorem covers the general convolution operator as defined in eq. (17). However, machine\nlearning practitioners using differential programming frameworks (Paszke et al., 2017; Abadi et al.,\n2015) might question if the theorem holds for all hyper-parameters of 2D convolutional layers:"
        },
        {
            "bounding_box": [
                {
                    "x": 543,
                    "y": 1492
                },
                {
                    "x": 2109,
                    "y": 1492
                },
                {
                    "x": 2109,
                    "y": 1968
                },
                {
                    "x": 543,
                    "y": 1968
                }
            ],
            "category": "paragraph",
            "html": "<p id='56' style='font-size:16px'>● Padding: a multi-head self-attention layer uses by default the \" SAME \" padding while a<br>convolutional layer would decrease the image size by K - 1 pixels. The correct way to<br>alleviate these boundary effects is to pad the input image with [K/2] zeros on each side.<br>In this case, the cropped output of a MHSA and a convolutional layer are the same.<br>● Stride: a strided convolution can be seen as a convolution followed by a fixed pooling<br>operation-with computational optimizations. Theorem 1 is defined for stride 1, but a<br>fixed pooling layer could be appended to the Self-Attention layer to simulate any stride.<br>● Dilation: a multi-head self-attention layer can express any dilated convolution as each head<br>can attend a value at any pixel shift and form a (dilated) grid pattern.</p>",
            "id": 56,
            "page": 4,
            "text": "● Padding: a multi-head self-attention layer uses by default the \" SAME \" padding while a\nconvolutional layer would decrease the image size by K - 1 pixels. The correct way to\nalleviate these boundary effects is to pad the input image with [K/2] zeros on each side.\nIn this case, the cropped output of a MHSA and a convolutional layer are the same.\n● Stride: a strided convolution can be seen as a convolution followed by a fixed pooling\noperation-with computational optimizations. Theorem 1 is defined for stride 1, but a\nfixed pooling layer could be appended to the Self-Attention layer to simulate any stride.\n● Dilation: a multi-head self-attention layer can express any dilated convolution as each head\ncan attend a value at any pixel shift and form a (dilated) grid pattern."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2023
                },
                {
                    "x": 2108,
                    "y": 2023
                },
                {
                    "x": 2108,
                    "y": 2393
                },
                {
                    "x": 441,
                    "y": 2393
                }
            ],
            "category": "paragraph",
            "html": "<p id='57' style='font-size:18px'>Remark for the 1D case. Convolutional layers acting on sequences are commonly used in the lit-<br>erature for text (Kim, 2014), as well as audio (van den Oord et al., 2016) and time series (Franceschi<br>et al., 2019). Theorem 1 can be straightforwardly extended to show that multi-head self-attention<br>with Nh heads can also simulate a 1D convolutional layer with a kernel of size K = Nh with<br>min(Dh, Dout) output channels using a positional encoding of dimension Dp ≥ 2. Since we have<br>not tested empirically if the preceding construction matches the behavior of 1D self-attention in<br>practice, we cannot claim that it actually learns to convolve an input sequence-only that it has the<br>capacity to do so.</p>",
            "id": 57,
            "page": 4,
            "text": "Remark for the 1D case. Convolutional layers acting on sequences are commonly used in the lit-\nerature for text (Kim, 2014), as well as audio (van den Oord et al., 2016) and time series (Franceschi\net al., 2019). Theorem 1 can be straightforwardly extended to show that multi-head self-attention\nwith Nh heads can also simulate a 1D convolutional layer with a kernel of size K = Nh with\nmin(Dh, Dout) output channels using a positional encoding of dimension Dp ≥ 2. Since we have\nnot tested empirically if the preceding construction matches the behavior of 1D self-attention in\npractice, we cannot claim that it actually learns to convolve an input sequence-only that it has the\ncapacity to do so."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2461
                },
                {
                    "x": 936,
                    "y": 2461
                },
                {
                    "x": 936,
                    "y": 2508
                },
                {
                    "x": 444,
                    "y": 2508
                }
            ],
            "category": "paragraph",
            "html": "<p id='58' style='font-size:16px'>PROOF OF MAIN THEOREM</p>",
            "id": 58,
            "page": 4,
            "text": "PROOF OF MAIN THEOREM"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 2553
                },
                {
                    "x": 1496,
                    "y": 2553
                },
                {
                    "x": 1496,
                    "y": 2600
                },
                {
                    "x": 445,
                    "y": 2600
                }
            ],
            "category": "paragraph",
            "html": "<p id='59' style='font-size:18px'>The proof follows directly from Lemmas 1 and 2 stated below:</p>",
            "id": 59,
            "page": 4,
            "text": "The proof follows directly from Lemmas 1 and 2 stated below:"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2620
                },
                {
                    "x": 2106,
                    "y": 2620
                },
                {
                    "x": 2106,
                    "y": 2761
                },
                {
                    "x": 442,
                    "y": 2761
                }
            ],
            "category": "paragraph",
            "html": "<p id='60' style='font-size:22px'>Lemma 1. Consider a multi-head self-attention layer consisting of Nh = K2 heads, Dh ≥ Dout<br>and let f : [Nh] → A K be a bijective mapping of heads onto shifts. Further, suppose that for<br>every head the following holds:</p>",
            "id": 60,
            "page": 4,
            "text": "Lemma 1. Consider a multi-head self-attention layer consisting of Nh = K2 heads, Dh ≥ Dout\nand let f : [Nh] → A K be a bijective mapping of heads onto shifts. Further, suppose that for\nevery head the following holds:"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2946
                },
                {
                    "x": 2105,
                    "y": 2946
                },
                {
                    "x": 2105,
                    "y": 3059
                },
                {
                    "x": 442,
                    "y": 3059
                }
            ],
            "category": "paragraph",
            "html": "<p id='61' style='font-size:22px'>Then, for any convolutional layer with a K x K kernel and Dout output channels, there exists<br>{W(h) }hE[Nn] MHSA(X) = Conv(X) for every X E R W xHxDin.<br>such that</p>",
            "id": 61,
            "page": 4,
            "text": "Then, for any convolutional layer with a K x K kernel and Dout output channels, there exists\n{W(h) }hE[Nn] MHSA(X) = Conv(X) for every X E R W xHxDin.\nsuch that"
        },
        {
            "bounding_box": [
                {
                    "x": 1259,
                    "y": 3135
                },
                {
                    "x": 1288,
                    "y": 3135
                },
                {
                    "x": 1288,
                    "y": 3170
                },
                {
                    "x": 1259,
                    "y": 3170
                }
            ],
            "category": "footer",
            "html": "<footer id='62' style='font-size:16px'>4</footer>",
            "id": 62,
            "page": 4,
            "text": "4"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 112
                },
                {
                    "x": 1226,
                    "y": 112
                },
                {
                    "x": 1226,
                    "y": 157
                },
                {
                    "x": 445,
                    "y": 157
                }
            ],
            "category": "header",
            "html": "<header id='63' style='font-size:14px'>Published as a conference paper at ICLR 2020</header>",
            "id": 63,
            "page": 5,
            "text": "Published as a conference paper at ICLR 2020"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 304
                },
                {
                    "x": 2124,
                    "y": 304
                },
                {
                    "x": 2124,
                    "y": 1044
                },
                {
                    "x": 442,
                    "y": 1044
                }
            ],
            "category": "figure",
            "html": "<figure><img id='64' style='font-size:14px' alt=\"Multi-Head Self-Attention Layer\nDh\n(1)\nx Din\nWval\nD out\nA(1) MHSA(X)\nX q,\nDX\nDim\nW Dout\nW\nDX\nx\nH Wv(2) x\nDX NhDh\nH\nA (2)\nq⌀ concatenate\na key pixel the query pixel\nWout\nat position k at positionq\nx\nA(Nh) W(Nh)\nA (Nh) � Xk' : Aq,k' Filter matrices\nq⌀: k'E[W]x[H]\nAttention maps for pixel q\" data-coord=\"top-left:(442,304); bottom-right:(2124,1044)\" /></figure>",
            "id": 64,
            "page": 5,
            "text": "Multi-Head Self-Attention Layer\nDh\n(1)\nx Din\nWval\nD out\nA(1) MHSA(X)\nX q,\nDX\nDim\nW Dout\nW\nDX\nx\nH Wv(2) x\nDX NhDh\nH\nA (2)\nq⌀ concatenate\na key pixel the query pixel\nWout\nat position k at positionq\nx\nA(Nh) W(Nh)\nA (Nh) � Xk' : Aq,k' Filter matrices\nq⌀: k'E[W]x[H]\nAttention maps for pixel q"
        },
        {
            "bounding_box": [
                {
                    "x": 439,
                    "y": 1072
                },
                {
                    "x": 2108,
                    "y": 1072
                },
                {
                    "x": 2108,
                    "y": 1227
                },
                {
                    "x": 439,
                    "y": 1227
                }
            ],
            "category": "caption",
            "html": "<caption id='65' style='font-size:16px'>Figure 1: Illustration of a Multi-Head Self-Attention layer applied to a tensor image X. Each head h<br>attends pixel values around shift △ (h) and learn a filter matrix Wv(h) · We show attention maps<br>computed for a query pixel at position q.</caption>",
            "id": 65,
            "page": 5,
            "text": "Figure 1: Illustration of a Multi-Head Self-Attention layer applied to a tensor image X. Each head h\nattends pixel values around shift △ (h) and learn a filter matrix Wv(h) · We show attention maps\ncomputed for a query pixel at position q."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1279
                },
                {
                    "x": 2105,
                    "y": 1279
                },
                {
                    "x": 2105,
                    "y": 1372
                },
                {
                    "x": 442,
                    "y": 1372
                }
            ],
            "category": "paragraph",
            "html": "<p id='66' style='font-size:18px'>Proof. Our first step will be to rework the expression of the Multi-Head Self-Attention operator from<br>equation (1) and equation (4) such that the effect of the multiple heads becomes more transparent:</p>",
            "id": 66,
            "page": 5,
            "text": "Proof. Our first step will be to rework the expression of the Multi-Head Self-Attention operator from\nequation (1) and equation (4) such that the effect of the multiple heads becomes more transparent:"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 1553
                },
                {
                    "x": 2106,
                    "y": 1553
                },
                {
                    "x": 2106,
                    "y": 1752
                },
                {
                    "x": 440,
                    "y": 1752
                }
            ],
            "category": "paragraph",
            "html": "<p id='67' style='font-size:18px'>Note that each head's value matrix Wv(h) E RDinxDh and each block of the projection matrix Wout<br>of dimension Dh x Dout are learned. Assuming that Dh ≥ Dout, we can replace each pair of<br>matrices by a learned matrix W(h) for each head. We consider one output pixel of the multi-head<br>self-attention:</p>",
            "id": 67,
            "page": 5,
            "text": "Note that each head's value matrix Wv(h) E RDinxDh and each block of the projection matrix Wout\nof dimension Dh x Dout are learned. Assuming that Dh ≥ Dout, we can replace each pair of\nmatrices by a learned matrix W(h) for each head. We consider one output pixel of the multi-head\nself-attention:"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1936
                },
                {
                    "x": 2106,
                    "y": 1936
                },
                {
                    "x": 2106,
                    "y": 2031
                },
                {
                    "x": 441,
                    "y": 2031
                }
            ],
            "category": "paragraph",
            "html": "<p id='68' style='font-size:16px'>Due to the conditions of the Lemma, for the h-th attention head the attention probability is one when<br>k = q - f(h) and zero otherwise. The layer's output at pixel q is thus equal to</p>",
            "id": 68,
            "page": 5,
            "text": "Due to the conditions of the Lemma, for the h-th attention head the attention probability is one when\nk = q - f(h) and zero otherwise. The layer's output at pixel q is thus equal to"
        },
        {
            "bounding_box": [
                {
                    "x": 439,
                    "y": 2194
                },
                {
                    "x": 2108,
                    "y": 2194
                },
                {
                    "x": 2108,
                    "y": 2349
                },
                {
                    "x": 439,
                    "y": 2349
                }
            ],
            "category": "paragraph",
            "html": "<p id='69' style='font-size:16px'>For K = VNh, the above can be seen to be equivalent to a convolutional layer expressed in eq. 17:<br>there is a one to one mapping (implied by map f) between the matrices W(h) for h = [Nh] and the<br>matrices Wk1,k2,:,: [K]2. ■<br>for all (k1, k2) E</p>",
            "id": 69,
            "page": 5,
            "text": "For K = VNh, the above can be seen to be equivalent to a convolutional layer expressed in eq. 17:\nthere is a one to one mapping (implied by map f) between the matrices W(h) for h = [Nh] and the\nmatrices Wk1,k2,:,: [K]2. ■\nfor all (k1, k2) E"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2393
                },
                {
                    "x": 2107,
                    "y": 2393
                },
                {
                    "x": 2107,
                    "y": 2766
                },
                {
                    "x": 442,
                    "y": 2766
                }
            ],
            "category": "paragraph",
            "html": "<p id='70' style='font-size:16px'>Remark about Dh and Dout· It is frequent in transformer-based architectures to set<br>Dh = Dout/Nh, hence Dh < Dout· case, W(h) can be seen to be of rank Dout - Dh,<br>In that<br>which does not suffice to express every convolutional layer with Dout channels. Nevertheless, it can<br>be seen that any Dh out of D out outputs of MHSA(X) can express the output of any convolutional<br>layer with Dh output channels. To cover both cases, in the statement of the main theorem we assert<br>that the output channels of the convolutional layer should be min(Dh, Dout). In practice, we advise<br>to concatenate heads of dimension Dh = Dout instead of splitting the Dout dimensions among heads<br>to have exact re-parametrization and no \"unused\" channels.</p>",
            "id": 70,
            "page": 5,
            "text": "Remark about Dh and Dout· It is frequent in transformer-based architectures to set\nDh = Dout/Nh, hence Dh < Dout· case, W(h) can be seen to be of rank Dout - Dh,\nIn that\nwhich does not suffice to express every convolutional layer with Dout channels. Nevertheless, it can\nbe seen that any Dh out of D out outputs of MHSA(X) can express the output of any convolutional\nlayer with Dh output channels. To cover both cases, in the statement of the main theorem we assert\nthat the output channels of the convolutional layer should be min(Dh, Dout). In practice, we advise\nto concatenate heads of dimension Dh = Dout instead of splitting the Dout dimensions among heads\nto have exact re-parametrization and no \"unused\" channels."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2778
                },
                {
                    "x": 2107,
                    "y": 2778
                },
                {
                    "x": 2107,
                    "y": 2935
                },
                {
                    "x": 441,
                    "y": 2935
                }
            ],
            "category": "paragraph",
            "html": "<p id='71' style='font-size:18px'>Lemma 2. There exists a relative encoding scheme {rs E RDp }�EZ2 with Dp ≥ 3 and parame-<br>ters Wqry, Wkey, Wkey, u with Dp ≤ Dk such that, for every △ E △ K there exists some vector v<br>(conditioned on △) yielding softmax(Aq,:)k = 1 ifk - q = △ and zero, otherwise.</p>",
            "id": 71,
            "page": 5,
            "text": "Lemma 2. There exists a relative encoding scheme {rs E RDp }�EZ2 with Dp ≥ 3 and parame-\nters Wqry, Wkey, Wkey, u with Dp ≤ Dk such that, for every △ E △ K there exists some vector v\n(conditioned on △) yielding softmax(Aq,:)k = 1 ifk - q = △ and zero, otherwise."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2960
                },
                {
                    "x": 2105,
                    "y": 2960
                },
                {
                    "x": 2105,
                    "y": 3055
                },
                {
                    "x": 441,
                    "y": 3055
                }
            ],
            "category": "paragraph",
            "html": "<p id='72' style='font-size:18px'>Proof. We show by construction the existence of a Dp = 3 dimensional relative encoding scheme<br>yielding the required attention probabilities.</p>",
            "id": 72,
            "page": 5,
            "text": "Proof. We show by construction the existence of a Dp = 3 dimensional relative encoding scheme\nyielding the required attention probabilities."
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3134
                },
                {
                    "x": 1289,
                    "y": 3134
                },
                {
                    "x": 1289,
                    "y": 3171
                },
                {
                    "x": 1260,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='73' style='font-size:14px'>5</footer>",
            "id": 73,
            "page": 5,
            "text": "5"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 111
                },
                {
                    "x": 1225,
                    "y": 111
                },
                {
                    "x": 1225,
                    "y": 156
                },
                {
                    "x": 444,
                    "y": 156
                }
            ],
            "category": "header",
            "html": "<header id='74' style='font-size:14px'>Published as a conference paper at ICLR 2020</header>",
            "id": 74,
            "page": 6,
            "text": "Published as a conference paper at ICLR 2020"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 346
                },
                {
                    "x": 2108,
                    "y": 346
                },
                {
                    "x": 2108,
                    "y": 550
                },
                {
                    "x": 442,
                    "y": 550
                }
            ],
            "category": "paragraph",
            "html": "<p id='75' style='font-size:16px'>As the attention probabilities are independent of the input tensor X, we set Wkey = Wqry = 0 which<br>leaves only the last term of eq. (8). Setting Wkey E RDkxDp to the identity matrix (with appropriate<br>T 8 := k - q. Above, we have assumed that Dp ≤ Dk<br>row padding), yields Aq,k = v rs where<br>such that no information from rs is lost.</p>",
            "id": 75,
            "page": 6,
            "text": "As the attention probabilities are independent of the input tensor X, we set Wkey = Wqry = 0 which\nleaves only the last term of eq. (8). Setting Wkey E RDkxDp to the identity matrix (with appropriate\nT 8 := k - q. Above, we have assumed that Dp ≤ Dk\nrow padding), yields Aq,k = v rs where\nsuch that no information from rs is lost."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 575
                },
                {
                    "x": 1026,
                    "y": 575
                },
                {
                    "x": 1026,
                    "y": 620
                },
                {
                    "x": 443,
                    "y": 620
                }
            ],
            "category": "paragraph",
            "html": "<p id='76' style='font-size:16px'>Now, suppose that we could write:</p>",
            "id": 76,
            "page": 6,
            "text": "Now, suppose that we could write:"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 728
                },
                {
                    "x": 2105,
                    "y": 728
                },
                {
                    "x": 2105,
                    "y": 865
                },
                {
                    "x": 441,
                    "y": 865
                }
            ],
            "category": "paragraph",
            "html": "<p id='77' style='font-size:14px'>for some constant c. In the above expression, the maximum attention score over Aq,: is -ac and it<br>is reached for Aq,k with 8 = △. On the other hand, the a coefficient can be used to scale arbitrarily<br>the difference between Aq,△ and the other attention scores.</p>",
            "id": 77,
            "page": 6,
            "text": "for some constant c. In the above expression, the maximum attention score over Aq,: is -ac and it\nis reached for Aq,k with 8 = △. On the other hand, the a coefficient can be used to scale arbitrarily\nthe difference between Aq,△ and the other attention scores."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 889
                },
                {
                    "x": 991,
                    "y": 889
                },
                {
                    "x": 991,
                    "y": 935
                },
                {
                    "x": 444,
                    "y": 935
                }
            ],
            "category": "caption",
            "html": "<caption id='78' style='font-size:14px'>In this way, for 8 = △, we have</caption>",
            "id": 78,
            "page": 6,
            "text": "In this way, for 8 = △, we have"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 1242
                },
                {
                    "x": 2105,
                    "y": 1242
                },
                {
                    "x": 2105,
                    "y": 1336
                },
                {
                    "x": 440,
                    "y": 1336
                }
            ],
            "category": "paragraph",
            "html": "<p id='79' style='font-size:18px'>and for 8 ≠ △, the equation becomes lima→� softmax(Aq,:)k = 0, exactly as needed to satisfy<br>the lemma statement.</p>",
            "id": 79,
            "page": 6,
            "text": "and for 8 ≠ △, the equation becomes lima→� softmax(Aq,:)k = 0, exactly as needed to satisfy\nthe lemma statement."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1359
                },
                {
                    "x": 2106,
                    "y": 1359
                },
                {
                    "x": 2106,
                    "y": 1499
                },
                {
                    "x": 441,
                    "y": 1499
                }
            ],
            "category": "paragraph",
            "html": "<p id='80' style='font-size:16px'>What remains is to prove that there exist v and {rs}8EZ2 for which eq. (14) holds. Expanding the<br>RHS of the equation, we have -�(=8 - △ 112 + c) = -�(118112 + ||||2 - 2(8,△) +c) · Now if we<br>set v = -a (1, -2△1, -2△2) and rs = (|8|1|2 , 81, 82), then</p>",
            "id": 80,
            "page": 6,
            "text": "What remains is to prove that there exist v and {rs}8EZ2 for which eq. (14) holds. Expanding the\nRHS of the equation, we have -�(=8 - △ 112 + c) = -�(118112 + ||||2 - 2(8,△) +c) · Now if we\nset v = -a (1, -2△1, -2△2) and rs = (|8|1|2 , 81, 82), then"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1527
                },
                {
                    "x": 2111,
                    "y": 1527
                },
                {
                    "x": 2111,
                    "y": 1667
                },
                {
                    "x": 441,
                    "y": 1667
                }
            ],
            "category": "paragraph",
            "html": "<p id='81' style='font-size:18px'>T -a([8]2-2△181-2△262) = -�(18|2 -2<8, △〉) = -�( � - △ 112 -||△||2),<br>Aq,k = v rs =<br>which matches eq. (14) with c = - ||△||2 and the proof is concluded.</p>",
            "id": 81,
            "page": 6,
            "text": "T -a([8]2-2△181-2△262) = -�(18|2 -2<8, △〉) = -�( � - △ 112 -||△||2),\nAq,k = v rs =\nwhich matches eq. (14) with c = - ||△||2 and the proof is concluded."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1720
                },
                {
                    "x": 2107,
                    "y": 1720
                },
                {
                    "x": 2107,
                    "y": 1998
                },
                {
                    "x": 442,
                    "y": 1998
                }
            ],
            "category": "paragraph",
            "html": "<p id='82' style='font-size:16px'>Remark on the magnitude of a. The exact representation of one pixel requires a (or the matrices<br>Wqry and Wkey) to be arbitrary large, despite the fact that the attention probabilities of all other<br>pixels converge exponentially to 0 as a grows. Nevertheless, practical implementations always rely<br>on finite precision arithmetic for which a constant a suffices to satisfy our construction. For instance,<br>since the smallest positive float32 scalar is approximately 10-45 = 46 would suffice<br>, setting a<br>to obtain hard attention.</p>",
            "id": 82,
            "page": 6,
            "text": "Remark on the magnitude of a. The exact representation of one pixel requires a (or the matrices\nWqry and Wkey) to be arbitrary large, despite the fact that the attention probabilities of all other\npixels converge exponentially to 0 as a grows. Nevertheless, practical implementations always rely\non finite precision arithmetic for which a constant a suffices to satisfy our construction. For instance,\nsince the smallest positive float32 scalar is approximately 10-45 = 46 would suffice\n, setting a\nto obtain hard attention."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 2068
                },
                {
                    "x": 838,
                    "y": 2068
                },
                {
                    "x": 838,
                    "y": 2119
                },
                {
                    "x": 445,
                    "y": 2119
                }
            ],
            "category": "paragraph",
            "html": "<p id='83' style='font-size:22px'>4 EXPERIMENTS</p>",
            "id": 83,
            "page": 6,
            "text": "4 EXPERIMENTS"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 2172
                },
                {
                    "x": 2107,
                    "y": 2172
                },
                {
                    "x": 2107,
                    "y": 2450
                },
                {
                    "x": 440,
                    "y": 2450
                }
            ],
            "category": "paragraph",
            "html": "<p id='84' style='font-size:18px'>The aim of this section is to validate the applicability of our theoretical results-which state that<br>self-attention can perform convolution-and to examine whether self-attention layers in practice<br>do actually learn to operate like convolutional layers when trained on standard image classification<br>tasks. In particular, we study the relationship between self-attention and convolution with quadratic<br>and learned relative positional encodings. We find that, for both cases, the attention probabilities<br>learned tend to respect the conditions of Lemma 1, supporting our hypothesis.</p>",
            "id": 84,
            "page": 6,
            "text": "The aim of this section is to validate the applicability of our theoretical results-which state that\nself-attention can perform convolution-and to examine whether self-attention layers in practice\ndo actually learn to operate like convolutional layers when trained on standard image classification\ntasks. In particular, we study the relationship between self-attention and convolution with quadratic\nand learned relative positional encodings. We find that, for both cases, the attention probabilities\nlearned tend to respect the conditions of Lemma 1, supporting our hypothesis."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2509
                },
                {
                    "x": 1040,
                    "y": 2509
                },
                {
                    "x": 1040,
                    "y": 2556
                },
                {
                    "x": 444,
                    "y": 2556
                }
            ],
            "category": "paragraph",
            "html": "<p id='85' style='font-size:16px'>4.1 IMPLEMENTATION DETAILS</p>",
            "id": 85,
            "page": 6,
            "text": "4.1 IMPLEMENTATION DETAILS"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2593
                },
                {
                    "x": 2107,
                    "y": 2593
                },
                {
                    "x": 2107,
                    "y": 3054
                },
                {
                    "x": 443,
                    "y": 3054
                }
            ],
            "category": "paragraph",
            "html": "<p id='86' style='font-size:18px'>We study a fully attentional model consisting of six multi-head self-attention layers. As it has already<br>been shown by Bello et al. (2019) that combining attention features with convolutional features<br>improves performance on Cifar-100 and ImageNet, we do not focus on attaining state-of-the-art<br>performance. Nevertheless, to validate that our model learns a meaningful classifier, we compare<br>it to the standard ResNet18 (He et al., 2015) on the CIFAR-10 dataset (Krizhevsky et al.). In all<br>experiments, we use a 2 x 2 invertible down-sampling (Jacobsen et al., 2018) on the input to reduce<br>the size of the image. As the size of the attention coefficient tensors (stored during forward) scales<br>quadratically with the size of the input image, full attention cannot be applied to bigger images.<br>The fixed size representation of the input image is computed as the average pooling of the last layer<br>representations and given to a linear classifier.</p>",
            "id": 86,
            "page": 6,
            "text": "We study a fully attentional model consisting of six multi-head self-attention layers. As it has already\nbeen shown by Bello et al. (2019) that combining attention features with convolutional features\nimproves performance on Cifar-100 and ImageNet, we do not focus on attaining state-of-the-art\nperformance. Nevertheless, to validate that our model learns a meaningful classifier, we compare\nit to the standard ResNet18 (He et al., 2015) on the CIFAR-10 dataset (Krizhevsky et al.). In all\nexperiments, we use a 2 x 2 invertible down-sampling (Jacobsen et al., 2018) on the input to reduce\nthe size of the image. As the size of the attention coefficient tensors (stored during forward) scales\nquadratically with the size of the input image, full attention cannot be applied to bigger images.\nThe fixed size representation of the input image is computed as the average pooling of the last layer\nrepresentations and given to a linear classifier."
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3136
                },
                {
                    "x": 1289,
                    "y": 3136
                },
                {
                    "x": 1289,
                    "y": 3171
                },
                {
                    "x": 1260,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='87' style='font-size:14px'>6</footer>",
            "id": 87,
            "page": 6,
            "text": "6"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 110
                },
                {
                    "x": 1226,
                    "y": 110
                },
                {
                    "x": 1226,
                    "y": 157
                },
                {
                    "x": 444,
                    "y": 157
                }
            ],
            "category": "header",
            "html": "<header id='88' style='font-size:16px'>Published as a conference paper at ICLR 2020</header>",
            "id": 88,
            "page": 7,
            "text": "Published as a conference paper at ICLR 2020"
        },
        {
            "bounding_box": [
                {
                    "x": 482,
                    "y": 378
                },
                {
                    "x": 2110,
                    "y": 378
                },
                {
                    "x": 2110,
                    "y": 1325
                },
                {
                    "x": 482,
                    "y": 1325
                }
            ],
            "category": "figure",
            "html": "<figure><img id='89' style='font-size:14px' alt=\"1.0\n0.9\nModels accuracy # of params # of FLOPS\naccuracy\nResNet18 0.938 11.2M 1.1B\n0.8 SA quadratic emb. 0.938 12.1M 6.2B\nTest\nSA learned emb. 0.918 12.3M 6.2B\nSA learned emb. + content 0.871 29.5M 15B\nResNet18\n0.7\nSA quadratic emb.\nSA learned emb.\nSA learned emb. + content-based att.\n0.6\n0 50 100 150 200 250 300\nEpoch\nFigure 2: Test accuracy on CIFAR-10. Table 1: Test accuracy on CIFAR-10 and model\nsizes. SA stands for Self-Attention.\nEpoch 0 Epoch 50 Epoch 100 Epoch 300\" data-coord=\"top-left:(482,378); bottom-right:(2110,1325)\" /></figure>",
            "id": 89,
            "page": 7,
            "text": "1.0\n0.9\nModels accuracy # of params # of FLOPS\naccuracy\nResNet18 0.938 11.2M 1.1B\n0.8 SA quadratic emb. 0.938 12.1M 6.2B\nTest\nSA learned emb. 0.918 12.3M 6.2B\nSA learned emb. + content 0.871 29.5M 15B\nResNet18\n0.7\nSA quadratic emb.\nSA learned emb.\nSA learned emb. + content-based att.\n0.6\n0 50 100 150 200 250 300\nEpoch\nFigure 2: Test accuracy on CIFAR-10. Table 1: Test accuracy on CIFAR-10 and model\nsizes. SA stands for Self-Attention.\nEpoch 0 Epoch 50 Epoch 100 Epoch 300"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1357
                },
                {
                    "x": 2108,
                    "y": 1357
                },
                {
                    "x": 2108,
                    "y": 1500
                },
                {
                    "x": 442,
                    "y": 1500
                }
            ],
            "category": "caption",
            "html": "<caption id='90' style='font-size:22px'>Figure 3: Centers of attention of each attention head (different colors) at layer 4 during the training<br>with quadratic relative positional encoding. The central black square is the query pixel, whereas<br>solid and dotted circles represent the 50% and 90% percentiles of each Gaussian, respectively.</caption>",
            "id": 90,
            "page": 7,
            "text": "Figure 3: Centers of attention of each attention head (different colors) at layer 4 during the training\nwith quadratic relative positional encoding. The central black square is the query pixel, whereas\nsolid and dotted circles represent the 50% and 90% percentiles of each Gaussian, respectively."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1551
                },
                {
                    "x": 2104,
                    "y": 1551
                },
                {
                    "x": 2104,
                    "y": 1648
                },
                {
                    "x": 443,
                    "y": 1648
                }
            ],
            "category": "paragraph",
            "html": "<p id='91' style='font-size:18px'>We used the PyTorch library (Paszke et al., 2017) and based our implementation on PyTorch Trans-<br>formers5. We release our code on Github6 and hyper-parameters are listed in Table 2 (Appendix).</p>",
            "id": 91,
            "page": 7,
            "text": "We used the PyTorch library (Paszke et al., 2017) and based our implementation on PyTorch Trans-\nformers5. We release our code on Github6 and hyper-parameters are listed in Table 2 (Appendix)."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1673
                },
                {
                    "x": 2107,
                    "y": 1673
                },
                {
                    "x": 2107,
                    "y": 2090
                },
                {
                    "x": 442,
                    "y": 2090
                }
            ],
            "category": "paragraph",
            "html": "<p id='92' style='font-size:20px'>Remark on accuracy. To verify that our self-attention models perform reasonably well, we dis-<br>play in Figure 6 the evolution of the test accuracy on CIFAR-10 over the 300 epochs of training<br>for our self-attention models against a small ResNet (Table 1). The ResNet is faster to converge,<br>but we cannot ascertain whether this corresponds to an inherent property of the architecture or an<br>artifact of the adopted optimization procedures. Our implementation could be optimized to exploit<br>the locality of Gaussian attention probabilities and reduce significantly the number of FLOPS. We<br>observed that learned embeddings with content-based attention were harder to train probably due to<br>their increased number of parameters. We believe that the performance gap can be bridged to match<br>the ResNet performance, but this is not the focus of this work.</p>",
            "id": 92,
            "page": 7,
            "text": "Remark on accuracy. To verify that our self-attention models perform reasonably well, we dis-\nplay in Figure 6 the evolution of the test accuracy on CIFAR-10 over the 300 epochs of training\nfor our self-attention models against a small ResNet (Table 1). The ResNet is faster to converge,\nbut we cannot ascertain whether this corresponds to an inherent property of the architecture or an\nartifact of the adopted optimization procedures. Our implementation could be optimized to exploit\nthe locality of Gaussian attention probabilities and reduce significantly the number of FLOPS. We\nobserved that learned embeddings with content-based attention were harder to train probably due to\ntheir increased number of parameters. We believe that the performance gap can be bridged to match\nthe ResNet performance, but this is not the focus of this work."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2119
                },
                {
                    "x": 972,
                    "y": 2119
                },
                {
                    "x": 972,
                    "y": 2165
                },
                {
                    "x": 443,
                    "y": 2165
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='93' style='font-size:22px'>4.2 QUADRATIC ENCODING</p>",
            "id": 93,
            "page": 7,
            "text": "4.2 QUADRATIC ENCODING"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2181
                },
                {
                    "x": 2107,
                    "y": 2181
                },
                {
                    "x": 2107,
                    "y": 2370
                },
                {
                    "x": 442,
                    "y": 2370
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='94' style='font-size:20px'>As a first step, we aim to verify that, with the relative position encoding introduced in equation (9),<br>attention layers learn to behave like convolutional layers. We train nine attention heads at each layer<br>to be on par with the 3 x 3 kernels used predominantly by the ResNet architecture. The center of<br>attention of each head h is initialized to △ (h) ~ N(0, 2I2).</p>",
            "id": 94,
            "page": 7,
            "text": "As a first step, we aim to verify that, with the relative position encoding introduced in equation (9),\nattention layers learn to behave like convolutional layers. We train nine attention heads at each layer\nto be on par with the 3 x 3 kernels used predominantly by the ResNet architecture. The center of\nattention of each head h is initialized to △ (h) ~ N(0, 2I2)."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2392
                },
                {
                    "x": 2107,
                    "y": 2392
                },
                {
                    "x": 2107,
                    "y": 2578
                },
                {
                    "x": 441,
                    "y": 2578
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='95' style='font-size:22px'>Figure 3 shows how the initial positions of the heads (different colors) at layer 4 changed during<br>training. We can see that after optimization, the heads attend on specific pixel of the image forming a<br>grid around the query pixel. Our intuition that Self-Attention applied to images learns convolutional<br>filters around the queried pixel is confirmed.</p>",
            "id": 95,
            "page": 7,
            "text": "Figure 3 shows how the initial positions of the heads (different colors) at layer 4 changed during\ntraining. We can see that after optimization, the heads attend on specific pixel of the image forming a\ngrid around the query pixel. Our intuition that Self-Attention applied to images learns convolutional\nfilters around the queried pixel is confirmed."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2600
                },
                {
                    "x": 2107,
                    "y": 2600
                },
                {
                    "x": 2107,
                    "y": 2925
                },
                {
                    "x": 442,
                    "y": 2925
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='96' style='font-size:20px'>Figure 4 displays all attention head at each layer of the model at the end of the training. It can be<br>seen that in the first few layers the heads tend to focus on local patterns (layers 1 and 2), while deeper<br>layers (layers 3-6) also attend to larger patterns by positioning the center of attention further from<br>the queried pixel position. We also include in the Appendix a plot of the attention positions for a<br>higher number of heads (Nh = 16). Figure 14 displays both local patterns similar to CNN and long<br>range dependencies. Interestingly, attention heads do not overlap and seem to take an arrangement<br>maximizing the coverage of the input space.</p>",
            "id": 96,
            "page": 7,
            "text": "Figure 4 displays all attention head at each layer of the model at the end of the training. It can be\nseen that in the first few layers the heads tend to focus on local patterns (layers 1 and 2), while deeper\nlayers (layers 3-6) also attend to larger patterns by positioning the center of attention further from\nthe queried pixel position. We also include in the Appendix a plot of the attention positions for a\nhigher number of heads (Nh = 16). Figure 14 displays both local patterns similar to CNN and long\nrange dependencies. Interestingly, attention heads do not overlap and seem to take an arrangement\nmaximizing the coverage of the input space."
        },
        {
            "bounding_box": [
                {
                    "x": 503,
                    "y": 2957
                },
                {
                    "x": 1496,
                    "y": 2957
                },
                {
                    "x": 1496,
                    "y": 3053
                },
                {
                    "x": 503,
                    "y": 3053
                }
            ],
            "category": "paragraph",
            "html": "<p id='97' style='font-size:20px'>5github · com/hnogingface/pytorch-tranaforters<br>'github · com/ epfml / attention-cnn</p>",
            "id": 97,
            "page": 7,
            "text": "5github · com/hnogingface/pytorch-tranaforters\n'github · com/ epfml / attention-cnn"
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3134
                },
                {
                    "x": 1289,
                    "y": 3134
                },
                {
                    "x": 1289,
                    "y": 3171
                },
                {
                    "x": 1260,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='98' style='font-size:14px'>7</footer>",
            "id": 98,
            "page": 7,
            "text": "7"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 110
                },
                {
                    "x": 1226,
                    "y": 110
                },
                {
                    "x": 1226,
                    "y": 157
                },
                {
                    "x": 444,
                    "y": 157
                }
            ],
            "category": "header",
            "html": "<header id='99' style='font-size:16px'>Published as a conference paper at ICLR 2020</header>",
            "id": 99,
            "page": 8,
            "text": "Published as a conference paper at ICLR 2020"
        },
        {
            "bounding_box": [
                {
                    "x": 454,
                    "y": 336
                },
                {
                    "x": 2091,
                    "y": 336
                },
                {
                    "x": 2091,
                    "y": 613
                },
                {
                    "x": 454,
                    "y": 613
                }
            ],
            "category": "figure",
            "html": "<figure><img id='100' style='font-size:14px' alt=\"Layer 1 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6\n.\n□\" data-coord=\"top-left:(454,336); bottom-right:(2091,613)\" /></figure>",
            "id": 100,
            "page": 8,
            "text": "Layer 1 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6\n.\n□"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 648
                },
                {
                    "x": 2108,
                    "y": 648
                },
                {
                    "x": 2108,
                    "y": 795
                },
                {
                    "x": 441,
                    "y": 795
                }
            ],
            "category": "caption",
            "html": "<caption id='101' style='font-size:22px'>Figure 4: Centers of attention of each attention head (different colors) for the 6 self-attention layers<br>using quadratic positional encoding. The central black square is the query pixel, whereas solid and<br>dotted circles represent the 50% and 90% percentiles of each Gaussian, respectively.</caption>",
            "id": 101,
            "page": 8,
            "text": "Figure 4: Centers of attention of each attention head (different colors) for the 6 self-attention layers\nusing quadratic positional encoding. The central black square is the query pixel, whereas solid and\ndotted circles represent the 50% and 90% percentiles of each Gaussian, respectively."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 863
                },
                {
                    "x": 1354,
                    "y": 863
                },
                {
                    "x": 1354,
                    "y": 910
                },
                {
                    "x": 443,
                    "y": 910
                }
            ],
            "category": "paragraph",
            "html": "<p id='102' style='font-size:20px'>4.3 LEARNED RELATIVE POSITIONAL ENCODING</p>",
            "id": 102,
            "page": 8,
            "text": "4.3 LEARNED RELATIVE POSITIONAL ENCODING"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 949
                },
                {
                    "x": 2105,
                    "y": 949
                },
                {
                    "x": 2105,
                    "y": 998
                },
                {
                    "x": 443,
                    "y": 998
                }
            ],
            "category": "paragraph",
            "html": "<p id='103' style='font-size:22px'>We move on to study the positional encoding used in practice by fully-attentional models on images.</p>",
            "id": 103,
            "page": 8,
            "text": "We move on to study the positional encoding used in practice by fully-attentional models on images."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1019
                },
                {
                    "x": 2106,
                    "y": 1019
                },
                {
                    "x": 2106,
                    "y": 1433
                },
                {
                    "x": 442,
                    "y": 1433
                }
            ],
            "category": "paragraph",
            "html": "<p id='104' style='font-size:20px'>We implemented the 2D relative positional encoding scheme used by (Ramachandran et al., 2019;<br>Bello et al., 2019): we learn a [Dp/2] position encoding vector for each row and each column pixel<br>shift. Hence, the relative positional encoding of a key pixel at position k with a query pixel at posi-<br>tion q is the concatenation of the row shift embedding 81 and the column shift embedding 82 (where<br>8 = k - q). We chose Dp = Dout = 400 in the experiment. We differ from their (unpublished)<br>implementation in the following points: (i) we do not use convolution stem and ResNet bottlenecks<br>for downsampling, but only a 2 x 2 invertible downsampling layer (Jacobsen et al., 2018) at input,<br>(ii) we use Dh = D out instead of Dh = D out /Nh backed by our theory that the effective number of<br>learned filters is min(Dh, Dout).</p>",
            "id": 104,
            "page": 8,
            "text": "We implemented the 2D relative positional encoding scheme used by (Ramachandran et al., 2019;\nBello et al., 2019): we learn a [Dp/2] position encoding vector for each row and each column pixel\nshift. Hence, the relative positional encoding of a key pixel at position k with a query pixel at posi-\ntion q is the concatenation of the row shift embedding 81 and the column shift embedding 82 (where\n8 = k - q). We chose Dp = Dout = 400 in the experiment. We differ from their (unpublished)\nimplementation in the following points: (i) we do not use convolution stem and ResNet bottlenecks\nfor downsampling, but only a 2 x 2 invertible downsampling layer (Jacobsen et al., 2018) at input,\n(ii) we use Dh = D out instead of Dh = D out /Nh backed by our theory that the effective number of\nlearned filters is min(Dh, Dout)."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1455
                },
                {
                    "x": 2108,
                    "y": 1455
                },
                {
                    "x": 2108,
                    "y": 1777
                },
                {
                    "x": 442,
                    "y": 1777
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='105' style='font-size:20px'>At first, we discard the input data and compute the attention scores solely as the last term of eq. (8).<br>The attention probabilities of each head at each layer are displayed on Figure 5. The figure confirms<br>our hypothesis for the first two layers and partially for the third: even when left to learn the positional<br>encoding scheme from randomly initialized vectors, certain self-attention heads (depicted on the left)<br>learn to attend to individual pixels, closely matching the condition of Lemma 1 and thus Theorem<br>1. At the same time, other heads pay attention to horizontally-symmetric but non-localized patterns,<br>as well as to long-range pixel inter-dependencies.</p>",
            "id": 105,
            "page": 8,
            "text": "At first, we discard the input data and compute the attention scores solely as the last term of eq. (8).\nThe attention probabilities of each head at each layer are displayed on Figure 5. The figure confirms\nour hypothesis for the first two layers and partially for the third: even when left to learn the positional\nencoding scheme from randomly initialized vectors, certain self-attention heads (depicted on the left)\nlearn to attend to individual pixels, closely matching the condition of Lemma 1 and thus Theorem\n1. At the same time, other heads pay attention to horizontally-symmetric but non-localized patterns,\nas well as to long-range pixel inter-dependencies."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1800
                },
                {
                    "x": 2107,
                    "y": 1800
                },
                {
                    "x": 2107,
                    "y": 1939
                },
                {
                    "x": 441,
                    "y": 1939
                }
            ],
            "category": "paragraph",
            "html": "<p id='106' style='font-size:16px'>We move on to a more realistic setting where the attention scores are computed using both positional<br>T 2019)) which corresponds to<br>I k + q r in (Ramachandran et al.,<br>and content-based attention (i.e., q<br>a full-blown standalone self-attention model.</p>",
            "id": 106,
            "page": 8,
            "text": "We move on to a more realistic setting where the attention scores are computed using both positional\nT 2019)) which corresponds to\nI k + q r in (Ramachandran et al.,\nand content-based attention (i.e., q\na full-blown standalone self-attention model."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1961
                },
                {
                    "x": 2107,
                    "y": 1961
                },
                {
                    "x": 2107,
                    "y": 2425
                },
                {
                    "x": 441,
                    "y": 2425
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='107' style='font-size:20px'>The attention probabilities of each head at each layer are displayed in Figure 6. We average the<br>attention probabilities over a batch of 100 test images to outline the focus of each head and remove<br>the dependency on the input image. Our hypothesis is confirmed for some heads of layer 2 and 3:<br>even when left to learn the encoding from the data, certain self-attention heads only exploit position-<br>based attention to attend to distinct pixels at a fixed shift from the query pixel reproducing the<br>receptive field of a convolutional kernel. Other heads use more content-based attention (see Figures 8<br>to 10 in Appendix for non-averaged probabilities) leveraging the advantage of Self-Attention over<br>CNN which does not contradict our theory. In practice, it was shown by Bello et al. (2019) that<br>combining CNN and self-attention features outperforms each taken separately. Our experiments<br>shows that such combination is learned when optimizing an unconstrained fully-attentional model.</p>",
            "id": 107,
            "page": 8,
            "text": "The attention probabilities of each head at each layer are displayed in Figure 6. We average the\nattention probabilities over a batch of 100 test images to outline the focus of each head and remove\nthe dependency on the input image. Our hypothesis is confirmed for some heads of layer 2 and 3:\neven when left to learn the encoding from the data, certain self-attention heads only exploit position-\nbased attention to attend to distinct pixels at a fixed shift from the query pixel reproducing the\nreceptive field of a convolutional kernel. Other heads use more content-based attention (see Figures 8\nto 10 in Appendix for non-averaged probabilities) leveraging the advantage of Self-Attention over\nCNN which does not contradict our theory. In practice, it was shown by Bello et al. (2019) that\ncombining CNN and self-attention features outperforms each taken separately. Our experiments\nshows that such combination is learned when optimizing an unconstrained fully-attentional model."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2443
                },
                {
                    "x": 2108,
                    "y": 2443
                },
                {
                    "x": 2108,
                    "y": 2859
                },
                {
                    "x": 441,
                    "y": 2859
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='108' style='font-size:20px'>The similarity between convolution and multi-head self-attention is striking when the query pixel is<br>slid over the image: the localized attention patterns visible in Figure 6 follow the query pixel. This<br>characteristic behavior materializes when comparing Figure 6 with the attention probabilities at a<br>different query pixel (see Figure 7 in Appendix). Attention patterns in layers 2 and 3 are not only<br>localized but stand at a constant shift from the query pixel, similarly to convolving the receptive<br>field of a convolutional kernel over an image. This phenomenon is made evident on our interactive<br>website7 This tool is designed to explore different components of attention for diverse images with<br>or without content-based attention. We believe that it is a useful instrument to further understand<br>how MHSA learns to process images.</p>",
            "id": 108,
            "page": 8,
            "text": "The similarity between convolution and multi-head self-attention is striking when the query pixel is\nslid over the image: the localized attention patterns visible in Figure 6 follow the query pixel. This\ncharacteristic behavior materializes when comparing Figure 6 with the attention probabilities at a\ndifferent query pixel (see Figure 7 in Appendix). Attention patterns in layers 2 and 3 are not only\nlocalized but stand at a constant shift from the query pixel, similarly to convolving the receptive\nfield of a convolutional kernel over an image. This phenomenon is made evident on our interactive\nwebsite7 This tool is designed to explore different components of attention for diverse images with\nor without content-based attention. We believe that it is a useful instrument to further understand\nhow MHSA learns to process images."
        },
        {
            "bounding_box": [
                {
                    "x": 509,
                    "y": 3005
                },
                {
                    "x": 1179,
                    "y": 3005
                },
                {
                    "x": 1179,
                    "y": 3054
                },
                {
                    "x": 509,
                    "y": 3054
                }
            ],
            "category": "paragraph",
            "html": "<p id='109' style='font-size:14px'>7 epfml · github . io/ attention-cnn</p>",
            "id": 109,
            "page": 8,
            "text": "7 epfml · github . io/ attention-cnn"
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3133
                },
                {
                    "x": 1288,
                    "y": 3133
                },
                {
                    "x": 1288,
                    "y": 3171
                },
                {
                    "x": 1260,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='110' style='font-size:18px'>8</footer>",
            "id": 110,
            "page": 8,
            "text": "8"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 112
                },
                {
                    "x": 1226,
                    "y": 112
                },
                {
                    "x": 1226,
                    "y": 156
                },
                {
                    "x": 445,
                    "y": 156
                }
            ],
            "category": "header",
            "html": "<header id='111' style='font-size:18px'>Published as a conference paper at ICLR 2020</header>",
            "id": 111,
            "page": 9,
            "text": "Published as a conference paper at ICLR 2020"
        },
        {
            "bounding_box": [
                {
                    "x": 481,
                    "y": 434
                },
                {
                    "x": 2061,
                    "y": 434
                },
                {
                    "x": 2061,
                    "y": 1440
                },
                {
                    "x": 481,
                    "y": 1440
                }
            ],
            "category": "figure",
            "html": "<figure><img id='112' style='font-size:22px' alt=\"T\nJakel\nご Jaker\n♪ Jayel\n수 Jayel\n-\nJe⌀el\n■\n9 Jayel\n□ ■ □\" data-coord=\"top-left:(481,434); bottom-right:(2061,1440)\" /></figure>",
            "id": 112,
            "page": 9,
            "text": "T\nJakel\nご Jaker\n♪ Jayel\n수 Jayel\n-\nJe⌀el\n■\n9 Jayel\n□ ■ □"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1476
                },
                {
                    "x": 2106,
                    "y": 1476
                },
                {
                    "x": 2106,
                    "y": 1615
                },
                {
                    "x": 442,
                    "y": 1615
                }
            ],
            "category": "caption",
            "html": "<caption id='113' style='font-size:20px'>Figure 5: Attention probabilities of each head (column) at each layer (row) using learned relative<br>positional encoding without content-based attention. The central black square is the query pixel. We<br>reordered the heads for visualization and zoomed on the 7x7 pixels around the query pixel.</caption>",
            "id": 113,
            "page": 9,
            "text": "Figure 5: Attention probabilities of each head (column) at each layer (row) using learned relative\npositional encoding without content-based attention. The central black square is the query pixel. We\nreordered the heads for visualization and zoomed on the 7x7 pixels around the query pixel."
        },
        {
            "bounding_box": [
                {
                    "x": 485,
                    "y": 1726
                },
                {
                    "x": 2061,
                    "y": 1726
                },
                {
                    "x": 2061,
                    "y": 2781
                },
                {
                    "x": 485,
                    "y": 2781
                }
            ],
            "category": "figure",
            "html": "<figure><img id='114' style='font-size:14px' alt=\"layerl\n□ □\n2 layer\n3\nlayer\n□ □ □ □ □\n4\nlayer\n□ □\n5\nlayer\n9 layer\n□\" data-coord=\"top-left:(485,1726); bottom-right:(2061,2781)\" /></figure>",
            "id": 114,
            "page": 9,
            "text": "layerl\n□ □\n2 layer\n3\nlayer\n□ □ □ □ □\n4\nlayer\n□ □\n5\nlayer\n9 layer\n□"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 2814
                },
                {
                    "x": 2106,
                    "y": 2814
                },
                {
                    "x": 2106,
                    "y": 3006
                },
                {
                    "x": 440,
                    "y": 3006
                }
            ],
            "category": "caption",
            "html": "<caption id='115' style='font-size:20px'>Figure 6: Attention probabilities for a model with 6 layers (rows) and 9 heads (columns) using<br>learned relative positional encoding and content-content based attention. Attention maps are aver-<br>aged over 100 test images to display head behavior and remove the dependence on the input content.<br>The black square is the query pixel. More examples are presented in Appendix A.</caption>",
            "id": 115,
            "page": 9,
            "text": "Figure 6: Attention probabilities for a model with 6 layers (rows) and 9 heads (columns) using\nlearned relative positional encoding and content-content based attention. Attention maps are aver-\naged over 100 test images to display head behavior and remove the dependence on the input content.\nThe black square is the query pixel. More examples are presented in Appendix A."
        },
        {
            "bounding_box": [
                {
                    "x": 1260,
                    "y": 3134
                },
                {
                    "x": 1288,
                    "y": 3134
                },
                {
                    "x": 1288,
                    "y": 3169
                },
                {
                    "x": 1260,
                    "y": 3169
                }
            ],
            "category": "footer",
            "html": "<footer id='116' style='font-size:16px'>9</footer>",
            "id": 116,
            "page": 9,
            "text": "9"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 110
                },
                {
                    "x": 1226,
                    "y": 110
                },
                {
                    "x": 1226,
                    "y": 157
                },
                {
                    "x": 444,
                    "y": 157
                }
            ],
            "category": "header",
            "html": "<header id='117' style='font-size:14px'>Published as a conference paper at ICLR 2020</header>",
            "id": 117,
            "page": 10,
            "text": "Published as a conference paper at ICLR 2020"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 340
                },
                {
                    "x": 885,
                    "y": 340
                },
                {
                    "x": 885,
                    "y": 394
                },
                {
                    "x": 445,
                    "y": 394
                }
            ],
            "category": "paragraph",
            "html": "<p id='118' style='font-size:22px'>5 RELATED WORK</p>",
            "id": 118,
            "page": 10,
            "text": "5 RELATED WORK"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 444
                },
                {
                    "x": 2086,
                    "y": 444
                },
                {
                    "x": 2086,
                    "y": 492
                },
                {
                    "x": 443,
                    "y": 492
                }
            ],
            "category": "paragraph",
            "html": "<p id='119' style='font-size:14px'>In this section, we review the known differences and similarities between CNNs and transformers.</p>",
            "id": 119,
            "page": 10,
            "text": "In this section, we review the known differences and similarities between CNNs and transformers."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 515
                },
                {
                    "x": 2108,
                    "y": 515
                },
                {
                    "x": 2108,
                    "y": 883
                },
                {
                    "x": 441,
                    "y": 883
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='120' style='font-size:16px'>The use of CNN networks for text-at word level (Gehring et al., 2017) or character level (Kim,<br>2014)-is more seldom than transformers (or RNN). Transformers and convolutional models have<br>been extensively compared empirically on tasks of Natural Language Processing and Neural Ma-<br>chine Translation. It was observed that transformers have a competitive advantage over convolu-<br>tional model applied to text (Vaswani et al., 2017). It is only recently that Bello et al. (2019);<br>Ramachandran et al. (2019) used transformers on images and showed that they achieve similar ac-<br>curacy as ResNets. However, their comparison only covers performance and number of parameters<br>and FLOPS but not expressive power.</p>",
            "id": 120,
            "page": 10,
            "text": "The use of CNN networks for text-at word level (Gehring et al., 2017) or character level (Kim,\n2014)-is more seldom than transformers (or RNN). Transformers and convolutional models have\nbeen extensively compared empirically on tasks of Natural Language Processing and Neural Ma-\nchine Translation. It was observed that transformers have a competitive advantage over convolu-\ntional model applied to text (Vaswani et al., 2017). It is only recently that Bello et al. (2019);\nRamachandran et al. (2019) used transformers on images and showed that they achieve similar ac-\ncuracy as ResNets. However, their comparison only covers performance and number of parameters\nand FLOPS but not expressive power."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 905
                },
                {
                    "x": 2109,
                    "y": 905
                },
                {
                    "x": 2109,
                    "y": 1182
                },
                {
                    "x": 441,
                    "y": 1182
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='121' style='font-size:18px'>Beyond performance and computational-cost comparisons of transformers and CNN, the study of<br>expressiveness of these architectures has focused on their ability to capture long-term dependencies<br>(Dai et al., 2019). Another interesting line of research has demonstrated that transformers are Turing-<br>complete (Dehghani et al., 2018; Perez et al., 2019), which is an important theoretical result but is<br>not informative for practitioners. To the best of our knowledge, we are the first to show that the class<br>of functions expressed by a layer of self-attention encloses all convolutional filters.</p>",
            "id": 121,
            "page": 10,
            "text": "Beyond performance and computational-cost comparisons of transformers and CNN, the study of\nexpressiveness of these architectures has focused on their ability to capture long-term dependencies\n(Dai et al., 2019). Another interesting line of research has demonstrated that transformers are Turing-\ncomplete (Dehghani et al., 2018; Perez et al., 2019), which is an important theoretical result but is\nnot informative for practitioners. To the best of our knowledge, we are the first to show that the class\nof functions expressed by a layer of self-attention encloses all convolutional filters."
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 1205
                },
                {
                    "x": 2107,
                    "y": 1205
                },
                {
                    "x": 2107,
                    "y": 1666
                },
                {
                    "x": 440,
                    "y": 1666
                }
            ],
            "category": "paragraph",
            "html": "<p id='122' style='font-size:16px'>The closest work in bridging the gap between attention and convolution is due to Andreoli<br>(2019). They cast attention and convolution into a unified framework leveraging tensor outer-<br>product. In this framework, the receptive field of a convolution is represented by a \"basis\" tensor<br>A E RKxKxH x W xHxW For instance, the receptive field of a classical K x K convolutional<br>·<br>kernel would be encoded by A△,q,k = 1{k - q = △ } for △ E △K. The author distinguishes<br>this index-based convolution with content-based convolution where A is computed from the value<br>of the input, e.g., using a key/query dot-product attention. Our work moves further and presents<br>sufficient conditions for relative positional encoding injected into the input content (as done in prac-<br>tice) to allow content-based convolution to express any index-based convolution. We further show<br>experimentally that such behavior is learned in practice.</p>",
            "id": 122,
            "page": 10,
            "text": "The closest work in bridging the gap between attention and convolution is due to Andreoli\n(2019). They cast attention and convolution into a unified framework leveraging tensor outer-\nproduct. In this framework, the receptive field of a convolution is represented by a \"basis\" tensor\nA E RKxKxH x W xHxW For instance, the receptive field of a classical K x K convolutional\n·\nkernel would be encoded by A△,q,k = 1{k - q = △ } for △ E △K. The author distinguishes\nthis index-based convolution with content-based convolution where A is computed from the value\nof the input, e.g., using a key/query dot-product attention. Our work moves further and presents\nsufficient conditions for relative positional encoding injected into the input content (as done in prac-\ntice) to allow content-based convolution to express any index-based convolution. We further show\nexperimentally that such behavior is learned in practice."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 1732
                },
                {
                    "x": 819,
                    "y": 1732
                },
                {
                    "x": 819,
                    "y": 1782
                },
                {
                    "x": 445,
                    "y": 1782
                }
            ],
            "category": "paragraph",
            "html": "<p id='123' style='font-size:22px'>6 CONCLUSION</p>",
            "id": 123,
            "page": 10,
            "text": "6 CONCLUSION"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1833
                },
                {
                    "x": 2108,
                    "y": 1833
                },
                {
                    "x": 2108,
                    "y": 2160
                },
                {
                    "x": 441,
                    "y": 2160
                }
            ],
            "category": "paragraph",
            "html": "<p id='124' style='font-size:16px'>We showed that self-attention layers applied to images can express any convolutional layer (given<br>sufficiently many heads) and that fully-attentional models learn to combine local behavior (similar<br>to convolution) and global attention based on input content. More generally, fully-attentional mod-<br>els seem to learn a generalization of CNNs where the kernel pattern is learned at the same time as<br>the filters-similar to deformable convolutions (Dai et al., 2017; Zampieri, 2019). Interesting di-<br>rections for future work include translating existing insights from the rich CNNs literature back to<br>transformers on various data modalities, including images, text and time series.</p>",
            "id": 124,
            "page": 10,
            "text": "We showed that self-attention layers applied to images can express any convolutional layer (given\nsufficiently many heads) and that fully-attentional models learn to combine local behavior (similar\nto convolution) and global attention based on input content. More generally, fully-attentional mod-\nels seem to learn a generalization of CNNs where the kernel pattern is learned at the same time as\nthe filters-similar to deformable convolutions (Dai et al., 2017; Zampieri, 2019). Interesting di-\nrections for future work include translating existing insights from the rich CNNs literature back to\ntransformers on various data modalities, including images, text and time series."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 2210
                },
                {
                    "x": 838,
                    "y": 2210
                },
                {
                    "x": 838,
                    "y": 2254
                },
                {
                    "x": 445,
                    "y": 2254
                }
            ],
            "category": "paragraph",
            "html": "<p id='125' style='font-size:20px'>ACKNOWLEDGMENTS</p>",
            "id": 125,
            "page": 10,
            "text": "ACKNOWLEDGMENTS"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2286
                },
                {
                    "x": 2108,
                    "y": 2286
                },
                {
                    "x": 2108,
                    "y": 2428
                },
                {
                    "x": 442,
                    "y": 2428
                }
            ],
            "category": "paragraph",
            "html": "<p id='126' style='font-size:18px'>Jean-Baptiste Cordonnier is thankful to the Swiss Data Science Center (SDSC) for funding this<br>work. Andreas Loukas was supported by the Swiss National Science Foundation (project Deep<br>Learning for Graph Structured Data, grant number PZ00P2 179981).</p>",
            "id": 126,
            "page": 10,
            "text": "Jean-Baptiste Cordonnier is thankful to the Swiss Data Science Center (SDSC) for funding this\nwork. Andreas Loukas was supported by the Swiss National Science Foundation (project Deep\nLearning for Graph Structured Data, grant number PZ00P2 179981)."
        },
        {
            "bounding_box": [
                {
                    "x": 1253,
                    "y": 3131
                },
                {
                    "x": 1300,
                    "y": 3131
                },
                {
                    "x": 1300,
                    "y": 3172
                },
                {
                    "x": 1253,
                    "y": 3172
                }
            ],
            "category": "footer",
            "html": "<footer id='127' style='font-size:14px'>10</footer>",
            "id": 127,
            "page": 10,
            "text": "10"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 112
                },
                {
                    "x": 1225,
                    "y": 112
                },
                {
                    "x": 1225,
                    "y": 156
                },
                {
                    "x": 444,
                    "y": 156
                }
            ],
            "category": "header",
            "html": "<header id='128' style='font-size:14px'>Published as a conference paper at ICLR 2020</header>",
            "id": 128,
            "page": 11,
            "text": "Published as a conference paper at ICLR 2020"
        },
        {
            "bounding_box": [
                {
                    "x": 446,
                    "y": 342
                },
                {
                    "x": 734,
                    "y": 342
                },
                {
                    "x": 734,
                    "y": 392
                },
                {
                    "x": 446,
                    "y": 392
                }
            ],
            "category": "paragraph",
            "html": "<p id='129' style='font-size:20px'>REFERENCES</p>",
            "id": 129,
            "page": 11,
            "text": "REFERENCES"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 420
                },
                {
                    "x": 2108,
                    "y": 420
                },
                {
                    "x": 2108,
                    "y": 788
                },
                {
                    "x": 442,
                    "y": 788
                }
            ],
            "category": "paragraph",
            "html": "<p id='130' style='font-size:20px'>Martin Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S.<br>Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew<br>Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath<br>Kudlur, Josh Levenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah,<br>Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vin-<br>cent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Watten-<br>berg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning<br>on heterogeneous systems, 2015. Software available from tensorflow.org.</p>",
            "id": 130,
            "page": 11,
            "text": "Martin Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S.\nCorrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew\nHarp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath\nKudlur, Josh Levenberg, Dan Mane, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah,\nMike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vin-\ncent Vanhoucke, Vijay Vasudevan, Fernanda Viegas, Oriol Vinyals, Pete Warden, Martin Watten-\nberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning\non heterogeneous systems, 2015. Software available from tensorflow.org."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 820
                },
                {
                    "x": 2107,
                    "y": 820
                },
                {
                    "x": 2107,
                    "y": 911
                },
                {
                    "x": 443,
                    "y": 911
                }
            ],
            "category": "paragraph",
            "html": "<p id='131' style='font-size:18px'>Jean-Marc Andreoli. Convolution, attention and structure embedding. NeurIPS 2019 workshop on<br>Graph Representation Learning, Dec 13, 2019, Vancouver, BC, Canada, 2019.</p>",
            "id": 131,
            "page": 11,
            "text": "Jean-Marc Andreoli. Convolution, attention and structure embedding. NeurIPS 2019 workshop on\nGraph Representation Learning, Dec 13, 2019, Vancouver, BC, Canada, 2019."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 943
                },
                {
                    "x": 2105,
                    "y": 943
                },
                {
                    "x": 2105,
                    "y": 1080
                },
                {
                    "x": 442,
                    "y": 1080
                }
            ],
            "category": "paragraph",
            "html": "<p id='132' style='font-size:18px'>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly<br>learning to align and translate. In 3rd International Conference on Learning Representations,<br>ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.</p>",
            "id": 132,
            "page": 11,
            "text": "Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. In 3rd International Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1113
                },
                {
                    "x": 2106,
                    "y": 1113
                },
                {
                    "x": 2106,
                    "y": 1205
                },
                {
                    "x": 442,
                    "y": 1205
                }
            ],
            "category": "paragraph",
            "html": "<p id='133' style='font-size:18px'>Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens, and Quoc V. Le. Attention Augmented<br>Convolutional Networks. arXiv:1904.09925 [cs], April 2019.</p>",
            "id": 133,
            "page": 11,
            "text": "Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens, and Quoc V. Le. Attention Augmented\nConvolutional Networks. arXiv:1904.09925 [cs], April 2019."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 1238
                },
                {
                    "x": 2106,
                    "y": 1238
                },
                {
                    "x": 2106,
                    "y": 1330
                },
                {
                    "x": 444,
                    "y": 1330
                }
            ],
            "category": "paragraph",
            "html": "<p id='134' style='font-size:18px'>Jifeng Dai, Haozhi Qi, Yuwen Xiong, YiLi, Guodong Zhang, Han Hu, and Yichen Wei. Deformable<br>convolutional networks. CoRR, abs/1703.06211, 2017.</p>",
            "id": 134,
            "page": 11,
            "text": "Jifeng Dai, Haozhi Qi, Yuwen Xiong, YiLi, Guodong Zhang, Han Hu, and Yichen Wei. Deformable\nconvolutional networks. CoRR, abs/1703.06211, 2017."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1362
                },
                {
                    "x": 2105,
                    "y": 1362
                },
                {
                    "x": 2105,
                    "y": 1498
                },
                {
                    "x": 442,
                    "y": 1498
                }
            ],
            "category": "paragraph",
            "html": "<p id='135' style='font-size:18px'>Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc V. Le, and Ruslan Salakhut-<br>dinov. Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context. CoRR,<br>abs/1901.02860, 2019.</p>",
            "id": 135,
            "page": 11,
            "text": "Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc V. Le, and Ruslan Salakhut-\ndinov. Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context. CoRR,\nabs/1901.02860, 2019."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1532
                },
                {
                    "x": 2104,
                    "y": 1532
                },
                {
                    "x": 2104,
                    "y": 1623
                },
                {
                    "x": 443,
                    "y": 1623
                }
            ],
            "category": "paragraph",
            "html": "<p id='136' style='font-size:18px'>Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal<br>transformers. CoRR, abs/1807.03819, 2018.</p>",
            "id": 136,
            "page": 11,
            "text": "Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal\ntransformers. CoRR, abs/1807.03819, 2018."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1657
                },
                {
                    "x": 2105,
                    "y": 1657
                },
                {
                    "x": 2105,
                    "y": 1749
                },
                {
                    "x": 443,
                    "y": 1749
                }
            ],
            "category": "paragraph",
            "html": "<p id='137' style='font-size:18px'>Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep<br>bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018.</p>",
            "id": 137,
            "page": 11,
            "text": "Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep\nbidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1783
                },
                {
                    "x": 2106,
                    "y": 1783
                },
                {
                    "x": 2106,
                    "y": 1873
                },
                {
                    "x": 443,
                    "y": 1873
                }
            ],
            "category": "paragraph",
            "html": "<p id='138' style='font-size:18px'>Jean-Yves Franceschi, Aymeric Dieuleveut, and Martin Jaggi. Unsupervised scalable representation<br>learning for multivariate time series. In NeurIPS 2019, 2019.</p>",
            "id": 138,
            "page": 11,
            "text": "Jean-Yves Franceschi, Aymeric Dieuleveut, and Martin Jaggi. Unsupervised scalable representation\nlearning for multivariate time series. In NeurIPS 2019, 2019."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1907
                },
                {
                    "x": 2106,
                    "y": 1907
                },
                {
                    "x": 2106,
                    "y": 1998
                },
                {
                    "x": 443,
                    "y": 1998
                }
            ],
            "category": "paragraph",
            "html": "<p id='139' style='font-size:18px'>Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional<br>sequence to sequence learning. CoRR, abs/1 705.03122, 2017.</p>",
            "id": 139,
            "page": 11,
            "text": "Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional\nsequence to sequence learning. CoRR, abs/1 705.03122, 2017."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2032
                },
                {
                    "x": 2105,
                    "y": 2032
                },
                {
                    "x": 2105,
                    "y": 2121
                },
                {
                    "x": 443,
                    "y": 2121
                }
            ],
            "category": "paragraph",
            "html": "<p id='140' style='font-size:22px'>Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-<br>nition. CoRR, abs/1512.03385, 2015.</p>",
            "id": 140,
            "page": 11,
            "text": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. CoRR, abs/1512.03385, 2015."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2157
                },
                {
                    "x": 2107,
                    "y": 2157
                },
                {
                    "x": 2107,
                    "y": 2246
                },
                {
                    "x": 442,
                    "y": 2246
                }
            ],
            "category": "paragraph",
            "html": "<p id='141' style='font-size:20px'>Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):<br>1735-1780, 1997.</p>",
            "id": 141,
            "page": 11,
            "text": "Sepp Hochreiter and Jurgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):\n1735-1780, 1997."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2278
                },
                {
                    "x": 2105,
                    "y": 2278
                },
                {
                    "x": 2105,
                    "y": 2416
                },
                {
                    "x": 442,
                    "y": 2416
                }
            ],
            "category": "paragraph",
            "html": "<p id='142' style='font-size:18px'>Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In 2018 IEEE Conference on<br>Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22,<br>2018, pp. 7132-7141, 2018.</p>",
            "id": 142,
            "page": 11,
            "text": "Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In 2018 IEEE Conference on\nComputer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22,\n2018, pp. 7132-7141, 2018."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2449
                },
                {
                    "x": 2106,
                    "y": 2449
                },
                {
                    "x": 2106,
                    "y": 2542
                },
                {
                    "x": 443,
                    "y": 2542
                }
            ],
            "category": "paragraph",
            "html": "<p id='143' style='font-size:16px'>Jrn-Henrik Jacobsen, Arnold W.M. Smeulders, and Edouard Oyallon. i-revnet: Deep invertible<br>networks. In International Conference on Learning Representations, 2018.</p>",
            "id": 143,
            "page": 11,
            "text": "Jrn-Henrik Jacobsen, Arnold W.M. Smeulders, and Edouard Oyallon. i-revnet: Deep invertible\nnetworks. In International Conference on Learning Representations, 2018."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 2574
                },
                {
                    "x": 2108,
                    "y": 2574
                },
                {
                    "x": 2108,
                    "y": 2756
                },
                {
                    "x": 445,
                    "y": 2756
                }
            ],
            "category": "paragraph",
            "html": "<p id='144' style='font-size:18px'>Yoon Kim. Convolutional neural networks for sentence classification. In Proceedings of the<br>2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1746-<br>1751, Doha, Qatar, October 2014. Association for Computational Linguistics. doi: 10.3115/v1/<br>D14-1181.</p>",
            "id": 144,
            "page": 11,
            "text": "Yoon Kim. Convolutional neural networks for sentence classification. In Proceedings of the\n2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1746-\n1751, Doha, Qatar, October 2014. Association for Computational Linguistics. doi: 10.3115/v1/\nD14-1181."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 2789
                },
                {
                    "x": 2104,
                    "y": 2789
                },
                {
                    "x": 2104,
                    "y": 2879
                },
                {
                    "x": 444,
                    "y": 2879
                }
            ],
            "category": "paragraph",
            "html": "<p id='145' style='font-size:16px'>Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced re-<br>search).</p>",
            "id": 145,
            "page": 11,
            "text": "Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-10 (canadian institute for advanced re-\nsearch)."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2916
                },
                {
                    "x": 2108,
                    "y": 2916
                },
                {
                    "x": 2108,
                    "y": 3053
                },
                {
                    "x": 443,
                    "y": 3053
                }
            ],
            "category": "paragraph",
            "html": "<p id='146' style='font-size:18px'>Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,<br>Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in<br>PyTorch. In NIPS Autodiff Workshop, 2017.</p>",
            "id": 146,
            "page": 11,
            "text": "Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,\nZeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in\nPyTorch. In NIPS Autodiff Workshop, 2017."
        },
        {
            "bounding_box": [
                {
                    "x": 1252,
                    "y": 3133
                },
                {
                    "x": 1297,
                    "y": 3133
                },
                {
                    "x": 1297,
                    "y": 3172
                },
                {
                    "x": 1252,
                    "y": 3172
                }
            ],
            "category": "footer",
            "html": "<footer id='147' style='font-size:14px'>11</footer>",
            "id": 147,
            "page": 11,
            "text": "11"
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 111
                },
                {
                    "x": 1226,
                    "y": 111
                },
                {
                    "x": 1226,
                    "y": 157
                },
                {
                    "x": 444,
                    "y": 157
                }
            ],
            "category": "header",
            "html": "<header id='148' style='font-size:14px'>Published as a conference paper at ICLR 2020</header>",
            "id": 148,
            "page": 12,
            "text": "Published as a conference paper at ICLR 2020"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 345
                },
                {
                    "x": 2106,
                    "y": 345
                },
                {
                    "x": 2106,
                    "y": 437
                },
                {
                    "x": 442,
                    "y": 437
                }
            ],
            "category": "paragraph",
            "html": "<p id='149' style='font-size:18px'>Jorge P�rez, Javier Marinkovic, and Pablo Barcelo. On the turing completeness of modern neural<br>network architectures. CoRR, abs/1901.03429, 2019.</p>",
            "id": 149,
            "page": 12,
            "text": "Jorge P�rez, Javier Marinkovic, and Pablo Barcelo. On the turing completeness of modern neural\nnetwork architectures. CoRR, abs/1901.03429, 2019."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 471
                },
                {
                    "x": 2108,
                    "y": 471
                },
                {
                    "x": 2108,
                    "y": 565
                },
                {
                    "x": 443,
                    "y": 565
                }
            ],
            "category": "paragraph",
            "html": "<p id='150' style='font-size:20px'>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language<br>models are unsupervised multitask learners. 2018.</p>",
            "id": 150,
            "page": 12,
            "text": "Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. 2018."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 595
                },
                {
                    "x": 2104,
                    "y": 595
                },
                {
                    "x": 2104,
                    "y": 687
                },
                {
                    "x": 442,
                    "y": 687
                }
            ],
            "category": "paragraph",
            "html": "<p id='151' style='font-size:20px'>Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jonathon<br>Shlens. Stand-alone self-attention in vision models. CoRR, abs/1906.05909, 2019.</p>",
            "id": 151,
            "page": 12,
            "text": "Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jonathon\nShlens. Stand-alone self-attention in vision models. CoRR, abs/1906.05909, 2019."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 721
                },
                {
                    "x": 2107,
                    "y": 721
                },
                {
                    "x": 2107,
                    "y": 858
                },
                {
                    "x": 443,
                    "y": 858
                }
            ],
            "category": "paragraph",
            "html": "<p id='152' style='font-size:20px'>Aron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alexander<br>Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative<br>model for raw audio. arXiv preprint arXiv:1609.03499, 2016.</p>",
            "id": 152,
            "page": 12,
            "text": "Aron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alexander\nGraves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative\nmodel for raw audio. arXiv preprint arXiv:1609.03499, 2016."
        },
        {
            "bounding_box": [
                {
                    "x": 444,
                    "y": 890
                },
                {
                    "x": 2104,
                    "y": 890
                },
                {
                    "x": 2104,
                    "y": 983
                },
                {
                    "x": 444,
                    "y": 983
                }
            ],
            "category": "paragraph",
            "html": "<p id='153' style='font-size:18px'>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,<br>Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. CoRR, abs/1706.03762, 2017.</p>",
            "id": 153,
            "page": 12,
            "text": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. CoRR, abs/1706.03762, 2017."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1014
                },
                {
                    "x": 2107,
                    "y": 1014
                },
                {
                    "x": 2107,
                    "y": 1153
                },
                {
                    "x": 442,
                    "y": 1153
                }
            ],
            "category": "paragraph",
            "html": "<p id='154' style='font-size:18px'>Xiaolong Wang, Ross B. Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In<br>2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City,<br>UT, USA, June 18-22, 2018, pp. 7794-7803, 2018.</p>",
            "id": 154,
            "page": 12,
            "text": "Xiaolong Wang, Ross B. Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In\n2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City,\nUT, USA, June 18-22, 2018, pp. 7794-7803, 2018."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1184
                },
                {
                    "x": 2106,
                    "y": 1184
                },
                {
                    "x": 2106,
                    "y": 1321
                },
                {
                    "x": 441,
                    "y": 1321
                }
            ],
            "category": "paragraph",
            "html": "<p id='155' style='font-size:22px'>Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V.<br>Le. Xlnet: Generalized autoregressive pretraining for language understanding. CoRR,<br>abs/1906.08237, 2019.</p>",
            "id": 155,
            "page": 12,
            "text": "Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell, Ruslan Salakhutdinov, and Quoc V.\nLe. Xlnet: Generalized autoregressive pretraining for language understanding. CoRR,\nabs/1906.08237, 2019."
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 1354
                },
                {
                    "x": 2104,
                    "y": 1354
                },
                {
                    "x": 2104,
                    "y": 1446
                },
                {
                    "x": 440,
                    "y": 1446
                }
            ],
            "category": "paragraph",
            "html": "<p id='156' style='font-size:22px'>Luca Zampieri. Geometric deep learning for volumetric computational fluid dynamics. pp. 67,<br>2019.</p>",
            "id": 156,
            "page": 12,
            "text": "Luca Zampieri. Geometric deep learning for volumetric computational fluid dynamics. pp. 67,\n2019."
        },
        {
            "bounding_box": [
                {
                    "x": 1253,
                    "y": 3132
                },
                {
                    "x": 1300,
                    "y": 3132
                },
                {
                    "x": 1300,
                    "y": 3172
                },
                {
                    "x": 1253,
                    "y": 3172
                }
            ],
            "category": "footer",
            "html": "<footer id='157' style='font-size:16px'>12</footer>",
            "id": 157,
            "page": 12,
            "text": "12"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 110
                },
                {
                    "x": 1226,
                    "y": 110
                },
                {
                    "x": 1226,
                    "y": 157
                },
                {
                    "x": 445,
                    "y": 157
                }
            ],
            "category": "header",
            "html": "<header id='158' style='font-size:16px'>Published as a conference paper at ICLR 2020</header>",
            "id": 158,
            "page": 13,
            "text": "Published as a conference paper at ICLR 2020"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 332
                },
                {
                    "x": 775,
                    "y": 332
                },
                {
                    "x": 775,
                    "y": 404
                },
                {
                    "x": 445,
                    "y": 404
                }
            ],
            "category": "paragraph",
            "html": "<p id='159' style='font-size:22px'>APPENDIX</p>",
            "id": 159,
            "page": 13,
            "text": "APPENDIX"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 476
                },
                {
                    "x": 1691,
                    "y": 476
                },
                {
                    "x": 1691,
                    "y": 529
                },
                {
                    "x": 445,
                    "y": 529
                }
            ],
            "category": "paragraph",
            "html": "<p id='160' style='font-size:20px'>A MORE EXAMPLES WITH CONTENT-BASED ATTENTION</p>",
            "id": 160,
            "page": 13,
            "text": "A MORE EXAMPLES WITH CONTENT-BASED ATTENTION"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 585
                },
                {
                    "x": 2107,
                    "y": 585
                },
                {
                    "x": 2107,
                    "y": 724
                },
                {
                    "x": 442,
                    "y": 724
                }
            ],
            "category": "paragraph",
            "html": "<p id='161' style='font-size:18px'>We present more examples of attention probabilities computed by self-attention model. Figure 7<br>shows average attention at a different query pixel than Figure 6. Figures 8 to 10 display attention for<br>single images.</p>",
            "id": 161,
            "page": 13,
            "text": "We present more examples of attention probabilities computed by self-attention model. Figure 7\nshows average attention at a different query pixel than Figure 6. Figures 8 to 10 display attention for\nsingle images."
        },
        {
            "bounding_box": [
                {
                    "x": 608,
                    "y": 735
                },
                {
                    "x": 1933,
                    "y": 735
                },
                {
                    "x": 1933,
                    "y": 1621
                },
                {
                    "x": 608,
                    "y": 1621
                }
            ],
            "category": "figure",
            "html": "<figure><img id='162' style='font-size:14px' alt=\"□ □\nT\nlayer\n2\nlayer\n□ □ □ □ □ □\n3\nlayer\n□ □ □ 0 0 □ □ □\n4\nlayer\n□\n5\nlayer\n0\n6\nlayer\" data-coord=\"top-left:(608,735); bottom-right:(1933,1621)\" /></figure>",
            "id": 162,
            "page": 13,
            "text": "□ □\nT\nlayer\n2\nlayer\n□ □ □ □ □ □\n3\nlayer\n□ □ □ 0 0 □ □ □\n4\nlayer\n□\n5\nlayer\n0\n6\nlayer"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1638
                },
                {
                    "x": 2110,
                    "y": 1638
                },
                {
                    "x": 2110,
                    "y": 1779
                },
                {
                    "x": 442,
                    "y": 1779
                }
            ],
            "category": "caption",
            "html": "<br><caption id='163' style='font-size:18px'>Figure 7: Attention probabilities for a model with 6 layers (rows) and 9 heads (columns) using<br>learned relative positional encoding and content-content attention. We present the average of 100<br>test images. The black square is the query pixel.</caption>",
            "id": 163,
            "page": 13,
            "text": "Figure 7: Attention probabilities for a model with 6 layers (rows) and 9 heads (columns) using\nlearned relative positional encoding and content-content attention. We present the average of 100\ntest images. The black square is the query pixel."
        },
        {
            "bounding_box": [
                {
                    "x": 605,
                    "y": 1847
                },
                {
                    "x": 1934,
                    "y": 1847
                },
                {
                    "x": 1934,
                    "y": 2884
                },
                {
                    "x": 605,
                    "y": 2884
                }
            ],
            "category": "figure",
            "html": "<figure><img id='164' style='font-size:14px' alt=\"original\nI layer\n2\nlayer\n□ □ □\n3\nlayer\n4 □ □ □ 0\nlayer\n5\nlayer\n□\n6\nlayer\" data-coord=\"top-left:(605,1847); bottom-right:(1934,2884)\" /></figure>",
            "id": 164,
            "page": 13,
            "text": "original\nI layer\n2\nlayer\n□ □ □\n3\nlayer\n4 □ □ □ 0\nlayer\n5\nlayer\n□\n6\nlayer"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2901
                },
                {
                    "x": 2108,
                    "y": 2901
                },
                {
                    "x": 2108,
                    "y": 3040
                },
                {
                    "x": 443,
                    "y": 3040
                }
            ],
            "category": "caption",
            "html": "<br><caption id='165' style='font-size:18px'>Figure 8: Attention probabilities for a model with 6 layers (rows) and 9 heads (columns) using<br>learned relative positional encoding and content-content based attention. The query pixel (black<br>square) is on the frog head.</caption>",
            "id": 165,
            "page": 13,
            "text": "Figure 8: Attention probabilities for a model with 6 layers (rows) and 9 heads (columns) using\nlearned relative positional encoding and content-content based attention. The query pixel (black\nsquare) is on the frog head."
        },
        {
            "bounding_box": [
                {
                    "x": 1253,
                    "y": 3132
                },
                {
                    "x": 1299,
                    "y": 3132
                },
                {
                    "x": 1299,
                    "y": 3173
                },
                {
                    "x": 1253,
                    "y": 3173
                }
            ],
            "category": "footer",
            "html": "<footer id='166' style='font-size:18px'>13</footer>",
            "id": 166,
            "page": 13,
            "text": "13"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 111
                },
                {
                    "x": 1226,
                    "y": 111
                },
                {
                    "x": 1226,
                    "y": 157
                },
                {
                    "x": 445,
                    "y": 157
                }
            ],
            "category": "header",
            "html": "<header id='167' style='font-size:18px'>Published as a conference paper at ICLR 2020</header>",
            "id": 167,
            "page": 14,
            "text": "Published as a conference paper at ICLR 2020"
        },
        {
            "bounding_box": [
                {
                    "x": 602,
                    "y": 330
                },
                {
                    "x": 1937,
                    "y": 330
                },
                {
                    "x": 1937,
                    "y": 1372
                },
                {
                    "x": 602,
                    "y": 1372
                }
            ],
            "category": "figure",
            "html": "<figure><img id='168' style='font-size:14px' alt=\"original\nlayer1\n2\nlayer\n3\n□\nlayer\n4\nlayer\n5\nlayer\n6\n□\nlayer\" data-coord=\"top-left:(602,330); bottom-right:(1937,1372)\" /></figure>",
            "id": 168,
            "page": 14,
            "text": "original\nlayer1\n2\nlayer\n3\n□\nlayer\n4\nlayer\n5\nlayer\n6\n□\nlayer"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1384
                },
                {
                    "x": 2108,
                    "y": 1384
                },
                {
                    "x": 2108,
                    "y": 1523
                },
                {
                    "x": 441,
                    "y": 1523
                }
            ],
            "category": "caption",
            "html": "<br><caption id='169' style='font-size:20px'>Figure 9: Attention probabilities for a model with 6 layers (rows) and 9 heads (columns) using<br>learned relative positional encoding and content-content based attention. The query pixel (black<br>square) is on the horse head.</caption>",
            "id": 169,
            "page": 14,
            "text": "Figure 9: Attention probabilities for a model with 6 layers (rows) and 9 heads (columns) using\nlearned relative positional encoding and content-content based attention. The query pixel (black\nsquare) is on the horse head."
        },
        {
            "bounding_box": [
                {
                    "x": 605,
                    "y": 1623
                },
                {
                    "x": 1937,
                    "y": 1623
                },
                {
                    "x": 1937,
                    "y": 2658
                },
                {
                    "x": 605,
                    "y": 2658
                }
            ],
            "category": "figure",
            "html": "<figure><img id='170' style='font-size:14px' alt=\"original\n1\nlayer\n2\nlayer\n□ □ □\n3\nlayer\n□ □ □\n4\nlayer\n5\nlayer\n□ □ □ □\n6\nlayer\" data-coord=\"top-left:(605,1623); bottom-right:(1937,2658)\" /></figure>",
            "id": 170,
            "page": 14,
            "text": "original\n1\nlayer\n2\nlayer\n□ □ □\n3\nlayer\n□ □ □\n4\nlayer\n5\nlayer\n□ □ □ □\n6\nlayer"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2671
                },
                {
                    "x": 2108,
                    "y": 2671
                },
                {
                    "x": 2108,
                    "y": 2814
                },
                {
                    "x": 442,
                    "y": 2814
                }
            ],
            "category": "caption",
            "html": "<br><caption id='171' style='font-size:22px'>Figure 10: Attention probabilities for a model with 6 layers (rows) and 9 heads (columns) using<br>learned relative positional encoding and content-content based attention. The query pixel (black<br>square) is on the building in the background.</caption>",
            "id": 171,
            "page": 14,
            "text": "Figure 10: Attention probabilities for a model with 6 layers (rows) and 9 heads (columns) using\nlearned relative positional encoding and content-content based attention. The query pixel (black\nsquare) is on the building in the background."
        },
        {
            "bounding_box": [
                {
                    "x": 1253,
                    "y": 3133
                },
                {
                    "x": 1300,
                    "y": 3133
                },
                {
                    "x": 1300,
                    "y": 3173
                },
                {
                    "x": 1253,
                    "y": 3173
                }
            ],
            "category": "footer",
            "html": "<footer id='172' style='font-size:16px'>14</footer>",
            "id": 172,
            "page": 14,
            "text": "14"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 112
                },
                {
                    "x": 1225,
                    "y": 112
                },
                {
                    "x": 1225,
                    "y": 156
                },
                {
                    "x": 445,
                    "y": 156
                }
            ],
            "category": "header",
            "html": "<header id='173' style='font-size:14px'>Published as a conference paper at ICLR 2020</header>",
            "id": 173,
            "page": 15,
            "text": "Published as a conference paper at ICLR 2020"
        },
        {
            "bounding_box": [
                {
                    "x": 446,
                    "y": 342
                },
                {
                    "x": 1601,
                    "y": 342
                },
                {
                    "x": 1601,
                    "y": 393
                },
                {
                    "x": 446,
                    "y": 393
                }
            ],
            "category": "caption",
            "html": "<caption id='174' style='font-size:20px'>B HYPER-PARAMETERS USED IN OUR EXPERIMENTS</caption>",
            "id": 174,
            "page": 15,
            "text": "B HYPER-PARAMETERS USED IN OUR EXPERIMENTS"
        },
        {
            "bounding_box": [
                {
                    "x": 931,
                    "y": 461
                },
                {
                    "x": 1623,
                    "y": 461
                },
                {
                    "x": 1623,
                    "y": 1215
                },
                {
                    "x": 931,
                    "y": 1215
                }
            ],
            "category": "table",
            "html": "<table id='175' style='font-size:18px'><tr><td>Hyper-parameters</td><td></td></tr><tr><td>number of layers</td><td>6</td></tr><tr><td>number of heads</td><td>9</td></tr><tr><td>hidden dimension</td><td>400</td></tr><tr><td>intermediate dimension</td><td>512</td></tr><tr><td>invertible pooling width</td><td>2</td></tr><tr><td>dropout probability</td><td>0.1</td></tr><tr><td>layer normalization epsilon</td><td>10-12</td></tr><tr><td>number of epochs</td><td>300</td></tr><tr><td>batch size</td><td>100</td></tr><tr><td>learning rate</td><td>0.1</td></tr><tr><td>weight decay</td><td>0.0001</td></tr><tr><td>momentum</td><td>0.9</td></tr><tr><td>cosine decay</td><td>V</td></tr><tr><td>linear warm up ratio</td><td>0.05</td></tr></table>",
            "id": 175,
            "page": 15,
            "text": "Hyper-parameters \n number of layers 6\n number of heads 9\n hidden dimension 400\n intermediate dimension 512\n invertible pooling width 2\n dropout probability 0.1\n layer normalization epsilon 10-12\n number of epochs 300\n batch size 100\n learning rate 0.1\n weight decay 0.0001\n momentum 0.9\n cosine decay V\n linear warm up ratio"
        },
        {
            "bounding_box": [
                {
                    "x": 917,
                    "y": 1254
                },
                {
                    "x": 1633,
                    "y": 1254
                },
                {
                    "x": 1633,
                    "y": 1299
                },
                {
                    "x": 917,
                    "y": 1299
                }
            ],
            "category": "caption",
            "html": "<caption id='176' style='font-size:16px'>Table 2: Self-attention network parameters</caption>",
            "id": 176,
            "page": 15,
            "text": "Table 2: Self-attention network parameters"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 1406
                },
                {
                    "x": 1341,
                    "y": 1406
                },
                {
                    "x": 1341,
                    "y": 1457
                },
                {
                    "x": 445,
                    "y": 1457
                }
            ],
            "category": "paragraph",
            "html": "<p id='177' style='font-size:22px'>C POSITIONAL ENCODING REFERENCES</p>",
            "id": 177,
            "page": 15,
            "text": "C POSITIONAL ENCODING REFERENCES"
        },
        {
            "bounding_box": [
                {
                    "x": 633,
                    "y": 1520
                },
                {
                    "x": 1915,
                    "y": 1520
                },
                {
                    "x": 1915,
                    "y": 2070
                },
                {
                    "x": 633,
                    "y": 2070
                }
            ],
            "category": "table",
            "html": "<table id='178' style='font-size:16px'><tr><td rowspan=\"2\">Model</td><td colspan=\"2\">type of positional encoding</td><td rowspan=\"2\">relative</td></tr><tr><td>sinusoids</td><td>learned quadratic</td></tr><tr><td>Vaswani et al. (2017)</td><td>V</td><td></td><td></td></tr><tr><td>Radford et al. (2018) Devlin et al. (2018)</td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td></tr><tr><td>Dai et al. (2019)</td><td></td><td></td><td></td></tr><tr><td>Yang et al. (2019)</td><td></td><td></td><td>V</td></tr><tr><td>Bello et al. (2019)</td><td></td><td></td><td>V</td></tr><tr><td>Ramachandran et al. (2019) Our work</td><td></td><td></td><td>V</td></tr></table>",
            "id": 178,
            "page": 15,
            "text": "Model type of positional encoding relative\n sinusoids learned quadratic\n Vaswani et al. (2017) V  \n Radford et al. (2018) Devlin et al. (2018)   \n    \n Dai et al. (2019)   \n Yang et al. (2019)   V\n Bello et al. (2019)   V\n Ramachandran et al. (2019) Our work"
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2104
                },
                {
                    "x": 2105,
                    "y": 2104
                },
                {
                    "x": 2105,
                    "y": 2203
                },
                {
                    "x": 442,
                    "y": 2203
                }
            ],
            "category": "paragraph",
            "html": "<p id='179' style='font-size:18px'>Table 3: Types of positional encoding used by transformers models applied to text (top) and images<br>(bottom). When multiple encoding types have been tried, we report the one advised by the authors.</p>",
            "id": 179,
            "page": 15,
            "text": "Table 3: Types of positional encoding used by transformers models applied to text (top) and images\n(bottom). When multiple encoding types have been tried, we report the one advised by the authors."
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 2304
                },
                {
                    "x": 1066,
                    "y": 2304
                },
                {
                    "x": 1066,
                    "y": 2357
                },
                {
                    "x": 445,
                    "y": 2357
                }
            ],
            "category": "paragraph",
            "html": "<p id='180' style='font-size:20px'>D GENERALIZED LEMMA 1</p>",
            "id": 180,
            "page": 15,
            "text": "D GENERALIZED LEMMA 1"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2409
                },
                {
                    "x": 2107,
                    "y": 2409
                },
                {
                    "x": 2107,
                    "y": 2546
                },
                {
                    "x": 443,
                    "y": 2546
                }
            ],
            "category": "paragraph",
            "html": "<p id='181' style='font-size:16px'>We present a generalization of Lemma 1 that replaces the necessity of hard attention (to single<br>pixels) by a milder assumption: the attention probabilities should span the grid receptive field. The<br>conditions of this Lemma are still satisfied by Lemma 2, hence Theorem 1 follows.</p>",
            "id": 181,
            "page": 15,
            "text": "We present a generalization of Lemma 1 that replaces the necessity of hard attention (to single\npixels) by a milder assumption: the attention probabilities should span the grid receptive field. The\nconditions of this Lemma are still satisfied by Lemma 2, hence Theorem 1 follows."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 2562
                },
                {
                    "x": 2109,
                    "y": 2562
                },
                {
                    "x": 2109,
                    "y": 2770
                },
                {
                    "x": 442,
                    "y": 2770
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='182' style='font-size:20px'>Lemma 3. Consider a multi-head self-attention layer consisting of Nh ≥ K2 heads, Dh ≥ Dout<br>and letw : [H] x [W] → [HW] be a pixel indexing. Then, for any convolutional layer with a K x<br>K kernel and Dout output channels, there exists {W.(h) }hE[Nh] and Wout MHSA (X) =<br>such that<br>Conv (X) for every X E RWxHxDin [H] x [W], 8<br>if and only if, for all q E</p>",
            "id": 182,
            "page": 15,
            "text": "Lemma 3. Consider a multi-head self-attention layer consisting of Nh ≥ K2 heads, Dh ≥ Dout\nand letw : [H] x [W] → [HW] be a pixel indexing. Then, for any convolutional layer with a K x\nK kernel and Dout output channels, there exists {W.(h) }hE[Nh] and Wout MHSA (X) =\nsuch that\nConv (X) for every X E RWxHxDin [H] x [W], 8\nif and only if, for all q E"
        },
        {
            "bounding_box": [
                {
                    "x": 505,
                    "y": 3007
                },
                {
                    "x": 1476,
                    "y": 3007
                },
                {
                    "x": 1476,
                    "y": 3052
                },
                {
                    "x": 505,
                    "y": 3052
                }
            ],
            "category": "paragraph",
            "html": "<p id='183' style='font-size:14px'>8the vectorization operator vect(·) flattens a matrix into a vector</p>",
            "id": 183,
            "page": 15,
            "text": "8the vectorization operator vect(·) flattens a matrix into a vector"
        },
        {
            "bounding_box": [
                {
                    "x": 1252,
                    "y": 3133
                },
                {
                    "x": 1300,
                    "y": 3133
                },
                {
                    "x": 1300,
                    "y": 3172
                },
                {
                    "x": 1252,
                    "y": 3172
                }
            ],
            "category": "footer",
            "html": "<footer id='184' style='font-size:16px'>15</footer>",
            "id": 184,
            "page": 15,
            "text": "15"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 112
                },
                {
                    "x": 1226,
                    "y": 112
                },
                {
                    "x": 1226,
                    "y": 156
                },
                {
                    "x": 445,
                    "y": 156
                }
            ],
            "category": "header",
            "html": "<header id='185' style='font-size:14px'>Published as a conference paper at ICLR 2020</header>",
            "id": 185,
            "page": 16,
            "text": "Published as a conference paper at ICLR 2020"
        },
        {
            "bounding_box": [
                {
                    "x": 432,
                    "y": 329
                },
                {
                    "x": 2107,
                    "y": 329
                },
                {
                    "x": 2107,
                    "y": 892
                },
                {
                    "x": 432,
                    "y": 892
                }
            ],
            "category": "figure",
            "html": "<figure><img id='186' style='font-size:14px' alt=\"HW\nHW\n(1)\na\nq,w(i)\nNh vect(softmax(A.)\nK2 ew(q+△)\nvect(W △,:,:) vect(W(2))\nEq Aq\nout\nout\nD\n0 0 D\nDin\nDin\nW conv Vconv\nq WSA VSA\nq\" data-coord=\"top-left:(432,329); bottom-right:(2107,892)\" /></figure>",
            "id": 186,
            "page": 16,
            "text": "HW\nHW\n(1)\na\nq,w(i)\nNh vect(softmax(A.)\nK2 ew(q+△)\nvect(W △,:,:) vect(W(2))\nEq Aq\nout\nout\nD\n0 0 D\nDin\nDin\nW conv Vconv\nq WSA VSA\nq"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 924
                },
                {
                    "x": 2107,
                    "y": 924
                },
                {
                    "x": 2107,
                    "y": 1066
                },
                {
                    "x": 440,
                    "y": 1066
                }
            ],
            "category": "caption",
            "html": "<caption id='187' style='font-size:16px'>Figure 11: Factorization of the vectorized weight matrices Vconv and VSA used to compute the<br>output at position q for an input image of dimension H x W. On the left: a convolution of kernel<br>2 x 2, on the right: a self-attention with Nh = 5 heads. Din = 2, Dout = 3 in both cases.</caption>",
            "id": 187,
            "page": 16,
            "text": "Figure 11: Factorization of the vectorized weight matrices Vconv and VSA used to compute the\noutput at position q for an input image of dimension H x W. On the left: a convolution of kernel\n2 x 2, on the right: a self-attention with Nh = 5 heads. Din = 2, Dout = 3 in both cases."
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 1142
                },
                {
                    "x": 2105,
                    "y": 1142
                },
                {
                    "x": 2105,
                    "y": 1235
                },
                {
                    "x": 443,
                    "y": 1235
                }
            ],
            "category": "paragraph",
            "html": "<p id='188' style='font-size:18px'>Proof. Our first step will be to rework the expression of the Multi-Head Self-Attention operator from<br>equation (1) and equation (4) such that the effect of the multiple heads becomes more transparent:</p>",
            "id": 188,
            "page": 16,
            "text": "Proof. Our first step will be to rework the expression of the Multi-Head Self-Attention operator from\nequation (1) and equation (4) such that the effect of the multiple heads becomes more transparent:"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 1387
                },
                {
                    "x": 2107,
                    "y": 1387
                },
                {
                    "x": 2107,
                    "y": 1585
                },
                {
                    "x": 440,
                    "y": 1585
                }
            ],
            "category": "paragraph",
            "html": "<p id='189' style='font-size:18px'>Note that each head's value matrix Wv(h) E RDinxDh each block of the projection matrix Wout<br>and<br>of dimension Dh x Dout are learned. Assuming that Dh ≥ Dout, we can replace each pair of<br>matrices by a learned matrix W(h) for each head. We consider one output pixel of the multi-head<br>self-attention and drop the bias term for simplicity:</p>",
            "id": 189,
            "page": 16,
            "text": "Note that each head's value matrix Wv(h) E RDinxDh each block of the projection matrix Wout\nand\nof dimension Dh x Dout are learned. Assuming that Dh ≥ Dout, we can replace each pair of\nmatrices by a learned matrix W(h) for each head. We consider one output pixel of the multi-head\nself-attention and drop the bias term for simplicity:"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1802
                },
                {
                    "x": 2106,
                    "y": 1802
                },
                {
                    "x": 2106,
                    "y": 1877
                },
                {
                    "x": 441,
                    "y": 1877
                }
            ],
            "category": "paragraph",
            "html": "<p id='190' style='font-size:18px'>(ん) Of )k. We rewrite the output of a convolution at pixel q in the same manner:<br>with a = softmax (A<br>q,k</p>",
            "id": 190,
            "page": 16,
            "text": "(ん) Of )k. We rewrite the output of a convolution at pixel q in the same manner:\nwith a = softmax (A\nq,k"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2033
                },
                {
                    "x": 2106,
                    "y": 2033
                },
                {
                    "x": 2106,
                    "y": 2864
                },
                {
                    "x": 441,
                    "y": 2864
                }
            ],
            "category": "paragraph",
            "html": "<p id='191' style='font-size:18px'>Equality between equations (16) and (17) holds for any input X if and only if the linear transfor-<br>mations for each pair of key/query pixels are equal, i.e. Wconv = Wq,k Vq, k. We vectorize the<br>q,k<br>weight matrices into matrices of dimension Din Dout x HW as Vconv := [vect(Wow)]ke[8]x[8]x[W]<br>q<br>and VSA := [vect( www.)]KE[H]x[W]. Hence, to show that Conv (X) = MHSA(X) for all X, we<br>must show that Vconv = VSA for all q.<br>The matrix Vconv has a restricted support: only the columns associated with a pixel shift △ E �<br>K<br>in the receptive field of pixel q can be non-zero. This leads to the factorization Vconv = Wconv Eq<br>displayed in Figure 11 where Wconv E RDin Dout xK2 RK2xHW<br>and Eq E<br>· Given an ordering of<br>the shifts △ E △ K indexed by j, set (Wconv):,j = vect(W △,:,:) and (Eq)j,: = ew(q+△). On the<br>(h)<br>other hand, we decompose VSA = WSA Aq with (WSA):,h = vect(W (h)) and (Aq)h,i = a<br>q,w(i)·<br>The proofis concluded by showing that row (Eq) 드 row(Aq) is a necessary and sufficient condition<br>for the existence of a WSA such that any Vconv = Wconv Eq can be written as WSA Aq.<br>Sufficient. Given that row(Eq) 드 row(Aq), there exists X E RK2xNh such that Eq = ⌀Aq and a<br>valid decomposition is WSA = Wconv f which gives WSA Aq = Vconv. ·</p>",
            "id": 191,
            "page": 16,
            "text": "Equality between equations (16) and (17) holds for any input X if and only if the linear transfor-\nmations for each pair of key/query pixels are equal, i.e. Wconv = Wq,k Vq, k. We vectorize the\nq,k\nweight matrices into matrices of dimension Din Dout x HW as Vconv := [vect(Wow)]ke[8]x[8]x[W]\nq\nand VSA := [vect( www.)]KE[H]x[W]. Hence, to show that Conv (X) = MHSA(X) for all X, we\nmust show that Vconv = VSA for all q.\nThe matrix Vconv has a restricted support: only the columns associated with a pixel shift △ E �\nK\nin the receptive field of pixel q can be non-zero. This leads to the factorization Vconv = Wconv Eq\ndisplayed in Figure 11 where Wconv E RDin Dout xK2 RK2xHW\nand Eq E\n· Given an ordering of\nthe shifts △ E △ K indexed by j, set (Wconv):,j = vect(W △,:,:) and (Eq)j,: = ew(q+△). On the\n(h)\nother hand, we decompose VSA = WSA Aq with (WSA):,h = vect(W (h)) and (Aq)h,i = a\nq,w(i)·\nThe proofis concluded by showing that row (Eq) 드 row(Aq) is a necessary and sufficient condition\nfor the existence of a WSA such that any Vconv = Wconv Eq can be written as WSA Aq.\nSufficient. Given that row(Eq) 드 row(Aq), there exists X E RK2xNh such that Eq = ⌀Aq and a\nvalid decomposition is WSA = Wconv f which gives WSA Aq = Vconv. ·"
        },
        {
            "bounding_box": [
                {
                    "x": 440,
                    "y": 2877
                },
                {
                    "x": 2103,
                    "y": 2877
                },
                {
                    "x": 2103,
                    "y": 2988
                },
                {
                    "x": 440,
                    "y": 2988
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='192' style='font-size:16px'>T<br>Necessary. Assume there exists x E RHW such that x E row(Eq) and x & row (Aq) and set x<br>to be a row of Vconv Then, WSA Aq ≠ Vconv for any WSA and there is no possible decomposition.<br>q<br>q</p>",
            "id": 192,
            "page": 16,
            "text": "T\nNecessary. Assume there exists x E RHW such that x E row(Eq) and x & row (Aq) and set x\nto be a row of Vconv Then, WSA Aq ≠ Vconv for any WSA and there is no possible decomposition.\nq\nq"
        },
        {
            "bounding_box": [
                {
                    "x": 1252,
                    "y": 3135
                },
                {
                    "x": 1301,
                    "y": 3135
                },
                {
                    "x": 1301,
                    "y": 3172
                },
                {
                    "x": 1252,
                    "y": 3172
                }
            ],
            "category": "footer",
            "html": "<footer id='193' style='font-size:16px'>16</footer>",
            "id": 193,
            "page": 16,
            "text": "16"
        },
        {
            "bounding_box": [
                {
                    "x": 446,
                    "y": 112
                },
                {
                    "x": 1224,
                    "y": 112
                },
                {
                    "x": 1224,
                    "y": 156
                },
                {
                    "x": 446,
                    "y": 156
                }
            ],
            "category": "header",
            "html": "<header id='194' style='font-size:14px'>Published as a conference paper at ICLR 2020</header>",
            "id": 194,
            "page": 17,
            "text": "Published as a conference paper at ICLR 2020"
        },
        {
            "bounding_box": [
                {
                    "x": 446,
                    "y": 343
                },
                {
                    "x": 1638,
                    "y": 343
                },
                {
                    "x": 1638,
                    "y": 393
                },
                {
                    "x": 446,
                    "y": 393
                }
            ],
            "category": "paragraph",
            "html": "<p id='195' style='font-size:22px'>E GENERALIZED QUADRATIC POSITIONAL ENCODING</p>",
            "id": 195,
            "page": 17,
            "text": "E GENERALIZED QUADRATIC POSITIONAL ENCODING"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 444
                },
                {
                    "x": 2103,
                    "y": 444
                },
                {
                    "x": 2103,
                    "y": 535
                },
                {
                    "x": 443,
                    "y": 535
                }
            ],
            "category": "paragraph",
            "html": "<p id='196' style='font-size:18px'>We noticed the similarity of the attention probabilities in the quadratic positional encoding (Sec-<br>tion 3) to isotropic bivariate Gaussian distributions with bounded support:</p>",
            "id": 196,
            "page": 17,
            "text": "We noticed the similarity of the attention probabilities in the quadratic positional encoding (Sec-\ntion 3) to isotropic bivariate Gaussian distributions with bounded support:"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 696
                },
                {
                    "x": 2105,
                    "y": 696
                },
                {
                    "x": 2105,
                    "y": 834
                },
                {
                    "x": 443,
                    "y": 834
                }
            ],
            "category": "paragraph",
            "html": "<p id='197' style='font-size:16px'>Building on this observation, we further extended our attention mechanism to non-isotropic Gaus-<br>sian distribution over pixel positions. Each head is parametrized by a center of attention △ and a<br>covariance matrix � to obtain the following attention scores,</p>",
            "id": 197,
            "page": 17,
            "text": "Building on this observation, we further extended our attention mechanism to non-isotropic Gaus-\nsian distribution over pixel positions. Each head is parametrized by a center of attention △ and a\ncovariance matrix � to obtain the following attention scores,"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 964
                },
                {
                    "x": 2106,
                    "y": 964
                },
                {
                    "x": 2106,
                    "y": 1143
                },
                {
                    "x": 441,
                    "y": 1143
                }
            ],
            "category": "paragraph",
            "html": "<p id='198' style='font-size:16px'>where, once more, 8 = k - q. The last term can be discarded because the softmax is shift invariant<br>and we rewrite the attention coefficient as a dot product between the head target vector v and the<br>relative position encoding rs (consisting of the first and second order combinations of the shift in<br>pixels 8):</p>",
            "id": 198,
            "page": 17,
            "text": "where, once more, 8 = k - q. The last term can be discarded because the softmax is shift invariant\nand we rewrite the attention coefficient as a dot product between the head target vector v and the\nrelative position encoding rs (consisting of the first and second order combinations of the shift in\npixels 8):"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1292
                },
                {
                    "x": 2106,
                    "y": 1292
                },
                {
                    "x": 2106,
                    "y": 1716
                },
                {
                    "x": 441,
                    "y": 1716
                }
            ],
            "category": "paragraph",
            "html": "<p id='199' style='font-size:16px'>Evaluation. We trained our model using this generalized quadratic relative position encoding. We<br>were curious to see if, using the above encoding the self-attention model would learn to attend to<br>non-isotropic groups of pixels-thus forming unseen patterns in CNNs. Each head was parametrized<br>by △ E R2 and �-1/2 R2x2 to ensure that the covariance matrix remained positive semi-definite.<br>E<br>We initialized the center of attention to △ (h) ~ N(0, 2I2) and �-1/2 = I2 + N(0, 0.01I2) so that<br>initial attention probabilities were close to an isotropic Gaussian. Figure 12 shows that the network<br>did learn non-isotropic attention probability patterns, especially in high layers. Nevertheless, the fact<br>that we do not obtain any performance improvement seems to suggest that attention non-isotropy is<br>not particularly helpful in practice-the quadratic positional encoding suffices.</p>",
            "id": 199,
            "page": 17,
            "text": "Evaluation. We trained our model using this generalized quadratic relative position encoding. We\nwere curious to see if, using the above encoding the self-attention model would learn to attend to\nnon-isotropic groups of pixels-thus forming unseen patterns in CNNs. Each head was parametrized\nby △ E R2 and �-1/2 R2x2 to ensure that the covariance matrix remained positive semi-definite.\nE\nWe initialized the center of attention to △ (h) ~ N(0, 2I2) and �-1/2 = I2 + N(0, 0.01I2) so that\ninitial attention probabilities were close to an isotropic Gaussian. Figure 12 shows that the network\ndid learn non-isotropic attention probability patterns, especially in high layers. Nevertheless, the fact\nthat we do not obtain any performance improvement seems to suggest that attention non-isotropy is\nnot particularly helpful in practice-the quadratic positional encoding suffices."
        },
        {
            "bounding_box": [
                {
                    "x": 457,
                    "y": 1756
                },
                {
                    "x": 2089,
                    "y": 1756
                },
                {
                    "x": 2089,
                    "y": 2029
                },
                {
                    "x": 457,
                    "y": 2029
                }
            ],
            "category": "figure",
            "html": "<figure><img id='200' style='font-size:14px' alt=\"Layer 1 Layer 2 Layer 3 Layer 4 Layer 5 Layer +\" data-coord=\"top-left:(457,1756); bottom-right:(2089,2029)\" /></figure>",
            "id": 200,
            "page": 17,
            "text": "Layer 1 Layer 2 Layer 3 Layer 4 Layer 5 Layer +"
        },
        {
            "bounding_box": [
                {
                    "x": 443,
                    "y": 2070
                },
                {
                    "x": 2105,
                    "y": 2070
                },
                {
                    "x": 2105,
                    "y": 2213
                },
                {
                    "x": 443,
                    "y": 2213
                }
            ],
            "category": "paragraph",
            "html": "<p id='201' style='font-size:16px'>Figure 12: Centers of attention of each attention head (different colors) for the 6 self-attention layers<br>using non-isotropic Gaussian parametrization. The central black square is the query pixel, whereas<br>solid and dotted circles represent the 50% and 90% percentiles of each Gaussian, respectively.</p>",
            "id": 201,
            "page": 17,
            "text": "Figure 12: Centers of attention of each attention head (different colors) for the 6 self-attention layers\nusing non-isotropic Gaussian parametrization. The central black square is the query pixel, whereas\nsolid and dotted circles represent the 50% and 90% percentiles of each Gaussian, respectively."
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 2286
                },
                {
                    "x": 2106,
                    "y": 2286
                },
                {
                    "x": 2106,
                    "y": 2839
                },
                {
                    "x": 441,
                    "y": 2839
                }
            ],
            "category": "paragraph",
            "html": "<p id='202' style='font-size:16px'>Pruning degenerated heads. Some non-isotropic attention heads attend on \"non-intuitive\"<br>patches of pixels: either attending a very thin stripe of pixels, when �-1 was almost singular, or<br>attending all pixels uniformly, when E-1 0 (i.e. constant attention scores). We asked<br>was close to<br>ourselves, are such attention patterns indeed useful for the model or are these heads degenerated and<br>unused? To find out, we pruned all heads having largest eigen-values smaller than 10-5 or condition<br>number (ratio of the biggest and smallest eigen-values) greater than 105. Specifically in our model<br>with 6-layer and 9-heads each, we pruned [2, 4, 1, 2, 6, 0] heads from the first to the last layer. This<br>means that these layers cannot express a 3 x 3 kernel anymore. As shown in yellow on fig. 2, this<br>ablation initially hurts a bit the performance, probably due to off biases, but after a few epochs of<br>continued training with a smaller learning rate (divided by 10) the accuracy recovers its unpruned<br>value. Hence, without sacrificing performance, we reduce the size of the parameters and the number<br>of FLOPS by a fourth.</p>",
            "id": 202,
            "page": 17,
            "text": "Pruning degenerated heads. Some non-isotropic attention heads attend on \"non-intuitive\"\npatches of pixels: either attending a very thin stripe of pixels, when �-1 was almost singular, or\nattending all pixels uniformly, when E-1 0 (i.e. constant attention scores). We asked\nwas close to\nourselves, are such attention patterns indeed useful for the model or are these heads degenerated and\nunused? To find out, we pruned all heads having largest eigen-values smaller than 10-5 or condition\nnumber (ratio of the biggest and smallest eigen-values) greater than 105. Specifically in our model\nwith 6-layer and 9-heads each, we pruned [2, 4, 1, 2, 6, 0] heads from the first to the last layer. This\nmeans that these layers cannot express a 3 x 3 kernel anymore. As shown in yellow on fig. 2, this\nablation initially hurts a bit the performance, probably due to off biases, but after a few epochs of\ncontinued training with a smaller learning rate (divided by 10) the accuracy recovers its unpruned\nvalue. Hence, without sacrificing performance, we reduce the size of the parameters and the number\nof FLOPS by a fourth."
        },
        {
            "bounding_box": [
                {
                    "x": 446,
                    "y": 2904
                },
                {
                    "x": 1333,
                    "y": 2904
                },
                {
                    "x": 1333,
                    "y": 2956
                },
                {
                    "x": 446,
                    "y": 2956
                }
            ],
            "category": "paragraph",
            "html": "<p id='203' style='font-size:20px'>F INCREASING THE NUMBER OF HEADS</p>",
            "id": 203,
            "page": 17,
            "text": "F INCREASING THE NUMBER OF HEADS"
        },
        {
            "bounding_box": [
                {
                    "x": 446,
                    "y": 3006
                },
                {
                    "x": 2066,
                    "y": 3006
                },
                {
                    "x": 2066,
                    "y": 3052
                },
                {
                    "x": 446,
                    "y": 3052
                }
            ],
            "category": "paragraph",
            "html": "<p id='204' style='font-size:16px'>For completeness, we also tested increasing the number of heads of our architecture from 9 to 16.</p>",
            "id": 204,
            "page": 17,
            "text": "For completeness, we also tested increasing the number of heads of our architecture from 9 to 16."
        },
        {
            "bounding_box": [
                {
                    "x": 1253,
                    "y": 3133
                },
                {
                    "x": 1299,
                    "y": 3133
                },
                {
                    "x": 1299,
                    "y": 3171
                },
                {
                    "x": 1253,
                    "y": 3171
                }
            ],
            "category": "footer",
            "html": "<footer id='205' style='font-size:16px'>17</footer>",
            "id": 205,
            "page": 17,
            "text": "17"
        },
        {
            "bounding_box": [
                {
                    "x": 445,
                    "y": 111
                },
                {
                    "x": 1226,
                    "y": 111
                },
                {
                    "x": 1226,
                    "y": 157
                },
                {
                    "x": 445,
                    "y": 157
                }
            ],
            "category": "header",
            "html": "<header id='206' style='font-size:18px'>Published as a conference paper at ICLR 2020</header>",
            "id": 206,
            "page": 18,
            "text": "Published as a conference paper at ICLR 2020"
        },
        {
            "bounding_box": [
                {
                    "x": 467,
                    "y": 386
                },
                {
                    "x": 1201,
                    "y": 386
                },
                {
                    "x": 1201,
                    "y": 873
                },
                {
                    "x": 467,
                    "y": 873
                }
            ],
            "category": "figure",
            "html": "<figure><img id='207' style='font-size:14px' alt=\"1.0\n0.9\naccuracy\n0.8\nTest\nResNet18\nSA quadratic emb.\nSA quadratic emb. gen.\n0.7\nSA quadratic emb. gen. pruned\nSA learned emb.\nSA learned emb. + content-based att.\n0.6\n0 50 100 150 200 250 300 350 400\nEpoch\" data-coord=\"top-left:(467,386); bottom-right:(1201,873)\" /></figure>",
            "id": 207,
            "page": 18,
            "text": "1.0\n0.9\naccuracy\n0.8\nTest\nResNet18\nSA quadratic emb.\nSA quadratic emb. gen.\n0.7\nSA quadratic emb. gen. pruned\nSA learned emb.\nSA learned emb. + content-based att.\n0.6\n0 50 100 150 200 250 300 350 400\nEpoch"
        },
        {
            "bounding_box": [
                {
                    "x": 1290,
                    "y": 516
                },
                {
                    "x": 2105,
                    "y": 516
                },
                {
                    "x": 2105,
                    "y": 756
                },
                {
                    "x": 1290,
                    "y": 756
                }
            ],
            "category": "table",
            "html": "<br><table id='208' style='font-size:16px'><tr><td>Models</td><td>accuracy</td><td># of params</td><td>#ofFLOPS</td></tr><tr><td>ResNet18</td><td>0.938</td><td>11.2M</td><td>1.1B</td></tr><tr><td>SA quadratic emb.</td><td>0.938</td><td>12.1M</td><td>6.2B</td></tr><tr><td>SA quadratic emb. gen.</td><td>0.934</td><td>12.1M</td><td>6.2B</td></tr><tr><td>SA quadratic emb. gen. pruned</td><td>0.934</td><td>9.7M</td><td>4.9B</td></tr><tr><td>SA learned emb.</td><td>0.918</td><td>12.3M</td><td>6.2B</td></tr><tr><td>SA learned emb. + content</td><td>0.871</td><td>29.5M</td><td>15B</td></tr></table>",
            "id": 208,
            "page": 18,
            "text": "Models accuracy # of params #ofFLOPS\n ResNet18 0.938 11.2M 1.1B\n SA quadratic emb. 0.938 12.1M 6.2B\n SA quadratic emb. gen. 0.934 12.1M 6.2B\n SA quadratic emb. gen. pruned 0.934 9.7M 4.9B\n SA learned emb. 0.918 12.3M 6.2B\n SA learned emb. + content 0.871 29.5M"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 915
                },
                {
                    "x": 2106,
                    "y": 915
                },
                {
                    "x": 2106,
                    "y": 1058
                },
                {
                    "x": 441,
                    "y": 1058
                }
            ],
            "category": "caption",
            "html": "<caption id='209' style='font-size:20px'>Figure 13: Evolution of test accuracy on CIFAR- Table 4: Number of parameters and accuracy<br>10. Pruned model (yellow) is continued training on CIFAR-10 per model. SA stands for Self-<br>of the non-isotropic model (orange). Attention.</caption>",
            "id": 209,
            "page": 18,
            "text": "Figure 13: Evolution of test accuracy on CIFAR- Table 4: Number of parameters and accuracy\n10. Pruned model (yellow) is continued training on CIFAR-10 per model. SA stands for Self-\nof the non-isotropic model (orange). Attention."
        },
        {
            "bounding_box": [
                {
                    "x": 453,
                    "y": 1101
                },
                {
                    "x": 2091,
                    "y": 1101
                },
                {
                    "x": 2091,
                    "y": 1379
                },
                {
                    "x": 453,
                    "y": 1379
                }
            ],
            "category": "figure",
            "html": "<figure><img id='210' style='font-size:14px' alt=\"Layer 1 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6\" data-coord=\"top-left:(453,1101); bottom-right:(2091,1379)\" /></figure>",
            "id": 210,
            "page": 18,
            "text": "Layer 1 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6"
        },
        {
            "bounding_box": [
                {
                    "x": 441,
                    "y": 1415
                },
                {
                    "x": 2108,
                    "y": 1415
                },
                {
                    "x": 2108,
                    "y": 1558
                },
                {
                    "x": 441,
                    "y": 1558
                }
            ],
            "category": "caption",
            "html": "<caption id='211' style='font-size:22px'>Figure 14: Centers of attention for 16 attention heads (different colors) for the 6 self-attention layers<br>using quadratic positional encoding. The central black square is the query pixel, whereas solid and<br>dotted circles represent the 50% and 90% percentiles of each Gaussian, respectively.</caption>",
            "id": 211,
            "page": 18,
            "text": "Figure 14: Centers of attention for 16 attention heads (different colors) for the 6 self-attention layers\nusing quadratic positional encoding. The central black square is the query pixel, whereas solid and\ndotted circles represent the 50% and 90% percentiles of each Gaussian, respectively."
        },
        {
            "bounding_box": [
                {
                    "x": 442,
                    "y": 1647
                },
                {
                    "x": 2106,
                    "y": 1647
                },
                {
                    "x": 2106,
                    "y": 1832
                },
                {
                    "x": 442,
                    "y": 1832
                }
            ],
            "category": "paragraph",
            "html": "<p id='212' style='font-size:20px'>Similar to Figure 4, we see that the network distinguishes two main types of attention patterns.<br>Localized heads (i.e., those that attend to nearly individual pixels) appear more frequently in the first<br>few layers. The self-attention layer uses these heads to act in a manner similar to how convolutional<br>layers do. Heads with less-localized attention become more common at higher layers.</p>",
            "id": 212,
            "page": 18,
            "text": "Similar to Figure 4, we see that the network distinguishes two main types of attention patterns.\nLocalized heads (i.e., those that attend to nearly individual pixels) appear more frequently in the first\nfew layers. The self-attention layer uses these heads to act in a manner similar to how convolutional\nlayers do. Heads with less-localized attention become more common at higher layers."
        },
        {
            "bounding_box": [
                {
                    "x": 1253,
                    "y": 3133
                },
                {
                    "x": 1300,
                    "y": 3133
                },
                {
                    "x": 1300,
                    "y": 3173
                },
                {
                    "x": 1253,
                    "y": 3173
                }
            ],
            "category": "footer",
            "html": "<footer id='213' style='font-size:18px'>18</footer>",
            "id": 213,
            "page": 18,
            "text": "18"
        }
    ]
}