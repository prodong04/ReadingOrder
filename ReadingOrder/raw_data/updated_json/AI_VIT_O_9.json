{
    "id": "62a96f1a-0f92-11ef-8230-426932df3dcf",
    "pdf_path": "/root/data/pdf/2111.07624v1.pdf",
    "elements": [
        {
            "bounding_box": [
                {
                    "x": 194,
                    "y": 110
                },
                {
                    "x": 1075,
                    "y": 110
                },
                {
                    "x": 1075,
                    "y": 149
                },
                {
                    "x": 194,
                    "y": 149
                }
            ],
            "category": "header",
            "html": "<header id='0' style='font-size:14px'>JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015</header>",
            "id": 0,
            "page": 1,
            "text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015"
        },
        {
            "bounding_box": [
                {
                    "x": 2330,
                    "y": 114
                },
                {
                    "x": 2352,
                    "y": 114
                },
                {
                    "x": 2352,
                    "y": 143
                },
                {
                    "x": 2330,
                    "y": 143
                }
            ],
            "category": "header",
            "html": "<br><header id='1' style='font-size:14px'>1</header>",
            "id": 1,
            "page": 1,
            "text": "1"
        },
        {
            "bounding_box": [
                {
                    "x": 325,
                    "y": 223
                },
                {
                    "x": 2229,
                    "y": 223
                },
                {
                    "x": 2229,
                    "y": 450
                },
                {
                    "x": 325,
                    "y": 450
                }
            ],
            "category": "paragraph",
            "html": "<p id='2' style='font-size:22px'>Attention Mechanisms in Computer Vision:<br>A Survey</p>",
            "id": 2,
            "page": 1,
            "text": "Attention Mechanisms in Computer Vision:\nA Survey"
        },
        {
            "bounding_box": [
                {
                    "x": 207,
                    "y": 500
                },
                {
                    "x": 2339,
                    "y": 500
                },
                {
                    "x": 2339,
                    "y": 619
                },
                {
                    "x": 207,
                    "y": 619
                }
            ],
            "category": "paragraph",
            "html": "<p id='3' style='font-size:22px'>Meng-Hao Guo, Tian-Xing Xu, Jiang-Jiang Liu, Zheng-Ning Liu, Peng-Tao Jiang, Tai-Jiang Mu, Song-Hai<br>Zhang, Ralph R. Martin, Ming-Ming Cheng, Senior Member, IEEE, Shi-Min Hu, Senior Member, IEEE,</p>",
            "id": 3,
            "page": 1,
            "text": "Meng-Hao Guo, Tian-Xing Xu, Jiang-Jiang Liu, Zheng-Ning Liu, Peng-Tao Jiang, Tai-Jiang Mu, Song-Hai\nZhang, Ralph R. Martin, Ming-Ming Cheng, Senior Member, IEEE, Shi-Min Hu, Senior Member, IEEE,"
        },
        {
            "bounding_box": [
                {
                    "x": 274,
                    "y": 689
                },
                {
                    "x": 2275,
                    "y": 689
                },
                {
                    "x": 2275,
                    "y": 1041
                },
                {
                    "x": 274,
                    "y": 1041
                }
            ],
            "category": "paragraph",
            "html": "<p id='4' style='font-size:16px'>Abstract-Humans can naturally and effectively find salient regions in complex scenes. Motivated by this observation, attention<br>mechanisms were introduced into computer vision with the aim of imitating this aspect of the human visual system. Such an attention<br>mechanism can be regarded as a dynamic weight adjustment process based on features of the input image. Attention mechanisms have<br>achieved great success in many visual tasks, including image classification, object detection, semantic segmentation, video<br>understanding, image generation, 3D vision, multi-modal tasks and self-supervised learning. In this survey, we provide a comprehensive<br>review of various attention mechanisms in computer vision and categorize them according to approach, such as channel attention, spatial<br>attention, temporal attention and branch attention; a related repository https://hub.com/MengfasoGuokAreernermew-Aters is<br>dedicated to collecting related work. We also suggest future directions for attention mechanism research.</p>",
            "id": 4,
            "page": 1,
            "text": "Abstract-Humans can naturally and effectively find salient regions in complex scenes. Motivated by this observation, attention\nmechanisms were introduced into computer vision with the aim of imitating this aspect of the human visual system. Such an attention\nmechanism can be regarded as a dynamic weight adjustment process based on features of the input image. Attention mechanisms have\nachieved great success in many visual tasks, including image classification, object detection, semantic segmentation, video\nunderstanding, image generation, 3D vision, multi-modal tasks and self-supervised learning. In this survey, we provide a comprehensive\nreview of various attention mechanisms in computer vision and categorize them according to approach, such as channel attention, spatial\nattention, temporal attention and branch attention; a related repository https://hub.com/MengfasoGuokAreernermew-Aters is\ndedicated to collecting related work. We also suggest future directions for attention mechanism research."
        },
        {
            "bounding_box": [
                {
                    "x": 276,
                    "y": 1081
                },
                {
                    "x": 1617,
                    "y": 1081
                },
                {
                    "x": 1617,
                    "y": 1128
                },
                {
                    "x": 276,
                    "y": 1128
                }
            ],
            "category": "paragraph",
            "html": "<p id='5' style='font-size:18px'>Index Terms-Attention, Transformer, Survey, Computer Vision, Deep Learning, Salience.</p>",
            "id": 5,
            "page": 1,
            "text": "Index Terms-Attention, Transformer, Survey, Computer Vision, Deep Learning, Salience."
        },
        {
            "bounding_box": [
                {
                    "x": 1256,
                    "y": 1153
                },
                {
                    "x": 1290,
                    "y": 1153
                },
                {
                    "x": 1290,
                    "y": 1187
                },
                {
                    "x": 1256,
                    "y": 1187
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='6' style='font-size:18px'>◆</p>",
            "id": 6,
            "page": 1,
            "text": "◆"
        },
        {
            "bounding_box": [
                {
                    "x": 197,
                    "y": 1242
                },
                {
                    "x": 590,
                    "y": 1242
                },
                {
                    "x": 590,
                    "y": 1297
                },
                {
                    "x": 197,
                    "y": 1297
                }
            ],
            "category": "paragraph",
            "html": "<p id='7' style='font-size:20px'>1 INTRODUCTION</p>",
            "id": 7,
            "page": 1,
            "text": "1 INTRODUCTION"
        },
        {
            "bounding_box": [
                {
                    "x": 190,
                    "y": 1329
                },
                {
                    "x": 1259,
                    "y": 1329
                },
                {
                    "x": 1259,
                    "y": 2202
                },
                {
                    "x": 190,
                    "y": 2202
                }
            ],
            "category": "paragraph",
            "html": "<p id='8' style='font-size:20px'>M tant regions of an image and disregarding irrelevant<br>ETHODS for diverting attention to the most impor-<br>parts are called attention mechanisms; the human visual<br>system uses one [1], [2], [3], [4] to assist in analyzing and<br>understanding complex scenes efficiently and effectively.<br>This in turn has inspired researchers to introduce attention<br>mechanisms into computer vision systems to improve their<br>performance. In a vision system, an attention mechanism can<br>be treated as a dynamic selection process that is realized by<br>adaptively weighting features according to the importance<br>of the input. Attention mechanisms have provided benefits<br>in very many visual tasks, e.g. image classification [5], [6],<br>object detection [7], [8], semantic segmentation [9], [10], face<br>recognition [11], [12], person re-identification [13], [14], action<br>recognition [15], [16], few-show learning [17], [18], medical<br>image processing [19], [20], image generation [21], [22], pose<br>estimation [23], super resolution [24], [25], 3D vision [26],<br>[27], and multi-modal task [28], [29].</p>",
            "id": 8,
            "page": 1,
            "text": "M tant regions of an image and disregarding irrelevant\nETHODS for diverting attention to the most impor-\nparts are called attention mechanisms; the human visual\nsystem uses one [1], [2], [3], [4] to assist in analyzing and\nunderstanding complex scenes efficiently and effectively.\nThis in turn has inspired researchers to introduce attention\nmechanisms into computer vision systems to improve their\nperformance. In a vision system, an attention mechanism can\nbe treated as a dynamic selection process that is realized by\nadaptively weighting features according to the importance\nof the input. Attention mechanisms have provided benefits\nin very many visual tasks, e.g. image classification [5], [6],\nobject detection [7], [8], semantic segmentation [9], [10], face\nrecognition [11], [12], person re-identification [13], [14], action\nrecognition [15], [16], few-show learning [17], [18], medical\nimage processing [19], [20], image generation [21], [22], pose\nestimation [23], super resolution [24], [25], 3D vision [26],\n[27], and multi-modal task [28], [29]."
        },
        {
            "bounding_box": [
                {
                    "x": 190,
                    "y": 2204
                },
                {
                    "x": 1258,
                    "y": 2204
                },
                {
                    "x": 1258,
                    "y": 2688
                },
                {
                    "x": 190,
                    "y": 2688
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='9' style='font-size:18px'>In the past decade, the attention mechanism has played<br>an increasingly important role in computer vision; Fig. 3,<br>briefly summarizes the history of attention-based models<br>in computer vision in the deep learning era. Progress can<br>be coarsely divided into four phases. The first phase begins<br>from RAM [31], pioneering work that combined deep neural<br>networks with attention mechanisms. It recurrently predicts<br>the important region and updates the whole network in an<br>end-to-end manner through a policy gradient. Later, various<br>works [21], [35] adopted a similar strategy for attention in</p>",
            "id": 9,
            "page": 1,
            "text": "In the past decade, the attention mechanism has played\nan increasingly important role in computer vision; Fig. 3,\nbriefly summarizes the history of attention-based models\nin computer vision in the deep learning era. Progress can\nbe coarsely divided into four phases. The first phase begins\nfrom RAM [31], pioneering work that combined deep neural\nnetworks with attention mechanisms. It recurrently predicts\nthe important region and updates the whole network in an\nend-to-end manner through a policy gradient. Later, various\nworks [21], [35] adopted a similar strategy for attention in"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 2750
                },
                {
                    "x": 1258,
                    "y": 2750
                },
                {
                    "x": 1258,
                    "y": 3095
                },
                {
                    "x": 192,
                    "y": 3095
                }
            ],
            "category": "paragraph",
            "html": "<p id='10' style='font-size:16px'>● M.H.Guo, T.X.Xu, Z.N.Liu, T.J.Mu, S.H.Zhang and S.M.Hu are with<br>the BNRist, Department of Computer Science and Technology, Tsinghua<br>University, Beijing 100084, China.<br>● J.J.Liu, P.T.Jiang and M.M. Cheng are with TKLNDST, College of Computer<br>Science, Nankai University, Tianjin 300350, China.<br>● R.R.Martin was with the School of Computer Science and Informatics,<br>Cardiff University, UK.<br>● S.M.Hu is the corresponding author.<br>E-mail: shimin@tsinghua.edu.cn.</p>",
            "id": 10,
            "page": 1,
            "text": "● M.H.Guo, T.X.Xu, Z.N.Liu, T.J.Mu, S.H.Zhang and S.M.Hu are with\nthe BNRist, Department of Computer Science and Technology, Tsinghua\nUniversity, Beijing 100084, China.\n● J.J.Liu, P.T.Jiang and M.M. Cheng are with TKLNDST, College of Computer\nScience, Nankai University, Tianjin 300350, China.\n● R.R.Martin was with the School of Computer Science and Informatics,\nCardiff University, UK.\n● S.M.Hu is the corresponding author.\nE-mail: shimin@tsinghua.edu.cn."
        },
        {
            "bounding_box": [
                {
                    "x": 1297,
                    "y": 1327
                },
                {
                    "x": 2350,
                    "y": 1327
                },
                {
                    "x": 2350,
                    "y": 2328
                },
                {
                    "x": 1297,
                    "y": 2328
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='11' style='font-size:16px' alt=\"Channel Attention Spatial Attention\ne.g.,SENet e.g.,Self-attention\nChannel & Spatial\nWhat to attend Where to attend\nAttention\ne.g.,CBAM\nO Spatial& Temporal\nAttention\ne.g., STA-LSTM\nTemporal Attention\ne.g., GLTR\nBranch Attention\nWhen to attend\ne.g., SKNet\nWhich to attend\" data-coord=\"top-left:(1297,1327); bottom-right:(2350,2328)\" /></figure>",
            "id": 11,
            "page": 1,
            "text": "Channel Attention Spatial Attention\ne.g.,SENet e.g.,Self-attention\nChannel & Spatial\nWhat to attend Where to attend\nAttention\ne.g.,CBAM\nO Spatial& Temporal\nAttention\ne.g., STA-LSTM\nTemporal Attention\ne.g., GLTR\nBranch Attention\nWhen to attend\ne.g., SKNet\nWhich to attend"
        },
        {
            "bounding_box": [
                {
                    "x": 1290,
                    "y": 2417
                },
                {
                    "x": 2360,
                    "y": 2417
                },
                {
                    "x": 2360,
                    "y": 2616
                },
                {
                    "x": 1290,
                    "y": 2616
                }
            ],
            "category": "caption",
            "html": "<caption id='12' style='font-size:16px'>Fig. 1. Attention mechanisms can be categorised according to data<br>domain. These include four fundamental categories of channel attention,<br>spatial attention, temporal attention and branch attention, and two hybrid<br>categories, combining channel & spatial attention and spatial & temporal<br>attention. 0 means such combinations do not (yet) exist.</caption>",
            "id": 12,
            "page": 1,
            "text": "Fig. 1. Attention mechanisms can be categorised according to data\ndomain. These include four fundamental categories of channel attention,\nspatial attention, temporal attention and branch attention, and two hybrid\ncategories, combining channel & spatial attention and spatial & temporal\nattention. 0 means such combinations do not (yet) exist."
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 2727
                },
                {
                    "x": 2361,
                    "y": 2727
                },
                {
                    "x": 2361,
                    "y": 3118
                },
                {
                    "x": 1291,
                    "y": 3118
                }
            ],
            "category": "paragraph",
            "html": "<p id='13' style='font-size:18px'>vision. In this phase, recurrent neural networks(RNNs) were<br>necessary tools for an attention mechanism. At the start of<br>the second phase, Jaderberg et al. [32] proposed the STN<br>which introduces a sub-network to predict an affine trans-<br>formation used to to select important regions in the input.<br>Explicitly predicting discriminatory input features is the<br>major characteristic of the second phase; DCNs [7], [36] are<br>representative works. The third phase began with SENet [5]</p>",
            "id": 13,
            "page": 1,
            "text": "vision. In this phase, recurrent neural networks(RNNs) were\nnecessary tools for an attention mechanism. At the start of\nthe second phase, Jaderberg et al. [32] proposed the STN\nwhich introduces a sub-network to predict an affine trans-\nformation used to to select important regions in the input.\nExplicitly predicting discriminatory input features is the\nmajor characteristic of the second phase; DCNs [7], [36] are\nrepresentative works. The third phase began with SENet [5]"
        },
        {
            "bounding_box": [
                {
                    "x": 56,
                    "y": 867
                },
                {
                    "x": 154,
                    "y": 867
                },
                {
                    "x": 154,
                    "y": 2330
                },
                {
                    "x": 56,
                    "y": 2330
                }
            ],
            "category": "footer",
            "html": "<br><footer id='14' style='font-size:14px'>2021<br>Nov<br>15<br>[cs.CV]<br>arXiv:2111.07624v1</footer>",
            "id": 14,
            "page": 1,
            "text": "2021\nNov\n15\n[cs.CV]\narXiv:2111.07624v1"
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 110
                },
                {
                    "x": 1076,
                    "y": 110
                },
                {
                    "x": 1076,
                    "y": 149
                },
                {
                    "x": 193,
                    "y": 149
                }
            ],
            "category": "header",
            "html": "<header id='15' style='font-size:16px'>JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015</header>",
            "id": 15,
            "page": 2,
            "text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015"
        },
        {
            "bounding_box": [
                {
                    "x": 2327,
                    "y": 113
                },
                {
                    "x": 2353,
                    "y": 113
                },
                {
                    "x": 2353,
                    "y": 146
                },
                {
                    "x": 2327,
                    "y": 146
                }
            ],
            "category": "header",
            "html": "<br><header id='16' style='font-size:14px'>2</header>",
            "id": 16,
            "page": 2,
            "text": "2"
        },
        {
            "bounding_box": [
                {
                    "x": 186,
                    "y": 172
                },
                {
                    "x": 1259,
                    "y": 172
                },
                {
                    "x": 1259,
                    "y": 1757
                },
                {
                    "x": 186,
                    "y": 1757
                }
            ],
            "category": "figure",
            "html": "<figure><img id='17' style='font-size:16px' alt=\"Channel Attention Channel&Spatial Attention\nH,W H,W\nT C T\nSpatial Attention Spatial& Temporal Attention\nH,W H,W\nC T T\nTemporal Attention Branch Attention\nBranch 1 Branch 2 ··· Branch n\nH,W T\n: Where attention is used.\" data-coord=\"top-left:(186,172); bottom-right:(1259,1757)\" /></figure>",
            "id": 17,
            "page": 2,
            "text": "Channel Attention Channel&Spatial Attention\nH,W H,W\nT C T\nSpatial Attention Spatial& Temporal Attention\nH,W H,W\nC T T\nTemporal Attention Branch Attention\nBranch 1 Branch 2 ··· Branch n\nH,W T\n: Where attention is used."
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 1854
                },
                {
                    "x": 1258,
                    "y": 1854
                },
                {
                    "x": 1258,
                    "y": 2014
                },
                {
                    "x": 192,
                    "y": 2014
                }
            ],
            "category": "caption",
            "html": "<caption id='18' style='font-size:18px'>Fig. 2. Channel, spatial and temporal attention can be regarded as<br>operating on different domains. C represents the channel domain, H<br>and W represent spatial domains, and T means the temporal domain.<br>Branch attention is complementary to these. Figure following [30].</caption>",
            "id": 18,
            "page": 2,
            "text": "Fig. 2. Channel, spatial and temporal attention can be regarded as\noperating on different domains. C represents the channel domain, H\nand W represent spatial domains, and T means the temporal domain.\nBranch attention is complementary to these. Figure following [30]."
        },
        {
            "bounding_box": [
                {
                    "x": 190,
                    "y": 2099
                },
                {
                    "x": 1258,
                    "y": 2099
                },
                {
                    "x": 1258,
                    "y": 3017
                },
                {
                    "x": 190,
                    "y": 3017
                }
            ],
            "category": "paragraph",
            "html": "<p id='19' style='font-size:22px'>that presented a novel channel-attention network which<br>implicitly and adaptively predicts the potential key features.<br>CBAM [6] and ECANet [37] are representative works of this<br>phase. The last phase is the self-attention era. Self-attention<br>was firstly proposed in [33] and rapidly provided great<br>advances in the field of natural language processing [33],<br>[38], [39]. Wang et al. [15] took the lead in introducing self-<br>attention to computer vision and presented a novel non-local<br>network with great success in video understanding and<br>object detection. It was followed by a series of works such as<br>EMANet [40], CCNet [41], HamNet [42] and the Stand-Alone<br>Network [43], which improved speed, quality of results,<br>and generalization capability. Recently, various pure deep<br>self-attention networks (visual transformers) [27], [34], [44],<br>[45], [46], [47], [48], [49] have appeared, showing the huge<br>potential of attention-based models. It is clear that attention-<br>based models have the potential to replace convolutional<br>neural networks and become a more powerful and general<br>architecture in computer vision.</p>",
            "id": 19,
            "page": 2,
            "text": "that presented a novel channel-attention network which\nimplicitly and adaptively predicts the potential key features.\nCBAM [6] and ECANet [37] are representative works of this\nphase. The last phase is the self-attention era. Self-attention\nwas firstly proposed in [33] and rapidly provided great\nadvances in the field of natural language processing [33],\n[38], [39]. Wang et al. [15] took the lead in introducing self-\nattention to computer vision and presented a novel non-local\nnetwork with great success in video understanding and\nobject detection. It was followed by a series of works such as\nEMANet [40], CCNet [41], HamNet [42] and the Stand-Alone\nNetwork [43], which improved speed, quality of results,\nand generalization capability. Recently, various pure deep\nself-attention networks (visual transformers) [27], [34], [44],\n[45], [46], [47], [48], [49] have appeared, showing the huge\npotential of attention-based models. It is clear that attention-\nbased models have the potential to replace convolutional\nneural networks and become a more powerful and general\narchitecture in computer vision."
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 3017
                },
                {
                    "x": 1257,
                    "y": 3017
                },
                {
                    "x": 1257,
                    "y": 3116
                },
                {
                    "x": 193,
                    "y": 3116
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='20' style='font-size:20px'>The goal of this paper is to summarize and classify<br>current attention methods in computer vision. Our approach</p>",
            "id": 20,
            "page": 2,
            "text": "The goal of this paper is to summarize and classify\ncurrent attention methods in computer vision. Our approach"
        },
        {
            "bounding_box": [
                {
                    "x": 1756,
                    "y": 186
                },
                {
                    "x": 1890,
                    "y": 186
                },
                {
                    "x": 1890,
                    "y": 222
                },
                {
                    "x": 1756,
                    "y": 222
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='21' style='font-size:14px'>TABLE 1</p>",
            "id": 21,
            "page": 2,
            "text": "TABLE 1"
        },
        {
            "bounding_box": [
                {
                    "x": 1297,
                    "y": 223
                },
                {
                    "x": 2357,
                    "y": 223
                },
                {
                    "x": 2357,
                    "y": 263
                },
                {
                    "x": 1297,
                    "y": 263
                }
            ],
            "category": "caption",
            "html": "<br><caption id='22' style='font-size:18px'>Key notation in this paper. Other minor notation is explained where used.</caption>",
            "id": 22,
            "page": 2,
            "text": "Key notation in this paper. Other minor notation is explained where used."
        },
        {
            "bounding_box": [
                {
                    "x": 1459,
                    "y": 302
                },
                {
                    "x": 2189,
                    "y": 302
                },
                {
                    "x": 2189,
                    "y": 890
                },
                {
                    "x": 1459,
                    "y": 890
                }
            ],
            "category": "paragraph",
            "html": "<p id='23' style='font-size:18px'>Symbol Description<br>X input feature map, X E RCxHx W<br>Y output feature map<br>W learnable kernel weight<br>FC fully-connected layer<br>Conv convolution<br>GAP global average pooling<br>GMP global max pooling<br>I ] concatenation<br>8 ReLU activation [51]<br>0 sigmoid activation<br>tanh tanh activation<br>Softmax softmax activation<br>BN batch normalization [52]<br>Expand expan input by repetition</p>",
            "id": 23,
            "page": 2,
            "text": "Symbol Description\nX input feature map, X E RCxHx W\nY output feature map\nW learnable kernel weight\nFC fully-connected layer\nConv convolution\nGAP global average pooling\nGMP global max pooling\nI ] concatenation\n8 ReLU activation [51]\n0 sigmoid activation\ntanh tanh activation\nSoftmax softmax activation\nBN batch normalization [52]\nExpand expan input by repetition"
        },
        {
            "bounding_box": [
                {
                    "x": 1294,
                    "y": 966
                },
                {
                    "x": 2359,
                    "y": 966
                },
                {
                    "x": 2359,
                    "y": 1546
                },
                {
                    "x": 1294,
                    "y": 1546
                }
            ],
            "category": "paragraph",
            "html": "<p id='24' style='font-size:22px'>is shown in Fig. 1 and further explained in Fig. 2: it is<br>based around data domain. Some methods consider the<br>question of when the important data occurs, or others where it<br>occurs, etc., and accordingly try to find key times or locations<br>in the data. We divide existing attention methods into<br>six categories which include four basic categories: channel<br>attention (what to pay attention to [50]), spatial attention (where<br>to pay attention), temporal attention (when to pay attention)<br>and branch channel (which to pay attention to), along with two<br>hybrid combined categories: channel & spatial attention and<br>spatial & temporal attention. These ideas are further briefly<br>summarized together with related works in Tab. 2.</p>",
            "id": 24,
            "page": 2,
            "text": "is shown in Fig. 1 and further explained in Fig. 2: it is\nbased around data domain. Some methods consider the\nquestion of when the important data occurs, or others where it\noccurs, etc., and accordingly try to find key times or locations\nin the data. We divide existing attention methods into\nsix categories which include four basic categories: channel\nattention (what to pay attention to [50]), spatial attention (where\nto pay attention), temporal attention (when to pay attention)\nand branch channel (which to pay attention to), along with two\nhybrid combined categories: channel & spatial attention and\nspatial & temporal attention. These ideas are further briefly\nsummarized together with related works in Tab. 2."
        },
        {
            "bounding_box": [
                {
                    "x": 1355,
                    "y": 1546
                },
                {
                    "x": 2084,
                    "y": 1546
                },
                {
                    "x": 2084,
                    "y": 1595
                },
                {
                    "x": 1355,
                    "y": 1595
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='25' style='font-size:20px'>The main contributions of this paper are:</p>",
            "id": 25,
            "page": 2,
            "text": "The main contributions of this paper are:"
        },
        {
            "bounding_box": [
                {
                    "x": 1352,
                    "y": 1617
                },
                {
                    "x": 2360,
                    "y": 1617
                },
                {
                    "x": 2360,
                    "y": 2051
                },
                {
                    "x": 1352,
                    "y": 2051
                }
            ],
            "category": "paragraph",
            "html": "<p id='26' style='font-size:20px'>· a systematic review of visual attention methods, COV-<br>ering the unified description of attention mechanisms,<br>the development of visual attention mechanisms as<br>well as current research,<br>● a categorisation grouping attention methods accord-<br>ing to their data domain, allowing us to link visual<br>attention methods independently of their particular<br>application, and<br>● suggestions for future research in visual attention.</p>",
            "id": 26,
            "page": 2,
            "text": "· a systematic review of visual attention methods, COV-\nering the unified description of attention mechanisms,\nthe development of visual attention mechanisms as\nwell as current research,\n● a categorisation grouping attention methods accord-\ning to their data domain, allowing us to link visual\nattention methods independently of their particular\napplication, and\n● suggestions for future research in visual attention."
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 2070
                },
                {
                    "x": 2357,
                    "y": 2070
                },
                {
                    "x": 2357,
                    "y": 2218
                },
                {
                    "x": 1292,
                    "y": 2218
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='27' style='font-size:20px'>Sec. 2 considers related surveys, then Sec. 3 is the main<br>body of our survey. Suggestions for future research are given<br>in Sec. 4 and finally, we give conclusions in Sec. 5.</p>",
            "id": 27,
            "page": 2,
            "text": "Sec. 2 considers related surveys, then Sec. 3 is the main\nbody of our survey. Suggestions for future research are given\nin Sec. 4 and finally, we give conclusions in Sec. 5."
        },
        {
            "bounding_box": [
                {
                    "x": 1293,
                    "y": 2277
                },
                {
                    "x": 1723,
                    "y": 2277
                },
                {
                    "x": 1723,
                    "y": 2330
                },
                {
                    "x": 1293,
                    "y": 2330
                }
            ],
            "category": "paragraph",
            "html": "<p id='28' style='font-size:22px'>2 OTHER SURVEYS</p>",
            "id": 28,
            "page": 2,
            "text": "2 OTHER SURVEYS"
        },
        {
            "bounding_box": [
                {
                    "x": 1293,
                    "y": 2344
                },
                {
                    "x": 2361,
                    "y": 2344
                },
                {
                    "x": 2361,
                    "y": 3118
                },
                {
                    "x": 1293,
                    "y": 3118
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='29' style='font-size:20px'>In this section, we briefly compare this paper to various<br>existing surveys which have reviewed attention methods<br>and visual transformers. Chaudhari et al. [140] provide<br>a survey of attention models in deep neural networks<br>which concentrates on their application to natural language<br>processing, while our work focuses on computer vision.<br>Three more specific surveys [141], [142], [143] summarize the<br>development of visual transformers while our paper reviews<br>attention mechanisms in vision more generally, not just self-<br>attention mechanisms. Wang et al. [144] present a survey of<br>attention models in computer vision, but it only considers<br>RNN-based attention models, which form just a part of our<br>survey. In addition, unlike previous surveys, we provide<br>a classification which groups various attention methods<br>according to their data domain, rather than according to<br>their field of application. Doing so allows us to concentrate</p>",
            "id": 29,
            "page": 2,
            "text": "In this section, we briefly compare this paper to various\nexisting surveys which have reviewed attention methods\nand visual transformers. Chaudhari et al. [140] provide\na survey of attention models in deep neural networks\nwhich concentrates on their application to natural language\nprocessing, while our work focuses on computer vision.\nThree more specific surveys [141], [142], [143] summarize the\ndevelopment of visual transformers while our paper reviews\nattention mechanisms in vision more generally, not just self-\nattention mechanisms. Wang et al. [144] present a survey of\nattention models in computer vision, but it only considers\nRNN-based attention models, which form just a part of our\nsurvey. In addition, unlike previous surveys, we provide\na classification which groups various attention methods\naccording to their data domain, rather than according to\ntheir field of application. Doing so allows us to concentrate"
        },
        {
            "bounding_box": [
                {
                    "x": 194,
                    "y": 109
                },
                {
                    "x": 1076,
                    "y": 109
                },
                {
                    "x": 1076,
                    "y": 149
                },
                {
                    "x": 194,
                    "y": 149
                }
            ],
            "category": "header",
            "html": "<header id='30' style='font-size:14px'>JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015</header>",
            "id": 30,
            "page": 3,
            "text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015"
        },
        {
            "bounding_box": [
                {
                    "x": 2327,
                    "y": 114
                },
                {
                    "x": 2352,
                    "y": 114
                },
                {
                    "x": 2352,
                    "y": 145
                },
                {
                    "x": 2327,
                    "y": 145
                }
            ],
            "category": "header",
            "html": "<br><header id='31' style='font-size:14px'>3</header>",
            "id": 31,
            "page": 3,
            "text": "3"
        },
        {
            "bounding_box": [
                {
                    "x": 187,
                    "y": 181
                },
                {
                    "x": 2250,
                    "y": 181
                },
                {
                    "x": 2250,
                    "y": 558
                },
                {
                    "x": 187,
                    "y": 558
                }
            ],
            "category": "paragraph",
            "html": "<p id='32' style='font-size:16px'>Non-Local Network 2018-2020<br>2020-Now<br>RAM Highway Network 2017.11.21 (Some popular<br>2014.06.24 2015.07.22 (It first successfully attention related (Various transformer<br>variants such as PCT,<br>(It adopts RNN and (It proposes to uses self-attention to network such as<br>Deformable DETR DeiT,<br>reinforcement learning combine different model non-local SAGAN, OCNet,<br>T2T-ViT, IPT,PVT and<br>to achieve spatial branches by using relationship in DANet, EMANet,<br>Swin- Transformer)<br>attention) attention method) computer vision) OCRNet and HamNet)</p>",
            "id": 32,
            "page": 3,
            "text": "Non-Local Network 2018-2020\n2020-Now\nRAM Highway Network 2017.11.21 (Some popular\n2014.06.24 2015.07.22 (It first successfully attention related (Various transformer\nvariants such as PCT,\n(It adopts RNN and (It proposes to uses self-attention to network such as\nDeformable DETR DeiT,\nreinforcement learning combine different model non-local SAGAN, OCNet,\nT2T-ViT, IPT,PVT and\nto achieve spatial branches by using relationship in DANet, EMANet,\nSwin- Transformer)\nattention) attention method) computer vision) OCRNet and HamNet)"
        },
        {
            "bounding_box": [
                {
                    "x": 352,
                    "y": 606
                },
                {
                    "x": 2061,
                    "y": 606
                },
                {
                    "x": 2061,
                    "y": 963
                },
                {
                    "x": 352,
                    "y": 963
                }
            ],
            "category": "table",
            "html": "<table id='33' style='font-size:14px'><tr><td>STN 2015.06.05</td><td>SENet</td><td>CBAM</td><td>ViT</td></tr><tr><td>(It proposes to select important region by learning affine transformation)</td><td>2017.09.05 (It proposes to adaptively recalibrate channel by using attention weight)</td><td>2018.07.17 (It proposes tp combine channel attention with spatial attention)</td><td>2020.10.22 (The first pure transformer structure achieves great results in computer vision)</td></tr></table>",
            "id": 33,
            "page": 3,
            "text": "STN 2015.06.05 SENet CBAM ViT\n (It proposes to select important region by learning affine transformation) 2017.09.05 (It proposes to adaptively recalibrate channel by using attention weight) 2018.07.17 (It proposes tp combine channel attention with spatial attention)"
        },
        {
            "bounding_box": [
                {
                    "x": 195,
                    "y": 1064
                },
                {
                    "x": 2359,
                    "y": 1064
                },
                {
                    "x": 2359,
                    "y": 1188
                },
                {
                    "x": 195,
                    "y": 1188
                }
            ],
            "category": "paragraph",
            "html": "<p id='34' style='font-size:16px'>Fig. 3. Brief summary of key developments in attention in computer vision, which have loosely occurred in four phases. Phase 1 adopted RNNs to<br>construct attention, a representative method being RAM [31]. Phase 2 explicitly predicted important regions, a representative method being STN [32].<br>Phase 3 implicitly completed the attention process, a representative method being SENet [5]. Phase 4 used self-attention methods [15], [33], [34].</p>",
            "id": 34,
            "page": 3,
            "text": "Fig. 3. Brief summary of key developments in attention in computer vision, which have loosely occurred in four phases. Phase 1 adopted RNNs to\nconstruct attention, a representative method being RAM [31]. Phase 2 explicitly predicted important regions, a representative method being STN [32].\nPhase 3 implicitly completed the attention process, a representative method being SENet [5]. Phase 4 used self-attention methods [15], [33], [34]."
        },
        {
            "bounding_box": [
                {
                    "x": 823,
                    "y": 1237
                },
                {
                    "x": 1727,
                    "y": 1237
                },
                {
                    "x": 1727,
                    "y": 1317
                },
                {
                    "x": 823,
                    "y": 1317
                }
            ],
            "category": "caption",
            "html": "<caption id='35' style='font-size:16px'>TABLE 2<br>Brief summary of attention categories and key related works.</caption>",
            "id": 35,
            "page": 3,
            "text": "TABLE 2\nBrief summary of attention categories and key related works."
        },
        {
            "bounding_box": [
                {
                    "x": 247,
                    "y": 1356
                },
                {
                    "x": 2307,
                    "y": 1356
                },
                {
                    "x": 2307,
                    "y": 2252
                },
                {
                    "x": 247,
                    "y": 2252
                }
            ],
            "category": "table",
            "html": "<table id='36' style='font-size:16px'><tr><td>Attention category</td><td>Description</td><td>Related work</td></tr><tr><td>Channel attention</td><td>Generate attention mask across the channel domain and use it to select important channels.</td><td>[5], [37], [53], [54], [55], [56], [57], [58] [25], [59], [60]</td></tr><tr><td>Spatial attention</td><td>Generate attention mask across spatial domains and use it to select important spatial regions (e.g. [15], [61]) or predict the most relevant spatial position directly (e.g. [7], [31]).</td><td>[8],[9], [15], [21], [31], [32], [34], [35] , [22], [26], [62], [63], [64], [65], [66], [67] , [41], [68], [69], [70], [71], [72], [73], [74] , [8], [34], [42], [43], [75], [76], [77], [78] , [27], [44], [45], [46], [79], [80], [81], [82] , [61], [83], [84], [85], [86], [87], [88], [89] , [47], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104] , [20], [105], [106], [107], [108], [109]</td></tr><tr><td>Temporal attention</td><td>Generate attention mask in time and use it to select key frames.</td><td>[110], [111], [112]</td></tr><tr><td>Branch attention</td><td>Generate attention mask across the different branches and use it to select important branches.</td><td>[113], [114], [115], [116]</td></tr><tr><td>Channel & spatial atten- tion</td><td>Predict channel and spatial attention masks separately (e.g. [6], [117]) or generate a joint 3-D channel, height, width attention mask directly (e.g. [118], [119]) and use it to select important features.</td><td>[6], [50], [117], [119], [120], [121], [122], [10], [101], [118], [123], [124], [125], [126] , [13], [14], [127], [128], [129]</td></tr><tr><td>Spatial & temporal at- tention</td><td>Compute temporal and spatial attention masks sepa- rately (e.g. [16], [130]), or produce a joint spatiotemporal attention mask (e.g. [131]), to focus on informative regions.</td><td>[130], [132], [133], [134], [135], [136], [137], [138], [139]</td></tr></table>",
            "id": 36,
            "page": 3,
            "text": "Attention category Description Related work\n Channel attention Generate attention mask across the channel domain and use it to select important channels. [5], [37], [53], [54], [55], [56], [57], [58] [25], [59], [60]\n Spatial attention Generate attention mask across spatial domains and use it to select important spatial regions (e.g. [15], [61]) or predict the most relevant spatial position directly (e.g. [7], [31]). [8],[9], [15], [21], [31], [32], [34], [35] , [22], [26], [62], [63], [64], [65], [66], [67] , [41], [68], [69], [70], [71], [72], [73], [74] , [8], [34], [42], [43], [75], [76], [77], [78] , [27], [44], [45], [46], [79], [80], [81], [82] , [61], [83], [84], [85], [86], [87], [88], [89] , [47], [90], [91], [92], [93], [94], [95], [96], [97], [98], [99], [100], [101], [102], [103], [104] , [20], [105], [106], [107], [108], [109]\n Temporal attention Generate attention mask in time and use it to select key frames. [110], [111], [112]\n Branch attention Generate attention mask across the different branches and use it to select important branches. [113], [114], [115], [116]\n Channel & spatial atten- tion Predict channel and spatial attention masks separately (e.g. [6], [117]) or generate a joint 3-D channel, height, width attention mask directly (e.g. [118], [119]) and use it to select important features. [6], [50], [117], [119], [120], [121], [122], [10], [101], [118], [123], [124], [125], [126] , [13], [14], [127], [128], [129]\n Spatial & temporal at- tention Compute temporal and spatial attention masks sepa- rately (e.g. [16], [130]), or produce a joint spatiotemporal attention mask (e.g. [131]), to focus on informative regions."
        },
        {
            "bounding_box": [
                {
                    "x": 191,
                    "y": 2331
                },
                {
                    "x": 1255,
                    "y": 2331
                },
                {
                    "x": 1255,
                    "y": 2429
                },
                {
                    "x": 191,
                    "y": 2429
                }
            ],
            "category": "paragraph",
            "html": "<p id='37' style='font-size:18px'>on the attention methods in their own right, rather than<br>treating them as supplementary to other tasks.</p>",
            "id": 37,
            "page": 3,
            "text": "on the attention methods in their own right, rather than\ntreating them as supplementary to other tasks."
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 2569
                },
                {
                    "x": 1170,
                    "y": 2569
                },
                {
                    "x": 1170,
                    "y": 2627
                },
                {
                    "x": 192,
                    "y": 2627
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='40' style='font-size:20px'>3 ATTENTION METHODS IN COMPUTER VISION</p>",
            "id": 38,
            "page": 3,
            "text": "3 ATTENTION METHODS IN COMPUTER VISION"
        },
        {
            "bounding_box": [
                {
                    "x": 191,
                    "y": 2677
                },
                {
                    "x": 1261,
                    "y": 2677
                },
                {
                    "x": 1261,
                    "y": 3115
                },
                {
                    "x": 191,
                    "y": 3115
                }
            ],
            "category": "paragraph",
            "html": "<p id='41' style='font-size:20px'>In this section, we first sum up a general form for the<br>attention mechanism based on the recognition process of<br>human visual system in Sec. 3.1. Then we review various<br>categories of attention models given in Fig. 1, with a<br>subsection dedicated to each category. In each, we tabularize<br>representative works for that category. We also introduce<br>that category of attention strategy more deeply, considering<br>its development in terms of motivation, formulation and<br>function.</p>",
            "id": 39,
            "page": 3,
            "text": "In this section, we first sum up a general form for the\nattention mechanism based on the recognition process of\nhuman visual system in Sec. 3.1. Then we review various\ncategories of attention models given in Fig. 1, with a\nsubsection dedicated to each category. In each, we tabularize\nrepresentative works for that category. We also introduce\nthat category of attention strategy more deeply, considering\nits development in terms of motivation, formulation and\nfunction."
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 2328
                },
                {
                    "x": 1645,
                    "y": 2328
                },
                {
                    "x": 1645,
                    "y": 2377
                },
                {
                    "x": 1292,
                    "y": 2377
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='38' style='font-size:20px'>3.1 General form</p>",
            "id": 40,
            "page": 3,
            "text": "3.1 General form"
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 2398
                },
                {
                    "x": 2361,
                    "y": 2398
                },
                {
                    "x": 2361,
                    "y": 2547
                },
                {
                    "x": 1292,
                    "y": 2547
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='39' style='font-size:20px'>When seeing a scene in our daily life, we will focus on the<br>discriminative regions, and process these regions quickly.<br>The above process can be formulated as:</p>",
            "id": 41,
            "page": 3,
            "text": "When seeing a scene in our daily life, we will focus on the\ndiscriminative regions, and process these regions quickly.\nThe above process can be formulated as:"
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 2725
                },
                {
                    "x": 2358,
                    "y": 2725
                },
                {
                    "x": 2358,
                    "y": 2967
                },
                {
                    "x": 1291,
                    "y": 2967
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='42' style='font-size:20px'>Here g(x) can represent to generate attention which<br>corresponds to the process of attending to the discriminative<br>regions. f(g(x), x) means processing input x based on the<br>attention g(x) which is consistent with processing critical<br>regions and getting information.</p>",
            "id": 42,
            "page": 3,
            "text": "Here g(x) can represent to generate attention which\ncorresponds to the process of attending to the discriminative\nregions. f(g(x), x) means processing input x based on the\nattention g(x) which is consistent with processing critical\nregions and getting information."
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 2970
                },
                {
                    "x": 2361,
                    "y": 2970
                },
                {
                    "x": 2361,
                    "y": 3117
                },
                {
                    "x": 1291,
                    "y": 3117
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='43' style='font-size:20px'>With the above definition, we find that almost all existing<br>attention mechanisms can be written into the above formu-<br>lation. Here we take self-attention [15] and squeeze-and-</p>",
            "id": 43,
            "page": 3,
            "text": "With the above definition, we find that almost all existing\nattention mechanisms can be written into the above formu-\nlation. Here we take self-attention [15] and squeeze-and-"
        },
        {
            "bounding_box": [
                {
                    "x": 191,
                    "y": 108
                },
                {
                    "x": 1077,
                    "y": 108
                },
                {
                    "x": 1077,
                    "y": 150
                },
                {
                    "x": 191,
                    "y": 150
                }
            ],
            "category": "header",
            "html": "<header id='44' style='font-size:14px'>JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015</header>",
            "id": 44,
            "page": 4,
            "text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015"
        },
        {
            "bounding_box": [
                {
                    "x": 2328,
                    "y": 113
                },
                {
                    "x": 2353,
                    "y": 113
                },
                {
                    "x": 2353,
                    "y": 146
                },
                {
                    "x": 2328,
                    "y": 146
                }
            ],
            "category": "header",
            "html": "<br><header id='45' style='font-size:14px'>4</header>",
            "id": 45,
            "page": 4,
            "text": "4"
        },
        {
            "bounding_box": [
                {
                    "x": 195,
                    "y": 165
                },
                {
                    "x": 2351,
                    "y": 165
                },
                {
                    "x": 2351,
                    "y": 2300
                },
                {
                    "x": 195,
                    "y": 2300
                }
            ],
            "category": "figure",
            "html": "<figure><img id='46' style='font-size:14px' alt=\"Improve squeeze\nGSoP-Net, FcaNet, etc.\nmodule\nImprove excitation\nChannel ECANet\nSENet module\nattention\nImprove both squeeze\nSRM, GCT, etc.\nand excitation module\nRNN-based attention RAM DRAW, Glimpse Net, etc.\nPredict the relevant\nSTN DCN, DCN V2, etc.\nregion explicitly\nPredict a soft mask\nSpatial attention\nGENet Efficient self-attention CCNet, EMANet, etc.\nimplicitly\nLocal self-attention SASA, SAN, etc.\nSelf-attention based Non-local\nVisual Transformers DETR, ViT, etc.\nCombininglocal\nattention and global TAM\nTemporal attention\nattention\nSelf-attention based GLTR\nAttention\nCombining features of Highway\nMechanisms SKNet, ReNeSt, etc.\ndifferent branches Network\nBranch attention\nCombine different\nCondConv Dynamic Conv.\nconvolution kernels\nCross-dimension\nTriplet Attention\ninteraction\nSplit channel attention Long-range Coordinate Attention,\nCBAM, BAM, scSE et al.\nand spatial attention dependencies DANet\nChannel &\nSpatial attention Relation-aware\nRGA\nattention\nDirectly Estimate 3D Residual SimAM, Strip Pooling,\nattention map attention SCNet\nSplit temporal\nattention and spatial RSTAN\nattention\nTemporal & Jointly produce spatial\nSTA\nSpatial attention and temporal attention\nPairwise relation-\nSTGCN\nbased\" data-coord=\"top-left:(195,165); bottom-right:(2351,2300)\" /></figure>",
            "id": 46,
            "page": 4,
            "text": "Improve squeeze\nGSoP-Net, FcaNet, etc.\nmodule\nImprove excitation\nChannel ECANet\nSENet module\nattention\nImprove both squeeze\nSRM, GCT, etc.\nand excitation module\nRNN-based attention RAM DRAW, Glimpse Net, etc.\nPredict the relevant\nSTN DCN, DCN V2, etc.\nregion explicitly\nPredict a soft mask\nSpatial attention\nGENet Efficient self-attention CCNet, EMANet, etc.\nimplicitly\nLocal self-attention SASA, SAN, etc.\nSelf-attention based Non-local\nVisual Transformers DETR, ViT, etc.\nCombininglocal\nattention and global TAM\nTemporal attention\nattention\nSelf-attention based GLTR\nAttention\nCombining features of Highway\nMechanisms SKNet, ReNeSt, etc.\ndifferent branches Network\nBranch attention\nCombine different\nCondConv Dynamic Conv.\nconvolution kernels\nCross-dimension\nTriplet Attention\ninteraction\nSplit channel attention Long-range Coordinate Attention,\nCBAM, BAM, scSE et al.\nand spatial attention dependencies DANet\nChannel &\nSpatial attention Relation-aware\nRGA\nattention\nDirectly Estimate 3D Residual SimAM, Strip Pooling,\nattention map attention SCNet\nSplit temporal\nattention and spatial RSTAN\nattention\nTemporal & Jointly produce spatial\nSTA\nSpatial attention and temporal attention\nPairwise relation-\nSTGCN\nbased"
        },
        {
            "bounding_box": [
                {
                    "x": 194,
                    "y": 2386
                },
                {
                    "x": 916,
                    "y": 2386
                },
                {
                    "x": 916,
                    "y": 2436
                },
                {
                    "x": 194,
                    "y": 2436
                }
            ],
            "category": "caption",
            "html": "<caption id='47' style='font-size:16px'>Fig. 4. Developmental context of visual attention.</caption>",
            "id": 47,
            "page": 4,
            "text": "Fig. 4. Developmental context of visual attention."
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 2514
                },
                {
                    "x": 1261,
                    "y": 2514
                },
                {
                    "x": 1261,
                    "y": 2613
                },
                {
                    "x": 192,
                    "y": 2613
                }
            ],
            "category": "paragraph",
            "html": "<p id='48' style='font-size:18px'>excitation(SE) attention [5] as examples. For self-attention,<br>g(x) and f(g(x), x) can be written as</p>",
            "id": 48,
            "page": 4,
            "text": "excitation(SE) attention [5] as examples. For self-attention,\ng(x) and f(g(x), x) can be written as"
        },
        {
            "bounding_box": [
                {
                    "x": 253,
                    "y": 2906
                },
                {
                    "x": 1053,
                    "y": 2906
                },
                {
                    "x": 1053,
                    "y": 2957
                },
                {
                    "x": 253,
                    "y": 2957
                }
            ],
            "category": "paragraph",
            "html": "<p id='49' style='font-size:18px'>For SE, g(x) and f(g(x),x) can be written as</p>",
            "id": 49,
            "page": 4,
            "text": "For SE, g(x) and f(g(x),x) can be written as"
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 2514
                },
                {
                    "x": 2359,
                    "y": 2514
                },
                {
                    "x": 2359,
                    "y": 2615
                },
                {
                    "x": 1292,
                    "y": 2615
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='50' style='font-size:16px'>In the following, we will introduce various attention<br>mechanisms and specify them to the above formulation.</p>",
            "id": 50,
            "page": 4,
            "text": "In the following, we will introduce various attention\nmechanisms and specify them to the above formulation."
        },
        {
            "bounding_box": [
                {
                    "x": 1288,
                    "y": 2703
                },
                {
                    "x": 1740,
                    "y": 2703
                },
                {
                    "x": 1740,
                    "y": 2750
                },
                {
                    "x": 1288,
                    "y": 2750
                }
            ],
            "category": "paragraph",
            "html": "<p id='51' style='font-size:18px'>3.2 Channel Attention</p>",
            "id": 51,
            "page": 4,
            "text": "3.2 Channel Attention"
        },
        {
            "bounding_box": [
                {
                    "x": 1197,
                    "y": 2826
                },
                {
                    "x": 1257,
                    "y": 2826
                },
                {
                    "x": 1257,
                    "y": 2876
                },
                {
                    "x": 1197,
                    "y": 2876
                }
            ],
            "category": "paragraph",
            "html": "<p id='52' style='font-size:22px'>(5)</p>",
            "id": 52,
            "page": 4,
            "text": "(5)"
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 2774
                },
                {
                    "x": 2361,
                    "y": 2774
                },
                {
                    "x": 2361,
                    "y": 3117
                },
                {
                    "x": 1292,
                    "y": 3117
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='53' style='font-size:18px'>In deep neural networks, different channels in different<br>feature maps usually represent different objects [50]. Channel<br>attention adaptively recalibrates the weight of each channel,<br>and can be viewed as an object selection process, thus<br>determining what to pay attention to. Hu et al. [5] first<br>proposed the concept of channel attention and presented<br>SENet for this purpose. As Fig. 4 shows, and we discuss</p>",
            "id": 53,
            "page": 4,
            "text": "In deep neural networks, different channels in different\nfeature maps usually represent different objects [50]. Channel\nattention adaptively recalibrates the weight of each channel,\nand can be viewed as an object selection process, thus\ndetermining what to pay attention to. Hu et al. [5] first\nproposed the concept of channel attention and presented\nSENet for this purpose. As Fig. 4 shows, and we discuss"
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 109
                },
                {
                    "x": 1075,
                    "y": 109
                },
                {
                    "x": 1075,
                    "y": 149
                },
                {
                    "x": 193,
                    "y": 149
                }
            ],
            "category": "header",
            "html": "<header id='54' style='font-size:14px'>JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015</header>",
            "id": 54,
            "page": 5,
            "text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015"
        },
        {
            "bounding_box": [
                {
                    "x": 2328,
                    "y": 115
                },
                {
                    "x": 2352,
                    "y": 115
                },
                {
                    "x": 2352,
                    "y": 143
                },
                {
                    "x": 2328,
                    "y": 143
                }
            ],
            "category": "header",
            "html": "<br><header id='55' style='font-size:14px'>5</header>",
            "id": 55,
            "page": 5,
            "text": "5"
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 181
                },
                {
                    "x": 1255,
                    "y": 181
                },
                {
                    "x": 1255,
                    "y": 273
                },
                {
                    "x": 193,
                    "y": 273
                }
            ],
            "category": "paragraph",
            "html": "<p id='56' style='font-size:18px'>shortly, three streams of work continue to improve channel<br>attention in different ways.</p>",
            "id": 56,
            "page": 5,
            "text": "shortly, three streams of work continue to improve channel\nattention in different ways."
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 278
                },
                {
                    "x": 1258,
                    "y": 278
                },
                {
                    "x": 1258,
                    "y": 521
                },
                {
                    "x": 192,
                    "y": 521
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='57' style='font-size:18px'>In this section, we first summarize the representative<br>channel attention works and specify process g(x) and<br>f(g(x), x) described as Eq. 1 in Tab. 3 and Fig. 5. Then<br>we discuss various channel attention methods along with<br>their development process respectively.</p>",
            "id": 57,
            "page": 5,
            "text": "In this section, we first summarize the representative\nchannel attention works and specify process g(x) and\nf(g(x), x) described as Eq. 1 in Tab. 3 and Fig. 5. Then\nwe discuss various channel attention methods along with\ntheir development process respectively."
        },
        {
            "bounding_box": [
                {
                    "x": 195,
                    "y": 566
                },
                {
                    "x": 450,
                    "y": 566
                },
                {
                    "x": 450,
                    "y": 610
                },
                {
                    "x": 195,
                    "y": 610
                }
            ],
            "category": "paragraph",
            "html": "<p id='58' style='font-size:18px'>3.2.1 SENet</p>",
            "id": 58,
            "page": 5,
            "text": "3.2.1 SENet"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 624
                },
                {
                    "x": 1257,
                    "y": 624
                },
                {
                    "x": 1257,
                    "y": 816
                },
                {
                    "x": 192,
                    "y": 816
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='59' style='font-size:20px'>SENet [5] pioneered channel attention. The core of SENet<br>is a squeeze-and-excitation (SE) block which is used to collect<br>global information, capture channel-wise relationships and<br>improve representation ability.</p>",
            "id": 59,
            "page": 5,
            "text": "SENet [5] pioneered channel attention. The core of SENet\nis a squeeze-and-excitation (SE) block which is used to collect\nglobal information, capture channel-wise relationships and\nimprove representation ability."
        },
        {
            "bounding_box": [
                {
                    "x": 191,
                    "y": 818
                },
                {
                    "x": 1257,
                    "y": 818
                },
                {
                    "x": 1257,
                    "y": 1300
                },
                {
                    "x": 191,
                    "y": 1300
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='60' style='font-size:18px'>SE blocks are divided into two parts, a squeeze module<br>and an excitation module. Global spatial information is<br>collected in the squeeze module by global average pooling.<br>The excitation module captures channel-wise relationships<br>and outputs an attention vector by using fully-connected<br>layers and non-linear layers (ReLU and sigmoid). Then, each<br>channel of the input feature is scaled by multiplying the<br>corresponding element in the attention vector. Overall, a<br>squeeze-and-excitation block Fse (with parameter 0) which<br>takes X as input and outputs Y can be formulated as:</p>",
            "id": 60,
            "page": 5,
            "text": "SE blocks are divided into two parts, a squeeze module\nand an excitation module. Global spatial information is\ncollected in the squeeze module by global average pooling.\nThe excitation module captures channel-wise relationships\nand outputs an attention vector by using fully-connected\nlayers and non-linear layers (ReLU and sigmoid). Then, each\nchannel of the input feature is scaled by multiplying the\ncorresponding element in the attention vector. Overall, a\nsqueeze-and-excitation block Fse (with parameter 0) which\ntakes X as input and outputs Y can be formulated as:"
        },
        {
            "bounding_box": [
                {
                    "x": 191,
                    "y": 1464
                },
                {
                    "x": 1257,
                    "y": 1464
                },
                {
                    "x": 1257,
                    "y": 2046
                },
                {
                    "x": 191,
                    "y": 2046
                }
            ],
            "category": "paragraph",
            "html": "<p id='61' style='font-size:20px'>SE blocks play the role of emphasizing important channels<br>while suppressing noise. An SE block can be added after each<br>residual unit [145] due to their low computational resource<br>requirements. However, SE blocks have shortcomings. In<br>the squeeze module, global average pooling is too simple<br>to capture complex global information. In the excitation<br>module, fully-connected layers increase the complexity of the<br>model. As Fig. 4 indicates, later works attempt to improve the<br>outputs of the squeeze module (e.g. GSoP-Net [54]), reduce<br>the complexity of the model by improving the excitation<br>module (e.g. ECANet [37]), or improve both the squeeze<br>module and the excitation module (e.g. SRM [55]).</p>",
            "id": 61,
            "page": 5,
            "text": "SE blocks play the role of emphasizing important channels\nwhile suppressing noise. An SE block can be added after each\nresidual unit [145] due to their low computational resource\nrequirements. However, SE blocks have shortcomings. In\nthe squeeze module, global average pooling is too simple\nto capture complex global information. In the excitation\nmodule, fully-connected layers increase the complexity of the\nmodel. As Fig. 4 indicates, later works attempt to improve the\noutputs of the squeeze module (e.g. GSoP-Net [54]), reduce\nthe complexity of the model by improving the excitation\nmodule (e.g. ECANet [37]), or improve both the squeeze\nmodule and the excitation module (e.g. SRM [55])."
        },
        {
            "bounding_box": [
                {
                    "x": 195,
                    "y": 2089
                },
                {
                    "x": 515,
                    "y": 2089
                },
                {
                    "x": 515,
                    "y": 2136
                },
                {
                    "x": 195,
                    "y": 2136
                }
            ],
            "category": "paragraph",
            "html": "<p id='62' style='font-size:18px'>3.2.2 GSoP-Net</p>",
            "id": 62,
            "page": 5,
            "text": "3.2.2 GSoP-Net"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 2150
                },
                {
                    "x": 1257,
                    "y": 2150
                },
                {
                    "x": 1257,
                    "y": 2340
                },
                {
                    "x": 192,
                    "y": 2340
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='63' style='font-size:22px'>An SE block captures global information by only using global<br>average pooling (i.e. first-order statistics), which limits its<br>modeling capability, in particular the ability to capture high-<br>order statistics.</p>",
            "id": 63,
            "page": 5,
            "text": "An SE block captures global information by only using global\naverage pooling (i.e. first-order statistics), which limits its\nmodeling capability, in particular the ability to capture high-\norder statistics."
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 2344
                },
                {
                    "x": 1258,
                    "y": 2344
                },
                {
                    "x": 1258,
                    "y": 2535
                },
                {
                    "x": 192,
                    "y": 2535
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='64' style='font-size:20px'>To address this issue, Gao et al. [54] proposed to improve<br>the squeeze module by using a global second-order pooling<br>(GSoP) block to model high-order statistics while gathering<br>global information.</p>",
            "id": 64,
            "page": 5,
            "text": "To address this issue, Gao et al. [54] proposed to improve\nthe squeeze module by using a global second-order pooling\n(GSoP) block to model high-order statistics while gathering\nglobal information."
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 2537
                },
                {
                    "x": 1258,
                    "y": 2537
                },
                {
                    "x": 1258,
                    "y": 2919
                },
                {
                    "x": 192,
                    "y": 2919
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='65' style='font-size:16px'>Like an SE block, a GSoP block also has a squeeze<br>module and an excitation module. In the squeeze module,<br>a GSoP block firstly reduces the number of channels from<br>c to c' (c' < c) using a 1x1 convolution, then computes a<br>c' x c' covariance matrix for the different channels to obtain<br>their correlation. Next, row-wise normalization is performed<br>on the covariance matrix. Each (i,j) in the normalized<br>covariance matrix explicitly relates channel i to channel j.</p>",
            "id": 65,
            "page": 5,
            "text": "Like an SE block, a GSoP block also has a squeeze\nmodule and an excitation module. In the squeeze module,\na GSoP block firstly reduces the number of channels from\nc to c' (c' < c) using a 1x1 convolution, then computes a\nc' x c' covariance matrix for the different channels to obtain\ntheir correlation. Next, row-wise normalization is performed\non the covariance matrix. Each (i,j) in the normalized\ncovariance matrix explicitly relates channel i to channel j."
        },
        {
            "bounding_box": [
                {
                    "x": 1293,
                    "y": 180
                },
                {
                    "x": 2355,
                    "y": 180
                },
                {
                    "x": 2355,
                    "y": 275
                },
                {
                    "x": 1293,
                    "y": 275
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='66' style='font-size:18px'>multiplies the input features by the attention vector, as in an<br>SE block. A GSoP block can be formulated as:</p>",
            "id": 66,
            "page": 5,
            "text": "multiplies the input features by the attention vector, as in an\nSE block. A GSoP block can be formulated as:"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 2922
                },
                {
                    "x": 1258,
                    "y": 2922
                },
                {
                    "x": 1258,
                    "y": 3117
                },
                {
                    "x": 192,
                    "y": 3117
                }
            ],
            "category": "paragraph",
            "html": "<p id='67' style='font-size:18px'>In the excitation module, a GSoP block performs row-wise<br>convolution to maintain structural information and output a<br>vector. Then a fully-connected layer and a sigmoid function<br>are applied to get a c-dimensional attention vector. Finally, it</p>",
            "id": 67,
            "page": 5,
            "text": "In the excitation module, a GSoP block performs row-wise\nconvolution to maintain structural information and output a\nvector. Then a fully-connected layer and a sigmoid function\nare applied to get a c-dimensional attention vector. Finally, it"
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 458
                },
                {
                    "x": 2358,
                    "y": 458
                },
                {
                    "x": 2358,
                    "y": 603
                },
                {
                    "x": 1291,
                    "y": 603
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='68' style='font-size:18px'>Here, Conv(·) reduces the number of channels, Cov(·)<br>computes the covariance matrix and RC(·) means row-wise<br>convolution.</p>",
            "id": 68,
            "page": 5,
            "text": "Here, Conv(·) reduces the number of channels, Cov(·)\ncomputes the covariance matrix and RC(·) means row-wise\nconvolution."
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 607
                },
                {
                    "x": 2358,
                    "y": 607
                },
                {
                    "x": 2358,
                    "y": 848
                },
                {
                    "x": 1291,
                    "y": 848
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='69' style='font-size:18px'>By using second-order pooling, GSoP blocks have im-<br>proved the ability to collect global information over the<br>SE block. However, this comes at the cost of additional<br>computation. Thus, a single GSoP block is typically added<br>after several residual blocks.</p>",
            "id": 69,
            "page": 5,
            "text": "By using second-order pooling, GSoP blocks have im-\nproved the ability to collect global information over the\nSE block. However, this comes at the cost of additional\ncomputation. Thus, a single GSoP block is typically added\nafter several residual blocks."
        },
        {
            "bounding_box": [
                {
                    "x": 1295,
                    "y": 905
                },
                {
                    "x": 1523,
                    "y": 905
                },
                {
                    "x": 1523,
                    "y": 952
                },
                {
                    "x": 1295,
                    "y": 952
                }
            ],
            "category": "paragraph",
            "html": "<p id='70' style='font-size:18px'>3.2.3 SRM</p>",
            "id": 70,
            "page": 5,
            "text": "3.2.3 SRM"
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 971
                },
                {
                    "x": 2358,
                    "y": 971
                },
                {
                    "x": 2358,
                    "y": 1402
                },
                {
                    "x": 1291,
                    "y": 1402
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='71' style='font-size:20px'>Motivated by successes in style transfer, Lee et al. [55] pro-<br>posed the lightweight style-based recalibration module (SRM).<br>SRM combines style transfer with an attention mechanism. Its<br>main contribution is style pooling which utilizes both mean<br>and standard deviation of the input features to improve<br>its capability to capture global information. It also adopts<br>a lightweight channel-wise fully-connected (CFC) layer, in<br>place of the original fully-connected layer, to reduce the<br>computational requirements.</p>",
            "id": 71,
            "page": 5,
            "text": "Motivated by successes in style transfer, Lee et al. [55] pro-\nposed the lightweight style-based recalibration module (SRM).\nSRM combines style transfer with an attention mechanism. Its\nmain contribution is style pooling which utilizes both mean\nand standard deviation of the input features to improve\nits capability to capture global information. It also adopts\na lightweight channel-wise fully-connected (CFC) layer, in\nplace of the original fully-connected layer, to reduce the\ncomputational requirements."
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 1404
                },
                {
                    "x": 2359,
                    "y": 1404
                },
                {
                    "x": 2359,
                    "y": 1840
                },
                {
                    "x": 1291,
                    "y": 1840
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='72' style='font-size:18px'>Given an input feature map X E RCxHxW<br>SRM first<br>collects global information by using style pooling (SP(·))<br>which combines global average pooling and global standard<br>deviation pooling. Then a channel-wise fully connected<br>(CFC(·)) layer (i.e. fully connected per channel), batch nor-<br>malization BN and sigmoid function 0 are used to provide<br>the attention vector. Finally, as in an SE block, the input<br>features are multiplied by the attention vector. Overall, an<br>SRM can be written as:</p>",
            "id": 72,
            "page": 5,
            "text": "Given an input feature map X E RCxHxW\nSRM first\ncollects global information by using style pooling (SP(·))\nwhich combines global average pooling and global standard\ndeviation pooling. Then a channel-wise fully connected\n(CFC(·)) layer (i.e. fully connected per channel), batch nor-\nmalization BN and sigmoid function 0 are used to provide\nthe attention vector. Finally, as in an SE block, the input\nfeatures are multiplied by the attention vector. Overall, an\nSRM can be written as:"
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 2022
                },
                {
                    "x": 2357,
                    "y": 2022
                },
                {
                    "x": 2357,
                    "y": 2166
                },
                {
                    "x": 1291,
                    "y": 2166
                }
            ],
            "category": "paragraph",
            "html": "<p id='73' style='font-size:16px'>The SRM block improves both squeeze and excitation mod-<br>ules, yet can be added after each residual unit like an SE<br>block.</p>",
            "id": 73,
            "page": 5,
            "text": "The SRM block improves both squeeze and excitation mod-\nules, yet can be added after each residual unit like an SE\nblock."
        },
        {
            "bounding_box": [
                {
                    "x": 1293,
                    "y": 2226
                },
                {
                    "x": 1518,
                    "y": 2226
                },
                {
                    "x": 1518,
                    "y": 2273
                },
                {
                    "x": 1293,
                    "y": 2273
                }
            ],
            "category": "paragraph",
            "html": "<p id='74' style='font-size:18px'>3.2.4 GCT</p>",
            "id": 74,
            "page": 5,
            "text": "3.2.4 GCT"
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 2292
                },
                {
                    "x": 2356,
                    "y": 2292
                },
                {
                    "x": 2356,
                    "y": 2678
                },
                {
                    "x": 1291,
                    "y": 2678
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='75' style='font-size:20px'>Due to the computational demand and number of parameters<br>of the fully connected layer in the excitation module, it<br>is impractical to use an SE block after each convolution<br>layer. Furthermore, using fully connected layers to model<br>channel relationships is an implicit procedure. To overcome<br>the above problems, Yang et al. [56] propose the gated channel<br>transformation (GCT) to efficiently collect information while<br>explicitly modeling channel-wise relationships.</p>",
            "id": 75,
            "page": 5,
            "text": "Due to the computational demand and number of parameters\nof the fully connected layer in the excitation module, it\nis impractical to use an SE block after each convolution\nlayer. Furthermore, using fully connected layers to model\nchannel relationships is an implicit procedure. To overcome\nthe above problems, Yang et al. [56] propose the gated channel\ntransformation (GCT) to efficiently collect information while\nexplicitly modeling channel-wise relationships."
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 2682
                },
                {
                    "x": 2360,
                    "y": 2682
                },
                {
                    "x": 2360,
                    "y": 3116
                },
                {
                    "x": 1291,
                    "y": 3116
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='76' style='font-size:18px'>Unlike previous methods, GCT first collects global in-<br>formation by computing the l2-norm of each channel. Next,<br>a learnable vector a is applied to scale the feature. Then a<br>competition mechanism is adopted by channel normalization<br>to interact between channels. Like other common normal-<br>ization methods, a learnable scale parameter 2 and bias �<br>are applied to rescale the normalization. However, unlike<br>previous methods, GCT adopts tanh activation to control<br>the attention vector. Finally, it not only multiplies the input</p>",
            "id": 76,
            "page": 5,
            "text": "Unlike previous methods, GCT first collects global in-\nformation by computing the l2-norm of each channel. Next,\na learnable vector a is applied to scale the feature. Then a\ncompetition mechanism is adopted by channel normalization\nto interact between channels. Like other common normal-\nization methods, a learnable scale parameter 2 and bias �\nare applied to rescale the normalization. However, unlike\nprevious methods, GCT adopts tanh activation to control\nthe attention vector. Finally, it not only multiplies the input"
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 110
                },
                {
                    "x": 1075,
                    "y": 110
                },
                {
                    "x": 1075,
                    "y": 150
                },
                {
                    "x": 193,
                    "y": 150
                }
            ],
            "category": "header",
            "html": "<header id='77' style='font-size:14px'>JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015</header>",
            "id": 77,
            "page": 6,
            "text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015"
        },
        {
            "bounding_box": [
                {
                    "x": 2329,
                    "y": 115
                },
                {
                    "x": 2351,
                    "y": 115
                },
                {
                    "x": 2351,
                    "y": 144
                },
                {
                    "x": 2329,
                    "y": 144
                }
            ],
            "category": "header",
            "html": "<br><header id='78' style='font-size:14px'>6</header>",
            "id": 78,
            "page": 6,
            "text": "6"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 181
                },
                {
                    "x": 1258,
                    "y": 181
                },
                {
                    "x": 1258,
                    "y": 274
                },
                {
                    "x": 192,
                    "y": 274
                }
            ],
            "category": "paragraph",
            "html": "<p id='79' style='font-size:18px'>by the attention vector but also adds an identity connection.<br>GCT can be written as:</p>",
            "id": 79,
            "page": 6,
            "text": "by the attention vector but also adds an identity connection.\nGCT can be written as:"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 438
                },
                {
                    "x": 1255,
                    "y": 438
                },
                {
                    "x": 1255,
                    "y": 533
                },
                {
                    "x": 192,
                    "y": 533
                }
            ],
            "category": "paragraph",
            "html": "<p id='80' style='font-size:18px'>where a, B and 2 are trainable parameters. Norm(·) indicates<br>the L2-norm of each channel. CN is channel normalization.</p>",
            "id": 80,
            "page": 6,
            "text": "where a, B and 2 are trainable parameters. Norm(·) indicates\nthe L2-norm of each channel. CN is channel normalization."
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 536
                },
                {
                    "x": 1258,
                    "y": 536
                },
                {
                    "x": 1258,
                    "y": 679
                },
                {
                    "x": 193,
                    "y": 679
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='81' style='font-size:18px'>A GCT block has fewer parameters than an SE block, and<br>as it is lightweight, can be added after each convolutional<br>layer of a CNN.</p>",
            "id": 81,
            "page": 6,
            "text": "A GCT block has fewer parameters than an SE block, and\nas it is lightweight, can be added after each convolutional\nlayer of a CNN."
        },
        {
            "bounding_box": [
                {
                    "x": 194,
                    "y": 721
                },
                {
                    "x": 477,
                    "y": 721
                },
                {
                    "x": 477,
                    "y": 766
                },
                {
                    "x": 194,
                    "y": 766
                }
            ],
            "category": "paragraph",
            "html": "<p id='82' style='font-size:18px'>3.2.5 ECANet</p>",
            "id": 82,
            "page": 6,
            "text": "3.2.5 ECANet"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 779
                },
                {
                    "x": 1258,
                    "y": 779
                },
                {
                    "x": 1258,
                    "y": 1162
                },
                {
                    "x": 192,
                    "y": 1162
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='83' style='font-size:20px'>To avoid high model complexity, SENet reduces the number<br>of channels. However, this strategy fails to directly model<br>correspondence between weight vectors and inputs, reducing<br>the quality of results. To overcome this drawback, Wang<br>et al. [37] proposed the efficient channel attention (ECA)<br>block which instead uses a 1D convolution to determine<br>the interaction between channels, instead of dimensionality<br>reduction.</p>",
            "id": 83,
            "page": 6,
            "text": "To avoid high model complexity, SENet reduces the number\nof channels. However, this strategy fails to directly model\ncorrespondence between weight vectors and inputs, reducing\nthe quality of results. To overcome this drawback, Wang\net al. [37] proposed the efficient channel attention (ECA)\nblock which instead uses a 1D convolution to determine\nthe interaction between channels, instead of dimensionality\nreduction."
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 1164
                },
                {
                    "x": 1259,
                    "y": 1164
                },
                {
                    "x": 1259,
                    "y": 1502
                },
                {
                    "x": 192,
                    "y": 1502
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='84' style='font-size:18px'>An ECA block has similar formulation to an SE block<br>including a squeeze module for aggregating global spatial<br>information and an efficient excitation module for modeling<br>cross-channel interaction. Instead of indirect correspondence,<br>an ECA block only considers direct interaction between<br>each channel and its k-nearest neighbors to control model<br>complexity. Overall, the formulation of an ECA block is:</p>",
            "id": 84,
            "page": 6,
            "text": "An ECA block has similar formulation to an SE block\nincluding a squeeze module for aggregating global spatial\ninformation and an efficient excitation module for modeling\ncross-channel interaction. Instead of indirect correspondence,\nan ECA block only considers direct interaction between\neach channel and its k-nearest neighbors to control model\ncomplexity. Overall, the formulation of an ECA block is:"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 1660
                },
                {
                    "x": 1258,
                    "y": 1660
                },
                {
                    "x": 1258,
                    "y": 1950
                },
                {
                    "x": 192,
                    "y": 1950
                }
            ],
            "category": "paragraph",
            "html": "<p id='85' style='font-size:20px'>where Conv1D(·) denotes 1D convolution with a kernel of<br>shape k across the channel domain, to model local cross-<br>channel interaction. The parameter k decides the coverage<br>of interaction, and in ECA the kernel size k is adaptively<br>determined from the channel dimensionality C instead of by<br>manual tuning, using cross-validation:</p>",
            "id": 85,
            "page": 6,
            "text": "where Conv1D(·) denotes 1D convolution with a kernel of\nshape k across the channel domain, to model local cross-\nchannel interaction. The parameter k decides the coverage\nof interaction, and in ECA the kernel size k is adaptively\ndetermined from the channel dimensionality C instead of by\nmanual tuning, using cross-validation:"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 2096
                },
                {
                    "x": 1255,
                    "y": 2096
                },
                {
                    "x": 1255,
                    "y": 2194
                },
                {
                    "x": 192,
                    "y": 2194
                }
            ],
            "category": "paragraph",
            "html": "<p id='86' style='font-size:18px'>where 2 and 6 are hyperparameters. |x|odd indicates the<br>nearest odd function of x.</p>",
            "id": 86,
            "page": 6,
            "text": "where 2 and 6 are hyperparameters. |x|odd indicates the\nnearest odd function of x."
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 2195
                },
                {
                    "x": 1257,
                    "y": 2195
                },
                {
                    "x": 1257,
                    "y": 2340
                },
                {
                    "x": 193,
                    "y": 2340
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='87' style='font-size:20px'>Compared to SENet, ECANet has an improved excitation<br>module, and provides an efficient and effective block which<br>can readily be incorporated into various CNNs.</p>",
            "id": 87,
            "page": 6,
            "text": "Compared to SENet, ECANet has an improved excitation\nmodule, and provides an efficient and effective block which\ncan readily be incorporated into various CNNs."
        },
        {
            "bounding_box": [
                {
                    "x": 1295,
                    "y": 180
                },
                {
                    "x": 2356,
                    "y": 180
                },
                {
                    "x": 2356,
                    "y": 276
                },
                {
                    "x": 1295,
                    "y": 276
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='88' style='font-size:16px'>ReLU activation and a sigmoid are used to get the attention<br>vector as in an SE block. This can be formulated as:</p>",
            "id": 88,
            "page": 6,
            "text": "ReLU activation and a sigmoid are used to get the attention\nvector as in an SE block. This can be formulated as:"
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 465
                },
                {
                    "x": 2354,
                    "y": 465
                },
                {
                    "x": 2354,
                    "y": 558
                },
                {
                    "x": 1292,
                    "y": 558
                }
            ],
            "category": "paragraph",
            "html": "<p id='89' style='font-size:20px'>where Group(·) indicates dividing the input into many<br>groups and DCT(·) is the 2D discrete cosine transform.</p>",
            "id": 89,
            "page": 6,
            "text": "where Group(·) indicates dividing the input into many\ngroups and DCT(·) is the 2D discrete cosine transform."
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 565
                },
                {
                    "x": 2357,
                    "y": 565
                },
                {
                    "x": 2357,
                    "y": 708
                },
                {
                    "x": 1292,
                    "y": 708
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='90' style='font-size:18px'>This work based on information compression and discrete<br>cosine transforms achieves excellent performance on the<br>classification task.</p>",
            "id": 90,
            "page": 6,
            "text": "This work based on information compression and discrete\ncosine transforms achieves excellent performance on the\nclassification task."
        },
        {
            "bounding_box": [
                {
                    "x": 1293,
                    "y": 767
                },
                {
                    "x": 1565,
                    "y": 767
                },
                {
                    "x": 1565,
                    "y": 815
                },
                {
                    "x": 1293,
                    "y": 815
                }
            ],
            "category": "paragraph",
            "html": "<p id='91' style='font-size:20px'>3.2.7 EncNet</p>",
            "id": 91,
            "page": 6,
            "text": "3.2.7 EncNet"
        },
        {
            "bounding_box": [
                {
                    "x": 195,
                    "y": 2379
                },
                {
                    "x": 463,
                    "y": 2379
                },
                {
                    "x": 463,
                    "y": 2427
                },
                {
                    "x": 195,
                    "y": 2427
                }
            ],
            "category": "paragraph",
            "html": "<p id='92' style='font-size:20px'>3.2.6 FcaNet</p>",
            "id": 92,
            "page": 6,
            "text": "3.2.6 FcaNet"
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 835
                },
                {
                    "x": 2357,
                    "y": 835
                },
                {
                    "x": 2357,
                    "y": 1075
                },
                {
                    "x": 1291,
                    "y": 1075
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='93' style='font-size:22px'>Inspired by SENet, Zhang et al. [53] proposed the context<br>encoding module (CEM) incorporating semantic encoding loss<br>(SE-loss) to model the relationship between scene context<br>and the probabilities of object categories, thus utilizing global<br>scene contextual information for semantic segmentation.</p>",
            "id": 93,
            "page": 6,
            "text": "Inspired by SENet, Zhang et al. [53] proposed the context\nencoding module (CEM) incorporating semantic encoding loss\n(SE-loss) to model the relationship between scene context\nand the probabilities of object categories, thus utilizing global\nscene contextual information for semantic segmentation."
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 1076
                },
                {
                    "x": 2358,
                    "y": 1076
                },
                {
                    "x": 2358,
                    "y": 1513
                },
                {
                    "x": 1291,
                    "y": 1513
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='94' style='font-size:18px'>Given an input feature map X E RCxHxW CEM first<br>, a<br>learns K cluster centers D = {d1, · · · , dK} and a set of<br>smoothing factors S = {S1, · · · , SK} in the training phase.<br>Next, it sums the difference between the local descriptors<br>in the input and the corresponding cluster centers using<br>soft-assignment weights to obtain a permutation-invariant<br>descriptor. Then, it applies aggregation to the descriptors of<br>the K cluster centers instead of concatenation for computa-<br>tional efficiency. Formally, CEM can be written as:</p>",
            "id": 94,
            "page": 6,
            "text": "Given an input feature map X E RCxHxW CEM first\n, a\nlearns K cluster centers D = {d1, · · · , dK} and a set of\nsmoothing factors S = {S1, · · · , SK} in the training phase.\nNext, it sums the difference between the local descriptors\nin the input and the corresponding cluster centers using\nsoft-assignment weights to obtain a permutation-invariant\ndescriptor. Then, it applies aggregation to the descriptors of\nthe K cluster centers instead of concatenation for computa-\ntional efficiency. Formally, CEM can be written as:"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 2438
                },
                {
                    "x": 1257,
                    "y": 2438
                },
                {
                    "x": 1257,
                    "y": 2820
                },
                {
                    "x": 192,
                    "y": 2820
                }
            ],
            "category": "paragraph",
            "html": "<p id='95' style='font-size:20px'>Only using global average pooling in the squeeze module<br>limits representational ability. To obtain a more powerful<br>representation ability, Qin et al. [57] rethought global infor-<br>mation captured from the viewpoint of compression and<br>analysed global average pooling in the frequency domain.<br>They proved that global average pooling is a special case of<br>the discrete cosine transform (DCT) and used this observa-<br>tion to propose a novel multi-spectral channel attention.</p>",
            "id": 95,
            "page": 6,
            "text": "Only using global average pooling in the squeeze module\nlimits representational ability. To obtain a more powerful\nrepresentation ability, Qin et al. [57] rethought global infor-\nmation captured from the viewpoint of compression and\nanalysed global average pooling in the frequency domain.\nThey proved that global average pooling is a special case of\nthe discrete cosine transform (DCT) and used this observa-\ntion to propose a novel multi-spectral channel attention."
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 2823
                },
                {
                    "x": 1259,
                    "y": 2823
                },
                {
                    "x": 1259,
                    "y": 3115
                },
                {
                    "x": 192,
                    "y": 3115
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='96' style='font-size:18px'>Given an input feature map X E RCxHxW<br>, multi-<br>spectral channel attention first splits X into many parts<br>xi E RC\"xHxW<br>Then it applies a 2D DCT to each part<br>xi. Note that a 2D DCT can use pre-processing results to<br>reduce computation. After processing each part, all results<br>are concatenated into a vector. Finally, fully connected layers,</p>",
            "id": 96,
            "page": 6,
            "text": "Given an input feature map X E RCxHxW\n, multi-\nspectral channel attention first splits X into many parts\nxi E RC\"xHxW\nThen it applies a 2D DCT to each part\nxi. Note that a 2D DCT can use pre-processing results to\nreduce computation. After processing each part, all results\nare concatenated into a vector. Finally, fully connected layers,"
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 1962
                },
                {
                    "x": 2359,
                    "y": 1962
                },
                {
                    "x": 2359,
                    "y": 2255
                },
                {
                    "x": 1291,
                    "y": 2255
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='97' style='font-size:20px'>where dk E RC and Sk E R are learnable parameters.<br>○ denotes batch normalization with ReLU activation. In<br>addition to channel-wise scaling vectors, the compact con-<br>textual descriptor e is also applied to compute the SE-loss<br>to regularize training, which improves the segmentation of<br>small objects.</p>",
            "id": 97,
            "page": 6,
            "text": "where dk E RC and Sk E R are learnable parameters.\n○ denotes batch normalization with ReLU activation. In\naddition to channel-wise scaling vectors, the compact con-\ntextual descriptor e is also applied to compute the SE-loss\nto regularize training, which improves the segmentation of\nsmall objects."
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 2260
                },
                {
                    "x": 2357,
                    "y": 2260
                },
                {
                    "x": 2357,
                    "y": 2502
                },
                {
                    "x": 1291,
                    "y": 2502
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='98' style='font-size:20px'>Not only does CEM enhance class-dependent feature<br>maps, but it also forces the network to consider big and<br>small objects equally by incorporating SE-loss. Due to its<br>lightweight architecture, CEM can be applied to various<br>backbones with only low computational overhead.</p>",
            "id": 98,
            "page": 6,
            "text": "Not only does CEM enhance class-dependent feature\nmaps, but it also forces the network to consider big and\nsmall objects equally by incorporating SE-loss. Due to its\nlightweight architecture, CEM can be applied to various\nbackbones with only low computational overhead."
        },
        {
            "bounding_box": [
                {
                    "x": 1294,
                    "y": 2562
                },
                {
                    "x": 1736,
                    "y": 2562
                },
                {
                    "x": 1736,
                    "y": 2608
                },
                {
                    "x": 1294,
                    "y": 2608
                }
            ],
            "category": "paragraph",
            "html": "<p id='99' style='font-size:20px'>3.2.8 Bilinear Attention</p>",
            "id": 99,
            "page": 6,
            "text": "3.2.8 Bilinear Attention"
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 2628
                },
                {
                    "x": 2357,
                    "y": 2628
                },
                {
                    "x": 2357,
                    "y": 2918
                },
                {
                    "x": 1292,
                    "y": 2918
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='100' style='font-size:20px'>Following GSoP-Net [54], Fang et al. [146] claimed that<br>previous attention models only use first-order information<br>and disregard higher-order statistical information. They thus<br>proposed a new bilinear attention block (bi-attention) to capture<br>local pairwise feature interactions within each channel, while<br>preserving spatial information.</p>",
            "id": 100,
            "page": 6,
            "text": "Following GSoP-Net [54], Fang et al. [146] claimed that\nprevious attention models only use first-order information\nand disregard higher-order statistical information. They thus\nproposed a new bilinear attention block (bi-attention) to capture\nlocal pairwise feature interactions within each channel, while\npreserving spatial information."
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 2921
                },
                {
                    "x": 2358,
                    "y": 2921
                },
                {
                    "x": 2358,
                    "y": 3115
                },
                {
                    "x": 1291,
                    "y": 3115
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='101' style='font-size:20px'>Bi-attention employs the attention-in-attention (AiA) mech-<br>anism to capture second-order statistical information: the<br>outer point-wise channel attention vectors are computed<br>from the output of the inner channel attention. Formally,</p>",
            "id": 101,
            "page": 6,
            "text": "Bi-attention employs the attention-in-attention (AiA) mech-\nanism to capture second-order statistical information: the\nouter point-wise channel attention vectors are computed\nfrom the output of the inner channel attention. Formally,"
        },
        {
            "bounding_box": [
                {
                    "x": 191,
                    "y": 108
                },
                {
                    "x": 1077,
                    "y": 108
                },
                {
                    "x": 1077,
                    "y": 150
                },
                {
                    "x": 191,
                    "y": 150
                }
            ],
            "category": "header",
            "html": "<header id='102' style='font-size:14px'>JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015</header>",
            "id": 102,
            "page": 7,
            "text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015"
        },
        {
            "bounding_box": [
                {
                    "x": 2330,
                    "y": 114
                },
                {
                    "x": 2352,
                    "y": 114
                },
                {
                    "x": 2352,
                    "y": 141
                },
                {
                    "x": 2330,
                    "y": 141
                }
            ],
            "category": "header",
            "html": "<br><header id='103' style='font-size:14px'>7</header>",
            "id": 103,
            "page": 7,
            "text": "7"
        },
        {
            "bounding_box": [
                {
                    "x": 195,
                    "y": 176
                },
                {
                    "x": 2355,
                    "y": 176
                },
                {
                    "x": 2355,
                    "y": 812
                },
                {
                    "x": 195,
                    "y": 812
                }
            ],
            "category": "figure",
            "html": "<figure><img id='104' style='font-size:14px' alt=\"Feature Feature Feature Feature Feature Feature Feature\nL2-norm\nGAP Conv2d GAP DCT\nEncode\nGAP STD a\nFC Cov pool FC\nCN\nConv1d FC\nReLU RW Conv CFC\nReLU\nB\nFC FC BN FC\nY\nSigmoid\nSigmoid\nSigmoid Sigmoid Sigmoid tanh Sigmoid\nOutput Output Output Output Output Output Output\n(a)SE Block (b)GSoP Block (c)SRM Block (d)GCT Block (e)ECA Block (f)Fca Block (g)Enc Block\" data-coord=\"top-left:(195,176); bottom-right:(2355,812)\" /></figure>",
            "id": 104,
            "page": 7,
            "text": "Feature Feature Feature Feature Feature Feature Feature\nL2-norm\nGAP Conv2d GAP DCT\nEncode\nGAP STD a\nFC Cov pool FC\nCN\nConv1d FC\nReLU RW Conv CFC\nReLU\nB\nFC FC BN FC\nY\nSigmoid\nSigmoid\nSigmoid Sigmoid Sigmoid tanh Sigmoid\nOutput Output Output Output Output Output Output\n(a)SE Block (b)GSoP Block (c)SRM Block (d)GCT Block (e)ECA Block (f)Fca Block (g)Enc Block"
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 908
                },
                {
                    "x": 2359,
                    "y": 908
                },
                {
                    "x": 2359,
                    "y": 1028
                },
                {
                    "x": 193,
                    "y": 1028
                }
            ],
            "category": "caption",
            "html": "<caption id='105' style='font-size:18px'>Fig. 5. Various channel attention mechanisms. GAP=global average pooling, GMP=global max pooling, FC=fully-connected layer, Cov<br>pool=Covariance pooling, RW Conv=row-wise convolution, CFC=channel-wise fully connected, CN=channel normalization, DCT=discrete cosine<br>transform.</caption>",
            "id": 105,
            "page": 7,
            "text": "Fig. 5. Various channel attention mechanisms. GAP=global average pooling, GMP=global max pooling, FC=fully-connected layer, Cov\npool=Covariance pooling, RW Conv=row-wise convolution, CFC=channel-wise fully connected, CN=channel normalization, DCT=discrete cosine\ntransform."
        },
        {
            "bounding_box": [
                {
                    "x": 195,
                    "y": 1092
                },
                {
                    "x": 2357,
                    "y": 1092
                },
                {
                    "x": 2357,
                    "y": 1306
                },
                {
                    "x": 195,
                    "y": 1306
                }
            ],
            "category": "caption",
            "html": "<caption id='106' style='font-size:16px'>TABLE 3<br>Representative channel attention mechanisms ordered by category and publication date. Their key aims are to emphasize important channels and<br>capture global information. Application areas include: Cls = classification, Det = detection, SSeg = semantic segmentation, ISeg = instance<br>segmentation, ST = style transfer, Action = action recognition.g(x) and f(g(x), x) are the attention process described by Eq. 1. Ranges means the<br>ranges of attention map. S or H means soft or hard attention.(A) channel-wise product. (I) emphasize important channels, (II) capture global<br>information.</caption>",
            "id": 106,
            "page": 7,
            "text": "TABLE 3\nRepresentative channel attention mechanisms ordered by category and publication date. Their key aims are to emphasize important channels and\ncapture global information. Application areas include: Cls = classification, Det = detection, SSeg = semantic segmentation, ISeg = instance\nsegmentation, ST = style transfer, Action = action recognition.g(x) and f(g(x), x) are the attention process described by Eq. 1. Ranges means the\nranges of attention map. S or H means soft or hard attention.(A) channel-wise product. (I) emphasize important channels, (II) capture global\ninformation."
        },
        {
            "bounding_box": [
                {
                    "x": 214,
                    "y": 1339
                },
                {
                    "x": 2334,
                    "y": 1339
                },
                {
                    "x": 2334,
                    "y": 2155
                },
                {
                    "x": 214,
                    "y": 2155
                }
            ],
            "category": "table",
            "html": "<table id='107' style='font-size:14px'><tr><td>Category</td><td>Method</td><td>Publication</td><td>Tasks</td><td>g(x)</td><td>f(g(x), x)</td><td>Ranges</td><td>S or H</td><td>Goals</td></tr><tr><td>Squeeze-and- excitation network</td><td>SENet [5]</td><td>CVPR2018</td><td>Cls, Det</td><td>global average pooling -> MLP -> sigmoid.</td><td>(A)</td><td>(0,1)</td><td>S</td><td>(I),(II)</td></tr><tr><td rowspan=\"3\">Improve squeeze module</td><td>EncNet [53]</td><td>CVPR2018</td><td>SSeg</td><td>encoder -> MLP -> sig- moid.</td><td>(A)</td><td>(0,1)</td><td>S</td><td>(1),(II)</td></tr><tr><td>GSoP-Net [54]</td><td>CVPR2019</td><td>Cls</td><td>2nd-order pooling -> convolution & MLP -> sigmoid</td><td>(A)</td><td>(0,1)</td><td>S</td><td></td></tr><tr><td>FcaNet [57]</td><td>ICCV2021</td><td>Cls, Det, ISeg</td><td>discrete cosine trans- form -> MLP -> sig- moid.</td><td>(A)</td><td>(0,1)</td><td>S</td><td>(I),(II)</td></tr><tr><td>Improve excita- tion module</td><td>ECANet [37]</td><td>CVPR2020</td><td>Cls, Det, ISeg</td><td>global average pooling -> conv1d -> sigmoid.</td><td>(A)</td><td>(0,1)</td><td>S</td><td>(1),(II)</td></tr><tr><td rowspan=\"2\">Improve both squeeze and excitation module</td><td>SRM [55]</td><td>arXiv2019</td><td>Cls, ST</td><td>style pooling -> convolu- tion & MLP -> sigmoid.</td><td>(A)</td><td>(0,1)</td><td>S</td><td>(I),(II)</td></tr><tr><td>GCT [56]</td><td>CVPR2020</td><td>Cls, Det, Action</td><td>compute L2-norm on spatial -> channel nor- malization -> tanh.</td><td>(A)</td><td>(-1,1)</td><td>S</td><td>(1)</td></tr></table>",
            "id": 107,
            "page": 7,
            "text": "Category Method Publication Tasks g(x) f(g(x), x) Ranges S or H Goals\n Squeeze-and- excitation network SENet [5] CVPR2018 Cls, Det global average pooling -> MLP -> sigmoid. (A) (0,1) S (I),(II)\n Improve squeeze module EncNet [53] CVPR2018 SSeg encoder -> MLP -> sig- moid. (A) (0,1) S (1),(II)\n GSoP-Net [54] CVPR2019 Cls 2nd-order pooling -> convolution & MLP -> sigmoid (A) (0,1) S \n FcaNet [57] ICCV2021 Cls, Det, ISeg discrete cosine trans- form -> MLP -> sig- moid. (A) (0,1) S (I),(II)\n Improve excita- tion module ECANet [37] CVPR2020 Cls, Det, ISeg global average pooling -> conv1d -> sigmoid. (A) (0,1) S (1),(II)\n Improve both squeeze and excitation module SRM [55] arXiv2019 Cls, ST style pooling -> convolu- tion & MLP -> sigmoid. (A) (0,1) S (I),(II)\n GCT [56] CVPR2020 Cls, Det, Action compute L2-norm on spatial -> channel nor- malization -> tanh. (A) (-1,1) S"
        },
        {
            "bounding_box": [
                {
                    "x": 191,
                    "y": 2233
                },
                {
                    "x": 1257,
                    "y": 2233
                },
                {
                    "x": 1257,
                    "y": 2331
                },
                {
                    "x": 191,
                    "y": 2331
                }
            ],
            "category": "paragraph",
            "html": "<p id='108' style='font-size:20px'>given the input feature map X, bi-attention first uses bilinear<br>pooling to capture second-order information</p>",
            "id": 108,
            "page": 7,
            "text": "given the input feature map X, bi-attention first uses bilinear\npooling to capture second-order information"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 2423
                },
                {
                    "x": 1258,
                    "y": 2423
                },
                {
                    "x": 1258,
                    "y": 2728
                },
                {
                    "x": 192,
                    "y": 2728
                }
            ],
            "category": "paragraph",
            "html": "<p id='109' style='font-size:18px'>where O denotes an embedding function used for dimen-<br>sionality reduction, ⌀(x)T is the transpose of ⌀(x) across<br>the channel domain, Utri(·) extracts the upper triangular<br>elements of a matrix and Vec(·) is vectorization. Then bi-<br>attention applies the inner channel attention mechanism to<br>c'(c'+1) xHxW<br>the feature map x E R 2</p>",
            "id": 109,
            "page": 7,
            "text": "where O denotes an embedding function used for dimen-\nsionality reduction, ⌀(x)T is the transpose of ⌀(x) across\nthe channel domain, Utri(·) extracts the upper triangular\nelements of a matrix and Vec(·) is vectorization. Then bi-\nattention applies the inner channel attention mechanism to\nc'(c'+1) xHxW\nthe feature map x E R 2"
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 2820
                },
                {
                    "x": 1257,
                    "y": 2820
                },
                {
                    "x": 1257,
                    "y": 2968
                },
                {
                    "x": 193,
                    "y": 2968
                }
            ],
            "category": "paragraph",
            "html": "<p id='110' style='font-size:20px'>Here 3 and 4 are embedding functions. Finally the output<br>feature map x is used to compute the spatial channel atten-<br>tion weights of the outer point-wise attention mechanism:</p>",
            "id": 110,
            "page": 7,
            "text": "Here 3 and 4 are embedding functions. Finally the output\nfeature map x is used to compute the spatial channel atten-\ntion weights of the outer point-wise attention mechanism:"
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 2232
                },
                {
                    "x": 2360,
                    "y": 2232
                },
                {
                    "x": 2360,
                    "y": 2576
                },
                {
                    "x": 1292,
                    "y": 2576
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='111' style='font-size:20px'>The bi-attention block uses bilinear pooling to model the<br>local pairwise feature interactions along each channel, while<br>preserving the spatial information. Using the proposed AiA,<br>the model pays more attention to higher-order statistical<br>information compared with other attention-based models.<br>Bi-attention can be incorporated into any CNN backbone to<br>improve its representational power while suppressing noise.</p>",
            "id": 111,
            "page": 7,
            "text": "The bi-attention block uses bilinear pooling to model the\nlocal pairwise feature interactions along each channel, while\npreserving the spatial information. Using the proposed AiA,\nthe model pays more attention to higher-order statistical\ninformation compared with other attention-based models.\nBi-attention can be incorporated into any CNN backbone to\nimprove its representational power while suppressing noise."
        },
        {
            "bounding_box": [
                {
                    "x": 1293,
                    "y": 2619
                },
                {
                    "x": 1713,
                    "y": 2619
                },
                {
                    "x": 1713,
                    "y": 2666
                },
                {
                    "x": 1293,
                    "y": 2666
                }
            ],
            "category": "paragraph",
            "html": "<p id='112' style='font-size:22px'>3.3 Spatial Attention</p>",
            "id": 112,
            "page": 7,
            "text": "3.3 Spatial Attention"
        },
        {
            "bounding_box": [
                {
                    "x": 1293,
                    "y": 2681
                },
                {
                    "x": 2361,
                    "y": 2681
                },
                {
                    "x": 2361,
                    "y": 3115
                },
                {
                    "x": 1293,
                    "y": 3115
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='113' style='font-size:18px'>Spatial attention can be seen as an adaptive spatial region<br>selection mechanism: where to pay attention. As Fig. 4 shows,<br>RAM [31], STN [32], GENet [61] and Non-Local [15] are<br>representative of different kinds of spatial attention methods.<br>RAM represents RNN-based methods. STN represents those<br>use a sub-network to explicitly predict relevant regions.<br>GENet represents those that use a sub-network implicitly to<br>predict a soft mask to select important regions. Non-Local<br>represents self-attention related methods. In this subsection,</p>",
            "id": 113,
            "page": 7,
            "text": "Spatial attention can be seen as an adaptive spatial region\nselection mechanism: where to pay attention. As Fig. 4 shows,\nRAM [31], STN [32], GENet [61] and Non-Local [15] are\nrepresentative of different kinds of spatial attention methods.\nRAM represents RNN-based methods. STN represents those\nuse a sub-network to explicitly predict relevant regions.\nGENet represents those that use a sub-network implicitly to\npredict a soft mask to select important regions. Non-Local\nrepresents self-attention related methods. In this subsection,"
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 109
                },
                {
                    "x": 1076,
                    "y": 109
                },
                {
                    "x": 1076,
                    "y": 150
                },
                {
                    "x": 193,
                    "y": 150
                }
            ],
            "category": "header",
            "html": "<header id='114' style='font-size:14px'>JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015</header>",
            "id": 114,
            "page": 8,
            "text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015"
        },
        {
            "bounding_box": [
                {
                    "x": 2328,
                    "y": 115
                },
                {
                    "x": 2351,
                    "y": 115
                },
                {
                    "x": 2351,
                    "y": 143
                },
                {
                    "x": 2328,
                    "y": 143
                }
            ],
            "category": "header",
            "html": "<br><header id='115' style='font-size:14px'>8</header>",
            "id": 115,
            "page": 8,
            "text": "8"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 181
                },
                {
                    "x": 1258,
                    "y": 181
                },
                {
                    "x": 1258,
                    "y": 327
                },
                {
                    "x": 192,
                    "y": 327
                }
            ],
            "category": "paragraph",
            "html": "<p id='116' style='font-size:18px'>we first summarize representative spatial attention mecha-<br>nisms and specify process g(x) and f(g(x), x) described as<br>Eq. 1 in Tab. 4, then discuss them according to Fig. 4.</p>",
            "id": 116,
            "page": 8,
            "text": "we first summarize representative spatial attention mecha-\nnisms and specify process g(x) and f(g(x), x) described as\nEq. 1 in Tab. 4, then discuss them according to Fig. 4."
        },
        {
            "bounding_box": [
                {
                    "x": 195,
                    "y": 358
                },
                {
                    "x": 423,
                    "y": 358
                },
                {
                    "x": 423,
                    "y": 402
                },
                {
                    "x": 195,
                    "y": 402
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='117' style='font-size:16px'>3.3.1 RAM</p>",
            "id": 117,
            "page": 8,
            "text": "3.3.1 RAM"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 414
                },
                {
                    "x": 1258,
                    "y": 414
                },
                {
                    "x": 1258,
                    "y": 798
                },
                {
                    "x": 192,
                    "y": 798
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='118' style='font-size:18px'>Convolutional neural networks have huge computational<br>costs, especially for large inputs. In order to concentrate<br>limited computing resources on important regions, Mnih et<br>al. [31] proposed the recurrent attention model (RAM) that<br>adopts RNNs [147] and reinforcement learning (RL) [148]<br>to make the network learn where to pay attention. RAM<br>pioneered the use of RNNs for visual attention, and was<br>followed by many other RNN-based methods [21], [35], [88].</p>",
            "id": 118,
            "page": 8,
            "text": "Convolutional neural networks have huge computational\ncosts, especially for large inputs. In order to concentrate\nlimited computing resources on important regions, Mnih et\nal. [31] proposed the recurrent attention model (RAM) that\nadopts RNNs [147] and reinforcement learning (RL) [148]\nto make the network learn where to pay attention. RAM\npioneered the use of RNNs for visual attention, and was\nfollowed by many other RNN-based methods [21], [35], [88]."
        },
        {
            "bounding_box": [
                {
                    "x": 191,
                    "y": 795
                },
                {
                    "x": 1257,
                    "y": 795
                },
                {
                    "x": 1257,
                    "y": 1375
                },
                {
                    "x": 191,
                    "y": 1375
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='119' style='font-size:18px'>As shown in Fig. 6, the RAM has three key elements: (A)<br>a glimpse sensor, (B) a glimpse network and (C) an RNN<br>model. The glimpse sensor takes a coordinate lt-1 and an<br>image Xt. It outputs multiple resolution patches p(Xt, lt-1)<br>centered on lt-1. The glimpse network fg(0(g)) includes a<br>glimpse sensor and outputs the feature representation 9t<br>for input coordinate lt-1 and image Xt. The RNN model<br>considers 9t and an internal state ht-1 and outputs the next<br>center coordinate lt and the action at, e.g. the softmax result<br>in an image classification task. Since the whole process is not<br>differentiable, it applies reinforcement learning strategies in<br>the update process.</p>",
            "id": 119,
            "page": 8,
            "text": "As shown in Fig. 6, the RAM has three key elements: (A)\na glimpse sensor, (B) a glimpse network and (C) an RNN\nmodel. The glimpse sensor takes a coordinate lt-1 and an\nimage Xt. It outputs multiple resolution patches p(Xt, lt-1)\ncentered on lt-1. The glimpse network fg(0(g)) includes a\nglimpse sensor and outputs the feature representation 9t\nfor input coordinate lt-1 and image Xt. The RNN model\nconsiders 9t and an internal state ht-1 and outputs the next\ncenter coordinate lt and the action at, e.g. the softmax result\nin an image classification task. Since the whole process is not\ndifferentiable, it applies reinforcement learning strategies in\nthe update process."
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 1376
                },
                {
                    "x": 1258,
                    "y": 1376
                },
                {
                    "x": 1258,
                    "y": 1568
                },
                {
                    "x": 192,
                    "y": 1568
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='120' style='font-size:18px'>This provides a simple but effective method to focus<br>the network on key regions, thus reducing the number of<br>calculations performed by the network, especially for large<br>inputs, while improving image classification results.</p>",
            "id": 120,
            "page": 8,
            "text": "This provides a simple but effective method to focus\nthe network on key regions, thus reducing the number of\ncalculations performed by the network, especially for large\ninputs, while improving image classification results."
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 1599
                },
                {
                    "x": 635,
                    "y": 1599
                },
                {
                    "x": 635,
                    "y": 1647
                },
                {
                    "x": 193,
                    "y": 1647
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='121' style='font-size:20px'>3.3.2 Glimpse Network</p>",
            "id": 121,
            "page": 8,
            "text": "3.3.2 Glimpse Network"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 1658
                },
                {
                    "x": 1258,
                    "y": 1658
                },
                {
                    "x": 1258,
                    "y": 2088
                },
                {
                    "x": 192,
                    "y": 2088
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='122' style='font-size:18px'>Inspired by how humans perform visual recognition se-<br>quentially, Ba et al. [88] proposed a deep recurrent network,<br>similar to RAM [31], capable of processing a multi-resolution<br>crop of the input image, called a glimpse, for multiple object<br>recognition task. The proposed network updates its hidden<br>state using a glimpse as input, and then predicts a new object<br>as well as the next glimpse location at each step. The glimpse<br>is usually much smaller than the whole image, which makes<br>the network computationally efficient.</p>",
            "id": 122,
            "page": 8,
            "text": "Inspired by how humans perform visual recognition se-\nquentially, Ba et al. [88] proposed a deep recurrent network,\nsimilar to RAM [31], capable of processing a multi-resolution\ncrop of the input image, called a glimpse, for multiple object\nrecognition task. The proposed network updates its hidden\nstate using a glimpse as input, and then predicts a new object\nas well as the next glimpse location at each step. The glimpse\nis usually much smaller than the whole image, which makes\nthe network computationally efficient."
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 2089
                },
                {
                    "x": 1258,
                    "y": 2089
                },
                {
                    "x": 1258,
                    "y": 2521
                },
                {
                    "x": 192,
                    "y": 2521
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='123' style='font-size:16px'>The proposed deep recurrent visual attention model<br>consists of a context network, glimpse network, recurrent<br>network, emission network, and classification network. First,<br>the context network takes the down-sampled whole image as<br>input to provide the initial state for the recurrent network as<br>well as the location of the first glimpse. Then, at the current<br>time step t, given the current glimpse Xt and its location<br>tuple lt, the goal of the glimpse network is to extract useful<br>information, expressed as</p>",
            "id": 123,
            "page": 8,
            "text": "The proposed deep recurrent visual attention model\nconsists of a context network, glimpse network, recurrent\nnetwork, emission network, and classification network. First,\nthe context network takes the down-sampled whole image as\ninput to provide the initial state for the recurrent network as\nwell as the location of the first glimpse. Then, at the current\ntime step t, given the current glimpse Xt and its location\ntuple lt, the goal of the glimpse network is to extract useful\ninformation, expressed as"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 2615
                },
                {
                    "x": 1258,
                    "y": 2615
                },
                {
                    "x": 1258,
                    "y": 2952
                },
                {
                    "x": 192,
                    "y": 2952
                }
            ],
            "category": "paragraph",
            "html": "<p id='124' style='font-size:18px'>where fimage (X) and floc (lt) are non-linear functions which<br>both output vectors having the same dimension, and ·<br>denotes element-wise product, used for fusing information<br>from two branches. Then, the recurrent network, which con-<br>sists of two stacked recurrent layers, aggregates information<br>gathered from each individual glimpse. The outputs of the<br>recurrent layers are:</p>",
            "id": 124,
            "page": 8,
            "text": "where fimage (X) and floc (lt) are non-linear functions which\nboth output vectors having the same dimension, and ·\ndenotes element-wise product, used for fusing information\nfrom two branches. Then, the recurrent network, which con-\nsists of two stacked recurrent layers, aggregates information\ngathered from each individual glimpse. The outputs of the\nrecurrent layers are:"
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 171
                },
                {
                    "x": 2359,
                    "y": 171
                },
                {
                    "x": 2359,
                    "y": 325
                },
                {
                    "x": 1292,
                    "y": 325
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='125' style='font-size:16px'>(2)<br>of the recurrent network,<br>Given the current hidden state rt<br>the emission network predicts where to crop the next glimpse.<br>Formally, it can be written as</p>",
            "id": 125,
            "page": 8,
            "text": "(2)\nof the recurrent network,\nGiven the current hidden state rt\nthe emission network predicts where to crop the next glimpse.\nFormally, it can be written as"
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 428
                },
                {
                    "x": 2356,
                    "y": 428
                },
                {
                    "x": 2356,
                    "y": 579
                },
                {
                    "x": 1291,
                    "y": 579
                }
            ],
            "category": "paragraph",
            "html": "<p id='126' style='font-size:14px'>Finally, the classification network outputs a prediction for the<br>정신<br>class label y based on the hidden state of the recurrent<br>network</p>",
            "id": 126,
            "page": 8,
            "text": "Finally, the classification network outputs a prediction for the\n정신\nclass label y based on the hidden state of the recurrent\nnetwork"
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 679
                },
                {
                    "x": 2357,
                    "y": 679
                },
                {
                    "x": 2357,
                    "y": 1062
                },
                {
                    "x": 1291,
                    "y": 1062
                }
            ],
            "category": "paragraph",
            "html": "<p id='127' style='font-size:18px'>Compared to a CNN operating on the entire image, the<br>computational cost of the proposed model is much lower, and<br>it can naturally tackle images of different sizes because it only<br>processes a glimpse in each step. Robustness is additionally<br>improved by the recurrent attention mechanism, which also<br>alleviates the problem of over-fitting. This pipeline can be<br>incorporated into any state-of-the-art CNN backbones or<br>RNN units.</p>",
            "id": 127,
            "page": 8,
            "text": "Compared to a CNN operating on the entire image, the\ncomputational cost of the proposed model is much lower, and\nit can naturally tackle images of different sizes because it only\nprocesses a glimpse in each step. Robustness is additionally\nimproved by the recurrent attention mechanism, which also\nalleviates the problem of over-fitting. This pipeline can be\nincorporated into any state-of-the-art CNN backbones or\nRNN units."
        },
        {
            "bounding_box": [
                {
                    "x": 1295,
                    "y": 1101
                },
                {
                    "x": 1835,
                    "y": 1101
                },
                {
                    "x": 1835,
                    "y": 1148
                },
                {
                    "x": 1295,
                    "y": 1148
                }
            ],
            "category": "paragraph",
            "html": "<p id='128' style='font-size:18px'>3.3.3 Hard and soft attention</p>",
            "id": 128,
            "page": 8,
            "text": "3.3.3 Hard and soft attention"
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 1159
                },
                {
                    "x": 2359,
                    "y": 1159
                },
                {
                    "x": 2359,
                    "y": 1348
                },
                {
                    "x": 1292,
                    "y": 1348
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='129' style='font-size:16px'>To visualize where and what an image caption generation<br>model should focus on, Xu et al. [35] introduced an attention-<br>based model as well as two variant attention mechanisms,<br>hard attention and soft attention.</p>",
            "id": 129,
            "page": 8,
            "text": "To visualize where and what an image caption generation\nmodel should focus on, Xu et al. [35] introduced an attention-\nbased model as well as two variant attention mechanisms,\nhard attention and soft attention."
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 1350
                },
                {
                    "x": 2359,
                    "y": 1350
                },
                {
                    "x": 2359,
                    "y": 1783
                },
                {
                    "x": 1291,
                    "y": 1783
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='130' style='font-size:16px'>Given a set of feature vectors a = {a1, · · · , aL}, ai E RD<br>extracted from the input image, the model aims to produce<br>a caption by generating one word at each time step. Thus<br>they adopt a long short-term memory (LSTM) network as<br>a decoder; an attention mechanism is used to generate a<br>contextual vector Zt conditioned on the feature set a and the<br>previous hidden state ht-1, where t denotes the time step.<br>Formally, the weight at,i vector ai at the t-th<br>of the feature<br>time step is defined as</p>",
            "id": 130,
            "page": 8,
            "text": "Given a set of feature vectors a = {a1, · · · , aL}, ai E RD\nextracted from the input image, the model aims to produce\na caption by generating one word at each time step. Thus\nthey adopt a long short-term memory (LSTM) network as\na decoder; an attention mechanism is used to generate a\ncontextual vector Zt conditioned on the feature set a and the\nprevious hidden state ht-1, where t denotes the time step.\nFormally, the weight at,i vector ai at the t-th\nof the feature\ntime step is defined as"
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 1995
                },
                {
                    "x": 2357,
                    "y": 1995
                },
                {
                    "x": 2357,
                    "y": 2382
                },
                {
                    "x": 1291,
                    "y": 2382
                }
            ],
            "category": "paragraph",
            "html": "<p id='131' style='font-size:18px'>where fatt is implemented by a multilayer perceptron condi-<br>tioned on the previous hidden state ht-1. The positive weight<br>at,i can be interpreted either as the probability that location i<br>is the right place to focus on (hard attention), or as the relative<br>importance of location i to the next word (soft attention). To<br>obtain the contextual vector Zt, the hard attention mechanism<br>assigns a multinoulli distribution parametrized by {at,i} and<br>views Zt as a random variable:</p>",
            "id": 131,
            "page": 8,
            "text": "where fatt is implemented by a multilayer perceptron condi-\ntioned on the previous hidden state ht-1. The positive weight\nat,i can be interpreted either as the probability that location i\nis the right place to focus on (hard attention), or as the relative\nimportance of location i to the next word (soft attention). To\nobtain the contextual vector Zt, the hard attention mechanism\nassigns a multinoulli distribution parametrized by {at,i} and\nviews Zt as a random variable:"
        },
        {
            "bounding_box": [
                {
                    "x": 1293,
                    "y": 2609
                },
                {
                    "x": 2356,
                    "y": 2609
                },
                {
                    "x": 2356,
                    "y": 2707
                },
                {
                    "x": 1293,
                    "y": 2707
                }
            ],
            "category": "paragraph",
            "html": "<p id='132' style='font-size:16px'>On the other hand, the soft attention mechanism directly<br>uses the expectation of the context vector Zt,</p>",
            "id": 132,
            "page": 8,
            "text": "On the other hand, the soft attention mechanism directly\nuses the expectation of the context vector Zt,"
        },
        {
            "bounding_box": [
                {
                    "x": 1290,
                    "y": 2871
                },
                {
                    "x": 2358,
                    "y": 2871
                },
                {
                    "x": 2358,
                    "y": 3115
                },
                {
                    "x": 1290,
                    "y": 3115
                }
            ],
            "category": "paragraph",
            "html": "<p id='133' style='font-size:18px'>The use of the attention mechanism improves the in-<br>terpretability of the image caption generation process by<br>allowing the user to understand what and where the model<br>is focusing on. It also helps to improve the representational<br>capability of the network.</p>",
            "id": 133,
            "page": 8,
            "text": "The use of the attention mechanism improves the in-\nterpretability of the image caption generation process by\nallowing the user to understand what and where the model\nis focusing on. It also helps to improve the representational\ncapability of the network."
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 109
                },
                {
                    "x": 1077,
                    "y": 109
                },
                {
                    "x": 1077,
                    "y": 149
                },
                {
                    "x": 192,
                    "y": 149
                }
            ],
            "category": "header",
            "html": "<header id='134' style='font-size:14px'>JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015</header>",
            "id": 134,
            "page": 9,
            "text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015"
        },
        {
            "bounding_box": [
                {
                    "x": 2328,
                    "y": 115
                },
                {
                    "x": 2352,
                    "y": 115
                },
                {
                    "x": 2352,
                    "y": 145
                },
                {
                    "x": 2328,
                    "y": 145
                }
            ],
            "category": "header",
            "html": "<br><header id='135' style='font-size:14px'>9</header>",
            "id": 135,
            "page": 9,
            "text": "9"
        },
        {
            "bounding_box": [
                {
                    "x": 1206,
                    "y": 184
                },
                {
                    "x": 1344,
                    "y": 184
                },
                {
                    "x": 1344,
                    "y": 221
                },
                {
                    "x": 1206,
                    "y": 221
                }
            ],
            "category": "header",
            "html": "<header id='136' style='font-size:14px'>TABLE 4</header>",
            "id": 136,
            "page": 9,
            "text": "TABLE 4"
        },
        {
            "bounding_box": [
                {
                    "x": 195,
                    "y": 220
                },
                {
                    "x": 2355,
                    "y": 220
                },
                {
                    "x": 2355,
                    "y": 454
                },
                {
                    "x": 195,
                    "y": 454
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='137' style='font-size:16px'>Representative spatial attention mechanisms sorted by category and date. Application areas include: Cls = classification, FGCls = fine-grained<br>classification, Det = detection, SSeg = semantic segmentation, ISeg = instance segmentation, ST = style transfer, Action = action recognition, ICap =<br>image captioning.g(x) and f(g(x), x) are the attention process described by Eq. 1. Ranges means the ranges of attention map. S or H means soft or<br>hard attention.(A) choose region according to the prediction. (B) element-wise product, (C)aggregate information via attention map. (I)focus the<br>network on discriminative regions, (II) avoid excessive computation for large input images, (III) provide more transformation invariance, (IV) capture<br>long-range dependencies, (V) denoise input feature map (VI) adaptively aggregate neighborhood information, (VII) reduce inductive bias.</p>",
            "id": 137,
            "page": 9,
            "text": "Representative spatial attention mechanisms sorted by category and date. Application areas include: Cls = classification, FGCls = fine-grained\nclassification, Det = detection, SSeg = semantic segmentation, ISeg = instance segmentation, ST = style transfer, Action = action recognition, ICap =\nimage captioning.g(x) and f(g(x), x) are the attention process described by Eq. 1. Ranges means the ranges of attention map. S or H means soft or\nhard attention.(A) choose region according to the prediction. (B) element-wise product, (C)aggregate information via attention map. (I)focus the\nnetwork on discriminative regions, (II) avoid excessive computation for large input images, (III) provide more transformation invariance, (IV) capture\nlong-range dependencies, (V) denoise input feature map (VI) adaptively aggregate neighborhood information, (VII) reduce inductive bias."
        },
        {
            "bounding_box": [
                {
                    "x": 217,
                    "y": 495
                },
                {
                    "x": 2332,
                    "y": 495
                },
                {
                    "x": 2332,
                    "y": 1684
                },
                {
                    "x": 217,
                    "y": 1684
                }
            ],
            "category": "table",
            "html": "<table id='138' style='font-size:14px'><tr><td>Category</td><td>Method</td><td>Publication</td><td>Tasks</td><td>g(x)</td><td>f(g(x),x)</td><td>Ranges</td><td>S or H</td><td>Goals</td></tr><tr><td rowspan=\"2\">RNN-based methods</td><td>RAM [31]</td><td>NIPS2014</td><td>Cls</td><td>use RNN to recurrently predict important re- gions</td><td>(A)</td><td>(0,1)</td><td>H</td><td>(I), (II).</td></tr><tr><td>Hard and soft attention [35]</td><td>ICML2015</td><td>ICap</td><td>a)compute similarity be- tween visual features and previous hidden state -> interpret atten- tion weight.</td><td>(C)</td><td>(0,1)</td><td>S, H</td><td>(I).</td></tr><tr><td rowspan=\"2\">Predict the rele- vant region ex- plictly</td><td>STN [32]</td><td>NIPS2015</td><td>Cls, FG- Cls</td><td>use sub-network to pre- dict an affine transfor- mation.</td><td>(A)</td><td>(0,1)</td><td>H</td><td>(I), (III).</td></tr><tr><td>DCN [7]</td><td>ICCV2017</td><td>Det, SSeg</td><td>use sub-network to pre- dict offset coordinates.</td><td>(A)</td><td>(0,1)</td><td>H</td><td>(I), (III).</td></tr><tr><td rowspan=\"2\">Predict the rele- vant region im- plictly</td><td>GENet [61]</td><td>NIPS2018</td><td>Cls, Det</td><td>average pooling or depth-wise convolution -> interpolation -> sigmoid</td><td>(B)</td><td>(0,1)</td><td>S</td><td>(I).</td></tr><tr><td>PSANet [87]</td><td>ECCV2018</td><td>SSeg</td><td>predict an attention map using a sub-network.</td><td>(C)</td><td>(0,1)</td><td>S</td><td>(I), (IV).</td></tr><tr><td rowspan=\"3\">Self-attention based methods</td><td>Non-Local [15]</td><td>CVPR2018</td><td>Action, Det, ISeg</td><td>Dot product between query and key -> soft- max</td><td>(C)</td><td>(0,1)</td><td>S</td><td>(I), (IV), (V)</td></tr><tr><td>SASA [43]</td><td>NeurIPS2019</td><td>Cls, Det</td><td>Dot product between query and key -> soft- max.</td><td>(C)</td><td>(0,1)</td><td>S</td><td>(I), (VI)</td></tr><tr><td>ViT [34]</td><td>ICLR2021</td><td>Cls</td><td>divide the feature map into multiple groups - > Dot product between query and key -> soft- max.</td><td>(C)</td><td>(0,1)</td><td>S</td><td>(I),(IV), (VII).</td></tr></table>",
            "id": 138,
            "page": 9,
            "text": "Category Method Publication Tasks g(x) f(g(x),x) Ranges S or H Goals\n RNN-based methods RAM [31] NIPS2014 Cls use RNN to recurrently predict important re- gions (A) (0,1) H (I), (II).\n Hard and soft attention [35] ICML2015 ICap a)compute similarity be- tween visual features and previous hidden state -> interpret atten- tion weight. (C) (0,1) S, H (I).\n Predict the rele- vant region ex- plictly STN [32] NIPS2015 Cls, FG- Cls use sub-network to pre- dict an affine transfor- mation. (A) (0,1) H (I), (III).\n DCN [7] ICCV2017 Det, SSeg use sub-network to pre- dict offset coordinates. (A) (0,1) H (I), (III).\n Predict the rele- vant region im- plictly GENet [61] NIPS2018 Cls, Det average pooling or depth-wise convolution -> interpolation -> sigmoid (B) (0,1) S (I).\n PSANet [87] ECCV2018 SSeg predict an attention map using a sub-network. (C) (0,1) S (I), (IV).\n Self-attention based methods Non-Local [15] CVPR2018 Action, Det, ISeg Dot product between query and key -> soft- max (C) (0,1) S (I), (IV), (V)\n SASA [43] NeurIPS2019 Cls, Det Dot product between query and key -> soft- max. (C) (0,1) S (I), (VI)\n ViT [34] ICLR2021 Cls divide the feature map into multiple groups - > Dot product between query and key -> soft- max. (C) (0,1) S"
        },
        {
            "bounding_box": [
                {
                    "x": 194,
                    "y": 1760
                },
                {
                    "x": 588,
                    "y": 1760
                },
                {
                    "x": 588,
                    "y": 1808
                },
                {
                    "x": 194,
                    "y": 1808
                }
            ],
            "category": "paragraph",
            "html": "<p id='139' style='font-size:20px'>3.3.4 Attention Gate</p>",
            "id": 139,
            "page": 9,
            "text": "3.3.4 Attention Gate"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 1826
                },
                {
                    "x": 1257,
                    "y": 1826
                },
                {
                    "x": 1257,
                    "y": 2161
                },
                {
                    "x": 192,
                    "y": 2161
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='140' style='font-size:18px'>Previous approaches to MR segmentation usually operate on<br>particular regions of interest (ROI), which requires excessive<br>and wasteful use of computational resources and model<br>parameters. To address this issue, Oktay et al. [19] proposed<br>a simple and yet effective mechanism, the attention gate<br>(AG), to focus on targeted regions while suppressing feature<br>activations in irrelevant regions.</p>",
            "id": 140,
            "page": 9,
            "text": "Previous approaches to MR segmentation usually operate on\nparticular regions of interest (ROI), which requires excessive\nand wasteful use of computational resources and model\nparameters. To address this issue, Oktay et al. [19] proposed\na simple and yet effective mechanism, the attention gate\n(AG), to focus on targeted regions while suppressing feature\nactivations in irrelevant regions."
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 2165
                },
                {
                    "x": 1258,
                    "y": 2165
                },
                {
                    "x": 1258,
                    "y": 2600
                },
                {
                    "x": 192,
                    "y": 2600
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='141' style='font-size:18px'>Given the input feature map X and the gating signal<br>G E RC'xHxW<br>which is collected at a coarse scale and<br>contains contextual information, the attention gate uses<br>additive attention to obtain the gating coefficient. Both the<br>input X and the gating signal are first linearly mapped<br>dimensional space, and then the output<br>to an RFxHxW<br>is squeezed in the channel domain to produce a spatial<br>The overall process can<br>attention weight map S E R1xHxW<br>be written as</p>",
            "id": 141,
            "page": 9,
            "text": "Given the input feature map X and the gating signal\nG E RC'xHxW\nwhich is collected at a coarse scale and\ncontains contextual information, the attention gate uses\nadditive attention to obtain the gating coefficient. Both the\ninput X and the gating signal are first linearly mapped\ndimensional space, and then the output\nto an RFxHxW\nis squeezed in the channel domain to produce a spatial\nThe overall process can\nattention weight map S E R1xHxW\nbe written as"
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 2775
                },
                {
                    "x": 1254,
                    "y": 2775
                },
                {
                    "x": 1254,
                    "y": 2869
                },
                {
                    "x": 193,
                    "y": 2869
                }
            ],
            "category": "paragraph",
            "html": "<p id='142' style='font-size:18px'>where 4, ⌀x and ⌀g are linear transformations implemented<br>as 1 x 1 convolutions.</p>",
            "id": 142,
            "page": 9,
            "text": "where 4, ⌀x and ⌀g are linear transformations implemented\nas 1 x 1 convolutions."
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 2874
                },
                {
                    "x": 1258,
                    "y": 2874
                },
                {
                    "x": 1258,
                    "y": 3116
                },
                {
                    "x": 193,
                    "y": 3116
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='143' style='font-size:18px'>The attention gate guides the model's attention to impor-<br>tant regions while suppressing feature activation in unrelated<br>areas. It substantially enhances the representational power of<br>the model without a significant increase in computing cost or<br>number of model parameters due to its lightweight design.</p>",
            "id": 143,
            "page": 9,
            "text": "The attention gate guides the model's attention to impor-\ntant regions while suppressing feature activation in unrelated\nareas. It substantially enhances the representational power of\nthe model without a significant increase in computing cost or\nnumber of model parameters due to its lightweight design."
        },
        {
            "bounding_box": [
                {
                    "x": 1293,
                    "y": 1763
                },
                {
                    "x": 2355,
                    "y": 1763
                },
                {
                    "x": 2355,
                    "y": 1856
                },
                {
                    "x": 1293,
                    "y": 1856
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='144' style='font-size:18px'>It is general and modular, making it simple to use in various<br>CNN models.</p>",
            "id": 144,
            "page": 9,
            "text": "It is general and modular, making it simple to use in various\nCNN models."
        },
        {
            "bounding_box": [
                {
                    "x": 1294,
                    "y": 1901
                },
                {
                    "x": 1514,
                    "y": 1901
                },
                {
                    "x": 1514,
                    "y": 1946
                },
                {
                    "x": 1294,
                    "y": 1946
                }
            ],
            "category": "paragraph",
            "html": "<p id='145' style='font-size:18px'>3.3.5 STN</p>",
            "id": 145,
            "page": 9,
            "text": "3.3.5 STN"
        },
        {
            "bounding_box": [
                {
                    "x": 1295,
                    "y": 1959
                },
                {
                    "x": 2361,
                    "y": 1959
                },
                {
                    "x": 2361,
                    "y": 2583
                },
                {
                    "x": 1295,
                    "y": 2583
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='146' style='font-size:18px'>The property of translation equivariance makes CNNs<br>suitable for processing image data. However, CNNs lack<br>other transformation invariance such as rotational invariance,<br>scaling invariance and warping invariance. To achieve these<br>attributes while making CNNs focus on important regions,<br>Jaderberg et al. [32] proposed spatial transformer networks<br>(STN) that use an explicit procedure to learn invariance to<br>translation, scaling, rotation and other more general warps,<br>making the network pay attention to the most relevant<br>regions. STN was the first attention mechanism to explicitly<br>predict important regions and provide a deep neural network<br>with transformation invariance. Various following works [7],<br>[36] have had even greater success.</p>",
            "id": 146,
            "page": 9,
            "text": "The property of translation equivariance makes CNNs\nsuitable for processing image data. However, CNNs lack\nother transformation invariance such as rotational invariance,\nscaling invariance and warping invariance. To achieve these\nattributes while making CNNs focus on important regions,\nJaderberg et al. [32] proposed spatial transformer networks\n(STN) that use an explicit procedure to learn invariance to\ntranslation, scaling, rotation and other more general warps,\nmaking the network pay attention to the most relevant\nregions. STN was the first attention mechanism to explicitly\npredict important regions and provide a deep neural network\nwith transformation invariance. Various following works [7],\n[36] have had even greater success."
        },
        {
            "bounding_box": [
                {
                    "x": 1294,
                    "y": 2587
                },
                {
                    "x": 2360,
                    "y": 2587
                },
                {
                    "x": 2360,
                    "y": 2680
                },
                {
                    "x": 1294,
                    "y": 2680
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='147' style='font-size:16px'>Taking a 2D image as an example, a 2D affine transforma-<br>tion can be formulated as:</p>",
            "id": 147,
            "page": 9,
            "text": "Taking a 2D image as an example, a 2D affine transforma-\ntion can be formulated as:"
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 2966
                },
                {
                    "x": 2358,
                    "y": 2966
                },
                {
                    "x": 2358,
                    "y": 3118
                },
                {
                    "x": 1291,
                    "y": 3118
                }
            ],
            "category": "paragraph",
            "html": "<p id='148' style='font-size:18px'>Here, U is the input feature map, and floc can be any<br>differentiable function, such as a lightweight fully-connected<br>network or convolutional neural network. xs and Yi are</p>",
            "id": 148,
            "page": 9,
            "text": "Here, U is the input feature map, and floc can be any\ndifferentiable function, such as a lightweight fully-connected\nnetwork or convolutional neural network. xs and Yi are"
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 109
                },
                {
                    "x": 1077,
                    "y": 109
                },
                {
                    "x": 1077,
                    "y": 150
                },
                {
                    "x": 193,
                    "y": 150
                }
            ],
            "category": "header",
            "html": "<header id='149' style='font-size:14px'>JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015</header>",
            "id": 149,
            "page": 10,
            "text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015"
        },
        {
            "bounding_box": [
                {
                    "x": 2315,
                    "y": 114
                },
                {
                    "x": 2352,
                    "y": 114
                },
                {
                    "x": 2352,
                    "y": 145
                },
                {
                    "x": 2315,
                    "y": 145
                }
            ],
            "category": "header",
            "html": "<br><header id='150' style='font-size:14px'>10</header>",
            "id": 150,
            "page": 10,
            "text": "10"
        },
        {
            "bounding_box": [
                {
                    "x": 189,
                    "y": 182
                },
                {
                    "x": 2350,
                    "y": 182
                },
                {
                    "x": 2350,
                    "y": 1082
                },
                {
                    "x": 189,
                    "y": 1082
                }
            ],
            "category": "figure",
            "html": "<figure><img id='151' style='font-size:20px' alt=\"5 5\nA) C) lt-1\n/\nt-1 p(xt, lt-1)\nfg(0g) fg(0g)\nXt\nGlimpse Sensor\ngt |gt+1\nI\nht+1\nfh(Oh) fh(oh)\nB)\n5 Glimpse\nSensor\np(xt, lt-1)\nXt 0g gt fa(0a) fl(01) fa(0a) fl(Ol)\n01\ng\nt-1 0g)\nGlimpse Network .. fg(\na I+ at+ 7 t+ \" data-coord=\"top-left:(189,182); bottom-right:(2350,1082)\" /></figure>",
            "id": 151,
            "page": 10,
            "text": "5 5\nA) C) lt-1\n/\nt-1 p(xt, lt-1)\nfg(0g) fg(0g)\nXt\nGlimpse Sensor\ngt |gt+1\nI\nht+1\nfh(Oh) fh(oh)\nB)\n5 Glimpse\nSensor\np(xt, lt-1)\nXt 0g gt fa(0a) fl(01) fa(0a) fl(Ol)\n01\ng\nt-1 0g)\nGlimpse Network .. fg(\na I+ at+ 7 t+ "
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 1163
                },
                {
                    "x": 2359,
                    "y": 1163
                },
                {
                    "x": 2359,
                    "y": 1289
                },
                {
                    "x": 192,
                    "y": 1289
                }
            ],
            "category": "caption",
            "html": "<caption id='152' style='font-size:16px'>Fig. 6. Attention process in RAM [31]. (A): a glimpse sensor takes image and center coordinates as input and outputs multiple resolution patches. (B):<br>a glimpse network includes a glimpse sensor, taking image and center coordinates as input and outputting a feature vector. (C) the entire network<br>recurrently uses a glimpse network, outputting the predicted result as well as the next center coordinates. Figure is taken from [31].</caption>",
            "id": 152,
            "page": 10,
            "text": "Fig. 6. Attention process in RAM [31]. (A): a glimpse sensor takes image and center coordinates as input and outputs multiple resolution patches. (B):\na glimpse network includes a glimpse sensor, taking image and center coordinates as input and outputting a feature vector. (C) the entire network\nrecurrently uses a glimpse network, outputting the predicted result as well as the next center coordinates. Figure is taken from [31]."
        },
        {
            "bounding_box": [
                {
                    "x": 191,
                    "y": 1366
                },
                {
                    "x": 1258,
                    "y": 1366
                },
                {
                    "x": 1258,
                    "y": 1751
                },
                {
                    "x": 191,
                    "y": 1751
                }
            ],
            "category": "paragraph",
            "html": "<p id='153' style='font-size:18px'>coordinates in the output feature map, while xt and yt are<br>corresponding coordinates in the input feature map and<br>the 0 matrix is the learnable affine matrix. After obtaining<br>the correspondence, the network can sample relevant input<br>regions using the correspondence. To ensure that the whole<br>process is differentiable and can be updated in an end-to-<br>end manner, bilinear sampling is used to sample the input<br>features</p>",
            "id": 153,
            "page": 10,
            "text": "coordinates in the output feature map, while xt and yt are\ncorresponding coordinates in the input feature map and\nthe 0 matrix is the learnable affine matrix. After obtaining\nthe correspondence, the network can sample relevant input\nregions using the correspondence. To ensure that the whole\nprocess is differentiable and can be updated in an end-to-\nend manner, bilinear sampling is used to sample the input\nfeatures"
        },
        {
            "bounding_box": [
                {
                    "x": 195,
                    "y": 1753
                },
                {
                    "x": 1255,
                    "y": 1753
                },
                {
                    "x": 1255,
                    "y": 1849
                },
                {
                    "x": 195,
                    "y": 1849
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='154' style='font-size:18px'>STNs focus on discriminative regions automatically and<br>learn invariance to some geometric transformations.</p>",
            "id": 154,
            "page": 10,
            "text": "STNs focus on discriminative regions automatically and\nlearn invariance to some geometric transformations."
        },
        {
            "bounding_box": [
                {
                    "x": 194,
                    "y": 1890
                },
                {
                    "x": 963,
                    "y": 1890
                },
                {
                    "x": 963,
                    "y": 1941
                },
                {
                    "x": 194,
                    "y": 1941
                }
            ],
            "category": "paragraph",
            "html": "<p id='155' style='font-size:18px'>3.3.6 Deformable Convolutional Networks</p>",
            "id": 155,
            "page": 10,
            "text": "3.3.6 Deformable Convolutional Networks"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 1950
                },
                {
                    "x": 1257,
                    "y": 1950
                },
                {
                    "x": 1257,
                    "y": 2141
                },
                {
                    "x": 192,
                    "y": 2141
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='156' style='font-size:18px'>With similar purpose to STNs, Dai et al. [7] proposed<br>deformable convolutional networks (deformable ConvNets) to<br>be invariant to geometric transformations, but they pay<br>attention to the important regions in a different manner.</p>",
            "id": 156,
            "page": 10,
            "text": "With similar purpose to STNs, Dai et al. [7] proposed\ndeformable convolutional networks (deformable ConvNets) to\nbe invariant to geometric transformations, but they pay\nattention to the important regions in a different manner."
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 2143
                },
                {
                    "x": 1259,
                    "y": 2143
                },
                {
                    "x": 1259,
                    "y": 2430
                },
                {
                    "x": 192,
                    "y": 2430
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='157' style='font-size:18px'>Specifically, deformable ConvNets do not learn an affine<br>transformation. They divide convolution into two steps,<br>firstly sampling features on a regular grid R from the input<br>feature map, then aggregating sampled features by weighted<br>summation using a convolution kernel. The process can be<br>written as:</p>",
            "id": 157,
            "page": 10,
            "text": "Specifically, deformable ConvNets do not learn an affine\ntransformation. They divide convolution into two steps,\nfirstly sampling features on a regular grid R from the input\nfeature map, then aggregating sampled features by weighted\nsummation using a convolution kernel. The process can be\nwritten as:"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 2636
                },
                {
                    "x": 1258,
                    "y": 2636
                },
                {
                    "x": 1258,
                    "y": 2829
                },
                {
                    "x": 192,
                    "y": 2829
                }
            ],
            "category": "paragraph",
            "html": "<p id='158' style='font-size:18px'>The deformable convolution augments the sampling process<br>by introducing a group of learnable offsets △pi which can<br>be generated by a lightweight CNN. Using the offsets △pi,<br>the deformable convolution can be formulated as:</p>",
            "id": 158,
            "page": 10,
            "text": "The deformable convolution augments the sampling process\nby introducing a group of learnable offsets △pi which can\nbe generated by a lightweight CNN. Using the offsets △pi,\nthe deformable convolution can be formulated as:"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 2967
                },
                {
                    "x": 1257,
                    "y": 2967
                },
                {
                    "x": 1257,
                    "y": 3116
                },
                {
                    "x": 192,
                    "y": 3116
                }
            ],
            "category": "paragraph",
            "html": "<p id='159' style='font-size:20px'>Through the above method, adaptive sampling is achieved.<br>However, △pi is a floating point value unsuited to grid<br>sampling. To address this problem, bilinear interpolation is</p>",
            "id": 159,
            "page": 10,
            "text": "Through the above method, adaptive sampling is achieved.\nHowever, △pi is a floating point value unsuited to grid\nsampling. To address this problem, bilinear interpolation is"
        },
        {
            "bounding_box": [
                {
                    "x": 1293,
                    "y": 1368
                },
                {
                    "x": 2356,
                    "y": 1368
                },
                {
                    "x": 2356,
                    "y": 1460
                },
                {
                    "x": 1293,
                    "y": 1460
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='160' style='font-size:20px'>used. Deformable RoI pooling is also used, which greatly<br>improves object detection.</p>",
            "id": 160,
            "page": 10,
            "text": "used. Deformable RoI pooling is also used, which greatly\nimproves object detection."
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 1465
                },
                {
                    "x": 2356,
                    "y": 1465
                },
                {
                    "x": 2356,
                    "y": 1656
                },
                {
                    "x": 1292,
                    "y": 1656
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='161' style='font-size:18px'>Deformable ConvNets adaptively select the important<br>regions and enlarge the valid receptive field of convolutional<br>neural networks; this is important in object detection and<br>semantic segmentation tasks.</p>",
            "id": 161,
            "page": 10,
            "text": "Deformable ConvNets adaptively select the important\nregions and enlarge the valid receptive field of convolutional\nneural networks; this is important in object detection and\nsemantic segmentation tasks."
        },
        {
            "bounding_box": [
                {
                    "x": 1294,
                    "y": 1692
                },
                {
                    "x": 1893,
                    "y": 1692
                },
                {
                    "x": 1893,
                    "y": 1741
                },
                {
                    "x": 1294,
                    "y": 1741
                }
            ],
            "category": "paragraph",
            "html": "<p id='162' style='font-size:18px'>3.3.7 Self-attention and variants</p>",
            "id": 162,
            "page": 10,
            "text": "3.3.7 Self-attention and variants"
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 1752
                },
                {
                    "x": 2360,
                    "y": 1752
                },
                {
                    "x": 2360,
                    "y": 2134
                },
                {
                    "x": 1292,
                    "y": 2134
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='163' style='font-size:18px'>Self-attention was proposed and has had great success in<br>the field of natural language processing (NLP) [33], [38], [39],<br>[149], [150], [151], [152]. Recently, it has also shown the<br>potential to become a dominant tool in computer vision [8],<br>[15], [34], [78], [153]. Typically, self-attention is used as a<br>spatial attention mechanism to capture global information.<br>We now summarize the self-attention mechanism and its<br>common variants in computer vision.</p>",
            "id": 163,
            "page": 10,
            "text": "Self-attention was proposed and has had great success in\nthe field of natural language processing (NLP) [33], [38], [39],\n[149], [150], [151], [152]. Recently, it has also shown the\npotential to become a dominant tool in computer vision [8],\n[15], [34], [78], [153]. Typically, self-attention is used as a\nspatial attention mechanism to capture global information.\nWe now summarize the self-attention mechanism and its\ncommon variants in computer vision."
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 2136
                },
                {
                    "x": 2359,
                    "y": 2136
                },
                {
                    "x": 2359,
                    "y": 2375
                },
                {
                    "x": 1292,
                    "y": 2375
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='164' style='font-size:18px'>Due to the localisation of the convolutional operation,<br>CNNs have inherently narrow receptive fields [154], [155],<br>which limits the ability of CNNs to understand scenes<br>globally. To increase the receptive field, Wang et al. [15]<br>introduced self-attention into computer vision.</p>",
            "id": 164,
            "page": 10,
            "text": "Due to the localisation of the convolutional operation,\nCNNs have inherently narrow receptive fields [154], [155],\nwhich limits the ability of CNNs to understand scenes\nglobally. To increase the receptive field, Wang et al. [15]\nintroduced self-attention into computer vision."
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 2378
                },
                {
                    "x": 2359,
                    "y": 2378
                },
                {
                    "x": 2359,
                    "y": 2615
                },
                {
                    "x": 1292,
                    "y": 2615
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='165' style='font-size:16px'>Taking a 2D image as an example, given a feature map<br>F E RCxHxW first computes the queries,<br>, self-attention<br>keys and values Q, K, V E RC'xN = H x W by linear<br>, N<br>projection and reshaping operations. Then self-attention can<br>be formulated as:</p>",
            "id": 165,
            "page": 10,
            "text": "Taking a 2D image as an example, given a feature map\nF E RCxHxW first computes the queries,\n, self-attention\nkeys and values Q, K, V E RC'xN = H x W by linear\n, N\nprojection and reshaping operations. Then self-attention can\nbe formulated as:"
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 2770
                },
                {
                    "x": 2357,
                    "y": 2770
                },
                {
                    "x": 2357,
                    "y": 3018
                },
                {
                    "x": 1292,
                    "y": 3018
                }
            ],
            "category": "paragraph",
            "html": "<p id='166' style='font-size:18px'>where A E RNxN is the attention matrix and �i,j<br>is the<br>relationship between the i-th and j-th elements. The whole<br>process is shown in Fig. 7(left). Self-attention is a powerful<br>tool to model global information and is useful in many visual<br>tasks [9], [22], [26], [62], [63], [64], [65], [66], [67].</p>",
            "id": 166,
            "page": 10,
            "text": "where A E RNxN is the attention matrix and �i,j\nis the\nrelationship between the i-th and j-th elements. The whole\nprocess is shown in Fig. 7(left). Self-attention is a powerful\ntool to model global information and is useful in many visual\ntasks [9], [22], [26], [62], [63], [64], [65], [66], [67]."
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 3018
                },
                {
                    "x": 2360,
                    "y": 3018
                },
                {
                    "x": 2360,
                    "y": 3116
                },
                {
                    "x": 1292,
                    "y": 3116
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='167' style='font-size:18px'>However, the self-attention mechanism has several short-<br>comings, particularly its quadratic complexity, which limit its</p>",
            "id": 167,
            "page": 10,
            "text": "However, the self-attention mechanism has several short-\ncomings, particularly its quadratic complexity, which limit its"
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 109
                },
                {
                    "x": 1076,
                    "y": 109
                },
                {
                    "x": 1076,
                    "y": 149
                },
                {
                    "x": 193,
                    "y": 149
                }
            ],
            "category": "header",
            "html": "<header id='168' style='font-size:14px'>JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015</header>",
            "id": 168,
            "page": 11,
            "text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015"
        },
        {
            "bounding_box": [
                {
                    "x": 2314,
                    "y": 114
                },
                {
                    "x": 2351,
                    "y": 114
                },
                {
                    "x": 2351,
                    "y": 144
                },
                {
                    "x": 2314,
                    "y": 144
                }
            ],
            "category": "header",
            "html": "<br><header id='169' style='font-size:14px'>11</header>",
            "id": 169,
            "page": 11,
            "text": "11"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 180
                },
                {
                    "x": 1259,
                    "y": 180
                },
                {
                    "x": 1259,
                    "y": 420
                },
                {
                    "x": 192,
                    "y": 420
                }
            ],
            "category": "paragraph",
            "html": "<p id='170' style='font-size:20px'>applicability. Several variants have been introduced to allevi-<br>ate these problems. The disentangled non-local approach [74]<br>improves self-attention's accuracy and effectiveness, but<br>most variants focus on reducing its computational complex-<br>ity.</p>",
            "id": 170,
            "page": 11,
            "text": "applicability. Several variants have been introduced to allevi-\nate these problems. The disentangled non-local approach [74]\nimproves self-attention's accuracy and effectiveness, but\nmost variants focus on reducing its computational complex-\nity."
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 423
                },
                {
                    "x": 1258,
                    "y": 423
                },
                {
                    "x": 1258,
                    "y": 758
                },
                {
                    "x": 192,
                    "y": 758
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='171' style='font-size:20px'>CCNet [41] regards the self-attention operation as a<br>graph convolution and replaces the densely-connected graph<br>processed by self-attention with several sparsely-connected<br>graphs. To do so, it proposes criss-cross attention which<br>considers row attention and column attention recurrently<br>to obtain global information. CCNet reduces the complexity<br>of self-attention from O(N2) to O(N VN).</p>",
            "id": 171,
            "page": 11,
            "text": "CCNet [41] regards the self-attention operation as a\ngraph convolution and replaces the densely-connected graph\nprocessed by self-attention with several sparsely-connected\ngraphs. To do so, it proposes criss-cross attention which\nconsiders row attention and column attention recurrently\nto obtain global information. CCNet reduces the complexity\nof self-attention from O(N2) to O(N VN)."
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 760
                },
                {
                    "x": 1258,
                    "y": 760
                },
                {
                    "x": 1258,
                    "y": 1044
                },
                {
                    "x": 192,
                    "y": 1044
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='172' style='font-size:18px'>EMANet [40] views self-attention in terms of expectation<br>maximization (EM). It proposes EM attention which adopts<br>the EM algorithm to get a set of compact bases instead of<br>using all points as reconstruction bases. This reduces the<br>complexity from O(N2) to O(NK), where K is the number<br>of compact bases.</p>",
            "id": 172,
            "page": 11,
            "text": "EMANet [40] views self-attention in terms of expectation\nmaximization (EM). It proposes EM attention which adopts\nthe EM algorithm to get a set of compact bases instead of\nusing all points as reconstruction bases. This reduces the\ncomplexity from O(N2) to O(NK), where K is the number\nof compact bases."
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 1048
                },
                {
                    "x": 1258,
                    "y": 1048
                },
                {
                    "x": 1258,
                    "y": 1236
                },
                {
                    "x": 192,
                    "y": 1236
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='173' style='font-size:18px'>ANN [68] suggests that using all positional features as<br>key and vectors is redundant and adopts spatial pyramid<br>pooling [156], [157] to obtain a few representative key and<br>value features to use instead, to reduce computation.</p>",
            "id": 173,
            "page": 11,
            "text": "ANN [68] suggests that using all positional features as\nkey and vectors is redundant and adopts spatial pyramid\npooling [156], [157] to obtain a few representative key and\nvalue features to use instead, to reduce computation."
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 1239
                },
                {
                    "x": 1259,
                    "y": 1239
                },
                {
                    "x": 1259,
                    "y": 1623
                },
                {
                    "x": 192,
                    "y": 1623
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='174' style='font-size:18px'>GCNet [69] analyses the attention map used in self-<br>attention and finds that the global contexts obtained by self-<br>attention are similar for different query positions in the same<br>image. Thus, it first proposes to predict a single attention map<br>shared by all query points, and then gets global information<br>from a weighted sum of input features according to this<br>attention map. This is like average pooling, but is a more<br>general process for collecting global information.</p>",
            "id": 174,
            "page": 11,
            "text": "GCNet [69] analyses the attention map used in self-\nattention and finds that the global contexts obtained by self-\nattention are similar for different query positions in the same\nimage. Thus, it first proposes to predict a single attention map\nshared by all query points, and then gets global information\nfrom a weighted sum of input features according to this\nattention map. This is like average pooling, but is a more\ngeneral process for collecting global information."
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 1625
                },
                {
                    "x": 1259,
                    "y": 1625
                },
                {
                    "x": 1259,
                    "y": 1909
                },
                {
                    "x": 192,
                    "y": 1909
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='175' style='font-size:20px'>A2Net [70] is motivated by SENet to divide attention<br>into feature gathering and feature distribution processes,<br>using two different kinds of attention. The first aggregates<br>global information via second-order attention pooling and<br>the second distributes the global descriptors by soft selection<br>attention.</p>",
            "id": 175,
            "page": 11,
            "text": "A2Net [70] is motivated by SENet to divide attention\ninto feature gathering and feature distribution processes,\nusing two different kinds of attention. The first aggregates\nglobal information via second-order attention pooling and\nthe second distributes the global descriptors by soft selection\nattention."
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 1911
                },
                {
                    "x": 1259,
                    "y": 1911
                },
                {
                    "x": 1259,
                    "y": 2249
                },
                {
                    "x": 192,
                    "y": 2249
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='176' style='font-size:18px'>GloRe [71] understands self-attention from a graph<br>learning perspective. It first collects N input features into<br>M 《 N nodes and then learns an adjacency matrix<br>of global interactions between nodes. Finally, the nodes<br>distribute global information to input features. A similar<br>idea can be found in LatentGNN [72], MLP-Mixer [158] and<br>ResMLP [159].</p>",
            "id": 176,
            "page": 11,
            "text": "GloRe [71] understands self-attention from a graph\nlearning perspective. It first collects N input features into\nM 《 N nodes and then learns an adjacency matrix\nof global interactions between nodes. Finally, the nodes\ndistribute global information to input features. A similar\nidea can be found in LatentGNN [72], MLP-Mixer [158] and\nResMLP [159]."
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 2250
                },
                {
                    "x": 1258,
                    "y": 2250
                },
                {
                    "x": 1258,
                    "y": 2582
                },
                {
                    "x": 192,
                    "y": 2582
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='177' style='font-size:18px'>OCRNet [73] proposes the concept of object-contextual<br>representation which is a weighted aggregation of all object<br>regions' representations in the same category, such as a<br>weighted average of all car region representations. It replaces<br>the key and vector with this object-contextual representation<br>leading to successful improvements in both speed and<br>effectiveness.</p>",
            "id": 177,
            "page": 11,
            "text": "OCRNet [73] proposes the concept of object-contextual\nrepresentation which is a weighted aggregation of all object\nregions' representations in the same category, such as a\nweighted average of all car region representations. It replaces\nthe key and vector with this object-contextual representation\nleading to successful improvements in both speed and\neffectiveness."
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 2585
                },
                {
                    "x": 1260,
                    "y": 2585
                },
                {
                    "x": 1260,
                    "y": 3017
                },
                {
                    "x": 192,
                    "y": 3017
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='178' style='font-size:18px'>The disentangled non-local approach was motivated by [15],<br>[69]. Yin et al [74] deeply analyzed the self-attention mecha-<br>nism resulting in the core idea of decoupling self-attention<br>into a pairwise term and a unary term. The pairwise term<br>focuses on modeling relationships while the unary term<br>focuses on salient boundaries. This decomposition prevents<br>unwanted interactions between the two terms, greatly im-<br>proving semantic segmentation, object detection and action<br>recognition.</p>",
            "id": 178,
            "page": 11,
            "text": "The disentangled non-local approach was motivated by [15],\n[69]. Yin et al [74] deeply analyzed the self-attention mecha-\nnism resulting in the core idea of decoupling self-attention\ninto a pairwise term and a unary term. The pairwise term\nfocuses on modeling relationships while the unary term\nfocuses on salient boundaries. This decomposition prevents\nunwanted interactions between the two terms, greatly im-\nproving semantic segmentation, object detection and action\nrecognition."
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 3016
                },
                {
                    "x": 1258,
                    "y": 3016
                },
                {
                    "x": 1258,
                    "y": 3116
                },
                {
                    "x": 193,
                    "y": 3116
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='179' style='font-size:20px'>HamNet [42] models capturing global relationships as<br>a low-rank completion problem and designs a series of</p>",
            "id": 179,
            "page": 11,
            "text": "HamNet [42] models capturing global relationships as\na low-rank completion problem and designs a series of"
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 181
                },
                {
                    "x": 2356,
                    "y": 181
                },
                {
                    "x": 2356,
                    "y": 322
                },
                {
                    "x": 1292,
                    "y": 322
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='180' style='font-size:20px'>white-box methods to capture global context using matrix<br>decomposition. This not only reduces the complexity, but<br>increases the interpretability of self-attention.</p>",
            "id": 180,
            "page": 11,
            "text": "white-box methods to capture global context using matrix\ndecomposition. This not only reduces the complexity, but\nincreases the interpretability of self-attention."
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 324
                },
                {
                    "x": 2358,
                    "y": 324
                },
                {
                    "x": 2358,
                    "y": 756
                },
                {
                    "x": 1291,
                    "y": 756
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='181' style='font-size:18px'>EANet [75] proposes that self-attention should only<br>consider correlation in a single sample and should ignore<br>potential relationships between different samples. To explore<br>the correlation between different samples and reduce com-<br>putation, it makes use of an external attention that adopts<br>learnable, lightweight and shared key and value vectors. It<br>further reveals that using softmax to normalize the attention<br>map is not optimal and presents double normalization as a<br>better alternative.</p>",
            "id": 181,
            "page": 11,
            "text": "EANet [75] proposes that self-attention should only\nconsider correlation in a single sample and should ignore\npotential relationships between different samples. To explore\nthe correlation between different samples and reduce com-\nputation, it makes use of an external attention that adopts\nlearnable, lightweight and shared key and value vectors. It\nfurther reveals that using softmax to normalize the attention\nmap is not optimal and presents double normalization as a\nbetter alternative."
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 757
                },
                {
                    "x": 2360,
                    "y": 757
                },
                {
                    "x": 2360,
                    "y": 999
                },
                {
                    "x": 1291,
                    "y": 999
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='182' style='font-size:18px'>In addition to being a complementary approach to<br>CNNs, self-attention also can be used to replace convolu-<br>tion operations for aggregating neighborhood information.<br>Convolution operations can be formulated as dot products<br>between the input feature X and a convolution kernel W:</p>",
            "id": 182,
            "page": 11,
            "text": "In addition to being a complementary approach to\nCNNs, self-attention also can be used to replace convolu-\ntion operations for aggregating neighborhood information.\nConvolution operations can be formulated as dot products\nbetween the input feature X and a convolution kernel W:"
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 1153
                },
                {
                    "x": 1413,
                    "y": 1153
                },
                {
                    "x": 1413,
                    "y": 1195
                },
                {
                    "x": 1292,
                    "y": 1195
                }
            ],
            "category": "paragraph",
            "html": "<p id='183' style='font-size:18px'>where</p>",
            "id": 183,
            "page": 11,
            "text": "where"
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 1301
                },
                {
                    "x": 2357,
                    "y": 1301
                },
                {
                    "x": 2357,
                    "y": 1543
                },
                {
                    "x": 1292,
                    "y": 1543
                }
            ],
            "category": "paragraph",
            "html": "<p id='184' style='font-size:18px'>k is the kernel size and c indicates the channel. The above<br>formulation can be viewed as a process of aggregating neigh-<br>borhood information by using a weighted sum through a<br>convolution kernel. The process of aggregating neighborhood<br>information can be defined more generally as:</p>",
            "id": 184,
            "page": 11,
            "text": "k is the kernel size and c indicates the channel. The above\nformulation can be viewed as a process of aggregating neigh-\nborhood information by using a weighted sum through a\nconvolution kernel. The process of aggregating neighborhood\ninformation can be defined more generally as:"
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 1704
                },
                {
                    "x": 2356,
                    "y": 1704
                },
                {
                    "x": 2356,
                    "y": 1854
                },
                {
                    "x": 1291,
                    "y": 1854
                }
            ],
            "category": "paragraph",
            "html": "<p id='185' style='font-size:20px'>where Rel(i,j, a, 6) is the relation between position (i,j) and<br>position (a, 6). With this definition, local self-attention is a<br>special case.</p>",
            "id": 185,
            "page": 11,
            "text": "where Rel(i,j, a, 6) is the relation between position (i,j) and\nposition (a, 6). With this definition, local self-attention is a\nspecial case."
        },
        {
            "bounding_box": [
                {
                    "x": 1289,
                    "y": 2052
                },
                {
                    "x": 2356,
                    "y": 2052
                },
                {
                    "x": 2356,
                    "y": 2199
                },
                {
                    "x": 1289,
                    "y": 2199
                }
            ],
            "category": "paragraph",
            "html": "<p id='186' style='font-size:20px'>where q, k and v are linear projections of input feature x, and<br>is the relative positional embedding of (i,j) and<br>ra-i,b-j<br>(a,b).</p>",
            "id": 186,
            "page": 11,
            "text": "where q, k and v are linear projections of input feature x, and\nis the relative positional embedding of (i,j) and\nra-i,b-j\n(a,b)."
        },
        {
            "bounding_box": [
                {
                    "x": 1293,
                    "y": 2199
                },
                {
                    "x": 2361,
                    "y": 2199
                },
                {
                    "x": 2361,
                    "y": 2294
                },
                {
                    "x": 1293,
                    "y": 2294
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='187' style='font-size:16px'>We now consider several specific works using local self-<br>attention as basic neural network blocks</p>",
            "id": 187,
            "page": 11,
            "text": "We now consider several specific works using local self-\nattention as basic neural network blocks"
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 2297
                },
                {
                    "x": 2358,
                    "y": 2297
                },
                {
                    "x": 2358,
                    "y": 2724
                },
                {
                    "x": 1292,
                    "y": 2724
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='188' style='font-size:20px'>SASA [43] suggests that using self-attention to collect<br>global information is too computationally intensive and<br>instead adopts local self-attention to replace all spatial<br>convolution in a CNN. The authors show that doing so<br>improves speed, number of parameters and quality of results.<br>They also explores the behavior of positional embedding and<br>show that relative positional embeddings [160] are suitable.<br>Their work also studies how to combinie local self-attention<br>with convolution.</p>",
            "id": 188,
            "page": 11,
            "text": "SASA [43] suggests that using self-attention to collect\nglobal information is too computationally intensive and\ninstead adopts local self-attention to replace all spatial\nconvolution in a CNN. The authors show that doing so\nimproves speed, number of parameters and quality of results.\nThey also explores the behavior of positional embedding and\nshow that relative positional embeddings [160] are suitable.\nTheir work also studies how to combinie local self-attention\nwith convolution."
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 2728
                },
                {
                    "x": 2358,
                    "y": 2728
                },
                {
                    "x": 2358,
                    "y": 2966
                },
                {
                    "x": 1291,
                    "y": 2966
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='189' style='font-size:20px'>LR-Net [76] appeared concurrently with SASA. It also<br>studies how to model local relationships by using local<br>self-attention. A comprehensive study probed the effects of<br>positional embedding, kernel size, appearance composability<br>and adversarial attacks.</p>",
            "id": 189,
            "page": 11,
            "text": "LR-Net [76] appeared concurrently with SASA. It also\nstudies how to model local relationships by using local\nself-attention. A comprehensive study probed the effects of\npositional embedding, kernel size, appearance composability\nand adversarial attacks."
        },
        {
            "bounding_box": [
                {
                    "x": 1290,
                    "y": 2969
                },
                {
                    "x": 2357,
                    "y": 2969
                },
                {
                    "x": 2357,
                    "y": 3117
                },
                {
                    "x": 1290,
                    "y": 3117
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='190' style='font-size:20px'>SAN [77] explored two modes, pairwise and patchwise, of<br>utilizing attention for local feature aggregation. It proposed<br>a novel vector attention adaptive both in content and</p>",
            "id": 190,
            "page": 11,
            "text": "SAN [77] explored two modes, pairwise and patchwise, of\nutilizing attention for local feature aggregation. It proposed\na novel vector attention adaptive both in content and"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 107
                },
                {
                    "x": 1077,
                    "y": 107
                },
                {
                    "x": 1077,
                    "y": 149
                },
                {
                    "x": 192,
                    "y": 149
                }
            ],
            "category": "header",
            "html": "<header id='191' style='font-size:16px'>JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015</header>",
            "id": 191,
            "page": 12,
            "text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015"
        },
        {
            "bounding_box": [
                {
                    "x": 2316,
                    "y": 113
                },
                {
                    "x": 2353,
                    "y": 113
                },
                {
                    "x": 2353,
                    "y": 144
                },
                {
                    "x": 2316,
                    "y": 144
                }
            ],
            "category": "header",
            "html": "<br><header id='192' style='font-size:14px'>12</header>",
            "id": 192,
            "page": 12,
            "text": "12"
        },
        {
            "bounding_box": [
                {
                    "x": 1290,
                    "y": 180
                },
                {
                    "x": 2356,
                    "y": 180
                },
                {
                    "x": 2356,
                    "y": 369
                },
                {
                    "x": 1290,
                    "y": 369
                }
            ],
            "category": "paragraph",
            "html": "<p id='193' style='font-size:18px'>ViT demonstrates that a pure attention-based network can<br>achieve better results than a convolutional neural network<br>especially for large datasets such as JFT-300 [164] and<br>ImageNet-21K [165].</p>",
            "id": 193,
            "page": 12,
            "text": "ViT demonstrates that a pure attention-based network can\nachieve better results than a convolutional neural network\nespecially for large datasets such as JFT-300 [164] and\nImageNet-21K [165]."
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 170
                },
                {
                    "x": 1259,
                    "y": 170
                },
                {
                    "x": 1259,
                    "y": 734
                },
                {
                    "x": 192,
                    "y": 734
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='194' style='font-size:14px' alt=\"Vision Transformer (ViT) Transformer Encoder\nClass Lx\n+\nBird MLP\nBall\nHead\nCar MLP\n···\nNorm\nTransformer Encoder\n+\nPatch + Position Multi-Head\n*\nEmbedding\nAttention\n* Extra learnable\n[class] embedding Linear Projection of Flattened Patches\nNorm\nEmbedded\nPatches\" data-coord=\"top-left:(192,170); bottom-right:(1259,734)\" /></figure>",
            "id": 194,
            "page": 12,
            "text": "Vision Transformer (ViT) Transformer Encoder\nClass Lx\n+\nBird MLP\nBall\nHead\nCar MLP\n···\nNorm\nTransformer Encoder\n+\nPatch + Position Multi-Head\n*\nEmbedding\nAttention\n* Extra learnable\n[class] embedding Linear Projection of Flattened Patches\nNorm\nEmbedded\nPatches"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 818
                },
                {
                    "x": 1258,
                    "y": 818
                },
                {
                    "x": 1258,
                    "y": 1018
                },
                {
                    "x": 192,
                    "y": 1018
                }
            ],
            "category": "caption",
            "html": "<caption id='195' style='font-size:16px'>Fig. 7. Vision transformer [34]. Left: architecture. Vision transformer first<br>splits the image into different patches and projects them into feature<br>space where a transformer encoder processes them to produce the final<br>result. Right: basic vision transformer block with multi-head attention core.<br>Figure is taken from [34].</caption>",
            "id": 195,
            "page": 12,
            "text": "Fig. 7. Vision transformer [34]. Left: architecture. Vision transformer first\nsplits the image into different patches and projects them into feature\nspace where a transformer encoder processes them to produce the final\nresult. Right: basic vision transformer block with multi-head attention core.\nFigure is taken from [34]."
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 1069
                },
                {
                    "x": 1255,
                    "y": 1069
                },
                {
                    "x": 1255,
                    "y": 1642
                },
                {
                    "x": 193,
                    "y": 1642
                }
            ],
            "category": "figure",
            "html": "<figure><img id='196' style='font-size:14px' alt=\"Scaled Dot-Product Attention Multi-Head Attention\nLinear\nMatMul\nConcat\nSoftMax\nMask (opt.) Scaled Dot-Product\nh\nAttention\nScale\nMatMul Linear Linear Linear\nQ K\nV K Q\" data-coord=\"top-left:(193,1069); bottom-right:(1255,1642)\" /></figure>",
            "id": 196,
            "page": 12,
            "text": "Scaled Dot-Product Attention Multi-Head Attention\nLinear\nMatMul\nConcat\nSoftMax\nMask (opt.) Scaled Dot-Product\nh\nAttention\nScale\nMatMul Linear Linear Linear\nQ K\nV K Q"
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 1733
                },
                {
                    "x": 1255,
                    "y": 1733
                },
                {
                    "x": 1255,
                    "y": 1820
                },
                {
                    "x": 193,
                    "y": 1820
                }
            ],
            "category": "caption",
            "html": "<caption id='197' style='font-size:16px'>Fig. 8. Left: Self-attention. Right: Multi-head self-attention. Figure<br>from [33].</caption>",
            "id": 197,
            "page": 12,
            "text": "Fig. 8. Left: Self-attention. Right: Multi-head self-attention. Figure\nfrom [33]."
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 1908
                },
                {
                    "x": 1258,
                    "y": 1908
                },
                {
                    "x": 1258,
                    "y": 2105
                },
                {
                    "x": 192,
                    "y": 2105
                }
            ],
            "category": "paragraph",
            "html": "<p id='198' style='font-size:18px'>channel, and assessed its effectiveness both theoretically and<br>practically. In addition to providing significant improvements<br>in the image domain, it also has been proven useful in 3D<br>point cloud processing [80].</p>",
            "id": 198,
            "page": 12,
            "text": "channel, and assessed its effectiveness both theoretically and\npractically. In addition to providing significant improvements\nin the image domain, it also has been proven useful in 3D\npoint cloud processing [80]."
        },
        {
            "bounding_box": [
                {
                    "x": 195,
                    "y": 2169
                },
                {
                    "x": 684,
                    "y": 2169
                },
                {
                    "x": 684,
                    "y": 2218
                },
                {
                    "x": 195,
                    "y": 2218
                }
            ],
            "category": "paragraph",
            "html": "<p id='199' style='font-size:18px'>3.3.8 Vision Transformers</p>",
            "id": 199,
            "page": 12,
            "text": "3.3.8 Vision Transformers"
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 373
                },
                {
                    "x": 2358,
                    "y": 373
                },
                {
                    "x": 2358,
                    "y": 803
                },
                {
                    "x": 1291,
                    "y": 803
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='200' style='font-size:20px'>Following ViT, many transformer-based architectures<br>such as PCT [27], IPT [79], T2T-ViT [44], DeepViT [166],<br>SETR [81], PVT [45], CaiT [167], TNT [82], Swin-<br>transformer [46], Query2Label [83], MoCoV3 [84], BEiT [85],<br>SegFormer [86], FuseFormer [168] and MAE [169] have<br>appeared, with excellent results for many kind of visual tasks<br>including image classification, object detection, semantic<br>segmentation, point cloud processing, action recognition and<br>self-supervised learning.</p>",
            "id": 200,
            "page": 12,
            "text": "Following ViT, many transformer-based architectures\nsuch as PCT [27], IPT [79], T2T-ViT [44], DeepViT [166],\nSETR [81], PVT [45], CaiT [167], TNT [82], Swin-\ntransformer [46], Query2Label [83], MoCoV3 [84], BEiT [85],\nSegFormer [86], FuseFormer [168] and MAE [169] have\nappeared, with excellent results for many kind of visual tasks\nincluding image classification, object detection, semantic\nsegmentation, point cloud processing, action recognition and\nself-supervised learning."
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 2238
                },
                {
                    "x": 1258,
                    "y": 2238
                },
                {
                    "x": 1258,
                    "y": 2625
                },
                {
                    "x": 193,
                    "y": 2625
                }
            ],
            "category": "paragraph",
            "html": "<p id='201' style='font-size:18px'>Transformers have had great success in natural language<br>processing [33], [38], [149], [150], [152], [161]. Recently,<br>iGPT [78] and DETR [8] demonstrated the huge potential for<br>transformer-based models in computer vision. Motivated by<br>this, Dosovitskiy et al [34] proposed the vision transformer<br>(ViT) which is the first pure transformer architecture for<br>image processing. It is capable of achieving comparable<br>results to modern convolutional neural networks.</p>",
            "id": 201,
            "page": 12,
            "text": "Transformers have had great success in natural language\nprocessing [33], [38], [149], [150], [152], [161]. Recently,\niGPT [78] and DETR [8] demonstrated the huge potential for\ntransformer-based models in computer vision. Motivated by\nthis, Dosovitskiy et al [34] proposed the vision transformer\n(ViT) which is the first pure transformer architecture for\nimage processing. It is capable of achieving comparable\nresults to modern convolutional neural networks."
        },
        {
            "bounding_box": [
                {
                    "x": 191,
                    "y": 2630
                },
                {
                    "x": 1258,
                    "y": 2630
                },
                {
                    "x": 1258,
                    "y": 3116
                },
                {
                    "x": 191,
                    "y": 3116
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='202' style='font-size:18px'>As Fig 7 shows, the main part of ViT is the multi-head<br>attention (MHA) module. MHA takes a sequence as input.<br>It first concatenates a class token with the input feature<br>F E RNxC where N is the number of pixels. Then it gets<br>,<br>Q, K E RNxC' and V E RNxC by linear projection. Next,<br>Q, K and V are divided into H heads in the channel domain<br>and self-attention separately applied to them. The MHA<br>approach is shown in Fig. 8. ViT stacks a number of MHA<br>layers with fully connected layers, layer normalization [162]<br>and the GELU [163] activation function.</p>",
            "id": 202,
            "page": 12,
            "text": "As Fig 7 shows, the main part of ViT is the multi-head\nattention (MHA) module. MHA takes a sequence as input.\nIt first concatenates a class token with the input feature\nF E RNxC where N is the number of pixels. Then it gets\n,\nQ, K E RNxC' and V E RNxC by linear projection. Next,\nQ, K and V are divided into H heads in the channel domain\nand self-attention separately applied to them. The MHA\napproach is shown in Fig. 8. ViT stacks a number of MHA\nlayers with fully connected layers, layer normalization [162]\nand the GELU [163] activation function."
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 805
                },
                {
                    "x": 2358,
                    "y": 805
                },
                {
                    "x": 2358,
                    "y": 948
                },
                {
                    "x": 1292,
                    "y": 948
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='203' style='font-size:18px'>A detailed survey of vision transformers is omitted here<br>as other recent surveys [141], [142], [143], [170] comprehen-<br>sively review the use of transformer methods for visual tasks.</p>",
            "id": 203,
            "page": 12,
            "text": "A detailed survey of vision transformers is omitted here\nas other recent surveys [141], [142], [143], [170] comprehen-\nsively review the use of transformer methods for visual tasks."
        },
        {
            "bounding_box": [
                {
                    "x": 1295,
                    "y": 983
                },
                {
                    "x": 1552,
                    "y": 983
                },
                {
                    "x": 1552,
                    "y": 1028
                },
                {
                    "x": 1295,
                    "y": 1028
                }
            ],
            "category": "paragraph",
            "html": "<p id='204' style='font-size:18px'>3.3.9 GENet</p>",
            "id": 204,
            "page": 12,
            "text": "3.3.9 GENet"
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 1040
                },
                {
                    "x": 2356,
                    "y": 1040
                },
                {
                    "x": 2356,
                    "y": 1184
                },
                {
                    "x": 1292,
                    "y": 1184
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='205' style='font-size:20px'>Inspired by SENet, Hu et al. [61] designed GENet to capture<br>long-range spatial contextual information by providing a<br>recalibration function in the spatial domain.</p>",
            "id": 205,
            "page": 12,
            "text": "Inspired by SENet, Hu et al. [61] designed GENet to capture\nlong-range spatial contextual information by providing a\nrecalibration function in the spatial domain."
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 1185
                },
                {
                    "x": 2359,
                    "y": 1185
                },
                {
                    "x": 2359,
                    "y": 1570
                },
                {
                    "x": 1291,
                    "y": 1570
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='206' style='font-size:18px'>GENet combines part gathering and excitation operations.<br>In the first step, it aggregates input features over large neigh-<br>borhoods and models the relationship between different<br>spatial locations. In the second step, it first generates an<br>attention map of the same size as the input feature map,<br>using interpolation. Then each position in the input feature<br>map is scaled by multiplying by the corresponding element<br>in the attention map. This process can be described by:</p>",
            "id": 206,
            "page": 12,
            "text": "GENet combines part gathering and excitation operations.\nIn the first step, it aggregates input features over large neigh-\nborhoods and models the relationship between different\nspatial locations. In the second step, it first generates an\nattention map of the same size as the input feature map,\nusing interpolation. Then each position in the input feature\nmap is scaled by multiplying by the corresponding element\nin the attention map. This process can be described by:"
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 1785
                },
                {
                    "x": 2355,
                    "y": 1785
                },
                {
                    "x": 2355,
                    "y": 1926
                },
                {
                    "x": 1292,
                    "y": 1926
                }
            ],
            "category": "paragraph",
            "html": "<p id='207' style='font-size:20px'>Here, fgather can take any form which captures spatial<br>correlations, such as global average pooling or a sequence of<br>depth-wise convolutions; Interp(·) denotes interpolation.</p>",
            "id": 207,
            "page": 12,
            "text": "Here, fgather can take any form which captures spatial\ncorrelations, such as global average pooling or a sequence of\ndepth-wise convolutions; Interp(·) denotes interpolation."
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 1930
                },
                {
                    "x": 2358,
                    "y": 1930
                },
                {
                    "x": 2358,
                    "y": 2075
                },
                {
                    "x": 1291,
                    "y": 2075
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='208' style='font-size:18px'>The gather-excite module is lightweight and can be in-<br>serted into each residual unit like an SE block. It emphasizes<br>important features while suppressing noise.</p>",
            "id": 208,
            "page": 12,
            "text": "The gather-excite module is lightweight and can be in-\nserted into each residual unit like an SE block. It emphasizes\nimportant features while suppressing noise."
        },
        {
            "bounding_box": [
                {
                    "x": 1294,
                    "y": 2107
                },
                {
                    "x": 1598,
                    "y": 2107
                },
                {
                    "x": 1598,
                    "y": 2153
                },
                {
                    "x": 1294,
                    "y": 2153
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='209' style='font-size:18px'>3.3.10 PSANet</p>",
            "id": 209,
            "page": 12,
            "text": "3.3.10 PSANet"
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 2166
                },
                {
                    "x": 2358,
                    "y": 2166
                },
                {
                    "x": 2358,
                    "y": 2454
                },
                {
                    "x": 1292,
                    "y": 2454
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='210' style='font-size:18px'>Motivated by success in capturing long-range dependencies<br>in convolutional neural networks, Zhao et al. [87] presented<br>the novel PSANet framework to aggregate global informa-<br>tion. It models information aggregation as an information<br>flow and proposes a bidirectional information propagation<br>mechanism to make information flow globally.</p>",
            "id": 210,
            "page": 12,
            "text": "Motivated by success in capturing long-range dependencies\nin convolutional neural networks, Zhao et al. [87] presented\nthe novel PSANet framework to aggregate global informa-\ntion. It models information aggregation as an information\nflow and proposes a bidirectional information propagation\nmechanism to make information flow globally."
        },
        {
            "bounding_box": [
                {
                    "x": 1354,
                    "y": 2453
                },
                {
                    "x": 2200,
                    "y": 2453
                },
                {
                    "x": 2200,
                    "y": 2499
                },
                {
                    "x": 1354,
                    "y": 2499
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='211' style='font-size:16px'>PSANet formulates information aggregation as:</p>",
            "id": 211,
            "page": 12,
            "text": "PSANet formulates information aggregation as:"
        },
        {
            "bounding_box": [
                {
                    "x": 1290,
                    "y": 2642
                },
                {
                    "x": 2358,
                    "y": 2642
                },
                {
                    "x": 2358,
                    "y": 2929
                },
                {
                    "x": 1290,
                    "y": 2929
                }
            ],
            "category": "paragraph",
            "html": "<p id='212' style='font-size:18px'>where △ij indicates the positional relationship between 2<br>and j. F(xi, xj, △ij) is a function that takes Xi, xj and △ij<br>into consideration to controls information flow from j to 2.<br>Ii represents the aggregation neighborhood of position i; if<br>we wish to capture global information, �i should include all<br>spatial positions.</p>",
            "id": 212,
            "page": 12,
            "text": "where △ij indicates the positional relationship between 2\nand j. F(xi, xj, △ij) is a function that takes Xi, xj and △ij\ninto consideration to controls information flow from j to 2.\nIi represents the aggregation neighborhood of position i; if\nwe wish to capture global information, �i should include all\nspatial positions."
        },
        {
            "bounding_box": [
                {
                    "x": 1293,
                    "y": 2930
                },
                {
                    "x": 2355,
                    "y": 2930
                },
                {
                    "x": 2355,
                    "y": 3032
                },
                {
                    "x": 1293,
                    "y": 3032
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='213' style='font-size:20px'>Due to the complexity of calculating function<br>F(xi, xj, △ij), it is decomposed into an approximation:</p>",
            "id": 213,
            "page": 12,
            "text": "Due to the complexity of calculating function\nF(xi, xj, △ij), it is decomposed into an approximation:"
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 108
                },
                {
                    "x": 1076,
                    "y": 108
                },
                {
                    "x": 1076,
                    "y": 150
                },
                {
                    "x": 193,
                    "y": 150
                }
            ],
            "category": "header",
            "html": "<header id='214' style='font-size:16px'>JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015</header>",
            "id": 214,
            "page": 13,
            "text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015"
        },
        {
            "bounding_box": [
                {
                    "x": 2314,
                    "y": 111
                },
                {
                    "x": 2355,
                    "y": 111
                },
                {
                    "x": 2355,
                    "y": 146
                },
                {
                    "x": 2314,
                    "y": 146
                }
            ],
            "category": "header",
            "html": "<br><header id='215' style='font-size:14px'>13</header>",
            "id": 215,
            "page": 13,
            "text": "13"
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 171
                },
                {
                    "x": 2345,
                    "y": 171
                },
                {
                    "x": 2345,
                    "y": 1056
                },
                {
                    "x": 193,
                    "y": 1056
                }
            ],
            "category": "figure",
            "html": "<figure><img id='216' style='font-size:14px' alt=\"1 2 3 4 5 6 7 8\n·\n9 10 11 12 13 14 15 16\n17 18 19 20 21 22 23 24\n25 26 27 28 29 30 31 32\n33 34 35 36 37 38 39 40\" data-coord=\"top-left:(193,171); bottom-right:(2345,1056)\" /></figure>",
            "id": 216,
            "page": 13,
            "text": "1 2 3 4 5 6 7 8\n·\n9 10 11 12 13 14 15 16\n17 18 19 20 21 22 23 24\n25 26 27 28 29 30 31 32\n33 34 35 36 37 38 39 40"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 1144
                },
                {
                    "x": 1999,
                    "y": 1144
                },
                {
                    "x": 1999,
                    "y": 1197
                },
                {
                    "x": 192,
                    "y": 1197
                }
            ],
            "category": "caption",
            "html": "<caption id='217' style='font-size:16px'>Fig. 9. Attention map results from [34]. The network focuses on the discriminative regions of each image. Figure from [34].</caption>",
            "id": 217,
            "page": 13,
            "text": "Fig. 9. Attention map results from [34]. The network focuses on the discriminative regions of each image. Figure from [34]."
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 1274
                },
                {
                    "x": 890,
                    "y": 1274
                },
                {
                    "x": 890,
                    "y": 1322
                },
                {
                    "x": 192,
                    "y": 1322
                }
            ],
            "category": "paragraph",
            "html": "<p id='218' style='font-size:20px'>whereupon Eq. 55 can be simplified to:</p>",
            "id": 218,
            "page": 13,
            "text": "whereupon Eq. 55 can be simplified to:"
        },
        {
            "bounding_box": [
                {
                    "x": 191,
                    "y": 1474
                },
                {
                    "x": 1259,
                    "y": 1474
                },
                {
                    "x": 1259,
                    "y": 1669
                },
                {
                    "x": 191,
                    "y": 1669
                }
            ],
            "category": "paragraph",
            "html": "<p id='219' style='font-size:18px'>The first term can be viewed as collecting information at<br>position i while the second term distributes information at<br>position j. Functions F�ij (xi) and F�ij (xj) can be seen as<br>adaptive attention weights.</p>",
            "id": 219,
            "page": 13,
            "text": "The first term can be viewed as collecting information at\nposition i while the second term distributes information at\nposition j. Functions F�ij (xi) and F�ij (xj) can be seen as\nadaptive attention weights."
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 1671
                },
                {
                    "x": 1257,
                    "y": 1671
                },
                {
                    "x": 1257,
                    "y": 1867
                },
                {
                    "x": 192,
                    "y": 1867
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='220' style='font-size:18px'>The above process aggregates global information while<br>emphasizing the relevant features. It can be added to the end<br>of a convolutional neural network as an effective complement<br>to greatly improve semantic segmentation.</p>",
            "id": 220,
            "page": 13,
            "text": "The above process aggregates global information while\nemphasizing the relevant features. It can be added to the end\nof a convolutional neural network as an effective complement\nto greatly improve semantic segmentation."
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 1929
                },
                {
                    "x": 660,
                    "y": 1929
                },
                {
                    "x": 660,
                    "y": 1978
                },
                {
                    "x": 193,
                    "y": 1978
                }
            ],
            "category": "paragraph",
            "html": "<p id='221' style='font-size:22px'>3.4 Temporal Attention</p>",
            "id": 221,
            "page": 13,
            "text": "3.4 Temporal Attention"
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 1992
                },
                {
                    "x": 1259,
                    "y": 1992
                },
                {
                    "x": 1259,
                    "y": 2430
                },
                {
                    "x": 193,
                    "y": 2430
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='222' style='font-size:18px'>Temporal attention can be seen as a dynamic time selection<br>mechanism determining when to pay attention, and is thus<br>usually used for video processing. Previous works [171],<br>[172] often emphasise how to capture both short-term and<br>long-term cross-frame feature dependencies. Here, we first<br>summarize representative temporal attention mechanisms<br>and specify process g(x) and f(g(x), x) described as Eq. 1 in<br>Tab. 5, and then discuss various such mechanisms according<br>to the order in Fig. 4.</p>",
            "id": 222,
            "page": 13,
            "text": "Temporal attention can be seen as a dynamic time selection\nmechanism determining when to pay attention, and is thus\nusually used for video processing. Previous works [171],\n[172] often emphasise how to capture both short-term and\nlong-term cross-frame feature dependencies. Here, we first\nsummarize representative temporal attention mechanisms\nand specify process g(x) and f(g(x), x) described as Eq. 1 in\nTab. 5, and then discuss various such mechanisms according\nto the order in Fig. 4."
        },
        {
            "bounding_box": [
                {
                    "x": 195,
                    "y": 2473
                },
                {
                    "x": 795,
                    "y": 2473
                },
                {
                    "x": 795,
                    "y": 2524
                },
                {
                    "x": 195,
                    "y": 2524
                }
            ],
            "category": "paragraph",
            "html": "<p id='223' style='font-size:18px'>3.4.1 Self-attention and variants</p>",
            "id": 223,
            "page": 13,
            "text": "3.4.1 Self-attention and variants"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 2534
                },
                {
                    "x": 1258,
                    "y": 2534
                },
                {
                    "x": 1258,
                    "y": 2774
                },
                {
                    "x": 192,
                    "y": 2774
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='224' style='font-size:20px'>RNN and temporal pooling or weight learning have been<br>widely used in work on video representation learning to<br>capture interaction between frames, but these methods have<br>limitations in terms of either efficiency or temporal relation<br>modeling.</p>",
            "id": 224,
            "page": 13,
            "text": "RNN and temporal pooling or weight learning have been\nwidely used in work on video representation learning to\ncapture interaction between frames, but these methods have\nlimitations in terms of either efficiency or temporal relation\nmodeling."
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 2777
                },
                {
                    "x": 1259,
                    "y": 2777
                },
                {
                    "x": 1259,
                    "y": 3117
                },
                {
                    "x": 192,
                    "y": 3117
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='225' style='font-size:20px'>To overcome them, Li et al. [171] proposed a global-<br>local temporal representation (GLTR) to exploit multi-scale<br>temporal cues in a video sequence. GLTR consists of a dilated<br>temporal pyramid (DTP) for local temporal context learning<br>and a temporal self attention module for capturing global<br>temporal interaction. DTP adopts dilated convolution with<br>dilatation rates increasing progressively to cover various</p>",
            "id": 225,
            "page": 13,
            "text": "To overcome them, Li et al. [171] proposed a global-\nlocal temporal representation (GLTR) to exploit multi-scale\ntemporal cues in a video sequence. GLTR consists of a dilated\ntemporal pyramid (DTP) for local temporal context learning\nand a temporal self attention module for capturing global\ntemporal interaction. DTP adopts dilated convolution with\ndilatation rates increasing progressively to cover various"
        },
        {
            "bounding_box": [
                {
                    "x": 1293,
                    "y": 1275
                },
                {
                    "x": 2356,
                    "y": 1275
                },
                {
                    "x": 2356,
                    "y": 1422
                },
                {
                    "x": 1293,
                    "y": 1422
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='226' style='font-size:16px'>temporal ranges, and then concatenates the various outputs<br>to aggregate multi-scale information. Given input frame-wise<br>features F = {f1, · · · fT}, DTP can be written as:</p>",
            "id": 226,
            "page": 13,
            "text": "temporal ranges, and then concatenates the various outputs\nto aggregate multi-scale information. Given input frame-wise\nfeatures F = {f1, · · · fT}, DTP can be written as:"
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 1601
                },
                {
                    "x": 2356,
                    "y": 1601
                },
                {
                    "x": 2356,
                    "y": 1898
                },
                {
                    "x": 1292,
                    "y": 1898
                }
            ],
            "category": "paragraph",
            "html": "<p id='227' style='font-size:18px'>where DConv(r) (·) denotes dilated convolution with dilation<br>rate r. The self-attention mechanism adopts convolution<br>layers followed by batch normalization and ReLU activation<br>to generate the query Q E RdxT the key K E RdxT and<br>,<br>F' =<br>based on the input feature map<br>the value V E RdxT<br>{fi, · · · fT}, which can be written as</p>",
            "id": 227,
            "page": 13,
            "text": "where DConv(r) (·) denotes dilated convolution with dilation\nrate r. The self-attention mechanism adopts convolution\nlayers followed by batch normalization and ReLU activation\nto generate the query Q E RdxT the key K E RdxT and\n,\nF' =\nbased on the input feature map\nthe value V E RdxT\n{fi, · · · fT}, which can be written as"
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 1999
                },
                {
                    "x": 2363,
                    "y": 1999
                },
                {
                    "x": 2363,
                    "y": 2095
                },
                {
                    "x": 1292,
                    "y": 2095
                }
            ],
            "category": "paragraph",
            "html": "<p id='228' style='font-size:18px'>where g denotes a linear mapping implemented by a convo-<br>lution.</p>",
            "id": 228,
            "page": 13,
            "text": "where g denotes a linear mapping implemented by a convo-\nlution."
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 2096
                },
                {
                    "x": 2359,
                    "y": 2096
                },
                {
                    "x": 2359,
                    "y": 2535
                },
                {
                    "x": 1291,
                    "y": 2535
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='229' style='font-size:20px'>The short-term temporal contextual information from<br>neighboring frames helps to distinguish visually similar<br>regions while the long-term temporal information serves to<br>overcome occlusions and noise. GLTR combines the advan-<br>tages of both modules, enhancing representation capability<br>and suppressing noise. It can be incorporated into any state-<br>of-the-art CNN backbone to learn a global descriptor for<br>a whole video. However, the self-attention mechanism has<br>quadratic time complexity, limiting its application.</p>",
            "id": 229,
            "page": 13,
            "text": "The short-term temporal contextual information from\nneighboring frames helps to distinguish visually similar\nregions while the long-term temporal information serves to\novercome occlusions and noise. GLTR combines the advan-\ntages of both modules, enhancing representation capability\nand suppressing noise. It can be incorporated into any state-\nof-the-art CNN backbone to learn a global descriptor for\na whole video. However, the self-attention mechanism has\nquadratic time complexity, limiting its application."
        },
        {
            "bounding_box": [
                {
                    "x": 1295,
                    "y": 2574
                },
                {
                    "x": 1515,
                    "y": 2574
                },
                {
                    "x": 1515,
                    "y": 2620
                },
                {
                    "x": 1295,
                    "y": 2620
                }
            ],
            "category": "paragraph",
            "html": "<p id='230' style='font-size:20px'>3.4.2 TAM</p>",
            "id": 230,
            "page": 13,
            "text": "3.4.2 TAM"
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 2633
                },
                {
                    "x": 2359,
                    "y": 2633
                },
                {
                    "x": 2359,
                    "y": 2870
                },
                {
                    "x": 1292,
                    "y": 2870
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='231' style='font-size:20px'>To capture complex temporal relationships both efficiently<br>and flexibly, Liu et al. [172] proposed a temporal adaptive<br>module (TAM). It adopts an adaptive kernel instead of self-<br>attention to capture global contextual information, with<br>lower time complexity than GLTR [171].</p>",
            "id": 231,
            "page": 13,
            "text": "To capture complex temporal relationships both efficiently\nand flexibly, Liu et al. [172] proposed a temporal adaptive\nmodule (TAM). It adopts an adaptive kernel instead of self-\nattention to capture global contextual information, with\nlower time complexity than GLTR [171]."
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 2872
                },
                {
                    "x": 2358,
                    "y": 2872
                },
                {
                    "x": 2358,
                    "y": 3116
                },
                {
                    "x": 1291,
                    "y": 3116
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='232' style='font-size:18px'>TAM has two branches, a local branch and a global branch.<br>Given the input feature map X E RCxTxHxW<br>, global spatial<br>average pooling GAP is first applied to the feature map to<br>ensure TAM has a low computational cost. Then the local<br>branch in TAM employs several 1D convolutions with ReLU</p>",
            "id": 232,
            "page": 13,
            "text": "TAM has two branches, a local branch and a global branch.\nGiven the input feature map X E RCxTxHxW\n, global spatial\naverage pooling GAP is first applied to the feature map to\nensure TAM has a low computational cost. Then the local\nbranch in TAM employs several 1D convolutions with ReLU"
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 109
                },
                {
                    "x": 1077,
                    "y": 109
                },
                {
                    "x": 1077,
                    "y": 150
                },
                {
                    "x": 193,
                    "y": 150
                }
            ],
            "category": "header",
            "html": "<header id='233' style='font-size:14px'>JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015</header>",
            "id": 233,
            "page": 14,
            "text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015"
        },
        {
            "bounding_box": [
                {
                    "x": 2316,
                    "y": 114
                },
                {
                    "x": 2353,
                    "y": 114
                },
                {
                    "x": 2353,
                    "y": 144
                },
                {
                    "x": 2316,
                    "y": 144
                }
            ],
            "category": "header",
            "html": "<br><header id='234' style='font-size:14px'>14</header>",
            "id": 234,
            "page": 14,
            "text": "14"
        },
        {
            "bounding_box": [
                {
                    "x": 1205,
                    "y": 184
                },
                {
                    "x": 1345,
                    "y": 184
                },
                {
                    "x": 1345,
                    "y": 221
                },
                {
                    "x": 1205,
                    "y": 221
                }
            ],
            "category": "header",
            "html": "<header id='235' style='font-size:16px'>TABLE 5</header>",
            "id": 235,
            "page": 14,
            "text": "TABLE 5"
        },
        {
            "bounding_box": [
                {
                    "x": 195,
                    "y": 218
                },
                {
                    "x": 2355,
                    "y": 218
                },
                {
                    "x": 2355,
                    "y": 373
                },
                {
                    "x": 195,
                    "y": 373
                }
            ],
            "category": "caption",
            "html": "<br><caption id='236' style='font-size:16px'>Representative temporal attention mechanisms sorted by date. ReID = re-identification, Action = action recognition. Ranges means the ranges of<br>attention map. S or H means soft or hard attention. g(x) and f(g(x), x) are the attention process described by Eq. 1. (A) aggregate information via<br>attention map. (I) exploit multi-scale short-term temporal contextual information (II)capture long-term temporal feature dependencies (III) capture local<br>temporal contexts</caption>",
            "id": 236,
            "page": 14,
            "text": "Representative temporal attention mechanisms sorted by date. ReID = re-identification, Action = action recognition. Ranges means the ranges of\nattention map. S or H means soft or hard attention. g(x) and f(g(x), x) are the attention process described by Eq. 1. (A) aggregate information via\nattention map. (I) exploit multi-scale short-term temporal contextual information (II)capture long-term temporal feature dependencies (III) capture local\ntemporal contexts"
        },
        {
            "bounding_box": [
                {
                    "x": 214,
                    "y": 413
                },
                {
                    "x": 2333,
                    "y": 413
                },
                {
                    "x": 2333,
                    "y": 815
                },
                {
                    "x": 214,
                    "y": 815
                }
            ],
            "category": "table",
            "html": "<table id='237' style='font-size:16px'><tr><td>Category</td><td>Method</td><td>Publication</td><td>Tasks</td><td>g(x)</td><td>f(g(x),x)</td><td>Ranges</td><td>S or H</td><td>Goals</td></tr><tr><td>Self-attention based methods</td><td>GLTR 171 I</td><td>ICCV2019</td><td>ReID</td><td>dilated 1D Convs -> self- attention in temporal di- mension</td><td>(A)</td><td>(0,1)</td><td>S</td><td>(I), (II).</td></tr><tr><td>Combine local attention and global attention</td><td>TAM [172]</td><td>Arxiv2020</td><td>Action</td><td>a)local: global spatial average pooling -> 1D Convs, b) global: global spatial average pooling -> MLP -> adaptive con- volution</td><td>(A)</td><td>(0,1)</td><td>S</td><td>(II), (III).</td></tr></table>",
            "id": 237,
            "page": 14,
            "text": "Category Method Publication Tasks g(x) f(g(x),x) Ranges S or H Goals\n Self-attention based methods GLTR 171 I ICCV2019 ReID dilated 1D Convs -> self- attention in temporal di- mension (A) (0,1) S (I), (II).\n Combine local attention and global attention TAM [172] Arxiv2020 Action a)local: global spatial average pooling -> 1D Convs, b) global: global spatial average pooling -> MLP -> adaptive con- volution (A) (0,1) S"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 897
                },
                {
                    "x": 1260,
                    "y": 897
                },
                {
                    "x": 1260,
                    "y": 1039
                },
                {
                    "x": 192,
                    "y": 1039
                }
            ],
            "category": "paragraph",
            "html": "<p id='238' style='font-size:18px'>nonlinearity across the temporal domain to produce location-<br>sensitive importance maps for enhancing frame-wise features.<br>The local branch can be written as</p>",
            "id": 238,
            "page": 14,
            "text": "nonlinearity across the temporal domain to produce location-\nsensitive importance maps for enhancing frame-wise features.\nThe local branch can be written as"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 1206
                },
                {
                    "x": 1257,
                    "y": 1206
                },
                {
                    "x": 1257,
                    "y": 1399
                },
                {
                    "x": 192,
                    "y": 1399
                }
            ],
            "category": "paragraph",
            "html": "<p id='239' style='font-size:18px'>Unlike the local branch, the global branch is location invari-<br>ant and focuses on generating a channel-wise adaptive kernel<br>based on global temporal information in each channel. For<br>the c-th channel, the kernel can be written as</p>",
            "id": 239,
            "page": 14,
            "text": "Unlike the local branch, the global branch is location invari-\nant and focuses on generating a channel-wise adaptive kernel\nbased on global temporal information in each channel. For\nthe c-th channel, the kernel can be written as"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 1501
                },
                {
                    "x": 1261,
                    "y": 1501
                },
                {
                    "x": 1261,
                    "y": 1603
                },
                {
                    "x": 192,
                    "y": 1603
                }
            ],
            "category": "paragraph",
            "html": "<p id='240' style='font-size:20px'>where Oc E RK and K is the adaptive kernel size. Finally,<br>TAM convolves the adaptive kernel 日 with Xout:</p>",
            "id": 240,
            "page": 14,
            "text": "where Oc E RK and K is the adaptive kernel size. Finally,\nTAM convolves the adaptive kernel 日 with Xout:"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 1705
                },
                {
                    "x": 1257,
                    "y": 1705
                },
                {
                    "x": 1257,
                    "y": 1946
                },
                {
                    "x": 192,
                    "y": 1946
                }
            ],
            "category": "paragraph",
            "html": "<p id='241' style='font-size:18px'>With the help of the local branch and global branch, TAM<br>can capture the complex temporal structures in video and<br>enhance per-frame features at low computational cost. Due<br>to its flexibility and lightweight design, TAM can be added<br>to any existing 2D CNNs.</p>",
            "id": 241,
            "page": 14,
            "text": "With the help of the local branch and global branch, TAM\ncan capture the complex temporal structures in video and\nenhance per-frame features at low computational cost. Due\nto its flexibility and lightweight design, TAM can be added\nto any existing 2D CNNs."
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 2009
                },
                {
                    "x": 619,
                    "y": 2009
                },
                {
                    "x": 619,
                    "y": 2056
                },
                {
                    "x": 193,
                    "y": 2056
                }
            ],
            "category": "paragraph",
            "html": "<p id='242' style='font-size:18px'>3.5 Branch Attention</p>",
            "id": 242,
            "page": 14,
            "text": "3.5 Branch Attention"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 2072
                },
                {
                    "x": 1260,
                    "y": 2072
                },
                {
                    "x": 1260,
                    "y": 2359
                },
                {
                    "x": 192,
                    "y": 2359
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='243' style='font-size:18px'>Branch attention can be seen as a dynamic branch selection<br>mechanism: which to pay attention to, used with a multi-branch<br>structure. We first summarize representative branch atten-<br>tion mechanisms and specify process g(x) and f(g(x), x)<br>described as Eq. 1 in Tab. 6, then discuss various ones in<br>detail.</p>",
            "id": 243,
            "page": 14,
            "text": "Branch attention can be seen as a dynamic branch selection\nmechanism: which to pay attention to, used with a multi-branch\nstructure. We first summarize representative branch atten-\ntion mechanisms and specify process g(x) and f(g(x), x)\ndescribed as Eq. 1 in Tab. 6, then discuss various ones in\ndetail."
        },
        {
            "bounding_box": [
                {
                    "x": 194,
                    "y": 2406
                },
                {
                    "x": 654,
                    "y": 2406
                },
                {
                    "x": 654,
                    "y": 2456
                },
                {
                    "x": 194,
                    "y": 2456
                }
            ],
            "category": "paragraph",
            "html": "<p id='244' style='font-size:20px'>3.5.1 Highway networks</p>",
            "id": 244,
            "page": 14,
            "text": "3.5.1 Highway networks"
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 2465
                },
                {
                    "x": 1257,
                    "y": 2465
                },
                {
                    "x": 1257,
                    "y": 2656
                },
                {
                    "x": 193,
                    "y": 2656
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='245' style='font-size:20px'>Inspired by the long short term memory network, Srivastava<br>et al. [113] proposed highway networks that employ adaptive<br>gating mechanisms to enable information flows across layers<br>to address the problem of training very deep networks.</p>",
            "id": 245,
            "page": 14,
            "text": "Inspired by the long short term memory network, Srivastava\net al. [113] proposed highway networks that employ adaptive\ngating mechanisms to enable information flows across layers\nto address the problem of training very deep networks."
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 2662
                },
                {
                    "x": 1258,
                    "y": 2662
                },
                {
                    "x": 1258,
                    "y": 2801
                },
                {
                    "x": 193,
                    "y": 2801
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='246' style='font-size:18px'>Supposing a plain neural network consists of L layers,<br>and Hl(X) denotes a non-linear transformation on the l-th<br>layer, a highway network can be expressed as</p>",
            "id": 246,
            "page": 14,
            "text": "Supposing a plain neural network consists of L layers,\nand Hl(X) denotes a non-linear transformation on the l-th\nlayer, a highway network can be expressed as"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 2966
                },
                {
                    "x": 1256,
                    "y": 2966
                },
                {
                    "x": 1256,
                    "y": 3114
                },
                {
                    "x": 192,
                    "y": 3114
                }
            ],
            "category": "paragraph",
            "html": "<p id='247' style='font-size:20px'>where Ti(X) denotes the transform gate regulating the<br>information flow for the l-th layer. Xl and Yl are the inputs<br>and outputs of the l-th layer.</p>",
            "id": 247,
            "page": 14,
            "text": "where Ti(X) denotes the transform gate regulating the\ninformation flow for the l-th layer. Xl and Yl are the inputs\nand outputs of the l-th layer."
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 895
                },
                {
                    "x": 2358,
                    "y": 895
                },
                {
                    "x": 2358,
                    "y": 1187
                },
                {
                    "x": 1292,
                    "y": 1187
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='248' style='font-size:20px'>The gating mechanism and skip-connection structure<br>make it possible to directly train very deep highway net-<br>works using simple gradient descent methods. Unlike fixed<br>skip-connections, the gating mechanism adapts to the input,<br>which helps to route information across layers. A highway<br>network can be incorporated in any CNN.</p>",
            "id": 248,
            "page": 14,
            "text": "The gating mechanism and skip-connection structure\nmake it possible to directly train very deep highway net-\nworks using simple gradient descent methods. Unlike fixed\nskip-connections, the gating mechanism adapts to the input,\nwhich helps to route information across layers. A highway\nnetwork can be incorporated in any CNN."
        },
        {
            "bounding_box": [
                {
                    "x": 1295,
                    "y": 1245
                },
                {
                    "x": 1549,
                    "y": 1245
                },
                {
                    "x": 1549,
                    "y": 1292
                },
                {
                    "x": 1295,
                    "y": 1292
                }
            ],
            "category": "paragraph",
            "html": "<p id='249' style='font-size:18px'>3.5.2 SKNet</p>",
            "id": 249,
            "page": 14,
            "text": "3.5.2 SKNet"
        },
        {
            "bounding_box": [
                {
                    "x": 1293,
                    "y": 1312
                },
                {
                    "x": 2357,
                    "y": 1312
                },
                {
                    "x": 2357,
                    "y": 1550
                },
                {
                    "x": 1293,
                    "y": 1550
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='250' style='font-size:20px'>Research in the neuroscience community suggests that visual<br>cortical neurons adaptively adjust the sizes of their receptive<br>fields (RFs) according to the input stimulus [174]. This<br>inspired Li et al. [114] to propose an automatic selection<br>operation called selective kernel (SK) convolution.</p>",
            "id": 250,
            "page": 14,
            "text": "Research in the neuroscience community suggests that visual\ncortical neurons adaptively adjust the sizes of their receptive\nfields (RFs) according to the input stimulus [174]. This\ninspired Li et al. [114] to propose an automatic selection\noperation called selective kernel (SK) convolution."
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 1557
                },
                {
                    "x": 2357,
                    "y": 1557
                },
                {
                    "x": 2357,
                    "y": 1988
                },
                {
                    "x": 1292,
                    "y": 1988
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='251' style='font-size:18px'>SK convolution is implemented using three operations:<br>split, fuse and select. During split, transformations with<br>different kernel sizes are applied to the feature map to obtain<br>different sized RFs. Information from all branches is then<br>fused together via element-wise summation to compute the<br>gate vector. This is used to control information flows from the<br>multiple branches. Finally, the output feature map is obtained<br>by aggregating feature maps for all branches, guided by the<br>gate vector. This can be expressed as:</p>",
            "id": 251,
            "page": 14,
            "text": "SK convolution is implemented using three operations:\nsplit, fuse and select. During split, transformations with\ndifferent kernel sizes are applied to the feature map to obtain\ndifferent sized RFs. Information from all branches is then\nfused together via element-wise summation to compute the\ngate vector. This is used to control information flows from the\nmultiple branches. Finally, the output feature map is obtained\nby aggregating feature maps for all branches, guided by the\ngate vector. This can be expressed as:"
        },
        {
            "bounding_box": [
                {
                    "x": 1474,
                    "y": 2086
                },
                {
                    "x": 1510,
                    "y": 2086
                },
                {
                    "x": 1510,
                    "y": 2117
                },
                {
                    "x": 1474,
                    "y": 2117
                }
            ],
            "category": "paragraph",
            "html": "<p id='252' style='font-size:14px'>K</p>",
            "id": 252,
            "page": 14,
            "text": "K"
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 2580
                },
                {
                    "x": 2358,
                    "y": 2580
                },
                {
                    "x": 2358,
                    "y": 2869
                },
                {
                    "x": 1291,
                    "y": 2869
                }
            ],
            "category": "paragraph",
            "html": "<p id='253' style='font-size:18px'>Here, each transformation Fk has a unique kernel size to<br>provide different scales of information for each branch. For<br>efficiency, Fk is implemented by grouped or depthwise<br>convolutions followed by dilated convolution, batch nor-<br>malization and ReLU activation in sequence. t(c) denotes the<br>c-th element of vector t, or the c-th row of matrix t.</p>",
            "id": 253,
            "page": 14,
            "text": "Here, each transformation Fk has a unique kernel size to\nprovide different scales of information for each branch. For\nefficiency, Fk is implemented by grouped or depthwise\nconvolutions followed by dilated convolution, batch nor-\nmalization and ReLU activation in sequence. t(c) denotes the\nc-th element of vector t, or the c-th row of matrix t."
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 2874
                },
                {
                    "x": 2358,
                    "y": 2874
                },
                {
                    "x": 2358,
                    "y": 3116
                },
                {
                    "x": 1291,
                    "y": 3116
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='254' style='font-size:18px'>SK convolutions enable the network to adaptively adjust<br>neurons' RF sizes according to the input, giving a notable<br>improvement in results at little computational cost. The gate<br>mechanism in SK convolutions is used to fuse information<br>from multiple branches. Due to its lightweight design, SK</p>",
            "id": 254,
            "page": 14,
            "text": "SK convolutions enable the network to adaptively adjust\nneurons' RF sizes according to the input, giving a notable\nimprovement in results at little computational cost. The gate\nmechanism in SK convolutions is used to fuse information\nfrom multiple branches. Due to its lightweight design, SK"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 109
                },
                {
                    "x": 1076,
                    "y": 109
                },
                {
                    "x": 1076,
                    "y": 150
                },
                {
                    "x": 192,
                    "y": 150
                }
            ],
            "category": "header",
            "html": "<header id='255' style='font-size:16px'>JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015</header>",
            "id": 255,
            "page": 15,
            "text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015"
        },
        {
            "bounding_box": [
                {
                    "x": 2315,
                    "y": 113
                },
                {
                    "x": 2353,
                    "y": 113
                },
                {
                    "x": 2353,
                    "y": 145
                },
                {
                    "x": 2315,
                    "y": 145
                }
            ],
            "category": "header",
            "html": "<br><header id='256' style='font-size:14px'>15</header>",
            "id": 256,
            "page": 15,
            "text": "15"
        },
        {
            "bounding_box": [
                {
                    "x": 1205,
                    "y": 185
                },
                {
                    "x": 1345,
                    "y": 185
                },
                {
                    "x": 1345,
                    "y": 221
                },
                {
                    "x": 1205,
                    "y": 221
                }
            ],
            "category": "paragraph",
            "html": "<p id='257' style='font-size:16px'>TABLE 6</p>",
            "id": 257,
            "page": 15,
            "text": "TABLE 6"
        },
        {
            "bounding_box": [
                {
                    "x": 195,
                    "y": 221
                },
                {
                    "x": 2351,
                    "y": 221
                },
                {
                    "x": 2351,
                    "y": 337
                },
                {
                    "x": 195,
                    "y": 337
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='258' style='font-size:18px'>Representative branch attention mechanisms sorted by date. Cls = classification, Det=Object Detection. g(x) and f(g(x), x) are the attention<br>process described by Eq. 1 .Ranges means the ranges of attention map. S or H means soft or hard attention. (A) element-wise product. (B)<br>channel-wise product. (C) aggergate information via attention. (I)overcome the problem of vanishing gradient (II) dynamically fuse different branches.</p>",
            "id": 258,
            "page": 15,
            "text": "Representative branch attention mechanisms sorted by date. Cls = classification, Det=Object Detection. g(x) and f(g(x), x) are the attention\nprocess described by Eq. 1 .Ranges means the ranges of attention map. S or H means soft or hard attention. (A) element-wise product. (B)\nchannel-wise product. (C) aggergate information via attention. (I)overcome the problem of vanishing gradient (II) dynamically fuse different branches."
        },
        {
            "bounding_box": [
                {
                    "x": 200,
                    "y": 337
                },
                {
                    "x": 2352,
                    "y": 337
                },
                {
                    "x": 2352,
                    "y": 410
                },
                {
                    "x": 200,
                    "y": 410
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='259' style='font-size:18px'>(III) adaptively select a suitable receptive field (IV) improve the performance of standard convolution (be) dynamically fuse different convolution<br>kernels.</p>",
            "id": 259,
            "page": 15,
            "text": "(III) adaptively select a suitable receptive field (IV) improve the performance of standard convolution (be) dynamically fuse different convolution\nkernels."
        },
        {
            "bounding_box": [
                {
                    "x": 220,
                    "y": 445
                },
                {
                    "x": 2331,
                    "y": 445
                },
                {
                    "x": 2331,
                    "y": 770
                },
                {
                    "x": 220,
                    "y": 770
                }
            ],
            "category": "table",
            "html": "<table id='260' style='font-size:16px'><tr><td>Category</td><td>Method</td><td>Publication</td><td>Tasks</td><td>g(x)</td><td>f(g(x),x)</td><td>Ranges</td><td>S or H</td><td>Goals</td></tr><tr><td rowspan=\"2\">Combine differ- ent branches</td><td>Highway Net- work [113]</td><td>ICML2015W</td><td>Cls</td><td>linear layer -> sigmoid</td><td>(A)</td><td>(0,1)</td><td>S</td><td>(I), (II).</td></tr><tr><td>SKNet [114]</td><td>CVPR2019</td><td>Cls</td><td>global average pooling -> MLP -> softmax</td><td>(B)</td><td>(0,1)</td><td>S</td><td>(11), (111)</td></tr><tr><td>Combine differ- ent convolution kernels</td><td>CondConv [173]</td><td>NeurIPS2019</td><td>Cls, Det</td><td>global average pooling - > linear layer -> sigmoid</td><td>( 0</td><td>(0,1)</td><td>S</td><td>(IV), (V).</td></tr></table>",
            "id": 260,
            "page": 15,
            "text": "Category Method Publication Tasks g(x) f(g(x),x) Ranges S or H Goals\n Combine differ- ent branches Highway Net- work [113] ICML2015W Cls linear layer -> sigmoid (A) (0,1) S (I), (II).\n SKNet [114] CVPR2019 Cls global average pooling -> MLP -> softmax (B) (0,1) S (11), (111)\n Combine differ- ent convolution kernels CondConv [173] NeurIPS2019 Cls, Det global average pooling - > linear layer -> sigmoid ( 0 (0,1) S"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 852
                },
                {
                    "x": 1258,
                    "y": 852
                },
                {
                    "x": 1258,
                    "y": 1093
                },
                {
                    "x": 192,
                    "y": 1093
                }
            ],
            "category": "paragraph",
            "html": "<p id='261' style='font-size:20px'>convolution can be applied to any CNN backbone by replac-<br>ing all large kernel convolutions. ResNeSt [115] also adopts<br>this attention mechanism to improve the CNN backbone in a<br>more general way, giving excellent results on ResNet [145]<br>and ResNeXt [175].</p>",
            "id": 261,
            "page": 15,
            "text": "convolution can be applied to any CNN backbone by replac-\ning all large kernel convolutions. ResNeSt [115] also adopts\nthis attention mechanism to improve the CNN backbone in a\nmore general way, giving excellent results on ResNet [145]\nand ResNeXt [175]."
        },
        {
            "bounding_box": [
                {
                    "x": 196,
                    "y": 1138
                },
                {
                    "x": 521,
                    "y": 1138
                },
                {
                    "x": 521,
                    "y": 1187
                },
                {
                    "x": 196,
                    "y": 1187
                }
            ],
            "category": "paragraph",
            "html": "<p id='262' style='font-size:20px'>3.5.3 CondConv</p>",
            "id": 262,
            "page": 15,
            "text": "3.5.3 CondConv"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 1197
                },
                {
                    "x": 1257,
                    "y": 1197
                },
                {
                    "x": 1257,
                    "y": 1535
                },
                {
                    "x": 192,
                    "y": 1535
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='263' style='font-size:18px'>A basic assumption in CNNs is that all convolution kernels<br>are the same. Given this, the typical way to enhance the<br>representational power of a network is to increase its depth<br>or width, which introduces significant extra computational<br>cost. In order to more efficiently increase the capacity of<br>convolutional neural networks, Yang et al. [173] proposed a<br>novel multi-branch operator called CondConv.</p>",
            "id": 263,
            "page": 15,
            "text": "A basic assumption in CNNs is that all convolution kernels\nare the same. Given this, the typical way to enhance the\nrepresentational power of a network is to increase its depth\nor width, which introduces significant extra computational\ncost. In order to more efficiently increase the capacity of\nconvolutional neural networks, Yang et al. [173] proposed a\nnovel multi-branch operator called CondConv."
        },
        {
            "bounding_box": [
                {
                    "x": 256,
                    "y": 1537
                },
                {
                    "x": 966,
                    "y": 1537
                },
                {
                    "x": 966,
                    "y": 1581
                },
                {
                    "x": 256,
                    "y": 1581
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='264' style='font-size:18px'>An ordinary convolution can be written</p>",
            "id": 264,
            "page": 15,
            "text": "An ordinary convolution can be written"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 1689
                },
                {
                    "x": 1255,
                    "y": 1689
                },
                {
                    "x": 1255,
                    "y": 1832
                },
                {
                    "x": 192,
                    "y": 1832
                }
            ],
            "category": "paragraph",
            "html": "<p id='265' style='font-size:18px'>where * denotes convolution. The learnable parameter W<br>is the same for all samples. CondConv adaptively combines<br>multiple convolution kernels and can be written as:</p>",
            "id": 265,
            "page": 15,
            "text": "where * denotes convolution. The learnable parameter W\nis the same for all samples. CondConv adaptively combines\nmultiple convolution kernels and can be written as:"
        },
        {
            "bounding_box": [
                {
                    "x": 195,
                    "y": 1939
                },
                {
                    "x": 1075,
                    "y": 1939
                },
                {
                    "x": 1075,
                    "y": 1988
                },
                {
                    "x": 195,
                    "y": 1988
                }
            ],
            "category": "paragraph",
            "html": "<p id='266' style='font-size:20px'>Here, a is a learnable weight vector computed by</p>",
            "id": 266,
            "page": 15,
            "text": "Here, a is a learnable weight vector computed by"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 2092
                },
                {
                    "x": 1260,
                    "y": 2092
                },
                {
                    "x": 1260,
                    "y": 2187
                },
                {
                    "x": 192,
                    "y": 2187
                }
            ],
            "category": "paragraph",
            "html": "<p id='267' style='font-size:18px'>This process is equivalent to an ensemble of multiple experts,<br>as shown in Fig. 10.</p>",
            "id": 267,
            "page": 15,
            "text": "This process is equivalent to an ensemble of multiple experts,\nas shown in Fig. 10."
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 2190
                },
                {
                    "x": 1257,
                    "y": 2190
                },
                {
                    "x": 1257,
                    "y": 2382
                },
                {
                    "x": 193,
                    "y": 2382
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='268' style='font-size:18px'>CondConv makes full use of the advantages of the multi-<br>branch structure using a branch attention method with little<br>computing cost. It presents a novel manner to efficiently<br>increase the capability of networks.</p>",
            "id": 268,
            "page": 15,
            "text": "CondConv makes full use of the advantages of the multi-\nbranch structure using a branch attention method with little\ncomputing cost. It presents a novel manner to efficiently\nincrease the capability of networks."
        },
        {
            "bounding_box": [
                {
                    "x": 195,
                    "y": 2427
                },
                {
                    "x": 706,
                    "y": 2427
                },
                {
                    "x": 706,
                    "y": 2476
                },
                {
                    "x": 195,
                    "y": 2476
                }
            ],
            "category": "paragraph",
            "html": "<p id='269' style='font-size:22px'>3.5.4 Dynamic Convolution</p>",
            "id": 269,
            "page": 15,
            "text": "3.5.4 Dynamic Convolution"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 2488
                },
                {
                    "x": 1257,
                    "y": 2488
                },
                {
                    "x": 1257,
                    "y": 2870
                },
                {
                    "x": 192,
                    "y": 2870
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='270' style='font-size:20px'>The extremely low computational cost of lightweight CNNs<br>constrains the depth and width of the networks, further<br>decreasing their representational power. To address the above<br>problem, Chen et al. [116] proposed dynamic convolution, a<br>novel operator design that increases representational power<br>with negligible additional computational cost and does not<br>change the width or depth of the network in parallel with<br>CondConv [173].</p>",
            "id": 270,
            "page": 15,
            "text": "The extremely low computational cost of lightweight CNNs\nconstrains the depth and width of the networks, further\ndecreasing their representational power. To address the above\nproblem, Chen et al. [116] proposed dynamic convolution, a\nnovel operator design that increases representational power\nwith negligible additional computational cost and does not\nchange the width or depth of the network in parallel with\nCondConv [173]."
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 2873
                },
                {
                    "x": 1258,
                    "y": 2873
                },
                {
                    "x": 1258,
                    "y": 3115
                },
                {
                    "x": 192,
                    "y": 3115
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='271' style='font-size:18px'>Dynamic convolution uses K parallel convolution kernels<br>of the same size and input/ output dimensions instead of<br>one kernel per layer. Like SE blocks, it adopts a squeeze-<br>and-excitation mechanism to generate the attention weights<br>for the different convolution kernels. These kernels are</p>",
            "id": 271,
            "page": 15,
            "text": "Dynamic convolution uses K parallel convolution kernels\nof the same size and input/ output dimensions instead of\none kernel per layer. Like SE blocks, it adopts a squeeze-\nand-excitation mechanism to generate the attention weights\nfor the different convolution kernels. These kernels are"
        },
        {
            "bounding_box": [
                {
                    "x": 1288,
                    "y": 845
                },
                {
                    "x": 2358,
                    "y": 845
                },
                {
                    "x": 2358,
                    "y": 1229
                },
                {
                    "x": 1288,
                    "y": 1229
                }
            ],
            "category": "figure",
            "html": "<br><figure><img id='272' style='font-size:14px' alt=\"PREV LAYER OUTPUT PREV LAYER OUTPUT\nW, W W.\nROUTE FN W W, W3\nCOMBINE ROUTE FN CONV, CONV2 CONV3\nCONV COMBINE\nBN BN\nReLU ReLU\n(a) CondConv: (�1 W1 + + an Wn) *x (b) Mixture of Experts: �1 (W1 *x)+...+an(Wn*x)\" data-coord=\"top-left:(1288,845); bottom-right:(2358,1229)\" /></figure>",
            "id": 272,
            "page": 15,
            "text": "PREV LAYER OUTPUT PREV LAYER OUTPUT\nW, W W.\nROUTE FN W W, W3\nCOMBINE ROUTE FN CONV, CONV2 CONV3\nCONV COMBINE\nBN BN\nReLU ReLU\n(a) CondConv: (�1 W1 + + an Wn) *x (b) Mixture of Experts: �1 (W1 *x)+...+an(Wn*x)"
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 1319
                },
                {
                    "x": 2358,
                    "y": 1319
                },
                {
                    "x": 2358,
                    "y": 1516
                },
                {
                    "x": 1291,
                    "y": 1516
                }
            ],
            "category": "caption",
            "html": "<caption id='273' style='font-size:16px'>Fig. 10. CondConv [173]. (a) CondConv first combines different con-<br>volution kernels and then uses the combined kernel for convolution. (b)<br>Mixture of experts first uses multiple convolution kernels for convolution<br>and then merges the results. While (a) and (b) are equivalent, (a) has<br>much lower computational cost. Figure is taken from [173].</caption>",
            "id": 273,
            "page": 15,
            "text": "Fig. 10. CondConv [173]. (a) CondConv first combines different con-\nvolution kernels and then uses the combined kernel for convolution. (b)\nMixture of experts first uses multiple convolution kernels for convolution\nand then merges the results. While (a) and (b) are equivalent, (a) has\nmuch lower computational cost. Figure is taken from [173]."
        },
        {
            "bounding_box": [
                {
                    "x": 1294,
                    "y": 1593
                },
                {
                    "x": 2355,
                    "y": 1593
                },
                {
                    "x": 2355,
                    "y": 1689
                },
                {
                    "x": 1294,
                    "y": 1689
                }
            ],
            "category": "paragraph",
            "html": "<p id='274' style='font-size:22px'>then aggregated dynamically by weighted summation and<br>applied to the input feature map X:</p>",
            "id": 274,
            "page": 15,
            "text": "then aggregated dynamically by weighted summation and\napplied to the input feature map X:"
        },
        {
            "bounding_box": [
                {
                    "x": 1689,
                    "y": 1776
                },
                {
                    "x": 1726,
                    "y": 1776
                },
                {
                    "x": 1726,
                    "y": 1806
                },
                {
                    "x": 1689,
                    "y": 1806
                }
            ],
            "category": "paragraph",
            "html": "<p id='275' style='font-size:14px'>K</p>",
            "id": 275,
            "page": 15,
            "text": "K"
        },
        {
            "bounding_box": [
                {
                    "x": 1676,
                    "y": 1874
                },
                {
                    "x": 1737,
                    "y": 1874
                },
                {
                    "x": 1737,
                    "y": 1903
                },
                {
                    "x": 1676,
                    "y": 1903
                }
            ],
            "category": "paragraph",
            "html": "<p id='276' style='font-size:14px'>i=1</p>",
            "id": 276,
            "page": 15,
            "text": "i=1"
        },
        {
            "bounding_box": [
                {
                    "x": 1293,
                    "y": 1984
                },
                {
                    "x": 2353,
                    "y": 1984
                },
                {
                    "x": 2353,
                    "y": 2080
                },
                {
                    "x": 1293,
                    "y": 2080
                }
            ],
            "category": "paragraph",
            "html": "<p id='277' style='font-size:18px'>Here the convolutions are combined by summation of<br>weights and biases of convolutional kernels.</p>",
            "id": 277,
            "page": 15,
            "text": "Here the convolutions are combined by summation of\nweights and biases of convolutional kernels."
        },
        {
            "bounding_box": [
                {
                    "x": 1293,
                    "y": 2083
                },
                {
                    "x": 2357,
                    "y": 2083
                },
                {
                    "x": 2357,
                    "y": 2369
                },
                {
                    "x": 1293,
                    "y": 2369
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='278' style='font-size:18px'>Compared to applying convolution to the feature<br>map, the computational cost of squeeze-and-excitation and<br>weighted summation is extremely low. Dynamic convolution<br>thus provides an efficient operation to improve representa-<br>tional power and can be easily used as a replacement for any<br>convolution.</p>",
            "id": 278,
            "page": 15,
            "text": "Compared to applying convolution to the feature\nmap, the computational cost of squeeze-and-excitation and\nweighted summation is extremely low. Dynamic convolution\nthus provides an efficient operation to improve representa-\ntional power and can be easily used as a replacement for any\nconvolution."
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 2426
                },
                {
                    "x": 1919,
                    "y": 2426
                },
                {
                    "x": 1919,
                    "y": 2474
                },
                {
                    "x": 1291,
                    "y": 2474
                }
            ],
            "category": "paragraph",
            "html": "<p id='279' style='font-size:20px'>3.6 Channel & Spatial Attention</p>",
            "id": 279,
            "page": 15,
            "text": "3.6 Channel & Spatial Attention"
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 2488
                },
                {
                    "x": 2358,
                    "y": 2488
                },
                {
                    "x": 2358,
                    "y": 2921
                },
                {
                    "x": 1292,
                    "y": 2921
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='280' style='font-size:20px'>Channel & spatial attention combines the advantages of<br>channel attention and spatial attention. It adaptively selects<br>both important objects and regions [50]. The residual attention<br>network [119] pioneered the field of channel & spatial atten-<br>tion, emphasizing the importance of informative features in<br>both spatial and channel dimensions. It adopts a bottom-up<br>structure consisting of several convolutions to produce a 3D<br>(height, width, channel) attention map. However, it has high<br>computational cost and limited receptive fields.</p>",
            "id": 280,
            "page": 15,
            "text": "Channel & spatial attention combines the advantages of\nchannel attention and spatial attention. It adaptively selects\nboth important objects and regions [50]. The residual attention\nnetwork [119] pioneered the field of channel & spatial atten-\ntion, emphasizing the importance of informative features in\nboth spatial and channel dimensions. It adopts a bottom-up\nstructure consisting of several convolutions to produce a 3D\n(height, width, channel) attention map. However, it has high\ncomputational cost and limited receptive fields."
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 2922
                },
                {
                    "x": 2356,
                    "y": 2922
                },
                {
                    "x": 2356,
                    "y": 3116
                },
                {
                    "x": 1291,
                    "y": 3116
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='281' style='font-size:22px'>To leverage global spatial information later works [6],<br>[117] enhance discrimination of features by introducing<br>global average pooling, as well as decoupling channel<br>attention and spatial channel attention for computational</p>",
            "id": 281,
            "page": 15,
            "text": "To leverage global spatial information later works [6],\n[117] enhance discrimination of features by introducing\nglobal average pooling, as well as decoupling channel\nattention and spatial channel attention for computational"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 109
                },
                {
                    "x": 1076,
                    "y": 109
                },
                {
                    "x": 1076,
                    "y": 149
                },
                {
                    "x": 192,
                    "y": 149
                }
            ],
            "category": "header",
            "html": "<header id='282' style='font-size:14px'>JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015</header>",
            "id": 282,
            "page": 16,
            "text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015"
        },
        {
            "bounding_box": [
                {
                    "x": 2314,
                    "y": 112
                },
                {
                    "x": 2353,
                    "y": 112
                },
                {
                    "x": 2353,
                    "y": 145
                },
                {
                    "x": 2314,
                    "y": 145
                }
            ],
            "category": "header",
            "html": "<br><header id='283' style='font-size:14px'>16</header>",
            "id": 283,
            "page": 16,
            "text": "16"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 180
                },
                {
                    "x": 1258,
                    "y": 180
                },
                {
                    "x": 1258,
                    "y": 371
                },
                {
                    "x": 192,
                    "y": 371
                }
            ],
            "category": "paragraph",
            "html": "<p id='284' style='font-size:18px'>efficiency. Other works [10], [101] apply self-attention mech-<br>anisms for channel & spatial attention to explore pairwise<br>interaction. Yet further works [120], [124] adopt the spatial-<br>channel attention mechanism to enlarge the receptive field.</p>",
            "id": 284,
            "page": 16,
            "text": "efficiency. Other works [10], [101] apply self-attention mech-\nanisms for channel & spatial attention to explore pairwise\ninteraction. Yet further works [120], [124] adopt the spatial-\nchannel attention mechanism to enlarge the receptive field."
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 380
                },
                {
                    "x": 1258,
                    "y": 380
                },
                {
                    "x": 1258,
                    "y": 525
                },
                {
                    "x": 193,
                    "y": 525
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='285' style='font-size:16px'>Representative channel & spatial attention mechanisms<br>and specific process g(x) and f(g(x), x) described as Eq. 1<br>are in given Tab. 7; we next discuss various ones in detail.</p>",
            "id": 285,
            "page": 16,
            "text": "Representative channel & spatial attention mechanisms\nand specific process g(x) and f(g(x), x) described as Eq. 1\nare in given Tab. 7; we next discuss various ones in detail."
        },
        {
            "bounding_box": [
                {
                    "x": 1293,
                    "y": 181
                },
                {
                    "x": 2354,
                    "y": 181
                },
                {
                    "x": 2354,
                    "y": 275
                },
                {
                    "x": 1293,
                    "y": 275
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='286' style='font-size:18px'>it has two parallel branches using max-pool and avg-pool<br>operations:</p>",
            "id": 286,
            "page": 16,
            "text": "it has two parallel branches using max-pool and avg-pool\noperations:"
        },
        {
            "bounding_box": [
                {
                    "x": 194,
                    "y": 606
                },
                {
                    "x": 813,
                    "y": 606
                },
                {
                    "x": 813,
                    "y": 655
                },
                {
                    "x": 194,
                    "y": 655
                }
            ],
            "category": "paragraph",
            "html": "<p id='287' style='font-size:16px'>3.6.1 Residual Attention Network</p>",
            "id": 287,
            "page": 16,
            "text": "3.6.1 Residual Attention Network"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 687
                },
                {
                    "x": 1258,
                    "y": 687
                },
                {
                    "x": 1258,
                    "y": 879
                },
                {
                    "x": 192,
                    "y": 879
                }
            ],
            "category": "paragraph",
            "html": "<p id='288' style='font-size:18px'>Inspired by the success of ResNet [145], Wang et al. [119]<br>proposed the very deep convolutional residual attention<br>network (RAN) by combining an attention mechanism with<br>residual connections.</p>",
            "id": 288,
            "page": 16,
            "text": "Inspired by the success of ResNet [145], Wang et al. [119]\nproposed the very deep convolutional residual attention\nnetwork (RAN) by combining an attention mechanism with\nresidual connections."
        },
        {
            "bounding_box": [
                {
                    "x": 191,
                    "y": 886
                },
                {
                    "x": 1256,
                    "y": 886
                },
                {
                    "x": 1256,
                    "y": 1368
                },
                {
                    "x": 191,
                    "y": 1368
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='289' style='font-size:16px'>Each attention module stacked in a residual attention<br>network can be divided into a mask branch and a trunk<br>branch. The trunk branch processes features, and can be<br>implemented by any state-of-the-art structure including a<br>pre-activation residual unit and an inception block. The mask<br>branch uses a bottom-up top-down structure to learn a mask<br>of the same size that softly weights output features from<br>the trunk branch. A sigmoid layer normalizes the output to<br>[0, 1] after two 1 x 1 convolution layers. Overall the residual<br>attention mechanism can be written as</p>",
            "id": 289,
            "page": 16,
            "text": "Each attention module stacked in a residual attention\nnetwork can be divided into a mask branch and a trunk\nbranch. The trunk branch processes features, and can be\nimplemented by any state-of-the-art structure including a\npre-activation residual unit and an inception block. The mask\nbranch uses a bottom-up top-down structure to learn a mask\nof the same size that softly weights output features from\nthe trunk branch. A sigmoid layer normalizes the output to\n[0, 1] after two 1 x 1 convolution layers. Overall the residual\nattention mechanism can be written as"
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 627
                },
                {
                    "x": 2357,
                    "y": 627
                },
                {
                    "x": 2357,
                    "y": 917
                },
                {
                    "x": 1291,
                    "y": 917
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='290' style='font-size:16px'>where GAPs and GMPs denote global average pooling and<br>global max pooling operations in the spatial domain. The<br>spatial attention sub-module models the spatial relationships<br>of features, and is complementary to channel attention.<br>Unlike channel attention, it applies a convolution layer with<br>a large kernel to generate the attention map</p>",
            "id": 290,
            "page": 16,
            "text": "where GAPs and GMPs denote global average pooling and\nglobal max pooling operations in the spatial domain. The\nspatial attention sub-module models the spatial relationships\nof features, and is complementary to channel attention.\nUnlike channel attention, it applies a convolution layer with\na large kernel to generate the attention map"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 1589
                },
                {
                    "x": 1259,
                    "y": 1589
                },
                {
                    "x": 1259,
                    "y": 1926
                },
                {
                    "x": 192,
                    "y": 1926
                }
            ],
            "category": "paragraph",
            "html": "<p id='291' style='font-size:16px'>where hup is a bottom-up structure, using max-pooling sev-<br>eral times after residual units to increase the receptive field,<br>while hdown is the top-down part using linear interpolation to<br>keep the output size the same as the input feature map. There<br>are also skip-connections between the two parts, which are<br>omitted from the formulation. f represents the trunk branch<br>which can be any state-of-the-art structure.</p>",
            "id": 291,
            "page": 16,
            "text": "where hup is a bottom-up structure, using max-pooling sev-\neral times after residual units to increase the receptive field,\nwhile hdown is the top-down part using linear interpolation to\nkeep the output size the same as the input feature map. There\nare also skip-connections between the two parts, which are\nomitted from the formulation. f represents the trunk branch\nwhich can be any state-of-the-art structure."
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 1931
                },
                {
                    "x": 1260,
                    "y": 1931
                },
                {
                    "x": 1260,
                    "y": 2321
                },
                {
                    "x": 192,
                    "y": 2321
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='292' style='font-size:18px'>Inside each attention module, a bottom-up top-down<br>feedforward structure models both spatial and cross-channel<br>dependencies, leading to a consistent performance improve-<br>ment. Residual attention can be incorporated into any<br>deep network structure in an end-to-end training fashion.<br>However, the proposed bottom-up top-down structure fails<br>to leverage global spatial information. Furthermore, directly<br>predicting a 3D attention map has high computational cost.</p>",
            "id": 292,
            "page": 16,
            "text": "Inside each attention module, a bottom-up top-down\nfeedforward structure models both spatial and cross-channel\ndependencies, leading to a consistent performance improve-\nment. Residual attention can be incorporated into any\ndeep network structure in an end-to-end training fashion.\nHowever, the proposed bottom-up top-down structure fails\nto leverage global spatial information. Furthermore, directly\npredicting a 3D attention map has high computational cost."
        },
        {
            "bounding_box": [
                {
                    "x": 195,
                    "y": 2400
                },
                {
                    "x": 451,
                    "y": 2400
                },
                {
                    "x": 451,
                    "y": 2446
                },
                {
                    "x": 195,
                    "y": 2446
                }
            ],
            "category": "paragraph",
            "html": "<p id='293' style='font-size:16px'>3.6.2 CBAM</p>",
            "id": 293,
            "page": 16,
            "text": "3.6.2 CBAM"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 2480
                },
                {
                    "x": 1259,
                    "y": 2480
                },
                {
                    "x": 1259,
                    "y": 2771
                },
                {
                    "x": 192,
                    "y": 2771
                }
            ],
            "category": "paragraph",
            "html": "<p id='294' style='font-size:18px'>To enhance informative channels as well as important regions,<br>Woo et al. [6] proposed the convolutional block attention module<br>(CBAM) which stacks channel attention and spatial attention<br>in series. It decouples the channel attention map and spatial<br>attention map for computational efficiency, and leverages<br>spatial global information by introducing global pooling.</p>",
            "id": 294,
            "page": 16,
            "text": "To enhance informative channels as well as important regions,\nWoo et al. [6] proposed the convolutional block attention module\n(CBAM) which stacks channel attention and spatial attention\nin series. It decouples the channel attention map and spatial\nattention map for computational efficiency, and leverages\nspatial global information by introducing global pooling."
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 2778
                },
                {
                    "x": 1258,
                    "y": 2778
                },
                {
                    "x": 1258,
                    "y": 3118
                },
                {
                    "x": 192,
                    "y": 3118
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='295' style='font-size:16px'>CBAM has two sequential sub-modules, channel and<br>spatial. Given an input feature map X E RCxHxW<br>it<br>sequentially infers a 1D channel attention vector Sc E RC<br>and a 2D spatial attention map Ss E RHxW The formulation<br>of the channel attention sub-module is similar to that of<br>an SE block, except that it adopts more than one type of<br>pooling operation to aggregate global information. In detail,</p>",
            "id": 295,
            "page": 16,
            "text": "CBAM has two sequential sub-modules, channel and\nspatial. Given an input feature map X E RCxHxW\nit\nsequentially infers a 1D channel attention vector Sc E RC\nand a 2D spatial attention map Ss E RHxW The formulation\nof the channel attention sub-module is similar to that of\nan SE block, except that it adopts more than one type of\npooling operation to aggregate global information. In detail,"
        },
        {
            "bounding_box": [
                {
                    "x": 1293,
                    "y": 1267
                },
                {
                    "x": 2356,
                    "y": 1267
                },
                {
                    "x": 2356,
                    "y": 1459
                },
                {
                    "x": 1293,
                    "y": 1459
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='296' style='font-size:16px'>where Conv(·) represents a convolution operation, while<br>GAPc and GMPc are global pooling operations in the channel<br>domain. I denotes concatenation over channels. The overall<br>attention process can be summarized as</p>",
            "id": 296,
            "page": 16,
            "text": "where Conv(·) represents a convolution operation, while\nGAPc and GMPc are global pooling operations in the channel\ndomain. I denotes concatenation over channels. The overall\nattention process can be summarized as"
        },
        {
            "bounding_box": [
                {
                    "x": 1294,
                    "y": 1689
                },
                {
                    "x": 2359,
                    "y": 1689
                },
                {
                    "x": 2359,
                    "y": 2265
                },
                {
                    "x": 1294,
                    "y": 2265
                }
            ],
            "category": "paragraph",
            "html": "<p id='297' style='font-size:16px'>Combining channel attention and spatial attention se-<br>quentially, CBAM can utilize both spatial and cross-channel<br>relationships of features to tell the network what to focus on<br>and where to focus. To be more specific, it emphasizes useful<br>channels as well as enhancing informative local regions. Due<br>to its lightweight design, CBAM can be integrated into any<br>CNN architecture seamlessly with negligible additional cost.<br>Nevertheless, there is still room for improvement in the<br>channel & spatial attention mechanism. For instance, CBAM<br>adopts a convolution to produce the spatial attention map, so<br>the spatial sub-module may suffer from a limited receptive<br>field.</p>",
            "id": 297,
            "page": 16,
            "text": "Combining channel attention and spatial attention se-\nquentially, CBAM can utilize both spatial and cross-channel\nrelationships of features to tell the network what to focus on\nand where to focus. To be more specific, it emphasizes useful\nchannels as well as enhancing informative local regions. Due\nto its lightweight design, CBAM can be integrated into any\nCNN architecture seamlessly with negligible additional cost.\nNevertheless, there is still room for improvement in the\nchannel & spatial attention mechanism. For instance, CBAM\nadopts a convolution to produce the spatial attention map, so\nthe spatial sub-module may suffer from a limited receptive\nfield."
        },
        {
            "bounding_box": [
                {
                    "x": 1294,
                    "y": 2351
                },
                {
                    "x": 1522,
                    "y": 2351
                },
                {
                    "x": 1522,
                    "y": 2397
                },
                {
                    "x": 1294,
                    "y": 2397
                }
            ],
            "category": "paragraph",
            "html": "<p id='298' style='font-size:16px'>3.6.3 BAM</p>",
            "id": 298,
            "page": 16,
            "text": "3.6.3 BAM"
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 2433
                },
                {
                    "x": 2357,
                    "y": 2433
                },
                {
                    "x": 2357,
                    "y": 2721
                },
                {
                    "x": 1291,
                    "y": 2721
                }
            ],
            "category": "paragraph",
            "html": "<p id='299' style='font-size:16px'>At the same time as CBAM, Park et al. [117] proposed<br>the bottleneck attention module (BAM), aiming to efficiently<br>improve the representational capability of networks. It uses<br>dilated convolution to enlarge the receptive field of the<br>spatial attention sub-module, and build a bottleneck structure<br>as suggested by ResNet to save computational cost.</p>",
            "id": 299,
            "page": 16,
            "text": "At the same time as CBAM, Park et al. [117] proposed\nthe bottleneck attention module (BAM), aiming to efficiently\nimprove the representational capability of networks. It uses\ndilated convolution to enlarge the receptive field of the\nspatial attention sub-module, and build a bottleneck structure\nas suggested by ResNet to save computational cost."
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 2729
                },
                {
                    "x": 2359,
                    "y": 2729
                },
                {
                    "x": 2359,
                    "y": 3116
                },
                {
                    "x": 1291,
                    "y": 3116
                }
            ],
            "category": "paragraph",
            "html": "<p id='300' style='font-size:16px'>For a given input feature map X, BAM infers the channel<br>in<br>attention Sc E RC and spatial attention Ss E RHxW<br>two parallel streams, then sums the two attention maps<br>after resizing both branch outputs to RCxHxW<br>· The channel<br>attention branch, like an SE block, applies global average<br>pooling to the feature map to aggregate global information,<br>and then uses an MLP with channel dimensionality reduction.<br>In order to utilize contextual information effectively, the</p>",
            "id": 300,
            "page": 16,
            "text": "For a given input feature map X, BAM infers the channel\nin\nattention Sc E RC and spatial attention Ss E RHxW\ntwo parallel streams, then sums the two attention maps\nafter resizing both branch outputs to RCxHxW\n· The channel\nattention branch, like an SE block, applies global average\npooling to the feature map to aggregate global information,\nand then uses an MLP with channel dimensionality reduction.\nIn order to utilize contextual information effectively, the"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 108
                },
                {
                    "x": 1078,
                    "y": 108
                },
                {
                    "x": 1078,
                    "y": 150
                },
                {
                    "x": 192,
                    "y": 150
                }
            ],
            "category": "header",
            "html": "<header id='301' style='font-size:14px'>JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015</header>",
            "id": 301,
            "page": 17,
            "text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015"
        },
        {
            "bounding_box": [
                {
                    "x": 2316,
                    "y": 115
                },
                {
                    "x": 2353,
                    "y": 115
                },
                {
                    "x": 2353,
                    "y": 143
                },
                {
                    "x": 2316,
                    "y": 143
                }
            ],
            "category": "header",
            "html": "<br><header id='302' style='font-size:14px'>17</header>",
            "id": 302,
            "page": 17,
            "text": "17"
        },
        {
            "bounding_box": [
                {
                    "x": 1206,
                    "y": 184
                },
                {
                    "x": 1344,
                    "y": 184
                },
                {
                    "x": 1344,
                    "y": 221
                },
                {
                    "x": 1206,
                    "y": 221
                }
            ],
            "category": "paragraph",
            "html": "<p id='303' style='font-size:14px'>TABLE 7</p>",
            "id": 303,
            "page": 17,
            "text": "TABLE 7"
        },
        {
            "bounding_box": [
                {
                    "x": 233,
                    "y": 218
                },
                {
                    "x": 2321,
                    "y": 218
                },
                {
                    "x": 2321,
                    "y": 415
                },
                {
                    "x": 233,
                    "y": 415
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='304' style='font-size:16px'>Representative channel & spatial attention mechanisms sorted by date. Cls = classification, ICap = image captioning, Det = detection, Seg =<br>segmentation, ISeg = instance segmentation, KP = keypoint detection, ReID = re-identification.g(z) and f(g(x), x) are the attention process<br>described by Eq. 1. Ranges means the ranges of attention map. S or H means soft or hard attention. (A) element-wise product. (B) aggregate<br>information via attention map.(l) focus the network on the discriminative region, (II) emphasize important channels, (III) capture long-range<br>information, (IV) capture cross-domain interaction between any two domains.</p>",
            "id": 304,
            "page": 17,
            "text": "Representative channel & spatial attention mechanisms sorted by date. Cls = classification, ICap = image captioning, Det = detection, Seg =\nsegmentation, ISeg = instance segmentation, KP = keypoint detection, ReID = re-identification.g(z) and f(g(x), x) are the attention process\ndescribed by Eq. 1. Ranges means the ranges of attention map. S or H means soft or hard attention. (A) element-wise product. (B) aggregate\ninformation via attention map.(l) focus the network on the discriminative region, (II) emphasize important channels, (III) capture long-range\ninformation, (IV) capture cross-domain interaction between any two domains."
        },
        {
            "bounding_box": [
                {
                    "x": 219,
                    "y": 451
                },
                {
                    "x": 2332,
                    "y": 451
                },
                {
                    "x": 2332,
                    "y": 2252
                },
                {
                    "x": 219,
                    "y": 2252
                }
            ],
            "category": "table",
            "html": "<table id='305' style='font-size:14px'><tr><td>Category</td><td>Method</td><td>Publication</td><td>Tasks</td><td>g(x)</td><td>f(g(x),x)</td><td>Ranges</td><td>S or H</td><td>Goals</td></tr><tr><td rowspan=\"3\">Jointly predict channel & spatial attention map</td><td>Residual Atten- tion [119]</td><td>CVPR2017</td><td>Cls</td><td>top-down network -> bottom down network - > 1x1 Convs -> Sigmoid</td><td>(A)</td><td>(0,1)</td><td>S</td><td>(I), (II)</td></tr><tr><td>SCNet [120]</td><td>CVPR2020</td><td>Cls, Det, ISeg, KP</td><td>top-down network -> bottom down network -> identity add -> sig- moid</td><td>(A)</td><td>(0,1)</td><td>S</td><td>(II), (III)</td></tr><tr><td>Strip Pooling [124]</td><td>CVPR2020</td><td>Seg</td><td>a)horizontal/vertical global pooling -> 1D Conv -> point-wise summation -> 1 x 1 Conv -> Sigmoid</td><td>(A)</td><td>(0,1)</td><td>S</td><td>(1), (II), (III)</td></tr><tr><td rowspan=\"7\">Separately pre- dict channel & spatial attention maps</td><td>SCA-CNN [50]</td><td>CVPR2017</td><td>ICap</td><td>a)spatial: fuse hidden state -> 1 x 1 Conv -> Softmax, b)channel: global average pooling -> MLP -> Softmax</td><td>(A)</td><td>(0,1)</td><td>S</td><td>(I), (II), (III)</td></tr><tr><td>CBAM [6]</td><td>ECCV2018</td><td>Cls, Det</td><td>a)spatial: global pooling in channel dimension- > Conv -> Sigmoid, b)channel: global pool- ing in spatial dimension -> MLP -> Sigmoid</td><td>(A)</td><td>(0,1)</td><td>S</td><td>(I), (II), (III)</td></tr><tr><td>BAM [6]</td><td>BMVC2018</td><td>Cls, Det</td><td>a)spatial: dilated Convs, b)channel: global aver- age pooling -> MLP, c)fuse two branches</td><td>(A)</td><td>(0,1)</td><td>S</td><td>(I), (II), (III)</td></tr><tr><td>scSE 123</td><td>TMI2018</td><td>Seg</td><td>a)spatial: 1 x 1 Conv -> Sigmoid, b)channel: global average pooling -> MLP -> Sigmoid, c)fuse two branches</td><td>(A)</td><td>(0,1)</td><td>S</td><td>(I), (II), (III)</td></tr><tr><td>Dual Attention [10]</td><td>CVPR2019</td><td>Seg</td><td>a)spatial: self-attention in spatial dimension, b)channel: self-attention in channel dimension, c) fuse two branches</td><td>(B)</td><td>(0,1)</td><td>S</td><td>(I), (II), (III)</td></tr><tr><td>RGA [101]</td><td>CVPR2020</td><td>ReID</td><td>use self-attention to cap- ture pairwise relations -> compute attention maps with the input and relation vectors</td><td>(A)</td><td>(0,1)</td><td>S</td><td>(I), (II), (III)</td></tr><tr><td>Triplet Attention [121]</td><td>WACV2021</td><td>Cls, Det</td><td>compute attention maps for pairs of domains -> fuse different branches</td><td>(A)</td><td>(0,1)</td><td>S</td><td>(I), (IV)</td></tr></table>",
            "id": 305,
            "page": 17,
            "text": "Category Method Publication Tasks g(x) f(g(x),x) Ranges S or H Goals\n Jointly predict channel & spatial attention map Residual Atten- tion [119] CVPR2017 Cls top-down network -> bottom down network - > 1x1 Convs -> Sigmoid (A) (0,1) S (I), (II)\n SCNet [120] CVPR2020 Cls, Det, ISeg, KP top-down network -> bottom down network -> identity add -> sig- moid (A) (0,1) S (II), (III)\n Strip Pooling [124] CVPR2020 Seg a)horizontal/vertical global pooling -> 1D Conv -> point-wise summation -> 1 x 1 Conv -> Sigmoid (A) (0,1) S (1), (II), (III)\n Separately pre- dict channel & spatial attention maps SCA-CNN [50] CVPR2017 ICap a)spatial: fuse hidden state -> 1 x 1 Conv -> Softmax, b)channel: global average pooling -> MLP -> Softmax (A) (0,1) S (I), (II), (III)\n CBAM [6] ECCV2018 Cls, Det a)spatial: global pooling in channel dimension- > Conv -> Sigmoid, b)channel: global pool- ing in spatial dimension -> MLP -> Sigmoid (A) (0,1) S (I), (II), (III)\n BAM [6] BMVC2018 Cls, Det a)spatial: dilated Convs, b)channel: global aver- age pooling -> MLP, c)fuse two branches (A) (0,1) S (I), (II), (III)\n scSE 123 TMI2018 Seg a)spatial: 1 x 1 Conv -> Sigmoid, b)channel: global average pooling -> MLP -> Sigmoid, c)fuse two branches (A) (0,1) S (I), (II), (III)\n Dual Attention [10] CVPR2019 Seg a)spatial: self-attention in spatial dimension, b)channel: self-attention in channel dimension, c) fuse two branches (B) (0,1) S (I), (II), (III)\n RGA [101] CVPR2020 ReID use self-attention to cap- ture pairwise relations -> compute attention maps with the input and relation vectors (A) (0,1) S (I), (II), (III)\n Triplet Attention [121] WACV2021 Cls, Det compute attention maps for pairs of domains -> fuse different branches (A) (0,1) S"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 2320
                },
                {
                    "x": 1257,
                    "y": 2320
                },
                {
                    "x": 1257,
                    "y": 2418
                },
                {
                    "x": 192,
                    "y": 2418
                }
            ],
            "category": "paragraph",
            "html": "<p id='306' style='font-size:16px'>spatial attention branch combines a bottleneck structure and<br>dilated convolutions. Overall, BAM can be written as</p>",
            "id": 306,
            "page": 17,
            "text": "spatial attention branch combines a bottleneck structure and\ndilated convolutions. Overall, BAM can be written as"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 2725
                },
                {
                    "x": 1257,
                    "y": 2725
                },
                {
                    "x": 1257,
                    "y": 3014
                },
                {
                    "x": 192,
                    "y": 3014
                }
            ],
            "category": "paragraph",
            "html": "<p id='307' style='font-size:20px'>where Wi, bi denote weights and biases of fully connected<br>layers respectively, Conv1x1 and Conv1x1 are convolution<br>layers used for channel reduction. DC3x3 denotes a dilated<br>convolution with 3 x 3 kernel, applied to utilize contextual<br>information effectively. Expand expands the attention maps<br>Ss and Sc to RCxHxW</p>",
            "id": 307,
            "page": 17,
            "text": "where Wi, bi denote weights and biases of fully connected\nlayers respectively, Conv1x1 and Conv1x1 are convolution\nlayers used for channel reduction. DC3x3 denotes a dilated\nconvolution with 3 x 3 kernel, applied to utilize contextual\ninformation effectively. Expand expands the attention maps\nSs and Sc to RCxHxW"
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 3016
                },
                {
                    "x": 1259,
                    "y": 3016
                },
                {
                    "x": 1259,
                    "y": 3116
                },
                {
                    "x": 193,
                    "y": 3116
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='308' style='font-size:18px'>BAM can emphasize or suppress features in both spatial<br>and channel dimensions, as well as improving the representa-</p>",
            "id": 308,
            "page": 17,
            "text": "BAM can emphasize or suppress features in both spatial\nand channel dimensions, as well as improving the representa-"
        },
        {
            "bounding_box": [
                {
                    "x": 1290,
                    "y": 2320
                },
                {
                    "x": 2362,
                    "y": 2320
                },
                {
                    "x": 2362,
                    "y": 2660
                },
                {
                    "x": 1290,
                    "y": 2660
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='309' style='font-size:18px'>tional power. Dimensional reduction applied to both channel<br>and spatial attention branches enables it to be integrated<br>with any convolutional neural network with little extra<br>computational cost. However, although dilated convolutions<br>enlarge the receptive field effectively, it still fails to capture<br>long-range contextual information as well as encoding cross-<br>domain relationships.</p>",
            "id": 309,
            "page": 17,
            "text": "tional power. Dimensional reduction applied to both channel\nand spatial attention branches enables it to be integrated\nwith any convolutional neural network with little extra\ncomputational cost. However, although dilated convolutions\nenlarge the receptive field effectively, it still fails to capture\nlong-range contextual information as well as encoding cross-\ndomain relationships."
        },
        {
            "bounding_box": [
                {
                    "x": 1294,
                    "y": 2712
                },
                {
                    "x": 1527,
                    "y": 2712
                },
                {
                    "x": 1527,
                    "y": 2760
                },
                {
                    "x": 1294,
                    "y": 2760
                }
            ],
            "category": "paragraph",
            "html": "<p id='310' style='font-size:18px'>3.6.4 scSE</p>",
            "id": 310,
            "page": 17,
            "text": "3.6.4 scSE"
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 2775
                },
                {
                    "x": 2360,
                    "y": 2775
                },
                {
                    "x": 2360,
                    "y": 3115
                },
                {
                    "x": 1291,
                    "y": 3115
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='311' style='font-size:20px'>To aggregate global spatial information, an SE block applies<br>global pooling to the feature map. However, it ignores<br>pixel-wise spatial information, which is important in dense<br>prediction tasks. Therefore, Roy et al. [123] proposed spatial<br>and channel SE blocks (scSE). Like BAM, spatial SE blocks are<br>used, complementing SE blocks, to provide spatial attention<br>weights to focus on important regions.</p>",
            "id": 311,
            "page": 17,
            "text": "To aggregate global spatial information, an SE block applies\nglobal pooling to the feature map. However, it ignores\npixel-wise spatial information, which is important in dense\nprediction tasks. Therefore, Roy et al. [123] proposed spatial\nand channel SE blocks (scSE). Like BAM, spatial SE blocks are\nused, complementing SE blocks, to provide spatial attention\nweights to focus on important regions."
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 110
                },
                {
                    "x": 1074,
                    "y": 110
                },
                {
                    "x": 1074,
                    "y": 149
                },
                {
                    "x": 193,
                    "y": 149
                }
            ],
            "category": "header",
            "html": "<header id='312' style='font-size:14px'>JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015</header>",
            "id": 312,
            "page": 18,
            "text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015"
        },
        {
            "bounding_box": [
                {
                    "x": 2315,
                    "y": 115
                },
                {
                    "x": 2352,
                    "y": 115
                },
                {
                    "x": 2352,
                    "y": 143
                },
                {
                    "x": 2315,
                    "y": 143
                }
            ],
            "category": "header",
            "html": "<br><header id='313' style='font-size:14px'>18</header>",
            "id": 313,
            "page": 18,
            "text": "18"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 181
                },
                {
                    "x": 1258,
                    "y": 181
                },
                {
                    "x": 1258,
                    "y": 516
                },
                {
                    "x": 192,
                    "y": 516
                }
            ],
            "category": "paragraph",
            "html": "<p id='314' style='font-size:18px'>Given the input feature map X, two parallel modules,<br>spatial SE and channel SE, are applied to feature maps to<br>encode spatial and channel information respectively. The<br>channel SE module is an ordinary SE block, while the spatial<br>SE module adopts 1 x 1 convolution for spatial squeezing.<br>The outputs from the two modules are fused. The overall<br>process can be written as</p>",
            "id": 314,
            "page": 18,
            "text": "Given the input feature map X, two parallel modules,\nspatial SE and channel SE, are applied to feature maps to\nencode spatial and channel information respectively. The\nchannel SE module is an ordinary SE block, while the spatial\nSE module adopts 1 x 1 convolution for spatial squeezing.\nThe outputs from the two modules are fused. The overall\nprocess can be written as"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 857
                },
                {
                    "x": 1257,
                    "y": 857
                },
                {
                    "x": 1257,
                    "y": 951
                },
                {
                    "x": 192,
                    "y": 951
                }
            ],
            "category": "paragraph",
            "html": "<p id='315' style='font-size:18px'>where f denotes the fusion function, which can be maximum,<br>addition, multiplication or concatenation.</p>",
            "id": 315,
            "page": 18,
            "text": "where f denotes the fusion function, which can be maximum,\naddition, multiplication or concatenation."
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 955
                },
                {
                    "x": 1258,
                    "y": 955
                },
                {
                    "x": 1258,
                    "y": 1242
                },
                {
                    "x": 193,
                    "y": 1242
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='316' style='font-size:16px'>The proposed scSE block combines channel and spatial<br>attention to enhance features as well as capturing pixel-wise<br>spatial information. Segmentation tasks are greatly benefited<br>as a result. The integration of an scSE block in F-CNNs<br>makes a consistent improvement in semantic segmentation<br>at negligible extra cost.</p>",
            "id": 316,
            "page": 18,
            "text": "The proposed scSE block combines channel and spatial\nattention to enhance features as well as capturing pixel-wise\nspatial information. Segmentation tasks are greatly benefited\nas a result. The integration of an scSE block in F-CNNs\nmakes a consistent improvement in semantic segmentation\nat negligible extra cost."
        },
        {
            "bounding_box": [
                {
                    "x": 194,
                    "y": 1280
                },
                {
                    "x": 607,
                    "y": 1280
                },
                {
                    "x": 607,
                    "y": 1326
                },
                {
                    "x": 194,
                    "y": 1326
                }
            ],
            "category": "paragraph",
            "html": "<p id='317' style='font-size:20px'>3.6.5 Triplet Attention</p>",
            "id": 317,
            "page": 18,
            "text": "3.6.5 Triplet Attention"
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 1338
                },
                {
                    "x": 1259,
                    "y": 1338
                },
                {
                    "x": 1259,
                    "y": 1623
                },
                {
                    "x": 193,
                    "y": 1623
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='318' style='font-size:18px'>In CBAM and BAM, channel attention and spatial attention<br>are computed independently, ignoring relationships between<br>these two domains [121]. Motivated by spatial attention,<br>Misra et al. [121] proposed triplet attention, a lightweight<br>but effective attention mechanism to capture cross-domain<br>interaction.</p>",
            "id": 318,
            "page": 18,
            "text": "In CBAM and BAM, channel attention and spatial attention\nare computed independently, ignoring relationships between\nthese two domains [121]. Motivated by spatial attention,\nMisra et al. [121] proposed triplet attention, a lightweight\nbut effective attention mechanism to capture cross-domain\ninteraction."
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 1627
                },
                {
                    "x": 1258,
                    "y": 1627
                },
                {
                    "x": 1258,
                    "y": 2058
                },
                {
                    "x": 193,
                    "y": 2058
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='319' style='font-size:16px'>Given an input feature map X, triplet attention uses<br>three branches, each of which plays a role in capturing<br>cross-domain interaction between any two domains from<br>H, W and C. In each branch, rotation operations along<br>different axes are applied to the input first, and then a Z-<br>pool layer is responsible for aggregating information in the<br>zeroth dimension. Finally, a standard convolution layer with<br>kernel size k x k models the relationship between the last<br>two domains. This process can be written as</p>",
            "id": 319,
            "page": 18,
            "text": "Given an input feature map X, triplet attention uses\nthree branches, each of which plays a role in capturing\ncross-domain interaction between any two domains from\nH, W and C. In each branch, rotation operations along\ndifferent axes are applied to the input first, and then a Z-\npool layer is responsible for aggregating information in the\nzeroth dimension. Finally, a standard convolution layer with\nkernel size k x k models the relationship between the last\ntwo domains. This process can be written as"
        },
        {
            "bounding_box": [
                {
                    "x": 1295,
                    "y": 180
                },
                {
                    "x": 1562,
                    "y": 180
                },
                {
                    "x": 1562,
                    "y": 224
                },
                {
                    "x": 1295,
                    "y": 224
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='320' style='font-size:18px'>3.6.6 SimAM</p>",
            "id": 320,
            "page": 18,
            "text": "3.6.6 SimAM"
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 261
                },
                {
                    "x": 2359,
                    "y": 261
                },
                {
                    "x": 2359,
                    "y": 645
                },
                {
                    "x": 1291,
                    "y": 645
                }
            ],
            "category": "paragraph",
            "html": "<p id='321' style='font-size:18px'>Yang et al. [118] also stress the importance of learning<br>attention weights that vary across both channel and spa-<br>tial domains in proposing SimAM, a simple, parameter-<br>free attention module capable of directly estimating 3D<br>weights instead of expanding 1D or 2D weights. The design<br>of SimAM is based on well-known neuroscience theory,<br>thus avoiding need for manual fine tuning of the network<br>structure.</p>",
            "id": 321,
            "page": 18,
            "text": "Yang et al. [118] also stress the importance of learning\nattention weights that vary across both channel and spa-\ntial domains in proposing SimAM, a simple, parameter-\nfree attention module capable of directly estimating 3D\nweights instead of expanding 1D or 2D weights. The design\nof SimAM is based on well-known neuroscience theory,\nthus avoiding need for manual fine tuning of the network\nstructure."
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 652
                },
                {
                    "x": 2359,
                    "y": 652
                },
                {
                    "x": 2359,
                    "y": 844
                },
                {
                    "x": 1291,
                    "y": 844
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='322' style='font-size:18px'>Motivated by the spatial suppression phenomenon [176],<br>they propose that a neuron which shows suppression effects<br>should be emphasized and define an energy function for<br>each neuron as:</p>",
            "id": 322,
            "page": 18,
            "text": "Motivated by the spatial suppression phenomenon [176],\nthey propose that a neuron which shows suppression effects\nshould be emphasized and define an energy function for\neach neuron as:"
        },
        {
            "bounding_box": [
                {
                    "x": 1290,
                    "y": 1074
                },
                {
                    "x": 2357,
                    "y": 1074
                },
                {
                    "x": 2357,
                    "y": 1213
                },
                {
                    "x": 1290,
                    "y": 1213
                }
            ],
            "category": "paragraph",
            "html": "<p id='323' style='font-size:16px'>where t = wtt+bt, xi = wtxi +bt, and t and Xi are the target<br>unit and all other units in the same channel; i E 1, · · · , N,<br>and N = H x W.</p>",
            "id": 323,
            "page": 18,
            "text": "where t = wtt+bt, xi = wtxi +bt, and t and Xi are the target\nunit and all other units in the same channel; i E 1, · · · , N,\nand N = H x W."
        },
        {
            "bounding_box": [
                {
                    "x": 1353,
                    "y": 1224
                },
                {
                    "x": 2258,
                    "y": 1224
                },
                {
                    "x": 2258,
                    "y": 1272
                },
                {
                    "x": 1353,
                    "y": 1272
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='324' style='font-size:18px'>An optimal closed-form solution for Eq. 106 exists:</p>",
            "id": 324,
            "page": 18,
            "text": "An optimal closed-form solution for Eq. 106 exists:"
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 1485
                },
                {
                    "x": 2357,
                    "y": 1485
                },
                {
                    "x": 2357,
                    "y": 1677
                },
                {
                    "x": 1291,
                    "y": 1677
                }
            ],
            "category": "paragraph",
            "html": "<p id='325' style='font-size:16px'>where � is the mean of the input feature and 82 is its variance.<br>A sigmoid function is used to control the output range of<br>the attention vector; an element-product is applied to get the<br>final output:</p>",
            "id": 325,
            "page": 18,
            "text": "where � is the mean of the input feature and 82 is its variance.\nA sigmoid function is used to control the output range of\nthe attention vector; an element-product is applied to get the\nfinal output:"
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 1930
                },
                {
                    "x": 2357,
                    "y": 1930
                },
                {
                    "x": 2357,
                    "y": 2121
                },
                {
                    "x": 1291,
                    "y": 2121
                }
            ],
            "category": "paragraph",
            "html": "<p id='326' style='font-size:16px'>This work simplifies the process of designing attention<br>and successfully proposes a novel 3-D weight parameter-free<br>attention module based on mathematics and neuroscience<br>theories.</p>",
            "id": 326,
            "page": 18,
            "text": "This work simplifies the process of designing attention\nand successfully proposes a novel 3-D weight parameter-free\nattention module based on mathematics and neuroscience\ntheories."
        },
        {
            "bounding_box": [
                {
                    "x": 191,
                    "y": 2487
                },
                {
                    "x": 1256,
                    "y": 2487
                },
                {
                    "x": 1256,
                    "y": 2681
                },
                {
                    "x": 191,
                    "y": 2681
                }
            ],
            "category": "paragraph",
            "html": "<p id='327' style='font-size:18px'>where Pm1 and Pm2 denote rotation through 90° anti-<br>clockwise about the H and W axes respectively, while Pm2-1<br>denotes the inverse. Z-Pool concatenates max-pooling and<br>average pooling along the zeroth dimension.</p>",
            "id": 327,
            "page": 18,
            "text": "where Pm1 and Pm2 denote rotation through 90° anti-\nclockwise about the H and W axes respectively, while Pm2-1\ndenotes the inverse. Z-Pool concatenates max-pooling and\naverage pooling along the zeroth dimension."
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 2776
                },
                {
                    "x": 1259,
                    "y": 2776
                },
                {
                    "x": 1259,
                    "y": 3112
                },
                {
                    "x": 192,
                    "y": 3112
                }
            ],
            "category": "paragraph",
            "html": "<p id='328' style='font-size:18px'>Unlike CBAM and BAM, triplet attention stresses the<br>importance of capturing cross-domain interactions instead<br>of computing spatial attention and channel attention inde-<br>pendently. This helps to capture rich discriminative feature<br>representations. Due to its simple but efficient structure,<br>triplet attention can be easily added to classical backbone<br>networks.</p>",
            "id": 328,
            "page": 18,
            "text": "Unlike CBAM and BAM, triplet attention stresses the\nimportance of capturing cross-domain interactions instead\nof computing spatial attention and channel attention inde-\npendently. This helps to capture rich discriminative feature\nrepresentations. Due to its simple but efficient structure,\ntriplet attention can be easily added to classical backbone\nnetworks."
        },
        {
            "bounding_box": [
                {
                    "x": 1294,
                    "y": 2209
                },
                {
                    "x": 1788,
                    "y": 2209
                },
                {
                    "x": 1788,
                    "y": 2254
                },
                {
                    "x": 1294,
                    "y": 2254
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='329' style='font-size:16px'>3.6.7 Coordinate attention</p>",
            "id": 329,
            "page": 18,
            "text": "3.6.7 Coordinate attention"
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 2288
                },
                {
                    "x": 2357,
                    "y": 2288
                },
                {
                    "x": 2357,
                    "y": 2722
                },
                {
                    "x": 1291,
                    "y": 2722
                }
            ],
            "category": "paragraph",
            "html": "<p id='330' style='font-size:18px'>An SE block aggregates global spatial information using<br>global pooling before modeling cross-channel relationships,<br>but neglects the importance of positional information. BAM<br>and CBAM adopt convolutions to capture local relations,<br>but fail to model long-range dependencies. To solve these<br>problems, Hou et al. [129] proposed coordinate attention,<br>a novel attention mechanism which embeds positional<br>information into channel attention, SO that the network can<br>focus on large important regions at little computational cost.</p>",
            "id": 330,
            "page": 18,
            "text": "An SE block aggregates global spatial information using\nglobal pooling before modeling cross-channel relationships,\nbut neglects the importance of positional information. BAM\nand CBAM adopt convolutions to capture local relations,\nbut fail to model long-range dependencies. To solve these\nproblems, Hou et al. [129] proposed coordinate attention,\na novel attention mechanism which embeds positional\ninformation into channel attention, SO that the network can\nfocus on large important regions at little computational cost."
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 2731
                },
                {
                    "x": 2357,
                    "y": 2731
                },
                {
                    "x": 2357,
                    "y": 3115
                },
                {
                    "x": 1291,
                    "y": 3115
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='331' style='font-size:18px'>The coordinate attention mechanism has two consecutive<br>steps, coordinate information embedding and coordinate<br>attention generation. First, two spatial extents of pooling<br>kernels encode each channel horizontally and vertically. In<br>the second step, a shared 1 x 1 convolutional transformation<br>function is applied to the concatenated outputs of the two<br>pooling layers. Then coordinate attention splits the resulting<br>tensor into two separate tensors to yield attention vectors</p>",
            "id": 331,
            "page": 18,
            "text": "The coordinate attention mechanism has two consecutive\nsteps, coordinate information embedding and coordinate\nattention generation. First, two spatial extents of pooling\nkernels encode each channel horizontally and vertically. In\nthe second step, a shared 1 x 1 convolutional transformation\nfunction is applied to the concatenated outputs of the two\npooling layers. Then coordinate attention splits the resulting\ntensor into two separate tensors to yield attention vectors"
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 109
                },
                {
                    "x": 1075,
                    "y": 109
                },
                {
                    "x": 1075,
                    "y": 150
                },
                {
                    "x": 193,
                    "y": 150
                }
            ],
            "category": "header",
            "html": "<header id='332' style='font-size:14px'>JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015</header>",
            "id": 332,
            "page": 19,
            "text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015"
        },
        {
            "bounding_box": [
                {
                    "x": 2314,
                    "y": 113
                },
                {
                    "x": 2353,
                    "y": 113
                },
                {
                    "x": 2353,
                    "y": 145
                },
                {
                    "x": 2314,
                    "y": 145
                }
            ],
            "category": "header",
            "html": "<br><header id='333' style='font-size:14px'>19</header>",
            "id": 333,
            "page": 19,
            "text": "19"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 181
                },
                {
                    "x": 1256,
                    "y": 181
                },
                {
                    "x": 1256,
                    "y": 275
                },
                {
                    "x": 192,
                    "y": 275
                }
            ],
            "category": "paragraph",
            "html": "<p id='334' style='font-size:16px'>with the same number of channels for horizontal and vertical<br>coordinates of the input X along. This can be written as</p>",
            "id": 334,
            "page": 19,
            "text": "with the same number of channels for horizontal and vertical\ncoordinates of the input X along. This can be written as"
        },
        {
            "bounding_box": [
                {
                    "x": 191,
                    "y": 768
                },
                {
                    "x": 1254,
                    "y": 768
                },
                {
                    "x": 1254,
                    "y": 912
                },
                {
                    "x": 191,
                    "y": 912
                }
            ],
            "category": "paragraph",
            "html": "<p id='335' style='font-size:16px'>where GAPh and GAPw denote pooling functions for vertical<br>and horizontal coordinates, and sh E RCx1xW<br>and sw E<br>RCxHx1 attention weights.<br>represent corresponding</p>",
            "id": 335,
            "page": 19,
            "text": "where GAPh and GAPw denote pooling functions for vertical\nand horizontal coordinates, and sh E RCx1xW\nand sw E\nRCxHx1 attention weights.\nrepresent corresponding"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 917
                },
                {
                    "x": 1258,
                    "y": 917
                },
                {
                    "x": 1258,
                    "y": 1253
                },
                {
                    "x": 192,
                    "y": 1253
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='336' style='font-size:16px'>Using coordinate attention, the network can accurately<br>obtain the position of a targeted object. This approach has<br>a larger receptive field than BAM and CBAM. Like an SE<br>block, it also models cross-channel relationships, effectively<br>enhancing the expressive power of the learned features. Due<br>to its lightweight design and flexibility, it can be easily used<br>in classical building blocks of mobile networks.</p>",
            "id": 336,
            "page": 19,
            "text": "Using coordinate attention, the network can accurately\nobtain the position of a targeted object. This approach has\na larger receptive field than BAM and CBAM. Like an SE\nblock, it also models cross-channel relationships, effectively\nenhancing the expressive power of the learned features. Due\nto its lightweight design and flexibility, it can be easily used\nin classical building blocks of mobile networks."
        },
        {
            "bounding_box": [
                {
                    "x": 195,
                    "y": 1303
                },
                {
                    "x": 450,
                    "y": 1303
                },
                {
                    "x": 450,
                    "y": 1349
                },
                {
                    "x": 195,
                    "y": 1349
                }
            ],
            "category": "paragraph",
            "html": "<p id='337' style='font-size:16px'>3.6.8 DANet</p>",
            "id": 337,
            "page": 19,
            "text": "3.6.8 DANet"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 1363
                },
                {
                    "x": 1258,
                    "y": 1363
                },
                {
                    "x": 1258,
                    "y": 1843
                },
                {
                    "x": 192,
                    "y": 1843
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='338' style='font-size:16px'>In the field of scene segmentation, encoder-decoder struc-<br>tures cannot make use of the global relationships between<br>objects, whereas RNN-based structures heavily rely on the<br>output of the long-term memorization. To address the above<br>problems, Fu et al. [10] proposed a novel framework, the<br>dual attention network (DANet), for natural scene image<br>segmentation. Unlike CBAM and BAM, it adopts a self-<br>attention mechanism instead of simply stacking convolutions<br>to compute the spatial attention map, which enables the<br>network to capture global information directly.</p>",
            "id": 338,
            "page": 19,
            "text": "In the field of scene segmentation, encoder-decoder struc-\ntures cannot make use of the global relationships between\nobjects, whereas RNN-based structures heavily rely on the\noutput of the long-term memorization. To address the above\nproblems, Fu et al. [10] proposed a novel framework, the\ndual attention network (DANet), for natural scene image\nsegmentation. Unlike CBAM and BAM, it adopts a self-\nattention mechanism instead of simply stacking convolutions\nto compute the spatial attention map, which enables the\nnetwork to capture global information directly."
        },
        {
            "bounding_box": [
                {
                    "x": 191,
                    "y": 1849
                },
                {
                    "x": 1258,
                    "y": 1849
                },
                {
                    "x": 1258,
                    "y": 2571
                },
                {
                    "x": 191,
                    "y": 2571
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='339' style='font-size:16px'>DANet uses in parallel a position attention module and a<br>channel attention module to capture feature dependencies<br>in spatial and channel domains. Given the input feature<br>map X, convolution layers are applied first in the position<br>attention module to obtain new feature maps. Then the<br>position attention module selectively aggregates the features<br>at each position using a weighted sum of features at all<br>positions, where the weights are determined by feature<br>similarity between corresponding pairs of positions. The<br>channel attention module has a similar form except for<br>dimensional reduction to model cross-channel relations.<br>Finally the outputs from the two branches are fused to obtain<br>final feature representations. For simplicity, we reshape the<br>feature map X to C x (H x W) whereupon the overall<br>process can be written as</p>",
            "id": 339,
            "page": 19,
            "text": "DANet uses in parallel a position attention module and a\nchannel attention module to capture feature dependencies\nin spatial and channel domains. Given the input feature\nmap X, convolution layers are applied first in the position\nattention module to obtain new feature maps. Then the\nposition attention module selectively aggregates the features\nat each position using a weighted sum of features at all\npositions, where the weights are determined by feature\nsimilarity between corresponding pairs of positions. The\nchannel attention module has a similar form except for\ndimensional reduction to model cross-channel relations.\nFinally the outputs from the two branches are fused to obtain\nfinal feature representations. For simplicity, we reshape the\nfeature map X to C x (H x W) whereupon the overall\nprocess can be written as"
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 179
                },
                {
                    "x": 2359,
                    "y": 179
                },
                {
                    "x": 2359,
                    "y": 471
                },
                {
                    "x": 1292,
                    "y": 471
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='340' style='font-size:18px'>the channel attention module is responsible for enhancing<br>useful channels as well as suppressing noise. Taking spa-<br>tial and channel relationships into consideration explicitly<br>improves the feature representation for scene segmentation.<br>However, it is computationally costly, especially for large<br>input feature maps.</p>",
            "id": 340,
            "page": 19,
            "text": "the channel attention module is responsible for enhancing\nuseful channels as well as suppressing noise. Taking spa-\ntial and channel relationships into consideration explicitly\nimproves the feature representation for scene segmentation.\nHowever, it is computationally costly, especially for large\ninput feature maps."
        },
        {
            "bounding_box": [
                {
                    "x": 1294,
                    "y": 509
                },
                {
                    "x": 1520,
                    "y": 509
                },
                {
                    "x": 1520,
                    "y": 554
                },
                {
                    "x": 1294,
                    "y": 554
                }
            ],
            "category": "paragraph",
            "html": "<p id='341' style='font-size:16px'>3.6.9 RGA</p>",
            "id": 341,
            "page": 19,
            "text": "3.6.9 RGA"
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 568
                },
                {
                    "x": 2355,
                    "y": 568
                },
                {
                    "x": 2355,
                    "y": 805
                },
                {
                    "x": 1292,
                    "y": 805
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='342' style='font-size:18px'>Unlike coordinate attention and DANet, which emphasise<br>capturing long-range context, in relation-aware global attention<br>(RGA) [101], Zhang et al. stress the importance of global<br>structural information provided by pairwise relations, and<br>uses it to produce attention maps.</p>",
            "id": 342,
            "page": 19,
            "text": "Unlike coordinate attention and DANet, which emphasise\ncapturing long-range context, in relation-aware global attention\n(RGA) [101], Zhang et al. stress the importance of global\nstructural information provided by pairwise relations, and\nuses it to produce attention maps."
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 809
                },
                {
                    "x": 2355,
                    "y": 809
                },
                {
                    "x": 2355,
                    "y": 996
                },
                {
                    "x": 1291,
                    "y": 996
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='343' style='font-size:16px'>RGA comes in two forms, spatial RGA (RGA-S) and<br>channel RGA (RGA-C). RGA-S first reshapes the input feature<br>map X to C x (H x W) and the pairwise relation matrix<br>R E R(HxW)x(HxW) is computed using</p>",
            "id": 343,
            "page": 19,
            "text": "RGA comes in two forms, spatial RGA (RGA-S) and\nchannel RGA (RGA-C). RGA-S first reshapes the input feature\nmap X to C x (H x W) and the pairwise relation matrix\nR E R(HxW)x(HxW) is computed using"
        },
        {
            "bounding_box": [
                {
                    "x": 1293,
                    "y": 1227
                },
                {
                    "x": 2355,
                    "y": 1227
                },
                {
                    "x": 2355,
                    "y": 1321
                },
                {
                    "x": 1293,
                    "y": 1321
                }
            ],
            "category": "paragraph",
            "html": "<p id='344' style='font-size:16px'>The relation vector ri at position i is defined by stacking<br>pairwise relations at all positions:</p>",
            "id": 344,
            "page": 19,
            "text": "The relation vector ri at position i is defined by stacking\npairwise relations at all positions:"
        },
        {
            "bounding_box": [
                {
                    "x": 1293,
                    "y": 1425
                },
                {
                    "x": 2320,
                    "y": 1425
                },
                {
                    "x": 2320,
                    "y": 1471
                },
                {
                    "x": 1293,
                    "y": 1471
                }
            ],
            "category": "paragraph",
            "html": "<p id='345' style='font-size:14px'>and the spatial relation-aware feature Yi can be written as</p>",
            "id": 345,
            "page": 19,
            "text": "and the spatial relation-aware feature Yi can be written as"
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 1573
                },
                {
                    "x": 2356,
                    "y": 1573
                },
                {
                    "x": 2356,
                    "y": 1716
                },
                {
                    "x": 1292,
                    "y": 1716
                }
            ],
            "category": "paragraph",
            "html": "<p id='346' style='font-size:18px'>where gavg denotes global average pooling in the channel<br>domain. Finally, the spatial attention score at position i is<br>given by</p>",
            "id": 346,
            "page": 19,
            "text": "where gavg denotes global average pooling in the channel\ndomain. Finally, the spatial attention score at position i is\ngiven by"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 2868
                },
                {
                    "x": 1256,
                    "y": 2868
                },
                {
                    "x": 1256,
                    "y": 2967
                },
                {
                    "x": 192,
                    "y": 2967
                }
            ],
            "category": "paragraph",
            "html": "<p id='347' style='font-size:16px'>where Wq, Wk, Wv E RCxC are used to generate new feature<br>maps.</p>",
            "id": 347,
            "page": 19,
            "text": "where Wq, Wk, Wv E RCxC are used to generate new feature\nmaps."
        },
        {
            "bounding_box": [
                {
                    "x": 1294,
                    "y": 1816
                },
                {
                    "x": 2354,
                    "y": 1816
                },
                {
                    "x": 2354,
                    "y": 1912
                },
                {
                    "x": 1294,
                    "y": 1912
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='348' style='font-size:16px'>RGA-C has the same form as RGA-S, except for taking the<br>input feature map as a set of H x W-dimensional features.</p>",
            "id": 348,
            "page": 19,
            "text": "RGA-C has the same form as RGA-S, except for taking the\ninput feature map as a set of H x W-dimensional features."
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 1914
                },
                {
                    "x": 2357,
                    "y": 1914
                },
                {
                    "x": 2357,
                    "y": 2248
                },
                {
                    "x": 1291,
                    "y": 2248
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='349' style='font-size:16px'>RGA uses global relations to generate the attention<br>score for each feature node, so provides valuable structural<br>information and significantly enhances the representational<br>power. RGA-S and RGA-C are flexible enough to be used in<br>any CNN network; Zhang et al. propose using them jointly<br>in sequence to better capture both spatial and cross-channel<br>relationships.</p>",
            "id": 349,
            "page": 19,
            "text": "RGA uses global relations to generate the attention\nscore for each feature node, so provides valuable structural\ninformation and significantly enhances the representational\npower. RGA-S and RGA-C are flexible enough to be used in\nany CNN network; Zhang et al. propose using them jointly\nin sequence to better capture both spatial and cross-channel\nrelationships."
        },
        {
            "bounding_box": [
                {
                    "x": 1294,
                    "y": 2290
                },
                {
                    "x": 1955,
                    "y": 2290
                },
                {
                    "x": 1955,
                    "y": 2337
                },
                {
                    "x": 1294,
                    "y": 2337
                }
            ],
            "category": "paragraph",
            "html": "<p id='350' style='font-size:16px'>3.6.10 Self-Calibrated Convolutions</p>",
            "id": 350,
            "page": 19,
            "text": "3.6.10 Self-Calibrated Convolutions"
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 2347
                },
                {
                    "x": 2356,
                    "y": 2347
                },
                {
                    "x": 2356,
                    "y": 2490
                },
                {
                    "x": 1292,
                    "y": 2490
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='351' style='font-size:16px'>Motivated by the success of group convolution, Liu et at [120]<br>presented self-calibrated convolution as a means to enlarge the<br>receptive field at each spatial location.</p>",
            "id": 351,
            "page": 19,
            "text": "Motivated by the success of group convolution, Liu et at [120]\npresented self-calibrated convolution as a means to enlarge the\nreceptive field at each spatial location."
        },
        {
            "bounding_box": [
                {
                    "x": 1293,
                    "y": 2493
                },
                {
                    "x": 2356,
                    "y": 2493
                },
                {
                    "x": 2356,
                    "y": 2732
                },
                {
                    "x": 1293,
                    "y": 2732
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='352' style='font-size:16px'>Self-calibrated convolution is used together with a stan-<br>dard convolution. It first divides the input feature X into<br>X1 and X2 in the channel domain. The self-calibrated<br>convolution first uses average pooling to reduce the input<br>size and enlarge the receptive field:</p>",
            "id": 352,
            "page": 19,
            "text": "Self-calibrated convolution is used together with a stan-\ndard convolution. It first divides the input feature X into\nX1 and X2 in the channel domain. The self-calibrated\nconvolution first uses average pooling to reduce the input\nsize and enlarge the receptive field:"
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 2831
                },
                {
                    "x": 2357,
                    "y": 2831
                },
                {
                    "x": 2357,
                    "y": 3024
                },
                {
                    "x": 1292,
                    "y": 3024
                }
            ],
            "category": "paragraph",
            "html": "<p id='353' style='font-size:16px'>where r is the filter size and stride. Then a convolution<br>is used to model the channel relationship and a bilinear<br>interpolation operator Up is used to upsample the feature<br>map:</p>",
            "id": 353,
            "page": 19,
            "text": "where r is the filter size and stride. Then a convolution\nis used to model the channel relationship and a bilinear\ninterpolation operator Up is used to upsample the feature\nmap:"
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 2970
                },
                {
                    "x": 1258,
                    "y": 2970
                },
                {
                    "x": 1258,
                    "y": 3117
                },
                {
                    "x": 193,
                    "y": 3117
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='354' style='font-size:16px'>The position attention module enables DANet to capture<br>long-range contextual information and adaptively integrate<br>similar features at any scale from a global viewpoint, while</p>",
            "id": 354,
            "page": 19,
            "text": "The position attention module enables DANet to capture\nlong-range contextual information and adaptively integrate\nsimilar features at any scale from a global viewpoint, while"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 110
                },
                {
                    "x": 1075,
                    "y": 110
                },
                {
                    "x": 1075,
                    "y": 150
                },
                {
                    "x": 192,
                    "y": 150
                }
            ],
            "category": "header",
            "html": "<header id='355' style='font-size:14px'>JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015</header>",
            "id": 355,
            "page": 20,
            "text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015"
        },
        {
            "bounding_box": [
                {
                    "x": 2312,
                    "y": 114
                },
                {
                    "x": 2352,
                    "y": 114
                },
                {
                    "x": 2352,
                    "y": 145
                },
                {
                    "x": 2312,
                    "y": 145
                }
            ],
            "category": "header",
            "html": "<br><header id='356' style='font-size:14px'>20</header>",
            "id": 356,
            "page": 20,
            "text": "20"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 181
                },
                {
                    "x": 1255,
                    "y": 181
                },
                {
                    "x": 1255,
                    "y": 274
                },
                {
                    "x": 192,
                    "y": 274
                }
            ],
            "category": "paragraph",
            "html": "<p id='357' style='font-size:18px'>Next, element-wise multiplication finishes the self-calibrated<br>process:</p>",
            "id": 357,
            "page": 20,
            "text": "Next, element-wise multiplication finishes the self-calibrated\nprocess:"
        },
        {
            "bounding_box": [
                {
                    "x": 194,
                    "y": 376
                },
                {
                    "x": 987,
                    "y": 376
                },
                {
                    "x": 987,
                    "y": 422
                },
                {
                    "x": 194,
                    "y": 422
                }
            ],
            "category": "paragraph",
            "html": "<p id='358' style='font-size:18px'>Finally, the output feature map<br>of is formed:</p>",
            "id": 358,
            "page": 20,
            "text": "Finally, the output feature map\nof is formed:"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 639
                },
                {
                    "x": 1258,
                    "y": 639
                },
                {
                    "x": 1258,
                    "y": 882
                },
                {
                    "x": 192,
                    "y": 882
                }
            ],
            "category": "paragraph",
            "html": "<p id='359' style='font-size:18px'>Such self-calibrated convolution can enlarge the receptive<br>field of a network and improve its adaptability. It achieves<br>excellent results in image classification and certain down-<br>stream tasks such as instance segmentation, object detection<br>and keypoint detection.</p>",
            "id": 359,
            "page": 20,
            "text": "Such self-calibrated convolution can enlarge the receptive\nfield of a network and improve its adaptability. It achieves\nexcellent results in image classification and certain down-\nstream tasks such as instance segmentation, object detection\nand keypoint detection."
        },
        {
            "bounding_box": [
                {
                    "x": 195,
                    "y": 918
                },
                {
                    "x": 470,
                    "y": 918
                },
                {
                    "x": 470,
                    "y": 964
                },
                {
                    "x": 195,
                    "y": 964
                }
            ],
            "category": "paragraph",
            "html": "<p id='360' style='font-size:18px'>3.6.11 SPNet</p>",
            "id": 360,
            "page": 20,
            "text": "3.6.11 SPNet"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 976
                },
                {
                    "x": 1257,
                    "y": 976
                },
                {
                    "x": 1257,
                    "y": 1263
                },
                {
                    "x": 192,
                    "y": 1263
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='361' style='font-size:18px'>Spatial pooling usually operates on a small region which<br>limits its capability to capture long-range dependencies and<br>focus on distant regions. To overcome this, Hou et al. [124]<br>proposed strip pooling, a novel pooling method capable of<br>encoding long-range context in either horizontal or vertical<br>spatial domains.</p>",
            "id": 361,
            "page": 20,
            "text": "Spatial pooling usually operates on a small region which\nlimits its capability to capture long-range dependencies and\nfocus on distant regions. To overcome this, Hou et al. [124]\nproposed strip pooling, a novel pooling method capable of\nencoding long-range context in either horizontal or vertical\nspatial domains."
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 1265
                },
                {
                    "x": 1257,
                    "y": 1265
                },
                {
                    "x": 1257,
                    "y": 1408
                },
                {
                    "x": 192,
                    "y": 1408
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='362' style='font-size:18px'>Strip pooling has two branches for horizontal and vertical<br>strip pooling. The horizontal strip pooling part first pools<br>the input feature F E RCxHxW in the horizontal direction:</p>",
            "id": 362,
            "page": 20,
            "text": "Strip pooling has two branches for horizontal and vertical\nstrip pooling. The horizontal strip pooling part first pools\nthe input feature F E RCxHxW in the horizontal direction:"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 1506
                },
                {
                    "x": 1256,
                    "y": 1506
                },
                {
                    "x": 1256,
                    "y": 1698
                },
                {
                    "x": 192,
                    "y": 1698
                }
            ],
            "category": "paragraph",
            "html": "<p id='363' style='font-size:16px'>Then a 1D convolution with kernel size 3 is applied in y to<br>capture the relationship between different rows and channels.<br>This is repeated W times to make the output Yv consistent<br>with the input shape:</p>",
            "id": 363,
            "page": 20,
            "text": "Then a 1D convolution with kernel size 3 is applied in y to\ncapture the relationship between different rows and channels.\nThis is repeated W times to make the output Yv consistent\nwith the input shape:"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 1796
                },
                {
                    "x": 1257,
                    "y": 1796
                },
                {
                    "x": 1257,
                    "y": 1939
                },
                {
                    "x": 192,
                    "y": 1939
                }
            ],
            "category": "paragraph",
            "html": "<p id='364' style='font-size:18px'>Vertical strip pooling is performed in a similar way. Finally,<br>the outputs of the two branches are fused using element-wise<br>summation to produce the attention map:</p>",
            "id": 364,
            "page": 20,
            "text": "Vertical strip pooling is performed in a similar way. Finally,\nthe outputs of the two branches are fused using element-wise\nsummation to produce the attention map:"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 2095
                },
                {
                    "x": 1256,
                    "y": 2095
                },
                {
                    "x": 1256,
                    "y": 2340
                },
                {
                    "x": 192,
                    "y": 2340
                }
            ],
            "category": "paragraph",
            "html": "<p id='365' style='font-size:18px'>The strip pooling module (SPM) is further developed in<br>the mixed pooling module (MPM). Both consider spatial and<br>channel relationships to overcome the locality of convolu-<br>tional neural networks. SPNet achieves state-of-the-art results<br>for several complex semantic segmentation benchmarks.</p>",
            "id": 365,
            "page": 20,
            "text": "The strip pooling module (SPM) is further developed in\nthe mixed pooling module (MPM). Both consider spatial and\nchannel relationships to overcome the locality of convolu-\ntional neural networks. SPNet achieves state-of-the-art results\nfor several complex semantic segmentation benchmarks."
        },
        {
            "bounding_box": [
                {
                    "x": 195,
                    "y": 2376
                },
                {
                    "x": 538,
                    "y": 2376
                },
                {
                    "x": 538,
                    "y": 2422
                },
                {
                    "x": 195,
                    "y": 2422
                }
            ],
            "category": "paragraph",
            "html": "<p id='366' style='font-size:18px'>3.6.12 SCA-CNN</p>",
            "id": 366,
            "page": 20,
            "text": "3.6.12 SCA-CNN"
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 181
                },
                {
                    "x": 2357,
                    "y": 181
                },
                {
                    "x": 2357,
                    "y": 372
                },
                {
                    "x": 1292,
                    "y": 372
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='367' style='font-size:16px'>where ④ represents addition of a matrix and a vector. Simi-<br>larly, channel-wise attention aggregates global information<br>first, and then computes a channel-wise attention weight<br>vector with the hidden state ht-1:</p>",
            "id": 367,
            "page": 20,
            "text": "where ④ represents addition of a matrix and a vector. Simi-\nlarly, channel-wise attention aggregates global information\nfirst, and then computes a channel-wise attention weight\nvector with the hidden state ht-1:"
        },
        {
            "bounding_box": [
                {
                    "x": 191,
                    "y": 2434
                },
                {
                    "x": 1256,
                    "y": 2434
                },
                {
                    "x": 1256,
                    "y": 2963
                },
                {
                    "x": 191,
                    "y": 2963
                }
            ],
            "category": "paragraph",
            "html": "<p id='368' style='font-size:16px'>As CNN features are naturally spatial, channel-wise and<br>multi-layer, Chen et al. [50] proposed a novel spatial and<br>channel-wise attention-based convolutional neural network (SCA-<br>CNN). It was designed for the task of image captioning,<br>and uses an encoder-decoder framework where a CNN first<br>encodes an input image into a vector and then an LSTM<br>decodes the vector into a sequence of words. Given an input<br>feature map X and the previous time step LSTM hidden<br>state ht-1 E Rd, a spatial attention mechanism pays more<br>attention to the semantically useful regions, guided by LSTM<br>hidden state ht-1. The spatial attention model is:</p>",
            "id": 368,
            "page": 20,
            "text": "As CNN features are naturally spatial, channel-wise and\nmulti-layer, Chen et al. [50] proposed a novel spatial and\nchannel-wise attention-based convolutional neural network (SCA-\nCNN). It was designed for the task of image captioning,\nand uses an encoder-decoder framework where a CNN first\nencodes an input image into a vector and then an LSTM\ndecodes the vector into a sequence of words. Given an input\nfeature map X and the previous time step LSTM hidden\nstate ht-1 E Rd, a spatial attention mechanism pays more\nattention to the semantically useful regions, guided by LSTM\nhidden state ht-1. The spatial attention model is:"
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 550
                },
                {
                    "x": 2355,
                    "y": 550
                },
                {
                    "x": 2355,
                    "y": 694
                },
                {
                    "x": 1291,
                    "y": 694
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='369' style='font-size:16px'>Overall, the SCA mechanism can be written in one of two<br>ways. If channel-wise attention is applied before spatial<br>attention, we have</p>",
            "id": 369,
            "page": 20,
            "text": "Overall, the SCA mechanism can be written in one of two\nways. If channel-wise attention is applied before spatial\nattention, we have"
        },
        {
            "bounding_box": [
                {
                    "x": 1294,
                    "y": 814
                },
                {
                    "x": 1916,
                    "y": 814
                },
                {
                    "x": 1916,
                    "y": 858
                },
                {
                    "x": 1294,
                    "y": 858
                }
            ],
            "category": "paragraph",
            "html": "<p id='370' style='font-size:16px'>and if spatial attention comes first:</p>",
            "id": 370,
            "page": 20,
            "text": "and if spatial attention comes first:"
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 978
                },
                {
                    "x": 2355,
                    "y": 978
                },
                {
                    "x": 2355,
                    "y": 1121
                },
                {
                    "x": 1292,
                    "y": 1121
                }
            ],
            "category": "paragraph",
            "html": "<p id='371' style='font-size:16px'>where f(·) denotes the modulate function which takes the<br>feature map X and attention maps as input and then outputs<br>the modulated feature map Y.</p>",
            "id": 371,
            "page": 20,
            "text": "where f(·) denotes the modulate function which takes the\nfeature map X and attention maps as input and then outputs\nthe modulated feature map Y."
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 1125
                },
                {
                    "x": 2357,
                    "y": 1125
                },
                {
                    "x": 2357,
                    "y": 1510
                },
                {
                    "x": 1291,
                    "y": 1510
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='372' style='font-size:18px'>Unlike previous attention mechanisms which consider<br>each image region equally and use global spatial information<br>to tell the network where to focus, SCA-Net leverages the<br>semantic vector to produce the spatial attention map as well<br>as the channel-wise attention weight vector. Being more than<br>a powerful attention model, SCA-CNN also provides a better<br>understanding of where and what the model should focus<br>on during sentence generation.</p>",
            "id": 372,
            "page": 20,
            "text": "Unlike previous attention mechanisms which consider\neach image region equally and use global spatial information\nto tell the network where to focus, SCA-Net leverages the\nsemantic vector to produce the spatial attention map as well\nas the channel-wise attention weight vector. Being more than\na powerful attention model, SCA-CNN also provides a better\nunderstanding of where and what the model should focus\non during sentence generation."
        },
        {
            "bounding_box": [
                {
                    "x": 1295,
                    "y": 1564
                },
                {
                    "x": 1561,
                    "y": 1564
                },
                {
                    "x": 1561,
                    "y": 1609
                },
                {
                    "x": 1295,
                    "y": 1609
                }
            ],
            "category": "paragraph",
            "html": "<p id='373' style='font-size:16px'>3.6.13 GALA</p>",
            "id": 373,
            "page": 20,
            "text": "3.6.13 GALA"
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 1628
                },
                {
                    "x": 2356,
                    "y": 1628
                },
                {
                    "x": 2356,
                    "y": 1962
                },
                {
                    "x": 1291,
                    "y": 1962
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='374' style='font-size:18px'>Most attention mechanisms learn where to focus using<br>only weak supervisory signals from class labels, which<br>inspired Linsley et al. [122] to investigate how explicit human<br>supervision can affect the performance and interpretability<br>of attention models. As a proof of concept, Linsley et al.<br>proposed the global-and-local attention (GALA) module, which<br>extends an SE block with a spatial attention mechanism.</p>",
            "id": 374,
            "page": 20,
            "text": "Most attention mechanisms learn where to focus using\nonly weak supervisory signals from class labels, which\ninspired Linsley et al. [122] to investigate how explicit human\nsupervision can affect the performance and interpretability\nof attention models. As a proof of concept, Linsley et al.\nproposed the global-and-local attention (GALA) module, which\nextends an SE block with a spatial attention mechanism."
        },
        {
            "bounding_box": [
                {
                    "x": 1295,
                    "y": 1966
                },
                {
                    "x": 2358,
                    "y": 1966
                },
                {
                    "x": 2358,
                    "y": 2446
                },
                {
                    "x": 1295,
                    "y": 2446
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='375' style='font-size:18px'>Given the input feature map X, GALA uses an attention<br>mask that combines global and local attention to tell the<br>network where and on what to focus. As in SE blocks, global<br>attention aggregates global information by global average<br>pooling and then produces a channel-wise attention weight<br>vector using a multilayer perceptron. In local attention, two<br>consecutive 1 x 1 convolutions are conducted on the input<br>to produce a positional weight map. The outputs of the<br>local and global pathways are combined by addition and<br>multiplication. Formally, GALA can be represented as:</p>",
            "id": 375,
            "page": 20,
            "text": "Given the input feature map X, GALA uses an attention\nmask that combines global and local attention to tell the\nnetwork where and on what to focus. As in SE blocks, global\nattention aggregates global information by global average\npooling and then produces a channel-wise attention weight\nvector using a multilayer perceptron. In local attention, two\nconsecutive 1 x 1 convolutions are conducted on the input\nto produce a positional weight map. The outputs of the\nlocal and global pathways are combined by addition and\nmultiplication. Formally, GALA can be represented as:"
        },
        {
            "bounding_box": [
                {
                    "x": 1293,
                    "y": 2867
                },
                {
                    "x": 2356,
                    "y": 2867
                },
                {
                    "x": 2356,
                    "y": 2965
                },
                {
                    "x": 1293,
                    "y": 2965
                }
            ],
            "category": "paragraph",
            "html": "<p id='376' style='font-size:16px'>where a, m E RC are learnable parameters representing<br>channel-wise weight vectors.</p>",
            "id": 376,
            "page": 20,
            "text": "where a, m E RC are learnable parameters representing\nchannel-wise weight vectors."
        },
        {
            "bounding_box": [
                {
                    "x": 1290,
                    "y": 2971
                },
                {
                    "x": 2358,
                    "y": 2971
                },
                {
                    "x": 2358,
                    "y": 3114
                },
                {
                    "x": 1290,
                    "y": 3114
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='377' style='font-size:18px'>Supervised by human-provided feature importance maps,<br>GALA has significantly improved representational power<br>and can be combined with any CNN backbone.</p>",
            "id": 377,
            "page": 20,
            "text": "Supervised by human-provided feature importance maps,\nGALA has significantly improved representational power\nand can be combined with any CNN backbone."
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 109
                },
                {
                    "x": 1076,
                    "y": 109
                },
                {
                    "x": 1076,
                    "y": 149
                },
                {
                    "x": 193,
                    "y": 149
                }
            ],
            "category": "header",
            "html": "<header id='378' style='font-size:14px'>JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015</header>",
            "id": 378,
            "page": 21,
            "text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015"
        },
        {
            "bounding_box": [
                {
                    "x": 2312,
                    "y": 113
                },
                {
                    "x": 2350,
                    "y": 113
                },
                {
                    "x": 2350,
                    "y": 145
                },
                {
                    "x": 2312,
                    "y": 145
                }
            ],
            "category": "header",
            "html": "<br><header id='379' style='font-size:14px'>21</header>",
            "id": 379,
            "page": 21,
            "text": "21"
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 180
                },
                {
                    "x": 839,
                    "y": 180
                },
                {
                    "x": 839,
                    "y": 227
                },
                {
                    "x": 193,
                    "y": 227
                }
            ],
            "category": "paragraph",
            "html": "<p id='380' style='font-size:18px'>3.7 Spatial & Temporal Attention</p>",
            "id": 380,
            "page": 21,
            "text": "3.7 Spatial & Temporal Attention"
        },
        {
            "bounding_box": [
                {
                    "x": 191,
                    "y": 254
                },
                {
                    "x": 1258,
                    "y": 254
                },
                {
                    "x": 1258,
                    "y": 786
                },
                {
                    "x": 191,
                    "y": 786
                }
            ],
            "category": "paragraph",
            "html": "<p id='381' style='font-size:18px'>Spatial & temporal attention combines the advantages of spa-<br>tial attention and temporal attention as it adaptively selects<br>both important regions and key frames. Some works [16],<br>[130] compute temporal attention and spatial attention<br>separately, while others [131] produce joint spatiotemporal<br>attention maps. Further works focusing on capturing pair-<br>wise relations [177]. Representative spatial & temporal<br>attention attentions and specific process g(x) and f(g(x), x)<br>described as Eq. 1 are summarised in Tab. 8. We next discuss<br>specific spatial & temporal attention mechanisms according<br>to the order in Fig. 4.</p>",
            "id": 381,
            "page": 21,
            "text": "Spatial & temporal attention combines the advantages of spa-\ntial attention and temporal attention as it adaptively selects\nboth important regions and key frames. Some works [16],\n[130] compute temporal attention and spatial attention\nseparately, while others [131] produce joint spatiotemporal\nattention maps. Further works focusing on capturing pair-\nwise relations [177]. Representative spatial & temporal\nattention attentions and specific process g(x) and f(g(x), x)\ndescribed as Eq. 1 are summarised in Tab. 8. We next discuss\nspecific spatial & temporal attention mechanisms according\nto the order in Fig. 4."
        },
        {
            "bounding_box": [
                {
                    "x": 196,
                    "y": 852
                },
                {
                    "x": 527,
                    "y": 852
                },
                {
                    "x": 527,
                    "y": 900
                },
                {
                    "x": 196,
                    "y": 900
                }
            ],
            "category": "paragraph",
            "html": "<p id='382' style='font-size:16px'>3.7.1 STA-LSTM</p>",
            "id": 382,
            "page": 21,
            "text": "3.7.1 STA-LSTM"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 923
                },
                {
                    "x": 1258,
                    "y": 923
                },
                {
                    "x": 1258,
                    "y": 1406
                },
                {
                    "x": 192,
                    "y": 1406
                }
            ],
            "category": "paragraph",
            "html": "<p id='383' style='font-size:16px'>In human action recognition, each type of action generally<br>only depends on a few specific kinematic joints [130].<br>Furthermore, over time, multiple actions may be performed.<br>Motivated by these observations, Song et al. [130] proposed<br>a joint spatial and temporal attention network based on<br>LSTM [147], to adaptively find discriminative features and<br>keyframes. Its main attention-related components are a<br>spatial attention sub-network, to select important regions,<br>and a temporal attention sub-network, to select key frames.<br>The spatial attention sub-network can be written as:</p>",
            "id": 383,
            "page": 21,
            "text": "In human action recognition, each type of action generally\nonly depends on a few specific kinematic joints [130].\nFurthermore, over time, multiple actions may be performed.\nMotivated by these observations, Song et al. [130] proposed\na joint spatial and temporal attention network based on\nLSTM [147], to adaptively find discriminative features and\nkeyframes. Its main attention-related components are a\nspatial attention sub-network, to select important regions,\nand a temporal attention sub-network, to select key frames.\nThe spatial attention sub-network can be written as:"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 1665
                },
                {
                    "x": 1257,
                    "y": 1665
                },
                {
                    "x": 1257,
                    "y": 1858
                },
                {
                    "x": 192,
                    "y": 1858
                }
            ],
            "category": "paragraph",
            "html": "<p id='384' style='font-size:16px'>where Xt is the input feature at time t, Us, Whs, bsi, and bso<br>are learnable parameters, and ht-1 is the hidden state at step<br>t - 1. Note that use of the hidden state h means the attention<br>process takes temporal relationships into consideration.</p>",
            "id": 384,
            "page": 21,
            "text": "where Xt is the input feature at time t, Us, Whs, bsi, and bso\nare learnable parameters, and ht-1 is the hidden state at step\nt - 1. Note that use of the hidden state h means the attention\nprocess takes temporal relationships into consideration."
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 1863
                },
                {
                    "x": 1257,
                    "y": 1863
                },
                {
                    "x": 1257,
                    "y": 1963
                },
                {
                    "x": 192,
                    "y": 1963
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='385' style='font-size:16px'>The temporal attention sub-network is similar to the<br>spatial branch and produces its attention map using:</p>",
            "id": 385,
            "page": 21,
            "text": "The temporal attention sub-network is similar to the\nspatial branch and produces its attention map using:"
        },
        {
            "bounding_box": [
                {
                    "x": 1294,
                    "y": 182
                },
                {
                    "x": 2354,
                    "y": 182
                },
                {
                    "x": 2354,
                    "y": 275
                },
                {
                    "x": 1294,
                    "y": 275
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='386' style='font-size:16px'>mechanism aims to produce a global feature In for each<br>frame, which can be written as</p>",
            "id": 386,
            "page": 21,
            "text": "mechanism aims to produce a global feature In for each\nframe, which can be written as"
        },
        {
            "bounding_box": [
                {
                    "x": 1743,
                    "y": 383
                },
                {
                    "x": 1838,
                    "y": 383
                },
                {
                    "x": 1838,
                    "y": 417
                },
                {
                    "x": 1743,
                    "y": 417
                }
            ],
            "category": "paragraph",
            "html": "<p id='387' style='font-size:14px'>WxH</p>",
            "id": 387,
            "page": 21,
            "text": "WxH"
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 2097
                },
                {
                    "x": 1256,
                    "y": 2097
                },
                {
                    "x": 1256,
                    "y": 2241
                },
                {
                    "x": 193,
                    "y": 2241
                }
            ],
            "category": "paragraph",
            "html": "<p id='388' style='font-size:16px'>It adopts a ReLU function instead of a normalization function<br>for ease of optimization. It also uses a regularized objective<br>function to improve convergence.</p>",
            "id": 388,
            "page": 21,
            "text": "It adopts a ReLU function instead of a normalization function\nfor ease of optimization. It also uses a regularized objective\nfunction to improve convergence."
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 2248
                },
                {
                    "x": 1261,
                    "y": 2248
                },
                {
                    "x": 1261,
                    "y": 2392
                },
                {
                    "x": 192,
                    "y": 2392
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='389' style='font-size:16px'>Overall, this paper presents a joint spatiotemporal atten-<br>tion method to focus on important joints and keyframes,<br>with excellent results on the action recognition task.</p>",
            "id": 389,
            "page": 21,
            "text": "Overall, this paper presents a joint spatiotemporal atten-\ntion method to focus on important joints and keyframes,\nwith excellent results on the action recognition task."
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 696
                },
                {
                    "x": 2357,
                    "y": 696
                },
                {
                    "x": 2357,
                    "y": 890
                },
                {
                    "x": 1291,
                    "y": 890
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='390' style='font-size:16px'>where Ya is introduced to control the sharpness of the<br>location-score map. After obtaining frame-wise features<br>{l1,. . · , lT}, RSTAN uses a temporal attention mechanism to<br>estimate the importance of each frame feature</p>",
            "id": 390,
            "page": 21,
            "text": "where Ya is introduced to control the sharpness of the\nlocation-score map. After obtaining frame-wise features\n{l1,. . · , lT}, RSTAN uses a temporal attention mechanism to\nestimate the importance of each frame feature"
        },
        {
            "bounding_box": [
                {
                    "x": 1741,
                    "y": 998
                },
                {
                    "x": 1774,
                    "y": 998
                },
                {
                    "x": 1774,
                    "y": 1030
                },
                {
                    "x": 1741,
                    "y": 1030
                }
            ],
            "category": "paragraph",
            "html": "<p id='391' style='font-size:14px'>T</p>",
            "id": 391,
            "page": 21,
            "text": "T"
        },
        {
            "bounding_box": [
                {
                    "x": 195,
                    "y": 2459
                },
                {
                    "x": 465,
                    "y": 2459
                },
                {
                    "x": 465,
                    "y": 2507
                },
                {
                    "x": 195,
                    "y": 2507
                }
            ],
            "category": "paragraph",
            "html": "<p id='392' style='font-size:16px'>3.7.2 RSTAN</p>",
            "id": 392,
            "page": 21,
            "text": "3.7.2 RSTAN"
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 2532
                },
                {
                    "x": 1255,
                    "y": 2532
                },
                {
                    "x": 1255,
                    "y": 2675
                },
                {
                    "x": 193,
                    "y": 2675
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='393' style='font-size:18px'>To capture spatiotemporal contexts in video frames, Du<br>et al. [16] introduced spatiotemporal attention to adaptively<br>identify key features in a global way.</p>",
            "id": 393,
            "page": 21,
            "text": "To capture spatiotemporal contexts in video frames, Du\net al. [16] introduced spatiotemporal attention to adaptively\nidentify key features in a global way."
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 1312
                },
                {
                    "x": 2356,
                    "y": 1312
                },
                {
                    "x": 2356,
                    "y": 1556
                },
                {
                    "x": 1291,
                    "y": 1556
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='394' style='font-size:16px'>The spatiotemporal attention mechanism used in RSTAN<br>identifies those regions in both spatial and temporal domains<br>which are strongly related to the prediction in the current<br>step of the RNN. This efficiently enhances the representation<br>power of any 2D CNN.</p>",
            "id": 394,
            "page": 21,
            "text": "The spatiotemporal attention mechanism used in RSTAN\nidentifies those regions in both spatial and temporal domains\nwhich are strongly related to the prediction in the current\nstep of the RNN. This efficiently enhances the representation\npower of any 2D CNN."
        },
        {
            "bounding_box": [
                {
                    "x": 1293,
                    "y": 1627
                },
                {
                    "x": 1504,
                    "y": 1627
                },
                {
                    "x": 1504,
                    "y": 1673
                },
                {
                    "x": 1293,
                    "y": 1673
                }
            ],
            "category": "paragraph",
            "html": "<p id='395' style='font-size:16px'>3.7.3 STA</p>",
            "id": 395,
            "page": 21,
            "text": "3.7.3 STA"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 2682
                },
                {
                    "x": 1258,
                    "y": 2682
                },
                {
                    "x": 1258,
                    "y": 3115
                },
                {
                    "x": 192,
                    "y": 3115
                }
            ],
            "category": "paragraph",
            "html": "<p id='396' style='font-size:16px'>The spatiotemporal attention mechanism in RSTAN<br>consists of a spatial attention module and a temporal<br>attention module applied serially. Given an input feature<br>and the previous hidden state ht-1<br>map X E RDxTxHxW<br>of an RNN model, spatiotemporal attention aims to produce<br>a spatiotemporal feature representation for action recognition.<br>First, the given feature map X is reshaped to RDxTx(HxW)<br>and we define X(n, k) as the feature vector for the k-th<br>location of the n-th frame. At time t, the spatial attention</p>",
            "id": 396,
            "page": 21,
            "text": "The spatiotemporal attention mechanism in RSTAN\nconsists of a spatial attention module and a temporal\nattention module applied serially. Given an input feature\nand the previous hidden state ht-1\nmap X E RDxTxHxW\nof an RNN model, spatiotemporal attention aims to produce\na spatiotemporal feature representation for action recognition.\nFirst, the given feature map X is reshaped to RDxTx(HxW)\nand we define X(n, k) as the feature vector for the k-th\nlocation of the n-th frame. At time t, the spatial attention"
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 1703
                },
                {
                    "x": 2357,
                    "y": 1703
                },
                {
                    "x": 2357,
                    "y": 2037
                },
                {
                    "x": 1292,
                    "y": 2037
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='397' style='font-size:18px'>Previous attention-based methods for video-based person<br>re-identification only assigned an attention weight to each<br>frame and failed to capture joint spatial and temporal<br>relationships. To address this issue, Fu et al. [131] propose a<br>novel spatiotemporal attention (STA) approach, which assigns<br>attention scores for each spatial region in different frames<br>without any extra parameters.</p>",
            "id": 397,
            "page": 21,
            "text": "Previous attention-based methods for video-based person\nre-identification only assigned an attention weight to each\nframe and failed to capture joint spatial and temporal\nrelationships. To address this issue, Fu et al. [131] propose a\nnovel spatiotemporal attention (STA) approach, which assigns\nattention scores for each spatial region in different frames\nwithout any extra parameters."
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 2043
                },
                {
                    "x": 2355,
                    "y": 2043
                },
                {
                    "x": 2355,
                    "y": 2233
                },
                {
                    "x": 1291,
                    "y": 2233
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='398' style='font-size:16px'>Given the feature maps of an input video {Xn|Xn E<br>RCxHxW }n=1, STA first generates frame-wise attention<br>maps by using the l2 norm on the squares sum in the channel<br>domain:</p>",
            "id": 398,
            "page": 21,
            "text": "Given the feature maps of an input video {Xn|Xn E\nRCxHxW }n=1, STA first generates frame-wise attention\nmaps by using the l2 norm on the squares sum in the channel\ndomain:"
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 2428
                },
                {
                    "x": 2356,
                    "y": 2428
                },
                {
                    "x": 2356,
                    "y": 2621
                },
                {
                    "x": 1292,
                    "y": 2621
                }
            ],
            "category": "paragraph",
            "html": "<p id='399' style='font-size:16px'>Then both the feature maps and attention maps are divided<br>into K local regions horizontally, each of which represents<br>one part of the person. The spatial attention score for region<br>k is obtained using</p>",
            "id": 399,
            "page": 21,
            "text": "Then both the feature maps and attention maps are divided\ninto K local regions horizontally, each of which represents\none part of the person. The spatial attention score for region\nk is obtained using"
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 2816
                },
                {
                    "x": 2356,
                    "y": 2816
                },
                {
                    "x": 2356,
                    "y": 2959
                },
                {
                    "x": 1292,
                    "y": 2959
                }
            ],
            "category": "paragraph",
            "html": "<p id='400' style='font-size:18px'>To capture the relationships between regions in different<br>frames, STA applies l1 normalization to the attention scores<br>in the temporal domain, using</p>",
            "id": 400,
            "page": 21,
            "text": "To capture the relationships between regions in different\nframes, STA applies l1 normalization to the attention scores\nin the temporal domain, using"
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 109
                },
                {
                    "x": 1076,
                    "y": 109
                },
                {
                    "x": 1076,
                    "y": 149
                },
                {
                    "x": 193,
                    "y": 149
                }
            ],
            "category": "header",
            "html": "<header id='401' style='font-size:16px'>JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015</header>",
            "id": 401,
            "page": 22,
            "text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015"
        },
        {
            "bounding_box": [
                {
                    "x": 2312,
                    "y": 113
                },
                {
                    "x": 2353,
                    "y": 113
                },
                {
                    "x": 2353,
                    "y": 145
                },
                {
                    "x": 2312,
                    "y": 145
                }
            ],
            "category": "header",
            "html": "<br><header id='402' style='font-size:14px'>22</header>",
            "id": 402,
            "page": 22,
            "text": "22"
        },
        {
            "bounding_box": [
                {
                    "x": 1206,
                    "y": 184
                },
                {
                    "x": 1344,
                    "y": 184
                },
                {
                    "x": 1344,
                    "y": 221
                },
                {
                    "x": 1206,
                    "y": 221
                }
            ],
            "category": "header",
            "html": "<header id='403' style='font-size:16px'>TABLE 8</header>",
            "id": 403,
            "page": 22,
            "text": "TABLE 8"
        },
        {
            "bounding_box": [
                {
                    "x": 202,
                    "y": 222
                },
                {
                    "x": 2347,
                    "y": 222
                },
                {
                    "x": 2347,
                    "y": 340
                },
                {
                    "x": 202,
                    "y": 340
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='404' style='font-size:16px'>Representative spatial & temporal attentions sorted by date. Action=action recognition, ReID = re-identification. Ranges means the ranges of<br>attention map. S or H means soft or hard attention. g(x) and f(g(x), x) are the attention process described by Eq. 1. (A)element-wise product.(B)<br>aggregate information via attention map.(l) emphasize key points in both spatial and temporal domains, (II)capture global information.</p>",
            "id": 404,
            "page": 22,
            "text": "Representative spatial & temporal attentions sorted by date. Action=action recognition, ReID = re-identification. Ranges means the ranges of\nattention map. S or H means soft or hard attention. g(x) and f(g(x), x) are the attention process described by Eq. 1. (A)element-wise product.(B)\naggregate information via attention map.(l) emphasize key points in both spatial and temporal domains, (II)capture global information."
        },
        {
            "bounding_box": [
                {
                    "x": 216,
                    "y": 379
                },
                {
                    "x": 2335,
                    "y": 379
                },
                {
                    "x": 2335,
                    "y": 1082
                },
                {
                    "x": 216,
                    "y": 1082
                }
            ],
            "category": "table",
            "html": "<table id='405' style='font-size:16px'><tr><td>Category</td><td>Method</td><td>Publication</td><td>Tasks</td><td>g(x)</td><td>f(g(x),x)</td><td>Ranges</td><td>S or H</td><td>Goals</td></tr><tr><td rowspan=\"2\">Separately pre- dict spatial & temporal atten- tion</td><td>STA- LSTM [130]</td><td>AAAI2017</td><td>Action</td><td>a)spatial: fuse hidden state -> MLP -> Softmax, b)temporal: fuse hidden state -> MLP -> ReLU</td><td>(A)</td><td>(0,1), (0,+00)</td><td>S</td><td>(I)</td></tr><tr><td>RSTAN [16]</td><td>TIP2018</td><td>Action</td><td>a)spatial: fuse hidden state -> MLP -> Softmax, b)temporal: fuse hidden state -> MLP -> Softmax</td><td>(B)</td><td>(0,1)</td><td>S</td><td>(1) -</td></tr><tr><td>Jointly predict spatial & tempo- ral attention</td><td>STA [131</td><td>AAAI2019</td><td>ReID</td><td>a) tenporal: produce per- frame attention maps us- ing l2 norm b) spatial: obtain spatial scores for each patch by summa- tion using l1 norm.</td><td>(B)</td><td>(0,1)</td><td>S</td><td>(1)</td></tr><tr><td>Pairwise relation-based method</td><td>STGCN [177]</td><td>CVPR2020</td><td>ReID</td><td>construct a patch graph using pairwise similar- ity</td><td>(B)</td><td>(0,1)</td><td>S</td><td></td></tr></table>",
            "id": 405,
            "page": 22,
            "text": "Category Method Publication Tasks g(x) f(g(x),x) Ranges S or H Goals\n Separately pre- dict spatial & temporal atten- tion STA- LSTM [130] AAAI2017 Action a)spatial: fuse hidden state -> MLP -> Softmax, b)temporal: fuse hidden state -> MLP -> ReLU (A) (0,1), (0,+00) S (I)\n RSTAN [16] TIP2018 Action a)spatial: fuse hidden state -> MLP -> Softmax, b)temporal: fuse hidden state -> MLP -> Softmax (B) (0,1) S (1) -\n Jointly predict spatial & tempo- ral attention STA [131 AAAI2019 ReID a) tenporal: produce per- frame attention maps us- ing l2 norm b) spatial: obtain spatial scores for each patch by summa- tion using l1 norm. (B) (0,1) S (1)\n Pairwise relation-based method STGCN [177] CVPR2020 ReID construct a patch graph using pairwise similar- ity (B) (0,1) S"
        },
        {
            "bounding_box": [
                {
                    "x": 194,
                    "y": 1156
                },
                {
                    "x": 1259,
                    "y": 1156
                },
                {
                    "x": 1259,
                    "y": 1261
                },
                {
                    "x": 194,
                    "y": 1261
                }
            ],
            "category": "paragraph",
            "html": "<p id='406' style='font-size:18px'>Finally, STA splits the input feature map Xi into K regions<br>{Xn,1, · · · , Xn,K} and computes the output using</p>",
            "id": 406,
            "page": 22,
            "text": "Finally, STA splits the input feature map Xi into K regions\n{Xn,1, · · · , Xn,K} and computes the output using"
        },
        {
            "bounding_box": [
                {
                    "x": 393,
                    "y": 1361
                },
                {
                    "x": 798,
                    "y": 1361
                },
                {
                    "x": 798,
                    "y": 1392
                },
                {
                    "x": 393,
                    "y": 1392
                }
            ],
            "category": "paragraph",
            "html": "<p id='407' style='font-size:14px'>N N</p>",
            "id": 407,
            "page": 22,
            "text": "N N"
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 1596
                },
                {
                    "x": 1257,
                    "y": 1596
                },
                {
                    "x": 1257,
                    "y": 1932
                },
                {
                    "x": 193,
                    "y": 1932
                }
            ],
            "category": "paragraph",
            "html": "<p id='408' style='font-size:20px'>Instead of computing spatial attention maps frame by<br>frame, STA considers spatial and temporal attention infor-<br>mation simultaneously, fully using the discriminative parts<br>in both dimensions. This reduces the influence of occlusion.<br>Because of its non-parametric design, STA can tackle input<br>video sequences of variable length; it can be combined with<br>any 2D CNN backbone.</p>",
            "id": 408,
            "page": 22,
            "text": "Instead of computing spatial attention maps frame by\nframe, STA considers spatial and temporal attention infor-\nmation simultaneously, fully using the discriminative parts\nin both dimensions. This reduces the influence of occlusion.\nBecause of its non-parametric design, STA can tackle input\nvideo sequences of variable length; it can be combined with\nany 2D CNN backbone."
        },
        {
            "bounding_box": [
                {
                    "x": 195,
                    "y": 1996
                },
                {
                    "x": 473,
                    "y": 1996
                },
                {
                    "x": 473,
                    "y": 2044
                },
                {
                    "x": 195,
                    "y": 2044
                }
            ],
            "category": "paragraph",
            "html": "<p id='409' style='font-size:18px'>3.7.4 STGCN</p>",
            "id": 409,
            "page": 22,
            "text": "3.7.4 STGCN"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 2067
                },
                {
                    "x": 1256,
                    "y": 2067
                },
                {
                    "x": 1256,
                    "y": 2356
                },
                {
                    "x": 192,
                    "y": 2356
                }
            ],
            "category": "paragraph",
            "html": "<p id='410' style='font-size:18px'>To model the spatial relations within a frame and temporal<br>relations across frames, Yang et al. [177] proposed a novel<br>spatiotemporal graph convolutional network (STGCN) to learn<br>a discriminative descriptor for a video. It constructs a<br>patch graph using pairwise similarity, and then uses graph<br>convolution to aggregate information.</p>",
            "id": 410,
            "page": 22,
            "text": "To model the spatial relations within a frame and temporal\nrelations across frames, Yang et al. [177] proposed a novel\nspatiotemporal graph convolutional network (STGCN) to learn\na discriminative descriptor for a video. It constructs a\npatch graph using pairwise similarity, and then uses graph\nconvolution to aggregate information."
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 2361
                },
                {
                    "x": 1257,
                    "y": 2361
                },
                {
                    "x": 1257,
                    "y": 2799
                },
                {
                    "x": 193,
                    "y": 2799
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='411' style='font-size:18px'>STGCN includes two parallel GCN branches, the tempo-<br>ral graph module and the structural graph module. Given the<br>feature maps of a video, STGCN first horizontally partitions<br>each frame into P patches and applies average pooling to<br>generate patch-wise features X1, · · · , XN, where the total<br>number of patches is N = TP. For the temporal module, it<br>takes each patch as a graph node and construct a patch graph<br>for the video, where the adjacency matrix A is obtained by<br>normalizing the pairwise relation matrix E, defined as</p>",
            "id": 411,
            "page": 22,
            "text": "STGCN includes two parallel GCN branches, the tempo-\nral graph module and the structural graph module. Given the\nfeature maps of a video, STGCN first horizontally partitions\neach frame into P patches and applies average pooling to\ngenerate patch-wise features X1, · · · , XN, where the total\nnumber of patches is N = TP. For the temporal module, it\ntakes each patch as a graph node and construct a patch graph\nfor the video, where the adjacency matrix A is obtained by\nnormalizing the pairwise relation matrix E, defined as"
        },
        {
            "bounding_box": [
                {
                    "x": 800,
                    "y": 2900
                },
                {
                    "x": 835,
                    "y": 2900
                },
                {
                    "x": 835,
                    "y": 2931
                },
                {
                    "x": 800,
                    "y": 2931
                }
            ],
            "category": "paragraph",
            "html": "<p id='412' style='font-size:14px'>N</p>",
            "id": 412,
            "page": 22,
            "text": "N"
        },
        {
            "bounding_box": [
                {
                    "x": 1293,
                    "y": 1149
                },
                {
                    "x": 2356,
                    "y": 1149
                },
                {
                    "x": 2356,
                    "y": 1267
                },
                {
                    "x": 1293,
                    "y": 1267
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='413' style='font-size:20px'>where D(i,i) = EJ=1(A + I)(i,j). Given the adjacency<br>matrix A, the m-th graph convolution can be found using</p>",
            "id": 413,
            "page": 22,
            "text": "where D(i,i) = EJ=1(A + I)(i,j). Given the adjacency\nmatrix A, the m-th graph convolution can be found using"
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 1371
                },
                {
                    "x": 2358,
                    "y": 1371
                },
                {
                    "x": 2358,
                    "y": 1661
                },
                {
                    "x": 1291,
                    "y": 1661
                }
            ],
            "category": "paragraph",
            "html": "<p id='414' style='font-size:18px'>where X E RNxc the hidden features for all<br>represents<br>patches and Wm E Rcxc denotes the learnable weight matrix<br>for the m-th layer. For the spatial module, STGCN follows a<br>similar approach of adjacency matrix and graph convolution,<br>except for modeling the spatial relations of different regions<br>within a frame.</p>",
            "id": 414,
            "page": 22,
            "text": "where X E RNxc the hidden features for all\nrepresents\npatches and Wm E Rcxc denotes the learnable weight matrix\nfor the m-th layer. For the spatial module, STGCN follows a\nsimilar approach of adjacency matrix and graph convolution,\nexcept for modeling the spatial relations of different regions\nwithin a frame."
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 1665
                },
                {
                    "x": 2360,
                    "y": 1665
                },
                {
                    "x": 2360,
                    "y": 2050
                },
                {
                    "x": 1291,
                    "y": 2050
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='415' style='font-size:20px'>Flattening spatial and temporal dimensions into a se-<br>quence, STGCN applies the GCN to capture the spatiotempo-<br>ral relationships of patches across different frames. Pairwise<br>attention is used to obtain the weighted adjacency matrix.<br>By leveraging spatial and temporal relationships between<br>patches, STGCN overcomes the occlusion problem while also<br>enhancing informative features. It can used with any CNN<br>backbone to process video.</p>",
            "id": 415,
            "page": 22,
            "text": "Flattening spatial and temporal dimensions into a se-\nquence, STGCN applies the GCN to capture the spatiotempo-\nral relationships of patches across different frames. Pairwise\nattention is used to obtain the weighted adjacency matrix.\nBy leveraging spatial and temporal relationships between\npatches, STGCN overcomes the occlusion problem while also\nenhancing informative features. It can used with any CNN\nbackbone to process video."
        },
        {
            "bounding_box": [
                {
                    "x": 1293,
                    "y": 2118
                },
                {
                    "x": 1806,
                    "y": 2118
                },
                {
                    "x": 1806,
                    "y": 2169
                },
                {
                    "x": 1293,
                    "y": 2169
                }
            ],
            "category": "paragraph",
            "html": "<p id='416' style='font-size:20px'>4 FUTURE DIRECTIONS</p>",
            "id": 416,
            "page": 22,
            "text": "4 FUTURE DIRECTIONS"
        },
        {
            "bounding_box": [
                {
                    "x": 1293,
                    "y": 2184
                },
                {
                    "x": 2360,
                    "y": 2184
                },
                {
                    "x": 2360,
                    "y": 2280
                },
                {
                    "x": 1293,
                    "y": 2280
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='417' style='font-size:18px'>We present our thoughts on potential future research direc-<br>tions.</p>",
            "id": 417,
            "page": 22,
            "text": "We present our thoughts on potential future research direc-\ntions."
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 2346
                },
                {
                    "x": 2297,
                    "y": 2346
                },
                {
                    "x": 2297,
                    "y": 2393
                },
                {
                    "x": 1292,
                    "y": 2393
                }
            ],
            "category": "paragraph",
            "html": "<p id='418' style='font-size:20px'>4.1 Necessary and sufficient condition for attention</p>",
            "id": 418,
            "page": 22,
            "text": "4.1 Necessary and sufficient condition for attention"
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 2410
                },
                {
                    "x": 2357,
                    "y": 2410
                },
                {
                    "x": 2357,
                    "y": 2796
                },
                {
                    "x": 1292,
                    "y": 2796
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='419' style='font-size:18px'>We find the Eq. 1 is a necessary condition but not a necessary<br>and sufficient condition. For instance, GoogleNet [178]<br>conforms to the above formula, but does not belong to the<br>attention mechanisms. Unfortunately, we find it difficult to<br>find a necessary and sufficient condition for all attention<br>mechanisms. The necessary and sufficient conditions for the<br>attention mechanism are still worth exploring which can<br>promote our understanding of attention mechanisms.</p>",
            "id": 419,
            "page": 22,
            "text": "We find the Eq. 1 is a necessary condition but not a necessary\nand sufficient condition. For instance, GoogleNet [178]\nconforms to the above formula, but does not belong to the\nattention mechanisms. Unfortunately, we find it difficult to\nfind a necessary and sufficient condition for all attention\nmechanisms. The necessary and sufficient conditions for the\nattention mechanism are still worth exploring which can\npromote our understanding of attention mechanisms."
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 2859
                },
                {
                    "x": 1835,
                    "y": 2859
                },
                {
                    "x": 1835,
                    "y": 2905
                },
                {
                    "x": 1292,
                    "y": 2905
                }
            ],
            "category": "paragraph",
            "html": "<p id='420' style='font-size:18px'>4.2 General attention block</p>",
            "id": 420,
            "page": 22,
            "text": "4.2 General attention block"
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 2921
                },
                {
                    "x": 2359,
                    "y": 2921
                },
                {
                    "x": 2359,
                    "y": 3115
                },
                {
                    "x": 1291,
                    "y": 3115
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='421' style='font-size:18px'>At present, a special attention mechanism needs to be<br>designed for each different task, which requires considerable<br>effort to explore potential attention methods. For instance,<br>channel attention is a good choice for image classification,</p>",
            "id": 421,
            "page": 22,
            "text": "At present, a special attention mechanism needs to be\ndesigned for each different task, which requires considerable\neffort to explore potential attention methods. For instance,\nchannel attention is a good choice for image classification,"
        },
        {
            "bounding_box": [
                {
                    "x": 194,
                    "y": 109
                },
                {
                    "x": 1075,
                    "y": 109
                },
                {
                    "x": 1075,
                    "y": 149
                },
                {
                    "x": 194,
                    "y": 149
                }
            ],
            "category": "header",
            "html": "<header id='422' style='font-size:14px'>JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015</header>",
            "id": 422,
            "page": 23,
            "text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015"
        },
        {
            "bounding_box": [
                {
                    "x": 2311,
                    "y": 111
                },
                {
                    "x": 2354,
                    "y": 111
                },
                {
                    "x": 2354,
                    "y": 146
                },
                {
                    "x": 2311,
                    "y": 146
                }
            ],
            "category": "header",
            "html": "<br><header id='423' style='font-size:14px'>23</header>",
            "id": 423,
            "page": 23,
            "text": "23"
        },
        {
            "bounding_box": [
                {
                    "x": 191,
                    "y": 177
                },
                {
                    "x": 1258,
                    "y": 177
                },
                {
                    "x": 1258,
                    "y": 665
                },
                {
                    "x": 191,
                    "y": 665
                }
            ],
            "category": "paragraph",
            "html": "<p id='424' style='font-size:16px'>while spatial attention is well-suited to dense prediction<br>tasks such as semantic segmentation and object detection.<br>Channel attention focuses on what to pay attention to while<br>spatial attention considers where to pay attention. Based on this<br>observation, we encourage consideration as to whether there<br>could be a general attention block that takes advantage of all<br>kinds of attention mechanisms. For example, a soft selection<br>mechanism (branch attention) could choose between channel<br>attention, spatial attention and temporal attention according<br>to the specific task undertaken.</p>",
            "id": 424,
            "page": 23,
            "text": "while spatial attention is well-suited to dense prediction\ntasks such as semantic segmentation and object detection.\nChannel attention focuses on what to pay attention to while\nspatial attention considers where to pay attention. Based on this\nobservation, we encourage consideration as to whether there\ncould be a general attention block that takes advantage of all\nkinds of attention mechanisms. For example, a soft selection\nmechanism (branch attention) could choose between channel\nattention, spatial attention and temporal attention according\nto the specific task undertaken."
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 718
                },
                {
                    "x": 979,
                    "y": 718
                },
                {
                    "x": 979,
                    "y": 768
                },
                {
                    "x": 193,
                    "y": 768
                }
            ],
            "category": "paragraph",
            "html": "<p id='425' style='font-size:20px'>4.3 Characterisation and interpretability</p>",
            "id": 425,
            "page": 23,
            "text": "4.3 Characterisation and interpretability"
        },
        {
            "bounding_box": [
                {
                    "x": 191,
                    "y": 779
                },
                {
                    "x": 1259,
                    "y": 779
                },
                {
                    "x": 1259,
                    "y": 1364
                },
                {
                    "x": 191,
                    "y": 1364
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='426' style='font-size:18px'>Attention mechanisms are motivated by the human visual<br>system and are a step towards the goal of building an inter-<br>pretable computer vision system. Typically, attention-based<br>models are understood by rendering attention maps, as in<br>Fig. 9. However, this can only give an intuitive feel for what<br>is happening, rather than precise understanding. However,<br>applications in which security or safety are important, such<br>as medical diagnostics and automated driving systems, often<br>have stricter requirements. Better characterisation of how<br>methods work, including modes of failure, is needed in such<br>areas. Developing characterisable and interpretable attention<br>models could make them more widely applicable.</p>",
            "id": 426,
            "page": 23,
            "text": "Attention mechanisms are motivated by the human visual\nsystem and are a step towards the goal of building an inter-\npretable computer vision system. Typically, attention-based\nmodels are understood by rendering attention maps, as in\nFig. 9. However, this can only give an intuitive feel for what\nis happening, rather than precise understanding. However,\napplications in which security or safety are important, such\nas medical diagnostics and automated driving systems, often\nhave stricter requirements. Better characterisation of how\nmethods work, including modes of failure, is needed in such\nareas. Developing characterisable and interpretable attention\nmodels could make them more widely applicable."
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 1415
                },
                {
                    "x": 626,
                    "y": 1415
                },
                {
                    "x": 626,
                    "y": 1464
                },
                {
                    "x": 193,
                    "y": 1464
                }
            ],
            "category": "paragraph",
            "html": "<p id='427' style='font-size:20px'>4.4 Sparse activation</p>",
            "id": 427,
            "page": 23,
            "text": "4.4 Sparse activation"
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 1475
                },
                {
                    "x": 1259,
                    "y": 1475
                },
                {
                    "x": 1259,
                    "y": 1868
                },
                {
                    "x": 192,
                    "y": 1868
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='428' style='font-size:16px'>We visualize some attention map and obtains consistent<br>conclusion with ViT [34] shown in Fig. 9 that attention mech-<br>anisms can produce sparse activation. There phenomenon<br>give us a inspiration that sparse activation can achieve a<br>strong performance in deep neural networks. It is worth<br>noting that sparse activation is similar with human cognition.<br>Those motivate us to explore which kind of architecture can<br>simulate human visual system.</p>",
            "id": 428,
            "page": 23,
            "text": "We visualize some attention map and obtains consistent\nconclusion with ViT [34] shown in Fig. 9 that attention mech-\nanisms can produce sparse activation. There phenomenon\ngive us a inspiration that sparse activation can achieve a\nstrong performance in deep neural networks. It is worth\nnoting that sparse activation is similar with human cognition.\nThose motivate us to explore which kind of architecture can\nsimulate human visual system."
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 1920
                },
                {
                    "x": 968,
                    "y": 1920
                },
                {
                    "x": 968,
                    "y": 1969
                },
                {
                    "x": 193,
                    "y": 1969
                }
            ],
            "category": "paragraph",
            "html": "<p id='429' style='font-size:18px'>4.5 Attention-based pre-trained models</p>",
            "id": 429,
            "page": 23,
            "text": "4.5 Attention-based pre-trained models"
        },
        {
            "bounding_box": [
                {
                    "x": 191,
                    "y": 1981
                },
                {
                    "x": 1257,
                    "y": 1981
                },
                {
                    "x": 1257,
                    "y": 2516
                },
                {
                    "x": 191,
                    "y": 2516
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='430' style='font-size:18px'>Large-scale attention-based pre-trained models have had<br>great success in natural language processing [85], [179].<br>Recently, MoCoV3 [84], DINO [180], BEiT [85] and MAE [169]<br>have demonstrated that attention-based models are also well<br>suited to visual tasks. Due to their ability to adapt to varying<br>inputs, attention-based models can deal with unseen objects<br>and are naturally suited to transferring pretrained weights<br>to a variety of tasks. We believe that the combination of pre-<br>training and attention models should be further explored:<br>training approach, model structures, pre-training tasks and<br>the scale of data are all worth investigating.</p>",
            "id": 430,
            "page": 23,
            "text": "Large-scale attention-based pre-trained models have had\ngreat success in natural language processing [85], [179].\nRecently, MoCoV3 [84], DINO [180], BEiT [85] and MAE [169]\nhave demonstrated that attention-based models are also well\nsuited to visual tasks. Due to their ability to adapt to varying\ninputs, attention-based models can deal with unseen objects\nand are naturally suited to transferring pretrained weights\nto a variety of tasks. We believe that the combination of pre-\ntraining and attention models should be further explored:\ntraining approach, model structures, pre-training tasks and\nthe scale of data are all worth investigating."
        },
        {
            "bounding_box": [
                {
                    "x": 194,
                    "y": 2569
                },
                {
                    "x": 537,
                    "y": 2569
                },
                {
                    "x": 537,
                    "y": 2618
                },
                {
                    "x": 194,
                    "y": 2618
                }
            ],
            "category": "paragraph",
            "html": "<p id='431' style='font-size:18px'>4.6 Optimization</p>",
            "id": 431,
            "page": 23,
            "text": "4.6 Optimization"
        },
        {
            "bounding_box": [
                {
                    "x": 191,
                    "y": 2629
                },
                {
                    "x": 1255,
                    "y": 2629
                },
                {
                    "x": 1255,
                    "y": 3115
                },
                {
                    "x": 191,
                    "y": 3115
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='432' style='font-size:18px'>SGD [181] and Adam [182] are well-suited for optimizing<br>convolutional neural networks. For visual transformers,<br>AdamW [183] works better. Recently, Chen et al. [184]<br>significantly improved visual transformers by using a novel<br>optimizer, the sharpness-aware minimizer (SAM) [185]. It<br>is clear that attention-based networks and convolutional<br>neural networks are different models; different optimization<br>methods may work better for different models. Investigating<br>new optimzation methods for attention models is likely to<br>be worthwhile.</p>",
            "id": 432,
            "page": 23,
            "text": "SGD [181] and Adam [182] are well-suited for optimizing\nconvolutional neural networks. For visual transformers,\nAdamW [183] works better. Recently, Chen et al. [184]\nsignificantly improved visual transformers by using a novel\noptimizer, the sharpness-aware minimizer (SAM) [185]. It\nis clear that attention-based networks and convolutional\nneural networks are different models; different optimization\nmethods may work better for different models. Investigating\nnew optimzation methods for attention models is likely to\nbe worthwhile."
        },
        {
            "bounding_box": [
                {
                    "x": 1293,
                    "y": 178
                },
                {
                    "x": 1624,
                    "y": 178
                },
                {
                    "x": 1624,
                    "y": 227
                },
                {
                    "x": 1293,
                    "y": 227
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='433' style='font-size:20px'>4.7 Deployment</p>",
            "id": 433,
            "page": 23,
            "text": "4.7 Deployment"
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 240
                },
                {
                    "x": 2360,
                    "y": 240
                },
                {
                    "x": 2360,
                    "y": 678
                },
                {
                    "x": 1292,
                    "y": 678
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='434' style='font-size:18px'>Convolutional neural networks have a simple, uniform struc-<br>ture which makes them easy to deploy on various hardware<br>devices. However, it is difficult to optimize complex and<br>varied attention-based models on edge devices. Nevertheless,<br>experiments in [46], [47], [48] show that attention-based<br>models provide better results than convolutional neural<br>networks, so it is worth trying to find simple, efficient<br>and effective attention-based models which can be widely<br>deployed.</p>",
            "id": 434,
            "page": 23,
            "text": "Convolutional neural networks have a simple, uniform struc-\nture which makes them easy to deploy on various hardware\ndevices. However, it is difficult to optimize complex and\nvaried attention-based models on edge devices. Nevertheless,\nexperiments in [46], [47], [48] show that attention-based\nmodels provide better results than convolutional neural\nnetworks, so it is worth trying to find simple, efficient\nand effective attention-based models which can be widely\ndeployed."
        },
        {
            "bounding_box": [
                {
                    "x": 1293,
                    "y": 745
                },
                {
                    "x": 1679,
                    "y": 745
                },
                {
                    "x": 1679,
                    "y": 796
                },
                {
                    "x": 1293,
                    "y": 796
                }
            ],
            "category": "paragraph",
            "html": "<p id='435' style='font-size:22px'>5 CONCLUSIONS</p>",
            "id": 435,
            "page": 23,
            "text": "5 CONCLUSIONS"
        },
        {
            "bounding_box": [
                {
                    "x": 1294,
                    "y": 811
                },
                {
                    "x": 2361,
                    "y": 811
                },
                {
                    "x": 2361,
                    "y": 1539
                },
                {
                    "x": 1294,
                    "y": 1539
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='436' style='font-size:18px'>Attention mechanisms have become an indispensable tech-<br>nique in the field of computer vision in the era of deep<br>learning. This survey has systematically reviewed and sum-<br>marized attention mechanisms for deep neural networks<br>in computer vision. We have grouped different attention<br>methods according to their domain of operation, rather than<br>by application task, and show that attention models can be<br>regarded as an independent topic in their own right. We have<br>concluded with some potential directions for future research.<br>We hope that this work will encourage a variety of potential<br>application developers to put attention mechanisms to use<br>to improve their deep learning results. We also hope that<br>this survey will give researchers a deeper understanding of<br>various attention mechanisms and the relationships between<br>them, as a springboard for future research.</p>",
            "id": 436,
            "page": 23,
            "text": "Attention mechanisms have become an indispensable tech-\nnique in the field of computer vision in the era of deep\nlearning. This survey has systematically reviewed and sum-\nmarized attention mechanisms for deep neural networks\nin computer vision. We have grouped different attention\nmethods according to their domain of operation, rather than\nby application task, and show that attention models can be\nregarded as an independent topic in their own right. We have\nconcluded with some potential directions for future research.\nWe hope that this work will encourage a variety of potential\napplication developers to put attention mechanisms to use\nto improve their deep learning results. We also hope that\nthis survey will give researchers a deeper understanding of\nvarious attention mechanisms and the relationships between\nthem, as a springboard for future research."
        },
        {
            "bounding_box": [
                {
                    "x": 1294,
                    "y": 1604
                },
                {
                    "x": 1737,
                    "y": 1604
                },
                {
                    "x": 1737,
                    "y": 1656
                },
                {
                    "x": 1294,
                    "y": 1656
                }
            ],
            "category": "paragraph",
            "html": "<p id='437' style='font-size:22px'>ACKNOWLEDGMENTS</p>",
            "id": 437,
            "page": 23,
            "text": "ACKNOWLEDGMENTS"
        },
        {
            "bounding_box": [
                {
                    "x": 1292,
                    "y": 1670
                },
                {
                    "x": 2360,
                    "y": 1670
                },
                {
                    "x": 2360,
                    "y": 1915
                },
                {
                    "x": 1292,
                    "y": 1915
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='438' style='font-size:20px'>This work was supported by the Natural Science Foundation<br>of China (Project 61521002, 62132012). We would like to<br>thank Cheng-Ze Lu, Zhengyang Geng, Shilong liu, He Wang,<br>Huiying Lu and Chenxi Huang for their helpful discussions<br>and insightful suggestions.</p>",
            "id": 438,
            "page": 23,
            "text": "This work was supported by the Natural Science Foundation\nof China (Project 61521002, 62132012). We would like to\nthank Cheng-Ze Lu, Zhengyang Geng, Shilong liu, He Wang,\nHuiying Lu and Chenxi Huang for their helpful discussions\nand insightful suggestions."
        },
        {
            "bounding_box": [
                {
                    "x": 1293,
                    "y": 1982
                },
                {
                    "x": 1585,
                    "y": 1982
                },
                {
                    "x": 1585,
                    "y": 2034
                },
                {
                    "x": 1293,
                    "y": 2034
                }
            ],
            "category": "paragraph",
            "html": "<p id='439' style='font-size:22px'>REFERENCES</p>",
            "id": 439,
            "page": 23,
            "text": "REFERENCES"
        },
        {
            "bounding_box": [
                {
                    "x": 1299,
                    "y": 2050
                },
                {
                    "x": 2359,
                    "y": 2050
                },
                {
                    "x": 2359,
                    "y": 3114
                },
                {
                    "x": 1299,
                    "y": 3114
                }
            ],
            "category": "paragraph",
            "html": "<p id='440' style='font-size:16px'>[1] L. Itti, C. Koch, and E. Niebur, \"A model of saliency-based visual<br>attention for rapid scene analysis,\" IEEE Transactions on Pattern<br>Analysis and Machine Intelligence, vol. 20, no. 11, pp. 1254-1259,<br>1998.<br>[2] M. Hayhoe and D. Ballard, \"Eye movements in natural behavior,\"<br>Trends in cognitive sciences, vol. 9, no. 4, pp. 188-194, 2005.<br>[3] R. A. Rensink, \"The dynamic representation of scenes,\" Visual<br>cognition, vol. 7, no. 1-3, pp. 17-42, 2000.<br>[4] M. Corbetta and G. L. Shulman, \"Control of goal-directed and<br>stimulus-driven attention in the brain, \" Nature reviews neuroscience,<br>vol. 3, no. 3, pp. 201-215, 2002.<br>[5] J. Hu, L. Shen, S. Albanie, G. Sun, and E. Wu, \"Squeeze-and-<br>excitation networks, \" 2019.<br>[6] S. Woo, J. Park, J. Lee, and I. S. Kweon, \"CBAM: convolutional<br>block attention module, \" in Computer Vision - ECCV 2018<br>- 15th European Conference, Munich, Germany, September 8-14,<br>2018, Proceedings, Part VII, ser. Lecture Notes in Computer<br>Science, V. Ferrari, M. Hebert, C. Sminchisescu, and Y. Weiss,<br>Eds., vol. 11211. Springer, 2018, pp. 3-19. [Online]. Available:<br>https:/ /doi.org/10.1007/978-3-030-01234-2_1<br>[7] J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei,<br>\"Deformable convolutional networks,\" 2017.<br>[8] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and<br>S. Zagoruyko, \"End-to-end object detection with transformers,\"<br>2020.<br>[9] Y. Yuan, L. Huang, J. Guo, C. Zhang, X. Chen, and J. Wang,<br>\"Ocnet: Object context network for scene parsing,\" arXiv preprint<br>arXiv:1809.00916, 2018.</p>",
            "id": 440,
            "page": 23,
            "text": "[1] L. Itti, C. Koch, and E. Niebur, \"A model of saliency-based visual\nattention for rapid scene analysis,\" IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, vol. 20, no. 11, pp. 1254-1259,\n1998.\n[2] M. Hayhoe and D. Ballard, \"Eye movements in natural behavior,\"\nTrends in cognitive sciences, vol. 9, no. 4, pp. 188-194, 2005.\n[3] R. A. Rensink, \"The dynamic representation of scenes,\" Visual\ncognition, vol. 7, no. 1-3, pp. 17-42, 2000.\n[4] M. Corbetta and G. L. Shulman, \"Control of goal-directed and\nstimulus-driven attention in the brain, \" Nature reviews neuroscience,\nvol. 3, no. 3, pp. 201-215, 2002.\n[5] J. Hu, L. Shen, S. Albanie, G. Sun, and E. Wu, \"Squeeze-and-\nexcitation networks, \" 2019.\n[6] S. Woo, J. Park, J. Lee, and I. S. Kweon, \"CBAM: convolutional\nblock attention module, \" in Computer Vision - ECCV 2018\n- 15th European Conference, Munich, Germany, September 8-14,\n2018, Proceedings, Part VII, ser. Lecture Notes in Computer\nScience, V. Ferrari, M. Hebert, C. Sminchisescu, and Y. Weiss,\nEds., vol. 11211. Springer, 2018, pp. 3-19. [Online]. Available:\nhttps:/ /doi.org/10.1007/978-3-030-01234-2_1\n[7] J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei,\n\"Deformable convolutional networks,\" 2017.\n[8] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, \"End-to-end object detection with transformers,\"\n2020.\n[9] Y. Yuan, L. Huang, J. Guo, C. Zhang, X. Chen, and J. Wang,\n\"Ocnet: Object context network for scene parsing,\" arXiv preprint\narXiv:1809.00916, 2018."
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 110
                },
                {
                    "x": 1076,
                    "y": 110
                },
                {
                    "x": 1076,
                    "y": 148
                },
                {
                    "x": 192,
                    "y": 148
                }
            ],
            "category": "header",
            "html": "<header id='441' style='font-size:16px'>JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015</header>",
            "id": 441,
            "page": 24,
            "text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015"
        },
        {
            "bounding_box": [
                {
                    "x": 2312,
                    "y": 112
                },
                {
                    "x": 2353,
                    "y": 112
                },
                {
                    "x": 2353,
                    "y": 146
                },
                {
                    "x": 2312,
                    "y": 146
                }
            ],
            "category": "header",
            "html": "<br><header id='442' style='font-size:14px'>24</header>",
            "id": 442,
            "page": 24,
            "text": "24"
        },
        {
            "bounding_box": [
                {
                    "x": 195,
                    "y": 155
                },
                {
                    "x": 1265,
                    "y": 155
                },
                {
                    "x": 1265,
                    "y": 3125
                },
                {
                    "x": 195,
                    "y": 3125
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='443' style='font-size:20px'>[10] J. Fu, J. Liu, H. Tian, Y. Li, Y. Bao, Z. Fang, and H. Lu, \"Dual<br>attention network for scene segmentation, in Proceedings of the<br>IEEE/CVF Conference on Computer Vision and Pattern Recognition<br>(CVPR), June 2019.<br>[11] J. Yang, P. Ren, D. Zhang, D. Chen, F. Wen, H. Li, and G. Hua,<br>\"Neural aggregation network for video face recognition, \" 2017.<br>[12] Q. Wang, T. Wu, H. Zheng, and G. Guo, \"Hierarchical pyramid<br>diverse attention networks for face recognition,\" in Proceedings of<br>the IEEE/CVF Conference on Computer Vision and Pattern Recognition<br>(CVPR), June 2020.<br>[13] W. Li, X. Zhu, and S. Gong, \"Harmonious attention network for<br>person re-identification, \" in Proceedings of the IEEE conference on<br>computer vision and pattern recognition, 2018, pp. 2285-2294.<br>[14] B. Chen, W. Deng, and J. Hu, \"Mixed high-order attention network<br>for person re-identification, \" in Proceedings of the IEEE/CVF<br>International Conference on Computer Vision, 2019, pp. 371-381.<br>[15] X. Wang, R. Girshick, A. Gupta, and K. He, \"Non-local neural<br>networks, \" in Proceedings of the IEEE Conference on Computer Vision<br>and Pattern Recognition (CVPR), June 2018.<br>[16] W. Du, Y. Wang, and Y. Qiao, \"Recurrent spatial-temporal attention<br>network for action recognition in videos, \" IEEE Transactions on<br>Image Processing, vol. 27, no. 3, pp. 1347-1360, 2018.<br>[17] Y. Peng, X. He, and J. Zhao, \"Object-part attention model for<br>fine-grained image classification, IEEE Transactions on Image<br>Processing, vol. 27, no. 3, p. 1487-1500, Mar 2018. [Online].<br>Available: http:/ /dx.doi.org/10.1109/TIP2017.2774041<br>[18] P. He, W. Huang, T. He, Q. Zhu, Y. Qiao, and X. Li, \"Single shot<br>text detector with regional attention, \" 2017.<br>[19] 0. Oktay, J. Schlemper, L. L. Folgoc, M. Lee, M. Heinrich,<br>K. Misawa, K. Mori, S. McDonagh, N. Y. Hammerla, B. Kainz<br>et al., \"Attention u-net: Learning where to look for the pancreas,<br>arXiv preprint arXiv:1804.03999, 2018.<br>[20] Q. Guan, Y. Huang, Z. Zhong, Z. Zheng, L. Zheng, and Y. Yang,<br>\"Diagnose like a radiologist: Attention guided convolutional<br>neural network for thorax disease classification, \" arXiv preprint<br>arXiv:1801.09927, 2018.<br>[21] K. Gregor, I. Danihelka, A. Graves, D. J. Rezende, and D. Wierstra,<br>\"Draw: A recurrent neural network for image generation,\" 2015.<br>[22] H. Zhang, I. Goodfellow, D. Metaxas, and A. Odena, \"Self-<br>attention generative adversarial networks, \" 2019.<br>[23] X. Chu, W. Yang, W. Ouyang, C. Ma, A. L. Yuille, and X. Wang,<br>\"Multi-context attention for human pose estimation, \" 2017.<br>[24] T. Dai, J. Cai, Y. Zhang, S.-T. Xia, and L. Zhang, \"Second-order<br>attention network for single image super-resolution,\" in 2019<br>IEEE/CVF Conference on Computer Vision and Pattern Recognition<br>(CVPR), 2019, pp. 11 057-11 066.<br>[25] Y. Zhang, K. Li, K. Li, L. Wang, B. Zhong, and Y. Fu, \"Image super-<br>resolution using very deep residual channel attention networks,<br>2018.<br>[26] S. Xie, S. Liu, Z. Chen, and Z. Tu, \"Attentional shapecontextnet<br>for point cloud recognition,\" in Proceedings of the IEEE Conference<br>on Computer Vision and Pattern Recognition (CVPR), June 2018.<br>[27] M.-H. Guo, J.-X. Cai, Z.-N. Liu, T.-J. Mu, R. R. Martin, and<br>S.-M. Hu, \"Pct: Point cloud transformer,\" Computational Visual<br>Media, vol. 7, no. 2, p. 187-199, Apr 2021. [Online]. Available:<br>http:/ /dx.doi.org/10.1007/s41095-021-0229-5<br>[28] W. Su, X. Zhu, Y. Cao, B. Li, L. Lu, F. Wei, and J. Dai, \"Vl-bert:<br>Pre-training of generic visual-linguistic representations,\" 2020.<br>[29] T. Xu, P. Zhang, Q. Huang, H. Zhang, Z. Gan, X. Huang, and X. He,<br>\" Attngan: Fine-grained text to image generation with attentional<br>generative adversarial networks, 2017.<br>[30] Y. Wu and K. He, \"Group normalization, \" 2018.<br>[31] V. Mnih, N. Heess, A. Graves, and K. Kavukcuoglu, \"Recurrent<br>models of visual attention, \" 2014.<br>[32] M. Jaderberg, K. Simonyan, A. Zisserman, and K. Kavukcuoglu,<br>\"Spatial transformer networks, \" 2016.<br>[33] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.<br>Gomez, L. Kaiser, and I. Polosukhin, \" Attention is all you need,\"<br>2017.<br>[34] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,<br>T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,<br>J. Uszkoreit, and N. Houlsby, \"An image is worth 16x16 words:<br>Transformers for image recognition at scale, \" ICLR, 2021.<br>[35] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov,<br>R. Zemel, and Y. Bengio, \"Show, attend and tell: Neural image<br>caption generation with visual attention, \" in International confer-<br>ence on machine learning. PMLR, 2015, pp. 2048-2057.</p>",
            "id": 443,
            "page": 24,
            "text": "[10] J. Fu, J. Liu, H. Tian, Y. Li, Y. Bao, Z. Fang, and H. Lu, \"Dual\nattention network for scene segmentation, in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), June 2019.\n[11] J. Yang, P. Ren, D. Zhang, D. Chen, F. Wen, H. Li, and G. Hua,\n\"Neural aggregation network for video face recognition, \" 2017.\n[12] Q. Wang, T. Wu, H. Zheng, and G. Guo, \"Hierarchical pyramid\ndiverse attention networks for face recognition,\" in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), June 2020.\n[13] W. Li, X. Zhu, and S. Gong, \"Harmonious attention network for\nperson re-identification, \" in Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, 2018, pp. 2285-2294.\n[14] B. Chen, W. Deng, and J. Hu, \"Mixed high-order attention network\nfor person re-identification, \" in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, 2019, pp. 371-381.\n[15] X. Wang, R. Girshick, A. Gupta, and K. He, \"Non-local neural\nnetworks, \" in Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), June 2018.\n[16] W. Du, Y. Wang, and Y. Qiao, \"Recurrent spatial-temporal attention\nnetwork for action recognition in videos, \" IEEE Transactions on\nImage Processing, vol. 27, no. 3, pp. 1347-1360, 2018.\n[17] Y. Peng, X. He, and J. Zhao, \"Object-part attention model for\nfine-grained image classification, IEEE Transactions on Image\nProcessing, vol. 27, no. 3, p. 1487-1500, Mar 2018. [Online].\nAvailable: http:/ /dx.doi.org/10.1109/TIP2017.2774041\n[18] P. He, W. Huang, T. He, Q. Zhu, Y. Qiao, and X. Li, \"Single shot\ntext detector with regional attention, \" 2017.\n[19] 0. Oktay, J. Schlemper, L. L. Folgoc, M. Lee, M. Heinrich,\nK. Misawa, K. Mori, S. McDonagh, N. Y. Hammerla, B. Kainz\net al., \"Attention u-net: Learning where to look for the pancreas,\narXiv preprint arXiv:1804.03999, 2018.\n[20] Q. Guan, Y. Huang, Z. Zhong, Z. Zheng, L. Zheng, and Y. Yang,\n\"Diagnose like a radiologist: Attention guided convolutional\nneural network for thorax disease classification, \" arXiv preprint\narXiv:1801.09927, 2018.\n[21] K. Gregor, I. Danihelka, A. Graves, D. J. Rezende, and D. Wierstra,\n\"Draw: A recurrent neural network for image generation,\" 2015.\n[22] H. Zhang, I. Goodfellow, D. Metaxas, and A. Odena, \"Self-\nattention generative adversarial networks, \" 2019.\n[23] X. Chu, W. Yang, W. Ouyang, C. Ma, A. L. Yuille, and X. Wang,\n\"Multi-context attention for human pose estimation, \" 2017.\n[24] T. Dai, J. Cai, Y. Zhang, S.-T. Xia, and L. Zhang, \"Second-order\nattention network for single image super-resolution,\" in 2019\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), 2019, pp. 11 057-11 066.\n[25] Y. Zhang, K. Li, K. Li, L. Wang, B. Zhong, and Y. Fu, \"Image super-\nresolution using very deep residual channel attention networks,\n2018.\n[26] S. Xie, S. Liu, Z. Chen, and Z. Tu, \"Attentional shapecontextnet\nfor point cloud recognition,\" in Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), June 2018.\n[27] M.-H. Guo, J.-X. Cai, Z.-N. Liu, T.-J. Mu, R. R. Martin, and\nS.-M. Hu, \"Pct: Point cloud transformer,\" Computational Visual\nMedia, vol. 7, no. 2, p. 187-199, Apr 2021. [Online]. Available:\nhttp:/ /dx.doi.org/10.1007/s41095-021-0229-5\n[28] W. Su, X. Zhu, Y. Cao, B. Li, L. Lu, F. Wei, and J. Dai, \"Vl-bert:\nPre-training of generic visual-linguistic representations,\" 2020.\n[29] T. Xu, P. Zhang, Q. Huang, H. Zhang, Z. Gan, X. Huang, and X. He,\n\" Attngan: Fine-grained text to image generation with attentional\ngenerative adversarial networks, 2017.\n[30] Y. Wu and K. He, \"Group normalization, \" 2018.\n[31] V. Mnih, N. Heess, A. Graves, and K. Kavukcuoglu, \"Recurrent\nmodels of visual attention, \" 2014.\n[32] M. Jaderberg, K. Simonyan, A. Zisserman, and K. Kavukcuoglu,\n\"Spatial transformer networks, \" 2016.\n[33] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, \" Attention is all you need,\"\n2017.\n[34] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby, \"An image is worth 16x16 words:\nTransformers for image recognition at scale, \" ICLR, 2021.\n[35] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov,\nR. Zemel, and Y. Bengio, \"Show, attend and tell: Neural image\ncaption generation with visual attention, \" in International confer-\nence on machine learning. PMLR, 2015, pp. 2048-2057."
        },
        {
            "bounding_box": [
                {
                    "x": 1290,
                    "y": 174
                },
                {
                    "x": 2365,
                    "y": 174
                },
                {
                    "x": 2365,
                    "y": 3119
                },
                {
                    "x": 1290,
                    "y": 3119
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='444' style='font-size:20px'>[36] X. Zhu, H. Hu, S. Lin, and J. Dai, \"Deformable convnets v2:<br>More deformable, better results, \" in Proceedings of the IEEE/CVF<br>Conference on Computer Vision and Pattern Recognition (CVPR), June<br>2019.<br>[37] Q. Wang, B. Wu, P. Zhu, P. Li, W. Zuo, and Q. Hu, \"Eca-<br>net: Efficient channel attention for deep convolutional neural<br>networks, in Proceedings of the IEEE/CVF Conference on Computer<br>Vision and Pattern Recognition (CVPR), June 2020.<br>[38] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \"Bert: Pre-<br>training of deep bidirectional transformers for language under-<br>standing, \" 2019.<br>[39] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. Salakhutdinov, and Q. V.<br>Le, \"Xlnet: Generalized autoregressive pretraining for language<br>understanding, \" 2020.<br>[40] X. Li, Z. Zhong, J. Wu, Y. Yang, Z. Lin, and H. Liu, \"Expectation-<br>maximization attention networks for semantic segmentation, in<br>International Conference on Computer Vision, 2019.<br>[41] Z. Huang, X. Wang, Y. Wei, L. Huang, H. Shi, W. Liu, and T. S.<br>Huang, \"Ccnet: Criss-cross attention for semantic segmentation,<br>2020.<br>[42] Z. Geng, M.-H. Guo, H. Chen, X. Li, K. Wei, and Z. Lin, \"Is<br>attention better than matrix decomposition?\" in International<br>Conference on Learning Representations, 2021.<br>[43] P. Ramachandran, N. Parmar, A. Vaswani, I. Bello, A. Levskaya,<br>and J. Shlens, \"Stand-alone self-attention in vision models, \" 2019.<br>[44] L. Yuan, Y. Chen, T. Wang, W. Yu, Y. Shi, F. E. Tay, J. Feng, and<br>S. Yan, \"Tokens-to-token vit: Training vision transformers from<br>scratch on imagenet, \" arXiv preprint arXiv:2101.11986, 2021.<br>[45] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo,<br>and L. Shao, \"Pyramid vision transformer: A versatile backbone<br>for dense prediction without convolutions,\" 2021.<br>[46] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo,<br>\"Swin transformer: Hierarchical vision transformer using shifted<br>windows, \" 2021.<br>[47] H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, and<br>L. Zhang, \"Cvt: Introducing convolutions to vision transformers,\"<br>in International Conference on Computer Vision (ICCV), Oct. 2021.<br>[48] L. Yuan, Q. Hou, Z. Jiang, J. Feng, and S. Yan, \"Volo: Vision<br>outlooker for visual recognition, \" 2021.<br>[49] Z. Dai, H. Liu, Q. V. Le, and M. Tan, \"Coatnet: Marrying<br>convolution and attention for all data sizes, \" 2021.<br>[50] L. Chen, H. Zhang, J. Xiao, L. Nie, J. Shao, W. Liu, and<br>T. Chua, \"SCA-CNN: spatial and channel-wise attention<br>in convolutional networks for image captioning, \" in 2017<br>IEEE Conference on Computer Vision and Pattern Recognition,<br>CVPR 2017, Honolulu, HI, uSA, July 21-26, 2017. IEEE<br>Computer Society, 2017, pp. 6298-6306. [Online]. Available:<br>https:// doi.org/10.1109 /CVPR.2017.667<br>[51] V. Nair and G. E. Hinton, \"Rectified Linear Units Improve<br>Restricted Boltzmann Machines,\" in Proceedings of the 27th In-<br>ternational Conference on Machine Learning. Omnipress, 2010, pp.<br>807-814.<br>[52] S. Ioffe and C. Szegedy, \"Batch normalization: Accelerating deep<br>network training by reducing internal covariate shift,\" 2015.<br>[53] H. Zhang, K. J. Dana, J. Shi, Z. Zhang, X. Wang, A. Tyagi,<br>and A. Agrawal, \"Context encoding for semantic segmentation,<br>in 2018 IEEE Conference on Computer Vision and Pattern<br>Recognition, CVPR 2018, Salt Lake City, uT, uSA, June 18-22, 2018.<br>IEEE Computer Society, 2018, pp· 7151-7160. [Online].<br>Available: http:/ / openaccess.theovf.com/content_cpr_2018/<br>html/ Zhang_ Context_ Encoding _for_ CVPR 2018 _paper.html<br>[54] G. Zilin, X. Jiangtao, W. Qilong, and L. Peihua, \"Global second-<br>order pooling convolutional networks, \" in The IEEE Conference on<br>Computer Vision and Pattern Recognition (CVPR), 2019.<br>[55] H. Lee, H.-E. Kim, and H. Nam, \"Srm : A style-based recalibration<br>module for convolutional neural networks, 2019.<br>[56] Z. Yang, L. Zhu, Y. Wu, and Y. Yang, \"Gated channel transfor-<br>mation for visual recognition, \" in Proceedings of the IEEE/CVF<br>Conference on Computer Vision and Pattern Recognition, 2020, pp.<br>11 794-11 803.<br>[57] Z. Qin, P. Zhang, F. Wu, and X. Li, \"Fcanet: Frequency channel<br>attention networks, \" 2021.<br>[58] A. Diba, M. Fayyaz, V. Sharma, M. M. Arzani, R. Yousefzadeh,<br>J. Gall, and L. V. Gool, \"Spatio-temporal channel correlation<br>networks for action classification, \" 2019.<br>[59] Z. Chen, Y. Li, S. Bengio, and S. Si, \"You look twice: Gaternet for<br>dynamic filter selection in cnns, \" 2019.</p>",
            "id": 444,
            "page": 24,
            "text": "[36] X. Zhu, H. Hu, S. Lin, and J. Dai, \"Deformable convnets v2:\nMore deformable, better results, \" in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), June\n2019.\n[37] Q. Wang, B. Wu, P. Zhu, P. Li, W. Zuo, and Q. Hu, \"Eca-\nnet: Efficient channel attention for deep convolutional neural\nnetworks, in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), June 2020.\n[38] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \"Bert: Pre-\ntraining of deep bidirectional transformers for language under-\nstanding, \" 2019.\n[39] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. Salakhutdinov, and Q. V.\nLe, \"Xlnet: Generalized autoregressive pretraining for language\nunderstanding, \" 2020.\n[40] X. Li, Z. Zhong, J. Wu, Y. Yang, Z. Lin, and H. Liu, \"Expectation-\nmaximization attention networks for semantic segmentation, in\nInternational Conference on Computer Vision, 2019.\n[41] Z. Huang, X. Wang, Y. Wei, L. Huang, H. Shi, W. Liu, and T. S.\nHuang, \"Ccnet: Criss-cross attention for semantic segmentation,\n2020.\n[42] Z. Geng, M.-H. Guo, H. Chen, X. Li, K. Wei, and Z. Lin, \"Is\nattention better than matrix decomposition?\" in International\nConference on Learning Representations, 2021.\n[43] P. Ramachandran, N. Parmar, A. Vaswani, I. Bello, A. Levskaya,\nand J. Shlens, \"Stand-alone self-attention in vision models, \" 2019.\n[44] L. Yuan, Y. Chen, T. Wang, W. Yu, Y. Shi, F. E. Tay, J. Feng, and\nS. Yan, \"Tokens-to-token vit: Training vision transformers from\nscratch on imagenet, \" arXiv preprint arXiv:2101.11986, 2021.\n[45] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo,\nand L. Shao, \"Pyramid vision transformer: A versatile backbone\nfor dense prediction without convolutions,\" 2021.\n[46] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo,\n\"Swin transformer: Hierarchical vision transformer using shifted\nwindows, \" 2021.\n[47] H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, and\nL. Zhang, \"Cvt: Introducing convolutions to vision transformers,\"\nin International Conference on Computer Vision (ICCV), Oct. 2021.\n[48] L. Yuan, Q. Hou, Z. Jiang, J. Feng, and S. Yan, \"Volo: Vision\noutlooker for visual recognition, \" 2021.\n[49] Z. Dai, H. Liu, Q. V. Le, and M. Tan, \"Coatnet: Marrying\nconvolution and attention for all data sizes, \" 2021.\n[50] L. Chen, H. Zhang, J. Xiao, L. Nie, J. Shao, W. Liu, and\nT. Chua, \"SCA-CNN: spatial and channel-wise attention\nin convolutional networks for image captioning, \" in 2017\nIEEE Conference on Computer Vision and Pattern Recognition,\nCVPR 2017, Honolulu, HI, uSA, July 21-26, 2017. IEEE\nComputer Society, 2017, pp. 6298-6306. [Online]. Available:\nhttps:// doi.org/10.1109 /CVPR.2017.667\n[51] V. Nair and G. E. Hinton, \"Rectified Linear Units Improve\nRestricted Boltzmann Machines,\" in Proceedings of the 27th In-\nternational Conference on Machine Learning. Omnipress, 2010, pp.\n807-814.\n[52] S. Ioffe and C. Szegedy, \"Batch normalization: Accelerating deep\nnetwork training by reducing internal covariate shift,\" 2015.\n[53] H. Zhang, K. J. Dana, J. Shi, Z. Zhang, X. Wang, A. Tyagi,\nand A. Agrawal, \"Context encoding for semantic segmentation,\nin 2018 IEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2018, Salt Lake City, uT, uSA, June 18-22, 2018.\nIEEE Computer Society, 2018, pp· 7151-7160. [Online].\nAvailable: http:/ / openaccess.theovf.com/content_cpr_2018/\nhtml/ Zhang_ Context_ Encoding _for_ CVPR 2018 _paper.html\n[54] G. Zilin, X. Jiangtao, W. Qilong, and L. Peihua, \"Global second-\norder pooling convolutional networks, \" in The IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2019.\n[55] H. Lee, H.-E. Kim, and H. Nam, \"Srm : A style-based recalibration\nmodule for convolutional neural networks, 2019.\n[56] Z. Yang, L. Zhu, Y. Wu, and Y. Yang, \"Gated channel transfor-\nmation for visual recognition, \" in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2020, pp.\n11 794-11 803.\n[57] Z. Qin, P. Zhang, F. Wu, and X. Li, \"Fcanet: Frequency channel\nattention networks, \" 2021.\n[58] A. Diba, M. Fayyaz, V. Sharma, M. M. Arzani, R. Yousefzadeh,\nJ. Gall, and L. V. Gool, \"Spatio-temporal channel correlation\nnetworks for action classification, \" 2019.\n[59] Z. Chen, Y. Li, S. Bengio, and S. Si, \"You look twice: Gaternet for\ndynamic filter selection in cnns, \" 2019."
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 110
                },
                {
                    "x": 1076,
                    "y": 110
                },
                {
                    "x": 1076,
                    "y": 148
                },
                {
                    "x": 193,
                    "y": 148
                }
            ],
            "category": "header",
            "html": "<header id='445' style='font-size:16px'>JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015</header>",
            "id": 445,
            "page": 25,
            "text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015"
        },
        {
            "bounding_box": [
                {
                    "x": 2313,
                    "y": 113
                },
                {
                    "x": 2354,
                    "y": 113
                },
                {
                    "x": 2354,
                    "y": 145
                },
                {
                    "x": 2313,
                    "y": 145
                }
            ],
            "category": "header",
            "html": "<br><header id='446' style='font-size:14px'>25</header>",
            "id": 446,
            "page": 25,
            "text": "25"
        },
        {
            "bounding_box": [
                {
                    "x": 194,
                    "y": 159
                },
                {
                    "x": 1263,
                    "y": 159
                },
                {
                    "x": 1263,
                    "y": 3114
                },
                {
                    "x": 194,
                    "y": 3114
                }
            ],
            "category": "paragraph",
            "html": "<p id='447' style='font-size:20px'>[60] H. Shi, G. Lin, H. Wang, T.-Y. Hung, and Z. Wang, \"Spsequencenet:<br>Semantic segmentation network on 4d point clouds, in Proceed-<br>ings of the IEEE/CVF Conference on Computer Vision and Pattern<br>Recognition, 2020, pp. 4574-4583.<br>[61] J. Hu, L. Shen, S. Albanie, G. Sun, and A. Vedaldi, \"Gather-excite:<br>Exploiting feature context in convolutional neural networks, \"<br>2019.<br>[62] X. Yan, C. Zheng, Z. Li, S. Wang, and S. Cui, \"Pointasnl: Robust<br>point clouds processing using nonlocal neural networks with<br>adaptive sampling, in Proceedings of the IEEE/CVF Conference on<br>Computer Vision and Pattern Recognition, 2020, pp. 5589-5598.<br>[63] H. Hu, J. Gu, Z. Zhang, J. Dai, and Y. Wei, \"Relation networks for<br>object detection, \" 2018.<br>[64] H. Zhang, H. Zhang, C. Wang, andJ. Xie, \"Co-occurrent features in<br>semantic segmentation, in Proceedings of the IEEE/CVF Conference<br>on Computer Vision and Pattern Recognition (CVPR), June 2019.<br>[65] I. Bello, B. Zoph, A. Vaswani, J. Shlens, and Q. V. Le, \"At-<br>tention augmented convolutional networks, in Proceedings of<br>the IEEE/CVF International Conference on Computer Vision (ICCV),<br>October 2019.<br>[66] X. Zhu, D. Cheng, Z. Zhang, S. Lin, and J. Dai, \"An empirical<br>study of spatial attention mechanisms in deep networks, 2019.<br>[67] X. Li, Y. Yang, Q. Zhao, T. Shen, Z. Lin, and H. Liu, \"Spatial<br>pyramid based graph reasoning for semantic segmentation, in<br>Proceedings of the IEEE/CVF Conference on Computer Vision and<br>Pattern Recognition (CVPR), June 2020.<br>[68] Z. Zhu, M. Xu, S. Bai, T. Huang, and X. Bai, \"Asymmetric<br>non-local neural networks for semantic segmentation,\" in<br>International Conference on Computer Vision, 2019. [Online].<br>Available: http:/ / arxiv.org / abs/1908.07678<br>[69] Y. Cao, J. Xu, S. Lin, F. Wei, and H. Hu, \"Gcnet: Non-local networks<br>meet squeeze-excitation networks and beyond,\" arXiv preprint<br>arXiv:1904.11492, 2019.<br>[70] Y. Chen, Y. Kalantidis, J. Li, S. Yan, and J. Feng, \"a2-nets: Double<br>attention networks, 2018.<br>[71] Y. Chen, M. Rohrbach, Z. Yan, S. Yan, J. Feng, and Y. Kalantidis,<br>\"Graph-based global reasoning networks, \" 2018.<br>[72] S. Zhang, S. Yan, and X. He, \"Latentgnn: Learning efficient non-<br>local relations for visual recognition, \" 2019.<br>[73] Y. Yuan, X. Chen, X. Chen, and J. Wang, \"Segmentation trans-<br>former: Object-contextual representations for semantic segmenta-<br>tion, \" 2021.<br>[74] M. Yin, Z. Yao, Y. Cao, X. Li, Z. Zhang, S. Lin, and H. Hu,<br>\"Disentangled non-local neural networks,\" 2020.<br>[75] M.-H. Guo, Z.-N. Liu, T.-J. Mu, and S.-M. Hu, \"Beyond self-<br>attention: External attention using two linear layers for visual<br>tasks, \" 2021.<br>[76] H. Hu, Z. Zhang, Z. Xie, and S. Lin, \"Local relation networks for<br>image recognition, \" 2019.<br>[77] H. Zhao, J. Jia, and V. Koltun, \"Exploring self-attention for image<br>recognition, in Proceedings of the IEEE/CVF Conference on Computer<br>Vision and Pattern Recognition (CVPR), June 2020.<br>[78] M. Chen, A. Radford, R. Child, J. Wu, H. Jun, D. Luan, and<br>I. Sutskever, \"Generative pretraining from pixels, \" in Proceedings<br>of the 37th International Conference on Machine Learning, ser.<br>Proceedings of Machine Learning Research, H. D. III and A. Singh,<br>Eds., vol. 119. PMLR, 13-18 Jul 2020, pp. 1691-1703. [Online].<br>Available: https:/ / proceedings.mlr.press/v119/ chen20s.html<br>[79] H. Chen, Y. Wang, T. Guo, C. Xu, Y. Deng, Z. Liu, S. Ma, C. Xu,<br>C. Xu, and W. Gao, \"Pre-trained image processing transformer,\"<br>2021.<br>[80] H. Zhao, L. Jiang, J. Jia, P. Torr, and V. Koltun, \"Point transformer,\"<br>2020.<br>[81] S. Zheng, J. Lu, H. Zhao, X. Zhu, Z. Luo, Y. Wang, Y. Fu, J. Feng,<br>T. Xiang, P. H. Torr, and L. Zhang, \"Rethinking semantic segmenta-<br>tion from a sequence-to-sequence perspective with transformers,\"<br>in CVPR, 2021.<br>[82] K. Han, A. Xiao, E. Wu, J. Guo, C. Xu, and Y. Wang, \"Transformer<br>in transformer, \" 2021.<br>[83] S. Liu, L. Zhang, X. Yang, H. Su, and J. Zhu, \"Query2label: A<br>simple transformer way to multi-label classification, \" 2021.<br>[84] X. Chen, S. Xie, and K. He, \"An empirical study of training self-<br>supervised vision transformers, \" 2021.<br>[85] H. Bao, L. Dong, and F. Wei, \"Beit: Bert pre-training of image<br>transformers,\" 2021.</p>",
            "id": 447,
            "page": 25,
            "text": "[60] H. Shi, G. Lin, H. Wang, T.-Y. Hung, and Z. Wang, \"Spsequencenet:\nSemantic segmentation network on 4d point clouds, in Proceed-\nings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2020, pp. 4574-4583.\n[61] J. Hu, L. Shen, S. Albanie, G. Sun, and A. Vedaldi, \"Gather-excite:\nExploiting feature context in convolutional neural networks, \"\n2019.\n[62] X. Yan, C. Zheng, Z. Li, S. Wang, and S. Cui, \"Pointasnl: Robust\npoint clouds processing using nonlocal neural networks with\nadaptive sampling, in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2020, pp. 5589-5598.\n[63] H. Hu, J. Gu, Z. Zhang, J. Dai, and Y. Wei, \"Relation networks for\nobject detection, \" 2018.\n[64] H. Zhang, H. Zhang, C. Wang, andJ. Xie, \"Co-occurrent features in\nsemantic segmentation, in Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), June 2019.\n[65] I. Bello, B. Zoph, A. Vaswani, J. Shlens, and Q. V. Le, \"At-\ntention augmented convolutional networks, in Proceedings of\nthe IEEE/CVF International Conference on Computer Vision (ICCV),\nOctober 2019.\n[66] X. Zhu, D. Cheng, Z. Zhang, S. Lin, and J. Dai, \"An empirical\nstudy of spatial attention mechanisms in deep networks, 2019.\n[67] X. Li, Y. Yang, Q. Zhao, T. Shen, Z. Lin, and H. Liu, \"Spatial\npyramid based graph reasoning for semantic segmentation, in\nProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), June 2020.\n[68] Z. Zhu, M. Xu, S. Bai, T. Huang, and X. Bai, \"Asymmetric\nnon-local neural networks for semantic segmentation,\" in\nInternational Conference on Computer Vision, 2019. [Online].\nAvailable: http:/ / arxiv.org / abs/1908.07678\n[69] Y. Cao, J. Xu, S. Lin, F. Wei, and H. Hu, \"Gcnet: Non-local networks\nmeet squeeze-excitation networks and beyond,\" arXiv preprint\narXiv:1904.11492, 2019.\n[70] Y. Chen, Y. Kalantidis, J. Li, S. Yan, and J. Feng, \"a2-nets: Double\nattention networks, 2018.\n[71] Y. Chen, M. Rohrbach, Z. Yan, S. Yan, J. Feng, and Y. Kalantidis,\n\"Graph-based global reasoning networks, \" 2018.\n[72] S. Zhang, S. Yan, and X. He, \"Latentgnn: Learning efficient non-\nlocal relations for visual recognition, \" 2019.\n[73] Y. Yuan, X. Chen, X. Chen, and J. Wang, \"Segmentation trans-\nformer: Object-contextual representations for semantic segmenta-\ntion, \" 2021.\n[74] M. Yin, Z. Yao, Y. Cao, X. Li, Z. Zhang, S. Lin, and H. Hu,\n\"Disentangled non-local neural networks,\" 2020.\n[75] M.-H. Guo, Z.-N. Liu, T.-J. Mu, and S.-M. Hu, \"Beyond self-\nattention: External attention using two linear layers for visual\ntasks, \" 2021.\n[76] H. Hu, Z. Zhang, Z. Xie, and S. Lin, \"Local relation networks for\nimage recognition, \" 2019.\n[77] H. Zhao, J. Jia, and V. Koltun, \"Exploring self-attention for image\nrecognition, in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), June 2020.\n[78] M. Chen, A. Radford, R. Child, J. Wu, H. Jun, D. Luan, and\nI. Sutskever, \"Generative pretraining from pixels, \" in Proceedings\nof the 37th International Conference on Machine Learning, ser.\nProceedings of Machine Learning Research, H. D. III and A. Singh,\nEds., vol. 119. PMLR, 13-18 Jul 2020, pp. 1691-1703. [Online].\nAvailable: https:/ / proceedings.mlr.press/v119/ chen20s.html\n[79] H. Chen, Y. Wang, T. Guo, C. Xu, Y. Deng, Z. Liu, S. Ma, C. Xu,\nC. Xu, and W. Gao, \"Pre-trained image processing transformer,\"\n2021.\n[80] H. Zhao, L. Jiang, J. Jia, P. Torr, and V. Koltun, \"Point transformer,\"\n2020.\n[81] S. Zheng, J. Lu, H. Zhao, X. Zhu, Z. Luo, Y. Wang, Y. Fu, J. Feng,\nT. Xiang, P. H. Torr, and L. Zhang, \"Rethinking semantic segmenta-\ntion from a sequence-to-sequence perspective with transformers,\"\nin CVPR, 2021.\n[82] K. Han, A. Xiao, E. Wu, J. Guo, C. Xu, and Y. Wang, \"Transformer\nin transformer, \" 2021.\n[83] S. Liu, L. Zhang, X. Yang, H. Su, and J. Zhu, \"Query2label: A\nsimple transformer way to multi-label classification, \" 2021.\n[84] X. Chen, S. Xie, and K. He, \"An empirical study of training self-\nsupervised vision transformers, \" 2021.\n[85] H. Bao, L. Dong, and F. Wei, \"Beit: Bert pre-training of image\ntransformers,\" 2021."
        },
        {
            "bounding_box": [
                {
                    "x": 1289,
                    "y": 172
                },
                {
                    "x": 2363,
                    "y": 172
                },
                {
                    "x": 2363,
                    "y": 3122
                },
                {
                    "x": 1289,
                    "y": 3122
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='448' style='font-size:20px'>[86] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo,<br>\"Segformer: Simple and efficient design for semantic segmentation<br>with transformers, \" 2021.<br>[87] H. Zhao, Y. Zhang, S. Liu, J. Shi, C. C. Loy, D. Lin, and J. Jia,<br>\"Psanet: Point-wise spatial attention network for scene parsing,<br>in Proceedings of the European Conference on Computer Vision (ECCV),<br>September 2018.<br>[88] J. Ba, V. Mnih, and K. Kavukcuoglu, \"Multiple object recognition<br>with visual attention, \" 2015.<br>[89] S. Sharma, R. Kiros, and R. Salakhutdinov, \"Action recognition<br>using visual attention, \" 2016.<br>[90] R. Girdhar and D. Ramanan, \"Attentional pooling for action<br>recognition, \" 2017.<br>[91] Z. Li, E. Gavves, M. Jain, and C. G. M. Snoek, \"Videolstm<br>convolves, attends and flows for action recognition,\" 2016.<br>[92] K. Yue, M. Sun, Y. Yuan, F. Zhou, E. Ding, and F. Xu, \"Compact<br>generalized non-local network, \" 2018.<br>[93] X. Liu, Z. Han, X. Wen, Y.-S. Liu, and M. Zwicker, \"L2g auto-<br>encoder: Understanding point clouds by local-to-global recon-<br>struction with hierarchical self-attention, in Proceedings of the 27th<br>ACM International Conference on Multimedia, 2019, pp. 989-997.<br>[94] A. Paigwar, O. Erkent, C. Wolf, and C. Laugier, \" Attentional<br>pointnet for 3d-object detection in point clouds,\" in Proceedings of<br>the IEEE/CVF Conference on Computer Vision and Pattern Recognition<br>Workshops, 2019, pp. 0-0.<br>[95] X. Wen, Z. Han, G. Youk, and Y.-S. Liu, \"Cf-sis: Semantic-instance<br>segmentation of 3d point clouds by context fusion with self-<br>attention, \" in Proceedings of the 28th ACM International Conference<br>on Multimedia, 2020, pp. 1661-1669.<br>[96] J. Yang, Q. Zhang, B. Ni, L. Li, J. Liu, M. Zhou, and Q. Tian,<br>\"Modeling point clouds with self-attention and gumbel subset<br>sampling, in Proceedings of the IEEE/CVF Conference on Computer<br>Vision and Pattern Recognition, 2019, pp. 3323-3332.<br>[97] J. Xu, R. Zhao, F. Zhu, H. Wang, and W. Ouyang, \" Attention-aware<br>compositional network for person re-identification, \" in Proceedings<br>of the IEEE conference on computer vision and pattern recognition, 2018,<br>pp. 2119-2128.<br>[98] H. Liu, J. Feng, M. Qi, J. Jiang, and S. Yan, \"End-to-end com-<br>parative attention networks for person re-identification, \" IEEE<br>Transactions on Image Processing, vol. 26, no. 7, pp. 3492-3506, 2017.<br>[99] Z. Zheng, L. Zheng, and Y. Yang, \"Pedestrian alignment network<br>for large-scale person re-identification, IEEE Transactions on<br>Circuits and Systems for Video Technology, vol. 29, no. 10, pp. 3037-<br>3045, 2018.<br>[100] K. Li, Z. Wu, K.-C. Peng, J. Ernst, and Y. Fu, \"Tell me where to<br>look: Guided attention inference network, \" in Proceedings of the<br>IEEE Conference on Computer Vision and Pattern Recognition, 2018,<br>pp. 9215-9223.<br>[101] Z. Zhang, C. Lan, W. Zeng, X. Jin, and Z. Chen, \"Relation-aware<br>global attention for person re-identification, in Proceedings of the<br>ieee/cof conference on computer vision and pattern recognition, 2020,<br>pp. 3186-3195.<br>[102] B. N. Xia, Y. Gong, Y. Zhang, and C. Poellabauer, \"Second-order<br>non-local attention networks for person re-identification, \" in<br>Proceedings of the IEEE/CVF International Conference on Computer<br>Vision, 2019, pp. 3760-3769.<br>[103] B. Zhao, X. Wu, J. Feng, Q. Peng, and S. Yan, \"Diversified visual<br>attention networks for fine-grained object classification, \" IEEE<br>Transactions on Multimedia, vol. 19, no. 6, pp. 1245-1256, 2017.<br>[104] H. Zheng, J. Fu, T. Mei, and J. Luo, \"Learning multi-attention<br>convolutional neural network for fine-grained image recognition,<br>in Proceedings of the IEEE international conference on computer vision,<br>2017, pp. 5209-5217.<br>[105] J. Fu, H. Zheng, and T. Mei, \"Look closer to see better: Recurrent<br>attention convolutional neural network for fine-grained image<br>recognition, \" in Proceedings of the IEEE conference on computer vision<br>and pattern recognition, 2017, pp. 4438-4446.<br>[106] Anonymous, \"DAB-DETR: Dynamic anchor boxes are better<br>queries for DETR, \" in Submitted to The Tenth International Conference<br>on Learning Representations, 2022, under review. [Online]. Available:<br>https:/ / operreview.net/forum?id=oM19M19P1069]l<br>[107] G.-Y. Yang, X.-L. Li, R. R. Martin, and S.-M. Hu, \"Sampling<br>equivariant self-attention networks for object detection in aerial<br>images, \" 2021.<br>[108] H. Zheng, J. Fu, Z.-J. Zha, and J. Luo, \"Looking for the devil<br>in the details: Learning trilinear attention sampling network for<br>fine-grained image recognition,\" in Proceedings of the IEEE/CVF</p>",
            "id": 448,
            "page": 25,
            "text": "[86] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo,\n\"Segformer: Simple and efficient design for semantic segmentation\nwith transformers, \" 2021.\n[87] H. Zhao, Y. Zhang, S. Liu, J. Shi, C. C. Loy, D. Lin, and J. Jia,\n\"Psanet: Point-wise spatial attention network for scene parsing,\nin Proceedings of the European Conference on Computer Vision (ECCV),\nSeptember 2018.\n[88] J. Ba, V. Mnih, and K. Kavukcuoglu, \"Multiple object recognition\nwith visual attention, \" 2015.\n[89] S. Sharma, R. Kiros, and R. Salakhutdinov, \"Action recognition\nusing visual attention, \" 2016.\n[90] R. Girdhar and D. Ramanan, \"Attentional pooling for action\nrecognition, \" 2017.\n[91] Z. Li, E. Gavves, M. Jain, and C. G. M. Snoek, \"Videolstm\nconvolves, attends and flows for action recognition,\" 2016.\n[92] K. Yue, M. Sun, Y. Yuan, F. Zhou, E. Ding, and F. Xu, \"Compact\ngeneralized non-local network, \" 2018.\n[93] X. Liu, Z. Han, X. Wen, Y.-S. Liu, and M. Zwicker, \"L2g auto-\nencoder: Understanding point clouds by local-to-global recon-\nstruction with hierarchical self-attention, in Proceedings of the 27th\nACM International Conference on Multimedia, 2019, pp. 989-997.\n[94] A. Paigwar, O. Erkent, C. Wolf, and C. Laugier, \" Attentional\npointnet for 3d-object detection in point clouds,\" in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition\nWorkshops, 2019, pp. 0-0.\n[95] X. Wen, Z. Han, G. Youk, and Y.-S. Liu, \"Cf-sis: Semantic-instance\nsegmentation of 3d point clouds by context fusion with self-\nattention, \" in Proceedings of the 28th ACM International Conference\non Multimedia, 2020, pp. 1661-1669.\n[96] J. Yang, Q. Zhang, B. Ni, L. Li, J. Liu, M. Zhou, and Q. Tian,\n\"Modeling point clouds with self-attention and gumbel subset\nsampling, in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2019, pp. 3323-3332.\n[97] J. Xu, R. Zhao, F. Zhu, H. Wang, and W. Ouyang, \" Attention-aware\ncompositional network for person re-identification, \" in Proceedings\nof the IEEE conference on computer vision and pattern recognition, 2018,\npp. 2119-2128.\n[98] H. Liu, J. Feng, M. Qi, J. Jiang, and S. Yan, \"End-to-end com-\nparative attention networks for person re-identification, \" IEEE\nTransactions on Image Processing, vol. 26, no. 7, pp. 3492-3506, 2017.\n[99] Z. Zheng, L. Zheng, and Y. Yang, \"Pedestrian alignment network\nfor large-scale person re-identification, IEEE Transactions on\nCircuits and Systems for Video Technology, vol. 29, no. 10, pp. 3037-\n3045, 2018.\n[100] K. Li, Z. Wu, K.-C. Peng, J. Ernst, and Y. Fu, \"Tell me where to\nlook: Guided attention inference network, \" in Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition, 2018,\npp. 9215-9223.\n[101] Z. Zhang, C. Lan, W. Zeng, X. Jin, and Z. Chen, \"Relation-aware\nglobal attention for person re-identification, in Proceedings of the\nieee/cof conference on computer vision and pattern recognition, 2020,\npp. 3186-3195.\n[102] B. N. Xia, Y. Gong, Y. Zhang, and C. Poellabauer, \"Second-order\nnon-local attention networks for person re-identification, \" in\nProceedings of the IEEE/CVF International Conference on Computer\nVision, 2019, pp. 3760-3769.\n[103] B. Zhao, X. Wu, J. Feng, Q. Peng, and S. Yan, \"Diversified visual\nattention networks for fine-grained object classification, \" IEEE\nTransactions on Multimedia, vol. 19, no. 6, pp. 1245-1256, 2017.\n[104] H. Zheng, J. Fu, T. Mei, and J. Luo, \"Learning multi-attention\nconvolutional neural network for fine-grained image recognition,\nin Proceedings of the IEEE international conference on computer vision,\n2017, pp. 5209-5217.\n[105] J. Fu, H. Zheng, and T. Mei, \"Look closer to see better: Recurrent\nattention convolutional neural network for fine-grained image\nrecognition, \" in Proceedings of the IEEE conference on computer vision\nand pattern recognition, 2017, pp. 4438-4446.\n[106] Anonymous, \"DAB-DETR: Dynamic anchor boxes are better\nqueries for DETR, \" in Submitted to The Tenth International Conference\non Learning Representations, 2022, under review. [Online]. Available:\nhttps:/ / operreview.net/forum?id=oM19M19P1069]l\n[107] G.-Y. Yang, X.-L. Li, R. R. Martin, and S.-M. Hu, \"Sampling\nequivariant self-attention networks for object detection in aerial\nimages, \" 2021.\n[108] H. Zheng, J. Fu, Z.-J. Zha, and J. Luo, \"Looking for the devil\nin the details: Learning trilinear attention sampling network for\nfine-grained image recognition,\" in Proceedings of the IEEE/CVF"
        },
        {
            "bounding_box": [
                {
                    "x": 193,
                    "y": 110
                },
                {
                    "x": 1075,
                    "y": 110
                },
                {
                    "x": 1075,
                    "y": 148
                },
                {
                    "x": 193,
                    "y": 148
                }
            ],
            "category": "header",
            "html": "<header id='449' style='font-size:14px'>JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015</header>",
            "id": 449,
            "page": 26,
            "text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015"
        },
        {
            "bounding_box": [
                {
                    "x": 2313,
                    "y": 114
                },
                {
                    "x": 2352,
                    "y": 114
                },
                {
                    "x": 2352,
                    "y": 145
                },
                {
                    "x": 2313,
                    "y": 145
                }
            ],
            "category": "header",
            "html": "<br><header id='450' style='font-size:14px'>26</header>",
            "id": 450,
            "page": 26,
            "text": "26"
        },
        {
            "bounding_box": [
                {
                    "x": 284,
                    "y": 186
                },
                {
                    "x": 1255,
                    "y": 186
                },
                {
                    "x": 1255,
                    "y": 258
                },
                {
                    "x": 284,
                    "y": 258
                }
            ],
            "category": "paragraph",
            "html": "<p id='451' style='font-size:20px'>Conference on Computer Vision and Pattern Recognition, 2019, pp·<br>5012-5021.</p>",
            "id": 451,
            "page": 26,
            "text": "Conference on Computer Vision and Pattern Recognition, 2019, pp·\n5012-5021."
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 241
                },
                {
                    "x": 1264,
                    "y": 241
                },
                {
                    "x": 1264,
                    "y": 3121
                },
                {
                    "x": 192,
                    "y": 3121
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='452' style='font-size:20px'>[109] J. Lee, Y. Lee, J. Kim, A. R. Kosiorek, S. Choi, and<br>Y. W. Teh, \"Set transformer: A framework for attention-based<br>permutation-invariant neural networks, in Proceedings of the<br>36th International Conference on Machine Learning, ICML 2019,<br>9-15 June 2019, Long Beach, California, uSA, ser. Proceedings of<br>Machine Learning Research, K. Chaudhuri and R. Salakhutdinov,<br>Eds., vol. 97. PMLR, 2019, pp. 3744-3753. [Online]. Available:<br>http:/ /proceedings.mlr.press/v97 /lee19d.html<br>[110] S. Xu, Y. Cheng, K. Gu, Y. Yang, S. Chang, and P. Zhou, \"Jointly<br>attentive spatial-temporal pooling networks for video-based<br>person re-identification, in Proceedings of the IEEE international<br>conference on computer vision, 2017, pp. 4733-4742.<br>[111] R. Zhang, J. Li, H. Sun, Y. Ge, P. Luo, X. Wang, and L. Lin,<br>\"Scan: Self-and-collaborative attention network for video person<br>re-identification, \" IEEE Transactions on Image Processing, vol. 28,<br>no. 10, pp. 4870-4882, 2019.<br>[112] D. Chen, H. Li, T. Xiao, S. Yi, and X. Wang, \"Video person re-<br>identification with competitive snippet-similarity aggregation<br>and co-attentive snippet embedding, in Proceedings of the IEEE<br>Conference on Computer Vision and Pattern Recognition, 2018, pp.<br>1169-1178.<br>[113] R. K. Srivastava, K. Greff, and J. Schmidhuber, \"Training very<br>deep networks, arXiv preprint arXiv:1507.06228, 2015.<br>[114] X. Li, W. Wang, X. Hu, and J. Yang, \"Selective kernel networks,\"<br>in Proceedings of the IEEE/CVF Conference on Computer Vision and<br>Pattern Recognition, 2019, pp. 510-519.<br>[115] H. Zhang, C. Wu, Z. Zhang, Y. Zhu, H. Lin, Z. Zhang, Y. Sun,<br>T. He, J. Mueller, R. Manmatha, M. Li, and A. Smola, \"Resnest:<br>Split-attention networks,\" 2020.<br>[116] Y. Chen, X. Dai, M. Liu, D. Chen, L. Yuan, and Z. Liu, \"Dynamic<br>convolution: Attention over convolution kernels, in Proceedings of<br>the IEEE/CVF Conference on Computer Vision and Pattern Recognition,<br>2020, pp. 11 030-11 039.<br>[117] J. Park, S. Woo, J.-Y. Lee, and I. S. Kweon, \"Bam: Bottleneck<br>attention module, \" 2018.<br>[118] L. Yang, R.-Y. Zhang, L. Li, and X. Xie, \"Simam: A<br>simple, parameter-free attention module for convolutional<br>neural networks,\" in Proceedings of the 38th International<br>Conference on Machine Learning, ser. Proceedings of Machine<br>Learning Research, M. Meila and T. Zhang, Eds., vol. 139.<br>PMLR, 18-24 Jul 2021, pp. 11 863-11 874. [Online]. Available:<br>http:/ /pmosedings.minpres/v139/yang210.html<br>[119] F. Wang, M. Jiang, C. Qian, S. Yang, C. Li, H. Zhang, X. Wang,<br>and X. Tang, \"Residual attention network for image classification, \"<br>in Proceedings of the IEEE conference on computer vision and pattern<br>recognition, 2017, pp. 3156-3164.<br>[120] J.-J. Liu, Q. Hou, M.-M. Cheng, C. Wang, and J. Feng, \"Improving<br>convolutional networks with self-calibrated convolutions, in<br>Proceedings of the IEEE/CVF Conference on Computer Vision and<br>Pattern Recognition (CVPR), June 2020.<br>[121] D. Misra, T. Nalamada, A. U. Arasanipalai, and Q. Hou, \"Rotate to<br>attend: Convolutional triplet attention module, \" in Proceedings of<br>the IEEE/CVF Winter Conference on Applications of Computer Vision,<br>2021, pp. 3139-3148.<br>[122] D. Linsley, D. Shiebler, S. Eberhardt, and T. Serre, \"Learning<br>what and where to attend,\" in 7th International Conference on<br>Learning Representations, ICLR 2019, New Orleans, LA, USA,<br>May 6-9, 2019. OpenReview.net, 2019. [Online]. Available:<br>https:/ / openneview.net/forum?id=BIgLg3R9KQ<br>[123] A. G. Roy, N. Navab, and C. Wachinger, \"Recalibrating fully<br>convolutional networks with spatial and channel \"squeeze and<br>excitation\" blocks, \" IEEE transactions on medical imaging, vol. 38,<br>no. 2, pp. 540-549, 2018.<br>[124] Q. Hou, L. Zhang, M.-M. Cheng, and J. Feng, \"Strip Pooling:<br>Rethinking spatial pooling for scene parsing, in CVPR, 2020.<br>[125] H. You, Y. Feng, R. Ji, and Y. Gao, \"Pvnet: A joint convolutional<br>network of point cloud and multi-view for 3d shape recognition, \"<br>in Proceedings of the 26th ACM international conference on Multimedia,<br>2018, pp. 1310-1318.<br>[126] Q. Xie, Y.-K. Lai, J. Wu, Z. Wang, Y. Zhang, K. Xu, and J. Wang,<br>\"Mlcvnet: Multi-level context votenet for 3d object detection, in<br>Proceedings of the IEEE/CVF conference on computer vision and pattern<br>recognition, 2020, pp. 10 447-10 456.<br>[127] C. Wang, Q. Zhang, C. Huang, W. Liu, and X. Wang, \"Mancs:<br>A multi-task attentional network with curriculum sampling for</p>",
            "id": 452,
            "page": 26,
            "text": "[109] J. Lee, Y. Lee, J. Kim, A. R. Kosiorek, S. Choi, and\nY. W. Teh, \"Set transformer: A framework for attention-based\npermutation-invariant neural networks, in Proceedings of the\n36th International Conference on Machine Learning, ICML 2019,\n9-15 June 2019, Long Beach, California, uSA, ser. Proceedings of\nMachine Learning Research, K. Chaudhuri and R. Salakhutdinov,\nEds., vol. 97. PMLR, 2019, pp. 3744-3753. [Online]. Available:\nhttp:/ /proceedings.mlr.press/v97 /lee19d.html\n[110] S. Xu, Y. Cheng, K. Gu, Y. Yang, S. Chang, and P. Zhou, \"Jointly\nattentive spatial-temporal pooling networks for video-based\nperson re-identification, in Proceedings of the IEEE international\nconference on computer vision, 2017, pp. 4733-4742.\n[111] R. Zhang, J. Li, H. Sun, Y. Ge, P. Luo, X. Wang, and L. Lin,\n\"Scan: Self-and-collaborative attention network for video person\nre-identification, \" IEEE Transactions on Image Processing, vol. 28,\nno. 10, pp. 4870-4882, 2019.\n[112] D. Chen, H. Li, T. Xiao, S. Yi, and X. Wang, \"Video person re-\nidentification with competitive snippet-similarity aggregation\nand co-attentive snippet embedding, in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, 2018, pp.\n1169-1178.\n[113] R. K. Srivastava, K. Greff, and J. Schmidhuber, \"Training very\ndeep networks, arXiv preprint arXiv:1507.06228, 2015.\n[114] X. Li, W. Wang, X. Hu, and J. Yang, \"Selective kernel networks,\"\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2019, pp. 510-519.\n[115] H. Zhang, C. Wu, Z. Zhang, Y. Zhu, H. Lin, Z. Zhang, Y. Sun,\nT. He, J. Mueller, R. Manmatha, M. Li, and A. Smola, \"Resnest:\nSplit-attention networks,\" 2020.\n[116] Y. Chen, X. Dai, M. Liu, D. Chen, L. Yuan, and Z. Liu, \"Dynamic\nconvolution: Attention over convolution kernels, in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2020, pp. 11 030-11 039.\n[117] J. Park, S. Woo, J.-Y. Lee, and I. S. Kweon, \"Bam: Bottleneck\nattention module, \" 2018.\n[118] L. Yang, R.-Y. Zhang, L. Li, and X. Xie, \"Simam: A\nsimple, parameter-free attention module for convolutional\nneural networks,\" in Proceedings of the 38th International\nConference on Machine Learning, ser. Proceedings of Machine\nLearning Research, M. Meila and T. Zhang, Eds., vol. 139.\nPMLR, 18-24 Jul 2021, pp. 11 863-11 874. [Online]. Available:\nhttp:/ /pmosedings.minpres/v139/yang210.html\n[119] F. Wang, M. Jiang, C. Qian, S. Yang, C. Li, H. Zhang, X. Wang,\nand X. Tang, \"Residual attention network for image classification, \"\nin Proceedings of the IEEE conference on computer vision and pattern\nrecognition, 2017, pp. 3156-3164.\n[120] J.-J. Liu, Q. Hou, M.-M. Cheng, C. Wang, and J. Feng, \"Improving\nconvolutional networks with self-calibrated convolutions, in\nProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), June 2020.\n[121] D. Misra, T. Nalamada, A. U. Arasanipalai, and Q. Hou, \"Rotate to\nattend: Convolutional triplet attention module, \" in Proceedings of\nthe IEEE/CVF Winter Conference on Applications of Computer Vision,\n2021, pp. 3139-3148.\n[122] D. Linsley, D. Shiebler, S. Eberhardt, and T. Serre, \"Learning\nwhat and where to attend,\" in 7th International Conference on\nLearning Representations, ICLR 2019, New Orleans, LA, USA,\nMay 6-9, 2019. OpenReview.net, 2019. [Online]. Available:\nhttps:/ / openneview.net/forum?id=BIgLg3R9KQ\n[123] A. G. Roy, N. Navab, and C. Wachinger, \"Recalibrating fully\nconvolutional networks with spatial and channel \"squeeze and\nexcitation\" blocks, \" IEEE transactions on medical imaging, vol. 38,\nno. 2, pp. 540-549, 2018.\n[124] Q. Hou, L. Zhang, M.-M. Cheng, and J. Feng, \"Strip Pooling:\nRethinking spatial pooling for scene parsing, in CVPR, 2020.\n[125] H. You, Y. Feng, R. Ji, and Y. Gao, \"Pvnet: A joint convolutional\nnetwork of point cloud and multi-view for 3d shape recognition, \"\nin Proceedings of the 26th ACM international conference on Multimedia,\n2018, pp. 1310-1318.\n[126] Q. Xie, Y.-K. Lai, J. Wu, Z. Wang, Y. Zhang, K. Xu, and J. Wang,\n\"Mlcvnet: Multi-level context votenet for 3d object detection, in\nProceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, 2020, pp. 10 447-10 456.\n[127] C. Wang, Q. Zhang, C. Huang, W. Liu, and X. Wang, \"Mancs:\nA multi-task attentional network with curriculum sampling for"
        },
        {
            "bounding_box": [
                {
                    "x": 1385,
                    "y": 186
                },
                {
                    "x": 2356,
                    "y": 186
                },
                {
                    "x": 2356,
                    "y": 260
                },
                {
                    "x": 1385,
                    "y": 260
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='453' style='font-size:20px'>person re-identification, \" in Proceedings of the European Conference<br>on Computer Vision (ECCV), 2018, pp. 365-381.</p>",
            "id": 453,
            "page": 26,
            "text": "person re-identification, \" in Proceedings of the European Conference\non Computer Vision (ECCV), 2018, pp. 365-381."
        },
        {
            "bounding_box": [
                {
                    "x": 1291,
                    "y": 244
                },
                {
                    "x": 2363,
                    "y": 244
                },
                {
                    "x": 2363,
                    "y": 3111
                },
                {
                    "x": 1291,
                    "y": 3111
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='454' style='font-size:16px'>[128] T. Chen, S. Ding, J. Xie, Y. Yuan, W. Chen, Y. Yang, Z. Ren, and<br>Z. Wang, \" Abd-net: Attentive but diverse person re-identification, \"<br>in Proceedings of the IEEE/CVF International Conference on Computer<br>Vision, 2019, pp. 8351-8361.<br>[129] Q. Hou, D. Zhou, and J. Feng, \"Coordinate attention for efficient<br>mobile network design, \" in Proceedings of the IEEE/CVF Conference<br>on Computer Vision and Pattern Recognition, 2021, pp. 13713-13722.<br>[130] S. Song, C. Lan, J. Xing, W. Zeng, and J. Liu, \"An end-to-end<br>spatio-temporal attention model for human action recognition<br>from skeleton data, \" in AAAI Conference on Artificial Intelligence,<br>2017, pp. 4263-4270.<br>[131] Y. Fu, X. Wang, Y. Wei, and T. Huang, \"Sta: Spatial-<br>temporal attention for large-scale video-based person re-<br>identification, \" Proceedings of the AAAI Conference on Artificial<br>Intelligence, vol. 33, p. 8287-8294, Jul 2019. [Online]. Available:<br>http:/ /dx.doi.org/10.1609 / aaai.v33i01.33018287<br>[132] L. Gao, X. Li, J. Song, and H. T. Shen, \"Hierarchical lstms with<br>adaptive attention for visual captioning,\" IEEE transactions on<br>pattern analysis and machine intelligence, vol. 42, no. 5, pp. 1112-<br>1131, 2019.<br>[133] C. Yan, Y. Tu, X. Wang, Y. Zhang, X. Hao, Y. Zhang, and Q. Dai,<br>\"Stat: Spatial-temporal attention mechanism for video captioning,<br>IEEE Transactions on Multimedia, vol. 22, no. 1, pp. 229-241, 2020.<br>[134] L. Meng, B. Zhao, B. Chang, G. Huang, W. Sun, F. Tung, and<br>L. Sigal, \"Interpretable spatio-temporal attention for video action<br>recognition,\" 2019.<br>[135] B. He, X. Yang, Z. Wu, H. Chen, S.-N. Lim, and A. Shrivastava,<br>\"Gta: Global temporal attention for video action understanding,<br>2021.<br>[136] S. Li, S. Bak, P. Carr, and X. Wang, \"Diversity regularized<br>spatiotemporal attention for video-based person re-identification,\"<br>in Proceedings of the IEEE Conference on Computer Vision and Pattern<br>Recognition, 2018, pp. 369-378.<br>[137] Z. Zhang, C. Lan, W. Zeng, and Z. Chen, \"Multi-granularity<br>reference-aided attentive feature aggregation for video-based<br>person re-identification,\" in Proceedings of the IEEE/CVF Conference<br>on Computer Vision and Pattern Recognition, 2020, pp. 10 407-10416.<br>[138] M. Shim, H.-I. Ho, J. Kim, and D. Wee, \"Read: Reciprocal attention<br>discriminator for image-to-video re-identification, in European<br>Conference on Computer Vision. Springer, 2020, pp. 335-350.<br>[139] R. Liu, H. Deng, Y. Huang, X. Shi, L. Lu, W. Sun, X. Wang, J. Dai,<br>and H. Li, \"Decoupled spatial-temporal transformer for video<br>inpainting, \" 2021.<br>[140] S. Chaudhari, V. Mithal, G. Polatkan, and R. Ramanath, \"An<br>attentive survey of attention models, \" 2021.<br>[141] Y. Xu, H. Wei, M. Lin, Y. Deng, K. Sheng, M. Zhang, F. Tang,<br>W. Dong, F. Huang, and C. Xu, \"Transformers in computational<br>visual media: A survey,\" Computational Visual Media, vol. 8, no. 1,<br>pp. 33-62, 2022.<br>[142] K. Han, Y. Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y. Tang, A. Xiao,<br>C. Xu, Y. Xu, Z. Yang, Y. Zhang, and D. Tao, \"A survey on visual<br>transformer, \" 2021.<br>[143] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, and M. Shah,<br>\"Transformers in vision: A survey,\" 2021.<br>[144] F. Wang and D. M. J. Tax, \"Survey on the attention based rnn<br>model and its applications in computer vision, \" 2016.<br>[145] K. He, X. Zhang, S. Ren, and J. Sun, \"Deep residual learning for<br>image recognition, \" 2015.<br>[146] P. Fang, J. Zhou, S. K. Roy, L. Petersson, and M. Harandi, \"Bilinear<br>attention networks for person retrieval,\" in Proceedings of the<br>IEEE/CVF International Conference on Computer Vision, 2019, pp.<br>8030-8039.<br>[147] S. Hochreiter and J. Schmidhuber, \"Long short-term memory,\"<br>Neural Computation, vol. 9, no. 8, pp. 1735-1780, 1997.<br>[148] R. S. Sutton, D. A. McAllester, S. P. Singh, and Y. Mansour,<br>\"Policy gradient methods for reinforcement learning with function<br>approximation, \" in Advances in neural information processing systems,<br>2000, pp. 1057-1063.<br>[149] D. Bahdanau, K. Cho, and Y. Bengio, \"Neural machine translation<br>by jointly learning to align and translate, \" 2016.<br>[150] Z. Lin, M. Feng, C. N. dos Santos, M. Yu, B. Xiang, B. Zhou, and<br>Y. Bengio, \"A structured self-attentive sentence embedding, 2017.<br>[151] Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le, and R. Salakhutdi-<br>nov, \"Transformer-xl: Attentive language models beyond a fixed-<br>length context,\" 2019.</p>",
            "id": 454,
            "page": 26,
            "text": "[128] T. Chen, S. Ding, J. Xie, Y. Yuan, W. Chen, Y. Yang, Z. Ren, and\nZ. Wang, \" Abd-net: Attentive but diverse person re-identification, \"\nin Proceedings of the IEEE/CVF International Conference on Computer\nVision, 2019, pp. 8351-8361.\n[129] Q. Hou, D. Zhou, and J. Feng, \"Coordinate attention for efficient\nmobile network design, \" in Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 2021, pp. 13713-13722.\n[130] S. Song, C. Lan, J. Xing, W. Zeng, and J. Liu, \"An end-to-end\nspatio-temporal attention model for human action recognition\nfrom skeleton data, \" in AAAI Conference on Artificial Intelligence,\n2017, pp. 4263-4270.\n[131] Y. Fu, X. Wang, Y. Wei, and T. Huang, \"Sta: Spatial-\ntemporal attention for large-scale video-based person re-\nidentification, \" Proceedings of the AAAI Conference on Artificial\nIntelligence, vol. 33, p. 8287-8294, Jul 2019. [Online]. Available:\nhttp:/ /dx.doi.org/10.1609 / aaai.v33i01.33018287\n[132] L. Gao, X. Li, J. Song, and H. T. Shen, \"Hierarchical lstms with\nadaptive attention for visual captioning,\" IEEE transactions on\npattern analysis and machine intelligence, vol. 42, no. 5, pp. 1112-\n1131, 2019.\n[133] C. Yan, Y. Tu, X. Wang, Y. Zhang, X. Hao, Y. Zhang, and Q. Dai,\n\"Stat: Spatial-temporal attention mechanism for video captioning,\nIEEE Transactions on Multimedia, vol. 22, no. 1, pp. 229-241, 2020.\n[134] L. Meng, B. Zhao, B. Chang, G. Huang, W. Sun, F. Tung, and\nL. Sigal, \"Interpretable spatio-temporal attention for video action\nrecognition,\" 2019.\n[135] B. He, X. Yang, Z. Wu, H. Chen, S.-N. Lim, and A. Shrivastava,\n\"Gta: Global temporal attention for video action understanding,\n2021.\n[136] S. Li, S. Bak, P. Carr, and X. Wang, \"Diversity regularized\nspatiotemporal attention for video-based person re-identification,\"\nin Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2018, pp. 369-378.\n[137] Z. Zhang, C. Lan, W. Zeng, and Z. Chen, \"Multi-granularity\nreference-aided attentive feature aggregation for video-based\nperson re-identification,\" in Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 2020, pp. 10 407-10416.\n[138] M. Shim, H.-I. Ho, J. Kim, and D. Wee, \"Read: Reciprocal attention\ndiscriminator for image-to-video re-identification, in European\nConference on Computer Vision. Springer, 2020, pp. 335-350.\n[139] R. Liu, H. Deng, Y. Huang, X. Shi, L. Lu, W. Sun, X. Wang, J. Dai,\nand H. Li, \"Decoupled spatial-temporal transformer for video\ninpainting, \" 2021.\n[140] S. Chaudhari, V. Mithal, G. Polatkan, and R. Ramanath, \"An\nattentive survey of attention models, \" 2021.\n[141] Y. Xu, H. Wei, M. Lin, Y. Deng, K. Sheng, M. Zhang, F. Tang,\nW. Dong, F. Huang, and C. Xu, \"Transformers in computational\nvisual media: A survey,\" Computational Visual Media, vol. 8, no. 1,\npp. 33-62, 2022.\n[142] K. Han, Y. Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y. Tang, A. Xiao,\nC. Xu, Y. Xu, Z. Yang, Y. Zhang, and D. Tao, \"A survey on visual\ntransformer, \" 2021.\n[143] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, and M. Shah,\n\"Transformers in vision: A survey,\" 2021.\n[144] F. Wang and D. M. J. Tax, \"Survey on the attention based rnn\nmodel and its applications in computer vision, \" 2016.\n[145] K. He, X. Zhang, S. Ren, and J. Sun, \"Deep residual learning for\nimage recognition, \" 2015.\n[146] P. Fang, J. Zhou, S. K. Roy, L. Petersson, and M. Harandi, \"Bilinear\nattention networks for person retrieval,\" in Proceedings of the\nIEEE/CVF International Conference on Computer Vision, 2019, pp.\n8030-8039.\n[147] S. Hochreiter and J. Schmidhuber, \"Long short-term memory,\"\nNeural Computation, vol. 9, no. 8, pp. 1735-1780, 1997.\n[148] R. S. Sutton, D. A. McAllester, S. P. Singh, and Y. Mansour,\n\"Policy gradient methods for reinforcement learning with function\napproximation, \" in Advances in neural information processing systems,\n2000, pp. 1057-1063.\n[149] D. Bahdanau, K. Cho, and Y. Bengio, \"Neural machine translation\nby jointly learning to align and translate, \" 2016.\n[150] Z. Lin, M. Feng, C. N. dos Santos, M. Yu, B. Xiang, B. Zhou, and\nY. Bengio, \"A structured self-attentive sentence embedding, 2017.\n[151] Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le, and R. Salakhutdi-\nnov, \"Transformer-xl: Attentive language models beyond a fixed-\nlength context,\" 2019."
        },
        {
            "bounding_box": [
                {
                    "x": 192,
                    "y": 110
                },
                {
                    "x": 1075,
                    "y": 110
                },
                {
                    "x": 1075,
                    "y": 148
                },
                {
                    "x": 192,
                    "y": 148
                }
            ],
            "category": "header",
            "html": "<header id='455' style='font-size:16px'>JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015</header>",
            "id": 455,
            "page": 27,
            "text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015"
        },
        {
            "bounding_box": [
                {
                    "x": 2312,
                    "y": 113
                },
                {
                    "x": 2353,
                    "y": 113
                },
                {
                    "x": 2353,
                    "y": 145
                },
                {
                    "x": 2312,
                    "y": 145
                }
            ],
            "category": "header",
            "html": "<br><header id='456' style='font-size:14px'>27</header>",
            "id": 456,
            "page": 27,
            "text": "27"
        },
        {
            "bounding_box": [
                {
                    "x": 194,
                    "y": 165
                },
                {
                    "x": 1276,
                    "y": 165
                },
                {
                    "x": 1276,
                    "y": 3110
                },
                {
                    "x": 194,
                    "y": 3110
                }
            ],
            "category": "paragraph",
            "html": "<p id='457' style='font-size:20px'>[152] K. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane,<br>T. Sarlos, P. Hawkins, J. Davis, A. Mohiuddin, L. Kaiser, D. Be-<br>langer, L. Colwell, and A. Weller, \"Rethinking attention with<br>performers, \" 2021.<br>[153] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, \"Deformable detr:<br>Deformable transformers for end-to-end object detection,\" 2021.<br>[154] W. Liu, A. Rabinovich, and A. C. Berg, \"Parsenet: Looking wider<br>to see better, \" 2015.<br>[155] C. Peng, X. Zhang, G. Yu, G. Luo, and J. Sun, \"Large kernel<br>matters - improve semantic segmentation by global convolutional<br>network, \" in Proceedings of the IEEE Conference on Computer Vision<br>and Pattern Recognition (CVPR), July 2017.<br>[156] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, \"Pyramid scene parsing<br>network, \" in 2017 IEEE Conference on Computer Vision and Pattern<br>Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017.<br>IEEE Computer Society, 2017, pp. 6230-6239. [Online]. Available:<br>https:/ /doi.org/10.1109/CVPR.2017.660<br>[157] K. He, X. Zhang, S. Ren, and J. Sun, \"Spatial pyramid pooling<br>in deep convolutional networks for visual recognition, \" Lecture<br>Notes in Computer Science, p. 346-361, 2014. [Online]. Available:<br>http:/ / dx.doi.org/ 10.1007 /978-3-319-10578-9 23<br>[158] I. Tolstikhin, N. Houlsby, A. Kolesnikov, L. Beyer, X. Zhai, T. Un-<br>terthiner,J. Yung, A. Steiner, D. Keysers,J. Uszkoreit, M. Lucic, and<br>A. Dosovitskiy, \"Mlp-mixer: An all-mlp architecture for vision,\"<br>2021.<br>[159] H. Touvron, P. Bojanowski, M. Caron, M. Cord, A. El-Nouby,<br>E. Grave, G. Izacard, A. Joulin, G. Synnaeve, J. Verbeek, and<br>H. Jegou, \"Resmlp: Feedforward networks for image classification<br>with data-efficient training, \" 2021.<br>[160] P. Shaw, J. Uszkoreit, and A. Vaswani, \"Self-attention with relative<br>position representations, \" 2018.<br>[161] T. B. Brown, B. Mann, N. Ryder, M. Subbiah,J. Kaplan, P. Dhariwal,<br>A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal,<br>A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh,<br>D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler,<br>M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish,<br>A. Radford, I. Sutskever, and D. Amodei, \"Language models are<br>few-shot learners, \" 2020.<br>[162] J. L. Ba, J. R. Kiros, and G. E. Hinton, \"Layer normalization, \" 2016.<br>[163] D. Hendrycks and K. Gimpel, \"Gaussian error linear units (gelus),\"<br>2020.<br>[164] C. Sun, A. Shrivastava, S. Singh, and A. Gupta, \"Revisiting<br>unreasonable effectiveness of data in deep learning era, 2017.<br>[165] J. Deng, W. Dong, R. Socher, L. Li, K. Li, and F. Li, \"Imagenet:<br>A large-scale hierarchical image database, \" in CVPR, 2009, pp.<br>248-255.<br>[166] D. Zhou, B. Kang, X. Jin, L. Yang, X. Lian, Z. Jiang, Q. Hou, and<br>J. Feng, \"Deepvit: Towards deeper vision transformer,\" 2021.<br>[167] H. Touvron, M. Cord, A. Sablayrolles, G. Synnaeve, and H. Jegou,<br>\"Going deeper with image transformers, \" 2021.<br>[168] R. Liu, H. Deng, Y. Huang, X. Shi, L. Lu, W. Sun, X. Wang,<br>J. Dai, and H. Li, \"Fuseformer: Fusing fine-grained information in<br>transformers for video inpainting, 2021.<br>[169] K. He, X. Chen, S. Xie, Y. Li, P. Dollar, and R. Girshick, \"Masked<br>autoencoders are scalable vision learners, \" 2021.<br>[170] M.-H. Guo, Z.-N. Liu, T.-J. Mu, D. Liang, R. R. Martin, and S.-M.<br>Hu, \"Can attention enable mlps to catch up with cnns?\" 2021.<br>[171] J. Li, J. Wang, Q. Tian, W. Gao, and S. Zhang, \"Global-local<br>temporal representations for video person re-identification, \" in<br>Proceedings of the IEEE/CVF International Conference on Computer<br>Vision, 2019, pp. 3958-3967.<br>[172] Z. Liu, L. Wang, W. Wu, C. Qian, and T. Lu, \"Tam: Temporal adap-<br>tive module for video recognition,\" arXiv preprint arXiv:2005.06803,<br>2020.<br>[173] B. Yang, G. Bender, Q. V. Le, and J. Ngiam, \"Condconv: Condi-<br>tionally parameterized convolutions for efficient inference,\" 2020.<br>[174] L. Spillmann, B. Dresp-Langley, and C.-H. Tseng, \"Beyond the<br>classical receptive field: the effect of contextual stimuli, Journal of<br>Vision, vol. 15, no. 9, pp. 7-7, 2015.<br>[175] S. Xie, R. Girshick, P. Dollar, Z. Tu, and K. He, \"Aggregated<br>residual transformations for deep neural networks,\" 2017.<br>[176] W. BS, D. NT, S. SG, T. C, and L. P, \"Early and late mechanisms of<br>surround suppression in striate cortex of macaque, 2005.<br>[177] J. Yang, W.-S. Zheng, Q. Yang, Y.-C. Chen, and Q. Tian, \"Spatial-<br>temporal graph convolutional network for video-based person<br>re-identification, in Proceedings of the IEEE/CVF Conference on<br>Computer Vision and Pattern Recognition, 2020, pp. 3289-3299.</p>",
            "id": 457,
            "page": 27,
            "text": "[152] K. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane,\nT. Sarlos, P. Hawkins, J. Davis, A. Mohiuddin, L. Kaiser, D. Be-\nlanger, L. Colwell, and A. Weller, \"Rethinking attention with\nperformers, \" 2021.\n[153] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, \"Deformable detr:\nDeformable transformers for end-to-end object detection,\" 2021.\n[154] W. Liu, A. Rabinovich, and A. C. Berg, \"Parsenet: Looking wider\nto see better, \" 2015.\n[155] C. Peng, X. Zhang, G. Yu, G. Luo, and J. Sun, \"Large kernel\nmatters - improve semantic segmentation by global convolutional\nnetwork, \" in Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), July 2017.\n[156] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, \"Pyramid scene parsing\nnetwork, \" in 2017 IEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017.\nIEEE Computer Society, 2017, pp. 6230-6239. [Online]. Available:\nhttps:/ /doi.org/10.1109/CVPR.2017.660\n[157] K. He, X. Zhang, S. Ren, and J. Sun, \"Spatial pyramid pooling\nin deep convolutional networks for visual recognition, \" Lecture\nNotes in Computer Science, p. 346-361, 2014. [Online]. Available:\nhttp:/ / dx.doi.org/ 10.1007 /978-3-319-10578-9 23\n[158] I. Tolstikhin, N. Houlsby, A. Kolesnikov, L. Beyer, X. Zhai, T. Un-\nterthiner,J. Yung, A. Steiner, D. Keysers,J. Uszkoreit, M. Lucic, and\nA. Dosovitskiy, \"Mlp-mixer: An all-mlp architecture for vision,\"\n2021.\n[159] H. Touvron, P. Bojanowski, M. Caron, M. Cord, A. El-Nouby,\nE. Grave, G. Izacard, A. Joulin, G. Synnaeve, J. Verbeek, and\nH. Jegou, \"Resmlp: Feedforward networks for image classification\nwith data-efficient training, \" 2021.\n[160] P. Shaw, J. Uszkoreit, and A. Vaswani, \"Self-attention with relative\nposition representations, \" 2018.\n[161] T. B. Brown, B. Mann, N. Ryder, M. Subbiah,J. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal,\nA. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh,\nD. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler,\nM. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish,\nA. Radford, I. Sutskever, and D. Amodei, \"Language models are\nfew-shot learners, \" 2020.\n[162] J. L. Ba, J. R. Kiros, and G. E. Hinton, \"Layer normalization, \" 2016.\n[163] D. Hendrycks and K. Gimpel, \"Gaussian error linear units (gelus),\"\n2020.\n[164] C. Sun, A. Shrivastava, S. Singh, and A. Gupta, \"Revisiting\nunreasonable effectiveness of data in deep learning era, 2017.\n[165] J. Deng, W. Dong, R. Socher, L. Li, K. Li, and F. Li, \"Imagenet:\nA large-scale hierarchical image database, \" in CVPR, 2009, pp.\n248-255.\n[166] D. Zhou, B. Kang, X. Jin, L. Yang, X. Lian, Z. Jiang, Q. Hou, and\nJ. Feng, \"Deepvit: Towards deeper vision transformer,\" 2021.\n[167] H. Touvron, M. Cord, A. Sablayrolles, G. Synnaeve, and H. Jegou,\n\"Going deeper with image transformers, \" 2021.\n[168] R. Liu, H. Deng, Y. Huang, X. Shi, L. Lu, W. Sun, X. Wang,\nJ. Dai, and H. Li, \"Fuseformer: Fusing fine-grained information in\ntransformers for video inpainting, 2021.\n[169] K. He, X. Chen, S. Xie, Y. Li, P. Dollar, and R. Girshick, \"Masked\nautoencoders are scalable vision learners, \" 2021.\n[170] M.-H. Guo, Z.-N. Liu, T.-J. Mu, D. Liang, R. R. Martin, and S.-M.\nHu, \"Can attention enable mlps to catch up with cnns?\" 2021.\n[171] J. Li, J. Wang, Q. Tian, W. Gao, and S. Zhang, \"Global-local\ntemporal representations for video person re-identification, \" in\nProceedings of the IEEE/CVF International Conference on Computer\nVision, 2019, pp. 3958-3967.\n[172] Z. Liu, L. Wang, W. Wu, C. Qian, and T. Lu, \"Tam: Temporal adap-\ntive module for video recognition,\" arXiv preprint arXiv:2005.06803,\n2020.\n[173] B. Yang, G. Bender, Q. V. Le, and J. Ngiam, \"Condconv: Condi-\ntionally parameterized convolutions for efficient inference,\" 2020.\n[174] L. Spillmann, B. Dresp-Langley, and C.-H. Tseng, \"Beyond the\nclassical receptive field: the effect of contextual stimuli, Journal of\nVision, vol. 15, no. 9, pp. 7-7, 2015.\n[175] S. Xie, R. Girshick, P. Dollar, Z. Tu, and K. He, \"Aggregated\nresidual transformations for deep neural networks,\" 2017.\n[176] W. BS, D. NT, S. SG, T. C, and L. P, \"Early and late mechanisms of\nsurround suppression in striate cortex of macaque, 2005.\n[177] J. Yang, W.-S. Zheng, Q. Yang, Y.-C. Chen, and Q. Tian, \"Spatial-\ntemporal graph convolutional network for video-based person\nre-identification, in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2020, pp. 3289-3299."
        },
        {
            "bounding_box": [
                {
                    "x": 1297,
                    "y": 180
                },
                {
                    "x": 2363,
                    "y": 180
                },
                {
                    "x": 2363,
                    "y": 1206
                },
                {
                    "x": 1297,
                    "y": 1206
                }
            ],
            "category": "paragraph",
            "html": "<br><p id='458' style='font-size:20px'>[178] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,<br>D. Erhan, V. Vanhoucke, and A. Rabinovich, \"Going deeper with<br>convolutions, \" 2014.<br>[179] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal,<br>A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal,<br>A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh,<br>D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler,<br>M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish,<br>A. Radford, I. Sutskever, and D. Amodei, \"Language models are<br>few-shot learners, \" 2020.<br>[180] M. Caron, H. Touvron, I. Misra, H. Jegou, J. Mairal, P. Bojanowski,<br>and A. Joulin, \"Emerging properties in self-supervised vision<br>transformers, \" 2021.<br>[181] N. Qian, \"On the momentum term in gradient descent learning<br>algorithms. \" Neural Networks, vol. 12, no. 1, pp. 145-151, 1999.<br>[Online]. Available: http:/ / dblp.uni-trier.de/ db /journals / nn/<br>nn12.html#Qian99<br>[182] D. P. Kingma and J. Ba, \"Adam: A method for stochastic optimiza-<br>tion, \" 2017.<br>[183] I. Loshchilov and F. Hutter, \"Decoupled weight decay regulariza-<br>tion, \" 2019.<br>[184] X. Chen, C.-J. Hsieh, and B. Gong, \"When vision transformers<br>outperform resnets without pretraining or strong data augmenta-<br>tions,\" 2021.<br>[185] P. Foret, A. Kleiner, H. Mobahi, and B. Neyshabur, \"Sharpness-<br>aware minimization for efficiently improving generalization,\"<br>2021.</p>",
            "id": 458,
            "page": 27,
            "text": "[178] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov,\nD. Erhan, V. Vanhoucke, and A. Rabinovich, \"Going deeper with\nconvolutions, \" 2014.\n[179] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal,\nA. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh,\nD. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler,\nM. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish,\nA. Radford, I. Sutskever, and D. Amodei, \"Language models are\nfew-shot learners, \" 2020.\n[180] M. Caron, H. Touvron, I. Misra, H. Jegou, J. Mairal, P. Bojanowski,\nand A. Joulin, \"Emerging properties in self-supervised vision\ntransformers, \" 2021.\n[181] N. Qian, \"On the momentum term in gradient descent learning\nalgorithms. \" Neural Networks, vol. 12, no. 1, pp. 145-151, 1999.\n[Online]. Available: http:/ / dblp.uni-trier.de/ db /journals / nn/\nnn12.html#Qian99\n[182] D. P. Kingma and J. Ba, \"Adam: A method for stochastic optimiza-\ntion, \" 2017.\n[183] I. Loshchilov and F. Hutter, \"Decoupled weight decay regulariza-\ntion, \" 2019.\n[184] X. Chen, C.-J. Hsieh, and B. Gong, \"When vision transformers\noutperform resnets without pretraining or strong data augmenta-\ntions,\" 2021.\n[185] P. Foret, A. Kleiner, H. Mobahi, and B. Neyshabur, \"Sharpness-\naware minimization for efficiently improving generalization,\"\n2021."
        }
    ]
}